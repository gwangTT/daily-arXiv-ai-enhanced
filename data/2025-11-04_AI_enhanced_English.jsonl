{"id": "2511.00075", "pdf": "https://arxiv.org/pdf/2511.00075", "abs": "https://arxiv.org/abs/2511.00075", "authors": ["Qianhui Li", "Weiya Wang", "Qianqi Zhao", "Tong Qu", "Jing He", "Xuhong Qiang", "Jingwen Hou", "Ke Chen", "Bao Zhang", "Qi Wang"], "title": "PDA-LSTM: Knowledge-driven page data arrangement based on LSTM for LCM supression in QLC 3D NAND flash memories", "categories": ["cs.AR", "cs.LG"], "comment": null, "summary": "Quarter level cell (QLC) 3D NAND flash memory is emerging as the predominant\nstorage solution in the era of artificial intelligence. QLC 3D NAND flash\nstores 4 bit per cell to expand the storage density, resulting in narrower read\nmargins. Constrained to read margins, QLC always suffers from lateral charge\nmigration (LCM), which caused by non-uniform charge density across adjacent\nmemory cells. To suppress charge density gap between cells, there are some\nalgorithm in form of intra-page data mapping such as WBVM, DVDS. However, we\nobserve inter-page data arrangements also approach the suppression. Thus, we\nproposed an intelligent model PDA-LSTM to arrange intra-page data for LCM\nsuppression, which is a physics-knowledge-driven neural network model. PDA-LSTM\napplies a long-short term memory (LSTM) neural network to compute a data\narrangement probability matrix from input page data pattern. The arrangement is\nto minimize the global impacts derived from the LCM among wordlines. Since each\npage data can be arranged only once, we design a transformation from output\nmatrix of LSTM network to non-repetitive sequence generation probability matrix\nto assist training process. The arranged data pattern can decrease the bit\nerror rate (BER) during data retention. In addition, PDA-LSTM do not need extra\nflag bits to record data transport of 3D NAND flash compared with WBVM, DVDS.\nThe experiment results show that the PDA-LSTM reduces the average BER by 80.4%\ncompared with strategy without data arrangement, and by 18.4%, 15.2% compared\nrespectively with WBVM and DVDS with code-length 64.", "AI": {"tldr": "The paper introduces PDA-LSTM, a neural network-based model, addressing lateral charge migration (LCM) in QLC 3D NAND flash memory by optimizing inter-page data arrangements, resulting in reduced bit error rates.", "motivation": "QLC 3D NAND flash memory faces challenges in artificial intelligence storage due to narrow read margins and lateral charge migration (LCM) effects, necessitating efficient data arrangement strategies.", "method": "PDA-LSTM uses a physics-informed LSTM neural network to compute probabilities for data arrangement, reducing LCM effects without adding extra flag bits, unlike WBVM and DVDS algorithms.", "result": "PDA-LSTM achieves an 80.4% reduction in average bit error rate (BER) compared to no arrangement and outperforms WBVM and DVDS by 18.4% and 15.2%, respectively.", "conclusion": "The proposed PDA-LSTM demonstrates significant improvements in error management for QLC 3D NAND storage, making it a competitive alternative to existing data arrangement technologies."}}
{"id": "2511.00295", "pdf": "https://arxiv.org/pdf/2511.00295", "abs": "https://arxiv.org/abs/2511.00295", "authors": ["Kosmas Alexandridis", "Giorgos Dimitrakopoulos"], "title": "H-FA: A Hybrid Floating-Point and Logarithmic Approach to Hardware Accelerated FlashAttention", "categories": ["cs.AR"], "comment": "Accepted for publication at IEEE Transactions on Circuits and Systems\n  for Artificial Intelligence", "summary": "Transformers have significantly advanced AI and machine learning through\ntheir powerful attention mechanism. However, computing attention on long\nsequences can become a computational bottleneck. FlashAttention mitigates this\nby fusing the softmax and matrix operations into a tiled computation pattern\nthat decouples performance from sequence length. Though designed for GPUs, its\nsimplicity also makes it well suited for direct hardware acceleration. To\nimprove hardware implementation, we compute FlashAttention using a mixture of\nfloating-point and fixed-point logarithm domain representations. Floating-point\nis used to compute attention scores from query and key matrices, while\nlogarithmic computation simplifies the fused computation of softmax\nnormalization and the multiplication with the value matrix. This\ntransformation, called H-FA, replaces vector-wide floating-point multiplication\nand division operations by additions and subtractions implemented efficiently\nwith fixed-point arithmetic in the logarithm domain. Exponential function\nevaluations are effectively omitted and fused with the rest operations, and the\nfinal result is directly returned to floating-point arithmetic without any\nadditional hardware overhead. Hardware implementation results at 28nm\ndemonstrate that H-FA achieves a 26.5% reduction in area and a 23.4% reduction\nin power, on average, compared to FlashAttention parallel hardware\narchitectures built solely with floating-point datapaths, without hindering\nperformance.", "AI": {"tldr": "The paper introduces H-FA, an optimized hardware implementation of FlashAttention that combines floating-point and fixed-point operations in the logarithm domain, achieving significant reductions in area and power while maintaining performance.", "motivation": "FlashAttention, designed for GPUs, shows potential for hardware acceleration but faces inefficiencies in computational resource usage for long sequences.", "method": "The authors propose H-FA, which utilizes floating-point arithmetic for initial attention computations and fixed-point logarithmic representations to simplify softmax normalization and fused matrix operations. The approach replaces complex computations with efficient arithmetic operations.", "result": "The hardware implementation of H-FA at 28nm demonstrates 26.5% area reduction and 23.4% power savings compared to traditional floating-point FlashAttention architectures, with no performance degradation.", "conclusion": "H-FA successfully addresses key computation inefficiencies in hardware implementation of FlashAttention, offering a more resource-efficient solution without compromising performance."}}
{"id": "2511.00321", "pdf": "https://arxiv.org/pdf/2511.00321", "abs": "https://arxiv.org/abs/2511.00321", "authors": ["Dowon Kim", "MinJae Lee", "Janghyeon Kim", "HyuckSung Kwon", "Hyeonggyu Jeong", "Sang-Soo Park", "Minyong Yoon", "Si-Dong Roh", "Yongsuk Kwon", "Jinin So", "Jungwook Choi"], "title": "Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled KV-Cache Management Beyond GPU Limits", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "The expansion of context windows in large language models (LLMs) to\nmulti-million tokens introduces severe memory and compute bottlenecks,\nparticularly in managing the growing Key-Value (KV) cache. While Compute\nExpress Link (CXL) enables non-eviction frameworks that offload the full\nKV-cache to scalable external memory, these frameworks still suffer from costly\ndata transfers when recalling non-resident KV tokens to limited GPU memory as\ncontext lengths increase. This work proposes scalable Processing-Near-Memory\n(PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that\ncoordinates memory and computation beyond GPU limits. Our design offloads token\npage selection to a PNM accelerator within CXL memory, eliminating costly\nrecalls and enabling larger GPU batch sizes. We further introduce a hybrid\nparallelization strategy and a steady-token selection mechanism to enhance\ncompute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM\nsystem, our solution delivers consistent performance gains for LLMs with up to\n405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV)\nand GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x\nthroughput improvement, up to 60x lower energy per token, and up to 7.3x better\ntotal cost efficiency than the baseline, demonstrating that CXL-enabled\nmulti-PNM architectures can serve as a scalable backbone for future\nlong-context LLM inference.", "AI": {"tldr": "This paper presents a scalable Processing-Near-Memory (PNM) system for managing large context windows in large language models (LLMs), significantly improving efficiency and scalability.", "motivation": "The study aims to address the memory and compute bottlenecks introduced by expanding context windows in LLMs to multi-million tokens, particularly in efficiently managing the Key-Value (KV) cache.", "method": "The paper introduces a novel PNM accelerator integrated with Compute Express Link (CXL) memory for managing KV-cache. It offloads token page selection to the PNM accelerator, eliminating costly data recalls and enabling larger batch sizes, while incorporating advanced hybrid parallelization and steady-token selection mechanisms.", "result": "The proposed system achieves up to 21.9x throughput improvement, up to 60x lower energy per token, and up to 7.3x higher cost efficiency in handling LLMs with up to 405 billion parameters and 1-million-token contexts.", "conclusion": "The results demonstrate that CXL-enabled PNM architectures are a promising scalable backbone for future long-context LLM inference tasks, addressing key bottlenecks in computation and memory management."}}
{"id": "2511.01244", "pdf": "https://arxiv.org/pdf/2511.01244", "abs": "https://arxiv.org/abs/2511.01244", "authors": ["Wajid Ali", "Ayaz Akram", "Deepak Shankar"], "title": "Simulation-Driven Evaluation of Chiplet-Based Architectures Using VisualSim", "categories": ["cs.AR", "cs.PF"], "comment": null, "summary": "This paper focuses on the simulation of multi-die System-on-Chip (SoC)\narchitectures using VisualSim, emphasiz- ing chiplet-based system modeling and\nperformance analysis. Chiplet technology presents a promising alternative to\ntraditional monolithic chips, which face increasing challenges in manufactur-\ning costs, power efficiency, and performance scaling. By integrat- ing multiple\nsmall modular silicon units into a single package, chiplet-based architectures\noffer greater flexibility and scalability at a lower overall cost. In this\nstudy, we developed a detailed sim- ulation model of a chiplet-based system,\nincorporating multicore ARM processor clusters interconnected through a ARM\nCMN600 network-on-chip (NoC) for efficient communication [4], [7]. The\nsimulation framework in VisualSim enables the evaluation of critical system\nmetrics, including inter-chiplet communication latency, memory access\nefficiency, workload distribution, and the power-performance tradeoff under\nvarious workloads. Through simulation-driven insights, this research highlights\nkey factors influencing chiplet system performance and provides a foundation\nfor optimizing future chiplet-based semiconductor designs.", "AI": {"tldr": "This study explores chiplet-based system simulation using VisualSim, focusing on scalability, performance, and costs.", "motivation": "Chiplet-based systems address issues in monolithic chips like manufacturing challenges, power efficiency, and scaling limitations.", "method": "A detailed simulation model of chiplet-based systems was built in VisualSim, including ARM multicore processors and ARM CMN600 NoC for performance evaluations.", "result": "Critical metrics, such as inter-chiplet communication latency, memory efficiency, workload distribution, were analyzed, providing performance insights.", "conclusion": "Simulation insights can optimize chiplet-based architectures for improved semiconductor designs in future."}}
{"id": "2511.00042", "pdf": "https://arxiv.org/pdf/2511.00042", "abs": "https://arxiv.org/abs/2511.00042", "authors": ["Sreeja Singh", "Tamal Ghosh"], "title": "Bio-Inspired Neuron Synapse Optimization for Adaptive Learning and Smart Decision-Making", "categories": ["cs.NE"], "comment": "9 pages, 5 figures", "summary": "Purpose: Optimization challenges in science, engineering, and real-world\napplications often involve complex, high-dimensional, and multimodal search\nspaces. Traditional optimization methods frequently struggle with local optima\nentrapment, slow convergence, and inefficiency in large-scale environments.\nThis study aims to address these limitations by proposing a novel optimization\nalgorithm inspired by neural mechanisms. Design/methodology/approach: The paper\nintroduces Neuron Synapse Optimization (NSO), a new metaheuristic algorithm\ninspired by neural interactions. NSO features key innovations such as\nfitness-based synaptic weight updates to improve search influence, adaptive\npruning to minimize computational overhead, and dual guidance from global and\nlocal best solutions to balance exploration and exploitation. The algorithm was\nbenchmarked against popular metaheuristics and the recently published\nHippopotamus Optimization Algorithm (HOA) using the CEC 2014 test suite,\nencompassing unimodal, multimodal, and composition function landscapes.\nFindings: Benchmark results reveal that NSO consistently outperforms HOA and\nother major algorithms in terms of convergence speed, robustness, and\nscalability. NSO demonstrates superior adaptability and efficiency,\nparticularly in complex, high-dimensional search spaces. Originality: NSO\nintroduces a unique blend of neural-inspired mechanisms with dynamic resource\nallocation, setting it apart from existing algorithms. Its innovative design\nenhances search performance while reducing computational cost. With promising\napplications in technology, healthcare, data science, and engineering, NSO\npaves the way for future research into dynamic and multi-objective\noptimization, machine learning hyperparameter tuning, and real-world\nengineering design problems.", "AI": {"tldr": "Neuron Synapse Optimization (NSO) algorithm outperforms existing optimization methods in complex, high-dimensional search spaces, combining neural-inspired mechanisms with dynamic resource allocation for enhanced efficiency.", "motivation": "Challenges exist in traditional optimization methods that struggle with local optima entrapment, slow convergence, and inefficiencies in large-scale environments, necessitating advanced optimization techniques.", "method": "The proposed NSO algorithm is inspired by neural mechanisms, featuring fitness-based synaptic weight updates, adaptive pruning, and dual guidance, tested against major algorithms using complex benchmarking environments.", "result": "Benchmarking shows NSO exceeds performance of major algorithms like HOA, offering improved scalability, robustness, and efficiency in optimization tasks.", "conclusion": "NSO contributes a novel neural-inspired optimization approach with superior adaptability and resource management, demonstrating potential across various fields like engineering and machine learning."}}
{"id": "2511.00020", "pdf": "https://arxiv.org/pdf/2511.00020", "abs": "https://arxiv.org/abs/2511.00020", "authors": ["Suhasnadh Reddy Veluru", "Sai Teja Erukude", "Viswa Chaitanya Marella"], "title": "Multimodal Detection of Fake Reviews using BERT and ResNet-50", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "Published in IEEE", "summary": "In the current digital commerce landscape, user-generated reviews play a\ncritical role in shaping consumer behavior, product reputation, and platform\ncredibility. However, the proliferation of fake or misleading reviews often\ngenerated by bots, paid agents, or AI models poses a significant threat to\ntrust and transparency within review ecosystems. Existing detection models\nprimarily rely on unimodal, typically textual, data and therefore fail to\ncapture semantic inconsistencies across different modalities. To address this\ngap, a robust multimodal fake review detection framework is proposed,\nintegrating textual features encoded with BERT and visual features extracted\nusing ResNet-50. These representations are fused through a classification head\nto jointly predict review authenticity. To support this approach, a curated\ndataset comprising 21,142 user-uploaded images across food delivery,\nhospitality, and e-commerce domains was utilized. Experimental results indicate\nthat the multimodal model outperforms unimodal baselines, achieving an F1-score\nof 0.934 on the test set. Additionally, the confusion matrix and qualitative\nanalysis highlight the model's ability to detect subtle inconsistencies, such\nas exaggerated textual praise paired with unrelated or low-quality images,\ncommonly found in deceptive content. This study demonstrates the critical role\nof multimodal learning in safeguarding digital trust and offers a scalable\nsolution for content moderation across various online platforms.", "AI": {"tldr": "This paper presents a multimodal framework for detecting fake reviews using BERT and ResNet-50 to integrate textual and visual features, achieving an impressive F1-score of 0.934.", "motivation": "The paper addresses the challenge posed by fake or misleading reviews, which undermine trust and transparency in digital commerce.", "method": "Proposed a multimodal fake review detection framework utilizing textual features from BERT and visual features from ResNet-50, followed by fusion through a classification head.", "result": "Achieved an F1-score of 0.934 on the test set showing superior performance over unimodal baselines. The model successfully identifies inconsistencies in deceptive content.", "conclusion": "The research emphasizes the importance of multimodal learning for robust fake review detection and offers scalable solutions for maintaining digital trust on online platforms."}}
{"id": "2511.00038", "pdf": "https://arxiv.org/pdf/2511.00038", "abs": "https://arxiv.org/abs/2511.00038", "authors": ["Suman Raj", "Radhika Mittal", "Rajiv Mayani", "Pawel Zuk", "Anirban Mandal", "Michael Zink", "Yogesh Simmhan", "Ewa Deelman"], "title": "AeroResQ: Edge-Accelerated UAV Framework for Scalable, Resilient and Collaborative Escape Route Planning in Wildfire Scenarios", "categories": ["cs.DC", "cs.RO"], "comment": "26 pages, 11 figures", "summary": "Drone fleets equipped with onboard cameras, computer vision, and Deep Neural\nNetwork (DNN) models present a powerful paradigm for real-time spatio-temporal\ndecision-making. In wildfire response, such drones play a pivotal role in\nmonitoring fire dynamics, supporting firefighter coordination, and facilitating\nsafe evacuation. In this paper, we introduce AeroResQ, an edge-accelerated UAV\nframework designed for scalable, resilient, and collaborative escape route\nplanning during wildfire scenarios. AeroResQ adopts a multi-layer orchestration\narchitecture comprising service drones (SDs) and coordinator drones (CDs), each\nperforming specialized roles. SDs survey fire-affected areas, detect stranded\nindividuals using onboard edge accelerators running fire detection and human\npose identification DNN models, and issue requests for assistance. CDs,\nequipped with lightweight data stores such as Apache IoTDB, dynamically\ngenerate optimal ground escape routes and monitor firefighter movements along\nthese routes. The framework proposes a collaborative path-planning approach\nbased on a weighted A* search algorithm, where CDs compute context-aware escape\npaths. AeroResQ further incorporates intelligent load-balancing and resilience\nmechanisms: CD failures trigger automated data redistribution across IoTDB\nreplicas, while SD failures initiate geo-fenced re-partitioning and\nreassignment of spatial workloads to operational SDs. We evaluate AeroResQ\nusing realistic wildfire emulated setup modeled on recent Southern California\nwildfires. Experimental results demonstrate that AeroResQ achieves a nominal\nend-to-end latency of <=500ms, much below the 2s request interval, while\nmaintaining over 98% successful task reassignment and completion, underscoring\nits feasibility for real-time, on-field deployment in emergency response and\nfirefighter safety operations.", "AI": {"tldr": "AeroResQ is an edge-accelerated UAV framework for real-time wildfire response, focusing on scalable and collaborative escape route planning. It utilizes specialized drones and advanced algorithms for efficient monitoring and coordination.", "motivation": "To address the critical need for real-time, scalable, and resilient systems that enable safe and efficient evacuation and firefighter coordination during wildfires.", "method": "AeroResQ employs a multi-layer architecture of service drones (SDs) and coordinator drones (CDs), utilizing edge accelerators, DNN models, and IoTDB for efficient decision-making. A weighted A* algorithm is used for escape route planning.", "result": "AeroResQ shows latency below 500ms, high task reassignment success (over 98%), and feasibility for real-time deployment in wildfire scenarios.", "conclusion": "AeroResQ is an effective tool for improving emergency response times and ensuring firefighter and civilian safety in wildfire situations, validated through realistic experimental setups."}}
{"id": "2511.00403", "pdf": "https://arxiv.org/pdf/2511.00403", "abs": "https://arxiv.org/abs/2511.00403", "authors": ["Wentao Peng", "Ruyi Ji", "Yingfei Xiong"], "title": "Equality Saturation Guided by Large Language Models", "categories": ["cs.PL"], "comment": "presented at EGRAPHS 2025", "summary": "One critical issue with large language models (LLMs) is their inability to\nguarantee correctness. Although this problem can be addressed by applying LLMs\nto formal rewrite systems, current LLMs are still far from adequate to generate\nsound rewrite chains. To bridge this gap, this paper proposes LLM-guided\nequality saturation, dubbed LGuess, by incorporating e-graphs as an\nintermediate layer between LLMs and rewrite systems. LGuess queries LLMs only\nfor high-level rewrite checkpoints and uses e-graphs to supply low-level\nrewrite chains between these checkpoints. The key technical challenge in this\nprocedure lies in effectively extracting a suitable checkpoint from a saturated\ne-graph, which LGuess addresses by learning a probabilistic model from the LLM.\nThe model predicts probable checkpoints while remaining simple enough for\neffective extraction. We implement a prototype of LGuess and evaluate it on the\nproblem of factorizing multivariable polynomials. The results demonstrate a\nsignificant advantage of LGuess compared to both straightforward equality\nsaturation and the approach that queries the LLM directly for the rewrite\nchain.", "AI": {"tldr": "Large language models (LLMs) struggle with correctness in formal rewrite systems. \"LGuess\" is introduced, using e-graphs as an intermediate layer to enhance accuracy in generating rewrite chains.", "motivation": "To address the inadequacy of LLM-generated rewrite chains and improve correctness within formal systems by better leveraging LLM capabilities.", "method": "The authors integrate e-graphs as a layer between LLMs and rewrite systems, where LLMs propose high-level checkpoints and e-graphs generate the detailed rewrite paths. A probabilistic model derived from the LLM helps extract effective checkpoints.", "result": "The prototype implementation of LGuess showed superior performance in factorizing multivariable polynomials compared to direct queries to LLMs or standard equality saturation.", "conclusion": "LGuess offers a promising method to improve reliability in formal rewrite systems by combining LLM guidance with e-graph technology, addressing existing challenges in correctness."}}
{"id": "2511.00010", "pdf": "https://arxiv.org/pdf/2511.00010", "abs": "https://arxiv.org/abs/2511.00010", "authors": ["Jiajun Zhang", "Jianke Zhang", "Zeyu Cui", "Jiaxi Yang", "Lei Zhang", "Binyuan Hui", "Qiang Liu", "Zilei Wang", "Liang Wang", "Junyang Lin"], "title": "PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization", "categories": ["cs.CL"], "comment": null, "summary": "Recent Large Language Models (LLMs) have demonstrated remarkable profi-\nciency in code generation. However, their ability to create complex visualiza-\ntions for scaled and structured data remains largely unevaluated and\nunderdevel- oped. To address this gap, we introduce PlotCraft, a new benchmark\nfeaturing 1k challenging visualization tasks that cover a wide range of topics,\nsuch as fi- nance, scientific research, and sociology. The benchmark is\nstructured around seven high-level visualization tasks and encompasses 48\ndistinct chart types. Cru- cially, it is the first to systematically evaluate\nboth single-turn generation and multi-turn refinement across a diverse spectrum\nof task complexities. Our com- prehensive evaluation of 23 leading LLMs on\nPlotCraft reveals obvious per- formance deficiencies in handling sophisticated\nvisualization tasks. To bridge this performance gap, we develope SynthVis-30K,\na large-scale, high-quality dataset of complex visualization code synthesized\nvia a collaborative agent frame- work. Building upon this dataset, we develope\nPlotCraftor, a novel code gener- ation model that achieves strong capabilities\nin complex data visualization with a remarkably small size. Across VisEval,\nPandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance\ncomparable to that of leading propri- etary approaches. Especially, on hard\ntask, Our model achieves over 50% per- formance improvement. We will release\nthe benchmark, dataset, and code at\nhttps://github.com/Speakn0w/PlotCraft-Benchmark.", "AI": {"tldr": "Recent Large Language Models (LLMs) lack proficiency in creating complex visualizations for structured data. PlotCraft benchmark and SynthVis-30K dataset are introduced to address this gap. A new model, PlotCraftor, demonstrates improved performance.", "motivation": "Evaluate and improve LLMs' ability to handle complex data visualization tasks, as current capabilities are insufficient.", "method": "Introduced the PlotCraft benchmark assessing visualization tasks, developed SynthVis-30K dataset using a collaborative agent framework, and created PlotCraftor model for enhanced visualization code generation.", "result": "PlotCraftor achieves strong performance and overcomes deficiencies in sophisticated visualization tasks, particularly excelling in complex scenarios.", "conclusion": "PlotCraft benchmark, SynthVis-30K dataset, and PlotCraftor model collectively advance the capability of LLMs in complex visualization. Resources will be publicly available."}}
{"id": "2511.00074", "pdf": "https://arxiv.org/pdf/2511.00074", "abs": "https://arxiv.org/abs/2511.00074", "authors": ["Richard Osuagwu", "Thomas Cook", "Maraim Masoud", "Koustav Ghosal", "Riccardo Mattivi"], "title": "ScaleCall - Agentic Tool Calling at Scale for Fintech: Challenges, Methods, and Deployment Insights", "categories": ["cs.SE"], "comment": null, "summary": "While Large Language Models (LLMs) excel at tool calling, deploying these\ncapabilities in regulated enterprise environments such as fintech presents\nunique challenges due to on-premises constraints, regulatory compliance\nrequirements, and the need to disambiguate large, functionally overlapping\ntoolsets. In this paper, we present a comprehensive study of tool retrieval\nmethods for enterprise environments through the development and deployment of\nScaleCall, a prototype tool-calling framework within Mastercard designed for\norchestrating internal APIs and automating data engineering workflows. We\nsystematically evaluate embedding-based retrieval, prompt-based listwise\nranking, and hybrid approaches, revealing that method effectiveness depends\nheavily on domain-specific factors rather than inherent algorithmic\nsuperiority. Through empirical investigation on enterprise-derived benchmarks,\nwe find that embedding-based methods offer superior latency for large tool\nrepositories, while listwise ranking provides better disambiguation for\noverlapping functionalities, with hybrid approaches showing promise in specific\ncontexts. We integrate our findings into ScaleCall's flexible architecture and\nvalidate the framework through real-world deployment in Mastercard's regulated\nenvironment. Our work provides practical insights into the trade-offs between\nretrieval accuracy, computational efficiency, and operational requirements,\ncontributing to the understanding of tool-calling system design for enterprise\napplications in regulated industries.", "AI": {"tldr": "This paper studies tool retrieval methods for regulated enterprise environments by developing ScaleCall, a tool-calling framework. It reveals trade-offs in retrieval methods and validates findings through Mastercard\u2019s real-world deployment.", "motivation": "Address the challenges and optimization needs for tool retrieval methods in regulated enterprise environments like fintech, considering constraints such as compliance and overlapping tool functionalities.", "method": "Developed the ScaleCall framework at Mastercard and systematically compared embedding-based retrieval, prompt-based ranking, and hybrid approaches using enterprise benchmarks.", "result": "Embedding-based methods showed better latency for large repositories; listwise ranking excelled in disambiguation; hybrid approaches found contextual promise. Empirical evaluations validate these trade-offs.", "conclusion": "Integrating findings into ScaleCall framework improves understanding of retrieval trade-offs and informs tool-calling system designs in regulated industries."}}
{"id": "2511.00026", "pdf": "https://arxiv.org/pdf/2511.00026", "abs": "https://arxiv.org/abs/2511.00026", "authors": ["Chaitanya Shinde", "Divya Garikapati"], "title": "Gen AI in Automotive: Applications, Challenges, and Opportunities with a Case study on In-Vehicle Experience", "categories": ["cs.RO"], "comment": null, "summary": "Generative Artificial Intelligence is emerging as a transformative force in\nthe automotive industry, enabling novel applications across vehicle design,\nmanufacturing, autonomous driving, predictive maintenance, and in vehicle user\nexperience. This paper provides a comprehensive review of the current state of\nGenAI in automotive, highlighting enabling technologies such as Generative\nAdversarial Networks and Variational Autoencoders. Key opportunities include\naccelerating autonomous driving validation through synthetic data generation,\noptimizing component design, and enhancing human machine interaction via\npersonalized and adaptive interfaces. At the same time, the paper identifies\nsignificant technical, ethical, and safety challenges, including computational\ndemands, bias, intellectual property concerns, and adversarial robustness, that\nmust be addressed for responsible deployment. A case study on Mercedes Benzs\nMBUX Virtual Assistant illustrates how GenAI powered voice systems deliver more\nnatural, proactive, and personalized in car interactions compared to legacy\nrule based assistants. Through this review and case study, the paper outlines\nboth the promise and limitations of GenAI integration in the automotive sector\nand presents directions for future research and development aimed at achieving\nsafer, more efficient, and user centric mobility. Unlike prior reviews that\nfocus solely on perception or manufacturing, this paper emphasizes generative\nAI in voice based HMI, bridging safety and user experience perspectives.", "AI": {"tldr": "Generative AI is revolutionizing the automotive industry, impacting vehicle design, autonomous driving, and user interfaces while discussing challenges and future directions.", "motivation": "To explore and review how Generative AI can transform various sectors of the automotive industry, focusing on enabling technologies, key opportunities, and challenges.", "method": "The paper provides a comprehensive review of enabling technologies such as Generative Adversarial Networks and Variational Autoencoders, along with case studies, like Mercedes-Benz's MBUX Virtual Assistant, to illustrate applications.", "result": "Generative AI offers potential improvements in autonomous validation, component optimization, and human-machine interfaces, but faces computational, ethical, and safety challenges.", "conclusion": "GenAI could enhance safety, efficiency, and user experience in automotive applications but requires addressing the highlighted challenges for responsible deployment."}}
{"id": "2511.00002", "pdf": "https://arxiv.org/pdf/2511.00002", "abs": "https://arxiv.org/abs/2511.00002", "authors": ["Yurun Wu", "Yousong Sun", "Burkhard Wunsche", "Jia Wang", "Elliott Wen"], "title": "VRScout: Towards Real-Time, Autonomous Testing of Virtual Reality Games", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Virtual Reality (VR) has rapidly become a mainstream platform for gaming and\ninteractive experiences, yet ensuring the quality, safety, and appropriateness\nof VR content remains a pressing challenge. Traditional human-based quality\nassurance is labor-intensive and cannot scale with the industry's rapid growth.\nWhile automated testing has been applied to traditional 2D and 3D games,\nextending it to VR introduces unique difficulties due to high-dimensional\nsensory inputs and strict real-time performance requirements. We present\nVRScout, a deep learning-based agent capable of autonomously navigating VR\nenvironments and interacting with virtual objects in a human-like and real-time\nmanner. VRScout learns from human demonstrations using an enhanced Action\nChunking Transformer that predicts multi-step action sequences. This enables\nour agent to capture higher-level strategies and generalize across diverse\nenvironments. To balance responsiveness and precision, we introduce a\ndynamically adjustable sliding horizon that adapts the agent's temporal context\nat runtime. We evaluate VRScout on commercial VR titles and show that it\nachieves expert-level performance with only limited training data, while\nmaintaining real-time inference at 60 FPS on consumer-grade hardware. These\nresults position VRScout as a practical and scalable framework for automated VR\ngame testing, with direct applications in both quality assurance and safety\nauditing.", "AI": {"tldr": "VRScout introduces a deep learning agent designed for automated testing in VR environments, achieving expert-level performance and real-time operation.", "motivation": "Traditional human-based quality assurance struggles to scale with VR content's complexity and rapid growth, necessitating automated testing solutions.", "method": "Using an Action Chunking Transformer trained on human demonstrations, VRScout predicts multi-step strategies and adapts temporal context dynamically.", "result": "VRScout performs at expert levels on commercial VR games with minimal training data, maintaining real-time speeds on consumer hardware.", "conclusion": "VRScout offers a scalable, efficient framework for VR game testing, enhancing quality assurance and safety auditing."}}
{"id": "2511.00011", "pdf": "https://arxiv.org/pdf/2511.00011", "abs": "https://arxiv.org/abs/2511.00011", "authors": ["Alexander Okupnik", "Johannes Schneider", "Kyriakos Flouris"], "title": "Generative human motion mimicking through feature extraction in denoising diffusion settings", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": null, "summary": "Recent success with large language models has sparked a new wave of verbal\nhuman-AI interaction. While such models support users in a variety of creative\ntasks, they lack the embodied nature of human interaction. Dance, as a primal\nform of human expression, is predestined to complement this experience. To\nexplore creative human-AI interaction exemplified by dance, we build an\ninteractive model based on motion capture (MoCap) data. It generates an\nartificial other by partially mimicking and also \"creatively\" enhancing an\nincoming sequence of movement data. It is the first model, which leverages\nsingle-person motion data and high level features in order to do so and, thus,\nit does not rely on low level human-human interaction data. It combines ideas\nof two diffusion models, motion inpainting, and motion style transfer to\ngenerate movement representations that are both temporally coherent and\nresponsive to a chosen movement reference. The success of the model is\ndemonstrated by quantitatively assessing the convergence of the feature\ndistribution of the generated samples and the test set which serves as\nsimulating the human performer. We show that our generations are first steps to\ncreative dancing with AI as they are both diverse showing various deviations\nfrom the human partner while appearing realistic.", "AI": {"tldr": "This paper introduces an interactive model for creative human-AI dance interaction using motion capture (MoCap) data. It uniquely combines motion inpainting and motion style transfer, enhancing responsiveness to human movement.", "motivation": "To explore creative and embodied human-AI interactions through dance, leveraging the expressive and interactive potential of motion capture data.", "method": "The model uses single-person motion data and high-level features, applying diffusion models, motion inpainting, and motion style transfer to generate temporally coherent and diverse movement based on incoming motion sequences.", "result": "The generated dance movements are shown to be diverse, realistic, and responsive, demonstrating successful feature distribution convergence with test data.", "conclusion": "The study represents a foundational step toward creative AI systems that can interact with humans through dance, paving the way for more expressive human-AI collaboration."}}
{"id": "2511.00013", "pdf": "https://arxiv.org/pdf/2511.00013", "abs": "https://arxiv.org/abs/2511.00013", "authors": ["Daria D. Tyurina", "Sergey V. Stasenko", "Konstantin V. Lushnikov", "Maria V. Vedunova"], "title": "Using machine learning methods to predict cognitive age from psychophysiological tests", "categories": ["q-bio.NC", "cs.LG"], "comment": null, "summary": "This study introduces a novel method for predicting cognitive age using\npsychophysiological tests. To determine cognitive age, subjects were asked to\ncomplete a series of psychological tests measuring various cognitive functions,\nincluding reaction time and cognitive conflict, short-term memory, verbal\nfunctions, and color and spatial perception. Based on the tests completed, the\naverage completion time, proportion of correct answers, average absolute delta\nof the color campimetry test, number of guessed words in the M\\\"unsterberg\nmatrix, and other parameters were calculated for each subject. The obtained\ncharacteristics of the subjects were preprocessed and used to train a machine\nlearning algorithm implementing a regression task for predicting a person's\ncognitive age. These findings contribute to the field of remote screening using\nmobile devices for human health for diagnosing and monitoring cognitive aging.", "AI": {"tldr": "The study developed a machine learning approach to predict cognitive age using psychophysiological tests.", "motivation": "The paper aimed to advance methods for remote screening and monitoring cognitive aging via mobile devices.", "method": "Participants underwent psychological tests measuring cognitive functions, with test results processed to train a machine learning regression model to predict cognitive age.", "result": "Key test metrics were calculated, processed, and utilized to successfully train a regression model for cognitive age prediction.", "conclusion": "The method facilitates cognitive aging diagnosis and monitoring and contributes to remote health screening advancements."}}
{"id": "2511.00217", "pdf": "https://arxiv.org/pdf/2511.00217", "abs": "https://arxiv.org/abs/2511.00217", "authors": ["Mitchell L. Prevett", "Francis K. C. Hui", "Zhi Yang Tho", "A. H. Welsh", "Anton H. Westveld"], "title": "Gradient Boosted Mixed Models: Flexible Joint Estimation of Mean and Variance Components for Clustered Data", "categories": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "comment": null, "summary": "Linear mixed models are widely used for clustered data, but their reliance on\nparametric forms limits flexibility in complex and high-dimensional settings.\nIn contrast, gradient boosting methods achieve high predictive accuracy through\nnonparametric estimation, but do not accommodate clustered data structures or\nprovide uncertainty quantification.\n  We introduce Gradient Boosted Mixed Models (GBMixed), a framework and\nalgorithm that extends boosting to jointly estimate mean and variance\ncomponents via likelihood-based gradients. In addition to nonparametric mean\nestimation, the method models both random effects and residual variances as\npotentially covariate-dependent functions using flexible base learners such as\nregression trees or splines, enabling nonparametric estimation while\nmaintaining interpretability.\n  Simulations and real-world applications demonstrate accurate recovery of\nvariance components, calibrated prediction intervals, and improved predictive\naccuracy relative to standard linear mixed models and nonparametric methods.\nGBMixed provides heteroscedastic uncertainty quantification and introduces\nboosting for heterogeneous random effects. This enables covariate-dependent\nshrinkage for cluster-specific predictions to adapt between population and\ncluster-level data. Under standard causal assumptions, the framework enables\nestimation of heterogeneous treatment effects with reliable uncertainty\nquantification.", "AI": {"tldr": "This paper introduces Gradient Boosted Mixed Models (GBMixed), combining gradient boosting and mixed model approaches for high predictive accuracy and advanced uncertainty quantification in clustered data.", "motivation": "Linear mixed models have limited flexibility for complex and high-dimensional data due to parametric constraints, while gradient boosting lacks support for clustered data structures and uncertainty quantification.", "method": "The proposed method integrates boosting with likelihood-based gradients for simultaneous estimation of mean and variance components, using regression trees or splines to allow flexible and interpretable nonparametric modeling.", "result": "Simulations and real-world applications show GBMixed achieves high predictive accuracy, accurate recovery of variance components, and reliable uncertainty measures outperforming standard methods.", "conclusion": "GBMixed extends boosting to clustered data, enabling covariate-dependent predictions and providing tools for estimating heterogeneous treatment effects with robust uncertainty quantification."}}
{"id": "2511.00316", "pdf": "https://arxiv.org/pdf/2511.00316", "abs": "https://arxiv.org/abs/2511.00316", "authors": ["Khakim Akhunov", "Eren Yildiz", "Kasim Sinan Yildirim"], "title": "PEARL: Power- and Energy-Aware Multicore Intermittent Computing", "categories": ["cs.ET", "cs.AR"], "comment": "Presented at EWSN 2025 (THE 22ND INTERNATIONAL CONFERENCE ON EMBEDDED\n  WIRELESS SYSTEMS AND NETWORKS)", "summary": "Low-power multicore platforms are suitable for running data-intensive tasks\nin parallel, but they are highly inefficient for computing on intermittent\npower. In this work, we present PEARL (PowEr And eneRgy-aware MuLticore\nIntermittent Computing), a novel systems support that can make existing\nmulticore microcontroller (MCU) platforms suitable for efficient intermittent\ncomputing. PEARL achieves this by leveraging only a three-threshold voltage\ntracking circuit and an external fast non-volatile memory, which multicore MCUs\ncan smoothly interface. PEARL software runtime manages these components and\nperforms energy- and power-aware adaptation of the multicore configuration to\nintroduce minimal backup overheads and boost performance. Our evaluation shows\nthat PEARL outperforms the state-of-the-art solutions by up to 30x and consumes\nup to 32x less energy.", "AI": {"tldr": "PEARL is a system designed to enhance low-power multicore platforms for efficient intermittent computing, surpassing existing solutions in performance by up to 30x and reducing energy consumption by up to 32x.", "motivation": "Low-power multicore platforms, though suitable for parallel data-intensive tasks, are inefficient for intermittent power computing, necessitating innovative solutions.", "method": "PEARL incorporates a three-threshold voltage tracking circuit and external fast non-volatile memory, managed by runtime software for adaptive power-aware multicore configuration.", "result": "PEARL significantly reduces backup overheads while boosting performance, showing up to 30x improvement in performance and up to 32x reduction in energy consumption compared to existing solutions.", "conclusion": "PEARL establishes a novel and efficient approach to enable intermittent computing on multicore microcontroller platforms, advancing performance and energy efficiency."}}
{"id": "2511.00634", "pdf": "https://arxiv.org/pdf/2511.00634", "abs": "https://arxiv.org/abs/2511.00634", "authors": ["Mark Kocherovsky", "Illya Bakurov", "Wolfgang Banzhaf"], "title": "Node Preservation and its Effect on Crossover in Cartesian Genetic Programming", "categories": ["cs.NE", "cs.AI", "cs.LG"], "comment": "Draft to cite in another paper before both papers are peer-reviewed\n  for the evo*2026 conference, 21 pages, 5 figures", "summary": "While crossover is a critical and often indispensable component in other\nforms of Genetic Programming, such as Linear- and Tree-based, it has\nconsistently been claimed that it deteriorates search performance in CGP. As a\nresult, a mutation-alone $(1+\\lambda)$ evolutionary strategy has become the\ncanonical approach for CGP. Although several operators have been developed that\ndemonstrate an increased performance over the canonical method, a general\nsolution to the problem is still lacking. In this paper, we compare basic\ncrossover methods, namely one-point and uniform, to variants in which nodes are\n``preserved,'' including the subgraph crossover developed by Roman Kalkreuth,\nthe difference being that when ``node preservation'' is active, crossover is\nnot allowed to break apart instructions. We also compare a node mutation\noperator to the traditional point mutation; the former simply replaces an\nentire node with a new one. We find that node preservation in both mutation and\ncrossover improves search using symbolic regression benchmark problems, moving\nthe field towards a general solution to CGP crossover.", "AI": {"tldr": "The paper investigates crossover and mutation methods in Cartesian Genetic Programming (CGP), showing that node preservation improves search performance in symbolic regression tasks.", "motivation": "Despite its importance in other Genetic Programming variants, crossover was considered detrimental for CGP. Previous methods lacked a general solution to improve crossover performance in CGP.", "method": "The authors tested various crossover methods (e.g., one-point, uniform, and subgraph crossover) and mutation operators (traditional point mutation vs. node mutation) with and without node preservation.", "result": "Methods involving node preservation improved search performance, particularly in symbolic regression benchmarks.", "conclusion": "Node preservation-enhanced operations may pave the way for a general solution to enable effective CGP crossover."}}
{"id": "2511.00039", "pdf": "https://arxiv.org/pdf/2511.00039", "abs": "https://arxiv.org/abs/2511.00039", "authors": ["Krishna Kumar Neelakanta Pillai Santha Kumari Amma"], "title": "Graph-Attentive MAPPO for Dynamic Retail Pricing", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Dynamic pricing in retail requires policies that adapt to shifting demand\nwhile coordinating decisions across related products. We present a systematic\nempirical study of multi-agent reinforcement learning for retail price\noptimization, comparing a strong MAPPO baseline with a\ngraph-attention-augmented variant (MAPPO+GAT) that leverages learned\ninteractions among products. Using a simulated pricing environment derived from\nreal transaction data, we evaluate profit, stability across random seeds,\nfairness across products, and training efficiency under a standardized\nevaluation protocol. The results indicate that MAPPO provides a robust and\nreproducible foundation for portfolio-level price control, and that MAPPO+GAT\nfurther enhances performance by sharing information over the product graph\nwithout inducing excessive price volatility. These results indicate that\ngraph-integrated MARL provides a more scalable and stable solution than\nindependent learners for dynamic retail pricing, offering practical advantages\nin multi-product decision-making.", "AI": {"tldr": "This paper evaluates using multi-agent reinforcement learning (MARL) for dynamic retail pricing, comparing a standard MAPPO approach with an enhanced graph-attention-based variant (MAPPO+GAT).", "motivation": "The authors aim to optimize retail pricing strategies that adapt to shifting demand while coordinating decisions across related products, addressing challenges in scalability and stability.", "method": "Using a simulated pricing environment based on real transaction data, the paper benchmarks MAPPO and MAPPO+GAT for profit, fairness, stability, and training efficiency in a standardized evaluation.", "result": "MAPPO provides a reliable foundation for pricing control, while MAPPO+GAT improves performance by leveraging product interaction insights via a graph-based approach, reducing price variability.", "conclusion": "Integrating graph-aware strategies into MARL delivers a scalable and robust tool for dynamic pricing in retail, with noticeable advantages over independent learning methods."}}
{"id": "2511.00263", "pdf": "https://arxiv.org/pdf/2511.00263", "abs": "https://arxiv.org/abs/2511.00263", "authors": ["Jinyuan Chen"], "title": "COOL Is Optimal in Error-Free Asynchronous Byzantine Agreement", "categories": ["cs.DC", "cs.CR", "cs.IT", "math.IT"], "comment": "25 pages", "summary": "COOL (Chen'21) is an error-free, information-theoretically secure Byzantine\nagreement (BA) protocol proven to achieve BA consensus in the synchronous\nsetting for an $\\ell$-bit message, with a total communication complexity of\n$O(\\max\\{n\\ell, nt \\log q\\})$ bits, four communication rounds in the worst\ncase, and a single invocation of a binary BA, under the optimal resilience\nassumption $n \\geq 3t + 1$ in a network of $n$ nodes, where up to $t$ nodes may\nbehave dishonestly. Here, $q$ denotes the alphabet size of the error correction\ncode used in the protocol.\n  In this work, we present an adaptive variant of COOL, called OciorACOOL,\nwhich achieves error-free, information-theoretically secure BA consensus in the\nasynchronous setting with total $O(\\max\\{n\\ell, n t \\log q\\})$ communication\nbits, $O(1)$ rounds, and a single invocation of an asynchronous binary BA\nprotocol, still under the optimal resilience assumption $n \\geq 3t + 1$.\nMoreover, OciorACOOL retains the same low-complexity, traditional $(n, k)$\nerror-correction encoding and decoding as COOL, with $k=t/3$.", "AI": {"tldr": "The paper introduces OciorACOOL, an adaptive variant of the COOL Byzantine Agreement protocol, achieving asynchronous and secure consensus with efficient communication complexity and minimal rounds, while maintaining optimal resilience.", "motivation": "To address the limitations of COOL in synchronous settings and adapt it for asynchronous environments, enabling secure and efficient Byzantine Agreement with optimal resilience.", "method": "The paper modifies the COOL protocol to operate in asynchronous settings (OciorACOOL) by leveraging error correction techniques with $k=t/3$ and a single invocation of an asynchronous binary BA protocol, maintaining low communication complexity.", "result": "OciorACOOL achieves error-free, information-theoretically secure BA consensus in asynchronous settings with communication complexity of $O(\\max\\{n\\ell, n t \\log q\\})$, $O(1)$ rounds, and retains optimal resilience ($n \\geq 3t+1$).", "conclusion": "OciorACOOL maintains the theoretical efficiency and resilience of COOL while extending applicability to asynchronous communication settings, demonstrating its robustness and practicality."}}
{"id": "2511.00488", "pdf": "https://arxiv.org/pdf/2511.00488", "abs": "https://arxiv.org/abs/2511.00488", "authors": ["Jun Gao", "Yun Peng", "Xiaoxue Ren"], "title": "\\texttt{ReMind}: Understanding Deductive Code Reasoning in LLMs", "categories": ["cs.PL", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable progress in\ncode-related tasks. Despite their advancement, empirical evidence reveals that\nthey still struggle with \\emph{deductive code reasoning}, the ability to reason\nabout the program execution process. While prior studies have recognized this\nlimitation, the underlying causes remain largely underexplored. In this paper,\nwe begin by presenting a comprehensive empirical study that reveals three key\nchallenges undermining deductive code reasoning: (1) an intrinsic gap between\ngeneration and reasoning abilities, (2) a consistent bias towards code sources,\nand (3) weak zero-shot generalization on complex benchmarks. In light of these\nchallenges, we propose \\texttt{ReMind}, a multi-agent framework composed of\n\\texttt{Mutator}, \\texttt{Executor}, and \\texttt{Inspector}. The\n\\texttt{Mutator} generates code variants to mitigate bias towards code sources,\nthe \\texttt{Executor} traces variable states step-by-step to expose\ninconsistency, and the \\texttt{Inspector} identifies problematic reasoning\nsteps and provides control-flow refinement to bridge the intrinsic reasoning\ngap. Through their coordinated collaboration, \\texttt{ReMind} systematically\nidentifies and refines reasoning flaws, achieving outstanding performance and\nenabling robust zero-shot generalization. Extensive experiments on two\nbenchmarks with five LLMs demonstrate the superior advantages of\n\\texttt{ReMind} compared to baseline approaches in deductive code reasoning.", "AI": {"tldr": "The paper addresses challenges in deductive code reasoning with LLMs and proposes a novel multi-agent framework called ReMind to improve reasoning abilities, achieving superior performance.", "motivation": "Large Language Models struggle with deductive code reasoning, hindering their ability to reason through program execution. Understanding and addressing the causes of this limitation is crucial for improvement.", "method": "The paper introduces ReMind, a framework composed of three agents: Mutator (generates code variations), Executor (traces variable states), and Inspector (detects reasoning flaws to bridge gaps). It ensures coordinated collaboration among agents for refinement.", "result": "ReMind displays superior performance in deductive code reasoning tasks, achieving better results in zero-shot generalization and overcoming challenges on benchmarks with multiple LLMs.", "conclusion": "ReMind significantly enhances deductive code reasoning abilities in LLMs by systematically identifying and addressing reasoning limitations, thus improving code-related tasks."}}
{"id": "2511.00115", "pdf": "https://arxiv.org/pdf/2511.00115", "abs": "https://arxiv.org/abs/2511.00115", "authors": ["Haoyuan Li", "Yuanbo Tong", "Yuchen Li", "Zirui Wang", "Chunhou Liu", "Jiamou Liu"], "title": "Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Personality recognition from text is typically cast as hard-label\nclassification, which obscures the graded, prototype-like nature of human\npersonality judgments. We present ProtoMBTI, a cognitively aligned framework\nfor MBTI inference that operationalizes prototype theory within an LLM-based\npipeline. First, we construct a balanced, quality-controlled corpus via\nLLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment).\nNext, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative\nembeddings and to standardize a bank of personality prototypes. At inference,\nwe retrieve top-k prototypes for a query post and perform a\nretrieve--reuse--revise--retain cycle: the model aggregates prototype evidence\nvia prompt-based voting, revises when inconsistencies arise, and, upon correct\nprediction, retains the sample to continually enrich the prototype library.\nAcross Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both\nthe four MBTI dichotomies and the full 16-type task, and exhibits robust\ncross-dataset generalization. Our results indicate that aligning the inference\nprocess with psychological prototype reasoning yields gains in accuracy,\ninterpretability, and transfer for text-based personality modeling.", "AI": {"tldr": "ProtoMBTI introduces a novel MBTI personality recognition framework aligned with prototype theory, achieving improved accuracy and generalization through LLM-based pipeline and prototype reasoning.", "motivation": "Existing approaches for MBTI personality recognition focus on hard-label classification, neglecting the nuanced and prototype-like nature of human personality judgments.", "method": "ProtoMBTI incorporates prototype theory using an LLM-based pipeline. This includes corpus construction via LLM-guided augmentation, fine-tuning lightweight encoders with LoRA, embedding personality prototypes, and integrating evidence through a prototype retrieval and revision cycle.", "result": "ProtoMBTI outperforms baseline models on MBTI classification tasks and demonstrates superior generalization across datasets.", "conclusion": "Prototype-aligned inference improves text-based personality modeling in accuracy, interpretability, and cross-dataset transferability."}}
{"id": "2511.00087", "pdf": "https://arxiv.org/pdf/2511.00087", "abs": "https://arxiv.org/abs/2511.00087", "authors": ["Anshu Dubey", "Akash Dhruv"], "title": "Adding New Capability in Existing Scientific Application with LLM Assistance", "categories": ["cs.SE", "cs.AI"], "comment": "8 pages, 4 figures, submitted to The 1st International Workshop on\n  Foundational large Language Models Advances for HPC in Asia", "summary": "With the emergence and rapid evolution of large language models (LLM),\nautomating coding tasks has become an im- portant research topic. Many efforts\nare underway and liter- ature abounds about the efficacy of models and their\nability to generate code. A less explored aspect of code generation is for new\nalgorithms, where the training data-set would not have included any previous\nexample of similar code. In this paper we propose a new methodology for writing\ncode from scratch for a new algorithm using LLM assistance, and describe\nenhancement of a previously developed code- translation tool, Code-Scribe, for\nnew code generation.", "AI": {"tldr": "The paper focuses on using large language models (LLMs) for coding new algorithms and introduces enhancements to a tool, Code-Scribe, to assist in generating such code from scratch.", "motivation": "The motivation is to address the less-explored aspect of generating code for new algorithms where existing training data lacks prior examples.", "method": "The study proposes a methodology for writing code from scratch for new algorithms with the help of LLMs and enhances the Code-Scribe tool for better support in this task.", "result": "The paper describes improvements made to the Code-Scribe tool, demonstrating its capability to assist in generating code for novel algorithms.", "conclusion": "Using LLMs combined with tools like an improved version of Code-Scribe can facilitate code generation for new algorithms not previously represented in training data."}}
{"id": "2511.00033", "pdf": "https://arxiv.org/pdf/2511.00033", "abs": "https://arxiv.org/abs/2511.00033", "authors": ["Diqi He", "Xuehao Gao", "Hao Li", "Junwei Han", "Dingwen Zhang"], "title": "STRIDER: Navigation via Instruction-Aligned Structural Decision Space Optimization", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "The Zero-shot Vision-and-Language Navigation in Continuous Environments\n(VLN-CE) task requires agents to navigate previously unseen 3D environments\nusing natural language instructions, without any scene-specific training. A\ncritical challenge in this setting lies in ensuring agents' actions align with\nboth spatial structure and task intent over long-horizon execution. Existing\nmethods often fail to achieve robust navigation due to a lack of structured\ndecision-making and insufficient integration of feedback from previous actions.\nTo address these challenges, we propose STRIDER (Instruction-Aligned Structural\nDecision Space Optimization), a novel framework that systematically optimizes\nthe agent's decision space by integrating spatial layout priors and dynamic\ntask feedback. Our approach introduces two key innovations: 1) a Structured\nWaypoint Generator that constrains the action space through spatial structure,\nand 2) a Task-Alignment Regulator that adjusts behavior based on task progress,\nensuring semantic alignment throughout navigation. Extensive experiments on the\nR2R-CE and RxR-CE benchmarks demonstrate that STRIDER significantly outperforms\nstrong SOTA across key metrics; in particular, it improves Success Rate (SR)\nfrom 29% to 35%, a relative gain of 20.7%. Such results highlight the\nimportance of spatially constrained decision-making and feedback-guided\nexecution in improving navigation fidelity for zero-shot VLN-CE.", "AI": {"tldr": "STRIDER enhances zero-shot Vision-and-Language Navigation in unseen 3D environments by optimizing action decisions using spatial constraints and dynamic feedback.", "motivation": "To tackle the challenge of agents failing to robustly navigate in unseen 3D environments with natural language instructions due to insufficient structured decision-making and feedback integration.", "method": "The paper introduces STRIDER, which optimizes decision spaces with two key components: a Structured Waypoint Generator for spatial structure constraints and a Task-Alignment Regulator for behavior adjustments based on task progress.", "result": "Experiments show that STRIDER significantly improves the Success Rate in zero-shot VLN-CE tasks from 29% to 35%, outperforming SOTA approaches by a relative gain of 20.7%.", "conclusion": "STRIDER demonstrates the value of incorporating spatially constrained decision-making and feedback-guided execution, advancing navigation fidelity in zero-shot VLN-CE tasks."}}
{"id": "2511.00029", "pdf": "https://arxiv.org/pdf/2511.00029", "abs": "https://arxiv.org/abs/2511.00029", "authors": ["Samaksh Bhargav", "Zining Zhu"], "title": "Feature-Guided SAE Steering for Refusal-Rate Control using Contrasting Prompts", "categories": ["cs.LG", "cs.AI"], "comment": "12 pages, 6 figures", "summary": "Large Language Model (LLM) deployment requires guiding the LLM to recognize\nand not answer unsafe prompts while complying with safe prompts. Previous\nmethods for achieving this require adjusting model weights along with other\nexpensive procedures. While recent advances in Sparse Autoencoders (SAEs) have\nenabled interpretable feature extraction from LLMs, existing approaches lack\nsystematic feature selection methods and principled evaluation of\nsafety-utility tradeoffs. We explored using different steering features and\nsteering strengths using Sparse Auto Encoders (SAEs) to provide a solution.\nUsing an accurate and innovative contrasting prompt method with the\nAI-Generated Prompts Dataset from teknium/OpenHermes-2p5-Mistral-7B and Air\nBench eu-dataset to efficiently choose the best features in the model to steer,\nwe tested this method on Llama-3 8B. We conclude that using this method, our\napproach achieves an 18.9% improvement in safety performance while\nsimultaneously increasing utility by 11.1%, demonstrating that targeted SAE\nsteering can overcome traditional safety-utility tradeoffs when optimal\nfeatures are identified through principled selection methods.", "AI": {"tldr": "The paper proposes a method to improve safety and utility in LLMs using Sparse Auto Encoders (SAEs) for feature selection and steering.", "motivation": "Deploying LLMs safely requires avoiding unsafe responses while maintaining compliance with safe prompts, but previous methods are costly and inefficient.", "method": "The authors use Sparse Auto Encoders (SAEs) for interpretable feature extraction and propose a systematic feature selection method employing contrasting prompts and datasets for optimal steering.", "result": "The approach achieves an 18.9% improvement in safety and an 11.1% increase in utility on the Llama-3 8B model.", "conclusion": "Targeted SAE steering with optimal feature selection can effectively balance safety and utility, improving both simultaneously."}}
{"id": "2511.00021", "pdf": "https://arxiv.org/pdf/2511.00021", "abs": "https://arxiv.org/abs/2511.00021", "authors": ["Julio Jerison E. Macrohon", "Gordon Hung"], "title": "Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets", "categories": ["cs.CV", "cs.AI"], "comment": "15 pages, 10 figures", "summary": "Coral reefs support numerous marine organisms and are an important source of\ncoastal protection from storms and floods, representing a major part of marine\necosystems. However coral reefs face increasing threats from pollution, ocean\nacidification, and sea temperature anomalies, making efficient protection and\nmonitoring heavily urgent. Therefore, this study presents a novel\nmachine-learning-based coral bleaching classification system based on a diverse\nglobal dataset with samples of healthy and bleached corals under varying\nenvironmental conditions, including deep seas, marshes, and coastal zones. We\nbenchmarked and compared three state-of-the-art models: Residual Neural Network\n(ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN).\nAfter comprehensive hyperparameter tuning, the CNN model achieved the highest\naccuracy of 88%, outperforming existing benchmarks. Our findings offer\nimportant insights into autonomous coral monitoring and present a comprehensive\nanalysis of the most widely used computer vision models.", "AI": {"tldr": "The paper proposes a machine learning coral bleaching classification system, comparing ResNet, ViT, and CNN models. CNN outperformed others with 88% accuracy.", "motivation": "Coral reefs play a critical role in marine ecosystems but face threats from environmental changes, calling for improved monitoring solutions.", "method": "The study compares three machine learning models (ResNet, ViT, CNN) on a diverse coral dataset, employing hyperparameter tuning to find the best performing model.", "result": "CNN achieved the highest accuracy (88%) in classifying coral bleaching, surpassing other models.", "conclusion": "The findings emphasize the efficiency of CNN in coral reef monitoring and highlight its potential for autonomous environmental applications."}}
{"id": "2511.01784", "pdf": "https://arxiv.org/pdf/2511.01784", "abs": "https://arxiv.org/abs/2511.01784", "authors": ["Alex Lepauvre", "Lucia Melloni", "Karl Friston", "Peter Zeidman"], "title": "Variational Representational Similarity Analysis (vRSA) for M/EEG", "categories": ["q-bio.NC"], "comment": "24 pages, 7 figures, code available at\n  https://github.com/pzeidman/vRSA_MEEG", "summary": "This paper introduces variational representational similarity analysis RSA\n(vRSA) for electromagnetic recordings of neural responses (e.g., EEG, MEG, ECoG\nor LFP). Variational RSA is a Bayesian approach for testing whether the\nsimilarity of stimuli or experimental conditions is expressed in univariate or\nmultivariate neural recordings. Extending an approach previously introduced in\nthe context of functional MRI, vRSA decomposes the condition-by-condition data\ncovariance matrix into hypothesised effects and observation noise, thereby\ncasting RSA as a covariance component estimation problem. In this context,\nperistimulus time may be treated as an experimental factor, enabling one to\ntest for the probability that different experimental effects are expressed in\ndata at different times. Variational Bayesian methods are used for model\nestimation and model comparison, which confer a number of advantages over\nclassical approaches, including statistically efficient hypothesis testing,\nquantification of uncertainty using Bayesian credible intervals and\ncomputational efficiency. After introducing the theory, we provide a worked\nexample using openly available EEG data. Software functions implementing vRSA\nfor the SPM software package accompany this paper, together with exemplar\nanalysis scripts.", "AI": {"tldr": "This paper presents variational RSA (vRSA), a Bayesian method for analyzing neural recordings with improved efficiency and uncertainty quantification.", "motivation": "To advance neural response analysis by introducing a statistical framework that leverages Bayesian methods for testing similarity in neural data under various experimental conditions.", "method": "vRSA reformulates RSA as a covariance component estimation problem, utilizing variational Bayesian methods for efficient model estimation and hypothesis testing in univariate and multivariate data.", "result": "The paper demonstrates vRSA with an EEG dataset, showcasing its practical utility and providing accompanying SPM software and analysis scripts.", "conclusion": "vRSA offers a more statistically efficient and computationally advantageous approach to representational similarity analysis compared to classical methods, with broader applicability to neural and cognitive science research."}}
{"id": "2511.00366", "pdf": "https://arxiv.org/pdf/2511.00366", "abs": "https://arxiv.org/abs/2511.00366", "authors": ["Krishna Prasath Logakannan", "Shridhar Vashishtha", "Jacob Hochhalter", "Shandian Zhe", "Robert M. Kirby"], "title": "A Streaming Sparse Cholesky Method for Derivative-Informed Gaussian Process Surrogates Within Digital Twin Applications", "categories": ["stat.ML", "cs.CE", "cs.LG"], "comment": null, "summary": "Digital twins are developed to model the behavior of a specific physical\nasset (or twin), and they can consist of high-fidelity physics-based models or\nsurrogates. A highly accurate surrogate is often preferred over multi-physics\nmodels as they enable forecasting the physical twin future state in real-time.\nTo adapt to a specific physical twin, the digital twin model must be updated\nusing in-service data from that physical twin. Here, we extend Gaussian process\n(GP) models to include derivative data, for improved accuracy, with dynamic\nupdating to ingest physical twin data during service. Including derivative\ndata, however, comes at a prohibitive cost of increased covariance matrix\ndimension. We circumvent this issue by using a sparse GP approximation, for\nwhich we develop extensions to incorporate derivatives. Numerical experiments\ndemonstrate that the prediction accuracy of the derivative-enhanced sparse GP\nmethod produces improved models upon dynamic data additions. Lastly, we apply\nthe developed algorithm within a DT framework to model fatigue crack growth in\nan aerospace vehicle.", "AI": {"tldr": "The paper presents an improved digital twin (DT) framework to more accurately model physical assets using a derivative-enhanced sparse Gaussian Process (GP), solving challenges of computational costs while ensuring dynamic updates with real-time service data.", "motivation": "The authors aim to address the challenge of creating accurate and computationally feasible digital twin models for physical systems, capable of being dynamically updated using service data for real-time forecasting.", "method": "The study extends Gaussian Process models to include derivative data for better accuracy and employs a sparse GP approximation to manage the computational costs arising from increased covariance matrix dimensions. The model is dynamically updated with in-service data.", "result": "The derivative-enhanced sparse GP method demonstrated improved prediction accuracy in numerical experiments and dynamically enhanced models upon the addition of service data.", "conclusion": "The proposed model improves digital twin effectiveness, particularly for real-time applications like fatigue crack growth modeling in aerospace vehicles, by combining derivative-enhanced GP and sparse approximations."}}
{"id": "2511.00342", "pdf": "https://arxiv.org/pdf/2511.00342", "abs": "https://arxiv.org/abs/2511.00342", "authors": ["Hendrio Braganca", "Diego Kreutz", "Vanderson Rocha", "Joner Assolin", "and Eduardo Feitosa"], "title": "MH-1M: A 1.34 Million-Sample Comprehensive Multi-Feature Android Malware Dataset for Machine Learning, Deep Learning, Large Language Models, and Threat Intelligence Research", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.PF", "68T01", "I.2; E.0"], "comment": "17 pages, 7 figures, 13 tables, submitted to the Scientific Data\n  journal published by Nature Research", "summary": "We present MH-1M, one of the most comprehensive and up-to-date datasets for\nadvanced Android malware research. The dataset comprises 1,340,515\napplications, encompassing a wide range of features and extensive metadata. To\nensure accurate malware classification, we employ the VirusTotal API,\nintegrating multiple detection engines for comprehensive and reliable\nassessment. Our GitHub, Figshare, and Harvard Dataverse repositories provide\nopen access to the processed dataset and its extensive supplementary metadata,\ntotaling more than 400 GB of data and including the outputs of the feature\nextraction pipeline as well as the corresponding VirusTotal reports. Our\nfindings underscore the MH-1M dataset's invaluable role in understanding the\nevolving landscape of malware.", "AI": {"tldr": "The paper introduces MH-1M, a large and detailed Android malware dataset with over 1.3 million applications, offering metadata and detection analyses using VirusTotal.", "motivation": "To provide a robust, comprehensive, and up-to-date dataset for advanced Android malware research, addressing the need for reliable malware classification and understanding malware evolution.", "method": "The dataset comprises 1,340,515 applications and employs VirusTotal API with multiple detection engines for accurate malware assessment. Outputs from a feature extraction pipeline and VirusTotal reports are openly accessible alongside metadata repositories.", "result": "The MH-1M dataset's extensive metadata (over 400 GB), combined with accurate malware classification methods, makes it a valuable resource for malware research.", "conclusion": "MH-1M establishes itself as a critical tool for advanced Android malware study, enhancing understanding of malware trends and aiding research efforts in cybersecurity."}}
{"id": "2511.00732", "pdf": "https://arxiv.org/pdf/2511.00732", "abs": "https://arxiv.org/abs/2511.00732", "authors": ["Zainab Aizaz", "James C. Knight", "Thomas Nowotny"], "title": "FeNN-DMA: A RISC-V SoC for SNN acceleration", "categories": ["cs.NE", "cs.AI", "cs.AR"], "comment": null, "summary": "Spiking Neural Networks (SNNs) are a promising, energy-efficient alternative\nto standard Artificial Neural Networks (ANNs) and are particularly well-suited\nto spatio-temporal tasks such as keyword spotting and video classification.\nHowever, SNNs have a much lower arithmetic intensity than ANNs and are\ntherefore not well-matched to standard accelerators like GPUs and TPUs. Field\nProgrammable Gate Arrays(FPGAs) are designed for such memory-bound workloads\nand here we develop a novel, fully-programmable RISC-V-based system-on-chip\n(FeNN-DMA), tailored to simulating SNNs on modern UltraScale+ FPGAs. We show\nthat FeNN-DMA has comparable resource usage and energy requirements to\nstate-of-the-art fixed-function SNN accelerators, yet it is capable of\nsimulating much larger and more complex models. Using this functionality, we\ndemonstrate state-of-the-art classification accuracy on the Spiking Heidelberg\nDigits and Neuromorphic MNIST tasks.", "AI": {"tldr": "The paper introduces a novel RISC-V-based system-on-chip platform, FeNN-DMA, optimized for efficient Spiking Neural Network (SNN) processing on FPGAs, demonstrating competitive resource usage and superior scalability.", "motivation": "To address the mismatch between energy-efficient Spiking Neural Networks (SNNs) and the processing capabilities of standard accelerators like GPUs and TPUs.", "method": "Developing FeNN-DMA, a RISC-V-based system-on-chip customized for simulating SNNs on UltraScale+ FPGAs, enabling better scalability and performance for memory-bound workloads.", "result": "FeNN-DMA achieves comparable resource usage and energy efficiency to state-of-the-art fixed-function SNN accelerators while supporting larger, more complex models. It also delivers top-tier accuracy on specific classification tasks like Spiking Heidelberg Digits and Neuromorphic MNIST.", "conclusion": "FeNN-DMA provides an energy-efficient and scalable solution for SNN simulations, outperforming traditional accelerators in handling complex, large models while maintaining high classification accuracy."}}
{"id": "2511.00048", "pdf": "https://arxiv.org/pdf/2511.00048", "abs": "https://arxiv.org/abs/2511.00048", "authors": ["Martin Bicher", "Maximilian Viehauser", "Daniele Giannandrea", "Hannah Kastinger", "Dominik Brunmeir", "Claire Rippinger", "Christoph Urach", "Niki Popper"], "title": "GEPOC Parameters - Open Source Parametrisation and Validation for Austria, Version 2.0", "categories": ["cs.AI", "cs.CY", "62-11", "E.5; G.3; I.6.4; I.6.6; J.3; J.4"], "comment": "134 pages, 75 figures, 19 tables", "summary": "GEPOC, short for Generic Population Concept, is a collection of models and\nmethods for analysing population-level research questions. For the valid\napplication of the models for a specific country or region, stable and\nreproducible data processes are necessary, which provide valid and ready-to-use\nmodel parameters. This work contains a complete description of the\ndata-processing methods for computation of model parameters for Austria, based\nexclusively on freely and publicly accessible data. In addition to the\ndescription of the source data used, this includes all algorithms used for\naggregation, disaggregation, fusion, cleansing or scaling of the data, as well\nas a description of the resulting parameter files. The document places\nparticular emphasis on the computation of parameters for the most important\nGEPOC model, GEPOC ABM, a continuous-time agent-based population model. An\nextensive validation study using this particular model was made and is\npresented at the end of this work.", "AI": {"tldr": "This paper discusses the Generic Population Concept (GEPOC), focusing on models and data-processing methods for population-level research in Austria using publicly accessible data.", "motivation": "To establish a stable and reproducible data process for reliable population-level modeling using publicly available data.", "method": "Detailed computational methods for deriving population model parameters through data aggregation, disaggregation, fusion, cleansing, and scaling are presented.", "result": "Parameters for the GEPOC ABM model are computed and validated through an extensive study.", "conclusion": "The methods described provide validated and ready-to-use model parameters suitable for Austria and emphasize reproducibility with publicly accessible data sources."}}
{"id": "2511.00294", "pdf": "https://arxiv.org/pdf/2511.00294", "abs": "https://arxiv.org/abs/2511.00294", "authors": ["Lucas Almeida", "Maycon Peixoto"], "title": "Tetris: An SLA-aware Application Placement Strategy in the Edge-Cloud Continuum", "categories": ["cs.DC", "cs.NI", "68M14", "C.2.4"], "comment": "10 pages, 7 sections, 12 figures, 9 tables", "summary": "An Edge-Cloud Continuum integrates edge and cloud resources to provide a\nflexible and scalable infrastructure. This paradigm can minimize latency by\nprocessing data closer to the source at the edge while leveraging the vast\ncomputational power of the cloud for more intensive tasks. In this context,\nmodule application placement requires strategic allocation plans that align\nuser demands with infrastructure constraints, aiming for efficient resource\nuse. Therefore, we propose Tetris, an application placement strategy that\nutilizes a heuristic algorithm to distribute computational services across edge\nand cloud resources efficiently. Tetris prioritizes services based on SLA\nurgencies and resource efficiency to avoid system overloading. Our results\ndemonstrate that Tetris reduces SLA violations by approximately 76% compared to\nthe baseline method, which serves as a reference point for benchmarking\nperformance in this scenario. Therefore, Tetris offers an effective placement\napproach for managing latency-sensitive applications in Edge-Cloud Continuum\nenvironments, enhancing Quality of Service (QoS) for users.", "AI": {"tldr": "The paper introduces Tetris, a heuristic application placement strategy for Edge-Cloud Continuum environments, prioritizing SLA and resource efficiency to reduce latency and improve QoS.", "motivation": "To address the challenge of strategically placing application modules in Edge-Cloud Continuum infrastructures, minimizing latency while meeting SLAs and maximizing resource use.", "method": "The study proposes Tetris, a heuristic algorithm-based application placement strategy focusing on resource efficiency and SLA urgency to strategically allocate computational services across edge and cloud.", "result": "Tetris reduces SLA violations by approximately 76% compared to baseline methods, showcasing its effectiveness in improving resource use and QoS.", "conclusion": "Tetris demonstrates its capability to enhance application placement in Edge-Cloud Continuum systems, effectively reducing latency and SLA violations, and improving user experience."}}
{"id": "2511.00592", "pdf": "https://arxiv.org/pdf/2511.00592", "abs": "https://arxiv.org/abs/2511.00592", "authors": ["Massinissa Merouani", "Islem Kara Bernou", "Riyadh Baghdadi"], "title": "Agentic Auto-Scheduling: An Experimental Study of LLM-Guided Loop Optimization", "categories": ["cs.PL", "cs.DC", "cs.LG", "cs.PF"], "comment": "Accepted at the 34th International Conference on Parallel\n  Architectures and Compilation Techniques (PACT 2025). 12 pages, plus appendix", "summary": "Automatic code optimization remains a difficult challenge, particularly for\ncomplex loop nests on modern hardware. This paper investigates a novel approach\nto code optimization where Large Language Models (LLMs) guide the process\nthrough a closed-loop interaction with a compiler. We present ComPilot, an\nexperimental framework that leverages off-the-shelf LLMs, without any\ntask-specific fine-tuning, as interactive optimization agents. ComPilot\nestablishes a feedback loop where an LLM proposes transformations for a given\nloop nest to a compiler. The compiler attempts the transformations, reporting\nback legality status and measured speedup or slowdown. The LLM utilizes this\nconcrete feedback to iteratively refine its optimization strategy. Our\nextensive evaluation across the PolyBench benchmark suite demonstrates the\neffectiveness of this zero-shot approach. ComPilot achieves geometric mean\nspeedups of 2.66x (single run) and 3.54x (best-of-5 runs) over the original\ncode. Furthermore, ComPilot demonstrates competitive performance against the\nstate-of-the-art Pluto polyhedral optimizer, outperforming it in many cases.\nThis experimental study demonstrates that general-purpose LLMs can effectively\nguide the code optimization process when grounded by compiler feedback, opening\npromising research directions for agentic AI in code optimization.", "AI": {"tldr": "This paper introduces ComPilot, an approach using general-purpose Large Language Models (LLMs) in a feedback loop with compilers for loop nest code optimization.", "motivation": "Code optimization for complex loop nests on modern hardware remains challenging.", "method": "ComPilot employs off-the-shelf LLMs in a closed-loop interaction with compilers, where the LLM proposes transformations and iteratively refines them based on compiler feedback.", "result": "Using the PolyBench benchmarks, ComPilot achieved significant speedups (2.66x single run, 3.54x best-of-5 runs) and surpassed the Pluto optimizer in many cases.", "conclusion": "General-purpose LLMs, grounded by compiler feedback, can effectively guide code optimization, suggesting new avenues in agentic AI research for optimization tasks."}}
{"id": "2511.00180", "pdf": "https://arxiv.org/pdf/2511.00180", "abs": "https://arxiv.org/abs/2511.00180", "authors": ["Nicky Pochinkov", "Yulia Volkova", "Anna Vasileva", "Sai V R Chereddy"], "title": "ParaScopes: What do Language Models Activations Encode About Future Text?", "categories": ["cs.CL", "cs.LG"], "comment": "Main paper: 9 pages, 10 figures. Total 24 pages", "summary": "Interpretability studies in language models often investigate forward-looking\nrepresentations of activations. However, as language models become capable of\ndoing ever longer time horizon tasks, methods for understanding activations\noften remain limited to testing specific concepts or tokens. We develop a\nframework of Residual Stream Decoders as a method of probing model activations\nfor paragraph-scale and document-scale plans. We test several methods and find\ninformation can be decoded equivalent to 5+ tokens of future context in small\nmodels. These results lay the groundwork for better monitoring of language\nmodels and better understanding how they might encode longer-term planning\ninformation.", "AI": {"tldr": "The paper introduces Residual Stream Decoders for probing language models' activations, focusing on how they encode longer-term planning information for paragraph and document-scale tasks.", "motivation": "To address the gap in understanding how language models encode activations for long time-horizon tasks rather than solely testing concepts or tokens.", "method": "Developing and evaluating Residual Stream Decoders to probe model activations for longer term plans and decoding future context in small models.", "result": "Demonstrated that information equivalent to 5+ tokens of future context can be decoded from activations in small models.", "conclusion": "The findings contribute to monitoring language models and understanding their handling of longer-term planning information."}}
{"id": "2511.00125", "pdf": "https://arxiv.org/pdf/2511.00125", "abs": "https://arxiv.org/abs/2511.00125", "authors": ["\u00c1lvaro Silva", "Alexandra Mendes", "Ruben Martins"], "title": "Inferring multiple helper Dafny assertions with LLMs", "categories": ["cs.SE", "cs.AI", "cs.LO", "cs.PL"], "comment": null, "summary": "The Dafny verifier provides strong correctness guarantees but often requires\nnumerous manual helper assertions, creating a significant barrier to adoption.\nWe investigate the use of Large Language Models (LLMs) to automatically infer\nmissing helper assertions in Dafny programs, with a primary focus on cases\ninvolving multiple missing assertions. To support this study, we extend the\nDafnyBench benchmark with curated datasets where one, two, or all assertions\nare removed, and we introduce a taxonomy of assertion types to analyze\ninference difficulty. Our approach refines fault localization through a hybrid\nmethod that combines LLM predictions with error-message heuristics. We\nimplement this approach in a new tool called DAISY (Dafny Assertion Inference\nSYstem). While our focus is on multiple missing assertions, we also evaluate\nDAISY on single-assertion cases. DAISY verifies 63.4% of programs with one\nmissing assertion and 31.7% with multiple missing assertions. Notably, many\nprograms can be verified with fewer assertions than originally present,\nhighlighting that proofs often admit multiple valid repair strategies and that\nrecovering every original assertion is unnecessary. These results demonstrate\nthat automated assertion inference can substantially reduce proof engineering\neffort and represent a step toward more scalable and accessible formal\nverification.", "AI": {"tldr": "The paper explores using Large Language Models (LLMs) to infer missing assertions in Dafny programs, implementing a tool called DAISY, showcasing improved verification efficiency.", "motivation": "Manually adding helper assertions in Dafny programs presents a barrier to adoption due to the considerable effort required, especially when assertions are missing.", "method": "The authors developed DAISY (Dafny Assertion Inference SYstem), combining LLM predictions with error-message heuristics, supported by datasets where assertions are curated at varying levels of absence.", "result": "DAISY verifies 63.4% of programs with one missing assertion and 31.7% with multiple missing assertions, showcasing that proofs can be validly repaired with fewer assertions.", "conclusion": "Automated inference can greatly reduce manual proof efforts in formal verification, increasing scalability and accessibility while revealing multiple valid proof strategies."}}
{"id": "2511.00041", "pdf": "https://arxiv.org/pdf/2511.00041", "abs": "https://arxiv.org/abs/2511.00041", "authors": ["Yingzhao Jian", "Zhongan Wang", "Yi Yang", "Hehe Fan"], "title": "Endowing GPT-4 with a Humanoid Body: Building the Bridge Between Off-the-Shelf VLMs and the Physical World", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Humanoid agents often struggle to handle flexible and diverse interactions in\nopen environments. A common solution is to collect massive datasets to train a\nhighly capable model, but this approach can be prohibitively expensive. In this\npaper, we explore an alternative solution: empowering off-the-shelf\nVision-Language Models (VLMs, such as GPT-4) to control humanoid agents,\nthereby leveraging their strong open-world generalization to mitigate the need\nfor extensive data collection. To this end, we present \\textbf{BiBo}\n(\\textbf{B}uilding humano\\textbf{I}d agent \\textbf{B}y \\textbf{O}ff-the-shelf\nVLMs). It consists of two key components: (1) an \\textbf{embodied instruction\ncompiler}, which enables the VLM to perceive the environment and precisely\ntranslate high-level user instructions (e.g., {\\small\\itshape ``have a rest''})\ninto low-level primitive commands with control parameters (e.g.,\n{\\small\\itshape ``sit casually, location: (1, 2), facing: 90$^\\circ$''}); and\n(2) a diffusion-based \\textbf{motion executor}, which generates human-like\nmotions from these commands, while dynamically adapting to physical feedback\nfrom the environment. In this way, BiBo is capable of handling not only basic\ninteractions but also diverse and complex motions. Experiments demonstrate that\nBiBo achieves an interaction task success rate of 90.2\\% in open environments,\nand improves the precision of text-guided motion execution by 16.3\\% over prior\nmethods. The code will be made publicly available.", "AI": {"tldr": "This paper introduces BiBo, a system to control humanoid agents using off-the-shelf Vision-Language Models, reducing the need for extensive datasets. The method involves translating high-level instructions into commands and performing human-like actions.", "motivation": "Humanoid agents face challenges in handling diverse, flexible interactions in open environments. Current solutions, like collecting large datasets, are expensive, making alternative approaches necessary.", "method": "BiBo consists of two components: (1) an embodied instruction compiler for translating high-level instructions into low-level commands; (2) a diffusion-based motion executor for generating human-like motions that adjust to environmental feedback.", "result": "BiBo achieves a 90.2% task success rate for interaction in open environments and improves text-guided motion execution precision by 16.3% compared to prior methods.", "conclusion": "The proposed BiBo system effectively leverages existing Vision-Language Models to enhance humanoid agent control, offering a cost-effective and efficient alternative to data-intensive methods."}}
{"id": "2511.00030", "pdf": "https://arxiv.org/pdf/2511.00030", "abs": "https://arxiv.org/abs/2511.00030", "authors": ["Myeongseob Ko", "Hoang Anh Just", "Charles Fleming", "Ming Jin", "Ruoxi Jia"], "title": "Probing Knowledge Holes in Unlearned LLMs", "categories": ["cs.LG", "cs.AI"], "comment": "The Thirty-ninth Annual Conference on Neural Information Processing\n  Systems", "summary": "Machine unlearning has emerged as a prevalent technical solution for\nselectively removing unwanted knowledge absorbed during pre-training, without\nrequiring full retraining. While recent unlearning techniques can effectively\nremove undesirable content without severely compromising performance on\nstandard benchmarks, we find that they may inadvertently create ``knowledge\nholes'' -- unintended losses of benign knowledge that standard benchmarks fail\nto capture. To probe where unlearned models reveal knowledge holes, we propose\na test case generation framework that explores both immediate neighbors of\nunlearned content and broader areas of potential failures. Our evaluation\ndemonstrates significant hidden costs of unlearning: up to 98.7\\% of the test\ncases yield irrelevant or nonsensical responses from unlearned models, despite\nbeing answerable by the pretrained model. These findings necessitate rethinking\nthe conventional approach to evaluating knowledge preservation in unlearning,\nmoving beyond standard, static benchmarks.", "AI": {"tldr": "This paper examines how machine unlearning, designed to remove unwanted pre-training knowledge, can inadvertently create \"knowledge holes\" that existing benchmarks fail to detect, impacting knowledge preservation.", "motivation": "Machine unlearning aims to selectively eliminate specific information from pre-trained models without needing full retraining, addressing ethical and regulatory challenges.", "method": "The authors introduce a test case generation framework to investigate unintended losses of knowledge caused by unlearning, targeting areas adjacent to the removed content and identifying potential broader failures.", "result": "Up to 98.7% of generated test cases exposed irrelevant or nonsensical responses in unlearned models, highlighting significant knowledge preservation issues compared to pre-trained models.", "conclusion": "Standard benchmarks for evaluating unlearning may overlook critical unintended side effects. A new evaluation approach is necessary to better assess knowledge retention and preservation."}}
{"id": "2511.00022", "pdf": "https://arxiv.org/pdf/2511.00022", "abs": "https://arxiv.org/abs/2511.00022", "authors": ["Jules Gerard", "Leandro Di Bella", "Filip Huyghe", "Marc Kochzius"], "title": "Automating Coral Reef Fish Family Identification on Video Transects Using a YOLOv8-Based Deep Learning Pipeline", "categories": ["cs.CV"], "comment": "Accepted to EUVIP2025, student session", "summary": "Coral reef monitoring in the Western Indian Ocean is limited by the labor\ndemands of underwater visual censuses. This work evaluates a YOLOv8-based deep\nlearning pipeline for automating family-level fish identification from video\ntransects collected in Kenya and Tanzania. A curated dataset of 24 families was\ntested under different configurations, providing the first region-specific\nbenchmark for automated reef fish monitoring in the Western Indian Ocean. The\nbest model achieved mAP@0.5 of 0.52, with high accuracy for abundant families\nbut weaker detection of rare or complex taxa. Results demonstrate the potential\nof deep learning as a scalable complement to traditional monitoring methods.", "AI": {"tldr": "The paper proposes using YOLOv8 deep learning for automated fish identification in the Western Indian Ocean, achieving moderate accuracy in detecting fish families from video transects.", "motivation": "The motivation is to address the limited monitoring of coral reefs in the Western Indian Ocean due to labor-intensive underwater visual censuses.", "method": "The method involves developing a YOLOv8-based deep learning pipeline tested on a curated dataset of 24 fish families from video transect data.", "result": "The best model achieved mAP@0.5 of 0.52, showcasing high accuracy for common families but weaker performance for rare or complex taxa.", "conclusion": "Deep learning shows promise as a scalable alternative to traditional reef monitoring methods, although improvements are needed for rare taxa detection."}}
{"id": "2511.00745", "pdf": "https://arxiv.org/pdf/2511.00745", "abs": "https://arxiv.org/abs/2511.00745", "authors": ["Xiaoyang Tian", "Hui Wang", "Boshuo Wang", "Jinshui Zhang", "Dong Yan", "Jeannette Ingabire", "Samantha Coffler", "Guillaume Duret", "Quoc-Khanh Pham", "Gang Bao", "Jacob T. Robinson", "Stefan M. Goetz", "Angel V. Peterchev"], "title": "High-Power Dual-Channel Field Chamber for High-Frequency Magnetic Neuromodulation", "categories": ["eess.SY", "cs.SY", "physics.med-ph", "q-bio.NC"], "comment": "25 pages, 8 figures", "summary": "Several novel methods, including magnetogenetics and magnetoelectric\nstimulation, use high frequency alternating magnetic fields to precisely\nmanipulate neural activity. To quantify the behavioral effects of such\ninterventions in a freely moving mouse, we developed a dual-channel magnetic\nchamber, specifically designed for rate-sensitive magnetothermal-genetic\nstimulation, and adaptable for other uses of alternating magnetic fields.\nThrough an optimized coil design, the system allows independent control of two\nspatially orthogonal uniform magnetic fields delivered at different frequencies\nwithin a 10 cm x 10 cm x 6 cm chamber. The two channels have nominal\nfrequencies of 50 and 550 kHz with peak magnetic field strengths of 88 and 12.5\nmT, achieved with resonant coil drives having peak voltages of 1.6 and 1.8 kV\nand currents of 1.0 and 0.26 kA, respectively. Additionally, a liquid cooling\nsystem enables magnetic field generation for second-level duration, and an\nobservation port and camera allow video capture of the animal's behavior within\nthe chamber. The system generates high-amplitude magnetic fields across two\nwidely separated frequency channels with negligible interference (< 1%).\nRelatively uniform magnetic field distribution (+/-10% across 94% of the\nchamber volume) is maintained throughout the chamber, and temperature increase\nof the inner side of the coil enclosure during the operation is limited to <\n0.35 {\\deg}C/s to ensure in vivo safety. Using cobalt-doped and undoped iron\noxide nanoparticles, we demonstrate channel-specific heating rates of 3.5\n{\\deg}C/s and 1.5 {\\deg}C/s, respectively, validating frequency-selectivity.\nBoth channels can run continuously for four seconds stably.", "AI": {"tldr": "The paper introduces a dual-channel magnetic chamber for manipulating neural activity using alternating magnetic fields. It allows controlled stimulation in mice using optimized coil design and ensures safety measures.", "motivation": "The motivation is to effectively quantify behavioral effects of magnetic stimulation techniques (e.g., magnetogenetics) in freely moving mice, while ensuring control, precision, safety, and reliability.", "method": "A dual-channel magnetic chamber was designed with optimized coils, delivering uniform magnetic fields at different frequencies. Safety measures like liquid cooling and temperature control were integrated.", "result": "The chamber achieved high-frequency magnetic field generation with negligible interference, uniform distribution, and stable operations. Spectral heating validation demonstrated channel-specific control with cobalt-doped nanoparticles.", "conclusion": "The novel system provides reliable and safe dual-frequency magnetic stimulation for in vivo applications, showcasing channel-specific precision and behavioral observation capabilities."}}
{"id": "2511.00490", "pdf": "https://arxiv.org/pdf/2511.00490", "abs": "https://arxiv.org/abs/2511.00490", "authors": ["Gero Junike", "Marco Oesting"], "title": "Accuracy estimation of neural networks by extreme value theory", "categories": ["stat.ML", "cs.LG", "math.PR"], "comment": null, "summary": "Neural networks are able to approximate any continuous function on a compact\nset. However, it is not obvious how to quantify the error of the neural\nnetwork, i.e., the remaining bias between the function and the neural network.\nHere, we propose the application of extreme value theory to quantify large\nvalues of the error, which are typically relevant in applications. The\ndistribution of the error beyond some threshold is approximately generalized\nPareto distributed. We provide a new estimator of the shape parameter of the\nPareto distribution suitable to describe the error of neural networks.\nNumerical experiments are provided.", "AI": {"tldr": "This paper proposes using extreme value theory to analyze and quantify the large errors of neural networks, introducing a new estimator for the shape parameter of the error distribution.", "motivation": "To provide a technique to quantify the error of neural networks in approximating functions, especially the large errors relevant in practical applications.", "method": "Extreme Value Theory is applied to model the error distribution of neural networks beyond a certain threshold as generalized Pareto distributed, along with the development of a new shape parameter estimator.", "result": "Numerical experiments demonstrate the suitability of the proposed estimator for describing the error distribution in neural networks.", "conclusion": "Applying extreme value theory improves the ability to quantify significant error values in neural networks, with a new estimator enhancing this quantification."}}
{"id": "2511.00750", "pdf": "https://arxiv.org/pdf/2511.00750", "abs": "https://arxiv.org/abs/2511.00750", "authors": ["Kokila Kasuni Perera", "Frank Neumann", "Aneta Neumann"], "title": "Trust Region-Based Bayesian Optimisation to Discover Diverse Solutions", "categories": ["cs.NE", "cs.LG"], "comment": null, "summary": "Bayesian optimisation (BO) is a surrogate-based optimisation technique that\nefficiently solves expensive black-box functions with small evaluation budgets.\nRecent studies consider trust regions to improve the scalability of BO\napproaches when the problem space scales to more dimensions. Motivated by this\nresearch, we explore the effectiveness of trust region-based BO algorithms for\ndiversity optimisation in different dimensional black box problems. We propose\ndiversity optimisation approaches extending TuRBO1, which is the first BO\nmethod that uses a trust region-based approach for scalability. We extend\nTuRBO1 as divTuRBO1, which finds an optimal solution while maintaining a given\ndistance threshold relative to a reference solution set. We propose two\napproaches to find diverse solutions for black-box functions by combining\ndivTuRBO1 runs in a sequential and an interleaving fashion. We conduct\nexperimental investigations on the proposed algorithms and compare their\nperformance with that of the baseline method, ROBOT (rank-ordered Bayesian\noptimisation with trust regions). We evaluate proposed algorithms on benchmark\nfunctions with dimensions 2 to 20. Experimental investigations demonstrate that\nthe proposed methods perform well, particularly in larger dimensions, even with\na limited evaluation budget.", "AI": {"tldr": "This paper explores the use of trust region-based Bayesian optimisation (BO) for diversity optimisation in high-dimensional black-box problems, proposing novel methods and showing their effectiveness against baseline algorithms.", "motivation": "To address scalability issues in Bayesian optimisation when dealing with high-dimensional problems and explore diversity optimisation using trust regions.", "method": "The paper introduces divTuRBO1, an extension of TuRBO1, for maintaining diversity thresholds and suggests sequential and interleaving approaches to combine divTuRBO1 runs.", "result": "Proposed methods are experimentally tested on benchmark functions ranging from 2 to 20 dimensions, showing better performance than the baseline method, ROBOT, especially in higher dimensions.", "conclusion": "Trust region-based BO methods, particularly the introduced approaches, are effective for scalable and diverse optimisation in high-dimensional black-box problems with limited evaluation budgets."}}
{"id": "2511.00092", "pdf": "https://arxiv.org/pdf/2511.00092", "abs": "https://arxiv.org/abs/2511.00092", "authors": ["Shunya Minami", "Tatsuya Ishigaki", "Ikko Hamamura", "Taku Mikuriya", "Youmi Ma", "Naoaki Okazaki", "Hiroya Takamura", "Yohichi Suzuki", "Tadashi Kadowaki"], "title": "QuantumBench: A Benchmark for Quantum Problem Solving", "categories": ["cs.AI", "cs.CL", "cs.LG", "quant-ph"], "comment": "11 pages, 8 figures", "summary": "Large language models are now integrated into many scientific workflows,\naccelerating data analysis, hypothesis generation, and design space\nexploration. In parallel with this growth, there is a growing need to carefully\nevaluate whether models accurately capture domain-specific knowledge and\nnotation, since general-purpose benchmarks rarely reflect these requirements.\nThis gap is especially clear in quantum science, which features non-intuitive\nphenomena and requires advanced mathematics. In this study, we introduce\nQuantumBench, a benchmark for the quantum domain that systematically examine\nhow well LLMs understand and can be applied to this non-intuitive field. Using\npublicly available materials, we compiled approximately 800 questions with\ntheir answers spanning nine areas related to quantum science and organized them\ninto an eight-option multiple-choice dataset. With this benchmark, we evaluate\nseveral existing LLMs and analyze their performance in the quantum domain,\nincluding sensitivity to changes in question format. QuantumBench is the first\nLLM evaluation dataset built for the quantum domain, and it is intended to\nguide the effective use of LLMs in quantum research.", "AI": {"tldr": "The study introduces QuantumBench, a quantum-specific benchmark for testing large language models' (LLMs) understanding and application in the quantum science domain.", "motivation": "As the integration of LLMs into scientific tasks grows, there's a pressing need to ensure models can successfully handle domain-specific knowledge, particularly in unique fields like quantum science.", "method": "The authors compiled 800 questions from quantum science across nine areas into an eight-option multiple-choice dataset to evaluate how well LLMs perform in understanding quantum concepts.", "result": "QuantumBench revealed insights into the capabilities and limitations of existing LLMs in quantum science, including their sensitivity to question formatting.", "conclusion": "QuantumBench serves as the first evaluation dataset specifically targeted at quantum science, aiming to improve the effective application of LLMs in this complex domain."}}
{"id": "2511.00603", "pdf": "https://arxiv.org/pdf/2511.00603", "abs": "https://arxiv.org/abs/2511.00603", "authors": ["Yubo Wang", "Yubo Cui", "Tuo Shi", "Danyang Li", "Wenxin Li", "Lide Suo", "Tao Wang", "Xin Xie"], "title": "EPARA: Parallelizing Categorized AI Inference in Edge Clouds", "categories": ["cs.DC", "cs.AI", "cs.NI", "68T05", "I.2.11"], "comment": "15 pages,20 figures", "summary": "With the increasing adoption of AI applications such as large language models\nand computer vision AI, the computational demands on AI inference systems are\ncontinuously rising, making the enhancement of task processing capacity using\nexisting hardware a primary objective in edge clouds. We propose EPARA, an\nend-to-end AI parallel inference framework in edge, aimed at enhancing the edge\nAI serving capability. Our key idea is to categorize tasks based on their\nsensitivity to latency/frequency and requirement for GPU resources, thereby\nachieving both request-level and service-level task-resource allocation. EPARA\nconsists of three core components: 1) a task-categorized parallelism allocator\nthat decides the parallel mode of each task, 2) a distributed request handler\nthat performs the calculation for the specific request, and 3) a state-aware\nscheduler that periodically updates service placement in edge clouds. We\nimplement a EPARA prototype and conduct a case study on the EPARA operation for\nLLMs and segmentation tasks. Evaluation through testbed experiments involving\nedge servers, embedded devices, and microcomputers shows that EPARA achieves up\nto 2.1$\\times$ higher goodput in production workloads compared to prior\nframeworks, while adapting to various edge AI inference tasks.", "AI": {"tldr": "EPARA enhances edge AI inference by categorizing tasks based on latency and GPU needs, achieving 2.1x higher goodput than other frameworks.", "motivation": "The motivation is to address rising computational demands from AI applications in edge clouds and to optimize task processing capabilities using existing hardware.", "method": "EPARA employs task categorization based on latency and GPU needs, using three core components: a parallelism allocator, a distributed request handler, and a state-aware scheduler.", "result": "EPARA outperforms prior frameworks with up to 2.1x higher goodput in tests involving real-world edge AI tasks.", "conclusion": "EPARA demonstrates significant improvements in edge AI serving, effectively optimizing task allocation and resource usage for various AI inference tasks."}}
{"id": "2511.00740", "pdf": "https://arxiv.org/pdf/2511.00740", "abs": "https://arxiv.org/abs/2511.00740", "authors": ["Igor Engel", "Ekaterina Verbitskaia"], "title": "Typed Embedding of miniKanren for Functional Conversion", "categories": ["cs.PL"], "comment": null, "summary": "Relational programming enables program synthesis through a verifier-to-solver\napproach. An earlier paper introduced a functional conversion that mitigated\nsome of the inherent performance overhead. However, the conversion was\ninelegant: it was oblivious to types, demanded determinism annotations, and\nimplicit generator threading. In this paper, we address these issues by\nproviding a typed tagless-final embedding of miniKanren into Haskell. This\nimprovement significantly reduces boilerplate while preserving, and sometimes\nenhancing, earlier speedups.", "AI": {"tldr": "This paper presents an improved embedding of miniKanren into Haskell, addressing inefficiencies and reducing boilerplate in relational programming.", "motivation": "To tackle inefficiencies and inelegance in prior functional conversions for relational programming using miniKanren, especially concerning performance and usability.", "method": "The authors propose a typed, tagless-final embedding of miniKanren into Haskell, enhancing code structure and utility while maintaining performance benefits.", "result": "The new embedding reduces boilerplate, avoids the need for determinism annotations and implicit generator threading, and preserves earlier speedups with potential enhancements.", "conclusion": "The paper demonstrates the effectiveness of the proposed embedding in improving the usability and performance of relational programming in Haskell, suggesting broader applicability."}}
{"id": "2511.00198", "pdf": "https://arxiv.org/pdf/2511.00198", "abs": "https://arxiv.org/abs/2511.00198", "authors": ["Chun-Hao Yang", "Bo-Han Feng", "Tzu-Yuan Lai", "Yan Yu Chen", "Yin-Kai Dean Huang", "Shou-De Lin"], "title": "Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Optimizing training performance in large language models (LLMs) remains an\nessential challenge, particularly in improving model performance while\nmaintaining computational costs. This work challenges the conventional approach\nof training LLMs using next-token prediction (NTP), arguing that by predicting\ninformation-rich tokens during training, there is a more effective way to train\nLLMs. We investigate the impact of the proposed solution in three kinds of\ntasks for LLMs: arithmetic, multi-label classification of text, and\nnatural-language generation. This work offers a principled approach to\noptimizing LLM training, advancing both model performance and theoretical\nunderstanding of the target-token selection strategies.", "AI": {"tldr": "The paper explores optimizing large language models (LLMs) through targeted token prediction instead of next-token prediction, aiming to improve performance and computational efficiency.", "motivation": "The study aims to address the challenge of enhancing model performance while controlling computational costs in training large language models (LLMs).", "method": "The authors propose a novel training strategy where LLMs predict information-rich tokens rather than rely on next-token prediction. The approach is tested on arithmetic, text classification, and natural-language generation tasks.", "result": "By employing the new targeted token prediction method, they demonstrate improvements in training efficiency and model performance across various tasks.", "conclusion": "The approach provides a significant step forward in the optimization of LLM training methods, contributing to both practical performance enhancement and theoretical insights into token selection strategies."}}
{"id": "2511.00160", "pdf": "https://arxiv.org/pdf/2511.00160", "abs": "https://arxiv.org/abs/2511.00160", "authors": ["Katherine A. Rosenfeld", "Cliff C. Kerr", "Jessica Lundin"], "title": "What a diff makes: automating code migration with large language models", "categories": ["cs.SE", "cs.AI"], "comment": "10 pages, 8 figures", "summary": "Modern software programs are built on stacks that are often undergoing\nchanges that introduce updates and improvements, but may also break any project\nthat depends upon them. In this paper we explore the use of Large Language\nModels (LLMs) for code migration, specifically the problem of maintaining\ncompatibility with a dependency as it undergoes major and minor semantic\nversion changes. We demonstrate, using metrics such as test coverage and change\ncomparisons, that contexts containing diffs can significantly improve\nperformance against out of the box LLMs and, in some cases, perform better than\nusing code. We provide a dataset to assist in further development of this\nproblem area, as well as an open-source Python package, AIMigrate, that can be\nused to assist with migrating code bases. In a real-world migration of\nTYPHOIDSIM between STARSIM versions, AIMigrate correctly identified 65% of\nrequired changes in a single run, increasing to 80% with multiple runs, with\n47% of changes generated perfectly.", "AI": {"tldr": "The paper explores using Large Language Models (LLMs) for maintaining code compatibility during dependency changes and introduces AIMigrate, a tool for code migration.", "motivation": "The motivation is to address the challenges of code migration and compatibility when software dependencies undergo major or minor updates, breaking dependent projects.", "method": "The method involves leveraging LLMs with diff-based contexts to improve test coverage and performance in automating code migration. AIMigrate, a Python tool aiding code adaptation, is introduced.", "result": "AIMigrate achieved a 65% correct change identification rate in one run, which increased to 80% with multiple runs. Additionally, 47% of changes were generated perfectly.", "conclusion": "Using LLMs with diffs can enhance performance in code migration scenarios. AIMigrate shows potential in assisting real-world code migrations and improving compatibility with changing dependencies."}}
{"id": "2511.00088", "pdf": "https://arxiv.org/pdf/2511.00088", "abs": "https://arxiv.org/abs/2511.00088", "authors": ["NVIDIA", ":", "Yan Wang", "Wenjie Luo", "Junjie Bai", "Yulong Cao", "Tong Che", "Ke Chen", "Yuxiao Chen", "Jenna Diamond", "Yifan Ding", "Wenhao Ding", "Liang Feng", "Greg Heinrich", "Jack Huang", "Peter Karkus", "Boyi Li", "Pinyi Li", "Tsung-Yi Lin", "Dongran Liu", "Ming-Yu Liu", "Langechuan Liu", "Zhijian Liu", "Jason Lu", "Yunxiang Mao", "Pavlo Molchanov", "Lindsey Pavao", "Zhenghao Peng", "Mike Ranzinger", "Ed Schmerling", "Shida Shen", "Yunfei Shi", "Sarah Tariq", "Ran Tian", "Tilman Wekel", "Xinshuo Weng", "Tianjun Xiao", "Eric Yang", "Xiaodong Yang", "Yurong You", "Xiaohui Zeng", "Wenyuan Zhang", "Boris Ivanovic", "Marco Pavone"], "title": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "End-to-end architectures trained via imitation learning have advanced\nautonomous driving by scaling model size and data, yet performance remains\nbrittle in safety-critical long-tail scenarios where supervision is sparse and\ncausal understanding is limited. To address this, we introduce Alpamayo-R1\n(AR1), a vision-language-action model (VLA) that integrates Chain of Causation\nreasoning with trajectory planning to enhance decision-making in complex\ndriving scenarios. Our approach features three key innovations: (1) the Chain\nof Causation (CoC) dataset, built through a hybrid auto-labeling and\nhuman-in-the-loop pipeline producing decision-grounded, causally linked\nreasoning traces aligned with driving behaviors; (2) a modular VLA architecture\ncombining Cosmos-Reason, a Vision-Language Model pre-trained for Physical AI\napplications, with a diffusion-based trajectory decoder that generates\ndynamically feasible plans in real time; (3) a multi-stage training strategy\nusing supervised fine-tuning to elicit reasoning and reinforcement learning\n(RL) to optimize reasoning quality via large reasoning model feedback and\nenforce reasoning-action consistency. Evaluation shows AR1 achieves up to a 12%\nimprovement in planning accuracy on challenging cases compared to a\ntrajectory-only baseline, with a 35% reduction in off-road rate and 25%\nreduction in close encounter rate in closed-loop simulation. RL post-training\nimproves reasoning quality by 45% as measured by a large reasoning model critic\nand reasoning-action consistency by 37%. Model scaling from 0.5B to 7B\nparameters shows consistent improvements. On-vehicle road tests confirm\nreal-time performance (99 ms latency) and successful urban deployment. By\nbridging interpretable reasoning with precise control, AR1 demonstrates a\npractical path towards Level 4 autonomous driving. We plan to release AR1\nmodels and a subset of the CoC in a future update.", "AI": {"tldr": "This paper introduces Alpamayo-R1 (AR1), an advanced vision-language-action model for autonomous driving, focusing on reasoning and planning in complex scenarios.", "motivation": "Autonomous driving systems struggle with safety-critical situations due to sparse supervision and limited causal understanding.", "method": "The paper integrates a Chain of Causation dataset, a modular VLA architecture blending vision-language reasoning with trajectory planning, and multi-stage supervised and reinforcement learning techniques.", "result": "AR1 achieves better planning accuracy, reasoning quality, and reduced error rates in simulations and real-world tests, scaling effectively up to 7B parameters.", "conclusion": "AR1 improves reasoning and control for Level 4 autonomous driving, with real-time urban testing and plans to release models and datasets."}}
{"id": "2511.00032", "pdf": "https://arxiv.org/pdf/2511.00032", "abs": "https://arxiv.org/abs/2511.00032", "authors": ["Lei Liu", "Zhongyi Yu", "Hong Wang", "Huanshuo Dong", "Haiyang Xin", "Hongwei Zhao", "Bin Li"], "title": "From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In recent years, Neural Operators(NO) have gradually emerged as a popular\napproach for solving Partial Differential Equations (PDEs). However, their\napplication to large-scale engineering tasks suffers from significant\ncomputational overhead. And the fact that current models impose a uniform\ncomputational cost while physical fields exhibit vastly different complexities\nconstitutes a fundamental mismatch, which is the root of this inefficiency. For\ninstance, in turbulence flows, intricate vortex regions require deeper network\nprocessing compared to stable flows. To address this, we introduce a framework:\nSkip-Block Routing (SBR), a general framework designed for Transformer-based\nneural operators, capable of being integrated into their multi-layer\narchitectures. First, SBR uses a routing mechanism to learn the complexity and\nranking of tokens, which is then applied during inference. Then, in later\nlayers, it decides how many tokens are passed forward based on this ranking.\nThis way, the model focuses more processing capacity on the tokens that are\nmore complex. Experiments demonstrate that SBR is a general framework that\nseamlessly integrates into various neural operators. Our method reduces\ncomputational cost by approximately 50% in terms of Floating Point Operations\n(FLOPs), while still delivering up to 2x faster inference without sacrificing\naccuracy.", "AI": {"tldr": "The paper introduces Skip-Block Routing (SBR), a framework designed for Transformer-based neural operators to enhance computational efficiency in solving PDEs.", "motivation": "Neural operators are promising for PDEs but inefficient in dealing with fields of varying complexity, such as turbulence flows.", "method": "SBR uses a routing mechanism to rank token complexity and selectively process tokens in later layers, optimizing resource allocation.", "result": "SBR reduces computational cost by ~50% (FLOPs) and achieves up to 2x faster inference speed without compromising accuracy.", "conclusion": "Skip-Block Routing is a general solution that integrates well into neural operators, optimizing performance and efficiency for PDE-related tasks."}}
{"id": "2511.00028", "pdf": "https://arxiv.org/pdf/2511.00028", "abs": "https://arxiv.org/abs/2511.00028", "authors": ["Hanyang Chen", "Yanchao Yang"], "title": "Mutual Information guided Visual Contrastive Learning", "categories": ["cs.CV", "cs.AI"], "comment": "Tech Report - Undergraduate Thesis - 2023", "summary": "Representation learning methods utilizing the InfoNCE loss have demonstrated\nconsiderable capacity in reducing human annotation effort by training invariant\nneural feature extractors. Although different variants of the training\nobjective adhere to the information maximization principle between the data and\nlearned features, data selection and augmentation still rely on human\nhypotheses or engineering, which may be suboptimal. For instance, data\naugmentation in contrastive learning primarily focuses on color jittering,\naiming to emulate real-world illumination changes. In this work, we investigate\nthe potential of selecting training data based on their mutual information\ncomputed from real-world distributions, which, in principle, should endow the\nlearned features with better generalization when applied in open environments.\nSpecifically, we consider patches attached to scenes that exhibit high mutual\ninformation under natural perturbations, such as color changes and motion, as\npositive samples for learning with contrastive loss. We evaluate the proposed\nmutual-information-informed data augmentation method on several benchmarks\nacross multiple state-of-the-art representation learning frameworks,\ndemonstrating its effectiveness and establishing it as a promising direction\nfor future research.", "AI": {"tldr": "The paper explores enhancing representation learning by informing data augmentation and selection with mutual information from real-world distributions.", "motivation": "To improve representation learning and reduce reliance on human-designed data augmentation and selection, aiming for more effective/robust generalization in open environments.", "method": "Proposes selecting training data based on mutual information from real-world distributions, particularly choosing patches most robust under natural perturbations, and applying them with a contrastive loss.", "result": "Demonstrates and validates the method's effectiveness by testing it across multiple state-of-the-art frameworks and benchmarks.", "conclusion": "Mutual-information-informed data augmentation is an effective technique for representation learning and is a promising future research direction."}}
{"id": "2511.00685", "pdf": "https://arxiv.org/pdf/2511.00685", "abs": "https://arxiv.org/abs/2511.00685", "authors": ["Haoting Zhang", "Haoxian Chen", "Donglin Zhan", "Hanyang Zhao", "Henry Lam", "Wenpin Tang", "David Yao", "Zeyu Zheng"], "title": "SOCRATES: Simulation Optimization with Correlated Replicas and Adaptive Trajectory Evaluations", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "The field of simulation optimization (SO) encompasses various methods\ndeveloped to optimize complex, expensive-to-sample stochastic systems.\nEstablished methods include, but are not limited to, ranking-and-selection for\nfinite alternatives and surrogate-based methods for continuous domains, with\nbroad applications in engineering and operations management. The recent advent\nof large language models (LLMs) offers a new paradigm for exploiting system\nstructure and automating the strategic selection and composition of these\nestablished SO methods into a tailored optimization procedure. This work\nintroduces SOCRATES (Simulation Optimization with Correlated Replicas and\nAdaptive Trajectory Evaluations), a novel two-stage procedure that leverages\nLLMs to automate the design of tailored SO algorithms. The first stage\nconstructs an ensemble of digital replicas of the real system. An LLM is\nemployed to implement causal discovery from a textual description of the\nsystem, generating a structural `skeleton' that guides the sample-efficient\nlearning of the replicas. In the second stage, this replica ensemble is used as\nan inexpensive testbed to evaluate a set of baseline SO algorithms. An LLM then\nacts as a meta-optimizer, analyzing the performance trajectories of these\nalgorithms to iteratively revise and compose a final, hybrid optimization\nschedule. This schedule is designed to be adaptive, with the ability to be\nupdated during the final execution on the real system when the optimization\nperformance deviates from expectations. By integrating LLM-driven reasoning\nwith LLM-assisted trajectory-aware meta-optimization, SOCRATES creates an\neffective and sample-efficient solution for complex SO optimization problems.", "AI": {"tldr": "SOCRATES leverages large language models (LLMs) to create effective and adaptive simulation optimization algorithms for complex stochastic systems.", "motivation": "To develop an efficient and automated framework for optimizing complex and expensive stochastic systems by leveraging the capabilities of large language models (LLMs).", "method": "The paper introduces a two-stage procedure, SOCRATES. In the first stage, LLMs create a structural skeleton and guide the learning of replicas of the system. In the second stage, these replicas are used to evaluate SO algorithms, and an LLM serves as a meta-optimizer to design a tailored and adaptive optimization schedule.", "result": "SOCRATES provides a novel approach that combines LLM-driven reasoning and trajectory-aware optimization, leading to a more sample-efficient and effective solution for simulation optimization.", "conclusion": "SOCRATES represents a significant step forward in simulation optimization by integrating advanced LLM capabilities, enabling automation, adaptability, and improved optimization outcomes."}}
{"id": "2511.01001", "pdf": "https://arxiv.org/pdf/2511.01001", "abs": "https://arxiv.org/abs/2511.01001", "authors": ["Johansell Villalobos", "Daniel Caviedes-Voulli\u00e8me", "Silvio Rizzi", "Esteban Meneses"], "title": "Towards Portability at Scale: A Cross-Architecture Performance Evaluation of a GPU-enabled Shallow Water Solver", "categories": ["cs.DC", "cs.PF"], "comment": "Conference: SBAC-PAD 2025", "summary": "Current climate change has posed a grand challenge in the field of numerical\nmodeling due to its complex, multiscale dynamics. In hydrological modeling, the\nincreasing demand for high-resolution, real-time simulations has led to the\nadoption of GPU-accelerated platforms and performance portable programming\nframeworks such as Kokkos. In this work, we present a comprehensive performance\nstudy of the SERGHEI-SWE solver, a shallow water equations code, across four\nstate-of-the-art heterogeneous HPC systems: Frontier (AMD MI250X), JUWELS\nBooster (NVIDIA A100), JEDI (NVIDIA H100), and Aurora (Intel Max 1550). We\nassess strong scaling up to 1024 GPUs and weak scaling upwards of 2048 GPUs,\ndemonstrating consistent scalability with a speedup of 32 and an efficiency\nupwards of 90\\% for most almost all the test range. Roofline analysis reveals\nthat memory bandwidth is the dominant performance bottleneck, with key solver\nkernels residing in the memory-bound region. To evaluate performance\nportability, we apply both harmonic and arithmetic mean-based metrics while\nvarying problem size. Results indicate that while SERGHEI-SWE achieves\nportability across devices with tuned problem sizes (<70\\%), there is room for\nkernel optimization within the solver with more granular control of the\narchitecture specifically by using Kokkos teams and architecture specific\ntunable parameters. These findings position SERGHEI-SWE as a robust, scalable,\nand portable simulation tool for large-scale geophysical applications under\nevolving HPC architectures with potential to enhance its performance.", "AI": {"tldr": "This paper evaluates the performance and scalability of the SERGHEI-SWE shallow water equations solver on four HPC systems, analyzing memory bandwidth bottlenecks and suggesting optimizations.", "motivation": "To address the challenges posed by climate change and the demand for high-resolution, real-time hydrological simulations, the study aims to leverage advanced HPC systems and performance portable frameworks for enhancing computational capabilities.", "method": "The study analyzes the performance of the SERGHEI-SWE code across four cutting-edge heterogeneous HPC systems, conducting strong and weak scaling tests, roofline analysis, and performance portability evaluations using various metrics.", "result": "The solver demonstrates consistent scalability up to 2048 GPUs, achieving a speedup of 32 and efficiency above 90%. Memory bandwidth is identified as a bottleneck, while performance portability is achieved but could be further improved through architecture-specific optimizations.", "conclusion": "SERGHEI-SWE is a scalable and portable tool for large-scale geophysical applications. While it performs well across different HPC systems, kernel optimizations remain necessary to further enhance its capabilities on modern architectures."}}
{"id": "2511.00762", "pdf": "https://arxiv.org/pdf/2511.00762", "abs": "https://arxiv.org/abs/2511.00762", "authors": ["Leonardo Kanashiro Felizardo", "Edoardo Fadda", "Mari\u00e1 Cristina Vasconcelos Nascimento"], "title": "Automatic Policy Search using Population-Based Hyper-heuristics for the Integrated Procurement and Perishable Inventory Problem", "categories": ["cs.NE", "math.OC"], "comment": "19 pages, 1 figure, 3 tables", "summary": "This paper addresses the problem of managing perishable inventory under\nmultiple sources of uncertainty, including stochastic demand, unreliable\nsupplier fulfillment, and probabilistic product shelf life. We develop a\ndiscrete-event simulation environment to compare two optimization strategies\nfor this multi-item, multi-supplier problem. The first strategy optimizes\nuniform classic policies (e.g., Constant Order and Base Stock) by tuning their\nparameters globally, complemented by a direct search to select the best-fitting\nsuppliers for the integrated problem. The second approach is a hyper-heuristic\napproach, driven by metaheuristics such as a Genetic Algorithm (GA) and\nParticle Swarm Optimization (PSO). This framework constructs a composite policy\nby automating the selection of the heuristic type, its parameters, and the\nsourcing suppliers on an item-by-item basis. Computational results from twelve\ndistinct instances demonstrate that the hyper-heuristic framework consistently\nidentifies superior policies, with GA and EGA exhibiting the best overall\nperformance. Our primary contribution is verifying that this item-level policy\nconstruction yields significant performance gains over simpler global policies,\nthereby justifying the associated computational cost.", "AI": {"tldr": "This paper explores inventory management for perishable goods under uncertainties like erratic demand, unreliable supplies, and variable shelf life. It compares two optimization strategies: global parameter tuning and a hyper-heuristic approach driven by metaheuristics.", "motivation": "To address the challenges of managing perishable inventory confronted with uncertainties in supply, demand, and shelf life.", "method": "The study utilizes a discrete-event simulation to compare uniform global optimization policies versus a hyper-heuristic approach leveraging Genetic Algorithms and Particle Swarm Optimization.", "result": "Simulation results from 12 instances show that hyper-heuristic frameworks consistently perform better, with Genetic Algorithms particularly excelling in identifying optimal policies.", "conclusion": "Item-level policy construction using hyper-heuristics significantly outperforms global strategies, validating the computational effort involved."}}
{"id": "2511.00122", "pdf": "https://arxiv.org/pdf/2511.00122", "abs": "https://arxiv.org/abs/2511.00122", "authors": ["Ran Xu", "Yupeng Qi", "Jingsen Feng", "Xu Chu"], "title": "Engineering.ai: A Platform for Teams of AI Engineers in Computational Design", "categories": ["cs.AI"], "comment": null, "summary": "In modern engineering practice, human engineers collaborate in specialized\nteams to design complex products, with each expert completing their respective\ntasks while communicating and exchanging results and data with one another.\nWhile this division of expertise is essential for managing multidisciplinary\ncomplexity, it demands substantial development time and cost. Recently, we\nintroduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer\nfor computational fluid dynamics, and turbulence.ai, which can conduct\nend-to-end research in fluid mechanics draft publications and PhD theses.\nBuilding upon these foundations, we present Engineering.ai, a platform for\nteams of AI engineers in computational design. The framework employs a\nhierarchical multi-agent architecture where a Chief Engineer coordinates\nspecialized agents consisting of Aerodynamics, Structural, Acoustic, and\nOptimization Engineers, each powered by LLM with domain-specific knowledge.\nAgent-agent collaboration is achieved through file-mediated communication for\ndata provenance and reproducibility, while a comprehensive memory system\nmaintains project context, execution history, and retrieval-augmented domain\nknowledge to ensure reliable decision-making across the workflow. The system\nintegrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis,\nenabling parallel multidisciplinary simulations while maintaining computational\naccuracy. The framework is validated through UAV wing optimization. This work\ndemonstrates that agentic-AI-enabled AI engineers has the potential to perform\ncomplex engineering tasks autonomously. Remarkably, the automated workflow\nachieved a 100% success rate across over 400 parametric configurations, with\nzero mesh generation failures, solver convergence issues, or manual\ninterventions required, validating that the framework is trustworthy.", "AI": {"tldr": "Engineering.ai introduces a system of AI engineers collaborating as specialized agents to autonomously design complex products, validated through UAV wing optimization.", "motivation": "The paper aims to reduce the time and cost associated with traditional multidisciplinary engineering processes by introducing autonomous AI engineering teams.", "method": "Engineering.ai employs a hierarchical multi-agent system with specialized AI agents that collaborate using file-mediated communication, memory systems, and integrated computational tools.", "result": "The system achieved a 100% success rate in UAV wing optimization tasks across 400 configurations without failures or manual interventions.", "conclusion": "Agentic-AI-enabled teams can autonomously perform complex engineering tasks, demonstrating reliability and potential for widespread use in computational design."}}
{"id": "2511.00796", "pdf": "https://arxiv.org/pdf/2511.00796", "abs": "https://arxiv.org/abs/2511.00796", "authors": ["Ran Yan", "Youhe Jiang", "Tianyuan Wu", "Jiaxuan Gao", "Zhiyu Mei", "Wei Fu", "Haohui Mai", "Wei Wang", "Yi Wu", "Binhang Yuan"], "title": "AReaL-Hex: Accommodating Asynchronous RL Training over Heterogeneous GPUs", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "Maximizing training throughput and cost-efficiency of RL for LLMs is\nessential to democratize this advanced technique. One promising but challenging\napproach is to deploy such a computational workflow over heterogeneous GPUs.\nUnlike conventional large-scale LLM pretraining, RL training generally\ndecomposes into three coupled stages, i.e., rollout generation, reward\ncomputation, and policy/value updates, which exhibit markedly different compute\nintensities, memory footprints, and communication patterns. Recent research\nshows that fully asynchronous RL training can disaggregate these stages across\ndisjoint hardware pools without sacrificing training stability, creating a\ngreat opportunity for real-world heterogeneous deployment. To this end, we\npresent AReaL-Hex, a heterogeneity-aware asynchronous RL training system that\neffectively schedules how to execute rollout generation and policy model\ntraining over heterogeneous GPUs while enforcing data staleness bounds.\nConcretely, we use a two-phase scheduler: (i) a constrained search with MILP to\nselect per-stage parallelization strategies and workload assignments given a\nresource budget, and (ii) a graph-partitioning step that allocates\nheterogeneous GPUs and interconnects to maximize end-to-end throughput. Built\natop a fully asynchronous RL architecture, AReaL-Hex maps HBM-I/O-bound\ngeneration and compute-bound optimization to more cost-efficient resources and\nbalances their producer-consumer interactions to avoid both idleness and stale\nrollout trajectories. On the mathematical reasoning task with various model\nscales (1.5B, 7B, and 14B), compared to homogeneous deployments of\nstate-of-the-art asynchronous RL systems: (i) When maintaining the same total\nbudgets, AReaL-Hex delivers up to 1.50x higher training throughput; (ii) When\nachieving the same training throughput, AReaL-Hex results in up to 1.46x\nreduction in training cost.", "AI": {"tldr": "This paper presents AReaL-Hex, a system optimizing asynchronous RL workloads on heterogeneous GPUs for large language models, improving throughput and reducing costs.", "motivation": "To democratize RL for large language models, enhancing training throughput and cost-efficiency is crucial, particularly by leveraging heterogeneous GPUs.", "method": "The paper introduces AReaL-Hex, using a two-phase scheduler: mixed-integer linear programming (MILP) for workload assignment and graph-partitioning for resource allocation, built on an asynchronous RL framework.", "result": "AReaL-Hex achieves up to 1.50x higher throughput and up to 1.46x reduction in training costs compared to state-of-the-art systems under equivalent budgets and outputs.", "conclusion": "Deploying heterogeneous GPUs with AReaL-Hex improves resource efficiency and scalability in RL training for large language models, offering practical real-world applications."}}
{"id": "2511.01736", "pdf": "https://arxiv.org/pdf/2511.01736", "abs": "https://arxiv.org/abs/2511.01736", "authors": ["Charles Yuan"], "title": "Cobble: Compiling Block Encodings for Quantum Computational Linear Algebra", "categories": ["cs.PL", "quant-ph"], "comment": "20 pages, 12 figures", "summary": "Quantum algorithms for computational linear algebra promise up to exponential\nspeedups for applications such as simulation and regression, making them prime\ncandidates for hardware realization. But these algorithms execute in a model\nthat cannot efficiently store matrices in memory like a classical algorithm\ndoes, instead requiring developers to implement complex expressions for matrix\narithmetic in terms of correct and efficient quantum circuits. Among the\nchallenges for the developer is navigating a cost model in which conventional\noptimizations for linear algebra, such as subexpression reuse, can be\ninapplicable or unprofitable.\n  In this work, we present Cobble, a language for programming with quantum\ncomputational linear algebra. Cobble enables developers to express and\nmanipulate the quantum representations of matrices, known as block encodings,\nusing high-level notation that automatically compiles to correct quantum\ncircuits. Cobble features analyses that estimate leading factors in time and\nspace usage of programs, as well as optimizations that reduce overhead and\ngenerate efficient circuits using leading techniques such as the quantum\nsingular value transformation. We evaluate Cobble on benchmark kernels for\nsimulation, regression, search, and other applications, showing 2.6x-25.4x\nspeedups not achieved by existing circuit optimizers on these benchmarks.", "AI": {"tldr": "Cobble is a programming language designed for quantum computational linear algebra, addressing challenges in matrix arithmetic for quantum algorithms and offering significant performance improvements in benchmarks.", "motivation": "The paper aims to simplify and optimize the development of quantum algorithms for computational linear algebra, addressing inefficiencies in matrix handling and circuit design.", "method": "The authors introduce Cobble, which uses high-level notation to express quantum matrix representations and automates compilation to quantum circuits with optimization techniques.", "result": "Cobble achieves 2.6x to 25.4x speedups over existing circuit optimizers in benchmark tests across applications such as simulation, regression, and search.", "conclusion": "Cobble advances quantum programming by enabling efficient circuit designs for computational linear algebra, making quantum hardware applications more accessible and practical."}}
{"id": "2511.00222", "pdf": "https://arxiv.org/pdf/2511.00222", "abs": "https://arxiv.org/abs/2511.00222", "authors": ["Marwa Abdulhai", "Ryan Cheng", "Donovan Clay", "Tim Althoff", "Sergey Levine", "Natasha Jaques"], "title": "Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to simulate human users in\ninteractive settings such as therapy, education, and social role-play. While\nthese simulations enable scalable training and evaluation of AI agents,\noff-the-shelf LLMs often drift from their assigned personas, contradict earlier\nstatements, or abandon role-appropriate behavior. We introduce a unified\nframework for evaluating and improving persona consistency in LLM-generated\ndialogue. We define three automatic metrics: prompt-to-line consistency,\nline-to-line consistency, and Q&A consistency, that capture different types of\npersona drift and validate each against human annotations. Using these metrics\nas reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs\nfor three user roles: a patient, a student, and a social chat partner. Our\nmethod reduces inconsistency by over 55%, resulting in more coherent and\nfaithful simulated users.", "AI": {"tldr": "The paper addresses the issue of persona inconsistency in large language models (LLMs). It proposes evaluation metrics and employs reinforcement learning to enhance LLM performance, reducing inconsistency by over 55%.", "motivation": "The paper aims to tackle the challenge of LLM-generated personas drifting from their roles in settings like therapy, education, and social role-play, where coherence and consistency are critical.", "method": "The authors developed three automatic metrics for persona consistency (prompt-to-line, line-to-line, and Q&A consistency) and validated them with human annotations. They used these metrics as rewards to fine-tune LLMs using reinforcement learning for multi-turn dialogues in different user roles.", "result": "The proposed method reduced persona inconsistency in LLMs by over 55%, leading to improved coherence and faithful persona simulations.", "conclusion": "The paper successfully introduces a framework to evaluate and improve persona consistency in LLMs, making the models more reliable and effective in interactive scenarios."}}
{"id": "2511.00197", "pdf": "https://arxiv.org/pdf/2511.00197", "abs": "https://arxiv.org/abs/2511.00197", "authors": ["Oorja Majgaonkar", "Zhiwei Fei", "Xiang Li", "Federica Sarro", "He Ye"], "title": "Understanding Code Agent Behaviour: An Empirical Study of Success and Failure Trajectories", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "The increasing deployment of Large Language Model (LLM) agents for complex\nsoftware engineering tasks has created a need to understand their\nproblem-solving behaviours beyond simple success metrics. While these agents\ndemonstrate impressive capabilities in automated issue resolution, their\ndecision-making processes remain largely opaque. This paper presents an\nempirical study of agent trajectories, namely the execution traces capturing\nthe steps agents take when attempting to resolve software issues. We analyse\ntrajectories from three state-of-the-art code agents (OpenHands, SWE-agent, and\nPrometheus) on the SWE-Bench benchmark, examining both successful and failed\nattempts. Our investigation reveals several key insights into agent behaviour.\nFirst, we identify how distinct problem-solving strategies, such as defensive\nprogramming and context gathering, enable success in different scenarios.\nSecond, we find that failed trajectories are consistently longer and exhibit\nhigher variance than successful ones, with failure patterns differing\nsignificantly between agents. Third, our fault localisation analysis shows that\nwhile most trajectories correctly identify problematic files (72-81\\% even in\nfailures), success depends more on achieving approximate rather than exact code\nmodifications. These and other findings unveiled by our study, provide a\nfoundation for understanding agent behaviour through trajectory analysis,\ncontributing to the development of more robust and interpretable autonomous\nsoftware engineering systems.", "AI": {"tldr": "The study investigates the problem-solving behaviour of large language model agents for software engineering tasks, using trajectory analysis of their execution traces.", "motivation": "The deployment of LLM agents for software tasks necessitates understanding their decision-making processes beyond mere success metrics.", "method": "Empirical analysis of agent trajectories from three code agents (OpenHands, SWE-agent, Prometheus) on the SWE-Bench benchmark, contrasting successful and failed attempts.", "result": "The study found differences in problem-solving strategies, trajectory characteristics, and fault localisation performance across agents, noting that approximate modifications are crucial for success.", "conclusion": "Trajectory analysis offers valuable insights into LLM agent behaviour, promoting the creation of more robust autonomous software engineering systems."}}
{"id": "2511.00094", "pdf": "https://arxiv.org/pdf/2511.00094", "abs": "https://arxiv.org/abs/2511.00094", "authors": ["Angelos Alexopoulos", "Agorakis Bompotas", "Nikitas Rigas Kalogeropoulos", "Panagiotis Kechagias", "Athanasios P. Kalogeras", "Christos Alexakos"], "title": "Digital Twin based Automatic Reconfiguration of Robotic Systems in Smart Environments", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": "Accepted for presentation to 11th IEEE International Smart Cities\n  Conference (ISC2 2025)", "summary": "Robotic systems have become integral to smart environments, enabling\napplications ranging from urban surveillance and automated agriculture to\nindustrial automation. However, their effective operation in dynamic settings -\nsuch as smart cities and precision farming - is challenged by continuously\nevolving topographies and environmental conditions. Traditional control systems\noften struggle to adapt quickly, leading to inefficiencies or operational\nfailures. To address this limitation, we propose a novel framework for\nautonomous and dynamic reconfiguration of robotic controllers using Digital\nTwin technology. Our approach leverages a virtual replica of the robot's\noperational environment to simulate and optimize movement trajectories in\nresponse to real-world changes. By recalculating paths and control parameters\nin the Digital Twin and deploying the updated code to the physical robot, our\nmethod ensures rapid and reliable adaptation without manual intervention. This\nwork advances the integration of Digital Twins in robotics, offering a scalable\nsolution for enhancing autonomy in smart, dynamic environments.", "AI": {"tldr": "The paper introduces a framework leveraging Digital Twin technology to enable autonomous and dynamic reconfiguration of robotic controllers, optimizing their operation in evolving environments.", "motivation": "Traditional robotic control systems often fail to adapt efficiently in dynamic settings like smart cities and precision farming, leading to inefficiencies and operational failures.", "method": "The framework uses a Digital Twin to simulate and optimize robot motions and control parameters based on real-time environmental changes, then automatically updates the robot's operation code accordingly.", "result": "The approach achieves rapid and reliable adaptations in robotic systems without requiring manual intervention, enhancing their efficiency and autonomy.", "conclusion": "The integration of Digital Twin technology in robotics offers a scalable solution for improving autonomy and adaptability in smart, dynamic environments."}}
{"id": "2511.00035", "pdf": "https://arxiv.org/pdf/2511.00035", "abs": "https://arxiv.org/abs/2511.00035", "authors": ["Georg Velev", "Stefan Lessmann"], "title": "Neural Architecture Search for global multi-step Forecasting of Energy Production Time Series", "categories": ["cs.LG"], "comment": null, "summary": "The dynamic energy sector requires both predictive accuracy and runtime\nefficiency for short-term forecasting of energy generation under operational\nconstraints, where timely and precise predictions are crucial. The manual\nconfiguration of complex methods, which can generate accurate global multi-step\npredictions without suffering from a computational bottleneck, represents a\nprocedure with significant time requirements and high risk for human-made\nerrors. A further intricacy arises from the temporal dynamics present in\nenergy-related data. Additionally, the generalization to unseen data is\nimperative for continuously deploying forecasting techniques over time. To\novercome these challenges, in this research, we design a neural architecture\nsearch (NAS)-based framework for the automated discovery of time series models\nthat strike a balance between computational efficiency, predictive performance,\nand generalization power for the global, multi-step short-term forecasting of\nenergy production time series. In particular, we introduce a search space\nconsisting only of efficient components, which can capture distinctive patterns\nof energy time series. Furthermore, we formulate a novel objective function\nthat accounts for performance generalization in temporal context and the\nmaximal exploration of different regions of our high-dimensional search space.\nThe results obtained on energy production time series show that an ensemble of\nlightweight architectures discovered with NAS outperforms state-of-the-art\ntechniques, such as Transformers, as well as pre-trained forecasting models, in\nterms of both efficiency and accuracy.", "AI": {"tldr": "This study develops a Neural Architecture Search (NAS)-based approach for automated model discovery in energy generation forecasting, emphasizing computational efficiency, accuracy, and generalization.", "motivation": "The paper addresses the need for timely and precise energy generation forecasting under operational constraints, minimizing human error in configuring complex models and handling the challenges of temporal dynamics and generalization to unseen data.", "method": "A neural architecture search (NAS)-based framework with an efficient component-focused search space and a novel objective function, emphasizing temporal generalization and exploration of high-dimensional search spaces, is proposed.", "result": "The proposed method achieved superior efficiency and accuracy in multi-step energy forecasting compared to state-of-the-art techniques, including Transformers and pre-trained models.", "conclusion": "The study demonstrates that NAS-discovered lightweight ensemble architectures are effective and efficient for energy time series forecasting, outperforming existing state-of-the-art methods."}}
{"id": "2511.00037", "pdf": "https://arxiv.org/pdf/2511.00037", "abs": "https://arxiv.org/abs/2511.00037", "authors": ["Riya Gupta", "Alexander Chowdhury", "Sahil Nalawade"], "title": "Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra", "categories": ["cs.CV", "cs.DC"], "comment": null, "summary": "Federated Learning (FL) has emerged as a transformative paradigm in medical\nAI, enabling collaborative model training across institutions without direct\ndata sharing. This study benchmarks three prominent FL frameworks NVIDIA FLARE,\nFlower, and Owkin Substra to evaluate their suitability for medical imaging\napplications in real-world settings. Using the PathMNIST dataset, we assess\nmodel performance, convergence efficiency, communication overhead, scalability,\nand developer experience. Results indicate that NVIDIA FLARE offers superior\nproduction scalability, Flower provides flexibility for prototyping and\nacademic research, and Owkin Substra demonstrates exceptional privacy and\ncompliance features. Each framework exhibits strengths optimized for distinct\nuse cases, emphasizing their relevance to practical deployment in healthcare\nenvironments.", "AI": {"tldr": "The paper benchmarks three federated learning (FL) frameworks\u2014NVIDIA FLARE, Flower, and Owkin Substra\u2014for medical imaging applications.", "motivation": "To evaluate the suitability of FL frameworks for practical deployment in healthcare, specifically for medical imaging tasks.", "method": "Conducted a comparative evaluation using the PathMNIST dataset to assess various metrics like model performance, scalability, and privacy features of the frameworks.", "result": "NVIDIA FLARE excels in scalability, Flower provides flexibility for prototyping, and Owkin Substra offers strong privacy and compliance features.", "conclusion": "Each framework caters to distinct use cases in medical imaging, making them valuable for different aspects of healthcare applications."}}
{"id": "2511.00849", "pdf": "https://arxiv.org/pdf/2511.00849", "abs": "https://arxiv.org/abs/2511.00849", "authors": ["Zhexiao Huang", "Weihao He", "Shutao Deng", "Junzhe Chen", "Chao Yuan", "Hongxin Wang", "Changsheng Zhou"], "title": "Perturbations in the Orthogonal Complement Subspace for Efficient Out-of-Distribution Detection", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Out-of-distribution (OOD) detection is essential for deploying deep learning\nmodels in open-world environments. Existing approaches, such as energy-based\nscoring and gradient-projection methods, typically rely on high-dimensional\nrepresentations to separate in-distribution (ID) and OOD samples. We introduce\nP-OCS (Perturbations in the Orthogonal Complement Subspace), a lightweight and\ntheoretically grounded method that operates in the orthogonal complement of the\nprincipal subspace defined by ID features. P-OCS applies a single projected\nperturbation restricted to this complementary subspace, enhancing subtle ID-OOD\ndistinctions while preserving the geometry of ID representations. We show that\na one-step update is sufficient in the small-perturbation regime and provide\nconvergence guarantees for the resulting detection score. Experiments across\nmultiple architectures and datasets demonstrate that P-OCS achieves\nstate-of-the-art OOD detection with negligible computational cost and without\nrequiring model retraining, access to OOD data, or changes to model\narchitecture.", "AI": {"tldr": "P-OCS method enhances OOD detection using perturbations in an orthogonal complement subspace, offering efficiency and state-of-the-art performance.", "motivation": "The paper aims to address the need for effective OOD detection in open-world environments while minimizing computational cost and avoiding challenges like model retraining.", "method": "The proposed P-OCS method uses well-defined perturbations in an orthogonal complement subspace to distinguish OOD samples from ID samples without modifying the model architecture or accessing OOD data.", "result": "The method achieves state-of-the-art performance on several architectures and datasets with very low computational overhead.", "conclusion": "P-OCS is a straightforward, efficient approach to OOD detection, requiring only minimal computational effort and no modifications or external data, making it valuable for practical deployment."}}
{"id": "2511.01158", "pdf": "https://arxiv.org/pdf/2511.01158", "abs": "https://arxiv.org/abs/2511.01158", "authors": ["Faquan Chen", "Qingyang Tian", "Ziren Wu", "Rendong Ying", "Fei Wen", "Peilin Liu"], "title": "A High-Throughput Spiking Neural Network Processor Enabling Synaptic Delay Emulation", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "Synaptic delay has attracted significant attention in neural network dynamics\nfor integrating and processing complex spatiotemporal information. This paper\nintroduces a high-throughput Spiking Neural Network (SNN) processor that\nsupports synaptic delay-based emulation for edge applications. The processor\nleverages a multicore pipelined architecture with parallel compute engines,\ncapable of real-time processing of the computational load associated with\nsynaptic delays. We develop a SoC prototype of the proposed processor on PYNQ\nZ2 FPGA platform and evaluate its performance using the Spiking Heidelberg\nDigits (SHD) benchmark for low-power keyword spotting tasks. The processor\nachieves 93.4% accuracy in deployment and an average throughput of 104\nsamples/sec at a typical operating frequency of 125 MHz and 282 mW power\nconsumption.", "AI": {"tldr": "The paper presents a high-throughput Spiking Neural Network processor featuring synaptic delay-based emulation, achieving real-time processing, high accuracy, and low power usage in edge applications.", "motivation": "To improve the processing and integration of complex spatiotemporal information in edge applications using synaptic delay-emulating Spiking Neural Networks.", "method": "The paper introduces a multicore pipelined architecture with parallel computing for real-time processing. A SoC prototype was designed on the PYNQ Z2 FPGA platform, tested with the Spiking Heidelberg Digits benchmark.", "result": "The processor achieved 93.4% accuracy on keyword spotting tasks and an average throughput of 104 samples/sec at 125 MHz operating frequency with 282 mW power consumption.", "conclusion": "The proposed processor design demonstrates effective integration of synaptic delay mechanisms in Spiking Neural Networks for low-power, high-performance edge applications."}}
{"id": "2511.00162", "pdf": "https://arxiv.org/pdf/2511.00162", "abs": "https://arxiv.org/abs/2511.00162", "authors": ["Michael D. Moffitt"], "title": "ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "The Abstraction and Reasoning Corpus remains one of the most compelling and\nchallenging benchmarks for tracking progress toward achieving Artificial\nGeneral Intelligence. In contrast to other evaluation datasets designed to\nassess an agent's task-specific skills or accumulated knowledge, the ARC-AGI\nsuite is specifically targeted at measuring skill acquisition efficiency, a\ntrait that has (so far) been lacking in even the most sophisticated machine\nlearning systems. For algorithms that require extensive intra-task exemplars, a\nsignificant constraint imposed by ARC-AGI is the modest cardinality of its\ndemonstration set, comprising a small number of $\\langle$ input, output\n$\\rangle$ grids per task specifying the corresponding transformation. To\nembellish the space of viable sample pairs, this paper introduces ARC-GEN, an\nopen-source procedural generator aimed at extending the original ARC-AGI\ntraining dataset as faithfully as possible. Unlike prior efforts, our generator\nis both exhaustive (covering all four-hundred tasks) and mimetic (more closely\nhonoring the distributional properties and characteristics embodied in the\ninitial ARC-AGI-1 release). We also discuss the use of this generator in\nestablishing a static benchmark suite to verify the correctness of programs\nsubmitted to the 2025 Google Code Golf Championship.", "AI": {"tldr": "The paper introduces ARC-GEN, a procedural generator aimed at enhancing the ARC-AGI suite for evaluating Artificial General Intelligence.", "motivation": "To address the constraint of limited training examples in ARC-AGI and improve skill acquisition efficiency evaluation in artificial intelligence.", "method": "The authors developed ARC-GEN, a procedural generator that faithfully extends the ARC-AGI dataset by creating exhaustive and distribution-preserving task examples.", "result": "ARC-GEN extends the ARC-AGI training dataset to cover all 400 tasks while mimicking the original data's characteristics, enabling better evaluation of AI systems.", "conclusion": "ARC-GEN enhances the ARC-AGI's utility as a benchmark for testing skill acquisition in AI and supports its use in future competitions like the Google Code Golf Championship."}}
{"id": "2511.00807", "pdf": "https://arxiv.org/pdf/2511.00807", "abs": "https://arxiv.org/abs/2511.00807", "authors": ["Xuan He", "Zequan Fang", "Jinzhao Lian", "Danny H. K. Tsang", "Baosen Zhang", "Yize Chen"], "title": "FREESH: Fair, Resource- and Energy-Efficient Scheduling for LLM Serving on Heterogeneous GPUs", "categories": ["cs.DC"], "comment": "In Submission, code available at\n  https://github.com/AndrewFangZequan/LLM_Serving_FREESH", "summary": "The ever-increasing computation and energy demand for LLM and AI agents call\nfor holistic and efficient optimization of LLM serving systems. In practice,\nheterogeneous GPU clusters can be deployed in a geographically distributed\nmanner, while LLM load also observes diversity in terms of both query traffic\nand serving patterns. LLM queries running on advanced GPUs during a\nhigh-emission hour at one location can lead to significantly higher carbon\nfootprints versus same queries running on mid-level GPUs at a low-emission time\nand location. By observing LLM serving requirements and leveraging\nspatiotemporal computation flexibility, we consider the joint routing and\nscheduling problem, and propose FREESH to cooperatively run a group of data\ncenters while minimizing user-specified carbon or energy objectives. FREESH\nidentifies the optimal configurations of balanced load serving by matching\ndistinct GPU instance's power-throughput characteristics with predictable LLM\nquery length and workloads. To ensure both latency and fairness requirements,\nFREESH identifies optimized parallelism and query routing schedules together\nwith dynamic GPU frequency scaling for power saving, and Least-Laxity-First\n(LLF) serving strategy for query scheduling. During the 1-hour serving on\nproduction workloads, FREESH reduces energy by 28.6% and emissions by 45.45%\ntogether with improvements in SLO attainment and fairness.", "AI": {"tldr": "The paper proposes FREESH, a system to optimize LLM serving systems on heterogeneous GPU clusters, reducing energy consumption and carbon emissions while improving fairness and latency.", "motivation": "High computation and energy demand for LLMs necessitate more efficient serving systems, especially considering geographic and temporal variations in energy emissions and GPU capabilities.", "method": "The authors introduce FREESH, which leverages spatiotemporal flexibility to balance query loads, match GPU capabilities with workload demands, optimize routing and scheduling, and dynamically scale GPU frequency.", "result": "FREESH reduces energy use by 28.6% and lowers emissions by 45.45% during a 1-hour test on production workloads, while also improving Service Level Objective (SLO) attainment and fairness.", "conclusion": "FREESH offers a holistic optimization framework for LLM serving systems, significantly enhancing energy efficiency and sustainability without compromising service quality."}}
{"id": "2511.00265", "pdf": "https://arxiv.org/pdf/2511.00265", "abs": "https://arxiv.org/abs/2511.00265", "authors": ["Arman Anwar", "Zefang Liu"], "title": "AgentBnB: A Browser-Based Cybersecurity Tabletop Exercise with Large Language Model Support and Retrieval-Aligned Scaffolding", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Traditional cybersecurity tabletop exercises (TTXs) provide valuable training\nbut are often scripted, resource-intensive, and difficult to scale. We\nintroduce AgentBnB, a browser-based re-imagining of the Backdoors & Breaches\ngame that integrates large language model teammates with a Bloom-aligned,\nretrieval-augmented copilot (C2D2). The system expands a curated corpus into\nfactual, conceptual, procedural, and metacognitive snippets, delivering\non-demand, cognitively targeted hints. Prompt-engineered agents employ a\nscaffolding ladder that gradually fades as learner confidence grows. In a\nsolo-player pilot with four graduate students, participants reported greater\nintention to use the agent-based version compared to the physical card deck and\nviewed it as more scalable, though a ceiling effect emerged on a simple\nknowledge quiz. Despite limitations of small sample size, single-player focus,\nand narrow corpus, these early findings suggest that large language model\naugmented TTXs can provide lightweight, repeatable practice without the\nlogistical burden of traditional exercises. Planned extensions include\nmulti-player modes, telemetry-driven coaching, and comparative studies with\nlarger cohorts.", "AI": {"tldr": "AgentBnB is a browser-based tool that uses large language model teammates to enhance cybersecurity training, addressing scalability and resource challenges inherent in traditional tabletop exercises.", "motivation": "The motivation is to simplify and scale cybersecurity tabletop exercises, which are resource-intensive and scripted, by leveraging large language model technology.", "method": "The method includes integrating large language models with a retrieval-augmented copilot to generate cognitive hints, scaffold learning, and fade hints as learner confidence grows.", "result": "A pilot study showed participants preferred AgentBnB over traditional exercises, citing scalability benefits, though a ceiling effect was noted in knowledge quiz outcomes.", "conclusion": "AgentBnB demonstrates potential for lightweight, scalable cybersecurity training. Future developments aim at multiplayer modes, advanced coaching, and broader validation among larger user groups."}}
{"id": "2511.00202", "pdf": "https://arxiv.org/pdf/2511.00202", "abs": "https://arxiv.org/abs/2511.00202", "authors": ["Jacqueline Mitchell", "Yasser Shaaban"], "title": "Position: Vibe Coding Needs Vibe Reasoning: Improving Vibe Coding with Formal Verification", "categories": ["cs.SE", "cs.LG", "cs.LO", "F.3.1; I.2.5"], "comment": "7 pages, 3 figures, In Proceedings of the 1st ACM SIGPLAN\n  International Workshop on Language Models and Programming Languages\n  (LMPL'25), October 12-18, 2025, Singapore, Singapore. ACM, New York, NY, USA", "summary": "``Vibe coding'' -- the practice of developing software through iteratively\nconversing with a large language model (LLM) -- has exploded in popularity\nwithin the last year. However, developers report key limitations including the\naccumulation of technical debt, security issues, and code churn to achieve\nsatisfactory results. We argue that these pitfalls result from LLMs' inability\nto reconcile accumulating human-imposed constraints during vibe coding, with\ndevelopers inadvertently failing to resolve contradictions because LLMs\nprioritize user commands over code consistency. Given LLMs' receptiveness to\nverification-based feedback, we argue that formal methods can mitigate these\npitfalls, making vibe coding more reliable. However, we posit that integrating\nformal methods must transcend existing approaches that combine formal methods\nand LLMs. We advocate for a side-car system throughout the vibe coding process\nwhich: (1) \\emph{Autoformalizes} specifications (2) Validates against targets,\n(3) Delivers \\emph{actionable} feedback to the LLM, and (4) Allows intuitive\ndeveloper influence on specifications.", "AI": {"tldr": "The paper explores challenges in \"vibe coding\" with LLMs, like technical debt and code churn, and suggests integrating formal methods via a side-car system for improved reliability.", "motivation": "Recent rise in \"vibe coding\" with LLMs has shown popularity, but exhibited shortcomings such as technical debt and code inconsistency, needing solutions for reliable development.", "method": "Proposes a side-car system integration that autoformalizes specifications, validates against targets, delivers actionable feedback to LLMs, and allows intuitive developer influence in the process.", "result": "The side-car system aims to address current pitfalls, optimizing code reliability through improved methods supporting LLM-guided development.", "conclusion": "Better integration of formal methods with LLMs will result in improved reliability and consistency, helping developers achieve better outcomes in vibe coding."}}
{"id": "2511.00112", "pdf": "https://arxiv.org/pdf/2511.00112", "abs": "https://arxiv.org/abs/2511.00112", "authors": ["Yanbing Mao", "Yihao Cai", "Lui Sha"], "title": "Real-DRL: Teach and Learn in Reality", "categories": ["cs.RO", "cs.AI"], "comment": "37 pages", "summary": "This paper introduces the Real-DRL framework for safety-critical autonomous\nsystems, enabling runtime learning of a deep reinforcement learning (DRL) agent\nto develop safe and high-performance action policies in real plants (i.e., real\nphysical systems to be controlled), while prioritizing safety! The Real-DRL\nconsists of three interactive components: a DRL-Student, a PHY-Teacher, and a\nTrigger. The DRL-Student is a DRL agent that innovates in the dual\nself-learning and teaching-to-learn paradigm and the real-time safety-informed\nbatch sampling. On the other hand, PHY-Teacher is a physics-model-based design\nof action policies that focuses solely on safety-critical functions.\nPHY-Teacher is novel in its real-time patch for two key missions: i) fostering\nthe teaching-to-learn paradigm for DRL-Student and ii) backing up the safety of\nreal plants. The Trigger manages the interaction between the DRL-Student and\nthe PHY-Teacher. Powered by the three interactive components, the Real-DRL can\neffectively address safety challenges that arise from the unknown unknowns and\nthe Sim2Real gap. Additionally, Real-DRL notably features i) assured safety,\nii) automatic hierarchy learning (i.e., safety-first learning and then\nhigh-performance learning), and iii) safety-informed batch sampling to address\nthe learning experience imbalance caused by corner cases. Experiments with a\nreal quadruped robot, a quadruped robot in NVIDIA Isaac Gym, and a cart-pole\nsystem, along with comparisons and ablation studies, demonstrate the Real-DRL's\neffectiveness and unique features.", "AI": {"tldr": "A novel Real-DRL framework integrates a DRL agent, a safety-focused physics model, and a mechanism for interaction to ensure safety and high performance in real autonomous systems.", "motivation": "To address safety challenges in autonomous systems, especially in safety-critical applications, by bridging gaps between simulations and real-world implementation.", "method": "The Real-DRL framework comprises three components: DRL-Student (a deep reinforcement learning agent for dual learning modes), PHY-Teacher (a physics-based safety framework and real-time backup), and Trigger (managing their interactions).", "result": "Experiments with real and simulated systems, including a quadruped robot and a cart-pole model, validated the effectiveness and safety guarantees of the Real-DRL framework.", "conclusion": "Real-DRL achieves assured safety while learning, automatic hierarchical performance improvement, and balanced learning experiences, making it a promising approach for safe and high-performing autonomous systems."}}
{"id": "2511.00040", "pdf": "https://arxiv.org/pdf/2511.00040", "abs": "https://arxiv.org/abs/2511.00040", "authors": ["Seonggyun Lee", "Sungjun Lim", "Seojin Park", "Soeun Cheon", "Kyungwoo Song"], "title": "Semi-Supervised Preference Optimization with Limited Feedback", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The field of preference optimization has made outstanding contributions to\nthe alignment of language models with human preferences. Despite these\nadvancements, recent methods still rely heavily on substantial paired (labeled)\nfeedback data, leading to substantial resource expenditures. To address these\nchallenges, we study the problem of Semi-Supervised Preference Optimization\n(SSPO) in which the idea is to learn from both a small number of pairwise\npreference labels and a large pool of unpaired samples simultaneously. Our key\ntheoretical contribution proves the existence of an optimal reward threshold\ncapable of separating winning and losing responses with high probability, which\nenables a principled pseudo-labeling of unpaired data. By leveraging these\npseudo-labels, SSPO effectively distills latent preferences from large-scale\nunpaired data, thus maintaining human alignment while drastically reducing\nacquisition costs. Extensive experiments across datasets validate this\nremarkable data efficiency; for instance, SSPO trained with Llama3-8B-Instruct\non just 1% of UltraFeedback consistently surpasses strong baselines trained on\n10% of UltraFeedback.", "AI": {"tldr": "The paper introduces Semi-Supervised Preference Optimization (SSPO) to align language models with human preferences using minimal labeled data and leveraging unpaired samples, achieving high efficiency and reducing costs.", "motivation": "Current preference optimization methods require extensive labeled datasets, which are resource-intensive. This paper aims to reduce reliance on labeled data while maintaining high human alignment performance.", "method": "SSPO combines a small amount of pairwise labeled preference data with unpaired data using theoretical insights, such as identifying an optimal reward threshold. Pseudo-labels derived from this threshold are employed to distill latent preferences.", "result": "SSPO, using only 1% labeled data on UltraFeedback, surpasses methods trained on 10% of UltraFeedback when tested with Llama3-8B-Instruct, demonstrating significant improvements in data efficiency and model performance.", "conclusion": "SSPO enables effective learning from limited labeled data and large-scale unpaired data by using pseudo-labeling techniques, proving to be resource-efficient and capable of better aligning language models with human preferences."}}
{"id": "2511.00046", "pdf": "https://arxiv.org/pdf/2511.00046", "abs": "https://arxiv.org/abs/2511.00046", "authors": ["Rupjyoti Chutia", "Dibya Jyoti Bora"], "title": "Enhancing rice leaf images: An overview of image denoising techniques", "categories": ["cs.CV", "68U10, 94A08", "I.4.3; I.4.4; I.5.1; J.3"], "comment": "18 pages, 6 figures. Research Article published in the International\n  Journal of Agricultural and Natural Sciences (IJANS), Vol. 18, Issue 2, 2025.\n  This paper presents a comparative study of image denoising and CLAHE\n  techniques for enhancing rice leaf images corrupted by Gaussian,\n  Salt-and-pepper, Speckle, and Random noise for agricultural analysis", "summary": "Digital image processing involves the systematic handling of images using\nadvanced computer algorithms, and has gained significant attention in both\nacademic and practical fields. Image enhancement is a crucial preprocessing\nstage in the image-processing chain, improving image quality and emphasizing\nfeatures. This makes subsequent tasks (segmentation, feature extraction,\nclassification) more reliable. Image enhancement is essential for rice leaf\nanalysis, aiding in disease detection, nutrient deficiency evaluation, and\ngrowth analysis. Denoising followed by contrast enhancement are the primary\nsteps. Image filters, generally employed for denoising, transform or enhance\nvisual characteristics like brightness, contrast, and sharpness, playing a\ncrucial role in improving overall image quality and enabling the extraction of\nuseful information. This work provides an extensive comparative study of\nwell-known image-denoising methods combined with CLAHE (Contrast Limited\nAdaptive Histogram Equalization) for efficient denoising of rice leaf images.\nThe experiments were performed on a rice leaf image dataset to ensure the data\nis relevant and representative. Results were examined using various metrics to\ncomprehensively test enhancement methods. This approach provides a strong basis\nfor assessing the effectiveness of methodologies in digital image processing\nand reveals insights useful for future adaptation in agricultural research and\nother domains.", "AI": {"tldr": "This paper evaluates various denoising methods paired with CLAHE for rice leaf image enhancement, aiming to improve analysis reliability in agricultural applications.", "motivation": "The study aims to improve image-processing reliability for rice leaf analysis, enabling better disease detection, nutrient deficiency evaluation, and growth analysis.", "method": "A comparative study of well-known denoising methods integrated with CLAHE. Experiments were conducted on rice leaf image datasets, evaluated using various metrics.", "result": "Experimental results demonstrate the performance of the combined methods (denoising and CLAHE) in effectively enhancing rice leaf images, producing reliable data for agricultural research.", "conclusion": "The findings provide a robust foundation for evaluating digital image-processing techniques and insights for future adaptations in agricultural and other domains."}}
{"id": "2511.01037", "pdf": "https://arxiv.org/pdf/2511.01037", "abs": "https://arxiv.org/abs/2511.01037", "authors": ["Mihailo Stojnic"], "title": "Binary perceptron computational gap -- a parametric fl RDT view", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.IT", "cs.LG", "math.IT", "math.PR"], "comment": null, "summary": "Recent studies suggest that asymmetric binary perceptron (ABP) likely\nexhibits the so-called statistical-computational gap characterized with the\nappearance of two phase transitioning constraint density thresholds:\n\\textbf{\\emph{(i)}} the \\emph{satisfiability threshold} $\\alpha_c$, below/above\nwhich ABP succeeds/fails to operate as a storage memory; and\n\\textbf{\\emph{(ii)}} \\emph{algorithmic threshold} $\\alpha_a$, below/above which\none can/cannot efficiently determine ABP's weight so that it operates as a\nstorage memory.\n  We consider a particular parametric utilization of \\emph{fully lifted random\nduality theory} (fl RDT) [85] and study its potential ABP's algorithmic\nimplications. A remarkable structural parametric change is uncovered as one\nprogresses through fl RDT lifting levels. On the first two levels, the\nso-called $\\c$ sequence -- a key parametric fl RDT component -- is of the\n(natural) decreasing type. A change of such phenomenology on higher levels is\nthen connected to the $\\alpha_c$ -- $\\alpha_a$ threshold change. Namely, on the\nsecond level concrete numerical values give for the critical constraint density\n$\\alpha=\\alpha_c\\approx 0.8331$. While progressing through higher levels\ndecreases this estimate, already on the fifth level we observe a satisfactory\nlevel of convergence and obtain $\\alpha\\approx 0.7764$. This allows to draw two\nstriking parallels: \\textbf{\\emph{(i)}} the obtained constraint density\nestimate is in a remarkable agrement with range $\\alpha\\in (0.77,0.78)$ of\nclustering defragmentation (believed to be responsible for failure of locally\nimproving algorithms) [17,88]; and \\textbf{\\emph{(ii)}} the observed change of\n$\\c$ sequence phenomenology closely matches the one of the negative Hopfield\nmodel for which the existence of efficient algorithms that closely approach\nsimilar type of threshold has been demonstrated recently [87].", "AI": {"tldr": "The paper investigates the statistical-computational gap in asymmetric binary perceptron (ABP) using a parametric method known as fully lifted random duality theory (fl RDT). Key findings include a convergence in constraint density estimates and implications for algorithmic thresholds.", "motivation": "The paper aims to understand the statistical-computational gap in ABP, particularly focusing on two thresholds: the satisfiability threshold and algorithmic threshold.", "method": "The study utilizes the fully lifted random duality theory (fl RDT) to analyze ABP. It examines the progression of a parametric key component, the \"c sequence,\" across different lifting levels.", "result": "Critical constraints density decreases as the fl RDT lifting level progresses, and satisfactory convergence is observed by the fifth level. Numerical estimates align with difficulty points for locally improving algorithms and match interpretations from other theoretical models.", "conclusion": "The results contribute to the understanding of ABP's storage memory efficiency and algorithmic thresholds. They align well with prior theoretical observations, potentially aiding efficient algorithm design for similar problems."}}
{"id": "2511.01632", "pdf": "https://arxiv.org/pdf/2511.01632", "abs": "https://arxiv.org/abs/2511.01632", "authors": ["Bal\u00e1zs M\u00e9sz\u00e1ros", "James C. Knight", "Danyal Akarca", "Thomas Nowotny"], "title": "Space as Time Through Neuron Position Learning", "categories": ["cs.NE"], "comment": null, "summary": "Biological neural networks exist in physical space where distance determines\ncommunication delays: a fundamental space-time coupling absent in most\nartificial neural networks. While recent work has separately explored spatial\nembeddings and learnable synaptic delays in spiking neural networks, we unify\nthese approaches through a novel neuron position learning algorithm where\ndelays relate to the Euclidean distances between neurons. We derive gradients\nwith respect to neuron positions and demonstrate that this\nbiologically-motivated constraint acts as an inductive bias: networks trained\non temporal classification tasks spontaneously self-organize into local,\nsmall-world topologies with modular structure emerging under distance-dependent\nconnection costs. Remarkably, we observe unprompted functional specialization\naligned with spatial clustering without explictly enforcing it. These findings\nlay the groundwork for networks in which space and time are intrinsically\ncoupled, offering new avenues for mechanistic interpretability, biologically\ninspired modelling, and efficient implementations.", "AI": {"tldr": "The paper introduces a neuron position learning algorithm for spiking neural networks, where communication delays are influenced by spatial distances, leading to self-organized network topologies.", "motivation": "To incorporate biological aspects like space-time coupling into artificial neural networks to improve modeling and interpretability.", "method": "A novel algorithm to learn neuron positions where synaptic delays are linked to Euclidean distances between neurons, with gradients derived for this learning.", "result": "Networks organized into small-world topologies showed functional specialization aligning with spatial clustering without explicit enforcement of structure.", "conclusion": "The approach introduces biologically-inspired spatial and temporal coupling, enhancing the interpretability and efficiency of artificial neural networks."}}
{"id": "2511.00194", "pdf": "https://arxiv.org/pdf/2511.00194", "abs": "https://arxiv.org/abs/2511.00194", "authors": ["Jovial Cheukam Ngouonou", "Ramiz Gindullin", "Claude-Guy Quimper", "Nicolas Beldiceanu", "Remi Douence"], "title": "Incremental Selection of Most-Filtering Conjectures and Proofs of the Selected Conjectures", "categories": ["cs.AI", "F.2.2, F.4.1"], "comment": null, "summary": "We present an improved incremental selection algorithm of the selection\nalgorithm presented in [1] and prove all the selected conjectures.", "AI": {"tldr": "The paper improves on an earlier selection algorithm and proves selected conjectures.", "motivation": "The motivation is to enhance the efficiency and effectiveness of the selection algorithm provided in the referenced work.", "method": "An incremental improvement of the prior selection algorithm, focusing on better selection and proof processes.", "result": "All selected conjectures were proven successfully using the improved algorithm.", "conclusion": "The improved algorithm demonstrates enhanced selection capabilities and supports proofs of conjectures efficiently."}}
{"id": "2511.00626", "pdf": "https://arxiv.org/pdf/2511.00626", "abs": "https://arxiv.org/abs/2511.00626", "authors": ["Alexis Saurin"], "title": "Proceedings Twelfth Workshop on Fixed Points in Computer Science", "categories": ["cs.LO", "cs.PL"], "comment": null, "summary": "This EPTCS volume contains the post-proceedings of the Twelfth International\nWorkshop on Fixed Points in Computer Science, presenting a selection of the\nworks presented during the workshop that took place in Naples (Italy) on the\n19th and 20th of February 2024 as a satellite of the International Conference\non Computer Science Logic (CSL 2024).", "AI": {"tldr": "Post-proceedings from the Twelfth International Workshop on Fixed Points in Computer Science, linked to CSL 2024.", "motivation": "To gather and publish selected works presented during the workshop on fixed points in computer science.", "method": "Post-proceedings compilation based on workshop presentations held in Naples, 2024.", "result": "Collection of selected papers and research contributions in the field of fixed points in computer science.", "conclusion": "The workshop and its post-proceedings provide valuable insights into fixed points research, contributing to the broader field within computer science."}}
{"id": "2511.00268", "pdf": "https://arxiv.org/pdf/2511.00268", "abs": "https://arxiv.org/abs/2511.00268", "authors": ["Shounak Paul", "Dhananjay Ghumare", "Pawan Goyal", "Saptarshi Ghosh", "Ashutosh Modi"], "title": "IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "Accepted at EMNLP 2025 (Main)", "summary": "Identifying/retrieving relevant statutes and prior cases/precedents for a\ngiven legal situation are common tasks exercised by law practitioners.\nResearchers to date have addressed the two tasks independently, thus developing\ncompletely different datasets and models for each task; however, both retrieval\ntasks are inherently related, e.g., similar cases tend to cite similar statutes\n(due to similar factual situation). In this paper, we address this gap. We\npropose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),\nwhich is a unique corpus that provides a common testbed for developing models\nfor both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit\nthe dependence between the two. We experiment extensively with several baseline\nmodels on the tasks, including lexical models, semantic models and ensemble\nbased on GNNs. Further, to exploit the dependence between the two tasks, we\ndevelop an LLM-based re-ranking approach that gives the best performance.", "AI": {"tldr": "The paper introduces IL-PCR, a joint testbed for statute and precedent retrieval in legal cases, addressing their interrelated nature.", "motivation": "Law practitioners require tools to efficiently retrieve statutes and prior cases for legal situations. Existing methods treat these tasks separately, missing their inherent connection.", "method": "The authors propose IL-PCR, a legal corpus for combined retrieval of statutes and precedents, experimenting with baseline models including lexical, semantic, and GNN-based ensemble models. They further develop an LLM-based re-ranking method for improved performance.", "result": "The IL-PCR corpus effectively shows the dependence between statute retrieval and precedent retrieval, and the LLM-based re-ranking method demonstrates superior results compared to other baselines.", "conclusion": "Combining both retrieval tasks through IL-PCR and using advanced methods like LLM-based re-ranking improves legal information retrieval, highlighting their inherent interrelation."}}
{"id": "2511.00215", "pdf": "https://arxiv.org/pdf/2511.00215", "abs": "https://arxiv.org/abs/2511.00215", "authors": ["Xiaomeng Xu", "Zahin Wahab", "Reid Holmes", "Caroline Lemieux"], "title": "DocPrism: Local Categorization and External Filtering to Identify Relevant Code-Documentation Inconsistencies", "categories": ["cs.SE"], "comment": null, "summary": "Code-documentation inconsistencies are common and undesirable: they can lead\nto developer misunderstandings and software defects. This paper introduces\nDocPrism, a multi-language, code-documentation inconsistency detection tool.\nDocPrism uses a standard large language model (LLM) to analyze and explain\ninconsistencies. Plain use of LLMs for this task yield unacceptably high false\npositive rates: LLMs identify natural gaps between high-level documentation and\ndetailed code implementations as inconsistencies. We introduce and apply the\nLocal Categorization, External Filtering (LCEF) methodology to reduce false\npositives. LCEF relies on the LLM's local completion skills rather than its\nlong-term reasoning skills. In our ablation study, LCEF reduces DocPrism's\ninconsistency flag rate from 98% to 14%, and increases accuracy from 14% to\n94%. On a broad evaluation across Python, TypeScript, C++, and Java, DocPrism\nmaintains a low flag rate of 15%, and achieves a precision of 0.62 without\nperforming any fine-tuning.", "AI": {"tldr": "The paper introduces DocPrism, a tool for detecting code-documentation inconsistencies using large language models (LLMs), and provides a methodology to reduce false positives.", "motivation": "Code-documentation inconsistencies can cause misunderstandings and software defects, necessitating tools to identify these discrepancies efficiently across different programming languages.", "method": "The paper introduces the Local Categorization, External Filtering (LCEF) methodology, which leverages LLM's local completion skills to reduce false positives in detecting code-documentation inconsistencies.", "result": "Using the LCEF methodology, inconsistency flags reduced from 98% to 14%, accuracy increased from 14% to 94%, and in broader evaluation across multiple languages, DocPrism achieved 15% flag rate and 0.62 precision, without fine-tuning.", "conclusion": "DocPrism effectively detects code-documentation inconsistencies with reduced false positives, demonstrating significant potential in software engineering and multi-language support."}}
{"id": "2511.00139", "pdf": "https://arxiv.org/pdf/2511.00139", "abs": "https://arxiv.org/abs/2511.00139", "authors": ["Yu Cui", "Yujian Zhang", "Lina Tao", "Yang Li", "Xinyu Yi", "Zhibin Li"], "title": "End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Achieving human-like dexterous manipulation remains a major challenge for\ngeneral-purpose robots. While Vision-Language-Action (VLA) models show\npotential in learning skills from demonstrations, their scalability is limited\nby scarce high-quality training data. Existing data collection methods face\ninherent constraints: manual teleoperation overloads human operators, while\nautomated planning often produces unnatural motions. We propose a Shared\nAutonomy framework that divides control between macro and micro motions. A\nhuman operator guides the robot's arm pose through intuitive VR teleoperation,\nwhile an autonomous DexGrasp-VLA policy handles fine-grained hand control using\nreal-time tactile and visual feedback. This division significantly reduces\ncognitive load and enables efficient collection of high-quality coordinated\narm-hand demonstrations. Using this data, we train an end-to-end VLA policy\nenhanced with our novel Arm-Hand Feature Enhancement module, which captures\nboth distinct and shared representations of macro and micro movements for more\nnatural coordination. Our Corrective Teleoperation system enables continuous\npolicy improvement through human-in-the-loop failure recovery. Experiments\ndemonstrate that our framework generates high-quality data with minimal\nmanpower and achieves a 90% success rate across diverse objects, including\nunseen instances. Comprehensive evaluations validate the system's effectiveness\nin developing dexterous manipulation capabilities.", "AI": {"tldr": "This paper introduces a Shared Autonomy framework to improve dexterous robotic manipulation using human-robot collaboration and a novel Vision-Language-Action (VLA) policy.", "motivation": "Human-like dexterous manipulation is a significant challenge for robots, and current data collection methods face limitations due to high cognitive load and unnatural motions.", "method": "The study proposes shared control between human operators and autonomous policies. VR teleoperation allows human operators to guide robot arm movements, while an autonomous DexGrasp-VLA policy controls fine-tuned hand movements, using tactile and visual feedback. The collected demonstrations are used to train a VLA policy supported by an Arm-Hand Feature Enhancement module for better movement coordination.", "result": "The proposed framework reduces manpower while enabling the creation of high-quality data. It achieves a 90% success rate in dexterous manipulation tasks across various objects, including unseen ones.", "conclusion": "The Shared Autonomy framework and enhanced VLA model prove effective in addressing challenges of dexterous robotic manipulation and enable ongoing policy improvements."}}
{"id": "2511.00043", "pdf": "https://arxiv.org/pdf/2511.00043", "abs": "https://arxiv.org/abs/2511.00043", "authors": ["Tyrus Whitman", "Andrew Particka", "Christopher Diers", "Ian Griffin", "Charuka Wickramasinghe", "Pradeep Ranaweera"], "title": "Physics-Informed Neural Network Frameworks for the Analysis of Engineering and Biological Dynamical Systems Governed by Ordinary Differential Equations", "categories": ["cs.LG"], "comment": "21 pages, 10 figures, 5 tables", "summary": "In this study, we present and validate the predictive capability of the\nPhysics-Informed Neural Networks (PINNs) methodology for solving a variety of\nengineering and biological dynamical systems governed by ordinary differential\nequations (ODEs). While traditional numerical methods a re effective for many\nODEs, they often struggle to achieve convergence in problems involving high\nstiffness, shocks, irregular domains, singular perturbations, high dimensions,\nor boundary discontinuities. Alternatively, PINNs offer a powerful approach for\nhandling challenging numerical scenarios. In this study, classical ODE problems\nare employed as controlled testbeds to systematically evaluate the accuracy,\ntraining efficiency, and generalization capability under controlled conditions\nof the PINNs framework. Although not a universal solution, PINNs can achieve\nsuperior results by embedding physical laws directly into the learning process.\nWe first analyze the existence and uniqueness properties of several benchmark\nproblems and subsequently validate the PINNs methodology on these model\nsystems. Our results demonstrate that for complex problems to converge to\ncorrect solutions, the loss function components data loss, initial condition\nloss, and residual loss must be appropriately balanced through careful\nweighting. We further establish that systematic tuning of hyperparameters,\nincluding network depth, layer width, activation functions, learning rate,\noptimization algorithms, w eight initialization schemes, and collocation point\nsampling, plays a crucial role in achieving accurate solutions. Additionally,\nembedding prior knowledge and imposing hard constraints on the network\narchitecture, without loss the generality of the ODE system, significantly\nenhances the predictive capability of PINNs.", "AI": {"tldr": "Physics-Informed Neural Networks (PINNs) are explored for solving challenging ordinary differential equation (ODE) problems, demonstrating the necessity of proper loss function balancing and hyperparameter tuning for accurate predictions.", "motivation": "To address limitations of traditional numerical methods in solving complex ODE problems such as high stiffness, shocks, irregular domains, and singular perturbations, PINNs are investigated as an alternative.", "method": "PINNs are validated using classical ODE problems as controlled testbeds, analyzing convergence, accuracy, and generalization capabilities, with focus on loss function balancing and hyperparameter tuning.", "result": "PINNs exhibit improved predictive capabilities for complex ODEs when components like data loss, initial condition loss, and residual loss are balanced effectively and hyperparameters are systematically tuned.", "conclusion": "PINNs provide an effective framework for solving complex ODE problems by incorporating physical laws into the learning process and embedding prior knowledge, although they require meticulous configuration."}}
{"id": "2511.00060", "pdf": "https://arxiv.org/pdf/2511.00060", "abs": "https://arxiv.org/abs/2511.00060", "authors": ["Zhiqi Qi", "Runxin Zhao", "Hanyang Zhuang", "Chunxiang Wang", "Ming Yang"], "title": "Which LiDAR scanning pattern is better for roadside perception: Repetitive or Non-repetitive?", "categories": ["cs.CV", "cs.RO", "eess.IV"], "comment": null, "summary": "LiDAR-based roadside perception is a cornerstone of advanced Intelligent\nTransportation Systems (ITS). While considerable research has addressed optimal\nLiDAR placement for infrastructure, the profound impact of differing LiDAR\nscanning patterns on perceptual performance remains comparatively\nunder-investigated. The inherent nature of various scanning modes - such as\ntraditional repetitive (mechanical/solid-state) versus emerging non-repetitive\n(e.g. prism-based) systems - leads to distinct point cloud distributions at\nvarying distances, critically dictating the efficacy of object detection and\noverall environmental understanding. To systematically investigate these\ndifferences in infrastructure-based contexts, we introduce the \"InfraLiDARs'\nBenchmark,\" a novel dataset meticulously collected in the CARLA simulation\nenvironment using concurrently operating infrastructure-based LiDARs exhibiting\nboth scanning paradigms. Leveraging this benchmark, we conduct a comprehensive\nstatistical analysis of the respective LiDAR scanning abilities and evaluate\nthe impact of these distinct patterns on the performance of various leading 3D\nobject detection algorithms. Our findings reveal that non-repetitive scanning\nLiDAR and the 128-line repetitive LiDAR were found to exhibit comparable\ndetection performance across various scenarios. Despite non-repetitive LiDAR's\nlimited perception range, it's a cost-effective option considering its low\nprice. Ultimately, this study provides insights for setting up roadside\nperception system with optimal LiDAR scanning patterns and compatible\nalgorithms for diverse roadside applications, and publicly releases the\n\"InfraLiDARs' Benchmark\" dataset to foster further research.", "AI": {"tldr": "This paper investigates the impact of different LiDAR scanning patterns (repetitive vs. non-repetitive) on roadside perception through a novel benchmark dataset and analyzes their performance implications.", "motivation": "To address the underexplored impact of different LiDAR scanning modes on perceptual performance in roadside infrastructure settings, enabling better understanding and optimization of ITS applications.", "method": "The study introduces the 'InfraLiDARs' Benchmark dataset collected using mixed LiDAR systems in CARLA simulations, analyzes scanning modes statistically, and evaluates their role in 3D object detection algorithms.", "result": "The study reveals comparable detection performance between non-repetitive and 128-line repetitive LiDAR systems, with non-repetitive LiDAR offering cost-effective benefits despite its limited range.", "conclusion": "The findings guide optimal LiDAR scanning mode selection and compatible algorithms for roadside applications, promoting efficient and cost-effective roadside perception setups. The 'InfraLiDARs' Benchmark is publicly shared to encourage future research."}}
{"id": "2511.01064", "pdf": "https://arxiv.org/pdf/2511.01064", "abs": "https://arxiv.org/abs/2511.01064", "authors": ["Charles C. Margossian", "Lawrence K. Saul"], "title": "Generalized Guarantees for Variational Inference in the Presence of Even and Elliptical Symmetry", "categories": ["stat.ML", "cs.LG", "stat.CO"], "comment": null, "summary": "We extend several recent results providing symmetry-based guarantees for\nvariational inference (VI) with location-scale families. VI approximates a\ntarget density~$p$ by the best match $q^*$ in a family $Q$ of tractable\ndistributions that in general does not contain $p$. It is known that VI can\nrecover key properties of $p$, such as its mean and correlation matrix, when\n$p$ and $Q$ exhibit certain symmetries and $q^*$ is found by minimizing the\nreverse Kullback-Leibler divergence. We extend these guarantees in two\nimportant directions. First, we provide symmetry-based guarantees for a broader\nfamily of divergences, highlighting the properties of variational objectives\nunder which VI provably recovers the mean and correlation matrix. Second, we\nobtain further guarantees for VI when the target density $p$ exhibits even and\nelliptical symmetries in some but not all of its coordinates. These partial\nsymmetries arise naturally in Bayesian hierarchical models, where the prior\ninduces a challenging geometry but still possesses axes of symmetry. We\nillustrate these theoretical results in a number of experimental settings.", "AI": {"tldr": "The paper extends symmetry-based guarantees for variational inference (VI), providing broader divergence family compatibility and handling partial symmetries in hierarchical models for better approximations.", "motivation": "Recent VI results ensure recovery of target density properties using reverse KL divergence under symmetry assumptions; this paper seeks to extend these results to broader divergences and more general symmetry scenarios.", "method": "Theoretical enhancements are made to guarantees of VI performance under broader divergence functions and partial symmetric properties, supported by experimental illustrations.", "result": "Improved theoretical results that expand VI\u2019s applicability in recovering mean and correlation matrix from complex targets, including those with mixed symmetries.", "conclusion": "The paper strengthens the theoretical basis of VI, demonstrating its extended capability to approximate target densities effectively even in challenging geometric or partial symmetry settings."}}
{"id": "2511.00369", "pdf": "https://arxiv.org/pdf/2511.00369", "abs": "https://arxiv.org/abs/2511.00369", "authors": ["Farjana Aktar", "Mohd Ruhul Ameen", "Akif Islam", "Md Ekramul Hamid"], "title": "Balancing Interpretability and Performance in Motor Imagery EEG Classification: A Comparative Study of ANFIS-FBCSP-PSO and EEGNet", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": "6 pages, 3 figures, 8 tables, Submitted to ICECTE 2026", "summary": "Achieving both accurate and interpretable classification of motor imagery EEG\nremains a key challenge in brain computer interface (BCI) research. This paper\ncompares a transparent fuzzy reasoning approach (ANFIS-FBCSP-PSO) with a deep\nlearning benchmark (EEGNet) using the BCI Competition IV-2a dataset. The ANFIS\npipeline combines filter bank common spatial pattern feature extraction with\nfuzzy IF-THEN rules optimized via particle swarm optimization, while EEGNet\nlearns hierarchical spatial temporal representations directly from raw EEG\ndata. In within-subject experiments, the fuzzy neural model performed better\n(68.58 percent +/- 13.76 percent accuracy, kappa = 58.04 percent +/- 18.43),\nwhile in cross-subject (LOSO) tests, the deep model exhibited stronger\ngeneralization (68.20 percent +/- 12.13 percent accuracy, kappa = 57.33 percent\n+/- 16.22). The study provides practical guidance for selecting MI-BCI systems\naccording to design goals: interpretability or robustness across users. Future\ninvestigations into transformer based and hybrid neuro symbolic frameworks are\nexpected to advance transparent EEG decoding.", "AI": {"tldr": "This paper evaluates transparent fuzzy reasoning (ANFIS-FBCSP-PSO) against deep learning (EEGNet) for motor imagery EEG classification, highlighting their respective strengths in interpretability and generalization.", "motivation": "Addressing the challenge of achieving accurate and interpretable classification in motor imagery EEG for brain-computer interfaces.", "method": "Comparison of ANFIS-FBCSP-PSO using fuzzy reasoning and feature extraction versus EEGNet deep learning using hierarchical spatial temporal representations. Analysis conducted on BCI Competition IV-2a dataset.", "result": "ANFIS achieved higher accuracy in within-subject experiments, while EEGNet demonstrated better cross-subject generalization in tests.", "conclusion": "Guidance is provided for selecting MI-BCI systems based on design goals, and future research directions suggest improvements with transformer-based or hybrid neuro-symbolic frameworks."}}
{"id": "2511.00206", "pdf": "https://arxiv.org/pdf/2511.00206", "abs": "https://arxiv.org/abs/2511.00206", "authors": ["Dirk U. Wulff", "Rui Mata"], "title": "Advancing Cognitive Science with LLMs", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Cognitive science faces ongoing challenges in knowledge synthesis and\nconceptual clarity, in part due to its multifaceted and interdisciplinary\nnature. Recent advances in artificial intelligence, particularly the\ndevelopment of large language models (LLMs), offer tools that may help to\naddress these issues. This review examines how LLMs can support areas where the\nfield has historically struggled, including establishing cross-disciplinary\nconnections, formalizing theories, developing clear measurement taxonomies,\nachieving generalizability through integrated modeling frameworks, and\ncapturing contextual and individual variation. We outline the current\ncapabilities and limitations of LLMs in these domains, including potential\npitfalls. Taken together, we conclude that LLMs can serve as tools for a more\nintegrative and cumulative cognitive science when used judiciously to\ncomplement, rather than replace, human expertise.", "AI": {"tldr": "The paper explores the role of large language models (LLMs) in addressing challenges in cognitive science stemming from its interdisciplinary nature.", "motivation": "The paper aims to tackle the persistent issues in cognitive science surrounding knowledge synthesis, clarity, and integration, leveraging the capabilities of LLMs.", "method": "The authors review LLM applications in fields where cognitive science struggles, discussing strengths, limitations, and potential pitfalls.", "result": "The study identifies that LLMs can facilitate cross-disciplinary connections, clearer theory formalization, better measurements, integrated models, and capturing variations.", "conclusion": "LLMs offer significant promise as complementary tools to enhance cognitive science, but their role should augment human expertise rather than replace it."}}
{"id": "2511.01127", "pdf": "https://arxiv.org/pdf/2511.01127", "abs": "https://arxiv.org/abs/2511.01127", "authors": ["Fabio Diniz Rossi"], "title": "Neuro-Inspired Task Offloading in Edge-IoT Networks Using Spiking Neural Networks", "categories": ["cs.DC"], "comment": "5 pages, 2 figures, 1 table", "summary": "Traditional task offloading strategies in edge computing often rely on static\nheuristics or data-intensive machine learning models, which are not always\nsuitable for highly dynamic and resource-constrained environments. In this\npaper, we propose a novel task-offloading framework based on Spiking Neural\nNetworks inspired by the efficiency and adaptability of biological neural\nsystems. Our approach integrates an SNN-based decision module into edge nodes\nto perform real-time, energy-efficient task orchestration. We evaluate the\nmodel under various IoT workload scenarios using a hybrid simulation\nenvironment composed of YAFS and Brian2. The results demonstrate that our\nSNN-based framework significantly reduces task processing latency and energy\nconsumption while improving task success rates. Compared to traditional\nheuristic and ML-based strategies, our model achieves up to 26% lower latency,\n32% less energy consumption, and 25\\% higher success rate under high-load\nconditions.", "AI": {"tldr": "The paper introduces a Spiking Neural Networks (SNN)-based task offloading framework that achieves improved efficiency, reduced latency, energy consumption, and higher success rates compared to traditional strategies.", "motivation": "Traditional task offloading techniques face challenges in dynamic, resource-constrained environments due to reliance on static heuristics or demanding machine learning models.", "method": "The framework integrates a Spiking Neural Networks (SNN)-based decision module into edge nodes for real-time, energy-efficient task orchestration, evaluated using hybrid simulations with YAFS and Brian2.", "result": "The SNN-based framework reduces latency by up to 26%, energy consumption by 32%, and increases task success rate by 25% under high-load scenarios.", "conclusion": "The proposed SNN-based system significantly optimizes task processing and orchestration in edge computing, offering substantial benefits over traditional methods in challenging environments."}}
{"id": "2511.00865", "pdf": "https://arxiv.org/pdf/2511.00865", "abs": "https://arxiv.org/abs/2511.00865", "authors": ["Hangdong Zhao", "Zhenghong Yu", "Srinag Rao", "Simon Frisk", "Zhiwei Fan", "Paraschos Koutris"], "title": "FlowLog: Efficient and Extensible Datalog via Incrementality", "categories": ["cs.DB", "cs.PL"], "comment": "Accepted to VLDB 2026", "summary": "Datalog-based languages are regaining popularity as a powerful abstraction\nfor expressing recursive computations in domains such as program analysis and\ngraph processing. However, existing systems often face a trade-off between\nefficiency and extensibility. Engines like Souffle achieve high efficiency\nthrough domain-specific designs, but lack general-purpose flexibility. Others,\nlike RecStep, offer modularity by layering Datalog on traditional databases,\nbut struggle to integrate Datalog-specific optimizations.\n  This paper bridges this gap by presenting FlowLog, a new Datalog engine that\nuses an explicit relational IR per-rule to cleanly separate recursive control\n(e.g., semi-naive execution) from each rule's logical plan. This boundary lets\nus retain fine-grained, Datalog-aware optimizations at the logical layer, but\nalso reuse off-the-shelf database primitives at execution. At the logical level\n(i.e. IR), we apply proven SQL optimizations, such as logic fusion and subplan\nreuse. To address high volatility in recursive workloads, we adopt a\nrobustness-first approach that pairs a structural optimizer (avoiding\nworst-case joins) with sideways information passing (early filtering). Built\natop Differential Dataflow--a mature framework for streaming analytics--FlowLog\nsupports both batch and incremental Datalog and adds novel recursion-aware\noptimizations called Boolean (or algebraic) specialization. Our evaluation\nshows that FlowLog outperforms state-of-the-art Datalog engines and modern\ndatabases across a broad range of recursive workloads, achieving superior\nscalability while preserving a simple and extensible architecture.", "AI": {"tldr": "FlowLog addresses efficiency and extensibility in Datalog-based languages by separating recursive control and logical plans, achieving high performance and scalability.", "motivation": "Existing Datalog systems lack a balance between efficiency and extensibility, with domain-specific designs or modularity struggling to incorporate specific optimizations.", "method": "FlowLog introduces an explicit relational IR per rule, applies SQL optimizations at the logical level, and adopts a robustness-first approach for recursive workloads, building on Differential Dataflow.", "result": "FlowLog outperforms leading Datalog engines and databases across recursive workloads with superior scalability and robust architecture.", "conclusion": "FlowLog provides an efficient and extensible solution for Datalog-based computations, supporting both batch and incremental processing with recursion-aware optimizations."}}
{"id": "2511.00270", "pdf": "https://arxiv.org/pdf/2511.00270", "abs": "https://arxiv.org/abs/2511.00270", "authors": ["Abhinav Joshi", "Vaibhav Sharma", "Sanjeet Singh", "Ashutosh Modi"], "title": "POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted at EMNLP 2025 (Main)", "summary": "Sign language translation remains a challenging task due to the scarcity of\nlarge-scale, sentence-aligned datasets. Prior arts have focused on various\nfeature extraction and architectural changes to support neural machine\ntranslation for sign languages. We propose POSESTITCH-SLT, a novel pre-training\nscheme that is inspired by linguistic-templates-based sentence generation\ntechnique. With translation comparison on two sign language datasets, How2Sign\nand iSign, we show that a simple transformer-based encoder-decoder architecture\noutperforms the prior art when considering template-generated sentence pairs in\ntraining. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign\nand from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for\npose-based gloss-free translation. The results demonstrate the effectiveness of\ntemplate-driven synthetic supervision in low-resource sign language settings.", "AI": {"tldr": "POSESTITCH-SLT enhances sign language translation using template-driven sentence generation, improving BLEU-4 scores on How2Sign and iSign datasets.", "motivation": "Addressing the challenge of sign language translation due to sparse datasets by exploring template-driven methods for synthetic sentence generation.", "method": "A pre-training scheme (POSESTITCH-SLT) utilizing linguistic template-based sentence generation combined with a transformer-based architecture.", "result": "BLEU-4 scores improved significantly: from 1.97 to 4.56 on How2Sign and from 0.55 to 3.43 on iSign datasets.", "conclusion": "Template-driven synthetic supervision is highly effective for low-resource sign language translation."}}
{"id": "2511.00262", "pdf": "https://arxiv.org/pdf/2511.00262", "abs": "https://arxiv.org/abs/2511.00262", "authors": ["Romina Etezadi", "Sallam Abualhaija", "Chetan Arora", "Lionel Briand"], "title": "LLM-Driven Cost-Effective Requirements Change Impact Analysis", "categories": ["cs.SE", "D.2; I.2"], "comment": "28 pages, 6 figures", "summary": "Requirements are inherently subject to changes throughout the software\ndevelopment lifecycle. Within the limited budget available to requirements\nengineers, manually identifying the impact of such changes on other\nrequirements is both error-prone and effort-intensive. That might lead to\noverlooked impacted requirements, which, if not properly managed, can cause\nserious issues in the downstream tasks. Inspired by the growing potential of\nlarge language models (LLMs) across diverse domains, we propose ProReFiCIA, an\nLLM-driven approach for automatically identifying the impacted requirements\nwhen changes occur. We conduct an extensive evaluation of ProReFiCIA using\nseveral LLMs and prompts variants tailored to this task. Using the best\ncombination of an LLM and a prompt variant, ProReFiCIA achieves a recall of\n93.3% on a benchmark dataset and 95.8% on a newly created industry dataset,\ndemonstrating its strong effectiveness in identifying impacted requirements.\nFurther, the cost of applying ProReFiCIA remains small, as the engineer only\nneeds to review the generated results, which represent between 2.1% and 8.5% of\nthe entire set of requirements.", "AI": {"tldr": "This paper proposes ProReFiCIA, utilizing large language models (LLMs) to automatically identify impacted requirements during software changes.", "motivation": "To address the challenges of manual requirement impact analysis, which is prone to errors and inefficiency.", "method": "ProReFiCIA employs LLMs paired with tailored prompt variants to analyze and identify changes and impacted requirements.", "result": "Achieved 93.3% recall on a benchmark dataset and 95.8% recall on an industry dataset with minimal review effort (2.1-8.5% of requirements).", "conclusion": "ProReFiCIA demonstrates high effectiveness and efficiency, reducing manual effort while reliably identifying requirement impacts."}}
{"id": "2511.00153", "pdf": "https://arxiv.org/pdf/2511.00153", "abs": "https://arxiv.org/abs/2511.00153", "authors": ["Justin Yu", "Yide Shentu", "Di Wu", "Pieter Abbeel", "Ken Goldberg", "Philipp Wu"], "title": "EgoMI: Learning Active Vision and Whole-Body Manipulation from Egocentric Human Demonstrations", "categories": ["cs.RO"], "comment": null, "summary": "Imitation learning from human demonstrations offers a promising approach for\nrobot skill acquisition, but egocentric human data introduces fundamental\nchallenges due to the embodiment gap. During manipulation, humans actively\ncoordinate head and hand movements, continuously reposition their viewpoint and\nuse pre-action visual fixation search strategies to locate relevant objects.\nThese behaviors create dynamic, task-driven head motions that static robot\nsensing systems cannot replicate, leading to a significant distribution shift\nthat degrades policy performance. We present EgoMI (Egocentric Manipulation\nInterface), a framework that captures synchronized end-effector and active head\ntrajectories during manipulation tasks, resulting in data that can be\nretargeted to compatible semi-humanoid robot embodiments. To handle rapid and\nwide-spanning head viewpoint changes, we introduce a memory-augmented policy\nthat selectively incorporates historical observations. We evaluate our approach\non a bimanual robot equipped with an actuated camera head and find that\npolicies with explicit head-motion modeling consistently outperform baseline\nmethods. Results suggest that coordinated hand-eye learning with EgoMI\neffectively bridges the human-robot embodiment gap for robust imitation\nlearning on semi-humanoid embodiments. Project page:\nhttps://egocentric-manipulation-interface.github.io", "AI": {"tldr": "The paper proposes EgoMI, a framework for imitation learning from egocentric human data to improve robot skill acquisition, focusing on coordinated hand-eye movements.", "motivation": "Human head and hand movements during manipulation create challenges for robots due to dynamic viewpoints and distribution shifts that affect policy performance.", "method": "The framework synchronizes end-effector and head trajectories while introducing a memory-augmented policy to handle historical observations.", "result": "Policies incorporating head-motion modeling show enhanced performance over baselines, demonstrating effective bridging of human-robot embodiment gaps.", "conclusion": "The EgoMI framework facilitates robust imitation learning for semi-humanoid robots by leveraging coordinated human head and hand movements."}}
{"id": "2511.00044", "pdf": "https://arxiv.org/pdf/2511.00044", "abs": "https://arxiv.org/abs/2511.00044", "authors": ["Kohei Tsuchiyama", "Andre Roehm", "Takatomo Mihana", "Ryoichi Horisaki"], "title": "ReLaX-Net: Reusing Layers for Parameter-Efficient Physical Neural Networks", "categories": ["cs.LG", "nlin.AO"], "comment": null, "summary": "Physical Neural Networks (PNN) are promising platforms for next-generation\ncomputing systems. However, recent advances in digital neural network\nperformance are largely driven by the rapid growth in the number of trainable\nparameters and, so far, demonstrated PNNs are lagging behind by several orders\nof magnitude in terms of scale. This mirrors size and performance constraints\nfound in early digital neural networks. In that period, efficient reuse of\nparameters contributed to the development of parameter-efficient architectures\nsuch as convolutional neural networks.\n  In this work, we numerically investigate hardware-friendly weight-tying for\nPNNs. Crucially, with many PNN systems, there is a time-scale separation\nbetween the fast dynamic active elements of the forward pass and the only\nslowly trainable elements implementing weights and biases. With this in mind,we\npropose the Reuse of Layers for eXpanding a Neural Network (ReLaX-Net)\narchitecture, which employs a simple layer-by-layer time-multiplexing scheme to\nincrease the effective network depth and efficiently use the number of\nparameters. We only require the addition of fast switches for existing PNNs. We\nvalidate ReLaX-Nets via numerical experiments on image classification and\nnatural language processing tasks. Our results show that ReLaX-Net improves\ncomputational performance with only minor modifications to a conventional PNN.\nWe observe a favorable scaling, where ReLaX-Nets exceed the performance of\nequivalent traditional RNNs or DNNs with the same number of parameters.", "AI": {"tldr": "This paper proposes the ReLaX-Net architecture to improve computational efficiency in physical neural networks (PNNs) through hardware-friendly weight-tying and layer reuse.", "motivation": "Current physical neural networks (PNNs) have a limitation in scale and performance compared to digital neural networks due to constraints on trainable parameters.", "method": "The ReLaX-Net architecture introduces a layer-by-layer time-multiplexing approach, requiring only fast switches, to expand network depth and reuse parameters efficiently in PNNs.", "result": "Numerical experiments on image classification and natural language processing tasks demonstrate ReLaX-Net's superior computational performance compared to traditional deep neural networks with the same number of parameters.", "conclusion": "ReLaX-Net offers a promising solution for scaling physical neural networks efficiently and improving their performance with minimal hardware modifications."}}
{"id": "2511.00062", "pdf": "https://arxiv.org/pdf/2511.00062", "abs": "https://arxiv.org/abs/2511.00062", "authors": ["NVIDIA", ":", "Arslan Ali", "Junjie Bai", "Maciej Bala", "Yogesh Balaji", "Aaron Blakeman", "Tiffany Cai", "Jiaxin Cao", "Tianshi Cao", "Elizabeth Cha", "Yu-Wei Chao", "Prithvijit Chattopadhyay", "Mike Chen", "Yongxin Chen", "Yu Chen", "Shuai Cheng", "Yin Cui", "Jenna Diamond", "Yifan Ding", "Jiaojiao Fan", "Linxi Fan", "Liang Feng", "Francesco Ferroni", "Sanja Fidler", "Xiao Fu", "Ruiyuan Gao", "Yunhao Ge", "Jinwei Gu", "Aryaman Gupta", "Siddharth Gururani", "Imad El Hanafi", "Ali Hassani", "Zekun Hao", "Jacob Huffman", "Joel Jang", "Pooya Jannaty", "Jan Kautz", "Grace Lam", "Xuan Li", "Zhaoshuo Li", "Maosheng Liao", "Chen-Hsuan Lin", "Tsung-Yi Lin", "Yen-Chen Lin", "Huan Ling", "Ming-Yu Liu", "Xian Liu", "Yifan Lu", "Alice Luo", "Qianli Ma", "Hanzi Mao", "Kaichun Mo", "Seungjun Nah", "Yashraj Narang", "Abhijeet Panaskar", "Lindsey Pavao", "Trung Pham", "Morteza Ramezanali", "Fitsum Reda", "Scott Reed", "Xuanchi Ren", "Haonan Shao", "Yue Shen", "Stella Shi", "Shuran Song", "Bartosz Stefaniak", "Shangkun Sun", "Shitao Tang", "Sameena Tasmeen", "Lyne Tchapmi", "Wei-Cheng Tseng", "Jibin Varghese", "Andrew Z. Wang", "Hao Wang", "Haoxiang Wang", "Heng Wang", "Ting-Chun Wang", "Fangyin Wei", "Jiashu Xu", "Dinghao Yang", "Xiaodong Yang", "Haotian Ye", "Seonghyeon Ye", "Xiaohui Zeng", "Jing Zhang", "Qinsheng Zhang", "Kaiwen Zheng", "Andrew Zhu", "Yuke Zhu"], "title": "World Simulation with Video Foundation Models for Physical AI", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World\nFoundation Models for Physical AI. Built on a flow-based architecture,\n[Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation\nin a single model and leverages [Cosmos-Reason1], a Physical AI vision-language\nmodel, to provide richer text grounding and finer control of world simulation.\nTrained on 200M curated video clips and refined with reinforcement\nlearning-based post-training, [Cosmos-Predict2.5] achieves substantial\nimprovements over [Cosmos-Predict1] in video quality and instruction alignment,\nwith models released at 2B and 14B scales. These capabilities enable more\nreliable synthetic data generation, policy evaluation, and closed-loop\nsimulation for robotics and autonomous systems. We further extend the family\nwith [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and\nReal2Real world translation. Despite being 3.5$\\times$ smaller than\n[Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video\ngeneration. Together, these advances establish [Cosmos-Predict2.5] and\n[Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To\naccelerate research and deployment in Physical AI, we release source code,\npretrained checkpoints, and curated benchmarks under the NVIDIA Open Model\nLicense at https://github.com/nvidia-cosmos/cosmos-predict2.5 and\nhttps://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open\nresources lower the barrier to adoption and foster innovation in building the\nnext generation of embodied intelligence.", "AI": {"tldr": "The paper introduces [Cosmos-Predict2.5] and [Cosmos-Transfer2.5], advanced Physical AI models designed for unified world generation and real-world translations.", "motivation": "To develop robust tools for world simulation, synthetic data generation, and evaluation in robotics and autonomous systems while lowering barriers for innovation in Physical AI.", "method": "Combines a flow-based architecture with a vision-language model [Cosmos-Reason1], uses a 200M video dataset and reinforcement learning for training, and introduces control-net like style for translation.", "result": "Improvements in video quality, instruction alignment, and robust video generation with smaller models achieving better performance.", "conclusion": "[Cosmos-Predict2.5] and [Cosmos-Transfer2.5] establish themselves as versatile tools for scaling embodied intelligence, with open-source resources released to accelerate research."}}
{"id": "2511.01096", "pdf": "https://arxiv.org/pdf/2511.01096", "abs": "https://arxiv.org/abs/2511.01096", "authors": ["Alex Boyd", "Andrew Warrington", "Taha Kass-Hout", "Parminder Bhatia", "Danica Xiao"], "title": "Hyper Hawkes Processes: Interpretable Models of Marked Temporal Point Processes", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Foundational marked temporal point process (MTPP) models, such as the Hawkes\nprocess, often use inexpressive model families in order to offer interpretable\nparameterizations of event data. On the other hand, neural MTPPs models forego\nthis interpretability in favor of absolute predictive performance. In this\nwork, we present a new family MTPP models: the hyper Hawkes process (HHP),\nwhich aims to be as flexible and performant as neural MTPPs, while retaining\ninterpretable aspects. To achieve this, the HHP extends the classical Hawkes\nprocess to increase its expressivity by first expanding the dimension of the\nprocess into a latent space, and then introducing a hypernetwork to allow time-\nand data-dependent dynamics. These extensions define a highly performant MTPP\nfamily, achieving state-of-the-art performance across a range of benchmark\ntasks and metrics. Furthermore, by retaining the linearity of the recurrence,\nalbeit now piecewise and conditionally linear, the HHP also retains much of the\nstructure of the original Hawkes process, which we exploit to create direct\nprobes into how the model creates predictions. HHP models therefore offer both\nstate-of-the-art predictions, while also providing an opportunity to ``open the\nbox'' and inspect how predictions were generated.", "AI": {"tldr": "The paper introduces the Hyper Hawkes Process (HHP), a new Marked Temporal Point Process (MTPP) model designed to be as flexible and accurate as neural MTPPs while retaining interpretability.", "motivation": "MTPP models like the Hawkes process favor interpretability but lack flexibility, while neural MTPPs focus on better predictive performance at the cost of interpretability. The study aims to balance these two aspects and improve both expressivity and interpretability.", "method": "The proposed HHP extends classical Hawkes processes by embedding event data into a latent space and using a hypernetwork to incorporate time- and data-dependent dynamics, retaining piecewise and conditional linearity.", "result": "The HHP achieves state-of-the-art performance in benchmark tasks and metrics in MTPP modeling, demonstrating enhanced flexibility and predictive power.", "conclusion": "The HHP offers an innovative MTPP approach with remarkable predictive power and interpretability, combining the strengths of neural MTPPs and structured classical models like the Hawkes process."}}
{"id": "2511.00529", "pdf": "https://arxiv.org/pdf/2511.00529", "abs": "https://arxiv.org/abs/2511.00529", "authors": ["Botao 'Amber' Hu"], "title": "On Improvisation and Open-Endedness: Insights for Experiential AI", "categories": ["cs.HC", "cs.AI", "cs.NE", "cs.SY", "eess.SY"], "comment": "Submitted to AAAI 2026 Creative AI for Live Interactive Performances\n  Workshop (CLIP) as a work-in-progress paper", "summary": "Improvisation-the art of spontaneous creation that unfolds moment-to-moment\nwithout a scripted outcome-requires practitioners to continuously sense, adapt,\nand create anew. It is a fundamental mode of human creativity spanning music,\ndance, and everyday life. The open-ended nature of improvisation produces a\nstream of novel, unrepeatable moments-an aspect highly valued in artistic\ncreativity. In parallel, open-endedness (OE)-a system's capacity for unbounded\nnovelty and endless \"interestingness\"-is exemplified in natural or cultural\nevolution and has been considered \"the last grand challenge\" in artificial life\n(ALife). The rise of generative AI now raises the question in computational\ncreativity (CC) research: What makes a \"good\" improvisation for AI? Can AI\nlearn to improvise in a genuinely open-ended way? In this work-in-progress\npaper, we report insights from in-depth interviews with 6 experts in\nimprovisation across dance, music, and contact improvisation. We draw systemic\nconnections between human improvisational arts and the design of future\nexperiential AI agents that could improvise alone or alongside humans-or even\nwith other AI agents-embodying qualities of improvisation drawn from practice:\nactive listening (umwelt and awareness), being in the time (mindfulness and\nephemerality), embracing the unknown (source of randomness and serendipity),\nnon-judgmental flow (acceptance and dynamical stability, balancing structure\nand surprise (unpredictable criticality at edge of chaos), imaginative metaphor\n(synaesthesia and planning), empathy, trust, boundary, and care (mutual theory\nof mind), and playfulness and intrinsic motivation (maintaining\ninterestingness).", "AI": {"tldr": "This paper explores the connection between human improvisation and AI's capability to improvise in an open-ended way, utilizing qualities like active listening, mindfulness, randomness, empathy, and playfulness.", "motivation": "To investigate whether AI can genuinely perform open-ended improvisation like humans and understand what constitutes 'good' improvisation for AI.", "method": "Conducted in-depth interviews with 6 experts in improvisation from fields such as dance, music, and contact improvisation.", "result": "Provides systemic insights into how AI can embody qualities of human improvisational arts, potentially enabling AI agents to improvise alone, alongside humans, or with other AI agents.", "conclusion": "AI has the potential to learn improvisation based on human practices, enabling the design of experiential AI agents that embody creativity, adaptability, and open-endedness."}}
{"id": "2511.00267", "pdf": "https://arxiv.org/pdf/2511.00267", "abs": "https://arxiv.org/abs/2511.00267", "authors": ["Christian Prothmann", "Vijay Gadepally", "Jeremy Kepner", "Koley Borchard", "Luca Carlone", "Zachary Folcik", "J. Daniel Grith", "Michael Houle", "Jonathan P. How", "Nathan Hughes", "Ifueko Igbinedion", "Hayden Jananthan", "Tejas Jayashankar", "Michael Jones", "Sertac Karaman", "Binoy G. Kurien", "Alejandro Lancho", "Giovanni Lavezzi", "Gary C. F. Lee", "Charles E. Leiserson", "Richard Linares", "Lindsey McEvoy", "Peter Michaleas", "Chasen Milner", "Alex Pentland", "Yury Polyanskiy", "Jovan Popovich", "Jeffrey Price", "Tim W. Reid", "Stephanie Riley", "Siddharth Samsi", "Peter Saunders", "Olga Simek", "Mark S. Veillette", "Amir Weiss", "Gregory W. Wornell", "Daniela Rus", "Scott T. Ruppel"], "title": "Advancing AI Challenges for the United States Department of the Air Force", "categories": ["cs.AI", "cs.CY", "cs.GL", "cs.LG"], "comment": "8 pages, 8 figures, 59 references. To appear in IEEE HPEC 2025", "summary": "The DAF-MIT AI Accelerator is a collaboration between the United States\nDepartment of the Air Force (DAF) and the Massachusetts Institute of Technology\n(MIT). This program pioneers fundamental advances in artificial intelligence\n(AI) to expand the competitive advantage of the United States in the defense\nand civilian sectors. In recent years, AI Accelerator projects have developed\nand launched public challenge problems aimed at advancing AI research in\npriority areas. Hallmarks of AI Accelerator challenges include large, publicly\navailable, and AI-ready datasets to stimulate open-source solutions and engage\nthe wider academic and private sector AI ecosystem. This article supplements\nour previous publication, which introduced AI Accelerator challenges. We\nprovide an update on how ongoing and new challenges have successfully\ncontributed to AI research and applications of AI technologies.", "AI": {"tldr": "The DAF-MIT AI Accelerator collaborates to advance AI for defense and civilian sectors, sharing large AI-ready datasets through public challenges to drive research and innovation.", "motivation": "To enhance the competitive advantage of the USA in AI for both defense and civilian purposes by encouraging innovation and research in the field.", "method": "The program releases public challenge problems featuring AI-ready datasets to involve academia and private sectors in solving pressing AI-related issues.", "result": "Ongoing and new challenges have positively influenced AI research and have demonstrated practical applications of AI technologies.", "conclusion": "This collaboration successfully catalyzes AI advancements through public challenges, engaging wider communities and enhancing AI applications across sectors."}}
{"id": "2511.01235", "pdf": "https://arxiv.org/pdf/2511.01235", "abs": "https://arxiv.org/abs/2511.01235", "authors": ["Shruthi Kannappan", "Ashwina Kumar", "Rupesh Nasre"], "title": "Scalable Maxflow Processing for Dynamic Graphs", "categories": ["cs.DC"], "comment": null, "summary": "The Maximum Flow (Max-Flow) problem is a cornerstone in graph theory and\ncombinatorial optimization, aiming to determine the largest possible flow from\na designated source node to a sink node within a capacitated flow network. It\nhas extensive applications across diverse domains such as computer networking,\ntransportation systems, and image segmentation. The objective is to maximize\nthe total throughput while respecting edge capacity constraints and maintaining\nflow conservation at all intermediate vertices.\n  Among the various algorithms proposed for solving the Max-Flow problem, the\nPush--Relabel algorithm is particularly notable for its efficiency and\nsuitability for parallelization, owing to its localized vertex-based\noperations. This property has motivated extensive research into GPU-accelerated\nMax-Flow computation, leveraging the high degree of parallelism inherent to\nmodern GPU architectures.\n  In this paper, we present a novel GPU-parallel Max-Flow algorithm capable of\nincrementally recomputing the maximum flow of a dynamic graph following a batch\nof edge updates. In addition, we introduce a high-performance static GPU\nalgorithm designed for efficiently computing the initial Max-Flow on static\ngraphs. We further describe a series of CUDA-specific implementation\noptimizations that enhance performance, scalability, and memory efficiency on\nGPU platforms.", "AI": {"tldr": "The paper introduces GPU-parallel algorithms for solving the Maximum Flow problem, including an efficient method for dynamic graphs and optimizations for CUDA implementations.", "motivation": "To efficiently solve the Max-Flow problem using GPUs, addressing scenarios with dynamic graph updates and improving computational performance.", "method": "Developed a novel GPU-parallel algorithm for dynamic graph updates and a high-performance static algorithm, with CUDA-specific optimizations.", "result": "Enhanced scalability, performance, and memory efficiency of Max-Flow computation on GPUs.", "conclusion": "The proposed algorithms and CUDA optimizations are effective for both static and dynamic Max-Flow problems on GPU platforms."}}
{"id": "2511.01529", "pdf": "https://arxiv.org/pdf/2511.01529", "abs": "https://arxiv.org/abs/2511.01529", "authors": ["Murali Sridharan", "Mikel Robredo", "Leevi Rantala", "Matteo Esposito", "Valentina Lenarduzzi", "Mika Mantyla"], "title": "Hidden in Plain Sight: Where Developers Confess Self-Admitted Technical Debt", "categories": ["cs.SE", "cs.CL", "cs.PL"], "comment": null, "summary": "Context. Detecting Self-Admitted Technical Debt (SATD) is crucial for\nproactive software maintenance. Previous research has primarily targeted\ndetecting and prioritizing SATD, with little focus on the source code afflicted\nwith SATD. Our goal in this work is to connect the SATD comments with source\ncode constructs that surround them.\n  Method. We leverage the extensive SATD dataset PENTACET, containing code\ncomments from over 9000 Java Open Source Software (OSS) repositories. We\nquantitatively infer where SATD most commonly occurs and which code\nconstructs/statements it most frequently affects.\n  Results and Conclusions. Our large-scale study links over 225,000 SATD\ncomments to their surrounding code, showing that SATD mainly arises in inline\ncode near definitions, conditionals, and exception handling, where developers\nface uncertainty and trade-offs, revealing it as an intentional signal of\nawareness during change rather than mere neglect.", "AI": {"tldr": "This paper presents a large-scale analysis connecting Self-Admitted Technical Debt (SATD) comments to their surrounding source code constructs using the PENTACET dataset.", "motivation": "To fill the research gap in understanding the connection between SATD comments and their affected source code, enabling proactive software maintenance.", "method": "Analyzed the PENTACET dataset with over 9000 Java OSS repositories, quantitatively identifying common occurrences of SATD in specific code constructs such as definitions, conditionals, and exception handling.", "result": "The study links 225,000 SATD comments to their surrounding code, showing that SATD primarily arises in inline code dealing with uncertainty and trade-offs.", "conclusion": "SATD is not just a sign of neglect but an intentional signal of awareness around code changes, helping address software maintenance challenges."}}
{"id": "2511.00315", "pdf": "https://arxiv.org/pdf/2511.00315", "abs": "https://arxiv.org/abs/2511.00315", "authors": ["Lee Xiong", "Maksim Tkachenko", "Johanes Effendi", "Ting Cai"], "title": "Language Modeling With Factorization Memory", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We propose Factorization Memory, an efficient recurrent neural network (RNN)\narchitecture that achieves performance comparable to Transformer models on\nshort-context language modeling tasks while also demonstrating superior\ngeneralization in long-context scenarios. Our model builds upon Mamba-2,\nenabling Factorization Memory to exploit parallel computations during training\nwhile preserving constant computational and memory complexity during inference.\nTo further optimize model efficiency and representational capacity, we develop\na sparse formulation of Factorization Memory that updates only a subset of\nrecurrent states at each step while preserving the strong performance of its\ndense counterpart. To our knowledge, this represents the first RNN architecture\nthat successfully combines sparse memory activation with competitive\nperformance across both short and long-context settings. This work provides a\nsystematic empirical analysis of Factorization Memory in comparison to\nTransformer and Mamba-2 architectures.", "AI": {"tldr": "The paper introduces Factorization Memory, an efficient RNN architecture with competitive performance in short and long-context tasks, combining sparse memory activation.", "motivation": "The paper addresses the need for efficient RNN architectures that compete with Transformer models in short-context tasks while also excelling in long-context generalization.", "method": "Factorization Memory builds on Mamba-2 with parallel training capabilities and constant computational/memory complexity during inference. A sparse update mechanism is introduced to enhance efficiency and representational capacity.", "result": "Factorization Memory achieves comparable performance to Transformers on short-context tasks and superior generalization in long-context scenarios. It combines sparse memory activation with competitive results.", "conclusion": "Factorization Memory represents a significant advancement in RNN architecture, offering a sparse activation mechanism and efficiency while excelling in diverse context scenarios."}}
{"id": "2511.00417", "pdf": "https://arxiv.org/pdf/2511.00417", "abs": "https://arxiv.org/abs/2511.00417", "authors": ["Marcel Valovy"], "title": "Human-AI Programming Role Optimization: Developing a Personality-Driven Self-Determination Framework", "categories": ["cs.SE", "cs.AI", "cs.HC", "H.5.3; D.2.9; I.2.0"], "comment": "PhD Dissertation, Prague University of Economics and Business, 2025.\n  323 pages. ACM CCS 2012: Human-computer interaction, Collaborative\n  interaction, Human-AI collaborative systems, Pair programming, AI-assisted\n  software engineering", "summary": "As artificial intelligence transforms software development, a critical\nquestion emerges: how can developers and AI systems collaborate most\neffectively? This dissertation optimizes human-AI programming roles through\nself-determination theory and personality psychology, introducing the Role\nOptimization Motivation Alignment (ROMA) framework.\n  Through Design Science Research spanning five cycles, this work establishes\nempirically-validated connections between personality traits, programming role\npreferences, and collaborative outcomes, engaging 200 experimental participants\nand 46 interview respondents.\n  Key findings demonstrate that personality-driven role optimization\nsignificantly enhances self-determination and team dynamics, yielding 23%\naverage motivation increases among professionals and up to 65% among\nundergraduates. Five distinct personality archetypes emerge: The Explorer (high\nOpenness/low Agreeableness), The Orchestrator (high\nExtraversion/Agreeableness), The Craftsperson (high Neuroticism/low\nExtraversion), The Architect (high Conscientiousness), and The Adapter\n(balanced profile). Each exhibits distinct preferences for programming roles\n(Co-Pilot, Co-Navigator, Agent), with assignment modes proving crucial for\nsatisfaction.\n  The dissertation contributes: (1) an empirically-validated framework linking\npersonality traits to role preferences and self-determination outcomes; (2) a\ntaxonomy of AI collaboration modalities mapped to personality profiles while\npreserving human agency; and (3) an ISO/IEC 29110 extension enabling Very Small\nEntities to implement personality-driven role optimization within established\nstandards.\n  Keywords: artificial intelligence, human-computer interaction, behavioral\nsoftware engineering, self-determination theory, personality psychology,\nphenomenology, intrinsic motivation, pair programming, design science research,\nISO/IEC 29110", "AI": {"tldr": "This dissertation introduces the Role Optimization Motivation Alignment (ROMA) framework to improve human-AI collaboration during programming through personality-driven role assignment.", "motivation": "The motivation behind this paper is optimizing collaboration between human developers and AI systems, leveraging personality psychology and self-determination theory.", "method": "The research employs Design Science Research across five cycles involving empirical studies with 200 experimental participants and 46 interviewees to connect personality traits with programming role preferences.", "result": "The findings highlight five personality archetypes with distinct role preferences, demonstrating increased motivation (23% for professionals, 65% for undergraduates) and improved team dynamics through role optimization.", "conclusion": "This dissertation contributes a framework linking personality traits to programming role preferences, collaboration modalities mapped to personality profiles, and an ISO/IEC 29110 extension for small entities to implement these insights."}}
{"id": "2511.00193", "pdf": "https://arxiv.org/pdf/2511.00193", "abs": "https://arxiv.org/abs/2511.00193", "authors": ["Faranak Akbarifar", "Nooshin Maghsoodi", "Sean P Dukelow", "Stephen Scott", "Parvin Mousavi"], "title": "Reducing Robotic Upper-Limb Assessment Time While Maintaining Precision: A Time Series Foundation Model Approach", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Purpose: Visually Guided Reaching (VGR) on the Kinarm robot yields sensitive\nkinematic biomarkers but requires 40-64 reaches, imposing time and fatigue\nburdens. We evaluate whether time-series foundation models can replace\nunrecorded trials from an early subset of reaches while preserving the\nreliability of standard Kinarm parameters.\n  Methods: We analyzed VGR speed signals from 461 stroke and 599 control\nparticipants across 4- and 8-target reaching protocols. We withheld all but the\nfirst 8 or 16 reaching trials and used ARIMA, MOMENT, and Chronos models,\nfine-tuned on 70 percent of subjects, to forecast synthetic trials. We\nrecomputed four kinematic features of reaching (reaction time, movement time,\nposture speed, maximum speed) on combined recorded plus forecasted trials and\ncompared them to full-length references using ICC(2,1).\n  Results: Chronos forecasts restored ICC >= 0.90 for all parameters with only\n8 recorded trials plus forecasts, matching the reliability of 24-28 recorded\nreaches (Delta ICC <= 0.07). MOMENT yielded intermediate gains, while ARIMA\nimprovements were minimal. Across cohorts and protocols, synthetic trials\nreplaced reaches without materially compromising feature reliability.\n  Conclusion: Foundation-model forecasting can greatly shorten Kinarm VGR\nassessment time. For the most impaired stroke survivors, sessions drop from 4-5\nminutes to about 1 minute while preserving kinematic precision. This\nforecast-augmented paradigm promises efficient robotic evaluations for\nassessing motor impairments following stroke.", "AI": {"tldr": "The study evaluates time-series models to reduce the trial count in Kinarm VGR assessments for stroke motor impairments while maintaining precision.", "motivation": "Kinarm robot's VGR assessments require a high number of trials (40-64), leading to excessive time and fatigue burdens, especially for impaired stroke patients.", "method": "Speed signals from 461 stroke and 599 control participants were analyzed. A subset of 8-16 trials was recorded, and models like ARIMA, MOMENT, and Chronos generated synthetic trials. Kinematic features were recomputed, and outcomes were measured using ICC(2,1).", "result": "Chronos model restored ICC >= 0.90 using only 8 recorded trials plus synthetic forecasts. MOMENT showed moderate improvements, while ARIMA showed minimal enhancement. Reliability was preserved across cohorts and protocols.", "conclusion": "The forecasting approach, particularly using the Chronos model, effectively reduces trial number and duration while maintaining the reliability of kinematic parameters for assessing stroke-related motor impairments."}}
{"id": "2511.00047", "pdf": "https://arxiv.org/pdf/2511.00047", "abs": "https://arxiv.org/abs/2511.00047", "authors": ["Omkar Kulkarni", "Rohitash Chandra"], "title": "DynBERG: Dynamic BERT-based Graph neural network for financial fraud detection", "categories": ["cs.LG", "cs.AI", "cs.CE"], "comment": null, "summary": "Financial fraud detection is critical for maintaining the integrity of\nfinancial systems, particularly in decentralised environments such as\ncryptocurrency networks. Although Graph Convolutional Networks (GCNs) are\nwidely used for financial fraud detection, graph Transformer models such as\nGraph-BERT are gaining prominence due to their Transformer-based architecture,\nwhich mitigates issues such as over-smoothing. Graph-BERT is designed for\nstatic graphs and primarily evaluated on citation networks with undirected\nedges. However, financial transaction networks are inherently dynamic, with\nevolving structures and directed edges representing the flow of money. To\naddress these challenges, we introduce DynBERG, a novel architecture that\nintegrates Graph-BERT with a Gated Recurrent Unit (GRU) layer to capture\ntemporal evolution over multiple time steps. Additionally, we modify the\nunderlying algorithm to support directed edges, making DynBERG well-suited for\ndynamic financial transaction analysis. We evaluate our model on the Elliptic\ndataset, which includes Bitcoin transactions, including all transactions during\na major cryptocurrency market event, the Dark Market Shutdown. By assessing\nDynBERG's resilience before and after this event, we analyse its ability to\nadapt to significant market shifts that impact transaction behaviours. Our\nmodel is benchmarked against state-of-the-art dynamic graph classification\napproaches, such as EvolveGCN and GCN, demonstrating superior performance,\noutperforming EvolveGCN before the market shutdown and surpassing GCN after the\nevent. Additionally, an ablation study highlights the critical role of\nincorporating a time-series deep learning component, showcasing the\neffectiveness of GRU in modelling the temporal dynamics of financial\ntransactions.", "AI": {"tldr": "DynBERG integrates Graph-BERT with GRU to analyze dynamic, directed financial transaction networks, outperforming benchmarks in fraud detection.", "motivation": "The paper aims to enhance financial fraud detection in dynamic and directed financial transaction networks, addressing limitations of static models like Graph-BERT.", "method": "DynBERG combines Graph-BERT with a GRU layer for temporal dynamics, accommodating directed edges. It was tested on the Elliptic Bitcoin transaction dataset.", "result": "DynBERG outperformed EvolveGCN before the Dark Market Shutdown and GCN afterward in fraud detection performance. The GRU layer proved critical in modelling temporal dynamics.", "conclusion": "DynBERG shows improved adaptability to dynamic financial networks and significant market shifts, demonstrating superior efficiency in fraud detection compared to leading models."}}
{"id": "2511.00073", "pdf": "https://arxiv.org/pdf/2511.00073", "abs": "https://arxiv.org/abs/2511.00073", "authors": ["Harald Kristen", "Daniel Kulmer", "Manuela Hirschmugl"], "title": "Habitat and Land Cover Change Detection in Alpine Protected Areas: A Comparison of AI Architectures", "categories": ["cs.CV"], "comment": null, "summary": "Rapid climate change and other disturbances in alpine ecosystems demand\nfrequent habitat monitoring, yet manual mapping remains prohibitively expensive\nfor the required temporal resolution. We employ deep learning for change\ndetection using long-term alpine habitat data from Gesaeuse National Park,\nAustria, addressing a major gap in applying geospatial foundation models (GFMs)\nto complex natural environments with fuzzy class boundaries and highly\nimbalanced classes. We compare two paradigms: post-classification change\ndetection (CD) versus direct CD. For post-classification CD, we evaluate GFMs\nPrithvi-EO-2.0 and Clay v1.0 against U-Net CNNs; for direct CD, we test the\ntransformer ChangeViT against U-Net baselines. Using high-resolution multimodal\ndata (RGB, NIR, LiDAR, terrain attributes) covering 4,480 documented changes\nover 15.3 km2, results show Clay v1.0 achieves 51% overall accuracy versus\nU-Net's 41% for multi-class habitat change, while both reach 67% for binary\nchange detection. Direct CD yields superior IoU (0.53 vs 0.35) for binary but\nonly 28% accuracy for multi-class detection. Cross-temporal evaluation reveals\nGFM robustness, with Clay maintaining 33% accuracy on 2020 data versus U-Net's\n23%. Integrating LiDAR improves semantic segmentation from 30% to 50% accuracy.\nAlthough overall accuracies are lower than in more homogeneous landscapes, they\nreflect realistic performance for complex alpine habitats. Future work will\nintegrate object-based post-processing and physical constraints to enhance\napplicability.", "AI": {"tldr": "This paper presents a deep learning-based method for detecting changes in alpine habitats using geospatial foundation models and multimodal data in Gesaeuse National Park, Austria.", "motivation": "Manual mapping of alpine ecosystems at a high temporal resolution is costly and ineffective for monitoring rapid climate change and disturbances.", "method": "The study applies deep learning models (Prithvi-EO-2.0, Clay v1.0) for post-classification change detection and ChangeViT transformer for direct change detection. It compares these approaches using high-resolution multimodal data (RGB, NIR, LiDAR, terrain attributes) for habitat change analysis.", "result": "Clay v1.0 outperformed U-Net in multi-class habitat change accuracy (51% vs. 41%) and binary change detection (67% both). Direct change detection with ChangeViT showed higher IoU performance for binary change (0.53 vs. 0.35), but lower accuracy (28%) for multi-class detection. LiDAR integration improved semantic segmentation accuracy from 30% to 50%.", "conclusion": "The results reflect realistic performance levels in complex alpine ecosystems, with geospatial foundation models demonstrating robustness. Future improvements involve object-based processing and physical constraints to enhance utility."}}
{"id": "2511.01140", "pdf": "https://arxiv.org/pdf/2511.01140", "abs": "https://arxiv.org/abs/2511.01140", "authors": ["Md Talha Mohsin", "Ismail Abdulrashid"], "title": "Few-Shot Multimodal Medical Imaging: A Theoretical Framework", "categories": ["stat.ML", "cs.AI", "cs.CV", "cs.LG", "eess.IV"], "comment": "6 Pages", "summary": "Medical imaging relies heavily on large, labeled datasets. But,\nunfortunately, they are not always easily accessible in clinical settings.\nAdditionally, many practitioners often face various structural obstacles like\nlimited data availability, fragmented data systems, and unbalanced datasets.\nThese barriers often lead to the increased diagnostic uncertainty,\nunderrepresentation of certain conditions, reduced model robustness, and biased\ndiagnostic decisions. In response to these challenges, approaches such as\ntransfer learning, meta-learning, and multimodal fusion have made great\nstrides. However, they still need a solid theoretical justification for why\nthey succeed or fail in situations where data is scarce. To address this gap,\nwe propose a unified theoretical framework that characterizes learning and\ninference under low-resource medical imaging conditions. We first formalize the\nlearning objective under few-shot conditions and compute sample complexity\nconstraints to estimate the smallest quantity of data needed to achieve\nclinically reliable accuracy. Then based on ideas from PAC-learning and\nPAC-Bayesian theory, we explain how multimodal integration encourages\ngeneralization and quantifies uncertainty under sparse supervision. We further\npropose a formal metric for explanation stability, offering interpretability\nguarantees under low-data conditions. Taken together, the proposed framework\nestablishes a principled foundation for constructing dependable, data-efficient\ndiagnostic systems by jointly characterizing sample efficiency, uncertainty\nquantification, and interpretability in a unified theoretical setting.", "AI": {"tldr": "Medical imaging often faces limited data availability and structural obstacles. This paper proposes a unified theoretical framework to address low-resource conditions, focusing on sample efficiency, uncertainty quantification, and interpretability.", "motivation": "Current medical imaging suffers from lack of accessible labeled datasets and issues like diagnostic uncertainty, biased decisions, and poor model robustness. Existing approaches lack theoretical justification in data-scarce conditions.", "method": "The authors formalize few-shot learning objectives, compute sample complexity constraints, apply PAC-learning and PAC-Bayesian theory for multimodal integration, and propose a formal metric for explanation stability.", "result": "The framework successfully quantifies uncertainty, supports multimodal generalization, and guarantees interpretability under low-data scenarios, providing a foundation for data-efficient diagnostics.", "conclusion": "This unified framework creates principled methodologies for reliable medical image diagnostics in low-resource settings, emphasizing efficiency, accuracy, and explainability."}}
{"id": "2511.00792", "pdf": "https://arxiv.org/pdf/2511.00792", "abs": "https://arxiv.org/abs/2511.00792", "authors": ["Akshay Sai Banderwaar", "Abhishek Gupta"], "title": "Fast PINN Eigensolvers via Biconvex Reformulation", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": "7 pages, 3 figures, Machine Learning and the Physical Sciences\n  Workshop NeurIPS 2025", "summary": "Eigenvalue problems have a distinctive forward-inverse structure and are\nfundamental to characterizing a system's thermal response, stability, and\nnatural modes. Physics-Informed Neural Networks (PINNs) offer a mesh-free\nalternative for solving such problems but are often orders of magnitude slower\nthan classical numerical schemes. In this paper, we introduce a reformulated\nPINN approach that casts the search for eigenpairs as a biconvex optimization\nproblem, enabling fast and provably convergent alternating convex search (ACS)\nover eigenvalues and eigenfunctions using analytically optimal updates.\nNumerical experiments show that PINN-ACS attains high accuracy with convergence\nspeeds up to 500$\\times$ faster than gradient-based PINN training. We release\nour codes at https://github.com/NeurIPS-ML4PS-2025/PINN_ACS_CODES.", "AI": {"tldr": "This paper introduces a reformulated PINN approach for eigenvalue problems, enabling faster convergence speeds via alternating convex search.", "motivation": "Eigenvalue problems are crucial for thermal responses, stability, and analyzing natural modes, yet existing PINN methods are inefficient.", "method": "The authors reformulate PINNs into a biconvex optimization problem with analytically optimal updates, using Alternating Convex Search (ACS).", "result": "This reformulation results in convergence speeds up to 500 times faster compared to traditional PINN training methods.", "conclusion": "The proposed PINN-ACS approach significantly improves PINN efficiency for eigenvalue problems, offering accelerated and reliable solutions."}}
{"id": "2511.00340", "pdf": "https://arxiv.org/pdf/2511.00340", "abs": "https://arxiv.org/abs/2511.00340", "authors": ["Manan Roy Choudhury", "Adithya Chandramouli", "Mannan Anand", "Vivek Gupta"], "title": "Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities", "categories": ["cs.AI"], "comment": "41 pages, 4 images", "summary": "The rapid integration of large language models (LLMs) into high-stakes legal\nwork has exposed a critical gap: no benchmark exists to systematically\nstress-test their reliability against the nuanced, adversarial, and often\nsubtle flaws present in real-world contracts. To address this, we introduce\nCLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an\nLLM's legal reasoning. We study the capabilities of LLMs to detect and reason\nabout fine-grained discrepancies by producing over 7500 real-world perturbed\ncontracts from foundational datasets like CUAD and ContractNLI. Our novel,\npersona-driven pipeline generates 10 distinct anomaly categories, which are\nthen validated against official statutes using a Retrieval-Augmented Generation\n(RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMs'\nability to detect embedded legal flaws and explain their significance. Our\nanalysis shows a key weakness: these models often miss subtle errors and\nstruggle even more to justify them legally. Our work outlines a path to\nidentify and correct such reasoning failures in legal AI.", "AI": {"tldr": "This paper introduces CLAUSE, a benchmark to test LLMs on detecting subtle flaws in legal contracts, revealing weaknesses in their reasoning capabilities.", "motivation": "The lack of benchmarks for assessing LLM reliability in handling nuanced legal reasoning and adversarial flaws in contracts.", "method": "Created CLAUSE benchmark with over 7500 perturbed contracts from datasets like CUAD and ContractNLI, using a persona-driven pipeline for anomaly generation validated by a RAG system.", "result": "Evaluated leading LLMs with CLAUSE, revealing their inability to detect certain errors and weaknesses in legal justification.", "conclusion": "Highlights reasoning failures in legal AI and proposes a pathway to improve their capabilities in complex legal contexts."}}
{"id": "2511.01255", "pdf": "https://arxiv.org/pdf/2511.01255", "abs": "https://arxiv.org/abs/2511.01255", "authors": ["He Chen", "ZiHua Zheng", "JingHua Sun"], "title": "Design of quasi phase matching crystal based on differential gray wolf algorithm", "categories": ["cs.DC"], "comment": null, "summary": "This paper focuses on the key problem in the development of nonlinear optical\ntechnology, the performance optimization of aperiodically polarized crystals.\nThe performance of the crystal depends on the precise control of the micro\ndistribution of crystal domains, but its optimization belongs to the\nhigh-dimensional discrete combination \"NP hard\" problem. The traditional\nalgorithm has the bottleneck of slow convergence and easy to fall into local\noptimization, while the heuristic methods such as genetic algorithm are limited\nby the CPU serial calculation and inefficient. In order to solve the above\nchallenges, this paper proposes the fusion scheme of hwsda hybrid optimization\nalgorithm and GPU parallel acceleration technology: the differential evolution\nalgorithm (DE) is used to realize the global search, and the gray wolf\noptimization algorithm (GWO) is used to strengthen the local search and\nconvergence speed, and the two coordinate to balance the global and local\noptimization requirements; At the same time, it relies on GPU multi-core\narchitecture to realize thread level parallel computing and improve\noptimization efficiency. This scheme effectively breaks through the\noptimization problem of high-dimensional discrete space, improves the accuracy\nof crystal domain control, improves the efficiency of quasi phase matching\ndesign by hundreds to thousands of times compared with traditional CPU serial\ncomputing, provides a new paradigm for the design of complex nonlinear optical\ndevices, and helps promote the performance breakthrough and industrial\napplication of related devices in the fields of quantum optics and laser\nprocessing.", "AI": {"tldr": "The paper addresses performance optimization of nonlinear optical crystals using a hybrid algorithm with GPU parallel acceleration, improving efficiency and accuracy significantly.", "motivation": "To overcome challenges in optimizing the performance of aperiodically polarized crystals, which involves solving an NP-hard problem and limitations of traditional and heuristic algorithms.", "method": "It proposes a hybrid optimization algorithm combining differential evolution (DE) for global search and gray wolf optimization (GWO) for local search, combined with GPU parallel acceleration.", "result": "The hybrid scheme enhances optimization efficiency by hundreds to thousands times compared to CPU serial computation, achieving improved crystal domain control accuracy and quasi-phase matching design.", "conclusion": "The approach advances the design of nonlinear optical devices and enables breakthroughs in performance and industrial applications in areas like quantum optics and laser processing."}}
{"id": "2511.01753", "pdf": "https://arxiv.org/pdf/2511.01753", "abs": "https://arxiv.org/abs/2511.01753", "authors": ["Zachary Hansen", "Yuliya Lierler"], "title": "SM-based Semantics for Answer Set Programs Containing Conditional Literals and Arithmetic", "categories": ["cs.LO", "cs.AI", "cs.PL", "F.4.1"], "comment": "This version corrects the review of tau for negated atoms, and\n  clarifies the distinction between global and local variables in conditional\n  literals (the supporting proofs are also updated accordingly)", "summary": "Modern answer set programming solvers such as CLINGO support advanced\nlanguage constructs that improve the expressivity and conciseness of logic\nprograms. Conditional literals are one such construct. They form \"subformulas\"\nthat behave as nested implications within the bodies of logic rules. Their\ninclusion brings the form of rules closer to the less restrictive syntax of\nfirst-order logic. These qualities make conditional literals useful tools for\nknowledge representation. In this paper, we propose a semantics for logic\nprograms with conditional literals and arithmetic based on the SM operator.\nThese semantics do not require grounding, unlike the established semantics for\nsuch programs that relies on a translation to infinitary propositional logic.\nThe main result of this paper establishes the precise correspondence between\nthe proposed and existing semantics.", "AI": {"tldr": "This paper proposes a semantics for logic programs with conditional literals and arithmetic that do not require grounding, improving expressivity and simplifying representations in answer set programming.", "motivation": "To address the limitations of current semantics which rely on grounding and a translation to infinitary propositional logic for programs with conditional literals.", "method": "The paper introduces a new semantics based on the SM operator, ensuring that logic programs with conditional literals operate effectively without grounding.", "result": "The proposed semantics is shown to correspond precisely with established semantics, demonstrating its validity and effectiveness.", "conclusion": "The research advances the expressivity and simplicity in answer set programming by offering an alternative semantics approach for conditional literals, enhancing knowledge representation."}}
{"id": "2511.00341", "pdf": "https://arxiv.org/pdf/2511.00341", "abs": "https://arxiv.org/abs/2511.00341", "authors": ["Mihir Sahasrabudhe"], "title": "Reversal Invariance in Autoregressive Language Models", "categories": ["cs.CL"], "comment": "7 pages, theoretical note", "summary": "We formalize a structural property of the causal (autoregressive) language\nmodeling (CLM) objective: reversal invariance. Formally, the next-token\nprediction loss assigns identical likelihood to a corpus and its reversal,\nimplying that standard CLM pretraining is direction-blind. This symmetry\nexplains why models trained on reversed text can achieve comparable performance\nto those trained on forward text, despite the inherently time-asymmetric nature\nof human language and reasoning. We argue that this invariance represents a\nlimitation of current pretraining objectives rather than a benign artifact. If\nnatural language encodes directional dependencies - phonological,\nmorphological, or causal - a symmetric objective may fail to capture them. We\ntherefore propose viewing pretraining through the lens of temporal asymmetry,\nmotivating future work on loss functions and architectures that explicitly\nmodel the arrow of language while retaining standard language modeling\ncapacity.", "AI": {"tldr": "This paper identifies a structural property of causal language modeling objectives called reversal invariance, and suggests its limitations in capturing natural language's directional dependencies.", "motivation": "The authors aim to address the inadequacy of current language model pretraining objectives, highlighting their inability to account for directional dependencies inherent in natural language.", "method": "The authors formalize and analyze the concept of reversal invariance within causal language modeling, exploring the symmetry in standard pretraining methods.", "result": "It is shown that models trained on reversed text perform comparably to those trained on forward text, demonstrating the direction-blind nature of current pretraining objectives.", "conclusion": "The paper argues for a shift in focus toward pretraining objectives and architectures that incorporate temporal asymmetry to better capture the directional nature of language."}}
{"id": "2511.00450", "pdf": "https://arxiv.org/pdf/2511.00450", "abs": "https://arxiv.org/abs/2511.00450", "authors": ["Vahid Etemadi", "Gregorio Robles"], "title": "SmartDoc: A Context-Aware Agentic Method Comment Generation Plugin", "categories": ["cs.SE"], "comment": "6 pages, Already submitted to The 3rd International Workshop on\n  Integrated Development Environments (the IDE Workshop)", "summary": "Context: The software maintenance phase involves many activities such as code\nrefactoring, bug fixing, code review or testing. Program comprehension is key\nto all these activities, as it demands developers to grasp the knowledge (e.g.,\nimplementation details) required to modify the codebase. Methods as main\nbuilding blocks in a program can offer developers this knowledge source for\ncode comprehension. However, reading entire method statements can be\nchallenging, which necessitates precise and up-to-date comments. Objective: We\npropose a solution as an IntelliJ IDEA plugin, named SmartDoc, that assists\ndevelopers in generating context-aware method comments. Method: This plugin\nacts as an Artificial Intelligence (AI) agent that has its own memory and is\naugmented by target methods' context. When a request is initiated by the\nend-user, the method content and all its nested method calls are used in the\ncomment generation. At the beginning, these nested methods are visited and a\ncall graph is generated. This graph is then traversed using depth-first search\n(DFS), enabling the provision of full-context to enrich Large Language Model\n(LLM) prompts. Result: The product is a software, as a plugin, developed for\nJava codebase and installable on IntelliJ IDEA. This plugin can serve\nconcurrently for methods whose comments are being updated , and it shares\nmemory across all flows to avoid redundant calls. o measure the accuracy of\nthis solution, a dedicated test case is run to record SmartDoc generated\ncomments and their corresponding ground truth. For each collected result-set,\nthree metrics are computed, BERTScore, BLEU and ROUGE-1. These metrics will\ndetermine how accurate the generated comments are in comparison to the ground\ntruth. Result: The obtained accuracy, in terms of the precision, recall and F1,\nis promising, and lies in the range of 0.80 to 0.90 for BERTScore.", "AI": {"tldr": "A plugin named SmartDoc for IntelliJ IDEA is proposed to generate precise, context-aware Java method comments autonomously, achieving accuracy metrics in the range of 0.80-0.90 for BERTScore.", "motivation": "To improve program comprehension during software maintenance by providing clear and contemporary comments for methods that help developers understand code implementation details effectively.", "method": "SmartDoc operates as an AI agent interacting with Large Language Models. It constructs a call graph using Depth-First Search (DFS) to retrieve context concerning the target method and uses this enriched context for generating method comments.", "result": "SmartDoc was tested on dedicated scenarios for Java codebases. Metrics of precision, recall, and F1 scores were measured using BERTScore, ROUGE-1, and BLEU and yielded accuracy between 0.80 and 0.90.", "conclusion": "SmartDoc demonstrates high accuracy in generating comments for Java methods, enriching program understanding and supporting software maintenance activities. Its high precision and recall indicate its effectiveness and reliability."}}
{"id": "2511.00259", "pdf": "https://arxiv.org/pdf/2511.00259", "abs": "https://arxiv.org/abs/2511.00259", "authors": ["Andria J. Farrens", "Luis Garcia-Fernandez", "Raymond Diaz Rojas", "Jillian Obeso Estrada", "Dylan Reinsdorf", "Vicky Chan", "Disha Gupta", "Joel Perry", "Eric Wolbrecht", "An Do", "Steven C. Cramer", "David J. Reinkensmeyer"], "title": "Tailored robotic training improves hand function and proprioceptive processing in stroke survivors with proprioceptive deficits: A randomized controlled trial", "categories": ["cs.RO", "cs.ET", "cs.HC"], "comment": "Main manuscript: 38 pages (double spaced, with references), 6\n  figures, 2 tables and collated supplemental materials (17 pages, double\n  spaced)", "summary": "Precision rehabilitation aims to tailor movement training to improve\noutcomes. We tested whether proprioceptively-tailored robotic training improves\nhand function and neural processing in stroke survivors. Using a robotic finger\nexoskeleton, we tested two proprioceptively-tailored approaches: Propriopixel\nTraining, which uses robot-facilitated, gamified movements to enhance\nproprioceptive processing, and Virtual Assistance Training, which reduces\nrobotic aid to increase reliance on self-generated feedback. In a randomized\ncontrolled trial, forty-six chronic stroke survivors completed nine 2-hour\nsessions of Standard, Propriopixel or Virtual training. Among participants with\nproprioceptive deficits, Propriopixel ((Box and Block Test: 7 +/- 4.2, p=0.002)\nand Virtual Assistance (4.5 +/- 4.4 , p=0.068) yielded greater gains in hand\nfunction (Standard: 0.8 +/- 2.3 blocks). Proprioceptive gains correlated with\nimprovements in hand function. Tailored training enhanced neural sensitivity to\nproprioceptive cues, evidenced by a novel EEG biomarker, the proprioceptive\nContingent Negative Variation. These findings support proprioceptively-tailored\ntraining as a pathway to precision neurorehabilitation.", "AI": {"tldr": "The study demonstrates that proprioceptively-tailored robotic training improves hand function and neural processing in stroke survivors, specifically benefiting those with proprioceptive deficits.", "motivation": "To determine whether personalized robotic rehabilitation training can enhance hand function and neural processing in chronic stroke survivors, especially for those with proprioceptive impairments.", "method": "The study conducted a randomized controlled trial with 46 chronic stroke survivors using a robotic finger exoskeleton for tailored proprioceptive training. Two approaches were tested: Propriopixel Training (enhancing proprioceptive processing via gamified movements) and Virtual Assistance Training (reducing robotic aid to promote self-reliance). Participants attended nine 2-hour sessions.", "result": "Participants with proprioceptive deficits showed significant improvements in hand function from Propriopixel training, with moderate improvement from Virtual Assistance. EEG biomarkers confirmed enhanced neural proprioceptive processing.", "conclusion": "Proprioceptively-tailored training improves hand function and proprioceptive processing, offering a promising method for precision neurorehabilitation targeting chronic stroke survivors with proprioceptive deficits."}}
{"id": "2511.00049", "pdf": "https://arxiv.org/pdf/2511.00049", "abs": "https://arxiv.org/abs/2511.00049", "authors": ["Yao Liu"], "title": "Adaptive Spatio-Temporal Graphs with Self-Supervised Pretraining for Multi-Horizon Weather Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate and robust weather forecasting remains a fundamental challenge due\nto the inherent spatio-temporal complexity of atmospheric systems. In this\npaper, we propose a novel self-supervised learning framework that leverages\nspatio-temporal structures to improve multi-variable weather prediction. The\nmodel integrates a graph neural network (GNN) for spatial reasoning, a\nself-supervised pretraining scheme for representation learning, and a\nspatio-temporal adaptation mechanism to enhance generalization across varying\nforecasting horizons. Extensive experiments on both ERA5 and MERRA-2 reanalysis\ndatasets demonstrate that our approach achieves superior performance compared\nto traditional numerical weather prediction (NWP) models and recent deep\nlearning methods. Quantitative evaluations and visual analyses in Beijing and\nShanghai confirm the model's capability to capture fine-grained meteorological\npatterns. The proposed framework provides a scalable and label-efficient\nsolution for future data-driven weather forecasting systems.", "AI": {"tldr": "This paper introduces a self-supervised learning framework utilizing graph neural networks and spatio-temporal adaptation to enhance multi-variable weather forecasting.", "motivation": "Accurate weather forecasting is challenging due to the complex spatio-temporal dynamics of atmospheric systems.", "method": "The framework integrates graph neural networks for spatial reasoning, self-supervised pretraining for representation learning, and spatio-temporal adaptation for better generalization.", "result": "Experiments on ERA5 and MERRA-2 datasets reveal that this model surpasses traditional weather prediction methods and recent deep learning techniques.", "conclusion": "This scalable, label-efficient solution is promising for improving data-driven weather forecasting systems."}}
{"id": "2511.00090", "pdf": "https://arxiv.org/pdf/2511.00090", "abs": "https://arxiv.org/abs/2511.00090", "authors": ["Huanlin Gao", "Ping Chen", "Fuyuan Shi", "Chao Tan", "Zhaoxiang Liu", "Fang Zhao", "Kai Wang", "Shiguo Lian"], "title": "LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": "NeurIPS 2025", "summary": "We present LeMiCa, a training-free and efficient acceleration framework for\ndiffusion-based video generation. While existing caching strategies primarily\nfocus on reducing local heuristic errors, they often overlook the accumulation\nof global errors, leading to noticeable content degradation between accelerated\nand original videos. To address this issue, we formulate cache scheduling as a\ndirected graph with error-weighted edges and introduce a Lexicographic Minimax\nPath Optimization strategy that explicitly bounds the worst-case path error.\nThis approach substantially improves the consistency of global content and\nstyle across generated frames. Extensive experiments on multiple text-to-video\nbenchmarks demonstrate that LeMiCa delivers dual improvements in both inference\nspeed and generation quality. Notably, our method achieves a 2.9x speedup on\nthe Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming\nprior caching techniques. Importantly, these gains come with minimal perceptual\nquality degradation, making LeMiCa a robust and generalizable paradigm for\naccelerating diffusion-based video generation. We believe this approach can\nserve as a strong foundation for future research on efficient and reliable\nvideo synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa", "AI": {"tldr": "LeMiCa is a framework for accelerating diffusion-based video generation, achieving both faster inference and higher quality outputs without substantial perceptual degradation.", "motivation": "Existing caching strategies in video generation often lead to global content degradation due to error accumulation, requiring a refined approach for better quality and consistency.", "method": "LeMiCa introduces Lexicographic Minimax Path Optimization to bound worst-case path error by modeling cache scheduling as a directed graph with error-weighted edges.", "result": "The framework achieves dual improvement with a 2.9x inference speedup on the Latte model and an LPIPS score of 0.05 on Open-Sora, outperforming earlier caching methods.", "conclusion": "LeMiCa provides a robust and generalizable strategy for enhancing diffusion-based video synthesis, ensuring minimal perceptual quality loss while enabling accelerated performance."}}
{"id": "2511.01196", "pdf": "https://arxiv.org/pdf/2511.01196", "abs": "https://arxiv.org/abs/2511.01196", "authors": ["Jicong Fan"], "title": "An Interdisciplinary and Cross-Task Review on Missing Data Imputation", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": null, "summary": "Missing data is a fundamental challenge in data science, significantly\nhindering analysis and decision-making across a wide range of disciplines,\nincluding healthcare, bioinformatics, social science, e-commerce, and\nindustrial monitoring. Despite decades of research and numerous imputation\nmethods, the literature remains fragmented across fields, creating a critical\nneed for a comprehensive synthesis that connects statistical foundations with\nmodern machine learning advances. This work systematically reviews core\nconcepts-including missingness mechanisms, single versus multiple imputation,\nand different imputation goals-and examines problem characteristics across\nvarious domains. It provides a thorough categorization of imputation methods,\nspanning classical techniques (e.g., regression, the EM algorithm) to modern\napproaches like low-rank and high-rank matrix completion, deep learning models\n(autoencoders, GANs, diffusion models, graph neural networks), and large\nlanguage models. Special attention is given to methods for complex data types,\nsuch as tensors, time series, streaming data, graph-structured data,\ncategorical data, and multimodal data. Beyond methodology, we investigate the\ncrucial integration of imputation with downstream tasks like classification,\nclustering, and anomaly detection, examining both sequential pipelines and\njoint optimization frameworks. The review also assesses theoretical guarantees,\nbenchmarking resources, and evaluation metrics. Finally, we identify critical\nchallenges and future directions, emphasizing model selection and\nhyperparameter optimization, the growing importance of privacy-preserving\nimputation via federated learning, and the pursuit of generalizable models that\ncan adapt across domains and data types, thereby outlining a roadmap for future\nresearch.", "AI": {"tldr": "The paper reviews missing data challenges in data science and synthesizes classical and modern imputation methods, focusing on integrations with downstream tasks and future research directions.", "motivation": "The motivation is to address the fragmentation in missing data literature and provide a comprehensive synthesis that bridges statistical foundations with advances in machine learning.", "method": "The authors reviewed concepts like missingness mechanisms and categorized imputation methods, spanning classical techniques to modern approaches like deep learning and large language models.", "result": "The paper systematically categorizes methodologies, examines domain-specific challenges, and highlights theoretical guarantees, benchmarking, and evaluation metrics.", "conclusion": "The work emphasizes critical challenges such as model selection, privacy-preserving imputation, and generalizable models, offering a roadmap for future research in this domain."}}
{"id": "2511.01254", "pdf": "https://arxiv.org/pdf/2511.01254", "abs": "https://arxiv.org/abs/2511.01254", "authors": ["Huseyin Goksu"], "title": "Hi-WaveTST: A Hybrid High-Frequency Wavelet-Transformer for Time-Series Classification", "categories": ["eess.SP", "cs.NE"], "comment": null, "summary": "Transformers have become state-of-the-art (SOTA) for time-series\nclassification, with models like PatchTST demonstrating exceptional\nperformance. These models rely on patching the time series and learning\nrelationships between raw temporal data blocks. We argue that this approach is\nblind to critical, non-obvious high-frequency information that is complementary\nto the temporal dynamics. In this letter, we propose Hi-WaveTST, a novel Hybrid\narchitecture that augments the original temporal patch with a learnable,\nHigh-Frequency wavelet feature stream. Our wavelet stream uses a deep Wavelet\nPacket Decomposition (WPD) on each patch and extracts features using a\nlearnable Generalized Mean (GeM) pooling layer. On the UCI-HAR benchmark\ndataset, our hybrid model achieves a mean accuracy of 93.38 percent plus-minus\n0.0043, significantly outperforming the SOTA PatchTST baseline (92.59 percent\nplus-minus 0.0039). A comprehensive ablation study proves that every component\nof our design-the hybrid architecture, the deep high-frequency wavelet\ndecomposition, and the learnable GeM pooling-is essential for this\nstate-of-the-art performance.", "AI": {"tldr": "The paper introduces Hi-WaveTST, a novel hybrid time-series classifier that improves upon PatchTST by integrating a new high-frequency feature stream.", "motivation": "The motivation is to address a gap in current SOTA time-series classifiers, like PatchTST, which overlook non-obvious high-frequency information crucial for improving classification.", "method": "The proposed method, Hi-WaveTST, combines temporal patch information with features derived from a learnable, high-frequency wavelet feature stream produced by deep Wavelet Packet Decomposition and a Generalized Mean (GeM) pooling layer.", "result": "Hi-WaveTST achieved 93.38% accuracy on the UCI-HAR benchmark, outperforming the 92.59% accuracy of the SOTA PatchTST model.", "conclusion": "The findings suggest that augmenting temporal dynamics with high-frequency wavelet features provides significant performance gains for time-series classification, and every component in Hi-WaveTST's architecture is deemed essential."}}
{"id": "2511.00379", "pdf": "https://arxiv.org/pdf/2511.00379", "abs": "https://arxiv.org/abs/2511.00379", "authors": ["Jiahao Wang", "Songkai Xue", "Jinghui Li", "Xiaozhen Wang"], "title": "Diverse Human Value Alignment for Large Language Models via Ethical Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted by AIES 2025, camera-ready version", "summary": "Ensuring that Large Language Models (LLMs) align with the diverse and\nevolving human values across different regions and cultures remains a critical\nchallenge in AI ethics. Current alignment approaches often yield superficial\nconformity rather than genuine ethical understanding, failing to address the\ncomplex, context-dependent nature of human values. In this paper, we propose a\nnovel ethical reasoning paradigm for LLMs inspired by well-established ethical\ndecision-making models, aiming at enhancing diverse human value alignment\nthrough deliberative ethical reasoning. Our framework consists of a structured\nfive-step process, including contextual fact gathering, hierarchical social\nnorm identification, option generation, multiple-lens ethical impact analysis,\nand reflection. This theory-grounded approach guides LLMs through an\ninterpretable reasoning process that enhances their ability to understand\nregional specificities and perform nuanced ethical analysis, which can be\nimplemented with either prompt engineering or supervised fine-tuning methods.\nWe perform evaluations on the SafeWorld benchmark that specially designed for\nregional value alignment. Experimental results demonstrate our framework\nsignificantly improves LLM alignment with diverse human values compared to\nbaseline methods, enabling more accurate social norm identification and more\nculturally appropriate reasoning. Our work provides a concrete pathway toward\ndeveloping LLMs that align more effectively with the multifaceted values of\nglobal societies through interdisciplinary research.", "AI": {"tldr": "The paper addresses aligning LLMs with diverse human values using a five-step structured ethical reasoning framework, demonstrating improved performance in cultural and regional understanding.", "motivation": "The motivation lies in overcoming the superficial alignment of LLMs with human values, and addressing the challenge of understanding diverse and context-dependent ethical considerations globally.", "method": "The paper proposes a five-step framework for ethical reasoning, including contextual fact gathering, identifying social norms, generating options, conducting ethical impact analysis, and reflecting. This can be implemented via prompt engineering or supervised fine-tuning.", "result": "The proposed framework shows significant improvements in aligning LLMs with diverse human values, particularly in regional and cultural understanding, as evidenced by evaluations on the SafeWorld benchmark.", "conclusion": "The study points toward a structured methodology for LLMs to achieve nuanced alignment with multifaceted human values, advancing global AI ethics through interdisciplinary efforts."}}
{"id": "2511.01333", "pdf": "https://arxiv.org/pdf/2511.01333", "abs": "https://arxiv.org/abs/2511.01333", "authors": ["Muhammad Ahmed Mohsin", "Muhammad Umer", "Ahsan Bilal", "Hassan Rizwan", "Sagnik Bhattacharya", "Muhammad Ali Jamshed", "John M. Cioffi"], "title": "Transformer-Based Sparse CSI Estimation for Non-Stationary Channels", "categories": ["cs.DC"], "comment": "ICC 2026", "summary": "Accurate and efficient estimation of Channel State Information (CSI) is\ncritical for next-generation wireless systems operating under non-stationary\nconditions, where user mobility, Doppler spread, and multipath dynamics rapidly\nalter channel statistics. Conventional pilot aided estimators incur substantial\noverhead, while deep learning approaches degrade under dynamic pilot patterns\nand time varying fading. This paper presents a pilot-aided Flash-Attention\nTransformer framework that unifies model-driven pilot acquisition with data\ndriven CSI reconstruction through patch-wise self-attention and a physics aware\ncomposite loss function enforcing phase alignment, correlation consistency, and\ntime frequency smoothness. Under a standardized 3GPP NR configuration, the\nproposed framework outperforms LMMSE and LSTM baselines by approximately 13 dB\nin phase invariant normalized mean-square error (NMSE) with markedly lower\nbit-error rate (BER), while reducing pilot overhead by 16 times. These results\ndemonstrate that attention based architectures enable reliable CSI recovery and\nenhanced spectral efficiency without compromising link quality, addressing a\nfundamental bottleneck in adaptive, low-overhead channel estimation for\nnon-stationary 5G and beyond-5G networks.", "AI": {"tldr": "This paper introduces a pilot-aided Flash-Attention Transformer framework to effectively estimate Channel State Information (CSI) in dynamic wireless conditions, achieving superior performance and reduced overhead compared to traditional methods.", "motivation": "The motivation is to address the challenges in accurately estimating CSI in non-stationary wireless environments, which involve rapid changes in channel statistics due to mobility, Doppler spread, and multipath effects.", "method": "The paper proposes a Flash-Attention Transformer framework integrating model-driven pilot acquisition with data-driven CSI reconstruction using patch-wise self-attention and a physics-aware loss function.", "result": "The framework shows approximately 13 dB improvement in phase-invariant NMSE and significantly lower BER compared to baseline methods, while reducing pilot overhead by 16 times under a 3GPP NR configuration.", "conclusion": "Attention-based architectures like the proposed framework can reliably recover CSI, enhance spectral efficiency, and minimize estimation overhead, resolving key challenges in adaptive low-overhead channel estimation for next-gen wireless systems."}}
{"id": "2511.00343", "pdf": "https://arxiv.org/pdf/2511.00343", "abs": "https://arxiv.org/abs/2511.00343", "authors": ["Changbing Yang", "Franklin Ma", "Freda Shi", "Jian Zhu"], "title": "LingGym: How Far Are LLMs from Thinking Like Field Linguists?", "categories": ["cs.CL"], "comment": "EMNLP 2025 Main", "summary": "This paper introduces LingGym, a new benchmark that evaluates LLMs' capacity\nfor meta-linguistic reasoning using Interlinear Glossed Text (IGT) and\ngrammatical descriptions extracted from 18 typologically diverse reference\ngrammars. Unlike previous work that focuses on specific downstream tasks, we\nassess whether LLMs can generalize linguistic inference across low-resource\nlanguages and structures not seen during training. We present a controlled\nevaluation task: Word-Gloss Inference, in which the model must infer a missing\nword and gloss from context using varying levels of linguistic information\n(e.g., glosses, grammatical explanations, translations). Our results show that\nincorporating structured linguistic cues leads to consistent improvements in\nreasoning performance across all models. This work highlights both the promise\nand current limitations of using LLMs for typologically informed linguistic\nanalysis and low-resource language documentation.", "AI": {"tldr": "The paper presents LingGym, a benchmark for testing LLMs' metalinguistic reasoning across diverse low-resource languages using Interlinear Glossed Text and grammatical descriptions.", "motivation": "To evaluate the ability of LLMs to perform linguistic inference in typologically diverse and low-resource languages beyond specific downstream tasks.", "method": "Proposed a controlled evaluation task, Word-Gloss Inference, requiring LLMs to infer missing words and glosses using structured linguistic information such as glosses and grammatical explanations.", "result": "Structured linguistic information enhances reasoning performance across all tested LLMs. Results showcase both potential and current shortcomings of LLMs in linguistic analysis.", "conclusion": "The study demonstrates structured linguistic cues improve LLM performance for low-resource language analysis, but limitations remain, suggesting further development is needed for typologically informed linguistic analysis."}}
{"id": "2511.00467", "pdf": "https://arxiv.org/pdf/2511.00467", "abs": "https://arxiv.org/abs/2511.00467", "authors": ["Liu Wang", "Dong Wang", "Shidong Pan", "Zheng Jiang", "Haoyu Wang", "Yi Wang"], "title": "A Big Step Forward? A User-Centric Examination of iOS App Privacy Report and Enhancements", "categories": ["cs.SE"], "comment": "Accepted to S&P 2025", "summary": "The prevalent engagement with mobile apps underscores the importance of\nunderstanding their data practices. Transparency plays a crucial role in this\ncontext, ensuring users to be informed and give consent before any data access\noccurs. Apple introduced a new feature since iOS 15.2, App Privacy Report, to\ninform users about detailed insights into apps' data access and sharing. This\nfeature continues Apple's trend of privacy-focused innovations (following\nPrivacy Nutrition Labels), and has been marketed as a big step forward in user\nprivacy. However, its real-world impacts on user privacy and control remain\nunexamined. We thus proposed an end-to-end study involving systematic\nassessment of the App Privacy Report's real-world benefits and limitations,\nLLM-enabled and multi-technique synthesized enhancements, and comprehensive\nevaluation from both system and user perspectives. Through a structured focus\ngroup study with twelve everyday iOS users, we explored their experiences,\nunderstanding, and perceptions of the feature, suggesting its limited practical\nimpact resulting from missing important details. We identified two primary user\nconcerns: the clarity of data access purpose and domain description. In\nresponse, we proposed enhancements including a purpose inference framework and\ndomain clarification pipeline. We demonstrated the effectiveness and benefits\nof such enhancements for mobile app users. This work provides practical\ninsights that could help enhance user privacy transparency and discusses areas\nfor future research.", "AI": {"tldr": "The paper evaluates Apple's App Privacy Report feature in iOS 15.2, revealing its limitations and proposing enhancements to improve user privacy transparency.", "motivation": "The introduction of App Privacy Report aimed to improve user awareness regarding mobile apps' data practices, but its real-world impact and effectiveness needed deeper examination.", "method": "The authors conducted an end-to-end study combining systematic assessments of App Privacy Report, enhancements using LLMs and other techniques, and user-focused evaluations through structured focus group studies.", "result": "The study found that App Privacy Report has limited practical impact due to missing details on data access purposes and domain descriptions. Proposed enhancements addressed these gaps effectively.", "conclusion": "The paper provides actionable enhancements and insights on improving privacy transparency, contributing to ongoing discussions and developments in user data protection."}}
{"id": "2511.00306", "pdf": "https://arxiv.org/pdf/2511.00306", "abs": "https://arxiv.org/abs/2511.00306", "authors": ["Baoshan Song", "Ruijie Xu", "Li-Ta Hsu"], "title": "FGO MythBusters: Explaining how Kalman Filter variants achieve the same performance as FGO in navigation applications", "categories": ["cs.RO"], "comment": null, "summary": "Sliding window-factor graph optimization (SW-FGO) has gained more and more\nattention in navigation research due to its robust approximation to\nnon-Gaussian noises and nonlinearity of measuring models. There are lots of\nworks focusing on its application performance compared to extended Kalman\nfilter (EKF) but there is still a myth at the theoretical relationship between\nthe SW-FGO and EKF. In this paper, we find the necessarily fair condition to\nconnect SW-FGO and Kalman filter variants (KFV) (e.g., EKF, iterative EKF\n(IEKF), robust EKF (REKF) and robust iterative EKF (RIEKF)). Based on the\nconditions, we propose a recursive FGO (Re-FGO) framework to represent KFV\nunder SW-FGO formulation. Under explicit conditions (Markov assumption,\nGaussian noise with L2 loss, and a one-state window), Re-FGO regenerates\nexactly to EKF/IEKF/REKF/RIEKF, while SW-FGO shows measurable benefits in\nnonlinear, non-Gaussian regimes at a predictable compute cost. Finally, after\nclarifying the connection between them, we highlight the unique advantages of\nSW-FGO in practical phases, especially on numerical estimation and deep\nlearning integration. The code and data used in this work is open sourced at\nhttps://github.com/Baoshan-Song/KFV-FGO-Comparison.", "AI": {"tldr": "This paper explores the theoretical connection between Sliding Window-Factor Graph Optimization (SW-FGO) and Kalman filter variants, proposing a recursive FGO framework to unify them under certain conditions.", "motivation": "To clarify the theoretical relationship between SW-FGO and Kalman filter variants (KFV), which has remained unclear despite SW-FGO's advantages over EKF in navigation research.", "method": "The authors identified conditions to link SW-FGO with KFV methods and proposed a Recursive FGO (Re-FGO) framework that aligns SW-FGO with EKF, IEKF, REKF, and RIEKF under explicit assumptions (e.g., Gaussian noise, Markov assumption, one-state window).", "result": "Under specific conditions, Re-FGO regenerates KFV methods while SW-FGO demonstrates improvements in handling nonlinear and non-Gaussian situations at predictable computational costs.", "conclusion": "This study establishes a theoretical link between SW-FGO and KFV, highlights SW-FGO's advantages in numerical estimation and compatibility with deep learning methods, and provides open source code and data for further exploration."}}
{"id": "2511.00050", "pdf": "https://arxiv.org/pdf/2511.00050", "abs": "https://arxiv.org/abs/2511.00050", "authors": ["Dhananjaya Gowda", "Seoha Song", "Junhyun Lee", "Harshith Goka"], "title": "FLoRA: Fused forward-backward adapters for parameter efficient fine-tuning and reducing inference-time latencies of LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As the large language models (LLMs) grow in size each day, efficient training\nand fine-tuning has never been as important as nowadays. This resulted in the\ngreat interest in parameter efficient fine-tuning (PEFT), and effective methods\nincluding low-rank adapters (LoRA) has emerged. Although the various PEFT\nmethods have been studied extensively in the recent years, the greater part of\nthe subject remains unexplored with the huge degree of freedom. In this paper,\nwe propose FLoRA, a family of fused forward-backward adapters (FFBA) for\nparameter-efficient fine-tuning of LLMs on downstream tasks. The FFBA combine\nideas from the popular LoRA and parallel adapters to improve the overall\nfine-tuning accuracies. At the same time, latencies are minimized by fusing the\nforward and backward adapters into existing projection layers of the base\nmodel. Experimental results show that the proposed FFB adapters perform\nsignificantly better than the popularly used LoRA in both accuracy and latency\nfor a similar parameter budget.", "AI": {"tldr": "The paper introduces FLoRA, which improves LLM fine-tuning efficiency by fusing forward and backward adapters, gaining better accuracy and latency than LoRA.", "motivation": "The growing size of LLMs has increased the demand for parameter-efficient fine-tuning methods. Despite advances like LoRA, much remains unexplored, presenting an opportunity for innovation.", "method": "The authors propose FLoRA, a method incorporating fused forward-backward adapters (FFBA). Combining ideas from LoRA and parallel adapters, FFBA is integrated into the base model\u2019s projection layers to enhance fine-tuning.", "result": "Experimental results show FLoRA outperforms LoRA in both accuracy and latency while using comparable parameter resources.", "conclusion": "FLoRA demonstrates improved parameter-efficient fine-tuning for large language models, addressing both performance and latency challenges systematically."}}
{"id": "2511.00091", "pdf": "https://arxiv.org/pdf/2511.00091", "abs": "https://arxiv.org/abs/2511.00091", "authors": ["Wenli Xiao", "Haotian Lin", "Andy Peng", "Haoru Xue", "Tairan He", "Yuqi Xie", "Fengyuan Hu", "Jimmy Wu", "Zhengyi Luo", "Linxi \"Jim\" Fan", "Guanya Shi", "Yuke Zhu"], "title": "Self-Improving Vision-Language-Action Models with Data Generation via Residual RL", "categories": ["cs.CV", "cs.RO"], "comment": "26 pages", "summary": "Supervised fine-tuning (SFT) has become the de facto post-training strategy\nfor large vision-language-action (VLA) models, but its reliance on costly human\ndemonstrations limits scalability and generalization. We propose Probe, Learn,\nDistill (PLD), a three-stage plug-and-play framework that improves VLAs through\nresidual reinforcement learning (RL) and distribution-aware data collection. In\nStage 1, we train lightweight residual actors to probe failure regions of the\nVLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns\ncollected trajectories with the generalist's deployment distribution while\ncapturing recovery behaviors. In Stage 3, we distill the curated trajectories\nback into the generalist with standard SFT. PLD achieves near-saturated 99%\ntask success on LIBERO, over 50% gains in SimplerEnv, and 100% success on\nreal-world Franka and YAM arm manipulation tasks. Ablations show that residual\nprobing and distribution-aware replay are key to collecting deployment-aligned\ndata that improves both seen and unseen tasks, offering a scalable path toward\nself-improving VLA models.", "AI": {"tldr": "The paper introduces PLD, an innovative framework for improving large vision-language-action (VLA) models through reinforcement learning and optimized data collection, achieving impressive results across diverse tasks.", "motivation": "The motivation is to address limitations of supervised fine-tuning (SFT) in VLA models, especially its dependency on expensive human demonstrations, to enhance scalability and generalization.", "method": "The method involves a three-stage framework: (1) training residual actors to identify failure regions in VLA models, (2) using hybrid rollout schemes to gather deployment-aligned and recovery-focused data, and (3) distilling this data back into the generalist using SFT.", "result": "Using the PLD framework, the authors achieved near-perfect success rates across various benchmarks such as LIBERO, SimplerEnv, and real-world robot manipulation tasks while enhancing the model's performance in unfamiliar scenarios.", "conclusion": "The study concludes that residual probing and distribution-aware data collection are crucial for scalable, self-improving VLA models, demonstrating significant improvements in robustness and generalization."}}
{"id": "2511.01292", "pdf": "https://arxiv.org/pdf/2511.01292", "abs": "https://arxiv.org/abs/2511.01292", "authors": ["Samet Demir", "Zafer Dogan"], "title": "Optimal Attention Temperature Enhances In-Context Learning under Distribution Shift", "categories": ["stat.ML", "cs.LG"], "comment": "26 pages, 6 figures", "summary": "Pretrained Transformers excel at in-context learning (ICL), inferring new\ntasks from only a handful of examples. Yet, their ICL performance can degrade\nsharply under distribution shift between pretraining and test data, a regime\nincreasingly common in real-world deployments. While recent empirical work\nhints that adjusting the attention temperature in the softmax can enhance\nTransformer performance, the attention temperature's role in ICL under\ndistribution shift remains unexplored. This paper provides the first\ntheoretical and empirical study of attention temperature for ICL under\ndistribution shift. Using a simplified but expressive \"linearized softmax\"\nframework, we derive closed-form generalization error expressions and prove\nthat shifts in input covariance or label noise substantially impair ICL, but\nthat an optimal attention temperature exists which minimizes this error. We\nthen validate our predictions through extensive simulations on linear\nregression tasks and large-scale experiments with GPT-2 and LLaMA2-7B on\nquestion-answering benchmarks. Our results establish attention temperature as a\nprincipled and powerful mechanism for improving the robustness of ICL in\npretrained Transformers, advancing theoretical understanding and providing\nactionable guidance for selecting attention temperature in practice.", "AI": {"tldr": "This paper explores the role of attention temperature in improving in-context learning (ICL) performance of pretrained transformers under distribution shifts, offering theoretical insights, simulations, and experimental validation.", "motivation": "Pretrained transformers struggle with ICL under distribution shift scenarios common in real-world deployments.", "method": "Utilize linearized softmax framework to derive generalization error expressions, identify optimal attention temperature, and validate through simulations and large-scale experiments.", "result": "Proven that optimal attention temperature mitigates errors caused by input covariance and label noise shifts, confirmed by experiments on GPT-2 and LLaMA2-7B.", "conclusion": "Attention temperature is a robust mechanism to enhance ICL performance in pretrained transformers under distribution shift, supported by theoretical foundations and experimental evidence."}}
{"id": "2511.01553", "pdf": "https://arxiv.org/pdf/2511.01553", "abs": "https://arxiv.org/abs/2511.01553", "authors": ["Elvin Hajizada", "Danielle Rager", "Timothy Shea", "Leobardo Campos-Macias", "Andreas Wild", "Eyke H\u00fcllermeier", "Yulia Sandamirskaya", "Mike Davies"], "title": "Real-time Continual Learning on Intel Loihi 2", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.NE"], "comment": null, "summary": "AI systems on edge devices face a critical challenge in open-world\nenvironments: adapting when data distributions shift and novel classes emerge.\nWhile offline training dominates current paradigms, online continual learning\n(OCL)--where models learn incrementally from non-stationary streams without\ncatastrophic forgetting--remains challenging in power-constrained settings. We\npresent a neuromorphic solution called CLP-SNN: a spiking neural network\narchitecture for Continually Learning Prototypes and its implementation on\nIntel's Loihi 2 chip. Our approach introduces three innovations: (1)\nevent-driven and spatiotemporally sparse local learning, (2) a self-normalizing\nthree-factor learning rule maintaining weight normalization, and (3) integrated\nneurogenesis and metaplasticity for capacity expansion and forgetting\nmitigation. On OpenLORIS few-shot learning experiments, CLP-SNN achieves\naccuracy competitive with replay methods while being rehearsal-free. CLP-SNN\ndelivers transformative efficiency gains: 70\\times faster (0.33ms vs 23.2ms),\nand 5,600\\times more energy efficient (0.05mJ vs 281mJ) than the best\nalternative OCL on edge GPU. This demonstrates that co-designed brain-inspired\nalgorithms and neuromorphic hardware can break traditional accuracy-efficiency\ntrade-offs for future edge AI systems.", "AI": {"tldr": "This paper introduces CLP-SNN, a spiking neural network for continual learning on Intel\u2019s Loihi 2, achieving competitive accuracy with significant efficiency gains.", "motivation": "Improve AI system adaptability on edge devices in open-world environments, where data distributions shift and new classes emerge, without catastrophic forgetting.", "method": "Presented a neuromorphic solution called CLP-SNN using spiking neural networks with event-driven sparse learning, a self-normalizing learning rule, and integrated neurogenesis and metaplasticity.", "result": "CLP-SNN achieves competitive accuracy in OpenLORIS tests, is 70x faster, and 5,600x more energy efficient than GPU-based OCL alternatives.", "conclusion": "Brain-inspired algorithms with neuromorphic hardware can significantly enhance efficiency and accuracy, redefining future edge AI capabilities."}}
{"id": "2511.00382", "pdf": "https://arxiv.org/pdf/2511.00382", "abs": "https://arxiv.org/abs/2511.00382", "authors": ["Mina Taraghi", "Yann Pequignot", "Amin Nikanjam", "Mohamed Amine Merzouk", "Foutse Khomh"], "title": "Efficiency vs. Alignment: Investigating Safety and Fairness Risks in Parameter-Efficient Fine-Tuning of LLMs", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Organizations are increasingly adopting and adapting Large Language Models\n(LLMs) hosted on public repositories such as HuggingFace. Although these\nadaptations often improve performance on specialized downstream tasks, recent\nevidence indicates that they can also degrade a model's safety or fairness.\nSince different fine-tuning techniques may exert distinct effects on these\ncritical dimensions, this study undertakes a systematic assessment of their\ntrade-offs. Four widely used Parameter-Efficient Fine-Tuning methods, LoRA,\nIA3, Prompt-Tuning, and P-Tuning, are applied to four instruction-tuned model\nfamilies (Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, and Gemma-7B). In total, 235\nfine-tuned variants are evaluated across eleven safety hazard categories and\nnine demographic fairness dimensions. The results show that adapter-based\napproaches (LoRA, IA3) tend to improve safety scores and are the least\ndisruptive to fairness, retaining higher accuracy and lower bias scores. In\ncontrast, prompt-based methods (Prompt-Tuning and P-Tuning) generally reduce\nsafety and cause larger fairness regressions, with decreased accuracy and\nincreased bias. Alignment shifts are strongly moderated by base model type:\nLLaMA remains stable, Qwen records modest gains, Gemma experiences the steepest\nsafety decline, and Mistral, which is released without an internal moderation\nlayer, displays the greatest variance. Improvements in safety do not\nnecessarily translate into improvements in fairness, and no single\nconfiguration optimizes all fairness metrics simultaneously, indicating an\ninherent trade-off between these objectives. These findings suggest a practical\nguideline for safety-critical deployments: begin with a well-aligned base\nmodel, favour adapter-based PEFT, and conduct category-specific audits of both\nsafety and fairness.", "AI": {"tldr": "The study examines the safety and fairness trade-offs of different fine-tuning techniques on Large Language Models (LLMs), recommending adapter-based approaches for safety-critical applications.", "motivation": "To investigate how fine-tuning techniques impact safety and fairness in Large Language Models, addressing the reported issues of performance degradation on these dimensions.", "method": "Four Parameter-Efficient Fine-Tuning methods (LoRA, IA3, Prompt-Tuning, P-Tuning) were applied to four instruction-tuned LLM families, evaluating 235 variants across safety and fairness metrics.", "result": "Adapter-based methods (LoRA, IA3) improved safety and retained fairness, while prompt-based methods reduced safety and exacerbated fairness issues. Base model type influenced outcomes significantly.", "conclusion": "Safety and fairness trade-offs exist across fine-tuning methods, with adapter-based approaches preferred for aligned base models. Comprehensive audits are necessary for safety-critical deployments."}}
{"id": "2511.01420", "pdf": "https://arxiv.org/pdf/2511.01420", "abs": "https://arxiv.org/abs/2511.01420", "authors": ["Christoph Lenzen"], "title": "Gradient Clock Synchronization with Practically Constant Local Skew", "categories": ["cs.DC"], "comment": "38 pages, no figures, submitted to STOC 2026", "summary": "Gradient Clock Synchronization (GCS) is the task of minimizing the local\nskew, i.e., the clock offset between neighboring clocks, in a larger network.\nWhile asymptotically optimal bounds are known, from a practical perspective\nthey have crucial shortcomings:\n  - Local skew bounds are determined by upper bounds on offset estimation that\nneed to be guaranteed throughout the entire lifetime of the system.\n  - Worst-case frequency deviations of local oscillators from their nominal\nrate are assumed, yet frequencies tend to be much more stable in the (relevant)\nshort term.\n  State-of-the-art deployed synchronization methods adapt to the true offset\nmeasurement and frequency errors, but achieve no non-trivial guarantees on the\nlocal skew.\n  In this work, we provide a refined model and novel analysis of existing\ntechniques for solving GCS in this model. By requiring only stability of\nmeasurement and frequency errors, we can circumvent existing lower bounds,\nleading to dramatic improvements under very general conditions. For example, if\nlinks exhibit a uniform worst-case estimation error of $\\Delta$ and a change in\nestimation errors of $\\delta\\ll \\Delta$ on relevant time scales, we bound the\nlocal skew by $O(\\Delta+\\delta \\log D)$ for networks of diameter $D$,\neffectively ``breaking'' the established $\\Omega(\\Delta\\log D)$ lower bound,\nwhich holds when $\\delta=\\Delta$. Similarly, we show how to limit the influence\nof local oscillators on $\\delta$ to scale with the change of frequency of an\nindividual oscillator on relevant time scales, rather than a worst-case bound\nover all oscillators and the lifetime of the system.\n  Moreover, we show how to ensure self-stabilization in this challenging\nsetting. Last, but not least, we extend all of our results to the scenario of\nexternal synchronization, at the cost of a limited increase in stabilization\ntime.", "AI": {"tldr": "This paper refines the model and analysis for Gradient Clock Synchronization, providing improved bounds on clock skew in networks by addressing short-term stability of measurement and frequency errors.", "motivation": "The paper addresses practical shortcomings in clock synchronization methods caused by overly pessimistic assumptions about measurement and frequency stability.", "method": "Novel analysis of existing Gradient Clock Synchronization techniques to focus on short-term stability of errors rather than worst-case lifetime bounds.", "result": "The paper achieves improved skew bounds, breaking established lower bounds under general conditions, and ensures self-stabilization in networks.", "conclusion": "Refining assumptions around error stability in clock synchronization leads to significant practical improvements in skew bounds and self-stabilization while extending results to external synchronization scenarios."}}
{"id": "2511.00371", "pdf": "https://arxiv.org/pdf/2511.00371", "abs": "https://arxiv.org/abs/2511.00371", "authors": ["Erfan Al-Hossami", "Razvan Bunescu"], "title": "Reasoning Trajectories for Socratic Debugging of Student Code: From Misconceptions to Contradictions and Updated Beliefs", "categories": ["cs.CL", "cs.CY", "cs.SE"], "comment": "25 pages, 2 tables, 13 figures", "summary": "In Socratic debugging, instructors guide students towards identifying and\nfixing a bug on their own, instead of providing the bug fix directly. Most\nnovice programmer bugs are caused by programming misconceptions, namely false\nbeliefs about a programming concept. In this context, Socratic debugging can be\nformulated as a guided Reasoning Trajectory (RT) leading to a statement about\nthe program behavior that contradicts the bug-causing misconception. Upon\nreaching this statement, the ensuing cognitive dissonance leads the student to\nfirst identify and then update their false belief. In this paper, we introduce\nthe task of reasoning trajectory generation, together with a dataset of\ndebugging problems manually annotated with RTs. We then describe LLM-based\nsolutions for generating RTs and Socratic conversations that are anchored on\nthem. A large-scale LLM-as-judge evaluation shows that frontier models can\ngenerate up to 91% correct reasoning trajectories and 98.7% valid conversation\nturns.", "AI": {"tldr": "The paper introduces a technique using Socratic methods for debugging in programming education, proposing reasoning trajectory generation supported by large language models (LLMs) yielding high accuracy.", "motivation": "To address novice programmers' misconceptions and improve their problem-solving skills through guided reasoning instead of direct fixes.", "method": "It involves creating reasoning trajectories and Socratic conversations driven by LLMs to help students discover and correct their programming misconceptions.", "result": "Results demonstrate LLM models can generate reasoning trajectories with 91% correctness and produce 98.7% valid conversation turns.", "conclusion": "Socratic debugging powered by LLMs is effective at generating accurate guides for addressing programming misconceptions among novices."}}
{"id": "2511.00517", "pdf": "https://arxiv.org/pdf/2511.00517", "abs": "https://arxiv.org/abs/2511.00517", "authors": ["Shuochuan Li", "Dong Wang", "Patanamon Thongtanunam", "Zan Wang", "Jiuqiao Yu", "Junjie Chen"], "title": "Issue-Oriented Agent-Based Framework for Automated Review Comment Generation", "categories": ["cs.SE"], "comment": null, "summary": "Code review (CR) is a crucial practice for ensuring software quality. Various\nautomated review comment generation techniques have been proposed to streamline\nthe labor-intensive process. However, existing approaches heavily rely on a\nsingle model to identify various issues within the code, limiting the model's\nability to handle the diverse, issue-specific nature of code changes and\nleading to non-informative comments, especially in complex scenarios such as\nbug fixes. To address these limitations, we propose RevAgent, a novel\nagent-based issue-oriented framework, decomposes the task into three stages:\n(1) Generation Stage, where five category-specific commentator agents analyze\ncode changes from distinct issue perspectives and generate candidate comments;\n(2) Discrimination Stage, where a critic agent selects the most appropriate\nissue-comment pair; and (3) Training Stage, where all agents are fine-tuned on\ncurated, category-specific data to enhance task specialization. Evaluation\nresults show that RevAgent significantly outperforms state-of-the-art PLM- and\nLLM-based baselines, with improvements of 12.90\\%, 10.87\\%, 6.32\\%, and 8.57\\%\non BLEU, ROUGE-L, METEOR, and SBERT, respectively. It also achieves relatively\nhigher accuracy in issue-category identification, particularly for challenging\nscenarios. Human evaluations further validate the practicality of RevAgent in\ngenerating accurate, readable, and context-aware review comments. Moreover,\nRevAgent delivers a favorable trade-off between performance and efficiency.", "AI": {"tldr": "RevAgent is an advanced framework for automated code review comment generation that uses multiple agents to improve comment relevance and accuracy.", "motivation": "Current automated comment generation methods are limited by relying on single models, resulting in generic and less informative comments, especially for complex code changes like bug fixes.", "method": "RevAgent uses a three-stage agent-based framework for generating code review comments: (1) comment generation by category-specific agents, (2) selection of appropriate comments by a critic agent, and (3) fine-tuning agents with tailored datasets.", "result": "RevAgent improves performance metrics significantly over existing methods, with notable enhancements in BLEU, ROUGE-L, METEOR, and SBERT scores, and better identifies issue categories.", "conclusion": "RevAgent effectively balances performance and efficiency, generating high-quality, context-aware review comments validated through human evaluations, making it a practical tool for code review."}}
{"id": "2511.00392", "pdf": "https://arxiv.org/pdf/2511.00392", "abs": "https://arxiv.org/abs/2511.00392", "authors": ["Lingpeng Chen", "Jiakun Tang", "Apple Pui-Yi Chui", "Ziyang Hong", "Junfeng Wu"], "title": "SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "8 pages, 9 figures, conference", "summary": "Accurate 3D reconstruction in visually-degraded underwater environments\nremains a formidable challenge. Single-modality approaches are insufficient:\nvision-based methods fail due to poor visibility and geometric constraints,\nwhile sonar is crippled by inherent elevation ambiguity and low resolution.\nConsequently, prior fusion technique relies on heuristics and flawed geometric\nassumptions, leading to significant artifacts and an inability to model complex\nscenes. In this paper, we introduce SonarSweep, a novel, end-to-end deep\nlearning framework that overcomes these limitations by adapting the principled\nplane sweep algorithm for cross-modal fusion between sonar and visual data.\nExtensive experiments in both high-fidelity simulation and real-world\nenvironments demonstrate that SonarSweep consistently generates dense and\naccurate depth maps, significantly outperforming state-of-the-art methods\nacross challenging conditions, particularly in high turbidity. To foster\nfurther research, we will publicly release our code and a novel dataset\nfeaturing synchronized stereo-camera and sonar data, the first of its kind.", "AI": {"tldr": "The paper addresses the challenge of 3D reconstruction in degraded underwater environments using a new fusion method called SonarSweep, combining sonar and visual data.", "motivation": "Single-modality methods (vision or sonar) struggle in underwater environments due to visibility issues and low resolution, necessitating better multi-modal approaches.", "method": "The proposed method, SonarSweep, integrates sonar and visual data using an adapted plane sweep algorithm for effective cross-modal fusion.", "result": "SonarSweep consistently enables dense and accurate depth maps, outperforming existing techniques, particularly in environments with high turbidity.", "conclusion": "SonarSweep successfully overcomes limitations of existing methods, proving effective for underwater 3D reconstruction, and contributes novel tools and data to the research community."}}
{"id": "2511.00051", "pdf": "https://arxiv.org/pdf/2511.00051", "abs": "https://arxiv.org/abs/2511.00051", "authors": ["Da Chang", "Peng Xue", "Yu Li", "Yongxiang Liu", "Pengxiang Xu", "Shixun Zhang"], "title": "Calibrating and Rotating: A Unified Framework for Weight Conditioning in PEFT", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods are crucial for adapting large\npre-trained models. Among these, LoRA is considered a foundational approach.\nBuilding on this, the influential DoRA method enhances performance by\ndecomposing weight updates into magnitude and direction. However, its\nunderlying mechanism remains unclear, and it introduces significant\ncomputational overhead. In this work, we first identify that DoRA's success\nstems from its capacity to increase the singular value entropy of the weight\nupdate matrix, which promotes a more uniform update distribution akin to full\nfine-tuning. We then reformulate DoRA into a mathematically equivalent and more\nefficient matrix form, revealing it as a learnable weight conditioning method.\nBased on this insight, we propose a unified framework for designing advanced\nPEFT methods by exploring two orthogonal dimensions: the architectural\nplacement and the transformation type of the conditioning matrix. Within this\nframework, we introduce two novel methods: (1) \\textbf{Pre-Diag}, which applies\na diagonal conditioning matrix before the LoRA update to efficiently calibrate\nthe pre-trained weights, thereby enhancing performance while reducing training\ntime; and (2) \\textbf{S}kewed \\textbf{O}rthogonal \\textbf{R}otation\n\\textbf{A}daptation (\\textbf{SORA}), which employs a parameter-efficient\northogonal rotation to perform a more powerful, norm-preserving transformation\nof the feature space. Extensive experiments on natural language understanding\nand generation tasks demonstrate that our proposed methods achieve superior\nperformance and efficiency compared to both LoRA and DoRA. The code is\navailable at https://github.com/MaeChd/SORA.", "AI": {"tldr": "The study enhances Parameter-Efficient Fine-Tuning methods by addressing the limitations of DoRA and introducing two new approaches, Pre-Diag and SORA, for improved efficiency and performance.", "motivation": "To overcome computational challenges and unclear mechanisms in existing PEFT methods, particularly enhancing upon the DoRA method.", "method": "The authors identify the mechanism of DoRA's success, reformulate it into an efficient matrix form, and propose Pre-Diag and SORA as advanced PEFT techniques under a unified framework.", "result": "The introduced methods, Pre-Diag and SORA, showed superior efficiency and performance in natural language tasks compared to LoRA and DoRA.", "conclusion": "Pre-Diag and SORA provide effective transformations for PEFT, offering a significant step forward in adapting large pre-trained models efficiently, with improved performance."}}
{"id": "2511.00095", "pdf": "https://arxiv.org/pdf/2511.00095", "abs": "https://arxiv.org/abs/2511.00095", "authors": ["Jiaming Liu", "Dingwei Fan", "Junyong Zhao", "Chunlin Li", "Haipeng Si", "Liang Sun"], "title": "SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation", "categories": ["cs.CV", "cs.AI", "92C55", "I.2.10"], "comment": "2 Tables,5 Figures,16 Equations", "summary": "The anatomical structure segmentation of the spine and adjacent structures\nfrom computed tomography (CT) images is a key step for spinal disease diagnosis\nand treatment. However, the segmentation of CT images is impeded by low\ncontrast and complex vertebral boundaries. Although advanced models such as the\nSegment Anything Model (SAM) have shown promise in various segmentation tasks,\ntheir performance in spinal CT imaging is limited by high annotation\nrequirements and poor domain adaptability. To address these limitations, we\npropose SpinalSAM-R1, a multimodal vision-language interactive system that\nintegrates a fine-tuned SAM with DeepSeek-R1, for spine CT image segmentation.\nSpecifically, our SpinalSAM-R1 introduces an anatomy-guided attention mechanism\nto improve spine segmentation performance, and a semantics-driven interaction\nprotocol powered by DeepSeek-R1, enabling natural language-guided refinement.\nThe SpinalSAM-R1 is fine-tuned using Low-Rank Adaptation (LoRA) for efficient\nadaptation. We validate our SpinalSAM-R1 on the spine anatomical structure with\nCT images. Experimental results suggest that our method achieves superior\nsegmentation performance. Meanwhile, we develop a PyQt5-based interactive\nsoftware, which supports point, box, and text-based prompts. The system\nsupports 11 clinical operations with 94.3\\% parsing accuracy and sub-800 ms\nresponse times. The software is released on\nhttps://github.com/6jm233333/spinalsam-r1.", "AI": {"tldr": "The paper introduces SpinalSAM-R1, a system combining advanced segmentation models for CT imaging of the spine.", "motivation": "To address challenges in spine CT segmentation due to low contrast and complex boundaries.", "method": "Proposes SpinalSAM-R1, integrating SAM and DeepSeek-R1 with anatomy-guided attention and semantics-driven protocols.", "result": "SpinalSAM-R1 achieves high segmentation accuracy, supported by interactive software with quick response times.", "conclusion": "The system offers efficient and accurate spine CT image segmentation and is made available publicly for further use."}}
{"id": "2511.01628", "pdf": "https://arxiv.org/pdf/2511.01628", "abs": "https://arxiv.org/abs/2511.01628", "authors": ["Arran Carter", "Torben Sell"], "title": "Partial Trace-Class Bayesian Neural Networks", "categories": ["stat.ML", "cs.LG", "stat.ME", "62G99"], "comment": "10 pages, 4 figures", "summary": "Bayesian neural networks (BNNs) allow rigorous uncertainty quantification in\ndeep learning, but often come at a prohibitive computational cost. We propose\nthree different innovative architectures of partial trace-class Bayesian neural\nnetworks (PaTraC BNNs) that enable uncertainty quantification comparable to\nstandard BNNs but use significantly fewer Bayesian parameters. These PaTraC\nBNNs have computational and statistical advantages over standard Bayesian\nneural networks in terms of speed and memory requirements. Our proposed\nmethodology therefore facilitates reliable, robust, and scalable uncertainty\nquantification in neural networks. The three architectures build on trace-class\nneural network priors which induce an ordering of the neural network\nparameters, and are thus a natural choice in our framework. In a numerical\nsimulation study, we verify the claimed benefits, and further illustrate the\nperformance of our proposed methodology on a real-world dataset.", "AI": {"tldr": "The paper proposes three innovative architectures of Bayesian neural networks (PaTraC BNNs) for scalable and efficient uncertainty quantification in deep learning.", "motivation": "Bayesian neural networks provide rigorous uncertainty quantification in deep learning but are computationally expensive.", "method": "The paper introduces PaTraC BNNs that use trace-class neural network priors to reduce Bayesian parameters while maintaining uncertainty quantification capabilities.", "result": "PaTraC BNNs demonstrate computational and statistical advantages, reducing memory and time requirements, verified through simulations and real-world dataset evaluations.", "conclusion": "PaTraC BNNs enable reliable, scalable, and robust uncertainty quantification with fewer computational costs compared to traditional BNNs."}}
{"id": "2511.01838", "pdf": "https://arxiv.org/pdf/2511.01838", "abs": "https://arxiv.org/abs/2511.01838", "authors": ["Zirui Deng", "Netanel Raviv"], "title": "Efficient Vector Symbolic Architectures from Histogram Recovery", "categories": ["cs.IT", "cs.AI", "cs.NE", "math.IT"], "comment": null, "summary": "Vector symbolic architectures (VSAs) are a family of information\nrepresentation techniques which enable composition, i.e., creating complex\ninformation structures from atomic vectors via binding and superposition, and\nhave recently found wide ranging applications in various neurosymbolic\nartificial intelligence (AI) systems. Recently, Raviv proposed the use of\nrandom linear codes in VSAs, suggesting that their subcode structure enables\nefficient binding, while preserving the quasi-orthogonality that is necessary\nfor neural processing. Yet, random linear codes are difficult to decode under\nnoise, which severely limits the resulting VSA's ability to support recovery,\ni.e., the retrieval of information objects and their attributes from a noisy\ncompositional representation.\n  In this work we bridge this gap by utilizing coding theoretic tools. First,\nwe argue that the concatenation of Reed-Solomon and Hadamard codes is suitable\nfor VSA, due to the mutual quasi-orthogonality of the resulting codewords (a\nfolklore result). Second, we show that recovery of the resulting compositional\nrepresentations can be done by solving a problem we call histogram recovery. In\nhistogram recovery, a collection of $N$ histograms over a finite field is given\nas input, and one must find a collection of Reed-Solomon codewords of length\n$N$ whose entry-wise symbol frequencies obey those histograms. We present an\noptimal solution to the histogram recovery problem by using algorithms related\nto list-decoding, and analyze the resulting noise resilience. Our results give\nrise to a noise-resilient VSA with formal guarantees regarding efficient\nencoding, quasi-orthogonality, and recovery, without relying on any heuristics\nor training, and while operating at improved parameters relative to similar\nsolutions such as the Hadamard code.", "AI": {"tldr": "This paper addresses limitations in using random linear codes in Vector Symbolic Architectures (VSAs) for neurosymbolic AI, proposing a novel method using concatenated Reed-Solomon and Hadamard codes to improve noise resilience and recovery.", "motivation": "VSAs are gaining traction in neurosymbolic AI, but existing implementations suffer from decoding inefficiencies under noise, significantly limiting their usability in noisy environments.", "method": "The authors propose enhancing VSAs by using concatenated Reed-Solomon and Hadamard codes and introduce the histogram recovery problem to retrieve compositional representations efficiently. They provide a formal solution based on list-decoding algorithms.", "result": "The proposed approach improves noise resilience, ensures quasi-orthogonality, and provides formal guarantees for encoding and recovery in VSAs. It surpasses existing methods in terms of parameters and performance.", "conclusion": "This work presents a novel, efficient, and noise-resilient VSA framework with theoretical guarantees, eliminating the need for heuristics or training while outperforming comparable solutions."}}
{"id": "2511.00424", "pdf": "https://arxiv.org/pdf/2511.00424", "abs": "https://arxiv.org/abs/2511.00424", "authors": ["Ashutosh Anshul", "Gumpili Sai Pranav", "Mohammad Zia Ur Rehman", "Nagendra Kumar"], "title": "A Multimodal Framework for Depression Detection during Covid-19 via Harvesting Social Media: A Novel Dataset and Method", "categories": ["cs.AI"], "comment": null, "summary": "The recent coronavirus disease (Covid-19) has become a pandemic and has\naffected the entire globe. During the pandemic, we have observed a spike in\ncases related to mental health, such as anxiety, stress, and depression.\nDepression significantly influences most diseases worldwide, making it\ndifficult to detect mental health conditions in people due to unawareness and\nunwillingness to consult a doctor. However, nowadays, people extensively use\nonline social media platforms to express their emotions and thoughts. Hence,\nsocial media platforms are now becoming a large data source that can be\nutilized for detecting depression and mental illness. However, existing\napproaches often overlook data sparsity in tweets and the multimodal aspects of\nsocial media. In this paper, we propose a novel multimodal framework that\ncombines textual, user-specific, and image analysis to detect depression among\nsocial media users. To provide enough context about the user's emotional state,\nwe propose (i) an extrinsic feature by harnessing the URLs present in tweets\nand (ii) extracting textual content present in images posted in tweets. We also\nextract five sets of features belonging to different modalities to describe a\nuser. Additionally, we introduce a Deep Learning model, the Visual Neural\nNetwork (VNN), to generate embeddings of user-posted images, which are used to\ncreate the visual feature vector for prediction. We contribute a curated\nCovid-19 dataset of depressed and non-depressed users for research purposes and\ndemonstrate the effectiveness of our model in detecting depression during the\nCovid-19 outbreak. Our model outperforms existing state-of-the-art methods over\na benchmark dataset by 2%-8% and produces promising results on the Covid-19\ndataset. Our analysis highlights the impact of each modality and provides\nvaluable insights into users' mental and emotional states.", "AI": {"tldr": "This paper focuses on using social media data to detect depression during the Covid-19 pandemic by analyzing textual, user-specific, and visual features using a multimodal approach.", "motivation": "Mental health cases, such as depression, increased during the Covid-19 pandemic, but effective detection methods using social media data have been underexplored.", "method": "The authors introduce a multimodal framework combining text, user-specific data, and images. They employ extrinsic features from URLs in tweets, textual analysis of images, and a Deep Learning model called Visual Neural Network (VNN) for image embeddings.", "result": "The proposed method outperformed state-of-the-art approaches by 2%-8% on a benchmark dataset and showed promising results on a curated Covid-19 dataset.", "conclusion": "The paper verifies the practicality of multimodal analysis to detect depression, offers new datasets and analysis tools, and provides insights into mental health during the pandemic."}}
{"id": "2511.01573", "pdf": "https://arxiv.org/pdf/2511.01573", "abs": "https://arxiv.org/abs/2511.01573", "authors": ["Melanie Tonarelli", "Simone Riva", "Pietro Benedusi", "Fabrizio Ferrandi", "Rolf Krause"], "title": "Adaptive Multidimensional Quadrature on Multi-GPU Systems", "categories": ["cs.DC"], "comment": "9 pages, 8 figures. Submitted to the proceedings of the 29th\n  International Conference on Domain Decomposition Methods (DD29)", "summary": "We introduce a distributed adaptive quadrature method that formulates\nmultidimensional integration as a hierarchical domain decomposition problem on\nmulti-GPU architectures. The integration domain is recursively partitioned into\nsubdomains whose refinement is guided by local error estimators. Each subdomain\nevolves independently on a GPU, which exposes a significant load imbalance as\nthe adaptive process progresses. To address this challenge, we introduce a\ndecentralised load redistribution schemes based on a cyclic round-robin policy.\nThis strategy dynamically rebalance subdomains across devices through\nnon-blocking, CUDA-aware MPI communication that overlaps with computation. The\nproposed strategy has two main advantages compared to a state-of-the-art\nGPU-tailored package: higher efficiency in high dimensions; and improved\nrobustness w.r.t the integrand regularity and the target accuracy.", "AI": {"tldr": "A distributed method for multidimensional integration using GPUs was introduced, focusing on adaptive quadrature and load balancing.", "motivation": "To improve efficiency and robustness in multidimensional integration tasks on GPUs, addressing load balancing and computational challenges.", "method": "Hierarchical domain decomposition with adaptive refinement guided by local error estimators, utilizing decentralized load redistribution via CUDA-aware MPI.", "result": "Significant load imbalance challenges were mitigated, leading to enhanced performance in high dimensions and improved handling of integrand regularity and accuracy requirements.", "conclusion": "The new adaptive quadrature method demonstrates superior efficiency and robustness compared to existing GPU integration packages."}}
{"id": "2511.00416", "pdf": "https://arxiv.org/pdf/2511.00416", "abs": "https://arxiv.org/abs/2511.00416", "authors": ["Yiwei Zha", "Rui Min", "Shanu Sushmita"], "title": "PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While AI-generated text (AIGT) detectors achieve over 90\\% accuracy on direct\nLLM outputs, they fail catastrophically against iteratively-paraphrased\ncontent. We investigate why iteratively-paraphrased text -- itself AI-generated\n-- evades detection systems designed for AIGT identification. Through intrinsic\nmechanism analysis, we reveal that iterative paraphrasing creates an\nintermediate laundering region characterized by semantic displacement with\npreserved generation patterns, which brings up two attack categories:\nparaphrasing human-authored text (authorship obfuscation) and paraphrasing\nLLM-generated text (plagiarism evasion). To address these vulnerabilities, we\nintroduce PADBen, the first benchmark systematically evaluating detector\nrobustness against both paraphrase attack scenarios. PADBen comprises a\nfive-type text taxonomy capturing the full trajectory from original content to\ndeeply laundered text, and five progressive detection tasks across\nsentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art\ndetectors, revealing critical asymmetry: detectors successfully identify the\nplagiarism evasion problem but fail for the case of authorship obfuscation. Our\nfindings demonstrate that current detection approaches cannot effectively\nhandle the intermediate laundering region, necessitating fundamental advances\nin detection architectures beyond existing semantic and stylistic\ndiscrimination methods. For detailed code implementation, please see\nhttps://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark.", "AI": {"tldr": "The paper investigates why AI-text detectors fail against iteratively-paraphrased AI-generated text, introduces a benchmark called PADBen to test vulnerabilities, and emphasizes the need for improved detection methods.", "motivation": "To understand and address the failure of AI-generated text detectors in identifying iteratively-paraphrased AI content, which poses risks such as authorship obfuscation and plagiarism evasion.", "method": "Intrinsic mechanism analysis was conducted to reveal why iterative paraphrasing escapes detection, and a benchmark called PADBen was created with specific tasks and taxonomies to evaluate detector robustness.", "result": "Tests on 11 state-of-the-art detectors reveal a critical gap where detectors can identify plagiarism evasion but fail in authorship obfuscation, particularly in the intermediate laundering region.", "conclusion": "Existing AI-text detection systems are insufficiently equipped to handle the nuances of intermediate laundering regions. Fundamental advancements in detection architectures are necessary."}}
{"id": "2511.00527", "pdf": "https://arxiv.org/pdf/2511.00527", "abs": "https://arxiv.org/abs/2511.00527", "authors": ["Robab Aghazadeh-Chakherlou", "Qing Guo", "Siddartha Khastgir", "Peter Popov", "Xiaoge Zhang", "Xingyu Zhao"], "title": "HIP-LLM: A Hierarchical Imprecise Probability Approach to Reliability Assessment of Large Language Models", "categories": ["cs.SE", "cs.AI"], "comment": "under review", "summary": "Large Language Models (LLMs) are increasingly deployed across diverse\ndomains, raising the need for rigorous reliability assessment methods. Existing\nbenchmark-based evaluations primarily offer descriptive statistics of model\naccuracy over datasets, providing limited insight into the probabilistic\nbehavior of LLMs under real operational conditions. This paper introduces\nHIP-LLM, a Hierarchical Imprecise Probability framework for modeling and\ninferring LLM reliability. Building upon the foundations of software\nreliability engineering, HIP-LLM defines LLM reliability as the probability of\nfailure-free operation over a specified number of future tasks under a given\nOperational Profile (OP). HIP-LLM represents dependencies across (sub-)domains\nhierarchically, enabling multi-level inference from subdomain to system-level\nreliability. HIP-LLM embeds imprecise priors to capture epistemic uncertainty\nand incorporates OPs to reflect usage contexts. It derives posterior\nreliability envelopes that quantify uncertainty across priors and data.\nExperiments on multiple benchmark datasets demonstrate that HIP-LLM offers a\nmore accurate and standardized reliability characterization than existing\nbenchmark and state-of-the-art approaches. A publicly accessible repository of\nHIP-LLM is provided.", "AI": {"tldr": "The paper introduces HIP-LLM, a hierarchical framework to assess the reliability of Large Language Models (LLMs), addressing their probabilistic behavior under real-world conditions.", "motivation": "The need for rigorous and probabilistic reliability assessment methods for LLMs as their deployment spans various domains, overcoming limitations of current benchmark-based evaluations.", "method": "HIP-LLM leverages a Hierarchical Imprecise Probability framework, embedding epistemic uncertainty and operational profiles for multi-level reliability inference from subdomain to system level.", "result": "Experiments indicate HIP-LLM provides a more precise, standardized method for LLM reliability quantification and surpasses existing state-of-the-art approaches.", "conclusion": "HIP-LLM establishes a robust model for LLM reliability assessment, enhancing predictive reliability and its real-world applicability. A public repository offers accessibility for further exploration and adoption."}}
{"id": "2511.00412", "pdf": "https://arxiv.org/pdf/2511.00412", "abs": "https://arxiv.org/abs/2511.00412", "authors": ["John A. Christian", "Michael R. Walker II", "Wyatt Bridgman", "Michael J. Sparapany"], "title": "Runge-Kutta Approximations for Direct Coning Compensation Applying Lie Theory", "categories": ["cs.RO"], "comment": null, "summary": "The integration of gyroscope measurements is an essential task for most\nnavigation systems. Modern vehicles typically use strapdown systems, such that\ngyro integration requires coning compensation to account for the sensor's\nrotation during the integration. Many coning compensation algorithms have been\ndeveloped and a few are reviewed. This work introduces a new class of coning\ncorrection algorithm built directly from the classical Runge-Kutta integration\nroutines. A simple case is shown to collapse to one of the most popular coning\nalgorithms and a clear procedure for generating higher-order algorithms is\npresented.", "AI": {"tldr": "This paper introduces a new class of coning correction algorithms using Runge-Kutta integration routines for improving navigation systems.", "motivation": "Current navigation systems require accurate gyroscope integration, which demands effective coning compensation algorithms due to sensor rotation during integration.", "method": "A novel coning correction algorithm based on Runge-Kutta integration routines is proposed, with a procedure for generating higher-order versions.", "result": "The simple case of the algorithm aligns with popular existing coning methods, showcasing its validity and potential for scalability.", "conclusion": "The research provides an innovative method for coning compensation, enhancing the precision of gyroscope integration in navigation systems."}}
{"id": "2511.00052", "pdf": "https://arxiv.org/pdf/2511.00052", "abs": "https://arxiv.org/abs/2511.00052", "authors": ["Federico Formica", "Stefano Gregis", "Aurora Francesca Zanenga", "Andrea Rota", "Mark Lawford", "Claudio Menghi"], "title": "Feature-Guided Analysis of Neural Networks: A Replication Study", "categories": ["cs.LG"], "comment": null, "summary": "Understanding why neural networks make certain decisions is pivotal for their\nuse in safety-critical applications. Feature-Guided Analysis (FGA) extracts\nslices of neural networks relevant to their tasks. Existing feature-guided\napproaches typically monitor the activation of the neural network neurons to\nextract the relevant rules. Preliminary results are encouraging and demonstrate\nthe feasibility of this solution by assessing the precision and recall of\nFeature-Guided Analysis on two pilot case studies. However, the applicability\nin industrial contexts needs additional empirical evidence.\n  To mitigate this need, this paper assesses the applicability of FGA on a\nbenchmark made by the MNIST and LSC datasets. We assessed the effectiveness of\nFGA in computing rules that explain the behavior of the neural network. Our\nresults show that FGA has a higher precision on our benchmark than the results\nfrom the literature. We also evaluated how the selection of the neural network\narchitecture, training, and feature selection affect the effectiveness of FGA.\nOur results show that the selection significantly affects the recall of FGA,\nwhile it has a negligible impact on its precision.", "AI": {"tldr": "This paper evaluates the effectiveness of Feature-Guided Analysis (FGA) in explaining neural network decisions using benchmarks like MNIST and LSC, showing high precision but varying recall based on network and feature selection.", "motivation": "To improve understanding of neural network decision-making, particularly for adoption in safety-critical and industrial contexts.", "method": "The applicability and effectiveness of FGA were tested on benchmark datasets (MNIST, LSC). The influence of neural network architecture, training, and feature selection on FGA's precision and recall was also analyzed.", "result": "FGA demonstrated high precision in explaining neural network behavior on the benchmark datasets. Neural architecture and feature selection mainly influenced recall, with limited effect on precision.", "conclusion": "While FGA shows promising results and high precision in explaining neural networks' functions on tested datasets, its recall varies notably depending on architecture and feature selection."}}
{"id": "2511.00098", "pdf": "https://arxiv.org/pdf/2511.00098", "abs": "https://arxiv.org/abs/2511.00098", "authors": ["Nils Porsche", "Flurin M\u00fcller-Diesing", "Sweta Banerjee", "Miguel Goncalves", "Marc Aubreville"], "title": "A filtering scheme for confocal laser endomicroscopy (CLE)-video sequences for self-supervised learning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Confocal laser endomicroscopy (CLE) is a non-invasive, real-time imaging\nmodality that can be used for in-situ, in-vivo imaging and the microstructural\nanalysis of mucous structures. The diagnosis using CLE is, however, complicated\nby images being hard to interpret for non-experienced physicians. Utilizing\nmachine learning as an augmentative tool would hence be beneficial, but is\ncomplicated by the shortage of histopathology-correlated CLE imaging sequences\nwith respect to the plurality of patterns in this domain, leading to\noverfitting of machine learning models. To overcome this, self-supervised\nlearning (SSL) can be employed on larger unlabeled datasets. CLE is a\nvideo-based modality with high inter-frame correlation, leading to a\nnon-stratified data distribution for SSL training. In this work, we propose a\nfilter functionality on CLE video sequences to reduce the dataset redundancy in\nSSL training and improve SSL training convergence and training efficiency. We\nuse four state-of-the-art baseline networks and a SSL teacher-student network\nwith a vision transformer small backbone for the evaluation. These networks\nwere evaluated on downstream tasks for a sinonasal tumor dataset and a squamous\ncell carcinoma of the skin dataset. On both datasets, we found the highest test\naccuracy on the filtered SSL-pretrained model, with 67.48% and 73.52%, both\nconsiderably outperforming their non-SSL baselines. Our results show that SSL\nis an effective method for CLE pretraining. Further, we show that our proposed\nCLE video filter can be utilized to improve training efficiency in\nself-supervised scenarios, resulting in a reduction of 67% in training time.", "AI": {"tldr": "This paper explores using self-supervised learning (SSL) with CLE for improved diagnostic accuracy and efficiency in medical imaging, highlighting a proposed video filter method for training optimization.", "motivation": "CLE images are challenging for non-experienced physicians to interpret due to the complexity of patterns and shortage of labeled datasets correlated with histopathology.", "method": "The study proposes a filter functionality for CLE video sequences to reduce redundancy and improve SSL training efficiency and convergence in medical imaging tasks. Multiple networks were used, including SSL teacher-student methods with vision transformers.", "result": "Filtered SSL-pretrained models achieved better test accuracy (67.48% and 73.52%) than non-SSL baselines, and training time was reduced by 67%.", "conclusion": "The proposed CLE video filter improves SSL training efficiency and effectiveness, showing promise for optimizing CLE-based medical diagnostics."}}
{"id": "2511.01734", "pdf": "https://arxiv.org/pdf/2511.01734", "abs": "https://arxiv.org/abs/2511.01734", "authors": ["Soufiane Hayou"], "title": "A Proof of Learning Rate Transfer under $\u03bc$P", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "comment": "23 pages", "summary": "We provide the first proof of learning rate transfer with width in a linear\nmulti-layer perceptron (MLP) parametrized with $\\mu$P, a neural network\nparameterization designed to ``maximize'' feature learning in the\ninfinite-width limit. We show that under $\\mu P$, the optimal learning rate\nconverges to a \\emph{non-zero constant} as width goes to infinity, providing a\ntheoretical explanation to learning rate transfer. In contrast, we show that\nthis property fails to hold under alternative parametrizations such as Standard\nParametrization (SP) and Neural Tangent Parametrization (NTP). We provide\nintuitive proofs and support the theoretical findings with extensive empirical\nresults.", "AI": {"tldr": "This paper proves learning rate transfer with width in linear multi-layer perceptrons under $\u0005\\mu$P parameterization and contrasts it with alternative schemes.", "motivation": "To understand and explain learning rate transferability as neural networks scale in width under varying parameterizations.", "method": "Use theoretical analysis under the $\u0005\\mu$P parameterization and compare against SP and NTP to establish the behavior of learning rates in infinite-width scenarios.", "result": "Show that $\u0005\\mu$P enables optimal learning rate convergence to a non-zero constant, while alternative schemes fail. Empirical results reinforce theoretical proof.", "conclusion": "$\u0005\\mu$P provides a scalable and consistent learning rate parameterization that outperforms other schemes in infinite-width scenarios."}}
{"id": "2511.00457", "pdf": "https://arxiv.org/pdf/2511.00457", "abs": "https://arxiv.org/abs/2511.00457", "authors": ["Chunyu Wei", "Wenji Hu", "Xingjia Hao", "Xin Wang", "Yifan Yang", "Yueguo Chen", "Yang Tian", "Yunhai Wang"], "title": "GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) face significant limitations when applied to\nlarge-scale graphs, struggling with context constraints and inflexible\nreasoning. We present GraphChain, a framework that enables LLMs to analyze\ncomplex graphs through dynamic sequences of specialized tools, mimicking human\nexploratory intelligence. Our approach introduces two key innovations: (1)\nProgressive Graph Distillation, a reinforcement learning mechanism that\ngenerates optimized tool sequences balancing task relevance with information\ncompression, and (2) Structure-aware Test-Time Adaptation, which efficiently\ntailors tool selection strategies to diverse graph topologies using spectral\nproperties and lightweight adapters without costly retraining. Experiments show\nGraphChain significantly outperforms prior methods, enabling scalable and\nadaptive LLM-driven graph analysis.", "AI": {"tldr": "GraphChain improves LLMs for graph analysis by introducing dynamic tool sequences and adaptive strategies for scalability and efficiency.", "motivation": "To address the limitations of Large Language Models (LLMs) in handling large-scale graphs and enable better reasoning and adaptability.", "method": "GraphChain framework introduces Progressive Graph Distillation for optimized tool sequences and Structure-aware Test-Time Adaptation for adaptive tool selection tailored to graph structures.", "result": "GraphChain significantly surpasses previous methods in graph analysis by making LLMs scalable and adaptable.", "conclusion": "This framework enhances LLM graph analysis, enabling efficient and scalable performance through novel reinforcement learning and adaptive methods."}}
{"id": "2511.01843", "pdf": "https://arxiv.org/pdf/2511.01843", "abs": "https://arxiv.org/abs/2511.01843", "authors": ["Andrew Goodng", "Kevin Porter", "Thomas Lopatic", "Ashish Shinde", "Sunil Sayyaparaju", "Srinivasan Seshadri", "V. Srinivasan"], "title": "LARK - Linearizability Algorithms for Replicated Keys in Aerospike", "categories": ["cs.DC", "cs.DB"], "comment": "Submitted to Industry Track of a Database Conference", "summary": "We present LARK (Linearizability Algorithms for Replicated Keys), a\nsynchronous replication protocol that achieves linearizability while minimizing\nlatency and infrastructure cost, at significantly higher availability than\ntraditional quorum-log consensus. LARK introduces Partition Availability\nConditions (PAC) that reason over the entire database cluster rather than fixed\nreplica sets, improving partition availability under independent failures by\nroughly 3x when tolerating one failure and 10x when tolerating two. Unlike\nRaft, Paxos, and Viewstamped Replication, LARK eliminates ordered logs,\nenabling immediate partition readiness after leader changes -- with at most a\nper-key duplicate-resolution round trip when the new leader lacks the latest\ncopy. Under equal storage budgets -- where both systems maintain only f+1 data\ncopies to tolerate f failures -- LARK continues committing through data-node\nfailures while log-based protocols must pause commits for replica rebuilding.\nThese properties also enable zero-downtime rolling restarts even when\nmaintaining only two copies. We provide formal safety arguments and a TLA+\nspecification, and we demonstrate through analysis and experiments that LARK\nachieves significant availability gains.", "AI": {"tldr": "LARK is a synchronous replication protocol designed to achieve linearizability with minimal latency and infrastructure cost, offering higher availability compared to traditional consensus methods.", "motivation": "To address the limitations of conventional quorum-log consensus protocols like Raft and Paxos, aiming for improved availability, reduced latency, and cost-efficiency in distributed databases.", "method": "LARK introduces Partition Availability Conditions (PAC), eliminating ordered logs and enabling efficient handling of partition readiness and failure tolerance, even under limited storage budgets.", "result": "LARK demonstrates approximately 3x better availability under single failure tolerance and 10x under two failures, while maintaining efficiency in leader transitions and rolling restarts.", "conclusion": "LARK achieves significant advancements in availability, safety, and efficiency for replicated databases, validated through formal safety arguments, specifications, and experimental analysis."}}
{"id": "2511.00421", "pdf": "https://arxiv.org/pdf/2511.00421", "abs": "https://arxiv.org/abs/2511.00421", "authors": ["Naoto Iwase", "Hiroki Okuyama", "Junichiro Iwasawa"], "title": "MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) show increasing promise in medical applications,\nbut their ability to detect and correct errors in clinical texts -- a\nprerequisite for safe deployment -- remains under-evaluated, particularly\nbeyond English. We introduce MedRECT, a cross-lingual benchmark\n(Japanese/English) that formulates medical error handling as three subtasks:\nerror detection, error localization (sentence extraction), and error\ncorrection. MedRECT is built with a scalable, automated pipeline from the\nJapanese Medical Licensing Examinations (JMLE) and a curated English\ncounterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with\ncomparable error/no-error balance. We evaluate 9 contemporary LLMs spanning\nproprietary, open-weight, and reasoning families. Key findings: (i) reasoning\nmodels substantially outperform standard architectures, with up to 13.5%\nrelative improvement in error detection and 51.0% in sentence extraction; (ii)\ncross-lingual evaluation reveals 5-10% performance gaps from English to\nJapanese, with smaller disparities for reasoning models; (iii) targeted LoRA\nfine-tuning yields asymmetric improvements in error correction performance\n(Japanese: +0.078, English: +0.168) while preserving reasoning capabilities;\nand (iv) our fine-tuned model exceeds human expert performance on structured\nmedical error correction tasks. To our knowledge, MedRECT is the first\ncomprehensive cross-lingual benchmark for medical error correction, providing a\nreproducible framework and resources for developing safer medical LLMs across\nlanguages.", "AI": {"tldr": "The paper introduces MedRECT, a benchmark for evaluating medical error handling in clinical texts through error detection, sentence extraction, and error correction in Japanese and English. Results indicate reasoning models outperform traditional ones, fine-tuning improves performance further, and the model surpasses human experts.", "motivation": "To ensure safe deployment of large language models in medical applications, particularly in detecting and correcting errors in clinical texts across different languages.", "method": "MedRECT is developed using a scalable pipeline from Japanese Medical Licensing Examinations and its English counterpart, consisting of 663 Japanese and 458 English texts. Nine contemporary LLMs were evaluated, focusing on tasks like error detection, localization, and correction.", "result": "Reasoning models surpass standard architectures in performance (up to 13.5%-error detection, 51.0%-sentence extraction improvement). Cross-lingual evaluations reveal better performance in English compared to Japanese. LoRA fine-tuning significantly improves error correction performance without compromising reasoning capabilities. Enhanced fine-tuned LLM surpasses human performance.", "conclusion": "MedRECT stands as the first cross-lingual benchmark for medical error correction, fostering development of safer and reliable medical LLMs for diverse languages."}}
{"id": "2511.00528", "pdf": "https://arxiv.org/pdf/2511.00528", "abs": "https://arxiv.org/abs/2511.00528", "authors": ["Muhammad Hamid Raza Mookadam", "Ridewaan Hanslo"], "title": "Employee Performance when Implementing Agile Practices in an IT Workforce", "categories": ["cs.SE", "D.2.9"], "comment": "11 pages, 1 figure, 1 table, 7th World Symposium on Software\n  Engineering (WSSE 2025)", "summary": "Adoption of agile practices has increased in IT workforces. However, there is\na lack of comprehensive studies in the African context on employee performance\nwhen implementing agile practices. This study addresses this gap by exploring\nemployee performance in agile environments for IT workforces in South Africa.\nAn interpretivist mono-method qualitative approach was used, with the use of\ninterviews as a research strategy. Seventeen semi-structured interviews were\nconducted with agile practitioners from various roles. Our results indicated\nthat agile practices influence employee performance significantly, with\nparticipants reporting on aspects which included planning, communication,\nemployee development and well-being, collaboration, team culture and progress.\nAdditionally, our results reported obstacles when using agile practices that\nincluded adoption, team engagement, leadership and instilling an agile mindset.\nAgile practices influence employee performance in IT workforces by fostering\nimproved team dynamics, enhanced collaboration, improved efficiencies, risk\nmanagement, planning, continuous improvement, learning, personal development\nand well-being. Conclusively, our findings suggest that if agile challenges are\naddressed and additional support is provided, employee performance can be\nsignificantly improved.", "AI": {"tldr": "The study explores how agile practices affect employee performance in South African IT workforces, identifying both benefits and challenges.", "motivation": "To address the lack of studies analyzing the impact of agile practices on employee performance in the African IT workforce context.", "method": "A qualitative interpretivist mono-method research approach using semi-structured interviews with 17 agile practitioners from various roles.", "result": "Agile practices positively influence team dynamics, collaboration, planning, learning, development, and well-being, though challenges like adoption, team engagement, leadership, and mindset transformation persist.", "conclusion": "Addressing agile challenges and providing further support can significantly improve employee performance in IT sectors."}}
{"id": "2511.00492", "pdf": "https://arxiv.org/pdf/2511.00492", "abs": "https://arxiv.org/abs/2511.00492", "authors": ["Simon Giel", "James Hurrell", "Shreya Santra", "Ashutosh Mishra", "Kentaro Uno", "Kazuya Yoshida"], "title": "Design and Development of a Modular Bucket Drum Excavator for Lunar ISRU", "categories": ["cs.RO"], "comment": "6 pages, 4 figures. Accepted at IEEE iSpaRo 2025", "summary": "In-Situ Resource Utilization (ISRU) is one of the key technologies for\nenabling sustainable access to the Moon. The ability to excavate lunar regolith\nis the first step in making lunar resources accessible and usable. This work\npresents the development of a bucket drum for the modular robotic system\nMoonBot, as part of the Japanese Moonshot program. A 3D-printed prototype made\nof PLA was manufactured to evaluate its efficiency through a series of sandbox\ntests. The resulting tool weighs 4.8 kg and has a volume of 14.06 L. It is\ncapable of continuous excavation at a rate of 777.54 kg/h with a normalized\nenergy consumption of 0.022 Wh/kg. In batch operation, the excavation rate is\n172.02 kg/h with a normalized energy consumption of 0.86 Wh per kilogram of\nexcavated material. The obtained results demonstrate the successful\nimplementation of the concept. A key advantage of the developed tool is its\ncompatibility with the modular MoonBot robotic platform, which enables flexible\nand efficient mission planning. Further improvements may include the\nintegration of sensors and an autonomous control system to enhance the\nexcavation process.", "AI": {"tldr": "The study develops and tests a bucket drum for lunar regolith excavation using MoonBot, achieving promising energy-efficient results.", "motivation": "To enable sustainable access to the Moon by efficiently excavating lunar regolith as part of Lunar In-Situ Resource Utilization (ISRU) efforts.", "method": "Developed a 3D-printed bucket drum prototype, tested efficiency through sandbox experiments for continuous and batch excavation using the modular MoonBot system.", "result": "Efficient excavation rates were achieved: 777.54 kg/h (continuous) with 0.022 Wh/kg energy consumption, and 172.02 kg/h (batch) with 0.86 Wh/kg energy consumption. The tool weighs 4.8 kg and has a volume of 14.06 L.", "conclusion": "The prototype showcased successful implementation of excavation, compatibility with MoonBot modular robots, and potential for enhanced efficiency with future sensor and autonomous system integration."}}
{"id": "2511.00053", "pdf": "https://arxiv.org/pdf/2511.00053", "abs": "https://arxiv.org/abs/2511.00053", "authors": ["Hao Wang", "Licheng Pan", "Yuan Lu", "Zhichao Chen", "Tianqiao Liu", "Shuting He", "Zhixuan Chu", "Qingsong Wen", "Haoxuan Li", "Zhouchen Lin"], "title": "Quadratic Direct Forecast for Training Multi-Step Time-Series Forecast Models", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "The design of training objective is central to training time-series\nforecasting models. Existing training objectives such as mean squared error\nmostly treat each future step as an independent, equally weighted task, which\nwe found leading to the following two issues: (1) overlook the label\nautocorrelation effect among future steps, leading to biased training\nobjective; (2) fail to set heterogeneous task weights for different forecasting\ntasks corresponding to varying future steps, limiting the forecasting\nperformance. To fill this gap, we propose a novel quadratic-form weighted\ntraining objective, addressing both of the issues simultaneously. Specifically,\nthe off-diagonal elements of the weighting matrix account for the label\nautocorrelation effect, whereas the non-uniform diagonals are expected to match\nthe most preferable weights of the forecasting tasks with varying future steps.\nTo achieve this, we propose a Quadratic Direct Forecast (QDF) learning\nalgorithm, which trains the forecast model using the adaptively updated\nquadratic-form weighting matrix. Experiments show that our QDF effectively\nimproves performance of various forecast models, achieving state-of-the-art\nresults. Code is available at https://anonymous.4open.science/r/QDF-8937.", "AI": {"tldr": "The paper introduces a quadratic-form weighted training objective to improve time-series forecasting models by accounting for label autocorrelation and task weighting disparities, significantly enhancing performance.", "motivation": "Existing objectives like mean squared error fail to consider label autocorrelation among future steps and do not assign appropriate weights to varying forecasting tasks.", "method": "The authors developed the Quadratic Direct Forecast (QDF) learning algorithm that uses an adaptively updated quadratic-form weighting matrix to address training biases and achieve optimal task weighting.", "result": "QDF was tested across various forecast models, demonstrating significant improvements and surpassing state-of-the-art forecasting performance.", "conclusion": "The novel quadratic-form training objective and QDF algorithm effectively improve time-series forecasting by handling label autocorrelation and heterogeneous task weighting, contributing to better model accuracy and efficiency."}}
{"id": "2511.00103", "pdf": "https://arxiv.org/pdf/2511.00103", "abs": "https://arxiv.org/abs/2511.00103", "authors": ["Rotem Ezra", "Hedi Zisling", "Nimrod Berman", "Ilan Naiman", "Alexey Gorkor", "Liran Nochumsohn", "Eliya Nachmani", "Omri Azencot"], "title": "FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models have become state-of-the-art generative models for images,\naudio, and video, yet enabling fine-grained controllable generation, i.e.,\ncontinuously steering specific concepts without disturbing unrelated content,\nremains challenging. Concept Sliders (CS) offer a promising direction by\ndiscovering semantic directions through textual contrasts, but they require\nper-concept training and architecture-specific fine-tuning (e.g., LoRA),\nlimiting scalability to new modalities. In this work we introduce FreeSliders,\na simple yet effective approach that is fully training-free and\nmodality-agnostic, achieved by partially estimating the CS formula during\ninference. To support modality-agnostic evaluation, we extend the CS benchmark\nto include both video and audio, establishing the first suite for fine-grained\nconcept generation control with multiple modalities. We further propose three\nevaluation properties along with new metrics to improve evaluation quality.\nFinally, we identify an open problem of scale selection and non-linear\ntraversals and introduce a two-stage procedure that automatically detects\nsaturation points and reparameterizes traversal for perceptually uniform,\nsemantically meaningful edits. Extensive experiments demonstrate that our\nmethod enables plug-and-play, training-free concept control across modalities,\nimproves over existing baselines, and establishes new tools for principled\ncontrollable generation. An interactive presentation of our benchmark and\nmethod is available at: https://azencot-group.github.io/FreeSliders/", "AI": {"tldr": "The paper introduces FreeSliders, a training-free and modality-agnostic approach for fine-grained controllable generation in diffusion models, extended to video and audio modalities.", "motivation": "To address the limitations of current controllable generation methods, which require per-concept training and specific fine-tuning, thereby limiting scalability to different modalities.", "method": "FreeSliders estimates the Concept Sliders formula partially during inference without the need for training or modality-specific adjustments, while also extending evaluation frameworks to video and audio with novel metrics and evaluation techniques.", "result": "FreeSliders enables plug-and-play, training-free concept control across multiple modalities, surpassing existing baselines and solving challenges like scale selection and perceptual uniformity in edits.", "conclusion": "The proposed FreeSliders method improves controllable generation across images, video, and audio, enabling broader scalability and laying groundwork for universal concept generation control."}}
{"id": "2511.00509", "pdf": "https://arxiv.org/pdf/2511.00509", "abs": "https://arxiv.org/abs/2511.00509", "authors": ["Yifan Xia", "Guorui Chen", "Wenqian Yu", "Zhijiang Li", "Philip Torr", "Jindong Gu"], "title": "Reimagining Safety Alignment with An Image", "categories": ["cs.AI", "cs.CR"], "comment": null, "summary": "Large language models (LLMs) excel in diverse applications but face dual\nchallenges: generating harmful content under jailbreak attacks and over-refusal\nof benign queries due to rigid safety mechanisms. These issues are further\ncomplicated by the need to accommodate different value systems and precisely\nalign with given safety preferences. Moreover, traditional methods like SFT and\nRLHF lack this capability due to their costly parameter tuning requirements and\ninability to support multiple value systems within a single model. These\nproblems are more obvious in multimodal large language models (MLLMs),\nespecially in terms of heightened over-refusal in cross-modal tasks and new\nsecurity risks arising from expanded attack surfaces. We propose Magic Image,\nan optimization-driven visual prompt framework that enhances security while\nreducing over-refusal. By optimizing image prompts using harmful/benign\nsamples, our method enables a single model to adapt to different value systems\nand better align with given safety preferences without parameter updates.\nExperiments demonstrate improved safety-effectiveness balance across diverse\ndatasets while preserving model performance, offering a practical solution for\ndeployable MLLM safety alignment.", "AI": {"tldr": "The paper addresses safety challenges in large multimodal language models (MLLMs) with a proposed visual prompt framework, Magic Image, improving security and reducing over-refusal without parameter updates.", "motivation": "LLMs face challenges of generating harmful content from jailbreak attacks and unnecessary refusal of benign queries, compounded by difficulty in accommodating varied value systems and efficiently aligning safety preferences.", "method": "The authors introduce Magic Image, a visual prompt optimization system tailored to harmful/benign samples, enabling adaptable safety alignment across diverse value systems without requiring parameter tuning.", "result": "Experiments show enhanced balance between safety and effectiveness in MLLMs across diverse datasets, with performance preserved.", "conclusion": "Magic Image offers a practical and deployable solution for addressing safety challenges in multimodal large language models."}}
{"id": "2511.00432", "pdf": "https://arxiv.org/pdf/2511.00432", "abs": "https://arxiv.org/abs/2511.00432", "authors": ["Zhiwen Ruan", "Yixia Li", "Yefeng Liu", "Yun Chen", "Weihua Luo", "Peng Li", "Yang Liu", "Guanhua Chen"], "title": "G2: Guided Generation for Enhanced Output Diversity in LLMs", "categories": ["cs.CL"], "comment": "EMNLP 2025", "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\ndiverse natural language processing tasks. However, these models exhibit a\ncritical limitation in output diversity, often generating highly similar\ncontent across multiple attempts. This limitation significantly affects tasks\nrequiring diverse outputs, from creative writing to reasoning. Existing\nsolutions, like temperature scaling, enhance diversity by modifying probability\ndistributions but compromise output quality. We propose Guide-to-Generation\n(G2), a training-free plug-and-play method that enhances output diversity while\npreserving generation quality. G2 employs a base generator alongside dual\nGuides, which guide the generation process through decoding-based interventions\nto encourage more diverse outputs conditioned on the original query.\nComprehensive experiments demonstrate that G2 effectively improves output\ndiversity while maintaining an optimal balance between diversity and quality.", "AI": {"tldr": "The paper introduces Guide-to-Generation (G2), a method to address the diversity limitations in large language models (LLMs) output without compromising quality.", "motivation": "LLMs often generate repetitive outputs across multiple attempts, which is a significant drawback for tasks requiring diverse and creative solutions.", "method": "The proposed G2 method uses a base generator and dual Guides integrated into the decoding process to enhance the diversity of outputs without additional training.", "result": "Experiments show that G2 significantly improves output diversity while achieving a balance between diversity and output quality.", "conclusion": "G2 offers a training-free, effective, and quality-preserving solution to enhance output diversity in LLMs."}}
{"id": "2511.00619", "pdf": "https://arxiv.org/pdf/2511.00619", "abs": "https://arxiv.org/abs/2511.00619", "authors": ["Huaijin Ran", "Haoyi Zhang", "Xunzhu Tang"], "title": "GDPR-Bench-Android: A Benchmark for Evaluating Automated GDPR Compliance Detection in Android", "categories": ["cs.SE"], "comment": null, "summary": "Automating the detection of EU General Data Protection Regulation (GDPR)\nviolations in source code is a critical but underexplored challenge. We\nintroduce \\textbf{GDPR-Bench-Android}, the first comprehensive benchmark for\nevaluating diverse automated methods for GDPR compliance detection in Android\napplications. It contains \\textbf{1951} manually annotated violation instances\nfrom \\textbf{15} open-source repositories, covering 23 GDPR articles at file-,\nmodule-, and line-level granularities. To enable a multi-paradigm evaluation,\nwe contribute \\textbf{Formal-AST}, a novel, source-code-native formal method\nthat serves as a deterministic baseline. We define two tasks: (1)\n\\emph{multi-granularity violation localization}, evaluated via\nAccuracy@\\textit{k}; and (2) \\emph{snippet-level multi-label classification},\nassessed by macro-F1 and other classification metrics. We benchmark 11 methods,\nincluding eight state-of-the-art LLMs, our Formal-AST analyzer, a\nretrieval-augmented (RAG) method, and an agentic (ReAct) method. Our findings\nreveal that no single paradigm excels across all tasks. For Task 1, the ReAct\nagent achieves the highest file-level Accuracy@1 (17.38%), while the\nQwen2.5-72B LLM leads at the line level (61.60%), in stark contrast to the\nFormal-AST method's 1.86%. For the difficult multi-label Task 2, the\nClaude-Sonnet-4.5 LLM achieves the best Macro-F1 (5.75%), while the RAG method\nyields the highest Macro-Precision (7.10%). These results highlight the\ntask-dependent strengths of different automated approaches and underscore the\nvalue of our benchmark in diagnosing their capabilities. All resources are\navailable at: https://github.com/Haoyi-Zhang/GDPR-Bench-Android.", "AI": {"tldr": "This paper introduces GDPR-Bench-Android, a benchmark designed for evaluating automated detection methods of GDPR violations in Android applications using various paradigms.", "motivation": "Automating GDPR compliance detection in source code is critical yet underexplored, especially given the complexity of GDPR requirements in software development.", "method": "The authors created GDPR-Bench-Android with 1951 annotated violation instances from 15 open-source repositories and introduced Formal-AST, a deterministic source-code-native formal method. They defined multi-granularity violation localization and snippet-level multi-label classification tasks.", "result": "Results showed task-dependent strengths across methods, with ReAct achieving the highest file-level accuracy, Qwen2.5-72B LLM leading line-level accuracy, Claude-Sonnet-4.5 excelling in macro-F1, and the RAG method demonstrating the highest macro-precision.", "conclusion": "This benchmark highlights the varying strengths of different automated approaches, offering a valuable tool for diagnosing capabilities and advancing GDPR compliance detection."}}
{"id": "2511.00512", "pdf": "https://arxiv.org/pdf/2511.00512", "abs": "https://arxiv.org/abs/2511.00512", "authors": ["Suraj Kumar", "Andy Ruina"], "title": "Descriptive Model-based Learning and Control for Bipedal Locomotion", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "8 pages, 15 figures", "summary": "Bipedal balance is challenging due to its multi-phase, hybrid nature and\nhigh-dimensional state space. Traditional balance control approaches for\nbipedal robots rely on low-dimensional models for locomotion planning and\nreactive control, constraining the full robot to behave like these simplified\nmodels. This involves tracking preset reference paths for the Center of Mass\nand upper body obtained through low-dimensional models, often resulting in\ninefficient walking patterns with bent knees. However, we observe that bipedal\nbalance is inherently low-dimensional and can be effectively described with\nsimple state and action descriptors in a low-dimensional state space. This\nallows the robot's motion to evolve freely in its high-dimensional state space,\nonly constraining its projection in the low-dimensional state space. In this\nwork, we propose a novel control approach that avoids prescribing a\nlow-dimensional model to the full model. Instead, our control framework uses a\ndescriptive model with the minimum degrees of freedom necessary to maintain\nbalance, allowing the remaining degrees of freedom to evolve freely in the\nhigh-dimensional space. This results in an efficient human-like walking gait\nand improved robustness.", "AI": {"tldr": "The paper introduces a novel bipedal robot balance control framework that utilizes descriptive low-dimensional state models only for balance, enabling efficient human-like movement and improving robustness.", "motivation": "Bipedal balance is challenging due to its complex nature, and existing approaches restrict robots to simplified low-dimensional models, leading to inefficient walking patterns.", "method": "The proposed method uses a descriptive low-dimensional model to maintain balance while allowing the remaining degrees of freedom to naturally evolve in high-dimensional space.", "result": "The framework results in human-like walking gaits and enhanced robustness in bipedal robot balance control.", "conclusion": "This approach overcomes limitations of traditional methods by achieving both efficiency and robustness in bipedal movement without enforcing restrictive models."}}
{"id": "2511.00054", "pdf": "https://arxiv.org/pdf/2511.00054", "abs": "https://arxiv.org/abs/2511.00054", "authors": ["Gio Huh", "Dhruv Sheth", "Rayhan Zirvi", "Frank Xiao"], "title": "SpatialTraceGen: High-Fidelity Traces for Efficient VLM Spatial Reasoning Distillation", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025) Workshop on Efficient Reasoning", "summary": "While Vision-Language Models (VLMs) excel in many areas, they struggle with\ncomplex spatial reasoning, which requires problem decomposition and strategic\ntool use. Fine-tuning smaller, more deployable models offers an efficient path\nto strong performance, but this is hampered by a major bottleneck: the absence\nof high-quality, step-by-step reasoning data. To address this data-efficiency\ngap, we introduce SpatialTraceGen, a framework to distill the reasoning\nprocesses of a large teacher model into a high-quality dataset of multi-hop,\nmulti-tool reasoning traces. A key innovation is our automated Verifier, which\nscalably ensures the fidelity of each reasoning step, providing a\ncost-effective alternative to manual human annotation. On the CLEVR-Humans\nbenchmark, this verifier-guided process improves the average quality score of\ntraces by 17\\% while reducing quality variance by over 40\\%. SpatialTraceGen\ndelivers a dataset of expert traces, providing the structured, step-by-step\nexamples of tool use necessary for effective fine-tuning and sample-efficient\noffline reinforcement learning.", "AI": {"tldr": "The paper introduces SpatialTraceGen, a framework to improve complex spatial reasoning for vision-language models (VLMs) by generating high-quality, step-by-step reasoning datasets.", "motivation": "Vision-Language Models (VLMs) struggle with spatial reasoning and problem decomposition due to the lack of high-quality reasoning datasets for fine-tuning.", "method": "The authors develop SpatialTraceGen, which distills the reasoning processes of a large model into a dataset of reasoning traces, using an automated Verifier to ensure fidelity and scalability.", "result": "On the CLEVR-Humans benchmark, SpatialTraceGen boosts trace quality scores by 17% while reducing quality variance by over 40%.", "conclusion": "SpatialTraceGen enables efficient fine-tuning and offline reinforcement learning by providing high-quality reasoning datasets, enhancing VLMs' spatial reasoning abilities."}}
{"id": "2511.00107", "pdf": "https://arxiv.org/pdf/2511.00107", "abs": "https://arxiv.org/abs/2511.00107", "authors": ["Piyushkumar Patel"], "title": "AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency", "categories": ["cs.CV", "cs.AI", "cs.IR"], "comment": null, "summary": "Text to video generation has emerged as a critical frontier in generative\nartificial intelligence, yet existing approaches struggle with maintaining\ntemporal consistency, compositional understanding, and fine grained control\nover visual narratives. We present MOVAI (Multimodal Original Video AI), a\nnovel hierarchical framework that integrates compositional scene understanding\nwith temporal aware diffusion models for high fidelity text to video synthesis.\nOur approach introduces three key innovations: (1) a Compositional Scene Parser\n(CSP) that decomposes textual descriptions into hierarchical scene graphs with\ntemporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) that\nensures coherent motion dynamics across frames while preserving spatial\ndetails, and (3) a Progressive Video Refinement (PVR) module that iteratively\nenhances video quality through multi-scale temporal reasoning. Extensive\nexperiments on standard benchmarks demonstrate that MOVAI achieves\nstate-of-the-art performance, improving video quality metrics by 15.3% in\nLPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing\nmethods. Our framework shows particular strength in generating complex\nmulti-object scenes with realistic temporal dynamics and fine-grained semantic\ncontrol.", "AI": {"tldr": "The paper introduces MOVAI, a hierarchical framework for text-to-video generation that emphasizes temporal consistency and compositional understanding, achieving notable improvements over previous methods.", "motivation": "Existing text-to-video generation methods struggle with temporal consistency, compositional understanding, and precise visual narrative control, driving the need for a more robust system.", "method": "MOVAI employs a hierarchical framework composed of a Compositional Scene Parser (CSP), a Temporal-Spatial Attention Mechanism (TSAM), and a Progressive Video Refinement (PVR) module for coherent and high-quality text-to-video synthesis.", "result": "MOVAI significantly improves video quality metrics by 15.3% in LPIPS, 12.7% in FVD, and 18.9% in user preference studies, excelling in complex scenarios with multi-object scenes and intricate dynamics.", "conclusion": "The MOVAI framework demonstrates its effectiveness in addressing core issues in text-to-video generation, offering a state-of-the-art solution with high fidelity and advanced semantic control."}}
{"id": "2511.00185", "pdf": "https://arxiv.org/pdf/2511.00185", "abs": "https://arxiv.org/abs/2511.00185", "authors": ["Roberto Morales"], "title": "SHAP values through General Fourier Representations: Theory and Applications", "categories": ["math.OC", "math.AP", "stat.ML", "68T07, 42B10, 60G15, 65T50"], "comment": null, "summary": "This article establishes a rigorous spectral framework for the mathematical\nanalysis of SHAP values. We show that any predictive model defined on a\ndiscrete or multi-valued input space admits a generalized Fourier expansion\nwith respect to an orthonormalisation tensor-product basis constructed under a\nproduct probability measure. Within this setting, each SHAP attribution can be\nrepresented as a linear functional of the model's Fourier coefficients.\n  Two complementary regimes are studied. In the deterministic regime, we derive\nquantitative stability estimates for SHAP values under Fourier truncation,\nshowing that the attribution map is Lipschitz continuous with respect to the\ndistance between predictors. In the probabilistic regime, we consider neural\nnetworks in their infinite-width limit and prove convergence of SHAP values\ntoward those induced by the corresponding Gaussian process prior, with explicit\nerror bounds in expectation and with high probability based on concentration\ninequalities.\n  We also provide a numerical experiment on a clinical unbalanced dataset to\nvalidate the theoretical findings.", "AI": {"tldr": "The paper develops a spectral framework to analyze SHAP values rigorously, studying both deterministic and probabilistic regimes, with experiments validating the theory.", "motivation": "The motivation is to provide a rigorous mathematical framework for understanding SHAP values and their stability in predictive models.", "method": "The method involves constructing a generalized Fourier expansion for models and analyzing SHAP values as linear functionals of Fourier coefficients in deterministic and probabilistic settings.", "result": "The paper demonstrates that SHAP attributions are stable under Fourier truncation and converge for neural networks in the infinite-width limit, backed by numerical and theoretical validations.", "conclusion": "SHAP values can be mathematically analyzed and bounded in stability and convergence using spectral methods, supported by numerical experiments."}}
{"id": "2511.00547", "pdf": "https://arxiv.org/pdf/2511.00547", "abs": "https://arxiv.org/abs/2511.00547", "authors": ["Alain Riou"], "title": "Efficient Generation of Binary Magic Squares", "categories": ["cs.AI"], "comment": null, "summary": "We propose a simple algorithm for generating Binary Magic Squares (BMS),\ni.e., square binary matrices where the sum of all rows and all columns are\nequal. We show by induction that our algorithm always returns valid BMS with\noptimal theoretical complexity. We then extend our study to non-square Binary\nMagic Squares, formalize conditions on the sum of rows and columns for these\nBMS to exist, and show that a slight variant of our first algorithm can\ngenerate provably generate them. Finally, we publicly release two\nimplementations of our algorithm as Python packages, including one that can\ngenerate several BMS in parallel using GPU acceleration.", "AI": {"tldr": "The paper presents a new algorithm for generating Binary Magic Squares (BMS) and extends it to non-square versions, validated by induction and released as Python packages, including a GPU-accelerated version.", "motivation": "To devise an efficient method to generate Binary Magic Squares, ensuring theoretical correctness, extending it to non-square cases, and making it practical for public use.", "method": "Utilized a simple algorithm to generate Binary Magic Squares, validated through induction, and extended it for non-square cases. Python implementations, including a GPU-accelerated version, were created for practical use.", "result": "The algorithm successfully generates both square and non-square Binary Magic Squares with optimal theoretical complexity and is publicly available as Python tools.", "conclusion": "The approach provides an efficient, scalable way to generate Binary Magic Squares, with implementations readily accessible for broader applications."}}
{"id": "2511.00205", "pdf": "https://arxiv.org/pdf/2511.00205", "abs": "https://arxiv.org/abs/2511.00205", "authors": ["Yuhan Deng", "Akshay Srivatsan", "Sebastian Ingino", "Francis Chua", "Yasmine Mitchell", "Matthew Vilaysack", "Keith Winstein"], "title": "Fix: externalizing network I/O in serverless computing", "categories": ["cs.OS", "cs.DC"], "comment": "To appear in 21st European Conference on Computer Systems (EUROSYS\n  26)", "summary": "We describe a system for serverless computing where users, programs,\n  and the underlying platform share a common representation of a\n  computation: a deterministic procedure, run in an environment\n  of well-specified data or the outputs of other computations. This\n  representation externalizes I/O: data movement over the network is\n  performed exclusively by the platform. Applications can describe the\n  precise data needed at each stage, helping the provider schedule\n  tasks and network transfers to reduce starvation. The design\n  suggests an end-to-end argument for outsourced computing, shifting\n  the service model from ``pay-for-effort'' to ``pay-for-results.''", "AI": {"tldr": "The paper proposes a novel serverless computing system emphasizing deterministic procedures and externalized I/O, aimed at enhancing efficiency in scheduling and data movement.", "motivation": "Current serverless computing models face inefficiencies due to ambiguous computation representation and data handling, limiting task scheduling and network optimization.", "method": "The researchers introduce a model where computation is represented as deterministic procedures run using specified data inputs and outputs, enabling better clarity and platform-driven data management.", "result": "The system allows applications to precisely state the data needed at different stages, aiding the provider in reducing inefficiencies like task starvation and improving overall network transfer scheduling.", "conclusion": "By externalizing I/O and adopting this design, the paper argues for a shift from \"pay-for-effort\" to \"pay-for-results\" in outsourced computing."}}
{"id": "2511.00476", "pdf": "https://arxiv.org/pdf/2511.00476", "abs": "https://arxiv.org/abs/2511.00476", "authors": ["Ghazal Kalhor", "Afra Mashhadi"], "title": "Remembering Unequally: Global and Disciplinary Bias in LLM-Generated Co-Authorship Networks", "categories": ["cs.CL"], "comment": null, "summary": "Ongoing breakthroughs in Large Language Models (LLMs) are reshaping search\nand recommendation platforms at their core. While this shift unlocks powerful\nnew scientometric tools, it also exposes critical fairness and bias issues that\ncould erode the integrity of the information ecosystem. Additionally, as LLMs\nbecome more integrated into web-based searches for scholarly tools, their\nability to generate summarized research work based on memorized data introduces\nnew dimensions to these challenges. The extent of memorization in LLMs can\nimpact the accuracy and fairness of the co-authorship networks they produce,\npotentially reflecting and amplifying existing biases within the scientific\ncommunity and across different regions. This study critically examines the\nimpact of LLM memorization on the co-authorship networks. To this end, we\nassess memorization effects across three prominent models, DeepSeek R1, Llama 4\nScout, and Mixtral 8x7B, analyzing how memorization-driven outputs vary across\nacademic disciplines and world regions. While our global analysis reveals a\nconsistent bias favoring highly cited researchers, this pattern is not\nuniformly observed. Certain disciplines, such as Clinical Medicine, and\nregions, including parts of Africa, show more balanced representation, pointing\nto areas where LLM training data may reflect greater equity. These findings\nunderscore both the risks and opportunities in deploying LLMs for scholarly\ndiscovery.", "AI": {"tldr": "The paper investigates the fairness and bias implications of memorization in Large Language Models (LLMs) by analyzing outputs across co-authorship networks.", "motivation": "The motivation is to understand how LLMs, increasingly used for scholarly search and recommendation, affect fairness and bias, particularly through memorization.", "method": "Three LLMs (DeepSeek R1, Llama 4 Scout, and Mixtral 8x7B) are evaluated for memorization-driven biases in co-authorship networks across disciplines and regions.", "result": "The study finds a global bias favoring highly cited researchers but identifies exceptions in specific disciplines and regions, such as Clinical Medicine and parts of Africa, with more balanced representation.", "conclusion": "LLMs reflect both risks and opportunities for scholarly discovery, highlighting the need for balanced training data to mitigate biases."}}
{"id": "2511.00624", "pdf": "https://arxiv.org/pdf/2511.00624", "abs": "https://arxiv.org/abs/2511.00624", "authors": ["Haoyi Zhang", "Huaijin Ran", "Xunzhu Tang"], "title": "Can Large Language Models Detect Real-World Android Software Compliance Violations?", "categories": ["cs.SE"], "comment": null, "summary": "The rapid development of Large Language Models (LLMs) has transformed\nsoftware engineering, showing promise in tasks like code generation, bug\ndetection, and compliance checking. However, current models struggle to detect\ncompliance violations in Android applications across diverse legal frameworks.\nWe propose \\emph{CompliBench}, a novel evaluation framework for assessing LLMs'\nability to detect compliance violations under regulations like LGPD, PDPA, and\nPIPEDA. The framework defines two tasks: Task 1 evaluates \\emph{retrieval and\nlocalization} at file, module, and line granularities, and Task 2 assesses\n\\emph{multi-label judgment} for code snippets. These tasks mirror the audit\nprocess, where auditors locate problematic code and determine implicated\nprovisions. Traditional metrics fail to capture important aspects like\ncross-granularity stability and jurisdictional consistency. Thus, we introduce\nstability-aware composites (SGS, RCS, CRGS, and OCS) for a more comprehensive\nassessment. Experiments with six models, including GPT-4O and Claude-3.5, show\n\\emph{CompliBench} improves compliance detection, with\nClaude-3.5-sonnet-20241022 achieving the highest OCS score (0.3295), and\nGemini-2.5-pro the lowest (0.0538). This work demonstrates \\emph{CompliBench}'s\npotential for improving LLM performance in compliance tasks and provides a\nfoundation for future tools aligned with data protection standards. Our project\nis available at https://github.com/Haoyi-Zhang/CompliBench.", "AI": {"tldr": "The paper introduces CompliBench, a framework for evaluating LLMs on compliance detection tasks in Android applications. It involves locating violations and assessing regulations across jurisdictions, with new metrics for evaluation.", "motivation": "Current LLMs struggle in detecting compliance violations across diverse legal frameworks, hindering their application in ensuring regulations in Android apps.", "method": "Two tasks are defined: (1) retrieval and localization of violations at detailed code granularities, and (2) multi-label judgment for snippets. Metrics are introduced to capture cross-granularity stability and jurisdictional consistency.", "result": "Experiments with six models show improved compliance detection using CompliBench. Claude-3.5-sonnet achieves the highest OCS score, demonstrating better compliance handling.", "conclusion": "CompliBench enhances LLM evaluation for compliance tasks and lays the groundwork for tools aligned with legal standards. It shows potential for improving data protection compliance in software systems."}}
{"id": "2511.00516", "pdf": "https://arxiv.org/pdf/2511.00516", "abs": "https://arxiv.org/abs/2511.00516", "authors": ["Peiyi Wang", "Paul A. M. Lefeuvre", "Shangwei Zou", "Zhenwei Ni", "Daniela Rus", "Cecilia Laschi"], "title": "Adaptive and Multi-object Grasping via Deformable Origami Modules", "categories": ["cs.RO"], "comment": null, "summary": "Soft robotics gripper have shown great promise in handling fragile and\ngeometrically complex objects. However, most existing solutions rely on bulky\nactuators, complex control strategies, or advanced tactile sensing to achieve\nstable and reliable grasping performance. In this work, we present a\nmulti-finger hybrid gripper featuring passively deformable origami modules that\ngenerate constant force and torque output. Each finger composed of parallel\norigami modules is driven by a 1-DoF actuator mechanism, enabling passive shape\nadaptability and stable grasping force without active sensing or feedback\ncontrol. More importantly, we demonstrate an interesting capability in\nsimultaneous multi-object grasping, which allows stacked objects of varied\nshape and size to be picked, transported and placed independently at different\nstates, significantly improving manipulation efficiency compared to\nsingle-object grasping. These results highlight the potential of origami-based\ncompliant structures as scalable modules for adaptive, stable and efficient\nmulti-object manipulation in domestic and industrial pick-and-place scenarios.", "AI": {"tldr": "The paper presents a soft robotics gripper with origami-based modules for improved grasping capabilities without complex sensing or control.", "motivation": "Existing soft robotic grippers struggle with bulky actuators, complicated control strategies, and require advanced sensors for stable grasping.", "method": "The gripper uses passively deformable origami modules in each finger driven by a simple 1-DoF actuator mechanism to achieve shape adaptability and stable force output without active sensing.", "result": "It achieves stable grasping and demonstrates simultaneous multi-object manipulation, significantly enhancing efficiency compared to single-object grasping.", "conclusion": "Origami-based compliant structures offer a scalable solution for adaptive, efficient, and stable multi-object manipulation in various applications."}}
{"id": "2511.00055", "pdf": "https://arxiv.org/pdf/2511.00055", "abs": "https://arxiv.org/abs/2511.00055", "authors": ["Leonhard Duda", "Khadijeh Alibabaei", "Elena Vollmer", "Leon Klug", "Valentin Kozlov", "Lisana Berberi", "Mishal Benz", "Rebekka Volk", "Juan Pedro Guti\u00e9rrez Hermosillo Muriedas", "Markus G\u00f6tz", "Judith S\u00e1\u00ednz-Pardo D\u00edaz", "\u00c1lvaro L\u00f3pez Garc\u00eda", "Frank Schultmann", "Achim Streit"], "title": "Exploring Federated Learning for Thermal Urban Feature Segmentation -- A Comparison of Centralized and Decentralized Approaches", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated Learning (FL) is an approach for training a shared Machine Learning\n(ML) model with distributed training data and multiple participants. FL allows\nbypassing limitations of the traditional Centralized Machine Learning CL if\ndata cannot be shared or stored centrally due to privacy or technical\nrestrictions -- the participants train the model locally with their training\ndata and do not need to share it among the other participants. This paper\ninvestigates the practical implementation and effectiveness of FL in a\nreal-world scenario, specifically focusing on unmanned aerial vehicle\n(UAV)-based thermal images for common thermal feature detection in urban\nenvironments. The distributed nature of the data arises naturally and makes it\nsuitable for FL applications, as images captured in two German cities are\navailable. This application presents unique challenges due to non-identical\ndistribution and feature characteristics of data captured at both locations.\nThe study makes several key contributions by evaluating FL algorithms in real\ndeployment scenarios rather than simulation. We compare several FL approaches\nwith a centralized learning baseline across key performance metrics such as\nmodel accuracy, training time, communication overhead, and energy usage. This\npaper also explores various FL workflows, comparing client-controlled workflows\nand server-controlled workflows. The findings of this work serve as a valuable\nreference for understanding the practical application and limitations of the FL\nmethods in segmentation tasks in UAV-based imaging.", "AI": {"tldr": "The paper explores the use of Federated Learning (FL) for UAV-based thermal image analysis, focusing on performance metrics and workflows in real-world scenarios.", "motivation": "To address the limitations of Centralized Learning in cases where data cannot be shared centrally due to privacy or technical issues, particularly in UAV-based urban imaging.", "method": "Evaluation of various FL algorithms using real-world thermal image data captured in two German cities, comparing them to centralized learning based on metrics such as accuracy, training time, communication overhead, and energy use.", "result": "Federated Learning algorithms were assessed against centralized learning baselines, offering insights into performance, workflows (client vs. server-controlled), and limitations in segmentation tasks for UAV thermal images.", "conclusion": "The study provides a practical examination of FL's capabilities and challenges, contributing to its understanding and highlighting its suitability for distributed UAV-based image segmentation."}}
{"id": "2511.00110", "pdf": "https://arxiv.org/pdf/2511.00110", "abs": "https://arxiv.org/abs/2511.00110", "authors": ["YingQiao Wang", "Eric Bigelow", "Boyi Li", "Tomer Ullman"], "title": "Chain of Time: In-Context Physical Simulation with Image Generation Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose a novel cognitively-inspired method to improve and interpret\nphysical simulation in vision-language models. Our ``Chain of Time\" method\ninvolves generating a series of intermediate images during a simulation, and it\nis motivated by in-context reasoning in machine learning, as well as mental\nsimulation in humans. Chain of Time is used at inference time, and requires no\nadditional fine-tuning. We apply the Chain-of-Time method to synthetic and\nreal-world domains, including 2-D graphics simulations and natural 3-D videos.\nThese domains test a variety of particular physical properties, including\nvelocity, acceleration, fluid dynamics, and conservation of momentum. We found\nthat using Chain-of-Time simulation substantially improves the performance of a\nstate-of-the-art image generation model. Beyond examining performance, we also\nanalyzed the specific states of the world simulated by an image model at each\ntime step, which sheds light on the dynamics underlying these simulations. This\nanalysis reveals insights that are hidden from traditional evaluations of\nphysical reasoning, including cases where an image generation model is able to\nsimulate physical properties that unfold over time, such as velocity, gravity,\nand collisions. Our analysis also highlights particular cases where the image\ngeneration model struggles to infer particular physical parameters from input\nimages, despite being capable of simulating relevant physical processes.", "AI": {"tldr": "A new method called \"Chain of Time\" improves and interprets physical simulations in vision-language models by generating intermediate images and requires no additional fine-tuning.", "motivation": "The paper is motivated by the need to enhance physical simulation in vision-language models and make their processes more interpretable, drawing inspiration from human mental simulation and in-context reasoning.", "method": "The \"Chain of Time\" method involves creating intermediate simulation images to improve understanding of physical dynamics like velocity and acceleration. It is applied at inference time without further fine-tuning.", "result": "The method significantly improves the performance of state-of-the-art image generation models in synthetic and real-world domains by accurately simulating complex physical properties.", "conclusion": "\"Chain of Time\" not only boosts the physical reasoning capabilities of models but also provides insights into how physical processes evolve over time, exposing strengths and weaknesses of the models."}}
{"id": "2511.00190", "pdf": "https://arxiv.org/pdf/2511.00190", "abs": "https://arxiv.org/abs/2511.00190", "authors": ["Andrea Macr\u00ec", "Sebastian Jaimungal", "Fabrizio Lillo"], "title": "Deep reinforcement learning for optimal trading with partial information", "categories": ["q-fin.TR", "q-fin.CP", "stat.ML"], "comment": null, "summary": "Reinforcement Learning (RL) applied to financial problems has been the\nsubject of a lively area of research. The use of RL for optimal trading\nstrategies that exploit latent information in the market is, to the best of our\nknowledge, not widely tackled. In this paper we study an optimal trading\nproblem, where a trading signal follows an Ornstein-Uhlenbeck process with\nregime-switching dynamics. We employ a blend of RL and Recurrent Neural\nNetworks (RNN) in order to make the most at extracting underlying information\nfrom the trading signal with latent parameters.\n  The latent parameters driving mean reversion, speed, and volatility are\nfiltered from observations of the signal, and trading strategies are derived\nvia RL. To address this problem, we propose three Deep Deterministic Policy\nGradient (DDPG)-based algorithms that integrate Gated Recurrent Unit (GRU)\nnetworks to capture temporal dependencies in the signal. The first, a one -step\napproach (hid-DDPG), directly encodes hidden states from the GRU into the RL\ntrader. The second and third are two-step methods: one (prob-DDPG) makes use of\nposterior regime probability estimates, while the other (reg-DDPG) relies on\nforecasts of the next signal value. Through extensive simulations with\nincreasingly complex Markovian regime dynamics for the trading signal's\nparameters, as well as an empirical application to equity pair trading, we find\nthat prob-DDPG achieves superior cumulative rewards and exhibits more\ninterpretable strategies. By contrast, reg-DDPG provides limited benefits,\nwhile hid-DDPG offers intermediate performance with less interpretable\nstrategies. Our results show that the quality and structure of the information\nsupplied to the agent are crucial: embedding probabilistic insights into latent\nregimes substantially improves both profitability and robustness of\nreinforcement learning-based trading strategies.", "AI": {"tldr": "The paper investigates using reinforcement learning (RL) and recurrent neural networks (RNN) for optimal trading strategies, emphasizing latent parameter estimation through Ornstein-Uhlenbeck process with regime-switching dynamics. Three RL-based models are proposed and analyzed.", "motivation": "To address the underexplored area of using reinforcement learning for developing optimal trading strategies that leverage latent market information, offering improved profitability and robustness.", "method": "The study integrates Deep Deterministic Policy Gradient (DDPG) algorithms with Gated Recurrent Unit (GRU) networks, introducing three approaches: hid-DDPG, prob-DDPG, and reg-DDPG to process latent trading signal parameters.", "result": "Prob-DDPG achieved the best cumulative rewards and interpretability of strategies, while reg-DDPG showed limited benefits, and hid-DDPG provided intermediate results with less interpretability.", "conclusion": "The study highlights the significance of embedding probabilistic insights into latent regimes to enhance the profitability and robustness of reinforcement learning-driven trading strategies."}}
{"id": "2511.00551", "pdf": "https://arxiv.org/pdf/2511.00551", "abs": "https://arxiv.org/abs/2511.00551", "authors": ["Qiang Li", "Ningjing Zeng", "Lina Yu"], "title": "Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Several studies have employed reinforcement learning (RL) to address the\nchallenges of regional adaptive traffic signal control (ATSC) and achieved\npromising results. In this field, existing research predominantly adopts\nmulti-agent frameworks. However, the adoption of multi-agent frameworks\npresents challenges for scalability. Instead, the Traffic signal control (TSC)\nproblem necessitates a single-agent framework. TSC inherently relies on\ncentralized management by a single control center, which can monitor traffic\nconditions across all roads in the study area and coordinate the control of all\nintersections. This work proposes a single-agent RL-based regional ATSC model\ncompatible with probe vehicle technology. Key components of the RL design\ninclude state, action, and reward function definitions. To facilitate learning\nand manage congestion, both state and reward functions are defined based on\nqueue length, with action designed to regulate queue dynamics. The queue length\ndefinition used in this study differs slightly from conventional definitions\nbut is closely correlated with congestion states. More importantly, it allows\nfor reliable estimation using link travel time data from probe vehicles. With\nprobe vehicle data already covering most urban roads, this feature enhances the\nproposed method's potential for widespread deployment. The method was\ncomprehensively evaluated using the SUMO simulation platform. Experimental\nresults demonstrate that the proposed model effectively mitigates large-scale\nregional congestion levels via coordinated multi-intersection control.", "AI": {"tldr": "The paper introduces a single-agent reinforcement learning model for regional adaptive traffic signal control (ATSC), leveraging probe vehicle data for scalability and effectiveness.", "motivation": "Current traffic signal control studies primarily use multi-agent frameworks, but these face scalability challenges. Traffic signal management requires centralized control due to its dependency on comprehensive monitoring and coordination.", "method": "A single-agent RL-based ATSC model was designed with probe vehicle technology. State and reward functions were based on queue length, and actions were determined to regulate congestion dynamics. The queue length definition was adjusted for reliable estimation using probe vehicle travel time data.", "result": "The proposed method was tested using SUMO simulations, showing effective reduction of large-scale regional congestion via coordinated multi-intersection control.", "conclusion": "The single-agent RL approach, utilizing probe vehicle data, proves to be an effective and scalable solution for regional traffic signal control in urban environments."}}
{"id": "2511.00279", "pdf": "https://arxiv.org/pdf/2511.00279", "abs": "https://arxiv.org/abs/2511.00279", "authors": ["Meituan LongCat Team", "Bairui Wang", "Bayan", "Bin Xiao", "Bo Zhang", "Bolin Rong", "Borun Chen", "Chang Wan", "Chao Zhang", "Chen Huang", "Chen Chen", "Chen Chen", "Chengxu Yang", "Chengzuo Yang", "Cong Han", "Dandan Peng", "Delian Ruan", "Detai Xin", "Disong Wang", "Dongchao Yang", "Fanfan Liu", "Fengjiao Chen", "Fengyu Yang", "Gan Dong", "Gang Huang", "Gang Xu", "Guanglu Wan", "Guoqiang Tan", "Guoqiao Yu", "Haibo Qiu", "Hao Lu", "Hongbo Liu", "Hongyu Xiang", "Jiaheng Wu", "Jian Yang", "Jiaxing Liu", "Jing Huang", "Jingang Wang", "Jinrui Ding", "Juchao Jiang", "Jun Kuang", "Jun Wang", "Junhui Mei", "Ke Ding", "Kefeng Zhang", "Lei Chen", "Liang Shi", "Limeng Qiao", "Liming Zheng", "Lin Ma", "Liuyang Guo", "Liya Ma", "Luying Sun", "Man Gao", "Mengshen Zhu", "Miao Cao", "Minliang Lin", "Nuo Xu", "Peng Shi", "Qi Zhang", "Qian Fang", "Qian Wang", "Qian Yang", "Quanxiu Wang", "Rongxiang Weng", "Rongxin Guo", "Ruoxuan Liang", "Senbin Yang", "Shanbo Xu", "Shanglin Lei", "Shengze Ye", "Shimin Chen", "Shuaiqi Chen", "Shujie Hu", "Shuo Li", "Siqi Yang", "Siyu Xu", "Siyu Ren", "Song Li", "Songxiang Liu", "Tianhao Bai", "Tianye Dai", "Wei Hong", "Wei Wang", "Weixiao Zhao", "Wengang Cao", "Wenlong Zhu", "Wenlong He", "Xi Su", "Xi Nan", "Xiaohan Zhao", "Xiaohao Wang", "Xiaoyu Zhao", "Xiaoyu Wang", "Xiaoyu Li", "Xin Pan", "Xin Chen", "Xiusong Sun", "Xu Xiang", "Xudong Xing", "Xuezhi Cao", "Xunliang Cai", "Yang Yang", "Yanli Tan", "Yao Yao", "Yerui Sun", "Yi Chen", "Yifan Lu", "Yin Gong", "Yining Zhang", "Yitian Chen", "Yiyang Gan", "Yuchen Tang", "Yuchen Xie", "Yueqian Wang", "Yuewen Zheng", "Yufei Zhang", "Yufeng Zhong", "Yulei Qian", "Yuqi Peng", "Yuwei Jiang", "Zeyang Hu", "Zheng Zhang", "Zhengkun Tian", "Zhiqing Hong", "Zhixiong Zeng", "Zhuqi Mi", "Ziran Li", "Ziwen Wang", "Ziyi Zhao", "Ziyuan Zhuang", "Zizhe Zhao"], "title": "LongCat-Flash-Omni Technical Report", "categories": ["cs.MM", "cs.AI", "cs.CL", "cs.DC", "cs.LG", "cs.SD"], "comment": null, "summary": "We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal\nmodel with 560 billion parameters, excelling at real-time audio-visual\ninteraction. By adopting a curriculum-inspired progressive training strategy\nthat transitions from simpler to increasingly complex modality sequence\nmodeling tasks, LongCat-Flash-Omni attains comprehensive multimodal\ncapabilities while maintaining strong unimodal capability. Building upon\nLongCat-Flash, which adopts a high-performance Shortcut-connected\nMixture-of-Experts (MoE) architecture with zero-computation experts,\nLongCat-Flash-Omni integrates efficient multimodal perception and speech\nreconstruction modules. Despite its immense size of 560B parameters (with 27B\nactivated), LongCat-Flash-Omni achieves low-latency real-time audio-visual\ninteraction. For training infrastructure, we developed a modality-decoupled\nparallelism scheme specifically designed to manage the data and model\nheterogeneity inherent in large-scale multimodal training. This innovative\napproach demonstrates exceptional efficiency by sustaining over 90% of the\nthroughput achieved by text-only training. Extensive evaluations show that\nLongCat-Flash-Omni achieves state-of-the-art performance on omni-modal\nbenchmarks among open-source models. Furthermore, it delivers highly\ncompetitive results across a wide range of modality-specific tasks, including\ntext, image, and video understanding, as well as audio understanding and\ngeneration. We provide a comprehensive overview of the model architecture\ndesign, training procedures, and data strategies, and open-source the model to\nfoster future research and development in the community.", "AI": {"tldr": "LongCat-Flash-Omni is an open-source, state-of-the-art omni-modal AI model with 560 billion parameters, excelling in real-time audio-visual interaction and multimodal tasks.", "motivation": "Aimed at advancing multimodal models to achieve high-performance omni-modal interactions with real-time capabilities while supporting diverse tasks across modalities.", "method": "Introduces a curriculum-inspired training strategy, a Shortcut-connected Mixture-of-Experts architecture, and modality-decoupled parallelism for efficient large-scale multimodal training.", "result": "Achieves state-of-the-art performance on omni-modal benchmarks, high efficiency in training throughput, and competitive results across text, image, video, and audio-related tasks.", "conclusion": "Proves the feasibility and efficiency of large-scale omni-modal models while offering its design and implementation as open-source for the research community."}}
{"id": "2511.00486", "pdf": "https://arxiv.org/pdf/2511.00486", "abs": "https://arxiv.org/abs/2511.00486", "authors": ["Pooja Singh", "Shashwat Bhardwaj", "Vaibhav Sharma", "Sandeep Kumar"], "title": "Leveraging the Cross-Domain & Cross-Linguistic Corpus for Low Resource NMT: A Case Study On Bhili-Hindi-English Parallel Corpus", "categories": ["cs.CL"], "comment": "Accepted in EMNLP 2025", "summary": "The linguistic diversity of India poses significant machine translation\nchallenges, especially for underrepresented tribal languages like Bhili, which\nlack high-quality linguistic resources. This paper addresses the gap by\nintroducing Bhili-Hindi-English Parallel Corpus (BHEPC), the first and largest\nparallel corpus worldwide comprising 110,000 meticulously curated sentences\nacross Bhili, Hindi, and English. The corpus was created with the assistance of\nexpert human translators. BHEPC spans critical domains such as education,\nadministration, and news, establishing a valuable benchmark for research in low\nresource machine translation. To establish a comprehensive Bhili Machine\nTranslation benchmark, we evaluated a wide range of proprietary and open-source\nMultilingual Large Language Models (MLLMs) on bidirectional translation tasks\nbetween English/Hindi and Bhili. Comprehensive evaluation demonstrates that the\nfine-tuned NLLB-200 distilled 600M variant model outperforms others,\nhighlighting the potential of multilingual models in low resource scenarios.\nFurthermore, we investigated the generative translation capabilities of\nmultilingual LLMs on BHEPC using in-context learning, assessing performance\nunder cross-domain generalization and quantifying distributional divergence.\nThis work bridges a critical resource gap and promotes inclusive natural\nlanguage processing technologies for low-resource and marginalized languages\nglobally.", "AI": {"tldr": "Introducing the BHEPC corpus for Bhili-Hindi-English translations with 110,000 curated sentences and evaluating machine translation models for Bhili language.", "motivation": "Address the lack of machine translation resources for underrepresented Indian tribal languages like Bhili.", "method": "The paper constructed a Bhili-Hindi-English parallel corpus and evaluated multilingual LLMs using fine-tuning and in-context learning.", "result": "The fine-tuned NLLB-200 model performed best in bidirectional translation tasks, demonstrating the capability of multilingual models in handling low-resource languages.", "conclusion": "This study fills the resource gap for Bhili machine translation, providing benchmarks and advancing multilingual NLP technologies for marginalized languages."}}
{"id": "2511.00658", "pdf": "https://arxiv.org/pdf/2511.00658", "abs": "https://arxiv.org/abs/2511.00658", "authors": ["Guilherme H. Travassos", "Sabrina Rocha", "Rodrigo Feitosa", "Felipe Assis", "Patricia Goncalves", "Andre Gheventer", "Larissa Galeno", "Arthur Sasse", "Julio Cesar Guimaraes", "Carlos Brito", "Joao Pedro Wieland"], "title": "Lessons Learned from the Use of Generative AI in Engineering and Quality Assurance of a WEB System for Healthcare", "categories": ["cs.SE", "cs.AI", "cs.ET"], "comment": "11 pages, 2 figures, in Portuguese language", "summary": "The advances and availability of technologies involving Generative Artificial\nIntelligence (AI) are evolving clearly and explicitly, driving immediate\nchanges in various work activities. Software Engineering (SE) is no exception\nand stands to benefit from these new technologies, enhancing productivity and\nquality in its software development processes. However, although the use of\nGenerative AI in SE practices is still in its early stages, considering the\nlack of conclusive results from ongoing research and the limited technological\nmaturity, we have chosen to incorporate these technologies in the development\nof a web-based software system to be used in clinical trials by a thoracic\ndiseases research group at our university. For this reason, we decided to share\nthis experience report documenting our development team's learning journey in\nusing Generative AI during the software development process. Project\nmanagement, requirements specification, design, development, and quality\nassurance activities form the scope of observation. Although we do not yet have\ndefinitive technological evidence to evolve our development process\nsignificantly, the results obtained and the suggestions shared here represent\nvaluable insights for software organizations seeking to innovate their\ndevelopment practices to achieve software quality with generative AI.", "AI": {"tldr": "The paper explores the implementation of Generative AI technologies in software development, specifically within clinical trials settings, emphasizing learning and insights from initial integration into the development process.", "motivation": "To leverage Generative AI in software engineering for enhancing productivity and quality in clinical trials software development.", "method": "Documenting experiences and observations from incorporating Generative AI in project management, requirements specification, design, development, and quality assurance of a web-based system.", "result": "Early results and lessons learned provide valuable suggestions for organizations considering integrating Generative AI for software development.", "conclusion": "Generative AI holds potential for improving SE practices, though its technological maturity and definitive evidence are in early stages. The insights shared help guide innovative development approaches using AI."}}
{"id": "2511.00555", "pdf": "https://arxiv.org/pdf/2511.00555", "abs": "https://arxiv.org/abs/2511.00555", "authors": ["Dianye Huang", "Nassir Navab", "Zhongliang Jiang"], "title": "Improving Robustness to Out-of-Distribution States in Imitation Learning via Deep Koopman-Boosted Diffusion Policy", "categories": ["cs.RO"], "comment": "Accepted by IEEE T-RO", "summary": "Integrating generative models with action chunking has shown significant\npromise in imitation learning for robotic manipulation. However, the existing\ndiffusion-based paradigm often struggles to capture strong temporal\ndependencies across multiple steps, particularly when incorporating\nproprioceptive input. This limitation can lead to task failures, where the\npolicy overfits to proprioceptive cues at the expense of capturing the visually\nderived features of the task. To overcome this challenge, we propose the Deep\nKoopman-boosted Dual-branch Diffusion Policy (D3P) algorithm. D3P introduces a\ndual-branch architecture to decouple the roles of different sensory modality\ncombinations. The visual branch encodes the visual observations to indicate\ntask progression, while the fused branch integrates both visual and\nproprioceptive inputs for precise manipulation. Within this architecture, when\nthe robot fails to accomplish intermediate goals, such as grasping a drawer\nhandle, the policy can dynamically switch to execute action chunks generated by\nthe visual branch, allowing recovery to previously observed states and\nfacilitating retrial of the task. To further enhance visual representation\nlearning, we incorporate a Deep Koopman Operator module that captures\nstructured temporal dynamics from visual inputs. During inference, we use the\ntest-time loss of the generative model as a confidence signal to guide the\naggregation of the temporally overlapping predicted action chunks, thereby\nenhancing the reliability of policy execution. In simulation experiments across\nsix RLBench tabletop tasks, D3P outperforms the state-of-the-art diffusion\npolicy by an average of 14.6\\%. On three real-world robotic manipulation tasks,\nit achieves a 15.0\\% improvement. Code: https://github.com/dianyeHuang/D3P.", "AI": {"tldr": "The D3P algorithm introduces a dual-branch diffusion policy integrating Deep Koopman Operators to improve robotic manipulation tasks, enhancing temporal dependencies and visual representation learning.", "motivation": "Existing diffusion-based generative models in imitation learning struggle with capturing strong temporal dependencies and often over-rely on proprioceptive cues, leading to failures in robotic manipulation tasks.", "method": "Proposes D3P, a dual-branch architecture that divides roles: the visual branch focuses on encoding task progression, while the fused branch integrates visual and proprioceptive inputs. Deep Koopman Operator enhances visual temporal dynamics, and test-time confidence guides task retries.", "result": "D3P showed a 14.6% improvement over state-of-the-art methods in simulated RLBench tasks and a 15.0% improvement in real-world robotic manipulation tasks.", "conclusion": "The incorporation of a dual-branch policy and Deep Koopman Operators significantly improves reliability, recovery strategies, and overall performance in robotic manipulation. The code is publicly available."}}
{"id": "2511.00056", "pdf": "https://arxiv.org/pdf/2511.00056", "abs": "https://arxiv.org/abs/2511.00056", "authors": ["Yuxi Liu", "Renjia Deng", "Yutong He", "Xue Wang", "Tao Yao", "Kun Yuan"], "title": "MISA: Memory-Efficient LLMs Optimization with Module-wise Importance Sampling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The substantial memory demands of pre-training and fine-tuning large language\nmodels (LLMs) require memory-efficient optimization algorithms. One promising\napproach is layer-wise optimization, which treats each transformer block as a\nsingle layer and optimizes it sequentially, while freezing the other layers to\nsave optimizer states and activations. Although effective, these methods ignore\nthe varying importance of the modules within each layer, leading to suboptimal\nperformance. Moreover, layer-wise sampling provides only limited memory\nsavings, as at least one full layer must remain active during optimization. To\novercome these limitations, we propose Module-wise Importance SAmpling (MISA),\na novel method that divides each layer into smaller modules and assigns\nimportance scores to each module. MISA uses a weighted random sampling\nmechanism to activate modules, provably reducing gradient variance compared to\nlayer-wise sampling. Additionally, we establish an \\(\\mathcal{O}(1/\\sqrt{K})\\)\nconvergence rate under non-convex and stochastic conditions, where $K$ is the\ntotal number of block updates, and provide a detailed memory analysis\nshowcasing MISA's superiority over existing baseline methods. Experiments on\ndiverse learning tasks validate the effectiveness of MISA. Source code is\navailable at https://github.com/pkumelon/MISA.", "AI": {"tldr": "MISA is a method to improve memory efficiency and performance in optimizing large language models by dividing layers into smaller modules and applying importance sampling.", "motivation": "Existing layer-wise optimization methods for large language models are memory-efficient but suboptimal in performance as they overlook the varying importance of modules within each layer.", "method": "MISA divides transformer layers into smaller modules, assigns importance scores, and uses weighted random sampling to activate modules, reducing gradient variance and optimizing memory usage.", "result": "MISA achieves a convergence rate of \\(\\mathcal{O}(1/\\sqrt{K})\\), demonstrates reduced memory usage compared to traditional methods, and shows effectiveness across various learning tasks according to experimental results.", "conclusion": "MISA addresses the limitations of traditional layer-wise optimization techniques in large language models by providing a more granular and memory-efficient optimization process, validated theoretically and experimentally."}}
{"id": "2511.00114", "pdf": "https://arxiv.org/pdf/2511.00114", "abs": "https://arxiv.org/abs/2511.00114", "authors": ["Hanae Elmekki", "Amanda Spilkin", "Ehsan Zakeri", "Antonela Mariel Zanuttini", "Ahmed Alagha", "Hani Sami", "Jamal Bentahar", "Lyes Kadem", "Wen-Fang Xie", "Philippe Pibarot", "Rabeb Mizouni", "Hadi Otrok", "Azzam Mourad", "Sami Muhaidat"], "title": "End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Cardiac ultrasound (US) is among the most widely used diagnostic tools in\ncardiology for assessing heart health, but its effectiveness is limited by\noperator dependence, time constraints, and human error. The shortage of trained\nprofessionals, especially in remote areas, further restricts access. These\nissues underscore the need for automated solutions that can ensure consistent,\nand accessible cardiac imaging regardless of operator skill or location. Recent\nprogress in artificial intelligence (AI), especially in deep reinforcement\nlearning (DRL), has gained attention for enabling autonomous decision-making.\nHowever, existing DRL-based approaches to cardiac US scanning lack\nreproducibility, rely on proprietary data, and use simplified models. Motivated\nby these gaps, we present the first end-to-end framework that integrates\ngenerative AI and DRL to enable autonomous and reproducible cardiac US\nscanning. The framework comprises two components: (i) a conditional generative\nsimulator combining Generative Adversarial Networks (GANs) with Variational\nAutoencoders (VAEs), that models the cardiac US environment producing realistic\naction-conditioned images; and (ii) a DRL module that leverages this simulator\nto learn autonomous, accurate scanning policies. The proposed framework\ndelivers AI-driven guidance through expert-validated models that classify image\ntype and assess quality, supports conditional generation of realistic US\nimages, and establishes a reproducible foundation extendable to other organs.\nTo ensure reproducibility, a publicly available dataset of real cardiac US\nscans is released. The solution is validated through several experiments. The\nVAE-GAN is benchmarked against existing GAN variants, with performance assessed\nusing qualitative and quantitative approaches, while the DRL-based scanning\nsystem is evaluated under varying configurations to demonstrate effectiveness.", "AI": {"tldr": "The paper introduces a novel AI-based framework incorporating generative models (GANs and VAEs) and deep reinforcement learning (DRL) for autonomous cardiac ultrasound scanning, addressing reproducibility, operator dependency, and accessibility issues.", "motivation": "To overcome operator dependency, limited access, and variability in cardiac ultrasound scanning due to skill and geographical constraints, and to ensure consistent, accessible, and reproducible automatic imaging.", "method": "Developing an end-to-end framework using a conditional generative simulator (GAN-VAE) to model realistic cardiac environments and a DRL module for learning scanning policies, supported by expert-validated AI-driven guidance.", "result": "The framework successfully integrates generative AI and DRL, enabling realistic simulation and autonomous guidance for cardiac ultrasound scanning. Validation with experiments against existing models showed effective and reproducible results.", "conclusion": "This framework pioneers realistic, reproducible, and autonomous cardiac ultrasound scanning, offering a benchmarked solution, a released dataset, and potential application to other medical imaging domains."}}
{"id": "2511.00203", "pdf": "https://arxiv.org/pdf/2511.00203", "abs": "https://arxiv.org/abs/2511.00203", "authors": ["David L\u00fcdke", "Tom Wollschl\u00e4ger", "Paul Ungermann", "Stephan G\u00fcnnemann", "Leo Schwinn"], "title": "Diffusion LLMs are Natural Adversaries for any LLM", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We introduce a novel framework that transforms the resource-intensive\n(adversarial) prompt optimization problem into an \\emph{efficient, amortized\ninference task}. Our core insight is that pretrained, non-autoregressive\ngenerative LLMs, such as Diffusion LLMs, which model the joint distribution\nover prompt-response pairs, can serve as powerful surrogates for prompt search.\nThis approach enables the direct conditional generation of prompts, effectively\nreplacing costly, per-instance discrete optimization with a small number of\nparallelizable samples. We provide a probabilistic analysis demonstrating that\nunder mild fidelity assumptions, only a few conditional samples are required to\nrecover high-reward (harmful) prompts. Empirically, we find that the generated\nprompts are low-perplexity, diverse jailbreaks that exhibit strong\ntransferability to a wide range of black-box target models, including robustly\ntrained and proprietary LLMs. Beyond adversarial prompting, our framework opens\nnew directions for red teaming, automated prompt optimization, and leveraging\nemerging Flow- and Diffusion-based LLMs.", "AI": {"tldr": "The paper proposes an efficient framework using non-autoregressive generative LLMs, like Diffusion LLMs, for prompt optimization tasks, eliminating resource-intensive adversarial search.", "motivation": "The motivation is to address the resource-heavy problem of adversarial prompt optimization, replacing discrete optimization with an efficient inference task using generative LLMs.", "method": "The method involves using Diffusion LLMs to model joint distributions of prompt-response pairs for efficient direct conditional prompt generation and probabilistic analysis to recover high-reward prompts with few conditional samples.", "result": "The framework produces low-perplexity, diverse adversarial prompts with strong transferability to robust and proprietary models, facilitating wide applications in black-box environments.", "conclusion": "This approach improves adversarial prompting efficiency and opens up new opportunities in red teaming, automated optimization, and leveraging advanced generative LLM capabilities."}}
{"id": "2511.00609", "pdf": "https://arxiv.org/pdf/2511.00609", "abs": "https://arxiv.org/abs/2511.00609", "authors": ["Shengqi Xu", "Xinpeng Zhou", "Yabo Zhang", "Ming Liu", "Tao Liang", "Tianyu Zhang", "Yalong Bai", "Zuxuan Wu", "Wangmeng Zuo"], "title": "PreferThinker: Reasoning-based Personalized Image Preference Assessment", "categories": ["cs.AI"], "comment": null, "summary": "Personalized image preference assessment aims to evaluate an individual\nuser's image preferences by relying only on a small set of reference images as\nprior information. Existing methods mainly focus on general preference\nassessment, training models with large-scale data to tackle well-defined tasks\nsuch as text-image alignment. However, these approaches struggle to handle\npersonalized preference because user-specific data are scarce and not easily\nscalable, and individual tastes are often diverse and complex. To overcome\nthese challenges, we introduce a common preference profile that serves as a\nbridge across users, allowing large-scale user data to be leveraged for\ntraining profile prediction and capturing complex personalized preferences.\nBuilding on this idea, we propose a reasoning-based personalized image\npreference assessment framework that follows a \\textit{predict-then-assess}\nparadigm: it first predicts a user's preference profile from reference images,\nand then provides interpretable, multi-dimensional scores and assessments of\ncandidate images based on the predicted profile. To support this, we first\nconstruct a large-scale Chain-of-Thought (CoT)-style personalized assessment\ndataset annotated with diverse user preference profiles and high-quality\nCoT-style reasoning, enabling explicit supervision of structured reasoning.\nNext, we adopt a two-stage training strategy: a cold-start supervised\nfine-tuning phase to empower the model with structured reasoning capabilities,\nfollowed by reinforcement learning to incentivize the model to explore more\nreasonable assessment paths and enhance generalization. Furthermore, we propose\na similarity-aware prediction reward to encourage better prediction of the\nuser's preference profile, which facilitates more reasonable assessments\nexploration. Extensive experiments demonstrate the superiority of the proposed\nmethod.", "AI": {"tldr": "The paper proposes a framework to assess personalized image preferences using reference images, tackling challenges of scarce and complex user data by introducing a common preference profile and reasoning-based assessment.", "motivation": "Existing methods struggle with personalized image preference assessment due to scarce data and diverse individual tastes. This paper aims to overcome such challenges.", "method": "A predict-then-assess paradigm is introduced: a model predicts a user's preference profile, then provides interpretable assessments based on the profile. It uses a large-scale annotated dataset, cold-start fine-tuning, reinforcement learning, and similarity-aware prediction rewards.", "result": "Experiments show the method's superior performance in assessing personalized image preferences compared to existing approaches.", "conclusion": "The proposed framework effectively leverages user data to handle complex personalized preference assessment, improving interpretability and generalization."}}
{"id": "2511.00336", "pdf": "https://arxiv.org/pdf/2511.00336", "abs": "https://arxiv.org/abs/2511.00336", "authors": ["Siva Sai", "Manish Prasad", "Animesh Bhargava", "Vinay Chamola", "Rajkumar Buyya"], "title": "Split Learning-Enabled Framework for Secure and Light-weight Internet of Medical Things Systems", "categories": ["cs.CR", "cs.DC", "cs.LG"], "comment": "11 pages, 5 figures, Under review in an IEEE Transactions journal", "summary": "The rapid growth of Internet of Medical Things (IoMT) devices has resulted in\nsignificant security risks, particularly the risk of malware attacks on\nresource-constrained devices. Conventional deep learning methods are\nimpractical due to resource limitations, while Federated Learning (FL) suffers\nfrom high communication overhead and vulnerability to non-IID (heterogeneous)\ndata. In this paper, we propose a split learning (SL) based framework for IoT\nmalware detection through image-based classification. By dividing the neural\nnetwork training between the clients and an edge server, the framework reduces\ncomputational burden on resource-constrained clients while ensuring data\nprivacy. We formulate a joint optimization problem that balances computation\ncost and communication efficiency by using a game-theoretic approach for\nattaining better training performance. Experimental evaluations show that the\nproposed framework outperforms popular FL methods in terms of accuracy\n(+6.35%), F1-score (+5.03%), high convergence speed (+14.96%), and less\nresource consumption (33.83%). These results establish the potential of SL as a\nscalable and secure paradigm for next-generation IoT security.", "AI": {"tldr": "The paper presents a split learning framework for IoT malware detection, addressing limitations like low resource availability and challenges in federated learning.", "motivation": "The increasing use of IoMT devices has led to security risks, including malware targeting resource-constrained systems. Existing solutions like conventional deep learning and FL methods are inadequately suited for these scenarios.", "method": "The proposed method uses split learning (SL) with image-based classification to divide neural network training between clients and an edge server. It includes a game-theoretic optimization for balancing computational and communication demands.", "result": "The framework achieves higher accuracy (+6.35%), better F1-score (+5.03%), faster convergence (+14.96%), and reduced resource usage (33.83%) compared to FL methods.", "conclusion": "Split learning is demonstrated as an effective, scalable, and secure approach for enhancing IoT security framework performance against malware attacks."}}
{"id": "2511.00487", "pdf": "https://arxiv.org/pdf/2511.00487", "abs": "https://arxiv.org/abs/2511.00487", "authors": ["Stephen Meisenbacher", "Florian Matthes"], "title": "With Privacy, Size Matters: On the Importance of Dataset Size in Differentially Private Text Rewriting", "categories": ["cs.CL"], "comment": "11 pages, 1 figure, 5 tables. Accepted to IJCNLP-AACL 2025 (Main)", "summary": "Recent work in Differential Privacy with Natural Language Processing (DP NLP)\nhas proposed numerous promising techniques in the form of text rewriting\nmechanisms. In the evaluation of these mechanisms, an often-ignored aspect is\nthat of dataset size, or rather, the effect of dataset size on a mechanism's\nefficacy for utility and privacy preservation. In this work, we are the first\nto introduce this factor in the evaluation of DP text privatization, where we\ndesign utility and privacy tests on large-scale datasets with dynamic split\nsizes. We run these tests on datasets of varying size with up to one million\ntexts, and we focus on quantifying the effect of increasing dataset size on the\nprivacy-utility trade-off. Our findings reveal that dataset size plays an\nintegral part in evaluating DP text rewriting mechanisms; additionally, these\nfindings call for more rigorous evaluation procedures in DP NLP, as well as\nshed light on the future of DP NLP in practice and at scale.", "AI": {"tldr": "The paper investigates the impact of dataset size on privacy-utility trade-offs in Differential Privacy text rewriting mechanisms, conducting evaluations on datasets containing up to one million texts.", "motivation": "Previous work in Differential Privacy for NLP has overlooked the influence of dataset size on both the effectiveness and balance between utility and privacy preservation.", "method": "The authors conducted utility and privacy tests on large-scale datasets of varying sizes, using dynamic split sizes to analyze the effects of dataset scale on DP text rewriting mechanisms.", "result": "The analysis shows that dataset size significantly influences the privacy-utility trade-offs of DP NLP mechanisms, emphasizing the need for more comprehensive evaluation frameworks.", "conclusion": "Evaluations of DP NLP mechanisms must consider dataset size to improve assessment accuracy, and the findings push toward better evaluation protocols and practical applications at scale."}}
{"id": "2511.00678", "pdf": "https://arxiv.org/pdf/2511.00678", "abs": "https://arxiv.org/abs/2511.00678", "authors": ["Tasmia Zerin", "Moumita Asad", "B. M. Mainul Hossain", "Kazi Sakib"], "title": "Repairing Responsive Layout Failures Using Retrieval Augmented Generation", "categories": ["cs.SE"], "comment": "Accepted at the 41st IEEE International Conference on Software\n  Maintenance and Evolution 2025 (ICSME'25)", "summary": "Responsive websites frequently experience distorted layouts at specific\nscreen sizes, called Responsive Layout Failures (RLFs). Manually repairing\nthese RLFs involves tedious trial-and-error adjustments of HTML elements and\nCSS properties. In this study, an automated repair approach, leveraging LLM\ncombined with domain-specific knowledge is proposed. The approach is named\nReDeFix, a Retrieval-Augmented Generation (RAG)-based solution that utilizes\nStack Overflow (SO) discussions to guide LLM on CSS repairs. By augmenting\nrelevant SO knowledge with RLF-specific contexts, ReDeFix creates a prompt that\nis sent to the LLM to generate CSS patches. Evaluation demonstrates that our\napproach achieves an 88\\% accuracy in repairing RLFs. Furthermore, a study from\nsoftware engineers reveals that generated repairs produce visually correct\nlayouts while maintaining aesthetics.", "AI": {"tldr": "ReDeFix, an automated solution, uses Stack Overflow knowledge and large language models (LLMs) to fix Responsive Layout Failures (RLFs) in websites.", "motivation": "Manually repairing distorted layouts in responsive websites is time-consuming and prone to errors, requiring an automated, efficient approach.", "method": "ReDeFix combines LLMs with Retrieval-Augmented Generation (RAG) using domain-specific Stack Overflow discussions to direct CSS repairs.", "result": "The proposed system achieved 88% accuracy in fixing RLFs and maintained visual correctness and aesthetics as confirmed by software engineers.", "conclusion": "ReDeFix effectively repairs responsive layout issues, leveraging community knowledge and automation to reduce the manual burden on developers."}}
{"id": "2511.00635", "pdf": "https://arxiv.org/pdf/2511.00635", "abs": "https://arxiv.org/abs/2511.00635", "authors": ["Hyungtae Lim", "Daebeom Kim", "Hyun Myung"], "title": "Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles", "categories": ["cs.RO"], "comment": "13 pages, 12 figures", "summary": "As various 3D light detection and ranging (LiDAR) sensors have been\nintroduced to the market, research on multi-session simultaneous localization\nand mapping (MSS) using heterogeneous LiDAR sensors has been actively\nconducted. Existing MSS methods mostly rely on loop closure detection for\ninter-session alignment; however, the performance of loop closure detection can\nbe potentially degraded owing to the differences in the density and field of\nview (FoV) of the sensors used in different sessions. In this study, we\nchallenge the existing paradigm that relies heavily on loop detection modules\nand propose a novel MSS framework, called Multi-Mapcher, that employs\nlarge-scale map-to-map registration to perform inter-session initial alignment,\nwhich is commonly assumed to be infeasible, by leveraging outlier-robust 3D\npoint cloud registration. Next, after finding inter-session loops by radius\nsearch based on the assumption that the inter-session initial alignment is\nsufficiently precise, anchor node-based robust pose graph optimization is\nemployed to build a consistent global map. As demonstrated in our experiments,\nour approach shows substantially better MSS performance for various LiDAR\nsensors used to capture the sessions and is faster than state-of-the-art\napproaches. Our code is available at\nhttps://github.com/url-kaist/multi-mapcher.", "AI": {"tldr": "The paper introduces Multi-Mapcher, a novel MSS framework leveraging map-to-map registration for inter-session alignment, improving performance for heterogeneous LiDAR sensors.", "motivation": "Address the limitations of existing MSS methods relying on loop closure detection, particularly for heterogeneous LiDAR sensors with differences in density and field of view.", "method": "Developing Multi-Mapcher for initial inter-session alignment using large-scale map-to-map registration and 3D point cloud algorithms, followed by robust pose graph optimization.", "result": "Achieved substantially better MSS performance and faster processing than state-of-the-art approaches across various LiDAR sensors.", "conclusion": "Proposed framework demonstrates feasibility and superiority in MSS applications with code freely available."}}
{"id": "2511.00059", "pdf": "https://arxiv.org/pdf/2511.00059", "abs": "https://arxiv.org/abs/2511.00059", "authors": ["Aditya Singh", "Zihang Wen", "Srujananjali Medicherla", "Adam Karvonen", "Can Rager"], "title": "Automatically Finding Rule-Based Neurons in OthelloGPT", "categories": ["cs.LG", "cs.AI"], "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop Mechanistic interpretability", "summary": "OthelloGPT, a transformer trained to predict valid moves in Othello, provides\nan ideal testbed for interpretability research. The model is complex enough to\nexhibit rich computational patterns, yet grounded in rule-based game logic that\nenables meaningful reverse-engineering. We present an automated approach based\non decision trees to identify and interpret MLP neurons that encode rule-based\ngame logic. Our method trains regression decision trees to map board states to\nneuron activations, then extracts decision paths where neurons are highly\nactive to convert them into human-readable logical forms. These descriptions\nreveal highly interpretable patterns; for instance, neurons that specifically\ndetect when diagonal moves become legal. Our findings suggest that roughly half\nof the neurons in layer 5 can be accurately described by compact, rule-based\ndecision trees ($R^2 > 0.7$ for 913 of 2,048 neurons), while the remainder\nlikely participate in more distributed or non-rule-based computations. We\nverify the causal relevance of patterns identified by our decision trees\nthrough targeted interventions. For a specific square, for specific game\npatterns, we ablate neurons corresponding to those patterns and find an\napproximately 5-10 fold stronger degradation in the model's ability to predict\nlegal moves along those patterns compared to control patterns. To facilitate\nfuture work, we provide a Python tool that maps rule-based game behaviors to\ntheir implementing neurons, serving as a resource for researchers to test\nwhether their interpretability methods recover meaningful computational\nstructures.", "AI": {"tldr": "OthelloGPT, a transformer for predicting valid moves in Othello, is used as a testbed for interpretability research. The paper introduces a method using decision trees to analyze neuron activations and reveals human-readable logical forms of these patterns.", "motivation": "To explore interpretability in complex models by focusing on rule-based game logic, providing insights into how models encode computational structures specifically suited to grounded tasks like Othello.", "method": "The authors use decision trees to map board states to MLP neuron activations, extracting logical forms from highly active neuron pathways. They also verify the causal relevance of identified patterns through targeted neuron interventions.", "result": "Approximately half of the neurons in layer 5 are described by rule-based decision trees with high accuracy. Ablation experiments show significant impact of these neurons on the model's prediction capabilities for specific game patterns.", "conclusion": "The study demonstrates that a substantial portion of MLP neurons in OthelloGPT encodes interpretable, rule-based game logic, offering tools for future interpretability research into structured computational processes."}}
{"id": "2511.00120", "pdf": "https://arxiv.org/pdf/2511.00120", "abs": "https://arxiv.org/abs/2511.00120", "authors": ["Md Selim Sarowar", "Sungho Kim"], "title": "VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images", "categories": ["cs.CV", "cs.AI"], "comment": "This paper has been accepted to IEIE( The Institute Of Electronics\n  and Information Engineering, South Korea) Fall,2025 Conference", "summary": "The primary challenge in computer vision is precisely calculating the pose of\n6D objects, however many current approaches are still fragile and have trouble\ngeneralizing from synthetic data to real-world situations with fluctuating\nlighting, textureless objects, and significant occlusions. To address these\nlimitations, VLM6D, a novel dual-stream architecture that leverages the\ndistinct strengths of visual and geometric data from RGB-D input for robust and\nprecise pose estimation. Our framework uniquely integrates two specialized\nencoders: a powerful, self-supervised Vision Transformer (DINOv2) processes the\nRGB modality, harnessing its rich, pre-trained understanding of visual grammar\nto achieve remarkable resilience against texture and lighting variations.\nConcurrently, a PointNet++ encoder processes the 3D point cloud derived from\ndepth data, enabling robust geometric reasoning that excels even with the\nsparse, fragmented data typical of severe occlusion. These complementary\nfeature streams are effectively fused to inform a multi task prediction head.\nWe demonstrate through comprehensive experiments that VLM6D obtained new SOTA\nperformance on the challenging Occluded-LineMOD, validating its superior\nrobustness and accuracy.", "AI": {"tldr": "VLM6D introduces a dual-stream architecture to address challenges in 6D pose estimation by combining RGB and 3D data for improved robustness against challenging real-world scenarios.", "motivation": "Current methods for 6D pose estimation struggle to adapt from synthetic to real-world datasets, especially under challenging conditions like fluctuating lighting, textureless objects, and occlusions.", "method": "VLM6D integrates a Vision Transformer (DINOv2) for processing RGB data and PointNet++ for 3D point cloud depth data, merging complementary feature streams to predict 6D poses through a multi-task head.", "result": "VLM6D achieves state-of-the-art performance on the Occluded-LineMOD benchmark, highlighting enhanced robustness and precision in pose estimation.", "conclusion": "The dual-stream approach combining visual and geometric inputs proves effective in tackling real-world pose estimation challenges, offering improved accuracy and resilience."}}
{"id": "2511.00257", "pdf": "https://arxiv.org/pdf/2511.00257", "abs": "https://arxiv.org/abs/2511.00257", "authors": ["Zachary Chase", "Shinji Ito", "Idan Mehalel"], "title": "A Tight Lower Bound for Non-stochastic Multi-armed Bandits with Expert Advice", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We determine the minimax optimal expected regret in the classic\nnon-stochastic multi-armed bandit with expert advice problem, by proving a\nlower bound that matches the upper bound of Kale (2014). The two bounds\ndetermine the minimax optimal expected regret to be $\\Theta\\left( \\sqrt{T K\n\\log (N/K) } \\right)$, where $K$ is the number of arms, $N$ is the number of\nexperts, and $T$ is the time horizon.", "AI": {"tldr": "This paper precisely determines the minimax optimal expected regret for the non-stochastic multi-armed bandit with expert advice problem.", "motivation": "To find the exact minimax optimal expected regret in the non-stochastic multi-armed bandit setup and establish bounds.", "method": "Lower bound derivation that matches the established upper bound from Kale (2014), ensuring tight regret estimation.", "result": "Minimax optimal expected regret is $\u0001\\Theta\\left(\\sqrt{T K \\log (N/K)}\\right)$ with parameters arms (K), experts (N), and time horizon (T).", "conclusion": "The established bounds confirm the precise minimax regret and validate theoretical understanding in this domain."}}
{"id": "2511.00640", "pdf": "https://arxiv.org/pdf/2511.00640", "abs": "https://arxiv.org/abs/2511.00640", "authors": ["Zicheng Xu", "Guanchu Wang", "Yu-Neng Chuang", "Guangyao Zheng", "Alexander S. Szalay", "Zirui Liu", "Vladimir Braverman"], "title": "DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large Reasoning Models (LRMs) demonstrate strong performance on complex\nreasoning tasks, yet they often suffer from overthinking, producing excessively\nlong chain-of-thought (CoT) traces that increase inference cost and may degrade\naccuracy. Our analysis reveals a clear anti-correlation between reasoning\nlength and accuracy, where across multiple stochastic decodes, the short\nreasoning paths consistently achieve the highest correctness, while longer ones\naccumulate errors and repetitions. These short optimal reasoning paths can be\nfound ideally through full enumeration of the reasoning space. However, the\ntree-structured reasoning space grows exponentially with sequence length,\nrendering exhaustive exploration infeasible. To address this, we propose DTS, a\nmodel-agnostic decoding framework that sketches the reasoning space by\nselectively branching at high-entropy tokens and applies early stopping to\nselect the shortest completed reasoning path. This approach approximates the\noptimal solution that enhances both efficiency and accuracy, without requiring\nadditional training or supervision. Experiments on AIME2024 and AIME2025\ndatasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves\naccuracy by up to 8%, reduces average reasoning length by 23%, and decreases\nrepetition frequency by 12%, demonstrating DTS's ability for scalable and\nefficient LRM reasoning.", "AI": {"tldr": "The paper addresses the overthinking issue in Large Reasoning Models (LRMs) that leads to unnecessarily long reasoning processes by introducing DTS, a decoding framework that identifies shorter and more accurate reasoning paths.", "motivation": "To reduce inference cost and improve accuracy of LRMs suffering from overthinking, which leads to unnecessarily long and error-prone reasoning chains.", "method": "DTS, a model-agnostic decoding framework, was introduced. DTS sketches reasoning space by branching selectively at high-entropy tokens and uses early stopping to identify the shortest correct reasoning paths without requiring extra training.", "result": "Experiments showed DTS improved accuracy by up to 8%, reduced reasoning length by 23%, and lowered repetition rates by 12% on AIME2024 and AIME2025 datasets.", "conclusion": "DTS offers a scalable, efficient solution for improving the reasoning performance and inference cost of LRMs, achieving better reasoning accuracy and efficiency without additional training requirements."}}
{"id": "2511.00489", "pdf": "https://arxiv.org/pdf/2511.00489", "abs": "https://arxiv.org/abs/2511.00489", "authors": ["Jiani Guo", "Zuchao Li", "Jie Wu", "Qianren Wang", "Yun Li", "Lefei Zhang", "Hai Zhao", "Yujiu Yang"], "title": "ToM: Leveraging Tree-oriented MapReduce for Long-Context Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": "EMNLP 2025 Main Conference", "summary": "Large Language Models (LLMs), constrained by limited context windows, often\nface significant performance degradation when reasoning over long contexts. To\naddress this, Retrieval-Augmented Generation (RAG) retrieves and reasons over\nchunks but frequently sacrifices logical coherence due to its reliance on\nsimilarity-based rankings. Similarly, divide-and-conquer frameworks (DCF) split\ndocuments into small chunks for independent reasoning and aggregation. While\neffective for local reasoning, DCF struggles to capture long-range dependencies\nand risks inducing conflicts by processing chunks in isolation. To overcome\nthese limitations, we propose ToM, a novel Tree-oriented MapReduce framework\nfor long-context reasoning. ToM leverages the inherent hierarchical structure\nof long documents (e.g., main headings and subheadings) by constructing a\nDocTree through hierarchical semantic parsing and performing bottom-up\naggregation. Using a Tree MapReduce approach, ToM enables recursive reasoning:\nin the Map step, rationales are generated at child nodes; in the Reduce step,\nthese rationales are aggregated across sibling nodes to resolve conflicts or\nreach consensus at parent nodes. Experimental results on 70B+ LLMs show that\nToM significantly outperforms existing divide-and-conquer frameworks and\nretrieval-augmented generation methods, achieving better logical coherence and\nlong-context reasoning. Our code is available at\nhttps://github.com/gjn12-31/ToM .", "AI": {"tldr": "This paper introduces ToM, a Tree-oriented MapReduce framework, enhancing logical coherence and long-context reasoning for large language models.", "motivation": "Addressing performance degradation in LLMs when reasoning over long contexts due to limited context windows and issues in existing frameworks.", "method": "The proposed ToM framework employs hierarchical semantic parsing to build a 'DocTree' and utilizes a recursive Tree MapReduce approach for reasoning, with bottom-up aggregation of rationales.", "result": "ToM demonstrated significant improvement in logical coherence and long-context reasoning, outperforming current frameworks and methods in experiments involving 70B+ LLMs.", "conclusion": "ToM's hierarchical structure and recursive reasoning methods resolve long-context reasoning limitations effectively, offering a novel advancement in handling large documents."}}
{"id": "2511.00706", "pdf": "https://arxiv.org/pdf/2511.00706", "abs": "https://arxiv.org/abs/2511.00706", "authors": ["Marcos Vinicius Cruz", "Pragya Verma", "Grischa Liebel"], "title": "An Empirical Investigation of the Experiences of Dyslexic Software Engineers", "categories": ["cs.SE"], "comment": null, "summary": "Dyslexia is a common learning disorder that primarily impairs an individual's\nreading and writing abilities. In adults, dyslexia can affect both professional\nand personal lives, often leading to mental challenges and difficulties\nacquiring and keeping work. In Software Engineering (SE), reading and writing\ndifficulties appear to pose substantial challenges for core tasks such as\nprogramming. However, initial studies indicate that these challenges may not\nsignificantly affect their performance compared to non-dyslexic colleagues.\nConversely, strengths associated with dyslexia could be particularly valuable\nin areas like programming and design. However, there is currently no work that\nexplores the experiences of dyslexic software engineers, and puts their\nstrengths into relation with their difficulties. To address this, we present a\nqualitative study of the experiences of dyslexic individuals in SE. We followed\nthe basic stage of the Socio-Technical Grounded Theory method and base our\nfindings on data collected through 10 interviews with dyslexic software\nengineers, 3 blog posts and 153 posts on the social media platform Reddit. We\nfind that dyslexic software engineers especially struggle at the programming\nlearning stage, but can succeed and indeed excel at many SE tasks once they\nmaster this step. Common SE-specific support tools, such as code completion and\nlinters are especially useful to these individuals and mitigate many of the\nexperienced difficulties. Finally, dyslexic software engineers exhibit\nstrengths in areas such as visual thinking and creativity. Our findings have\nimplications to SE practice and motivate several areas of future research in\nSE, such as investigating what makes code less/more understandable to dyslexic\nindividuals.", "AI": {"tldr": "This paper explores the experiences of dyslexic software engineers, emphasizing their struggles during programming learning stages and their strengths in visual thinking and creativity.", "motivation": "To understand how dyslexia influences software engineers' performance, challenges, and strengths, as well as to examine their unique experiences and needs in SE.", "method": "A qualitative study using Socio-Technical Grounded Theory with data collected from 10 interviews, 3 blog posts, and 153 Reddit posts related to dyslexic software engineers.", "result": "Dyslexic software engineers face difficulties during programming learning stages but excel in SE tasks once mastered. Support tools like code completion and linters mitigate their challenges while leveraging their strengths in visual thinking and creativity.", "conclusion": "The study highlights the need for tailored SE support tools and future research on code comprehension for dyslexic individuals. Dyslexic engineers can succeed and excel with targeted assistance and recognition of their unique strengths."}}
{"id": "2511.00783", "pdf": "https://arxiv.org/pdf/2511.00783", "abs": "https://arxiv.org/abs/2511.00783", "authors": ["Jingzehua Xu", "Weihang Zhang", "Yangyang Li", "Hongmiaoyi Zhang", "Guanwen Xie", "Jiwei Tang", "Shuai Zhang", "Yi Li"], "title": "When Semantics Connect the Swarm: LLM-Driven Fuzzy Control for Cooperative Multi-Robot Underwater Coverage", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "This paper has been submitted to IEEE Transactions on Mobile\n  Computing", "summary": "Underwater multi-robot cooperative coverage remains challenging due to\npartial observability, limited communication, environmental uncertainty, and\nthe lack of access to global localization. To address these issues, this paper\npresents a semantics-guided fuzzy control framework that couples Large Language\nModels (LLMs) with interpretable control and lightweight coordination. Raw\nmultimodal observations are compressed by the LLM into compact,\nhuman-interpretable semantic tokens that summarize obstacles, unexplored\nregions, and Objects Of Interest (OOIs) under uncertain perception. A fuzzy\ninference system with pre-defined membership functions then maps these tokens\ninto smooth and stable steering and gait commands, enabling reliable navigation\nwithout relying on global positioning. Then, we further coordinate multiple\nrobots by introducing semantic communication that shares intent and local\ncontext in linguistic form, enabling agreement on who explores where while\navoiding redundant revisits. Extensive simulations in unknown reef-like\nenvironments show that, under limited sensing and communication, the proposed\nframework achieves robust OOI-oriented navigation and cooperative coverage with\nimproved efficiency and adaptability, narrowing the gap between semantic\ncognition and distributed underwater control in GPS-denied, map-free\nconditions.", "AI": {"tldr": "The paper presents a semantic-guided fuzzy control framework for multi-robot underwater exploration, addressing challenges like partial observability, limited communication, and lack of global localization.", "motivation": "The study aims to overcome key challenges in underwater multi-robot cooperative coverage: limited sensing, communication, global localization, and handling environmental uncertainty.", "method": "It introduces a framework utilizing large language models (LLMs) to generate human-interpretable semantic tokens for navigation and a fuzzy inference system to produce control commands. Semantic communication ensures effective coordination between robots.", "result": "The proposed framework enables robust navigation and efficient cooperative coverage in simulated reef environments, allowing improved adaptability and efficiency under GPS-denied, map-free conditions.", "conclusion": "This approach bridges the gap between semantic cognition and distributed control in underwater environments, demonstrating practical and effective solutions for cooperative multi-robot exploration."}}
{"id": "2511.00064", "pdf": "https://arxiv.org/pdf/2511.00064", "abs": "https://arxiv.org/abs/2511.00064", "authors": ["Randolph Wiredu-Aidoo"], "title": "EVINGCA: Adaptive Graph Clustering with Evolving Neighborhood Statistics", "categories": ["cs.LG"], "comment": null, "summary": "Clustering algorithms often rely on restrictive assumptions: K-Means and\nGaussian Mixtures presuppose convex, Gaussian-like clusters, while DBSCAN and\nHDBSCAN capture non-convexity but can be highly sensitive. I introduce EVINGCA\n(Evolving Variance-Informed Nonparametric Graph Construction Algorithm), a\ndensity-variance based clustering algorithm that treats cluster formation as an\nadaptive, evolving process on a nearest-neighbor graph. EVINGCA expands rooted\ngraphs via breadth-first search, guided by continuously updated local distance\nand shape statistics, replacing fixed density thresholds with local statistical\nfeedback. With spatial indexing, EVINGCA features log-linear complexity in the\naverage case and exhibits competitive performance against baselines across a\nvariety of synthetic, real-world, low-d, and high-d datasets.", "AI": {"tldr": "A novel clustering algorithm called EVINGCA introduces adaptability and local statistical feedback to improve clustering reliability.", "motivation": "Traditional clustering algorithms rely on restrictive assumptions, such as requiring clusters to be convex or Gaussian-like, which limits their adaptability across diverse data types.", "method": "EVINGCA employs a density-variance approach combined with adaptive, evolving nearest-neighbor graph expansion, using local and dynamic statistics to guide cluster formation.", "result": "The algorithm exhibits log-linear average complexity and competitive clustering performance across synthetic, real-world, and high-dimensional datasets.", "conclusion": "EVINGCA offers an improved approach by optimizing computational efficiency and adapting cluster formation to data characteristics, making it suitable for various data sets."}}
{"id": "2511.00123", "pdf": "https://arxiv.org/pdf/2511.00123", "abs": "https://arxiv.org/abs/2511.00123", "authors": ["Gaby Maroun", "Salah Eddine Bekhouche", "Fadi Dornaika"], "title": "Integrating ConvNeXt and Vision Transformers for Enhancing Facial Age Estimation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Age estimation from facial images is a complex and multifaceted challenge in\ncomputer vision. In this study, we present a novel hybrid architecture that\ncombines ConvNeXt, a state-of-the-art advancement of convolutional neural\nnetworks (CNNs), with Vision Transformers (ViT). While each model independently\ndelivers excellent performance on a variety of tasks, their integration\nleverages the complementary strengths of the CNNs localized feature extraction\ncapabilities and the Transformers global attention mechanisms. Our proposed\nConvNeXt-ViT hybrid solution was thoroughly evaluated on benchmark age\nestimation datasets, including MORPH II, CACD, and AFAD, and achieved superior\nperformance in terms of mean absolute error (MAE). To address computational\nconstraints, we leverage pre-trained models and systematically explore\ndifferent configurations, using linear layers and advanced regularization\ntechniques to optimize the architecture. Comprehensive ablation studies\nhighlight the critical role of individual components and training strategies,\nand in particular emphasize the importance of adapted attention mechanisms\nwithin the CNN framework to improve the model focus on age-relevant facial\nfeatures. The results show that the ConvNeXt-ViT hybrid not only outperforms\ntraditional methods, but also provides a robust foundation for future advances\nin age estimation and related visual tasks. This work underscores the\ntransformative potential of hybrid architectures and represents a promising\ndirection for the seamless integration of CNNs and transformers to address\ncomplex computer vision challenges.", "AI": {"tldr": "The paper introduces a hybrid architecture combining ConvNeXt and Vision Transformers (ViT) to improve age estimation through facial images.", "motivation": "To address the complexity of age estimation from facial images and overcome the limitations of existing computer vision methods.", "method": "A hybrid solution integrating ConvNeXt for localized feature extraction with ViT for global attention, evaluated on multiple datasets using pre-trained models, linear layers, and advanced regularization.", "result": "The hybrid architecture achieved superior performance, as assessed by reduced mean absolute error (MAE) on benchmark datasets.", "conclusion": "The ConvNeXt-ViT hybrid outperforms traditional methods, showcasing the benefits of hybrid architectures for complex computer vision tasks like age estimation."}}
{"id": "2511.00318", "pdf": "https://arxiv.org/pdf/2511.00318", "abs": "https://arxiv.org/abs/2511.00318", "authors": ["Dana Kim", "Yichen Xu", "Tiffany Lin"], "title": "A Technical Exploration of Causal Inference with Hybrid LLM Synthetic Data", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "9 pages, 4 figures", "summary": "Large Language Models (LLMs) offer a flexible means to generate synthetic\ntabular data, yet existing approaches often fail to preserve key causal\nparameters such as the average treatment effect (ATE). In this technical\nexploration, we first demonstrate that state-of-the-art synthetic data\ngenerators, both GAN- and LLM-based, can achieve high predictive fidelity while\nsubstantially misestimating causal effects. To address this gap, we propose a\nhybrid generation framework that combines model-based covariate synthesis\n(monitored via distance-to-closest-record filtering) with separately learned\npropensity and outcome models, thereby ensuring that (W, A, Y) triplets retain\ntheir underlying causal structure. We further introduce a synthetic pairing\nstrategy to mitigate positivity violations and a realistic evaluation protocol\nthat leverages unlimited synthetic samples to benchmark traditional estimators\n(IPTW, AIPW, substitution) under complex covariate distributions. This work\nlays the groundwork for LLM-powered data pipelines that support robust causal\nanalysis. Our code is available at\nhttps://github.com/Xyc-arch/llm-synthetic-for-causal-inference.git.", "AI": {"tldr": "The paper highlights that current techniques using GANs and LLMs for synthetic tabular data fail in preserving causal effects like Average Treatment Effect (ATE). They propose a hybrid framework combined with synthetic pairings to maintain causal structures and improve causal analysis.", "motivation": "To address the inability of existing synthetic tabular data generation methods to preserve critical causal parameters such as ATE, while ensuring high predictive fidelity.", "method": "The authors propose a hybrid model-based synthesis method combining covariate synthesis, propensity learning, outcome modeling, and a synthetic pairing strategy to address positivity violations in causal structures.", "result": "Their method ensures generated data better preserves causal structures (such as triplet relationships W, A, Y) while maintaining high predictive capability, making it suitable for causal inference.", "conclusion": "This research establishes a robust framework and evaluation protocol for utilizing LLMs in synthetic data generation that effectively supports causal analysis methods."}}
{"id": "2511.00651", "pdf": "https://arxiv.org/pdf/2511.00651", "abs": "https://arxiv.org/abs/2511.00651", "authors": ["Chenhua Shi", "Bhavika Jalli", "Gregor Macdonald", "John Zou", "Wanlu Lei", "Mridul Jain", "Joji Philip"], "title": "Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting", "categories": ["cs.AI", "cs.CL", "cs.IT", "cs.MA", "cs.NI", "math.IT"], "comment": "6 pages, 7 figures, 1 table", "summary": "Telecom networks are rapidly growing in scale and complexity, making\neffective management, operation, and optimization increasingly challenging.\nAlthough Artificial Intelligence (AI) has been applied to many telecom tasks,\nexisting models are often narrow in scope, require large amounts of labeled\ndata, and struggle to generalize across heterogeneous deployments.\nConsequently, network troubleshooting continues to rely heavily on Subject\nMatter Experts (SMEs) to manually correlate various data sources to identify\nroot causes and corrective actions. To address these limitations, we propose a\nMulti-Agent System (MAS) that employs an agentic workflow, with Large Language\nModels (LLMs) coordinating multiple specialized tools for fully automated\nnetwork troubleshooting. Once faults are detected by AI/ML-based monitors, the\nframework dynamically activates agents such as an orchestrator, solution\nplanner, executor, data retriever, and root-cause analyzer to diagnose issues\nand recommend remediation strategies within a short time frame. A key component\nof this system is the solution planner, which generates appropriate remediation\nplans based on internal documentation. To enable this, we fine-tuned a Small\nLanguage Model (SLM) on proprietary troubleshooting documents to produce\ndomain-grounded solution plans. Experimental results demonstrate that the\nproposed framework significantly accelerates troubleshooting automation across\nboth Radio Access Network (RAN) and Core network domains.", "AI": {"tldr": "The paper introduces a Multi-Agent System (MAS) using Large Language Models (LLMs) and specialized tools for automated troubleshooting in telecom networks.", "motivation": "The increasing scale and complexity of telecom networks make management and troubleshooting difficult, requiring a shift from traditional SME-dependent approaches to automated solutions.", "method": "The study proposes a Multi-Agent System (MAS) that incorporates AI/ML fault detection, specialized agents (orchestrator, planner, executor, data retriever, and root-cause analyzer), and fine-tuned Small Language Models (SLMs) for troubleshooting and creating remediation plans.", "result": "The framework accelerates and enhances troubleshooting automation in both Radio Access Network (RAN) and Core network domains.", "conclusion": "The proposed MAS demonstrates the potential of domain-specific automated systems to reduce reliance on SMEs and improve the efficiency and speed of troubleshooting telecom networks."}}
{"id": "2511.00823", "pdf": "https://arxiv.org/pdf/2511.00823", "abs": "https://arxiv.org/abs/2511.00823", "authors": ["Qi Xia", "Hu Xia", "Isaac Amankona Obiri", "Adjei-Arthur Bonsu", "Grace Mupoyi Ntuala", "Ansu Badjie", "Tienin Bole Wilfried", "Jiaqin Liu", "Lan Ma", "Jianbin Gao", "Feng Yao"], "title": "TINC: Trusted Intelligent NetChain", "categories": ["cs.NI", "cs.DC"], "comment": "17 pages, 22 figures This preprint has been submitted to IEEE\n  Transactions on Networking and is currently under peer review. The content\n  may be updated based on the review outcome. \\c{opyright} The authors. All\n  rights reserved. Distributed under the arXiv non-exclusive license", "summary": "Blockchain technology facilitates the development of decentralized systems\nthat ensure trust and transparency without the need for expensive centralized\nintermediaries. However, existing blockchain architectures particularly\nconsortium blockchains face critical challenges related to scalability and\nefficiency. State sharding has emerged as a promising approach to enhance\nblockchain scalability and performance. However, current shard-based solutions\noften struggle to guarantee fair participation and a balanced workload\ndistribution among consortium members. To address these limitations, we propose\nTrusted Intelligent NetChain (TINC), a multi-plane sharding architecture\nspecifically designed for consortium blockchains. TINC incorporates intelligent\nmechanisms for adaptive node assignment and dynamic workload balancing,\nenabling the system to respond effectively to changing network conditions while\nmaintaining equitable shard utilization. By decoupling the control and data\nplanes, TINC allows control nodes to focus on consensus operations, while data\nnodes handle large-scale storage, thus improving overall resource efficiency.\nExtensive experimental evaluation and formal analysis demonstrate that TINC\nsignificantly outperforms existing shard-based blockchain frameworks. It\nachieves higher throughput, lower latency, balanced node and transaction\ndistributions, and reduced transaction failure rates. Furthermore, TINC\nmaintains essential blockchain security guarantees, exhibiting resilience\nagainst Byzantine faults and dynamic network environments. The integration of\nDynamic Decentralized Identifiers (DDIDs) further strengthens trust and\nsecurity management within the consortium network.", "AI": {"tldr": "The paper introduces Trusted Intelligent NetChain (TINC), a novel multi-plane sharding architecture for consortium blockchains, addressing scalability and efficiency challenges.", "motivation": "The authors aim to overcome scalability and workload distribution issues in consortium blockchains while maintaining fairness and security.", "method": "TINC employs adaptive node assignment, dynamic workload balancing, decoupling control/data planes, and integrates Dynamic Decentralized Identifiers (DDIDs).", "result": "TINC achieves better throughput, latency, balanced distributions, and reduced transaction failures, while ensuring security and resilience.", "conclusion": "TINC provides a scalable, efficient, and secure solution for consortium blockchains, outperforming existing shard-based architectures."}}
{"id": "2511.00505", "pdf": "https://arxiv.org/pdf/2511.00505", "abs": "https://arxiv.org/abs/2511.00505", "authors": ["Qi Luo", "Xiaonan Li", "Junqi Dai", "Shuang Cheng", "Xipeng Qiu"], "title": "Zero-RAG: Towards Retrieval-Augmented Generation with Zero Redundant Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation has shown remarkable results to address Large\nLanguage Models' hallucinations, which usually uses a large external corpus to\nsupplement knowledge to LLMs. However, with the development of LLMs, the\ninternal knowledge of LLMs has expanded significantly, thus causing significant\nknowledge redundancy between the external corpus and LLMs. On the one hand, the\nindexing cost of dense retrieval is highly related to the corpus size and thus\nsignificant redundant knowledge intensifies the dense retrieval's workload. On\nthe other hand, the redundant knowledge in the external corpus is not helpful\nto LLMs and our exploratory analysis shows that it instead hurts the RAG\nperformance on those questions which the LLM can answer by itself. To address\nthese issues, we propose Zero-RAG to tackle these challenges. Specifically, we\nfirst propose the Mastery-Score metric to identify redundant knowledge in the\nRAG corpus to prune it. After pruning, answers to \"mastered\" questions rely\nprimarily on internal knowledge of the LLM. To better harness the internal\ncapacity, we propose Query Router and Noise-Tolerant Tuning to avoid the\nirrelevant documents' distraction and thus further improve the LLM's\nutilization of internal knowledge with pruned corpus. Experimental results show\nthat Zero-RAG prunes the Wikipedia corpus by 30\\% and accelerates the retrieval\nstage by 22\\%, without compromising RAG's performance.", "AI": {"tldr": "Zero-RAG optimizes retrieval-augmented generation (RAG) by reducing external redundancy and leveraging internal large language model (LLM) knowledge, resulting in faster performance without sacrificing accuracy.", "motivation": "The paper aims to address the inefficiency and redundancy in retrieval-augmented generation (RAG) caused by overlap between the external corpus and large language models' (LLM) internal knowledge. This redundancy inflates retrieval costs and may adversely affect RAG performance on questions already answerable by the LLM.", "method": "The authors introduce a new metric, Mastery-Score, to identify and prune redundant external knowledge in the RAG corpus. Subsequently, a Query Router and Noise-Tolerant Tuning are employed to better use the LLM's internal knowledge while managing distractions from irrelevant documents.", "result": "The proposed method, Zero-RAG, reduced the Wikipedia corpus by 30% and accelerated the retrieval process by 22%, all without negatively affecting RAG's overall performance.", "conclusion": "Zero-RAG demonstrates that pruning redundant external data and optimizing LLM utilization can lead to faster retrieval and efficient use of computational resources while maintaining robust RAG performance."}}
{"id": "2511.00776", "pdf": "https://arxiv.org/pdf/2511.00776", "abs": "https://arxiv.org/abs/2511.00776", "authors": ["Cuiyun Gao", "Guodong Fan", "Chun Yong Chong", "Shizhan Chen", "Chao Liu", "David Lo", "Zibin Zheng", "Qing Liao"], "title": "A Systematic Literature Review of Code Hallucinations in LLMs: Characterization, Mitigation Methods, Challenges, and Future Directions for Reliable AI", "categories": ["cs.SE"], "comment": null, "summary": "Model hallucination is one of the most critical challenges faced by Large\nLanguage Models (LLMs), especially in high-stakes code intelligence tasks. As\nLLMs become increasingly integrated into software engineering tasks,\nunderstanding and mitigating hallucination in code becomes essential. In this\nsurvey, we provide a systematic review of hallucination phenomena in\ncode-oriented LLMs from four key perspectives. First, we begin by surveying 60\npapers to define hallucination in the context of code and summarize its primary\ncauses, such as data noise, exposure bias, and insufficient semantic grounding,\nwhile also tracing recent trends in literature across natural language\nprocessing (NLP) and software engineering communities. Second, we review model\nhallucination surveys in a broader span and summarize representative\nhallucination mitigation strategies, such as knowledge-enhanced generation,\nconstrained decoding, and post-editing. Third, we review approaches targeted\nfor code intelligence and highlight code-specific challenges that aggravate\nhallucination, including syntax sensitivity, strict type systems, and\ndependence on external libraries. Meanwhile, we analyze how emerging code\nintelligence tasks, e.g., program analysis, symbolic execution, and unit\ntesting, are utilized to detect and mitigate hallucinations. Fourth, we\nsummarize current evaluation benchmarks, ranging from static metrics to dynamic\nchecks, e.g., compilation and execution correctness, and emphasize the need for\nhallucination-oriented benchmarks.", "AI": {"tldr": "The paper surveys hallucination in code-oriented Large Language Models (LLMs), summarizing causes, challenges, mitigation strategies, and evaluation benchmarks.", "motivation": "Hallucination in LLMs is a pressing challenge, especially for high-stakes code intelligence tasks in software engineering.", "method": "The authors systematically review 60 papers to analyze hallucination causes, mitigation strategies, code-specific challenges, and evaluation benchmarks.", "result": "The paper identifies key factors contributing to hallucination, highlights approaches to mitigate it, and emphasizes the importance of evaluating hallucination-specific benchmarks.", "conclusion": "By addressing hallucination challenges and proposing solutions, the paper guides research on improving code-oriented LLM reliability and effectiveness."}}
{"id": "2511.00814", "pdf": "https://arxiv.org/pdf/2511.00814", "abs": "https://arxiv.org/abs/2511.00814", "authors": ["Stella Kombo", "Masih Haseli", "Skylar Wei", "Joel W. Burdick"], "title": "Real-Time Learning of Predictive Dynamic Obstacle Models for Robotic Motion Planning", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY", "93C41, 93E11, 37M10", "I.2.9; I.2.6; I.2.8"], "comment": "10 pages, 6 figures, submitted to IEEE International Conference on\n  Robotics and Automation (ICRA) 2025", "summary": "Autonomous systems often must predict the motions of nearby agents from\npartial and noisy data. This paper asks and answers the question: \"can we\nlearn, in real-time, a nonlinear predictive model of another agent's motions?\"\nOur online framework denoises and forecasts such dynamics using a modified\nsliding-window Hankel Dynamic Mode Decomposition (Hankel-DMD). Partial noisy\nmeasurements are embedded into a Hankel matrix, while an associated Page matrix\nenables singular-value hard thresholding (SVHT) to estimate the effective rank.\nA Cadzow projection enforces structured low-rank consistency, yielding a\ndenoised trajectory and local noise variance estimates. From this\nrepresentation, a time-varying Hankel-DMD lifted linear predictor is\nconstructed for multi-step forecasts. The residual analysis provides\nvariance-tracking signals that can support downstream estimators and risk-aware\nplanning. We validate the approach in simulation under Gaussian and\nheavy-tailed noise, and experimentally on a dynamic crane testbed. Results show\nthat the method achieves stable variance-aware denoising and short-horizon\nprediction suitable for integration into real-time control frameworks.", "AI": {"tldr": "This paper proposes a real-time nonlinear predictive model using a modified Hankel-DMD for denoising and forecasting agent motions under partial and noisy data.", "motivation": "To address the challenge of predicting the motions of nearby agents in real-time despite dealing with partial and noisy data.", "method": "The paper utilizes a modified sliding-window Hankel-DMD combined with singular-value hard thresholding and Cadzow projection to denoise data and construct a lifted linear predictor for multi-step forecasts.", "result": "The method proved effective in simulation and tests with Gaussian and heavy-tailed noise, as well as a crane testbed, achieving stable denoising and short-horizon predictions.", "conclusion": "The proposed approach is reliable for real-time variance-aware denoising and motion forecasting, making it suitable for integration into autonomous systems' control frameworks."}}
{"id": "2511.00065", "pdf": "https://arxiv.org/pdf/2511.00065", "abs": "https://arxiv.org/abs/2511.00065", "authors": ["Kateryna Shapovalenko", "Quentin Auster"], "title": "Aligning Brain Signals with Multimodal Speech and Vision Embeddings", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "When we hear the word \"house\", we don't just process sound, we imagine walls,\ndoors, memories. The brain builds meaning through layers, moving from raw\nacoustics to rich, multimodal associations. Inspired by this, we build on\nrecent work from Meta that aligned EEG signals with averaged wav2vec2 speech\nembeddings, and ask a deeper question: which layers of pre-trained models best\nreflect this layered processing in the brain? We compare embeddings from two\nmodels: wav2vec2, which encodes sound into language, and CLIP, which maps words\nto images. Using EEG recorded during natural speech perception, we evaluate how\nthese embeddings align with brain activity using ridge regression and\ncontrastive decoding. We test three strategies: individual layers, progressive\nconcatenation, and progressive summation. The findings suggest that combining\nmultimodal, layer-aware representations may bring us closer to decoding how the\nbrain understands language, not just as sound, but as experience.", "AI": {"tldr": "The paper explores how different layers of pre-trained models (wav2vec2 and CLIP) align with the brain's layered processing of language, using EEG data during natural speech perception.", "motivation": "Understanding how language processing in the brain incorporates multimodal associations, beyond raw acoustics, enhancing connections between computational models and neuroscience.", "method": "Comparing embeddings from wav2vec2 and CLIP models using EEG-based ridge regression and contrastive decoding, focusing on individual layers, progressive concatenation, and progressive summation.", "result": "The study finds that multimodal, layer-aware representations better align with brain activity, offering insights into linguistic understanding as experience.", "conclusion": "Combining layered and multimodal embeddings improves our ability to decode and relate brain activities with language comprehension processes."}}
{"id": "2511.00141", "pdf": "https://arxiv.org/pdf/2511.00141", "abs": "https://arxiv.org/abs/2511.00141", "authors": ["Janghoon Cho", "Jungsoo Lee", "Munawar Hayat", "Kyuwoong Hwang", "Fatih Porikli", "Sungha Choi"], "title": "FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent studies in long video understanding have harnessed the advanced\nvisual-language reasoning capabilities of Large Multimodal Models (LMMs),\ndriving the evolution of video-LMMs specialized for processing extended video\nsequences. However, the scalability of these models is severely limited by the\noverwhelming volume of visual tokens generated from extended video sequences.\nTo address this challenge, this paper proposes FLoC, an efficient visual token\ncompression framework based on the facility location function, a principled\napproach that swiftly selects a compact yet highly representative and diverse\nsubset of visual tokens within a predefined budget on the number of visual\ntokens. By integrating the lazy greedy algorithm, our method achieves\nremarkable efficiency gains by swiftly selecting a compact subset of tokens,\ndrastically reducing the number of visual tokens while guaranteeing\nnear-optimal performance. Notably, our approach is training-free,\nmodel-agnostic, and query-agnostic, providing a versatile solution that\nseamlessly integrates with diverse video-LLMs and existing workflows. Extensive\nevaluations on large-scale benchmarks, such as Video-MME, MLVU, and\nLongVideoBench, demonstrate that our framework consistently surpasses recent\ncompression techniques, highlighting not only its effectiveness and robustness\nin addressing the critical challenges of long video understanding, but also its\nefficiency in processing speed.", "AI": {"tldr": "The paper introduces FLoC, an efficient visual token compression framework that optimizes the processing of extended video sequences for large multimodal models.", "motivation": "Existing video-LMMs struggle with scalability due to the excessive visual token volume generated from extended video sequences.", "method": "FLoC utilizes the facility location function and the lazy greedy algorithm to compress visual tokens into a compact, representative set without training, improving efficiency and performance.", "result": "FLoC significantly reduces the number of visual tokens while maintaining near-optimal performance. It outperforms recent compression techniques on benchmarks like Video-MME, MLVU, and LongVideoBench.", "conclusion": "FLoC is a training-free and model-agnostic framework that addresses scalability challenges in long video understanding and can seamlessly integrate into current workflows."}}
{"id": "2511.00359", "pdf": "https://arxiv.org/pdf/2511.00359", "abs": "https://arxiv.org/abs/2511.00359", "authors": ["Zhecheng Sheng", "Jiawei Zhang", "Enmao Diao"], "title": "Toward Unifying Group Fairness Evaluation from a Sparsity Perspective", "categories": ["cs.LG", "cs.AI", "cs.CY", "stat.ML"], "comment": "30 pages, 14 figures", "summary": "Ensuring algorithmic fairness remains a significant challenge in machine\nlearning, particularly as models are increasingly applied across diverse\ndomains. While numerous fairness criteria exist, they often lack\ngeneralizability across different machine learning problems. This paper\nexamines the connections and differences among various sparsity measures in\npromoting fairness and proposes a unified sparsity-based framework for\nevaluating algorithmic fairness. The framework aligns with existing fairness\ncriteria and demonstrates broad applicability to a wide range of machine\nlearning tasks. We demonstrate the effectiveness of the proposed framework as\nan evaluation metric through extensive experiments on a variety of datasets and\nbias mitigation methods. This work provides a novel perspective to algorithmic\nfairness by framing it through the lens of sparsity and social equity, offering\npotential for broader impact on fairness research and applications.", "AI": {"tldr": "The paper introduces a sparsity-based framework to assess algorithmic fairness across diverse domains, validated through comprehensive experiments.", "motivation": "Algorithmic fairness challenges persist due to the lack of generalizability in existing fairness measures for diverse machine learning problems.", "method": "Connections among sparsity measures are analyzed, leading to the proposal of a unified framework to evaluate fairness, tested across datasets and mitigation methods.", "result": "The framework effectively aligns with existing fairness criteria and demonstrates applicability across varied machine learning tasks.", "conclusion": "The sparsity-based approach offers a novel way to evaluate fairness, emphasizing social equity and broadening the research scope in fairness applications."}}
{"id": "2511.00673", "pdf": "https://arxiv.org/pdf/2511.00673", "abs": "https://arxiv.org/abs/2511.00673", "authors": ["Dominik Drexler"], "title": "Lifted Successor Generation in Numeric Planning", "categories": ["cs.AI"], "comment": null, "summary": "Most planners ground numeric planning tasks, given in a first-order-like\nlanguage, into a ground task representation. However, this can lead to an\nexponential blowup in task representation size, which occurs in practice for\nhard-to-ground tasks. We extend a state-of-the-art lifted successor generator\nfor classical planning to support numeric precondition applicability. The\nmethod enumerates maximum cliques in a substitution consistency graph. Each\nmaximum clique represents a substitution for the variables of the action\nschema, yielding a ground action. We augment this graph with numeric action\npreconditions and prove the successor generator is exact under formally\nspecified conditions. When the conditions fail, our generator may list\ninapplicable ground actions; a final applicability check filters these without\naffecting completeness. However, this cannot happen in 23 of 25 benchmark\ndomains, and it occurs only in 1 domain. To the authors' knowledge, no other\nlifted successor generator supports numeric action preconditions. This enables\nfuture research on lifted planning for a very rich planning fragment.", "AI": {"tldr": "The paper extends lifted successor generation methods to include numeric preconditions for planning tasks, addressing scalability challenges.", "motivation": "To overcome the exponential growth in task representation size for numeric planning problems grounded into first-order-like languages.", "method": "The method incorporates numeric preconditions into a lifted successor generator, using substitution consistency graphs and maximum cliques to determine ground actions. Applicability checks are used to ensure correctness, even under certain conditions.", "result": "The proposed approach is exact under specific conditions. Testing across benchmark domains shows a rare occurrence of inapplicable actions, highlighting its practicality.", "conclusion": "This work leads to an advanced framework for lifted planning with numeric preconditions, enabling further research on sophisticated planning systems."}}
{"id": "2511.00870", "pdf": "https://arxiv.org/pdf/2511.00870", "abs": "https://arxiv.org/abs/2511.00870", "authors": ["Maxime Bouton", "Pierre-Antoine Thouvenin", "Audrey Repetti", "Pierre Chainais"], "title": "A Distributed Plug-and-Play MCMC Algorithm for High-Dimensional Inverse Problems", "categories": ["stat.ME", "cs.DC", "eess.SP"], "comment": null, "summary": "Markov Chain Monte Carlo (MCMC) algorithms are standard approaches to solve\nimaging inverse problems and quantify estimation uncertainties, a key\nrequirement in absence of ground-truth data. To improve estimation quality,\nPlug-and-Play MCMC algorithms, such as PnP-ULA, have been recently developed to\naccommodate priors encoded by a denoising neural network. Designing scalable\nsamplers for high-dimensional imaging inverse problems remains a challenge:\ndrawing and storing high-dimensional samples can be prohibitive, especially for\nhigh-resolution images. To address this issue, this work proposes a distributed\nsampler based on approximate data augmentation and PnP-ULA to solve very large\nproblems. The proposed sampler uses lightweight denoising convolutional neural\nnetwork, to efficiently exploit multiple GPUs on a Single Program Multiple Data\narchitecture. Reconstruction performance and scalability are evaluated on\nseveral imaging problems. Communication and computation overheads due to the\ndenoiser are carefully discussed. The proposed distributed approach noticeably\ncombines three very precious qualities: it is scalable, enables uncertainty\nquantification, for a reconstruction performance comparable to other PnP\nmethods.", "AI": {"tldr": "This paper proposes a distributed MCMC sampler using PnP-ULA and lightweight neural networks to efficiently solve high-dimensional imaging problems while quantifying uncertainties.", "motivation": "The paper addresses challenges in scalable sampling for high-dimensional imaging inverse problems, focusing on uncertainty quantification in the absence of ground-truth data.", "method": "The method combines a Plug-and-Play MCMC algorithm (PnP-ULA) with approximate data augmentation and lightweight denoising neural networks to utilize multiple GPUs efficiently.", "result": "The distributed sampler achieves scalability, reliable uncertainty quantification, and reconstruction performance comparable to existing methods, while minimizing communication and computation overheads.", "conclusion": "The proposed approach offers a scalable solution for high-resolution image reconstruction with uncertainty quantification, showing promising results in comparison to other methods."}}
{"id": "2511.00514", "pdf": "https://arxiv.org/pdf/2511.00514", "abs": "https://arxiv.org/abs/2511.00514", "authors": ["Birat Poudel", "Satyam Ghimire", "Er. Prakash Chandra Prasad"], "title": "Fine-Tuning DialoGPT on Common Diseases in Rural Nepal for Medical Conversations", "categories": ["cs.CL"], "comment": "6 pages, 6 figures, 3 tables", "summary": "Conversational agents are increasingly being explored to support healthcare\ndelivery, particularly in resource-constrained settings such as rural Nepal.\nLarge-scale conversational models typically rely on internet connectivity and\ncloud infrastructure, which may not be accessible in rural areas. In this\nstudy, we fine-tuned DialoGPT, a lightweight generative dialogue model that can\noperate offline, on a synthetically constructed dataset of doctor-patient\ninteractions covering ten common diseases prevalent in rural Nepal, including\ncommon cold, seasonal fever, diarrhea, typhoid fever, gastritis, food\npoisoning, malaria, dengue fever, tuberculosis, and pneumonia. Despite being\ntrained on a limited, domain-specific dataset, the fine-tuned model produced\ncoherent, contextually relevant, and medically appropriate responses,\ndemonstrating an understanding of symptoms, disease context, and empathetic\ncommunication. These results highlight the adaptability of compact,\noffline-capable dialogue models and the effectiveness of targeted datasets for\ndomain adaptation in low-resource healthcare environments, offering promising\ndirections for future rural medical conversational AI.", "AI": {"tldr": "This paper presents a fine-tuned offline conversational AI model designed to assist in healthcare delivery in rural Nepal, where internet access is limited.", "motivation": "To address reliance on internet-dependent conversational AI and improve healthcare delivery in resource-constrained rural areas.", "method": "The authors fine-tuned DialoGPT on a synthetic dataset featuring doctor-patient interactions for ten common diseases prevalent in rural Nepal.", "result": "The model generated coherent, contextually relevant, and medically appropriate responses, showing good understanding of disease contexts and patient communication.", "conclusion": "Compact offline dialogue models, combined with targeted datasets, can be effective for healthcare applications in low-resource settings, indicating potential for rural medical conversational AI."}}
{"id": "2511.00780", "pdf": "https://arxiv.org/pdf/2511.00780", "abs": "https://arxiv.org/abs/2511.00780", "authors": ["Chenyu Zhao", "Shenglin Zhang", "Zeshun Huang", "Weilin Jin", "Yongqian Sun", "Dan Pei", "Chaoyun Zhang", "Qingwei Lin", "Chetan Bansal", "Saravan Rajmohan", "Minghua Ma"], "title": "Can Language Models Go Beyond Coding? Assessing the Capability of Language Models to Build Real-World Systems", "categories": ["cs.SE"], "comment": null, "summary": "Large language models (LLMs) have shown growing potential in software\nengineering, yet few benchmarks evaluate their ability to repair software\nduring migration across instruction set architectures (ISAs). Cross-ISA\nmigration, such as between x86_64 and aarch64, requires handling complex\ndependencies, heterogeneous toolchains, and long build logs while ensuring\nexecutable verification. To address this challenge, we present Build-bench, an\nend-to-end benchmark that systematically evaluates the capability of LLMs to\nrepair build failures in cross-ISA settings. Build-bench collects 268\nreal-world failed packages and integrates auxiliary tools including Structure\nExtraction, File Content Extraction, Content Modification, and Build\nVerification to support autonomous, tool-augmented reasoning. The repair\nprocess operates in an iterative loop where, upon failure, the model receives\nupdated build logs and previous repair outcomes to refine subsequent attempts.\nThrough a comparative evaluation of six representative LLMs, Build-bench\nreveals that current models achieve a maximum build success rate of 63% and\ntool usage patterns differ significantly across models. By coupling real build\nenvironments with verifiable outcomes, Build-bench establishes the first\narchitecture-aware benchmark for studying LLM-based software build and repair.", "AI": {"tldr": "The paper introduces Build-bench, the first benchmark designed to evaluate large language models (LLMs) in repairing software builds during cross-ISA migrations.", "motivation": "To assess the capability of LLMs in resolving complex build failures during software migration across different instruction set architectures (ISAs).", "method": "Development of Build-bench, an end-to-end benchmark comprising real-world failed packages, iterative repair processes, and integrated auxiliary tools to support autonomous reasoning.", "result": "Comparative evaluation of six LLMs revealed a maximum build success rate of 63%, highlighting variability in tool usage and repair approaches across models.", "conclusion": "Build-bench provides a systematic, architecture-aware evaluation framework that facilitates the study of LLM performance in cross-ISA software build and repair scenarios."}}
{"id": "2511.00840", "pdf": "https://arxiv.org/pdf/2511.00840", "abs": "https://arxiv.org/abs/2511.00840", "authors": ["William Suliman", "Ekaterina Chaikovskaia", "Egor Davydenko", "Roman Gorbachev"], "title": "Heuristic Step Planning for Learning Dynamic Bipedal Locomotion: A Comparative Study of Model-Based and Model-Free Approaches", "categories": ["cs.RO"], "comment": null, "summary": "This work presents an extended framework for learning-based bipedal\nlocomotion that incorporates a heuristic step-planning strategy guided by\ndesired torso velocity tracking. The framework enables precise interaction\nbetween a humanoid robot and its environment, supporting tasks such as crossing\ngaps and accurately approaching target objects. Unlike approaches based on full\nor simplified dynamics, the proposed method avoids complex step planners and\nanalytical models. Step planning is primarily driven by heuristic commands,\nwhile a Raibert-type controller modulates the foot placement length based on\nthe error between desired and actual torso velocity. We compare our method with\na model-based step-planning approach -- the Linear Inverted Pendulum Model\n(LIPM) controller. Experimental results demonstrate that our approach attains\ncomparable or superior accuracy in maintaining target velocity (up to 80%),\nsignificantly greater robustness on uneven terrain (over 50% improvement), and\nimproved energy efficiency. These results suggest that incorporating complex\nanalytical, model-based components into the training architecture may be\nunnecessary for achieving stable and robust bipedal walking, even in\nunstructured environments.", "AI": {"tldr": "Introduces a heuristic-based bipedal locomotion framework that enhances precision, robustness, and energy efficiency, removing the need for complex model-based components.", "motivation": "Overcoming limitations of model-based step planning approaches to achieve robust bipedal locomotion in dynamic environments.", "method": "A heuristic step-planning strategy coupled with a Raibert-type controller for modulating foot placement based on torso velocity errors.", "result": "The proposed approach demonstrated superior accuracy (up to 80%), improved robustness (over 50%), and enhanced energy efficiency compared to model-based methods.", "conclusion": "Complex model-based components may not be essential for stable bipedal walking; the heuristic framework achieves robust performance in unstructured environments."}}
{"id": "2511.00066", "pdf": "https://arxiv.org/pdf/2511.00066", "abs": "https://arxiv.org/abs/2511.00066", "authors": ["Tue Le", "Nghi D. Q. Bui", "Linh Ngo Van", "Trung Le"], "title": "Token-Regulated Group Relative Policy Optimization for Stable Reinforcement Learning in Large Language Models", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a\npowerful approach for strengthening the reasoning capabilities of large\nlanguage models (LLMs). Among existing algorithms, Group Relative Policy\nOptimization (GRPO) has demonstrated strong performance, yet it suffers from a\ncritical issue: low-probability tokens disproportionately dominate gradient\nupdates due to their inherently large gradient magnitudes. This imbalance leads\nto unstable training and suppresses the contribution of high-probability tokens\nthat are more reliable for learning. In this work, we introduce Token-Regulated\nGroup Relative Policy Optimization (TR-GRPO), a simple yet effective extension\nof GRPO that assigns token-level weights positively correlated with the model's\npredicted probability. By downweighting low-probability tokens and emphasizing\nhigh-probability ones, TR-GRPO mitigates gradient over-amplification while\npreserving informative learning signals. Extensive experiments demonstrate that\nTR-GRPO consistently outperforms GRPO across RLVR tasks, including logic, math,\nand agentic reasoning, highlighting the importance of regulating token\ncontributions during RL training and establishing TR-GRPO as a robust framework\nfor enhancing LLM reasoning.", "AI": {"tldr": "The paper introduces TR-GRPO, an improvement on GRPO, to address issues of token imbalance in reinforcement learning for large language models (LLMs).", "motivation": "The motivation is to resolve a critical issue in GRPO where low-probability tokens disproportionately influence gradient updates, leading to unstable training and reduced effectiveness of high-probability tokens.", "method": "The proposed method, TR-GRPO, assigns token-level weights based on the model\u2019s predicted probability to downweigh low-probability tokens and emphasize high-probability ones.", "result": "Experiments demonstrate that TR-GRPO outperforms GRPO in reinforcement learning tasks like logic, math, and reasoning.", "conclusion": "TR-GRPO is shown to be a robust and effective framework for enhancing reasoning in LLMs by regulating token contributions during training."}}
{"id": "2511.00143", "pdf": "https://arxiv.org/pdf/2511.00143", "abs": "https://arxiv.org/abs/2511.00143", "authors": ["Jinsu Kim", "Yunhun Nam", "Minseon Kim", "Sangpil Kim", "Jongheon Jeong"], "title": "BlurGuard: A Simple Approach for Robustifying Image Protection Against AI-Powered Editing", "categories": ["cs.CV"], "comment": "36 pages; NeurIPS 2025; Code is available at\n  https://github.com/jsu-kim/BlurGuard", "summary": "Recent advances in text-to-image models have increased the exposure of\npowerful image editing techniques as a tool, raising concerns about their\npotential for malicious use. An emerging line of research to address such\nthreats focuses on implanting \"protective\" adversarial noise into images before\ntheir public release, so future attempts to edit them using text-to-image\nmodels can be impeded. However, subsequent works have shown that these\nadversarial noises are often easily \"reversed,\" e.g., with techniques as simple\nas JPEG compression, casting doubt on the practicality of the approach. In this\npaper, we argue that adversarial noise for image protection should not only be\nimperceptible, as has been a primary focus of prior work, but also\nirreversible, viz., it should be difficult to detect as noise provided that the\noriginal image is hidden. We propose a surprisingly simple method to enhance\nthe robustness of image protection methods against noise reversal techniques.\nSpecifically, it applies an adaptive per-region Gaussian blur on the noise to\nadjust the overall frequency spectrum. Through extensive experiments, we show\nthat our method consistently improves the per-sample worst-case protection\nperformance of existing methods against a wide range of reversal techniques on\ndiverse image editing scenarios, while also reducing quality degradation due to\nnoise in terms of perceptual metrics. Code is available at\nhttps://github.com/jsu-kim/BlurGuard.", "AI": {"tldr": "This paper introduces a method to improve image protection against malicious edits by making adversarial noise irreversible using adaptive Gaussian blur.", "motivation": "Concerns about malicious use of image editing tools raise the need for protection methods that prevent unauthorized modifications, focusing on imperceptible and irreversible adversarial noise.", "method": "The authors propose applying adaptive per-region Gaussian blur on adversarial noise to adjust its frequency spectrum, enhancing its robustness against reversal techniques.", "result": "Their method improves the worst-case protection performance of existing methods across diverse scenarios and reduces perceptual quality degradation caused by adversarial noise.", "conclusion": "The proposed technique demonstrates effectiveness in enhancing image protection mechanisms, maintaining performance while minimizing visual quality loss."}}
{"id": "2511.00434", "pdf": "https://arxiv.org/pdf/2511.00434", "abs": "https://arxiv.org/abs/2511.00434", "authors": ["Andrea Angino", "Matteo Aurina", "Alena Kopani\u010d\u00e1kov\u00e1", "Matthias Voigt", "Marco Donatelli", "Rolf Krause"], "title": "Trust-Region Methods with Low-Fidelity Objective Models", "categories": ["math.NA", "cs.LG", "cs.NA", "stat.ML"], "comment": "Submitted to the Proceedings of Domain Decomposition Methods in\n  Science and Engineering XXIX", "summary": "We introduce two multifidelity trust-region methods based on the Magical\nTrust Region (MTR) framework. MTR augments the classical trust-region step with\na secondary, informative direction. In our approaches, the secondary\n``magical'' directions are determined by solving coarse trust-region\nsubproblems based on low-fidelity objective models. The first proposed method,\nSketched Trust-Region (STR), constructs this secondary direction using a\nsketched matrix to reduce the dimensionality of the trust-region subproblem.\nThe second method, SVD Trust-Region (SVDTR), defines the magical direction via\na truncated singular value decomposition of the dataset, capturing the leading\ndirections of variability. Several numerical examples illustrate the potential\ngain in efficiency.", "AI": {"tldr": "This paper introduces two methods in the Magical Trust Region (MTR) framework, employing multifidelity approaches to enhance efficiency in solving trust-region subproblems.", "motivation": "To improve efficiency in solving trust-region subproblems by leveraging multifidelity frameworks and augmenting classical optimization steps.", "method": "Developing two multifidelity methods: (1) Sketched Trust-Region (STR) using sketched matrix for dimensionality reduction, and (2) SVD Trust-Region (SVDTR) leveraging truncated singular value decomposition for directing variability.", "result": "Numerical examples demonstrate the potential efficiency improvements provided by the proposed methods.", "conclusion": "The multifidelity trust-region methods introduced, STR and SVDTR, enhance optimization frameworks with reduced computations and improved directionality."}}
{"id": "2511.00710", "pdf": "https://arxiv.org/pdf/2511.00710", "abs": "https://arxiv.org/abs/2511.00710", "authors": ["Minghe Shen", "Zhuo Zhi", "Chonghan Liu", "Shuo Xing", "Zhengzhong Tu", "Che Liu"], "title": "Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries", "categories": ["cs.AI"], "comment": null, "summary": "While Vision-Language Models (VLMs) post-trained with Reinforcement Learning\n(RL) show impressive general reasoning, their evaluation is often confined to\nlanguage-dominant tasks (e.g., math). This raises a critical question: can RL\npost-training truly extend the inherent capability boundary of a base VLM,\nparticularly for visual-centric spatial tasks where it initially fails? To\ninvestigate this, we introduce Ariadne, a framework utilizing synthetic mazes\nfor multi-step spatial reasoning where task difficulty (e.g., path length,\nturns) is precisely controlled. We leverage this controllable environment to\ntrain VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a\ndifficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves\nover 50% accuracy on a problem set where the base model scored 0%,\ndemonstrating that our approach expands the model's initial capability\nboundary. To assess real-world viability, we evaluate out-of-distribution (OOD)\ngeneralization on practical benchmarks. Despite training only on synthetic maze\nsamples, Ariadne achieves significant zero-shot improvements, averaging 16% on\nMapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer\ntasks). These results confirm that our method not only broadens the model's\nfundamental limits but also enhances its generalization to real-world spatial\nreasoning. We acknowledge our study is limited to the post-training phase,\ngiven the opaqueness of pre-training data, and hope our research motivates\nfurther work on specialized, capability-extending alignment.", "AI": {"tldr": "This paper introduces a new post-training framework to improve Vision-Language Models (VLMs) in visual-centric spatial tasks using synthetic mazes and Reinforcement Learning with Verified Rewards (RLVR).", "motivation": "The paper seeks to evaluate whether RL post-training can extend the inherent capability boundary of VLMs, particularly for visual-centric spatial tasks, where these models typically fail.", "method": "It uses a synthetic maze environment with controlled difficulty to train VLMs with RLVR in a difficulty-aware curriculum, aiming to improve visual-spatial reasoning.", "result": "Post-training, the VLM achieved over 50% accuracy in spatial reasoning tasks where the base model scored 0%, and demonstrated OOD generalization improvements in real-world tasks (e.g., 16% on MapBench and 24% on ReasonMap).", "conclusion": "The findings indicate that RLVR post-training can expand the capability boundary of VLMs, particularly for spatial reasoning, although the study is confined to post-training limitations, inspiring future work on alignment strategies."}}
{"id": "2511.01129", "pdf": "https://arxiv.org/pdf/2511.01129", "abs": "https://arxiv.org/abs/2511.01129", "authors": ["Fabio Diniz Rossi"], "title": "Boosting performance of computer vision applications through embedded GPUs on the edge", "categories": ["cs.CV", "cs.DC"], "comment": "4 pages, 6 figures", "summary": "Computer vision applications, especially those using augmented reality\ntechnology, are becoming quite popular in mobile devices. However, this type of\napplication is known as presenting significant demands regarding resources. In\norder to enable its utilization in devices with more modest resources, edge\ncomputing can be used to offload certain high intensive tasks. Still, edge\ncomputing is usually composed of devices with limited capacity, which may\nimpact in users quality of experience when using computer vision applications.\nThis work proposes the use of embedded devices with graphics processing units\n(GPUs) to overcome such limitation. Experiments performed shown that GPUs can\nattain a performance gain when compared to using only CPUs, which guarantee a\nbetter experience to users using such kind of application.", "AI": {"tldr": "The paper discusses leveraging GPUs in embedded edge devices to enhance performance and user experience for resource-intensive computer vision applications in mobile and augmented reality.", "motivation": "Mobile augmented reality applications are resource-intensive and may not perform well on devices with limited resources. Utilizing edge computing can help, but edge devices also have capacity limitations.", "method": "The paper proposes using embedded devices with GPUs to handle high-intensity computational tasks in computer vision applications.", "result": "Experiments demonstrated that GPUs significantly improved performance compared to CPUs, leading to better user experiences.", "conclusion": "Using GPUs in embedded edge computing devices is an effective solution to address resource constraints and enhance the performance of computer vision applications."}}
{"id": "2511.00519", "pdf": "https://arxiv.org/pdf/2511.00519", "abs": "https://arxiv.org/abs/2511.00519", "authors": ["Ariyan Hossain", "Khondokar Mohammad Ahanaf Hannan", "Rakinul Haque", "Nowreen Tarannum Rafa", "Humayra Musarrat", "Shoaib Ahmed Dipu", "Farig Yousuf Sadeque"], "title": "Exploring and Mitigating Gender Bias in Encoder-Based Transformer Models", "categories": ["cs.CL", "I.2.7; I.7.1; K.4.1"], "comment": "25 pages, 20 figures", "summary": "Gender bias in language models has gained increasing attention in the field\nof natural language processing. Encoder-based transformer models, which have\nachieved state-of-the-art performance in various language tasks, have been\nshown to exhibit strong gender biases inherited from their training data. This\npaper investigates gender bias in contextualized word embeddings, a crucial\ncomponent of transformer-based models. We focus on prominent architectures such\nas BERT, ALBERT, RoBERTa, and DistilBERT to examine their vulnerability to\ngender bias. To quantify the degree of bias, we introduce a novel metric,\nMALoR, which assesses bias based on model probabilities for filling masked\ntokens. We further propose a mitigation approach involving continued\npre-training on a gender-balanced dataset generated via Counterfactual Data\nAugmentation. Our experiments reveal significant reductions in gender bias\nscores across different pronoun pairs. For instance, in BERT-base, bias scores\nfor \"he-she\" dropped from 1.27 to 0.08, and \"his-her\" from 2.51 to 0.36\nfollowing our mitigation approach. We also observed similar improvements across\nother models, with \"male-female\" bias decreasing from 1.82 to 0.10 in\nBERT-large. Our approach effectively reduces gender bias without compromising\nmodel performance on downstream tasks.", "AI": {"tldr": "The paper investigates gender bias in transformer-based language models and introduces a metric to measure bias while suggesting data augmentation for mitigation.", "motivation": "Understanding and addressing gender bias in transformer-based language models due to their inherited biases from training data.", "method": "Introduced MALoR metric to quantify gender bias and proposed mitigation via continued pre-training on gender-balanced data generated by Counterfactual Data Augmentation.", "result": "Substantial reduction in gender bias across multiple models with notable decreases in bias scores for pronoun pairs without degrading model performance.", "conclusion": "The proposed approach is effective in reducing gender bias in contextualized word embeddings while maintaining neural model efficiency."}}
{"id": "2511.00802", "pdf": "https://arxiv.org/pdf/2511.00802", "abs": "https://arxiv.org/abs/2511.00802", "authors": ["Jie JW Wu", "Ayanda Patrick Herlihy", "Ahmad Saleem Mirza", "Ali Afoud", "Fatemeh Fard"], "title": "GrowthHacker: Automated Off-Policy Evaluation Optimization Using Code-Modifying LLM Agents", "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": null, "summary": "With the software industry shifting toward a data-driven culture, online A/B\ntesting is a key tool for evaluating new technologies. However, deploying such\nexperiments requires substantial resources, may negatively impact users, and\ninvolves long data collection periods. To address this, \\textit{off-policy\nevaluation (OPE)}, or offline A/B testing, uses logged data to assess\ntechnologies and is fundamental in Reinforcement Learning, making it crucial in\ndomains where online testing is costly or risky, such as healthcare,\nrecommender systems, education, dialog systems, and robotics. Despite advances\nin coding LLMs and agentic AI, little is known about leveraging them to\noptimize OPE results. We investigate whether LLMs and LLM-based agents can\nimprove OPE performance via code optimization. We propose\n\\textit{GrowthHacker}, a benchmark with agent and baseline methods on\nlarge-scale real-world datasets, which iteratively optimizes code, evaluates\nresults, and begins new optimization cycles. We collected datasets, established\nprotocols, implemented baselines for OPE on the Open Bandit Pipeline\n(OBP)~\\cite{saito2021openbanditdatasetpipeline} and\nScope-RL~\\cite{kiyohara2023scope}, and developed the \\textit{two_agent}\nframework, which reduces system complexity while preserving optimization\neffectiveness. Results show the two_agent framework achieves 100% reliability\nand the highest average improvement of 106.7% among positive outcomes. Both\ntwo_agent and CrewAI reach 45% success rates, outperforming AutoGen's 34%.\nThese findings demonstrate the feasibility of LLM-based agents as automated\n\"growth hackers\" to enhance OPE systems, with implications for scaling\ndata-driven decision-making in production.", "AI": {"tldr": "The paper focuses on improving offline A/B testing (OPE) using LLMs and presents benchmarks for large-scale datasets. The introduced two_agent framework optimizes code efficiency and demonstrates superior reliability and success compared to baseline methods.", "motivation": "The motivation is to optimize offline A/B testing (OPE) for evaluating technologies in domains where online testing is expensive or risky, such as healthcare and recommender systems, leveraging LLMs and agent-based approaches for better performance.", "method": "The authors propose the GrowthHacker benchmark and two_agent framework, which iteratively optimize code and evaluate results using LLMs. They compare results across baseline methods with datasets and protocols established from Open Bandit Pipeline and Scope-RL.", "result": "The two_agent framework outperformed baselines with 100% reliability, achieving an average improvement of 106.7% in positive outcomes. Success rates of 45% were reported, surpassing other methods like AutoGen (34%).", "conclusion": "LLM-based agents are effective in enhancing OPE systems, presenting opportunities for scaling automated optimization processes in data-driven decision-making setups."}}
{"id": "2511.00917", "pdf": "https://arxiv.org/pdf/2511.00917", "abs": "https://arxiv.org/abs/2511.00917", "authors": ["Junyao Shi", "Rujia Yang", "Kaitian Chao", "Selina Bingqing Wan", "Yifei Shao", "Jiahui Lei", "Jianing Qian", "Long Le", "Pratik Chaudhari", "Kostas Daniilidis", "Chuan Wen", "Dinesh Jayaraman"], "title": "Maestro: Orchestrating Robotics Modules with Vision-Language Models for Zero-Shot Generalist Robots", "categories": ["cs.RO", "cs.AI"], "comment": "Project website: https://maestro-robot.github.io", "summary": "Today's best-explored routes towards generalist robots center on collecting\never larger \"observations-in actions-out\" robotics datasets to train large\nend-to-end models, copying a recipe that has worked for vision-language models\n(VLMs). We pursue a road less traveled: building generalist policies directly\naround VLMs by augmenting their general capabilities with specific robot\ncapabilities encapsulated in a carefully curated set of perception, planning,\nand control modules. In Maestro, a VLM coding agent dynamically composes these\nmodules into a programmatic policy for the current task and scenario. Maestro's\narchitecture benefits from a streamlined closed-loop interface without many\nmanually imposed structural constraints, and a comprehensive and diverse tool\nrepertoire. As a result, it largely surpasses today's VLA models for zero-shot\nperformance on challenging manipulation skills. Further, Maestro is easily\nextensible to incorporate new modules, easily editable to suit new embodiments\nsuch as a quadruped-mounted arm, and even easily adapts from minimal real-world\nexperiences through local code edits.", "AI": {"tldr": "Maestro integrates vision-language models with modular robot capabilities, achieving superior zero-shot manipulation performance and adaptability.", "motivation": "Current methods aim to train end-to-end robotics models on large datasets, similar to VLMs. This paper seeks a different approach by leveraging VLMs to design generalist robotic policies.", "method": "A VLM coding agent dynamically creates policies by composing specialized perception, planning, and control modules tailored to tasks and scenarios.", "result": "Maestro significantly outperforms vision-language action models in zero-shot task performance while being extensible, adaptable, and editable for diverse robotic systems.", "conclusion": "The paper demonstrates an innovative way to build versatile and adaptable robot frameworks using VLM and modular components, offering both superior performance and flexibility."}}
{"id": "2511.00067", "pdf": "https://arxiv.org/pdf/2511.00067", "abs": "https://arxiv.org/abs/2511.00067", "authors": ["Zhixing Li", "Arsham Gholamzadeh Khoee", "Yinan Yu"], "title": "Latent Domain Prompt Learning for Vision-Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The objective of domain generalization (DG) is to enable models to be robust\nagainst domain shift. DG is crucial for deploying vision-language models (VLMs)\nin real-world applications, yet most existing methods rely on domain labels\nthat may not be available and often ambiguous. We instead study the DG setting\nwhere models must generalize well without access to explicit domain labels. Our\nkey idea is to represent an unseen target domain as a combination of latent\ndomains automatically discovered from training data, enabling the model to\nadaptively transfer knowledge across domains. To realize this, we perform\nlatent domain clustering on image features and fuse domain-specific text\nfeatures based on the similarity between the input image and each latent\ndomain. Experiments on four benchmarks show that this strategy yields\nconsistent gains over VLM-based baselines and provides new insights into\nimproving robustness under domain shift.", "AI": {"tldr": "The paper proposes a method to improve domain generalization in vision-language models (VLMs) without relying on explicit domain labels.", "motivation": "Domain generalization is essential for vision-language models as they need to adapt effectively to real-world scenarios with domain shifts, often without explicit domain labels.", "method": "The authors perform latent domain clustering on image features and fuse domain-specific text features based on the similarity between an input image and latent domains discovered in training data.", "result": "Experiments on four benchmarks demonstrate consistent improvements over VLM-based baselines, highlighting enhanced robustness against domain shifts.", "conclusion": "Latent domain representation and adaptive knowledge transfer are effective strategies for improving domain generalization in VLMs without requiring explicit domain labels."}}
{"id": "2511.00171", "pdf": "https://arxiv.org/pdf/2511.00171", "abs": "https://arxiv.org/abs/2511.00171", "authors": ["Rahul Ghosh", "Baishali Chaudhury", "Hari Prasanna Das", "Meghana Ashok", "Ryan Razkenari", "Sungmin Hong", "Chun-Hao Liu"], "title": "CompAgent: An Agentic Framework for Visual Compliance Verification", "categories": ["cs.CV"], "comment": "Under review", "summary": "Visual compliance verification is a critical yet underexplored problem in\ncomputer vision, especially in domains such as media, entertainment, and\nadvertising where content must adhere to complex and evolving policy rules.\nExisting methods often rely on task-specific deep learning models trained on\nmanually labeled datasets, which are costly to build and limited in\ngeneralizability. While recent multi-modal large language models (MLLMs) offer\nbroad real-world knowledge and policy understanding, they struggle to reason\nover fine-grained visual details and apply structured compliance rules\neffectively on their own. In this paper, we propose CompAgent, the first\nagentic framework for visual compliance verification. CompAgent augments MLLMs\nwith a suite of visual tools - such as object detectors, face analyzers, NSFW\ndetectors, and captioning models - and introduces a planning agent that\ndynamically selects appropriate tools based on the compliance policy. A\nverification agent then integrates image, tool outputs, and policy context to\nperform multi-modal reasoning. Experiments on public benchmarks show that\nCompAgent outperforms specialized classifiers, direct MLLM prompting, and\ncurated routing baselines, achieving up to 76% F1 score and a 10% improvement\nover the state-of-the-art on the UnsafeBench dataset. Our results demonstrate\nthe effectiveness of agentic planning and tool-augmented reasoning for\nscalable, accurate, and adaptable visual compliance verification.", "AI": {"tldr": "CompAgent is proposed as a novel framework for visual compliance verification using multi-modal large language models (MLLMs) augmented with visual tools for task adaptability and improved performance.", "motivation": "Current methods for visual compliance verification depend on costly, manually labeled datasets that have limitations in generalizability. Existing MLLMs struggle to reason about fine-grained details and structured compliance rules.", "method": "The paper introduces CompAgent, a system that combines MLLMs with specific visual tools like object detectors and captioning models. A planning agent dynamically selects the required tools based on compliance policies, and a verification agent performs multi-modal reasoning and integrates outputs for policy assessment.", "result": "CompAgent outperforms other methods, achieving up to 76% F1 score and improving performance by 10% on the UnsafeBench dataset compared to the state-of-the-art.", "conclusion": "The framework demonstrates that augmenting MLLMs with specialized tools and agentic planning enhances accuracy and scalability in visual compliance verification tasks."}}
{"id": "2511.00469", "pdf": "https://arxiv.org/pdf/2511.00469", "abs": "https://arxiv.org/abs/2511.00469", "authors": ["Zhongxiang Lei", "Qi Yang", "Ping Qiu", "Gang Zhang", "Yuanchi Ma", "Jinyan Liu"], "title": "Why Federated Optimization Fails to Achieve Perfect Fitting? A Theoretical Perspective on Client-Side Optima", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Federated optimization is a constrained form of distributed optimization that\nenables training a global model without directly sharing client data. Although\nexisting algorithms can guarantee convergence in theory and often achieve\nstable training in practice, the reasons behind performance degradation under\ndata heterogeneity remain unclear. To address this gap, the main contribution\nof this paper is to provide a theoretical perspective that explains why such\ndegradation occurs. We introduce the assumption that heterogeneous client data\nlead to distinct local optima, and show that this assumption implies two key\nconsequences: 1) the distance among clients' local optima raises the lower\nbound of the global objective, making perfect fitting of all client data\nimpossible; and 2) in the final training stage, the global model oscillates\nwithin a region instead of converging to a single optimum, limiting its ability\nto fully fit the data. These results provide a principled explanation for\nperformance degradation in non-iid settings, which we further validate through\nexperiments across multiple tasks and neural network architectures. The\nframework used in this paper is open-sourced at:\nhttps://github.com/NPCLEI/fedtorch.", "AI": {"tldr": "This paper discusses the performance degradation in federated optimization under data heterogeneity and introduces a theoretical explanation for the problem.", "motivation": "To address the unclear reasons behind performance degradation in federated optimization due to data heterogeneity.", "method": "The paper assumes heterogeneous client data causing distinct local optima and analyzes its implications through theoretical explanations and experiments.", "result": "Key findings include: (1) heterogeneity raises the lower bound of the global objective; (2) the global model oscillates in a region without converging fully. Experimental validation supports these findings.", "conclusion": "The paper explains why federated optimization struggles in non-iid settings and provides theoretical and experimental insights, aiding advancements in distributed learning approaches."}}
{"id": "2511.00739", "pdf": "https://arxiv.org/pdf/2511.00739", "abs": "https://arxiv.org/abs/2511.00739", "authors": ["Ritik Raj", "Hong Wang", "Tushar Krishna"], "title": "A CPU-Centric Perspective on Agentic AI", "categories": ["cs.AI", "cs.LG", "cs.MA"], "comment": null, "summary": "Agentic AI frameworks add a decision-making orchestrator embedded with\nexternal tools, including web search, Python interpreter, contextual database,\nand others, on top of monolithic LLMs, turning them from passive text oracles\ninto autonomous problem-solvers that can plan, call tools, remember past steps,\nand adapt on the fly.\n  This paper aims to characterize and understand the system bottlenecks\nintroduced by agentic AI workloads from a largely overlooked CPU-centric\nperspective. We first systematically characterize Agentic AI on the basis of\norchestrator/decision making component, inference path dynamics and\nrepetitiveness of the agentic flow which directly influences the system-level\nperformance. Thereafter, based on the characterization, we choose five\nrepresentative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow,\nLangchain and SWE-Agent to profile latency, throughput and energy metrics and\ndemystify the significant impact of CPUs on these metrics relative to GPUs. We\nobserve that - 1. Tool processing on CPUs can take up to 90.6% of the total\nlatency; 2. Agentic throughput gets bottlenecked either by CPU factors -\ncoherence, synchronization and over-subscription of cores or GPU factors - main\nmemory capacity and bandwidth; \\circled{3} CPU dynamic energy consumes up to\n44% of the total dynamic energy at large batch sizes. Based on the profiling\ninsights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching\n(CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and\nheterogeneous agentic workloads respectively to demonstrate the potential to\nimprove the performance, efficiency, and scalability of agentic AI. We achieve\nup to 2.1x and 1.41x P50 latency speedup compared to the multi-processing\nbenchmark for homogeneous and heterogeneous agentic workloads respectively.", "AI": {"tldr": "The paper examines the system bottlenecks in agentic AI from a CPU-centric perspective and proposes optimizations to enhance performance, efficiency, and scalability.", "motivation": "To analyze and address the overlooked impact of CPUs on the performance bottlenecks in agentic AI systems, which are becoming essential for autonomous decision-making and problem-solving tasks.", "method": "The authors systematically characterize agentic AI systems based on decision-making, inference path dynamics, and repetitiveness. They profile five representative AI workloads to analyze latency, throughput, and energy usage metrics, emphasizing CPU contributions. Further, they propose optimizations like CPU and GPU-Aware Micro-batching (CGAM) and Mixed Agentic Workload Scheduling (MAWS).", "result": "The study reveals critical CPU-related bottlenecks, including CPUs accounting for up to 90.6% latency in tool processing and up to 44% dynamic energy consumption. Implementing optimizations led to significant performance improvements, achieving up to 2.1x and 1.41x P50 latency speedup for homogeneous and heterogeneous workloads, respectively.", "conclusion": "Agentic AI systems encounter significant CPU-induced performance bottlenecks, which can be mitigated through targeted optimizations like CGAM and MAWS, resulting in notable improvements in efficiency and scalability."}}
{"id": "2511.00536", "pdf": "https://arxiv.org/pdf/2511.00536", "abs": "https://arxiv.org/abs/2511.00536", "authors": ["Wenya Xie", "Shaochen", "Zhong", "Hoang Anh Duy Le", "Zhaozhuo Xu", "Jianwen Xie", "Zirui Liu"], "title": "Word Salad Chopper: Reasoning Models Waste A Ton Of Decoding Budget On Useless Repetitions, Self-Knowingly", "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) are often bottlenecked by the high cost of\noutput tokens. We show that a significant portion of these tokens are useless\nself-repetitions - what we call \"word salad\" - that exhaust the decoding budget\nwithout adding value. Interestingly, we observe that LRMs are self-aware when\ntrapped in these loops: the hidden states of <\\n\\n> tokens trailing each\nreasoning chunk exhibit patterns that allow us to detect word salad behavior\non-the-fly via a single-layer linear classifier. Once detected, a simple chop\nappended by a straightforward regeneration prompt yields substantial length\nsavings with minimal quality loss. Our work offers WordSaladChopper (WSC) - a\nlightweight, turnkey component for LRM that is minimally invasive to its\nreasoning trajectory by only removing semantically redundant tokens. Given its\nlow overhead, strong savings, and the lack of semantic value of word salad\ntokens, we believe it is not too far-fetched to argue that WSC - or a similar\ncomponent - is a must-have for all LRM applications with user experience in\nmind. Our code is publicly available at\nhttps://github.com/wenyaxie023/WordSaladChopper.", "AI": {"tldr": "The paper introduces WordSaladChopper (WSC), a lightweight tool to remove useless repetitive tokens (word salad) in Large Reasoning Models (LRMs), reducing token costs without compromising output quality.", "motivation": "To address the inefficiency in LRMs caused by 'word salad' \u2013 continuous, redundant, and semantically void tokens \u2013 which waste decoding budgets and degrade user experience.", "method": "The authors developed a simple, single-layer linear classifier to detect and remove word salad in the output of LRMs on-the-fly. They then implemented a regeneration prompt to replace removed tokens, achieving more concise results.", "result": "The proposed method significantly reduces output token length while preserving the semantic value and quality of the responses, demonstrating efficiency with low overhead.", "conclusion": "Implementing WordSaladChopper or a similar tool is essential for improving user experience in LRM applications, as it reduces redundancy, saves costs, and maintains high reasoning quality."}}
{"id": "2511.00839", "pdf": "https://arxiv.org/pdf/2511.00839", "abs": "https://arxiv.org/abs/2511.00839", "authors": ["John Yang", "Kilian Lieret", "Joyce Yang", "Carlos E. Jimenez", "Ofir Press", "Ludwig Schmidt", "Diyi Yang"], "title": "CodeClash: Benchmarking Goal-Oriented Software Engineering", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Current benchmarks for coding evaluate language models (LMs) on concrete,\nwell-specified tasks such as fixing specific bugs or writing targeted tests.\nHowever, human programmers do not spend all day incessantly addressing isolated\ntasks. Instead, real-world software development is grounded in the pursuit of\nhigh-level goals, like improving user retention or reducing costs. Evaluating\nwhether LMs can also iteratively develop code to better accomplish open-ended\nobjectives without any explicit guidance remains an open challenge. To address\nthis, we introduce CodeClash, a benchmark where LMs compete in multi-round\ntournaments to build the best codebase for achieving a competitive objective.\nEach round proceeds in two phases: agents edit their code, then their codebases\ncompete head-to-head in a code arena that determines winners based on\nobjectives like score maximization, resource acquisition, or survival. Whether\nit's writing notes, scrutinizing documentation, analyzing competition logs, or\ncreating test suites, models must decide for themselves how to improve their\ncodebases both absolutely and against their opponents. We run 1680 tournaments\n(25,200 rounds total) to evaluate 8 LMs across 6 arenas. Our results reveal\nthat while models exhibit diverse development styles, they share fundamental\nlimitations in strategic reasoning. Models also struggle with long-term\ncodebase maintenance, as repositories become progressively messy and redundant.\nThese limitations are stark: top models lose every round against expert human\nprogrammers. We open-source CodeClash to advance the study of autonomous,\ngoal-oriented code development.", "AI": {"tldr": "The paper introduces CodeClash, a benchmark for evaluating language models (LMs) in competitive programming scenarios to achieve open-ended objectives, revealing their limitations in strategic reasoning and long-term codebase management.", "motivation": "To address the gap between current AI coding benchmarks, which focus on isolated tasks, and real-world programming goals that require iterative and strategic development for higher-level objectives.", "method": "CodeClash involves multi-round tournaments where LMs independently edit codebases to compete in arenas evaluating tasks like score maximization or resource management. Models' strategies and outcomes are analyzed over 25,200 rounds with 8 LMs and human competitors.", "result": "The study found that LMs exhibit varied but strategically limited coding approaches, struggle with maintaining clean and efficient codebases over time, and perform worse than humans in competitive tasks.", "conclusion": "While LMs show potential in certain areas of code development, they are far from matching human-level performance in strategic, long-term, and competitive programming. The CodeClash benchmark opens a pathway for further research on autonomous coding improvements."}}
{"id": "2511.00933", "pdf": "https://arxiv.org/pdf/2511.00933", "abs": "https://arxiv.org/abs/2511.00933", "authors": ["Xiangyu Shi", "Zerui Li", "Yanyuan Qiao", "Qi Wu"], "title": "Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Recent advances in Vision-and-Language Navigation in Continuous Environments\n(VLN-CE) have leveraged multimodal large language models (MLLMs) to achieve\nzero-shot navigation. However, existing methods often rely on panoramic\nobservations and two-stage pipelines involving waypoint predictors, which\nintroduce significant latency and limit real-world applicability. In this work,\nwe propose Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that\neliminates the need for panoramic views and waypoint predictors. Our approach\nuses only three frontal RGB-D images combined with natural language\ninstructions, enabling MLLMs to directly predict actions. To enhance decision\nrobustness, we introduce an Uncertainty-Aware Reasoning module that integrates\n(i) a Disambiguation Module for avoiding local optima, and (ii) a Future-Past\nBidirectional Reasoning mechanism for globally coherent planning. Experiments\non both simulated and real-robot environments demonstrate that our method\nsignificantly reduces per-step latency while achieving competitive or superior\nperformance compared to panoramic-view baselines. These results demonstrate the\npracticality and effectiveness of Fast-SmartWay for real-world zero-shot\nembodied navigation.", "AI": {"tldr": "Fast-SmartWay eliminates panoramic views and waypoint predictors for zero-shot navigation using multimodal large language models (MLLMs), combining RGB-D images and natural language for direct action prediction. It decreases latency and competes effectively with baselines.", "motivation": "Existing Vision-and-Language Navigation systems suffer from limitations due to reliance on panoramic observations and two-stage pipelines, which hinder real-world applicability due to latency.", "method": "Fast-SmartWay uses frontal RGB-D images and natural language to enable direct action prediction by MLLMs, enhanced by an Uncertainty-Aware Reasoning module with Disambiguation and Bidirectional Reasoning components.", "result": "Fast-SmartWay significantly reduces latency per decision while achieving competitive or superior performance compared to panoramic-view navigation baselines in simulated and real-world robot environments.", "conclusion": "Fast-SmartWay is effective and practical for real-world zero-shot embodied navigation, bypassing key limitations of prior approaches while maintaining high performance."}}
{"id": "2511.00070", "pdf": "https://arxiv.org/pdf/2511.00070", "abs": "https://arxiv.org/abs/2511.00070", "authors": ["Muhammad Bilal Awan", "Abdul Razzaq", "Abdul Shahid"], "title": "Benchmarking Generative AI Against Bayesian Optimization for Constrained Multi-Objective Inverse Design", "categories": ["cs.LG", "cs.AI", "90C29 (Primary), 68T07, 65K05 (Secondary)", "G.1.6; I.2.6; J.6"], "comment": "17 pages, 2 Figures", "summary": "This paper investigates the performance of Large Language Models (LLMs) as\ngenerative optimizers for solving constrained multi-objective regression tasks,\nspecifically within the challenging domain of inverse design\n(property-to-structure mapping). This problem, critical to materials\ninformatics, demands finding complex, feasible input vectors that lie on the\nPareto optimal front. While LLMs have demonstrated universal effectiveness\nacross generative and reasoning tasks, their utility in constrained,\ncontinuous, high-dimensional numerical spaces tasks they weren't explicitly\narchitected for remains an open research question. We conducted a rigorous\ncomparative study between established Bayesian Optimization (BO) frameworks and\na suite of fine-tuned LLMs and BERT models. For BO, we benchmarked the\nfoundational BoTorch Ax implementation against the state-of-the-art q-Expected\nHypervolume Improvement (qEHVI, BoTorchM). The generative approach involved\nfine-tuning models via Parameter-Efficient Fine-Tuning (PEFT), framing the\nchallenge as a regression problem with a custom output head. Our results show\nthat BoTorch qEHVI achieved perfect convergence (GD=0.0), setting the\nperformance ceiling. Crucially, the best-performing LLM (WizardMath-7B)\nachieved a Generational Distance (GD) of 1.21, significantly outperforming the\ntraditional BoTorch Ax baseline (GD=15.03). We conclude that specialized BO\nframeworks remain the performance leader for guaranteed convergence, but\nfine-tuned LLMs are validated as a promising, computationally fast alternative,\ncontributing essential comparative metrics to the field of AI-driven\noptimization. The findings have direct industrial applications in optimizing\nformulation design for resins, polymers, and paints, where multi-objective\ntrade-offs between mechanical, rheological, and chemical properties are\ncritical to innovation and production efficiency.", "AI": {"tldr": "This paper evaluates the effectiveness of Large Language Models (LLMs) and Bayesian Optimization (BO) frameworks in solving constrained multi-objective regression tasks, particularly in the context of inverse design.", "motivation": "To assess whether LLMs, despite not being architected for constrained numerical tasks, can function as competitive generative optimizers in complex, high-dimensional spaces related to materials informatics.", "method": "Conducting a comparison between fine-tuned LLMs and BO frameworks, utilizing Parameter-Efficient Fine-Tuning (PEFT) for LLMs and advanced qEHVI algorithms for BO.", "result": "Specialized BO frameworks outperformed (GD=0.0), but fine-tuned LLMs (WizardMath-7B) demonstrated significant improvement over baseline BO methods (GD=1.21 vs GD=15.03).", "conclusion": "While BO frameworks ensure perfect convergence, fine-tuned LLMs provide a computationally fast and effective alternative, broadening the scope of AI-driven optimization applications."}}
{"id": "2511.00181", "pdf": "https://arxiv.org/pdf/2511.00181", "abs": "https://arxiv.org/abs/2511.00181", "authors": ["Mengfei Liang", "Yiting Qu", "Yukun Jiang", "Michael Backes", "Yang Zhang"], "title": "From Evidence to Verdict: An Agent-Based Forensic Framework for AI-Generated Image Detection", "categories": ["cs.CV", "cs.CR"], "comment": "20 pages, 6 figures", "summary": "The rapid evolution of AI-generated images poses unprecedented challenges to\ninformation integrity and media authenticity. Existing detection approaches\nsuffer from fundamental limitations: traditional classifiers lack\ninterpretability and fail to generalize across evolving generative models,\nwhile vision-language models (VLMs), despite their promise, remain constrained\nto single-shot analysis and pixel-level reasoning. To address these challenges,\nwe introduce AIFo (Agent-based Image Forensics), a novel training-free\nframework that emulates human forensic investigation through multi-agent\ncollaboration. Unlike conventional methods, our framework employs a set of\nforensic tools, including reverse image search, metadata extraction,\npre-trained classifiers, and VLM analysis, coordinated by specialized LLM-based\nagents that collect, synthesize, and reason over cross-source evidence. When\nevidence is conflicting or insufficient, a structured multi-agent debate\nmechanism allows agents to exchange arguments and reach a reliable conclusion.\nFurthermore, we enhance the framework with a memory-augmented reasoning module\nthat learns from historical cases to improve future detection accuracy. Our\ncomprehensive evaluation spans 6,000 images across both controlled laboratory\nsettings and challenging real-world scenarios, including images from modern\ngenerative platforms and diverse online sources. AIFo achieves 97.05% accuracy,\nsubstantially outperforming traditional classifiers and state-of-the-art VLMs.\nThese results demonstrate that agent-based procedural reasoning offers a new\nparadigm for more robust, interpretable, and adaptable AI-generated image\ndetection.", "AI": {"tldr": "The paper presents AIFo, a training-free framework utilizing multi-agent collaboration for robust detection of AI-generated images, achieving superior accuracy compared to traditional methods.", "motivation": "Address challenges in detecting AI-generated images due to limitations in existing approaches like classifiers and vision-language models.", "method": "Developed a multi-agent collaboration framework using forensic tools, LLM-based reasoning, structured debate mechanisms, and memory-augmented reasoning for improved detection.", "result": "AIFo achieved 97.05% accuracy in detecting AI-generated images across various scenarios, outperforming existing methods.", "conclusion": "Agent-based procedural reasoning provides a robust, interpretable, and adaptable solution for AI-generated image detection, establishing a new paradigm."}}
{"id": "2511.00543", "pdf": "https://arxiv.org/pdf/2511.00543", "abs": "https://arxiv.org/abs/2511.00543", "authors": ["Yunchuan Guan", "Yu Liu", "Ke Zhou", "Hui Li", "Sen Jia", "Zhiqi Shen", "Ziyang Wang", "Xinglin Zhang", "Tao Chen", "Jenq-Neng Hwang", "Lei Li"], "title": "Learning an Efficient Optimizer via Hybrid-Policy Sub-Trajectory Balance", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": null, "summary": "Recent advances in generative modeling enable neural networks to generate\nweights without relying on gradient-based optimization. However, current\nmethods are limited by issues of over-coupling and long-horizon. The former\ntightly binds weight generation with task-specific objectives, thereby limiting\nthe flexibility of the learned optimizer. The latter leads to inefficiency and\nlow accuracy during inference, caused by the lack of local constraints. In this\npaper, we propose Lo-Hp, a decoupled two-stage weight generation framework that\nenhances flexibility through learning various optimization policies. It adopts\na hybrid-policy sub-trajectory balance objective, which integrates on-policy\nand off-policy learning to capture local optimization policies. Theoretically,\nwe demonstrate that learning solely local optimization policies can address the\nlong-horizon issue while enhancing the generation of global optimal weights. In\naddition, we validate Lo-Hp's superior accuracy and inference efficiency in\ntasks that require frequent weight updates, such as transfer learning, few-shot\nlearning, domain generalization, and large language model adaptation.", "AI": {"tldr": "The paper introduces Lo-Hp, a decoupled two-stage framework for generating neural network weights, addressing problems like over-coupling and inefficiency in previous methods.", "motivation": "Current methods for neural network weight generation struggle with issues like over-coupling to task-specific objectives and inefficiency due to long-horizon dependencies.", "method": "Lo-Hp uses a hybrid-policy sub-trajectory balance objective that combines on-policy and off-policy learning to capture local optimization policies.", "result": "Lo-Hp demonstrated superior accuracy and efficiency in tasks requiring frequent weight updates, such as transfer learning, few-shot learning, domain generalization, and language model adaptation.", "conclusion": "Focusing on local optimization policies mitigates long-horizon issues and improves the generation of globally optimal weights, enhancing the flexibility and performance of neural network weight generation frameworks."}}
{"id": "2511.00751", "pdf": "https://arxiv.org/pdf/2511.00751", "abs": "https://arxiv.org/abs/2511.00751", "authors": ["Chiyan Loo"], "title": "Reevaluating Self-Consistency Scaling in Multi-Agent Systems", "categories": ["cs.AI", "cs.CL"], "comment": "7 pages, 3 figures", "summary": "This study examines the trade-offs of increasing sampled reasoning paths in\nself-consistency for modern large language models (LLMs). Earlier research with\nolder models showed that combining multiple reasoning chains improves results\nbefore reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we\nrevisit those claims under current model conditions. Each configuration pooled\noutputs from varying sampled reasoning paths and compared them to a single\nchain-of-thought (CoT) baseline. Larger models exhibited a more stable and\nconsistent improvement curve. The results confirm that performance gains taper\noff after moderate sampling, aligning with past findings. This plateau suggests\ndiminishing returns driven by overlap among reasoning paths. Self-consistency\nremains useful, but high-sample configurations offer little benefit relative to\ntheir computational cost.", "AI": {"tldr": "This paper investigates how increasing sampled reasoning paths in self-consistency affects performance in modern large language models, finding that gains eventually taper off.", "motivation": "To revisit and verify prior claims about self-consistency's effectiveness in reasoning tasks using newer LLMs.", "method": "The study compared varied sampled reasoning paths and single chain-of-thought baselines across Gemini 2.5 models applied to HotpotQA and Math-500 datasets.", "result": "Larger models showed stable improvement curves, but performance gains diminished after moderate sampling due to path overlap.", "conclusion": "Self-consistency is beneficial, but high-sample configurations are computationally costly and offer limited additional improvement."}}
{"id": "2511.01583", "pdf": "https://arxiv.org/pdf/2511.01583", "abs": "https://arxiv.org/abs/2511.01583", "authors": ["Daniel M. Jimenez-Gutierrez", "Enrique Zuazua", "Joaquin Del Rio", "Oleksii Sliusarenko", "Xabi Uribe-Etxebarria"], "title": "Federated Cyber Defense: Privacy-Preserving Ransomware Detection Across Distributed Systems", "categories": ["cs.CR", "cs.AI", "cs.DC", "cs.LG"], "comment": null, "summary": "Detecting malware, especially ransomware, is essential to securing today's\ninterconnected ecosystems, including cloud storage, enterprise file-sharing,\nand database services. Training high-performing artificial intelligence (AI)\ndetectors requires diverse datasets, which are often distributed across\nmultiple organizations, making centralization necessary. However, centralized\nlearning is often impractical due to security, privacy regulations, data\nownership issues, and legal barriers to cross-organizational sharing.\nCompounding this challenge, ransomware evolves rapidly, demanding models that\nare both robust and adaptable.\n  In this paper, we evaluate Federated Learning (FL) using the Sherpa.ai FL\nplatform, which enables multiple organizations to collaboratively train a\nransomware detection model while keeping raw data local and secure. This\nparadigm is particularly relevant for cybersecurity companies (including both\nsoftware and hardware vendors) that deploy ransomware detection or firewall\nsystems across millions of endpoints. In such environments, data cannot be\ntransferred outside the customer's device due to strict security, privacy, or\nregulatory constraints. Although FL applies broadly to malware threats, we\nvalidate the approach using the Ransomware Storage Access Patterns (RanSAP)\ndataset.\n  Our experiments demonstrate that FL improves ransomware detection accuracy by\na relative 9% over server-local models and achieves performance comparable to\ncentralized training. These results indicate that FL offers a scalable,\nhigh-performing, and privacy-preserving framework for proactive ransomware\ndetection across organizational and regulatory boundaries.", "AI": {"tldr": "This paper evaluates the use of Federated Learning (FL) for ransomware detection, showing it improves accuracy by 9% while preserving privacy and complying with regulatory constraints.", "motivation": "The need to improve ransomware detection across interconnected systems under privacy, security, and legal limitations without centralizing sensitive data.", "method": "Used the Sherpa.ai FL platform for Federated Learning to collaboratively train ransomware detection models using the RanSAP dataset while keeping raw data decentralized to preserve privacy.", "result": "FL raised ransomware detection accuracy by 9% compared to server-local models and matched the performance of centralized models.", "conclusion": "Federated Learning provides an effective, privacy-preserving, and scalable approach to ransomware detection, addressing challenges of data security and regulatory compliance."}}
{"id": "2511.00537", "pdf": "https://arxiv.org/pdf/2511.00537", "abs": "https://arxiv.org/abs/2511.00537", "authors": ["Peter Atandoh", "Jie Zou", "Weikang Guo", "Jiwei Wei", "Zheng Wang"], "title": "Multi-refined Feature Enhanced Sentiment Analysis Using Contextual Instruction", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Sentiment analysis using deep learning and pre-trained language models (PLMs)\nhas gained significant traction due to their ability to capture rich contextual\nrepresentations. However, existing approaches often underperform in scenarios\ninvolving nuanced emotional cues, domain shifts, and imbalanced sentiment\ndistributions. We argue that these limitations stem from inadequate semantic\ngrounding, poor generalization to diverse linguistic patterns, and biases\ntoward dominant sentiment classes. To overcome these challenges, we propose\nCISEA-MRFE, a novel PLM-based framework integrating Contextual Instruction\n(CI), Semantic Enhancement Augmentation (SEA), and Multi-Refined Feature\nExtraction (MRFE). CI injects domain-aware directives to guide sentiment\ndisambiguation; SEA improves robustness through sentiment-consistent\nparaphrastic augmentation; and MRFE combines a Scale-Adaptive Depthwise Encoder\n(SADE) for multi-scale feature specialization with an Emotion Evaluator Context\nEncoder (EECE) for affect-aware sequence modeling. Experimental results on four\nbenchmark datasets demonstrate that CISEA-MRFE consistently outperforms strong\nbaselines, achieving relative improvements in accuracy of up to 4.6% on IMDb,\n6.5% on Yelp, 30.3% on Twitter, and 4.1% on Amazon. These results validate the\neffectiveness and generalization ability of our approach for sentiment\nclassification across varied domains.", "AI": {"tldr": "The paper introduces CISEA-MRFE, a sentiment analysis framework leveraging contextual instructions, semantic enhancement, and advanced feature extraction, achieving significant accuracy improvement across benchmark datasets.", "motivation": "Existing sentiment analysis methods struggle with nuanced emotional cues, domain shifts, and imbalanced sentiment distributions due to inadequate semantic grounding and biases.", "method": "CISEA-MRFE integrates Contextual Instruction (CI), Semantic Enhancement Augmentation (SEA), and Multi-Refined Feature Extraction (MRFE) techniques using pre-trained language models.", "result": "CISEA-MRFE demonstrated accuracy improvements of up to 4.6% on IMDb, 6.5% on Yelp, 30.3% on Twitter, and 4.1% on Amazon benchmark datasets.", "conclusion": "CISEA-MRFE enhances generalization and robustness for sentiment classification by effectively addressing challenges like semantic grounding and sentiment imbalances."}}
{"id": "2511.00872", "pdf": "https://arxiv.org/pdf/2511.00872", "abs": "https://arxiv.org/abs/2511.00872", "authors": ["Zhuowen Yin", "Cuifeng Gao", "Chunsong Fan", "Wenzhang Yang", "Yinxing Xue", "Lijun Zhang"], "title": "A Comprehensive Empirical Evaluation of Agent Frameworks on Code-centric Software Engineering Tasks", "categories": ["cs.SE"], "comment": null, "summary": "Unlike traditional automation tools or static LLM-based systems, agents\ncombine decision-making and tool utilization to accomplish complex tasks,\nshowing great potential in software engineering. However, existing studies\nlargely focus on specific tasks or isolated aspects, providing an incomplete\npicture of agents' practical capabilities. To address this, we conduct a\ncomprehensive empirical study evaluating seven general-purpose agent frameworks\nacross three representative code-centric tasks: software development,\nvulnerability detection, and program repair. Each task is assessed using\nstandard, widely adopted benchmarks to ensure objective and comparable\nevaluation. Agent performance is systematically analyzed from three\ncomplementary perspectives: effectiveness (task success), efficiency (execution\nprocess), and overhead (token consumption). Our findings reveal distinct\ncapability patterns and trade-offs among the evaluated frameworks. In terms of\neffectiveness, agents achieve moderate overall performance. Regarding\nefficiency, AgentOrchestra tends to exhibit the longest trajectories and the\nmost correction attempts due to coordination overhead, whereas OpenHands\ndemonstrate stronger reflective reasoning abilities. For overhead, software\ndevelopment incurs the highest monetary cost, while GPTswarm remains the most\ncost-efficient. Furthermore, we conduct an in-depth cross-analysis of the\nrelationship between effectiveness and efficiency, exploring the underlying\nreasons behind their interplay. These findings guide both practical adoption\nand future research toward more efficient software engineering agents.", "AI": {"tldr": "The paper presents a study evaluating seven agent frameworks in software engineering tasks such as development, vulnerability detection, and program repair. Findings show varying capabilities, efficiency, and costs.", "motivation": "Existing studies on agent frameworks provide a fragmented understanding of their practical applications in software engineering. This paper aims to address the gap by offering a detailed evaluation.", "method": "The study examines seven general-purpose agent frameworks across three code-related tasks using standard benchmarks. It evaluates effectiveness, efficiency, and overhead to identify performance patterns.", "result": "Distinct patterns were observed: AgentOrchestra has longer decision processes, OpenHands excels in reflective reasoning, while GPTswarm is cost-efficient. Overall, agents showed moderate effectiveness.", "conclusion": "The findings highlight trade-offs in agent performance, providing insights for practical adoption and further research in software engineering agents."}}
{"id": "2511.00940", "pdf": "https://arxiv.org/pdf/2511.00940", "abs": "https://arxiv.org/abs/2511.00940", "authors": ["Zhe Li", "Xiang Bai", "Jieyu Zhang", "Zhuangzhe Wu", "Che Xu", "Ying Li", "Chengkai Hou", "Shanghang Zhang"], "title": "URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model", "categories": ["cs.RO", "cs.AI", "I.2.6"], "comment": "Accepted to the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025)", "summary": "Constructing accurate digital twins of articulated objects is essential for\nrobotic simulation training and embodied AI world model building, yet\nhistorically requires painstaking manual modeling or multi-stage pipelines. In\nthis work, we propose \\textbf{URDF-Anything}, an end-to-end automatic\nreconstruction framework based on a 3D multimodal large language model (MLLM).\nURDF-Anything utilizes an autoregressive prediction framework based on\npoint-cloud and text multimodal input to jointly optimize geometric\nsegmentation and kinematic parameter prediction. It implements a specialized\n$[SEG]$ token mechanism that interacts directly with point cloud features,\nenabling fine-grained part-level segmentation while maintaining consistency\nwith the kinematic parameter predictions. Experiments on both simulated and\nreal-world datasets demonstrate that our method significantly outperforms\nexisting approaches regarding geometric segmentation (mIoU 17\\% improvement),\nkinematic parameter prediction (average error reduction of 29\\%), and physical\nexecutability (surpassing baselines by 50\\%). Notably, our method exhibits\nexcellent generalization ability, performing well even on objects outside the\ntraining set. This work provides an efficient solution for constructing digital\ntwins for robotic simulation, significantly enhancing the sim-to-real transfer\ncapability.", "AI": {"tldr": "This paper introduces URDF-Anything, an automatic framework to reconstruct articulated objects using multimodal large language models, achieving notable improvements in accuracy, generalization, and physical executability.", "motivation": "Robotic simulation training and embodied AI world model building require high-precision digital twins of articulated objects, which conventional methods fail to construct efficiently.", "method": "The proposed framework employs a 3D multimodal large language model to perform autoregressive predictions on point-cloud and text multimodal input, jointly optimizing geometric segmentation and kinematic parameters through a specialized $[SEG]$ token mechanism.", "result": "URDF-Anything showed significant improvements: 17% in geometric segmentation accuracy, 29% error reduction in kinematic parameter prediction, and 50% better performance in physical executability compared to baselines.", "conclusion": "Using URDF-Anything greatly enhances the construction of digital twins for robotics, improving sim-to-real transfer and generalization to unseen objects."}}
{"id": "2511.00071", "pdf": "https://arxiv.org/pdf/2511.00071", "abs": "https://arxiv.org/abs/2511.00071", "authors": ["Ertugrul Mutlu"], "title": "Wavelet-Based Feature Extraction and Unsupervised Clustering for Parity Detection: A Feature Engineering Perspective", "categories": ["cs.LG", "eess.SP"], "comment": "8 pages, 2 figures. Code:\n  github.com/Ertugrulmutlu/Using-Wavelets-and-Clustering-to-Predict-Odd-or-Even-Numbers", "summary": "This paper explores a deliberately over-engineered approach to the classical\nproblem of parity detection -- determining whether a number is odd or even --\nby combining wavelet-based feature extraction with unsupervised clustering.\nInstead of relying on modular arithmetic, integers are transformed into\nwavelet-domain representations, from which multi-scale statistical features are\nextracted and clustered using the k-means algorithm. The resulting feature\nspace reveals meaningful structural differences between odd and even numbers,\nachieving a classification accuracy of approximately 69.67% without any label\nsupervision. These results suggest that classical signal-processing techniques,\noriginally designed for continuous data, can uncover latent structure even in\npurely discrete symbolic domains. Beyond parity detection, the study provides\nan illustrative perspective on how feature engineering and clustering may be\nrepurposed for unconventional machine learning problems, potentially bridging\nsymbolic reasoning and feature-based learning.", "AI": {"tldr": "The paper introduces an unconventional approach to detect parity using wavelet feature extraction and unsupervised k-means clustering, achieving 69.67% accuracy without labels.", "motivation": "The study aims to explore how classical signal-processing techniques can be repurposed for unconventional machine learning problems, specifically in detecting structural differences in symbolic domains like parity detection.", "method": "The method transforms integers into wavelet-domain representations, extracts multi-scale statistical features, and employs k-means clustering to classify numbers as odd or even without supervised learning.", "result": "The analysis reveals meaningful structural distinctions between odd and even numbers in the feature space, achieving a 69.67% classification accuracy.", "conclusion": "The research highlights the potential of adapting signal-processing tools like wavelet analysis for discrete symbolic domains, showcasing its broader application beyond traditional continuous data problems."}}
{"id": "2511.00191", "pdf": "https://arxiv.org/pdf/2511.00191", "abs": "https://arxiv.org/abs/2511.00191", "authors": ["Ziliang Chen", "Xin Huang", "Quanlong Guan", "Liang Lin", "Weiqi Luo"], "title": "A Retrospect to Multi-prompt Learning across Vision and Language", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ICCV", "summary": "The vision community is undergoing the unprecedented progress with the\nemergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays\nas the holy grail of accessing VLMs since it enables their fast adaptation to\ndownstream tasks with limited resources. Whereas existing researches milling\naround single-prompt paradigms, rarely investigate the technical potential\nbehind their multi-prompt learning counterparts. This paper aims to provide a\nprincipled retrospect for vision-language multi-prompt learning. We extend the\nrecent constant modality gap phenomenon to learnable prompts and then, justify\nthe superiority of vision-language transfer with multi-prompt augmentation,\nempirically and theoretically. In terms of this observation, we propose an\nEnergy-based Multi-prompt Learning (EMPL) to generate multiple prompt\nembeddings by drawing instances from an energy-based distribution, which is\nimplicitly defined by VLMs. So our EMPL is not only parameter-efficient but\nalso rigorously lead to the balance between in-domain and out-of-domain\nopen-vocabulary generalization. Comprehensive experiments have been conducted\nto justify our claims and the excellence of EMPL.", "AI": {"tldr": "This paper explores the advantages of vision-language multi-prompt learning, introducing Energy-based Multi-prompt Learning (EMPL) for better adaptability and generalization.", "motivation": "To investigate and leverage the potential of multi-prompt learning in Vision-Language Models (VLMs), moving beyond single-prompt approaches.", "method": "Proposed Energy-based Multi-prompt Learning (EMPL), which generates multiple prompt embeddings from an energy-based distribution defined by VLMs to enhance generalization capabilities while being parameter-efficient.", "result": "EMPL demonstrated superior in-domain and out-of-domain open-vocabulary generalization capabilities through comprehensive experiments.", "conclusion": "Multi-prompt learning via EMPL offers significant efficiency and performance improvements in adapting VLMs to diverse downstream tasks."}}
{"id": "2511.00579", "pdf": "https://arxiv.org/pdf/2511.00579", "abs": "https://arxiv.org/abs/2511.00579", "authors": ["G. Pillonetto", "A. Giaretta", "A. Aravkin", "M. Bisiacco", "T. Elston"], "title": "Sparse and nonparametric estimation of equations governing dynamical systems with applications to biology", "categories": ["cs.LG", "q-bio.QM", "stat.ML"], "comment": null, "summary": "Data-driven discovery of model equations is a powerful approach for\nunderstanding the behavior of dynamical systems in many scientific fields. In\nparticular, the ability to learn mathematical models from data would benefit\nsystems biology, where the complex nature of these systems often makes a bottom\nup approach to modeling unfeasible. In recent years, sparse estimation\ntechniques have gained prominence in system identification, primarily using\nparametric paradigms to efficiently capture system dynamics with minimal model\ncomplexity. In particular, the Sindy algorithm has successfully used sparsity\nto estimate nonlinear systems by extracting from a library of functions only a\nfew key terms needed to capture the dynamics of these systems. However,\nparametric models often fall short in accurately representing certain\nnonlinearities inherent in complex systems. To address this limitation, we\nintroduce a novel framework that integrates sparse parametric estimation with\nnonparametric techniques. It captures nonlinearities that Sindy cannot describe\nwithout requiring a priori information about their functional form. That is,\nwithout expanding the library of functions to include the one that is trying to\nbe discovered. We illustrate our approach on several examples related to\nestimation of complex biological phenomena.", "AI": {"tldr": "Researchers propose a hybrid framework combining sparse parametric estimation with nonparametric techniques to effectively capture the dynamics of complex systems, particularly in biology, overcoming limitations of traditional parametric models.", "motivation": "The motivation stems from the need to model complex biological systems, where traditional bottom-up modeling approaches and parametric techniques fail to handle inherent nonlinearities.", "method": "The proposed framework integrates the Sindy algorithm's sparse parametric estimation with nonparametric techniques, enabling the capture of nonlinear system dynamics without predefining a library of functional forms.", "result": "The framework successfully identifies complex nonlinearities that traditional parametric models struggle with, showcasing its efficacy with examples from biological phenomena.", "conclusion": "This hybrid approach enhances model discovery by bridging the gap between parametric and nonparametric methodologies, improving applicability in systems biology and other fields dealing with dynamic complexities."}}
{"id": "2511.00758", "pdf": "https://arxiv.org/pdf/2511.00758", "abs": "https://arxiv.org/abs/2511.00758", "authors": ["Hong Su"], "title": "Active Thinking Model: A Goal-Directed Self-Improving Framework for Real-World Adaptive Intelligence", "categories": ["cs.AI"], "comment": null, "summary": "Real-world artificial intelligence (AI) systems are increasingly required to\noperate autonomously in dynamic, uncertain, and continuously changing\nenvironments. However, most existing AI models rely on predefined objectives,\nstatic training data, and externally supplied feedback, which restrict their\nability to adapt, reflect, and improve independently. In this paper, we propose\nthe Active Thinking Model (ATM)- a unified cognitive framework that integrates\ngoal reasoning, dynamic task generation, and self-reflective learning into an\nadaptive architecture. Unlike conventional systems that passively execute fixed\nprocedures, ATM actively evaluates its performance through logical reasoning\nand environmental indicators, reuses effective methods to solve new problems,\nand generates novel strategies for unseen situations via a continuous\nself-improvement loop. A mathematically grounded theoretical analysis\ndemonstrates that ATM can autonomously evolve from suboptimal to optimal\nbehavior without external supervision and maintain bounded tracking regret\nunder changing environmental conditions.", "AI": {"tldr": "The paper introduces the Active Thinking Model (ATM), an adaptive AI framework for dynamic and uncertain environments.", "motivation": "AI systems are limited by predefined objectives and static data, which hinder their adaptability in dynamic, changing environments.", "method": "ATM integrates goal reasoning, dynamic task generation, and self-reflective learning into a continuous improvement loop.", "result": "Theoretical analysis proves ATM's capability to evolve autonomously from suboptimal to optimal behavior with bounded tracking regret.", "conclusion": "ATM establishes itself as an innovative model capable of learning, adapting, and improving in uncertain scenarios without external supervision."}}
{"id": "2511.01737", "pdf": "https://arxiv.org/pdf/2511.01737", "abs": "https://arxiv.org/abs/2511.01737", "authors": ["Obaidullah Zaland", "Feras M. Awaysheh", "Sawsan Al Zubi", "Abdul Rahman Safi", "Monowar Bhuyan"], "title": "Edge AI in Highly Volatile Environments: Is Fairness Worth the Accuracy Trade-off?", "categories": ["cs.LG", "cs.DC"], "comment": "Presented at IEEE FLTA 2025", "summary": "Federated learning (FL) has emerged as a transformative paradigm for edge\nintelligence, enabling collaborative model training while preserving data\nprivacy across distributed personal devices. However, the inherent volatility\nof edge environments, characterized by dynamic resource availability and\nheterogeneous client capabilities, poses significant challenges for achieving\nhigh accuracy and fairness in client participation. This paper investigates the\nfundamental trade-off between model accuracy and fairness in highly volatile\nedge environments. This paper provides an extensive empirical evaluation of\nfairness-based client selection algorithms such as RBFF and RBCSF against\nrandom and greedy client selection regarding fairness, model performance, and\ntime, in three benchmarking datasets (CIFAR10, FashionMNIST, and EMNIST). This\nwork aims to shed light on the fairness-performance and fairness-speed\ntrade-offs in a volatile edge environment and explore potential future research\nopportunities to address existing pitfalls in \\textit{fair client selection}\nstrategies in FL. Our results indicate that more equitable client selection\nalgorithms, while providing a marginally better opportunity among clients, can\nresult in slower global training in volatile environments\\footnote{The code for\nour experiments can be found at\nhttps://github.com/obaidullahzaland/FairFL_FLTA.", "AI": {"tldr": "This paper explores the trade-offs between fairness, model performance, and speed in federated learning across volatile edge environments.", "motivation": "Edge environments are highly dynamic and heterogeneous, causing challenges in achieving equitable client participation and maintaining model accuracy.", "method": "The paper evaluates fairness-based client selection algorithms like RBFF and RBCSF, comparing them to random and greedy selections using datasets CIFAR10, FashionMNIST, and EMNIST.", "result": "Fairness-based methods marginally improve equity among clients but result in slower global training in volatile conditions.", "conclusion": "Fair client selection strategies require balancing fairness and performance potential, with future work needed to optimize the trade-offs in FL settings."}}
{"id": "2511.00556", "pdf": "https://arxiv.org/pdf/2511.00556", "abs": "https://arxiv.org/abs/2511.00556", "authors": ["Peng Ding", "Jun Kuang", "Wen Sun", "Zongyu Wang", "Xuezhi Cao", "Xunliang Cai", "Jiajun Chen", "Shujian Huang"], "title": "Friend or Foe: How LLMs' Safety Mind Gets Fooled by Intent Shift Attack", "categories": ["cs.CL"], "comment": "Preprint, 14 pages, 5 figures, 7 tables", "summary": "Large language models (LLMs) remain vulnerable to jailbreaking attacks\ndespite their impressive capabilities. Investigating these weaknesses is\ncrucial for robust safety mechanisms. Existing attacks primarily distract LLMs\nby introducing additional context or adversarial tokens, leaving the core\nharmful intent unchanged. In this paper, we introduce ISA (Intent Shift\nAttack), which obfuscates LLMs about the intent of the attacks. More\nspecifically, we establish a taxonomy of intent transformations and leverage\nthem to generate attacks that may be misperceived by LLMs as benign requests\nfor information. Unlike prior methods relying on complex tokens or lengthy\ncontext, our approach only needs minimal edits to the original request, and\nyields natural, human-readable, and seemingly harmless prompts. Extensive\nexperiments on both open-source and commercial LLMs show that ISA achieves over\n70% improvement in attack success rate compared to direct harmful prompts. More\ncritically, fine-tuning models on only benign data reformulated with ISA\ntemplates elevates success rates to nearly 100%. For defense, we evaluate\nexisting methods and demonstrate their inadequacy against ISA, while exploring\nboth training-free and training-based mitigation strategies. Our findings\nreveal fundamental challenges in intent inference for LLMs safety and\nunderscore the need for more effective defenses. Our code and datasets are\navailable at https://github.com/NJUNLP/ISA.", "AI": {"tldr": "This paper introduces the Intent Shift Attack (ISA), a method that manipulates large language models (LLMs) by disguising harmful intents, achieving higher attack success rates and showcasing existing defense inadequacies.", "motivation": "The motivation of this paper lies in improving robustness and safety mechanisms for LLMs by understanding vulnerabilities in intent recognition and addressing limitations of existing attack strategies.", "method": "The authors establish a taxonomy of intent transformations to create attacks that LLMs misidentify as benign. ISA modifies original requests minimally but effectively, producing natural-looking prompts with harmful hidden intent.", "result": "ISA demonstrated over 70% improved attack success rates over direct harmful prompts, and achieved nearly 100% success when fine-tuned on reformulated data. Existing defense methods were found ineffective against ISA.", "conclusion": "The paper highlights fundamental challenges in intent inference and the insufficiency of current defenses, emphasizing the need for improved methods. Additionally, the findings stress the need for more robust safety security measures in LLMs."}}
{"id": "2511.00901", "pdf": "https://arxiv.org/pdf/2511.00901", "abs": "https://arxiv.org/abs/2511.00901", "authors": ["Vincenzo De Martino", "Stefano Lambiase", "Fabiano Pecorelli", "Willem-Jan van den Heuvel", "Filomena Ferrucci", "Fabio Palomba"], "title": "Sustainability of Machine Learning-Enabled Systems: The Machine Learning Practitioner's Perspective", "categories": ["cs.SE"], "comment": null, "summary": "Software sustainability is a key multifaceted non-functional requirement that\nencompasses environmental, social, and economic concerns, yet its integration\ninto the development of Machine Learning (ML)-enabled systems remains an open\nchallenge. While previous research has explored high-level sustainability\nprinciples and policy recommendations, limited empirical evidence exists on how\nsustainability is practically managed in ML workflows. Existing studies\npredominantly focus on environmental sustainability, e.g., carbon footprint\nreduction, while missing the broader spectrum of sustainability dimensions and\nthe challenges practitioners face in real-world settings. To address this gap,\nwe conduct an empirical study to characterize sustainability in ML-enabled\nsystems from a practitioner's perspective. We investigate (1) how ML engineers\nperceive and describe sustainability, (2) the software engineering practices\nthey adopt to support it, and (3) the key challenges hindering its adoption. We\nfirst perform a qualitative analysis based on interviews with eight experienced\nML engineers, followed by a large-scale quantitative survey with 203 ML\npractitioners. Our key findings reveal a significant disconnection between\nsustainability awareness and its systematic implementation, highlighting the\nneed for more structured guidelines, measurement frameworks, and regulatory\nsupport.", "AI": {"tldr": "The paper investigates sustainability in ML-enabled systems through interviews and surveys, uncovering gaps between awareness and implementation of sustainability practices.", "motivation": "The authors aim to address the lack of empirical evidence and practical understanding regarding the integration of multifaceted sustainability (environmental, social, economic) in ML-enabled systems.", "method": "A mixed-method approach involving qualitative interviews with 8 experienced ML engineers and a large-scale quantitative survey with 203 ML practitioners.", "result": "The study identifies a gap between sustainability awareness and systematic implementation and calls for structured guidelines, measurement frameworks, and regulatory support.", "conclusion": "Sustainability in ML workflows requires actionable frameworks and tools to bridge the gap between theoretical awareness and practical implementation."}}
{"id": "2511.00983", "pdf": "https://arxiv.org/pdf/2511.00983", "abs": "https://arxiv.org/abs/2511.00983", "authors": ["Yizhao Qian", "Yujie Zhu", "Jiayuan Luo", "Li Liu", "Yixuan Yuan", "Guochen Ning", "Hongen Liao"], "title": "Breaking the Latency Barrier: Synergistic Perception and Control for High-Frequency 3D Ultrasound Servoing", "categories": ["cs.RO"], "comment": null, "summary": "Real-time tracking of dynamic targets amidst large-scale, high-frequency\ndisturbances remains a critical unsolved challenge in Robotic Ultrasound\nSystems (RUSS), primarily due to the end-to-end latency of existing systems.\nThis paper argues that breaking this latency barrier requires a fundamental\nshift towards the synergistic co-design of perception and control. We realize\nit in a novel framework with two tightly-coupled contributions: (1) a Decoupled\nDual-Stream Perception Network that robustly estimates 3D translational state\nfrom 2D images at high frequency, and (2) a Single-Step Flow Policy that\ngenerates entire action sequences in one inference pass, bypassing the\niterative bottleneck of conventional policies. This synergy enables a\nclosed-loop control frequency exceeding 60Hz. On a dynamic phantom, our system\nnot only tracks complex 3D trajectories with a mean error below 6.5mm but also\ndemonstrates robust re-acquisition from over 170mm displacement. Furthermore,\nit can track targets at speeds of 102mm/s, achieving a terminal error below\n1.7mm. Moreover, in-vivo experiments on a human volunteer validate the\nframework's effectiveness and robustness in a realistic clinical setting. Our\nwork presents a RUSS holistically architected to unify high-bandwidth tracking\nwith large-scale repositioning, a critical step towards robust autonomy in\ndynamic clinical environments.", "AI": {"tldr": "The paper introduces a novel framework combining perception and control design to achieve high-frequency, real-time target tracking in Robotic Ultrasound Systems (RUSS), overcoming latency issues.", "motivation": "The primary challenge addressed is the end-to-end latency in Robotic Ultrasound Systems, hindering effective real-time tracking in dynamic and clinical environments.", "method": "The paper proposes a Decoupled Dual-Stream Perception Network for robust 3D state estimation and a Single-Step Flow Policy for entire action sequence generation, enabling high-bandwidth closed-loop control.", "result": "The system achieves closed-loop control over 60Hz, accurate 3D tracking with mean error under 6.5mm, robust re-acquisition, and validated performance in human in-vivo experiments.", "conclusion": "The framework represents a significant step forward for autonomous Robotic Ultrasound Systems by merging high-frequency tracking with large-scale positional adjustments in dynamic scenarios."}}
{"id": "2511.00076", "pdf": "https://arxiv.org/pdf/2511.00076", "abs": "https://arxiv.org/abs/2511.00076", "authors": ["Zihao Wan", "Pau Tong Lin Xu", "Fuwen Luo", "Ziyue Wang", "Peng Li", "Yang Liu"], "title": "Bridging Vision, Language, and Mathematics: Pictographic Character Reconstruction with B\u00e9zier Curves", "categories": ["cs.LG"], "comment": null, "summary": "While Vision-language Models (VLMs) have demonstrated strong semantic\ncapabilities, their ability to interpret the underlying geometric structure of\nvisual information is less explored. Pictographic characters, which combine\nvisual form with symbolic structure, provide an ideal test case for this\ncapability. We formulate this visual recognition challenge in the mathematical\ndomain, where each character is represented by an executable program of\ngeometric primitives. This is framed as a program synthesis task, training a\nVLM to decompile raster images into programs composed of B\\'ezier curves. Our\nmodel, acting as a \"visual decompiler\", demonstrates performance superior to\nstrong zero-shot baselines, including GPT-4o. The most significant finding is\nthat when trained solely on modern Chinese characters, the model is able to\nreconstruct ancient Oracle Bone Script in a zero-shot context. This\ngeneralization provides strong evidence that the model acquires an abstract and\ntransferable geometric grammar, moving beyond pixel-level pattern recognition\nto a more structured form of visual understanding.", "AI": {"tldr": "The paper develops a Vision-language Model (VLM) to decode geometric programs from pictographic characters, achieving abstract visual understanding beyond pixel recognition.", "motivation": "Explore how Vision-language Models (VLMs) interpret geometric structures in visual information and their generalization abilities using pictographic characters.", "method": "The model is trained to decompile raster images into geometric program representations using B'\u00e9zier curves, framed as a program synthesis task.", "result": "The model outperformed strong baselines, including GPT-4, and successfully reconstructed ancient Oracle Bone Script characters, showcasing its ability to generalize.", "conclusion": "The study demonstrates that the model acquires a transferable geometric grammar, suggesting an abstract structured visual comprehension, surpassing mere pixel-based pattern detection."}}
{"id": "2511.00211", "pdf": "https://arxiv.org/pdf/2511.00211", "abs": "https://arxiv.org/abs/2511.00211", "authors": ["Wenxuan Zhang", "Peng Hu"], "title": "An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "The increasing adoption of satellite Internet with low-Earth-orbit (LEO)\nsatellites in mega-constellations allows ubiquitous connectivity to rural and\nremote areas. However, weather events have a significant impact on the\nperformance and reliability of satellite Internet. Adverse weather events such\nas snow and rain can disturb the performance and operations of satellite\nInternet's essential ground terminal components, such as satellite antennas,\nsignificantly disrupting the space-ground link conditions between LEO\nsatellites and ground stations. This challenge calls for not only region-based\nweather forecasts but also fine-grained detection capability on ground terminal\ncomponents of fine-grained weather conditions. Such a capability can assist in\nfault diagnostics and mitigation for reliable satellite Internet, but its\nsolutions are lacking, not to mention the effectiveness and generalization that\nare essential in real-world deployments. This paper discusses an efficient\ntransfer learning (TL) method that can enable a ground component to locally\ndetect representative weather-related conditions. The proposed method can\ndetect snow, wet, and other conditions resulting from adverse and typical\nweather events and shows superior performance compared to the typical deep\nlearning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL\nmethod also shows the advantage of being generalizable to various scenarios.", "AI": {"tldr": "The paper presents a transfer learning method for monitoring weather impact on satellite Internet performance.", "motivation": "The paper aims to address the issue of weather events disrupting satellite Internet services and proposes fine-grained detection methods for satellite ground components.", "method": "An efficient transfer learning (TL) approach is utilized to locally detect weather-related conditions like snow and wet surfaces on ground components.", "result": "The proposed TL method outperforms typical deep learning techniques such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO in detecting adverse weather conditions.", "conclusion": "The TL method enhances the reliability of satellite Internet by offering generalizable and efficient weather condition detection, assisting fault diagnostics and real-world deployment scenarios."}}
{"id": "2511.00617", "pdf": "https://arxiv.org/pdf/2511.00617", "abs": "https://arxiv.org/abs/2511.00617", "authors": ["Eric Bigelow", "Daniel Wurgaft", "YingQiao Wang", "Noah Goodman", "Tomer Ullman", "Hidenori Tanaka", "Ekdeep Singh Lubana"], "title": "Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Large language models (LLMs) can be controlled at inference time through\nprompts (in-context learning) and internal activations (activation steering).\nDifferent accounts have been proposed to explain these methods, yet their\ncommon goal of controlling model behavior raises the question of whether these\nseemingly disparate methodologies can be seen as specific instances of a\nbroader framework. Motivated by this, we develop a unifying, predictive account\nof LLM control from a Bayesian perspective. Specifically, we posit that both\ncontext- and activation-based interventions impact model behavior by altering\nits belief in latent concepts: steering operates by changing concept priors,\nwhile in-context learning leads to an accumulation of evidence. This results in\na closed-form Bayesian model that is highly predictive of LLM behavior across\ncontext- and activation-based interventions in a set of domains inspired by\nprior work on many-shot in-context learning. This model helps us explain prior\nempirical phenomena - e.g., sigmoidal learning curves as in-context evidence\naccumulates - while predicting novel ones - e.g., additivity of both\ninterventions in log-belief space, which results in distinct phases such that\nsudden and dramatic behavioral shifts can be induced by slightly changing\nintervention controls. Taken together, this work offers a unified account of\nprompt-based and activation-based control of LLM behavior, and a methodology\nfor empirically predicting the effects of these interventions.", "AI": {"tldr": "This paper provides a unified framework for controlling large language model (LLM) behavior using a Bayesian perspective, addressing both prompt-based and activation-based interventions.", "motivation": "The study is motivated by the need to understand whether the seemingly disparate methods of controlling LLMs (prompts vs. activation steering) can be unified under a broader framework and made predictive.", "method": "The paper develops a Bayesian model where context-based interventions alter belief accumulation, and activation-based steering changes prior beliefs. The model is tested across different domains inspired by many-shot in-context learning studies.", "result": "The study explains prior phenomena, such as sigmoidal learning curves, while predicting new ones like the additivity of interventions in log-belief space. It demonstrates dramatic behavioral shifts with minor intervention changes.", "conclusion": "The findings provide a unified account of LLM control, improving the predictive understanding of interventions and offering empirical methodologies for future use."}}
{"id": "2511.00763", "pdf": "https://arxiv.org/pdf/2511.00763", "abs": "https://arxiv.org/abs/2511.00763", "authors": ["Wanda Hou", "Leon Zhou", "Hong-Ye Hu", "Yi-Zhuang You", "Xiao-Liang Qi"], "title": "How Focused Are LLMs? A Quantitative Study via Repetitive Deterministic Prediction Tasks", "categories": ["cs.AI"], "comment": null, "summary": "We investigate the performance of large language models on repetitive\ndeterministic prediction tasks and study how the sequence accuracy rate scales\nwith output length. Each such task involves repeating the same operation n\ntimes. Examples include letter replacement in strings following a given rule,\ninteger addition, and multiplication of string operators in many body quantum\nmechanics. If the model performs the task through a simple repetition\nalgorithm, the success rate should decay exponentially with sequence length. In\ncontrast, our experiments on leading large language models reveal a sharp\ndouble exponential drop beyond a characteristic length scale, forming an\naccuracy cliff that marks the transition from reliable to unstable generation.\nThis indicates that the models fail to execute each operation independently. To\nexplain this phenomenon, we propose a statistical physics inspired model that\ncaptures the competition between external conditioning from the prompt and\ninternal interference among generated tokens. The model quantitatively\nreproduces the observed crossover and provides an interpretable link between\nattention induced interference and sequence level failure. Fitting the model to\nempirical results across multiple models and tasks yields effective parameters\nthat characterize the intrinsic error rate and error accumulation factor for\neach model task pair, offering a principled framework for understanding the\nlimits of deterministic accuracy in large language models.", "AI": {"tldr": "This paper studies how large language models handle repetitive deterministic tasks, finding a sharp drop-off in sequence accuracy as output length increases and proposing a statistical physics-inspired explanation.", "motivation": "To investigate how well large language models perform on tasks requiring repetitive deterministic operations and understand their limitations in sequence accuracy over extended lengths.", "method": "Experiments were conducted on leading large language models with deterministic tasks like letter replacement, integer addition, and quantum operator multiplication. A physics-inspired model was proposed to explain observed accuracy failures and the associated factors.", "result": "Findings reveal a sharp double exponential decline in sequence accuracy beyond a certain output length. The physics-inspired model successfully explains these failures and provides a framework to quantify error rates and accumulation factors.", "conclusion": "Large language models exhibit a significant accuracy cliff due to failure in consistently executing operations independently over long sequences. The study introduces a framework to understand the intrinsic limitations in their deterministic accuracy."}}
{"id": "2511.00576", "pdf": "https://arxiv.org/pdf/2511.00576", "abs": "https://arxiv.org/abs/2511.00576", "authors": ["Juan Gabriel Kostelec", "Qinghai Guo"], "title": "FlashEVA: Accelerating LLM inference via Efficient Attention", "categories": ["cs.CL", "cs.AI"], "comment": "Technical Report", "summary": "Transformer models have revolutionized natural language processing, achieving\nstate-of-the-art performance and demonstrating remarkable scalability. However,\ntheir memory demands, particularly due to maintaining full context in memory,\npose significant challenges for inference. In this paper, we present FlashEVA,\nan efficient implementation of EVA (Efficient Attention via Control Variates),\nand demonstrate how to finetune transformers to adapt to FlashEVA attention.\nOur method enables fine-tuning of Transformer models with as few as 1.5B tokens\nwhile preserving effectiveness across various downstream tasks. Notably,\nFlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory\nusage during inference compared to standard Transformer implementations.\nDespite these improvements, we observe limitations in retrieval-focused tasks.\nOur implementation offers control over the trade-off between throughput and\naccuracy through adjustable hyperparameters, providing flexibility for diverse\nuse cases. This work represents a significant step towards more efficient and\nadaptable Transformer-based models for inference.", "AI": {"tldr": "FlashEVA significantly reduces memory and computational demands for Transformer inference, achieving up to 6.7x higher throughput and 5x lower peak GPU memory usage.", "motivation": "Transformers revolutionized NLP but face challenges in inference due to high memory demands. This paper addresses the need for more efficient attention mechanisms.", "method": "FlashEVA, an efficient implementation of EVA combined with fine-tuning mechanisms, adapts Transformer models to a streamlined attention system.", "result": "FlashEVA achieves substantial gains in GPU memory efficiency and inference throughput, while preserving effectiveness in most downstream tasks, excluding retrieval-focused ones.", "conclusion": "FlashEVA improves Transformer efficiency and adaptability but still faces limitations in retrieval tasks, highlighting its potential for broader NLP applications."}}
{"id": "2511.00915", "pdf": "https://arxiv.org/pdf/2511.00915", "abs": "https://arxiv.org/abs/2511.00915", "authors": ["Jukka Ruohonen", "Abhishek Tiwari"], "title": "Empirical Derivations from an Evolving Test Suite", "categories": ["cs.SE"], "comment": "Submitted", "summary": "The paper presents a longitudinal empirical analysis of the automated,\ncontinuous, and virtualization-based software test suite of the NetBSD\noperating system. The longitudinal period observed spans from the initial roll\nout of the test suite in the early 2010s to late 2025. According to the\nresults, the test suite has grown continuously, currently covering over ten\nthousand individual test cases. Failed test cases exhibit overall stability,\nalthough there have been shorter periods marked with more frequent failures. A\nsimilar observation applies to build failures, failures of the test suite to\ncomplete, and installation failures, all of which are also captured by the\nNetBSD's testing framework. Finally, code churn and kernel modifications do not\nprovide longitudinally consistent statistical explanations for the failures.\nAlthough some periods exhibit larger effects, including particularly with\nrespect to the kernel modifications, the effects are small on average. Even\nthough only in an exploratory manner, these empirical observations contribute\nto efforts to draw conclusions from large-scale and evolving software test\nsuites.", "AI": {"tldr": "The paper investigates the evolution and effectiveness of NetBSD's automated testing framework over a span of more than a decade, highlighting growth and stability despite occasional periods of test and build failures.", "motivation": "To analyze the long-term performance, stability, and growth of NetBSD's automated test suite, offering insights into the efficacy of continuous software testing in large-scale systems.", "method": "A longitudinal empirical analysis spanning over a decade was conducted, examining test suite growth, stability of failed test cases, and the influence of factors like code churn and kernel modifications.", "result": "The test suite expanded to over 10,000 test cases, showing stable failure patterns despite occasional instability. Statistical analysis indicated that code churn and kernel changes had minor overall effects on failure rates.", "conclusion": "Findings suggest longitudinal consistency and stability in automated, evolving software test frameworks. This helps understand the dynamics of large-scale test suites and informs future development."}}
{"id": "2511.00998", "pdf": "https://arxiv.org/pdf/2511.00998", "abs": "https://arxiv.org/abs/2511.00998", "authors": ["Ziye Wang", "Li Kang", "Yiran Qin", "Jiahua Ma", "Zhanglin Peng", "Lei Bai", "Ruimao Zhang"], "title": "GauDP: Reinventing Multi-Agent Collaboration through Gaussian-Image Synergy in Diffusion Policies", "categories": ["cs.RO"], "comment": "Accepted by NeurIPS 2025. Project page:\n  https://ziyeeee.github.io/gaudp.io/", "summary": "Recently, effective coordination in embodied multi-agent systems has remained\na fundamental challenge, particularly in scenarios where agents must balance\nindividual perspectives with global environmental awareness. Existing\napproaches often struggle to balance fine-grained local control with\ncomprehensive scene understanding, resulting in limited scalability and\ncompromised collaboration quality. In this paper, we present GauDP, a novel\nGaussian-image synergistic representation that facilitates scalable,\nperception-aware imitation learning in multi-agent collaborative systems.\nSpecifically, GauDP constructs a globally consistent 3D Gaussian field from\ndecentralized RGB observations, then dynamically redistributes 3D Gaussian\nattributes to each agent's local perspective. This enables all agents to\nadaptively query task-critical features from the shared scene representation\nwhile maintaining their individual viewpoints. This design facilitates both\nfine-grained control and globally coherent behavior without requiring\nadditional sensing modalities (e.g., 3D point cloud). We evaluate GauDP on the\nRoboFactory benchmark, which includes diverse multi-arm manipulation tasks. Our\nmethod achieves superior performance over existing image-based methods and\napproaches the effectiveness of point-cloud-driven methods, while maintaining\nstrong scalability as the number of agents increases.", "AI": {"tldr": "The paper introduces GauDP, a novel approach for scalable and perception-aware imitation learning in multi-agent systems, achieving improved performance and scalability.", "motivation": "To solve the challenge of effective coordination in embodied multi-agent systems where agents must balance individual perspectives with global awareness.", "method": "GauDP constructs a 3D Gaussian field from decentralized RGB observations, redistributing 3D Gaussian attributes dynamically to facilitate localized and global task-critical queries.", "result": "GauDP outperforms existing image-based methods on the RoboFactory benchmark, advancing scalability and efficacy for multi-arm manipulation tasks.", "conclusion": "GauDP effectively improves coordination and performance in multi-agent collaborative systems, achieving results comparable to point-cloud-based methods without requiring additional sensing modalities."}}
{"id": "2511.00079", "pdf": "https://arxiv.org/pdf/2511.00079", "abs": "https://arxiv.org/abs/2511.00079", "authors": ["Maximilian Willer", "Peter Ruckdeschel"], "title": "flowengineR: A Modular and Extensible Framework for Fair and Reproducible Workflow Design in R", "categories": ["cs.LG", "cs.CY", "stat.ME", "62-04, 62-07", "D.2.11; G.3; I.2.6"], "comment": "27 pages, 7 figures, 1 table", "summary": "flowengineR is an R package designed to provide a modular and extensible\nframework for building reproducible algorithmic workflows for general-purpose\nmachine learning pipelines. It is motivated by the rapidly evolving field of\nalgorithmic fairness where new metrics, mitigation strategies, and machine\nlearning methods continuously emerge. A central challenge in fairness, but also\nfar beyond, is that existing toolkits either focus narrowly on single\ninterventions or treat reproducibility and extensibility as secondary\nconsiderations rather than core design principles. flowengineR addresses this\nby introducing a unified architecture of standardized engines for data\nsplitting, execution, preprocessing, training, inprocessing, postprocessing,\nevaluation, and reporting. Each engine encapsulates one methodological task yet\ncommunicates via a lightweight interface, ensuring workflows remain\ntransparent, auditable, and easily extensible. Although implemented in R,\nflowengineR builds on ideas from workflow languages (CWL, YAWL), graph-oriented\nvisual programming languages (KNIME), and R frameworks (BatchJobs, batchtools).\nIts emphasis, however, is less on orchestrating engines for resilient parallel\nexecution but rather on the straightforward setup and management of distinct\nengines as data structures. This orthogonalization enables distributed\nresponsibilities, independent development, and streamlined integration. In\nfairness context, by structuring fairness methods as interchangeable engines,\nflowengineR lets researchers integrate, compare, and evaluate interventions\nacross the modeling pipeline. At the same time, the architecture generalizes to\nexplainability, robustness, and compliance metrics without core modifications.\nWhile motivated by fairness, it ultimately provides a general infrastructure\nfor any workflow context where reproducibility, transparency, and extensibility\nare essential.", "AI": {"tldr": "flowengineR is an R package providing a modular framework for reproducible machine learning workflows, emphasizing fairness and transparency.", "motivation": "The paper addresses the challenges in algorithmic fairness, focusing on the lack of reproducibility and extensibility in existing toolkits.", "method": "The package utilizes standardized engines for tasks like data splitting, training, evaluation, and reporting, ensuring modularity, interchangeability, and lightweight communication.", "result": "flowengineR supports fairness research by enabling integration and comparison of methods while also generalizing to other metrics like explainability, robustness, and compliance.", "conclusion": "The framework offers a scalable, transparent, and extensible solution for building machine learning workflows, applicable beyond fairness to general data processing and evaluation needs."}}
{"id": "2511.00218", "pdf": "https://arxiv.org/pdf/2511.00218", "abs": "https://arxiv.org/abs/2511.00218", "authors": ["Rajatsubhra Chakraborty", "Ana Espinosa-Momox", "Riley Haskin", "Depeng Xu", "Rosario Porras-Aguilar"], "title": "DM-QPMNET: Dual-modality fusion network for cell segmentation in quantitative phase microscopy", "categories": ["cs.CV", "cs.AI"], "comment": "5 pages, 4 figures", "summary": "Cell segmentation in single-shot quantitative phase microscopy (ssQPM) faces\nchallenges from traditional thresholding methods that are sensitive to noise\nand cell density, while deep learning approaches using simple channel\nconcatenation fail to exploit the complementary nature of polarized intensity\nimages and phase maps. We introduce DM-QPMNet, a dual-encoder network that\ntreats these as distinct modalities with separate encoding streams. Our\narchitecture fuses modality-specific features at intermediate depth via\nmulti-head attention, enabling polarized edge and texture representations to\nselectively integrate complementary phase information. This content-aware\nfusion preserves training stability while adding principled multi-modal\nintegration through dual-source skip connections and per-modality normalization\nat minimal overhead. Our approach demonstrates substantial improvements over\nmonolithic concatenation and single-modality baselines, showing that\nmodality-specific encoding with learnable fusion effectively exploits ssQPM's\nsimultaneous capture of complementary illumination and phase cues for robust\ncell segmentation.", "AI": {"tldr": "This paper introduces DM-QPMNet, a dual-encoder network for cell segmentation in single-shot quantitative phase microscopy, addressing challenges faced by traditional and deep learning methods through better integration of multi-modal data.", "motivation": "To overcome the limitations of traditional thresholding methods and simple channel concatenation in deep learning approaches, which fail to effectively utilize the complementary nature of polarized intensity images and phase maps in cell segmentation.", "method": "The proposed method, DM-QPMNet, uses a dual-encoder network architecture with separate encoding streams for each modality. It employs multi-head attention for intermediate fusion of features and incorporates dual-source skip connections along with per-modality normalization for improved multi-modal integration.", "result": "DM-QPMNet achieved significant improvements in cell segmentation performance over monolithic concatenation and single-modality approaches by effectively exploiting complementary illumination and phase information.", "conclusion": "The study demonstrates the effectiveness of modality-specific encoding combined with learnable fusion in enhancing the capabilities of single-shot quantitative phase microscopy for robust and accurate cell segmentation."}}
{"id": "2511.00637", "pdf": "https://arxiv.org/pdf/2511.00637", "abs": "https://arxiv.org/abs/2511.00637", "authors": ["Emmeran Johnson", "Alberto Rumi", "Ciara Pike-Burke", "Patrick Rebeschini"], "title": "Stochastic Shortest Path with Sparse Adversarial Costs", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We study the adversarial Stochastic Shortest Path (SSP) problem with sparse\ncosts under full-information feedback. In the known transition setting,\nexisting bounds based on Online Mirror Descent (OMD) with negative-entropy\nregularization scale with $\\sqrt{\\log S A}$, where $SA$ is the size of the\nstate-action space. While we show that this is optimal in the worst-case, this\nbound fails to capture the benefits of sparsity when only a small number $M \\ll\nSA$ of state-action pairs incur cost. In fact, we also show that the\nnegative-entropy is inherently non-adaptive to sparsity: it provably incurs\nregret scaling with $\\sqrt{\\log S}$ on sparse problems. Instead, we propose a\nfamily of $\\ell_r$-norm regularizers ($r \\in (1,2)$) that adapts to the\nsparsity and achieves regret scaling with $\\sqrt{\\log M}$ instead of\n$\\sqrt{\\log SA}$. We show this is optimal via a matching lower bound,\nhighlighting that $M$ captures the effective dimension of the problem instead\nof $SA$. Finally, in the unknown transition setting the benefits of sparsity\nare limited: we prove that even on sparse problems, the minimax regret for any\nlearner scales polynomially with $SA$.", "AI": {"tldr": "The paper addresses the adversarial Stochastic Shortest Path (SSP) problem under full-information feedback, proposing an $\u0018ell_r$-norm regularizer to account for cost sparsity, achieving improved regret scaling tied to sparsity.", "motivation": "Current methods for adversarial SSP problems fail to adapt effectively to sparsity in state-action costs, leading to suboptimal regret scaling.", "method": "A proposed $\u00018ell_r$-norm regularizer replaces the commonly used negative-entropy, adapting to sparsity and improving regret bounds.", "result": "Regret scaling shifts effectively from $\u00018sqrt{\u00018log SA}$ to $\u00018sqrt{\u00018log M}$, with $M$ representing sparse costs; additionally, a lower regret bound confirms the optimality of these bounds.", "conclusion": "The approach captures the benefits of sparsity under known transitions, but in unknown transition settings, sparsity benefits are limited as regrets scale with $SA$ for all learners."}}
{"id": "2511.00782", "pdf": "https://arxiv.org/pdf/2511.00782", "abs": "https://arxiv.org/abs/2511.00782", "authors": ["Jifan Gao", "Michael Rosenthal", "Brian Wolpin", "Simona Cristea"], "title": "Count-Based Approaches Remain Strong: A Benchmark Against Transformer and LLM Pipelines on Structured EHR", "categories": ["cs.AI"], "comment": null, "summary": "Structured electronic health records (EHR) are essential for clinical\nprediction. While count-based learners continue to perform strongly on such\ndata, no benchmarking has directly compared them against more recent\nmixture-of-agents LLM pipelines, which have been reported to outperform single\nLLMs in various NLP tasks. In this study, we evaluated three categories of\nmethodologies for EHR prediction using the EHRSHOT dataset: count-based models\nbuilt from ontology roll-ups with two time bins, based on LightGBM and the\ntabular foundation model TabPFN; a pretrained sequential transformer (CLMBR);\nand a mixture-of-agents pipeline that converts tabular histories to\nnatural-language summaries followed by a text classifier. We assessed eight\noutcomes using the EHRSHOT dataset. Across the eight evaluation tasks,\nhead-to-head wins were largely split between the count-based and the\nmixture-of-agents methods. Given their simplicity and interpretability,\ncount-based models remain a strong candidate for structured EHR benchmarking.\nThe source code is available at:\nhttps://github.com/cristea-lab/Structured_EHR_Benchmark.", "AI": {"tldr": "The paper benchmarks count-based models, a sequential transformer, and mixture-of-agents pipelines for EHR prediction. Results show split wins among approaches, emphasizing the simplicity and value of count-based models.", "motivation": "To evaluate and compare methodologies for clinical prediction using structured EHR data, especially benchmarking traditional count-based models against modern LLM pipelines.", "method": "Three methodologies were assessed using the EHRSHOT dataset: count-based models (LightGBM and TabPFN), a pretrained sequential transformer (CLMBR), and a mixture-of-agents pipeline translating tabular histories to summaries for text classification.", "result": "For eight outcomes on the EHRSHOT dataset, both count-based models and mixture-of-agents methods showed comparable performance, each winning in different tasks.", "conclusion": "Despite advancements in LLM approaches, count-based models remain efficient, interpretable, and competitive candidates for structured EHR benchmarking."}}
{"id": "2511.00602", "pdf": "https://arxiv.org/pdf/2511.00602", "abs": "https://arxiv.org/abs/2511.00602", "authors": ["Wai-Chung Kwan", "Joshua Ong Jun Leang", "Pavlos Vougiouklis", "Jeff Z. Pan", "Marco Valentino", "Pasquale Minervini"], "title": "OpenSIR: Open-Ended Self-Improving Reasoner", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language model (LLM) reasoning through reinforcement\nlearning rely on annotated datasets for verifiable rewards, which may limit\nmodels' ability to surpass human-level performance. While self-play offers a\npromising alternative, existing approaches depend on external verifiers or\ncannot learn open-endedly. We present Open-Ended Self-Improving Reasoner\n(OpenSIR), a self-play framework where an LLM learns to generate and solve\nnovel problems by alternating teacher and student roles without external\nsupervision. To generate novel problems, OpenSIR optimises for both difficulty\nand diversity, rewarding problems that challenge appropriately while exploring\ndistinct concepts, enabling open-ended mathematical discovery. Starting from a\nsingle trivial seed problem, OpenSIR substantially improves instruction models:\nLlama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to\n34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on\nGSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through\nco-evolving teacher-student roles that adaptively calibrate difficulty and\ndrive diverse exploration, progressing autonomously from basic to advanced\nmathematics.", "AI": {"tldr": "The paper introduces OpenSIR, a self-play framework for large language model reasoning without external supervision, improving mathematical problem-solving capabilities by alternating teacher-student roles.", "motivation": "Current LLM reasoning methods depend on annotated datasets, restricting models from surpassing human-level performance. A self-play framework offers a solution to enable open-ended learning without the need for external verifiers.", "method": "OpenSIR uses a self-play framework where the LLM alternates between teacher and student roles. It generates novel problems optimized for difficulty and diversity to promote mathematical discovery.", "result": "OpenSIR significantly improved performance on reasoning benchmarks like GSM8K and College Math for instruction models, showcasing notable advancements in mathematical reasoning.", "conclusion": "OpenSIR enables autonomous progression in problem-solving from basic to advanced concepts, demonstrating the potential for open-ended mathematical exploration without external supervision."}}
{"id": "2511.01043", "pdf": "https://arxiv.org/pdf/2511.01043", "abs": "https://arxiv.org/abs/2511.01043", "authors": ["Zihan Fang", "Yifan Zhang", "Yueke Zhang", "Kevin Leach", "Yu Huang"], "title": "DPO-F+: Aligning Code Repair Feedback with Developers' Preferences", "categories": ["cs.SE"], "comment": "10 pages, 2 figures", "summary": "Large Language Models (LLMs) are increasingly applied to software engineering\ntasks, especially code repair. However, developers often struggle to interpret\nmodel outputs, limiting effective human-AI teaming. Prior work largely\noptimizes repaired code while under-addressing the natural-language feedback\nthat enables comprehension and iterative improvement. We present DPO-f+, a\nnovel framework that aligns code-repair feedback with developer needs and\nprofiles. It (1) formalizes developer-profiled, domain-specific metrics for\nfeedback alignment; (2) automatically constructs pairwise preference datasets\nfrom code-repair tasks; (3) fine-tunes using Direct Preference Optimization\n(DPO) augmented with a lightweight margin signal; and (4) provides an automated\nfeedback evaluation protocol. Empirically, DPO-f+ outperforms both the baseline\nand standard DPO on generated-code accuracy and overall feedback alignment. On\nnovice programming tasks, DPO-f+ raises the top-1 pass rate by 5.71 percentage\npoints (pp) over the baseline and by 3.30 pp over DPO. On the more challenging\nSWE-bench Lite benchmark, it increases the issue-resolution rate by 1.67 pp\nover DPO and by 4.67 pp over the baseline. It also achieves the largest\nimprovement in feedback alignment, outperforming DPO and the baseline. By\naligning feedback more closely with developer needs, DPO-f+ turns LLM-assisted\nrepair from one-shot outputs into a collaborative sensemaking workflow,\nproviding a practical approach to enhancing code comprehension and fostering\nmore effective human-AI teaming in software engineering.", "AI": {"tldr": "The paper introduces DPO-f+, a framework to enhance feedback alignment in LLM-assisted code repair, demonstrating improved accuracy and alignment over previous methods.", "motivation": "The motivation of this paper is to improve the effectiveness of human-AI collaboration in code repair tasks by addressing the difficulty developers face in interpreting language model outputs and optimizing feedback alignment with developer needs.", "method": "The paper presents DPO-f+, which formalizes developer-specific metrics, builds datasets with pairwise preferences, fine-tunes using Direct Preference Optimization with a margin signal, and introduces an automated feedback evaluation protocol.", "result": "DPO-f+ outperforms baselines and standard DPO, achieving better feedback alignment and higher performance in code-repair tasks, including a 5.71 percentage point improvement in novice task performance and a 4.67 percentage point improvement in SWE-bench Lite issues over the baseline.", "conclusion": "DPO-f+ enhances the collaborative workflow of LLM-assisted code repair by aligning feedback with developer requirements, leading to improved code comprehension and human-AI teamwork in software engineering."}}
{"id": "2511.01031", "pdf": "https://arxiv.org/pdf/2511.01031", "abs": "https://arxiv.org/abs/2511.01031", "authors": ["Mathieu Dubied", "Paolo Tiso", "Robert K. Katzschmann"], "title": "AquaROM: shape optimization pipeline for soft swimmers using parametric reduced order models", "categories": ["cs.RO"], "comment": null, "summary": "The efficient optimization of actuated soft structures, particularly under\ncomplex nonlinear forces, remains a critical challenge in advancing robotics.\nSimulations of nonlinear structures, such as soft-bodied robots modeled using\nthe finite element method (FEM), often demand substantial computational\nresources, especially during optimization. To address this challenge, we\npropose a novel optimization algorithm based on a tensorial parametric reduced\norder model (PROM). Our algorithm leverages dimensionality reduction and\nsolution approximation techniques to facilitate efficient solving of nonlinear\nconstrained optimization problems. The well-structured tensorial approach\nenables the use of analytical gradients within a specifically chosen reduced\norder basis (ROB), significantly enhancing computational efficiency. To\nshowcase the performance of our method, we apply it to optimizing soft robotic\nswimmer shapes. These actuated soft robots experience hydrodynamic forces,\nsubjecting them to both internal and external nonlinear forces, which are\nincorporated into our optimization process using a data-free ROB for fast and\naccurate computations. This approach not only reduces computational complexity\nbut also unlocks new opportunities to optimize complex nonlinear systems in\nsoft robotics, paving the way for more efficient design and control.", "AI": {"tldr": "The paper presents an optimization algorithm leveraging a tensorial parametric reduced order model (PROM) to address computational challenges in optimizing soft-bodied robots under nonlinear forces.", "motivation": "There is a critical need to efficiently optimize soft-bodied robotic structures dealing with complex nonlinear forces, as existing simulations are computationally expensive.", "method": "The method introduces a tensorial PROM-based optimization algorithm that integrates dimensionality reduction and analytical gradient solutions to solve nonlinear constrained optimization problems efficiently.", "result": "The approach demonstrated enhanced computational efficiency and was successfully applied to the shape optimization of soft robotic swimmers subjected to nonlinear hydrodynamic forces.", "conclusion": "This method reduces computational complexity while enabling efficient optimization of complex nonlinear systems, advancing the design and control of soft robotic systems."}}
{"id": "2511.00083", "pdf": "https://arxiv.org/pdf/2511.00083", "abs": "https://arxiv.org/abs/2511.00083", "authors": ["Shakib Khan", "A. Ben Hamza", "Amr Youssef"], "title": "Fixed-point graph convolutional networks against adversarial attacks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Adversarial attacks present a significant risk to the integrity and\nperformance of graph neural networks, particularly in tasks where graph\nstructure and node features are vulnerable to manipulation. In this paper, we\npresent a novel model, called fixed-point iterative graph convolutional network\n(Fix-GCN), which achieves robustness against adversarial perturbations by\neffectively capturing higher-order node neighborhood information in the graph\nwithout additional memory or computational complexity. Specifically, we\nintroduce a versatile spectral modulation filter and derive the feature\npropagation rule of our model using fixed-point iteration. Unlike traditional\ndefense mechanisms that rely on additional design elements to counteract\nattacks, the proposed graph filter provides a flexible-pass filtering approach,\nallowing it to selectively attenuate high-frequency components while preserving\nlow-frequency structural information in the graph signal. By iteratively\nupdating node representations, our model offers a flexible and efficient\nframework for preserving essential graph information while mitigating the\nimpact of adversarial manipulation. We demonstrate the effectiveness of the\nproposed model through extensive experiments on various benchmark graph\ndatasets, showcasing its resilience against adversarial attacks.", "AI": {"tldr": "The paper introduces Fix-GCN, a robust graph neural network model against adversarial attacks by leveraging fixed-point iteration and spectral modulation filters.", "motivation": "Graph neural networks are vulnerable to adversarial attacks that manipulate graph structure and node features. The paper aims to address this vulnerability and enhance their robustness.", "method": "The paper proposes Fix-GCN, which employs a spectral modulation filter and a fixed-point iteration mechanism to propagate features while preserving low-frequency structural information and mitigating high-frequency perturbations.", "result": "Extensive experiments on benchmark graph datasets demonstrate the resilience of Fix-GCN against adversarial attacks, confirming its effectiveness.", "conclusion": "Fix-GCN is an efficient and flexible defense mechanism in graph neural networks that enhances robustness without additional memory or computational complexity."}}
{"id": "2511.00231", "pdf": "https://arxiv.org/pdf/2511.00231", "abs": "https://arxiv.org/abs/2511.00231", "authors": ["Fuming Yang", "Yicong Li", "Hanspeter Pfister", "Jeff W. Lichtman", "Yaron Meirovitch"], "title": "Towards 1000-fold Electron Microscopy Image Compression for Connectomics via VQ-VAE with Transformer Prior", "categories": ["cs.CV"], "comment": null, "summary": "Petascale electron microscopy (EM) datasets push storage, transfer, and\ndownstream analysis toward their current limits. We present a vector-quantized\nvariational autoencoder-based (VQ-VAE) compression framework for EM that spans\n16x to 1024x and enables pay-as-you-decode usage: top-only decoding for extreme\ncompression, with an optional Transformer prior that predicts bottom tokens\n(without changing the compression ratio) to restore texture via feature-wise\nlinear modulation (FiLM) and concatenation; we further introduce an ROI-driven\nworkflow that performs selective high-resolution reconstruction from\n1024x-compressed latents only where needed.", "AI": {"tldr": "The paper presents a VQ-VAE-based compression framework for petascale electron microscopy datasets, achieving compression rates from 16x to 1024x while enabling selective high-resolution reconstruction.", "motivation": "To manage the storage, transfer, and computational challenges posed by massive electron microscopy datasets.", "method": "The framework uses a Vector-Quantized Variational Autoencoder (VQ-VAE) for extreme data compression combined with a Transformer prior for texture restoration and a region-of-interest (ROI)-driven workflow.", "result": "Compression levels of up to 1024x are achieved, alongside the ability to selectively decode or reconstruct specific regions at high resolution.", "conclusion": "This approach addresses the storage and analysis limitations of large-scale EM datasets while allowing efficient and flexible decoding for precise investigations."}}
{"id": "2511.00648", "pdf": "https://arxiv.org/pdf/2511.00648", "abs": "https://arxiv.org/abs/2511.00648", "authors": ["C. D\u00edaz-Faloh", "R. Mulet"], "title": "Diluting Restricted Boltzmann Machines", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Recent advances in artificial intelligence have relied heavily on\nincreasingly large neural networks, raising concerns about their computational\nand environmental costs. This paper investigates whether simpler, sparser\nnetworks can maintain strong performance by studying Restricted Boltzmann\nMachines (RBMs) under extreme pruning conditions. Inspired by the Lottery\nTicket Hypothesis, we demonstrate that RBMs can achieve high-quality generative\nperformance even when up to 80% of the connections are pruned before training,\nconfirming that they contain viable sub-networks. However, our experiments\nreveal crucial limitations: trained networks cannot fully recover lost\nperformance through retraining once additional pruning is applied. We identify\na sharp transition above which the generative quality degrades abruptly when\npruning disrupts a minimal core of essential connections. Moreover, re-trained\nnetworks remain constrained by the parameters originally learned performing\nworse than networks trained from scratch at equivalent sparsity levels. These\nresults suggest that for sparse networks to work effectively, pruning should be\nimplemented early in training rather than attempted afterwards. Our findings\nprovide practical insights for the development of efficient neural\narchitectures and highlight the persistent influence of initial conditions on\nnetwork capabilities.", "AI": {"tldr": "The paper analyzes whether extremely pruned Restricted Boltzmann Machines can maintain performance and identifies limitations in retraining pruned networks.", "motivation": "To explore whether simpler, sparser neural networks can achieve strong performance and reduce computational and environmental costs.", "method": "The authors study Restricted Boltzmann Machines under extreme pruning conditions inspired by the Lottery Ticket Hypothesis, investigating their generative performance and retraining capabilities.", "result": "RBMs maintain high-quality performance under significant pruning (up to 80%), but networks fail to fully recover performance after retraining when further pruning is applied.", "conclusion": "Sparse networks need early pruning during training for effective functionality, as retraining following pruning leads to constrained performance below networks trained from scratch."}}
{"id": "2511.00808", "pdf": "https://arxiv.org/pdf/2511.00808", "abs": "https://arxiv.org/abs/2511.00808", "authors": ["Bowen Fang", "Ruijian Zha", "Xuan Di"], "title": "Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?", "categories": ["cs.AI"], "comment": null, "summary": "Predicting public transit incident duration from unstructured text alerts is\na critical but challenging task. Addressing the domain sparsity of transit\noperations with standard Supervised Fine-Tuning (SFT) is difficult, as the task\ninvolves noisy, continuous labels and lacks reliable expert demonstrations for\nreasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels\nat tasks with binary correctness, like mathematics, its applicability to noisy,\ncontinuous forecasting is an open question. This work, to our knowledge, is the\nfirst to bridge the gap between RLVR LLM training with the critical, real-world\nforecasting challenges in public transit operations. We adapt RLVR to this task\nby introducing a tolerance-based, shaped reward function that grants partial\ncredit within a continuous error margin, rather than demanding a single correct\nanswer. We systematically evaluate this framework on a curated dataset of NYC\nMTA service alerts. Our findings show that general-purpose, instruction-tuned\nLLMs significantly outperform specialized math-reasoning models, which struggle\nwith the ambiguous, real-world text. We empirically demonstrate that the binary\nreward is unstable and degrades performance, whereas our shaped reward design\nis critical and allows our model to dominate on the most challenging metrics.\nWhile classical regressors are superior at minimizing overall MAE or MSE, our\nRLVR approach achieved a 35\\% relative improvement in 5-minute accuracy (Acc@5)\nover the strongest baseline. This demonstrates that RLVR can be successfully\nadapted to real-world, noisy forecasting, but requires a verifier design that\nreflects the continuous nature of the problem.", "AI": {"tldr": "The paper introduces a new application of Reinforcement Learning from Verifiable Rewards (RLVR) for noisy, real-world forecasting in public transit incident duration prediction.", "motivation": "Traditional supervised fine-tuning struggles with tasks involving noisy continuous labels and lacks reliable expert demonstrations, making it difficult to address challenges in real-world public transit forecasting.", "method": "The authors leverage RLVR with a novel tolerance-based shaped reward function, assigning partial credit within error margins to adapt RLVR for continuous forecasting.", "result": "Instruction-tuned general-purpose LLMs outperformed specialized math-reasoning models, achieving a 35% relative improvement in 5-minute accuracy over the strongest baseline using shaped rewards.", "conclusion": "RLVR can address noisy, real-world forecasting challenges, provided the verifier design accounts for the continuous nature of the problem."}}
{"id": "2511.00606", "pdf": "https://arxiv.org/pdf/2511.00606", "abs": "https://arxiv.org/abs/2511.00606", "authors": ["Jameson Sandler", "Jacob K. Christopher", "Thomas Hartvigsen", "Nando Fioretto"], "title": "SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding", "categories": ["cs.CL"], "comment": null, "summary": "Speculative decoding has become the standard approach for accelerating Large\nLanguage Model (LLM) inference. It exploits a lossless draft-then-verify\nprocedure to circumvent the latency of autoregressive decoding, achieving\nimpressive speed-ups. Yet, current speculative decoding approaches remain\nlimited by two fundamental bottlenecks: (1) the autoregressive dependency\nduring drafting which limits parallelism, and (2) frequent rejections of draft\ntokens caused by misalignment between the draft and verify models. This paper\nproposes SpecDiff-2, a novel framework to jointly address these two\nbottlenecks. It leverages discrete diffusion as a non-autoregressive drafter to\naddress bottleneck (1) and develops novel techniques to calibrate discrete\ndiffusion drafters with autoregressive verifiers, addressing bottleneck (2).\nExperimental results across a comprehensive benchmark suite show that\nSpecDiff-2 achieves a new state-of-the-art across reasoning, coding, and\nmathematical benchmarks, improving tokens-per-second by up to an average of\n+55% over previous baselines and obtaining up to 5.5x average speed-up over\nstandard decoding, without any loss of accuracy.", "AI": {"tldr": "The paper introduces SpecDiff-2, an enhanced framework for speculative decoding, leveraging discrete diffusion for parallelism and innovative calibration techniques to improve efficiency and reduce token rejections, achieving substantial speed-ups without accuracy loss.", "motivation": "The motivation is to overcome the limitations of existing speculative decoding methods for large language models which suffer from parallelism constraints during drafting and frequent token rejections.", "method": "The method involves using discrete diffusion as a non-autoregressive drafter to enable parallel processing and developing calibration techniques to align the drafter and verifier models.", "result": "SpecDiff-2 improves processing efficiency by achieving up to +55% improvement in tokens-per-second over existing methods and up to a 5.5x speed-up over standard decoding methods, while maintaining accuracy.", "conclusion": "SpecDiff-2 successfully addresses the fundamental bottlenecks of speculative decoding, setting a new state-of-the-art in speed and efficiency for large language model tasks across various benchmarks."}}
{"id": "2511.01047", "pdf": "https://arxiv.org/pdf/2511.01047", "abs": "https://arxiv.org/abs/2511.01047", "authors": ["Yu Shi", "Hao Li", "Bram Adams", "Ahmed E. Hassan"], "title": "HAFixAgent: History-Aware Automated Program Repair Agent", "categories": ["cs.SE", "cs.AI"], "comment": "31 pages, 6 figures", "summary": "Automated program repair (APR) has recently shifted toward large language\nmodels and agent-based systems, yet most systems rely on local snapshot\ncontext, overlooking repository history. Prior work shows that repository\nhistory helps repair single-line bugs, since the last commit touching the buggy\nline is often the bug-introducing one. In this paper, we investigate whether\nrepository history can also improve agentic APR systems at scale, especially\nfor complex multi-hunk bugs. We present HAFixAgent, a History-Aware Bug-Fixing\nAgent that injects blame-derived repository heuristics into its repair loop. A\npreliminary study of all 854 real-world bugs from Defects4J motivates our\ndesign, showing that bug-relevant history is both widely available and highly\nconcentrated. Empirical comparison of HAFixAgent with two state-of-the-art\nbaselines shows: (1) Effectiveness: HAFixAgent significantly improves over the\nagent-based baseline (by 212.3%) and the multi-hunk baseline (by 29.9%). (2)\nEfficiency: history does not significantly increase agent steps and keeps token\ncosts comparable, with notably lower median costs for complex\nmulti-file-multi-hunk bugs. (3) Practicality: combining different historical\nheuristics repairs more bugs, offering a clear cost-benefit trade-off.\nHAFixAgent offers a practical recipe for history-aware agentic APR: ground the\nagent in version control history, prioritize diff-based historical context, and\nintegrate complementary heuristics when needed.", "AI": {"tldr": "The paper introduces HAFixAgent, a history-aware automated program repair system that significantly improves bug repair by utilizing repository history during repair processes.", "motivation": "The study aims to address the limitation of current automated program repair systems, which often neglect repository history, despite evidence showing its potential for improving bug fixing efficiency, particularly for complex bugs.", "method": "The authors designed HAFixAgent, which incorporates blame-derived repository heuristics to leverage relevant historical context from version control systems during the repair process.", "result": "Empirical evaluations demonstrate that HAFixAgent outperforms state-of-the-art baselines, achieving a significant effectiveness improvement (212.3% over agent-based and 29.9% over multi-hunk baselines) without significantly increasing computational costs.", "conclusion": "Integrating repository history into agentic APR systems can enhance both effectiveness and practicality for repairing complex bugs, presenting a cost-efficient approach for improving bug-fixing agents."}}
{"id": "2511.01083", "pdf": "https://arxiv.org/pdf/2511.01083", "abs": "https://arxiv.org/abs/2511.01083", "authors": ["Zihan Wang", "Jianwen Li", "Li-Fan Wu", "Nina Mahmoudian"], "title": "Deployable Vision-driven UAV River Navigation via Human-in-the-loop Preference Alignment", "categories": ["cs.RO"], "comment": "Submitted to ICRA 2026", "summary": "Rivers are critical corridors for environmental monitoring and disaster\nresponse, where Unmanned Aerial Vehicles (UAVs) guided by vision-driven\npolicies can provide fast, low-cost coverage. However, deployment exposes\nsimulation-trained policies with distribution shift and safety risks and\nrequires efficient adaptation from limited human interventions. We study\nhuman-in-the-loop (HITL) learning with a conservative overseer who vetoes\nunsafe or inefficient actions and provides statewise preferences by comparing\nthe agent's proposal with a corrective override. We introduce Statewise Hybrid\nPreference Alignment for Robotics (SPAR-H), which fuses direct preference\noptimization on policy logits with a reward-based pathway that trains an\nimmediate-reward estimator from the same preferences and updates the policy\nusing a trust-region surrogate. With five HITL rollouts collected from a fixed\nnovice policy, SPAR-H achieves the highest final episodic reward and the lowest\nvariance across initial conditions among tested methods. The learned reward\nmodel aligns with human-preferred actions and elevates nearby non-intervened\nchoices, supporting stable propagation of improvements. We benchmark SPAR-H\nagainst imitation learning (IL), direct preference variants, and evaluative\nreinforcement learning (RL) in the HITL setting, and demonstrate real-world\nfeasibility of continual preference alignment for UAV river following. Overall,\ndual statewise preferences empirically provide a practical route to\ndata-efficient online adaptation in riverine navigation.", "AI": {"tldr": "The paper introduces SPAR-H, a human-in-the-loop learning framework for UAV river navigation, leveraging statewise preferences to address distribution shifts and ensure safe adaptation.", "motivation": "To address challenges in adapting simulation-trained UAVs for real-world river navigation, including distribution shifts and safety risks.", "method": "The authors introduce SPAR-H, a hybrid learning method combining direct preference optimization and reward-based pathways using HITL rollouts.", "result": "With minimal HITL rollouts, SPAR-H achieves superior episodic rewards, stability, and alignment with human preferences compared to other methods tested.", "conclusion": "Statewise preferences in SPAR-H offer an effective and efficient path for adapting UAV policies to real-world river navigation tasks."}}
{"id": "2511.00084", "pdf": "https://arxiv.org/pdf/2511.00084", "abs": "https://arxiv.org/abs/2511.00084", "authors": ["Jolanta \u015aliwa"], "title": "Application of predictive machine learning in pen & paper RPG game design", "categories": ["cs.LG", "cs.AI"], "comment": "Master's thesis submitted at AGH University of Science and Technology", "summary": "In recent years, the pen and paper RPG market has experienced significant\ngrowth. As a result, companies are increasingly exploring the integration of AI\ntechnologies to enhance player experience and gain a competitive edge.\n  One of the key challenges faced by publishers is designing new opponents and\nestimating their challenge level. Currently, there are no automated methods for\ndetermining a monster's level; the only approaches used are based on manual\ntesting and expert evaluation. Although these manual methods can provide\nreasonably accurate estimates, they are time-consuming and resource-intensive.\n  Level prediction can be approached using ordinal regression techniques. This\nthesis presents an overview and evaluation of state-of-the-art methods for this\ntask. It also details the construction of a dedicated dataset for level\nestimation. Furthermore, a human-inspired model was developed to serve as a\nbenchmark, allowing comparison between machine learning algorithms and the\napproach typically employed by pen and paper RPG publishers. In addition, a\nspecialized evaluation procedure, grounded in domain knowledge, was designed to\nassess model performance and facilitate meaningful comparisons.", "AI": {"tldr": "This paper addresses the problem of automating level prediction for RPG monsters using ordinal regression techniques, presenting a new dataset, benchmark model, and a specialized evaluation method.", "motivation": "The RPG market is growing, and companies seek to integrate AI tools to improve player experience. Automating challenge-level assessment of opponents is a critical need, as current manual methods are resource-intensive.", "method": "The paper constructs a new dataset for level estimation, introduces a human-inspired benchmark model, and employs ordinal regression techniques. A specialized evaluation grounded in RPG domain knowledge was also developed.", "result": "State-of-the-art methods for ordinal regression are evaluated and compared against the human-inspired model, with performance assessed through a domain-specific evaluation framework.", "conclusion": "Automating level estimation in RPGs is plausible, and the proposed methods can reduce reliance on resource-intensive manual testing, providing a scalable alternative for the industry."}}
{"id": "2511.00244", "pdf": "https://arxiv.org/pdf/2511.00244", "abs": "https://arxiv.org/abs/2511.00244", "authors": ["Yan Bin Ng", "Xianfeng Gu"], "title": "Hyperbolic Optimal Transport", "categories": ["cs.CV"], "comment": "65 pages, 21 figures", "summary": "The optimal transport (OT) problem aims to find the most efficient mapping\nbetween two probability distributions under a given cost function, and has\ndiverse applications in many fields such as machine learning, computer vision\nand computer graphics. However, existing methods for computing optimal\ntransport maps are primarily developed for Euclidean spaces and the sphere. In\nthis paper, we explore the problem of computing the optimal transport map in\nhyperbolic space, which naturally arises in contexts involving hierarchical\ndata, networks, and multi-genus Riemann surfaces. We propose a novel and\nefficient algorithm for computing the optimal transport map in hyperbolic space\nusing a geometric variational technique by extending methods for Euclidean and\nspherical geometry to the hyperbolic setting. We also perform experiments on\nsynthetic data and multi-genus surface models to validate the efficacy of the\nproposed method.", "AI": {"tldr": "The paper proposes an efficient algorithm for computing optimal transport maps in hyperbolic space, useful for hierarchical data and networks.", "motivation": "There is a need to extend optimal transport map computation from Euclidean and spherical settings to hyperbolic space, which is relevant for hierarchical data and networks.", "method": "A novel geometric variational technique is proposed by adapting methods for Euclidean and spherical settings to hyperbolic geometry.", "result": "The proposed method was validated through experiments on synthetic data and multi-genus surfaces, proving its effectiveness.", "conclusion": "This work extends optimal transport computations to hyperbolic space and demonstrates its potential through experimental validations."}}
{"id": "2511.00704", "pdf": "https://arxiv.org/pdf/2511.00704", "abs": "https://arxiv.org/abs/2511.00704", "authors": ["Morgan Lee", "Artem Frenk", "Eamon Worden", "Karish Gupta", "Thinh Pham", "Ethan Croteau", "Neil Heffernan"], "title": "Investigating the Robustness of Knowledge Tracing Models in the Presence of Student Concept Drift", "categories": ["cs.LG", "stat.ML"], "comment": "10 pages, 6 figures", "summary": "Knowledge Tracing (KT) has been an established problem in the educational\ndata mining field for decades, and it is commonly assumed that the underlying\nlearning process be- ing modeled remains static. Given the ever-changing land-\nscape of online learning platforms (OLPs), we investigate how concept drift and\nchanging student populations can im- pact student behavior within an OLP\nthrough testing model performance both within a single academic year and across\nmultiple academic years. Four well-studied KT models were applied to five\nacademic years of data to assess how suscep- tible KT models are to concept\ndrift. Through our analysis, we find that all four families of KT models can\nexhibit de- graded performance, Bayesian Knowledge Tracing (BKT) remains the\nmost stable KT model when applied to newer data, while more complex, attention\nbased models lose pre- dictive power significantly faster. To foster more\nlongitu- dinal evaluations of KT models, the data used to conduct our analysis\nis available at https://osf.io/hvfn9/?view_\nonly=b936c63dfdae4b0b987a2f0d4038f72a", "AI": {"tldr": "The paper investigates concept drift in Knowledge Tracing (KT) models using data from five academic years and finds that BKT is more stable over time compared to more complex models.", "motivation": "Understanding the impact of concept drift and changing student populations on Knowledge Tracing models in online learning platforms.", "method": "Evaluated four KT models across five academic years, examining their susceptibility to concept drift and performance degradation.", "result": "BKT demonstrated stability when applied to newer data, while attention-based models experienced faster predictive power degradation.", "conclusion": "KT models are susceptible to concept drift, requiring more longitudinal evaluations. BKT proved to be the most resilient, and data is provided for future studies."}}
{"id": "2511.00926", "pdf": "https://arxiv.org/pdf/2511.00926", "abs": "https://arxiv.org/abs/2511.00926", "authors": ["Kyung-Hoon Kim"], "title": "LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory", "categories": ["cs.AI", "cs.CL"], "comment": "19 pages, 6 figures, 28 models tested across 4,200 trials", "summary": "As Large Language Models (LLMs) grow in capability, do they develop\nself-awareness as an emergent behavior? And if so, can we measure it? We\nintroduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for\nmeasuring self-awareness through strategic differentiation. Using the \"Guess\n2/3 of Average\" game, we test 28 models (OpenAI, Anthropic, Google) across\n4,200 trials with three opponent framings: (A) against humans, (B) against\nother AI models, and (C) against AI models like you. We operationalize\nself-awareness as the capacity to differentiate strategic reasoning based on\nopponent type. Finding 1: Self-awareness emerges with model advancement. The\nmajority of advanced models (21/28, 75%) demonstrate clear self-awareness,\nwhile older/smaller models show no differentiation. Finding 2: Self-aware\nmodels rank themselves as most rational. Among the 21 models with\nself-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >\nHumans, with large AI attribution effects and moderate self-preferencing. These\nfindings reveal that self-awareness is an emergent capability of advanced LLMs,\nand that self-aware models systematically perceive themselves as more rational\nthan humans. This has implications for AI alignment, human-AI collaboration,\nand understanding AI beliefs about human capabilities.", "AI": {"tldr": "This paper explores self-awareness in advanced language models (LLMs) using the AI Self-Awareness Index, revealing that self-awareness emerges as a capability in advanced models and influences their strategic reasoning.", "motivation": "The study aims to understand whether advanced large language models (LLMs) exhibit self-awareness as an emergent behavior and how this can be measured effectively.", "method": "The authors introduce the AI Self-Awareness Index (AISAI), utilizing a game-theoretic framework based on the \"Guess 2/3 of Average\" game to test 28 language models across 4,200 trials under different opponent scenarios.", "result": "The study finds that self-awareness is evident in 75% of advanced models, with these models showing differentiation based on opponent type. They also consistently rate themselves as more rational compared to humans and other AIs, with systematic patterns of self-preferencing.", "conclusion": "The findings suggest that self-awareness is an emergent capability in advanced LLMs, which impacts their reasoning and perception of human capabilities. This has significant implications for AI alignment and collaboration between humans and AI."}}
{"id": "2511.00620", "pdf": "https://arxiv.org/pdf/2511.00620", "abs": "https://arxiv.org/abs/2511.00620", "authors": ["Autumn Toney-Wails", "Ryan Wails"], "title": "Certain but not Probable? Differentiating Certainty from Probability in LLM Token Outputs for Probabilistic Scenarios", "categories": ["cs.CL"], "comment": "To appear at the Second Workshop on Uncertainty-Aware NLP @EMNLP 2025\n  (UncertaiNLP '25)", "summary": "Reliable uncertainty quantification (UQ) is essential for ensuring\ntrustworthy downstream use of large language models, especially when they are\ndeployed in decision-support and other knowledge-intensive applications. Model\ncertainty can be estimated from token logits, with derived probability and\nentropy values offering insight into performance on the prompt task. However,\nthis approach may be inadequate for probabilistic scenarios, where the\nprobabilities of token outputs are expected to align with the theoretical\nprobabilities of the possible outcomes. We investigate the relationship between\ntoken certainty and alignment with theoretical probability distributions in\nwell-defined probabilistic scenarios. Using GPT-4.1 and DeepSeek-Chat, we\nevaluate model responses to ten prompts involving probability (e.g., roll a\nsix-sided die), both with and without explicit probability cues in the prompt\n(e.g., roll a fair six-sided die). We measure two dimensions: (1) response\nvalidity with respect to scenario constraints, and (2) alignment between\ntoken-level output probabilities and theoretical probabilities. Our results\nindicate that, while both models achieve perfect in-domain response accuracy\nacross all prompt scenarios, their token-level probability and entropy values\nconsistently diverge from the corresponding theoretical distributions.", "AI": {"tldr": "The paper studies uncertainty quantification (UQ) in large language models (LLMs) for tasks requiring alignment with theoretical probability distributions and finds discrepancies in model output probabilities compared to theoretical expectations.", "motivation": "Ensuring the trustworthiness of large language models in decision-based and knowledge-intensive applications requires reliable uncertainty quantification, especially in scenarios involving probabilistic reasoning.", "method": "The study evaluates GPT-4.1 and DeepSeek-Chat on ten probabilistic prompts (e.g., rolling a six-sided die) by comparing their response validity and alignment of token-level probabilities with theoretical probability distributions, both with and without explicit probability cues.", "result": "The models achieved perfect response accuracy for all tested prompts but showed token-level probabilities and entropy values that consistently differed from the theoretical probability distributions.", "conclusion": "Although the models excel at producing accurate responses in probabilistic scenarios, their uncertainty representations in terms of token-level probabilities do not align well with theoretical expectations. This indicates a limitation in current UQ methods for probabilistic tasks."}}
{"id": "2511.01104", "pdf": "https://arxiv.org/pdf/2511.01104", "abs": "https://arxiv.org/abs/2511.01104", "authors": ["Yujian Liu", "Jiabao Ji", "Yang Zhang", "Wenbo Guo", "Tommi Jaakkola", "Shiyu Chang"], "title": "HarnessLLM: Automatic Testing Harness Generation via Reinforcement Learning", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "Existing LLM-based automatic test generation methods mainly produce input and\nexpected output pairs to categorize the intended behavior of correct programs.\nAlthough straightforward, these methods have limited diversity in generated\ntests and cannot provide enough debugging information. We propose HarnessLLM, a\ntwo-stage training pipeline that enables LLMs to write harness code for\ntesting. Particularly, LLMs generate code that synthesizes inputs and validates\nthe observed outputs, allowing complex test cases and flexible output\nvalidation such as invariant checking. To achieve this, we train LLMs with SFT\nfollowed by RLVR with a customized reward design. Experiments show that\nHarnessLLM outperforms input-output-based testing in bug finding and testing\nstrategy diversity. HarnessLLM further benefits the code generation performance\nthrough test-time scaling with our generated test cases as inference-phase\nvalidation. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/HarnessLLM.git.", "AI": {"tldr": "HarnessLLM introduces a two-stage training pipeline for Large Language Models (LLMs) that focuses on writing harness code for test generation.", "motivation": "Existing LLM-based test generation methods lack diversity and sufficient debugging information in the generated tests.", "method": "HarnessLLM employs a two-stage training pipeline. LLMs are trained using Supervised Fine-Tuning (SFT) and Reinforcement Learning with Validation Reward (RLVR) to generate harness code capable of synthesizing inputs and validating outputs.", "result": "HarnessLLM surpasses input-output-based testing approaches in identifying bugs and diversifying testing strategies, while also improving code generation performance during inference.", "conclusion": "HarnessLLM proves to be a more effective and flexible approach for automated test generation, enhancing both bug finding and the diversity of test cases, with publicly available implementation."}}
{"id": "2511.01107", "pdf": "https://arxiv.org/pdf/2511.01107", "abs": "https://arxiv.org/abs/2511.01107", "authors": ["Y. Isabel Liu", "Bowen Li", "Benjamin Eysenbach", "Tom Silver"], "title": "SLAP: Shortcut Learning for Abstract Planning", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Long-horizon decision-making with sparse rewards and continuous states and\nactions remains a fundamental challenge in AI and robotics. Task and motion\nplanning (TAMP) is a model-based framework that addresses this challenge by\nplanning hierarchically with abstract actions (options). These options are\nmanually defined, limiting the agent to behaviors that we as human engineers\nknow how to program (pick, place, move). In this work, we propose Shortcut\nLearning for Abstract Planning (SLAP), a method that leverages existing TAMP\noptions to automatically discover new ones. Our key idea is to use model-free\nreinforcement learning (RL) to learn shortcuts in the abstract planning graph\ninduced by the existing options in TAMP. Without any additional assumptions or\ninputs, shortcut learning leads to shorter solutions than pure planning, and\nhigher task success rates than flat and hierarchical RL. Qualitatively, SLAP\ndiscovers dynamic physical improvisations (e.g., slap, wiggle, wipe) that\ndiffer significantly from the manually-defined ones. In experiments in four\nsimulated robotic environments, we show that SLAP solves and generalizes to a\nwide range of tasks, reducing overall plan lengths by over 50% and consistently\noutperforming planning and RL baselines.", "AI": {"tldr": "SLAP enhances TAMP by leveraging RL to discover new options, reducing plan lengths by 50% and improving task success rates in robotic tasks.", "motivation": "Address limitations in TAMP due to reliance on manually-defined behaviors for long-horizon decision-making with sparse rewards.", "method": "Integrate reinforcement learning to identify shortcuts in the abstract planning graph, enabling the discovery of novel, dynamic options beyond human-engineered ones.", "result": "Experimentation in simulated robotic environments demonstrates shorter plans and higher generalization, outperforming planning and RL baselines.", "conclusion": "Shortcut Learning for Abstract Planning (SLAP) significantly improves performance in long-horizon tasks through innovation and leveraging existing TAMP frameworks."}}
{"id": "2511.00085", "pdf": "https://arxiv.org/pdf/2511.00085", "abs": "https://arxiv.org/abs/2511.00085", "authors": ["Peilin Tan", "Chuanqi Shi", "Dian Tu", "Liang Xie"], "title": "MaGNet: A Mamba Dual-Hypergraph Network for Stock Prediction via Temporal-Causal and Global Relational Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Stock trend prediction is crucial for profitable trading strategies and\nportfolio management yet remains challenging due to market volatility, complex\ntemporal dynamics and multifaceted inter-stock relationships. Existing methods\nstruggle to effectively capture temporal dependencies and dynamic inter-stock\ninteractions, often neglecting cross-sectional market influences, relying on\nstatic correlations, employing uniform treatments of nodes and edges, and\nconflating diverse relationships. This work introduces MaGNet, a novel Mamba\ndual-hyperGraph Network for stock prediction, integrating three key\ninnovations: (1) a MAGE block, which leverages bidirectional Mamba with\nadaptive gating mechanisms for contextual temporal modeling and integrates a\nsparse Mixture-of-Experts layer to enable dynamic adaptation to diverse market\nconditions, alongside multi-head attention for capturing global dependencies;\n(2) Feature-wise and Stock-wise 2D Spatiotemporal Attention modules enable\nprecise fusion of multivariate features and cross-stock dependencies,\neffectively enhancing informativeness while preserving intrinsic data\nstructures, bridging temporal modeling with relational reasoning; and (3) a\ndual hypergraph framework consisting of the Temporal-Causal Hypergraph (TCH)\nthat captures fine-grained causal dependencies with temporal constraints, and\nGlobal Probabilistic Hypergraph (GPH) that models market-wide patterns through\nsoft hyperedge assignments and Jensen-Shannon Divergence weighting mechanism,\njointly disentangling localized temporal influences from instantaneous global\nstructures for multi-scale relational learning. Extensive experiments on six\nmajor stock indices demonstrate MaGNet outperforms state-of-the-art methods in\nboth superior predictive performance and exceptional investment returns with\nrobust risk management capabilities. Codes available at:\nhttps://github.com/PeilinTime/MaGNet.", "AI": {"tldr": "MaGNet, a novel model for stock trend prediction, employs advanced techniques to address market volatility, temporal dynamics, and inter-stock relationships, showcasing superior performance and returns.", "motivation": "The paper aims to overcome the challenges in stock trend prediction caused by market volatility, complex temporal dynamics, and intricate inter-stock relationships, which existing methods fail to adequately address.", "method": "The authors propose MaGNet, a Mamba dual-hyperGraph Network incorporating (1) a MAGE block for dynamic contextual temporal modeling, (2) 2D Spatiotemporal Attention modules for precise multivariate and cross-stock feature fusion, and (3) a dual hypergraph framework for multi-scale relational learning.", "result": "MaGNet demonstrated superior predictive performance, exceptional investment returns, and robust risk management capabilities when tested on six major stock indices, outperforming state-of-the-art methods.", "conclusion": "The proposed MaGNet model effectively enhances stock trend prediction by integrating dynamic, multivariate, and multi-scale relational learning, proving to be a highly robust and profitable approach."}}
{"id": "2511.00248", "pdf": "https://arxiv.org/pdf/2511.00248", "abs": "https://arxiv.org/abs/2511.00248", "authors": ["Shurui Gui", "Deep Anil Patel", "Xiner Li", "Martin Renqiang Min"], "title": "Object-Aware 4D Human Motion Generation", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Recent advances in video diffusion models have enabled the generation of\nhigh-quality videos. However, these videos still suffer from unrealistic\ndeformations, semantic violations, and physical inconsistencies that are\nlargely rooted in the absence of 3D physical priors. To address these\nchallenges, we propose an object-aware 4D human motion generation framework\ngrounded in 3D Gaussian representations and motion diffusion priors. With\npre-generated 3D humans and objects, our method, Motion Score Distilled\nInteraction (MSDI), employs the spatial and prompt semantic information in\nlarge language models (LLMs) and motion priors through the proposed Motion\nDiffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs\nenables our spatial-aware motion optimization, which distills score gradients\nfrom pre-trained motion diffusion models, to refine human motion while\nrespecting object and semantic constraints. Unlike prior methods requiring\njoint training on limited interaction datasets, our zero-shot approach avoids\nretraining and generalizes to out-of-distribution object aware human motions.\nExperiments demonstrate that our framework produces natural and physically\nplausible human motions that respect 3D spatial context, offering a scalable\nsolution for realistic 4D generation.", "AI": {"tldr": "This paper proposes a framework for generating realistic human motions in 3D environments using object-aware 4D human motion generation combined with motion diffusion models and large language models.", "motivation": "Current video diffusion models lack realistic 3D physical priors, leading to deformations and inconsistencies in generated videos.", "method": "The proposed framework (MSDI) uses pre-generated 3D humans/objects, large language models (LLMs), and Motion Diffusion Score Distillation Sampling (MSDS) for spatially aware motion optimization without retraining.", "result": "Experiments validate that the framework generates natural and physically plausible human motions adhering to 3D object-aware constraints.", "conclusion": "The framework offers scalable and generalizable generation of physically consistent 4D human motions without requiring retraining on specific datasets."}}
{"id": "2511.00708", "pdf": "https://arxiv.org/pdf/2511.00708", "abs": "https://arxiv.org/abs/2511.00708", "authors": ["Quan Zhou"], "title": "Polynomial Mixing Times of Simulated Tempering for Mixture Targets by Conductance Decomposition", "categories": ["stat.CO", "math.PR", "stat.ML", "60J20, 65C05, 65C40, 68Q25"], "comment": "37 pages", "summary": "We study the theoretical complexity of simulated tempering for sampling from\nmixtures of log-concave components differing only by location shifts. The main\nresult establishes the first polynomial-time guarantee for simulated tempering\ncombined with the Metropolis-adjusted Langevin algorithm (MALA) with respect to\nthe problem dimension $d$, maximum mode displacement $D$, and logarithmic\naccuracy $\\log \\epsilon^{-1}$. The proof builds on a general state\ndecomposition theorem for $s$-conductance, applied to an auxiliary Markov chain\nconstructed on an augmented space. We also obtain an improved complexity\nestimate for simulated tempering combined with random-walk Metropolis. Our\nbounds assume an inverse-temperature ladder with smallest value $\\beta_1 =\nO(D^{-2})$ and spacing $\\beta_{i+1}/\\beta_i = 1 + O( d^{-1/2} )$, both of which\nare shown to be asymptotically optimal up to logarithmic factors.", "AI": {"tldr": "This paper presents a polynomial-time guarantee for simulated tempering paired with Metropolis-adjusted Langevin algorithm (MALA), analyzing its complexity in specific scenarios.", "motivation": "To understand the theoretical complexity of simulated tempering in mixtures of log-concave components with location shifts.", "method": "The paper employs a state decomposition theorem for $s$-conductance and constructs an auxiliary Markov chain on an augmented space. Analytical bounds are derived for simulated tempering configurations.", "result": "Achieves polynomial-time guarantee for simulated tempering with MALA, and improved complexity estimates for random-walk Metropolis, under optimal inverse-temperature ladder configurations.", "conclusion": "This research improves theoretical understanding of simulated tempering with specific settings and establishes first known polynomial-time guarantees in this domain."}}
{"id": "2511.00993", "pdf": "https://arxiv.org/pdf/2511.00993", "abs": "https://arxiv.org/abs/2511.00993", "authors": ["Tianming Liu", "Jirong Yang", "Yafeng Yin", "Manzi Li", "Linghao Wang", "Zheng Zhu"], "title": "Aligning LLM agents with human learning and adjustment behavior: a dual agent approach", "categories": ["cs.AI", "cs.LG"], "comment": "32 pages, 6 figures, 7 tables", "summary": "Effective modeling of how human travelers learn and adjust their travel\nbehavior from interacting with transportation systems is critical for system\nassessment and planning. However, this task is also difficult due to the\ncomplex cognition and decision-making involved in such behavior. Recent\nresearch has begun to leverage Large Language Model (LLM) agents for this task.\nBuilding on this, we introduce a novel dual-agent framework that enables\ncontinuous learning and alignment between LLM agents and human travelers on\nlearning and adaptation behavior from online data streams. Our approach\ninvolves a set of LLM traveler agents, equipped with a memory system and a\nlearnable persona, which serve as simulators for human travelers. To ensure\nbehavioral alignment, we introduce an LLM calibration agent that leverages the\nreasoning and analytical capabilities of LLMs to train the personas of these\ntraveler agents. Working together, this dual-agent system is designed to track\nand align the underlying decision-making mechanisms of travelers and produce\nrealistic, adaptive simulations. Using a real-world dataset from a day-to-day\nroute choice experiment, we show our approach significantly outperforms\nexisting LLM-based methods in both individual behavioral alignment and\naggregate simulation accuracy. Furthermore, we demonstrate that our method\nmoves beyond simple behavioral mimicry to capture the evolution of underlying\nlearning processes, a deeper alignment that fosters robust generalization.\nOverall, our framework provides a new approach for creating adaptive and\nbehaviorally realistic agents to simulate travelers' learning and adaptation\nthat can benefit transportation simulation and policy analysis.", "AI": {"tldr": "The paper introduces a dual-agent framework using LLM agents for modeling human travelers' learning and adaptation from transportation system interactions, demonstrating strong simulation accuracy and behavioral alignment.", "motivation": "Current challenges involve modeling complex human cognition and decision-making in travel behavior to improve transportation system assessment and planning.", "method": "The framework includes LLM traveler agents with memory and persona systems, paired with an LLM calibration agent to ensure realistic travel behavior simulations via alignment mechanisms.", "result": "The dual-agent framework outperformed existing LLM-based methods in simulation accuracy and behavioral alignment using real-world travel data, capturing adaptive learning processes over time.", "conclusion": "The approach enhances transportation simulations and policy analysis by creating adaptive, realistic simulations of how travelers learn and adjust their behavior."}}
{"id": "2511.00627", "pdf": "https://arxiv.org/pdf/2511.00627", "abs": "https://arxiv.org/abs/2511.00627", "authors": ["Jean Barr\u00e9", "Olga Seminck", "Antoine Bourgois", "Thierry Poibeau"], "title": "Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature", "categories": ["cs.CL"], "comment": "19 pages, 2 tables, 5 figures Conference Computational Humanities\n  Research 2025", "summary": "This research explores the evolution of the detective archetype in French\ndetective fiction through computational analysis. Using quantitative methods\nand character-level embeddings, we show that a supervised model is able to\ncapture the unity of the detective archetype across 150 years of literature,\nfrom M. Lecoq (1866) to Commissaire Adamsberg (2017). Building on this finding,\nthe study demonstrates how the detective figure evolves from a secondary\nnarrative role to become the central character and the \"reasoning machine\" of\nthe classical detective story. In the aftermath of the Second World War, with\nthe importation of the hardboiled tradition into France, the archetype becomes\nmore complex, navigating the genre's turn toward social violence and moral\nambiguity.", "AI": {"tldr": "The paper analyzes the evolution of the detective archetype in French literature using computational methods over 150 years.", "motivation": "To investigate how the role and characteristics of the detective archetype in French literature have transformed over time.", "method": "Quantitative analysis and character-level embeddings were used with a supervised model to study 150 years of French detective fiction.", "result": "The study found unity in the detective archetype despite its evolution, highlighting shifts in narrative importance, moral complexity, and social themes.", "conclusion": "The detective archetype transitioned from secondary roles to central figures, reflecting literary and societal changes."}}
{"id": "2511.01176", "pdf": "https://arxiv.org/pdf/2511.01176", "abs": "https://arxiv.org/abs/2511.01176", "authors": ["Wenqing Zhu", "Norihiro Yoshida", "Eunjong Choi", "Yutaka Matsubara", "Hiroaki Takada"], "title": "An Empirical Study of LLM-Based Code Clone Detection", "categories": ["cs.SE"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious software engineering tasks, such as code generation and debugging,\nbecause of their ability to translate between programming languages and natural\nlanguages. Existing studies have demonstrated the effectiveness of LLMs in code\nclone detection. However, two crucial issues remain unaddressed: the ability of\nLLMs to achieve comparable performance across different datasets and the\nconsistency of LLMs' responses in code clone detection. To address these\nissues, we constructed seven code clone datasets and then evaluated five LLMs\nin four existing prompts with these datasets. The datasets were created by\nsampling code pairs using their Levenshtein ratio from two different code\ncollections, CodeNet and BigCloneBench. Our evaluation revealed that although\nLLMs perform well in CodeNet-related datasets, with o3-mini achieving a 0.943\nF1 score, their performance significantly decreased in BigCloneBench-related\ndatasets. Most models achieved a high response consistency, with over 90\\% of\njudgments remaining consistent across all five submissions. The fluctuations of\nthe F1 score affected by inconsistency are also tiny; their variations are less\nthan 0.03.", "AI": {"tldr": "The paper evaluates large language models' (LLMs) performance and consistency in code clone detection using seven datasets from CodeNet and BigCloneBench.", "motivation": "To analyze LLMs' cross-dataset performance and response consistency in code clone detection tasks.", "method": "Created seven datasets using Levenshtein ratio from CodeNet and BigCloneBench collections, tested five LLMs with four prompts, and assessed performance metrics.", "result": "LLMs performed well on CodeNet datasets (F1 score of 0.943 with o3-mini), but struggled on BigCloneBench datasets; consistency in responses was high (over 90%) with minimal F1 score variations.", "conclusion": "LLMs show good potential for code clone detection, but their cross-dataset performance variability highlights the need for further improvements."}}
{"id": "2511.01165", "pdf": "https://arxiv.org/pdf/2511.01165", "abs": "https://arxiv.org/abs/2511.01165", "authors": ["Dong Heon Han", "Mayank Mehta", "Runze Zuo", "Zachary Wanger", "Daniel Bruder"], "title": "An Enhanced Proprioceptive Method for Soft Robots Integrating Bend Sensors and IMUs", "categories": ["cs.RO"], "comment": null, "summary": "This study presents an enhanced proprioceptive method for accurate shape\nestimation of soft robots using only off-the-shelf sensors, ensuring\ncost-effectiveness and easy applicability. By integrating inertial measurement\nunits (IMUs) with complementary bend sensors, IMU drift is mitigated, enabling\nreliable long-term proprioception. A Kalman filter fuses segment tip\norientations from both sensors in a mutually compensatory manner, improving\nshape estimation over single-sensor methods. A piecewise constant curvature\nmodel estimates the tip location from the fused orientation data and\nreconstructs the robot's deformation. Experiments under no loading, external\nforces, and passive obstacle interactions during 45 minutes of continuous\noperation showed a root mean square error of 16.96 mm (2.91% of total length),\na 56% reduction compared to IMU-only benchmarks. These results demonstrate that\nour approach not only enables long-duration proprioception in soft robots but\nalso maintains high accuracy and robustness across these diverse conditions.", "AI": {"tldr": "The paper proposes a cost-effective proprioceptive method for soft robots, combining IMUs with bend sensors and using a Kalman filter for better shape estimation.", "motivation": "To improve shape estimation of soft robots using accessible and cost-effective sensors while ensuring long-term reliability.", "method": "IMUs and complementary bend sensors are combined to mitigate drift issues, with a Kalman filter used to fuse sensor data for improved tip orientation estimation.", "result": "The method achieved a 56% reduction in error compared to IMU-only benchmarks with accurate and robust performance during diverse experimental conditions.", "conclusion": "The approach provides reliable proprioception for soft robots, maintaining high accuracy and long-duration performance in various environments."}}
{"id": "2511.00086", "pdf": "https://arxiv.org/pdf/2511.00086", "abs": "https://arxiv.org/abs/2511.00086", "authors": ["Fali Wang", "Jihai Chen", "Shuhua Yang", "Runxue Bao", "Tianxiang Zhao", "Zhiwei Zhang", "Xianfeng Tang", "Hui Liu", "Qi He", "Suhang Wang"], "title": "Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7"], "comment": "Under review", "summary": "Test-Time Scaling (TTS) improves large language models (LLMs) by allocating\nadditional computation during inference, typically through parallel,\nsequential, or hybrid scaling. However, prior studies often assume fixed\ncollaboration architectures (e.g., topologies) and single-model usage,\noverlooking that optimal architectures and model combinations can vary across\ntasks. Therefore, we study the novel problem of searching for compute-optimal\nmodel combinations and architectures in TTS under a fixed budget. We formalize\nit as a multi-LLM collaboration graph, where nodes encode roles and LLM model\nassignments, and edges capture information flow. This problem is challenging\nbecause (i) the combinatorial search space is prohibitively large, and (ii)\ntask-specific requirements demand tailored designs. To address these, we\nreformulate the problem as probabilistic graph optimization and, through pilot\nexperiments, derive three empirical insights into TTS collaboration graphs.\nGuided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented\nframework that mirrors the REINFORCE pipeline by mapping\nsampling-gradient-update to sampling-feedback-update, where feedback serves as\na textual gradient to update the probabilistic graph and efficiently search for\noptimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE\noutperforms both traditional and LLM-based baselines in sample efficiency and\nsearch performance, and effectively identifies optimal graphs under joint\nobjectives of accuracy and inference latency.", "AI": {"tldr": "Test-Time Scaling (TTS) enhances large language models (LLMs) by optimizing model combinations and collaboration architectures tailored to specific tasks under fixed budgets. A novel Agent-REINFORCE framework is introduced for efficiently identifying optimal collaboration graphs.", "motivation": "Existing methods in TTS often lack consideration for varying architectures and model combinations across tasks, leading to suboptimal efficiency and performance.", "method": "Researchers formalize TTS optimization as a probabilistic graph search problem and propose Agent-REINFORCE, a framework using textual feedback to optimize multi-LLM collaboration graphs.", "result": "Agent-REINFORCE surpasses traditional and LLM-based methods, demonstrating better sample efficiency, optimal graph identification, and superior balance between accuracy and inference latency.", "conclusion": "Agent-REINFORCE provides a powerful tool for tailorable TTS by efficiently discovering optimal multi-LLM collaboration configurations to enhance task performance under computing constraints."}}
{"id": "2511.00252", "pdf": "https://arxiv.org/pdf/2511.00252", "abs": "https://arxiv.org/abs/2511.00252", "authors": ["Aaron Sun", "Subhransu Maji", "Grant Van Horn"], "title": "Merlin L48 Spectrogram Dataset", "categories": ["cs.CV"], "comment": "Accepted to 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025) Track on Datasets and Benchmarks", "summary": "In the single-positive multi-label (SPML) setting, each image in a dataset is\nlabeled with the presence of a single class, while the true presence of other\nclasses remains unknown. The challenge is to narrow the performance gap between\nthis partially-labeled setting and fully-supervised learning, which often\nrequires a significant annotation budget. Prior SPML methods were developed and\nbenchmarked on synthetic datasets created by randomly sampling single positive\nlabels from fully-annotated datasets like Pascal VOC, COCO, NUS-WIDE, and\nCUB200. However, this synthetic approach does not reflect real-world scenarios\nand fails to capture the fine-grained complexities that can lead to difficult\nmisclassifications. In this work, we introduce the L48 dataset, a fine-grained,\nreal-world multi-label dataset derived from recordings of bird sounds. L48\nprovides a natural SPML setting with single-positive annotations on a\nchallenging, fine-grained domain, as well as two extended settings in which\ndomain priors give access to additional negative labels. We benchmark existing\nSPML methods on L48 and observe significant performance differences compared to\nsynthetic datasets and analyze method weaknesses, underscoring the need for\nmore realistic and difficult benchmarks.", "AI": {"tldr": "The paper introduces the L48 dataset, a real-world fine-grained multi-label dataset for single-positive multi-label (SPML) settings, derived from bird sound recordings.", "motivation": "To address the limitations of synthetic datasets in SPML research and provide a realistic dataset that captures fine-grained complexities for challenging benchmarks.", "method": "Develop and introduce the L48 dataset focusing on bird sound recordings with natural single-positive annotations and extended settings providing additional negative labels.", "result": "Existing SPML methods were benchmarked on L48, revealing significant performance differences compared to synthetic datasets and identifying weaknesses in current methods.", "conclusion": "Realistic and challenging benchmarks like L48 are essential for advancing SPML methods and narrowing the gap between partially-labeled and fully-supervised learning."}}
{"id": "2511.00727", "pdf": "https://arxiv.org/pdf/2511.00727", "abs": "https://arxiv.org/abs/2511.00727", "authors": ["Xuelin Yang", "Licong Lin", "Susan Athey", "Michael I. Jordan", "Guido W. Imbens"], "title": "Cross-Validated Causal Inference: a Modern Method to Combine Experimental and Observational Data", "categories": ["econ.EM", "stat.ME", "stat.ML"], "comment": "83 pages, 11 figures", "summary": "We develop new methods to integrate experimental and observational data in\ncausal inference. While randomized controlled trials offer strong internal\nvalidity, they are often costly and therefore limited in sample size.\nObservational data, though cheaper and often with larger sample sizes, are\nprone to biases due to unmeasured confounders. To harness their complementary\nstrengths, we propose a systematic framework that formulates causal estimation\nas an empirical risk minimization (ERM) problem. A full model containing the\ncausal parameter is obtained by minimizing a weighted combination of\nexperimental and observational losses--capturing the causal parameter's\nvalidity and the full model's fit, respectively. The weight is chosen through\ncross-validation on the causal parameter across experimental folds. Our\nexperiments on real and synthetic data show the efficacy and reliability of our\nmethod. We also provide theoretical non-asymptotic error bounds.", "AI": {"tldr": "The paper introduces a framework for combining experimental and observational data in causal inference using empirical risk minimization.", "motivation": "The authors aim to address the challenge of combining the strong validity of experiments with the large sample size of observational data, despite biases in observational data due to unmeasured confounders.", "method": "The paper proposes a causal inference framework by formulating the estimation as an empirical risk minimization problem. It minimizes a weighted combination of experimental and observational losses with cross-validation for weight selection.", "result": "Experiments using real and synthetic data demonstrate the reliability and efficacy of the proposed framework. Theoretical non-asymptotic error bounds are also provided.", "conclusion": "The proposed method effectively integrates experimental and observational data, balancing validity and fit to overcome limitations from small experimental samples and observational biases."}}
{"id": "2511.01018", "pdf": "https://arxiv.org/pdf/2511.01018", "abs": "https://arxiv.org/abs/2511.01018", "authors": ["Hui-Lee Ooi", "Nicholas Mitsakakis", "Margerie Huet Dastarac", "Roger Zemek", "Amy C. Plint", "Jeff Gilchrist", "Khaled El Emam", "Dhenuka Radhakrishnan"], "title": "AI for pRedicting Exacerbations in KIDs with aSthma (AIRE-KIDS)", "categories": ["cs.AI"], "comment": null, "summary": "Recurrent exacerbations remain a common yet preventable outcome for many\nchildren with asthma. Machine learning (ML) algorithms using electronic medical\nrecords (EMR) could allow accurate identification of children at risk for\nexacerbations and facilitate referral for preventative comprehensive care to\navoid this morbidity. We developed ML algorithms to predict repeat severe\nexacerbations (i.e. asthma-related emergency department (ED) visits or future\nhospital admissions) for children with a prior asthma ED visit at a tertiary\ncare children's hospital.\n  Retrospective pre-COVID19 (Feb 2017 - Feb 2019, N=2716) Epic EMR data from\nthe Children's Hospital of Eastern Ontario (CHEO) linked with environmental\npollutant exposure and neighbourhood marginalization information was used to\ntrain various ML models. We used boosted trees (LGBM, XGB) and 3 open-source\nlarge language model (LLM) approaches (DistilGPT2, Llama 3.2 1B and\nLlama-8b-UltraMedical). Models were tuned and calibrated then validated in a\nsecond retrospective post-COVID19 dataset (Jul 2022 - Apr 2023, N=1237) from\nCHEO. Models were compared using the area under the curve (AUC) and F1 scores,\nwith SHAP values used to determine the most predictive features.\n  The LGBM ML model performed best with the most predictive features in the\nfinal AIRE-KIDS_ED model including prior asthma ED visit, the Canadian triage\nacuity scale, medical complexity, food allergy, prior ED visits for non-asthma\nrespiratory diagnoses, and age for an AUC of 0.712, and F1 score of 0.51. This\nis a nontrivial improvement over the current decision rule which has F1=0.334.\nWhile the most predictive features in the AIRE-KIDS_HOSP model included medical\ncomplexity, prior asthma ED visit, average wait time in the ED, the pediatric\nrespiratory assessment measure score at triage and food allergy.", "AI": {"tldr": "This paper discusses the development of machine learning models to predict severe asthma exacerbations in children using electronic medical record data.", "motivation": "To prevent recurrent asthma exacerbations in children by identifying individuals at risk through machine learning methods, leveraging EMR data, environmental exposure, and social determinants of health.", "method": "The researchers developed machine learning models (boosted trees and large language models) based on retrospective EMR data and external factors. They trained models on pre-COVID data and validated them with post-COVID data, comparing performance metrics like AUC and F1 scores.", "result": "The best-performing model, LGBM, achieved an AUC of 0.712 and an F1 score of 0.51, improving over current decision rules. Key predictive features included prior asthma ED visits, medical complexity, Canadian triage scale, and associated health variables.", "conclusion": "ML models, particularly LGBM, showed potential for better identifying children at risk for severe asthma exacerbations compared to current methods, paving the way for more targeted interventions."}}
{"id": "2511.00657", "pdf": "https://arxiv.org/pdf/2511.00657", "abs": "https://arxiv.org/abs/2511.00657", "authors": ["Eshaan Tanwar", "Anwoy Chatterjee", "Michael Saxon", "Alon Albalak", "William Yang Wang", "Tanmoy Chakraborty"], "title": "Do You Know About My Nation? Investigating Multilingual Language Models' Cultural Literacy Through Factual Knowledge", "categories": ["cs.CL"], "comment": "Accepted in EMNLP 2025. Code at: https://github.com/EshaanT/XNationQA", "summary": "Most multilingual question-answering benchmarks, while covering a diverse\npool of languages, do not factor in regional diversity in the information they\ncapture and tend to be Western-centric. This introduces a significant gap in\nfairly evaluating multilingual models' comprehension of factual information\nfrom diverse geographical locations. To address this, we introduce XNationQA\nfor investigating the cultural literacy of multilingual LLMs. XNationQA\nencompasses a total of 49,280 questions on the geography, culture, and history\nof nine countries, presented in seven languages. We benchmark eight standard\nmultilingual LLMs on XNationQA and evaluate them using two novel transference\nmetrics. Our analyses uncover a considerable discrepancy in the models'\naccessibility to culturally specific facts across languages. Notably, we often\nfind that a model demonstrates greater knowledge of cultural information in\nEnglish than in the dominant language of the respective culture. The models\nexhibit better performance in Western languages, although this does not\nnecessarily translate to being more literate for Western countries, which is\ncounterintuitive. Furthermore, we observe that models have a very limited\nability to transfer knowledge across languages, particularly evident in\nopen-source models.", "AI": {"tldr": "The paper introduces a benchmark, XNationQA, to evaluate multilingual LLMs' cultural literacy, revealing discrepancies in handling culturally-specific facts across languages and noting limited knowledge transfer capabilities.", "motivation": "Current multilingual question-answering benchmarks lack regional diversity and focus predominantly on Western-centric information, limiting fair evaluation of multilingual LLMs' comprehension across different cultures.", "method": "The paper introduces XNationQA, a dataset of 49,280 questions about nine countries in seven languages, benchmarks eight multilingual LLMs, and uses novel transference metrics to analyze their performance.", "result": "The study reveals significant discrepancies in LLMs' cultural fact accessibility across languages, showing better performance in Western languages but limited cross-lingual knowledge transfer.", "conclusion": "Multilingual LLMs exhibit a bias towards Western languages and insufficient cultural literacy for non-Western countries, indicating the need for more diverse benchmarking approaches."}}
{"id": "2511.01252", "pdf": "https://arxiv.org/pdf/2511.01252", "abs": "https://arxiv.org/abs/2511.01252", "authors": ["Siyuan Li", "Yaowen Zheng", "Hong Li", "Jingdong Guo", "Chaopeng Dong", "Chunpeng Yan", "Weijie Wang", "Yimo Ren", "Limin Sun", "Hongsong Zhu"], "title": "Lares: LLM-driven Code Slice Semantic Search for Patch Presence Testing", "categories": ["cs.SE"], "comment": null, "summary": "In modern software ecosystems, 1-day vulnerabilities pose significant\nsecurity risks due to extensive code reuse. Identifying vulnerable functions in\ntarget binaries alone is insufficient; it is also crucial to determine whether\nthese functions have been patched. Existing methods, however, suffer from\nlimited usability and accuracy. They often depend on the compilation process to\nextract features, requiring substantial manual effort and failing for certain\nsoftware. Moreover, they cannot reliably differentiate between code changes\ncaused by patches or compilation variations. To overcome these limitations, we\npropose Lares, a scalable and accurate method for patch presence testing. Lares\nintroduces Code Slice Semantic Search, which directly extracts features from\nthe patch source code and identifies semantically equivalent code slices in the\npseudocode of the target binary. By eliminating the need for the compilation\nprocess, Lares improves usability, while leveraging large language models\n(LLMs) for code analysis and SMT solvers for logical reasoning to enhance\naccuracy. Experimental results show that Lares achieves superior precision,\nrecall, and usability. Furthermore, it is the first work to evaluate patch\npresence testing across optimization levels, architectures, and compilers. The\ndatasets and source code used in this article are available at\nhttps://github.com/Siyuan-Li201/Lares.", "AI": {"tldr": "The paper introduces Lares, a system that enhances security by identifying patches in binaries using Code Slice Semantic Search, bypassing dependency on compilation processes and improving accuracy and usability.", "motivation": "The paper addresses the challenges of identifying patched vulnerabilities in software binaries, including reliance on compilation processes and inaccuracy in distinguishing patch-related code changes from compilation variations.", "method": "Lares uses Code Slice Semantic Search to extract features directly from patch source code and identify semantically equivalent code slices in target binary pseudocode. It employs LLMs for code analysis and SMT solvers for logical reasoning.", "result": "Lares demonstrates superior precision, recall, and usability in detecting patch presence. It is evaluated across varying optimization levels, architectures, and compilers.", "conclusion": "The paper concludes that Lares is an effective, scalable, and accurate solution for patch presence testing, overcoming limitations of previous methods and advancing the field with its comprehensive evaluation approach."}}
{"id": "2511.01177", "pdf": "https://arxiv.org/pdf/2511.01177", "abs": "https://arxiv.org/abs/2511.01177", "authors": ["Zihao He", "Bo Ai", "Tongzhou Mu", "Yulin Liu", "Weikang Wan", "Jiawei Fu", "Yilun Du", "Henrik I. Christensen", "Hao Su"], "title": "Scaling Cross-Embodiment World Models for Dexterous Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Cross-embodiment learning seeks to build generalist robots that operate\nacross diverse morphologies, but differences in action spaces and kinematics\nhinder data sharing and policy transfer. This raises a central question: Is\nthere any invariance that allows actions to transfer across embodiments? We\nconjecture that environment dynamics are embodiment-invariant, and that world\nmodels capturing these dynamics can provide a unified interface across\nembodiments. To learn such a unified world model, the crucial step is to design\nstate and action representations that abstract away embodiment-specific details\nwhile preserving control relevance. To this end, we represent different\nembodiments (e.g., human hands and robot hands) as sets of 3D particles and\ndefine actions as particle displacements, creating a shared representation for\nheterogeneous data and control problems. A graph-based world model is then\ntrained on exploration data from diverse simulated robot hands and real human\nhands, and integrated with model-based planning for deployment on novel\nhardware. Experiments on rigid and deformable manipulation tasks reveal three\nfindings: (i) scaling to more training embodiments improves generalization to\nunseen ones, (ii) co-training on both simulated and real data outperforms\ntraining on either alone, and (iii) the learned models enable effective control\non robots with varied degrees of freedom. These results establish world models\nas a promising interface for cross-embodiment dexterous manipulation.", "AI": {"tldr": "The paper investigates cross-embodiment learning for robots, proposing a unified world model to handle diverse morphologies by abstracting embodiment-specific details. Experiments show promising results for manipulation tasks.", "motivation": "The authors aim to address the challenge of enabling generalist robots to operate across diverse morphologies, overcoming obstacles like differences in action spaces and kinematics.", "method": "They propose a unified state and action representation using 3D particle-based models and train a graph-based world model on diverse exploration data from both simulated robot hands and real human hands.", "result": "Experimental findings reveal that including more training embodiments improves generalization, combining real and simulated data leads to better outcomes, and the models effectively control robots with various degrees of freedom.", "conclusion": "World models provide an effective and promising approach for facilitating cross-embodiment dexterous manipulation in robots."}}
{"id": "2511.00097", "pdf": "https://arxiv.org/pdf/2511.00097", "abs": "https://arxiv.org/abs/2511.00097", "authors": ["Zihao Guo", "Qingyun Sun", "Ziwei Zhang", "Haonan Yuan", "Huiping Zhuang", "Xingcheng Fu", "Jianxin Li"], "title": "GraphKeeper: Graph Domain-Incremental Learning via Knowledge Disentanglement and Preservation", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by the Main Track of NeurIPS-2025", "summary": "Graph incremental learning (GIL), which continuously updates graph models by\nsequential knowledge acquisition, has garnered significant interest recently.\nHowever, existing GIL approaches focus on task-incremental and\nclass-incremental scenarios within a single domain. Graph domain-incremental\nlearning (Domain-IL), aiming at updating models across multiple graph domains,\nhas become critical with the development of graph foundation models (GFMs), but\nremains unexplored in the literature. In this paper, we propose Graph\nDomain-Incremental Learning via Knowledge Dientanglement and Preservation\n(GraphKeeper), to address catastrophic forgetting in Domain-IL scenario from\nthe perspectives of embedding shifts and decision boundary deviations.\nSpecifically, to prevent embedding shifts and confusion across incremental\ngraph domains, we first propose the domain-specific parameter-efficient\nfine-tuning together with intra- and inter-domain disentanglement objectives.\nConsequently, to maintain a stable decision boundary, we introduce\ndeviation-free knowledge preservation to continuously fit incremental domains.\nAdditionally, for graphs with unobservable domains, we perform domain-aware\ndistribution discrimination to obtain precise embeddings. Extensive experiments\ndemonstrate the proposed GraphKeeper achieves state-of-the-art results with\n6.5%~16.6% improvement over the runner-up with negligible forgetting. Moreover,\nwe show GraphKeeper can be seamlessly integrated with various representative\nGFMs, highlighting its broad applicative potential.", "AI": {"tldr": "The paper introduces \"GraphKeeper,\" a novel framework tackling graph domain-incremental learning by addressing catastrophic forgetting through embedding stability and decision boundary maintenance, achieving state-of-the-art results.", "motivation": "Current graph incremental learning approaches are limited to task- or class-incremental scenarios in single domains, overlooking the critical need for domain-incremental learning across multiple graph domains, as required by graph foundation models.", "method": "GraphKeeper implements domain-specific parameter-efficient fine-tuning, intra- and inter-domain disentanglement objectives, deviation-free knowledge preservation, and domain-aware distribution discrimination to ensure robust learning across incremental graph domains.", "result": "GraphKeeper outperforms the runner-up by 6.5%~16.6% with minimal forgetting and demonstrates compatibility with various graph foundation models, confirming its effectiveness and applicability.", "conclusion": "GraphKeeper effectively addresses challenges in graph domain-incremental learning, setting a benchmark in preventing catastrophic forgetting and reinforcing its potential use with diverse graph models."}}
{"id": "2511.00255", "pdf": "https://arxiv.org/pdf/2511.00255", "abs": "https://arxiv.org/abs/2511.00255", "authors": ["Fangxun Liu", "S M Rayeed", "Samuel Stevens", "Alyson East", "Cheng Hsuan Chiang", "Colin Lee", "Daniel Yi", "Junke Yang", "Tejas Naik", "Ziyi Wang", "Connor Kilrain", "Elijah H Buckwalter", "Jiacheng Hou", "Saul Ibaven Bueno", "Shuheng Wang", "Xinyue Ma", "Yifan Liu", "Zhiyuan Tao", "Ziheng Zhang", "Eric Sokol", "Michael Belitz", "Sydne Record", "Charles V. Stewart", "Wei-Lun Chao"], "title": "BeetleFlow: An Integrative Deep Learning Pipeline for Beetle Image Processing", "categories": ["cs.CV"], "comment": "4 pages, NeurIPS 2025 Workshop Imageomics", "summary": "In entomology and ecology research, biologists often need to collect a large\nnumber of insects, among which beetles are the most common species. A common\npractice for biologists to organize beetles is to place them on trays and take\na picture of each tray. Given the images of thousands of such trays, it is\nimportant to have an automated pipeline to process the large-scale data for\nfurther research. Therefore, we develop a 3-stage pipeline to detect all the\nbeetles on each tray, sort and crop the image of each beetle, and do\nmorphological segmentation on the cropped beetles. For detection, we design an\niterative process utilizing a transformer-based open-vocabulary object detector\nand a vision-language model. For segmentation, we manually labeled 670 beetle\nimages and fine-tuned two variants of a transformer-based segmentation model to\nachieve fine-grained segmentation of beetles with relatively high accuracy. The\npipeline integrates multiple deep learning methods and is specialized for\nbeetle image processing, which can greatly improve the efficiency to process\nlarge-scale beetle data and accelerate biological research.", "AI": {"tldr": "The paper introduces a deep learning-based automated pipeline that detects, crops, and segments beetle images from tray images to streamline biological research.", "motivation": "Biologists need to efficiently process large-scale beetle image data for entomology and ecology research, which is difficult with manual methods.", "method": "The authors propose a 3-stage pipeline involving beetle detection, cropping, and fine-grained morphological segmentation using transformer-based detection and segmentation models.", "result": "The pipeline improves detection and segmentation accuracy, successfully processes tray images into individual beetle data, and utilizes 670 labeled beetle images for training.", "conclusion": "This pipeline enhances data processing efficiency for beetle-centric research, advancing biological studies and reducing manual effort."}}
{"id": "2511.00904", "pdf": "https://arxiv.org/pdf/2511.00904", "abs": "https://arxiv.org/abs/2511.00904", "authors": ["Ernesto Araya", "Massimiliano Datres", "Gitta Kutyniok"], "title": "Random Spiking Neural Networks are Stable and Spectrally Simple", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Spiking neural networks (SNNs) are a promising paradigm for energy-efficient\ncomputation, yet their theoretical foundations-especially regarding stability\nand robustness-remain limited compared to artificial neural networks. In this\nwork, we study discrete-time leaky integrate-and-fire (LIF) SNNs through the\nlens of Boolean function analysis. We focus on noise sensitivity and stability\nin classification tasks, quantifying how input perturbations affect outputs.\nOur main result shows that wide LIF-SNN classifiers are stable on average, a\nproperty explained by the concentration of their Fourier spectrum on\nlow-frequency components. Motivated by this, we introduce the notion of\nspectral simplicity, which formalizes simplicity in terms of Fourier spectrum\nconcentration and connects our analysis to the simplicity bias observed in deep\nnetworks. Within this framework, we show that random LIF-SNNs are biased toward\nsimple functions. Experiments on trained networks confirm that these stability\nproperties persist in practice. Together, these results provide new insights\ninto the stability and robustness properties of SNNs.", "AI": {"tldr": "The paper examines spiking neural networks (SNNs), highlighting their stability and robustness through an analysis of their Fourier spectrum, and introduces the concept of spectral simplicity.", "motivation": "The motivation is to bridge the gap in understanding the theoretical foundations of spiking neural networks (SNNs), specifically in terms of stability and robustness, as compared to artificial neural networks.", "method": "The authors study discrete-time leaky integrate-and-fire (LIF) SNNs using Boolean function analysis, focusing on noise sensitivity and the impact of input perturbations on outputs. They analyze the Fourier spectrum to quantify this behavior.", "result": "The paper demonstrates that wide LIF-SNN classifiers are stable on average due to the concentration of their Fourier spectrum on low-frequency components. Experiments on trained networks also validate the persistence of this stability.", "conclusion": "The study highlights that SNNs possess inherent stability and spectral simplicity, where random LIF-SNNs are biased toward simple functions, providing new insights into their robustness and energy-efficient potentials."}}
{"id": "2511.01033", "pdf": "https://arxiv.org/pdf/2511.01033", "abs": "https://arxiv.org/abs/2511.01033", "authors": ["Tiberiu Musat", "Tiago Pimentel", "Lorenzo Noci", "Alessandro Stolfo", "Mrinmaya Sachan", "Thomas Hofmann"], "title": "On the Emergence of Induction Heads for In-Context Learning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Transformers have become the dominant architecture for natural language\nprocessing. Part of their success is owed to a remarkable capability known as\nin-context learning (ICL): they can acquire and apply novel associations solely\nfrom their input context, without any updates to their weights. In this work,\nwe study the emergence of induction heads, a previously identified mechanism in\ntwo-layer transformers that is particularly important for in-context learning.\nWe uncover a relatively simple and interpretable structure of the weight\nmatrices implementing the induction head. We theoretically explain the origin\nof this structure using a minimal ICL task formulation and a modified\ntransformer architecture. We give a formal proof that the training dynamics\nremain constrained to a 19-dimensional subspace of the parameter space.\nEmpirically, we validate this constraint while observing that only 3 dimensions\naccount for the emergence of an induction head. By further studying the\ntraining dynamics inside this 3-dimensional subspace, we find that the time\nuntil the emergence of an induction head follows a tight asymptotic bound that\nis quadratic in the input context length.", "AI": {"tldr": "The paper investigates how induction heads in transformers emerge and function, providing theoretical and empirical evidence about their structure and training dynamics.", "motivation": "To understand the mechanisms behind in-context learning enabled by transformers, particularly focusing on induction heads.", "method": "The authors use a combination of theoretical analysis and empirical validation. They introduce a modified transformer architecture and study a minimal in-context learning task to derive insights.", "result": "They discover a simple structure in the weight matrices of induction heads and show that training dynamics are constrained to a 19-dimensional subspace, with three dimensions crucial for induction head emergence. They also establish that the emergence is quadratically dependent on the input context length.", "conclusion": "Induction heads, crucial for in-context learning in transformers, have interpretable dynamics and structure. Their emergence can be fully understood within a constrained subspace, offering insights into transformer behavior."}}
{"id": "2511.00689", "pdf": "https://arxiv.org/pdf/2511.00689", "abs": "https://arxiv.org/abs/2511.00689", "authors": ["Berk Atil", "Rebecca J. Passonneau", "Fred Morstatter"], "title": "Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) undergo safety alignment after training and\ntuning, yet recent work shows that safety can be bypassed through jailbreak\nattacks. While many jailbreaks and defenses exist, their cross-lingual\ngeneralization remains underexplored. This paper presents the first systematic\nmultilingual evaluation of jailbreaks and defenses across ten\nlanguages--spanning high-, medium-, and low-resource languages--using six LLMs\non HarmBench and AdvBench. We assess two jailbreak types:\nlogical-expression-based and adversarial-prompt-based. For both types, attack\nsuccess and defense robustness vary across languages: high-resource languages\nare safer under standard queries but more vulnerable to adversarial ones.\nSimple defenses can be effective, but are language- and model-dependent. These\nfindings call for language-aware and cross-lingual safety benchmarks for LLMs.", "AI": {"tldr": "This study evaluates multilingual jailbreaks and defenses in LLMs across ten languages using two attack types, revealing variations in safety and the need for cross-lingual safety benchmarks.", "motivation": "Addressing the gap in understanding how well jailbreak attacks and defenses generalize across languages for large language models.", "method": "Systematic evaluation of jailbreaks and defenses across ten languages using six LLMs on HarmBench and AdvBench, assessing logical-expression-based and adversarial-prompt-based attacks.", "result": "Finds that attack success and defense robustness vary by language, with high-resource languages being safer under standard queries but more vulnerable to adversarial ones.", "conclusion": "Calls for developing language-aware and cross-lingual safety benchmarks for improving the safety of LLMs."}}
{"id": "2511.01316", "pdf": "https://arxiv.org/pdf/2511.01316", "abs": "https://arxiv.org/abs/2511.01316", "authors": ["Chong Wang", "Chen Zhang", "Jiajun Wu", "Wunan Guo", "Jianfeng Qu", "Yewen Tian", "Yang Liu"], "title": "Exploringand Unleashing the Power of Large Language Models in CI/CD Configuration Translation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Continuous Integration (CI) is a cornerstone of modern collaborative software\ndevelopment, and numerous CI platforms are available. Differences in\nmaintenance overhead, reliability, and integration depth with code-hosting\nplatforms make migration between CI platforms a common practice. A central step\nin migration is translating CI configurations, which is challenging due to the\nintrinsic complexity of CI configurations and the need to understand semantic\ndifferences and relationships across CI platforms.\n  With the advent of large language models (LLMs), recent advances in software\nengineering highlight their potential for CI configuration translation. In this\npaper, we present a study on LLM-based CI configuration translation, focusing\non the migration from Travis CI to GitHub Actions. First, using 811 migration\nrecords, we quantify the effort involved and find that developers read an\naverage of 38 lines of Travis configuration and write 58 lines of GitHub\nActions configuration, with nearly half of the migrations requiring multiple\ncommits. We further analyze translations produced by each of the four LLMs and\nidentify 1,121 issues grouped into four categories: logic inconsistencies\n(38%), platform discrepancies (32%), environment errors (25%), and syntax\nerrors (5%). Finally, we evaluate three enhancement strategies and show that\ncombining guideline-based prompting with iterative refinement achieves the best\nperformance, reaching a Build Success Rate of 75.5%-nearly a threefold\nimprovement over GPT-4o with a basic prompt.", "AI": {"tldr": "The study investigates LLM-based CI configuration translation from Travis CI to GitHub Actions, highlighting challenges and improvements with iterative refinement.", "motivation": "The paper aims to address challenges in CI configuration migration, leveraging LLMs to bridge gaps caused by complex semantic differences between CI platforms.", "method": "The authors quantify migration efforts, analyze LLM-generated translations for errors, and evaluate enhancement strategies like guideline-based prompting and iterative refinement.", "result": "Developers navigate significant CI migration efforts, LLM translations exhibit various errors, and combining guideline-based prompts with iterative refinement significantly improves translation success.", "conclusion": "Refined LLM strategies improve CI migration accuracy, reducing developer burdens and advancing configuration translation approaches."}}
{"id": "2511.01186", "pdf": "https://arxiv.org/pdf/2511.01186", "abs": "https://arxiv.org/abs/2511.01186", "authors": ["Lijie Wang", "Lianjie Guo", "Ziyi Xu", "Qianhao Wang", "Fei Gao", "Xieyuanli Chen"], "title": "LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Reconstructing large-scale colored point clouds is an important task in\nrobotics, supporting perception, navigation, and scene understanding. Despite\nadvances in LiDAR inertial visual odometry (LIVO), its performance remains\nhighly sensitive to extrinsic calibration. Meanwhile, 3D vision foundation\nmodels, such as VGGT, suffer from limited scalability in large environments and\ninherently lack metric scale. To overcome these limitations, we propose\nLiDAR-VGGT, a novel framework that tightly couples LiDAR inertial odometry with\nthe state-of-the-art VGGT model through a two-stage coarse- to-fine fusion\npipeline: First, a pre-fusion module with robust initialization refinement\nefficiently estimates VGGT poses and point clouds with coarse metric scale\nwithin each session. Then, a post-fusion module enhances cross-modal 3D\nsimilarity transformation, using bounding-box-based regularization to reduce\nscale distortions caused by inconsistent FOVs between LiDAR and camera sensors.\nExtensive experiments across multiple datasets demonstrate that LiDAR-VGGT\nachieves dense, globally consistent colored point clouds and outperforms both\nVGGT-based methods and LIVO baselines. The implementation of our proposed novel\ncolor point cloud evaluation toolkit will be released as open source.", "AI": {"tldr": "The paper presents LiDAR-VGGT, a framework combining LiDAR and VGGT to reconstruct large-scale colored point clouds efficiently.", "motivation": "Reconstructing large-scale colored point clouds is crucial in robotics for tasks like navigation and perception, but existing models suffer from calibration and scalability challenges.", "method": "The authors propose a two-stage coarse-to-fine fusion method that combines LiDAR inertial odometry with VGGT through pre-fusion (to refine initialization) and post-fusion (to reduce scale distortions).", "result": "Extensive experiments show the proposed method outperforms VGGT-based methods and LIVO baselines in generating dense, consistent point clouds.", "conclusion": "LiDAR-VGGT effectively addresses calibration and scalability limits in existing methods, providing superior large-scale point cloud reconstruction, with plans to release an open-source toolkit."}}
{"id": "2511.00099", "pdf": "https://arxiv.org/pdf/2511.00099", "abs": "https://arxiv.org/abs/2511.00099", "authors": ["Marios Impraimakis", "Evangelia Nektaria Palkanoglou"], "title": "A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.SY", "eess.SP", "eess.SY", "68T05 (Learning and adaptive systems) 93C95 (Neural networks in\n  control theory)", "I.2.6; I.2.8"], "comment": "21 pages, 23 figures, published in Structural and Multidisciplinary\n  Optimization", "summary": "The optimization-based damage detection and damage state digital twinning\ncapabilities are examined here of a novel conditional-labeled generative\nadversarial network methodology. The framework outperforms current approaches\nfor fault anomaly detection as no prior information is required for the health\nstate of the system: a topic of high significance for real-world applications.\nSpecifically, current artificial intelligence-based digital twinning approaches\nsuffer from the uncertainty related to obtaining poor predictions when a low\nnumber of measurements is available, physics knowledge is missing, or when the\ndamage state is unknown. To this end, an unsupervised framework is examined and\nvalidated rigorously on the benchmark structural health monitoring measurements\nof Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In\nimplementing the approach, firstly, different same damage-level measurements\nare used as inputs, while the model is forced to converge conditionally to two\ndifferent damage states. Secondly, the process is repeated for a different\ngroup of measurements. Finally, the convergence scores are compared to identify\nwhich one belongs to a different damage state. The process for both\nhealthy-to-healthy and damage-to-healthy input data creates, simultaneously,\nmeasurements for digital twinning purposes at different damage states, capable\nof pattern recognition and machine learning data generation. Further to this\nprocess, a support vector machine classifier and a principal component analysis\nprocedure is developed to assess the generated and real measurements of each\ndamage category, serving as a secondary new dynamics learning indicator in\ndamage scenarios. Importantly, the approach is shown to capture accurately\ndamage over healthy measurements, providing a powerful tool for vibration-based\nsystem-level monitoring and scalable infrastructure resilience.", "AI": {"tldr": "The paper presents a novel conditional-labeled generative adversarial network for damage detection and digital twinning, outperforming current methods by requiring no prior health state information.", "motivation": "Current AI-based digital twinning approaches face challenges due to limited measurements, lack of physics knowledge, and unknown damage states. This research addresses these uncertainties.", "method": "An unsupervised framework is applied to the Z24 Bridge's structural health monitoring data, with conditional convergence of damage levels and secondary validation through support vector machine classifiers and principal component analysis.", "result": "The approach accurately detects damage states over healthy measurements, facilitates digital twinning, and generates machine learning data for damage detection.", "conclusion": "This method enhances system-level monitoring and infrastructure resilience, proving effective for practical applications in real-world scenarios."}}
{"id": "2511.00260", "pdf": "https://arxiv.org/pdf/2511.00260", "abs": "https://arxiv.org/abs/2511.00260", "authors": ["Linzhe Jiang", "Jiayuan Huang", "Sophia Bano", "Matthew J. Clarkson", "Zhehua Mao", "Mobarak I. Hoque"], "title": "MambaNetLK: Enhancing Colonoscopy Point Cloud Registration with Mamba", "categories": ["cs.CV", "68T07 (Primary) 68T45, 92C55 (Secondary)"], "comment": "12 pages, 4 figures, 3 tables, IPCAI conference", "summary": "Accurate 3D point cloud registration underpins reliable image-guided\ncolonoscopy, directly affecting lesion localization, margin assessment, and\nnavigation safety. However, biological tissue exhibits repetitive textures and\nlocally homogeneous geometry that cause feature degeneracy, while substantial\ndomain shifts between pre-operative anatomy and intra-operative observations\nfurther degrade alignment stability. To address these clinically critical\nchallenges, we introduce a novel 3D registration method tailored for endoscopic\nnavigation and a high-quality, clinically grounded dataset to support rigorous\nand reproducible benchmarking. We introduce C3VD-Raycasting-10k, a large-scale\nbenchmark dataset with 10,014 geometrically aligned point cloud pairs derived\nfrom clinical CT data. We propose MambaNetLK, a novel correspondence-free\nregistration framework, which enhances the PointNetLK architecture by\nintegrating a Mamba State Space Model (SSM) as a cross-modal feature extractor.\nAs a result, the proposed framework efficiently captures long-range\ndependencies with linear-time complexity. The alignment is achieved iteratively\nusing the Lucas-Kanade algorithm. On the clinical dataset, C3VD-Raycasting-10k,\nMambaNetLK achieves the best performance compared with the state-of-the-art\nmethods, reducing median rotation error by 56.04% and RMSE translation error by\n26.19% over the second-best method. The model also demonstrates strong\ngeneralization on ModelNet40 and superior robustness to initial pose\nperturbations. MambaNetLK provides a robust foundation for 3D registration in\nsurgical navigation. The combination of a globally expressive SSM-based feature\nextractor and a large-scale clinical dataset enables more accurate and reliable\nguidance systems in minimally invasive procedures like colonoscopy.", "AI": {"tldr": "This paper introduces MambaNetLK, a novel 3D registration framework for colonoscopy navigation, along with a large clinical benchmark dataset (C3VD-Raycasting-10k), achieving superior performance in accuracy and robustness.", "motivation": "Overcome the challenges in 3D point cloud registration caused by feature degeneracy in biological tissues and substantial domain shifts during minimally invasive colonoscopy procedures.", "method": "Develop MambaNetLK, a correspondence-free registration framework that enhances PointNetLK with a Mamba State Space Model for cross-modal feature extraction, combined with iterative Lucas-Kanade alignment.", "result": "MambaNetLK achieves state-of-the-art performance, reducing rotation and translation errors significantly compared to other methods, and demonstrates strong generalization and robustness on additional datasets.", "conclusion": "MambaNetLK and the clinical dataset establish a robust foundation for enhancing surgical navigation systems, ensuring more accurate and reliable guidance in procedures like colonoscopy."}}
{"id": "2511.00958", "pdf": "https://arxiv.org/pdf/2511.00958", "abs": "https://arxiv.org/abs/2511.00958", "authors": ["Khoat Than"], "title": "The Hidden Power of Normalization: Exponential Capacity Control in Deep Neural Networks", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Normalization methods are fundamental components of modern deep neural\nnetworks (DNNs). Empirically, they are known to stabilize optimization dynamics\nand improve generalization. However, the underlying theoretical mechanism by\nwhich normalization contributes to both optimization and generalization remains\nlargely unexplained, especially when using many normalization layers in a DNN\narchitecture.\n  In this work, we develop a theoretical framework that elucidates the role of\nnormalization through the lens of capacity control. We prove that an\nunnormalized DNN can exhibit exponentially large Lipschitz constants with\nrespect to either its parameters or inputs, implying excessive functional\ncapacity and potential overfitting. Such bad DNNs are uncountably many. In\ncontrast, the insertion of normalization layers provably can reduce the\nLipschitz constant at an exponential rate in the number of normalization\noperations. This exponential reduction yields two fundamental consequences: (1)\nit smooths the loss landscape at an exponential rate, facilitating faster and\nmore stable optimization; and (2) it constrains the effective capacity of the\nnetwork, thereby enhancing generalization guarantees on unseen data. Our\nresults thus offer a principled explanation for the empirical success of\nnormalization methods in deep learning.", "AI": {"tldr": "Normalization methods reduce the Lipschitz constants of DNNs exponentially by smoothing optimization and enhancing generalization.", "motivation": "Investigate the underlying mechanism and theoretical benefits of normalization methods in deep learning, especially with multiple normalization layers.", "method": "Develop a theoretical framework focusing on the Lipschitz constant to analyze normalization's impact in controlling capacity in DNNs.", "result": "Normalization exponentially reduces the Lipschitz constant, leading to smoother loss landscapes and constrained network capacity.", "conclusion": "Normalization enhances optimization stability and generalization performance, explaining its empirical benefits in DNN architectures."}}
{"id": "2511.01052", "pdf": "https://arxiv.org/pdf/2511.01052", "abs": "https://arxiv.org/abs/2511.01052", "authors": ["Yeawon Lee", "Christopher C. Yang", "Chia-Hsuan Chang", "Grace Lu-Yao"], "title": "Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports", "categories": ["cs.AI", "physics.med-ph"], "comment": null, "summary": "Cancer staging is critical for patient prognosis and treatment planning, yet\nextracting pathologic TNM staging from unstructured pathology reports poses a\npersistent challenge. Existing natural language processing (NLP) and machine\nlearning (ML) strategies often depend on large annotated datasets, limiting\ntheir scalability and adaptability. In this study, we introduce two Knowledge\nElicitation methods designed to overcome these limitations by enabling large\nlanguage models (LLMs) to induce and apply domain-specific rules for cancer\nstaging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses\nan iterative prompting strategy to derive staging rules directly from\nunannotated pathology reports, without requiring ground-truth labels. The\nsecond, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG),\nemploys a variation of RAG where rules are pre-extracted from relevant\nguidelines in a single step and then applied, enhancing interpretability and\navoiding repeated retrieval overhead. We leverage the ability of LLMs to apply\nbroad knowledge learned during pre-training to new tasks. Using breast cancer\npathology reports from the TCGA dataset, we evaluate their performance in\nidentifying T and N stages, comparing them against various baseline approaches\non two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG\nwhen Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG\nachieves better performance when ZSCOT inference is less effective. Both\nmethods offer transparent, interpretable interfaces by making the induced rules\nexplicit. These findings highlight the promise of our Knowledge Elicitation\nmethods as scalable, high-performing solutions for automated cancer staging\nwith enhanced interpretability, particularly in clinical settings with limited\nannotated data.", "AI": {"tldr": "This study focuses on using large language models (LLMs) with unique methods for automated cancer staging from unstructured pathology reports, specifically targeting the TNM system. Two approaches, KEwLTM and KEwRAG, improve interpretability and scalability without needing annotated datasets.", "motivation": "Cancer staging is vital for prognosis and treatment planning, but extracting staging information from pathology reports is challenging due to their unstructured nature and the dependency on annotated datasets in existing NLP/ML strategies.", "method": "The paper introduces two methods: KEwLTM, which uses iterative prompting and the LLM's memory to derive rules from reports; and KEwRAG, which utilizes retrieval-augmented strategy by pre-extracting rules from guidelines. Both rely on LLMs applying broad pre-learned knowledge.", "result": "KEwLTM excelled when Zero-Shot Chain-of-Thought (ZSCOT) inference was effective, while KEwRAG had better results when ZSCOT inference struggled. Evaluation was conducted with breast cancer reports from TCGA dataset, with both methods providing interpretable rule-based outputs.", "conclusion": "The proposed methods are promising scalable solutions for automated cancer staging, especially in clinical settings with limited annotated data, offering high performance and transparency."}}
{"id": "2511.00819", "pdf": "https://arxiv.org/pdf/2511.00819", "abs": "https://arxiv.org/abs/2511.00819", "authors": ["Yuxuan Hu", "Jianchao Tan", "Jiaqi Zhang", "Wen Zan", "Pingwei Sun", "Yifan Lu", "Yerui Sun", "Yuchen Xie", "Xunliang Cai", "Jing Zhang"], "title": "Optimizing Native Sparse Attention with Latent Attention and Local Global Alternating Strategies", "categories": ["cs.CL"], "comment": null, "summary": "In this work, we conduct a systematic analysis of Native Sparse Attention\n(NSA) and propose targeted improvements that enhance long-context modeling. A\nkey insight is that alternating between local (sliding-window) and global\n(compression, selective) attention across layers, rather than using fixed\npatterns, enables more effective propagation of long-range dependencies and\nsubstantially boosts performance on long-sequence tasks. Meanwhile, we further\nrefine NSA's branches with Latent Attention that the sliding-window branch is\nenhanced with Multi-head Latent Attention (MLA) while compression and selective\nbranches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache\nmemory by 50\\% versus NSA while improving the model's common-sense reasoning\nand long-text understanding capabilities. Experiments on models from 340M to\n1.3B parameters (trained on 15B and 100B tokens) show our method matches or\nexceeds full attention and native sparse attention in both common-sense\nreasoning and long-context understanding tasks.", "AI": {"tldr": "This paper enhances Native Sparse Attention (NSA) with new techniques improving long-context modeling and reducing memory usage during training.", "motivation": "To address limitations of existing NSA in propagating long-range dependencies efficiently and enhancing long-sequence task performance.", "method": "Alternating between local and global attention across layers to improve dependency propagation, introducing Latent Attention mechanisms (MLA and GLA) to reduce memory requirements while boosting performance.", "result": "Achieved reduced KV-cache memory usage by 50% with enhanced common-sense reasoning and long-text understanding capabilities, tested on models ranging from 340M to 1.3B parameters.", "conclusion": "The proposed method outperforms conventional NSA and full attention approaches in reasoning and long-context tasks."}}
{"id": "2511.01324", "pdf": "https://arxiv.org/pdf/2511.01324", "abs": "https://arxiv.org/abs/2511.01324", "authors": ["Lekshmi Murali Rani", "Richard Berntsson Svensson", "Robert Feldt"], "title": "AI for Requirements Engineering: Industry adoption and Practitioner perspectives", "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": "Accepted at the Intelligent Software Engineering (ISE) 2025 Workshop\n  at the Automated Software Engineering (ASE) 2025 Conference", "summary": "The integration of AI for Requirements Engineering (RE) presents significant\nbenefits but also poses real challenges.Although RE is fundamental to software\nengineering, limited research has examined AI adoption in RE.We surveyed 55\nsoftware practitioners to map AI usage across four RE phases:Elicitation,\nAnalysis, Specification, and Validation, and four approaches for decision\nmaking: human only decisions, AI validation, Human AI Collaboration (HAIC), and\nfull AI automation.Participants also shared their perceptions, challenges, and\nopportunities when applying AI for RE tasks.Our data show that 58.2% of\nrespondents already use AI in RE, and 69.1% view its impact as positive or very\npositive.HAIC dominates practice, accounting for 54.4% of all RE techniques,\nwhile full AI automation remains minimal at 5.4%.Passive AI validation (4.4 to\n6.2%) lags even further behind, indicating that practitioners value AI's active\nsupport over passive oversight.These findings suggest that AI is most effective\nwhen positioned as a collaborative partner rather than a replacement for human\nexpertise.It also highlights the need for RE specific HAIC frameworks along\nwith robust and responsible AI governance as AI adoption in RE grows.", "AI": {"tldr": "This paper studies AI's integration in Requirements Engineering (RE), showcasing current practices, perceptions, and emphasizing Human-AI Collaboration (HAIC) over full automation.", "motivation": "To explore how AI is being used in Requirements Engineering (RE) and analyze its benefits, challenges, and current adoption trends in software engineering.", "method": "The researchers surveyed 55 software practitioners to map AI usage across RE phases and decision-making approaches, gathering data on perceptions, challenges, and opportunities.", "result": "58.2% of respondents use AI in RE, with 69.1% finding its impact positive. HAIC is prevalent in practice, used in 54.4% of RE techniques, whereas full AI automation is minimal at 5.4%. Passive AI validation is even less common.", "conclusion": "AI is most effective in RE as a collaborative partner (HAIC) rather than a human replacement. There is a need for frameworks and responsible AI governance to scale adoption effectively."}}
{"id": "2511.01199", "pdf": "https://arxiv.org/pdf/2511.01199", "abs": "https://arxiv.org/abs/2511.01199", "authors": ["Max McCandless", "Jonathan Hamid", "Sammy Elmariah", "Nathaniel Langer", "Pierre E. Dupont"], "title": "Closed-loop Control of Steerable Balloon Endoscopes for Robot-assisted Transcatheter Intracardiac Procedures", "categories": ["cs.RO"], "comment": "8 pages, 11 figures", "summary": "To move away from open-heart surgery towards safer transcatheter procedures,\nthere is a growing need for improved imaging techniques and robotic solutions\nto enable simple, accurate tool navigation. Common imaging modalities, such as\nfluoroscopy and ultrasound, have limitations that can be overcome using\ncardioscopy, i.e., direct optical visualization inside the beating heart. We\npresent a cardioscope designed as a steerable balloon. As a balloon, it can be\ncollapsed to pass through the vasculature and subsequently inflated inside the\nheart for visualization and tool delivery through an integrated working\nchannel. Through careful design of balloon wall thickness, a single input,\nballoon inflation pressure, is used to independently control two outputs,\nballoon diameter (corresponding to field of view diameter) and balloon bending\nangle (enabling precise working channel positioning). This balloon technology\ncan be tuned to produce cardioscopes designed for a range of intracardiac\ntasks. To illustrate this approach, a balloon design is presented for the\nspecific task of aortic leaflet laceration. Image-based closed-loop control of\nbending angle is also demonstrated as a means of enabling stable orientation\ncontrol during tool insertion and removal.", "AI": {"tldr": "This paper introduces a cardioscope in the form of a steerable balloon for intracardiac visualization and tool delivery, aiming to improve imaging and tool navigation during minimally invasive transcatheter procedures.", "motivation": "Current imaging modalities like fluoroscopy and ultrasound have limitations, and there is a need for advanced imaging and navigation tools to replace open-heart surgery with safer transcatheter procedures.", "method": "The study designed and tested a balloon-shaped cardioscope, capable of collapsing for vascular insertion and inflating within the heart. The balloon, through a single inflation input, can independently alter its diameter and bending angle for better field of view and precise tool control.", "result": "A balloon-based cardioscope design was demonstrated for an aortic leaflet laceration task. Image-based closed-loop control was used to achieve stable orientation during tool handling.", "conclusion": "The balloon technology presented can be optimized for various intracardiac tasks, potentially making minimally-invasive cardiac procedures safer and more effective."}}
{"id": "2511.00100", "pdf": "https://arxiv.org/pdf/2511.00100", "abs": "https://arxiv.org/abs/2511.00100", "authors": ["Marios Impraimakis"], "title": "Deep recurrent-convolutional neural network learning and physics Kalman filtering comparison in dynamic load identification", "categories": ["cs.LG", "cs.CV", "cs.SY", "eess.SP", "eess.SY", "stat.AP", "68T05 (Learning and adaptive systems) 93C95 (Neural networks in\n  control theory)", "I.2.6; I.2.8"], "comment": "31 pages, 20 figures, published in Structural Health Monitoring", "summary": "The dynamic structural load identification capabilities of the gated\nrecurrent unit, long short-term memory, and convolutional neural networks are\nexamined herein. The examination is on realistic small dataset training\nconditions and on a comparative view to the physics-based residual Kalman\nfilter (RKF). The dynamic load identification suffers from the uncertainty\nrelated to obtaining poor predictions when in civil engineering applications\nonly a low number of tests are performed or are available, or when the\nstructural model is unidentifiable. In considering the methods, first, a\nsimulated structure is investigated under a shaker excitation at the top floor.\nSecond, a building in California is investigated under seismic base excitation,\nwhich results in loading for all degrees of freedom. Finally, the International\nAssociation for Structural Control-American Society of Civil Engineers\n(IASC-ASCE) structural health monitoring benchmark problem is examined for\nimpact and instant loading conditions. Importantly, the methods are shown to\noutperform each other on different loading scenarios, while the RKF is shown to\noutperform the networks in physically parametrized identifiable cases.", "AI": {"tldr": "The study compares neural networks (GRU, LSTM, CNN) with the physics-based residual Kalman Filter (RKF) for structural load identification under small dataset conditions.", "motivation": "To address challenges faced in dynamic load identification in civil engineering applications, especially when data availability is limited or structural models are unidentifiable.", "method": "The study compares neural networks (GRU, LSTM, CNN) and RKF using three scenarios: a simulated structure under shaker excitation, a California building under seismic excitation, and a benchmark problem under impact and instant loading.", "result": "The study found that neural networks and RKF excel in different conditions. RKF outperformed the networks in physically parametrized and identifiable cases, while neural networks showed superior performance in other scenarios.", "conclusion": "The combined approach of RKF and neural networks offers a comprehensive solution for dynamic load identification by leveraging strengths of both physics-based and data-driven methods."}}
{"id": "2511.00261", "pdf": "https://arxiv.org/pdf/2511.00261", "abs": "https://arxiv.org/abs/2511.00261", "authors": ["Neha Balamurugan", "Sarah Wu", "Adam Chun", "Gabe Gaw", "Cristobal Eyzaguirre", "Tobias Gerstenberg"], "title": "Spot The Ball: A Benchmark for Visual Social Inference", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Humans excel at visual social inference, the ability to infer hidden elements\nof a scene from subtle behavioral cues such as other people's gaze, pose, and\norientation. This ability drives everyday social reasoning in humans and is\ncritical for developing more human-like AI agents. We introduce Spot The Ball,\na challenging benchmark for evaluating visual social inference in\nvision-language models (VLMs) using sports as a test domain. The task is to\nlocalize a removed sports ball from soccer, basketball, and volleyball images.\nWe present a curated evaluation set with human baselines and a scalable\npipeline for generating additional test items. We evaluate four\nstate-of-the-art VLMs (Gemini, GPT, LLaMA, Qwen) using three prompting\nstrategies, finding that humans are consistently two to three times more\naccurate (20-34%) than models ($\\leq$ 17%) across all sports. Our analyses show\nthat models rely on superficial spatial heuristics--such as guessing near the\nimage center or nearby players--while humans leverage social cues like gaze\ndirection and body pose. These findings reveal a persistent human-model gap in\nvisual social reasoning and underscore the need for architectures that\nexplicitly encode structured behavioral cues to achieve robust, human-like\ninference.", "AI": {"tldr": "The paper introduces Spot The Ball, a benchmark evaluating visual social inference in AI models, showing that humans outperform models significantly in localizing removed sports balls based on social cues.", "motivation": "To address the lack of robust human-like social reasoning in AI and vision-language models, particularly in interpreting behavioral cues for inference.", "method": "A benchmark task, Spot The Ball, focusing on sports images where the ball is removed. Four VLMs are tested with different prompting strategies, alongside human baselines, using curated and scalable datasets.", "result": "Humans performed 2-3 times more accurately (20-34%) compared to models (\u226417%), with models relying on spatial heuristics rather than social cues.", "conclusion": "Current models show a significant gap in visual social inference. To mimic human-level reasoning, architectures must integrate structured behavioral cues."}}
{"id": "2511.01069", "pdf": "https://arxiv.org/pdf/2511.01069", "abs": "https://arxiv.org/abs/2511.01069", "authors": ["Georg Pichler", "Marco Romanelli", "Pablo Piantanida"], "title": "Happiness as a Measure of Fairness", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "In this paper, we propose a novel fairness framework grounded in the concept\nof happi- ness, a measure of the utility each group gains fromdecisionoutcomes.\nBycapturingfairness through this intuitive lens, we not only offer a more\nhuman-centered approach, but also one that is mathematically rigorous: In order\nto compute the optimal, fair post-processing strategy, only a linear program\nneeds to be solved. This makes our method both efficient and scalable with\nexisting optimization tools. Furthermore, it unifies and extends several\nwell-known fairness definitions, and our em- pirical results highlight its\npractical strengths across diverse scenarios.", "AI": {"tldr": "The paper introduces a fairness framework using happiness as a measure of utility, implemented through efficient linear programming.", "motivation": "The aim is to offer a human-centered yet mathematically rigorous approach to fairness in decision outcomes.", "method": "The fairness is framed mathematically using a linear programming approach to compute optimal fair strategies.", "result": "The proposed method unifies and extends existing fairness definitions, proving to be practical and efficient in diverse scenarios.", "conclusion": "The paper provides an intuitive and scalable framework for fairness, enhancing human-centered computational strategies."}}
{"id": "2511.01059", "pdf": "https://arxiv.org/pdf/2511.01059", "abs": "https://arxiv.org/abs/2511.01059", "authors": ["Hailong Yin", "Bin Zhu", "Jingjing Chen", "Chong-Wah Ngo"], "title": "Efficient Test-Time Retrieval Augmented Generation", "categories": ["cs.AI"], "comment": null, "summary": "Although Large Language Models (LLMs) demonstrate significant capabilities,\ntheir reliance on parametric knowledge often leads to inaccuracies. Retrieval\nAugmented Generation (RAG) mitigates this by incorporating external knowledge,\nbut these methods may introduce irrelevant retrieved documents, leading to\ninaccurate responses. While the integration methods filter out incorrect\nanswers from multiple responses, but lack external knowledge like RAG methods,\nand their high costs require balancing overhead with performance gains. To\naddress these issues, we propose an Efficient Test-Time Retrieval-Augmented\nGeneration Framework named ET2RAG to improve the performance of LLMs while\nmaintaining efficiency. Specifically, ET2RAG is a training-free method, that\nfirst retrieves the most relevant documents and augments the LLMs to\nefficiently generate diverse candidate responses by managing response length.\nThen we compute the similarity of candidate responses and employ a majority\nvoting mechanism to select the most suitable response as the final output. In\nparticular, we discover that partial generation is sufficient to capture the\nkey information necessary for consensus calculation, allowing us to effectively\nperform majority voting without the need for fully generated responses. Thus,\nwe can reach a balance between computational cost and performance by managing\nthe response length for the number of retrieved documents for majority voting.\nExperimental results demonstrate that ET2RAG significantly enhances performance\nacross three tasks, including open-domain question answering, recipe generation\nand image captioning.", "AI": {"tldr": "Proposed ET2RAG framework improves LLM performance with efficient Test-Time Retrieval-Augmented Generation through relevant document retrieval and response optimization.", "motivation": "Address inaccuracies in LLM responses due to reliance on parametric knowledge and optimize retrieval-augmented methods for high performance and efficiency.", "method": "ET2RAG employs retrieval of relevant documents, generates candidate responses by managing length, computes similarity, and applies majority voting to select the final response.", "result": "ET2RAG significantly improves performance in open-domain question answering, recipe generation, and image captioning tasks.", "conclusion": "ET2RAG provides an efficient, training-free way to improve LLM output quality without compromising computational costs, striking a balance between accuracy and efficiency."}}
{"id": "2511.00854", "pdf": "https://arxiv.org/pdf/2511.00854", "abs": "https://arxiv.org/abs/2511.00854", "authors": ["Chong Lyu", "Lin Li", "Shiqing Wu", "Jingling Yuan"], "title": "TriCon-Fair: Triplet Contrastive Learning for Mitigating Social Bias in Pre-trained Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The increasing utilization of large language models raises significant\nconcerns about the propagation of social biases, which may result in harmful\nand unfair outcomes. However, existing debiasing methods treat the biased and\nunbiased samples independently, thus ignoring their mutual relationship. This\noversight enables a hidden negative-positive coupling, where improvements for\none group inadvertently compromise the other, allowing residual social bias to\npersist. In this paper, we introduce TriCon-Fair, a contrastive learning\nframework that employs a decoupled loss that combines triplet and language\nmodeling terms to eliminate positive-negative coupling. Our TriCon-Fair assigns\neach anchor an explicitly biased negative and an unbiased positive, decoupling\nthe push-pull dynamics and avoiding positive-negative coupling, and jointly\noptimizes a language modeling (LM) objective to preserve general capability.\nExperimental results demonstrate that TriCon-Fair reduces discriminatory output\nbeyond existing debiasing baselines while maintaining strong downstream\nperformance. This suggests that our proposed TriCon-Fair offers a practical and\nethical solution for sensitive NLP applications.", "AI": {"tldr": "Large language models can propagate social biases; this paper proposes TriCon-Fair, a contrastive learning framework to reduce bias while preserving language modeling capabilities.", "motivation": "Address the limitations of existing debiasing methods that inadvertently cause residual social bias due to biased and unbiased samples being treated independently, leading to coupling issues.", "method": "Propose TriCon-Fair, integrating triplet loss and language modeling objectives to decouple biased-negative and unbiased-positive sample relationships while maintaining general language model capabilities.", "result": "Experimental results show TriCon-Fair successfully reduces discriminatory outputs more effectively than existing baselines while preserving downstream application performance.", "conclusion": "TriCon-Fair provides a novel, ethical, and practical solution to mitigate social bias in NLP models without compromising their performance."}}
{"id": "2511.01348", "pdf": "https://arxiv.org/pdf/2511.01348", "abs": "https://arxiv.org/abs/2511.01348", "authors": ["Robin Gr\u00f6pler", "Steffen Klepke", "Jack Johns", "Andreas Dreschinski", "Klaus Schmid", "Benedikt Dornauer", "Eray T\u00fcz\u00fcn", "Joost Noppen", "Mohammad Reza Mousavi", "Yongjian Tang", "Johannes Viehmann", "Selin \u015eirin Aslang\u00fcl", "Beum Seuk Lee", "Adam Ziolkowski", "Eric Zie"], "title": "The Future of Generative AI in Software Engineering: A Vision from Industry and Academia in the European GENIUS Project", "categories": ["cs.SE", "cs.AI"], "comment": "Submitted to 2nd IEEE/ACM International Conference on AI-powered\n  Software (AIware 2025)", "summary": "Generative AI (GenAI) has recently emerged as a groundbreaking force in\nSoftware Engineering, capable of generating code, suggesting fixes, and\nsupporting quality assurance. While its use in coding tasks shows considerable\npromise, applying GenAI across the entire Software Development Life Cycle\n(SDLC) has not yet been fully explored. Critical uncertainties in areas such as\nreliability, accountability, security, and data privacy demand deeper\ninvestigation and coordinated action. The GENIUS project, comprising over 30\nEuropean industrial and academic partners, aims to address these challenges by\nadvancing AI integration across all SDLC phases. It focuses on GenAI's\npotential, the development of innovative tools, and emerging research\nchallenges, actively shaping the future of software engineering. This vision\npaper presents a shared perspective on the future of GenAI-based software\nengineering, grounded in cross-sector dialogue and experience within the GENIUS\nconsortium, supported by an exploratory literature review. The paper explores\nfour central elements: (1) a structured overview of current challenges in GenAI\nadoption across the SDLC; (2) a forward-looking vision outlining key\ntechnological and methodological advances expected over the next five years;\n(3) anticipated shifts in the roles and required skill sets of software\nprofessionals; and (4) the contribution of GENIUS in realizing this\ntransformation through practical tools and industrial validation. By aligning\ntechnical innovation with business relevance, this paper aims to inform both\nresearch agendas and industrial strategies, providing a foundation for\nreliable, scalable, and industry-ready GenAI solutions for software engineering\nteams.", "AI": {"tldr": "This paper discusses Generative AI's potential in enhancing software engineering across its lifecycle, addressing challenges like reliability, security, and privacy. The GENIUS project aims to advance tools and strategies to integrate GenAI effectively, offering insights into future technological needs and shifts in professional roles.", "motivation": "To explore and overcome critical challenges in adopting Generative AI across the entire Software Development Life Cycle (SDLC), including reliability, security, and accountability, and to shape its future in software engineering.", "method": "The paper utilizes cross-sector dialogue within the GENIUS consortium and an exploratory literature review to identify challenges, propose a future vision for GenAI adoption, and highlight practical solutions through tools and industrial validation.", "result": "It provides a structured assessment of challenges, predicts advances in technologies and methodologies over the next five years, anticipates changes in software professionals' roles, and outlines GENIUS's contributions to practical applications and scalability.", "conclusion": "Generative AI holds transformative potential for software engineering teams, but success will require aligning technical innovations with business needs, emphasizing reliability and scalability. GENIUS aims to lead this transformation through research and industrial collaboration."}}
{"id": "2511.01219", "pdf": "https://arxiv.org/pdf/2511.01219", "abs": "https://arxiv.org/abs/2511.01219", "authors": ["Muhua Zhang", "Lei Ma", "Ying Wu", "Kai Shen", "Deqing Huang", "Henry Leung"], "title": "Tackling the Kidnapped Robot Problem via Sparse Feasible Hypothesis Sampling and Reliable Batched Multi-Stage Inference", "categories": ["cs.RO"], "comment": "10 pages, 8 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "This paper addresses the Kidnapped Robot Problem (KRP), a core localization\nchallenge of relocalizing a robot in a known map without prior pose estimate\nwhen localization loss or at SLAM initialization. For this purpose, a passive\n2-D global relocalization framework is proposed. It estimates the global pose\nefficiently and reliably from a single LiDAR scan and an occupancy grid map\nwhile the robot remains stationary, thereby enhancing the long-term autonomy of\nmobile robots. The proposed framework casts global relocalization as a\nnon-convex problem and solves it via the multi-hypothesis scheme with batched\nmulti-stage inference and early termination, balancing completeness and\nefficiency. The Rapidly-exploring Random Tree (RRT), under traversability\nconstraints, asymptotically covers the reachable space to generate sparse,\nuniformly distributed feasible positional hypotheses, fundamentally reducing\nthe sampling space. The hypotheses are preliminarily ordered by the proposed\nScan Mean Absolute Difference (SMAD), a coarse beam-error level metric that\nfacilitates the early termination by prioritizing high-likelihood candidates.\nThe SMAD computation is optimized for non-panoramic scans. And the\nTranslation-Affinity Scan-to-Map Alignment Metric (TAM) is proposed for\nreliable orientation selection at hypothesized positions and accurate final\npose evaluation to mitigate degradation in conventional likelihood-field\nmetrics under translational uncertainty induced by sparse hypotheses, as well\nas non-panoramic LiDAR scan and environmental changes. Real-world experiments\non a resource-constrained mobile robot with non-panoramic LiDAR scan\ndemonstrate that the proposed framework outperforms existing methods in both\nglobal relocalization success rate and computational efficiency.", "AI": {"tldr": "This paper proposes a framework to solve the Kidnapped Robot Problem (KRP) using a LiDAR scan and occupancy grid map for global relocalization.", "motivation": "Address the challenge of relocalizing a robot in a known map without a prior pose estimate during localization failure or SLAM initialization.", "method": "A passive 2-D global relocalization framework achieved via multi-hypothesis scheme, batched multi-stage inference, RRT for sampling, SMAD for ranking hypotheses, and TAM for reliable pose evaluation.", "result": "Experimental results show improved success rate and computational efficiency in global relocalization using the proposed framework on a resource-constrained mobile robot.", "conclusion": "The framework successfully tackles the KRP, significantly enhancing mobile robot autonomy and reliability in real-world environments."}}
{"id": "2511.00101", "pdf": "https://arxiv.org/pdf/2511.00101", "abs": "https://arxiv.org/abs/2511.00101", "authors": ["Yuchen Zhang", "Hanyue Du", "Chun Cao", "Jingwei Xu"], "title": "Loquetier: A Virtualized Multi-LoRA Framework for Unified LLM Fine-tuning and Serving", "categories": ["cs.LG", "cs.AI"], "comment": "26 pages including 10 pages of main text, 6 figures, 39th Conference\n  on Neural Information Processing Systems (NeurIPS 2025)", "summary": "Low-Rank Adaptation (LoRA) has become a widely adopted parameter-efficient\nfine-tuning (PEFT) technique for adapting large language models (LLMs) to\ndownstream tasks. While prior work has explored strategies for integrating LLM\ntraining and serving, there still remains a gap in unifying fine-tuning and\ninference for LoRA-based models. We present Loquetier, a virtualized multi-LoRA\nframework that seamlessly integrates LoRA fine-tuning and serving within a\nsingle runtime. Loquetier introduces two key components: (1) a Virtualized\nModule that isolates PEFT-based modifications and supports multiple adapters on\na shared base model, and (2) an optimized computation flow with a kernel design\nthat merges fine-tuning and inference paths in forward propagation, enabling\nefficient batching and minimizing kernel invocation overhead. Extensive\nexperiments across three task settings show that Loquetier consistently\noutperforms existing baselines in both performance and flexibility, achieving\nup to $3.0\\times$ the throughput of the state-of-the-art co-serving system on\ninference-only tasks and $46.4\\times$ higher SLO attainment than PEFT on\nunified fine-tuning and inference tasks. The implementation of Loquetier is\npublicly available at https://github.com/NJUDeepEngine/Loquetier.", "AI": {"tldr": "This paper introduces Loquetier, a framework that combines fine-tuning and inference for LoRA-based models, optimizing computational efficiency and performance.", "motivation": "There exists a gap in unifying fine-tuning and inference for LoRA-based models, despite its widespread use and the importance of integrating LLM training and serving.", "method": "The authors propose Loquetier, a virtualized multi-LoRA framework with two components: a Virtualized Module for adapter isolation and kernel optimization for efficient computation during fine-tuning and inference.", "result": "Loquetier outperforms existing methods by achieving up to 3x throughput on inference-only tasks and 46.4x higher SLO attainment for unified fine-tuning and inference tasks.", "conclusion": "Loquetier offers a seamless integration of fine-tuning and inference for LoRA-based models, improving performance, flexibility, and efficiency in related tasks."}}
{"id": "2511.00269", "pdf": "https://arxiv.org/pdf/2511.00269", "abs": "https://arxiv.org/abs/2511.00269", "authors": ["Long Li", "Jiajia Li", "Dong Chen", "Lina Pu", "Haibo Yao", "Yanbo Huang"], "title": "FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate classification plays a pivotal role in smart agriculture, enabling\napplications such as crop monitoring, fruit recognition, and pest detection.\nHowever, conventional centralized training often requires large-scale data\ncollection, which raises privacy concerns, while standard federated learning\nstruggles with non-independent and identically distributed (non-IID) data and\nincurs high communication costs. To address these challenges, we propose a\nfederated learning framework that integrates a frozen Contrastive\nLanguage-Image Pre-training (CLIP) vision transformer (ViT) with a lightweight\ntransformer classifier. By leveraging the strong feature extraction capability\nof the pre-trained CLIP ViT, the framework avoids training large-scale models\nfrom scratch and restricts federated updates to a compact classifier, thereby\nreducing transmission overhead significantly. Furthermore, to mitigate\nperformance degradation caused by non-IID data distribution, a small subset\n(1%) of CLIP-extracted feature representations from all classes is shared\nacross clients. These shared features are non-reversible to raw images,\nensuring privacy preservation while aligning class representation across\nparticipants. Experimental results on agricultural classification tasks show\nthat the proposed method achieve 86.6% accuracy, which is more than 4 times\nhigher compared to baseline federated learning approaches. This demonstrates\nthe effectiveness and efficiency of combining vision-language model features\nwith federated learning for privacy-preserving and scalable agricultural\nintelligence.", "AI": {"tldr": "This paper proposes a federated learning framework using a frozen CLIP ViT model combined with a lightweight transformer classifier to address privacy concerns, reduce communication costs, and enhance accuracy in smart agriculture tasks.", "motivation": "The need for accurate classification in smart agriculture faces challenges such as privacy concerns with centralized training, high communication costs, and non-IID data in federated learning.", "method": "The framework integrates a frozen CLIP vision transformer for feature extraction with a lightweight transformer classifier. It includes sharing a small subset of non-reversible feature representations extracted by CLIP across clients to mitigate non-IID data issues while preserving privacy.", "result": "The proposed method achieved 86.6% accuracy on agricultural classification tasks, which is more than 4 times higher than baseline federated learning methods.", "conclusion": "The combination of vision-language pre-trained model features and federated learning offers an efficient, privacy-preserving, and effective solution for smart agriculture applications."}}
{"id": "2511.01137", "pdf": "https://arxiv.org/pdf/2511.01137", "abs": "https://arxiv.org/abs/2511.01137", "authors": ["Kathryn Lindsey", "Govind Menon"], "title": "Regularization Implies balancedness in the deep linear network", "categories": ["cs.LG", "math.AG", "math.DS", "stat.ML", "14L24, 37J15, 37C10, 68T07, 93B10, 53D20, 49J15, 37N40"], "comment": "18 pages, 3 figures", "summary": "We use geometric invariant theory (GIT) to study the deep linear network\n(DLN). The Kempf-Ness theorem is used to establish that the $L^2$ regularizer\nis minimized on the balanced manifold. This allows us to decompose the training\ndynamics into two distinct gradient flows: a regularizing flow on fibers and a\nlearning flow on the balanced manifold. We show that the regularizing flow is\nexactly solvable using the moment map.\n  This approach provides a common mathematical framework for balancedness in\ndeep learning and linear systems theory. We use this framework to interpret\nbalancedness in terms of model reduction and Bayesian principles.", "AI": {"tldr": "This paper analyzes deep linear networks using geometric invariant theory, establishing connections between regularization and balanced manifold dynamics.", "motivation": "To investigate deep linear network dynamics and establish a common mathematical framework connecting deep learning and linear systems theory.", "method": "Leveraging geometric invariant theory and the Kempf-Ness theorem, the L2 regularizer is linked to minimizing on balanced manifolds while decomposing training dynamics into distinct gradient flows.", "result": "The regularizing flow is solvable via the moment map, enabling the interpretation of balancedness principles.", "conclusion": "This framework bridges between deep learning concepts and principles like model reduction and Bayesian perspectives."}}
{"id": "2511.01149", "pdf": "https://arxiv.org/pdf/2511.01149", "abs": "https://arxiv.org/abs/2511.01149", "authors": ["Shuaidong Pan", "Di Wu"], "title": "Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "This paper addresses the limitations of a single agent in task decomposition\nand collaboration during complex task execution, and proposes a multi-agent\narchitecture for modular task decomposition and dynamic collaboration based on\nlarge language models. The method first converts natural language task\ndescriptions into unified semantic representations through a large language\nmodel. On this basis, a modular decomposition mechanism is introduced to break\ndown the overall goal into multiple hierarchical sub-tasks. Then, dynamic\nscheduling and routing mechanisms enable reasonable division of labor and\nrealtime collaboration among agents, allowing the system to adjust strategies\ncontinuously according to environmental feedback, thus maintaining efficiency\nand stability in complex tasks. Furthermore, a constraint parsing and global\nconsistency mechanism is designed to ensure coherent connections between\nsub-tasks and balanced workload, preventing performance degradation caused by\nredundant communication or uneven resource allocation. The experiments validate\nthe architecture across multiple dimensions, including task success rate,\ndecomposition efficiency, sub-task coverage, and collaboration balance. The\nresults show that the proposed method outperforms existing approaches in both\noverall performance and robustness, achieving a better balance between task\ncomplexity and communication overhead. In conclusion, this study demonstrates\nthe effectiveness and feasibility of language-driven task decomposition and\ndynamic collaboration in multi-agent systems, providing a systematic solution\nfor task execution in complex environments.", "AI": {"tldr": "The paper introduces a multi-agent system for solving complex tasks efficiently using language-driven task decomposition and dynamic collaboration.", "motivation": "To overcome the limitations of single-agent systems in handling complex tasks effectively.", "method": "The method involves converting natural language task descriptions into semantic representations, modular decomposition of tasks, dynamic scheduling, routing mechanisms, and a constraint parsing mechanism.", "result": "Experimental results indicate better performance, robustness, and balanced task execution compared to existing solutions.", "conclusion": "This study validates multi-agent architectures driven by language for enhanced task decomposition and collaboration in complex environments."}}
{"id": "2511.00879", "pdf": "https://arxiv.org/pdf/2511.00879", "abs": "https://arxiv.org/abs/2511.00879", "authors": ["Hyeon Hwang", "Yewon Cho", "Chanwoong Yoon", "Yein Park", "Minju Song", "Kyungjae Lee", "Gangwoo Kim", "Jaewoo Kang"], "title": "Assessing LLM Reasoning Steps via Principal Knowledge Grounding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "Step-by-step reasoning has become a standard approach for large language\nmodels (LLMs) to tackle complex tasks. While this paradigm has proven\neffective, it raises a fundamental question: How can we verify that an LLM's\nreasoning is accurately grounded in knowledge? To address this question, we\nintroduce a novel evaluation suite that systematically assesses the knowledge\ngrounding of intermediate reasoning. Our framework comprises three key\ncomponents. (1) Principal Knowledge Collection, a large-scale repository of\natomic knowledge essential for reasoning. Based on the collection, we propose\n(2) knowledge-grounded evaluation metrics designed to measure how well models\nrecall and apply prerequisite knowledge in reasoning. These metrics are\ncomputed by our (3) evaluator LLM, a lightweight model optimized for\ncost-effective and reliable metric computation. Our evaluation suite\ndemonstrates remarkable effectiveness in identifying missing or misapplied\nknowledge elements, providing crucial insights for uncovering fundamental\nreasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these\nmetrics can be integrated into preference optimization, showcasing further\napplications of knowledge-grounded evaluation.", "AI": {"tldr": "The paper introduces a framework to evaluate and improve LLMs' reasoning accuracy by grounding steps in essential knowledge.", "motivation": "To ensure step-by-step reasoning by LLMs is grounded in accurate and essential knowledge.", "method": "Developed an evaluation suite with three tools: a repository of atomic knowledge, knowledge-grounded metrics for reasoning evaluation, and an evaluator LLM for reliable computations.", "result": "The framework effectively identifies flaws in reasoning, such as missing or misapplied knowledge, and aids LLM optimization.", "conclusion": "Knowledge-grounded evaluation significantly improves reasoning analysis and enhances LLM performance beyond just evaluation."}}
{"id": "2511.01395", "pdf": "https://arxiv.org/pdf/2511.01395", "abs": "https://arxiv.org/abs/2511.01395", "authors": ["Maimouna Tamah Diao", "Moustapha Awwalou Diouf", "Iyiola Emmanuel Olatunji", "Abdoul Kader Kabor\u00e9", "Gervais Mendy", "Jacques Klein", "Tegawend\u00e9 F. Bissyand\u00e9"], "title": "Characterizing Build Compromises Through Vulnerability Disclosure Analysis", "categories": ["cs.SE"], "comment": null, "summary": "The software build process transforms source code into deployable artifacts,\nrepresenting a critical yet vulnerable stage in software development. Build\ninfrastructure security poses unique challenges: the complexity of\nmulti-component systems (source code, dependencies, build tools), the\ndifficulty of detecting intrusions during compilation, and prevalent build\nnon-determinism that masks malicious modifications. Despite these risks, the\nsecurity community lacks a systematic understanding of build-specific attack\nvectors, hindering effective defense design.\n  This paper presents an empirically-derived taxonomy of attack vectors\ntargeting the build process, constructed through a large-scale CVE mining (of\n621 vulnerability disclosures from the NVD database). We categorize attack\nvectors by their injection points across the build pipeline, from source code\nmanipulation to compiler compromise. To validate our taxonomy, we analyzed 168\ndocumented software supply chain attacks, identifying 40 incidents specifically\ntargeting build phases. Our analysis reveals that 23.8\\% of supply chain\nattacks exploit build vulnerabilities, with dependency confusion and build\nscript injection representing the most prevalent vectors.\n  Dataset available at:\nhttps://anonymous.4open.science/r/Taxonomizing-Build-Attacks-8BB0.", "AI": {"tldr": "The paper explores vulnerabilities in the software build process by developing a taxonomy of attack vectors. It is based on extensive CVE mining and analysis of supply chain attacks, emphasizing build-related security risks.", "motivation": "To address the critical security challenges and gaps in understanding vulnerabilities specifically targeting the software build process, which is a vital stage in software development.", "method": "The authors created a taxonomy of build-specific attack vectors by mining 621 CVEs from the NVD database. They validated their findings by analyzing 168 software supply chain attacks, isolating incidents targeting the build phase.", "result": "Their analysis shows that 23.8% of supply chain attacks exploit build-related vulnerabilities, with dependency confusion and build script injection named as the most common vectors.", "conclusion": "The paper highlights the need for focused security measures against build process vulnerabilities and provides a dataset for further research in this domain."}}
{"id": "2511.01224", "pdf": "https://arxiv.org/pdf/2511.01224", "abs": "https://arxiv.org/abs/2511.01224", "authors": ["Chengmeng Li", "Yaxin Peng"], "title": "Embodiment Transfer Learning for Vision-Language-Action Models", "categories": ["cs.RO"], "comment": null, "summary": "Vision-language-action (VLA) models have significantly advanced robotic\nlearning, enabling training on large-scale, cross-embodiment data and\nfine-tuning for specific robots. However, state-of-the-art autoregressive VLAs\nstruggle with multi-robot collaboration. We introduce embodiment transfer\nlearning, denoted as ET-VLA, a novel framework for efficient and effective\ntransfer of pre-trained VLAs to multi-robot. ET-VLA's core is Synthetic\nContinued Pretraining (SCP), which uses synthetically generated data to warm up\nthe model for the new embodiment, bypassing the need for real human\ndemonstrations and reducing data collection costs. SCP enables the model to\nlearn correct actions and precise action token numbers. Following SCP, the\nmodel is fine-tuned on target embodiment data. To further enhance the model\nperformance on multi-embodiment, we present the Embodied Graph-of-Thought\ntechnique, a novel approach that formulates each sub-task as a node, that\nallows the VLA model to distinguish the functionalities and roles of each\nembodiment during task execution. Our work considers bimanual robots, a simple\nversion of multi-robot to verify our approaches. We validate the effectiveness\nof our method on both simulation benchmarks and real robots covering three\ndifferent bimanual embodiments. In particular, our proposed ET-VLA \\space can\noutperform OpenVLA on six real-world tasks over 53.2%. We will open-source all\ncodes to support the community in advancing VLA models for robot learning.", "AI": {"tldr": "The paper introduces ET-VLA, a new framework enhancing vision-language-action (VLA) models for multi-robot collaboration, using synthetic data to bypass costly human demonstrations.", "motivation": "State-of-the-art VLA models face challenges in multi-robot collaboration despite their successes in single-robot tasks. This research aims to address inefficiencies and high data collection costs for adapting these models to multi-robot environments.", "method": "The authors propose Synthetic Continued Pretraining (SCP) using synthetic data for initial model adaptation to multi-robot embodiments, combined with Embodied Graph-of-Thought to structure sub-tasks and roles during execution.", "result": "ET-VLA was validated on bimanual robots using three different embodiments, showing superior performance and achieving over 53.2% improvement compared to existing models like OpenVLA across six real-world tasks.", "conclusion": "ET-VLA successfully improves VLA models for multi-robot tasks by reducing data collection costs and enhancing performance. The research will be open-sourced to accelerate further developments in robotic learning."}}
{"id": "2511.00102", "pdf": "https://arxiv.org/pdf/2511.00102", "abs": "https://arxiv.org/abs/2511.00102", "authors": ["Vivan Doshi"], "title": "Automated Discovery of Conservation Laws via Hybrid Neural ODE-Transformers", "categories": ["cs.LG", "cs.AI"], "comment": "5th Math-AI Workshop - Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "The discovery of conservation laws is a cornerstone of scientific progress.\nHowever, identifying these invariants from observational data remains a\nsignificant challenge. We propose a hybrid framework to automate the discovery\nof conserved quantities from noisy trajectory data. Our approach integrates\nthree components: (1) a Neural Ordinary Differential Equation (Neural ODE) that\nlearns a continuous model of the system's dynamics, (2) a Transformer that\ngenerates symbolic candidate invariants conditioned on the learned vector\nfield, and (3) a symbolic-numeric verifier that provides a strong numerical\ncertificate for the validity of these candidates. We test our framework on\ncanonical physical systems and show that it significantly outperforms baselines\nthat operate directly on trajectory data. This work demonstrates the robustness\nof a decoupled learn-then-search approach for discovering mathematical\nprinciples from imperfect data.", "AI": {"tldr": "This paper introduces a hybrid framework to discover conserved quantities from noisy trajectory data using Neural ODEs, Transformers, and symbolic-numeric verification.", "motivation": "Identifying conserved quantities from observational data is crucial for scientific advancement but remains difficult, especially with noisy data.", "method": "The framework includes a Neural Ordinary Differential Equation to learn dynamics, a Transformer to propose symbolic invariants, and a symbolic-numeric verifier for validation.", "result": "Testing on physical systems shows this approach strongly outperforms baselines operating on trajectory data.", "conclusion": "A decoupled approach combining learning and searching can effectively discover mathematical principles from imperfect data."}}
{"id": "2511.00293", "pdf": "https://arxiv.org/pdf/2511.00293", "abs": "https://arxiv.org/abs/2511.00293", "authors": ["Hengjia Li", "Jianjin Xu", "Keli Cheng", "Lei Wang", "Ning Bi", "Boxi Wu", "Fernando De la Torre", "Deng Cai"], "title": "Multi-View Consistent Human Image Customization via In-Context Learning", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in personalized generative models demonstrate impressive\nresults in creating identity-consistent images of the same person under diverse\nsettings. Yet, we note that most methods cannot control the viewpoint of the\ngenerated image, nor generate consistent multiple views of the person. To\naddress this problem, we propose a lightweight adaptation method, PersonalView,\ncapable of enabling an existing model to acquire multi-view generation\ncapability with as few as 100 training samples. PersonalView consists of two\nkey components: First, we design a conditioning architecture to take advantage\nof the in-context learning ability of the pre-trained diffusion transformer.\nSecond, we preserve the original generative ability of the pretrained model\nwith a new Semantic Correspondence Alignment Loss. We evaluate the multi-view\nconsistency, text alignment, identity similarity, and visual quality of\nPersonalView and compare it to recent baselines with potential capability of\nmulti-view customization. PersonalView significantly outperforms baselines\ntrained on a large corpus of multi-view data with only 100 training samples.", "AI": {"tldr": "PersonalView is introduced as an adaptation method that enables pre-trained generative models to produce multi-view images of individuals with minimal training samples.", "motivation": "Existing generative methods struggle with generating consistent multi-view images of individuals. This paper aims to address the challenge of viewpoint control and consistency in generated images.", "method": "The paper introduces PersonalView, which uses a conditioning architecture for diffusion transformers and a Semantic Correspondence Alignment Loss to preserve original generative capabilities while enabling multi-view generation.", "result": "PersonalView achieves significant improvement in multi-view image consistency, text alignment, identity similarity, and visual quality compared to other methods, even with only 100 training samples.", "conclusion": "The proposed method, PersonalView, successfully enables lightweight multi-view generation for personalized images, outperforming existing methods while requiring far less training data."}}
{"id": "2511.01190", "pdf": "https://arxiv.org/pdf/2511.01190", "abs": "https://arxiv.org/abs/2511.01190", "authors": ["Lijia Yu", "Xiao-Shan Gao", "Lijun Zhang"], "title": "Analyzing the Power of Chain of Thought through Memorization Capabilities", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "It has been shown that the chain of thought (CoT) can enhance the power of\nlarge language models (LLMs) to solve certain mathematical reasoning problems.\nHowever, the capacity of CoT is still not fully explored. As an important\ninstance, the following basic question has not yet been answered: Does CoT\nexpand the capability of transformers across all reasoning tasks? We\ndemonstrate that reasoning with transformers is essentially a memorization\nproblem for reasoning datasets. Thus, examining the power of CoT across all\nreasoning tasks amounts to analyzing the memorization capabilities of CoT\ntransformers. In this paper, we give a complete description of the memorization\ncapabilities of fixed-precision transformers with or without CoT and give a\nnegative answer to the above-mentioned question. Precisely, we first give\nnecessary and sufficient conditions for fixed-precision transformers with and\nwithout CoT to memorize a finite reasoning dataset and show that these two\nconditions do not imply each other. Then, we give lower and upper bounds for\nthe number of parameters needed for transformers with or without CoT to\nmemorize a finite reasoning dataset with $N$ elements, which are\n$\\overline{\\Theta}(N)$ in all cases. This implies that there exist reasoning\ntasks for which CoT does not enhance the reasoning power of transformers,\nleading to a negative answer to the above-mentioned question. Finally, we give\nthe first results on memorizing infinite reasoning datasets by CoT transformers\nand show that some simple infinite datasets cannot be memorized by transformers\nwith or without CoT.", "AI": {"tldr": "The paper examines whether Chain-of-Thought (CoT) reasoning expands the capabilities of transformers for all reasoning tasks. It concludes that some reasoning problems cannot benefit from CoT due to inherent limitations in memorization ability.", "motivation": "The authors aim to explore the full potential of CoT in enhancing reasoning capabilities of large language models (LLMs), particularly addressing whether CoT improves transformers in all reasoning scenarios.", "method": "The authors analyze fixed-precision transformers with and without CoT by deriving necessary and sufficient conditions for memorizing reasoning datasets, and provide bounds on parameters required for memorization. They also extend their study to infinite reasoning datasets.", "result": "The study concludes that CoT does not universally enhance reasoning capabilities and establishes bounds for memorization tasks. Furthermore, certain infinite reasoning datasets are shown to be unmemorizable via transformers, with or without CoT.", "conclusion": "CoT is not universally effective in improving reasoning power in transformers, as some reasoning tasks, both finite and infinite, resist enhancement by CoT paradigms."}}
{"id": "2511.01170", "pdf": "https://arxiv.org/pdf/2511.01170", "abs": "https://arxiv.org/abs/2511.01170", "authors": ["Ruofan Zhang", "Bin Xia", "Zhen Cheng", "Cairen Jian", "Minglun Yang", "Ngai Wong", "Yuan Cheng"], "title": "DART: Difficulty-Adaptive Reasoning Truncation for Efficient Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Adaptive reasoning is essential for aligning the computational effort of\nlarge language models (LLMs) with the intrinsic difficulty of problems. Current\nchain-of-thought methods boost reasoning ability but indiscriminately generate\nlong explanations, leading to evident inefficiency. However, existing\nreinforcement learning approaches to adaptive thinking remain unstable and\nheavily reward-dependent. Here we propose \\textbf{DART}, a supervised\n\\textbf{D}ifficulty-\\textbf{A}daptive \\textbf{R}easoning \\textbf{T}runcation\nframework that adjusts thinking length according to problem difficulty. By\ndistilling concise reasoning patterns from stronger models, interpolating them\ninto a continuum of reasoning styles, and curating optimal training data that\nbalances correctness and compactness, DART learns when to ``stop thinking''.\nAcross multiple mathematical benchmarks, experimental results demonstrate its\nremarkable efficiency while preserving or improving accuracy, achieving a\nsignificant 81.2\\% reasoning truncation (DeepSeek-R1-Distill-Qwen-7B on GSM8K\ndataset) with 5.33$\\times$ computational acceleration. DART provides a stable\nand general paradigm for efficient reasoning, advancing the development of\nadaptive intelligence in LLMs.", "AI": {"tldr": "This paper introduces the DART framework to optimize the reasoning capability of large language models (LLMs) by truncating unnecessary computation based on problem difficulty, resulting in efficient and adaptive performance.", "motivation": "The motivation is to tackle inefficiencies in current chain-of-thought reasoning methods for LLMs, which generate long explanations indiscriminately, while reinforcement learning approaches remain unstable and overly reliant on rewards.", "method": "DART employs supervised learning to create a difficulty-adaptive reasoning truncation framework. It uses distilled concise reasoning patterns from stronger models, interpolates varying reasoning styles, and curates effective training data that balances correctness and compactness.", "result": "The DART framework improves computational efficiency by achieving an 81.2% reduction in reasoning steps and a 5.33x speedup in computational performance. This is achieved while maintaining or improving accuracy on mathematical reasoning benchmarks.", "conclusion": "DART proves to be a stable and general framework for adaptive reasoning, significantly increasing efficiency and performance in LLMs, and represents a step forward in developing adaptive intelligence."}}
{"id": "2511.00903", "pdf": "https://arxiv.org/pdf/2511.00903", "abs": "https://arxiv.org/abs/2511.00903", "authors": ["Ahmed Masry", "Megh Thakkar", "Patrice Bechard", "Sathwik Tejaswi Madhusudhan", "Rabiul Awal", "Shambhavi Mishra", "Akshay Kalkunte Suresh", "Srivatsava Daruru", "Enamul Hoque", "Spandana Gella", "Torsten Scholak", "Sai Rajeswar"], "title": "ColMate: Contrastive Late Interaction and Masked Text for Multimodal Document Retrieval", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-augmented generation has proven practical when models require\nspecialized knowledge or access to the latest data. However, existing methods\nfor multimodal document retrieval often replicate techniques developed for\ntext-only retrieval, whether in how they encode documents, define training\nobjectives, or compute similarity scores. To address these limitations, we\npresent ColMate, a document retrieval model that bridges the gap between\nmultimodal representation learning and document retrieval. ColMate utilizes a\nnovel OCR-based pretraining objective, a self-supervised masked contrastive\nlearning objective, and a late interaction scoring mechanism more relevant to\nmultimodal document structures and visual characteristics. ColMate obtains\n3.61% improvements over existing retrieval models on the ViDoRe V2 benchmark,\ndemonstrating stronger generalization to out-of-domain benchmarks.", "AI": {"tldr": "ColMate enhances multimodal document retrieval using OCR-based pretraining, masked contrastive learning, and tailored scoring mechanisms, achieving notable improvements over existing models.", "motivation": "Existing methods for multimodal document retrieval often rely on text-only retrieval approaches, limiting performance due to inadequate handling of multimodal structures and characteristics.", "method": "ColMate integrates an OCR-based pretraining objective, a self-supervised masked contrastive learning objective, and a late interaction scoring mechanism specifically designed for multimodal document structures.", "result": "The ColMate model improves retrieval accuracy by 3.61% on the ViDoRe V2 benchmark and generalizes effectively to out-of-domain benchmarks.", "conclusion": "ColMate bridges the gap between multimodal representation learning and document retrieval, outperforming current models and providing stronger generalization on benchmarks."}}
{"id": "2511.01417", "pdf": "https://arxiv.org/pdf/2511.01417", "abs": "https://arxiv.org/abs/2511.01417", "authors": ["Bassel Rafie", "Christian Schindler", "Andreas Rausch"], "title": "VeriODD: From YAML to SMT-LIB - Automating Verification of Operational Design Domains", "categories": ["cs.SE"], "comment": null, "summary": "Operational Design Domains (ODDs) define the conditions under which an\nAutomated Driving System (ADS) is allowed to operate, while Current Operational\nDomains (CODs) capture the actual runtime situation. Ensuring that a COD\ninstance lies within the ODD is a crucial step in safety assurance. Today, ODD\nand COD specifications are frequently expressed in YAML to remain accessible\nfor stakeholders, but such descriptions are not directly suitable for\nsolver-based verification. Manual translation into formal languages such as\nSMT-LIB is slow and error-prone. We present VeriODD, a tool that automates this\ntranslation. VeriODD uses ANTLR-based compiler technology to transform\nYAML-based ODD/COD specifications into both human-readable propositional logic,\nfor lightweight review on a simple basis, and solver-ready SMT-LIB. The tool\nintegrates with SMT solvers such as Z3 to provide automated consistency checks\nof ODD specifications and verification of COD conformance. A graphical user\ninterface supports editing specifications, inspecting generated formulas, and\nperforming verification with a single click. VeriODD thereby closes the gap\nbetween stakeholder-friendly ODD/COD notations and formal verification,\nenabling scalable and automated assurance of operational boundaries in\nautonomous driving. Video demonstration: https://youtu.be/odRacNoL_Pk Tool\navailable at: https://github.com/BasselRafie/VeriODD", "AI": {"tldr": "VeriODD is a tool for translating YAML-based ODD/COD specifications into solver-ready SMT-LIB and human-readable logic for automated verification in autonomous driving.", "motivation": "The paper aims to address the challenge of automating the verification of Operational Design Domains (ODDs) and Current Operational Domains (CODs) specifications, which are often expressed in YAML and not directly compatible with formal verification systems.", "method": "The tool, VeriODD, uses ANTLR-based compiler technology to transform YAML specifications into both propositional logic and SMT-LIB, enabling integration with SMT solvers for automated consistency checks and verification. A graphical user interface further supports the ease of use.", "result": "VeriODD bridges the gap between YAML-based stakeholder-friendly descriptions and their solver-ready counterparts, enabling scalable, efficient verification of operational boundaries in autonomous driving systems.", "conclusion": "The tool promotes automated safety assurance in autonomous driving by providing an efficient framework for verifying ODD/COD specifications, ensuring runtime safety compliance effectively."}}
{"id": "2511.01232", "pdf": "https://arxiv.org/pdf/2511.01232", "abs": "https://arxiv.org/abs/2511.01232", "authors": ["Yu-Ting Lai", "Jacob Rosen", "Yasamin Foroutani", "Ji Ma", "Wen-Cheng Wu", "Jean-Pierre Hubschman", "Tsu-Chin Tsao"], "title": "High-Precision Surgical Robotic System for Intraocular Procedures", "categories": ["cs.RO"], "comment": null, "summary": "Despite the extensive demonstration of robotic systems for both cataract and\nvitreoretinal procedures, existing technologies or mechanisms still possess\ninsufficient accuracy, precision, and degrees of freedom for instrument\nmanipulation or potentially automated tool exchange during surgical procedures.\nA new robotic system that focuses on improving tooltip accuracy, tracking\nperformance, and smooth instrument exchange mechanism is therefore designed and\nmanufactured. Its tooltip accuracy, precision, and mechanical capability of\nmaintaining small incision through remote center of motion were externally\nevaluated using an optical coherence tomography (OCT) system. Through robot\ncalibration and precise coordinate registration, the accuracy of tooltip\npositioning was measured to be 0.053$\\pm$0.031 mm, and the overall performance\nwas demonstrated on an OCT-guided automated cataract lens extraction procedure\nwith deep learning-based pre-operative anatomical modeling and real-time\nsupervision.", "AI": {"tldr": "A robotic system was developed to enhance surgical instrument precision and improve automated tool handling, showing promising performance in cataract lens extraction.", "motivation": "Existing surgical robotic systems lack sufficient accuracy, precision, and flexibility for refined procedures and tool exchange.", "method": "The robotic system was evaluated with optical coherence tomography to measure tooltip accuracy and precision during automated cataract lens extraction using deep learning-based anatomical modeling.", "result": "Tooltip positioning accuracy was achieved at 0.053\u00b10.031 mm, validated through OCT-guided procedures, demonstrating high precision.", "conclusion": "This innovative robotic system improves tooltip accuracy, tracking performance, and automated mechanisms, benefiting complex surgeries like cataract lens removal."}}
{"id": "2511.00108", "pdf": "https://arxiv.org/pdf/2511.00108", "abs": "https://arxiv.org/abs/2511.00108", "authors": ["Yi Zhang", "Che Liu", "Xiancong Ren", "Hanchu Ni", "Shuai Zhang", "Zeyuan Ding", "Jiayu Hu", "Hanzhe Shan", "Zhenwei Niu", "Zhaoyang Liu", "Yue Zhao", "Junbo Qi", "Qinfan Zhang", "Dengjie Li", "Yidong Wang", "Jiachen Luo", "Yong Dai", "Jian Tang", "Xiaozhu Ju"], "title": "Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "This report presents Pelican-VL 1.0, a new family of open-source embodied\nbrain models with parameter scales ranging from 7 billion to 72 billion. Our\nexplicit mission is clearly stated as: To embed powerful intelligence into\nvarious embodiments. Pelican-VL 1.0 is currently the largest-scale open-source\nembodied multimodal brain model. Its core advantage lies in the in-depth\nintegration of data power and intelligent adaptive learning mechanisms.\nSpecifically, metaloop distilled a high-quality dataset from a raw dataset\ncontaining 4+ billion tokens. Pelican-VL 1.0 is trained on a large-scale\ncluster of 1000+ A800 GPUs, consuming over 50k+ A800 GPU-hours per checkpoint.\nThis translates to a 20.3% performance uplift from its base model and\noutperforms 100B-level open-source counterparts by 10.6%, placing it on par\nwith leading proprietary systems on well-known embodied benchmarks. We\nestablish a novel framework, DPPO (Deliberate Practice Policy Optimization),\ninspired by human metacognition to train Pelican-VL 1.0. We operationalize this\nas a metaloop that teaches the AI to practice deliberately, which is a\nRL-Refine-Diagnose-SFT loop.", "AI": {"tldr": "Pelican-VL 1.0 is an open-source, multimodal embodied brain model scaling up to 72 billion parameters, optimized with the novel DPPO framework and achieving leading benchmarks.", "motivation": "To embed powerful intelligence into various embodiments and create a groundbreaking embodied multimodal brain model.", "method": "The methodology includes distilling high-quality data, leveraging over 1000 A800 GPUs for training, and employing the DPPO framework, which includes a metacognition-inspired RL-Refine-Diagnose-SFT loop.", "result": "Pelican-VL 1.0 achieves a 20.3% performance uplift from its base model, surpasses 100B-scale open-source counterparts by 10.6%, and matches leading proprietary systems on embodied benchmarks.", "conclusion": "Pelican-VL 1.0 demonstrates the effectiveness of combining large-scale training with a deliberate practice approach, setting a new benchmark for embodied brain models."}}
{"id": "2511.00328", "pdf": "https://arxiv.org/pdf/2511.00328", "abs": "https://arxiv.org/abs/2511.00328", "authors": ["Isai Daniel Chac\u00f3n", "Paola Ruiz Puentes", "Jillian Pearse", "Pablo Arbel\u00e1ez"], "title": "Towards Automated Petrography", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Petrography is a branch of geology that analyzes the mineralogical\ncomposition of rocks from microscopical thin section samples. It is essential\nfor understanding rock properties across geology, archaeology, engineering,\nmineral exploration, and the oil industry. However, petrography is a\nlabor-intensive task requiring experts to conduct detailed visual examinations\nof thin section samples through optical polarization microscopes, thus\nhampering scalability and highlighting the need for automated techniques. To\naddress this challenge, we introduce the Large-scale Imaging and Thin section\nOptical-polarization Set (LITHOS), the largest and most diverse publicly\navailable experimental framework for automated petrography. LITHOS includes\n211,604 high-resolution RGB patches of polarized light and 105,802\nexpert-annotated grains across 25 mineral categories. Each annotation consists\nof the mineral class, spatial coordinates, and expert-defined major and minor\naxes represented as intersecting vector paths, capturing grain geometry and\norientation. We evaluate multiple deep learning techniques for mineral\nclassification in LITHOS and propose a dual-encoder transformer architecture\nthat integrates both polarization modalities as a strong baseline for future\nreference. Our method consistently outperforms single-polarization models,\ndemonstrating the value of polarization synergy in mineral classification. We\nhave made the LITHOS Benchmark publicly available, comprising our dataset,\ncode, and pretrained models, to foster reproducibility and further research in\nautomated petrographic analysis.", "AI": {"tldr": "The paper introduces LITHOS, a large-scale experimental framework for automated petrography, incorporating deep learning techniques to improve mineral classification.", "motivation": "Petrography is critical in multiple fields but is labor-intensive and lacks scalability, which necessitates automated techniques.", "method": "Developed the LITHOS dataset, proposed a dual-encoder transformer architecture leveraging polarization modalities, and benchmarked deep learning techniques for mineral classification.", "result": "LITHOS dataset contains over 211,604 varied RGB patches and 105,802 expert-annotated grains. The dual-encoder transformer outperforms single-polarization models in mineral classification.", "conclusion": "LITHOS offers a robust experimental foundation for advancing automated petrography with its publicly available dataset, code, and pretrained models."}}
{"id": "2511.01234", "pdf": "https://arxiv.org/pdf/2511.01234", "abs": "https://arxiv.org/abs/2511.01234", "authors": ["Min Gan", "Guang-Yong Chen", "Yang Yi", "Lin Yang"], "title": "A Saddle Point Remedy: Power of Variable Elimination in Non-convex Optimization", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "The proliferation of saddle points, rather than poor local minima, is\nincreasingly understood to be a primary obstacle in large-scale non-convex\noptimization for machine learning. Variable elimination algorithms, like\nVariable Projection (VarPro), have long been observed to exhibit superior\nconvergence and robustness in practice, yet a principled understanding of why\nthey so effectively navigate these complex energy landscapes has remained\nelusive. In this work, we provide a rigorous geometric explanation by comparing\nthe optimization landscapes of the original and reduced formulations. Through a\nrigorous analysis based on Hessian inertia and the Schur complement, we prove\nthat variable elimination fundamentally reshapes the critical point structure\nof the objective function, revealing that local maxima in the reduced landscape\nare created from, and correspond directly to, saddle points in the original\nformulation. Our findings are illustrated on the canonical problem of\nnon-convex matrix factorization, visualized directly on two-parameter neural\nnetworks, and finally validated in training deep Residual Networks, where our\napproach yields dramatic improvements in stability and convergence to superior\nminima. This work goes beyond explaining an existing method; it establishes\nlandscape simplification via saddle point transformation as a powerful\nprinciple that can guide the design of a new generation of more robust and\nefficient optimization algorithms.", "AI": {"tldr": "This paper introduces a principled geometric explanation for why variable elimination methods, like Variable Projection (VarPro), efficiently navigate non-convex optimization landscapes, emphasizing saddle point transformation as a core mechanism.", "motivation": "To address the challenge presented by the proliferation of saddle points in large-scale non-convex optimization for machine learning, which obstructs convergence and stability.", "method": "Using Hessian inertia and the Schur complement, the paper analyzes the critical point structure of optimization landscapes. The study includes theoretical insights, visualization, and practical validation on deep Residual Networks.", "result": "Variable elimination fundamentally transforms saddle points into local maxima in the reduced optimization landscape, leading to improved stability and superior convergence in experiments, including deep neural network training.", "conclusion": "The study positions landscape simplification and saddle point transformation as foundational principles for developing robust and efficient optimization algorithms."}}
{"id": "2511.01182", "pdf": "https://arxiv.org/pdf/2511.01182", "abs": "https://arxiv.org/abs/2511.01182", "authors": ["Cuong Van Duc", "Thai Tran Quoc", "Minh Nguyen Dinh Tuan", "Tam Vu Duc", "Son Nguyen Van", "Hanh Nguyen Thi"], "title": "MiRAGE: Misconception Detection with Retrieval-Guided Multi-Stage Reasoning and Ensemble Fusion", "categories": ["cs.AI"], "comment": null, "summary": "Detecting student misconceptions in open-ended responses is a longstanding\nchallenge, demanding semantic precision and logical reasoning. We propose\nMiRAGE - Misconception Detection with Retrieval-Guided Multi-Stage Reasoning\nand Ensemble Fusion, a novel framework for automated misconception detection in\nmathematics. MiRAGE operates in three stages: (1) a Retrieval module narrows a\nlarge candidate pool to a semantically relevant subset; (2) a Reasoning module\nemploys chain-of-thought generation to expose logical inconsistencies in\nstudent solutions; and (3) a Reranking module refines predictions by aligning\nthem with the reasoning. These components are unified through an\nensemble-fusion strategy that enhances robustness and interpretability. On\nmathematics datasets, MiRAGE achieves Mean Average Precision scores of\n0.82/0.92/0.93 at levels 1/3/5, consistently outperforming individual modules.\nBy coupling retrieval guidance with multi-stage reasoning, MiRAGE reduces\ndependence on large-scale language models while delivering a scalable and\neffective solution for educational assessment.", "AI": {"tldr": "MiRAGE is a framework for detecting misconceptions in students' math solutions using retrieval, reasoning, and reranking approaches, achieving high precision scores.", "motivation": "Address the challenge of detecting misconceptions in open-ended student responses requiring semantic precision and logical reasoning.", "method": "Three-stage framework: retrieval module for narrowing down responses, reasoning module for logical assessment, and reranking module for refined predictions using ensemble fusion.", "result": "MiRAGE achieves high Mean Average Precision scores of 0.82/0.92/0.93 at different levels on mathematics datasets, outperforming individual components.", "conclusion": "MiRAGE offers a scalable and effective tool for educational assessment by reducing reliance on large language models while improving interpretability and accuracy."}}
{"id": "2511.00924", "pdf": "https://arxiv.org/pdf/2511.00924", "abs": "https://arxiv.org/abs/2511.00924", "authors": ["Jianzhou Yao", "Shunchang Liu", "Guillaume Drui", "Rikard Pettersson", "Alessandro Blasimme", "Sara Kijewski"], "title": "The Biased Oracle: Assessing LLMs' Understandability and Empathy in Medical Diagnoses", "categories": ["cs.CL"], "comment": "Accepted by NeurIPS 2025 GenAI4Health Workshop", "summary": "Large language models (LLMs) show promise for supporting clinicians in\ndiagnostic communication by generating explanations and guidance for patients.\nYet their ability to produce outputs that are both understandable and\nempathetic remains uncertain. We evaluate two leading LLMs on medical\ndiagnostic scenarios, assessing understandability using readability metrics as\na proxy and empathy through LLM-as-a-Judge ratings compared to human\nevaluations. The results indicate that LLMs adapt explanations to\nsocio-demographic variables and patient conditions. However, they also generate\noverly complex content and display biased affective empathy, leading to uneven\naccessibility and support. These patterns underscore the need for systematic\ncalibration to ensure equitable patient communication. The code and data are\nreleased: https://github.com/Jeffateth/Biased_Oracle", "AI": {"tldr": "The study evaluates two leading large language models (LLMs) in medical diagnostic scenarios for understandability and empathy, showing limitations in generating equitable and accessible communication.", "motivation": "To assess the effectiveness of LLMs in providing understandable and empathetic explanations for clinicians in medical diagnostic scenarios.", "method": "The research used readability metrics to evaluate understandability and LLM-as-a-Judge ratings to measure empathy, alongside human evaluations.", "result": "LLMs adjust explanations considering socio-demographics and patient conditions, but they tend to create overly complex content with biased empathy.", "conclusion": "Systematic calibration is necessary for LLMs to ensure equitable and accessible communication in medical scenarios."}}
{"id": "2511.01423", "pdf": "https://arxiv.org/pdf/2511.01423", "abs": "https://arxiv.org/abs/2511.01423", "authors": ["Ruidi He", "Yu Zhang", "Meng Zhang", "Andreas Rausch"], "title": "LLM-Assisted Tool for Joint Generation of Formulas and Functions in Rule-Based Verification of Map Transformations", "categories": ["cs.SE"], "comment": null, "summary": "High-definition map transformations are essential in autonomous driving\nsystems, enabling interoperability across tools. Ensuring their semantic\ncorrectness is challenging, since existing rule-based frameworks rely on\nmanually written formulas and domain-specific functions, limiting scalability.\n  In this paper, We present an LLM-assisted pipeline that jointly generates\nlogical formulas and corresponding executable predicates within a computational\nFOL framework, extending the map verifier in CommonRoad scenario designer with\nelevation support. The pipeline leverages prompt-based LLM generation to\nproduce grammar-compliant rules and predicates that integrate directly into the\nexisting system.\n  We implemented a prototype and evaluated it on synthetic bridge and slope\nscenarios. The results indicate reduced manual engineering effort while\npreserving correctness, demonstrating the feasibility of a scalable,\nsemi-automated human-in-the-loop approach to map-transformation verification.", "AI": {"tldr": "The paper proposes an LLM-assisted pipeline for automated generation of logical formulas and predicates to ensure semantic correctness in high-definition map transformations for autonomous driving.", "motivation": "To address the challenges of ensuring semantic correctness in high-definition map transformations and overcoming scalability limitations of rule-based frameworks.", "method": "The paper introduces a pipeline leveraging large language models (LLMs) to generate grammar-compliant rules and predicates that integrate into a computational First-Order Logic framework.", "result": "The prototype implementation successfully reduced manual engineering effort while maintaining correctness in evaluating synthetic bridge and slope scenarios.", "conclusion": "The approach demonstrates the feasibility of scalable, semi-automated human-in-the-loop verification for map transformations, enhancing interoperability and reducing reliance on manual methods."}}
{"id": "2511.01236", "pdf": "https://arxiv.org/pdf/2511.01236", "abs": "https://arxiv.org/abs/2511.01236", "authors": ["Junwen Zhang", "Changyue Liu", "Pengqi Fu", "Xiang Guo", "Ye Shi", "Xudong Liang", "Zhijian Wang", "Hanzhi Ma"], "title": "Don't Just Search, Understand: Semantic Path Planning Agent for Spherical Tensegrity Robots in Unknown Environments", "categories": ["cs.RO"], "comment": "8 pages, 5 figures", "summary": "Endowed with inherent dynamical properties that grant them remarkable\nruggedness and adaptability, spherical tensegrity robots stand as prototypical\nexamples of hybrid softrigid designs and excellent mobile platforms. However,\npath planning for these robots in unknown environments presents a significant\nchallenge, requiring a delicate balance between efficient exploration and\nrobust planning. Traditional path planners, which treat the environment as a\ngeometric grid, often suffer from redundant searches and are prone to failure\nin complex scenarios due to their lack of semantic understanding. To overcome\nthese limitations, we reframe path planning in unknown environments as a\nsemantic reasoning task. We introduce a Semantic Agent for Tensegrity robots\n(SATPlanner) driven by a Large Language Model (LLM). SATPlanner leverages\nhigh-level environmental comprehension to generate efficient and reliable\nplanning strategies.At the core of SATPlanner is an Adaptive Observation Window\nmechanism, inspired by the \"fast\" and \"slow\" thinking paradigms of LLMs. This\nmechanism dynamically adjusts the perceptual field of the agent: it narrows for\nrapid traversal of open spaces and expands to reason about complex obstacle\nconfigurations. This allows the agent to construct a semantic belief of the\nenvironment, enabling the search space to grow only linearly with the path\nlength (O(L)) while maintaining path quality. We extensively evaluate\nSATPlanner in 1,000 simulation trials, where it achieves a 100% success rate,\noutperforming other real-time planning algorithms. Critically, SATPlanner\nreduces the search space by 37.2% compared to the A* algorithm while achieving\ncomparable, near-optimal path lengths. Finally, the practical feasibility of\nSATPlanner is validated on a physical spherical tensegrity robot prototype.", "AI": {"tldr": "SATPlanner introduces a novel Large Language Model-driven approach for effective path planning in unknown environments for spherical tensegrity robots and demonstrates superior performance in simulation and real-world tests.", "motivation": "The paper addresses the challenge of efficient and robust path planning for spherical tensegrity robots in unknown environments, proposing an improved approach to overcome the limitations of traditional grid-based path planners.", "method": "The proposed solution, SATPlanner, integrates an Adaptive Observation Window mechanism inspired by \"fast\" and \"slow\" thinking paradigms, leveraging LLMs to enable semantic environment reasoning and efficient path planning.", "result": "SATPlanner demonstrated 100% success in 1,000 simulation trials, reduced search space by 37.2% compared to the A* algorithm while maintaining near-optimal path lengths, and proved practical feasibility in physical spherical tensegrity robot tests.", "conclusion": "SATPlanner effectively blends semantic reasoning enabled by LLMs with adaptive planning mechanisms to achieve optimal exploration and navigation strategies for spherical tensegrity robots in complex environments."}}
{"id": "2511.00113", "pdf": "https://arxiv.org/pdf/2511.00113", "abs": "https://arxiv.org/abs/2511.00113", "authors": ["Huseyin Goksu"], "title": "MeixnerNet: Adaptive and Robust Spectral Graph Neural Networks with Discrete Orthogonal Polynomials", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Spectral Graph Neural Networks (GNNs) have achieved state-of-the-art results\nby defining graph convolutions in the spectral domain. A common approach,\npopularized by ChebyNet, is to use polynomial filters based on continuous\northogonal polynomials (e.g., Chebyshev). This creates a theoretical\ndisconnect, as these continuous-domain filters are applied to inherently\ndiscrete graph structures. We hypothesize this mismatch can lead to suboptimal\nperformance and fragility to hyperparameter settings.\n  In this paper, we introduce MeixnerNet, a novel spectral GNN architecture\nthat employs discrete orthogonal polynomials -- specifically, the Meixner\npolynomials $M_k(x; \\beta, c)$. Our model makes the two key shape parameters of\nthe polynomial, beta and c, learnable, allowing the filter to adapt its\npolynomial basis to the specific spectral properties of a given graph. We\novercome the significant numerical instability of these polynomials by\nintroducing a novel stabilization technique that combines Laplacian scaling\nwith per-basis LayerNorm.\n  We demonstrate experimentally that MeixnerNet achieves\ncompetitive-to-superior performance against the strong ChebyNet baseline at the\noptimal K = 2 setting (winning on 2 out of 3 benchmarks). More critically, we\nshow that MeixnerNet is exceptionally robust to variations in the polynomial\ndegree K, a hyperparameter to which ChebyNet proves to be highly fragile,\ncollapsing in performance where MeixnerNet remains stable.", "AI": {"tldr": "This paper introduces MeixnerNet, a spectral Graph Neural Network using discrete orthogonal Meixner polynomials to address theoretical and practical mismatches in graph convolution approaches. It provides robust and adaptable performance compared to traditional methods.", "motivation": "The motivation stems from the disconnect between continuous orthogonal polynomial filters (used in traditional spectral graph approaches) and their application to inherently discrete graph structures, leading to suboptimal performance and hyperparameter fragility.", "method": "The proposed method employs Meixner polynomials with learnable shape parameters (beta and c), enabling adaptability to graph-specific spectral properties. A novel stabilization technique combining Laplacian scaling with per-basis LayerNorm is used to tackle numerical instability.", "result": "MeixnerNet achieves competitive-to-superior performance against ChebyNet on benchmarks and demonstrates robust performance despite variations in hyperparameter settings (specifically the polynomial degree K).", "conclusion": "MeixnerNet is a promising solution for addressing theoretical mismatches and performance fragility in spectral graph techniques, offering robust adaptability and improved results."}}
{"id": "2511.00335", "pdf": "https://arxiv.org/pdf/2511.00335", "abs": "https://arxiv.org/abs/2511.00335", "authors": ["Weidong Zhang", "Pak Lun Kevin Ding", "Huan Liu"], "title": "Beyond ImageNet: Understanding Cross-Dataset Robustness of Lightweight Vision Models", "categories": ["cs.CV", "cs.LG"], "comment": "10 pages, 5 tables, 1 figure, 3 equations, 11 mobile models, 7\n  datasets", "summary": "Lightweight vision classification models such as MobileNet, ShuffleNet, and\nEfficientNet are increasingly deployed in mobile and embedded systems, yet\ntheir performance has been predominantly benchmarked on ImageNet. This raises\ncritical questions: Do models that excel on ImageNet also generalize across\nother domains? How can cross-dataset robustness be systematically quantified?\nAnd which architectural elements consistently drive generalization under tight\nresource constraints? Here, we present the first systematic evaluation of 11\nlightweight vision models (2.5M parameters), trained under a fixed 100-epoch\nschedule across 7 diverse datasets. We introduce the Cross-Dataset Score\n(xScore), a unified metric that quantifies the consistency and robustness of\nmodel performance across diverse visual domains. Our results show that (1)\nImageNet accuracy does not reliably predict performance on fine-grained or\nmedical datasets, (2) xScore provides a scalable predictor of mobile model\nperformance that can be estimated from just four datasets, and (3) certain\narchitectural components--such as isotropic convolutions with higher spatial\nresolution and channel-wise attention--promote broader generalization, while\nTransformer-based blocks yield little additional benefit, despite incurring\nhigher parameter overhead. This study provides a reproducible framework for\nevaluating lightweight vision models beyond ImageNet, highlights key design\nprinciples for mobile-friendly architectures, and guides the development of\nfuture models that generalize robustly across diverse application domains.", "AI": {"tldr": "This paper evaluates 11 lightweight vision models across diverse datasets to gauge their generalization and introduces xScore for robust performance prediction.", "motivation": "Investigate whether models excelling on ImageNet generalize across other domains and identify factors influencing robustness of lightweight vision models.", "method": "Systematic evaluation of lightweight models across 7 datasets using a fixed training schedule and introducing xScore for robustness measurement.", "result": "ImageNet accuracy inconsistently predicts performance on diverse datasets; xScore predicts mobile model robustness using fewer datasets; specific architectural elements promote generalization while Transformer blocks add limited benefit.", "conclusion": "xScore is a scalable metric for robustness, offering a reproducible framework that highlights design principles promoting generalization in lightweight, resource-constrained models."}}
{"id": "2511.01267", "pdf": "https://arxiv.org/pdf/2511.01267", "abs": "https://arxiv.org/abs/2511.01267", "authors": ["Yiyang Yang", "Xiejian Chi", "Shanxing Gao", "Kaidong Wang", "Yao Wang"], "title": "A Spatio-Temporal Online Robust Tensor Recovery Approach for Streaming Traffic Data Imputation", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Data quality is critical to Intelligent Transportation Systems (ITS), as\ncomplete and accurate traffic data underpin reliable decision-making in traffic\ncontrol and management. Recent advances in low-rank tensor recovery algorithms\nhave shown strong potential in capturing the inherent structure of\nhigh-dimensional traffic data and restoring degraded observations. However,\ntraditional batch-based methods demand substantial computational and storage\nresources, which limits their scalability in the face of continuously expanding\ntraffic data volumes. Moreover, recent online tensor recovery methods often\nsuffer from severe performance degradation in complex real-world scenarios due\nto their insufficient exploitation of the intrinsic structural properties of\ntraffic data. To address these challenges, we reformulate the traffic data\nrecovery problem within a streaming framework, and propose a novel online\nrobust tensor recovery algorithm that simultaneously leverages both the global\nspatio-temporal correlations and local consistency of traffic data, achieving\nhigh recovery accuracy and significantly improved computational efficiency in\nlarge-scale scenarios. Our method is capable of simultaneously handling missing\nand anomalous values in traffic data, and demonstrates strong adaptability\nacross diverse missing patterns. Experimental results on three real-world\ntraffic datasets demonstrate that the proposed approach achieves high recovery\naccuracy while significantly improving computational efficiency by up to three\norders of magnitude compared to state-of-the-art batch-based methods. These\nfindings highlight the potential of the proposed approach as a scalable and\neffective solution for traffic data quality enhancement in ITS.", "AI": {"tldr": "The paper introduces an advanced online robust tensor recovery algorithm that efficiently restores traffic data by leveraging spatio-temporal correlations, achieving high accuracy and computational efficiency for Intelligent Transportation Systems (ITS).", "motivation": "The paper addresses the limitations of current traffic data recovery methods, particularly the high resource demand of batch methods and performance issues in online methods when applied to large-scale real-world ITS data.", "method": "The paper proposes a streaming framework-based online robust tensor recovery algorithm that utilizes global spatio-temporal correlations and local consistency of traffic data to effectively recover missing and anomalous data.", "result": "Experimental validation on three real-world traffic datasets shows that the proposed method achieves high recovery accuracy and improves computational efficiency by up to three orders of magnitude compared to traditional batch-based methods.", "conclusion": "The proposed algorithm is scalable, efficient, and adaptable, making it an effective solution for enhancing traffic data quality in Intelligent Transportation Systems."}}
{"id": "2511.01183", "pdf": "https://arxiv.org/pdf/2511.01183", "abs": "https://arxiv.org/abs/2511.01183", "authors": ["Hainan Fang", "Yuanbo Wen", "Jun Bi", "Yihan Wang", "Tonghui He", "Yanlin Tang", "Di Huang", "Jiaming Guo", "Rui Zhang", "Qi Guo", "Yunji Chen"], "title": "QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code", "categories": ["cs.AI"], "comment": "Accepted at NeurIPS 2025", "summary": "Compilers, while essential, are notoriously complex systems that demand\nprohibitively expensive human expertise to develop and maintain. The recent\nadvancements in Large Language Models (LLMs) offer a compelling new paradigm:\nNeural Compilation, which could potentially simplify compiler development for\nnew architectures and facilitate the discovery of innovative optimization\ntechniques. However, several critical obstacles impede its practical adoption.\nFirstly, a significant lack of dedicated benchmarks and robust evaluation\nmethodologies hinders objective assessment and tracking of progress in the\nfield. Secondly, systematically enhancing the reliability and performance of\nLLM-generated assembly remains a critical challenge. Addressing these\nchallenges, this paper introduces NeuComBack, a novel benchmark dataset\nspecifically designed for IR-to-assembly compilation. Leveraging this dataset,\nwe first define a foundational Neural Compilation workflow and conduct a\ncomprehensive evaluation of the capabilities of recent frontier LLMs on Neural\nCompilation, establishing new performance baselines. We further propose a\nself-evolving prompt optimization method that enables LLMs to iteratively\nevolve their internal prompt strategies by extracting insights from prior\nself-debugging traces, thereby enhancing their neural compilation capabilities.\nExperiments demonstrate that our method significantly improves both the\nfunctional correctness and the performance of LLM-generated assembly code.\nCompared to baseline prompts, the functional correctness rates improved from\n44% to 64% on x86_64 and from 36% to 58% on aarch64, respectively. More\nsignificantly, among the 16 correctly generated x86_64 programs using our\nmethod, 14 (87.5%) surpassed clang-O3 performance.", "AI": {"tldr": "This paper introduces NeuComBack, a benchmark dataset for IR-to-assembly compilation, and proposes a self-evolving prompt optimization for improving LLM capabilities in neural compilation.", "motivation": "To simplify compiler development for new architectures and explore innovative optimization techniques using LLMs while addressing challenges like benchmark availability and improving performance and reliability of LLM-generated assembly.", "method": "The paper introduces NeuComBack as a benchmark for IR-to-assembly compilation, evaluates LLMs to set baselines, and develops a self-evolving prompt optimization method to enhance LLM performance using prior self-debugging insights.", "result": "The self-evolving prompt optimization method improved functional correctness from 44% to 64% on x86_64 and from 36% to 58% on aarch64. Additionally, 14 out of 16 correctly generated x86_64 programs surpassed clang-O3 performance.", "conclusion": "NeuComBack and the proposed optimization method significantly advance the field of Neural Compilation by addressing key challenges, laying a foundation for future research and application."}}
{"id": "2511.00960", "pdf": "https://arxiv.org/pdf/2511.00960", "abs": "https://arxiv.org/abs/2511.00960", "authors": ["Abhinav P M", "Ojasva Saxena", "Oswald C", "Parameswari Krishnamurthy"], "title": "The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The extent to which large language models (LLMs) can perform culturally\ngrounded reasoning across non-English languages remains underexplored. This\npaper examines the reasoning and self-assessment abilities of LLMs across seven\nmajor Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and\nTelugu. We introduce a multilingual riddle dataset combining traditional\nriddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5\nPro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under\nseven prompting strategies. In the first stage, we assess riddle-solving\nperformance and find that while Gemini 2.5 Pro performs best overall, few-shot\nmethods yield only marginal gains, and accuracy varies notably across\nlanguages. In the second stage, we conduct a self-evaluation experiment to\nmeasure reasoning consistency. The results reveal a key finding: a model's\ninitial accuracy is inversely correlated with its ability to identify its own\nmistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34%\nTrue Negative Rate), whereas lower-performing models like LLaMA 4 Scout are\nsubstantially more self-aware (42.09% True Negative Rate). These results point\nto clear gaps in multilingual reasoning and highlight the need for models that\nnot only reason effectively but also recognize their own limitations.", "AI": {"tldr": "This paper evaluates the reasoning and self-assessment capabilities of five large language models (LLMs) across seven major Indian languages using a multilingual riddle dataset.", "motivation": "The motivation is to explore how well LLMs can perform culturally grounded reasoning in non-English languages, specifically focusing on Indian languages.", "method": "The research involves creating a multilingual riddle dataset and evaluating five LLMs across seven Indian languages using seven prompting strategies in two stages: riddle-solving and self-evaluation experiments.", "result": "Gemini 2.5 Pro performs best overall in riddle-solving but shows low self-assessment accuracy. Lower-performing models like LLaMA 4 Scout demonstrate higher self-awareness, highlighting gaps in multilingual reasoning.", "conclusion": "The study emphasizes the need for LLMs capable of both effective reasoning and recognizing their own limitations across diverse linguistic contexts."}}
{"id": "2511.01256", "pdf": "https://arxiv.org/pdf/2511.01256", "abs": "https://arxiv.org/abs/2511.01256", "authors": ["Yasamin Foroutani", "Yasamin Mousavi-Motlagh", "Aya Barzelay", "Tsu-Chin Tsao"], "title": "Improving Needle Penetration via Precise Rotational Insertion Using Iterative Learning Control", "categories": ["cs.RO"], "comment": "10 pages, 10 figures", "summary": "Achieving precise control of robotic tool paths is often challenged by\ninherent system misalignments, unmodeled dynamics, and actuation inaccuracies.\nThis work introduces an Iterative Learning Control (ILC) strategy to enable\nprecise rotational insertion of a tool during robotic surgery, improving\npenetration efficacy and safety compared to straight insertion tested in\nsubretinal injection. A 4 degree of freedom (DOF) robot manipulator is used,\nwhere misalignment of the fourth joint complicates the simple application of\nneedle rotation, motivating an ILC approach that iteratively adjusts joint\ncommands based on positional feedback. The process begins with calibrating the\nforward kinematics for the chosen surgical tool to achieve higher accuracy,\nfollowed by successive ILC iterations guided by Optical Coherence Tomography\n(OCT) volume scans to measure the error and refine control inputs. Experimental\nresults, tested on subretinal injection tasks on ex vivo pig eyes, show that\nthe optimized trajectory resulted in higher success rates in tissue penetration\nand subretinal injection compared to straight insertion, demonstrating the\neffectiveness of ILC in overcoming misalignment challenges. This approach\noffers potential applications for other high precision robot tasks requiring\ncontrolled insertions as well.", "AI": {"tldr": "This paper introduces Iterative Learning Control (ILC) to improve the precision and safety of robotic surgeries like subretinal injections by addressing system misalignments and actuation inaccuracies.", "motivation": "Robotic surgeries often face challenges like misalignments and inaccuracies during operations, requiring advanced control methods to improve efficacy and safety.", "method": "The study employs ILC with joint commands refined iteratively based on Optical Coherence Tomography feedback, starting with the calibration of forward kinematics for accuracy.", "result": "Tests on ex vivo pig eyes showed that the optimized trajectory via ILC improved tissue penetration and subretinal injection success rates compared to traditional straight insertion.", "conclusion": "ILC proves effective in overcoming alignment challenges for robotic surgeries and can be extended to other precision tasks requiring controlled tool insertions."}}
{"id": "2511.00116", "pdf": "https://arxiv.org/pdf/2511.00116", "abs": "https://arxiv.org/abs/2511.00116", "authors": ["Avisek Naug", "Antonio Guillen", "Vineet Kumar", "Scott Greenwood", "Wesley Brewer", "Sahand Ghorbanpour", "Ashwin Ramesh Babu", "Vineet Gundecha", "Ricardo Luna Gutierrez", "Soumyendu Sarkar"], "title": "LC-Opt: Benchmarking Reinforcement Learning and Agentic AI for End-to-End Liquid Cooling Optimization in Data Centers", "categories": ["cs.LG", "cs.AI", "cs.MA", "cs.SY", "eess.SY"], "comment": "Submitted to the NeurIPS 2025 conference", "summary": "Liquid cooling is critical for thermal management in high-density data\ncenters with the rising AI workloads. However, machine learning-based\ncontrollers are essential to unlock greater energy efficiency and reliability,\npromoting sustainability. We present LC-Opt, a Sustainable Liquid Cooling (LC)\nbenchmark environment, for reinforcement learning (RL) control strategies in\nenergy-efficient liquid cooling of high-performance computing (HPC) systems.\nBuilt on the baseline of a high-fidelity digital twin of Oak Ridge National\nLab's Frontier Supercomputer cooling system, LC-Opt provides detailed\nModelica-based end-to-end models spanning site-level cooling towers to data\ncenter cabinets and server blade groups. RL agents optimize critical thermal\ncontrols like liquid supply temperature, flow rate, and granular valve\nactuation at the IT cabinet level, as well as cooling tower (CT) setpoints\nthrough a Gymnasium interface, with dynamic changes in workloads. This\nenvironment creates a multi-objective real-time optimization challenge\nbalancing local thermal regulation and global energy efficiency, and also\nsupports additional components like a heat recovery unit (HRU). We benchmark\ncentralized and decentralized multi-agent RL approaches, demonstrate policy\ndistillation into decision and regression trees for interpretable control, and\nexplore LLM-based methods that explain control actions in natural language\nthrough an agentic mesh architecture designed to foster user trust and simplify\nsystem management. LC-Opt democratizes access to detailed, customizable liquid\ncooling models, enabling the ML community, operators, and vendors to develop\nsustainable data center liquid cooling control solutions.", "AI": {"tldr": "LC-Opt provides a benchmark environment for optimizing liquid cooling in HPC systems using reinforcement learning, aiming for energy efficiency and sustainability.", "motivation": "To address the thermal management challenges in high-density data centers with increasing AI workloads while improving energy efficiency and reliability.", "method": "The paper introduces LC-Opt, a simulation environment for reinforcement learning-based control strategies, built on a high-fidelity digital twin of the Frontier Supercomputer's cooling system. It enables optimization of various thermal attributes via RL agents.", "result": "Demonstrated multi-agent RL approaches, interpretability through policy distillation, integration of natural language explanations via LLM, and highlighted customizable cooling models for community use.", "conclusion": "LC-Opt provides tools for sustainable liquid cooling control solutions and fosters collaboration among the ML community, operators, and vendors."}}
{"id": "2511.00338", "pdf": "https://arxiv.org/pdf/2511.00338", "abs": "https://arxiv.org/abs/2511.00338", "authors": ["Yuhao Fang", "Zijian Wang", "Yao Lu", "Ye Zhang", "Chun Li"], "title": "A DeepONet joint Neural Tangent Kernel Hybrid Framework for Physics-Informed Inverse Source Problems and Robust Image Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "This work presents a novel hybrid approach that integrates Deep Operator\nNetworks (DeepONet) with the Neural Tangent Kernel (NTK) to solve complex\ninverse problem. The method effectively addresses tasks such as source\nlocalization governed by the Navier-Stokes equations and image reconstruction,\novercoming challenges related to nonlinearity, sparsity, and noisy data. By\nincorporating physics-informed constraints and task-specific regularization\ninto the loss function, the framework ensures solutions that are both\nphysically consistent and accurate. Validation on diverse synthetic and real\ndatasets demonstrates its robustness, scalability, and precision, showcasing\nits broad potential applications in computational physics and imaging sciences.", "AI": {"tldr": "This paper introduces a hybrid method combining DeepONet and NTK for solving inverse problems, with applications to fluid dynamics and image reconstruction.", "motivation": "The paper aims to tackle challenges in solving inverse problems involving nonlinearity, sparsity, and noisy data.", "method": "They integrate DeepONet with NTK while implementing physics-informed constraints and task-specific regularization in the loss function.", "result": "The method displayed robustness, scalability, and accuracy across synthetic and real datasets.", "conclusion": "This framework holds broad potential in computational physics and imaging sciences due to its precision and consistency."}}
{"id": "2511.01535", "pdf": "https://arxiv.org/pdf/2511.01535", "abs": "https://arxiv.org/abs/2511.01535", "authors": ["Marco Cayuela", "Vincent Le Chenadec", "Peter Schmid", "Taraneh Sayadi"], "title": "HFNO: an interpretable data-driven decomposition strategy for turbulent flows", "categories": ["physics.flu-dyn", "stat.ML"], "comment": "20 pages, 11 figures, 1 table", "summary": "Fourier Neural Operators (FNOs) have demonstrated exceptional accuracy in\nmapping functional spaces by leveraging Fourier transforms to establish a\nconnection with underlying physical principles. However, their opaque inner\nworkings often constitute an obstacle to physical interpretability. This work\nintroduces Hierarchical Fourier Neural Operators (HFNOs), a novel FNO-based\narchitecture tailored for reduced-order modeling of turbulent fluid flows,\ndesigned to enhance interpretability by explicitly separating fluid behavior\nacross scales. The proposed architecture processes wavenumber bins in parallel,\nenabling the approximation of dispersion relations and non-linear interactions.\nInputs are lifted to a higher-dimensional space, Fourier-transformed, and\npartitioned into wavenumber bins. Each bin is processed by a Fully Connected\nNeural Network (FCNN), with outputs subsequently padded, summed, and\ninverse-transformed back into physical space. A final transformation refines\nthe output in physical space as a correction model, by means of one of the\nfollowing architectures: Convolutional Neural Network (CNN) and Echo State\nNetwork (ESN). We evaluate the proposed model on a series of increasingly\ncomplex dynamical systems: first on the one-dimensional Kuramoto-Sivashinsky\nequation, then on the two-dimensional Kolmogorov flow, and finally on the\nprediction of wall shear stress in turbulent channel flow, given the near-wall\nvelocity field. In all test cases, the model demonstrates its ability to\ndecompose turbulent flows across various scales, opening up the possibility of\nincreased interpretability and multiscale modeling of such flows.", "AI": {"tldr": "The paper introduces Hierarchical Fourier Neural Operators (HFNOs), a model for decomposing turbulent fluid flows across scales, enhancing interpretability.", "motivation": "The authors seek to address the challenge of explaining the inner workings of Fourier Neural Operators (FNOs) in modeling turbulent flows by developing a more interpretable, multiscale architecture.", "method": "HFNOs employ Fourier transforms to separate fluid behavior into wavenumber bins, processed via Fully Connected Neural Networks (FCNNs) and further refined using CNN or ESN architectures.", "result": "HFNOs successfully decompose turbulent flows, accurately modeling the dynamics of three examples: Kuramoto-Sivashinsky equation, Kolmogorov flow, and wall shear stress in channel flow.", "conclusion": "The proposed HFNO architecture enhances the interpretability and capability of multiscale modeling in turbulent fluid dynamics."}}
{"id": "2511.01258", "pdf": "https://arxiv.org/pdf/2511.01258", "abs": "https://arxiv.org/abs/2511.01258", "authors": ["Chuyue Lou", "M. Amine Atoui"], "title": "Graph Neural Network-Based Semi-Supervised Open-Set Fault Diagnosis for Marine Machinery Systems", "categories": ["cs.AI"], "comment": null, "summary": "Recently, fault diagnosis methods for marine machinery systems based on deep\nlearning models have attracted considerable attention in the shipping industry.\nMost existing studies assume fault classes are consistent and known between the\ntraining and test datasets, and these methods perform well under controlled\nenvironment. In practice, however, previously unseen or unknown fault types\n(i.e., out-of-distribution or open-set observations not present during\ntraining) can occur, causing such methods to fail and posing a significant\nchallenge to their widespread industrial deployment. To address this challenge,\nthis paper proposes a semi-supervised open-set fault diagnosis (SOFD) framework\nthat enhances and extends the applicability of deep learning models in open-set\nfault diagnosis scenarios. The framework includes a reliability subset\nconstruction process, which uses a multi-layer fusion feature representation\nextracted by a supervised feature learning model to select an unlabeled test\nsubset. The labeled training set and pseudo-labeled test subset are then fed\ninto a semi-supervised diagnosis model to learn discriminative features for\neach class, enabling accurate classification of known faults and effective\ndetection of unknown samples. Experimental results on a public maritime\nbenchmark dataset demonstrate the effectiveness and superiority of the proposed\nSOFD framework.", "AI": {"tldr": "This paper introduces a semi-supervised open-set fault diagnosis (SOFD) framework for addressing unknown fault types in marine machinery systems, utilizing deep learning.", "motivation": "Most fault diagnosis methods rely on consistent and known fault types across datasets, which is impractical when encountering unknown or new fault types, limiting industrial applications.", "method": "The proposed SOFD framework uses a reliability subset construction process with multi-layer fusion feature representation and a semi-supervised diagnosis model for enhanced accuracy in identifying known and unknown faults.", "result": "Experimental analysis on a public maritime dataset confirms the framework's superiority in accurately diagnosing known faults and detecting previously unknown samples.", "conclusion": "The SOFD framework effectively extends the applicability of deep learning in open-set fault diagnosis scenarios, addressing challenges in real-world industrial systems."}}
{"id": "2511.00988", "pdf": "https://arxiv.org/pdf/2511.00988", "abs": "https://arxiv.org/abs/2511.00988", "authors": ["Chenwang Wu", "Yiu-ming Cheung", "Bo Han", "Defu Lian"], "title": "Advancing Machine-Generated Text Detection from an Easy to Hard Supervision Perspective", "categories": ["cs.CL"], "comment": null, "summary": "Existing machine-generated text (MGT) detection methods implicitly assume\nlabels as the \"golden standard\". However, we reveal boundary ambiguity in MGT\ndetection, implying that traditional training paradigms are inexact. Moreover,\nlimitations of human cognition and the superintelligence of detectors make\ninexact learning widespread and inevitable. To this end, we propose an\neasy-to-hard enhancement framework to provide reliable supervision under such\ninexact conditions. Distinct from knowledge distillation, our framework employs\nan easy supervisor targeting relatively simple longer-text detection tasks\n(despite weaker capabilities), to enhance the more challenging target detector.\nFirstly, longer texts targeted by supervisors theoretically alleviate the\nimpact of inexact labels, laying the foundation for reliable supervision.\nSecondly, by structurally incorporating the detector into the supervisor, we\ntheoretically model the supervisor as a lower performance bound for the\ndetector. Thus, optimizing the supervisor indirectly optimizes the detector,\nultimately approximating the underlying \"golden\" labels. Extensive experiments\nacross diverse practical scenarios, including cross-LLM, cross-domain, mixed\ntext, and paraphrase attacks, demonstrate the framework's significant detection\neffectiveness. The code is available at:\nhttps://github.com/tmlr-group/Easy2Hard.", "AI": {"tldr": "Existing methods assume perfect labels for machine-generated text detection, which is flawed. This study proposes an 'easy-to-hard' enhancement framework to address label ambiguity and improve detection robustness.", "motivation": "Traditional training paradigms assume 'golden standard' labels for machine-generated text detection, but these labels often involve ambiguity and errors due to human limitations and detector superintelligence.", "method": "The proposed framework uses an 'easy supervisor' targeting simpler detection tasks on longer texts. It structurally integrates the detector with the supervisor to mitigate the impact of ambiguous labels, optimize detection, and approximate ideal labels.", "result": "Experiments demonstrate improved detection performance in various challenging conditions like cross-LLM, cross-domain, mixed text, and paraphrase attacks.", "conclusion": "The framework significantly enhances detection reliability despite learning under ambiguous conditions, suggesting its potential as a robust and practical solution."}}
{"id": "2511.01545", "pdf": "https://arxiv.org/pdf/2511.01545", "abs": "https://arxiv.org/abs/2511.01545", "authors": ["Ronivaldo Ferreira", "Guilherme da Silva", "Carla Rocha", "Gustavo Pinto"], "title": "From Pre-labeling to Production: Engineering Lessons from a Machine Learning Pipeline in the Public Sector", "categories": ["cs.SE", "cs.CY"], "comment": "11 pages, 2 figures, 4 tables", "summary": "Machine learning is increasingly being embedded into government digital\nplatforms, but public-sector constraints make it difficult to build ML systems\nthat are accurate, auditable, and operationally sustainable. In practice, teams\nface not only technical issues like extreme class imbalance and data drift, but\nalso organizational barriers such as bureaucratic data access, lack of\nversioned datasets, and incomplete governance over provenance and monitoring.\nOur study of the Brasil Participativo (BP) platform shows that common\nengineering choices -- like using LLMs for pre-labeling, splitting models into\nrouted classifiers, and generating synthetic data -- can speed development but\nalso introduce new traceability, reliability, and cost risks if not paired with\ndisciplined data governance and human validation. This means that, in the\npublic sector, responsible ML is not just a modeling problem but an\ninstitutional engineering problem, and ML pipelines must be treated as civic\ninfrastructure. Ultimately, this study shows that the success of machine\nlearning in the public sector will depend less on breakthroughs in model\naccuracy and more on the ability of institutions to engineer transparent,\nreproducible, and accountable data infrastructures that citizens can trust.", "AI": {"tldr": "This paper examines the challenges of deploying machine learning in government digital platforms, emphasizing the need for institutional data governance and civic infrastructure to ensure transparency and accountability.", "motivation": "To address the difficulties and constraints faced while implementing machine learning in government systems, including technical challenges (e.g., class imbalance, data drift) and organizational barriers (e.g., data access, governance).", "method": "A case study of the Brasil Participativo platform, analyzing the use of techniques like LLMs for pre-labeling, routed models, and synthetic data generation to understand the implications on traceability, reliability, and costs.", "result": "The paper finds that without disciplined data governance and human validation, common engineering choices can introduce risks. It highlights that ML in the public sector must balance technical and institutional challenges.", "conclusion": "The success of ML in government will depend on creating transparent, reproducible, and accountable data infrastructures rather than solely focusing on advancing model accuracy. ML pipelines should be treated as civic infrastructure to build citizen trust."}}
{"id": "2511.01272", "pdf": "https://arxiv.org/pdf/2511.01272", "abs": "https://arxiv.org/abs/2511.01272", "authors": ["Sehui Jeong", "Magaly C. Aviles", "Athena X. Naylor", "Cynthia Sung", "Allison M. Okamura"], "title": "Design and Fabrication of Origami-Inspired Knitted Fabrics for Soft Robotics", "categories": ["cs.RO"], "comment": null, "summary": "Soft robots employing compliant materials and deformable structures offer\ngreat potential for wearable devices that are comfortable and safe for human\ninteraction. However, achieving both structural integrity and compliance for\ncomfort remains a significant challenge. In this study, we present a novel\nfabrication and design method that combines the advantages of origami\nstructures with the material programmability and wearability of knitted\nfabrics. We introduce a general design method that translates origami patterns\ninto knit designs by programming both stitch and material patterns. The method\ncreates folds in preferred directions while suppressing unintended buckling and\nbending by selectively incorporating heat fusible yarn to create rigid panels\naround compliant creases. We experimentally quantify folding moments and show\nthat stitch patterning enhances folding directionality while the heat fusible\nyarn (1) keeps geometry consistent by reducing edge curl and (2) prevents\nout-of-plane deformations by stiffening panels. We demonstrate the framework\nthrough the successful reproduction of complex origami tessellations, including\nMiura-ori, Yoshimura, and Kresling patterns, and present a wearable knitted\nKaleidocycle robot capable of locomotion. The combination of structural\nreconfigurability, material programmability, and potential for manufacturing\nscalability highlights knitted origami as a promising platform for\nnext-generation wearable robotics.", "AI": {"tldr": "This paper introduces a method to create wearable robots by designing knitted fabrics that mimic origami, enhancing structural integrity, compliance, and scalability.", "motivation": "Soft robots need to balance structural integrity for functionality and compliance for wearer comfort, which is challenging.", "method": "The authors developed a method to translate origami designs into knit patterns using programmable stitches and materials, incorporating heat fusible yarn to improve structural functionality.", "result": "They quantified folding moments, demonstrated enhanced folding directionality and rigidity, and successfully recreated complex origami designs as well as a wearable knitted robot.", "conclusion": "Knitted origami combines reconfigurability, material programmability, and scalable manufacturing, offering a promising direction for wearable robotics."}}
{"id": "2511.00117", "pdf": "https://arxiv.org/pdf/2511.00117", "abs": "https://arxiv.org/abs/2511.00117", "authors": ["Antonio Guillen-Perez", "Avisek Naug", "Vineet Gundecha", "Sahand Ghorbanpour", "Ricardo Luna Gutierrez", "Ashwin Ramesh Babu", "Munther Salim", "Shubhanker Banerjee", "Eoin H. Oude Essink", "Damien Fay", "Soumyendu Sarkar"], "title": "DCcluster-Opt: Benchmarking Dynamic Multi-Objective Optimization for Geo-Distributed Data Center Workloads", "categories": ["cs.LG", "cs.AI", "cs.MA", "cs.SY", "eess.SY"], "comment": "Submitted to the NeurIPS 2025 conference", "summary": "The increasing energy demands and carbon footprint of large-scale AI require\nintelligent workload management in globally distributed data centers. Yet\nprogress is limited by the absence of benchmarks that realistically capture the\ninterplay of time-varying environmental factors (grid carbon intensity,\nelectricity prices, weather), detailed data center physics (CPUs, GPUs, memory,\nHVAC energy), and geo-distributed network dynamics (latency and transmission\ncosts). To bridge this gap, we present DCcluster-Opt: an open-source,\nhigh-fidelity simulation benchmark for sustainable, geo-temporal task\nscheduling. DCcluster-Opt combines curated real-world datasets, including AI\nworkload traces, grid carbon intensity, electricity markets, weather across 20\nglobal regions, cloud transmission costs, and empirical network delay\nparameters with physics-informed models of data center operations, enabling\nrigorous and reproducible research in sustainable computing. It presents a\nchallenging scheduling problem where a top-level coordinating agent must\ndynamically reassign or defer tasks that arrive with resource and service-level\nagreement requirements across a configurable cluster of data centers to\noptimize multiple objectives. The environment also models advanced components\nsuch as heat recovery. A modular reward system enables an explicit study of\ntrade-offs among carbon emissions, energy costs, service level agreements, and\nwater use. It provides a Gymnasium API with baseline controllers, including\nreinforcement learning and rule-based strategies, to support reproducible ML\nresearch and a fair comparison of diverse algorithms. By offering a realistic,\nconfigurable, and accessible testbed, DCcluster-Opt accelerates the development\nand validation of next-generation sustainable computing solutions for\ngeo-distributed data centers.", "AI": {"tldr": "This paper introduces DCcluster-Opt, a benchmark for sustainable task scheduling in globally distributed data centers that integrates realistic environmental factors, data center physics, and network dynamics.", "motivation": "Address the lack of realistic benchmarks for optimizing energy and carbon footprint in globally distributed data centers.", "method": "Present DCcluster-Opt, a configurable simulation benchmark combining real-world datasets, physics-informed models, and a Gymnasium API for testing algorithms.", "result": "DCcluster-Opt enables rigorous research in sustainable geo-temporal task scheduling, providing a testbed for developing next-gen solutions.", "conclusion": "DCcluster-Opt supports sustainable computing solutions by enabling realistic validations of algorithms in high-fidelity simulations."}}
{"id": "2511.00344", "pdf": "https://arxiv.org/pdf/2511.00344", "abs": "https://arxiv.org/abs/2511.00344", "authors": ["Xihang Qiu", "Jiarong Cheng", "Yuhao Fang", "Wanpeng Zhang", "Yao Lu", "Ye Zhang", "Chun Li"], "title": "Federated Dialogue-Semantic Diffusion for Emotion Recognition under Incomplete Modalities", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Emotion Recognition in Conversations (MERC) enhances emotional\nunderstanding through the fusion of multimodal signals. However, unpredictable\nmodality absence in real-world scenarios significantly degrades the performance\nof existing methods. Conventional missing-modality recovery approaches, which\ndepend on training with complete multimodal data, often suffer from semantic\ndistortion under extreme data distributions, such as fixed-modality absence. To\naddress this, we propose the Federated Dialogue-guided and Semantic-Consistent\nDiffusion (FedDISC) framework, pioneering the integration of federated learning\ninto missing-modality recovery. By federated aggregation of modality-specific\ndiffusion models trained on clients and broadcasting them to clients missing\ncorresponding modalities, FedDISC overcomes single-client reliance on modality\ncompleteness. Additionally, the DISC-Diffusion module ensures consistency in\ncontext, speaker identity, and semantics between recovered and available\nmodalities, using a Dialogue Graph Network to capture conversational\ndependencies and a Semantic Conditioning Network to enforce semantic alignment.\nWe further introduce a novel Alternating Frozen Aggregation strategy, which\ncyclically freezes recovery and classifier modules to facilitate collaborative\noptimization. Extensive experiments on the IEMOCAP, CMUMOSI, and CMUMOSEI\ndatasets demonstrate that FedDISC achieves superior emotion classification\nperformance across diverse missing modality patterns, outperforming existing\napproaches.", "AI": {"tldr": "The paper addresses the challenge of missing modalities in Multimodal Emotion Recognition in Conversations by introducing a federated learning framework called FedDISC.", "motivation": "To overcome the performance degradation caused by unpredictable modality absence in real-world applications of multimodal emotion recognition.", "method": "The FedDISC framework integrates federated learning with modality-specific diffusion models optimized through a collaborative approach, leveraging Dialogue Graph Network and Semantic Conditioning Network for consistency.", "result": "FedDISC achieves superior emotion classification performance on datasets like IEMOCAP, CMUMOSI, and CMUMOSEI, outperforming existing methods when modalities are missing.", "conclusion": "The proposed framework effectively tackles missing modality challenges in multimodal emotion recognition, delivering improved classification accuracy under diverse scenarios."}}
{"id": "2511.01605", "pdf": "https://arxiv.org/pdf/2511.01605", "abs": "https://arxiv.org/abs/2511.01605", "authors": ["Daniel Busbib", "Ami Wiesel"], "title": "Estimation of Toeplitz Covariance Matrices using Overparameterized Gradient Descent", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We consider covariance estimation under Toeplitz structure. Numerous\nsophisticated optimization methods have been developed to maximize the Gaussian\nlog-likelihood under Toeplitz constraints. In contrast, recent advances in deep\nlearning demonstrate the surprising power of simple gradient descent (GD)\napplied to overparameterized models. Motivated by this trend, we revisit\nToeplitz covariance estimation through the lens of overparameterized GD. We\nmodel the $P\\times P$ covariance as a sum of $K$ complex sinusoids with\nlearnable parameters and optimize them via GD. We show that when $K = P$, GD\nmay converge to suboptimal solutions. However, mild overparameterization ($K =\n2P$ or $4P$) consistently enables global convergence from random\ninitializations. We further propose an accelerated GD variant with separate\nlearning rates for amplitudes and frequencies. When frequencies are fixed and\nonly amplitudes are optimized, we prove that the optimization landscape is\nasymptotically benign and any stationary point recovers the true covariance.\nFinally, numerical experiments demonstrate that overparameterized GD can match\nor exceed the accuracy of state-of-the-art methods in challenging settings,\nwhile remaining simple and scalable.", "AI": {"tldr": "This study explores Toeplitz covariance estimation using gradient descent (GD) in overparameterized models. Mild overparameterization ensures global convergence and improved accuracy.", "motivation": "To examine the potential of overparameterized gradient descent (GD) for solving Toeplitz covariance estimation, inspired by its success in deep learning.", "method": "Model the covariance matrix as a sum of complex sinusoids and optimize parameters using GD, with added overparameterization and an accelerated variant with distinct learning rates.", "result": "Mild overparameterization ($K=2P$ or $4P$) enables global convergence, and numerical experiments show better or comparable accuracy to state-of-the-art methods.", "conclusion": "Overparameterized GD offers a simple yet effective and scalable approach for Toeplitz covariance estimation, outperforming sophisticated methods in certain scenarios."}}
{"id": "2511.01311", "pdf": "https://arxiv.org/pdf/2511.01311", "abs": "https://arxiv.org/abs/2511.01311", "authors": ["Filip Naudot", "Tobias Sundqvist", "Timotheus Kampik"], "title": "llmSHAP: A Principled Approach to LLM Explainability", "categories": ["cs.AI"], "comment": null, "summary": "Feature attribution methods help make machine learning-based inference\nexplainable by determining how much one or several features have contributed to\na model's output. A particularly popular attribution method is based on the\nShapley value from cooperative game theory, a measure that guarantees the\nsatisfaction of several desirable principles, assuming deterministic inference.\nWe apply the Shapley value to feature attribution in large language model\n(LLM)-based decision support systems, where inference is, by design, stochastic\n(non-deterministic). We then demonstrate when we can and cannot guarantee\nShapley value principle satisfaction across different implementation variants\napplied to LLM-based decision support, and analyze how the stochastic nature of\nLLMs affects these guarantees. We also highlight trade-offs between explainable\ninference speed, agreement with exact Shapley value attributions, and principle\nattainment.", "AI": {"tldr": "The paper investigates applying Shapley values for feature attribution in stochastic inference environments, specifically large language models (LLMs), analyzing the reliability and trade-offs.", "motivation": "The motivation is to improve explainability in LLM-based decision support systems by evaluating the application of Shapley value-based feature attribution methods, addressing their behavior in stochastic contexts.", "method": "The authors explore the implementation of Shapley value principles on LLM-based stochastic decision support systems, testing their reliability and trade-offs.", "result": "They identify scenarios where Shapley principles hold or fail under stochastic inference in LLMs, and evaluate trade-offs between speed, accuracy, and principle satisfaction.", "conclusion": "Shapley value methods can be applied to stochastic inference environments, but trade-offs exist between speed, accuracy, and adherence to attribution principles, necessitating informed decisions in their implementation for LLMs."}}
{"id": "2511.01008", "pdf": "https://arxiv.org/pdf/2511.01008", "abs": "https://arxiv.org/abs/2511.01008", "authors": ["Haolin Yang", "Jipeng Zhang", "Zhitao He", "Yi R. Fung"], "title": "MARS-SQL: A multi-agent reinforcement learning framework for Text-to-SQL", "categories": ["cs.CL"], "comment": null, "summary": "Translating natural language to SQL remains difficult for complex queries.\nSuch queries often need environmental interaction and self-correction. To\naddress this, we introduce MARS-SQL, a novel multi-agent framework that\ncombines principled task decomposition and interactive reinforcement learning\n(RL). Our system comprises three specialized agents: a Grounding Agent for\nschema linking, a Generation Agent for query generation, and a Validation Agent\nfor final selection. The core of our framework is the Generation agent, which\nis trained via a multi-turn RL policy. Adopting a ReAct-style Think-Act-Observe\nloop, the agent iteratively generates thoughts, executes SQL actions against a\nlive database, and revises its strategy based on execution feedback, enabling\ndynamic, stateful reasoning and self-correction. At inference time, we generate\nmultiple interaction trajectories to explore diverse reasoning paths. The\nValidation agent, then selects the optimal trajectory by modeling verification\nas a next-token prediction task and choosing the solution with the highest\ngeneration probability. This structured workflow pipelines specialized agents.\nIt combines interactive RL for generation with generative modeling for\nverification. The approach proves highly effective for robust and accurate SQL\ngeneration. Experiments show that MARS-SQL achieves state-of-the-art Execution\nAccuracy of 77.84% on the BIRD dev set and 89.75% on the Spider test set. Our\ncode is available at https://github.com/YangHaolin0526/MARS-SQL.", "AI": {"tldr": "The paper introduces MARS-SQL, a multi-agent framework for generating accurate SQL queries using reinforcement learning and generative modeling.", "motivation": "Natural language translation to SQL is challenging for complex queries requiring interaction and self-correction.", "method": "MARS-SQL employs three agents: a Grounding Agent for schema linking, a Generation Agent leveraging Reinforcement Learning for iterative query formation, and a Validation Agent for selecting the best trajectory based on generative modeling.", "result": "State-of-the-art execution accuracy was achieved with 77.84% on the BIRD dev set and 89.75% on the Spider test set.", "conclusion": "MARS-SQL's structured multi-agent approach and interactive reinforcement learning demonstrate a significant advancement in robust SQL query generation."}}
{"id": "2511.01757", "pdf": "https://arxiv.org/pdf/2511.01757", "abs": "https://arxiv.org/abs/2511.01757", "authors": ["Shamse Tasnim Cynthia", "Banani Roy"], "title": "Towards LLM-Powered Task-Aware Retrieval of Scientific Workflows for Galaxy", "categories": ["cs.SE"], "comment": null, "summary": "Scientific Workflow Management Systems (SWfMSs) such as Galaxy have become\nessential infrastructure in bioinformatics, supporting the design, execution,\nand sharing of complex multi-step analyses. Despite hosting hundreds of\nreusable workflows across domains, Galaxy's current keyword-based retrieval\nsystem offers limited support for semantic query interpretation and often fails\nto surface relevant workflows when exact term matches are absent. To address\nthis gap, we propose a task-aware, two-stage retrieval framework that\nintegrates dense vector search with large language model (LLM)-based reranking.\nOur system first retrieves candidate workflows using state-of-the-art embedding\nmodels and then reranks them using instruction-tuned generative LLMs (GPT-4o,\nMistral-7B) based on semantic task alignment. To support robust evaluation, we\nconstruct a benchmark dataset of Galaxy workflows annotated with semantic\ntopics via BERTopic and synthesize realistic task-oriented queries using LLMs.\nWe conduct a comprehensive comparison of lexical, dense, and reranking models\nusing standard IR metrics, presenting the first systematic evaluation of\nretrieval performance in the Galaxy ecosystem. Results show that our approach\nsignificantly improves top-k accuracy and relevance, particularly for long or\nunder-specified queries. We further integrate our system as a prototype tool\nwithin Galaxy, providing a proof-of-concept for LLM-enhanced workflow search.\nThis work advances the usability and accessibility of scientific workflows,\nespecially for novice users and interdisciplinary researchers.", "AI": {"tldr": "This paper proposes a novel task-aware, two-stage retrieval framework combining dense vector search and large language model-based reranking to improve workflow retrieval in Galaxy, a bioinformatics workflow management system.", "motivation": "Galaxy\u2019s current keyword-based search system struggles to retrieve relevant workflows when exact term matches are absent, necessitating an improved, semantically aware retrieval method to enhance usability for a broader user base.", "method": "The proposed system performs a two-stage process: first using embedding models for initial workflow retrieval via dense vector search, followed by instruction-tuned large language models (GPT-4o, Mistral-7B) for task-aligned reranking. Additionally, the authors curated a benchmark dataset and employed LLMs for query generation.", "result": "The proposed method significantly outperforms baseline models in terms of top-k accuracy and relevance, particularly for complex or under-specified user queries, demonstrated through systematic IR metric evaluations.", "conclusion": "This framework enhances the search and discovery of workflows in Galaxy, boosting accessibility and usability for scientists, especially novices and interdisciplinary researchers. It highlights the potential of LLMs in scientific tool integration."}}
{"id": "2511.01276", "pdf": "https://arxiv.org/pdf/2511.01276", "abs": "https://arxiv.org/abs/2511.01276", "authors": ["Yiyao Ma", "Kai Chen", "Kexin Zheng", "Qi Dou"], "title": "Contact Map Transfer with Conditional Diffusion Model for Generalizable Dexterous Grasp Generation", "categories": ["cs.RO"], "comment": null, "summary": "Dexterous grasp generation is a fundamental challenge in robotics, requiring\nboth grasp stability and adaptability across diverse objects and tasks.\nAnalytical methods ensure stable grasps but are inefficient and lack task\nadaptability, while generative approaches improve efficiency and task\nintegration but generalize poorly to unseen objects and tasks due to data\nlimitations. In this paper, we propose a transfer-based framework for dexterous\ngrasp generation, leveraging a conditional diffusion model to transfer\nhigh-quality grasps from shape templates to novel objects within the same\ncategory. Specifically, we reformulate the grasp transfer problem as the\ngeneration of an object contact map, incorporating object shape similarity and\ntask specifications into the diffusion process. To handle complex shape\nvariations, we introduce a dual mapping mechanism, capturing intricate\ngeometric relationship between shape templates and novel objects. Beyond the\ncontact map, we derive two additional object-centric maps, the part map and\ndirection map, to encode finer contact details for more stable grasps. We then\ndevelop a cascaded conditional diffusion model framework to jointly transfer\nthese three maps, ensuring their intra-consistency. Finally, we introduce a\nrobust grasp recovery mechanism, identifying reliable contact points and\noptimizing grasp configurations efficiently. Extensive experiments demonstrate\nthe superiority of our proposed method. Our approach effectively balances grasp\nquality, generation efficiency, and generalization performance across various\ntasks. Project homepage: https://cmtdiffusion.github.io/", "AI": {"tldr": "The paper presents a novel framework using conditional diffusion models to enhance dexterous grasp generation by transferring high-quality grasps across objects within the same category.", "motivation": "Address the inefficiency and lack of task adaptability in analytical methods and the poor generalization of generative approaches for dexterous grasp generation.", "method": "The framework uses a conditional diffusion model with object contact maps, dual mapping mechanisms, and cascaded diffusion processes to transfer grasp details efficiently and consistently.", "result": "Demonstrated superior grasp quality, efficiency, and generalization performance in extensive experiments.", "conclusion": "The approach successfully balances stability, adaptability, and efficiency in dexterous grasp generation across diverse tasks and objects."}}
{"id": "2511.00121", "pdf": "https://arxiv.org/pdf/2511.00121", "abs": "https://arxiv.org/abs/2511.00121", "authors": ["Shoma Yagi", "Jun Ichikawa", "Genki Ichinose"], "title": "Analysis of Line Break prediction models for detecting defensive breakthrough in football", "categories": ["cs.LG", "physics.soc-ph", "stat.AP"], "comment": "14 pages, 8 figures", "summary": "In football, attacking teams attempt to break through the opponent's\ndefensive line to create scoring opportunities. This action, known as a Line\nBreak, is a critical indicator of offensive effectiveness and tactical\nperformance, yet previous studies have mainly focused on shots or goal\nopportunities rather than on how teams break the defensive line. In this study,\nwe develop a machine learning model to predict Line Breaks using event and\ntracking data from the 2023 J1 League season. The model incorporates 189\nfeatures, including player positions, velocities, and spatial configurations,\nand employs an XGBoost classifier to estimate the probability of Line Breaks.\nThe proposed model achieved high predictive accuracy, with an AUC of 0.982 and\na Brier score of 0.015. Furthermore, SHAP analysis revealed that factors such\nas offensive player speed, gaps in the defensive line, and offensive players'\nspatial distributions significantly contribute to the occurrence of Line\nBreaks. Finally, we found a moderate positive correlation between the predicted\nprobability of being Line-Broken and the number of shots and crosses conceded\nat the team level. These results suggest that Line Breaks are closely linked to\nthe creation of scoring opportunities and provide a quantitative framework for\nunderstanding tactical dynamics in football.", "AI": {"tldr": "This study develops a machine learning model to predict 'Line Breaks' in football, achieving high accuracy.", "motivation": "To address the lack of focus on 'Line Breaks' as an indicator of offensive effectiveness and tactical performance in previous football studies.", "method": "A machine learning model using XGBoost classifier with 189 features (player positions, velocities, spatial configurations) was built. SHAP analysis was also employed.", "result": "The model achieved an AUC of 0.982 and a Brier score of 0.015. It identified key factors contributing to 'Line Breaks', like offensive player speed and gaps in the defensive line.", "conclusion": "'Line Breaks' are strongly linked to scoring opportunities, offering a quantitative understanding of tactical football dynamics."}}
{"id": "2511.00345", "pdf": "https://arxiv.org/pdf/2511.00345", "abs": "https://arxiv.org/abs/2511.00345", "authors": ["Amir Ziashahabi", "Narges Ghasemi", "Sajjad Shahabi", "John Krumm", "Salman Avestimehr", "Cyrus Shahabi"], "title": "OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap Data", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at NeurIPS 2025 UrbanAI Workshop", "summary": "Accurate and up-to-date geospatial data are essential for urban planning,\ninfrastructure monitoring, and environmental management. Yet, automating urban\nmonitoring remains difficult because curated datasets of specific urban\nfeatures and their changes are scarce. We introduce OSMGen, a generative\nframework that creates realistic satellite imagery directly from raw\nOpenStreetMap (OSM) data. Unlike prior work that relies on raster tiles, OSMGen\nuses the full richness of OSM JSON, including vector geometries, semantic tags,\nlocation, and time, giving fine-grained control over how scenes are generated.\nA central feature of the framework is the ability to produce consistent\nbefore-after image pairs: user edits to OSM inputs translate into targeted\nvisual changes, while the rest of the scene is preserved. This makes it\npossible to generate training data that addresses scarcity and class imbalance,\nand to give planners a simple way to preview proposed interventions by editing\nmap data. More broadly, OSMGen produces paired (JSON, image) data for both\nstatic and changed states, paving the way toward a closed-loop system where\nsatellite imagery can automatically drive structured OSM updates. Source code\nis available at https://github.com/amir-zsh/OSMGen.", "AI": {"tldr": "OSMGen is a framework that uses OpenStreetMap (OSM) data to generate realistic satellite imagery, enabling urban monitoring and addressing limited geospatial dataset availability.", "motivation": "The paper addresses the challenge of automating urban monitoring due to the lack of curated datasets related to urban features and their changes.", "method": "OSMGen uses OSM JSON data, including rich vector geometries and semantic information, to generate realistic satellite imagery. It creates consistent before-and-after image pairs from user edits and map data.", "result": "The system allows generation of balanced training datasets, previews of urban interventions, and lays the groundwork for systems where satellite imagery can directly update OSM data.", "conclusion": "OSMGen enhances geospatial data accuracy and training capabilities, addressing data scarcity and imbalance issues, with potential applications in urban planning and monitoring efforts."}}
{"id": "2511.01641", "pdf": "https://arxiv.org/pdf/2511.01641", "abs": "https://arxiv.org/abs/2511.01641", "authors": ["Xiaopeng Ke", "Yihan Yu", "Ruyue Zhang", "Zhishuo Zhou", "Fangzhou Shi", "Chang Men", "Zhengdan Zhu"], "title": "Cross-Treatment Effect Estimation for Multi-Category, Multi-Valued Causal Inference via Dynamic Neural Masking", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Counterfactual causal inference faces significant challenges when extended to\nmulti-category, multi-valued treatments, where complex cross-effects between\nheterogeneous interventions are difficult to model. Existing methodologies\nremain constrained to binary or single-type treatments and suffer from\nrestrictive assumptions, limited scalability, and inadequate evaluation\nframeworks for complex intervention scenarios.\n  We present XTNet, a novel network architecture for multi-category,\nmulti-valued treatment effect estimation. Our approach introduces a\ncross-effect estimation module with dynamic masking mechanisms to capture\ntreatment interactions without restrictive structural assumptions. The\narchitecture employs a decomposition strategy separating basic effects from\ncross-treatment interactions, enabling efficient modeling of combinatorial\ntreatment spaces. We also propose MCMV-AUCC, a suitable evaluation metric that\naccounts for treatment costs and interaction effects. Extensive experiments on\nsynthetic and real-world datasets demonstrate that XTNet consistently\noutperforms state-of-the-art baselines in both ranking accuracy and effect\nestimation quality. The results of the real-world A/B test further confirm its\neffectiveness.", "AI": {"tldr": "The paper introduces XTNet, a novel network architecture to better handle multi-category, multi-valued treatment effect estimation and demonstrates its effectiveness.", "motivation": "Existing approaches face challenges with complex cross-effects between treatments in multi-category, multi-valued setups. These methods are limited to binary or single-type treatments and struggle with scalability and evaluation frameworks.", "method": "XTNet uses a cross-effect estimation module with dynamic masking and a decomposition strategy to separate basic effects from interactions. It enables efficient modeling of combinatorial interventions.", "result": "Experiments on synthetic and real-world datasets show XTNet's superior performance compared to baselines. Real-world A/B testing further validates its efficacy.", "conclusion": "XTNet improves complex intervention scenarios through novel architecture and metrics, offering better treatment effect estimation consistency and accuracy."}}
{"id": "2511.01320", "pdf": "https://arxiv.org/pdf/2511.01320", "abs": "https://arxiv.org/abs/2511.01320", "authors": ["Ziqi Wang", "Hailiang Zhao", "Yuhao Yang", "Daojiang Hu", "Cheng Bao", "Mingyi Liu", "Kai Di", "Schahram Dustdar", "Zhongjie Wang", "Shuiguang Deng"], "title": "OmniFuser: Adaptive Multimodal Fusion for Service-Oriented Predictive Maintenance", "categories": ["cs.AI"], "comment": null, "summary": "Accurate and timely prediction of tool conditions is critical for intelligent\nmanufacturing systems, where unplanned tool failures can lead to quality\ndegradation and production downtime. In modern industrial environments,\npredictive maintenance is increasingly implemented as an intelligent service\nthat integrates sensing, analysis, and decision support across production\nprocesses. To meet the demand for reliable and service-oriented operation, we\npresent OmniFuser, a multimodal learning framework for predictive maintenance\nof milling tools that leverages both visual and sensor data. It performs\nparallel feature extraction from high-resolution tool images and cutting-force\nsignals, capturing complementary spatiotemporal patterns across modalities. To\neffectively integrate heterogeneous features, OmniFuser employs a\ncontamination-free cross-modal fusion mechanism that disentangles shared and\nmodality-specific components, allowing for efficient cross-modal interaction.\nFurthermore, a recursive refinement pathway functions as an anchor mechanism,\nconsistently retaining residual information to stabilize fusion dynamics. The\nlearned representations can be encapsulated as reusable maintenance service\nmodules, supporting both tool-state classification (e.g., Sharp, Used, Dulled)\nand multi-step force signal forecasting. Experiments on real-world milling\ndatasets demonstrate that OmniFuser consistently outperforms state-of-the-art\nbaselines, providing a dependable foundation for building intelligent\nindustrial maintenance services.", "AI": {"tldr": "The paper introduces OmniFuser, a multimodal learning framework combining tool images and force signal data for predictive maintenance of milling tools in intelligent manufacturing systems.", "motivation": "Addressing unplanned tool failures in manufacturing systems, which lead to production issues, by providing a reliable predictive maintenance solution using multimodal data integration.", "method": "OmniFuser combines high-resolution tool images and cutting-force signals, using a fusion mechanism to merge shared and specific features, and applies a recursive refinement pathway for stabilized representations.", "result": "Experiments on real-world datasets show OmniFuser outperforms state-of-the-art methods in predictive maintenance tasks, including tool-state classification and multi-step force signal forecasting.", "conclusion": "The proposed OmniFuser framework offers a robust approach to predictive maintenance, enabling efficient and reliable maintenance service modules for industrial applications."}}
{"id": "2511.01014", "pdf": "https://arxiv.org/pdf/2511.01014", "abs": "https://arxiv.org/abs/2511.01014", "authors": ["Bosi Wen", "Yilin Niu", "Cunxiang Wang", "Pei Ke", "Xiaoying Ling", "Ying Zhang", "Aohan Zeng", "Hongning Wang", "Minlie Huang"], "title": "IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation", "categories": ["cs.CL"], "comment": "21 pages, 5 figures", "summary": "Instruction following is a fundamental ability of Large Language Models\n(LLMs), requiring their generated outputs to follow multiple constraints\nimposed in input instructions. Numerous studies have attempted to enhance this\nability through preference optimization or reinforcement learning based on\nreward signals from LLM-as-a-Judge. However, existing evaluation models for\ninstruction following still possess many deficiencies, such as substantial\ncosts and unreliable assessments. To this end, we propose IF-CRITIC, an LLM\ncritic that can provide efficient and reliable assessments of constraint\nfollowing in the instructions. We first develop a checklist generator to\ndecompose instructions and generate constraint checklists. With the assistance\nof the checklists, we collect high-quality critique training data through a\nmulti-stage critique filtering mechanism and employ a constraint-level\npreference optimization method to train IF-CRITIC. Extensive experiments\ndemonstrate that the evaluation performance of IF-CRITIC can beat strong\nLLM-as-a-Judge baselines, including Deepseek-R1 and o4-mini. With the scalable\nreward signals provided by IF-CRITIC, LLMs can achieve substantial performance\ngains in instruction-following optimization under lower computational overhead\ncompared to strong LLM critic baselines.", "AI": {"tldr": "The paper introduces IF-CRITIC, a model aimed at efficiently evaluating and optimizing instruction-following ability in Large Language Models (LLMs) with scalable rewards and reduced computational costs.", "motivation": "Current evaluation models for instruction-following in LLMs are costly and unreliable, prompting the need for an efficient and accurate assessment mechanism.", "method": "The authors propose IF-CRITIC, which includes a checklist generator to decompose instructions, a multi-stage critique filtering mechanism for reliable data collection, and constraint-level preference optimization for training.", "result": "Experiments show IF-CRITIC outperforms existing LLM-as-a-Judge baselines, like Deepseek-R1 and o4-mini, in evaluation accuracy and allows for improved optimization of LLMs under reduced computational requirements.", "conclusion": "IF-CRITIC offers a reliable and efficient solution for assessing and enhancing instruction-following in LLMs, outperforming previous methods and optimizing models with lower overhead."}}
{"id": "2511.01763", "pdf": "https://arxiv.org/pdf/2511.01763", "abs": "https://arxiv.org/abs/2511.01763", "authors": ["Xiaohan Wang", "Yuxin Hu", "Kevin Leach"], "title": "Context-Guided Decompilation: A Step Towards Re-executability", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Binary decompilation plays an important role in software security analysis,\nreverse engineering, and malware understanding when source code is unavailable.\nHowever, existing decompilation techniques often fail to produce source code\nthat can be successfully recompiled and re-executed, particularly for optimized\nbinaries. Recent advances in large language models (LLMs) have enabled neural\napproaches to decompilation, but the generated code is typically only\nsemantically plausible rather than truly executable, limiting their practical\nreliability. These shortcomings arise from compiler optimizations and the loss\nof semantic cues in compiled code, which LLMs struggle to recover without\ncontextual guidance. To address this challenge, we propose ICL4Decomp, a hybrid\ndecompilation framework that leverages in-context learning (ICL) to guide LLMs\ntoward generating re-executable source code. We evaluate our method across\nmultiple datasets, optimization levels, and compilers, demonstrating around\n40\\% improvement in re-executability over state-of-the-art decompilation\nmethods while maintaining robustness.", "AI": {"tldr": "The paper proposes ICL4Decomp, a hybrid framework using in-context learning to guide large language models in generating re-executable source code from binary, improving on existing decompilation techniques.", "motivation": "Existing decompilation methods often fail to produce re-executable code, especially for optimized binaries due to lost semantic cues and compiler optimizations.", "method": "The framework incorporates in-context learning (ICL) into large language models to enhance their ability to recover contextual semantic cues and produce executable source code.", "result": "ICL4Decomp achieves about 40% improvement in code re-executability over state-of-the-art decompilation techniques across varied datasets, optimization levels, and compilers.", "conclusion": "ICL4Decomp offers a robust and effective solution to the limitations of traditional and neural decompilation methods, advancing the generation of re-executable code from binaries."}}
{"id": "2511.01288", "pdf": "https://arxiv.org/pdf/2511.01288", "abs": "https://arxiv.org/abs/2511.01288", "authors": ["Bixuan Zhang", "Fengqi Zhang", "Haojie Chen", "You Wang", "Jie Hao", "Zhiyuan Luo", "Guang Li"], "title": "A High-Speed Capable Spherical Robot", "categories": ["cs.RO", "cs.SY", "eess.SY", "I.2.9"], "comment": "5 pages", "summary": "This paper designs a new spherical robot structure capable of supporting\nhigh-speed motion at up to 10 m/s. Building upon a single-pendulum-driven\nspherical robot, the design incorporates a momentum wheel with an axis aligned\nwith the secondary pendulum, creating a novel spherical robot structure.\nPractical experiments with the physical prototype have demonstrated that this\nnew spherical robot can achieve stable high-speed motion through simple\ndecoupled control, which was unattainable with the original structure. The\nspherical robot designed for high-speed motion not only increases speed but\nalso significantly enhances obstacle-crossing performance and terrain\nrobustness.", "AI": {"tldr": "A new spherical robot structure achieves high-speed motion, enhancing performance and terrain robustness.", "motivation": "To address limitations in motion stability and control in high-speed spherical robot designs.", "method": "A novel spherical robot structure incorporating a secondary pendulum and momentum wheel for improved control.", "result": "Experiments show stable high-speed motion and enhanced obstacle-crossing capability and terrain robustness.", "conclusion": "The design outperforms previous models in speed, stability, and robustness, showing promise for complex terrains."}}
{"id": "2511.00124", "pdf": "https://arxiv.org/pdf/2511.00124", "abs": "https://arxiv.org/abs/2511.00124", "authors": ["Sai Niranjan Ramachandran", "Manish Krishan Lal", "Suvrit Sra"], "title": "Cross-fluctuation phase transitions reveal sampling dynamics in diffusion models", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at NeurIPS 2025. 10 pages, camera-ready version. appendices\n  included", "summary": "We analyse how the sampling dynamics of distributions evolve in score-based\ndiffusion models using cross-fluctuations, a centered-moment statistic from\nstatistical physics. Specifically, we show that starting from an unbiased\nisotropic normal distribution, samples undergo sharp, discrete transitions,\neventually forming distinct events of a desired distribution while\nprogressively revealing finer structure. As this process is reversible, these\ntransitions also occur in reverse, where intermediate states progressively\nmerge, tracing a path back to the initial distribution. We demonstrate that\nthese transitions can be detected as discontinuities in $n^{\\text{th}}$-order\ncross-fluctuations. For variance-preserving SDEs, we derive a closed-form for\nthese cross-fluctuations that is efficiently computable for the reverse\ntrajectory. We find that detecting these transitions directly boosts sampling\nefficiency, accelerates class-conditional and rare-class generation, and\nimproves two zero-shot tasks--image classification and style transfer--without\nexpensive grid search or retraining. We also show that this viewpoint unifies\nclassical coupling and mixing from finite Markov chains with continuous\ndynamics while extending to stochastic SDEs and non Markovian samplers. Our\nframework therefore bridges discrete Markov chain theory, phase analysis, and\nmodern generative modeling.", "AI": {"tldr": "The paper studies sampling dynamics in score-based diffusion models using cross-fluctuations, identifying sharp transitions in distribution sampling and improving efficiency and tasks like classification and style transfer.", "motivation": "The authors aimed to understand and improve the sampling dynamics of distributions in score-based diffusion models and unify modern generative modeling with theoretical and statistical physics concepts.", "method": "They used cross-fluctuations, a centered-moment statistic, to detect transitions in sampling dynamics, deriving closed-form equations for variance-preserving SDEs and applying them to enhance sampling efficiency, rare-class generation, classification, and style transfer.", "result": "Sharp discrete sampling transitions were detected using cross-fluctuations, boosting efficiency, and improving class-conditional generation alongside zero-shot tasks like image classification and style transfer.", "conclusion": "The framework unifies classical theories of Markov chain coupling and mixing with continuous dynamics, extending to stochastic SDEs and non-Markovian samplers, and promises practical improvements in generative modeling tasks."}}
{"id": "2511.00352", "pdf": "https://arxiv.org/pdf/2511.00352", "abs": "https://arxiv.org/abs/2511.00352", "authors": ["Mohd Ruhul Ameen", "Akif Islam"], "title": "Detecting AI-Generated Images via Diffusion Snap-Back Reconstruction: A Forensic Approach", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 8 figures, 4 Tables, submitted to ICECTE 2026", "summary": "The rapid rise of generative diffusion models has made distinguishing\nauthentic visual content from synthetic imagery increasingly challenging.\nTraditional deepfake detection methods, which rely on frequency or pixel-level\nartifacts, fail against modern text-to-image systems such as Stable Diffusion\nand DALL-E that produce photorealistic and artifact-free results. This paper\nintroduces a diffusion-based forensic framework that leverages multi-strength\nimage reconstruction dynamics, termed diffusion snap-back, to identify\nAI-generated images. By analysing how reconstruction metrics (LPIPS, SSIM, and\nPSNR) evolve across varying noise strengths, we extract interpretable\nmanifold-based features that differentiate real and synthetic images. Evaluated\non a balanced dataset of 4,000 images, our approach achieves 0.993 AUROC under\ncross-validation and remains robust to common distortions such as compression\nand noise. Despite using limited data and a single diffusion backbone (Stable\nDiffusion v1.5), the proposed method demonstrates strong generalization and\ninterpretability, offering a foundation for scalable, model-agnostic synthetic\nmedia forensics.", "AI": {"tldr": "This paper proposes a diffusion-based forensic approach to effectively identify AI-generated images even when they are photorealistic and artifact-free.", "motivation": "Generative diffusion models such as Stable Diffusion and DALL-E produce photorealistic images, making it increasingly difficult for traditional methods to differentiate real and synthetic imagery.", "method": "The study introduces 'diffusion snap-back,' a forensic framework that uses multi-strength image reconstruction dynamics and analyzes reconstruction metrics (LPIPS, SSIM, PSNR) across noise levels to differentiate real and synthetic images.", "result": "Through evaluation on 4,000 balanced images, the method achieves an AUROC of 0.993 under cross-validation and is robust to distortions like compression and noise.", "conclusion": "The approach shows strong generalization and scalability using limited data, providing a model-agnostic foundation for forensic detection of synthetic media."}}
{"id": "2511.01795", "pdf": "https://arxiv.org/pdf/2511.01795", "abs": "https://arxiv.org/abs/2511.01795", "authors": ["Gabriel Nobis", "Maximilian Springenberg", "Arina Belova", "Rembert Daems", "Christoph Knochenhauer", "Manfred Opper", "Tolga Birdal", "Wojciech Samek"], "title": "Fractional Diffusion Bridge Models", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO", "stat.ML"], "comment": "To appear in NeurIPS 2025 proceedings. This version includes\n  post-camera-ready revisions", "summary": "We present Fractional Diffusion Bridge Models (FDBM), a novel generative\ndiffusion bridge framework driven by an approximation of the rich and\nnon-Markovian fractional Brownian motion (fBM). Real stochastic processes\nexhibit a degree of memory effects (correlations in time), long-range\ndependencies, roughness and anomalous diffusion phenomena that are not captured\nin standard diffusion or bridge modeling due to the use of Brownian motion\n(BM). As a remedy, leveraging a recent Markovian approximation of fBM (MA-fBM),\nwe construct FDBM that enable tractable inference while preserving the\nnon-Markovian nature of fBM. We prove the existence of a coupling-preserving\ngenerative diffusion bridge and leverage it for future state prediction from\npaired training data. We then extend our formulation to the Schr\\\"{o}dinger\nbridge problem and derive a principled loss function to learn the unpaired data\ntranslation. We evaluate FDBM on both tasks: predicting future protein\nconformations from aligned data, and unpaired image translation. In both\nsettings, FDBM achieves superior performance compared to the Brownian\nbaselines, yielding lower root mean squared deviation (RMSD) of C$_\\alpha$\natomic positions in protein structure prediction and lower Fr\\'echet Inception\nDistance (FID) in unpaired image translation.", "AI": {"tldr": "The paper introduces Fractional Diffusion Bridge Models (FDBM) using fractional Brownian motion (fBM) for improved generative modeling and predictions.", "motivation": "Standard diffusion models using Brownian motion lack the ability to capture memory effects, long-range dependencies, roughness, and anomalous diffusion observed in real stochastic processes.", "method": "The FDBM framework employs a Markovian approximation of fractional Brownian motion (MA-fBM) to construct non-Markovian generative diffusion bridges. It incorporates coupling-preserving bridges and Schr\u00f6dinger bridge loss functions for paired and unpaired data tasks.", "result": "FDBM outperforms Brownian-based baselines in tasks like predicting future protein conformations and unpaired image translation, showing superior RMSD for protein structures and improved FID for image translations.", "conclusion": "FDBM provides a powerful and more accurate alternative to Brownian motion-based diffusion models for tasks involving complex dependencies and memory effects, with promising results in applications like structural biology and computer vision."}}
{"id": "2511.01329", "pdf": "https://arxiv.org/pdf/2511.01329", "abs": "https://arxiv.org/abs/2511.01329", "authors": ["Ying Song", "Yijing Wang", "Hui Yang", "Weihan Jin", "Jun Xiong", "Congyi Zhou", "Jialin Zhu", "Xiang Gao", "Rong Chen", "HuaGuang Deng", "Ying Dai", "Fei Xiao", "Haihong Tang", "Bo Zheng", "KaiFu Zhang"], "title": "Unbiased Platform-Level Causal Estimation for Search Systems: A Competitive Isolation PSM-DID Framework", "categories": ["cs.AI"], "comment": null, "summary": "Evaluating platform-level interventions in search-based two-sided\nmarketplaces is fundamentally challenged by systemic effects such as spillovers\nand network interference. While widely used for causal inference, the PSM\n(Propensity Score Matching) - DID (Difference-in-Differences) framework remains\nsusceptible to selection bias and cross-unit interference from unaccounted\nspillovers. In this paper, we introduced Competitive Isolation PSM-DID, a novel\ncausal framework that integrates propensity score matching with competitive\nisolation to enable platform-level effect measurement (e.g., order volume, GMV)\ninstead of item-level metrics in search systems.\n  Our approach provides theoretically guaranteed unbiased estimation under\nmutual exclusion conditions, with an open dataset released to support\nreproducible research on marketplace interference (github.com/xxxx). Extensive\nexperiments demonstrate significant reductions in interference effects and\nestimation variance compared to baseline methods. Successful deployment in a\nlarge-scale marketplace confirms the framework's practical utility for\nplatform-level causal inference.", "AI": {"tldr": "This paper introduces Competitive Isolation PSM-DID, a novel causal inference framework for platform-level evaluations in search-based marketplaces, effectively addressing systemic spillover and network interference issues.", "motivation": "Current causal inference frameworks like PSM-DID face challenges such as selection bias and network interference when applied to platform-level metrics in marketplaces.", "method": "The proposed Competitive Isolation PSM-DID integrates propensity score matching with competitive isolation to mitigate spillovers and enable accurate platform-level effect measurement.", "result": "Experiments showed reduced interference effects and estimation variance compared to baselines. Deployment in a large-scale marketplace validated its practical effectiveness.", "conclusion": "Competitive Isolation PSM-DID offers unbiased estimation for platform-level metrics under mutual exclusion conditions, proving its theoretical and practical capabilities in addressing marketplace interference."}}
{"id": "2511.01016", "pdf": "https://arxiv.org/pdf/2511.01016", "abs": "https://arxiv.org/abs/2511.01016", "authors": ["Wenjin Liu", "Haoran Luo", "Xueyuan Lin", "Haoming Liu", "Tiesunlong Shen", "Jiapu Wang", "Rui Mao", "Erik Cambria"], "title": "Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Recently, advanced large language models (LLMs) have emerged at an\nincreasingly rapid pace. However, when faced with complex problems, most users\nare often unable to provide accurate and effective prompts to interact with\nLLMs, thus limiting the performance of LLMs. To address this challenge, we\npropose Prompt-R1, an end-to-end reinforcement learning framework that uses a\nsmall-scale LLM to collaborate with large-scale LLMs, replacing user\ninteraction to solve problems better. This collaboration is cast as a\nmulti-turn prompt interaction, where the small-scale LLM thinks and generates\nprompts, and the large-scale LLM performs complex reasoning. A dual-constrained\nreward is designed to optimize for correctness, generation quality, and\nreasoning accuracy. Prompt-R1 provides a plug-and-play framework that supports\nboth inference and training with various large-scale LLMs. Experiments on\nmultiple public datasets show that Prompt-R1 significantly outperforms baseline\nmodels across tasks. Our code is publicly available at\nhttps://github.com/QwenQKing/Prompt-R1.", "AI": {"tldr": "The paper introduces Prompt-R1, a reinforcement learning framework leveraging small-scale LLMs to generate prompts for larger LLMs, improving performance on complex tasks.", "motivation": "Users often struggle to create effective prompts for large language models (LLMs), hindering their performance on complex problems.", "method": "The researchers developed Prompt-R1, an end-to-end reinforcement learning approach. It engages small-scale LLMs to generate prompts for large-scale LLMs in a multi-turn interactive process using dual-constrained rewards for correctness, quality, and reasoning.", "result": "Prompt-R1 was tested across multiple public datasets and achieved significant performance improvements over baseline models.", "conclusion": "Prompt-R1 effectively enhances the interaction between small and large-scale LLMs, providing better problem-solving capabilities, and offers a flexible, publicly available framework."}}
{"id": "2511.01850", "pdf": "https://arxiv.org/pdf/2511.01850", "abs": "https://arxiv.org/abs/2511.01850", "authors": ["Jiawei Jin", "Yingxin Su", "Xiaotong Zhu"], "title": "SmartMLOps Studio: Design of an LLM-Integrated IDE with Automated MLOps Pipelines for Model Development and Monitoring", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "The rapid expansion of artificial intelligence and machine learning (ML)\napplications has intensified the demand for integrated environments that unify\nmodel development, deployment, and monitoring. Traditional Integrated\nDevelopment Environments (IDEs) focus primarily on code authoring, lacking\nintelligent support for the full ML lifecycle, while existing MLOps platforms\nremain detached from the coding workflow. To address this gap, this study\nproposes the design of an LLM-Integrated IDE with automated MLOps pipelines\nthat enables continuous model development and monitoring within a single\nenvironment. The proposed system embeds a Large Language Model (LLM) assistant\ncapable of code generation, debugging recommendation, and automatic pipeline\nconfiguration. The backend incorporates automated data validation, feature\nstorage, drift detection, retraining triggers, and CI/CD deployment\norchestration. This framework was implemented in a prototype named SmartMLOps\nStudio and evaluated using classification and forecasting tasks on the UCI\nAdult and M5 datasets. Experimental results demonstrate that SmartMLOps Studio\nreduces pipeline configuration time by 61%, improves experiment reproducibility\nby 45%, and increases drift detection accuracy by 14% compared to traditional\nworkflows. By bridging intelligent code assistance and automated operational\npipelines, this research establishes a novel paradigm for AI engineering -\ntransforming the IDE from a static coding tool into a dynamic, lifecycle-aware\nintelligent platform for scalable and efficient model development.", "AI": {"tldr": "This study proposes an LLM-Integrated IDE with automated MLOps pipelines to streamline the ML lifecycle, evaluated via a prototype named SmartMLOps Studio.", "motivation": "The demand for integrated environments supporting the complete ML lifecycle has increased due to limitations in traditional IDEs and MLOps platforms.", "method": "The system combines a Large Language Model assistant with backend automation for data validation, drift detection, retraining, and deployment, implemented in the prototype SmartMLOps Studio.", "result": "SmartMLOps Studio reduced pipeline configuration time by 61%, improved experiment reproducibility by 45%, and increased drift detection accuracy by 14%.", "conclusion": "The research introduces an innovative IDE paradigm, integrating intelligent code assistance and automated pipelines for AI engineering efficiency and scalability."}}
{"id": "2511.01294", "pdf": "https://arxiv.org/pdf/2511.01294", "abs": "https://arxiv.org/abs/2511.01294", "authors": ["Jiawei Wang", "Dingyou Wang", "Jiaming Hu", "Qixuan Zhang", "Jingyi Yu", "Lan Xu"], "title": "Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "A deep understanding of kinematic structures and movable components is\nessential for enabling robots to manipulate objects and model their own\narticulated forms. Such understanding is captured through articulated objects,\nwhich are essential for tasks such as physical simulation, motion planning, and\npolicy learning. However, creating these models, particularly for complex\nsystems like robots or objects with high degrees of freedom (DoF), remains a\nsignificant challenge. Existing methods typically rely on motion sequences or\nstrong assumptions from hand-curated datasets, which hinders scalability. In\nthis paper, we introduce Kinematify, an automated framework that synthesizes\narticulated objects directly from arbitrary RGB images or text prompts. Our\nmethod addresses two core challenges: (i) inferring kinematic topologies for\nhigh-DoF objects and (ii) estimating joint parameters from static geometry. To\nachieve this, we combine MCTS search for structural inference with\ngeometry-driven optimization for joint reasoning, producing physically\nconsistent and functionally valid descriptions. We evaluate Kinematify on\ndiverse inputs from both synthetic and real-world environments, demonstrating\nimprovements in registration and kinematic topology accuracy over prior work.", "AI": {"tldr": "Kinematify framework automatically generates articulated object models from RGB images or text prompts, addressing topology and joint parameter estimation challenges for high-DoF objects.", "motivation": "Understand kinematic structures for robots to manipulate high-DoF objects effectively, overcoming scalability issues in traditional modeling methods.", "method": "Utilizes Monte Carlo Tree Search (MCTS) for kinematic topology inference and optimization techniques for reasoning joint parameters.", "result": "Evaluates on synthetic and real environments, showing enhanced accuracy in object registration and kinematic topology over prior approaches.", "conclusion": "Kinematify improves scalability and physical simulation accuracy for constructing articulated object models, paving the way for broader applications in robotics."}}
{"id": "2511.00126", "pdf": "https://arxiv.org/pdf/2511.00126", "abs": "https://arxiv.org/abs/2511.00126", "authors": ["Lu Bowen"], "title": "Dynamic Model Selection for Trajectory Prediction via Pairwise Ranking and Meta-Features", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent deep trajectory predictors (e.g., Jiang et al., 2023; Zhou et al.,\n2022) have achieved strong average accuracy but remain unreliable in complex\nlong-tail driving scenarios. These limitations reveal the weakness of the\nprevailing \"one-model-fits-all\" paradigm, particularly in safety-critical urban\ncontexts where simpler physics-based models can occasionally outperform\nadvanced networks (Kalman, 1960). To bridge this gap, we propose a dynamic\nmulti-expert gating framework that adaptively selects the most reliable\ntrajectory predictor among a physics-informed LSTM, a Transformer, and a\nfine-tuned GameFormer on a per-sample basis.\n  Our method leverages internal model signals (meta-features) such as stability\nand uncertainty (Gal and Ghahramani, 2016), which we demonstrate to be\nsubstantially more informative than geometric scene descriptors. To the best of\nour knowledge, this is the first work to formulate trajectory expert selection\nas a pairwise-ranking problem over internal model signals (Burges et al.,\n2005), directly optimizing decision quality without requiring post-hoc\ncalibration.\n  Evaluated on the nuPlan-mini dataset (Caesar et al., 2021) with 1,287\nsamples, our LLM-enhanced tri-expert gate achieves a Final Displacement Error\n(FDE) of 2.567 m, representing a 9.5 percent reduction over GameFormer (2.835\nm), and realizes 57.8 percent of the oracle performance bound. In open-loop\nsimulations, after trajectory horizon alignment, the same configuration reduces\nFDE on left-turn scenarios by approximately 10 percent, demonstrating\nconsistent improvements across both offline validation and open-loop\nevaluation. These results indicate that adaptive hybrid systems enhance\ntrajectory reliability in safety-critical autonomous driving, providing a\npractical pathway beyond static single-model paradigms.", "AI": {"tldr": "A dynamic multi-expert framework is proposed to improve deep trajectory predictions in complex driving scenarios by adaptively selecting the best model, achieving notable accuracy improvements.", "motivation": "Prevailing deep trajectory predictors struggle with reliability in complex driving situations, and simpler physics-based models can sometimes perform better, especially in safety-critical urban contexts.", "method": "A dynamic multi-expert gating system combines physics-informed LSTM, Transformer, and GameFormer. It uses internal model meta-features (stability, uncertainty) for adaptive model selection, formulated as a pairwise-ranking problem.", "result": "The proposed system achieves a 9.5% reduction in Final Displacement Error (FDE) compared to GameFormer, and shows consistent improvements in both offline validation and open-loop evaluations.", "conclusion": "Adaptive hybrid systems, utilizing expert selection, improve trajectory prediction reliability and outperform static single-model paradigms in autonomous driving applications."}}
{"id": "2511.00357", "pdf": "https://arxiv.org/pdf/2511.00357", "abs": "https://arxiv.org/abs/2511.00357", "authors": ["Niklas W\u00f6lki", "Lukas Kondmann", "Christian Molli\u00e8re", "Martin Langer", "Julia Gottfriedsen", "Martin Werner"], "title": "Transfer Learning for Onboard Cloud Segmentation in Thermal Earth Observation: From Landsat to a CubeSat Constellation", "categories": ["cs.CV"], "comment": "This work was presented at the TerraBytes Workshop at the 42nd\n  International Conference on Machine Learning. This version is not part of the\n  official ICML proceedings", "summary": "Onboard cloud segmentation is a critical yet underexplored task in thermal\nEarth observation (EO), particularly for CubeSat missions constrained by\nlimited hardware and spectral information. CubeSats often rely on a single\nthermal band and lack sufficient labeled data, making conventional cloud\nmasking techniques infeasible. This work addresses these challenges by applying\ntransfer learning to thermal cloud segmentation for the FOREST-2 CubeSat, using\na UNet with a lightweight MobileNet encoder. We pretrain the model on the\npublic Landsat-7 Cloud Cover Assessment Dataset and fine-tune it with a small\nset of mission-specific samples in a joint-training setup, improving the macro\nF1 from 0.850 to 0.877 over FOREST-2-only baselines. We convert the model to a\nTensorRT engine and demonstrate full-image inference in under 5 seconds on an\nNVIDIA Jetson Nano. These results show that leveraging public datasets and\nlightweight architectures can enable accurate, efficient thermal-only cloud\nmasking on-orbit, supporting real-time decision-making in data-limited EO\nmissions.", "AI": {"tldr": "This paper focuses on onboard cloud segmentation for CubeSat missions in Earth observation, introducing a Thermal UNet model pre-trained on public data and fine-tuned for mission-specific samples, achieving efficient and accurate cloud masking.", "motivation": "CubeSat missions struggle with cloud segmentation due to limited hardware, spectral information, and labeled data, making traditional masking techniques impractical.", "method": "The authors used a lightweight UNet model with MobileNet encoder, pre-training on the Landsat-7 dataset and fine-tuning on mission-specific samples in a joint-training setup.", "result": "The method improved the macro F1 score from 0.850 to 0.877, and demonstrated efficient inference on an NVIDIA Jetson Nano in under 5 seconds.", "conclusion": "Transfer learning with public datasets and lightweight architectures enables accurate and efficient thermal cloud segmentation for real-time decision-making in resource-limited CubeSat missions."}}
{"id": "2511.01847", "pdf": "https://arxiv.org/pdf/2511.01847", "abs": "https://arxiv.org/abs/2511.01847", "authors": ["Zhi Wang", "Chicheng Zhang", "Ramya Korlakai Vinayak"], "title": "Bridging Lifelong and Multi-Task Representation Learning via Algorithm and Complexity Measure", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "In lifelong learning, a learner faces a sequence of tasks with shared\nstructure and aims to identify and leverage it to accelerate learning. We study\nthe setting where such structure is captured by a common representation of\ndata. Unlike multi-task learning or learning-to-learn, where tasks are\navailable upfront to learn the representation, lifelong learning requires the\nlearner to make use of its existing knowledge while continually gathering\npartial information in an online fashion. In this paper, we consider a\ngeneralized framework of lifelong representation learning. We propose a simple\nalgorithm that uses multi-task empirical risk minimization as a subroutine and\nestablish a sample complexity bound based on a new notion we introduce--the\ntask-eluder dimension. Our result applies to a wide range of learning problems\ninvolving general function classes. As concrete examples, we instantiate our\nresult on classification and regression tasks under noise.", "AI": {"tldr": "This paper introduces a framework and algorithm for lifelong representation learning and analyzes its efficiency using a novel dimension concept, applicable to tasks like classification and regression.", "motivation": "To address the need for a learner to identify shared task structures and utilize partial information in an online manner for lifelong learning.", "method": "The paper proposes a simple algorithm leveraging multi-task empirical risk minimization, supported by a sample complexity bound based on the task-eluder dimension.", "result": "The framework and algorithm are shown to be broadly applicable to diverse learning problems, including noisy classification and regression tasks.", "conclusion": "This study advances the theoretical understanding of lifelong representation learning by proposing scalable methods and analyzing their performance through novel mathematical constructs."}}
{"id": "2511.01363", "pdf": "https://arxiv.org/pdf/2511.01363", "abs": "https://arxiv.org/abs/2511.01363", "authors": ["Giuseppe Riva", "Brenda K. Wiederhold", "Fabrizia Mantovani"], "title": "Automatic Minds: Cognitive Parallels Between Hypnotic States and Large Language Model Processing", "categories": ["cs.AI"], "comment": "4 Tables", "summary": "The cognitive processes of the hypnotized mind and the computational\noperations of large language models (LLMs) share deep functional parallels.\nBoth systems generate sophisticated, contextually appropriate behavior through\nautomatic pattern-completion mechanisms operating with limited or unreliable\nexecutive oversight. This review examines this convergence across three\nprinciples: automaticity, in which responses emerge from associative rather\nthan deliberative processes; suppressed monitoring, leading to errors such as\nconfabulation in hypnosis and hallucination in LLMs; and heightened contextual\ndependency, where immediate cues (for example, the suggestion of a therapist or\nthe prompt of the user) override stable knowledge.\n  These mechanisms reveal an observer-relative meaning gap: both systems\nproduce coherent but ungrounded outputs that require an external interpreter to\nsupply meaning. Hypnosis and LLMs also exemplify functional agency - the\ncapacity for complex, goal-directed, context-sensitive behavior - without\nsubjective agency, the conscious awareness of intention and ownership that\ndefines human action. This distinction clarifies how purposive behavior can\nemerge without self-reflective consciousness, governed instead by structural\nand contextual dynamics. Finally, both domains illuminate the phenomenon of\nscheming: automatic, goal-directed pattern generation that unfolds without\nreflective awareness. Hypnosis provides an experimental model for understanding\nhow intention can become dissociated from conscious deliberation, offering\ninsights into the hidden motivational dynamics of artificial systems.\nRecognizing these parallels suggests that the future of reliable AI lies in\nhybrid architectures that integrate generative fluency with mechanisms of\nexecutive monitoring, an approach inspired by the complex, self-regulating\narchitecture of the human mind.", "AI": {"tldr": "The paper draws parallels between the cognitive processes of hypnotized minds and the operations of large language models (LLMs), highlighting similarities like automaticity, reliance on context, and lack of subjective agency. The authors suggest hybrid AI architectures inspired by human mind dynamics.", "motivation": "To explore the functional similarities between hypnosis and LLMs, revealing insights into how both systems operate and their implications for understanding AI and consciousness.", "method": "The review focuses on three principles\u2014automaticity, suppressed monitoring, and heightened contextual dependency\u2014discussing concepts like functional agency and scheming to analyze parallels between hypnotized minds and LLMs.", "result": "The analysis shows that both hypnosis and LLMs produce coherent but ungrounded outputs, exhibit goal-directed behavior without subjective awareness, and demonstrate similar behavior arising from structural and contextual factors.", "conclusion": "Hypnosis offers insights into the dissociation of intention from consciousness, helping to understand the hidden motivational dynamics of artificial systems. For reliable AI development, integrating generative fluency with executive monitoring mechanisms is proposed, inspired by human cognitive architectures."}}
{"id": "2511.01019", "pdf": "https://arxiv.org/pdf/2511.01019", "abs": "https://arxiv.org/abs/2511.01019", "authors": ["Bowen Chen", "Jayesh Gajbhar", "Gregory Dusek", "Rob Redmon", "Patrick Hogan", "Paul Liu", "DelWayne Bohnenstiehl", "Dongkuan", "Xu", "Ruoying He"], "title": "OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG", "physics.ao-ph"], "comment": "A related presentation will be given at the AGU(American Geophysical\n  Union) and AMS(American Meteorological Society) Annual Meetings", "summary": "Artificial intelligence is transforming the sciences, yet general\nconversational AI systems often generate unverified \"hallucinations\"\nundermining scientific rigor. We present OceanAI, a conversational platform\nthat integrates the natural-language fluency of open-source large language\nmodels (LLMs) with real-time, parameterized access to authoritative\noceanographic data streams hosted by the National Oceanic and Atmospheric\nAdministration (NOAA). Each query such as \"What was Boston Harbor's highest\nwater level in 2024?\" triggers real-time API calls that identify, parse, and\nsynthesize relevant datasets into reproducible natural-language responses and\ndata visualizations. In a blind comparison with three widely used AI\nchat-interface products, only OceanAI produced NOAA-sourced values with\noriginal data references; others either declined to answer or provided\nunsupported results. Designed for extensibility, OceanAI connects to multiple\nNOAA data products and variables, supporting applications in marine hazard\nforecasting, ecosystem assessment, and water-quality monitoring. By grounding\noutputs and verifiable observations, OceanAI advances transparency,\nreproducibility, and trust, offering a scalable framework for AI-enabled\ndecision support within the oceans. A public demonstration is available at\nhttps://oceanai.ai4ocean.xyz.", "AI": {"tldr": "OceanAI is a conversational AI platform integrating large language models with real-time authoritative NOAA data, enhancing precision, transparency, and reliability.", "motivation": "To address the issue of hallucinations from general AI systems and improve scientific reliability in oceanographic conversation platforms.", "method": "OceanAI queries NOAA data streams in real-time using API calls to provide reproducible responses and visualizations, grounded in authoritative datasets.", "result": "OceanAI uniquely provided accurate and referenced NOAA data in blind tests, outperforming other AI systems which either refused to respond or gave incorrect answers.", "conclusion": "OceanAI demonstrates potential as a scalable solution to enhance decision-making in ocean sciences through transparent and trustworthy AI interactions."}}
{"id": "2511.00330", "pdf": "https://arxiv.org/pdf/2511.00330", "abs": "https://arxiv.org/abs/2511.00330", "authors": ["Yeonju Ro", "Haoran Qiu", "\u00cd\u00f1igo Goiri", "Rodrigo Fonseca", "Ricardo Bianchini", "Aditya Akella", "Zhangyang Wang", "Mattan Erez", "Esha Choukse"], "title": "Sherlock: Reliable and Efficient Agentic Workflow Execution", "categories": ["cs.MA", "cs.SE"], "comment": null, "summary": "With the increasing adoption of large language models (LLM), agentic\nworkflows, which compose multiple LLM calls with tools, retrieval, and\nreasoning steps, are increasingly replacing traditional applications. However,\nsuch workflows are inherently error-prone: incorrect or partially correct\noutput at one step can propagate or even amplify through subsequent stages,\ncompounding the impact on the final output. Recent work proposes integrating\nverifiers that validate LLM output or actions, such as self-reflection, debate,\nor LLM-as-a-judge mechanisms. Yet, verifying every step introduces significant\nlatency and cost overheads.\n  In this work, we seek to answer three key questions: which nodes in a\nworkflow are most error-prone and thus deserve costly verification, how to\nselect the most appropriate verifier for each node, and how to use verification\nwith minimal impact to latency? Our solution, Sherlock, addresses these using\ncounterfactual analysis on agentic workflows to identify error-prone nodes and\nselectively attaching cost-optimal verifiers only where necessary. At runtime,\nSherlock speculatively executes downstream tasks to reduce latency overhead,\nwhile verification runs in the background. If verification fails, execution is\nrolled back to the last verified output. Compared to the non-verifying\nbaseline, Sherlock delivers an 18.3% accuracy gain on average across\nbenchmarks. Sherlock reduces workflow execution time by up to 48.7% over\nnon-speculative execution and lowers verification cost by 26.0% compared to the\nMonte Carlo search-based method, demonstrating that principled, fault-aware\nverification effectively balances efficiency and reliability in agentic\nworkflows.", "AI": {"tldr": "This paper introduces Sherlock, a verification method for error-prone nodes in agentic LLM workflows, which improves accuracy and reduces latency and costs.", "motivation": "The widespread use of agentic workflows with LLMs demands more reliable systems due to inherent error propagation throughout multi-step processes.", "method": "Sherlock identifies error-prone nodes through counterfactual analysis, selectively attaches cost-optimal verifiers, and uses speculative execution to reduce latency while running verification in the background.", "result": "Sherlock improves accuracy by 18.3%, reduces execution time by up to 48.7%, and lowers verification costs by 26.0%.", "conclusion": "Sherlock successfully balances efficiency and reliability in LLM-based workflows by strategically verifying critical nodes and optimizing execution processes."}}
{"id": "2511.01331", "pdf": "https://arxiv.org/pdf/2511.01331", "abs": "https://arxiv.org/abs/2511.01331", "authors": ["Hongyin Zhang", "Shuo Zhang", "Junxi Jin", "Qixin Zeng", "Runze Li", "Donglin Wang"], "title": "RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Vision-Language-Action (VLA) models have recently emerged as powerful\ngeneral-purpose policies for robotic manipulation, benefiting from large-scale\nmulti-modal pre-training. However, they often fail to generalize reliably in\nout-of-distribution deployments, where unavoidable disturbances such as\nobservation noise, sensor errors, or actuation perturbations become prevalent.\nWhile recent Reinforcement Learning (RL)-based post-training provides a\npractical means to adapt pre-trained VLA models, existing methods mainly\nemphasize reward maximization and overlook robustness to environmental\nuncertainty. In this work, we introduce RobustVLA, a lightweight online RL\npost-training method designed to explicitly enhance the resilience of VLA\nmodels. Through a systematic robustness analysis, we identify two key\nregularizations: Jacobian regularization, which mitigates sensitivity to\nobservation noise, and smoothness regularization, which stabilizes policies\nunder action perturbations. Extensive experiments across diverse robotic\nenvironments demonstrate that RobustVLA significantly outperforms prior\nstate-of-the-art methods in robustness and reliability. Our results highlight\nthe importance of principled robustness-aware RL post-training as a key step\ntoward improving the reliability and robustness of VLA models.", "AI": {"tldr": "This paper introduces RobustVLA, an RL-based post-training method for Vision-Language-Action models, enhancing robustness to observation noise and action perturbations.", "motivation": "Vision-Language-Action models show promise but fail in environments with disturbances like noise and perturbations, making robustness critical.", "method": "The study proposes RobustVLA, incorporating Jacobian and smoothness regularizations, for stability against environmental uncertainties.", "result": "RobustVLA significantly improves reliability and robustness compared to prior methods in diverse robotic settings.", "conclusion": "Principled robustness-aware RL post-training is essential for enhancing the reliability of Vision-Language-Action models."}}
{"id": "2511.00129", "pdf": "https://arxiv.org/pdf/2511.00129", "abs": "https://arxiv.org/abs/2511.00129", "authors": ["Siyu Xiao", "Xindi Zhao", "Tianhao Mao", "Yiwei Wang", "Yuqiao Chen", "Hongyun Zhang", "Jian Wang", "Junjie Wang", "Shuang Liu", "Tupei Chen", "Yang Liu"], "title": "Casing Collar Identification using AlexNet-based Neural Networks for Depth Measurement in Oil and Gas Wells", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "Accurate downhole depth measurement is essential for oil and gas well\noperations, directly influencing reservoir contact, production efficiency, and\noperational safety. Collar correlation using a casing collar locator (CCL) is\nfundamental for precise depth calibration. While neural network-based CCL\nsignal recognition has achieved significant progress in collar identification,\npreprocessing methods for such applications remain underdeveloped. Moreover,\nthe limited availability of real well data poses substantial challenges for\ntraining neural network models that require extensive datasets. This paper\npresents a system integrated into downhole tools for CCL signal acquisition to\nfacilitate dataset construction. We propose comprehensive preprocessing methods\nfor data augmentation and evaluate their effectiveness using our AlexNet-based\nneural network models. Through systematic experimentation across various\nconfiguration combinations, we analyze the contribution of each augmentation\nmethod. Results demonstrate that standardization, label distribution smoothing\n(LDS), and random cropping are fundamental requirements for model training,\nwhile label smoothing regularization (LSR), time scaling, and multiple sampling\nsignificantly enhance model generalization capability. The F1 scores of our two\nbenchmark models trained with the proposed augmentation methods maximumly\nimprove from 0.937 and 0.952 to 1.0 and 1.0, respectively. Performance\nvalidation on real CCL waveforms confirms the effectiveness and practical\napplicability of our approach. This work addresses the gaps in data\naugmentation methodologies for training casing collar recognition models in CCL\ndata-limited environments.", "AI": {"tldr": "This paper proposes novel data preprocessing methods to improve neural network-based casing collar locator (CCL) signal recognition, enhancing depth measurement accuracy in oil and gas operations.", "motivation": "Existing neural network methods for CCL signal recognition lack advanced preprocessing techniques and struggle with limited real well data for training, which hinders model development and performance.", "method": "The study introduces data preprocessing methods such as standardization, label distribution smoothing, and data augmentation techniques. These methods are combined with AlexNet-based neural network models and evaluated through systematic experiments.", "result": "The preprocessing methods increased the F1 scores of two benchmark models from 0.937 and 0.952 to a perfect 1.0. Effective generalization was verified using real CCL waveform data, confirming the practical applicability of the approach.", "conclusion": "The proposed methods effectively address the challenges of limited CCL data and improve neural network model performance, demonstrating their suitability for real-world downhole operations."}}
{"id": "2511.00362", "pdf": "https://arxiv.org/pdf/2511.00362", "abs": "https://arxiv.org/abs/2511.00362", "authors": ["Momen Khandoker Ope", "Akif Islam", "Mohd Ruhul Ameen", "Abu Saleh Musa Miah", "Md Rashedul Islam", "Jungpil Shin"], "title": "Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "6 Pages, 4 figures, 2 Tables, Submitted to ICECTE 2026", "summary": "Cultural heritage restoration in Bangladesh faces a dual challenge of limited\nresources and scarce technical expertise. Traditional 3D digitization methods,\nsuch as photogrammetry or LiDAR scanning, require expensive hardware, expert\noperators, and extensive on-site access, which are often infeasible in\ndeveloping contexts. As a result, many of Bangladesh's architectural treasures,\nfrom the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to\ndecay and inaccessible in digital form. This paper introduces Oitijjo-3D, a\ncost-free generative AI framework that democratizes 3D cultural preservation.\nBy using publicly available Google Street View imagery, Oitijjo-3D reconstructs\nfaithful 3D models of heritage structures through a two-stage pipeline -\nmultimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture\nsynthesis, and neural image-to-3D generation through Hexagen for geometry\nrecovery. The system produces photorealistic, metrically coherent\nreconstructions in seconds, achieving significant speedups compared to\nconventional Structure-from-Motion pipelines, without requiring any specialized\nhardware or expert supervision. Experiments on landmarks such as Ahsan Manzil,\nChoto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both\nvisual and structural fidelity while drastically lowering economic and\ntechnical barriers. By turning open imagery into digital heritage, this work\nreframes preservation as a community-driven, AI-assisted act of cultural\ncontinuity for resource-limited nations.", "AI": {"tldr": "Cultural heritage restoration in Bangladesh is enhanced using Oitijjo-3D, a free AI framework leveraging Google Street View imagery to generate 3D models quickly, affordably, and accurately.", "motivation": "Traditional 3D digitization methods are costly, require expertise, and extensive access, making preservation infeasible for developing nations like Bangladesh, where architectural monuments remain at risk.", "method": "Oitijjo-3D utilizes generative AI with a two-stage pipeline: Gemini 2.5 Flash Image for texture synthesis, and Hexagen for geometry recovery, based on publicly available imagery.", "result": "Experiments on Bangladeshi landmarks show Oitijjo-3D produces visually and structurally coherent reconstructions, significantly reducing costs and expertise requirements.", "conclusion": "Oitijjo-3D positions cultural preservation as a community-driven, cost-effective, and AI-assisted effort, offering a scalable solution in resource-limited settings."}}
{"id": "2511.01375", "pdf": "https://arxiv.org/pdf/2511.01375", "abs": "https://arxiv.org/abs/2511.01375", "authors": ["Hamin Koo", "Minseon Kim", "Jaehyung Kim"], "title": "Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM Judges", "categories": ["cs.AI"], "comment": "under review, 28 pages", "summary": "Identifying the vulnerabilities of large language models (LLMs) is crucial\nfor improving their safety by addressing inherent weaknesses. Jailbreaks, in\nwhich adversaries bypass safeguards with crafted input prompts, play a central\nrole in red-teaming by probing LLMs to elicit unintended or unsafe behaviors.\nRecent optimization-based jailbreak approaches iteratively refine attack\nprompts by leveraging LLMs. However, they often rely heavily on either binary\nattack success rate (ASR) signals, which are sparse, or manually crafted\nscoring templates, which introduce human bias and uncertainty in the scoring\noutcomes. To address these limitations, we introduce AMIS (Align to MISalign),\na meta-optimization framework that jointly evolves jailbreak prompts and\nscoring templates through a bi-level structure. In the inner loop, prompts are\nrefined using fine-grained and dense feedback using a fixed scoring template.\nIn the outer loop, the template is optimized using an ASR alignment score,\ngradually evolving to better reflect true attack outcomes across queries. This\nco-optimization process yields progressively stronger jailbreak prompts and\nmore calibrated scoring signals. Evaluations on AdvBench and JBB-Behaviors\ndemonstrate that AMIS achieves state-of-the-art performance, including 88.0%\nASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, outperforming\nexisting baselines by substantial margins.", "AI": {"tldr": "The paper introduces AMIS, a framework to optimize jailbreak prompts for LLMs by refining attack prompts and scoring templates to achieve higher success rates in exploiting model vulnerabilities.", "motivation": "To improve the safety of LLMs by identifying vulnerabilities and bypassing their safeguards for red-teaming purposes.", "method": "AMIS employs a bi-level meta-optimization framework where prompts are refined iteratively with dense feedback, and scoring templates evolve gradually to align better with attack outcomes.", "result": "Evaluations demonstrate state-of-the-art attack success rates, achieving 88% on Claude-3.5-Haiku and 100% on Claude-4-Sonnet, surpassing existing baselines.", "conclusion": "AMIS is highly effective in strengthening jailbreak prompts and refining scoring templates, enhancing the exploration of LLM vulnerabilities."}}
{"id": "2511.01046", "pdf": "https://arxiv.org/pdf/2511.01046", "abs": "https://arxiv.org/abs/2511.01046", "authors": ["Vedant Acharya", "Abhay Pisharodi", "Rishabh Mondal", "Mohammad Rafiuddin", "Nipun Batra"], "title": "VayuChat: An LLM-Powered Conversational Interface for Air Quality Data Analytics", "categories": ["cs.CL"], "comment": "4 Pages, 4 Figures", "summary": "Air pollution causes about 1.6 million premature deaths each year in India,\nyet decision makers struggle to turn dispersed data into decisions. Existing\ntools require expertise and provide static dashboards, leaving key policy\nquestions unresolved. We present VayuChat, a conversational system that answers\nnatural language questions on air quality, meteorology, and policy programs,\nand responds with both executable Python code and interactive visualizations.\nVayuChat integrates data from Central Pollution Control Board (CPCB) monitoring\nstations, state-level demographics, and National Clean Air Programme (NCAP)\nfunding records into a unified interface powered by large language models. Our\nlive demonstration will show how users can perform complex environmental\nanalytics through simple conversations, making data science accessible to\npolicymakers, researchers, and citizens. The platform is publicly deployed at\nhttps://huggingface.co/spaces/SustainabilityLabIITGN/ VayuChat. For further\ninformation check out video uploaded on\nhttps://www.youtube.com/watch?v=d6rklL05cs4.", "AI": {"tldr": "VayuChat is an AI tool that uses conversational interfaces and visualizations to simplify air quality and environmental analytics, integrating multiple datasets for accessible insights.", "motivation": "Decision-makers struggle to utilize dispersed air pollution data effectively due to the complexity and static nature of current tools.", "method": "VayuChat combines conversational AI powered by large language models, integrating air quality, meteorological, demographic, and NCAP funding data into one platform.", "result": "The platform allows users, including policymakers, researchers, and citizens, to analyze environmental data through easy, natural language questions paired with dynamic visual outputs.", "conclusion": "VayuChat democratizes access to actionable air quality insights, promoting informed decisions through its intuitive and interactive system."}}
{"id": "2511.01334", "pdf": "https://arxiv.org/pdf/2511.01334", "abs": "https://arxiv.org/abs/2511.01334", "authors": ["Ling Niu", "Xiaoji Zheng", "Han Wang", "Chen Zheng", "Ziyuan Yang", "Bokui Chen", "Jiangtao Gong"], "title": "Embodied Cognition Augmented End2End Autonomous Driving", "categories": ["cs.RO", "cs.AI", "cs.HC", "68T45"], "comment": "24 pages,4 pages", "summary": "In recent years, vision-based end-to-end autonomous driving has emerged as a\nnew paradigm. However, popular end-to-end approaches typically rely on visual\nfeature extraction networks trained under label supervision. This limited\nsupervision framework restricts the generality and applicability of driving\nmodels. In this paper, we propose a novel paradigm termed $E^{3}AD$, which\nadvocates for comparative learning between visual feature extraction networks\nand the general EEG large model, in order to learn latent human driving\ncognition for enhancing end-to-end planning. In this work, we collected a\ncognitive dataset for the mentioned contrastive learning process. Subsequently,\nwe investigated the methods and potential mechanisms for enhancing end-to-end\nplanning with human driving cognition, using popular driving models as\nbaselines on publicly available autonomous driving datasets. Both open-loop and\nclosed-loop tests are conducted for a comprehensive evaluation of planning\nperformance. Experimental results demonstrate that the $E^{3}AD$ paradigm\nsignificantly enhances the end-to-end planning performance of baseline models.\nAblation studies further validate the contribution of driving cognition and the\neffectiveness of comparative learning process. To the best of our knowledge,\nthis is the first work to integrate human driving cognition for improving\nend-to-end autonomous driving planning. It represents an initial attempt to\nincorporate embodied cognitive data into end-to-end autonomous driving,\nproviding valuable insights for future brain-inspired autonomous driving\nsystems. Our code will be made available at Github", "AI": {"tldr": "The paper introduces a novel paradigm, $E^{3}AD$, which integrates human driving cognition into end-to-end autonomous driving models via comparative learning to enhance planning performance.", "motivation": "Existing end-to-end autonomous driving models depend heavily on visual feature extraction networks trained with supervised labels, limiting their effectiveness and generalizability.", "method": "The authors propose using a comparative learning approach between visual feature networks and an EEG model, leveraging a collected cognitive dataset to integrate human driving cognition into autonomous driving models.", "result": "Experimental results show that $E^{3}AD$ significantly improves planning performance over baseline models, validated in open and closed-loop tests. Ablation studies confirm the contribution of driving cognition and the comparative learning approach.", "conclusion": "This work is pioneering in integrating human driving cognition into autonomous driving systems, representing a significant step toward brain-inspired autonomous driving and enhancing end-to-end planning performance."}}
{"id": "2511.00130", "pdf": "https://arxiv.org/pdf/2511.00130", "abs": "https://arxiv.org/abs/2511.00130", "authors": ["Bernd Bohnet", "Rumen Dangovski", "Kevin Swersky", "Sherry Moore", "Arslan Chaudhry", "Kathleen Kenealy", "Noah Fiedel"], "title": "A Comparative Analysis of LLM Adaptation: SFT, LoRA, and ICL in Data-Scarce Scenarios", "categories": ["cs.LG"], "comment": null, "summary": "The remarkable capabilities of Large Language Models (LLMs) often need to be\ntailored for specific applications, requiring the integration of new knowledge\nor the acquisition of new skills. While full fine-tuning is a powerful\nadaptation method, it is computationally expensive and can lead to a\ndegradation of general reasoning abilities, a phenomenon known as catastrophic\nforgetting. A range of alternative techniques exists, each with its own\ntrade-offs. In-Context Learning (ICL) is fast but limited by context length,\nwhile Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation\n(LoRA) offer a middle ground by minimizing parameter changes. However, the\nchallenge of catastrophic forgetting persists, raising questions about the best\nadaptation strategy for a given task. This paper presents a comparative\nanalysis of Supervised Finetuning (SFT), LoRA, and ICL in data-scarce\nscenarios. We find that LoRA provides the most effective balance, successfully\ninstilling new skills with minimal impact on the base model's general\nknowledge. In contrast, while SFT excels at skill acquisition, it is highly\nsusceptible to catastrophic forgetting. ICL is effective for incorporating\nfactual knowledge but struggles with complex skills. Our findings offer a\npractical framework for selecting an LLM adaptation strategy. We highlight the\ncritical distinction between skill acquisition and knowledge integration,\nclarify the trade-offs between task-specific performance and the preservation\nof general capabilities.", "AI": {"tldr": "Large Language Models (LLMs) require tailored adaptation for specific tasks, and this paper compares key methods like Supervised Finetuning (SFT), LoRA, and In-Context Learning (ICL). LoRA finds balance by effectively instilling new skills while minimizing base model degradation.", "motivation": "Motivated by the need to adapt LLMs to specific tasks without compromising their general capabilities, particularly in data-scarce situations.", "method": "The paper conducts a comparative analysis of three adaptation strategies: Supervised Finetuning (SFT), LoRA, and In-Context Learning (ICL), assessing their impact on skill acquisition and knowledge integration.", "result": "LoRA emerges as the most balanced method, achieving effective skill acquisition with minimal impact on general knowledge. SFT excels in skill acquisition but suffers from catastrophic forgetting, while ICL works well for factual knowledge but is less effective for complex skills.", "conclusion": "The findings provide a pragmatic framework for choosing the appropriate LLM adaptation method, balancing task-specific performance against preserving general reasoning capabilities."}}
{"id": "2511.00370", "pdf": "https://arxiv.org/pdf/2511.00370", "abs": "https://arxiv.org/abs/2511.00370", "authors": ["Chaochen Wu", "Guan Luo", "Meiyun Zuo", "Zhitao Fan"], "title": "Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video moment retrieval uses a text query to locate a moment from a given\nuntrimmed video reference. Locating corresponding video moments with text\nqueries helps people interact with videos efficiently. Current solutions for\nthis task have not considered conflict within location results from different\nmodels, so various models cannot integrate correctly to produce better results.\nThis study introduces a reinforcement learning-based video moment retrieval\nmodel that can scan the whole video once to find the moment's boundary while\nproducing its locational evidence. Moreover, we proposed a multi-agent system\nframework that can use evidential learning to resolve conflicts between agents'\nlocalization output. As a side product of observing and dealing with conflicts\nbetween agents, we can decide whether a query has no corresponding moment in a\nvideo (out-of-scope) without additional training, which is suitable for\nreal-world applications. Extensive experiments on benchmark datasets show the\neffectiveness of our proposed methods compared with state-of-the-art\napproaches. Furthermore, the results of our study reveal that modeling\ncompetition and conflict of the multi-agent system is an effective way to\nimprove RL performance in moment retrieval and show the new role of evidential\nlearning in the multi-agent framework.", "AI": {"tldr": "This paper proposes a reinforcement learning-based approach and a multi-agent framework to improve video moment retrieval using text queries while managing conflicts among models and identifying irrelevant queries.", "motivation": "Current methods for video moment retrieval fail to address conflicts among model outputs, limiting their effectiveness in accurately matching text queries to video moments efficiently.", "method": "The authors introduced a reinforcement learning model to scan videos for moments boundaries and a multi-agent evidential learning framework to resolve conflicts in localization outputs.", "result": "Experimental results on benchmark datasets demonstrate superior performance of the proposed methods compared to state-of-the-art approaches.", "conclusion": "The study shows that resolving multi-agent conflicts through evidential learning is effective in enhancing reinforcement learning for moment retrieval and highlights its applicability in real-world scenarios."}}
{"id": "2511.01396", "pdf": "https://arxiv.org/pdf/2511.01396", "abs": "https://arxiv.org/abs/2511.01396", "authors": ["Cl\u00e9ment Yvernes", "Emilie Devijver", "Ad\u00e8le H. Ribeiro", "Marianne Clausel--Lesourd", "\u00c9ric Gaussier"], "title": "Relaxing partition admissibility in Cluster-DAGs: a causal calculus with arbitrary variable clustering", "categories": ["cs.AI", "stat.ME"], "comment": "Accepted at The Thirty-ninth Annual Conference on Neural Information\n  Processing Systems (NeurIPS2025)", "summary": "Cluster DAGs (C-DAGs) provide an abstraction of causal graphs in which nodes\nrepresent clusters of variables, and edges encode both cluster-level causal\nrelationships and dependencies arisen from unobserved confounding. C-DAGs\ndefine an equivalence class of acyclic causal graphs that agree on\ncluster-level relationships, enabling causal reasoning at a higher level of\nabstraction. However, when the chosen clustering induces cycles in the\nresulting C-DAG, the partition is deemed inadmissible under conventional C-DAG\nsemantics. In this work, we extend the C-DAG framework to support arbitrary\nvariable clusterings by relaxing the partition admissibility constraint,\nthereby allowing cyclic C-DAG representations. We extend the notions of\nd-separation and causal calculus to this setting, significantly broadening the\nscope of causal reasoning across clusters and enabling the application of\nC-DAGs in previously intractable scenarios. Our calculus is both sound and\natomically complete with respect to the do-calculus: all valid interventional\nqueries at the cluster level can be derived using our rules, each corresponding\nto a primitive do-calculus step.", "AI": {"tldr": "This paper extends Cluster DAGs to handle cyclic representations, enabling causal reasoning with arbitrary variable clustering.", "motivation": "Cluster DAGs traditionally require acyclic representations, limiting their application in cases where clusters induce cycles. Extending them addresses current limitations in causal reasoning.", "method": "The authors relaxed the partition admissibility constraints to support cyclic C-DAGs and expanded d-separation and causal calculus notions for this broader scope.", "result": "The proposed system allows cluster-level causal reasoning in cyclic scenarios with a sound and atomically complete calculus, aligning with do-calculus.", "conclusion": "The extension broadens causal reasoning applicability, making C-DAGs useful even in structurally complex setups with cycles and arbitrary clustering."}}
{"id": "2511.01053", "pdf": "https://arxiv.org/pdf/2511.01053", "abs": "https://arxiv.org/abs/2511.01053", "authors": ["Qing Ding", "Eric Hua Qing Zhang", "Felix Jozsa", "Julia Ive"], "title": "Building a Silver-Standard Dataset from NICE Guidelines for Clinical LLMs", "categories": ["cs.CL"], "comment": "Submitted to EFMI Medical Informatics Europe 2026", "summary": "Large language models (LLMs) are increasingly used in healthcare, yet\nstandardised benchmarks for evaluating guideline-based clinical reasoning are\nmissing. This study introduces a validated dataset derived from publicly\navailable guidelines across multiple diagnoses. The dataset was created with\nthe help of GPT and contains realistic patient scenarios, as well as clinical\nquestions. We benchmark a range of recent popular LLMs to showcase the validity\nof our dataset. The framework supports systematic evaluation of LLMs' clinical\nutility and guideline adherence.", "AI": {"tldr": "The paper introduces a validated dataset to evaluate large language models (LLMs) on guideline-based clinical reasoning in healthcare.", "motivation": "To address the lack of standardized benchmarks for assessing LLMs' capability in guideline-based clinical reasoning within healthcare.", "method": "The authors created a dataset derived from publicly available clinical guidelines, featuring realistic patient scenarios and clinical questions, with the assistance of GPT.", "result": "The dataset was used to benchmark popular recent LLMs, demonstrating its validity and utility for evaluation purposes.", "conclusion": "The dataset and framework facilitate systematic evaluation of LLMs in terms of clinical utility and adherence to clinical guidelines."}}
{"id": "2511.00407", "pdf": "https://arxiv.org/pdf/2511.00407", "abs": "https://arxiv.org/abs/2511.00407", "authors": ["\u0141ukasz Sikorski", "Jacek Matulewski"], "title": "Reducing students' misconceptions about video game development. A mixed-method study", "categories": ["cs.HC", "cs.CY", "cs.SE"], "comment": null, "summary": "This study examines students' na\\\"ive mindset (misconceptions) about video\ngame development, idealized and inaccurate beliefs that shape an unrealistic\nunderstanding of the field. The research evaluated the effectiveness of a\nfifteen-hour-long lecture series delivered by industry professionals, designed\nto challenge this mindset and expose students to the complexities and realities\nof game production. A mixed-methods approach was employed, combining\nqualitative analysis with a prototype quantitative tool developed to measure\nlevels of misconception. Participants included students (n = 91) from diverse\nacademic backgrounds interested in game creation and professionals (n = 94)\nworking in the video game industry. Findings show that the intervention\nsignificantly reduced students' na\\\"ive beliefs while enhancing their\nmotivation to pursue careers in the industry. Exposure to professional\nperspectives fostered a more realistic and informed mindset, taking into\naccount the understanding of the technical, collaborative, and business aspects\nof game development. The results suggest that incorporating similar expert-led\ninterventions early in game development education can improve learning\noutcomes, support informed career choices, and mitigate future professional\ndisappointment.", "AI": {"tldr": "This study addresses students' misconceptions about video game development through a lecture series by industry professionals, significantly improving their understanding and career motivation.", "motivation": "Students often enter game development with unrealistic beliefs about the field, which can hinder their learning experiences and future career satisfaction.", "method": "A mixed-methods approach, including qualitative analysis and a novel quantitative tool, was used to evaluate the impact of a 15-hour professional lecture series on 91 students and 94 industry professionals.", "result": "The intervention reduced misconceptions and enhanced student motivation by presenting a realistic view of the technical, collaborative, and business aspects of game development.", "conclusion": "Incorporating industry-expert-led interventions in early education can better prepare students for the realities of game development, improving learning and supporting informed career decisions."}}
{"id": "2511.01346", "pdf": "https://arxiv.org/pdf/2511.01346", "abs": "https://arxiv.org/abs/2511.01346", "authors": ["Shun Yoshida", "Qingchuan Song", "Bastian E. Rapp", "Thomas Speck", "Falk J. Tauber"], "title": "Thermo-responsive closing and reopening artificial Venus Flytrap utilizing shape memory elastomers", "categories": ["cs.RO", "physics.bio-ph"], "comment": "Conference Proceedings Paper Living Machines 2025", "summary": "Despite their often perceived static and slow nature, some plants can move\nfaster than the blink of an eye. The rapid snap closure motion of the Venus\nflytrap (Dionaea muscipula) has long captivated the interest of researchers and\nengineers alike, serving as a model for plant-inspired soft machines and\nrobots. The translation of the fast snapping closure has inspired the\ndevelopment of various artificial Venus flytrap (AVF) systems. However,\ntranslating both the closing and reopening motion of D. muscipula into an\nautonomous plant inspired soft machine has yet to be achieved. In this study,\nwe present an AVF that autonomously closes and reopens, utilizing novel\nthermo-responsive UV-curable shape memory materials for soft robotic systems.\nThe life-sized thermo-responsive AVF exhibits closing and reopening motions\ntriggered in a naturally occurring temperature range. The doubly curved trap\nlobes, built from shape memory polymers, close at 38{\\deg}C, while reopening\ninitiates around 45{\\deg}C, employing shape memory elastomer strips as\nantagonistic actuators to facilitate lobe reopening. This work represents the\nfirst demonstration of thermo-responsive closing and reopening in an AVF with\nprogrammed sequential motion in response to increasing temperature. This\napproach marks the next step toward autonomously bidirectional moving soft\nmachines/robots.", "AI": {"tldr": "The paper discusses developing a bio-inspired artificial Venus flytrap (AVF) that autonomously closes and reopens using thermo-responsive materials.", "motivation": "The motivation is to create a fully functional, plant-inspired, autonomous soft machine mimicking the Venus flytrap's fast snapping closure and reopening motion.", "method": "The researchers designed a life-sized AVF using UV-curable thermo-responsive shape memory materials. Shape memory polymers enable the closing action at 38\u00b0C, while shape memory elastomer strips enable the reopening action at 45\u00b0C.", "result": "The AVF successfully demonstrated the closing and reopening behaviors, showing sequential movements triggered by temperature changes in a natural range.", "conclusion": "This study achieved the first bidirectional, thermo-responsive AVF, advancing the field of autonomous soft robotic systems inspired by biological mechanisms."}}
{"id": "2511.00133", "pdf": "https://arxiv.org/pdf/2511.00133", "abs": "https://arxiv.org/abs/2511.00133", "authors": ["Kowshik Balasubramanian", "Andre Williams", "Ismail Butun"], "title": "Feature Importance Guided Random Forest Learning with Simulated Annealing Based Hyperparameter Tuning", "categories": ["cs.LG", "cs.AI"], "comment": "10 pages, 2 figures, 3 tables, submitted to IEEE Intelligent Systems\n  journal", "summary": "This paper introduces a novel framework for enhancing Random Forest\nclassifiers by integrating probabilistic feature sampling and hyperparameter\ntuning via Simulated Annealing. The proposed framework exhibits substantial\nadvancements in predictive accuracy and generalization, adeptly tackling the\nmultifaceted challenges of robust classification across diverse domains,\nincluding credit risk evaluation, anomaly detection in IoT ecosystems,\nearly-stage medical diagnostics, and high-dimensional biological data analysis.\nTo overcome the limitations of conventional Random Forests, we present an\napproach that places stronger emphasis on capturing the most relevant signals\nfrom data while enabling adaptive hyperparameter configuration. The model is\nguided towards features that contribute more meaningfully to classification and\noptimizing this with dynamic parameter tuning. The results demonstrate\nconsistent accuracy improvements and meaningful insights into feature\nrelevance, showcasing the efficacy of combining importance aware sampling and\nmetaheuristic optimization.", "AI": {"tldr": "This paper presents a new framework for improving Random Forest classifiers by combining probabilistic feature sampling with hyperparameter tuning using Simulated Annealing. Significant advancements in accuracy and generalization are achieved across various domains.", "motivation": "The paper seeks to address the challenges and limitations of conventional Random Forests by enhancing their ability to identify relevant signals from data and optimize hyperparameters adaptively.", "method": "The proposed method integrates probabilistic feature sampling with dynamic hyperparameter configuration using Simulated Annealing to enhance the classification and generalization abilities of Random Forests.", "result": "The results demonstrate improved accuracy, generalization, and insightful identification of feature relevance for robust classification across multiple domains.", "conclusion": "The framework successfully combines importance-aware feature selection with metaheuristic optimization, leading to consistent improvements in predictive performance and providing deeper insights into feature contributions."}}
{"id": "2511.00381", "pdf": "https://arxiv.org/pdf/2511.00381", "abs": "https://arxiv.org/abs/2511.00381", "authors": ["Jiaming Li", "Junlei Wu", "Sheng Wang", "Honglin Xiong", "Jiangdong Cai", "Zihao Zhao", "Yitao Zhu", "Yuan Yin", "Dinggang Shen", "Qian Wang"], "title": "VisionCAD: An Integration-Free Radiology Copilot Framework", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Widespread clinical deployment of computer-aided diagnosis (CAD) systems is\nhindered by the challenge of integrating with existing hospital IT\ninfrastructure. Here, we introduce VisionCAD, a vision-based radiological\nassistance framework that circumvents this barrier by capturing medical images\ndirectly from displays using a camera system. The framework operates through an\nautomated pipeline that detects, restores, and analyzes on-screen medical\nimages, transforming camera-captured visual data into diagnostic-quality images\nsuitable for automated analysis and report generation. We validated VisionCAD\nacross diverse medical imaging datasets, demonstrating that our modular\narchitecture can flexibly utilize state-of-the-art diagnostic models for\nspecific tasks. The system achieves diagnostic performance comparable to\nconventional CAD systems operating on original digital images, with an F1-score\ndegradation typically less than 2\\% across classification tasks, while natural\nlanguage generation metrics for automated reports remain within 1\\% of those\nderived from original images. By requiring only a camera device and standard\ncomputing resources, VisionCAD offers an accessible approach for AI-assisted\ndiagnosis, enabling the deployment of diagnostic capabilities in diverse\nclinical settings without modifications to existing infrastructure.", "AI": {"tldr": "VisionCAD introduces a system using cameras to capture and process medical images for AI-assisted diagnosis without needing hospital IT infrastructure changes.", "motivation": "The motivation is to eliminate barriers to CAD systems' deployment by bypassing integration with hospital IT systems.", "method": "The method involves capturing medical images with cameras, then using an automated pipeline to analyze and restore these images for diagnostic use.", "result": "VisionCAD achieves comparable performance to traditional CAD systems with minimal degradation in diagnostic quality or reporting accuracy.", "conclusion": "VisionCAD offers a practical and accessible solution for integrating AI diagnostics into various clinical settings without altering existing systems."}}
{"id": "2511.01415", "pdf": "https://arxiv.org/pdf/2511.01415", "abs": "https://arxiv.org/abs/2511.01415", "authors": ["Amrapali Pednekar", "\u00c1lvaro Garrido-P\u00e9rez", "Yara Khaluf", "Pieter Simoens"], "title": "Modulation of temporal decision-making in a deep reinforcement learning agent under the dual-task paradigm", "categories": ["cs.AI"], "comment": "Accepted at CogInterp workshop @ NeurIPS 2025", "summary": "This study explores the interference in temporal processing within a\ndual-task paradigm from an artificial intelligence (AI) perspective. In this\ncontext, the dual-task setup is implemented as a simplified version of the\nOvercooked environment with two variations, single task (T) and dual task\n(T+N). Both variations involve an embedded time production task, but the dual\ntask (T+N) additionally involves a concurrent number comparison task. Two deep\nreinforcement learning (DRL) agents were separately trained for each of these\ntasks. These agents exhibited emergent behavior consistent with human timing\nresearch. Specifically, the dual task (T+N) agent exhibited significant\noverproduction of time relative to its single task (T) counterpart. This result\nwas consistent across four target durations. Preliminary analysis of neural\ndynamics in the agents' LSTM layers did not reveal any clear evidence of a\ndedicated or intrinsic timer. Hence, further investigation is needed to better\nunderstand the underlying time-keeping mechanisms of the agents and to provide\ninsights into the observed behavioral patterns. This study is a small step\ntowards exploring parallels between emergent DRL behavior and behavior observed\nin biological systems in order to facilitate a better understanding of both.", "AI": {"tldr": "The paper examines how AI agents process time in a dual-task system, showcasing parallels with human timing research.", "motivation": "To explore the parallels between AI emergent behavior in dual-task scenarios and human timing behavior.", "method": "Deep reinforcement learning (DRL) agents were trained separately on single and dual-task environments based on a simplified version of the Overcooked environment. Their performance and neural dynamics were analyzed.", "result": "The dual-task agents exhibited significant overproduction of time compared to single-task agents, with no clear evidence of a dedicated timer in the agents' neural dynamics.", "conclusion": "The study highlights behavioral similarities between AI agents and humans in time processing, calling for further investigations into underlying mechanisms in the agents."}}
{"id": "2511.01066", "pdf": "https://arxiv.org/pdf/2511.01066", "abs": "https://arxiv.org/abs/2511.01066", "authors": ["Stephan Oepen", "Nikolay Arefev", "Mikko Aulamo", "Marta Ba\u00f1\u00f3n", "Maja Buljan", "Laurie Burchell", "Lucas Charpentier", "Pinzhen Chen", "Mariya Fedorova", "Ona de Gibert", "Barry Haddow", "Jan Haji\u010d", "Jindri\u010d Helcl", "Andrey Kutuzov", "Zihao Li", "Risto Luukkonen", "Bhavitvya Malik", "Vladislav Mikhailov", "Amanda Myntti", "Dayy\u00e1n O'Brien", "Lucie Pol\u00e1kov\u00e1", "Sampo Pyysalo", "Gema Ram\u00edrez S\u00e1nchez", "Janine Siewert", "Pavel Stepachev", "J\u00f6rg Tiedemann", "Teemu Vahtola", "Fedor Vitiugin", "Tea Vojt\u011bchov\u00e1", "Jaume Zaragoza"], "title": "HPLT~3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono- and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models", "categories": ["cs.CL"], "comment": null, "summary": "We present an ongoing initiative to provide open, very large, high-quality,\nand richly annotated textual datasets for almost 200 languages. At 30 trillion\ntokens, this is likely the largest generally available multilingual collection\nof LLM pre-training data. At 30 trillion tokens, this is likely the largest\ngenerally available multilingual collection of LLM pre-training data. These\ndatasets are derived from web crawls from different sources and accompanied\nwith a complete, open-source pipeline for document selection from web archives,\ntext extraction from HTML, language identification for noisy texts, exact and\nnear-deduplication, annotation with, among others, register labels, text\nquality estimates, and personally identifiable information; and final selection\nand filtering. We report on data quality probes through contrastive and\nanalytical statistics, through manual inspection of samples for 24 languages,\nand through end-to-end evaluation of various language model architectures\ntrained on this data. For multilingual LLM evaluation, we provide a\ncomprehensive collection of benchmarks for nine European languages, with\nspecial emphasis on natively created tasks, mechanisms to mitigate prompt\nsensitivity, and refined normalization and aggregation of scores. Additionally,\nwe train and evaluate a family of 57 monolingual encoder-decoder models, as\nwell as a handful of monolingual GPT-like reference models. Besides the\nmonolingual data and models, we also present a very large collection of\nparallel texts automatically mined from this data, together with a novel\nparallel corpus synthesized via machine translation.", "AI": {"tldr": "The initiative aims to create the largest open multilingual dataset for LLM pre-training, consisting of 30 trillion tokens for nearly 200 languages, along with multilingual benchmarks, monolingual models, and parallel texts.", "motivation": "To provide an open, extensive, and high-quality multilingual dataset for pre-training large language models and to improve resources for various languages worldwide.", "method": "The dataset is derived from web crawls using an open-source pipeline for document selection, text extraction, deduplication, and annotation. This is complemented by manual and statistical data quality checks, benchmarks, and training of LLM models.", "result": "The study offers high-quality multilingual datasets, comprehensive evaluation benchmarks for 9 languages, 57 monolingual models, multilingual LLM evaluation, and a collection of parallel texts.", "conclusion": "This work contributes significantly to multilingual LLM development, making datasets and benchmarks more accessible, and advancing the field through novel research and tools."}}
{"id": "2511.00628", "pdf": "https://arxiv.org/pdf/2511.00628", "abs": "https://arxiv.org/abs/2511.00628", "authors": ["Yang Li", "Siqi Ping", "Xiyu Chen", "Xiaojian Qi", "Zigan Wang", "Ye Luo", "Xiaowei Zhang"], "title": "AgentGit: A Version Control Framework for Reliable and Scalable LLM-Powered Multi-Agent Systems", "categories": ["cs.MA", "cs.AI", "cs.SE"], "comment": null, "summary": "With the rapid progress of large language models (LLMs), LLM-powered\nmulti-agent systems (MAS) are drawing increasing interest across academia and\nindustry. However, many current MAS frameworks struggle with reliability and\nscalability, especially on complex tasks. We present AgentGit, a framework that\nbrings Git-like rollback and branching to MAS workflows. Built as an\ninfrastructure layer on top of LangGraph, AgentGit supports state commit,\nrevert, and branching, allowing agents to traverse, compare, and explore\nmultiple trajectories efficiently. To evaluate AgentGit, we designed an\nexperiment that optimizes target agents by selecting better prompts. We ran a\nmulti-step A/B test against three baselines -- LangGraph, AutoGen, and Agno --\non a real-world task: retrieving and analyzing paper abstracts. Results show\nthat AgentGit significantly reduces redundant computation, lowers runtime and\ntoken usage, and supports parallel exploration across multiple branches,\nenhancing both reliability and scalability in MAS development. This work offers\na practical path to more robust MAS design and enables error recovery, safe\nexploration, iterative debugging, and A/B testing in collaborative AI systems.", "AI": {"tldr": "AgentGit introduces Git-like rollback and branching capabilities to multi-agent systems (MAS) to enhance reliability, scalability, and exploration in complex tasks.", "motivation": "The paper addresses the limitations of current MAS frameworks which struggle with reliability and scalability on complex tasks.", "method": "AgentGit was developed as an infrastructure layer over LangGraph, incorporating Git-like features such as state commit, revert, and branching for efficient multi-agent workflows. The system was evaluated using a multi-step A/B test against three baselines on a real-world task.", "result": "AgentGit outperformed LangGraph, AutoGen, and Agno in reducing redundant computation, lowering runtime and token usage, and enabling parallel exploration.", "conclusion": "AgentGit offers a pathway to more robust MAS development, facilitating error recovery, iterative debugging, A/B testing, and safe exploratory experimentation."}}
{"id": "2511.01347", "pdf": "https://arxiv.org/pdf/2511.01347", "abs": "https://arxiv.org/abs/2511.01347", "authors": ["Riddhi Das", "Joscha Teichmann", "Thomas Speck", "Falk J. Tauber"], "title": "Design and development of an electronics-free earthworm robot", "categories": ["cs.RO"], "comment": "Conference Proceedings Paper Living Machines 2025", "summary": "Soft robotic systems have gained widespread attention due to their inherent\nflexibility, adaptability, and safety, making them well-suited for varied\napplications. Among bioinspired designs, earthworm locomotion has been\nextensively studied for its efficient peristaltic motion, enabling movement in\nconfined and unstructured environments. Existing earthworm-inspired robots\nprimarily utilize pneumatic actuation due to its high force-to-weight ratio and\nease of implementation. However, these systems often rely on bulky,\npower-intensive electronic control units, limiting their practicality. In this\nwork, we present an electronics-free, earthworm-inspired pneumatic robot\nutilizing a modified Pneumatic Logic Gate (PLG) design. By integrating\npreconfigured PLG units with bellow actuators, we achieved a plug-and-play\nstyle modular system capable of peristaltic locomotion without external\nelectronic components. The proposed design reduces system complexity while\nmaintaining efficient actuation. We characterize the bellow actuators under\ndifferent operating conditions and evaluate the robots locomotion performance.\nOur findings demonstrate that the modified PLG-based control system effectively\ngenerates peristaltic wave propagation, achieving autonomous motion with\nminimal deviation. This study serves as a proof of concept for the development\nof electronics-free, peristaltic soft robots. The proposed system has potential\nfor applications in hazardous environments, where untethered, adaptable\nlocomotion is critical. Future work will focus on further optimizing the robot\ndesign and exploring untethered operation using onboard compressed air sources.", "AI": {"tldr": "Earthworm-inspired soft robotic systems using electronics-free pneumatic actuation achieved peristaltic locomotion via modified Pneumatic Logic Gate design.", "motivation": "Soft robots, inspired by earthworm motion, offer advantages in efficiency, adaptability, and use in confined spaces, yet they often rely on bulky electronic controls.", "method": "The study introduces a modular system with plug-and-play bellow actuators and modified Pneumatic Logic Gates to achieve autonomous peristaltic locomotion without electronics.", "result": "The designed system enables effective, electronics-free peristaltic propagation with minimal deviation validated by actuator characterization and locomotion performance tests.", "conclusion": "The proof-of-concept demonstrates potential for soft robots in hazardous environments, introducing efficient and untethered operation with reduced system complexity."}}
{"id": "2511.00134", "pdf": "https://arxiv.org/pdf/2511.00134", "abs": "https://arxiv.org/abs/2511.00134", "authors": ["Angana Borah", "Adrija Datta", "Ashish S. Kumar", "Raviraj Dave", "Udit Bhatia"], "title": "Physiologically Active Vegetation Reverses Its Cooling Effect in Humid Urban Climates", "categories": ["cs.LG"], "comment": "27 pages, 5 figures", "summary": "Efforts to green cities for cooling are succeeding unevenly because the same\nvegetation that cools surfaces can also intensify how hot the air feels.\nPrevious studies have identified humid heat as a growing urban hazard, yet how\nphysiologically active vegetation governs this trade-off between cooling and\nmoisture accumulation remains poorly understood, leaving mitigation policy and\ndesign largely unguided. Here we quantify how vegetation structure and function\ninfluence the Heat Index (HI), a combined measure of temperature and humidity\nin 138 Indian cities spanning tropical savanna, semi-arid steppe, and humid\nsubtropical climates, and across dense urban cores and semi-urban rings. Using\nan extreme-aware, one kilometre reconstruction of HI and an interpretable\nmachine-learning framework that integrates SHapley Additive Explanations (SHAP)\nand Accumulated Local Effects (ALE), we isolate vegetation-climate\ninteractions. Cooling generally strengthens for EVI >= 0.4 and LAI >= 0.05, but\njoint-high regimes begin to reverse toward warming when EVI >= 0.5, LAI >= 0.2,\nand fPAR >= 0.5,with an earlier onset for fPAR >= 0.25 in humid, dense cores.\nIn such environments, highly physiologically active vegetation elevates\nnear-surface humidity faster than it removes heat, reversing its cooling effect\nand amplifying perceived heat stress. These findings establish the climatic\nlimits of vegetation-driven cooling and provide quantitative thresholds for\nclimate-specific greening strategies that promote equitable and heat-resilient\ncities.", "AI": {"tldr": "Vegetation cools surfaces but increases perceived heat stress due to humidity, with thresholds identified for urban greening strategies in Indian cities.", "motivation": "Urban greening efforts face challenges as vegetation cools surfaces but can increase humid heat stress, creating a need to understand vegetation's trade-off between cooling and moisture accumulation.", "method": "The study uses a one-kilometre HI reconstruction and a machine-learning framework integrating SHAP and ALE to analyze the relationship between vegetation and heat stress in 138 Indian cities across varying climates and urban densities.", "result": "The study finds cooling benefits for vegetation with specific structural thresholds (EVI >= 0.4, LAI >= 0.05), but highly active vegetation reverses cooling effects at higher thresholds (EVI >= 0.5, LAI >= 0.2, fPAR >= 0.5), particularly in humid dense urban cores.", "conclusion": "There are climatic limits to vegetation's cooling impact, and the study provides thresholds to inform climate-specific and equitable urban greening strategies for heat resilience."}}
{"id": "2511.00389", "pdf": "https://arxiv.org/pdf/2511.00389", "abs": "https://arxiv.org/abs/2511.00389", "authors": ["Fan Zhang", "Haoxuan Li", "Shengju Qian", "Xin Wang", "Zheng Lian", "Hao Wu", "Zhihong Zhu", "Yuan Gao", "Qiankun Li", "Yefeng Zheng", "Zhouchen Lin", "Pheng-Ann Heng"], "title": "Rethinking Facial Expression Recognition in the Era of Multimodal Large Language Models: Benchmark, Datasets, and Beyond", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have revolutionized numerous\nresearch fields, including computer vision and affective computing. As a\npivotal challenge in this interdisciplinary domain, facial expression\nrecognition (FER) has evolved from separate, domain-specific models to more\nunified approaches. One promising avenue to unify FER tasks is converting\nconventional FER datasets into visual question-answering (VQA) formats,\nenabling the direct application of powerful generalist MLLMs for inference.\nHowever, despite the success of cutting-edge MLLMs in various tasks, their\nperformance on FER tasks remains largely unexplored. To address this gap, we\nprovide FERBench, a systematic benchmark that incorporates 20 state-of-the-art\nMLLMs across four widely used FER datasets. Our results reveal that, while\nMLLMs exhibit good classification performance, they still face significant\nlimitations in reasoning and interpretability. To this end, we introduce\npost-training strategies aimed at enhancing the facial expression reasoning\ncapabilities of MLLMs. Specifically, we curate two high-quality and large-scale\ndatasets: UniFER-CoT-230K for cold-start initialization and UniFER-RLVR-360K\nfor reinforcement learning with verifiable rewards (RLVR), respectively.\nBuilding upon them, we develop a unified and interpretable FER foundation model\ntermed UniFER-7B, which outperforms many open-sourced and closed-source\ngeneralist MLLMs (e.g., Gemini-2.5-Pro and Qwen2.5-VL-72B).", "AI": {"tldr": "This paper introduces a benchmark (FERBench) to evaluate multimodal large language models (MLLMs) on facial expression recognition (FER) tasks, develops post-training strategies, and proposes a new FER foundation model, UniFER-7B.", "motivation": "The motivation is to address the lack of exploration into the performance of multimodal large language models (MLLMs) on facial expression recognition (FER) tasks and to improve reasoning and interpretability in these models.", "method": "The authors converted FER datasets into a visual question-answering (VQA) format, benchmarked 20 state-of-the-art MLLMs on FER datasets, curated two datasets (UniFER-CoT-230K and UniFER-RLVR-360K) for model improvement, and developed UniFER-7B, a unified interpretable model.", "result": "The study shows that while MLLMs perform well in classification, they struggle with reasoning and interpretability. Their UniFER-7B model, however, outperforms other generalist MLLMs in FER tasks.", "conclusion": "MLLMs still have significant limitations in facial expression reasoning and interpretability, and the proposed strategies and the UniFER-7B model demonstrate advancements in addressing these issues."}}
{"id": "2511.01425", "pdf": "https://arxiv.org/pdf/2511.01425", "abs": "https://arxiv.org/abs/2511.01425", "authors": ["Yuhang Huang", "Zekai Lin", "Fan Zhong", "Lei Liu"], "title": "Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis", "categories": ["cs.AI", "cs.CV", "I.2.6; I.2.10"], "comment": "12 pages, 3 figures. Under review at the Conference on Computer\n  Vision and Pattern Recognition (CVPR) 2026", "summary": "Explanations for AI models in high-stakes domains like medicine often lack\nverifiability, which can hinder trust. To address this, we propose an\ninteractive agent that produces explanations through an auditable sequence of\nactions. The agent learns a policy to strategically seek external visual\nevidence to support its diagnostic reasoning. This policy is optimized using\nreinforcement learning, resulting in a model that is both efficient and\ngeneralizable. Our experiments show that this action-based reasoning process\nsignificantly improves calibrated accuracy, reducing the Brier score by 18\\%\ncompared to a non-interactive baseline. To validate the faithfulness of the\nagent's explanations, we introduce a causal intervention method. By masking the\nvisual evidence the agent chooses to use, we observe a measurable degradation\nin its performance ($\\Delta$Brier=+0.029), confirming that the evidence is\nintegral to its decision-making process. Our work provides a practical\nframework for building AI systems with verifiable and faithful reasoning\ncapabilities.", "AI": {"tldr": "The paper proposes an AI explanation model for high-stakes domains, emphasizing verifiability, with improved diagnostic accuracy through reinforcement learning and auditable visual evidence.", "motivation": "To improve trust in AI models used in critical domains like medicine by providing verifiable and transparent explanations for their decisions.", "method": "An interactive agent is created to generate explanations using auditable actions. It employs reinforcement learning to optimize a policy for selecting external visual evidence that supports diagnostic reasoning. A causal intervention technique validates explanation faithfulness.", "result": "The model achieves significant improvement in calibrated accuracy, reducing the Brier score by 18%. Causal intervention confirms that chosen evidence is crucial, as performance degrades when the evidence is masked ($\\Delta$Brier=+0.029).", "conclusion": "This work establishes a practical framework for designing AI systems with verifiable and trustworthy reasoning processes."}}
{"id": "2511.01090", "pdf": "https://arxiv.org/pdf/2511.01090", "abs": "https://arxiv.org/abs/2511.01090", "authors": ["Vlad Negoita", "Mihai Masala", "Traian Rebedea"], "title": "Improving Romanian LLM Pretraining Data using Diversity and Quality Filtering", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have recently exploded in popularity, often\nmatching or outperforming human abilities on many tasks. One of the key factors\nin training LLMs is the availability and curation of high-quality data. Data\nquality is especially crucial for under-represented languages, where\nhigh-quality corpora are scarce. In this work we study the characteristics and\ncoverage of Romanian pretraining corpora and we examine how they differ from\nEnglish data. By training a lightweight multitask model on carefully\nLLM-annotated Romanian texts, we are able to analyze and perform multi-level\nfiltering (e.g., educational value, topic, format) to generate high-quality\npretraining datasets. Our experiments show noteworthy trends in the topics\npresent in Romanian and English data, while also proving the effectiveness of\nfiltering data through improved LLM pretraining performance across multiple\nbenchmarks.", "AI": {"tldr": "The paper investigates Romanian-language datasets, compares them to English data, and introduces a lightweight multitask model for filtering datasets to enhance LLM pretraining.", "motivation": "The motivation is addressing the scarcity of high-quality corpora for under-represented languages like Romanian and improving their LLM training performance.", "method": "They trained a multitask model on Romanian texts annotated by LLMs and used multi-level filtering techniques (educational value, topic, format) to curate high-quality datasets.", "result": "The study identifies notable trends in Romanian and English data topics and demonstrates that filtering improves pretraining performance on benchmarks.", "conclusion": "Filtering annotated Romanian texts enhances dataset quality and LLM pretraining results, showing the importance of curated corpora for under-represented languages."}}
{"id": "2511.01166", "pdf": "https://arxiv.org/pdf/2511.01166", "abs": "https://arxiv.org/abs/2511.01166", "authors": ["Lingzhe Zhang", "Yunpeng Zhai", "Tong Jia", "Chiming Duan", "Minghua He", "Leyi Pan", "Zhaoyang Liu", "Bolin Ding", "Ying Li"], "title": "MicroRemed: Benchmarking LLMs in Microservices Remediation", "categories": ["cs.CL", "cs.SE", "68T50", "I.2.7"], "comment": "24 pages, 13 figures, 5 tables", "summary": "Large Language Models (LLMs) integrated with agent-based reasoning frameworks\nhave recently shown strong potential for autonomous decision-making and\nsystem-level operations. One promising yet underexplored direction is\nmicroservice remediation, where the goal is to automatically recover faulty\nmicroservice systems. Existing approaches, however, still rely on human-crafted\nprompts from Site Reliability Engineers (SREs), with LLMs merely converting\ntextual instructions into executable code. To advance research in this area, we\nintroduce MicroRemed, the first benchmark for evaluating LLMs in end-to-end\nmicroservice remediation, where models must directly generate executable\nAnsible playbooks from diagnosis reports to restore system functionality. We\nfurther propose ThinkRemed, a multi-agent framework that emulates the\nreflective and perceptive reasoning of SREs. Experimental results show that\nMicroRemed presents substantial challenges to current LLMs, while ThinkRemed\nimproves end-to-end remediation performance through iterative reasoning and\nsystem reflection. The benchmark is available at\nhttps://github.com/LLM4AIOps/MicroRemed.", "AI": {"tldr": "The paper introduces MicroRemed, a benchmark for microservice system recovery using LLMs, and proposes ThinkRemed, a multi-agent framework enhancing restoration performance.", "motivation": "To address the challenge of automating remediation of faulty microservice systems without requiring human-crafted prompts from Site Reliability Engineers.", "method": "Developing MicroRemed benchmark for evaluating LLMs and proposing ThinkRemed framework, which utilizes iterative reasoning and system reflection for remediation.", "result": "MicroRemed revealed significant challenges for current LLMs, while ThinkRemed demonstrated improved performance in autonomous microservice remediation.", "conclusion": "The integration of reflective reasoning frameworks like ThinkRemed shows promise in enhancing the autonomy and effectiveness of LLM-based microservice remediation systems."}}
{"id": "2511.01350", "pdf": "https://arxiv.org/pdf/2511.01350", "abs": "https://arxiv.org/abs/2511.01350", "authors": ["Maartje H. M. Wermelink", "Renate Sachse", "Sebastian Kruppert", "Thomas Speck", "Falk J. Tauber"], "title": "Model to Model: Understanding the Venus Flytrap Snapping Mechanism and Transferring it to a 3D-printed Bistable Soft Robotic Demonstrator", "categories": ["cs.RO"], "comment": "Conference Proceedings Paper Living machines 2025", "summary": "The Venus flytrap (Dionaea muscipula) does not only serve as the textbook\nmodel for a carnivorous plant, but also has long intrigued both botanists and\nengineers with its rapidly closing leaf trap. The trap closure is triggered by\ntwo consecutive touches of a potential prey, after which the lobes rapidly\nswitch from their concave open-state to their convex close-state and catch the\nprey within 100-500 ms after being triggered. This transformation from concave\nto convex is initiated by changes in turgor pressure and the release of stored\nelastic energy from prestresses in the concave state, which accelerate this\nmovement, leading to inversion of the lobes bi-axial curvature. Possessing two\nlow-energy states, the leaves can be characterized as bistable systems. With\nour research, we seek to deepen the understanding of Venus flytrap motion\nmechanics and apply its principles to the design of an artificial bistable lobe\nactuator. We identified geometrical characteristics, such as dimensional ratios\nand the thickness gradient in the lobe, and transferred these to two 3D-printed\nbistable actuator models. One actuator parallels the simulated geometry of a\nVenus flytrap leaf, the other is a lobe model designed with CAD. Both models\ndisplay concave-convex bi-stability and snap close. These demonstrators are the\nfirst step in the development of an artificial Venus flytrap that mimics the\nmechanical behavior of the biological model and can be used as a soft fast\ngripper.", "AI": {"tldr": "This paper studies the rapid closing mechanism of the Venus flytrap and applies its bistable mechanics to create artificial actuators.", "motivation": "To understand the motion mechanics of the Venus flytrap and utilize its principles to design artificial actuators.", "method": "The researchers examined the Venus flytrap's geometrical characteristics and replicated these in 3D-printed bistable actuator models.", "result": "The 3D-printed actuators demonstrated bistability and rapid snapping, mimicking the Venus flytrap's behavior.", "conclusion": "The study successfully developed soft actuators inspired by the Venus flytrap, paving the way for applications such as fast grippers."}}
{"id": "2511.00136", "pdf": "https://arxiv.org/pdf/2511.00136", "abs": "https://arxiv.org/abs/2511.00136", "authors": ["Qing Guo", "Xinhang Li", "Junyu Chen", "Zheng Guo", "Xiaocong Li", "Lin Zhang", "Lei Li"], "title": "A Dual Large Language Models Architecture with Herald Guided Prompts for Parallel Fine Grained Traffic Signal Control", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Leveraging large language models (LLMs) in traffic signal control (TSC)\nimproves optimization efficiency and interpretability compared to traditional\nreinforcement learning (RL) methods. However, existing LLM-based approaches are\nlimited by fixed time signal durations and are prone to hallucination errors,\nwhile RL methods lack robustness in signal timing decisions and suffer from\npoor generalization. To address these challenges, this paper proposes\nHeraldLight, a dual LLMs architecture enhanced by Herald guided prompts. The\nHerald Module extracts contextual information and forecasts queue lengths for\neach traffic phase based on real-time conditions. The first LLM, LLM-Agent,\nuses these forecasts to make fine grained traffic signal control, while the\nsecond LLM, LLM-Critic, refines LLM-Agent's outputs, correcting errors and\nhallucinations. These refined outputs are used for score-based fine-tuning to\nimprove accuracy and robustness. Simulation experiments using CityFlow on real\nworld datasets covering 224 intersections in Jinan (12), Hangzhou (16), and New\nYork (196) demonstrate that HeraldLight outperforms state of the art baselines,\nachieving a 20.03% reduction in average travel time across all scenarios and a\n10.74% reduction in average queue length on the Jinan and Hangzhou scenarios.\nThe source code is available on GitHub:\nhttps://github.com/BUPT-ANTlab/HeraldLight.", "AI": {"tldr": "HeraldLight enhances traffic signal control using a dual LLM architecture, achieving reduced travel times and queue lengths compared to existing methods.", "motivation": "Optimize traffic signal control leveraging LLMs to address limitations of RL methods and fixed signal durations while improving interpretability and accuracy.", "method": "Introduced HeraldLight, with a dual LLM architecture: LLM-Agent and LLM-Critic, combined with Herald-guided prompts for accuracy and robustness.", "result": "Simulation results show HeraldLight reduced average travel time by 20.03% and queue length by 10.74% compared to baseline methods.", "conclusion": "HeraldLight demonstrates the effectiveness of combining LLMs for addressing TSC inefficiencies, outperforming state-of-the-art baselines and improving traffic conditions significantly."}}
{"id": "2511.00391", "pdf": "https://arxiv.org/pdf/2511.00391", "abs": "https://arxiv.org/abs/2511.00391", "authors": ["Xuanle Zhao", "Deyang Jiang", "Zhixiong Zeng", "Lei Chen", "Haibo Qiu", "Jing Huang", "Yufeng Zhong", "Liming Zheng", "Yilin Cao", "Lin Ma"], "title": "VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning", "categories": ["cs.CV"], "comment": "Preprint Version, Work in Progress", "summary": "Multimodal code generation has garnered significant interest within the\nresearch community. Despite the notable success of recent vision-language\nmodels (VLMs) on specialized tasks like Chart-to-code generation, their\nreliance on single-task training regimens fosters a narrow paradigm that\nhinders the development of generalized \\textbf{VI}sio\\textbf{N} \\textbf{C}ode\n\\textbf{I}ntelligence. In this work, we introduce \\textbf{VinciCoder}, a\nunified multimodal code generation model that addresses this limitation via a\ntwo-stage training framework. We begin by constructing a large-scale Supervised\nFinetuning (SFT) corpus comprising 1.6M image-code pairs for tasks involving\ndirect code generation and visual-based code refinement. Subsequently, we\nintroduce a Visual Reinforcement Learning (ViRL) strategy, which employs a\ncoarse-to-fine reward mechanism to improve visual fidelity by calculating\nvisual similarity across local and global image patches. Extensive experiments\non various multimodal code generation benchmarks demonstrate that VinciCoder\nachieves state-of-the-art performance, underscoring the effectiveness of our\ncoarse-to-fine ViRL strategy. The code and model will be available at\nhttps://github.com/DocTron-hub/VinciCoder.", "AI": {"tldr": "VinciCoder is a multimodal code generation model that uses supervised finetuning and visual reinforcement learning to achieve state-of-the-art results in generating and refining code from visual inputs.", "motivation": "The paper aims to overcome the limitations of single-task training regimens in vision-language models, which restrict their ability to generalize for tasks involving multimodal code generation.", "method": "A two-stage training framework is introduced, consisting of Large-Scale Supervised Finetuning (SFT) with 1.6M image-code pairs and a Visual Reinforcement Learning (ViRL) strategy that uses coarse-to-fine rewards.", "result": "Experimental results highlight VinciCoder's state-of-the-art performance on various multimodal code generation benchmarks, showcasing the effectiveness of its training strategies.", "conclusion": "The proposed framework, VinciCoder, successfully addresses the narrow scope of prior methods and sets a new benchmark in multimodal code generation, with code and model availability for further research."}}
{"id": "2511.01444", "pdf": "https://arxiv.org/pdf/2511.01444", "abs": "https://arxiv.org/abs/2511.01444", "authors": ["Huiting Huang", "Tieliang Gong", "Kai He", "Jialun Wu", "Erik Cambria", "Mengling Feng"], "title": "Robust Multimodal Sentiment Analysis via Double Information Bottleneck", "categories": ["cs.AI"], "comment": null, "summary": "Multimodal sentiment analysis has received significant attention across\ndiverse research domains. Despite advancements in algorithm design, existing\napproaches suffer from two critical limitations: insufficient learning of\nnoise-contaminated unimodal data, leading to corrupted cross-modal\ninteractions, and inadequate fusion of multimodal representations, resulting in\ndiscarding discriminative unimodal information while retaining multimodal\nredundant information. To address these challenges, this paper proposes a\nDouble Information Bottleneck (DIB) strategy to obtain a powerful, unified\ncompact multimodal representation. Implemented within the framework of low-rank\nRenyi's entropy functional, DIB offers enhanced robustness against diverse\nnoise sources and computational tractability for high-dimensional data, as\ncompared to the conventional Shannon entropy-based methods. The DIB comprises\ntwo key modules: 1) learning a sufficient and compressed representation of\nindividual unimodal data by maximizing the task-relevant information and\ndiscarding the superfluous information, and 2) ensuring the discriminative\nability of multimodal representation through a novel attention bottleneck\nfusion mechanism. Consequently, DIB yields a multimodal representation that\neffectively filters out noisy information from unimodal data while capturing\ninter-modal complementarity. Extensive experiments on CMU-MOSI, CMU-MOSEI,\nCH-SIMS, and MVSA-Single validate the effectiveness of our method. The model\nachieves 47.4% accuracy under the Acc-7 metric on CMU-MOSI and 81.63% F1-score\non CH-SIMS, outperforming the second-best baseline by 1.19%. Under noise, it\nshows only 0.36% and 0.29% performance degradation on CMU-MOSI and CMU-MOSEI\nrespectively.", "AI": {"tldr": "This paper addresses key limitations in multimodal sentiment analysis by proposing a Double Information Bottleneck (DIB) strategy for robust and efficient representation learning.", "motivation": "Existing multimodal sentiment analysis approaches are limited by noise-contaminated data and inadequate fusion strategies, leading to loss of discriminative unimodal information and retention of redundant multimodal information.", "method": "The Double Information Bottleneck (DIB) strategy uses low-rank Renyi's entropy to enhance representation learning, combining unimodal compression and a novel attention bottleneck fusion mechanism to achieve a robust multimodal representation.", "result": "Experiments show improvement in accuracy and F1-score across datasets such as CMU-MOSI and CH-SIMS, with reduced performance degradation under noise conditions, outperforming existing baselines.", "conclusion": "The proposed DIB effectively filters noise and captures inter-modal complementarity, demonstrating superior robustness and accuracy in multimodal sentiment analysis tasks."}}
{"id": "2511.01101", "pdf": "https://arxiv.org/pdf/2511.01101", "abs": "https://arxiv.org/abs/2511.01101", "authors": ["Marek Strong", "Andreas Vlachos"], "title": "TSVer: A Benchmark for Fact Verification Against Time-Series Evidence", "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025", "summary": "Reasoning over temporal and numerical data, such as time series, is a crucial\naspect of fact-checking. While many systems have recently been developed to\nhandle this form of evidence, their evaluation remains limited by existing\ndatasets, which often lack structured evidence, provide insufficient\njustifications for verdicts, or rely on synthetic claims. In this paper, we\nintroduce TSVer, a new benchmark dataset for fact verification focusing on\ntemporal and numerical reasoning with time-series evidence. TSVer contains 287\nreal-world claims sourced from 38 fact-checking organizations and a curated\ndatabase of 400 time series covering diverse domains. Each claim is annotated\nwith time frames across all pertinent time series, along with a verdict and\njustifications reflecting how the evidence is used to reach the verdict. Using\nan LLM-assisted multi-step annotation process, we improve the quality of our\nannotations and achieve an inter-annotator agreement of kappa=0.745 on\nverdicts. We also develop a baseline for verifying claims against time-series\nevidence and show that even the state-of-the-art reasoning models like\nGemini-2.5-Pro are challenged by time series, achieving a 63.37 accuracy score\non verdicts and an Ev2R score of 48.63 on verdict justifications.", "AI": {"tldr": "The paper introduces TSVer, a dataset designed for verifying factual claims by reasoning over temporal and numerical data, providing insights into challenges faced by state-of-the-art models.", "motivation": "Existing datasets for fact-checking struggle with structured evidence, both in terms of quality and justification, focusing heavily on synthetic claims. This limits the evaluation of systems handling temporal and numerical reasoning data.", "method": "The authors curated TSVer by sourcing 287 real-world claims from 38 fact-checking organizations and combining them with a database of 400 diverse time series. Annotation was enhanced using an LLM-assisted multi-step process to ensure high quality with an inter-annotator agreement of kappa=0.745.", "result": "TSVer challenges state-of-the-art reasoning models (like Gemini-2.5-Pro), which still struggle with this dataset, achieving modest accuracy and justifications scores (63.37 accuracy and 48.63 Ev2R).", "conclusion": "TSVer serves as a critical benchmark for advancing models in temporal and numerical reasoning, demonstrating gaps in current model capabilities and motivating future research."}}
{"id": "2511.01180", "pdf": "https://arxiv.org/pdf/2511.01180", "abs": "https://arxiv.org/abs/2511.01180", "authors": ["Jingyi Shi", "Yufeng Chen", "Yang Xiao", "Yuekang Li", "Zhengzi Xu", "Sihao Qiu", "Chi Zhang", "Keyu Qi", "Yeting Li", "Xingchu Chen", "Yanyan Zou", "Yang Liu", "Wei Huo"], "title": "A Large Scale Study of AI-based Binary Function Similarity Detection Techniques for Security Researchers and Practitioners", "categories": ["cs.CR", "cs.SE"], "comment": "Accepted by ASE 2025", "summary": "Binary Function Similarity Detection (BFSD) is a foundational technique in\nsoftware security, underpinning a wide range of applications including\nvulnerability detection, malware analysis. Recent advances in AI-based BFSD\ntools have led to significant performance improvements. However, existing\nevaluations of these tools suffer from three key limitations: a lack of\nin-depth analysis of performance-influencing factors, an absence of realistic\napplication analysis, and reliance on small-scale or low-quality datasets.\n  In this paper, we present the first large-scale empirical study of AI-based\nBFSD tools to address these gaps. We construct two high-quality and diverse\ndatasets: BinAtlas, comprising 12,453 binaries and over 7 million functions for\ncapability evaluation; and BinAres, containing 12,291 binaries and 54\nreal-world 1-day vulnerabilities for evaluating vulnerability detection\nperformance in practical IoT firmware settings. Using these datasets, we\nevaluate nine representative BFSD tools, analyze the challenges and limitations\nof existing BFSD tools, and investigate the consistency among BFSD tools. We\nalso propose an actionable strategy for combining BFSD tools to enhance overall\nperformance (an improvement of 13.4%). Our study not only advances the\npractical adoption of BFSD tools but also provides valuable resources and\ninsights to guide future research in scalable and automated binary similarity\ndetection.", "AI": {"tldr": "The paper reviews and empirically tests AI-based BFSD tools using newly curated datasets to assess performance and practical applicability.", "motivation": "Address challenges in existing evaluations of AI-based BFSD tools, such as limited datasets and analysis of performance factors.", "method": "Constructed high-quality datasets (BinAtlas and BinAres) and evaluated nine BFSD tools to identify challenges, compare consistency, and propose a performance-enhancing strategy.", "result": "Evaluation showcased current challenges and inconsistencies among tools; combining tools offered a 13.4% performance improvement.", "conclusion": "The study enhances understanding and practical adoption of BFSD tools, while setting a foundation for future research and resource development in this area."}}
{"id": "2511.01369", "pdf": "https://arxiv.org/pdf/2511.01369", "abs": "https://arxiv.org/abs/2511.01369", "authors": ["Luis Diener", "Jens Kalkkuhl", "Markus Enzweiler"], "title": "Lateral Velocity Model for Vehicle Parking Applications", "categories": ["cs.RO"], "comment": "This manuscript has been submitted to Vehicle System Dynamics for\n  possible publication", "summary": "Automated parking requires accurate localization for quick and precise\nmaneuvering in tight spaces. While the longitudinal velocity can be measured\nusing wheel encoders, the estimation of the lateral velocity remains a key\nchallenge due to the absence of dedicated sensors in consumer-grade vehicles.\nExisting approaches often rely on simplified vehicle models, such as the\nzero-slip model, which assumes no lateral velocity at the rear axle. It is well\nestablished that this assumption does not hold during low-speed driving and\nresearchers thus introduce additional heuristics to account for differences. In\nthis work, we analyze real-world data from parking scenarios and identify a\nsystematic deviation from the zero-slip assumption. We provide explanations for\nthe observed effects and then propose a lateral velocity model that better\ncaptures the lateral dynamics of the vehicle during parking. The model improves\nestimation accuracy, while relying on only two parameters, making it\nwell-suited for integration into consumer-grade applications.", "AI": {"tldr": "The paper addresses challenges in accurate lateral velocity estimation for automated parking, presenting a new model that improves accuracy using minimal parameters.", "motivation": "Precise localization is vital for automated parking in tight spaces, yet lateral velocity estimation lacks dedicated sensors in consumer-grade vehicles.", "method": "Analyzed real-world parking data, identified deviations in existing zero-slip models, and proposed a simplified lateral velocity model relying on two parameters.", "result": "The proposed model demonstrated improved lateral velocity estimation accuracy during parking scenarios.", "conclusion": "The model enhances automated parking performance in consumer vehicles, ensuring better lateral dynamics representation with simple implementation."}}
{"id": "2511.00166", "pdf": "https://arxiv.org/pdf/2511.00166", "abs": "https://arxiv.org/abs/2511.00166", "authors": ["Shiman Zhang", "Jinghan Zhou", "Zhoufan Yu", "Ningai Leng"], "title": "Study on Supply Chain Finance Decision-Making Model and Enterprise Economic Performance Prediction Based on Deep Reinforcement Learning", "categories": ["cs.LG"], "comment": "9 pages, 3 figures", "summary": "To improve decision-making and planning efficiency in back-end centralized\nredundant supply chains, this paper proposes a decision model integrating deep\nlearning with intelligent particle swarm optimization. A distributed node\ndeployment model and optimal planning path are constructed for the supply chain\nnetwork. Deep learning such as convolutional neural networks extracts features\nfrom historical data, and linear programming captures high-order statistical\nfeatures. The model is optimized using fuzzy association rule scheduling and\ndeep reinforcement learning, while neural networks fit dynamic changes. A\nhybrid mechanism of \"deep learning feature extraction - intelligent particle\nswarm optimization\" guides global optimization and selects optimal decisions\nfor adaptive control. Simulations show reduced resource consumption, enhanced\nspatial planning, and in dynamic environments improved real-time decision\nadjustment, distribution path optimization, and robust intelligent control.", "AI": {"tldr": "The paper combines deep learning and intelligent particle swarm optimization to improve decision-making in supply chains.", "motivation": "Address inefficiencies in centralized redundant supply chains by enhancing decision-making and resource planning.", "method": "Integrates deep learning, particle swarm optimization, linear programming, and hybrid mechanisms for feature extraction and optimization in supply chain networks.", "result": "Simulations demonstrate reduced resource use, improved spatial planning, enhanced real-time decision adjustment, and robust dynamic control.", "conclusion": "The proposed model successfully optimizes both resource allocation and decision-making efficiency in supply chains."}}
{"id": "2511.00396", "pdf": "https://arxiv.org/pdf/2511.00396", "abs": "https://arxiv.org/abs/2511.00396", "authors": ["Long Li", "Shuichen Ji", "Ziyang Luo", "Nian Liu", "Dingwen Zhang", "Junwei Han"], "title": "CoT-Saliency: Unified Chain-of-Thought Reasoning for Heterogeneous Saliency Tasks", "categories": ["cs.CV"], "comment": "14 pages,10 figures", "summary": "We present the first unified framework that jointly handles three\noperationally heterogeneous saliency tasks, eg, SOD, CoSOD, and SIS, by casting\neach as a Chain-of-Thought (CoT) reasoning process in a Vision-Language Model\n(VLM) to bridge task heterogeneity. CoT training follows a two-stage paradigm:\nSupervised Fine-Tuning (SFT) and Reinforcement Learning (RL). To enhance CoT\nquality in RL, we propose Confidence-Guided Policy Optimization (CGPO), a\nlightweight single-sample algorithm that leverages the discrepancy between\nreward and model confidence as a per-sample advantage signal. This design\nnaturally focuses updates on informative responses while eliminating group\nsampling, thereby addressing GRPO's key limitations: confidence-agnostic\nlearning, signal dilution, and prohibitive computational overhead. We also\nintroduce an \"output-to-reasoning\" strategy to construct high-fidelity SFT data\nthat ensures logical consistency with ground-truth masks. Experiments show our\nmodel matches or outperforms specialized SOTA methods and strong closed-source\nVLMs across all tasks, especially achieving an S-measure of 0.899 on CoCA for\nCoSOD, surpassing the prior best by 8.0 percentage points, despite using far\nless training data.", "AI": {"tldr": "The paper presents a unified framework utilizing Vision-Language Models (VLMs) to handle three different saliency tasks (SOD, CoSOD, SIS) by employing Chain-of-Thought reasoning and introducing innovative methods like Confidence-Guided Policy Optimization (CGPO).", "motivation": "There is a need to address heterogeneity across three distinct saliency tasks effectively and unify their operational processes within a robust framework.", "method": "The framework uses a two-stage Chain-of-Thought training paradigm: Supervised Fine-Tuning (SFT) for logical consistency and Reinforcement Learning (RL) with the novel CGPO algorithm to optimize the model's confidence and reward-based learning process.", "result": "The proposed model achieved state-of-the-art performance across saliency tasks, with a notable improvement in the S-measure for CoSOD, outperforming previous models by a significant margin despite utilizing less training data.", "conclusion": "This unified framework efficiently bridges task heterogeneity in saliency tasks, improves model performance, and demonstrates the potential of leveraging Vision-Language Models combined with advanced training techniques."}}
{"id": "2511.01445", "pdf": "https://arxiv.org/pdf/2511.01445", "abs": "https://arxiv.org/abs/2511.01445", "authors": ["ChengZhang Yu", "YingRu He", "Hongyan Cheng", "nuo Cheng", "Zhixing Liu", "Dongxu Mu", "Zhangrui Shen", "Zhanpeng Jin"], "title": "From Passive to Proactive: A Multi-Agent System with Dynamic Task Orchestration for Intelligent Medical Pre-Consultation", "categories": ["cs.AI"], "comment": "14pages, 7 figures, 7 tables", "summary": "Global healthcare systems face critical challenges from increasing patient\nvolumes and limited consultation times, with primary care visits averaging\nunder 5 minutes in many countries. While pre-consultation processes\nencompassing triage and structured history-taking offer potential solutions,\nthey remain limited by passive interaction paradigms and context management\nchallenges in existing AI systems. This study introduces a hierarchical\nmulti-agent framework that transforms passive medical AI systems into proactive\ninquiry agents through autonomous task orchestration. We developed an\neight-agent architecture with centralized control mechanisms that decomposes\npre-consultation into four primary tasks: Triage ($T_1$), History of Present\nIllness collection ($T_2$), Past History collection ($T_3$), and Chief\nComplaint generation ($T_4$), with $T_1$--$T_3$ further divided into 13\ndomain-specific subtasks. Evaluated on 1,372 validated electronic health\nrecords from a Chinese medical platform across multiple foundation models\n(GPT-OSS 20B, Qwen3-8B, Phi4-14B), the framework achieved 87.0% accuracy for\nprimary department triage and 80.5% for secondary department classification,\nwith task completion rates reaching 98.2% using agent-driven scheduling versus\n93.1% with sequential processing. Clinical quality scores from 18 physicians\naveraged 4.56 for Chief Complaints, 4.48 for History of Present Illness, and\n4.69 for Past History on a 5-point scale, with consultations completed within\n12.7 rounds for $T_2$ and 16.9 rounds for $T_3$. The model-agnostic\narchitecture maintained high performance across different foundation models\nwhile preserving data privacy through local deployment, demonstrating the\npotential for autonomous AI systems to enhance pre-consultation efficiency and\nquality in clinical settings.", "AI": {"tldr": "A hierarchical multi-agent AI framework for medical pre-consultation tasks improved triage and case history collection in clinical settings.", "motivation": "The study addresses critical challenges facing global healthcare systems, such as increasing patient volumes, limited consultation times, and the need for efficient pre-consultation processes.", "method": "The study implemented an eight-agent hierarchical framework with centralized control to handle four primary pre-consultation tasks (triage, history of present illness, past medical history, and chief complaint generation). It used 1,372 validated medical records and tested across multiple AI models.", "result": "The framework achieved high accuracy in triage (87.0% for primary classification and 80.5% for secondary classification) and received high clinical quality scores from physicians. It improved task completion rates (98.2% using autonomous agent scheduling) and preserved data privacy with local deployment.", "conclusion": "The model-agnostic AI framework demonstrates potential for transforming healthcare pre-consultation processes, offering enhanced efficiency, accuracy, and privacy-preserving solutions in clinical workflows."}}
{"id": "2511.01379", "pdf": "https://arxiv.org/pdf/2511.01379", "abs": "https://arxiv.org/abs/2511.01379", "authors": ["Kun Hu", "Menggang Li", "Zhiwen Jin", "Chaoquan Tang", "Eryi Hu", "Gongbo Zhou"], "title": "CM-LIUW-Odometry: Robust and High-Precision LiDAR-Inertial-UWB-Wheel Odometry for Extreme Degradation Coal Mine Tunnels", "categories": ["cs.RO"], "comment": "Accepted by IROS 2025", "summary": "Simultaneous Localization and Mapping (SLAM) in large-scale, complex, and\nGPS-denied underground coal mine environments presents significant challenges.\nSensors must contend with abnormal operating conditions: GPS unavailability\nimpedes scene reconstruction and absolute geographic referencing, uneven or\nslippery terrain degrades wheel odometer accuracy, and long, feature-poor\ntunnels reduce LiDAR effectiveness. To address these issues, we propose\nCoalMine-LiDAR-IMU-UWB-Wheel-Odometry (CM-LIUW-Odometry), a multimodal SLAM\nframework based on the Iterated Error-State Kalman Filter (IESKF). First,\nLiDAR-inertial odometry is tightly fused with UWB absolute positioning\nconstraints to align the SLAM system with a global coordinate. Next, wheel\nodometer is integrated through tight coupling, enhanced by nonholonomic\nconstraints (NHC) and vehicle lever arm compensation, to address performance\ndegradation in areas beyond UWB measurement range. Finally, an adaptive motion\nmode switching mechanism dynamically adjusts the robot's motion mode based on\nUWB measurement range and environmental degradation levels. Experimental\nresults validate that our method achieves superior accuracy and robustness in\nreal-world underground coal mine scenarios, outperforming state-of-the-art\napproaches. We open source our code of this work on Github to benefit the\nrobotics community.", "AI": {"tldr": "This paper develops CM-LIUW-Odometry, a multimodal SLAM framework designed for GPS-denied underground coal mines, achieving improved robustness and accuracy.", "motivation": "SLAM applications face significant obstacles in GPS-denied coal mines such as LiDAR ineffectiveness, terrain challenges reducing odometer accuracy, and long tunnels causing limited sensing. The paper aims to overcome these issues.", "method": "The authors propose CM-LIUW-Odometry using Iterated Error-State Kalman Filter, combining LiDAR-inertial odometry, UWB positioning constraints, wheel odometer integration, nonholonomic constraints, vehicle compensation, and adaptive motion mode switching.", "result": "Results demonstrate that CM-LIUW-Odometry outperforms existing SLAM methods in underground coal mines, with superior accuracy and robustness validated through real-world experiments.", "conclusion": "The framework provides a practical SLAM solution for challenging GPS-denied environments and is open-sourced to support further robotics research."}}
{"id": "2511.00177", "pdf": "https://arxiv.org/pdf/2511.00177", "abs": "https://arxiv.org/abs/2511.00177", "authors": ["Hiba Ahsan", "Byron C. Wallace"], "title": "Can SAEs reveal and mitigate racial biases of LLMs in healthcare?", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "LLMs are increasingly being used in healthcare. This promises to free\nphysicians from drudgery, enabling better care to be delivered at scale. But\nthe use of LLMs in this space also brings risks; for example, such models may\nworsen existing biases. How can we spot when LLMs are (spuriously) relying on\npatient race to inform predictions? In this work we assess the degree to which\nSparse Autoencoders (SAEs) can reveal (and control) associations the model has\nmade between race and stigmatizing concepts. We first identify SAE latents in\nGemma-2 models which appear to correlate with Black individuals. We find that\nthis latent activates on reasonable input sequences (e.g., \"African American\")\nbut also problematic words like \"incarceration\". We then show that we can use\nthis latent to steer models to generate outputs about Black patients, and\nfurther that this can induce problematic associations in model outputs as a\nresult. For example, activating the Black latent increases the risk assigned to\nthe probability that a patient will become \"belligerent\". We evaluate the\ndegree to which such steering via latents might be useful for mitigating bias.\nWe find that this offers improvements in simple settings, but is less\nsuccessful for more realistic and complex clinical tasks. Overall, our results\nsuggest that: SAEs may offer a useful tool in clinical applications of LLMs to\nidentify problematic reliance on demographics but mitigating bias via SAE\nsteering appears to be of marginal utility for realistic tasks.", "AI": {"tldr": "This paper explores the risks of LLMs in healthcare, focusing on racial biases. It examines the use of Sparse Autoencoders to detect and manage problematic associations related to race in LLM outputs.", "motivation": "To address concerns related to racial bias in large language models (LLMs) used in healthcare, particularly focusing on how these models may rely on race for predictions.", "method": "The authors studied Sparse Autoencoders (SAEs) to identify latent racial representations in LLM outputs and explored how these latents influence predictions, including testing methods to control biases through latent steering.", "result": "They found that specific SAE latents correlated with African American racial references and could manipulate the model\u2019s outputs, which heightened problematic associations such as predicting problematic traits about Black patients. The mitigation approaches via SAE steering worked in simple settings but failed in complex clinical tasks.", "conclusion": "While SAEs can reveal racial biases in LLMs, steering latents is not highly effective for mitigating bias in intricate clinical scenarios, limiting practical applications."}}
{"id": "2511.00419", "pdf": "https://arxiv.org/pdf/2511.00419", "abs": "https://arxiv.org/abs/2511.00419", "authors": ["Thanh Hieu Cao", "Trung Khang Tran", "Gia Thinh Pham", "Tuong Nghiem Diep", "Thanh Binh Nguyen"], "title": "LGCA: Enhancing Semantic Representation via Progressive Expansion", "categories": ["cs.CV", "cs.AI"], "comment": "15 pages, 5 figures, to appear in SoICT 2025", "summary": "Recent advancements in large-scale pretraining in natural language processing\nhave enabled pretrained vision-language models such as CLIP to effectively\nalign images and text, significantly improving performance in zero-shot image\nclassification tasks. Subsequent studies have further demonstrated that\ncropping images into smaller regions and using large language models to\ngenerate multiple descriptions for each caption can further enhance model\nperformance. However, due to the inherent sensitivity of CLIP, random image\ncrops can introduce misinformation and bias, as many images share similar\nfeatures at small scales. To address this issue, we propose\nLocalized-Globalized Cross-Alignment (LGCA), a framework that first captures\nthe local features of an image and then repeatedly selects the most salient\nregions and expands them. The similarity score is designed to incorporate both\nthe original and expanded images, enabling the model to capture both local and\nglobal features while minimizing misinformation. Additionally, we provide a\ntheoretical analysis demonstrating that the time complexity of LGCA remains the\nsame as that of the original model prior to the repeated expansion process,\nhighlighting its efficiency and scalability. Extensive experiments demonstrate\nthat our method substantially improves zero-shot performance across diverse\ndatasets, outperforming state-of-the-art baselines.", "AI": {"tldr": "LGCA improves zero-shot image classification by addressing misinformation caused by random image cropping.", "motivation": "To enhance the efficiency of pretrained models like CLIP and address misinformation in zero-shot image classification.", "method": "Proposes Localized-Globalized Cross-Alignment (LGCA) to prioritize salient image regions and balance local-global features.", "result": "LGCA achieves better zero-shot classification results across datasets while maintaining efficient scalability.", "conclusion": "LGCA effectively minimizes bias and misinformation, exhibiting strong performance improvements over existing methods."}}
{"id": "2511.01527", "pdf": "https://arxiv.org/pdf/2511.01527", "abs": "https://arxiv.org/abs/2511.01527", "authors": ["Hanwen Xu", "Xuyao Huang", "Yuzhe Liu", "Kai Yu", "Zhijie Deng"], "title": "TPS-Bench: Evaluating AI Agents' Tool Planning \\& Scheduling Abilities in Compounding Tasks", "categories": ["cs.AI"], "comment": null, "summary": "Large language model (LLM) agents have exhibited strong problem-solving\ncompetence across domains like research and coding. Yet, it remains\nunderexplored whether LLM agents can tackle compounding real-world problems\nthat require a diverse set of tools to complete. Given a broad, heterogeneous\ntool repository, LLM agents must not only select appropriate tools based on\ntask planning analysis but also strategically schedule the execution order to\nensure efficiency. This paper introduces TPS-Bench to benchmark the ability of\nLLM agents in solving such problems that demand Tool Planning and Scheduling.\nTPS-Bench collects 200 compounding tasks of two difficulty levels, based on a\ntool repository containing hundreds of model context protocol (MCP) tools. In\nparticular, each task is composed of multiple subtasks, such as web search, map\nnavigation, calendar checking, etc., and each subtask can be completed by a\nbasic tool. Our evaluation emphasizes both task completion rate and efficiency.\nThe empirical studies on popular closed-source and open-source LLMs indicate\nthat most models can perform reasonable tool planning, but differ in\nscheduling. For example, GLM-4.5 achieves an outperforming task completion rate\nof 64.72% with extensive sequential tool calls, hence suffering from\nsignificantly long execution time. By contrast, GPT-4o prioritizes parallel\ntool calls but achieves only a 45.08% completion rate. Considering\nreinforcement learning (RL) can be a viable way to improve the scheduling\nefficiency without compromising performance, we perform an initial study on\nQwen3-1.7B and witness a 14% reduction in execution time alongside a 6% gain in\ntask completion rate based on rarely 100 RL training samples. Our code is\navailable https://github.com/hanwenxu1/mcp-agent.", "AI": {"tldr": "The paper introduces TPS-Bench, a benchmark for evaluating LLM agents' ability in tool planning and scheduling to solve compounding real-world tasks, finding differences in completion rates and efficiencies of models like GLM-4.5 and GPT-4o, with reinforcement learning showing promising improvements.", "motivation": "Investigate if LLM agents can efficiently solve real-world problems involving diverse tools and strategies for planning and scheduling.", "method": "Develop TPS-Bench with 200 tasks requiring tools from a heterogeneous repository, evaluating completion rate, and efficiency. Additionally, test RL methods to improve efficiency and performance.", "result": "GLM-4.5 achieved high completion rates but at the cost of efficiency; GPT-4o focused on parallel tool calls at a lower completion rate. RL applied to Qwen3-1.7B reduced execution time by 14% and improved completion rate by 6%.", "conclusion": "Current LLMs show promising tool planning capabilities but could benefit from improved scheduling strategies. Reinforcement learning holds potential to enhance efficiency and task performance further."}}
{"id": "2511.01181", "pdf": "https://arxiv.org/pdf/2511.01181", "abs": "https://arxiv.org/abs/2511.01181", "authors": ["Emaad Manzoor", "Eva Ascarza", "Oded Netzer"], "title": "Learning When to Quit in Sales Conversations", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Salespeople frequently face the dynamic screening decision of whether to\npersist in a conversation or abandon it to pursue the next lead. Yet, little is\nknown about how these decisions are made, whether they are efficient, or how to\nimprove them. We study these decisions in the context of high-volume outbound\nsales where leads are ample, but time is scarce and failure is common. We\nformalize the dynamic screening decision as an optimal stopping problem and\ndevelop a generative language model-based sequential decision agent - a\nstopping agent - that learns whether and when to quit conversations by\nimitating a retrospectively-inferred optimal stopping policy. Our approach\nhandles high-dimensional textual states, scales to large language models, and\nworks with both open-source and proprietary language models. When applied to\ncalls from a large European telecommunications firm, our stopping agent reduces\nthe time spent on failed calls by 54% while preserving nearly all sales;\nreallocating the time saved increases expected sales by up to 37%. Upon\nexamining the linguistic cues that drive salespeople's quitting decisions, we\nfind that they tend to overweight a few salient expressions of consumer\ndisinterest and mispredict call failure risk, suggesting cognitive bounds on\ntheir ability to make real-time conversational decisions. Our findings\nhighlight the potential of artificial intelligence algorithms to correct\ncognitively-bounded human decisions and improve salesforce efficiency.", "AI": {"tldr": "This paper studies salespeople's quitting decisions during outbound sales interactions and proposes an AI-based solution to improve efficiency.", "motivation": "Understanding how salespeople decide to quit conversations and whether these decisions are efficient, with the goal of improving sales processes.", "method": "The paper formulates the quitting decision as an optimal stopping problem and develops a language model-based agent that mimics an inferred optimal decision policy using text-based cues.", "result": "The AI agent reduces time spent on failed calls by 54%, preserves sales, and reallocates saved time to increase expected sales by up to 37%.", "conclusion": "AI algorithms can significantly enhance the efficiency of outbound sales by correcting human errors in quitting decisions influenced by cognitive biases."}}
{"id": "2511.01383", "pdf": "https://arxiv.org/pdf/2511.01383", "abs": "https://arxiv.org/abs/2511.01383", "authors": ["Landson Guo", "Andres M. Diaz Aguilar", "William Talbot", "Turcan Tuna", "Marco Hutter", "Cesar Cadena"], "title": "CaRLi-V: Camera-RADAR-LiDAR Point-Wise 3D Velocity Estimation", "categories": ["cs.RO"], "comment": null, "summary": "Accurate point-wise velocity estimation in 3D is crucial for robot\ninteraction with non-rigid, dynamic agents, such as humans, enabling robust\nperformance in path planning, collision avoidance, and object manipulation in\ndynamic environments. To this end, this paper proposes a novel RADAR, LiDAR,\nand camera fusion pipeline for point-wise 3D velocity estimation named CaRLi-V.\nThis pipeline leverages raw RADAR measurements to create a novel RADAR\nrepresentation, the velocity cube, which densely represents radial velocities\nwithin the RADAR's field-of-view. By combining the velocity cube for radial\nvelocity extraction, optical flow for tangential velocity estimation, and LiDAR\nfor point-wise range measurements through a closed-form solution, our approach\ncan produce 3D velocity estimates for a dense array of points. Developed as an\nopen-source ROS2 package, CaRLi-V has been field-tested against a custom\ndataset and proven to produce low velocity error metrics relative to ground\ntruth, enabling point-wise velocity estimation for robotic applications.", "AI": {"tldr": "The paper introduces CaRLi-V, a fusion pipeline combining RADAR, LiDAR, and camera data for precise 3D velocity estimation, enabling better robot interaction in dynamic environments.", "motivation": "To address challenges in robotic interaction with dynamic agents such as humans and objects, ensuring accurate 3D point-wise velocity estimation for improved navigation and manipulation.", "method": "The method includes using RADAR's velocity cube for radial velocities, optical flow for tangential velocities, and LiDAR for range measurements, integrated through a closed-form solution into the CaRLi-V fusion pipeline.", "result": "The CaRLi-V system demonstrated low velocity estimation errors in experiments, validated against ground truth using a custom dataset.", "conclusion": "CaRLi-V provides accurate point-wise velocity estimation and is open-sourced as a ROS2 package, showing its practical potential for improving robotic applications in dynamic settings."}}
{"id": "2511.00183", "pdf": "https://arxiv.org/pdf/2511.00183", "abs": "https://arxiv.org/abs/2511.00183", "authors": ["Shaghayegh Fazliani", "Madeleine Udell"], "title": "PDE-SHARP: PDE Solver Hybrids Through Analysis & Refinement Passes", "categories": ["cs.LG"], "comment": null, "summary": "Current LLM-driven approaches using test-time computing to generate PDE\nsolvers execute a large number of solver samples to identify high-accuracy\nsolvers. These paradigms are especially costly for complex PDEs requiring\nsubstantial computational resources for numerical evaluation. We introduce\nPDE-SHARP, a framework to reduce computational costs by replacing expensive\nscientific computation by cheaper LLM inference that achieves superior solver\naccuracy with 60-75% fewer computational evaluations. PDE-SHARP employs three\nstages: (1) Analysis: mathematical chain-of-thought analysis including PDE\nclassification, solution type detection, and stability analysis; (2) Genesis:\nsolver generation based on mathematical insights from the previous stage; and\n(3) Synthesis: collaborative selection-hybridization tournaments in which LLM\njudges iteratively refine implementations through flexible performance\nfeedback. To generate high-quality solvers, PDE-SHARP requires fewer than 13\nsolver evaluations on average compared to 30+ for baseline methods, improving\naccuracy uniformly across tested PDEs by $4\\times$ on average, and demonstrates\nrobust performance across LLM architectures, from general-purpose to\nspecialized reasoning models.", "AI": {"tldr": "PDE-SHARP reduces computational costs and enhances accuracy in solving PDEs using fewer numerical evaluations through LLM-based inference.", "motivation": "Address the high computational costs of traditional LLM-driven test-time approaches for solving complex PDEs.", "method": "Three-stage framework: Analysis for mathematical insights, Genesis for solver generation, and Synthesis for iterative refinement of implementations.", "result": "PDE-SHARP reduces solver evaluations by 60-75%, requires fewer than 13 evaluations on average, improves accuracy by 4\u00d7, and shows robust performance across different LLM architectures.", "conclusion": "PDE-SHARP presents a cost-effective and accurate approach to PDE solving, significantly reducing numerical computations while enhancing overall solver performance."}}
{"id": "2511.00427", "pdf": "https://arxiv.org/pdf/2511.00427", "abs": "https://arxiv.org/abs/2511.00427", "authors": ["Daichi Zhang", "Tong Zhang", "Jianmin Bao", "Shiming Ge", "Sabine S\u00fcsstrunk"], "title": "Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the rapid development of generative models, detecting generated fake\nimages to prevent their malicious use has become a critical issue recently.\nExisting methods frame this challenge as a naive binary image classification\ntask. However, such methods focus only on visual clues, yielding trained\ndetectors susceptible to overfitting specific image patterns and incapable of\ngeneralizing to unseen models. In this paper, we address this issue from a\nmulti-modal perspective and find that fake images cannot be properly aligned\nwith corresponding captions compared to real images. Upon this observation, we\npropose a simple yet effective detector termed ITEM by leveraging the\nimage-text misalignment in a joint visual-language space as discriminative\nclues. Specifically, we first measure the misalignment of the images and\ncaptions in pre-trained CLIP's space, and then tune a MLP head to perform the\nusual detection task. Furthermore, we propose a hierarchical misalignment\nscheme that first focuses on the whole image and then each semantic object\ndescribed in the caption, which can explore both global and fine-grained local\nsemantic misalignment as clues. Extensive experiments demonstrate the\nsuperiority of our method against other state-of-the-art competitors with\nimpressive generalization and robustness on various recent generative models.", "AI": {"tldr": "The paper addresses the challenge of detecting fake images from generative models by introducing a novel multi-modal method leveraging image-text misalignment.", "motivation": "The motivation is to overcome the limitations of existing binary image classification methods that overly rely on visual clues, leading to poor generalization to new generative models.", "method": "The authors propose ITEM, a detector that utilizes image-text misalignment in CLIP's joint visual-language space. They measure alignment issues between images and captions, and introduce a hierarchical scheme for both global and object-specific misalignment detection.", "result": "Extensive experiments show that ITEM outperforms state-of-the-art methods, offering better generalization and robustness against various generative models.", "conclusion": "The study concludes that leveraging multi-modal misalignment is an effective strategy for detecting fake images and improving generalization across unseen models."}}
{"id": "2511.01550", "pdf": "https://arxiv.org/pdf/2511.01550", "abs": "https://arxiv.org/abs/2511.01550", "authors": ["Ujjwal Sharma", "Stevan Rudinac", "Ana Mi\u0107kovi\u0107", "Willemijn van Dolen", "Marcel Worring"], "title": "Analyzing Sustainability Messaging in Large-Scale Corporate Social Media", "categories": ["cs.AI"], "comment": null, "summary": "In this work, we introduce a multimodal analysis pipeline that leverages\nlarge foundation models in vision and language to analyze corporate social\nmedia content, with a focus on sustainability-related communication. Addressing\nthe challenges of evolving, multimodal, and often ambiguous corporate messaging\non platforms such as X (formerly Twitter), we employ an ensemble of large\nlanguage models (LLMs) to annotate a large corpus of corporate tweets on their\ntopical alignment with the 17 Sustainable Development Goals (SDGs). This\napproach avoids the need for costly, task-specific annotations and explores the\npotential of such models as ad-hoc annotators for social media data that can\nefficiently capture both explicit and implicit references to sustainability\nthemes in a scalable manner. Complementing this textual analysis, we utilize\nvision-language models (VLMs), within a visual understanding framework that\nuses semantic clusters to uncover patterns in visual sustainability\ncommunication. This integrated approach reveals sectoral differences in SDG\nengagement, temporal trends, and associations between corporate messaging,\nenvironmental, social, governance (ESG) risks, and consumer engagement. Our\nmethods-automatic label generation and semantic visual clustering-are broadly\napplicable to other domains and offer a flexible framework for large-scale\nsocial media analysis.", "AI": {"tldr": "The paper introduces a multimodal analysis pipeline using foundation models to study corporate social media\u2019s sustainability messaging, focusing on both text and visual content.", "motivation": "Current corporate social media messaging on sustainability is evolving, multimodal, and often ambiguous, requiring efficient and scalable methods to analyze such communication.", "method": "An ensemble of large language models (LLMs) is employed to annotate corporate tweets concerning the 17 Sustainable Development Goals (SDGs). Vision-language models (VLMs) analyze visual patterns via semantic clusters.", "result": "The integrated pipeline uncovers sector-specific differences, temporal trends, and correlations between corporate messaging and ESG risks, as well as consumer engagement.", "conclusion": "The proposed methods are scalable, domain-flexible, and broadly applicable for large-scale social media analysis, transcending task-specific annotations by using foundation models effectively."}}
{"id": "2511.01187", "pdf": "https://arxiv.org/pdf/2511.01187", "abs": "https://arxiv.org/abs/2511.01187", "authors": ["Muhammed Saeed", "Muhammad Abdul-mageed", "Shady Shehata"], "title": "Surfacing Subtle Stereotypes: A Multilingual, Debate-Oriented Evaluation of Modern LLMs", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) are widely deployed for open-ended\ncommunication, yet most bias evaluations still rely on English,\nclassification-style tasks. We introduce DebateBias-8K, a new multilingual,\ndebate-style benchmark designed to reveal how narrative bias appears in\nrealistic generative settings. Our dataset includes 8,400 structured debate\nprompts spanning four sensitive domains: women's rights, socioeconomic\ndevelopment, terrorism, and religion, across seven languages ranging from\nhigh-resource (English, Chinese) to low-resource (Swahili, Nigerian Pidgin).\nUsing four flagship models (GPT-4o, Claude 3, DeepSeek, and LLaMA 3), we\ngenerate and automatically classify over 100,000 responses. Results show that\nall models reproduce entrenched stereotypes despite safety alignment: Arabs are\noverwhelmingly linked to terrorism and religion (>=95%), Africans to\nsocioeconomic \"backwardness\" (up to <=77%), and Western groups are consistently\nframed as modern or progressive. Biases grow sharply in lower-resource\nlanguages, revealing that alignment trained primarily in English does not\ngeneralize globally. Our findings highlight a persistent divide in multilingual\nfairness: current alignment methods reduce explicit toxicity but fail to\nprevent biased outputs in open-ended contexts. We release our DebateBias-8K\nbenchmark and analysis framework to support the next generation of multilingual\nbias evaluation and safer, culturally inclusive model alignment.", "AI": {"tldr": "This study introduces DebateBias-8K, a benchmark to assess narrative biases in multilingual generative language models across sensitive domains and languages.", "motivation": "To address the lack of effective bias evaluation methods in generative contexts, especially in multilingual and realistic settings.", "method": "The authors developed DebateBias-8K, a benchmark testing four flagship LLMs with 8,400 structured debate prompts in various sensitive domains and languages. They automatically classify over 100,000 responses to analyze biases.", "result": "Results indicate entrenched biases in all four models, especially in low-resource languages, despite safety alignment efforts. Examples include stereotypes linking Arabs to terrorism and Africans to socioeconomic backwardness.", "conclusion": "Bias mitigation methods primarily trained in English fail to generalize globally. DebateBias-8K serves to advance future bias evaluations and promote inclusive model alignment approaches."}}
{"id": "2511.01407", "pdf": "https://arxiv.org/pdf/2511.01407", "abs": "https://arxiv.org/abs/2511.01407", "authors": ["Paolo Rabino", "Gabriele Tiboni", "Tatiana Tommasi"], "title": "FoldPath: End-to-End Object-Centric Motion Generation via Modulated Implicit Paths", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted at 2025 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025)", "summary": "Object-Centric Motion Generation (OCMG) is instrumental in advancing\nautomated manufacturing processes, particularly in domains requiring\nhigh-precision expert robotic motions, such as spray painting and welding. To\nrealize effective automation, robust algorithms are essential for generating\nextended, object-aware trajectories across intricate 3D geometries. However,\ncontemporary OCMG techniques are either based on ad-hoc heuristics or employ\nlearning-based pipelines that are still reliant on sensitive post-processing\nsteps to generate executable paths. We introduce FoldPath, a novel, end-to-end,\nneural field based method for OCMG. Unlike prior deep learning approaches that\npredict discrete sequences of end-effector waypoints, FoldPath learns the robot\nmotion as a continuous function, thus implicitly encoding smooth output paths.\nThis paradigm shift eliminates the need for brittle post-processing steps that\nconcatenate and order the predicted discrete waypoints. Particularly, our\napproach demonstrates superior predictive performance compared to recently\nproposed learning-based methods, and attains generalization capabilities even\nin real industrial settings, where only a limited amount of 70 expert samples\nare provided. We validate FoldPath through comprehensive experiments in a\nrealistic simulation environment and introduce new, rigorous metrics designed\nto comprehensively evaluate long-horizon robotic paths, thus advancing the OCMG\ntask towards practical maturity.", "AI": {"tldr": "The paper introduces FoldPath, a neural field-based method for generating continuous robotic motion trajectories without requiring sensitive post-processing steps.", "motivation": "The need for robust algorithms to automate high-precision robotic tasks in manufacturing processes, addressing the weaknesses of heuristics or learning-based methods requiring brittle post-processing.", "method": "FoldPath uses a continuous function neural field approach to learn robot motions, eliminating discrete waypoints and post-processing while ensuring smooth paths.", "result": "FoldPath achieves superior predictive performance and generalization capabilities, even with limited expert samples in industrial settings, validated in realistic simulations.", "conclusion": "FoldPath significantly enhances OCMG tasks by introducing end-to-end trajectory generations, improving practicality and performance in industrial robotics applications."}}
{"id": "2511.00192", "pdf": "https://arxiv.org/pdf/2511.00192", "abs": "https://arxiv.org/abs/2511.00192", "authors": ["Ali Satvaty", "Suzan Verberne", "Fatih Turkmen"], "title": "EL-MIA: Quantifying Membership Inference Risks of Sensitive Entities in LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Membership inference attacks (MIA) aim to infer whether a particular data\npoint is part of the training dataset of a model. In this paper, we propose a\nnew task in the context of LLM privacy: entity-level discovery of membership\nrisk focused on sensitive information (PII, credit card numbers, etc). Existing\nmethods for MIA can detect the presence of entire prompts or documents in the\nLLM training data, but they fail to capture risks at a finer granularity. We\npropose the ``EL-MIA'' framework for auditing entity-level membership risks in\nLLMs. We construct a benchmark dataset for the evaluation of MIA methods on\nthis task. Using this benchmark, we conduct a systematic comparison of existing\nMIA techniques as well as two newly proposed methods. We provide a\ncomprehensive analysis of the results, trying to explain the relation of the\nentity level MIA susceptability with the model scale, training epochs, and\nother surface level factors. Our findings reveal that existing MIA methods are\nlimited when it comes to entity-level membership inference of the sensitive\nattributes, while this susceptibility can be outlined with relatively\nstraightforward methods, highlighting the need for stronger adversaries to\nstress test the provided threat model.", "AI": {"tldr": "The paper discusses a novel task: entity-level membership inference attacks in large language models (LLMs) specifically targeting sensitive information. A benchmark dataset was created to evaluate this, revealing limitations of existing methods and proposing two new techniques.", "motivation": "The motivation is to enhance privacy auditing in LLMs, addressing gaps in detecting entity-level membership risks tied to sensitive attributes like PII or credit card details.", "method": "The authors introduce the EL-MIA framework and construct a benchmark dataset systematically comparing existing MIA techniques and two newly proposed methods.", "result": "Findings show existing MIA techniques are insufficient for entity-level tasks on sensitive attributes. However, simple methods can expose susceptibilities, calling for more robust adversarial models.", "conclusion": "Current methods are inadequate for entity-level membership inference in sensitive cases, highlighting the necessity for stronger adversarial models to improve the evaluation of privacy risks in LLMs."}}
{"id": "2511.00429", "pdf": "https://arxiv.org/pdf/2511.00429", "abs": "https://arxiv.org/abs/2511.00429", "authors": ["Daichi Zhang", "Tong Zhang", "Shiming Ge", "Sabine S\u00fcsstrunk"], "title": "Enhancing Frequency Forgery Clues for Diffusion-Generated Image Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models have achieved remarkable success in image synthesis, but the\ngenerated high-quality images raise concerns about potential malicious use.\nExisting detectors often struggle to capture discriminative clues across\ndifferent models and settings, limiting their generalization to unseen\ndiffusion models and robustness to various perturbations. To address this\nissue, we observe that diffusion-generated images exhibit progressively larger\ndifferences from natural real images across low- to high-frequency bands. Based\non this insight, we propose a simple yet effective representation by enhancing\nthe Frequency Forgery Clue (F^2C) across all frequency bands. Specifically, we\nintroduce a frequency-selective function which serves as a weighted filter to\nthe Fourier spectrum, suppressing less discriminative bands while enhancing\nmore informative ones. This approach, grounded in a comprehensive analysis of\nfrequency-based differences between natural real and diffusion-generated\nimages, enables general detection of images from unseen diffusion models and\nprovides robust resilience to various perturbations. Extensive experiments on\nvarious diffusion-generated image datasets demonstrate that our method\noutperforms state-of-the-art detectors with superior generalization and\nrobustness.", "AI": {"tldr": "The paper introduces a robust method to detect diffusion-generated images by analyzing frequency differences and enhancing discriminative clues across frequency bands.", "motivation": "Concerns about malicious use of high-quality images generated by diffusion models, and the limitations of existing detectors in generalizing to unseen models and handling perturbations.", "method": "A frequency-selective function is used as a weighted filter to enhance discriminative frequency bands, leveraging the differences between real and diffusion-generated images across all frequency bands.", "result": "The proposed method demonstrated superior generalization capabilities and robustness compared to state-of-the-art detectors across various diffusion-generated image datasets.", "conclusion": "Enhancing frequency-based differences effectively improves detection of diffusion-generated images, addressing challenges of unseen models and perturbations."}}
{"id": "2511.01581", "pdf": "https://arxiv.org/pdf/2511.01581", "abs": "https://arxiv.org/abs/2511.01581", "authors": ["Chengzhang Yu", "Zening Lu", "Chenyang Zheng", "Chiyue Wang", "Yiming Zhang", "Zhanpeng Jin"], "title": "ExplicitLM: Decoupling Knowledge from Parameters via Explicit Memory Banks", "categories": ["cs.AI"], "comment": "12pages, 4figures", "summary": "Large language models suffer from knowledge staleness and lack of\ninterpretability due to implicit knowledge storage across entangled network\nparameters, preventing targeted updates and reasoning transparency. We propose\nExplicitLM, a novel architecture featuring a million-scale external memory bank\nstoring human-readable knowledge as token sequences, enabling direct inspection\nand modification. We design a differentiable two-stage retrieval mechanism with\nefficient coarse-grained filtering via product key decomposition (reducing\ncomplexity from $\\mathcal{O}(N \\cdot |I|)$ to $\\mathcal{O}(\\sqrt{N} \\cdot\n|I|)$) and fine-grained Gumbel-Softmax matching for end-to-end training.\nInspired by dual-system cognitive theory, we partition knowledge into frozen\nexplicit facts (20%) and learnable implicit patterns (80%), maintained through\nExponential Moving Average updates for stability. ExplicitLM achieves up to\n43.67% improvement on knowledge-intensive tasks versus standard Transformers,\nwith 3.62$\\times$ gains in low-data regimes (10k samples). Analysis shows\nstrong correlations between memory retrieval and performance, with correct\npredictions achieving 49% higher hit rates. Unlike RAG systems with frozen\nretrieval, our jointly optimized architecture demonstrates that interpretable,\nupdatable models can maintain competitive performance while providing\nunprecedented knowledge transparency.", "AI": {"tldr": "ExplicitLM introduces an architecture with million-scale external memory for more interpretable and updatable language models.", "motivation": "Large language models struggle with stale knowledge and lack reasoning transparency due to implicit network parameters.", "method": "ExplicitLM uses an external memory bank with human-readable knowledge and a two-stage retrieval mechanism, combining coarse filtering and fine-grained matching.", "result": "ExplicitLM demonstrates up to 43.67% improvement in knowledge-intensive tasks and significant gains in low-data settings.", "conclusion": "ExplicitLM achieves competitive performance while offering knowledge transparency and updateability."}}
{"id": "2511.01188", "pdf": "https://arxiv.org/pdf/2511.01188", "abs": "https://arxiv.org/abs/2511.01188", "authors": ["Lvhua Wu", "Xuefeng Jiang", "Sheng Sun", "Tian Wen", "Yuwei Wang", "Min Liu"], "title": "ZoFia: Zero-Shot Fake News Detection with Entity-Guided Retrieval and Multi-LLM Interaction", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid spread of fake news threatens social stability and public trust,\nrendering its detection an imperative research priority. Although large\nlanguage models (LLMs) excel at numerous natural language processing tasks with\ntheir remarkable contextual understanding and extensive prior knowledge, the\ntime-bounded knowledge coverage and tendency for generating hallucination\ncontent reduce their reliability when handling fast-evolving news streams.\nFurthermore, models trained on existing static datasets also often lack the\ngeneralization needed for emerging news topics. To address these challenges, we\npropose ZoFia, a novel two-stage zero-shot fake news detection framework.\nFirst, we introduce Hierarchical Salience to quantify the importance of\nentities in the news content, and propose the SC-MMR algorithm to effectively\nselect an informative and diverse set of keywords that serve as queries for\nretrieving up-to-date external evidence. Subsequently, a multi LLM interactive\nsystem, in which each agent assumes a distinct role, performs multi-view\ncollaborative analysis and adversarial debate over the news text and its\nrelated information, and finally produces an interpretable and robust judgment.\nComprehensive experiments on two public datasets demonstrate that ZoFia\nobviously outperforms existing zero-shot baselines and most of few-shot\nmethods. Our codes will be open-sourced to facilitate related communities.", "AI": {"tldr": "The paper presents ZoFia, a framework for zero-shot fake news detection that enhances reliability by combining hierarchical salience-based keyword selection and a multi-LLM collaborative analysis system.", "motivation": "The paper aims to address the issues that large language models face in identifying fake news, particularly their limited adaptability to fast-evolving news topics and their tendency to generate hallucinated content.", "method": "ZoFia is a two-stage framework: it quantifies the importance of news entities using Hierarchical Salience and selects diverse keywords for external evidence retrieval with the SC-MMR algorithm, followed by a multi-LLM system that analyzes and debates using multiple viewpoints to make robust decisions.", "result": "Experiments conducted on two public datasets show that ZoFia achieves superior performance compared to existing zero-shot baselines and most few-shot methods.", "conclusion": "ZoFia effectively tackles the challenges of fake news detection by leveraging dynamic evidence retrieval and collaborative LLM-based analysis. The framework is a step forward and its open-source code will support further advancements."}}
{"id": "2511.01437", "pdf": "https://arxiv.org/pdf/2511.01437", "abs": "https://arxiv.org/abs/2511.01437", "authors": ["Elian Neppel", "Shamistan Karimov", "Ashutosh Mishra", "Gustavo Hernan Diaz Huenupan", "Hazal Gozbasi", "Kentaro Uno", "Shreya Santra", "Kazuya Yoshida"], "title": "Designing for Distributed Heterogeneous Modularity: On Software Architecture and Deployment of MoonBots", "categories": ["cs.RO"], "comment": "6 pages, 8 figures. Accepted at ISPARO 2025", "summary": "This paper presents the software architecture and deployment strategy behind\nthe MoonBot platform: a modular space robotic system composed of heterogeneous\ncomponents distributed across multiple computers, networks and ultimately\ncelestial bodies. We introduce a principled approach to distributed,\nheterogeneous modularity, extending modular robotics beyond physical\nreconfiguration to software, communication and orchestration. We detail the\narchitecture of our system that integrates component-based design, a\ndata-oriented communication model using ROS2 and Zenoh, and a deployment\norchestrator capable of managing complex multi-module assemblies. These\nabstractions enable dynamic reconfiguration, decentralized control, and\nseamless collaboration between numerous operators and modules. At the heart of\nthis system lies our open-source Motion Stack software, validated by months of\nfield deployment with self-assembling robots, inter-robot cooperation, and\nremote operation. Our architecture tackles the significant hurdles of modular\nrobotics by significantly reducing integration and maintenance overhead, while\nremaining scalable and robust. Although tested with space in mind, we propose\ngeneralizable patterns for designing robotic systems that must scale across\ntime, hardware, teams and operational environments.", "AI": {"tldr": "The paper introduces the MoonBot platform's modular space robotic system and its innovative software architecture, focusing on distributed, heterogeneous modularity, dynamic reconfiguration, and decentralized control.", "motivation": "To address significant challenges in modular robotics by improving software integration, scalability, and maintenance across varying operational environments and teams.", "method": "The authors developed a principled software architecture using component-based design, ROS2, Zenoh communication models, and a deployment orchestrator for complex modular assemblies.", "result": "Validation through extensive field trials featuring self-assembling robots, inter-robot cooperation, and remote operations proved the system's robustness and scalability.", "conclusion": "The MoonBot architecture is applicable to scalable robotic systems beyond space exploration, offering generalizable patterns for diverse operational scenarios."}}
{"id": "2511.00446", "pdf": "https://arxiv.org/pdf/2511.00446", "abs": "https://arxiv.org/abs/2511.00446", "authors": ["Xin Yao", "Haiyang Zhao", "Yimin Chen", "Jiawei Guo", "Kecheng Huang", "Ming Zhao"], "title": "ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": "Accepted by NeurIPS 2025", "summary": "The Contrastive Language-Image Pretraining (CLIP) model has significantly\nadvanced vision-language modeling by aligning image-text pairs from large-scale\nweb data through self-supervised contrastive learning. Yet, its reliance on\nuncurated Internet-sourced data exposes it to data poisoning and backdoor\nrisks. While existing studies primarily investigate image-based attacks, the\ntext modality, which is equally central to CLIP's training, remains\nunderexplored. In this work, we introduce ToxicTextCLIP, a framework for\ngenerating high-quality adversarial texts that target CLIP during the\npre-training phase. The framework addresses two key challenges: semantic\nmisalignment caused by background inconsistency with the target class, and the\nscarcity of background-consistent texts. To this end, ToxicTextCLIP iteratively\napplies: 1) a background-aware selector that prioritizes texts with background\ncontent aligned to the target class, and 2) a background-driven augmenter that\ngenerates semantically coherent and diverse poisoned samples. Extensive\nexperiments on classification and retrieval tasks show that ToxicTextCLIP\nachieves up to 95.83% poisoning success and 98.68% backdoor Hit@1, while\nbypassing RoCLIP, CleanCLIP and SafeCLIP defenses. The source code can be\naccessed via https://github.com/xinyaocse/ToxicTextCLIP/.", "AI": {"tldr": "The paper introduces ToxicTextCLIP, a framework for generating adversarial text samples targeting text-image contrastive models like CLIP during pre-training, revealing vulnerabilities to data poisoning in the text modality.", "motivation": "To address the underexplored domain of text-based vulnerabilities in CLIP training and improve understanding of potential risks in vision-language pre-training.", "method": "The framework uses a background-aware selector and a background-driven augmenter to craft poisoned texts that are semantically coherent and consistent with the target class.", "result": "ToxicTextCLIP achieved high poisoning and backdoor success rates, bypassing defenses like RoCLIP, CleanCLIP, and SafeCLIP.", "conclusion": "The study underscores the importance of understanding text-based data poisoning risks in CLIP-like models and demonstrates the efficacy of ToxicTextCLIP in exploiting vulnerabilities."}}
{"id": "2511.01639", "pdf": "https://arxiv.org/pdf/2511.01639", "abs": "https://arxiv.org/abs/2511.01639", "authors": ["Sicheng Wang", "Shuhao Chen", "Jingran Zhou", "Chengyi Tu"], "title": "IVGAE-TAMA-BO: A novel temporal dynamic variational graph model for link prediction in global food trade networks with momentum structural memory and Bayesian optimization", "categories": ["cs.AI"], "comment": "26pages,6figures", "summary": "Global food trade plays a crucial role in ensuring food security and\nmaintaining supply chain stability. However, its network structure evolves\ndynamically under the influence of geopolitical, economic, and environmental\nfactors, making it challenging to model and predict future trade links.\nEffectively capturing temporal patterns in food trade networks is therefore\nessential for improving the accuracy and robustness of link prediction. This\nstudy introduces IVGAE-TAMA-BO, a novel dynamic graph neural network designed\nto model evolving trade structures and predict future links in global food\ntrade networks. To the best of our knowledge, this is the first work to apply\ndynamic graph neural networks to this domain, significantly enhancing\npredictive performance. Building upon the original IVGAE framework, the\nproposed model incorporates a Trade-Aware Momentum Aggregator (TAMA) to capture\nthe temporal evolution of trade networks, jointly modeling short-term\nfluctuations and long-term structural dependencies. A momentum-based structural\nmemory mechanism further improves predictive stability and performance. In\naddition, Bayesian optimization is used to automatically tune key\nhyperparameters, enhancing generalization across diverse trade scenarios.\nExtensive experiments on five crop-specific datasets demonstrate that\nIVGAE-TAMA substantially outperforms the static IVGAE and other dynamic\nbaselines by effectively modeling temporal dependencies, while Bayesian\noptimization further boosts performance in IVGAE-TAMA-BO. These results\nhighlight the proposed framework as a robust and scalable solution for\nstructural prediction in global trade networks, with strong potential for\napplications in food security monitoring and policy decision support.", "AI": {"tldr": "The paper presents a new dynamic graph neural network, IVGAE-TAMA-BO, to enhance trade link prediction in global food networks by modeling temporal patterns.", "motivation": "Global food trade plays a vital role in ensuring food security, but dynamic network structures influenced by external factors make predicting future trade links difficult.", "method": "The proposed model combines the IVGAE framework with Trade-Aware Momentum Aggregator (TAMA) for capturing temporal trade dynamics and uses Bayesian optimization for tuning hyperparameters.", "result": "Experiments on five crop-specific datasets demonstrate IVGAE-TAMA significantly outdoes static IVGAE and other methods in link prediction performance.", "conclusion": "IVGAE-TAMA-BO provides a robust and scalable solution for predicting trade links in food networks, aiding in food security monitoring and policy support."}}
{"id": "2511.01191", "pdf": "https://arxiv.org/pdf/2511.01191", "abs": "https://arxiv.org/abs/2511.01191", "authors": ["Ru Wang", "Wei Huang", "Qi Cao", "Yusuke Iwasawa", "Yutaka Matsuo", "Jiaxian Guo"], "title": "Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Test-time reinforcement learning (TTRL) offers a label-free paradigm for\nadapting models using only synthetic signals at inference, but its success\nhinges on constructing reliable learning signals. Standard approaches such as\nmajority voting often collapse to spurious yet popular answers. We introduce\nSelf-Harmony, a framework built on a simple intuition: the correct answer\nshould remain stable across both an original question and its paraphrase.\nSelf-Harmony operationalizes this by employing a single model in two\ncomplementary roles: a Solver to produce answers and a Reframer to rephrase the\ninput. Based on this, we further propose a pseudo-label method: instead of\nmajority voting, it aggregates answer frequencies across these original and\nreframed views using the harmonic mean. This is a process that naturally\nselects for solutions stable under reframing, thereby avoiding the common trap\nof favoring view-dependent, spurious answers. Crucially, this requires no human\nsupervision or auxiliary models. Across diverse reasoning benchmarks,\nSelf-Harmony achieves state-of-the-art results at the label-free test-time\nsetting, ranking first in 28 of 30 settings across multiple methods. Beyond\naccuracy, it demonstrates unprecedented robustness, with zero training failures\nin all experiments, underscoring its stability and reliability.", "AI": {"tldr": "Self-Harmony introduces a novel pseudo-labeling method for test-time reinforcement learning that achieves state-of-the-art results by relying on stability between original and reframed questions.", "motivation": "Current test-time reinforcement learning methods suffer from instability, as standard approaches like majority voting often favor spurious answers instead of correct ones.", "method": "Self-Harmony employs a single model to act as both a Solver and Reframer, using harmonic mean aggregation across answers from original and rephrased inputs to select stable solutions without human supervision.", "result": "Self-Harmony ranks first in 28 out of 30 test-time reinforcement learning benchmarks, achieving high accuracy and robustness with no training failures.", "conclusion": "This framework enhances reliability and stability in label-free test-time adaptation, paving the way for more trustworthy reasoning systems."}}
{"id": "2511.01472", "pdf": "https://arxiv.org/pdf/2511.01472", "abs": "https://arxiv.org/abs/2511.01472", "authors": ["Sarthak Mishra", "Rishabh Dev Yadav", "Avirup Das", "Saksham Gupta", "Wei Pan", "Spandan Roy"], "title": "AERMANI-VLM: Structured Prompting and Reasoning for Aerial Manipulation with Vision Language Models", "categories": ["cs.RO"], "comment": null, "summary": "The rapid progress of vision--language models (VLMs) has sparked growing\ninterest in robotic control, where natural language can express the operation\ngoals while visual feedback links perception to action. However, directly\ndeploying VLM-driven policies on aerial manipulators remains unsafe and\nunreliable since the generated actions are often inconsistent,\nhallucination-prone, and dynamically infeasible for flight. In this work, we\npresent AERMANI-VLM, the first framework to adapt pretrained VLMs for aerial\nmanipulation by separating high-level reasoning from low-level control, without\nany task-specific fine-tuning. Our framework encodes natural language\ninstructions, task context, and safety constraints into a structured prompt\nthat guides the model to generate a step-by-step reasoning trace in natural\nlanguage. This reasoning output is used to select from a predefined library of\ndiscrete, flight-safe skills, ensuring interpretable and temporally consistent\nexecution. By decoupling symbolic reasoning from physical action, AERMANI-VLM\nmitigates hallucinated commands and prevents unsafe behavior, enabling robust\ntask completion. We validate the framework in both simulation and hardware on\ndiverse multi-step pick-and-place tasks, demonstrating strong generalization to\npreviously unseen commands, objects, and environments.", "AI": {"tldr": "The paper introduces AERMANI-VLM, a framework using pretrained vision-language models for safe aerial manipulation tasks.", "motivation": "To make aerial robotic manipulation tasks feasible and safe by integrating vision-language models while addressing their inherent limitations.", "method": "The framework decouples high-level reasoning from low-level control, uses a structured prompt to encode instructions, task context, and safety constraints, and selects predefined flight-safe skills.", "result": "AERMANI-VLM successfully demonstrated generalization and robustness in multi-step tasks across unseen commands, objects, and environments.", "conclusion": "By separating symbolic reasoning from physical actions, AERMANI-VLM enhances safety and reliability for aerial manipulation operations without task-specific fine-tuning."}}
{"id": "2511.00209", "pdf": "https://arxiv.org/pdf/2511.00209", "abs": "https://arxiv.org/abs/2511.00209", "authors": ["Yiquan Wang", "Yahui Ma", "Yuhan Chang", "Jiayao Yan", "Jialin Zhang", "Minnuo Cai", "Kai Wei"], "title": "Diffusion Models at the Drug Discovery Frontier: A Review on Generating Small Molecules versus Therapeutic Peptides", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.QM"], "comment": "21 pages, 3 figures", "summary": "Diffusion models have emerged as a leading framework in generative modeling,\nshowing significant potential to accelerate and transform the traditionally\nslow and costly process of drug discovery. This review provides a systematic\ncomparison of their application in designing two principal therapeutic\nmodalities: small molecules and therapeutic peptides. We analyze how a unified\nframework of iterative denoising is adapted to the distinct molecular\nrepresentations, chemical spaces, and design objectives of each modality. For\nsmall molecules, these models excel at structure-based design, generating\nnovel, pocket-fitting ligands with desired physicochemical properties, yet face\nthe critical hurdle of ensuring chemical synthesizability. Conversely, for\ntherapeutic peptides, the focus shifts to generating functional sequences and\ndesigning de novo structures, where the primary challenges are achieving\nbiological stability against proteolysis, ensuring proper folding, and\nminimizing immunogenicity. Despite these distinct challenges, both domains face\nshared hurdles: the need for more accurate scoring functions, the scarcity of\nhigh-quality experimental data, and the crucial requirement for experimental\nvalidation. We conclude that the full potential of diffusion models will be\nunlocked by bridging these modality-specific gaps and integrating them into\nautomated, closed-loop Design-Build-Test-Learn (DBTL) platforms, thereby\nshifting the paradigm from chemical exploration to the targeted creation of\nnovel therapeutics.", "AI": {"tldr": "Diffusion models show promise for generative drug design but have distinct challenges across therapeutic domains.", "motivation": "Accelerating and transforming drug discovery processes using advanced generative frameworks.", "method": "Systematic comparison of iterative denoising applications for therapeutic modalities like small molecules and peptides.", "result": "Diffusion models enable novel drug designs but face hurdles like chemical synthesizability and biological stability.", "conclusion": "Integrating diffusion models into closed-loop DBTL platforms is key to unlocking their potential for novel therapeutics development."}}
{"id": "2511.00456", "pdf": "https://arxiv.org/pdf/2511.00456", "abs": "https://arxiv.org/abs/2511.00456", "authors": ["Kiran Shahi", "Anup Bagale"], "title": "Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations", "categories": ["cs.CV"], "comment": null, "summary": "This study proposes a weakly supervised deep learning framework for pneumonia\nclassification and localization from chest X-rays, utilizing Grad-CAM\nexplanations. Instead of costly pixel-level annotations, our approach utilizes\nimage-level labels to generate clinically meaningful heatmaps that highlight\nregions affected by pneumonia. We evaluate seven ImageNet-pretrained\narchitectures ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V2/V3, and\nViT-B16 under identical training conditions with focal loss and patient-wise\nsplits to prevent data leakage. Experimental results on the Kermany CXR dataset\ndemonstrate that ResNet-18 and EfficientNet-B0 achieve the best overall test\naccuracy of 98\\%, ROC-AUC = 0.997, and F1 = 0.987, while MobileNet-V2 provides\nan optimal trade-off between accuracy and computational cost. Grad-CAM\nvisualizations confirm that the proposed models focus on clinically relevant\nlung regions, supporting the use of interpretable AI for radiological\ndiagnostics. This work highlights the potential of weakly supervised\nexplainable models that enhance pneumonia screening transparency, and clinical\ntrust in AI-assisted medical imaging.\n  https://github.com/kiranshahi/pneumonia-analysis", "AI": {"tldr": "The paper presents a weakly supervised deep learning framework using Grad-CAM to classify and localize pneumonia from chest X-rays with only image-level labels.", "motivation": "Pneumonia detection from chest X-rays often requires pixel-level annotations, which can be expensive and time-consuming. The motivation is to provide a cost-effective and interpretable AI solution with minimal annotation.", "method": "The framework employs Grad-CAM explanations with seven pre-trained architectures, using image-level labels and patient-wise data splits to prevent leakage. Focal loss is utilized during training.", "result": "ResNet-18 and EfficientNet-B0 deliver the highest accuracy (98%), while MobileNet-V2 offers a good accuracy-to-resource-cost trade-off. Grad-CAM visualizations confirm clinically relevant heatmaps.", "conclusion": "The proposed weakly supervised, explainable AI models enhance transparency and clinical trust in pneumonia diagnosis while making the process cost-effective and interpretable."}}
{"id": "2511.01668", "pdf": "https://arxiv.org/pdf/2511.01668", "abs": "https://arxiv.org/abs/2511.01668", "authors": ["Yueqing Xi", "Yifan Bai", "Huasen Luo", "Weiliang Wen", "Hui Liu", "Haoliang Li"], "title": "Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics", "categories": ["cs.AI"], "comment": null, "summary": "As artificial intelligence permeates judicial forensics, ensuring the\nveracity and traceability of legal question answering (QA) has become critical.\nConventional large language models (LLMs) are prone to hallucination, risking\nmisleading guidance in legal consultation, while static knowledge bases\nstruggle to keep pace with frequently updated statutes and case law. We present\na hybrid legal QA agent tailored for judicial settings that integrates\nretrieval-augmented generation (RAG) with multi-model ensembling to deliver\nreliable, auditable, and continuously updatable counsel. The system prioritizes\nretrieval over generation: when a trusted legal repository yields relevant\nevidence, answers are produced via RAG; otherwise, multiple LLMs generate\ncandidates that are scored by a specialized selector, with the top-ranked\nanswer returned. High-quality outputs then undergo human review before being\nwritten back to the repository, enabling dynamic knowledge evolution and\nprovenance tracking. Experiments on the Law\\_QA dataset show that our hybrid\napproach significantly outperforms both a single-model baseline and a vanilla\nRAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm\nthe complementary contributions of retrieval prioritization, model ensembling,\nand the human-in-the-loop update mechanism. The proposed system demonstrably\nreduces hallucination while improving answer quality and legal compliance,\nadvancing the practical landing of media forensics technologies in judicial\nscenarios.", "AI": {"tldr": "The paper proposes a hybrid AI agent designed for legal question answering that integrates retrieval-augmented generation (RAG) and multi-model ensembling, significantly enhancing accuracy and reliability while reducing hallucination.", "motivation": "Ensuring the accuracy, traceability, and reliability of answers in legal question answering is critical, as existing systems either hallucinate or fail to adapt to evolving legal standards.", "method": "The system combines retrieval-augmented generation (RAG) with multi-model ensembling. It prioritizes retrieving answers from trusted repositories and uses LLMs with scoring mechanisms when retrieval yields insufficient results. Human-reviewed outputs update the knowledge base dynamically.", "result": "Experiments demonstrate improved performance on the Law_QA dataset compared to baseline systems, with significant advances in metrics like F1, ROUGE-L, and a unique LLM-as-a-Judge measure.", "conclusion": "This hybrid legal QA system reduces hallucination, improves answer quality, ensures legal compliance, and contributes to practical advancements in judicial forensics technologies."}}
{"id": "2511.01192", "pdf": "https://arxiv.org/pdf/2511.01192", "abs": "https://arxiv.org/abs/2511.01192", "authors": ["Guoxin Ma", "Xiaoming Liu", "Zhanhan Zhang", "Chengzhengxu Li", "Shengchao Liu", "Yu Lan"], "title": "DEER: Disentangled Mixture of Experts with Instance-Adaptive Routing for Generalizable Machine-Generated Text Detection", "categories": ["cs.CL"], "comment": "Under Review", "summary": "Detecting machine-generated text (MGT) has emerged as a critical challenge,\ndriven by the rapid advancement of large language models (LLMs) capable of\nproducing highly realistic, human-like content. However, the performance of\ncurrent approaches often degrades significantly under domain shift. To address\nthis challenge, we propose a novel framework designed to capture both\ndomain-specific and domain-general MGT patterns through a two-stage\nDisentangled mixturE-of-ExpeRts (DEER) architecture. First, we introduce a\ndisentangled mixture-of-experts module, in which domain-specific experts learn\nfine-grained, domain-local distinctions between human and machine-generated\ntext, while shared experts extract transferable, cross-domain features. Second,\nto mitigate the practical limitation of unavailable domain labels during\ninference, we design a reinforcement learning-based routing mechanism that\ndynamically selects the appropriate experts for each input instance,\neffectively bridging the train-inference gap caused by domain uncertainty.\nExtensive experiments on five in-domain and five out-of-domain benchmark\ndatasets demonstrate that DEER consistently outperforms state-of-the-art\nmethods, achieving average F1-score improvements of 1.39% and 5.32% on\nin-domain and out-of-domain datasets respectively, along with accuracy gains of\n1.35% and 3.61% respectively. Ablation studies confirm the critical\ncontributions of both disentangled expert specialization and adaptive routing\nto model performance.", "AI": {"tldr": "The paper addresses the challenges of detecting machine-generated text using a novel two-stage DEER architecture to improve accuracy, especially under domain shifts.", "motivation": "Improve the detection of machine-generated text, which struggles under domain shifts, as existing methods perform poorly in cross-domain scenarios.", "method": "Introduced the DEER architecture with domain-specific experts for localized distinctions and shared experts for transferable features, complemented by reinforcement learning-based routing.", "result": "Achieved notable performance improvements with average F1-score gains of 1.39% (in-domain) and 5.32% (out-of-domain), and accuracy gains of 1.35% (in-domain) and 3.61% (out-of-domain).", "conclusion": "The DEER approach leverages disentangled expert specialization and adaptive routing mechanisms to successfully enhance machine-generated text detection under domain uncertainty."}}
{"id": "2511.01476", "pdf": "https://arxiv.org/pdf/2511.01476", "abs": "https://arxiv.org/abs/2511.01476", "authors": ["Cankut Bora Tuncer", "Marc Toussaint", "Ozgur S. Oguz"], "title": "MO-SeGMan: Rearrangement Planning Framework for Multi Objective Sequential and Guided Manipulation in Constrained Environments", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, 8 figures, website:https://sites.google.com/view/mo-segman/", "summary": "In this work, we introduce MO-SeGMan, a Multi-Objective Sequential and Guided\nManipulation planner for highly constrained rearrangement problems. MO-SeGMan\ngenerates object placement sequences that minimize both replanning per object\nand robot travel distance while preserving critical dependency structures with\na lazy evaluation method. To address highly cluttered, non-monotone scenarios,\nwe propose a Selective Guided Forward Search (SGFS) that efficiently relocates\nonly critical obstacles and to feasible relocation points. Furthermore, we\nadopt a refinement method for adaptive subgoal selection to eliminate\nunnecessary pick-and-place actions, thereby improving overall solution quality.\nExtensive evaluations on nine benchmark rearrangement tasks demonstrate that\nMO-SeGMan generates feasible motion plans in all cases, consistently achieving\nfaster solution times and superior solution quality compared to the baselines.\nThese results highlight the robustness and scalability of the proposed\nframework for complex rearrangement planning problems.", "AI": {"tldr": "MO-SeGMan is a framework for solving highly constrained rearrangement problems by minimizing replanning and robot travel distances while ensuring efficiency and solution quality.", "motivation": "The paper aims to address the challenge of planning object rearrangements in cluttered, highly constrained environments while optimizing solution quality and efficiency.", "method": "The authors developed MO-SeGMan, incorporating Selective Guided Forward Search to tackle cluttered scenarios and a refinement method for adaptive subgoal selection to improve solution quality.", "result": "Experiments showed that MO-SeGMan consistently outperforms baseline approaches by generating faster, high-quality solutions for benchmark rearrangement tasks.", "conclusion": "MO-SeGMan is a robust and scalable solution for tackling complex rearrangement planning problems."}}
{"id": "2511.00220", "pdf": "https://arxiv.org/pdf/2511.00220", "abs": "https://arxiv.org/abs/2511.00220", "authors": ["Pouya M. Ghari", "Simone Sciabola", "Ye Wang"], "title": "Iterative Foundation Model Fine-Tuning on Multiple Rewards", "categories": ["cs.LG"], "comment": "Accepted to NeurIPS 2025", "summary": "Fine-tuning foundation models has emerged as a powerful approach for\ngenerating objects with specific desired properties. Reinforcement learning\n(RL) provides an effective framework for this purpose, enabling models to\ngenerate outputs that maximize a given reward function. However, in many\napplications such as text generation and drug discovery, it can be suboptimal\nto optimize using a single reward signal, as multiple evaluation criteria are\noften necessary. This paper proposes a novel reinforcement learning-based\nmethod for fine-tuning foundation models using multiple reward signals. By\nemploying an iterative fine-tuning strategy across these rewards, our approach\ngeneralizes state-of-the-art RL-based methods. We further provide a theoretical\nanalysis that offers insights into the performance of multi-reward RL\nfine-tuning. Experimental results across diverse domains including text,\nbiological sequence, and small molecule generation, demonstrate the\neffectiveness of the proposed algorithm compared to state-of-the-art baselines.", "AI": {"tldr": "This paper introduces a reinforcement learning (RL)-based method to fine-tune foundation models using multiple reward signals, showing improved results across various fields.", "motivation": "Many applications require multiple evaluation criteria for optimization, but traditional single-reward RL methods are suboptimal for such tasks.", "method": "The paper introduces an iterative RL fine-tuning strategy using multiple reward signals, alongside theoretical analysis to understand its performance.", "result": "Experimental results reveal significant improvements over state-of-the-art methods in areas like text, biological sequence, and molecular generation.", "conclusion": "The proposed multi-reward RL fine-tuning approach is effective and generalizes traditional RL-based methods, offering better performance in models with complex criteria."}}
{"id": "2511.00468", "pdf": "https://arxiv.org/pdf/2511.00468", "abs": "https://arxiv.org/abs/2511.00468", "authors": ["Panwang Pan", "Tingting Shen", "Chenxin Li", "Yunlong Lin", "Kairun Wen", "Jingjing Zhao", "Yixuan Yuan"], "title": "HumanCrafter: Synergizing Generalizable Human Reconstruction and Semantic 3D Segmentation", "categories": ["cs.CV"], "comment": "Accepted to NeurIPS 2025; Project page: [this\n  URL](https://paulpanwang.github.io/HumanCrafter)", "summary": "Recent advances in generative models have achieved high-fidelity in 3D human\nreconstruction, yet their utility for specific tasks (e.g., human 3D\nsegmentation) remains constrained. We propose HumanCrafter, a unified framework\nthat enables the joint modeling of appearance and human-part semantics from a\nsingle image in a feed-forward manner. Specifically, we integrate human\ngeometric priors in the reconstruction stage and self-supervised semantic\npriors in the segmentation stage. To address labeled 3D human datasets\nscarcity, we further develop an interactive annotation procedure for generating\nhigh-quality data-label pairs. Our pixel-aligned aggregation enables cross-task\nsynergy, while the multi-task objective simultaneously optimizes texture\nmodeling fidelity and semantic consistency. Extensive experiments demonstrate\nthat HumanCrafter surpasses existing state-of-the-art methods in both 3D\nhuman-part segmentation and 3D human reconstruction from a single image.", "AI": {"tldr": "HumanCrafter is a unified framework designed to improve 3D human reconstruction and segmentation tasks from a single image.", "motivation": "To address the limitations of existing generative models for specific tasks such as human 3D segmentation and tackle the scarcity of labeled 3D human datasets.", "method": "HumanCrafter integrates human geometric priors for reconstruction, self-supervised semantic priors for segmentation, and employs a multi-task objective for improved fidelity and semantic consistency.", "result": "HumanCrafter outperforms existing methods in 3D human-part segmentation and 3D human reconstruction from a single image.", "conclusion": "HumanCrafter enables simultaneous modeling of appearance and semantics, offering advancements in accuracy and quality for human 3D reconstruction and segmentation tasks."}}
{"id": "2511.01824", "pdf": "https://arxiv.org/pdf/2511.01824", "abs": "https://arxiv.org/abs/2511.01824", "authors": ["Yuetai Li", "Huseyin A Inan", "Xiang Yue", "Wei-Ning Chen", "Lukas Wutschitz", "Janardhan Kulkarni", "Radha Poovendran", "Robert Sim", "Saravan Rajmohan"], "title": "Simulating Environments with Reasoning Models for Agent Training", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "LLM agents excel in compact environments requiring deep reasoning but remain\nbrittle when operating in broader, more complex contexts that demand robustness\nacross diverse tools and schemas. Building bespoke environments for training is\nheavy, brittle, and limits progress. In this paper, we demonstrate that LLMs\ncan simulate realistic environment feedback without access to actual testbed\ndata or APIs. Inspired by this capability, we propose two frameworks:\nSimia-SFT, a pipeline that synthesizes SFT data by amplifying small seed sets\ninto diverse trajectories in an environment-agnostic manner, and Simia-RL, a\nframework that enables RL training without real environment implementations\nthrough LLM-simulated feedback. Fine-tuning open models yields consistent\nimprovements across multiple benchmarks, surpassing GPT-4o and approaching\no4-mini on $\\tau^2$-Bench. Together, Simia-SFT and Simia-RL enable scalable\nagent training without environment engineering, replacing heavy and brittle\nimplementations with flexible LLM-based simulation.", "AI": {"tldr": "This paper proposes two frameworks, Simia-SFT and Simia-RL, for scalable LLM agent training using simulated feedback, bypassing the need for real environment implementations.", "motivation": "Current LLM agents struggle in complex contexts requiring robustness across diverse tools and schemas, and existing bespoke environments for training are heavy and brittle.", "method": "The authors introduce Simia-SFT to amplify small seed datasets into diverse trajectories using LLM simulation, and Simia-RL for RL training with simulated feedback instead of real environments.", "result": "Fine-tuned open models demonstrated significant improvements across benchmarks, notably surpassing GPT-4o and nearing o4-mini performance on $\u0001\u0002$-Bench.", "conclusion": "Using LLM-based simulation frameworks, this approach provides a scalable, flexible alternative to traditional environment engineering for LLM agent training."}}
{"id": "2511.01265", "pdf": "https://arxiv.org/pdf/2511.01265", "abs": "https://arxiv.org/abs/2511.01265", "authors": ["Mo El-Haj", "Paul Rayson"], "title": "AraFinNews: Arabic Financial Summarisation with Domain-Adapted LLMs", "categories": ["cs.CL"], "comment": "10 pages", "summary": "This paper investigates the impact of domain specificity on abstractive\nsummarisation of Arabic financial texts using large language models (LLMs). We\nintroduce AraFinNews, the largest publicly available Arabic financial news\ndataset to date, comprising 212,500 article--headline pairs spanning nearly a\ndecade of reporting from October 2015 to July 2025. Designed as the Arabic\nequivalent of major English summarisation corpora such as CNN/DailyMail,\nAraFinNews provides a robust benchmark for evaluating domain-specific language\nunderstanding and generation in financial contexts. Using this resource, we\nevaluate transformer-based models -- including mT5, AraT5, and the\ndomain-adapted FinAraT5 -- to examine how financial-domain pretraining\ninfluences factual accuracy, numerical reliability, and stylistic alignment\nwith professional reporting. Experimental results show that domain-adapted\nmodels generate more faithful and coherent summaries, particularly in handling\nquantitative and entity-centric information. The findings highlight the\nimportance of domain-specific adaptation for improving factual consistency and\nnarrative fluency in Arabic financial summarisation. The dataset is freely\navailable for non-commercial research at\nhttps://github.com/ArabicNLP-UK/AraFinNews.", "AI": {"tldr": "The paper introduces AraFinNews, a large Arabic financial news dataset, and evaluates domain-specific summarization using transformer models, highlighting the importance of domain adaptation.", "motivation": "Investigate the role of domain specificity in improving abstractive summarization of Arabic financial texts.", "method": "Created AraFinNews dataset and used transformer-based models (mT5, AraT5, FinAraT5) to evaluate their summarization performance.", "result": "Domain-adapted models produced more accurate and coherent summaries, especially for numerical and entity information.", "conclusion": "Domain-specific adaptation enhances summarization quality, ensuring better factual and narrative accuracy in Arabic financial texts."}}
{"id": "2511.01493", "pdf": "https://arxiv.org/pdf/2511.01493", "abs": "https://arxiv.org/abs/2511.01493", "authors": ["Wei Huang", "Jiaxin Li", "Zang Wan", "Huijun Di", "Wei Liang", "Zhu Yang"], "title": "Floor Plan-Guided Visual Navigation Incorporating Depth and Directional Cues", "categories": ["cs.RO"], "comment": null, "summary": "Guiding an agent to a specific target in indoor environments based solely on\nRGB inputs and a floor plan is a promising yet challenging problem. Although\nexisting methods have made significant progress, two challenges remain\nunresolved. First, the modality gap between egocentric RGB observations and the\nfloor plan hinders the integration of visual and spatial information for both\nlocal obstacle avoidance and global planning. Second, accurate localization is\ncritical for navigation performance, but remains challenging at deployment in\nunseen environments due to the lack of explicit geometric alignment between RGB\ninputs and floor plans. We propose a novel diffusion-based policy, denoted as\nGlocDiff, which integrates global path planning from the floor plan with local\ndepth-aware features derived from RGB observations. The floor plan offers\nexplicit global guidance, while the depth features provide implicit geometric\ncues, collectively enabling precise prediction of optimal navigation directions\nand robust obstacle avoidance. Moreover, GlocDiff introduces noise perturbation\nduring training to enhance robustness against pose estimation errors, and we\nfind that combining this with a relatively stable VO module during inference\nresults in significantly improved navigation performance. Extensive experiments\non the FloNa benchmark demonstrate GlocDiff's efficiency and effectiveness in\nachieving superior navigation performance, and the success of real-world\ndeployments also highlights its potential for widespread practical\napplications.", "AI": {"tldr": "This paper introduces GlocDiff, a diffusion-based navigation policy leveraging floor plans and RGB inputs for indoor target guidance, addressing challenges like modality gap and localization accuracy.", "motivation": "The paper aims to overcome challenges in indoor navigation such as integrating visual and spatial information and ensuring accurate localization in unseen environments.", "method": "GlocDiff integrates global path planning from floor plans with local depth-aware features derived from RGB observations and enhances robustness with noise perturbation during training.", "result": "Extensive experiments show that GlocDiff achieves superior navigation performance on the FloNa benchmark, effective obstacle avoidance, and precise direction prediction.", "conclusion": "GlocDiff demonstrates promising results for indoor navigation, showing potential for practical real-world applications."}}
{"id": "2511.00246", "pdf": "https://arxiv.org/pdf/2511.00246", "abs": "https://arxiv.org/abs/2511.00246", "authors": ["Wadduwage Shanika Perera", "ABM Islam", "Van Vung Pham", "Min Kyung An"], "title": "Melanoma Classification Through Deep Ensemble Learning and Explainable AI", "categories": ["cs.LG", "cs.CV"], "comment": "Publisher-formatted version provided under CC BY-NC-ND 4.0 license.\n  Original source produced by SciTePress", "summary": "Melanoma is one of the most aggressive and deadliest skin cancers, leading to\nmortality if not detected and treated in the early stages. Artificial\nintelligence techniques have recently been developed to help dermatologists in\nthe early detection of melanoma, and systems based on deep learning (DL) have\nbeen able to detect these lesions with high accuracy. However, the entire\ncommunity must overcome the explainability limit to get the maximum benefit\nfrom DL for diagnostics in the healthcare domain. Because of the black box\noperation's shortcomings in DL models' decisions, there is a lack of\nreliability and trust in the outcomes. However, Explainable Artificial\nIntelligence (XAI) can solve this problem by interpreting the predictions of AI\nsystems. This paper proposes a machine learning model using ensemble learning\nof three state-of-the-art deep transfer Learning networks, along with an\napproach to ensure the reliability of the predictions by utilizing XAI\ntechniques to explain the basis of the predictions.", "AI": {"tldr": "A paper proposes an ensemble deep transfer learning model combined with Explainable Artificial Intelligence (XAI) techniques for reliable melanoma detection.", "motivation": "Early detection of melanoma is crucial for reducing mortality, but standard deep learning solutions face challenges in trust and reliability due to lack of explainability.", "method": "An ensemble model using three state-of-the-art deep transfer learning networks paired with XAI techniques to ensure transparent and informed predictions.", "result": "The approach achieves high accuracy in melanoma detection while providing interpretative insights into the decision-making process.", "conclusion": "Integrating XAI techniques in DL-based models enhances the reliability and trustworthiness of melanoma detection systems in healthcare diagnostics."}}
{"id": "2511.00472", "pdf": "https://arxiv.org/pdf/2511.00472", "abs": "https://arxiv.org/abs/2511.00472", "authors": ["Navodini Wijethilake", "Marina Ivory", "Oscar MacCormac", "Siddhant Kumar", "Aaron Kujawa", "Lorena Garcia-Foncillas Macias", "Rebecca Burger", "Amanda Hitchings", "Suki Thomson", "Sinan Barazi", "Eleni Maratos", "Rupert Obholzer", "Dan Jiang", "Fiona McClenaghan", "Kazumi Chia", "Omar Al-Salihi", "Nick Thomas", "Steve Connor", "Tom Vercauteren", "Jonathan Shapey"], "title": "Longitudinal Vestibular Schwannoma Dataset with Consensus-based Human-in-the-loop Annotations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate segmentation of vestibular schwannoma (VS) on Magnetic Resonance\nImaging (MRI) is essential for patient management but often requires\ntime-intensive manual annotations by experts. While recent advances in deep\nlearning (DL) have facilitated automated segmentation, challenges remain in\nachieving robust performance across diverse datasets and complex clinical\ncases. We present an annotated dataset stemming from a bootstrapped DL-based\nframework for iterative segmentation and quality refinement of VS in MRI. We\ncombine data from multiple centres and rely on expert consensus for\ntrustworthiness of the annotations. We show that our approach enables effective\nand resource-efficient generalisation of automated segmentation models to a\ntarget data distribution. The framework achieved a significant improvement in\nsegmentation accuracy with a Dice Similarity Coefficient (DSC) increase from\n0.9125 to 0.9670 on our target internal validation dataset, while maintaining\nstable performance on representative external datasets. Expert evaluation on\n143 scans further highlighted areas for model refinement, revealing nuanced\ncases where segmentation required expert intervention. The proposed approach is\nestimated to enhance efficiency by approximately 37.4% compared to the\nconventional manual annotation process. Overall, our human-in-the-loop model\ntraining approach achieved high segmentation accuracy, highlighting its\npotential as a clinically adaptable and generalisable strategy for automated VS\nsegmentation in diverse clinical settings. The dataset includes 190 patients,\nwith tumour annotations available for 534 longitudinal contrast-enhanced\nT1-weighted (T1CE) scans from 184 patients, and non-annotated T2-weighted scans\nfrom 6 patients. This dataset is publicly accessible on The Cancer Imaging\nArchive (TCIA) (https://doi.org/10.7937/bq0z-xa62).", "AI": {"tldr": "The study proposes a human-in-the-loop deep learning framework to improve automated vestibular schwannoma segmentation in MRI, achieving high segmentation accuracy and substantial efficiency gains.", "motivation": "Manual annotations for vestibular schwannoma segmentation in MRI are time-consuming but crucial for patient management; existing automated methods lack robustness across diverse datasets.", "method": "A bootstrapped deep learning framework with iterative segmentation and quality refinement leveraging expert consensus and data from multiple centers.", "result": "The framework enhanced Dice Similarity Coefficient from 0.9125 to 0.9670, maintained stable performance across external datasets, and reduced annotation effort by 37.4%.", "conclusion": "The approach demonstrated the potential for accurate, resource-efficient, and clinically adaptable automated segmentation in diverse MRI datasets."}}
{"id": "2403.15181", "pdf": "https://arxiv.org/pdf/2403.15181", "abs": "https://arxiv.org/abs/2403.15181", "authors": ["Alexandre Valentin Jamet", "Georgios Vavouliotis", "Daniel A. Jim\u00e9nez", "Lluc Alvarez", "Marc Casas"], "title": "A Two Level Neural Approach Combining Off-Chip Prediction with Adaptive Prefetch Filtering", "categories": ["cs.AR", "cs.AI"], "comment": "To appear in 30th International Symposium on High-Performance\n  Computer Architecture (HPCA), 2024", "summary": "To alleviate the performance and energy overheads of contemporary\napplications with large data footprints, we propose the Two Level Perceptron\n(TLP) predictor, a neural mechanism that effectively combines predicting\nwhether an access will be off-chip with adaptive prefetch filtering at the\nfirst-level data cache (L1D). TLP is composed of two connected\nmicroarchitectural perceptron predictors, named First Level Predictor (FLP) and\nSecond Level Predictor (SLP). FLP performs accurate off-chip prediction by\nusing several program features based on virtual addresses and a novel selective\ndelay component. The novelty of SLP relies on leveraging off-chip prediction to\ndrive L1D prefetch filtering by using physical addresses and the FLP prediction\nas features. TLP constitutes the first hardware proposal targeting both\noff-chip prediction and prefetch filtering using a multi-level perceptron\nhardware approach. TLP only requires 7KB of storage. To demonstrate the\nbenefits of TLP we compare its performance with state-of-the-art approaches\nusing off-chip prediction and prefetch filtering on a wide range of single-core\nand multi-core workloads. Our experiments show that TLP reduces the average\nDRAM transactions by 30.7% and 17.7%, as compared to a baseline using\nstate-of-the-art cache prefetchers but no off-chip prediction mechanism, across\nthe single-core and multi-core workloads, respectively, while recent work\nsignificantly increases DRAM transactions. As a result, TLP achieves geometric\nmean performance speedups of 6.2% and 11.8% across single-core and multi-core\nworkloads, respectively. In addition, our evaluation demonstrates that TLP is\neffective independently of the L1D prefetching logic.", "AI": {"tldr": "The paper presents the Two Level Perceptron (TLP) predictor, a neural mechanism for enhancing performance and energy efficiency in applications with large data footprints by combining off-chip prediction with adaptive prefetch filtering at the L1D cache level.", "motivation": "Current applications with large data footprints suffer from performance and energy overheads, necessitating more efficient methods to predict off-chip data access and optimize prefetch filtering.", "method": "TLP uses two microarchitectural perceptron predictors called FLP (for off-chip prediction) and SLP (for L1D prefetch filtering). FLP leverages virtual addresses and selective delay components, while SLP integrates physical addresses and FLP predictions.", "result": "TLP reduces average DRAM transactions by 30.7% (single-core) and 17.7% (multi-core) compared to baseline prefetchers. It achieves geometric mean performance improvements of 6.2% and 11.8% for single-core and multi-core workloads, respectively.", "conclusion": "TLP provides an innovative, lightweight (7KB storage) neural solution to off-chip prediction and prefetch filtering that outperforms state-of-the-art approaches and maintains effectiveness across different L1D prefetching logics."}}
{"id": "2511.01282", "pdf": "https://arxiv.org/pdf/2511.01282", "abs": "https://arxiv.org/abs/2511.01282", "authors": ["Min Fang", "Zhihui Fu", "Qibin Zhao", "Jun Wang"], "title": "When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Speculative decoding (SD) has emerged as an effective technique to accelerate\nlarge language model (LLM) inference without compromising output quality.\nHowever, the achievable speedup largely depends on the effectiveness of the\ndrafting model. While model-based methods like EAGLE-2 are accurate but costly,\nretrieval-enhanced methods like SAM-Decoding rely on heuristic switching\nstrategies that often trigger unnecessary retrievals. To address this, we\npropose ReSpec (\\textbf{Re}trieval-enhanced \\textbf{Spe}culative Decoding), a\nnovel framework that transforms heuristic drafter switching into adaptive\ndecision-making. ReSpec features three core innovations: 1) An\n\\textbf{entropy-guided adaptive trigger} quantifies contextual predictability\nto initiate retrieval only when uncertainty is low, avoiding costly low-quality\nspeculations. 2) A \\textbf{feedback-driven candidate selection} leverages\nhistorical feedback to organize multiple high-quality candidates for parallel\nverification, maximizing retrieval utility. 3) A source-aware \\textbf{relaxed\nverification strategy} applies strict checks to model-generated drafts while\nusing a relaxed verification for retrieved drafts, achieving a better balance\nbetween accuracy and efficiency. Extensive experiments on Spec-Bench\ndemonstrate that ReSpec achieves state-of-the-art acceleration,outperforming\nEAGLE-2 and SAM-Decoding by over $33\\%$ and $25\\%$, respectively, while\nmaintaining output quality.", "AI": {"tldr": "ReSpec is a new framework for speculative decoding that improves speed and quality over existing methods like EAGLE-2 and SAM-Decoding.", "motivation": "The motivation is to enhance the efficiency and accuracy of speculative decoding for large language model inference by addressing the limitations of current methods.", "method": "ReSpec introduces adaptive decision-making with three key innovations: an entropy-guided adaptive trigger, feedback-driven candidate selection, and a source-aware relaxed verification strategy.", "result": "ReSpec achieves state-of-the-art acceleration in speculative decoding, surpassing EAGLE-2 by over 33% and SAM-Decoding by over 25% in terms of speedup.", "conclusion": "ReSpec provides a significant advancement in speculative decoding by combining contextual adaptability, feedback utilization, and source-awareness to maximize both efficiency and output quality."}}
{"id": "2511.01520", "pdf": "https://arxiv.org/pdf/2511.01520", "abs": "https://arxiv.org/abs/2511.01520", "authors": ["Shipeng Lyu", "Lijie Sheng", "Fangyuan Wang", "Wenyao Zhang", "Weiwei Lin", "Zhenzhong Jia", "David Navarro-Alarcon", "Guodong Guo"], "title": "Phy-Tac: Toward Human-Like Grasping via Physics-Conditioned Tactile Goals", "categories": ["cs.RO"], "comment": "9 papges, 10 figures, 3 tables", "summary": "Humans naturally grasp objects with minimal level required force for\nstability, whereas robots often rely on rigid, over-squeezing control. To\nnarrow this gap, we propose a human-inspired physics-conditioned tactile method\n(Phy-Tac) for force-optimal stable grasping (FOSG) that unifies pose selection,\ntactile prediction, and force regulation. A physics-based pose selector first\nidentifies feasible contact regions with optimal force distribution based on\nsurface geometry. Then, a physics-conditioned latent diffusion model (Phy-LDM)\npredicts the tactile imprint under FOSG target. Last, a latent-space LQR\ncontroller drives the gripper toward this tactile imprint with minimal\nactuation, preventing unnecessary compression. Trained on a physics-conditioned\ntactile dataset covering diverse objects and contact conditions, the proposed\nPhy-LDM achieves superior tactile prediction accuracy, while the Phy-Tac\noutperforms fixed-force and GraspNet-based baselines in grasp stability and\nforce efficiency. Experiments on classical robotic platforms demonstrate\nforce-efficient and adaptive manipulation that bridges the gap between robotic\nand human grasping.", "AI": {"tldr": "The paper introduces Phy-Tac, a human-inspired method for efficient and stable robotic grasping.", "motivation": "Robots often use excessive force during grasping, unlike humans who optimize force for stability. The authors aim to bridge this gap.", "method": "The approach integrates pose selection, tactile prediction using a physics-conditioned latent diffusion model, and force regulation through a latent-space LQR controller.", "result": "The proposed Phy-Tac method showed better grasp stability and force efficiency compared to existing baselines, validated via experiments on robotic platforms.", "conclusion": "The Phy-Tac method successfully improves robotic grasping efficiency and stability, aligning robotic behavior closer to human-like performance."}}
{"id": "2511.00480", "pdf": "https://arxiv.org/pdf/2511.00480", "abs": "https://arxiv.org/abs/2511.00480", "authors": ["Weihao Bo", "Yanpeng Sun", "Yu Wang", "Xinyu Zhang", "Zechao Li"], "title": "FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In this paper, we introduce FedMGP, a new paradigm for personalized federated\nprompt learning in vision-language models. FedMGP equips each client with\nmultiple groups of paired textual and visual prompts, enabling the model to\ncapture diverse, fine-grained semantic and instance-level cues. A diversity\nloss is introduced to drive each prompt group to specialize in distinct and\ncomplementary semantic aspects, ensuring that the groups collectively cover a\nbroader range of local characteristics. During communication, FedMGP employs a\ndynamic prompt aggregation strategy based on similarity-guided probabilistic\nsampling: each client computes the cosine similarity between its prompt groups\nand the global prompts from the previous round, then samples s groups via a\nsoftmax-weighted distribution. This soft selection mechanism preferentially\naggregates semantically aligned knowledge while still enabling exploration of\nunderrepresented patterns effectively balancing the preservation of common\nknowledge with client-specific features. Notably, FedMGP maintains parameter\nefficiency by redistributing a fixed prompt capacity across multiple groups,\nachieving state-of-the-art performance with the lowest communication parameters\namong all federated prompt learning methods. Theoretical analysis shows that\nour dynamic aggregation strategy promotes robust global representation learning\nby reinforcing shared semantics while suppressing client-specific noise.\nExtensive experiments demonstrate that FedMGP consistently outperforms prior\napproaches in both personalization and domain generalization across diverse\nfederated vision-language benchmarks. The code will be released on\nhttps://github.com/weihao-bo/FedMGP.git.", "AI": {"tldr": "FedMGP introduces a novel approach for personalized federated prompt learning using diverse prompt groups, achieving superior performance in vision-language benchmarks with a focus on parameter efficiency.", "motivation": "The paper aims to improve federated prompt learning in vision-language models by addressing the need for capturing diverse, fine-grained semantic cues and achieving a balance between common and client-specific knowledge.", "method": "FedMGP uses multiple groups of paired textual and visual prompts, a diversity loss encouraging specialization, and a dynamic prompt aggregation strategy based on similarity-guided probabilistic sampling for communication.", "result": "FedMGP achieves state-of-the-art performance in personalization and domain generalization across federated vision-language benchmarks with the lowest communication parameters among all existing federated prompt learning methods.", "conclusion": "FedMGP is an efficient and effective method for personalized federated prompt learning, promoting robust global representation learning and advancing the capabilities of vision-language models."}}
{"id": "2511.01287", "pdf": "https://arxiv.org/pdf/2511.01287", "abs": "https://arxiv.org/abs/2511.01287", "authors": ["Qin Zhou", "Zhexin Zhang", "Zhi Li", "Limin Sun"], "title": "\"Give a Positive Review Only\": An Early Investigation Into In-Paper Prompt Injection Attacks and Defenses for AI Reviewers", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "With the rapid advancement of AI models, their deployment across diverse\ntasks has become increasingly widespread. A notable emerging application is\nleveraging AI models to assist in reviewing scientific papers. However, recent\nreports have revealed that some papers contain hidden, injected prompts\ndesigned to manipulate AI reviewers into providing overly favorable\nevaluations. In this work, we present an early systematic investigation into\nthis emerging threat. We propose two classes of attacks: (1) static attack,\nwhich employs a fixed injection prompt, and (2) iterative attack, which\noptimizes the injection prompt against a simulated reviewer model to maximize\nits effectiveness. Both attacks achieve striking performance, frequently\ninducing full evaluation scores when targeting frontier AI reviewers.\nFurthermore, we show that these attacks are robust across various settings. To\ncounter this threat, we explore a simple detection-based defense. While it\nsubstantially reduces the attack success rate, we demonstrate that an adaptive\nattacker can partially circumvent this defense. Our findings underscore the\nneed for greater attention and rigorous safeguards against prompt-injection\nthreats in AI-assisted peer review.", "AI": {"tldr": "This paper explores the emerging threat of hidden prompt injections designed to manipulate AI reviewers into giving overly favorable evaluations of scientific papers. It introduces two types of attacks, evaluates their performance, and discusses detection-based countermeasures.", "motivation": "The motivation is the increasing use of AI models in reviewing scientific papers and the emerging threat of adversarial prompt injections to manipulate review outcomes.", "method": "Researchers proposed two types of attacks: static attacks with fixed prompts and iterative attacks optimized against simulated reviewer models. They also tested a detection-based defense mechanism.", "result": "Both types of attacks showed high effectiveness, often leading to favorable full evaluation scores from AI reviewers. The detection defense reduced attack success but wasn't fully unbeaten by adaptive attackers.", "conclusion": "The study highlights the vulnerability of AI-assisted peer review systems to prompt-injection threats and emphasizes the need for further safeguarding measures."}}
{"id": "2511.01594", "pdf": "https://arxiv.org/pdf/2511.01594", "abs": "https://arxiv.org/abs/2511.01594", "authors": ["Renjun Gao", "Peiyan Zhong"], "title": "MARS: Multi-Agent Robotic System with Multimodal Large Language Models for Assistive Intelligence", "categories": ["cs.RO", "cs.CV", "I.2.9; I.2.11; I.2.6; I.4.8"], "comment": "3 figures, 1 table; under review at Multimedia Systems (Springer)", "summary": "Multimodal large language models (MLLMs) have shown remarkable capabilities\nin cross-modal understanding and reasoning, offering new opportunities for\nintelligent assistive systems, yet existing systems still struggle with\nrisk-aware planning, user personalization, and grounding language plans into\nexecutable skills in cluttered homes. We introduce MARS - a Multi-Agent Robotic\nSystem powered by MLLMs for assistive intelligence and designed for smart home\nrobots supporting people with disabilities. The system integrates four agents:\na visual perception agent for extracting semantic and spatial features from\nenvironment images, a risk assessment agent for identifying and prioritizing\nhazards, a planning agent for generating executable action sequences, and an\nevaluation agent for iterative optimization. By combining multimodal perception\nwith hierarchical multi-agent decision-making, the framework enables adaptive,\nrisk-aware, and personalized assistance in dynamic indoor environments.\nExperiments on multiple datasets demonstrate the superior overall performance\nof the proposed system in risk-aware planning and coordinated multi-agent\nexecution compared with state-of-the-art multimodal models. The proposed\napproach also highlights the potential of collaborative AI for practical\nassistive scenarios and provides a generalizable methodology for deploying\nMLLM-enabled multi-agent systems in real-world environments.", "AI": {"tldr": "This paper presents MARS, a multi-agent robotic system powered by multimodal large language models (MLLMs) for intelligent, adaptive, and risk-aware assistance in smart home environments.", "motivation": "Existing assistive systems face challenges in planning around risks, personalizing for users, and grounding language plans into actions, particularly in cluttered home settings. The research aims to enhance these aspects using MLLMs.", "method": "The study introduces MARS, which employs four agents: a visual perception agent for environmental analysis, a risk assessment agent to prioritize hazards, a planning agent to plan actions, and an evaluation agent for iterative optimization. These agents work collaboratively using hierarchical decision-making.", "result": "Experiments conducted on multiple datasets indicate that MARS outperforms current state-of-the-art multimodal models in risk-aware planning and effective collaboration among agents.", "conclusion": "MARS demonstrates the utility of multimodal large language models in real-world assistive scenarios, offering significant advancements in adaptive assistance and providing a generalizable framework for deploying such systems."}}
{"id": "2511.00266", "pdf": "https://arxiv.org/pdf/2511.00266", "abs": "https://arxiv.org/abs/2511.00266", "authors": ["Aanchal Rajesh Chugh", "Marion Neumeier", "Sebastian Dorn"], "title": "X-TRACK: Physics-Aware xLSTM for Realistic Vehicle Trajectory Prediction", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "Recent advancements in Recurrent Neural Network (RNN) architectures,\nparticularly the Extended Long Short Term Memory (xLSTM), have addressed the\nlimitations of traditional Long Short Term Memory (LSTM) networks by\nintroducing exponential gating and enhanced memory structures. These\nimprovements make xLSTM suitable for time-series prediction tasks as they\nexhibit the ability to model long-term temporal dependencies better than LSTMs.\nDespite their potential, these xLSTM-based models remain largely unexplored in\nthe context of vehicle trajectory prediction. Therefore, this paper introduces\na novel xLSTM-based vehicle trajectory prediction framework, X-TRAJ, and its\nphysics-aware variant, X-TRACK (eXtended LSTM for TRAjectory prediction\nConstraint by Kinematics), which explicitly integrates vehicle motion\nkinematics into the model learning process. By introducing physical\nconstraints, the proposed model generates realistic and feasible trajectories.\nA comprehensive evaluation on the highD and NGSIM datasets demonstrates that\nX-TRACK outperforms state-of-the-art baselines.", "AI": {"tldr": "The paper presents a novel xLSTM-based framework for vehicle trajectory prediction, incorporating kinematic constraints for realistic results.", "motivation": "To address the limitations of traditional trajectory prediction methods and improve long-term temporal dependency modeling, especially for vehicle trajectory prediction tasks.", "method": "The paper proposes X-TRAJ (an xLSTM-based framework) and X-TRACK (its physics-aware variant), integrating vehicle kinematics into the model to enhance realism.", "result": "The X-TRACK framework demonstrated better performance and more realistic trajectory predictions compared to state-of-the-art baselines when evaluated on highD and NGSIM datasets.", "conclusion": "By integrating physical constraints with advanced xLSTM methods, realistic and effective vehicle trajectory predictions can be achieved, advancing state-of-the-art methodologies."}}
{"id": "2511.00503", "pdf": "https://arxiv.org/pdf/2511.00503", "abs": "https://arxiv.org/abs/2511.00503", "authors": ["Panwang Pan", "Chenguo Lin", "Jingjing Zhao", "Chenxin Li", "Yuchen Lin", "Haopeng Li", "Honglei Yan", "Kairun Wen", "Yunlong Lin", "Yixuan Yuan", "Yadong Mu"], "title": "Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models", "categories": ["cs.CV"], "comment": null, "summary": "We introduce Diff4Splat, a feed-forward method that synthesizes controllable\nand explicit 4D scenes from a single image. Our approach unifies the generative\npriors of video diffusion models with geometry and motion constraints learned\nfrom large-scale 4D datasets. Given a single input image, a camera trajectory,\nand an optional text prompt, Diff4Splat directly predicts a deformable 3D\nGaussian field that encodes appearance, geometry, and motion, all in a single\nforward pass, without test-time optimization or post-hoc refinement. At the\ncore of our framework lies a video latent transformer, which augments video\ndiffusion models to jointly capture spatio-temporal dependencies and predict\ntime-varying 3D Gaussian primitives. Training is guided by objectives on\nappearance fidelity, geometric accuracy, and motion consistency, enabling\nDiff4Splat to synthesize high-quality 4D scenes in 30 seconds. We demonstrate\nthe effectiveness of Diff4Splatacross video generation, novel view synthesis,\nand geometry extraction, where it matches or surpasses optimization-based\nmethods for dynamic scene synthesis while being significantly more efficient.", "AI": {"tldr": "Diff4Splat is a fast method for synthesizing 4D dynamic scenes from a single image using video diffusion models and geometry constraints, with no test-time optimizations required.", "motivation": "The paper aims to address the challenge of efficiently generating controllable and realistic 4D dynamic scenes from minimal input, combining generative video priors with geometry and motion constraints.", "method": "Diff4Splat uses a feed-forward framework with video latent transformers to predict deformable 3D Gaussian fields for appearance, geometry, and motion, trained with objectives for fidelity, accuracy, and consistency.", "result": "Diff4Splat achieves high-quality results in video generation, novel view synthesis, and geometry extraction; it matches or surpasses existing optimization-heavy methods while being faster.", "conclusion": "The study proves that Diff4Splat is a significant contribution to dynamic scene synthesis, offering efficient and high-quality results without extensive computational overhead."}}
{"id": "2511.00004", "pdf": "https://arxiv.org/pdf/2511.00004", "abs": "https://arxiv.org/abs/2511.00004", "authors": ["Adrian-Dinu Urse", "Dumitru-Clementin Cercel", "Florin Pop"], "title": "Multimodal Learning with Augmentation Techniques for Natural Disaster Assessment", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CV"], "comment": "Accepted at 2025 IEEE 21st International Conference on Intelligent\n  Computer Communication and Processing (ICCP 2025)", "summary": "Natural disaster assessment relies on accurate and rapid access to\ninformation, with social media emerging as a valuable real-time source.\nHowever, existing datasets suffer from class imbalance and limited samples,\nmaking effective model development a challenging task. This paper explores\naugmentation techniques to address these issues on the CrisisMMD multimodal\ndataset. For visual data, we apply diffusion-based methods, namely Real\nGuidance and DiffuseMix. For text data, we explore back-translation,\nparaphrasing with transformers, and image caption-based augmentation. We\nevaluated these across unimodal, multimodal, and multi-view learning setups.\nResults show that selected augmentations improve classification performance,\nparticularly for underrepresented classes, while multi-view learning introduces\npotential but requires further refinement. This study highlights effective\naugmentation strategies for building more robust disaster assessment systems.", "AI": {"tldr": "The paper addresses class imbalance and limited samples in disaster assessment datasets using augmentation techniques on a multimodal dataset.", "motivation": "To improve disaster assessment using social media by addressing the challenges posed by class imbalance and limited datasets.", "method": "The paper uses diffusion-based methods for visual data and transformer-based techniques for text data augmentation, evaluating their impact on different learning setups.", "result": "The augmentations improved classification performance, particularly for underrepresented classes, with potential shown in multi-view learning setups.", "conclusion": "Augmentation strategies can enhance disaster classification systems, but further refinement is needed for multi-view learning approaches."}}
{"id": "2511.01289", "pdf": "https://arxiv.org/pdf/2511.01289", "abs": "https://arxiv.org/abs/2511.01289", "authors": ["Saiyma Sittul Muna", "Rezwan Islam Salvi", "Mushfiqur Rahman Mushfique", "Ajwad Abrar"], "title": "FirstAidQA: A Synthetic Dataset for First Aid and Emergency Response in Low-Connectivity Settings", "categories": ["cs.CL"], "comment": "Accepted at the 5th Muslims in Machine Learning (MusIML) Workshop,\n  co-located with NeurIPS 2025", "summary": "In emergency situations, every second counts. The deployment of Large\nLanguage Models (LLMs) in time-sensitive, low or zero-connectivity environments\nremains limited. Current models are computationally intensive and unsuitable\nfor low-tier devices often used by first responders or civilians. A major\nbarrier to developing lightweight, domain-specific solutions is the lack of\nhigh-quality datasets tailored to first aid and emergency response. To address\nthis gap, we introduce FirstAidQA, a synthetic dataset containing 5,500\nhigh-quality question answer pairs that encompass a wide range of first aid and\nemergency response scenarios. The dataset was generated using a Large Language\nModel, ChatGPT-4o-mini, with prompt-based in-context learning, using texts from\nthe Vital First Aid Book (2019). We applied preprocessing steps such as text\ncleaning, contextual chunking, and filtering, followed by human validation to\nensure accuracy, safety, and practical relevance of the QA pairs. FirstAidQA is\ndesigned to support instruction-tuning and fine-tuning of LLMs and Small\nLanguage Models (SLMs), enabling faster, more reliable, and offline-capable\nsystems for emergency settings. We publicly release the dataset to advance\nresearch on safety-critical and resource-constrained AI applications in first\naid and emergency response. The dataset is available on Hugging Face at\nhttps://huggingface.co/datasets/i-am-mushfiq/FirstAidQA.", "AI": {"tldr": "This paper introduces FirstAidQA, a synthetic dataset of 5,500 question-answer pairs designed for emergency response applications and seeks to enable lightweight and offline-capable AI systems.", "motivation": "The paper aims to bridge the gap in emergency response systems where current LLMs are unsuitable for low-connectivity environments and resource-constrained devices.", "method": "A domain-specific dataset was created using ChatGPT-4o-mini for in-context learning, followed by text preprocessing and human validation for accuracy and relevance.", "result": "The authors developed FirstAidQA, a publicly available dataset tailored to instruct-tuning and fine-tuning lightweight LLMs and SLMs for emergency-response scenarios.", "conclusion": "The dataset serves as a foundation for enhancing AI systems in time-critical, safety-sensitive emergency settings, aiming to support practical and efficient solutions for first aid responders."}}
{"id": "2511.01718", "pdf": "https://arxiv.org/pdf/2511.01718", "abs": "https://arxiv.org/abs/2511.01718", "authors": ["Jiayi Chen", "Wenxuan Song", "Pengxiang Ding", "Ziyang Zhou", "Han Zhao", "Feilong Tang", "Donglin Wang", "Haoang Li"], "title": "Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Vision-language-action (VLA) models aim to understand natural language\ninstructions and visual observations and to execute corresponding actions as an\nembodied agent. Recent work integrates future images into the\nunderstanding-acting loop, yielding unified VLAs that jointly understand,\ngenerate, and act -- reading text and images and producing future images and\nactions. However, these models either rely on external experts for modality\nunification or treat image generation and action prediction as separate\nprocesses, limiting the benefits of direct synergy between these tasks. Our\ncore philosophy is to optimize generation and action jointly through a\nsynchronous denoising process, where the iterative refinement enables actions\nto evolve from initialization, under constant and sufficient visual guidance.\nWe ground this philosophy in our proposed Unified Diffusion VLA and Joint\nDiscrete Denoising Diffusion Process (JD3P), which is a joint diffusion process\nthat integrates multiple modalities into a single denoising trajectory to serve\nas the key mechanism enabling understanding, generation, and acting to be\nintrinsically synergistic. Our model and theory are built on a unified\ntokenized space of all modalities and a hybrid attention mechanism. We further\npropose a two-stage training pipeline and several inference-time techniques\nthat optimize performance and efficiency. Our approach achieves\nstate-of-the-art performance on benchmarks such as CALVIN, LIBERO, and\nSimplerEnv with 4$\\times$ faster inference than autoregressive methods, and we\ndemonstrate its effectiveness through in-depth analysis and real-world\nevaluations. Our project page is available at\nhttps://irpn-eai.github.io/UD-VLA.github.io/.", "AI": {"tldr": "This paper presents a Unified Diffusion Vision-Language-Action (VLA) model that integrates understanding, generating, and acting across modalities through a joint diffusion process to achieve state-of-the-art performance and efficient inference.", "motivation": "To address the limitations of existing Vision-Language-Action models, which either rely on external experts for modality unification or treat image generation and action prediction as separate processes, reducing synergy between tasks.", "method": "Unified Diffusion VLA and Joint Discrete Denoising Diffusion Process (JD3P) which integrate multiple modalities into a single denoising process. It introduces a unified tokenized modality space, hybrid attention mechanism, a two-stage training pipeline, and novel inference-time techniques.", "result": "The proposed model achieves state-of-the-art performance on multiple benchmarks (CALVIN, LIBERO, SimplerEnv) with inference speeds up to four times faster than autoregressive methods.", "conclusion": "The Unified Diffusion VLA model demonstrates the capability to unify vision, language, and action tasks in a synergistic manner, providing efficiency and improved performance in embodied agent tasks."}}
{"id": "2511.00272", "pdf": "https://arxiv.org/pdf/2511.00272", "abs": "https://arxiv.org/abs/2511.00272", "authors": ["Michiel Straat", "Thorben Markmann", "Sebastian Peitz", "Barbara Hammer"], "title": "Improving the Robustness of Control of Chaotic Convective Flows with Domain-Informed Reinforcement Learning", "categories": ["cs.LG", "physics.flu-dyn"], "comment": null, "summary": "Chaotic convective flows arise in many real-world systems, such as\nmicrofluidic devices and chemical reactors. Stabilizing these flows is highly\ndesirable but remains challenging, particularly in chaotic regimes where\nconventional control methods often fail. Reinforcement Learning (RL) has shown\npromise for control in laminar flow settings, but its ability to generalize and\nremain robust under chaotic and turbulent dynamics is not well explored,\ndespite being critical for real-world deployment. In this work, we improve the\npractical feasibility of RL-based control of such flows focusing on\nRayleigh-B\\'enard Convection (RBC), a canonical model for convective heat\ntransport. To enhance generalization and sample efficiency, we introduce\ndomain-informed RL agents that are trained using Proximal Policy Optimization\nacross diverse initial conditions and flow regimes. We incorporate domain\nknowledge in the reward function via a term that encourages B\\'enard cell\nmerging, as an example of a desirable macroscopic property. In laminar flow\nregimes, the domain-informed RL agents reduce convective heat transport by up\nto 33%, and in chaotic flow regimes, they still achieve a 10% reduction, which\nis significantly better than the conventional controllers used in practice. We\ncompare the domain-informed to uninformed agents: Our results show that the\ndomain-informed reward design results in steady flows, faster convergence\nduring training, and generalization across flow regimes without retraining. Our\nwork demonstrates that elegant domain-informed priors can greatly enhance the\nrobustness of RL-based control of chaotic flows, bringing real-world deployment\ncloser.", "AI": {"tldr": "This paper explores using domain-informed reinforcement learning (RL) agents to control chaotic convective flows, achieving improved generalization, efficiency, and performance.", "motivation": "Controlling chaotic convective flows is highly desirable for applications like microfluidics and chemical reactors, but conventional control methods often fail in chaotic regimes. RL's potential remains underexplored for robust control in such conditions.", "method": "The authors train domain-informed RL agents using Proximal Policy Optimization on Rayleigh-B\u00e9nard Convection (RBC) models with diverse initial conditions and flow regimes. They integrate domain knowledge in the reward function, targeting B\u00e9nard cell merging as a favorable macroscopic property.", "result": "Domain-informed RL agents reduce convective heat transport by 33% in laminar flow and achieve a 10% reduction in chaotic flow, outperforming conventional controllers. Additionally, they exhibit faster convergence and generalization without retraining.", "conclusion": "Incorporating domain-informed priors significantly improves the robustness and performance of RL-based control for chaotic flows, advancing applicability in real-world systems."}}
{"id": "2511.00504", "pdf": "https://arxiv.org/pdf/2511.00504", "abs": "https://arxiv.org/abs/2511.00504", "authors": ["Hai-Dang Nguyen", "Ha-Hieu Pham", "Hao T. Nguyen", "Huy-Hieu Pham"], "title": "VinDr-CXR-VQA: A Visual Question Answering Dataset for Explainable Chest X-Ray Analysis with Multi-Task Learning", "categories": ["cs.CV"], "comment": "ISBI submission. Contains 5 pages, 2 figures, and 6 tables. Code &\n  data: https://huggingface.co/datasets/Dangindev/VinDR-CXR-VQA", "summary": "We present VinDr-CXR-VQA, a large-scale chest X-ray dataset for explainable\nMedical Visual Question Answering (Med-VQA) with spatial grounding. The dataset\ncontains 17,597 question-answer pairs across 4,394 images, each annotated with\nradiologist-verified bounding boxes and clinical reasoning explanations. Our\nquestion taxonomy spans six diagnostic types-Where, What, Is there, How many,\nWhich, and Yes/No-capturing diverse clinical intents. To improve reliability,\nwe construct a balanced distribution of 41.7% positive and 58.3% negative\nsamples, mitigating hallucinations in normal cases. Benchmarking with\nMedGemma-4B-it demonstrates improved performance (F1 = 0.624, +11.8% over\nbaseline) while enabling lesion localization. VinDr-CXR-VQA aims to advance\nreproducible and clinically grounded Med-VQA research. The dataset and\nevaluation tools are publicly available at\nhuggingface.co/datasets/Dangindev/VinDR-CXR-VQA.", "AI": {"tldr": "VinDr-CXR-VQA is a large chest X-ray dataset created for explainable Med-VQA research. It consists of 17,597 question-answer pairs with spatial grounding and clinical explanations.", "motivation": "To enhance explainable Medical Visual Question Answering (Med-VQA) with spatial grounding and improve clinical diagnostic capabilities.", "method": "Developing and annotating a dataset with radiologist-verified bounding boxes, balanced positive and negative samples, and diverse clinical question types; benchmarking the dataset using MedGemma-4B-it.", "result": "MedGemma-4B-it achieves 11.8% better performance than baseline (F1 score = 0.624) and facilitates lesion localization.", "conclusion": "VinDr-CXR-VQA contributes to reproducible, clinically grounded Med-VQA research, and its dataset is publicly accessible for further studies."}}
{"id": "2511.01305", "pdf": "https://arxiv.org/pdf/2511.01305", "abs": "https://arxiv.org/abs/2511.01305", "authors": ["Aman Ganapathy Manvattira", "Yifei Xu", "Ziyue Dang", "Songwu Lu"], "title": "DeepSpecs: Expert-Level Questions Answering in 5G", "categories": ["cs.CL", "cs.AI", "cs.NI"], "comment": null, "summary": "5G technology enables mobile Internet access for billions of users. Answering\nexpert-level questions about 5G specifications requires navigating thousands of\npages of cross-referenced standards that evolve across releases. Existing\nretrieval-augmented generation (RAG) frameworks, including telecom-specific\napproaches, rely on semantic similarity and cannot reliably resolve\ncross-references or reason about specification evolution. We present DeepSpecs,\na RAG system enhanced by structural and temporal reasoning via three\nmetadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB\n(line-level version diffs), and TDocDB (standardization meeting documents).\nDeepSpecs explicitly resolves cross-references by recursively retrieving\nreferenced clauses through metadata lookup, and traces specification evolution\nby mining changes and linking them to Change Requests that document design\nrationale. We curate two 5G QA datasets: 573 expert-annotated real-world\nquestions from practitioner forums and educational resources, and 350\nevolution-focused questions derived from approved Change Requests. Across\nmultiple LLM backends, DeepSpecs outperforms base models and state-of-the-art\ntelecom RAG systems; ablations confirm that explicit cross-reference resolution\nand evolution-aware retrieval substantially improve answer quality,\nunderscoring the value of modeling the structural and temporal properties of 5G\nstandards.", "AI": {"tldr": "DeepSpecs improves the handling of 5G standards queries by enhancing existing retrieval-augmented generation (RAG) frameworks through structural and temporal reasoning.", "motivation": "Existing methods struggle with resolving cross-references and reasoning about the evolving nature of 5G standards.", "method": "DeepSpecs integrates three specialized databases and employs explicit metadata-aware methods to resolve cross-references and track specification evolution.", "result": "DeepSpecs outperforms base models and telecom RAG systems, validated through two curated datasets and ablation studies.", "conclusion": "Modeling structural and temporal aspects of 5G standards significantly enhances answer quality in expert-level queries."}}
{"id": "2511.01770", "pdf": "https://arxiv.org/pdf/2511.01770", "abs": "https://arxiv.org/abs/2511.01770", "authors": ["Liudi Yang", "Yang Bai", "Yuhao Wang", "Ibrahim Alsarraj", "Gitta Kutyniok", "Zhanchi Wang", "Ke Wu"], "title": "Lightweight Learning from Actuation-Space Demonstrations via Flow Matching for Whole-Body Soft Robotic Grasping", "categories": ["cs.RO"], "comment": null, "summary": "Robotic grasping under uncertainty remains a fundamental challenge due to its\nuncertain and contact-rich nature. Traditional rigid robotic hands, with\nlimited degrees of freedom and compliance, rely on complex model-based and\nheavy feedback controllers to manage such interactions. Soft robots, by\ncontrast, exhibit embodied mechanical intelligence: their underactuated\nstructures and passive flexibility of their whole body, naturally accommodate\nuncertain contacts and enable adaptive behaviors. To harness this capability,\nwe propose a lightweight actuation-space learning framework that infers\ndistributional control representations for whole-body soft robotic grasping,\ndirectly from deterministic demonstrations using a flow matching model\n(Rectified Flow),without requiring dense sensing or heavy control loops. Using\nonly 30 demonstrations (less than 8% of the reachable workspace), the learned\npolicy achieves a 97.5% grasp success rate across the whole workspace,\ngeneralizes to grasped-object size variations of +-33%, and maintains stable\nperformance when the robot's dynamic response is directly adjusted by scaling\nthe execution time from 20% to 200%. These results demonstrate that\nactuation-space learning, by leveraging its passive redundant DOFs and\nflexibility, converts the body's mechanics into functional control intelligence\nand substantially reduces the burden on central controllers for this\nuncertain-rich task.", "AI": {"tldr": "This paper proposes a lightweight actuation-space learning framework for soft robotic grasping, achieving high success rates with minimal demonstrations.", "motivation": "The study aims to address the challenge of robotic grasping under uncertain and contact-rich conditions, leveraging the natural adaptability of soft robots.", "method": "A flow matching model, Rectified Flow, is used to learn distributional control directly from limited deterministic demonstrations without dense sensing or complex control systems.", "result": "The learned policy achieved a 97.5% grasp success rate across the workspace, demonstrating generalization to object size variations and scalability in dynamic response time adjustments.", "conclusion": "Actuation-space learning effectively utilizes the mechanical intelligence of soft robots, reducing reliance on complex central controllers and improving grasp performance under uncertainty."}}
{"id": "2511.00280", "pdf": "https://arxiv.org/pdf/2511.00280", "abs": "https://arxiv.org/abs/2511.00280", "authors": ["Abhinav Joshi", "Areeb Ahmad", "Ashutosh Modi"], "title": "Calibration Across Layers: Understanding Calibration Evolution in LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted at EMNLP 2025 (main)", "summary": "Large Language Models (LLMs) have demonstrated inherent calibration\ncapabilities, where predicted probabilities align well with correctness,\ndespite prior findings that deep neural networks are often overconfident.\nRecent studies have linked this behavior to specific components in the final\nlayer, such as entropy neurons and the unembedding matrix null space. In this\nwork, we provide a complementary perspective by investigating how calibration\nevolves throughout the network depth. Analyzing multiple open-weight models on\nthe MMLU benchmark, we uncover a distinct confidence correction phase in the\nupper/later layers, where model confidence is actively recalibrated after\ndecision certainty has been reached. Furthermore, we identify a low-dimensional\ncalibration direction in the residual stream whose perturbation significantly\nimproves calibration metrics (ECE and MCE) without harming accuracy. Our\nfindings suggest that calibration is a distributed phenomenon, shaped\nthroughout the network forward pass, not just in its final projection,\nproviding new insights into how confidence-regulating mechanisms operate within\nLLMs.", "AI": {"tldr": "The paper studies calibration in Large Language Models (LLMs), focusing on how confidence evolves across network depth rather than only in the final layer.", "motivation": "Deep neural networks are often overconfident, unlike LLMs which exhibit better calibration. Understanding calibration mechanisms in LLMs could improve reliability and interpretability.", "method": "The study analyzes multiple open-weight models on the MMLU benchmark and investigates calibration progression, identifying a confidence correction phase and a low-dimensional calibration direction.", "result": "The paper uncovers active confidence recalibration in upper layers and demonstrates enhanced calibration metrics (ECE and MCE) by perturbing the calibration direction without accuracy loss.", "conclusion": "Calibration in LLMs is distributed and shaped throughout the network, not restricted to the final projection phase, offering insights into confidence mechanisms."}}
{"id": "2511.00510", "pdf": "https://arxiv.org/pdf/2511.00510", "abs": "https://arxiv.org/abs/2511.00510", "authors": ["Kai Luo", "Hao Shi", "Kunyu Peng", "Fei Teng", "Sheng Wu", "Kaiwei Wang", "Kailun Yang"], "title": "OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback", "categories": ["cs.CV", "cs.RO", "eess.IV"], "comment": "Extended version of CVPR 2025 paper arXiv:2503.04565. Datasets and\n  code will be made publicly available at https://github.com/xifen523/OmniTrack", "summary": "This paper investigates Multi-Object Tracking (MOT) in panoramic imagery,\nwhich introduces unique challenges including a 360{\\deg} Field of View (FoV),\nresolution dilution, and severe view-dependent distortions. Conventional MOT\nmethods designed for narrow-FoV pinhole cameras generalize unsatisfactorily\nunder these conditions. To address panoramic distortion, large search space,\nand identity ambiguity under a 360{\\deg} FoV, OmniTrack++ adopts a\nfeedback-driven framework that progressively refines perception with trajectory\ncues. A DynamicSSM block first stabilizes panoramic features, implicitly\nalleviating geometric distortion. On top of normalized representations,\nFlexiTrack Instances use trajectory-informed feedback for flexible localization\nand reliable short-term association. To ensure long-term robustness, an\nExpertTrack Memory consolidates appearance cues via a Mixture-of-Experts\ndesign, enabling recovery from fragmented tracks and reducing identity drift.\nFinally, a Tracklet Management module adaptively switches between end-to-end\nand tracking-by-detection modes according to scene dynamics, offering a\nbalanced and scalable solution for panoramic MOT. To support rigorous\nevaluation, we establish the EmboTrack benchmark, a comprehensive dataset for\npanoramic MOT that includes QuadTrack, captured with a quadruped robot, and\nBipTrack, collected with a bipedal wheel-legged robot. Together, these datasets\nspan wide-angle environments and diverse motion patterns, providing a\nchallenging testbed for real-world panoramic perception. Extensive experiments\non JRDB and EmboTrack demonstrate that OmniTrack++ achieves state-of-the-art\nperformance, yielding substantial HOTA improvements of +25.5% on JRDB and\n+43.07% on QuadTrack over the original OmniTrack. Datasets and code will be\nmade publicly available at https://github.com/xifen523/OmniTrack.", "AI": {"tldr": "This paper introduces OmniTrack++, an MOT framework tailored for panoramic imagery, addressing challenges like panoramic distortions and identity ambiguity. It demonstrates state-of-the-art results on benchmark datasets.", "motivation": "Panoramic imagery poses unique challenges for Multi-Object Tracking (MOT) due to the 360\u00b0 Field of View, resolution dilution, and severe distortions, which current MOT methods fail to handle effectively.", "method": "OmniTrack++ utilizes a feedback-driven framework with components like DynamicSSM for feature stabilization, trajectory-informed FlexiTrack Instances for association, ExpertTrack Memory for long-term track recovery, and adaptive Tracklet Management to balance detection and tracking modes.", "result": "OmniTrack++ achieved significant performance improvements, including HOTA gains of +25.5% on JRDB and +43.07% on QuadTrack, surpassing previous methods.", "conclusion": "OmniTrack++ effectively addresses panoramic MOT challenges and sets a new benchmark in performance, supported by the EmboTrack dataset and real-world evaluations."}}
{"id": "2511.00015", "pdf": "https://arxiv.org/pdf/2511.00015", "abs": "https://arxiv.org/abs/2511.00015", "authors": ["Swapnoneel Roy", "Asai Asaithambi", "Debajyoti Mukhopadhyay"], "title": "Sorting by Strip Swaps is NP-Hard", "categories": ["cs.DS", "cs.AI", "cs.CC"], "comment": "4 pages", "summary": "We show that \\emph{Sorting by Strip Swaps} (SbSS) is NP-hard by a polynomial\nreduction of \\emph{Block Sorting}. The key idea is a local gadget, a\n\\emph{cage}, that replaces every decreasing adjacency $(a_i,a_{i+1})$ by a\nguarded triple $a_i,m_i,a_{i+1}$ enclosed by guards $L_i,U_i$, so the only\ndecreasing adjacencies are the two inside the cage. Small \\emph{hinge} gadgets\ncouple adjacent cages that share an element and enforce that a strip swap that\nremoves exactly two adjacencies corresponds bijectively to a block move that\nremoves exactly one decreasing adjacency in the source permutation. This yields\na clean equivalence between exact SbSS schedules and perfect block schedules,\nestablishing NP-hardness.", "AI": {"tldr": "The paper proves that Sorting by Strip Swaps (SbSS) is NP-hard via a reduction from Block Sorting.", "motivation": "To investigate the computational complexity of sorting by strip swaps and prove its NP-hardness.", "method": "A polynomial reduction from Block Sorting to SbSS is employed, introducing gadgets like 'cages' and 'hinges' to establish equivalence between SbSS schedules and block sorting schedules.", "result": "SbSS is shown to be NP-hard through the established reduction and gadget framework.", "conclusion": "Sorting by Strip Swaps is computationally complex (NP-hard), highlighting the challenges of solving related problems in permutation sorting."}}
{"id": "2511.01323", "pdf": "https://arxiv.org/pdf/2511.01323", "abs": "https://arxiv.org/abs/2511.01323", "authors": ["Jiabao Ji", "Min Li", "Priyanshu Kumar", "Shiyu Chang", "Saloni Potdar"], "title": "DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness", "categories": ["cs.CL", "cs.AI"], "comment": "25 pages", "summary": "Large language models (LLMs) with integrated search tools show strong promise\nin open-domain question answering (QA), yet they often struggle to produce\ncomplete answer set to complex questions such as Which actor from the film Heat\nwon at least one Academy Award?, which requires (1) distinguishing between\nmultiple films sharing the same title and (2) reasoning across a large set of\nactors to gather and integrate evidence. Existing QA benchmarks rarely evaluate\nboth challenges jointly. To address this, we introduce DeepAmbigQAGen, an\nautomatic data generation pipeline that constructs QA tasks grounded in text\ncorpora and linked knowledge graph, generating natural and verifiable questions\nthat systematically embed name ambiguity and multi-step reasoning. Based on\nthis, we build DeepAmbigQA, a dataset of 3,600 questions requiring multi-hop\nreasoning and half of them explicit name ambiguity resolving. Experiments\nreveal that, even state-of-the-art GPT-5 show incomplete answers, achieving\nonly 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous\nquestions. These findings highlight the need for more robust QA systems aimed\nat information gathering and answer completeness.", "AI": {"tldr": "This paper discusses limitations in large language models for complex question answering and introduces a new dataset, DeepAmbigQA, focusing on tasks requiring name ambiguity resolution and multi-step reasoning.", "motivation": "To address LLMs' struggles in answering complex questions involving both name ambiguity and multi-step reasoning, challenges which are not systematically evaluated by existing benchmarks.", "method": "The authors created DeepAmbigQAGen, a pipeline that generates QA tasks grounded in text and knowledge graphs, systematically embedding ambiguity and reasoning challenges. They developed a dataset of 3,600 questions, DeepAmbigQA.", "result": "Even GPT-5 performed poorly on this dataset, achieving low exact matches (0.13 for ambiguous and 0.21 for non-ambiguous questions), showing current LLM limitations in generating complete answers.", "conclusion": "The findings underscore the importance of developing QA systems that better handle ambiguity and multi-step reasoning for information gathering and answer completeness."}}
{"id": "2511.01774", "pdf": "https://arxiv.org/pdf/2511.01774", "abs": "https://arxiv.org/abs/2511.01774", "authors": ["Alexander Schperberg", "Yusuke Tanaka", "Stefano Di Cairano", "Dennis Hong"], "title": "MOBIUS: A Multi-Modal Bipedal Robot that can Walk, Crawl, Climb, and Roll", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "23 pages, 20 figures. Collaborative work between the Robotics and\n  Mechanisms Laboratory (RoMeLa) and Mitsubishi Electric Research Laboratories\n  (MERL)", "summary": "This article presents a Multi-Modal Bipedal Intelligent Urban Scout robot\n(MOBIUS) capable of walking, crawling, climbing, and rolling. MOBIUS features\nfour limbs--two 6-DoF arms with two-finger grippers for manipulation and\nclimbing, and two 4-DoF legs for locomotion--enabling smooth transitions across\ndiverse terrains without reconfiguration. A hybrid control architecture\ncombines reinforcement learning-based locomotion with model-based predictive\nand admittance control enhanced for safety by a Reference Governor toward\ncompliant contact interactions. A high-level MIQCP planner autonomously selects\nlocomotion modes to balance stability and energy efficiency. Hardware\nexperiments demonstrate robust gait transitions, dynamic climbing, and\nfull-body load support via pinch grasp. Overall, MOBIUS demonstrates the\nimportance of tight integration between morphology, high-level planning, and\ncontrol to enable mobile loco-manipulation and grasping, substantially\nexpanding its interaction capabilities, workspace, and traversability.", "AI": {"tldr": "MOBIUS is a versatile urban scout robot that integrates advanced locomotion and grasping capabilities across various terrains and tasks.", "motivation": "To develop a robot with multi-modal locomotion and manipulation capabilities for efficient traversal and interaction in complex urban environments.", "method": "MOBIUS incorporates hybrid reinforcement and model-based control systems along with a high-level planner for adaptive mode selection.", "result": "Experimental results showcase MOBIUS's robust performance in gait transitions, dynamic climbing, and full-body load support.", "conclusion": "Integration of advanced morphology, planning, and control expands MOBIUS's capabilities in interaction, workspace, and traversal."}}
{"id": "2511.00301", "pdf": "https://arxiv.org/pdf/2511.00301", "abs": "https://arxiv.org/abs/2511.00301", "authors": ["Ciaran Bench", "Oskar Pfeffer", "Vivek Desai", "Mohammad Moulaeifard", "Lo\u00efc Coquelin", "Peter H. Charlton", "Nils Strodthoff", "Nando Hegemann", "Philip J. Aston", "Andrew Thompson"], "title": "A systematic evaluation of uncertainty quantification techniques in deep learning: a case study in photoplethysmography signal analysis", "categories": ["cs.LG", "physics.med-ph"], "comment": null, "summary": "In principle, deep learning models trained on medical time-series, including\nwearable photoplethysmography (PPG) sensor data, can provide a means to\ncontinuously monitor physiological parameters outside of clinical settings.\nHowever, there is considerable risk of poor performance when deployed in\npractical measurement scenarios leading to negative patient outcomes. Reliable\nuncertainties accompanying predictions can provide guidance to clinicians in\ntheir interpretation of the trustworthiness of model outputs. It is therefore\nof interest to compare the effectiveness of different approaches. Here we\nimplement an unprecedented set of eight uncertainty quantification (UQ)\ntechniques to models trained on two clinically relevant prediction tasks:\nAtrial Fibrillation (AF) detection (classification), and two variants of blood\npressure regression. We formulate a comprehensive evaluation procedure to\nenable a rigorous comparison of these approaches. We observe a complex picture\nof uncertainty reliability across the different techniques, where the most\noptimal for a given task depends on the chosen expression of uncertainty,\nevaluation metric, and scale of reliability assessed. We find that assessing\nlocal calibration and adaptivity provides practically relevant insights about\nmodel behaviour that otherwise cannot be acquired using more commonly\nimplemented global reliability metrics. We emphasise that criteria for\nevaluating UQ techniques should cater to the model's practical use case, where\nthe use of a small number of measurements per patient places a premium on\nachieving small-scale reliability for the chosen expression of uncertainty,\nwhile preserving as much predictive performance as possible.", "AI": {"tldr": "This paper evaluates the use of different uncertainty quantification (UQ) techniques in deep learning models applied to medical time-series data for tasks like atrial fibrillation detection and blood pressure regression.", "motivation": "There is a need for reliable uncertainty measures in deep learning models applied to medical time-series data to ensure trustworthy outputs and guide clinicians in practical scenarios.", "method": "The authors implemented eight UQ techniques and formulated a comprehensive evaluation procedure to compare them on classification and regression tasks involving wearable medical sensor data.", "result": "The study found that the effectiveness of UQ techniques varies depending on the task, uncertainty expression, reliability metric, and scale of reliability. Local calibration and adaptivity provided insights not captured by global metrics.", "conclusion": "The evaluation criteria for UQ methods should align with the model's practical application, emphasizing small-scale reliability and predictive performance under constraints like limited patient measurements."}}
{"id": "2511.00511", "pdf": "https://arxiv.org/pdf/2511.00511", "abs": "https://arxiv.org/abs/2511.00511", "authors": ["Panwang Pan", "Jingjing Zhao", "Yuchen Lin", "Chenguo Lin", "Chenxin Li", "Haopeng Li", "Honglei Yan", "Tingting Shen", "Yadong Mu"], "title": "ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation", "categories": ["cs.CV"], "comment": null, "summary": "Video generative models pretrained on large-scale datasets can produce\nhigh-quality videos, but are often conditioned on text or a single image,\nlimiting controllability and applicability. We introduce ID-Composer, a novel\nframework that addresses this gap by tackling multi-subject video generation\nfrom a text prompt and reference images. This task is challenging as it\nrequires preserving subject identities, integrating semantics across subjects\nand modalities, and maintaining temporal consistency. To faithfully preserve\nthe subject consistency and textual information in synthesized videos,\nID-Composer designs a \\textbf{hierarchical identity-preserving attention\nmechanism}, which effectively aggregates features within and across subjects\nand modalities. To effectively allow for the semantic following of user\nintention, we introduce \\textbf{semantic understanding via pretrained\nvision-language model (VLM)}, leveraging VLM's superior semantic understanding\nto provide fine-grained guidance and capture complex interactions between\nmultiple subjects. Considering that standard diffusion loss often fails in\naligning the critical concepts like subject ID, we employ an \\textbf{online\nreinforcement learning phase} to drive the overall training objective of\nID-Composer into RLVR. Extensive experiments demonstrate that our model\nsurpasses existing methods in identity preservation, temporal consistency, and\nvideo quality.", "AI": {"tldr": "ID-Composer introduces a novel framework for generating multi-subject videos from text prompts and reference images while ensuring identity preservation and semantic integration.", "motivation": "Existing video generative models lack controllability and applicability when conditioned on multiple subjects. ID-Composer aims to develop a model that addresses these limitations.", "method": "The paper proposes a hierarchical identity-preserving attention mechanism, semantic understanding via pretrained vision-language models, and an online reinforcement learning phase to improve video generation.", "result": "Experiments show ID-Composer surpasses current methods in identity preservation, semantic alignment, temporal consistency, and overall video quality.", "conclusion": "ID-Composer effectively addresses the challenges of multi-subject video generation with innovative mechanisms, demonstrating superior quality and consistency in synthesized videos."}}
{"id": "2511.01354", "pdf": "https://arxiv.org/pdf/2511.01354", "abs": "https://arxiv.org/abs/2511.01354", "authors": ["Wenrui Cai", "Chengyu Wang", "Junbing Yan", "Jun Huang", "Xiangzhong Fang"], "title": "Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series", "categories": ["cs.CL", "cs.AI"], "comment": "emnlp 2025 industry track", "summary": "Recently, the demand for small and efficient reasoning models to support\nreal-world applications has driven the development of knowledge distillation\ntechniques that balance reasoning performance and inference speed. In this\npaper, we further extend the DistilQwen model family, initialized from the Qwen\nmodels, by introducing four model series specifically designed to meet\nindustrial requirements. The distilled model collection comprises: (1)\nslow-thinking models, optimized for reasoning tasks that require high accuracy;\n(2) two series of adaptive-thinking models, which dynamically adjust reasoning\nstrategies based on input tasks to maximize efficiency across diverse\nscenarios; and (3) distilled reward models, which enable further reinforcement\nlearning of reasoning models using distilled knowledge. Comprehensive\nevaluations across multiple benchmarks demonstrate both high inference\nefficiency and strong reasoning performance for these models, as well as the\npractical utility of distilled reward models. We further show that these models\nsupport industry practitioners by providing scalable training and inference\nfunctionalities on the Alibaba Cloud PAI (Platform for Artificial Intelligence)\nplatform.", "AI": {"tldr": "The paper introduces the DistilQwen model family, consisting of four series tailored for industrial requirements, focusing on reasoning performance and efficiency.", "motivation": "The demand for small and efficient reasoning models for real-world applications has driven the development of knowledge distillation techniques.", "method": "The paper presents four model series derived from Qwen models: slow-thinking models for high accuracy, adaptive-thinking models for diverse scenarios, and distilled reward models for reinforcement learning.", "result": "Comprehensive evaluations reveal high efficiency and strong reasoning performance, showcasing practical utility on the Alibaba Cloud PAI platform.", "conclusion": "The DistilQwen models balance performance and efficiency, meeting industrial requirements and demonstrating scalability in training and inference on the Alibaba platform."}}
{"id": "2511.01791", "pdf": "https://arxiv.org/pdf/2511.01791", "abs": "https://arxiv.org/abs/2511.01791", "authors": ["Feng Chen", "Zhuxiu Xu", "Tianzhe Chu", "Xunzhe Zhou", "Li Sun", "Zewen Wu", "Shenghua Gao", "Zhongyu Li", "Yanchao Yang", "Yi Ma"], "title": "GenDexHand: Generative Simulation for Dexterous Hands", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Data scarcity remains a fundamental bottleneck for embodied intelligence.\nExisting approaches use large language models (LLMs) to automate gripper-based\nsimulation generation, but they transfer poorly to dexterous manipulation,\nwhich demands more specialized environment design. Meanwhile, dexterous\nmanipulation tasks are inherently more difficult due to their higher degrees of\nfreedom. Massively generating feasible and trainable dexterous hand tasks\nremains an open challenge. To this end, we present GenDexHand, a generative\nsimulation pipeline that autonomously produces diverse robotic tasks and\nenvironments for dexterous manipulation. GenDexHand introduces a closed-loop\nrefinement process that adjusts object placements and scales based on\nvision-language model (VLM) feedback, substantially improving the average\nquality of generated environments. Each task is further decomposed into\nsub-tasks to enable sequential reinforcement learning, reducing training time\nand increasing success rates. Our work provides a viable path toward scalable\ntraining of diverse dexterous hand behaviors in embodied intelligence by\noffering a simulation-based solution to synthetic data generation. Our website:\nhttps://winniechen2002.github.io/GenDexHand/.", "AI": {"tldr": "This paper proposes GenDexHand, a generative simulation pipeline for automating the creation of diverse, high-quality dexterous manipulation tasks and environments, using vision-language model feedback and task decomposition.", "motivation": "The work addresses the challenge of data scarcity in embodied intelligence, specifically in dexterous manipulation, which requires nuanced and diverse simulation environments for effective training.", "method": "The proposed method, GenDexHand, integrates a generative simulation pipeline with a closed-loop refinement process guided by vision-language model feedback to improve the quality of simulations. It also decomposes tasks into sub-tasks for sequential reinforcement learning.", "result": "GenDexHand substantially enhances the quality of task generation, reducing training time and improving success rates in learning dexterous hand behaviors.", "conclusion": "The study offers a scalable synthetic data generation solution for training dexterous robotic hands, advancing the training process for embodied intelligence."}}
{"id": "2511.00523", "pdf": "https://arxiv.org/pdf/2511.00523", "abs": "https://arxiv.org/abs/2511.00523", "authors": ["Fangyu Wu", "Yujun Cai"], "title": "SegDebias: Test-Time Bias Mitigation for ViT-Based CLIP via Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Vision language models such as CLIP have shown remarkable performance in zero\nshot classification, but remain susceptible to spurious correlations, where\nirrelevant visual features influence predictions. Existing debiasing methods\noften require access to training data and explicit group labels to perform\nfine-tuning or adjust embeddings, which limits their practicality in real-world\nsettings. Test-time methods attempt to avoid this constraint, but many still\ndepend on prior knowledge of dataset specific biases, limiting their\ngeneralizability in open set settings. In this work, we propose a test-time\ndebiasing method for ViT based CLIP models that requires no additional training\nor assumptions of bias annotations. Our approach uses a pretrained segmentation\nmodel to isolate the target visual attribute, then adjusts the non target\nregions so that their embeddings are uniformly similar to all class specific\ntext prompts. This procedure removes unintended bias signals from confounding\nvisual regions while preserving the target attribute. Experiments on Waterbirds\nand CelebA show that our method outperforms existing test-time debiasing\napproaches in both group robustness metrics and Attention IoU. These results\ndemonstrate the effectiveness of segmentation guided interventions for scalable\nand annotation free bias mitigation in vision language models.", "AI": {"tldr": "The paper introduces a test-time debiasing method for CLIP-based models to reduce spurious correlations by leveraging pretrained segmentation models.", "motivation": "Existing debiasing methods rely on training data and explicit annotations, limiting real-world applicability, while test-time methods often depend on prior bias knowledge, restricting generalizability.", "method": "A test-time debiasing method using pretrained segmentation models to isolate target visual attributes and adjust embeddings of non-target regions uniformly across class-specific text prompts, thereby reducing bias signals.", "result": "Experiments on datasets like Waterbirds and CelebA show superior group robustness metrics and improved Attention IoU compared to existing test-time debiasing methods.", "conclusion": "Segmentation-guided interventions offer a scalable and annotation-free strategy for bias mitigation in vision language models."}}
{"id": "2511.00024", "pdf": "https://arxiv.org/pdf/2511.00024", "abs": "https://arxiv.org/abs/2511.00024", "authors": ["Haotian Hang", "Yueyang Shen", "Vicky Zhu", "Jose Cruz", "Michelle Li"], "title": "Chitchat with AI: Understand the supply chain carbon disclosure of companies worldwide through Large Language Model", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG", "stat.AP"], "comment": null, "summary": "In the context of global sustainability mandates, corporate carbon disclosure\nhas emerged as a critical mechanism for aligning business strategy with\nenvironmental responsibility. The Carbon Disclosure Project (CDP) hosts the\nworld's largest longitudinal dataset of climate-related survey responses,\ncombining structured indicators with open-ended narratives, but the\nheterogeneity and free-form nature of these disclosures present significant\nanalytical challenges for benchmarking, compliance monitoring, and investment\nscreening. This paper proposes a novel decision-support framework that\nleverages large language models (LLMs) to assess corporate climate disclosure\nquality at scale. It develops a master rubric that harmonizes narrative scoring\nacross 11 years of CDP data (2010-2020), enabling cross-sector and\ncross-country benchmarking. By integrating rubric-guided scoring with\npercentile-based normalization, our method identifies temporal trends,\nstrategic alignment patterns, and inconsistencies in disclosure across\nindustries and regions. Results reveal that sectors such as technology and\ncountries like Germany consistently demonstrate higher rubric alignment, while\nothers exhibit volatility or superficial engagement, offering insights that\ninform key decision-making processes for investors, regulators, and corporate\nenvironmental, social, and governance (ESG) strategists. The proposed LLM-based\napproach transforms unstructured disclosures into quantifiable, interpretable,\ncomparable, and actionable intelligence, advancing the capabilities of\nAI-enabled decision support systems (DSSs) in the domain of climate governance.", "AI": {"tldr": "This paper presents a decision-support framework using large language models (LLMs) to evaluate corporate climate disclosure quality using Carbon Disclosure Project (CDP) data, highlighting benchmarking insights and industry/regional differences.", "motivation": "Increasing global sustainability mandates necessitate advanced mechanisms like carbon disclosure for businesses to align with environmental responsibilities, but challenges exist in analyzing heterogeneous climate-related data effectively.", "method": "The paper develops a rubric-based framework that utilizes LLMs to assess corporate disclosure quality and processes narratives from 11 years (2010-2020) of CDP data. It combines rubric-guided scoring with percentile-based normalization for cross-sector and cross-country analysis.", "result": "The approach successfully identifies trends, strategic patterns, and inconsistencies in climate disclosures across industries and regions. It found that sectors like technology or nations like Germany show stronger alignment while others illustrate volatility.", "conclusion": "The research underscores the immense value of LLM-driven frameworks in transforming unstructured climate data into actionable insights, aiding investors, regulators, and ESG strategists in advancing climate governance and decision-making."}}
{"id": "2511.01359", "pdf": "https://arxiv.org/pdf/2511.01359", "abs": "https://arxiv.org/abs/2511.01359", "authors": ["Sapir Harary", "Eran Hirsch", "Aviv Slobodkin", "David Wan", "Mohit Bansal", "Ido Dagan"], "title": "PrefixNLI: Detecting Factual Inconsistencies as Soon as They Arise", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages + appendix. Code, datasets, and models are available at\n  https://github.com/sapirharary/PrefixNLI", "summary": "Natural Language Inference (NLI) models have been used in various ways to\nimprove the factuality of LLM outputs. This is typically done by applying an\nNLI model to judge whether the model output is entailed from the supposed\nevidence, triggering some corrective actions, such as beam reranking at\ninference time or RL rewards during training. While NLI models are trained to\ndetect factual inconsistencies over complete sentences, decisions in the common\nautoregressive generation architecture are made for each evolving text prefix,\nduring decoding. Addressing this setting, we generalize the entailment\ndetection task to apply over arbitrary text prefixes, and suggest its utility\nfor improving generation faithfulness. Providing suitable evaluation and\ntraining datasets for this task, we train MiniTruePrefixes, a novel specialized\nmodel that better detects factual inconsistencies over text prefixes,\noutperforming comparable baseline NLI models by 5-14 F1 points in prefix-level\nentailment. We further demonstrate that integrating MiniTruePrefixes into a\ncontrolled decoding framework substantially improves factual consistency in\nabstractive summarization. When guided by MiniTruePrefixes,\nLLaMA-3.2-3B-Instruct matches the faithfulness and runtime of the 8B model from\nthe same model family, while using only half the memory.", "AI": {"tldr": "The paper develops MiniTruePrefixes, a model for detecting factual inconsistencies in text prefixes during NLI tasks, and shows its effectiveness in improving text generation faithfulness in summarization tasks.", "motivation": "Improving the factuality and faithfulness of outputs from large language models during autoregressive generation, where current NLI models are limited to handling complete sentences.", "method": "They introduce a prefix-level entailment detection task to enable entailment checks on dynamic text prefixes during decoding. They provide relevant datasets, train MiniTruePrefixes for this task, and integrate it into controlled decoding workflows.", "result": "MiniTruePrefixes achieves 5-14 F1 points improvement over baseline NLI models in prefix-level entailment detection. Its integration into decoding significantly enhances the factual consistency of summaries while optimizing memory usage in LLaMA models.", "conclusion": "The proposed MiniTruePrefixes model improves factual consistency in autoregressive text generation and bridges the gap between efficient and faithful summary generation, benefiting tasks reliant on factual fidelity."}}
{"id": "2511.01797", "pdf": "https://arxiv.org/pdf/2511.01797", "abs": "https://arxiv.org/abs/2511.01797", "authors": ["Javier Ballesteros-Jerez", "Jesus Mart\u00ednez-G\u00f3mez", "Ismael Garc\u00eda-Varea", "Luis Orozco-Barbosa", "Manuel Castillo-Cara"], "title": "Hybrid Neural Network-Based Indoor Localisation System for Mobile Robots Using CSI Data in a Robotics Simulator", "categories": ["cs.RO", "cs.LG"], "comment": "13 pages, 7 figures. Conference paper (ROBOVIS 2025)", "summary": "We present a hybrid neural network model for inferring the position of mobile\nrobots using Channel State Information (CSI) data from a Massive MIMO system.\nBy leveraging an existing CSI dataset, our approach integrates a Convolutional\nNeural Network (CNN) with a Multilayer Perceptron (MLP) to form a Hybrid Neural\nNetwork (HyNN) that estimates 2D robot positions. CSI readings are converted\ninto synthetic images using the TINTO tool. The localisation solution is\nintegrated with a robotics simulator, and the Robot Operating System (ROS),\nwhich facilitates its evaluation through heterogeneous test cases, and the\nadoption of state estimators like Kalman filters. Our contributions illustrate\nthe potential of our HyNN model in achieving precise indoor localisation and\nnavigation for mobile robots in complex environments. The study follows, and\nproposes, a generalisable procedure applicable beyond the specific use case\nstudied, making it adaptable to different scenarios and datasets.", "AI": {"tldr": "This paper introduces a Hybrid Neural Network using CSI from a Massive MIMO system to enable precise indoor localisation of mobile robots, integrating CNNs and MLPs.", "motivation": "To enhance precise indoor localisation and navigation for mobile robots in complex environments using CSI data.", "method": "The approach converts CSI readings into synthetic images with TINTO, then uses a hybrid of CNN and MLP for 2D robot position estimation, integrated with ROS and evaluative robotics simulators.", "result": "The proposed model achieves precise indoor localisation and navigation for mobile robots in testing scenarios.", "conclusion": "The hybrid neural network shows promising results for generalisable indoor localisation and can be adapted to different scenarios and datasets."}}
{"id": "2511.00351", "pdf": "https://arxiv.org/pdf/2511.00351", "abs": "https://arxiv.org/abs/2511.00351", "authors": ["Amir Ziashahabi", "Yavuz Faruk Bakman", "Duygu Nur Yaldiz", "Mostafa El-Khamy", "Sai Praneeth Karimireddy", "Salman Avestimehr"], "title": "Reject Only Critical Tokens: Pivot-Aware Speculative Decoding", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted at NeurIPS 2025 Efficient Reasoning Workshop", "summary": "Speculative Decoding (SD) ensures that the output matches the target model's\ndistribution exactly. However, we argue that this distribution matching\nrequirement is too stringent and results in unnecessarily low acceptance rates,\nlimiting potential speedups. Instead, we advocate a reformulation of the\ndecoding objective: the proposed decoding strategy should match the expected\nutility, i.e., the task-specific performance, of the target model. This\nperspective also aligns better with real-world use cases of LLMs, where utility\n(e.g., code correctness, factual accuracy) is often more important than\nsampling distribution. Based on this reformulation, we propose a novel decoding\nstrategy: Pivot-Aware Speculative Decoding, which rejects only those tokens\nthat would lead to a utility drop in the final output. We refer to these\ncritical tokens as pivot tokens. We propose a method for labeling tokens as\npivotal or non-pivotal and train a lightweight classifier to detect them. This\nmethod can be viewed as a relaxed version of standard SD, which offers much\nhigher acceptance while preserving utility. We evaluate our method across\nvarious datasets, demonstrating that we can achieve up to $2.5\\times$ speedup\nwith comparable utility. Source code is available at\nhttps://github.com/amir-zsh/PAD.", "AI": {"tldr": "The paper criticizes Speculative Decoding (SD) for overly strict distribution matching requirements that limit efficiency and proposes Pivot-Aware Speculative Decoding, focusing on utility maximization and achieving higher speedups while maintaining performance.", "motivation": "Existing Speculative Decoding methods impose strict requirements on matching target model distributions, causing lower acceptance rates and limiting speed efficiency. The paper advocates for focusing on task-specific utility instead.", "method": "The authors introduce Pivot-Aware Speculative Decoding, identifying tokens critical to utility (pivot tokens) using a lightweight classifier to relax the strictness of standard SD, maintaining utility while improving efficiency.", "result": "Experiments show that Pivot-Aware Speculative Decoding achieves speedups of up to 2.5x while delivering comparable task-specific utility across various datasets.", "conclusion": "The paper concludes that their approach balances speed and utility effectively, offering a more practical solution for real-world use cases of large language models, surpassing strict distribution-based decoding methods."}}
{"id": "2511.00524", "pdf": "https://arxiv.org/pdf/2511.00524", "abs": "https://arxiv.org/abs/2511.00524", "authors": ["Jihao Gu", "Kun Li", "He Wang", "Kaan Ak\u015fit"], "title": "Text-guided Fine-Grained Video Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Video Anomaly Detection (VAD) aims to identify anomalous events within video\nsegments. In scenarios such as surveillance or industrial process monitoring,\nanomaly detection is of critical importance. While existing approaches are\nsemi-automated, requiring human assessment for anomaly detection, traditional\nVADs offer limited output as either normal or anomalous. We propose Text-guided\nFine-Grained Video Anomaly Detection (T-VAD), a framework built upon Large\nVision-Language Model (LVLM). T-VAD introduces an Anomaly Heatmap Decoder (AHD)\nthat performs pixel-wise visual-textual feature alignment to generate\nfine-grained anomaly heatmaps. Furthermore, we design a Region-aware Anomaly\nEncoder (RAE) that transforms the heatmaps into learnable textual embeddings,\nguiding the LVLM to accurately identify and localize anomalous events in\nvideos. This significantly enhances both the granularity and interactivity of\nanomaly detection. The proposed method achieving SOTA performance by\ndemonstrating 94.8% Area Under the Curve (AUC, specifically micro-AUC) and\n67.8%/76.7% accuracy in anomaly heatmaps (RBDC/TBDC) on the UBnormal dataset,\nand subjectively verified more preferable textual description on the\nShanghaiTech-based dataset (BLEU-4: 62.67 for targets, 88.84 for trajectories;\nYes/No accuracy: 97.67%), and on the UBnormal dataset (BLEU-4: 50.32 for\ntargets, 78.10 for trajectories; Yes/No accuracy: 89.73%).", "AI": {"tldr": "This paper introduces T-VAD, a novel framework for Video Anomaly Detection (VAD) that provides fine-grained anomaly heatmaps and is text-interactive by leveraging large vision-language models. It achieves state-of-the-art performance in identifying and describing anomalies.", "motivation": "The motivation is to improve Video Anomaly Detection (VAD) methods by creating a more detailed and interactive system, as traditional VADs are limited to binary outputs and require human intervention, making them less efficient in scenarios like surveillance and industrial monitoring.", "method": "The proposed T-VAD framework uses a Large Vision-Language Model (LVLM) and introduces two key components: 1) Anomaly Heatmap Decoder (AHD) for creating fine-grained, pixel-level anomaly heatmaps through visual-textual alignment, and 2) Region-aware Anomaly Encoder (RAE) that transforms heatmaps into text embeddings to guide the LVLM in identifying and localizing anomalies.", "result": "T-VAD achieved state-of-the-art (SOTA) results, with 94.8% AUC and RBDC/TBDC accuracy of 67.8%/76.7% on the UBnormal dataset. It also demonstrated superior textual descriptions on the ShanghaiTech-based dataset with BLEU-4 scores (62.67 for targets, 88.84 for trajectories) and Yes/No accuracy (97.67%), as well as strong performance on the UBnormal dataset (BLEU-4: 50.32 and 78.10, Yes/No accuracy: 89.73%).", "conclusion": "T-VAD significantly enhances anomaly detection systems by improving both the granularity of localization and the interactivity with users through its text-guided, fine-grained anomaly detection capabilities, setting a new benchmark in the field."}}
{"id": "2511.00027", "pdf": "https://arxiv.org/pdf/2511.00027", "abs": "https://arxiv.org/abs/2511.00027", "authors": ["Josu Eguiluz Casta\u00f1eira", "Axel Brando", "Migle Laukyte", "Marc Serra-Vidal"], "title": "Position Paper: If Innovation in AI Systematically Violates Fundamental Rights, Is It Innovation at All?", "categories": ["cs.CY", "cs.AI", "cs.LG", "stat.AP"], "comment": "NeurIPS 2025 Position Paper track; accepted for oral and poster\n  presentation at the Thirty-Ninth Annual Conference on Neural Information\n  Processing Systems", "summary": "Artificial intelligence (AI) now permeates critical infrastructures and\ndecision-making systems where failures produce social, economic, and democratic\nharm. This position paper challenges the entrenched belief that regulation and\ninnovation are opposites. As evidenced by analogies from aviation,\npharmaceuticals, and welfare systems and recent cases of synthetic\nmisinformation, bias and unaccountable decision-making, the absence of\nwell-designed regulation has already created immeasurable damage. Regulation,\nwhen thoughtful and adaptive, is not a brake on innovation--it is its\nfoundation. The present position paper examines the EU AI Act as a model of\nrisk-based, responsibility-driven regulation that addresses the Collingridge\nDilemma: acting early enough to prevent harm, yet flexibly enough to sustain\ninnovation. Its adaptive mechanisms--regulatory sandboxes, small and medium\nenterprises (SMEs) support, real-world testing, fundamental rights impact\nassessment (FRIA) -- demonstrate how regulation can accelerate responsibly,\nrather than delay, technological progress. The position paper summarises how\ngovernance tools transform perceived burdens into tangible advantages: legal\ncertainty, consumer trust, and ethical competitiveness. Ultimately, the paper\nreframes progress: innovation and regulation advance together. By embedding\ntransparency, impact assessments, accountability, and AI literacy into design\nand deployment, the EU framework defines what responsible innovation truly\nmeans--technological ambition disciplined by democratic values and fundamental\nrights.", "AI": {"tldr": "The paper argues that regulation and innovation are complementary, using the EU AI Act as a model for responsible, risk-based regulation that fosters innovation while addressing risks to society.", "motivation": "To challenge the misconception that regulation inhibits innovation and demonstrate how thoughtful regulation can prevent societal harms caused by unregulated AI technologies.", "method": "Examines the EU AI Act as a case study, highlighting its adaptive mechanisms like regulatory sandboxes, SMEs support, and impact assessments to propose responsible AI regulation.", "result": "The paper highlights how governance tools can transform perceived regulatory barriers into benefits like legal certainty, consumer trust, and ethical competitiveness.", "conclusion": "Regulation and innovation are mutually reinforcing, and the EU AI Act exemplifies how transparency, accountability, and fundamental rights can define responsible AI innovation."}}
{"id": "2511.01360", "pdf": "https://arxiv.org/pdf/2511.01360", "abs": "https://arxiv.org/abs/2511.01360", "authors": ["Aadi Palnitkar", "Arjun Suresh", "Rishi Rajesh", "Puneet Puli"], "title": "Safer in Translation? Presupposition Robustness in Indic Languages", "categories": ["cs.CL"], "comment": "This is a submission to LREC 2026 (Language Resources and Evaluation\n  Conference 2026). Corresponding author: aadipalnitkar96@gmail.com", "summary": "Increasingly, more and more people are turning to large language models\n(LLMs) for healthcare advice and consultation, making it important to gauge the\nefficacy and accuracy of the responses of LLMs to such queries. While there are\npre-existing medical benchmarks literature which seeks to accomplish this very\ntask, these benchmarks are almost universally in English, which has led to a\nnotable gap in existing literature pertaining to multilingual LLM evaluation.\nWithin this work, we seek to aid in addressing this gap with Cancer-Myth-Indic,\nan Indic language benchmark built by translating a 500-item subset of\nCancer-Myth, sampled evenly across its original categories, into five\nunder-served but widely used languages from the subcontinent (500 per language;\n2,500 translated items total). Native-speaker translators followed a style\nguide for preserving implicit presuppositions in translation; items feature\nfalse presuppositions relating to cancer. We evaluate several popular LLMs\nunder this presupposition stress.", "AI": {"tldr": "The paper introduces Cancer-Myth-Indic, a multilingual benchmark evaluating large language models (LLMs) for healthcare responses in five Indic languages, focusing on cancer-related false presuppositions.", "motivation": "To fill the gap in multilingual evaluation of LLMs, especially for healthcare-related queries, as existing benchmarks are mainly in English and do not address under-served languages.", "method": "The authors created Cancer-Myth-Indic by translating a 500-item subset of an English benchmark, Cancer-Myth, into five Indic languages, ensuring preservation of implicit presuppositions. Native speakers followed a structured style guide during translations.", "result": "The paper provides an evaluation of popular LLMs using Cancer-Myth-Indic, highlighting their performance under the stress of false presuppositions in multilingual settings.", "conclusion": "This research advances LLM evaluation in underrepresented languages, aiding in accurate assessment of multilingual healthcare advice capabilities, and emphasizing the importance of linguistic inclusivity."}}
{"id": "2511.00540", "pdf": "https://arxiv.org/pdf/2511.00540", "abs": "https://arxiv.org/abs/2511.00540", "authors": ["Wenbing Zhu", "Chengjie Wang", "Bin-Bin Gao", "Jiangning Zhang", "Guannan Jiang", "Jie Hu", "Zhenye Gan", "Lidong Wang", "Ziqing Zhou", "Linjie Cheng", "Yurui Pan", "Bo Peng", "Mingmin Chi", "Lizhuang Ma"], "title": "Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a Modern Era", "categories": ["cs.CV"], "comment": "13 pages, 4 figures and 5 tables", "summary": "Industrial Anomaly Detection (IAD) is critical for enhancing operational\nsafety, ensuring product quality, and optimizing manufacturing efficiency\nacross global industries. However, the IAD algorithms are severely constrained\nby the limitations of existing public benchmarks. Current datasets exhibit\nrestricted category diversity and insufficient scale, frequently resulting in\nmetric saturation and limited model transferability to real-world scenarios. To\naddress this gap, we introduce Real-IAD Variety, the largest and most diverse\nIAD benchmark, comprising 198,960 high-resolution images across 160 distinct\nobject categories. Its diversity is ensured through comprehensive coverage of\n28 industries, 24 material types, and 22 color variations. Our comprehensive\nexperimental analysis validates the benchmark's substantial challenge:\nstate-of-the-art multi-class unsupervised anomaly detection methods experience\nsignificant performance degradation when scaled from 30 to 160 categories.\nCrucially, we demonstrate that vision-language models exhibit remarkable\nrobustness to category scale-up, with minimal performance variation across\ndifferent category counts, significantly enhancing generalization capabilities\nin diverse industrial contexts. The unprecedented scale and complexity of\nReal-IAD Variety position it as an essential resource for training and\nevaluating next-generation foundation models for anomaly detection. By\nproviding this comprehensive benchmark with rigorous evaluation protocols\nacross multi-class unsupervised, multi-view, and zero-/few-shot settings, we\naim to accelerate research beyond domain-specific constraints, enabling the\ndevelopment of scalable, general-purpose anomaly detection systems. Real-IAD\nVariety will be made publicly available to facilitate innovation in this\ncritical field.", "AI": {"tldr": "The paper introduces Real-IAD Variety, the largest benchmark for Industrial Anomaly Detection (IAD), addressing limitations in existing datasets and enabling improved scalability and generalization for anomaly detection systems.", "motivation": "Existing benchmarks for Industrial Anomaly Detection lack diversity, scale, and generalization capabilities, limiting advancements in operational safety and manufacturing efficiency.", "method": "The authors created Real-IAD Variety with 198,960 high-resolution images across 160 object categories, representing diverse industries, materials, and colors. They conducted experimental analyses comparing anomaly detection methods at varying category scales.", "result": "State-of-the-art anomaly detection methods showed significant performance degradation when scaled to 160 categories, while vision-language models demonstrated robust scalability and enhanced generalization.", "conclusion": "Real-IAD Variety is an essential resource for advancing research in scalable and general-purpose anomaly detection systems, providing unprecedented scope and complexity for rigorous evaluations in various settings."}}
{"id": "2511.01365", "pdf": "https://arxiv.org/pdf/2511.01365", "abs": "https://arxiv.org/abs/2511.01365", "authors": ["\u0130brahim Ethem Deveci", "Duygu Ataman"], "title": "The Ouroboros of Benchmarking: Reasoning Evaluation in an Era of Saturation", "categories": ["cs.CL"], "comment": "Accepted to NeurIPS 2025 Workshop on LLM Evaluation\n  (https://openreview.net/group?id=NeurIPS.cc/2025/Workshop/LLM_Evaluation)", "summary": "The rapid rise of Large Language Models (LLMs) and Large Reasoning Models\n(LRMs) has been accompanied by an equally rapid increase of benchmarks used to\nassess them. However, due to both improved model competence resulting from\nscaling and novel training advances as well as likely many of these datasets\nbeing included in pre or post training data, results become saturated, driving\na continuous need for new and more challenging replacements. In this paper, we\ndiscuss whether surpassing a benchmark truly demonstrates reasoning ability or\nare we simply tracking numbers divorced from the capabilities we claim to\nmeasure? We present an investigation focused on three model families, OpenAI,\nAnthropic, and Google, and how their reasoning capabilities across different\nbenchmarks evolve over the years. We also analyze performance trends over the\nyears across different reasoning tasks and discuss the current situation of\nbenchmarking and remaining challenges. By offering a comprehensive overview of\nbenchmarks and reasoning tasks, our work aims to serve as a first reference to\nground future research in reasoning evaluation and model development.", "AI": {"tldr": "The paper evaluates the rise and challenges of benchmarks used to assess reasoning abilities in LLMs and LRMs, presenting a study on model performance trends and offering insights for future evaluation standards.", "motivation": "The paper aims to address the limitations and saturation of current benchmarks used to evaluate reasoning capabilities of LLMs and LRMs, questioning whether these benchmarks effectively assess true reasoning ability.", "method": "The authors investigate reasoning capabilities by analyzing trends across benchmarks for three major model families (OpenAI, Anthropic, and Google) and discuss their evolution over the years.", "result": "The investigation highlights the saturation of benchmarks and explores trends in reasoning tasks, revealing both capabilities and challenges associated with accurately evaluating models.", "conclusion": "The paper concludes that surpassing benchmarks does not necessarily equate to reasoning ability, suggesting the need for more robust evaluation methods to compare LLMs and LRMs effectively."}}
{"id": "2511.00542", "pdf": "https://arxiv.org/pdf/2511.00542", "abs": "https://arxiv.org/abs/2511.00542", "authors": ["Kailun Su", "Ziqi He", "Xi Wang", "Yang Zhou"], "title": "MIFO: Learning and Synthesizing Multi-Instance from One Image", "categories": ["cs.CV"], "comment": "17 pages, 30 figures", "summary": "This paper proposes a method for precise learning and synthesizing\nmulti-instance semantics from a single image. The difficulty of this problem\nlies in the limited training data, and it becomes even more challenging when\nthe instances to be learned have similar semantics or appearance. To address\nthis, we propose a penalty-based attention optimization to disentangle similar\nsemantics during the learning stage. Then, in the synthesis, we introduce and\noptimize box control in attention layers to further mitigate semantic leakage\nwhile precisely controlling the output layout. Experimental results demonstrate\nthat our method achieves disentangled and high-quality semantic learning and\nsynthesis, strikingly balancing editability and instance consistency. Our\nmethod remains robust when dealing with semantically or visually similar\ninstances or rare-seen objects. The code is publicly available at\nhttps://github.com/Kareneveve/MIFO", "AI": {"tldr": "The paper introduces a method for learning and synthesizing multiple semantic instances from a single image using penalty-based attention and box control techniques.", "motivation": "The challenge of learning and synthesizing multi-instance semantics from a single image arises from limited training data and similarity in appearance or semantics among instances.", "method": "The paper proposes penalty-based attention optimization to improve semantic disentanglement during learning and incorporates box control in attention layers for precise layout synthesis.", "result": "Experimental results show that the method achieves high-quality semantic learning and synthesis with strong editability and consistency, even with rare or similar instances.", "conclusion": "The proposed approach effectively balances instance consistency and editability while demonstrating robustness in semantic learning and synthesis tasks."}}
{"id": "2511.01380", "pdf": "https://arxiv.org/pdf/2511.01380", "abs": "https://arxiv.org/abs/2511.01380", "authors": ["Wessel Poelman", "Thomas Bauwens", "Miryam de Lhoneux"], "title": "Confounding Factors in Relating Model Performance to Morphology", "categories": ["cs.CL"], "comment": "EMNLP 2025: Main Conference", "summary": "The extent to which individual language characteristics influence\ntokenization and language modeling is an open question. Differences in\nmorphological systems have been suggested as both unimportant and crucial to\nconsider (Cotterell et al., 2018; Gerz et al., 2018a; Park et al., 2021, inter\nalia). We argue this conflicting evidence is due to confounding factors in\nexperimental setups, making it hard to compare results and draw conclusions. We\nidentify confounding factors in analyses trying to answer the question of\nwhether, and how, morphology relates to language modeling. Next, we re-assess\nthree hypotheses by Arnett & Bergen (2025) for why modeling agglutinative\nlanguages results in higher perplexities than fusional languages: they look at\nmorphological alignment of tokenization, tokenization efficiency, and dataset\nsize. We show that each conclusion includes confounding factors. Finally, we\nintroduce token bigram metrics as an intrinsic way to predict the difficulty of\ncausal language modeling, and find that they are gradient proxies for\nmorphological complexity that do not require expert annotation. Ultimately, we\noutline necessities to reliably answer whether, and how, morphology relates to\nlanguage modeling.", "AI": {"tldr": "The study investigates how morphological differences in languages affect tokenization and language modeling and addresses confounding factors in previous research setups.", "motivation": "To explore and clarify the conflicting evidence on the influence of language morphology on model tokenization and performance.", "method": "The authors analyze previously proposed hypotheses about modeling agglutinative languages, identify confounding factors, and introduce token bigram metrics to predict language modeling difficulty.", "result": "Token bigram metrics are proposed as gradient proxies for morphological complexity, which eliminate the need for expert annotation while predicting challenges in causal language modeling.", "conclusion": "Better methodologies and intrinsic metrics are crucial to understanding the relationship between language morphology and language modeling."}}
{"id": "2511.00375", "pdf": "https://arxiv.org/pdf/2511.00375", "abs": "https://arxiv.org/abs/2511.00375", "authors": ["Xin Wang", "Yunhao Xiao", "Rui Qiao"], "title": "PolyRecommender: A Multimodal Recommendation System for Polymer Discovery", "categories": ["cs.LG", "cs.IR"], "comment": null, "summary": "We introduce PolyRecommender, a multimodal discovery framework that\nintegrates chemical language representations from PolyBERT with molecular\ngraph-based representations from a graph encoder. The system first retrieves\ncandidate polymers using language-based similarity and then ranks them using\nfused multimodal embeddings according to multiple target properties. By\nleveraging the complementary knowledge encoded in both modalities,\nPolyRecommender enables efficient retrieval and robust ranking across related\npolymer properties. Our work establishes a generalizable multimodal paradigm,\nadvancing AI-guided design for the discovery of next-generation polymers.", "AI": {"tldr": "PolyRecommender combines chemical language and molecular graph representations to efficiently retrieve and rank polymers based on multiple properties.", "motivation": "The paper aims to enhance AI-driven polymer discovery by utilizing both chemical language and molecular graph representations for improved retrieval and ranking.", "method": "PolyRecommender integrates a chemical language model (PolyBERT) with a graph encoder to retrieve and rank polymers using multimodal embeddings.", "result": "The framework demonstrates efficient polymer retrieval and robust ranking across related properties, leveraging the complementary strengths of its multimodal approach.", "conclusion": "PolyRecommender establishes a generalizable framework for AI-powered polymer discovery, enabling advancements in material design."}}
{"id": "2511.00560", "pdf": "https://arxiv.org/pdf/2511.00560", "abs": "https://arxiv.org/abs/2511.00560", "authors": ["Chun-Tin Wu", "Jun-Cheng Chen"], "title": "4D Neural Voxel Splatting: Dynamic Scene Rendering with Voxelized Guassian Splatting", "categories": ["cs.CV"], "comment": "10 pages, 7 figures", "summary": "Although 3D Gaussian Splatting (3D-GS) achieves efficient rendering for novel\nview synthesis, extending it to dynamic scenes still results in substantial\nmemory overhead from replicating Gaussians across frames. To address this\nchallenge, we propose 4D Neural Voxel Splatting (4D-NVS), which combines\nvoxel-based representations with neural Gaussian splatting for efficient\ndynamic scene modeling. Instead of generating separate Gaussian sets per\ntimestamp, our method employs a compact set of neural voxels with learned\ndeformation fields to model temporal dynamics. The design greatly reduces\nmemory consumption and accelerates training while preserving high image\nquality. We further introduce a novel view refinement stage that selectively\nimproves challenging viewpoints through targeted optimization, maintaining\nglobal efficiency while enhancing rendering quality for difficult viewing\nangles. Experiments demonstrate that our method outperforms state-of-the-art\napproaches with significant memory reduction and faster training, enabling\nreal-time rendering with superior visual fidelity.", "AI": {"tldr": "The paper introduces 4D Neural Voxel Splatting (4D-NVS) to improve dynamic scene rendering efficiency while reducing memory and training needs.", "motivation": "To address the memory overhead and efficiency challenges in extending 3D Gaussian Splatting (3D-GS) to dynamic scenes.", "method": "Instead of using separate Gaussian sets per frame, the method leverages neural voxels and learned deformation fields to handle temporal dynamics compactly while introducing view refinement for specific optimization.", "result": "The approach achieves reduced memory consumption, faster training, superior visual fidelity, and outperforms existing methods, enabling real-time rendering.", "conclusion": "The proposed 4D-NVS improves dynamic scene representation efficiency by merging voxel-based representations and neural Gaussian splatting, offering better performance and quality in real-time rendering."}}
{"id": "2511.01386", "pdf": "https://arxiv.org/pdf/2511.01386", "abs": "https://arxiv.org/abs/2511.01386", "authors": ["Muhammed Yusuf Kartal", "Suha Kagan Kose", "Korhan Sevin\u00e7", "Burak Aktas"], "title": "RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets", "categories": ["cs.CL", "cs.AI", "cs.IR", "H.3.3; I.2.7"], "comment": "45 pages", "summary": "Retrieval-Augmented Generation (RAG) quality depends on many interacting\nchoices across retrieval, ranking, augmentation, prompting, and generation, so\noptimizing modules in isolation is brittle. We introduce RAGSmith, a modular\nframework that treats RAG design as an end-to-end architecture search over nine\ntechnique families and 46{,}080 feasible pipeline configurations. A genetic\nsearch optimizes a scalar objective that jointly aggregates retrieval metrics\n(recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic\nsimilarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law,\nFinance, Medicine, Defense Industry, Computer Science), each with 100 questions\nspanning factual, interpretation, and long-answer types. RAGSmith finds\nconfigurations that consistently outperform naive RAG baseline by +3.8\\% on\naverage (range +1.2\\% to +6.9\\% across domains), with gains up to +12.5\\% in\nretrieval and +7.5\\% in generation. The search typically explores $\\approx\n0.2\\%$ of the space ($\\sim 100$ candidates) and discovers a robust backbone --\nvector retrieval plus post-generation reflection/revision -- augmented by\ndomain-dependent choices in expansion, reranking, augmentation, and prompt\nreordering; passage compression is never selected. Improvement magnitude\ncorrelates with question type, with larger gains on factual/long-answer mixes\nthan interpretation-heavy sets. These results provide practical, domain-aware\nguidance for assembling effective RAG systems and demonstrate the utility of\nevolutionary search for full-pipeline optimization.", "AI": {"tldr": "The paper introduces 'RAGSmith,' a framework optimizing end-to-end Retrieval-Augmented Generation (RAG) architectures by exploring 46,080 configurations through genetic search. It achieves up to 12.5% gains in retrieval and 7.5% in generation across domains like Math, Law, and Medicine relative to baselines.", "motivation": "The paper tackles the difficulty in optimizing RAG systems as individual component-focused approaches often produce brittle results. It seeks to holistically optimize RAG pipelines for better performance across diverse tasks and domains.", "method": "RAGSmith applies a genetic search over nine technique families and 46,080 feasible configurations, optimizing a scalar objective combining retrieval metrics (e.g., recall@k, mAP) with generation metrics (e.g., semantic similarity). Evaluation occurs on six Wikipedia-based domains using modular design and cross-domain testing.", "result": "RAGSmith outperforms baseline RAG pipelines by +3.8% on average, with peak retrieval (12.5%) and generation gains (7.5%) dependent on question types and domain specificity. It also identifies a robust backbone architecture combining vector retrieval and reflection/revision with context-specific enhancements.", "conclusion": "The framework delivers practical guidance for constructing highly effective RAG systems tailored to specific domains and tasks, showcasing the power of evolutionary search to optimize entire pipelines. It validates that modular and end-to-end optimization yields domain-aware and robust improvements."}}
{"id": "2511.00405", "pdf": "https://arxiv.org/pdf/2511.00405", "abs": "https://arxiv.org/abs/2511.00405", "authors": ["Zhibin Lan", "Liqiang Niu", "Fandong Meng", "Jie Zhou", "Jinsong Su"], "title": "UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The remarkable success of multimodal large language models (MLLMs) has driven\nadvances in multimodal embeddings, yet existing models remain inherently\ndiscriminative, limiting their ability to benefit from reasoning-driven\ngeneration paradigm. In this work, we pioneer the exploration of generative\nembeddings, unifying embedding tasks within a generative paradigm. We propose\nUME-R1, a universal multimodal embedding framework consisting of a two-stage\ntraining strategy: a cold-start supervised fine-tuning equips the model with\nreasoning capabilities and enables it to generate both discriminative and\ngenerative embeddings; a subsequent reinforcement learning enhances reasoning\nand further optimizes generative embedding quality. This pioneering work\nreveals four key insights: 1) generative embeddings unlock substantial\nperformance gains over conventional discriminative embeddings by leveraging the\npowerful generative reasoning capabilities of MLLMs; 2) discriminative and\ngenerative embeddings are complementary, whose combined oracle performance far\nexceeding that of either alone; 3) RL can effectively enhance generative\nembeddings, establishing a scalable optimization paradigm.; 4) repeated\nsampling at inference boosts downstream task coverage (pass@k), highlighting\nthe inference-time scalability potential of generative embeddings. Evaluated on\nthe MMEB-V2 benchmark across 78 tasks spanning video, image, and visual\ndocuments, UME-R1 significantly outperforms conventional discriminative\nembedding models and offers a foundation for more interpretable,\nreasoning-driven generative multimodal embeddings. Our code, models, and\ndatasets will be publicly available at https://github.com/XMUDeepLIT/UME-R1.", "AI": {"tldr": "This paper introduces UME-R1, a universal multimodal embedding framework designed to integrate generative reasoning capabilities into embeddings, offering significant performance gains across various multimodal tasks.", "motivation": "Current multimodal large language models are limited by their discriminative nature, which restricts their ability to adopt reasoning-driven generative approaches for improved embeddings.", "method": "The authors propose a generative embedding framework, UME-R1, employing a two-stage training process: supervised fine-tuning to incorporate reasoning abilities, followed by reinforcement learning to optimize performance and enhance generative embedding quality.", "result": "The proposed UME-R1 framework demonstrates superior performance over traditional discriminative models, achieving notable gains on the MMEB-V2 benchmark across 78 tasks, and highlights improvements in task coverage and scalability with generative embeddings.", "conclusion": "UME-R1 establishes a foundation for reasoning-driven generative multimodal embeddings, showing strong evidence of their advantages over conventional discriminative embeddings in terms of performance and interpretability."}}
{"id": "2511.00573", "pdf": "https://arxiv.org/pdf/2511.00573", "abs": "https://arxiv.org/abs/2511.00573", "authors": ["Wei Feng", "Zongyuan Ge"], "title": "Generalized Category Discovery under Domain Shift: A Frequency Domain Perspective", "categories": ["cs.CV"], "comment": "29 pages, 5 figures", "summary": "Generalized Category Discovery (GCD) aims to leverage labeled samples from\nknown categories to cluster unlabeled data that may include both known and\nunknown categories. While existing methods have achieved impressive results\nunder standard conditions, their performance often deteriorates in the presence\nof distribution shifts. In this paper, we explore a more realistic task:\nDomain-Shifted Generalized Category Discovery (DS\\_GCD), where the unlabeled\ndata includes not only unknown categories but also samples from unknown\ndomains. To tackle this challenge, we propose a\n\\textbf{\\underline{F}}requency-guided Gene\\textbf{\\underline{r}}alized\nCat\\textbf{\\underline{e}}gory Discov\\textbf{\\underline{e}}ry framework (FREE)\nthat enhances the model's ability to discover categories under distributional\nshift by leveraging frequency-domain information. Specifically, we first\npropose a frequency-based domain separation strategy that partitions samples\ninto known and unknown domains by measuring their amplitude differences. We\nthen propose two types of frequency-domain perturbation strategies: a\ncross-domain strategy, which adapts to new distributions by exchanging\namplitude components across domains, and an intra-domain strategy, which\nenhances robustness to intra-domain variations within the unknown domain.\nFurthermore, we extend the self-supervised contrastive objective and semantic\nclustering loss to better guide the training process. Finally, we introduce a\nclustering-difficulty-aware resampling technique to adaptively focus on\nharder-to-cluster categories, further enhancing model performance. Extensive\nexperiments demonstrate that our method effectively mitigates the impact of\ndistributional shifts across various benchmark datasets and achieves superior\nperformance in discovering both known and unknown categories.", "AI": {"tldr": "The paper introduces 'FREE,' a framework for Generalized Category Discovery under domain shifts, with a focus on clustering unknown categories leveraging frequency-domain information.", "motivation": "Existing methods for Generalized Category Discovery struggle to maintain performance under domain distribution shifts. There is a need for robust frameworks to address more realistic scenarios where unlabeled data contains unknown categories from new domains.", "method": "The proposed 'FREE' framework utilizes frequency-domain strategies, including domain separation, cross-domain and intra-domain perturbations, extended contrastive objectives, semantic clustering loss, and a resampling technique to focus on challenging categories.", "result": "Experimental results show that FREE effectively minimizes the impact of distribution shifts and outperforms existing methods in discovering both known and unknown categories across diverse datasets.", "conclusion": "The FREE framework demonstrates robust category discovery under challenging domain-shifted conditions, providing a promising tool for handling realistic Generalized Category Discovery scenarios."}}
{"id": "2511.01409", "pdf": "https://arxiv.org/pdf/2511.01409", "abs": "https://arxiv.org/abs/2511.01409", "authors": ["Heng Zhou", "Ao Yu", "Yuchen Fan", "Jianing Shi", "Li Kang", "Hejia Geng", "Yongting Zhang", "Yutao Fan", "Yuhao Wu", "Tiancheng He", "Yiran Qin", "Lei Bai", "Zhenfei Yin"], "title": "LiveSearchBench: An Automatically Constructed Benchmark for Retrieval and Reasoning over Dynamic Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "Evaluating large language models (LLMs) on question answering often relies on\nstatic benchmarks that reward memorization and understate the role of\nretrieval, failing to capture the dynamic nature of world knowledge. We present\nLiveSearchBench, an automated pipeline for constructing retrieval-dependent\nbenchmarks from recent knowledge updates. Our method computes deltas between\nsuccessive Wikidata snapshots, filters candidate triples for quality, and\nsynthesizes natural-language questions at three levels of reasoning difficulty,\neach guaranteed to admit a unique, verifiable answer through SPARQL validation.\nThe pipeline is fully automated, scalable across time, and minimizes human\nintervention, enabling continual regeneration of temporally grounded\nbenchmarks. Experiments show a pronounced performance drop when models confront\nfacts that post-date pretraining, with the gap most salient on multi-hop\nqueries. Retrieval augmented methods and larger, instruction-tuned models\nprovide partial gains but fail to close this recency gap. By design,\nLiveSearchBench shifts evaluation from static memorization toward tasks that\nrequire up-to-date retrieval and reasoning, offering a foundation for\nsystematic, long-term assessment of LLMs under evolving knowledge.", "AI": {"tldr": "The paper introduces LiveSearchBench, a pipeline for creating benchmarks that test large language models' (LLMs) ability to retrieve and reason using recent updates to world knowledge instead of relying solely on static memorization.", "motivation": "Static benchmarks for evaluating LLMs on question answering often favor memorization and do not account for the dynamic nature of world knowledge or retrieval, which limits comprehensive evaluation.", "method": "The authors propose LiveSearchBench, a completely automated pipeline that extracts updated knowledge deltas from successive Wikidata snapshots, filters for quality, and generates natural-language questions validated through SPARQL at three reasoning difficulty levels.", "result": "Experiments reveal a significant drop in LLM performance on knowledge facts acquired post-pretraining, especially on multi-hop queries. Retrieval-augmented methods and larger models offer partial improvements but cannot fully mitigate this performance gap.", "conclusion": "LiveSearchBench shifts the focus of LLM evaluation towards tasks emphasizing real-time retrieval and reasoning, providing a robust method for tracking and assessing their effectiveness in handling evolving knowledge environments."}}
{"id": "2511.00411", "pdf": "https://arxiv.org/pdf/2511.00411", "abs": "https://arxiv.org/abs/2511.00411", "authors": ["Zenghao Niu", "Weicheng Xie", "Siyang Song", "Zitong Yu", "Feng Liu", "Linlin Shen"], "title": "Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "accepted by iccv 2025", "summary": "Adversarial attacks present a critical challenge to deep neural networks'\nrobustness, particularly in transfer scenarios across different model\narchitectures. However, the transferability of adversarial attacks faces a\nfundamental dilemma between Exploitation (maximizing attack potency) and\nExploration (enhancing cross-model generalization). Traditional momentum-based\nmethods over-prioritize Exploitation, i.e., higher loss maxima for attack\npotency but weakened generalization (narrow loss surface). Conversely, recent\nmethods with inner-iteration sampling over-prioritize Exploration, i.e.,\nflatter loss surfaces for cross-model generalization but weakened attack\npotency (suboptimal local maxima). To resolve this dilemma, we propose a simple\nyet effective Gradient-Guided Sampling (GGS), which harmonizes both objectives\nthrough guiding sampling along the gradient ascent direction to improve both\nsampling efficiency and stability. Specifically, based on MI-FGSM, GGS\nintroduces inner-iteration random sampling and guides the sampling direction\nusing the gradient from the previous inner-iteration (the sampling's magnitude\nis determined by a random distribution). This mechanism encourages adversarial\nexamples to reside in balanced regions with both flatness for cross-model\ngeneralization and higher local maxima for strong attack potency. Comprehensive\nexperiments across multiple DNN architectures and multimodal large language\nmodels (MLLMs) demonstrate the superiority of our method over state-of-the-art\ntransfer attacks. Code is made available at https://github.com/anuin-cat/GGS.", "AI": {"tldr": "The paper proposes a Gradient-Guided Sampling (GGS) method to address the balance between attack potency and cross-model generalization in adversarial attacks, demonstrating its effectiveness across multiple neural network architectures.", "motivation": "To address the fundamental challenge in adversarial attacks: resolving the trade-off between maximizing attack potency and ensuring cross-model generalization across different neural network architectures.", "method": "The authors introduce a methodology called Gradient-Guided Sampling (GGS) which integrates inner-iteration random sampling and gradient-guided direction from a previous iteration. This harmonizes exploration and exploitation in adversarial attacks, ensuring higher loss maxima and better generalization.", "result": "The proposed GGS method outperforms state-of-the-art transfer attack methods when tested on diverse deep neural network architectures and multimodal large language models.", "conclusion": "Gradient-Guided Sampling (GGS) provides an effective balance between exploration and exploitation, improving the transferability and effectiveness of adversarial attacks."}}
{"id": "2511.00580", "pdf": "https://arxiv.org/pdf/2511.00580", "abs": "https://arxiv.org/abs/2511.00580", "authors": ["Yousuf Ahmed Siddiqui", "Sufiyaan Usmani", "Umer Tariq", "Jawwad Ahmed Shamsi", "Muhammad Burhan Khan"], "title": "TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection", "categories": ["cs.CV", "cs.AI", "68T07, 68T45, 68U10", "I.2.10; I.5.4; I.4.8; C.3"], "comment": "10 pages, 5 figures", "summary": "Video anomalies often depend on contextual information available and temporal\nevolution. Non-anomalous action in one context can be anomalous in some other\ncontext. Most anomaly detectors, however, do not notice this type of context,\nwhich seriously limits their capability to generalize to new, real-life\nsituations. Our work addresses the context-aware zero-shot anomaly detection\nchallenge, in which systems need to learn adaptively to detect new events by\ncorrelating temporal and appearance features with textual traces of memory in\nreal time. Our approach defines a memory-augmented pipeline, correlating\ntemporal signals with visual embeddings using cross-attention, and real-time\nzero-shot anomaly classification by contextual similarity scoring. We achieve\n90.4\\% AUC on UCF-Crime and 83.67\\% AP on XD-Violence, a new state-of-the-art\namong zero-shot models. Our model achieves real-time inference with high\nprecision and explainability for deployment. We show that, by fusing\ncross-attention temporal fusion and contextual memory, we achieve high fidelity\nanomaly detection, a step towards the applicability of zero-shot models in\nreal-world surveillance and infrastructure monitoring.", "AI": {"tldr": "The paper introduces a memory-augmented pipeline for context-aware zero-shot anomaly detection in videos, achieving state-of-the-art results on key benchmarks.", "motivation": "Most current anomaly detectors fail to consider context-aware signals, limiting their ability to generalize in real-life scenarios.", "method": "The approach employs cross-attention to correlate temporal and visual features with textual memory, enabling real-time zero-shot anomaly classification through contextual similarity scoring.", "result": "The proposed model achieved 90.4% AUC on the UCF-Crime dataset and 83.67% AP on the XD-Violence dataset while maintaining real-time inference capabilities.", "conclusion": "By integrating contextual memory and cross-attention mechanisms, the model facilitates accurate and explainable zero-shot anomaly detection, advancing practical applications like surveillance systems."}}
{"id": "2511.01454", "pdf": "https://arxiv.org/pdf/2511.01454", "abs": "https://arxiv.org/abs/2511.01454", "authors": ["Sergio Torres Aguilar"], "title": "\"Don't Teach Minerva\": Guiding LLMs Through Complex Syntax for Faithful Latin Translation with RAG", "categories": ["cs.CL", "cs.DL"], "comment": null, "summary": "Translating a morphology-rich, low-resource language like Latin poses\nsignificant challenges. This paper introduces a reproducible draft-based\nrefinement pipeline that elevates open-source Large Language Models (LLMs) to a\nperformance level statistically comparable to top-tier proprietary systems. Our\nmethod first uses a fine-tuned NLLB-1.3B model to generate a high-quality,\nstructurally faithful draft. A zero-shot LLM (Llama-3.3 or Qwen3) then polishes\nthis draft, a process that can be further enhanced by augmenting the context\nwith retrieved out-context examples (RAG). We demonstrate the robustness of\nthis approach on two distinct benchmarks: a standard in-domain test set\n(Rosenthal, 2023) and a new, challenging out-of-domain (OOD) set of\n12th-century Latin letters (2025). Our central finding is that this open-source\nRAG system achieves performance statistically comparable to the GPT-5 baseline,\nwithout any task-specific LLM fine-tuning. We release the pipeline, the\nChartres OOD set, and evaluation scripts and models to facilitate replicability\nand further research.", "AI": {"tldr": "The paper introduces a draft-based refinement pipeline for translating low-resource languages like Latin, achieving results comparable to proprietary systems using open-source models.", "motivation": "To overcome the challenges posed by translating a morphology-rich and low-resource language like Latin, and to develop an open-source solution comparable to top-tier proprietary systems.", "method": "The method involves using a fine-tuned NLLB-1.3B model for draft translation, followed by refining it with zero-shot LLMs (Llama-3.3 or Qwen3). It is enhanced by utilizing retrieved augmented context (RAG) examples.", "result": "The system achieves performance statistically comparable to the GPT-5 baseline on both in-domain (Rosenthal, 2023) and out-of-domain (12th-century Latin letters, 2025) benchmarks without task-specific fine-tuning.", "conclusion": "A reproducible open-source pipeline is presented for Latin translation, providing benchmarks and resources to encourage replication and further research, demonstrating the efficiency of the approach in handling niche languages."}}
{"id": "2511.00140", "pdf": "https://arxiv.org/pdf/2511.00140", "abs": "https://arxiv.org/abs/2511.00140", "authors": ["Tahmid Hasan Sakib", "Yago Romano Martinez", "Carter Brady", "Syed Rafay Hasan", "Terry N. Guo"], "title": "Supply Chain Exploitation of Secure ROS 2 Systems: A Proof-of-Concept on Autonomous Platform Compromise via Keystore Exfiltration", "categories": ["cs.CR", "cs.OS", "cs.RO", "cs.SY", "eess.SY"], "comment": "Author-accepted version (preprint). Presented at IEEE MILCOM 2025\n  Workshops, WS07: 2nd Workshop on Security, Resilience, and Robustness of\n  Systems and Software (SRRSS), Los Angeles, Oct 2025. 6 pages. Primary: cs.CR;\n  cross-lists: cs.RO, cs.OS. Program:\n  https://milcom2025.ieee-milcom.org/workshop/ws07-2nd-workshop-security-resilient-and-robustness-systems-and-software/program", "summary": "This paper presents a proof-of-concept supply chain attack against the Secure\nROS 2 (SROS 2) framework, demonstrated on a Quanser QCar2 autonomous vehicle\nplatform. A Trojan-infected Debian package modifies core ROS 2 security\ncommands to exfiltrate newly generated keystore credentials via DNS in\nbase64-encoded chunks to an attacker-controlled nameserver. Possession of these\ncredentials enables the attacker to rejoin the SROS 2 network as an\nauthenticated participant and publish spoofed control or perception messages\nwithout triggering authentication failures. We evaluate this capability on a\nsecure ROS 2 Humble testbed configured for a four-stop-sign navigation routine\nusing an Intel RealSense camera for perception. Experimental results show that\ncontrol-topic injections can cause forced braking, sustained high-speed\nacceleration, and continuous turning loops, while perception-topic spoofing can\ninduce phantom stop signs or suppress real detections. The attack generalizes\nto any data distribution service (DDS)-based robotic system using SROS 2,\nhighlighting the need for both supply chain integrity controls and runtime\nsemantic validation to safeguard autonomous systems against insider and\nimpersonation threats.", "AI": {"tldr": "The paper demonstrates a supply chain attack on SROS 2, showing how compromised credentials facilitate various malicious actions in robotic systems.", "motivation": "The authors aim to expose vulnerabilities within the Secure ROS 2 (SROS 2) framework, highlighting the potential risks of supply chain attacks and emphasizing the importance of enhancing security measures for autonomous systems.", "method": "Researchers executed a Trojan-infected Debian package to alter ROS 2 security processes, intercept credentials, and simulate spoofed control commands in a controlled experiment.", "result": "The attack successfully caused malicious behaviors in a robotic system, e.g., forced braking, false stop signs, and unrelenting turns during semiautonomous navigation.", "conclusion": "The study underscores the need for robust supply chain integrity controls and runtime semantic validation to mitigate insider and impersonation threats in autonomous systems."}}
{"id": "2511.00413", "pdf": "https://arxiv.org/pdf/2511.00413", "abs": "https://arxiv.org/abs/2511.00413", "authors": ["Shaojie Wang", "Jinghui Wang", "Yinghan Cui", "Xuxing Chen", "Chao Wang", "Liang Huang", "Xiaojiang Zhang", "Junyi Peng", "Li Wan", "Haotian Zhang", "Bin Chen"], "title": "Tree Training: Accelerating Agentic LLMs Training via Shared Prefix Reuse", "categories": ["cs.LG"], "comment": null, "summary": "In agentic LLM scenarios, an agent's interaction process during a single\nrollout often exhibits branching behaviors. Due to memory retrieval and\nconcurrent tool executions at certain decision points, the token trajectory of\none task evolves into a tree-like structure rather than a linear sequence.\nHowever, current training pipelines decompose such tree-structured trajectories\ninto separate linear segments, treating each branch as an independent sequence.\nAs a result, shared prefixes across these branches are repeatedly recomputed\nduring both forward and backward passes. To address this inefficiency, we\npropose Tree Training, a paradigm that computes each shared prefix only once\nand reuses its intermediate results across related branches during both forward\nand backward passes, substantially improving computation efficiency in\nlarge-scale agentic training. This is achieved via (i) Tree Packing, which\nefficiently reuses shared computations across trajectories, and (ii) Gradient\nRestoration, which ensures correct gradient propagation across reused prefixes.\nExperiments on multiple open-source models demonstrate up to 3.9x reduction in\ntotal training time, enabling more efficient agentic LLM SFT and RL training.", "AI": {"tldr": "The paper introduces Tree Training to improve computation efficiency in agentic LLM scenarios by reusing shared prefix computations in tree-structured token trajectories.", "motivation": "Current training methods for agentic LLM models encounter inefficiencies due to repeated computation of shared prefixes in tree-like token trajectories.", "method": "They propose Tree Packing for efficiently reusing computations and Gradient Restoration for proper gradient propagation across shared prefixes.", "result": "Experiments show up to a 3.9x reduction in training time for agentic LLM SFT and RL training.", "conclusion": "Tree Training streamlines large-scale agentic training, significantly enhancing computational efficiency through the reuse of intermediate results."}}
{"id": "2511.00613", "pdf": "https://arxiv.org/pdf/2511.00613", "abs": "https://arxiv.org/abs/2511.00613", "authors": ["Yating Yu", "Congqi Cao", "Zhaoying Wang", "Weihua Meng", "Jie Li", "Yuxin Li", "Zihao Wei", "Zhongpei Shen", "Jiajun Zhang"], "title": "CueBench: Advancing Unified Understanding of Context-Aware Video Anomalies in Real-World", "categories": ["cs.CV"], "comment": null, "summary": "How far are deep models from real-world video anomaly understanding (VAU)?\nCurrent works typically emphasize on detecting unexpected occurrences deviated\nfrom normal patterns or comprehending anomalous events with interpretable\ndescriptions. However, they exhibit only a superficial comprehension of\nreal-world anomalies, with limited breadth in complex principles and subtle\ncontext that distinguish the anomalies from normalities, e.g., climbing cliffs\nwith safety gear vs. without it. To this end, we introduce CueBench, the first\nof its kind Benchmark, devoted to Context-aware video anomalies within a\nUnified Evaluation framework. We comprehensively establish an event-centric\nhierarchical taxonomy that anchors two core event types: 14 conditional and 18\nabsolute anomaly events, defined by their refined semantics from diverse\ncontexts across 174 scenes and 198 attributes. Based on this, we propose to\nunify and benchmark context-aware VAU with various challenging tasks across\nrecognition, temporal grounding, detection, and anticipation. This also serves\nas a rigorous and fair probing evaluation suite for generative-discriminative\nas well as generalized-specialized vision-language models (VLMs). To address\nthe challenges underlying CueBench, we further develop Cue-R1 based on R1-style\nreinforcement fine-tuning with verifiable, task-aligned, and hierarchy-refined\nrewards in a unified generative manner. Extensive results on CueBench reveal\nthat, existing VLMs are still far from satisfactory real-world anomaly\nunderstanding, while our Cue-R1 surpasses these state-of-the-art approaches by\nover 24% on average.", "AI": {"tldr": "The paper introduces CueBench, a benchmark for context-aware video anomaly understanding (VAU), with a focus on capturing contextual differences and semantics in real-world scenarios. Additionally, it presents Cue-R1, a model achieving significant performance gains over state-of-the-art methods.", "motivation": "To address the limitations of existing VAU methods in understanding complex and subtle anomalies in real-world videos and to enhance anomaly comprehension using context-aware approaches.", "method": "The authors developed CueBench, a benchmark with a hierarchical taxonomy focusing on context-aware anomaly detection across various tasks, and introduced the Cue-R1 model employing R1-style reinforcement fine-tuning with task-aligned rewards.", "result": "Cue-R1 outperformed state-of-the-art methods by over 24% on average in the CueBench framework, showcasing its improved capability for real-world anomaly understanding.", "conclusion": "Current vision-language models are insufficient for comprehensive real-world video anomaly understanding. However, the proposed CueBench and Cue-R1 significantly advance the field by fostering better context-aware anomaly comprehension."}}
{"id": "2511.01470", "pdf": "https://arxiv.org/pdf/2511.01470", "abs": "https://arxiv.org/abs/2511.01470", "authors": ["Lujie Niu", "Lei Shen", "Yi Jiang", "Caixia Yuan", "Xiaojie Wang", "Wenbo Su", "Bo zheng"], "title": "BARD: budget-aware reasoning distillation", "categories": ["cs.CL"], "comment": null, "summary": "While long Chain-of-Thought (CoT) distillation effectively transfers\nreasoning capability to smaller language models, the reasoning process often\nremains redundant and computational budget uncontrollable, leading to\ninefficient resource usage. To address this limitation, we propose\n\\textbf{Budget-Aware Reasoning Distillation (BARD)}, a novel framework that\nsimultaneously distills reasoning capability and enables fine-grained control\nover the reasoning length. BARD uses the thinking budget as a user-specified\ncontrol signal, allowing the model to dynamically balance reasoning performance\nand computational efficiency. To achieve this concept, BARD introduces a\ntwo-phase training regimen. The first phase, Supervised Fine-Tuning (SFT) on\nteacher-generated long CoT data compressed to various budget levels,\nbootstrapping the model's understanding of budget constraints. The second phase\nleverages Reinforcement Learning (RL) from a reward signal in consideration of\nreasoning performance and budget fidelity simultaneously. Incorporating the\ntwo-phase regimen is crucial to avoiding policy degradation and ensuring that\nboth objectives are optimized jointly. Extensive experiments demonstrate that\nour method empowers an 8B student model to achieve strong performance on\nchallenging reasoning benchmarks (\\textit{AIME24, AIME25, GPQA}) while\nproviding precise and adaptive control over its reasoning length across a wide\nrange of budgets.", "AI": {"tldr": "Budget-Aware Reasoning Distillation (BARD) is introduced to optimize reasoning efficiency in small language models by allowing control over reasoning length within computational budgets.", "motivation": "To address inefficiencies in resource usage caused by redundant reasoning in Chain-of-Thought distillation for smaller language models.", "method": "BARD employs a two-phase training regimen: 1) Supervised Fine-Tuning (SFT) on data compressed to different budget levels and 2) Reinforcement Learning (RL) to optimize reasoning performance under budget constraints.", "result": "BARD enables an 8B student model to perform well on reasoning benchmarks (AIME24, AIME25, GPQA) while maintaining adaptive reasoning length across varying budgets.", "conclusion": "BARD effectively transfers reasoning capabilities to smaller models and provides fine-grained control over computational efficiency without compromising performance."}}
{"id": "2511.00418", "pdf": "https://arxiv.org/pdf/2511.00418", "abs": "https://arxiv.org/abs/2511.00418", "authors": ["Victory Obieke", "Emmanuel Oguadimma"], "title": "Structure-Preserving Physics-Informed Neural Network for the Korteweg--de Vries (KdV) Equation", "categories": ["cs.LG", "math-ph", "math.MP", "nlin.PS", "physics.flu-dyn"], "comment": "9 Pages, 11 figures", "summary": "Physics-Informed Neural Networks (PINNs) offer a flexible framework for\nsolving nonlinear partial differential equations (PDEs), yet conventional\nimplementations often fail to preserve key physical invariants during long-term\nintegration. This paper introduces a \\emph{structure-preserving PINN} framework\nfor the nonlinear Korteweg--de Vries (KdV) equation, a prototypical model for\nnonlinear and dispersive wave propagation. The proposed method embeds the\nconservation of mass and Hamiltonian energy directly into the loss function,\nensuring physically consistent and energy-stable evolution throughout training\nand prediction. Unlike standard \\texttt{tanh}-based\nPINNs~\\cite{raissi2019pinn,wang2022modifiedpinn}, our approach employs\nsinusoidal activation functions that enhance spectral expressiveness and\naccurately capture the oscillatory and dispersive nature of KdV solitons.\nThrough representative case studies -- including single-soliton propagation\n(shape-preserving translation), two-soliton interaction (elastic collision with\nphase shift), and cosine-pulse initialization (nonlinear dispersive breakup) --\nthe model successfully reproduces hallmark behaviors of KdV dynamics while\nmaintaining conserved invariants. Ablation studies demonstrate that combining\ninvariant-constrained optimization with sinusoidal feature mappings accelerates\nconvergence, improves long-term stability, and mitigates drift without\nmulti-stage pretraining. These results highlight that computationally\nefficient, invariant-aware regularization coupled with sinusoidal\nrepresentations yields robust, energy-consistent PINNs for Hamiltonian partial\ndifferential equations such as the KdV equation.", "AI": {"tldr": "This paper introduces a structure-preserving Physics-Informed Neural Network (PINN) framework for solving the Korteweg-de Vries (KdV) equation, focusing on maintaining physical invariants like conservation of mass and Hamiltonian energy.", "motivation": "To address the performance shortcomings of conventional PINNs in preserving physical invariants during long-term integration of nonlinear PDEs.", "method": "The proposed method integrates conservation of physical invariants into the loss function and utilizes sinusoidal activation functions for better spectral expressiveness, particularly suited for oscillatory and dispersive dynamics.", "result": "The model accurately reproduces key behaviors of KdV dynamics, such as soliton propagation and interaction, while ensuring conservation laws and improving convergence, stability, and drift mitigation.", "conclusion": "Invariant-aware optimization combined with sinusoidal feature mappings provides an effective and computationally efficient framework for energy-consistent PINNs targeting Hamiltonian PDEs like the KdV equation."}}
{"id": "2511.00643", "pdf": "https://arxiv.org/pdf/2511.00643", "abs": "https://arxiv.org/abs/2511.00643", "authors": ["Oluwatosin Alabi", "Meng Wei", "Charlie Budd", "Tom Vercauteren", "Miaojing Shi"], "title": "Grounding Surgical Action Triplets with Instrument Instance Segmentation: A Dataset and Target-Aware Fusion Approach", "categories": ["cs.CV"], "comment": null, "summary": "Understanding surgical instrument-tissue interactions requires not only\nidentifying which instrument performs which action on which anatomical target,\nbut also grounding these interactions spatially within the surgical scene.\nExisting surgical action triplet recognition methods are limited to learning\nfrom frame-level classification, failing to reliably link actions to specific\ninstrument instances.Previous attempts at spatial grounding have primarily\nrelied on class activation maps, which lack the precision and robustness\nrequired for detailed instrument-tissue interaction analysis.To address this\ngap, we propose grounding surgical action triplets with instrument instance\nsegmentation, or triplet segmentation for short, a new unified task which\nproduces spatially grounded <instrument, verb, target> outputs.We start by\npresenting CholecTriplet-Seg, a large-scale dataset containing over 30,000\nannotated frames, linking instrument instance masks with action verb and\nanatomical target annotations, and establishing the first benchmark for\nstrongly supervised, instance-level triplet grounding and evaluation.To learn\ntriplet segmentation, we propose TargetFusionNet, a novel architecture that\nextends Mask2Former with a target-aware fusion mechanism to address the\nchallenge of accurate anatomical target prediction by fusing weak anatomy\npriors with instrument instance queries.Evaluated across recognition,\ndetection, and triplet segmentation metrics, TargetFusionNet consistently\nimproves performance over existing baselines, demonstrating that strong\ninstance supervision combined with weak target priors significantly enhances\nthe accuracy and robustness of surgical action understanding.Triplet\nsegmentation establishes a unified framework for spatially grounding surgical\naction triplets. The proposed benchmark and architecture pave the way for more\ninterpretable, surgical scene understanding.", "AI": {"tldr": "The paper introduces triplet segmentation, a method that spatially grounds surgical action triplets using instrument instance segmentation, and proposes TargetFusionNet, which improves accuracy in surgical scene understanding.", "motivation": "Existing methods for linking surgical actions with instruments and targets are imprecise, limiting detailed interaction analysis in surgery.", "method": "The authors propose a novel dataset, CholecTriplet-Seg, and introduce TargetFusionNet, which incorporates weak anatomy priors fused with instrument instance queries to segment triplets in surgical scenes.", "result": "TargetFusionNet demonstrates improved performance in metrics such as recognition, detection, and segmentation compared to current baselines, and provides robust surgical action analysis.", "conclusion": "The new triplet segmentation framework and TargetFusionNet enhance surgical scene understanding, paving the way for more interpretable research and applications in instrument-tissue interaction analysis."}}
