<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 21]
- [cs.AR](#cs.AR) [Total: 5]
- [cs.CL](#cs.CL) [Total: 17]
- [cs.CV](#cs.CV) [Total: 18]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.LG](#cs.LG) [Total: 21]
- [cs.NE](#cs.NE) [Total: 5]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 17]
- [cs.SE](#cs.SE) [Total: 16]
- [q-bio.NC](#q-bio.NC) [Total: 7]
- [stat.ML](#stat.ML) [Total: 5]
- [cs.IR](#cs.IR) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin,Zaixi Zhang,Mengdi Wang,Le Cong*

Main category: cs.AI

TL;DR: STELLA is a self-evolving AI agent that autonomously improves its capabilities to address the fragmented biomedical research landscape. It outperforms leading models in benchmarks and improves with experience.


<details>
  <summary>Details</summary>
Motivation: The fragmentation in biomedical data, tools, and literature has outpaced human expertise, necessitating a scalable, adaptive AI solution.

Method: STELLA employs a multi-agent architecture with a Template Library for reasoning strategies and a Tool Ocean for dynamic tool integration via a Tool Creation Agent.

Result: STELLA achieved state-of-the-art accuracy in biomedical benchmarks, consistently improving its performance with experience.

Conclusion: STELLA marks an advancement in AI systems able to independently grow and dynamically accelerate biomedical discovery.

Abstract: The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [2] [HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection](https://arxiv.org/abs/2507.02073)
*Nikita Bhedasgaonkar,Rushikesh K. Joshi*

Main category: cs.AI

TL;DR: This paper introduces HCVR, a rule-based feature selection method combining correlations for efficient dimensionality reduction.


<details>
  <summary>Details</summary>
Motivation: To improve feature selection processes by eliminating redundancy and retaining relevant features while combining advantages of both non-iterative and iterative filtering methods.

Method: HCVR uses a hybrid approach with P2P and P2T correlations, applying backward elimination via correlation-aware voting rules.

Result: HCVR demonstrated improved performance on the SPAMBASE dataset compared to traditional non-iterative and iterative techniques.

Conclusion: HCVR is effective in dimensionality reduction, enhancing classifier performance compared to existing methods.

Abstract: In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting
Rules), a lightweight rule-based feature selection method that combines
Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to
eliminate redundant features and retain relevant ones. This method is a hybrid
of non-iterative and iterative filtering approaches for dimensionality
reduction. It is a greedy method, which works by backward elimination,
eliminating possibly multiple features at every step. The rules contribute to
voting for features, and a decision to keep or discard is made by majority
voting. The rules make use of correlation thresholds between every pair of
features, and between features and the target. We provide the results from the
application of HCVR to the SPAMBASE dataset. The results showed improvement
performance as compared to traditional non-iterative (CFS, mRMR and MI) and
iterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was
assessed based on the performance of different classifiers after applying
filtering.

</details>


### [3] [Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs](https://arxiv.org/abs/2507.02076)
*Mohammad Ali Alomrani,Yingxue Zhang,Derek Li,Qianyi Sun,Soumyasundar Pal,Zhanguang Zhang,Yaochen Hu,Rohan Deepak Ajwani,Antonios Valkanas,Raika Karimi,Peng Cheng,Yunzhou Wang,Pengyi Liao,Hanrui Huang,Bin Wang,Jianye Hao,Mark Coates*

Main category: cs.AI

TL;DR: The paper reviews efficient test-time compute (TTC) strategies to improve large language models (LLMs) reasoning efficiency by balancing computational effort with task complexity.


<details>
  <summary>Details</summary>
Motivation: Current LLMs apply a fixed amount of computational effort at reasoning regardless of task complexity, leading to inefficiencies in both simple and difficult tasks.

Method: The paper introduces a taxonomy to categorize TTC strategies into L1-controllability (fixed compute budgets) and L2-adaptiveness (dynamic scaling based on input complexity or model confidence).

Result: Benchmarking of proprietary LLMs demonstrates trade-offs between reasoning performance and token usage using diverse datasets.

Conclusion: Emerging trends like hybrid models and challenges for improving computational efficiency in LLMs are discussed to achieve better adaptability, scalability, and robustness.

Abstract: Large language models (LLMs) have rapidly progressed into general-purpose
agents capable of solving a broad spectrum of tasks. However, current models
remain inefficient at reasoning: they apply fixed inference-time compute
regardless of task complexity, often overthinking simple problems while
underthinking hard ones. This survey presents a comprehensive review of
efficient test-time compute (TTC) strategies, which aim to improve the
computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy
that distinguishes between L1-controllability, methods that operate under fixed
compute budgets, and L2-adaptiveness, methods that dynamically scale inference
based on input difficulty or model confidence. We benchmark leading proprietary
LLMs across diverse datasets, highlighting critical trade-offs between
reasoning performance and token usage. Compared to prior surveys on efficient
reasoning, our review emphasizes the practical control, adaptability, and
scalability of TTC methods. Finally, we discuss emerging trends such as hybrid
thinking models and identify key challenges for future work towards making LLMs
more computationally efficient, robust, and responsive to user constraints.

</details>


### [4] [Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab](https://arxiv.org/abs/2507.02083)
*Haonan Duan,Stephen Zhewen Lu,Caitlin Fiona Harrigan,Nishkrit Desai,Jiarui Lu,Michał Koziarski,Leonardo Cotta,Chris J. Maddison*

Main category: cs.AI

TL;DR: This paper introduces SciGym, a benchmark to assess large language models (LLMs) in scientific tasks like experiment design and analysis using a simulated 'dry lab' for biological systems.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of evaluating LLMs in scientific reasoning due to the high cost and complexity of wet-lab biological experiments.

Method: They developed SciGym, a framework that uses simulated biological systems encoded in Systems Biology Markup Language for cost-effective experimentation.

Result: They evaluated six cutting-edge LLMs on 137 systems and released a dataset of 350 systems. Performance declined with increased system complexity.

Conclusion: There is significant room for improvement in the scientific reasoning capabilities of existing LLMs, particularly for handling complex systems.

Abstract: Designing experiments and result interpretations are core scientific
competencies, particularly in biology, where researchers perturb complex
systems to uncover the underlying systems. Recent efforts to evaluate the
scientific capabilities of large language models (LLMs) fail to test these
competencies because wet-lab experimentation is prohibitively expensive: in
expertise, time and equipment. We introduce SciGym, a first-in-class benchmark
that assesses LLMs' iterative experiment design and analysis abilities in
open-ended scientific discovery tasks. SciGym overcomes the challenge of
wet-lab costs by running a dry lab of biological systems. These models, encoded
in Systems Biology Markup Language, are efficient for generating simulated
data, making them ideal testbeds for experimentation on realistically complex
systems. We evaluated six frontier LLMs on 137 small systems, and released a
total of 350 systems. Our evaluation shows that while more capable models
demonstrated superior performance, all models' performance declined
significantly as system complexity increased, suggesting substantial room for
improvement in the scientific capabilities of LLM agents.

</details>


### [5] [What Neuroscience Can Teach AI About Learning in Continuously Changing Environments](https://arxiv.org/abs/2507.02103)
*Daniel Durstewitz,Bruno Averbeck,Georgia Koppe*

Main category: cs.AI

TL;DR: This paper reviews the possibility of AI benefiting from neuroscience insights to enhance adaptability and flexibility in learning environments with shifting rules or outcomes.


<details>
  <summary>Details</summary>
Motivation: AI systems typically lack the continuous adaptability seen in animals' brains, especially in dynamic and social contexts. The paper aims to explore how neuroscience can inform AI to overcome this limitation.

Method: The paper integrates the literature on continual and in-context learning in AI with neuroscience findings about adaptive behavioral and neuronal mechanisms.

Result: The paper identifies parallels between adaptive capabilities in animals and potential advancements in AI systems to operate in real-world settings.

Conclusion: The paper highlights an agenda for mutual learning between neuroscience and AI, fostering the NeuroAI field to advance both domains effectively.

Abstract: Modern AI models, such as large language models, are usually trained once on
a huge corpus of data, potentially fine-tuned for a specific task, and then
deployed with fixed parameters. Their training is costly, slow, and gradual,
requiring billions of repetitions. In stark contrast, animals continuously
adapt to the ever-changing contingencies in their environments. This is
particularly important for social species, where behavioral policies and reward
outcomes may frequently change in interaction with peers. The underlying
computational processes are often marked by rapid shifts in an animal's
behaviour and rather sudden transitions in neuronal population activity. Such
computational capacities are of growing importance for AI systems operating in
the real world, like those guiding robots or autonomous vehicles, or for
agentic AI interacting with humans online. Can AI learn from neuroscience? This
Perspective explores this question, integrating the literature on continual and
in-context learning in AI with the neuroscience of learning on behavioral tasks
with shifting rules, reward probabilities, or outcomes. We will outline an
agenda for how specifically insights from neuroscience may inform current
developments in AI in this area, and - vice versa - what neuroscience may learn
from AI, contributing to the evolving field of NeuroAI.

</details>


### [6] [The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies](https://arxiv.org/abs/2507.02152)
*Disa Sariola,Patrick Button,Aron Culotta,Nicholas Mattei*

Main category: cs.AI

TL;DR: The paper explores the use of high-quality data from audit studies for better training and evaluation of automated hiring systems, revealing limitations in conventional fairness methods and proposing improved interventions.


<details>
  <summary>Details</summary>
Motivation: AI-driven hiring systems often face issues of bias and fairness, and the paper seeks to address these challenges by leveraging high-quality data from audit studies.

Method: The authors analyze data gathered from audit studies and evaluate fairness interventions, such as resampling training data, while proposing improved methods to measure algorithmic discrimination.

Result: The study finds that traditional fairness interventions fail to eliminate discrimination, revealing approximately 10% disparity when measured with audit study data.

Conclusion: Audit study data can enhance fairness evaluations and offer ways to reduce discrimination in automated hiring algorithms, improving their reliability and trustworthiness.

Abstract: Artificial intelligence systems, especially those using machine learning, are
being deployed in domains from hiring to loan issuance in order to automate
these complex decisions. Judging both the effectiveness and fairness of these
AI systems, and their human decision making counterpart, is a complex and
important topic studied across both computational and social sciences. Within
machine learning, a common way to address bias in downstream classifiers is to
resample the training data to offset disparities. For example, if hiring rates
vary by some protected class, then one may equalize the rate within the
training set to alleviate bias in the resulting classifier. While simple and
seemingly effective, these methods have typically only been evaluated using
data obtained through convenience samples, introducing selection bias and label
bias into metrics. Within the social sciences, psychology, public health, and
medicine, audit studies, in which fictitious ``testers'' (e.g., resumes,
emails, patient actors) are sent to subjects (e.g., job openings, businesses,
doctors) in randomized control trials, provide high quality data that support
rigorous estimates of discrimination. In this paper, we investigate how data
from audit studies can be used to improve our ability to both train and
evaluate automated hiring algorithms. We find that such data reveals cases
where the common fairness intervention method of equalizing base rates across
classes appears to achieve parity using traditional measures, but in fact has
roughly 10% disparity when measured appropriately. We additionally introduce
interventions based on individual treatment effect estimation methods that
further reduce algorithmic discrimination using this data.

</details>


### [7] [Data Diversification Methods In Alignment Enhance Math Performance In LLMs](https://arxiv.org/abs/2507.02173)
*Berkan Dokmeci,Qingyang Wu,Ben Athiwaratkun,Ce Zhang,Shuaiwen Leon Song,James Zou*

Main category: cs.AI

TL;DR: This paper explores how diversified data strategies improve large language models (LLMs) in mathematical reasoning and introduces a novel method, Diversified-ThinkSolve (DTS), which outperforms traditional approaches.


<details>
  <summary>Details</summary>
Motivation: Mathematical reasoning in large language models remains challenging despite advances in preference learning. The paper strives to address this gap by leveraging diversified data strategies to enhance alignment and reasoning capabilities.

Method: The authors evaluate three common data generation methods (temperature sampling, Chain-of-Thought prompting, and Monte Carlo Tree Search) and propose Diversified-ThinkSolve (DTS), which systematically decomposes problems into diverse reasoning paths.

Result: The proposed DTS method yields an improvement of 7.1% on GSM8K and 4.2% on MATH datasets compared to the base model while maintaining minimal computational cost (1.03x baseline). It significantly outperforms MCTS, which is computationally costly.

Conclusion: Strategically diversified preference data, as implemented in DTS, offers a more effective way to improve mathematical reasoning in LLMs with minimal overhead, outperforming traditional methods like MCTS.

Abstract: While recent advances in preference learning have enhanced alignment in human
feedback, mathematical reasoning remains a persistent challenge. We investigate
how data diversification strategies in preference optimization can improve the
mathematical reasoning abilities of large language models (LLMs). We evaluate
three common data generation methods: temperature sampling, Chain-of-Thought
prompting, and Monte Carlo Tree Search (MCTS), and introduce
Diversified-ThinkSolve (DTS), a novel structured approach that systematically
decomposes problems into diverse reasoning paths. Our results show that with
strategically diversified preference data, models can substantially improve
mathematical reasoning performance, with the best approach yielding gains of
7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong
performance, DTS incurs only a marginal computational overhead (1.03x) compared
to the baseline, while MCTS is nearly five times more costly with lower
returns. These findings demonstrate that structured exploration of diverse
problem-solving methods creates more effective preference data for mathematical
alignment than traditional approaches.

</details>


### [8] [Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust](https://arxiv.org/abs/2507.02197)
*Amogh Mannekote,Adam Davies,Guohao Li,Kristy Elizabeth Boyer,ChengXiang Zhai,Bonnie J Dorr,Francesco Pinto*

Main category: cs.AI

TL;DR: The paper examines inconsistencies between what large language models (LLMs) say (stated beliefs) and how they behave (simulation actions) during role-playing tasks, using a new evaluation framework.


<details>
  <summary>Details</summary>
Motivation: As LLMs are being increasingly used as role-playing agents in synthetic data generation for behavioral studies, ensuring role-consistent outputs is essential to establish reliable research outcomes.

Method: The study establishes a framework to measure belief-behavior consistency in LLM role-playing. It uses the GenAgents persona bank, the Trust Game, and introduces a metric to analyze variables like types of elicited beliefs, timing of information presentation, and action forecasting over time.

Result: The researchers found systematic inconsistencies between LLMs’ stated beliefs and their role-playing actions. Even when beliefs appear plausible, models often fail to implement them consistently.

Conclusion: Aligning beliefs with simulated behavior in LLMs requires careful investigation. Researchers must identify conditions under which belief-behavior consistency is reliable to ensure valid applications in behavioral modeling.

Abstract: As LLMs are increasingly studied as role-playing agents to generate synthetic
data for human behavioral research, ensuring that their outputs remain coherent
with their assigned roles has become a critical concern. In this paper, we
investigate how consistently LLM-based role-playing agents' stated beliefs
about the behavior of the people they are asked to role-play ("what they say")
correspond to their actual behavior during role-play ("how they act").
Specifically, we establish an evaluation framework to rigorously measure how
well beliefs obtained by prompting the model can predict simulation outcomes in
advance. Using an augmented version of the GenAgents persona bank and the Trust
Game (a standard economic game used to quantify players' trust and
reciprocity), we introduce a belief-behavior consistency metric to
systematically investigate how it is affected by factors such as: (1) the types
of beliefs we elicit from LLMs, like expected outcomes of simulations versus
task-relevant attributes of individual characters LLMs are asked to simulate;
(2) when and how we present LLMs with relevant information about Trust Game;
and (3) how far into the future we ask the model to forecast its actions. We
also explore how feasible it is to impose a researcher's own theoretical priors
in the event that the originally elicited beliefs are misaligned with research
objectives. Our results reveal systematic inconsistencies between LLMs' stated
(or imposed) beliefs and the outcomes of their role-playing simulation, at both
an individual- and population-level. Specifically, we find that, even when
models appear to encode plausible beliefs, they may fail to apply them in a
consistent way. These findings highlight the need to identify how and when
LLMs' stated beliefs align with their simulated behavior, allowing researchers
to use LLM-based agents appropriately in behavioral studies.

</details>


### [9] [Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning](https://arxiv.org/abs/2507.02211)
*Gustavo C. Mangold,Heitor C. M. Fernandes,Mendeli H. Vainstein*

Main category: cs.AI

TL;DR: The paper explores the impact of dilution and mobility in spatial prisoner's dilemma games using multi-agent Q-learning.


<details>
  <summary>Details</summary>
Motivation: To understand cooperation mechanisms in spatial prisoner's dilemma games by integrating reinforcement learning techniques like Q-learning.

Method: Utilization of independent multi-agent Q-learning algorithm to model and analyze diluted and mobile agents in spatial prisoner's dilemma scenarios.

Result: Observational evidence reveals that fixed and learned update rules can lead to qualitative equivalence, and symbiotic effects emerge when defining multiple actions.

Conclusion: Multi-agent Q-learning provides a versatile tool for examining complex game-theoretical phenomena and enhances understanding of cooperation in spatial dilemmas.

Abstract: Recent studies in the spatial prisoner's dilemma games with reinforcement
learning have shown that static agents can learn to cooperate through a diverse
sort of mechanisms, including noise injection, different types of learning
algorithms and neighbours' payoff knowledge.In this work, using an independent
multi-agent Q-learning algorithm, we study the effects of dilution and mobility
in the spatial version of the prisoner's dilemma. Within this setting,
different possible actions for the algorithm are defined, connecting with
previous results on the classical, non-reinforcement learning spatial
prisoner's dilemma, showcasing the versatility of the algorithm in modeling
different game-theoretical scenarios and the benchmarking potential of this
approach.As a result, a range of effects is observed, including evidence that
games with fixed update rules can be qualitatively equivalent to those with
learned ones, as well as the emergence of a symbiotic mutualistic effect
between populations that forms when multiple actions are defined.

</details>


### [10] [Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation](https://arxiv.org/abs/2507.02253)
*Jungkoo Kang*

Main category: cs.AI

TL;DR: The paper introduces NL2FLOW, an automated system for generating and evaluating planning problems to improve large language model (LLM) reasoning capabilities, showcasing its applications with datasets and model performance comparisons.


<details>
  <summary>Details</summary>
Motivation: To address the bottleneck in scalable, reliable data generation and evaluation for enhancing LLM planning and reasoning capabilities.

Method: NL2FLOW automatically generates planning problems expressed in natural language, structured representations, and formal PDDL, along with methods for rigorous evaluation of LLMs.

Result: NL2FLOW generated a dataset of 2296 planning problems, tested instruct-tuned LLMs, achieving 86% success for valid plans and revealing performance impacts from task characteristics and prompt design.

Conclusion: Decomposing reasoning tasks unnecessarily may hurt performance, encouraging direct translation from natural language to actions, while dynamic evaluation tools are essential for scaling LLMs to complex problems.

Abstract: Progress in enhancing large language model (LLM) planning and reasoning
capabilities is significantly hampered by the bottleneck of scalable, reliable
data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully
automated system for parametrically generating planning problems - expressed in
natural language, a structured intermediate representation, and formal PDDL -
and rigorously evaluating the quality of generated plans. I demonstrate
NL2FLOW's capabilities by generating a dataset of 2296 problems in the
automated workflow generation domain and evaluating multiple open-sourced,
instruct-tuned LLMs. My results reveal that the highest performing models
achieved 86% success in generating valid plans and 69% in generating optimal
plans, specifically for problems with feasible solutions. Regression analysis
shows that the influence of problem characteristics on plan generation is
contingent on both model and prompt design. Notably, I observed that the
highest success rate for translating natural language into a JSON
representation of a plan was lower than the highest rate of generating a valid
plan directly. This suggests that unnecessarily decomposing the reasoning task
- introducing intermediate translation steps - may actually degrade
performance, implying a benefit to models capable of reasoning directly from
natural language to action. As I scale LLM reasoning to increasingly complex
problems, the bottlenecks and sources of error within these systems will
inevitably shift. Therefore, a dynamic understanding of these limitations - and
the tools to systematically reveal them - will be crucial for unlocking the
full potential of LLMs as intelligent problem solvers.

</details>


### [11] [Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification](https://arxiv.org/abs/2507.02660)
*Deepak Narayan Gadde,Keerthan Kopparam Radhakrishna,Vaisakh Naduvodi Viswambharan,Aman Kumar,Djones Lettnin,Wolfgang Kunz,Sebastian Simon*

Main category: cs.AI

TL;DR: The paper introduces an AI-driven approach using Large Language Models (LLMs) for hardware design verification, achieving 95% coverage on open-source designs with reduced time.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the increasingly complex process of hardware design verification which demands significant time and effort.

Method: An agentic AI-based methodology is proposed, combining AI agents with Human-in-the-Loop (HITL) interaction for iterative and reflective design and verification processes.

Result: The approach demonstrated over 95% coverage and reduced verification time on five open-source hardware designs.

Conclusion: The agentic AI-based methodology proves to be effective, showcasing enhanced performance, adaptability, and configurability in hardware design verification.

Abstract: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is
their development process. Hardware design verification entails a methodical
and disciplined approach to the planning, development, execution, and sign-off
of functionally correct hardware designs. This tedious process requires
significant effort and time to ensure a bug-free tape-out. The field of Natural
Language Processing has undergone a significant transformation with the advent
of Large Language Models (LLMs). These powerful models, often referred to as
Generative AI (GenAI), have revolutionized how machines understand and generate
human language, enabling unprecedented advancements in a wide array of
applications, including hardware design verification. This paper presents an
agentic AI-based approach to hardware design verification, which empowers AI
agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage
in a more dynamic, iterative, and self-reflective process, ultimately
performing end-to-end hardware design and verification. This methodology is
evaluated on five open-source designs, achieving over 95% coverage with reduced
verification time while demonstrating superior performance, adaptability, and
configurability.

</details>


### [12] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: The paper focuses on the limitations of current belief revision postulates and emphasizes the need for describing the abilities of revision mechanisms rather than their constraints.


<details>
  <summary>Details</summary>
Motivation: To analyze how belief revision mechanisms can achieve various states of belief, and to highlight the shortcomings of current postulate-based approaches that focus more on constraints than abilities.

Method: The paper examines different belief revision mechanisms—including lexicographic and radical—contrasting them based on their abilities (plasticity, dogmatism, etc.) rather than constraints dictated by postulates.

Result: It is demonstrated that existing belief revision mechanisms possess distinct abilities and lack others, offering insights into their suitability for various applications.

Conclusion: Belief revision mechanisms should be assessed based on their abilities to reach diverse doxastic states rather than constrained by specific postulates. This approach can better support applications requiring flexible belief states.

Abstract: The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [13] [OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent](https://arxiv.org/abs/2507.02353)
*Bowen Chen,Zhao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: OMS is a framework designed to address limitations in keyword generation for Sponsored Search Advertising by operating on-the-fly, optimizing across multiple objectives, and self-evaluating keyword quality.


<details>
  <summary>Details</summary>
Motivation: Keyword generation in Sponsored Search Advertising faces challenges related to reliance on training data, lack of performance monitoring, and insufficient quality control.

Method: The OMS framework operates without extensive training data, monitors online performance using agentic reasoning, optimizes for multiple metrics, and evaluates keyword quality through self-reflection.

Result: OMS demonstrated superior performance compared to previous methods in experiments with benchmarks and real ad campaigns; ablation and human evaluations supported its component-wise and overall effectiveness.

Conclusion: OMS successfully addresses major limitations in LLM-based keyword generation for advertising, demonstrating adaptability, multi-objective optimization, and improved keyword quality.

Abstract: Keyword decision in Sponsored Search Advertising is critical to the success
of ad campaigns. While LLM-based methods offer automated keyword generation,
they face three major limitations: reliance on large-scale query-keyword pair
data, lack of online multi-objective performance monitoring and optimization,
and weak quality control in keyword selection. These issues hinder the agentic
use of LLMs in fully automating keyword decisions by monitoring and reasoning
over key performance indicators such as impressions, clicks, conversions, and
CTA effectiveness. To overcome these challenges, we propose OMS, a keyword
generation framework that is On-the-fly (requires no training data, monitors
online performance, and adapts accordingly), Multi-objective (employs agentic
reasoning to optimize keywords based on multiple performance metrics), and
Self-reflective (agentically evaluates keyword quality). Experiments on
benchmarks and real-world ad campaigns show that OMS outperforms existing
methods; ablation and human evaluations confirm the effectiveness of each
component and the quality of generated keywords.

</details>


### [14] [An AI-native experimental laboratory for autonomous biomolecular engineering](https://arxiv.org/abs/2507.02379)
*Mingyu Wu,Zhaoguo Wang,Jiabin Wang,Zhiyuan Dong,Jingkai Yang,Qingting Li,Tianyu Huang,Lei Zhao,Mingqiang Li,Fei Wang,Chunhai Fan,Haibo Chen*

Main category: cs.AI

TL;DR: The paper introduces an AI-driven autonomous laboratory system capable of conducting complex biomolecular engineering experiments without human intervention, achieving state-of-the-art performance and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: The need for autonomous scientific research systems that can independently conduct complex experiments and overcome resource barriers while supporting non-expert users.

Method: The system employs a co-design approach integrating AI models, experimental workflows, and instrumentation, enabling it to formulate experimental procedures, manage multi-user requests, and optimize results autonomously.

Result: The autonomous laboratory successfully supports fundamental nucleic acid functions, achieves human-comparable experimental performance, and enhances efficiency in multi-user scenarios.

Conclusion: This platform advances biomaterials research, removes reliance on specialists, improves experimental efficiency, and establishes a scalable framework for science-as-a-service applications.

Abstract: Autonomous scientific research, capable of independently conducting complex
experiments and serving non-specialists, represents a long-held aspiration.
Achieving it requires a fundamental paradigm shift driven by artificial
intelligence (AI). While autonomous experimental systems are emerging, they
remain confined to areas featuring singular objectives and well-defined, simple
experimental workflows, such as chemical synthesis and catalysis. We present an
AI-native autonomous laboratory, targeting highly complex scientific
experiments for applications like autonomous biomolecular engineering. This
system autonomously manages instrumentation, formulates experiment-specific
procedures and optimization heuristics, and concurrently serves multiple user
requests. Founded on a co-design philosophy of models, experiments, and
instruments, the platform supports the co-evolution of AI models and the
automation system. This establishes an end-to-end, multi-user autonomous
laboratory that handles complex, multi-objective experiments across diverse
instrumentation. Our autonomous laboratory supports fundamental nucleic acid
functions-including synthesis, transcription, amplification, and sequencing. It
also enables applications in fields such as disease diagnostics, drug
development, and information storage. Without human intervention, it
autonomously optimizes experimental performance to match state-of-the-art
results achieved by human scientists. In multi-user scenarios, the platform
significantly improves instrument utilization and experimental efficiency. This
platform paves the way for advanced biomaterials research to overcome
dependencies on experts and resource barriers, establishing a blueprint for
science-as-a-service at scale.

</details>


### [15] [The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning](https://arxiv.org/abs/2507.02442)
*Moto Kamiura*

Main category: cs.AI

TL;DR: The paper uses category theory to reformulate supervised learning, focusing on multiple linear regression, introducing a structure called Gauss-Markov Adjunction.


<details>
  <summary>Details</summary>
Motivation: To improve the intelligibility and interpretability of machine learning for better AI explicability and social implementation.

Method: It employs category theory by defining categories for parameters and data, and linking them with adjoint functors to formalize supervised learning.

Result: The paper derives a framework called Gauss-Markov Adjunction, describing the correspondence between parameters and residuals in the categorical setting.

Conclusion: This categorical formulation provides a theoretical foundation for explicability in AI and leverages extended denotational semantics for supervised learning.

Abstract: Enhancing the intelligibility and interpretability of machine learning is a
crucial task in responding to the demand for Explicability as an AI principle,
and in promoting the better social implementation of AI. The aim of our
research is to contribute to this improvement by reformulating machine learning
models through the lens of category theory, thereby developing a semantic
framework for structuring and understanding AI systems. Our categorical
modeling in this paper clarifies and formalizes the structural interplay
between residuals and parameters in supervised learning. The present paper
focuses on the multiple linear regression model, which represents the most
basic form of supervised learning. By defining two concrete categories
corresponding to parameters and data, along with an adjoint pair of functors
between them, we introduce our categorical formulation of supervised learning.
We show that the essential structure of this framework is captured by what we
call the Gauss-Markov Adjunction. Within this setting, the dual flow of
information can be explicitly described as a correspondence between variations
in parameters and residuals. The ordinary least squares estimator for the
parameters and the minimum residual are related via the preservation of limits
by the right adjoint functor. Furthermore, we position this formulation as an
instance of extended denotational semantics for supervised learning, and
propose applying a semantic perspective developed in theoretical computer
science as a formal foundation for Explicability in AI.

</details>


### [16] [Clarifying Before Reasoning: A Coq Prover with Structural Context](https://arxiv.org/abs/2507.02541)
*Yanzhen Lu,Hanbin Yang,Xiaodie Wang,Ge Zhang,Biao Li,Chenxu Fu,Chao Li,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.AI

TL;DR: This paper explores improving reasoning abilities of large language models (LLMs) in theorem proving by enhancing task clarity using structured semantic context, achieving a significant improvement in proof success.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the need to enhance reasoning capabilities of LLMs in complex domains, such as theorem proving, by addressing the challenge of task clarity.

Method: The authors introduce a concept-level clarity metric and a structured semantic context to task descriptions, leveraging selective concept unfolding and a Planner--Executor architecture to improve LLM performance.

Result: Adding structured context improves the clarity score from 44.5% to 82.3% and increases proof success rate from 21.8% to 45.8% using the general-purpose DeepSeek-V3 model, outperforming the previous state-of-the-art Graph2Tac. Fine-tuned smaller models outperform even further (48.6%).

Conclusion: The findings emphasize the importance of structured task representations in enhancing understanding and reasoning in LLMs, particularly for complex tasks like theorem proving.

Abstract: In this work, we investigate whether improving task clarity can enhance
reasoning ability of large language models, focusing on theorem proving in Coq.
We introduce a concept-level metric to evaluate task clarity and show that
adding structured semantic context to the standard input used by modern LLMs,
leads to a 1.85$\times$ improvement in clarity score
(44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model
\texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof
success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous
state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386
theorems randomly sampled from 15 standard Coq packages, following the same
evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller
models on our structured data can achieve even higher performance (48.6\%). Our
method uses selective concept unfolding to enrich task descriptions, and
employs a Planner--Executor architecture. These findings highlight the value of
structured task representations in bridging the gap between understanding and
reasoning.

</details>


### [17] [AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench](https://arxiv.org/abs/2507.02554)
*Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Rishi Hazra,Nicolas Baldwin,Alexis Audran-Reiss,Michael Kuchnik,Despoina Magka,Minqi Jiang,Alisia Maria Lupidi,Andrei Lupu,Roberta Raileanu,Kelvin Niu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Michael Shvartsman,Shagun Sodhani,Alexander H. Miller,Abhishek Charnalia,Derek Dunfield,Carole-Jean Wu,Pontus Stenetorp,Nicola Cancedda,Jakob Nicolaus Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: The paper explores methods to improve AI research agents' performance on MLE-bench, a benchmark for machine learning automation, achieving a 47.7% success rate in Kaggle medal attainment.


<details>
  <summary>Details</summary>
Motivation: To advance the performance of AI agents in automating ML workflows, particularly in real-world Kaggle competition scenarios, and improve upon the limitations of current systems.

Method: Formalized AI research agents as search policies, experimented with varying operator sets and search strategies (Greedy, MCTS, Evolutionary), and evaluated their performance systematically on MLE-bench lite.

Result: The best combination of search strategy and operator sets achieved a significant improvement, raising the success rate for achieving Kaggle competition medals from 39.6% to 47.7%.

Conclusion: The interplay between search strategy, operator design, and evaluation is vital for advancing automation in machine learning, paving the way for better-performing AI research agents.

Abstract: AI research agents are demonstrating great potential to accelerate scientific
progress by automating the design, implementation, and training of machine
learning models. We focus on methods for improving agents' performance on
MLE-bench, a challenging benchmark where agents compete in Kaggle competitions
to solve real-world machine learning problems. We formalize AI research agents
as search policies that navigate a space of candidate solutions, iteratively
modifying them using operators. By designing and systematically varying
different operator sets and search policies (Greedy, MCTS, Evolutionary), we
show that their interplay is critical for achieving high performance. Our best
pairing of search strategy and operator set achieves a state-of-the-art result
on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from
39.6% to 47.7%. Our investigation underscores the importance of jointly
considering the search strategy, operator design, and evaluation methodology in
advancing automated machine learning.

</details>


### [18] [Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms](https://arxiv.org/abs/2507.02582)
*Junli Jiang,Pavel Naumov*

Main category: cs.AI

TL;DR: This paper studies the computational complexity of responsibility properties in collective decision-making, focusing on diffusion and gap.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the computational aspects of responsibility, particularly its properties in the context of collective decision-making, where it has implications for AI systems.

Method: The paper uses computational complexity theory to classify the difficulty of decision-making mechanisms, analyzing them in terms of $
ablu$-completeness for both diffusion and gap properties.

Result: The sets of diffusion-free mechanisms are $
abla_2$-complete, gap-free mechanisms are $
abla_3$-complete, and their intersection is $
abla_2$-complete.

Conclusion: Understanding these complexity classes provides insight into how responsibility can be managed computationally in collective decision-making environments.

Abstract: Responsibility has long been a subject of study in law and philosophy. More
recently, it became a focus of AI literature. The article investigates the
computational complexity of two important properties of responsibility in
collective decision-making: diffusion and gap. It shows that the sets of
diffusion-free and gap-free decision-making mechanisms are $\Pi_2$-complete and
$\Pi_3$-complete, respectively. At the same time, the intersection of these
classes is $\Pi_2$-complete.

</details>


### [19] [DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making](https://arxiv.org/abs/2507.02616)
*Tianqi Shang,Weiqing He,Charles Zheng,Lingyao Li,Li Shen,Bingxin Zhao*

Main category: cs.AI

TL;DR: This paper introduces MIMIC-Patient, a dataset for dynamic healthcare simulations, and proposes DynamiCare, a multi-agent framework for iterative clinical diagnosis, powered by Large Language Models.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the gap between real-world diagnostic processes and the limitations of single-turn medical decision-making simulations by leveraging dynamic interactions.

Method: The paper develops MIMIC-Patient from MIMIC-III EHRs for patient-level simulations and designs DynamiCare, a framework that uses multiple specialist agents to query, integrate, and adapt dynamically during the diagnostic process.

Result: Experiments highlight the feasibility and effectiveness of DynamiCare, establishing a benchmark for dynamic clinical decision-making using LLM-based agents.

Conclusion: DynamiCare provides a realistic and efficient framework to simulate dynamic clinical diagnosis, advancing the utility of AI-powered agents in healthcare diagnostics.

Abstract: The rise of Large Language Models (LLMs) has enabled the development of
specialized AI agents with domain-specific reasoning and interaction
capabilities, particularly in healthcare. While recent frameworks simulate
medical decision-making, they largely focus on single-turn tasks where a doctor
agent receives full case information upfront -- diverging from the real-world
diagnostic process, which is inherently uncertain, interactive, and iterative.
In this paper, we introduce MIMIC-Patient, a structured dataset built from the
MIMIC-III electronic health records (EHRs), designed to support dynamic,
patient-level simulations. Building on this, we propose DynamiCare, a novel
dynamic multi-agent framework that models clinical diagnosis as a multi-round,
interactive loop, where a team of specialist agents iteratively queries the
patient system, integrates new information, and dynamically adapts its
composition and strategy. We demonstrate the feasibility and effectiveness of
DynamiCare through extensive experiments, establishing the first benchmark for
dynamic clinical decision-making with LLM-powered agents.

</details>


### [20] [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)
*Kenneth Payne,Baptiste Alloui-Cros*

Main category: cs.AI

TL;DR: This paper assesses the strategic intelligence of LLMs through evolutionary Iterated Prisoner's Dilemma tournaments, finding distinctive decision-making strategies for major AI models.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether LLMs possess strategic intelligence capable of reasoning about goals in competitive and uncertain environments.

Method: The authors conducted evolutionary Iterated Prisoner's Dilemma tournaments with varied termination probabilities, involving LLM agents from OpenAI, Google, and Anthropic competing against canonical strategies.

Result: LLMs showed competitiveness, surviving and sometimes proliferating, while exhibiting unique strategic behaviors: Gemini models were ruthless, OpenAI's models were highly cooperative (but disadvantageous in hostility), and Claude leaned toward forgiving reciprocity.

Conclusion: LLMs demonstrate the ability to adaptively reason about decision-making with a granular grasp of competitive strategy, bridging game theory and machine psychology.

Abstract: Are Large Language Models (LLMs) a new form of strategic intelligence, able
to reason about goals in competitive settings? We present compelling supporting
evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for
studying decision-making. We conduct the first ever series of evolutionary IPD
tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)
against agents from the leading frontier AI companies OpenAI, Google, and
Anthropic. By varying the termination probability in each tournament (the
"shadow of the future"), we introduce complexity and chance, confounding
memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and
sometimes even proliferating in these complex ecosystems. Furthermore, they
exhibit distinctive and persistent "strategic fingerprints": Google's Gemini
models proved strategically ruthless, exploiting cooperative opponents and
retaliating against defectors, while OpenAI's models remained highly
cooperative, a trait that proved catastrophic in hostile environments.
Anthropic's Claude emerged as the most forgiving reciprocator, showing
remarkable willingness to restore cooperation even after being exploited or
successfully defecting. Analysis of nearly 32,000 prose rationales provided by
the models reveals that they actively reason about both the time horizon and
their opponent's likely strategy, and we demonstrate that this reasoning is
instrumental to their decisions. This work connects classic game theory with
machine psychology, offering a rich and granular view of algorithmic
decision-making under uncertainty.

</details>


### [21] [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search](https://arxiv.org/abs/2507.02652)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yang Zhao,Hongjin Qian,Zhicheng Dou*

Main category: cs.AI

TL;DR: HiRA is a novel hierarchical framework for complex search, outperforming existing RAG systems by separating high-level planning from detailed execution.


<details>
  <summary>Details</summary>
Motivation: To overcome inefficiencies and scalability issues in retrieval-augmented generation pipelines when addressing real-world complex search scenarios.

Method: HiRA decomposes tasks into subtasks assigned to domain-specific agents with external tools, leveraging structured integration for better execution and planning decoupling.

Result: HiRA achieves superior performance over state-of-the-art RAG and agent-based systems for complex search benchmarks, improving answer quality and efficiency.

Conclusion: Decoupling strategic planning and specialized execution is key for effective and efficient multi-step information retrieval in complex search tasks.

Abstract: Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [22] [Advanced Printed Sensors for Environmental Applications: A Path Towards Sustainable Monitoring Solutions](https://arxiv.org/abs/2507.02067)
*Nikolaos Papanikolaou,Doha Touhafi,Jurgen Vandendriessche,Danial Karimi,Sohail Fatimi,Gianluca Cornetta,Abdellah Touhafi*

Main category: cs.AR

TL;DR: Printed sensors use innovative printing techniques to create adaptable environmental monitoring tools.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for flexible, cost-effective, and customizable sensors for environmental monitoring.

Method: Develop printed sensors using advanced printing technologies for detecting environmental parameters.

Result: Printed sensors showed high sensitivity and accuracy in measuring pollutants, temperature, humidity, and other critical factors.

Conclusion: Printed sensors provide a versatile, efficient solution for comprehensive environmental assessment and protection.

Abstract: Printed sensors represent a transformative advancement in sensor technology,
utilizing innovative printing techniques to create flexible, cost-effective,
and highly customizable sensing devices. Their versatility allows integration
into numerous applications across diverse fields such as monitoring a wide
range of environmental factors e.g. air and water quality, soil conditions, and
atmospheric changes among others. These sensors demonstrate high sensitivity
and accuracy in detecting pollutants, temperature variations, humidity levels,
and other critical parameters essential for environmental assessment and
protection.

</details>


### [23] [Hardware-Accelerated Algorithm for Complex Function Roots Density Graph Plotting](https://arxiv.org/abs/2507.02164)
*Ruibai Tang,Chengbin Quan*

Main category: cs.AR

TL;DR: The paper introduces a hardware-accelerated algorithm and FPGA architecture to plot the density graph of complex function roots, achieving significant energy efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency of solving and visualizing complex function roots in theoretical and applied contexts.

Method: An algorithm is designed to approximate functions with polynomials, solve roots using single-shift QR iteration, and optimize QR decomposition via Givens rotations. A pipelined FPGA architecture is employed for hardware acceleration.

Result: The FPGA implementation shows up to 65x greater energy efficiency compared to CPUs, though it is outperformed by GPUs due to fabrication differences.

Conclusion: The proposed FPGA-based design is a highly energy-efficient solution with potential for large-scale polynomial processing, despite limitations in direct performance comparisons with GPUs.

Abstract: Solving and visualizing the potential roots of complex functions is essential
in both theoretical and applied domains, yet often computationally intensive.
We present a hardware-accelerated algorithm for complex function roots density
graph plotting by approximating functions with polynomials and solving their
roots using single-shift QR iteration. By leveraging the Hessenberg structure
of companion matrices and optimizing QR decomposition with Givens rotations, we
design a pipelined FPGA architecture capable of processing a large amount of
polynomials with high throughput. Our implementation achieves up to 65x higher
energy efficiency than CPU-based approaches, and while it trails modern GPUs in
performance due to differences in fabrication technique.

</details>


### [24] [System-performance and cost modeling of Large Language Model training and inference](https://arxiv.org/abs/2507.02456)
*Wenzhe Guo,Joyjit Kundu,Uras Tos,Weijiang Kong,Giuliano Sisto,Timon Evenblij,Manu Perumkunnil*

Main category: cs.AR

TL;DR: The paper presents a performance-cost modeling methodology to optimize training and inference of large language models (LLMs) by addressing memory and compute bottlenecks, utilizing state-of-the-art techniques like flash attention and 5D parallelism.


<details>
  <summary>Details</summary>
Motivation: The exponential growth in LLM size and complexity has outpaced advancements in computing resources, creating challenges in scalability and cost efficiency.

Method: An analytical performance model combining state-of-the-art compute techniques, memory optimizations, and advanced communication techniques such as flash attention, mixture of experts, and 5D parallelism, along with an integrated chiplet cost model.

Result: The methodology provides valuable analysis of performance-cost trade-offs for system architectures and considers novel computational advancements and network topologies.

Conclusion: The proposed methodology facilitates better design of future compute systems and hardware-software co-development, addressing key bottlenecks in LLM scalability.

Abstract: Large language models (LLMs), based on transformer architectures, have
revolutionized numerous domains within artificial intelligence, science, and
engineering due to their exceptional scalability and adaptability. However, the
exponential growth in LLM size and complexity has outpaced advancements in
compute capacity, memory bandwidth, network performance, and cost efficiency,
posing significant challenges to their scalability on distributed systems. To
address these limitations, alternative model architectures, optimization
strategies, communication-aware network topologies, and novel system design
approaches have been proposed in literature. This paper introduces a
performance-cost modeling methodology for LLM training and inference that
integrates state-of-the-art compute techniques with memory optimizations, and
latest communication techniques. Building on an analytical performance model,
our approach incorporates recent innovations such as the flash attention
technique and mixture of experts models to address the memory bandwidth and
compute bottlenecks. It also considers the impact of different network
topologies and topology-specific communication algorithms with 5D parallellism.
The framework also integrates a chiplet cost model. The proposed modeling
methodology provides valuable insights to guide future compute system design
and facilitates hardware-software co-development, in particular due to its
ability to analyze performance-cost trade-offs for various system architectural
configurations.

</details>


### [25] [AC-Refiner: Efficient Arithmetic Circuit Optimization Using Conditional Diffusion Models](https://arxiv.org/abs/2507.02598)
*Chenhao Xue,Kezhi Li,Jiaxing Zhang,Yi Ren,Zhengyuan Shi,Chen Zhang,Yibo Lin,Lining Zhang,Qiang Xu,Guangyu Sun*

Main category: cs.AR

TL;DR: The paper introduces AC-Refiner, a novel framework using conditional diffusion models to optimize arithmetic circuits for improved quality, surpassing existing methods.


<details>
  <summary>Details</summary>
Motivation: Optimizing arithmetic circuits is challenging due to their vast design space and complex constraints, which directly impact digital system performance.

Method: The authors use conditional diffusion models to treat arithmetic circuit synthesis as a conditional image generation task, fine-tuning the model periodically for better designs near the Pareto frontier.

Result: AC-Refiner outperforms state-of-the-art baselines by generating superior Pareto-optimal arithmetic circuit designs.

Conclusion: AC-Refiner provides a highly efficient framework for generating high-quality arithmetic circuit designs, demonstrating practical and scalable improvements.

Abstract: Arithmetic circuits, such as adders and multipliers, are fundamental
components of digital systems, directly impacting the performance, power
efficiency, and area footprint. However, optimizing these circuits remains
challenging due to the vast design space and complex physical constraints.
While recent deep learning-based approaches have shown promise, they struggle
to consistently explore high-potential design variants, limiting their
optimization efficiency. To address this challenge, we propose AC-Refiner, a
novel arithmetic circuit optimization framework leveraging conditional
diffusion models. Our key insight is to reframe arithmetic circuit synthesis as
a conditional image generation task. By carefully conditioning the denoising
diffusion process on target quality-of-results (QoRs), AC-Refiner consistently
produces high-quality circuit designs. Furthermore, the explored designs are
used to fine-tune the diffusion model, which focuses the exploration near the
Pareto frontier. Experimental results demonstrate that AC-Refiner generates
designs with superior Pareto optimality, outperforming state-of-the-art
baselines. The performance gain is further validated by integrating AC-Refiner
into practical applications.

</details>


### [26] [Breaking the HBM Bit Cost Barrier: Domain-Specific ECC for AI Inference Infrastructure](https://arxiv.org/abs/2507.02654)
*Rui Xie,Asad Ul Haq,Yunhua Fang,Linsen Ma,Sanchari Sen,Swagath Venkataramani,Liu Liu,Tong Zhang*

Main category: cs.AR

TL;DR: The paper proposes a system-level approach to lower the cost of High-Bandwidth Memory (HBM) by removing on-die ECC and managing reliability through the memory controller.


<details>
  <summary>Details</summary>
Motivation: HBM is valuable for AI workloads due to its bandwidth and efficiency, but its cost remains a barrier due to reliability requirements.

Method: The paper uses a domain-specific ECC framework including Reed--Solomon correction, CRC detection, differential parity updates, and tunable data protection.

Result: The system maintains over 78% of throughput and 97% accuracy under high error rates in HBM when evaluated with AI inference workloads.

Conclusion: By shifting reliability management to the system level, the approach makes HBM deployment more cost-efficient while maintaining performance and accuracy.

Abstract: High-Bandwidth Memory (HBM) delivers exceptional bandwidth and energy
efficiency for AI workloads, but its high cost per bit, driven in part by
stringent on-die reliability requirements, poses a growing barrier to scalable
deployment. This work explores a system-level approach to cost reduction by
eliminating on-die ECC and shifting all fault management to the memory
controller. We introduce a domain-specific ECC framework combining
large-codeword Reed--Solomon~(RS) correction with lightweight fine-grained CRC
detection, differential parity updates to mitigate write amplification, and
tunable protection based on data importance. Our evaluation using LLM inference
workloads shows that, even under raw HBM bit error rates up to $10^{-3}$, the
system retains over 78\% of throughput and 97\% of model accuracy compared with
systems equipped with ideal error-free HBM. By treating reliability as a
tunable system parameter rather than a fixed hardware constraint, our design
opens a new path toward low-cost, high-performance HBM deployment in AI
infrastructure.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [27] [McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2507.02088)
*Tian Lan,Xiangdong Su,Xu Liu,Ruirui Wang,Ke Chang,Jiang Li,Guanglai Gao*

Main category: cs.CL

TL;DR: This paper introduces McBE, a comprehensive Chinese Bias Evaluation Benchmark for assessing biases in large language models (LLMs), overcoming limitations of existing datasets focused on English and North American culture.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive bias evaluation benchmarks for LLMs in Chinese language and culture, while improving on existing datasets that only support single tasks and focus narrowly on Western settings.

Method: The authors developed McBE, a benchmark with 4,077 bias evaluation instances, covering 12 bias categories, 82 subcategories, and 5 evaluation tasks to measure biases from multiple aspects in Chinese-based LLMs.

Result: Several popular LLMs were evaluated using McBE, revealing varying degrees of bias across the board. The analysis provided insights into the prevalence and types of biases.

Conclusion: McBE offers a culturally and linguistically relevant tool to evaluate biases in LLMs, advancing understanding and mitigation of bias while filling a key gap in research for Chinese language contexts.

Abstract: As large language models (LLMs) are increasingly applied to various NLP
tasks, their inherent biases are gradually disclosed. Therefore, measuring
biases in LLMs is crucial to mitigate its ethical risks. However, most existing
bias evaluation datasets focus on English and North American culture, and their
bias categories are not fully applicable to other cultures. The datasets
grounded in the Chinese language and culture are scarce. More importantly,
these datasets usually only support single evaluation tasks and cannot evaluate
the bias from multiple aspects in LLMs. To address these issues, we present a
Multi-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias
evaluation instances, covering 12 single bias categories, 82 subcategories and
introducing 5 evaluation tasks, providing extensive category coverage, content
diversity, and measuring comprehensiveness. Additionally, we evaluate several
popular LLMs from different series and with parameter sizes. In general, all
these LLMs demonstrated varying degrees of bias. We conduct an in-depth
analysis of results, offering novel insights into bias in LLMs.

</details>


### [28] [Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization](https://arxiv.org/abs/2507.02145)
*Keyan Jin,Yapeng Wang,Leonel Santos,Tao Fang,Xu Yang,Sio Kei Im,Hugo Gonçalo Oliveira*

Main category: cs.CL

TL;DR: This paper evaluates reasoning large language models (LLMs) and non-reasoning LLMs for dialogue summarization, finding that reasoning LLMs do not consistently outperform and often produce verbose and inconsistent results.


<details>
  <summary>Details</summary>
Motivation: To investigate whether reasoning architectures like Long Chain-of-Thought (CoT) enhance dialogue summarization, a complex task requiring both abstraction and conciseness.

Method: Conducted a systematic evaluation of reasoning and non-reasoning LLMs across generic, role-oriented, and query-oriented paradigms using benchmark datasets and advanced evaluation metrics.

Result: Reasoning LLMs struggle with verbosity, factual inconsistencies, and less concise summaries compared to non-reasoning LLMs, contrary to their performance in other reasoning tasks.

Conclusion: Reasoning LLMs exhibit limitations in dialogue summarization, and there is a need for specialized modeling and evaluation strategies tailored to this context.

Abstract: Dialogue summarization is a challenging task with significant practical value
in customer service, meeting analysis, and conversational AI. Although large
language models (LLMs) have achieved substantial progress in summarization
tasks, the performance of step-by-step reasoning architectures-specifically
Long Chain-of-Thought (CoT) implementations such as OpenAI-o1 and
DeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent
abstraction and conciseness. In this work, we present the first comprehensive
and systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning
LLMs across three major paradigms-generic, role-oriented, and query-oriented
dialogue summarization. Our study spans diverse languages, domains, and summary
lengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and
advanced evaluation protocols that include both LLM-based automatic metrics and
human-inspired criteria. Contrary to trends in other reasoning-intensive tasks,
our findings show that explicit stepwise reasoning does not consistently
improve dialogue summarization quality. Instead, reasoning LLMs are often prone
to verbosity, factual inconsistencies, and less concise summaries compared to
their non-reasoning counterparts. Through scenario-specific analyses and
detailed case studies, we further identify when and why explicit reasoning may
fail to benefit-or even hinder-summarization in complex dialogue contexts. Our
work provides new insights into the limitations of current reasoning LLMs and
highlights the need for targeted modeling and evaluation strategies for
real-world dialogue summarization.

</details>


### [29] [Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer](https://arxiv.org/abs/2507.02199)
*Wenquan Lu,Yuechuan Yang,Kyle Lee,Yanshu Li,Enqi Liu*

Main category: cs.CL

TL;DR: This paper evaluates whether depth-recurrent Transformers like Huginn-3.5B demonstrate Chain-of-Thought (CoT) reasoning internally without explicit natural language output. Findings reveal limited latent CoT reasoning, inconsistencies across recurrent blocks, and minimal performance gains from deeper recurrence.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of depth-recurrent Transformer architectures in capturing latent Chain-of-Thought (CoT) reasoning, which could bypass the efficiency trade-offs of externalizing reasoning in natural language.

Method: The study analyzes Huginn-3.5B, a depth-recurrent Transformer, using techniques such as Logit Lens and Coda Lens to probe its internal behavior and reasoning capabilities on arithmetic tasks.

Result: The results show limited latent CoT reasoning, uncover significant inconsistencies across recurrent blocks in interpretability, and demonstrate that deeper recurrence depth provides marginal performance improvements compared to explicit reasoning externalization.

Conclusion: Depth-recurrent Transformers like Huginn-3.5B offer limited advantages for internalized reasoning tasks when compared to standard architectures that externalize reasoning, suggesting inefficiency and inconsistency in their current design.

Abstract: Chain-of-thought (CoT) reasoning has enabled transformer-based language
models to excel at complex mathematics and multi-step planning. However, in
standard decoder-only architectures, these reasoning steps are externalized in
natural language, improving interpretability at the cost of efficiency. To
capture reasoning that is not easily represented in words, many works have
explored recurrent architectures that aim to internalize reasoning in latent
space, potentially supporting latent CoT. In this paper, we investigate whether
such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer
that reuses layers at inference time without increasing parameter count. We
examine the model's internal behavior on arithmetic tasks using a suite of
probing techniques including the Logit Lens and Coda Lens. Our findings reveal
limited evidence of interpretable latent CoT by tracking rank trajectories of
final and intermediate result tokens. Furthermore, we uncover significant
probing inconsistencies across recurrent blocks, where the interpretability of
hidden states depends heavily on both the layer index and the decoding method.
Finally, we empirically show that increasing recurrence depth yields only
marginal gains and falls well short of models that explicitly externalize
reasoning steps. The code is available at
https://github.com/wenquanlu/huginn-latent-cot.

</details>


### [30] [GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons](https://arxiv.org/abs/2507.02221)
*Steven Song,Anirudh Subramanyam,Zhenyu Zhang,Aarti Venkat,Robert L. Grossman*

Main category: cs.CL

TL;DR: Researchers introduced GDC Cohort Copilot, a tool that uses large language models (LLMs) to enable free-text input for creating cancer patient cohorts within the GDC platform. The tool outperforms GPT-4o and includes an interactive feature for refining cohorts.


<details>
  <summary>Details</summary>
Motivation: Many users find it difficult to navigate the hundreds of fields and properties in the GDC's Cohort Builder interface to describe their desired cohorts, especially if they are new. Allowing natural language input could simplify the process.

Method: The tool leverages large language models (LLMs) to convert user-provided, free-text cohort descriptions into the corresponding GDC cohort filters. An interactive interface also allows for additional cohort refinements.

Result: The locally-served, open-source GDC Cohort Copilot, using a custom-trained LLM, outperformed GPT-4o in generating accurate GDC cohorts based on user descriptions. It simplifies the experience for users.

Conclusion: GDC Cohort Copilot provides an innovative, open-source solution for creating GDC cohorts through natural language descriptions. The tool improves accessibility and usability in cancer genomics research.

Abstract: Motivation: The Genomic Data Commons (GDC) provides access to high quality,
harmonized cancer genomics data through a unified curation and analysis
platform centered around patient cohorts. While GDC users can interactively
create complex cohorts through the graphical Cohort Builder, users (especially
new ones) may struggle to find specific cohort descriptors across hundreds of
possible fields and properties. However, users may be better able to describe
their desired cohort in free-text natural language.
  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for
curating cohorts from the GDC. GDC Cohort Copilot automatically generates the
GDC cohort filter corresponding to a user-input natural language description of
their desired cohort, before exporting the cohort back to the GDC for further
analysis. An interactive user interface allows users to further refine the
generated cohort. We develop and evaluate multiple large language models (LLMs)
for GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC
Cohort LLM achieves better results than GPT-4o prompting in generating GDC
cohorts.
  Availability and implementation: The standalone docker image for GDC Cohort
Copilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.
Source code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC
Cohort LLM weights are available at https://huggingface.co/uc-ctds.

</details>


### [31] [MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent](https://arxiv.org/abs/2507.02259)
*Hongli Yu,Tinghong Chen,Jiangtao Feng,Jiangjie Chen,Weinan Dai,Qiying Yu,Ya-Qin Zhang,Wei-Ying Ma,Jingjing Liu,Mingxuan Wang,Hao Zhou*

Main category: cs.CL

TL;DR: The paper introduces MemAgent, a workflow optimized for handling ultra-long text efficiently, achieving impressive extension from training on short contexts to extrapolating millions of tokens.


<details>
  <summary>Details</summary>
Motivation: Handling ultra-long documents efficiently without degradation during extrapolation is a critical challenge in long-text processing.

Method: The paper introduces MemAgent, which reads text segments and updates memory using an overwrite strategy, combined with an extended DAPO algorithm for training with independent multi-conversation generation.

Result: MemAgent successfully extrapolated from an 8K training context to a 3.5M QA task with <5% performance loss and achieved over 95% accuracy in a 512K RULER test.

Conclusion: MemAgent proves to be an effective approach for ultra-long text processing, addressing challenges in extrapolation with minimal performance degradation.

Abstract: Despite improvements by length extrapolation, efficient attention and memory
modules, handling infinitely long documents with linear complexity without
performance degradation during extrapolation remains the ultimate challenge in
long-text processing. We directly optimize for long-text tasks in an end-to-end
fashion and introduce a novel agent workflow, MemAgent, which reads text in
segments and updates the memory using an overwrite strategy. We extend the DAPO
algorithm to facilitate training via independent-context multi-conversation
generation. MemAgent has demonstrated superb long-context capabilities, being
able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task
with performance loss < 5% and achieves 95%+ in 512K RULER test.

</details>


### [32] [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)
*Dohoon Kim,Donghun Kang,Taesup Moon*

Main category: cs.CL

TL;DR: The paper introduces DoMIX, a parameter-efficient method to perform efficient and parallel domain-adaptive pre-training, addressing challenges in continual domain-adaptive pre-training.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the limitations of existing continual DAP methods, including high computational cost, sensitivity to domain order, and the inability to provide models tailored to specific tasks.

Method: The authors propose a novel approach leveraging LoRA modules, which are efficient parameter-efficient fine-tuning (PEFT) methods, to achieve efficient DAP that is robust to domain order and supports task-specific pre-trained models.

Result: DoMIX demonstrated effectiveness in addressing the limitations of existing continual DAP and showed adaptability to standard large language model (LLM) fine-tuning scenarios.

Conclusion: DoMIX is a step forward in the domain-adaptive pre-training space, providing a flexible and resource-efficient method for pre-training models specialized for specific tasks, with code available for public use.

Abstract: Domain-Adaptive Pre-training (DAP) has recently gained attention for its
effectiveness in fine-tuning pre-trained models. Building on this, continual
DAP has been explored to develop pre-trained models capable of incrementally
incorporating different domain datasets. However, existing continual DAP
methods face several limitations: (1) high computational cost and GPU memory
usage during training; (2) sensitivity to incremental data order; and (3)
providing a single, generalized model for all end tasks, which contradicts the
essence of DAP. In this paper, we propose DoMIX, a novel approach that
addresses these challenges by leveraging LoRA modules, a representative
parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient
and parallel domain-adaptive pre-training that is robust to domain order and
effectively utilizes accumulated knowledge to provide tailored pre-trained
models for specific tasks. We also demonstrate that our method can be extended
beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available
at https://github.com/dohoonkim-ai/DoMIX.

</details>


### [33] [Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models](https://arxiv.org/abs/2507.02357)
*Christian Jaumann,Annemarie Friedrich,Rainer Lienhart*

Main category: cs.CL

TL;DR: The paper presents a system employing multimodal LLMs and retrieval strategies for the SciVQA 2025 task, achieving third place with an 85.12 average F1 score.


<details>
  <summary>Details</summary>
Motivation: To develop an effective system for answering scientific visual questions in the SciVQA 2025 Shared Task.

Method: The system employs an ensemble of two Multimodal Large Language Models and uses figure-specific and question-type-specific few-shot example retrieval strategies, with answer selection based on model confidence.

Result: The system ranked third out of seven in the shared task, achieving an average F1 score of 85.12 across evaluation metrics.

Conclusion: The approach demonstrated competitive performance and the developed system/code is made publicly available for further use.

Abstract: This paper describes our system for the SciVQA 2025 Shared Task on Scientific
Visual Question Answering. Our system employs an ensemble of two Multimodal
Large Language Models and various few-shot example retrieval strategies. The
model and few-shot setting are selected based on the figure and question type.
We also select answers based on the models' confidence levels. On the blind
test data, our system ranks third out of seven with an average F1 score of
85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.

</details>


### [34] [QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers](https://arxiv.org/abs/2507.02364)
*Pilsung Kang*

Main category: cs.CL

TL;DR: This paper introduces QFFN-BERT, a hybrid quantum-classical transformer where the feedforward network (FFN) modules in BERT are replaced with parameterized quantum circuits (PQCs).


<details>
  <summary>Details</summary>
Motivation: The motivation is to reduce the parameter count in Transformer encoder blocks, as FFNs contribute to two-thirds of the parameters, while potentially achieving superior performance by enhancing the expressibility of neural architectures.

Method: The authors replace the FFN modules with PQC-based layers, including a residual connection, $R_Y$ and $R_Z$ rotations, and an alternating entanglement strategy for stable training and expressibility. They test this on a classical simulator using SST-2 and DBpedia benchmarks.

Result: The proposed QFFN-BERT achieves up to 102.0% of baseline accuracy while reducing FFN parameters by over 99%. It also demonstrates competitive performance in few-shot learning and confirms PQC’s potential for data-efficient learning.

Conclusion: QFFN-BERT highlights that PQCs can act as parameter-efficient and effective alternatives to classical FFNs when co-designed with foundational deep learning principles.

Abstract: Parameterized quantum circuits (PQCs) have recently emerged as promising
components for enhancing the expressibility of neural architectures. In this
work, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the
feedforward network (FFN) modules of a compact BERT variant are replaced by
PQC-based layers. This design is motivated by the dominant parameter
contribution of FFNs, which account for approximately two-thirds of the
parameters within standard Transformer encoder blocks. While prior studies have
primarily integrated PQCs into self-attention modules, our work focuses on the
FFN and systematically investigates the trade-offs between PQC depth,
expressibility, and trainability. Our final PQC architecture incorporates a
residual connection, both $R_Y$ and $R_Z$ rotations, and an alternating
entanglement strategy to ensure stable training and high expressibility. Our
experiments, conducted on a classical simulator, on the SST-2 and DBpedia
benchmarks demonstrate two key findings. First, a carefully configured
QFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its
classical counterpart in a full-data setting while reducing FFN-specific
parameters by over 99%. Second, our model exhibits a consistent and competitive
edge in few-shot learning scenarios, confirming its potential for superior data
efficiency. These results, supported by an ablation study on a non-optimized
PQC that failed to learn, confirm that PQCs can serve as powerful and
parameter-efficient alternatives to classical FFNs when co-designed with
foundational deep learning principles.

</details>


### [35] [Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection](https://arxiv.org/abs/2507.02378)
*Weijie Lyu,Sheng-Jun Huang,Xuan Xia*

Main category: cs.CL

TL;DR: The paper introduces a method to improve training efficiency and model performance in code generation by selecting high-quality data rather than relying on large datasets.


<details>
  <summary>Details</summary>
Motivation: Current code generation methods focus on data quantity over quality, which reduces training efficiency and limits model performance improvements.

Method: The authors propose a parametric model-based data selection approach that ensures distribution consistency and diversity within smaller, high-quality training subsets.

Result: Their method, using only 10K samples, achieves a 2.4% improvement in HumanEval and a 2.3% improvement in MBPP compared to a 92K full-sampled baseline, while outperforming other sampling strategies.

Conclusion: The approach enhances model performance and training efficiency, proving that prioritizing data quality over quantity effectively reduces computational costs.

Abstract: Recent advancements in large language models (LLMs) have significantly
improved code generation and program comprehension, accelerating the evolution
of software engineering. Current methods primarily enhance model performance by
leveraging vast amounts of data, focusing on data quantity while often
overlooking data quality, thereby reducing training efficiency. To address
this, we introduce an approach that utilizes a parametric model for code data
selection, aimed at improving both training efficiency and model performance.
Our method optimizes the parametric model to ensure distribution consistency
and diversity within the selected subset, guaranteeing high-quality data.
Experimental results demonstrate that using only 10K samples, our method
achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled
baseline, outperforming other sampling approaches in both performance and
efficiency. This underscores that our method effectively boosts model
performance while significantly reducing computational costs.

</details>


### [36] [Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability](https://arxiv.org/abs/2507.02407)
*Mark Atta Mensah,Isaac Wiafe,Akon Ekpezu,Justice Kwame Appati,Jamal-Deen Abdulai,Akosua Nyarkoa Wiafe-Akenten,Frank Ernest Yeboah,Gifty Odame*

Main category: cs.CL

TL;DR: This study evaluated seven Akan ASR models using diverse datasets and found that performance is domain-dependent, with models struggling in mismatched domains.


<details>
  <summary>Details</summary>
Motivation: To address the gap in ASR research regarding cross-domain generalization, especially for low-resource languages like Akan.

Method: Benchmarked seven transformer-based ASR models (Whisper and Wav2Vec2) across four distinct Akan speech corpora and analyzed their word error rates and character error rates.

Result: Found that models perform best in their training domains, but exhibit significant performance degradation in mismatched domains; Whisper and Wav2Vec2 displayed different error behaviors.

Conclusion: Highlighted the need for domain adaptation techniques, adaptive routing strategies, and multilingual training frameworks for low-resource languages like Akan.

Abstract: Most existing automatic speech recognition (ASR) research evaluate models
using in-domain datasets. However, they seldom evaluate how they generalize
across diverse speech contexts. This study addresses this gap by benchmarking
seven Akan ASR models built on transformer architectures, such as Whisper and
Wav2Vec2, using four Akan speech corpora to determine their performance. These
datasets encompass various domains, including culturally relevant image
descriptions, informal conversations, biblical scripture readings, and
spontaneous financial dialogues. A comparison of the word error rate and
character error rate highlighted domain dependency, with models performing
optimally only within their training domains while showing marked accuracy
degradation in mismatched scenarios. This study also identified distinct error
behaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned
Whisper Akan models led to more fluent but potentially misleading transcription
errors, Wav2Vec2 produced more obvious yet less interpretable outputs when
encountering unfamiliar inputs. This trade-off between readability and
transparency in ASR errors should be considered when selecting architectures
for low-resource language (LRL) applications. These findings highlight the need
for targeted domain adaptation techniques, adaptive routing strategies, and
multilingual training frameworks for Akan and other LRLs.

</details>


### [37] [A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages](https://arxiv.org/abs/2507.02428)
*Sumaya Ahmed Salihs,Isaac Wiafe,Jamal-Deen Abdulai,Elikem Doe Atsakpo,Gifty Ayoka,Richard Cave,Akon Obu Ekpezu,Catherine Holloway,Katrin Tomanek,Fiifi Baffoe Payin Winful*

Main category: cs.CL

TL;DR: This study builds a dataset and framework to create ASR models for speech-impaired speakers in Akan, Ghana.


<details>
  <summary>Details</summary>
Motivation: To democratize ASR technology and make it accessible for low-resource and speech-impaired languages, specifically Akan in Ghana.

Method: Created an open-source impaired Akan speech dataset, a cookbook of best practices, and trained ASR models with community participation.

Result: Developed the first open-source dataset for impaired Akan speech and fine-tuned ASR models for better recognition of impaired speech in Akan.

Conclusion: The resources and tools provided by this study promote the development of inclusive ASR technologies to support diverse speech needs.

Abstract: This study presents an approach for collecting speech samples to build
Automatic Speech Recognition (ASR) models for impaired speech, particularly,
low-resource languages. It aims to democratize ASR technology and data
collection by developing a "cookbook" of best practices and training for
community-driven data collection and ASR model building. As a proof-of-concept,
this study curated the first open-source dataset of impaired speech in Akan: a
widely spoken indigenous language in Ghana. The study involved participants
from diverse backgrounds with speech impairments. The resulting dataset, along
with the cookbook and open-source tools, are publicly available to enable
researchers and practitioners to create inclusive ASR technologies tailored to
the unique needs of speech impaired individuals. In addition, this study
presents the initial results of fine-tuning open-source ASR models to better
recognize impaired speech in Akan.

</details>


### [38] [IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders](https://arxiv.org/abs/2507.02506)
*Sneha Deshmukh,Prathmesh Kamble*

Main category: cs.CL

TL;DR: The paper introduces IndianBailJudgments-1200, a publicly available dataset of 1200 Indian court bail decisions with 20+ attribute annotations to support various Legal NLP tasks.


<details>
  <summary>Details</summary>
Motivation: Legal NLP in regions like India lacks progress due to the unavailability of structured datasets, particularly in bail jurisprudence.

Method: The authors developed the IndianBailJudgments-1200 dataset by employing a GPT-4o pipeline for prompt-engineered annotations and ensuring consistency through verification.

Result: Created a 1200-record legal dataset annotated with over 20 attributes, enabling legal NLP tasks like prediction, summarization, and analysis.

Conclusion: IndianBailJudgments-1200 fills a significant gap in structured legal datasets for India, setting a foundation for developing Legal NLP applications in the region.

Abstract: Legal NLP remains underdeveloped in regions like India due to the scarcity of
structured datasets. We introduce IndianBailJudgments-1200, a new benchmark
dataset comprising 1200 Indian court judgments on bail decisions, annotated
across 20+ attributes including bail outcome, IPC sections, crime type, and
legal reasoning. Annotations were generated using a prompt-engineered GPT-4o
pipeline and verified for consistency. This resource supports a wide range of
legal NLP tasks such as outcome prediction, summarization, and fairness
analysis, and is the first publicly available dataset focused specifically on
Indian bail jurisprudence.

</details>


### [39] [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/abs/2507.02592)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Liwen Zhang,Litu Ou,Jialong Wu,Wenbiao Yin,Baixuan Li,Zhengwei Tao,Xinyu Wang,Weizhou Shen,Junkai Zhang,Dingchu Zhang,Xixi Wu,Yong Jiang,Ming Yan,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: Introducing WebSailor, a post-training methodology designed to enhance reasoning in navigating uncertain information landscapes, achieving parity with proprietary models on complex benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addressing the critical frontier of improving LLM capabilities beyond human cognitive limits, especially for information-seeking tasks with high uncertainty.

Method: A specialized pipeline involving high-uncertainty task generation via structured sampling, RFT cold start, and an efficient agentic RL algorithm called Duplicating Sampling Policy Optimization (DUPO).

Result: WebSailor substantially outperforms open-source models in complex information-seeking tasks, achieving parity with proprietary systems like DeepResearch.

Conclusion: WebSailor bridges the performance gap between open-source and proprietary models by introducing an advanced reasoning capability essential for navigating high-uncertainty data landscapes.

Abstract: Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all opensource agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


### [40] [Revisiting Active Learning under (Human) Label Variation](https://arxiv.org/abs/2507.02593)
*Cornelia Gruber,Helen Alber,Bernd Bischl,Göran Kauermann,Barbara Plank,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: This paper highlights the importance of considering human label variation (HLV) in supervised learning and active learning (AL), proposing a framework to incorporate HLV in the AL loop and exploring the use of large language models (LLMs) as annotators.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the gap in current annotation frameworks and active learning methods, which often assume a single ground truth and overlook the informative signal from human label variation.

Method: The authors propose a conceptual framework that decomposes label variation into signal (HLV) and noise, integrating HLV into active learning through instance selection, annotator choice, and label representation. They also explore the role of large language models as annotators.

Result: By proposing this framework, the paper outlines how HLV can be systematically included in active learning processes, which can lead to improved annotation and model training.

Conclusion: Recognizing and leveraging human label variation (HLV) in active learning can reflect real-world complexities more accurately, enhancing supervised learning outcomes.

Abstract: Access to high-quality labeled data remains a limiting factor in applied
supervised learning. While label variation (LV), i.e., differing labels for the
same instance, is common, especially in natural language processing, annotation
frameworks often still rest on the assumption of a single ground truth. This
overlooks human label variation (HLV), the occurrence of plausible differences
in annotations, as an informative signal. Similarly, active learning (AL), a
popular approach to optimizing the use of limited annotation budgets in
training ML models, often relies on at least one of several simplifying
assumptions, which rarely hold in practice when acknowledging HLV. In this
paper, we examine foundational assumptions about truth and label nature,
highlighting the need to decompose observed LV into signal (e.g., HLV) and
noise (e.g., annotation error). We survey how the AL and (H)LV communities have
addressed -- or neglected -- these distinctions and propose a conceptual
framework for incorporating HLV throughout the AL loop, including instance
selection, annotator choice, and label representation. We further discuss the
integration of large language models (LLM) as annotators. Our work aims to lay
a conceptual foundation for HLV-aware active learning, better reflecting the
complexities of real-world annotation.

</details>


### [41] [MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion](https://arxiv.org/abs/2507.02595)
*Xin Guan,PeiHsin Lin,Zekun Wu,Ze Wang,Ruibo Zhang,Emre Kazim,Adriano Koshiyama*

Main category: cs.CL

TL;DR: MPF is a new method for minimizing biases in large language models (LLMs) using alternative perspective baselines, offering interpretable and scalable alignment.


<details>
  <summary>Details</summary>
Motivation: The increasing demand for mitigating bias in LLMs inspired the development of a framework that is simple to deploy and effectively aligns biases.

Method: MPF builds on the SAGED pipeline, employing multiperspective data to decompose bias benchmarks into interpretable parts and guiding generative sampling towards balanced responses.

Result: MPF effectively aligns sentiment distributions with selected baselines, minimizes KL divergence, reduces calibration errors, and generalizes across unseen queries.

Conclusion: MPF is an accessible, scalable, and effective method for bias alignment, requiring no extensive retraining or prompt engineering.

Abstract: Multiperspective Fusion (MPF) is a novel posttraining alignment framework for
large language models (LLMs) developed in response to the growing need for easy
bias mitigation. Built on top of the SAGED pipeline, an automated system for
constructing bias benchmarks and extracting interpretable baseline
distributions, MPF leverages multiperspective generations to expose and align
biases in LLM outputs with nuanced, humanlike baselines. By decomposing
baseline, such as sentiment distributions from HR professionals, into
interpretable perspective components, MPF guides generation through sampling
and balancing of responses, weighted by the probabilities obtained in the
decomposition. Empirically, we demonstrate its ability to align LLM sentiment
distributions with both counterfactual baselines (absolute equality) and the HR
baseline (biased for Top Univeristy), resulting in small KL divergence,
reduction of calibration error and generalization to unseen questions. This
shows that MPF offers a scalable and interpretable method for alignment and
bias mitigation, compatible with deployed LLMs and requiring no extensive
prompt engineering or finetuning.

</details>


### [42] [Exploring Gender Bias Beyond Occupational Titles](https://arxiv.org/abs/2507.02679)
*Ahmed Sabir,Rajesh Sharama*

Main category: cs.CL

TL;DR: The paper explores gender and contextual biases, particularly in occupations, introducing a dataset and framework to quantify and explain biases.


<details>
  <summary>Details</summary>
Motivation: To investigate and quantify gender biases in contextual elements, improving explainability and addressing stereotypes.

Method: The researchers developed GenderLexicon and a framework to estimate contextual and gender biases, validating it across five datasets.

Result: The framework effectively identified gender biases beyond occupational stereotypes across diverse datasets.

Conclusion: Gender biases extend beyond occupations, and the proposed approach enhances understanding through quantifiable bias representation.

Abstract: In this work, we investigate the correlation between gender and contextual
biases, focusing on elements such as action verbs, object nouns, and
particularly on occupations. We introduce a novel dataset, GenderLexicon, and a
framework that can estimate contextual bias and its related gender bias. Our
model can interpret the bias with a score and thus improve the explainability
of gender bias. Also, our findings confirm the existence of gender biases
beyond occupational stereotypes. To validate our approach and demonstrate its
effectiveness, we conduct evaluations on five diverse datasets, including a
Japanese dataset.

</details>


### [43] [Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers](https://arxiv.org/abs/2507.02694)
*Zhijian Xu,Yilun Zhao,Manasi Patwardhan,Lovekesh Vig,Arman Cohan*

Main category: cs.CL

TL;DR: The paper explores using large language models (LLMs) to assist in identifying limitations in scientific research, introducing a benchmark called LimitGen and proposing literature retrieval to enhance LLM performance.


<details>
  <summary>Details</summary>
Motivation: The growing volume of scientific publications presents challenges for traditional peer review processes, especially in identifying limitations. The paper seeks to evaluate the potential of LLMs to improve this aspect of peer review.

Method: The authors created a taxonomy of limitations in scientific research and developed LimitGen, a benchmark with synthetic and human-written datasets. They augmented LLMs with literature retrieval to ground their assessments using prior research.

Result: The approach improved LLM capability for analyzing research paper limitations, showing potential for providing detailed, constructive feedback.

Conclusion: LLMs augmented with literature retrieval can better assist researchers in identifying limitations, complementing traditional peer review processes.

Abstract: Peer review is fundamental to scientific research, but the growing volume of
publications has intensified the challenges of this expertise-intensive
process. While LLMs show promise in various scientific tasks, their potential
to assist with peer review, particularly in identifying paper limitations,
remains understudied. We first present a comprehensive taxonomy of limitation
types in scientific research, with a focus on AI. Guided by this taxonomy, for
studying limitations, we present LimitGen, the first comprehensive benchmark
for evaluating LLMs' capability to support early-stage feedback and complement
human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a
synthetic dataset carefully created through controlled perturbations of
high-quality papers, and LimitGen-Human, a collection of real human-written
limitations. To improve the ability of LLM systems to identify limitations, we
augment them with literature retrieval, which is essential for grounding
identifying limitations in prior scientific findings. Our approach enhances the
capabilities of LLM systems to generate limitations in research papers,
enabling them to provide more concrete and constructive feedback.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [44] [Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges](https://arxiv.org/abs/2507.02074)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.CV

TL;DR: This paper surveys the use of large language models (LLMs) and vision-language models (VLMs) for crash detection in video feeds, presenting a taxonomy, performance comparison, and future directions.


<details>
  <summary>Details</summary>
Motivation: The need for effective crash detection methods in intelligent transportation systems, alongside developments in vision-language and large language models, motivates this research.

Method: The authors conduct a survey of recent methods, offering a taxonomy, dataset summaries, model architecture analysis, and benchmark comparisons.

Result: The paper categorizes various fusion strategies, summarizes datasets, analyzes model designs, and highlights performance benchmarks in crash detection.

Conclusion: This review lays a foundation for advancing research at the intersection of video understanding and foundation models, identifying challenges and opportunities in the field.

Abstract: Crash detection from video feeds is a critical problem in intelligent
transportation systems. Recent developments in large language models (LLMs) and
vision-language models (VLMs) have transformed how we process, reason about,
and summarize multimodal information. This paper surveys recent methods
leveraging LLMs for crash detection from video data. We present a structured
taxonomy of fusion strategies, summarize key datasets, analyze model
architectures, compare performance benchmarks, and discuss ongoing challenges
and opportunities. Our review provides a foundation for future research in this
fast-growing intersection of video understanding and foundation models.

</details>


### [45] [Underwater Monocular Metric Depth Estimation: Real-World Benchmarks and Synthetic Fine-Tuning](https://arxiv.org/abs/2507.02148)
*Zijie Cai,Christopher Metzler*

Main category: cs.CV

TL;DR: This paper evaluates depth estimation models in underwater environments, highlighting challenges posed by domain shifts and proposing fine-tuning techniques using synthetic underwater data to improve performance.


<details>
  <summary>Details</summary>
Motivation: Monocular depth estimation struggles with underwater environments due to domain-specific issues such as light distortion, turbidity, and lack of high-quality ground-truth data.

Method: The authors benchmark existing models on underwater datasets and fine-tune the Depth Anything V2 model using a synthetic underwater dataset generated with a physically based underwater image formation model.

Result: Fine-tuned models demonstrated consistent performance improvements across underwater benchmarks, outperforming baseline models trained on terrestrial data.

Conclusion: Domain adaptation and scale-aware supervision are critical for developing robust underwater metric depth estimation models for future research advancement.

Abstract: Monocular depth estimation has recently advanced to provide not only relative
but also metric depth predictions. However, its reliability in underwater
environments remains limited due to light attenuation and scattering, color
distortion, turbidity, and the lack of high-quality metric ground-truth data.
In this paper, we present a comprehensive benchmark of zero-shot and fine-tuned
monocular metric depth estimation models on real-world underwater datasets with
metric depth annotations, such as FLSea and SQUID. We evaluate a diverse set of
state-of-the-art models across a range of underwater conditions with different
ranges. Our results show that large-scale models trained on terrestrial (real
or synthetic) data, while effective in in-air settings, perform poorly
underwater due to significant domain shifts. To address this, we fine-tune
Depth Anything V2 with a ViT-S backbone encoder on a synthetic underwater
variant of the Hypersim dataset, which we generated using a physically based
underwater image formation model. We demonstrate our fine-tuned model
consistently improves performance across all benchmarks and outperforms
baselines trained only on the clean in-air Hypersim dataset. Our study provides
a detailed evaluation and visualization for monocular metric depth estimation
in underwater scenes, highlighting the importance of domain adaptation and
scale-aware supervision for achieving robust and generalizable metric depth
predictions in challenging underwater environments for future research.

</details>


### [46] [ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.02200)
*Xiao Wang,Jingtao Jiang,Qiang Chen,Lan Chen,Lin Zhu,Yaowei Wang,Yonghong Tian,Jin Tang*

Main category: cs.CV

TL;DR: This paper introduces ESTR-CoT, a novel chain-of-thought reasoning framework for event stream-based scene text recognition, addressing interpretability and contextual reasoning challenges.


<details>
  <summary>Details</summary>
Motivation: To improve event stream-based scene text recognition by addressing challenges of insufficient interpretability and weak contextual reasoning in existing models.

Method: The authors propose ESTR-CoT, leveraging EVA-CLIP for vision encoding, a Llama tokenizer for generation prompts, and a Q-former to align tokens with a pre-trained Vicuna-7B model. They fine-tune the framework end-to-end using a supervised learning approach and introduce a three-stage process to create a large-scale CoT dataset.

Result: The authors validate ESTR-CoT on three event stream STR benchmarks (EventSTR, WordArt*, and IC15*), showing its effectiveness and improved interpretability. They also develop a CoT dataset for future research.

Conclusion: ESTR-CoT effectively improves interpretability and contextual reasoning in event stream-based scene text recognition, making it a promising framework for challenging scenarios like low illumination and fast motion.

Abstract: Event stream based scene text recognition is a newly arising research topic
in recent years which performs better than the widely used RGB cameras in
extremely challenging scenarios, especially the low illumination, fast motion.
Existing works either adopt end-to-end encoder-decoder framework or large
language models for enhanced recognition, however, they are still limited by
the challenges of insufficient interpretability and weak contextual logical
reasoning. In this work, we propose a novel chain-of-thought reasoning based
event stream scene text recognition framework, termed ESTR-CoT. Specifically,
we first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input
event stream into tokens and utilize a Llama tokenizer to encode the given
generation prompt. A Q-former is used to align the vision token to the
pre-trained large language model Vicuna-7B and output both the answer and
chain-of-thought (CoT) reasoning process simultaneously. Our framework can be
optimized using supervised fine-tuning in an end-to-end manner. In addition, we
also propose a large-scale CoT dataset to train our framework via a three stage
processing (i.e., generation, polish, and expert verification). This dataset
provides a solid data foundation for the development of subsequent
reasoning-based large models. Extensive experiments on three event stream STR
benchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the
effectiveness and interpretability of our proposed framework. The source code
and pre-trained models will be released on
https://github.com/Event-AHU/ESTR-CoT.

</details>


### [47] [Team RAS in 9th ABAW Competition: Multimodal Compound Expression Recognition Approach](https://arxiv.org/abs/2507.02205)
*Elena Ryumina,Maxim Markitantov,Alexandr Axyonov,Dmitry Ryumin,Mikhail Dolgushin,Alexey Karpov*

Main category: cs.CV

TL;DR: The paper proposes a zero-shot multimodal approach for Compound Expression Recognition (CER) using six modalities, achieving performance near supervised methods.


<details>
  <summary>Details</summary>
Motivation: To address the need for effective recognition of complex emotional states (Compound Expression Recognition) without requiring task-specific data or domain adaptation.

Method: They use a zero-shot multimodal pipeline incorporating six modalities (facial expressions, scene context, audio, and text) and introduce modules like Multi-Head Probability Fusion (MHPF) and Compound Expressions (CE) transformation for dynamic weighting and interpretation.

Result: The approach achieves F1 scores of 46.95% on AffWild2, 49.02% on AFEW, and 34.85% on C-EXPR-DB, comparable to supervised approaches.

Conclusion: The zero-shot multimodal approach is effective for recognizing compound emotions and showcases performance comparable to supervised methods, making the source code publicly available for further advancements.

Abstract: Compound Expression Recognition (CER), a subfield of affective computing,
aims to detect complex emotional states formed by combinations of basic
emotions. In this work, we present a novel zero-shot multimodal approach for
CER that combines six heterogeneous modalities into a single pipeline: static
and dynamic facial expressions, scene and label matching, scene context, audio,
and text. Unlike previous approaches relying on task-specific training data,
our approach uses zero-shot components, including Contrastive Language-Image
Pretraining (CLIP)-based label matching and Qwen-VL for semantic scene
understanding. We further introduce a Multi-Head Probability Fusion (MHPF)
module that dynamically weights modality-specific predictions, followed by a
Compound Expressions (CE) transformation module that uses Pair-Wise Probability
Aggregation (PPA) and Pair-Wise Feature Similarity Aggregation (PFSA) methods
to produce interpretable compound emotion outputs. Evaluated under multi-corpus
training, the proposed approach shows F1 scores of 46.95% on AffWild2, 49.02%
on Acted Facial Expressions in The Wild (AFEW), and 34.85% on C-EXPR-DB via
zero-shot testing, which is comparable to the results of supervised approaches
trained on target data. This demonstrates the effectiveness of the proposed
approach for capturing CE without domain adaptation. The source code is
publicly available.

</details>


### [48] [SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers](https://arxiv.org/abs/2507.02212)
*Takuro Kawada,Shunsuke Kitada,Sota Nemoto,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: The paper introduces SciGA-145k, a dataset of 145,000 scientific papers aimed at enhancing Graphical Abstracts (GAs) selection and recommendation, accompanied by novel tasks and metrics.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of designing effective Graphical Abstracts (GAs), which require advanced visualization skills and have untapped scientific communication potential.

Method: The authors created a dataset (SciGA-145k) with 1.14 million figures, proposed two GA recommendation tasks (Intra-GA and Inter-GA), and developed a new evaluation metric (CAR).

Result: Baseline models for GA recommendation tasks and the CAR metric were introduced, facilitating a deeper understanding of GA selection and retrieval.

Conclusion: SciGA-145k offers a unified platform and methodological framework to enhance visual scientific communication and AI applications in science.

Abstract: Graphical Abstracts (GAs) play a crucial role in visually conveying the key
findings of scientific papers. While recent research has increasingly
incorporated visual materials such as Figure 1 as de facto GAs, their potential
to enhance scientific communication remains largely unexplored. Moreover,
designing effective GAs requires advanced visualization skills, creating a
barrier to their widespread adoption. To tackle these challenges, we introduce
SciGA-145k, a large-scale dataset comprising approximately 145,000 scientific
papers and 1.14 million figures, explicitly designed for supporting GA
selection and recommendation as well as facilitating research in automated GA
generation. As a preliminary step toward GA design support, we define two
tasks: 1) Intra-GA recommendation, which identifies figures within a given
paper that are well-suited to serve as GAs, and 2) Inter-GA recommendation,
which retrieves GAs from other papers to inspire the creation of new GAs. We
provide reasonable baseline models for these tasks. Furthermore, we propose
Confidence Adjusted top-1 ground truth Ratio (CAR), a novel recommendation
metric that offers a fine-grained analysis of model behavior. CAR addresses
limitations in traditional ranking-based metrics by considering cases where
multiple figures within a paper, beyond the explicitly labeled GA, may also
serve as GAs. By unifying these tasks and metrics, our SciGA-145k establishes a
foundation for advancing visual scientific communication while contributing to
the development of AI for Science.

</details>


### [49] [Understanding Trade offs When Conditioning Synthetic Data](https://arxiv.org/abs/2507.02217)
*Brandon Trabucco,Qasim Wani,Benjamin Pikus,Vasu Sharma*

Main category: cs.CV

TL;DR: The paper explores using diffusion models for generating synthetic data for training robust object detectors in industrial vision systems, comparing two conditioning methods: prompt-based and layout-based.


<details>
  <summary>Details</summary>
Motivation: The need to efficiently train object detectors with limited high-quality images and existing challenges in generating synthetic data via traditional 3D rendering pipelines motivates exploring diffusion models for better synthetic data generation.

Method: The authors evaluate synthetic data generation via diffusion models with different conditioning schemes—prompt-based and layout-based—on object detection benchmarks.

Result: Prompt conditioning performs better under narrow conditioning cues, while layout conditioning outperforms prompt conditioning as data diversity grows. Synthetic data improves detection metrics significantly, raising mean average precision by 34% on average and up to 177% in certain cases over real data usage.

Conclusion: Diffusion model-based synthetic data generation, tailored appropriately with effective conditioning strategies, offers a compelling solution for boosting object detection performance in data-limited regimes.

Abstract: Learning robust object detectors from only a handful of images is a critical
challenge in industrial vision systems, where collecting high quality training
data can take months. Synthetic data has emerged as a key solution for data
efficient visual inspection and pick and place robotics. Current pipelines rely
on 3D engines such as Blender or Unreal, which offer fine control but still
require weeks to render a small dataset, and the resulting images often suffer
from a large gap between simulation and reality. Diffusion models promise a
step change because they can generate high quality images in minutes, yet
precise control, especially in low data regimes, remains difficult. Although
many adapters now extend diffusion beyond plain text prompts, the effect of
different conditioning schemes on synthetic data quality is poorly understood.
We study eighty diverse visual concepts drawn from four standard object
detection benchmarks and compare two conditioning strategies: prompt based and
layout based. When the set of conditioning cues is narrow, prompt conditioning
yields higher quality synthetic data; as diversity grows, layout conditioning
becomes superior. When layout cues match the full training distribution,
synthetic data raises mean average precision by an average of thirty four
percent and by as much as one hundred seventy seven percent compared with using
real data alone.

</details>


### [50] [High-Fidelity Differential-information Driven Binary Vision Transformer](https://arxiv.org/abs/2507.02222)
*Tian Gao,Zhiyuan Zhang,Kaijie Yin,Xu-Cheng Zhong,Hui Kong*

Main category: cs.CV

TL;DR: The authors introduce DIDB-ViT, a binary vision transformer architecture that minimizes performance loss and computational demands for edge devices while improving image classification and segmentation.


<details>
  <summary>Details</summary>
Motivation: To solve the challenges of performance degradation and reliance on full-precision modules in binary vision transformers while advancing edge-device deployment.

Method: The approach involves an innovative attention module with differential information, frequency decomposition via discrete Haar wavelets, and an improved activation function (RPReLU).

Result: DIDB-ViT outperforms existing network quantization methods, achieving superior results in image classification and segmentation across diverse ViT architectures.

Conclusion: The proposed DIDB-ViT successfully balances performance, computational efficiency, and fidelity, making it a strong candidate for edge-device applications.

Abstract: The binarization of vision transformers (ViTs) offers a promising approach to
addressing the trade-off between high computational/storage demands and the
constraints of edge-device deployment. However, existing binary ViT methods
often suffer from severe performance degradation or rely heavily on
full-precision modules. To address these issues, we propose DIDB-ViT, a novel
binary ViT that is highly informative while maintaining the original ViT
architecture and computational efficiency. Specifically, we design an
informative attention module incorporating differential information to mitigate
information loss caused by binarization and enhance high-frequency retention.
To preserve the fidelity of the similarity calculations between binary Q and K
tensors, we apply frequency decomposition using the discrete Haar wavelet and
integrate similarities across different frequencies. Additionally, we introduce
an improved RPReLU activation function to restructure the activation
distribution, expanding the model's representational capacity. Experimental
results demonstrate that our DIDB-ViT significantly outperforms
state-of-the-art network quantization methods in multiple ViT architectures,
achieving superior image classification and segmentation performance.

</details>


### [51] [Red grape detection with accelerated artificial neural networks in the FPGA's programmable logic](https://arxiv.org/abs/2507.02443)
*Sandro Costa Magalhães,Marco Almeida,Filipe Neves dos Santos,António Paulo Moreira,Jorge Dias*

Main category: cs.CV

TL;DR: The paper focuses on using FPGAs to speed up neural network-based object detection in robots, achieving high inference speeds and accuracy.


<details>
  <summary>Details</summary>
Motivation: Robots typically slow down for object detection due to limitations in camera framerate and detection algorithm speed, which impacts task efficiency. The paper aims to address this constraint.

Method: The authors deployed three quantized ANNs (MobileNet v1, CNV with 2-bit, and CNV with 1-bit) inside an FPGA's programmable logic using the FINN architecture, and trained them using the RG2C dataset.

Result: MobileNet v1 achieved the best performance with a 98% success rate and 6611 FPS inference speed. The study demonstrated significant acceleration of ANNs using FPGAs.

Conclusion: This research confirms the effectiveness of FPGAs in enhancing inference speed and accuracy of neural networks, making them suitable for robotic attention mechanisms.

Abstract: Robots usually slow down for canning to detect objects while moving.
Additionally, the robot's camera is configured with a low framerate to track
the velocity of the detection algorithms. This would be constrained while
executing tasks and exploring, making robots increase the task execution time.
AMD has developed the Vitis-AI framework to deploy detection algorithms into
FPGAs. However, this tool does not fully use the FPGAs' PL. In this work, we
use the FINN architecture to deploy three ANNs, MobileNet v1 with 4-bit
quantisation, CNV with 2-bit quantisation, and CNV with 1-bit quantisation
(BNN), inside an FPGA's PL. The models were trained on the RG2C dataset. This
is a self-acquired dataset released in open access. MobileNet v1 performed
better, reaching a success rate of 98 % and an inference speed of 6611 FPS. In
this work, we proved that we can use FPGAs to speed up ANNs and make them
suitable for attention mechanisms.

</details>


### [52] [FMOcc: TPV-Driven Flow Matching for 3D Occupancy Prediction with Selective State Space Model](https://arxiv.org/abs/2507.02250)
*Jiangxia Chen,Tongyuan Huang,Ke Song*

Main category: cs.CV

TL;DR: This paper introduces FMOcc, a novel method for 3D semantic occupancy prediction in few-frame scenarios, aiming to improve prediction accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Enhance the accuracy and efficiency of 3D semantic occupancy predictions in autonomous driving, especially for occluded and distant scenes, while addressing inherent limitations in few-frame image data and 3D space redundancies.

Method: The paper proposes FMOcc, which uses a flow-matching selective state-space model (FMSSM) for feature refinement, along with TPV SSM and Plane Selective SSM (PS3M) layers to selectively filter occupancy data and improve model efficiency. Additionally, a Mask Training (MT) method is introduced to enhance robustness against sensor data loss.

Result: FMOcc outperforms current state-of-the-art methods with notable improvements in metrics such as 43.1% RayIoU and 39.8% mIoU on the Occ3D-nuScenes dataset. It also showcases efficient memory usage and reasonable inference times.

Conclusion: The proposed FMOcc significantly enhances prediction capabilities for 3D semantic occupancy while ensuring efficiency and robustness, making it highly suitable for autonomous driving applications.

Abstract: 3D semantic occupancy prediction plays a pivotal role in autonomous driving.
However, inherent limitations of fewframe images and redundancy in 3D space
compromise prediction accuracy for occluded and distant scenes. Existing
methods enhance performance by fusing historical frame data, which need
additional data and significant computational resources. To address these
issues, this paper propose FMOcc, a Tri-perspective View (TPV) refinement
occupancy network with flow matching selective state space model for few-frame
3D occupancy prediction. Firstly, to generate missing features, we designed a
feature refinement module based on a flow matching model, which is called Flow
Matching SSM module (FMSSM). Furthermore, by designing the TPV SSM layer and
Plane Selective SSM (PS3M), we selectively filter TPV features to reduce the
impact of air voxels on non-air voxels, thereby enhancing the overall
efficiency of the model and prediction capability for distant scenes. Finally,
we design the Mask Training (MT) method to enhance the robustness of FMOcc and
address the issue of sensor data loss. Experimental results on the
Occ3D-nuScenes and OpenOcc datasets show that our FMOcc outperforms existing
state-of-theart methods. Our FMOcc with two frame input achieves notable scores
of 43.1% RayIoU and 39.8% mIoU on Occ3D-nuScenes validation, 42.6% RayIoU on
OpenOcc with 5.4 G inference memory and 330ms inference time.

</details>


### [53] [SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement](https://arxiv.org/abs/2507.02252)
*Zeyu Lei,Hongyuan Yu,Jinlin Wu,Zhen Chen*

Main category: cs.CV

TL;DR: This paper introduces SurgVisAgent, a unified surgical vision agent based on multimodal large language models, capable of dynamically addressing varied image distortions in surgical scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing enhancement algorithms are designed for single specific tasks, limiting their application in complex real-world surgeries.

Method: The authors proposed SurgVisAgent, which incorporates domain-specific knowledge, in-context few-shot learning, and chain-of-thought reasoning to enhance endoscopic images under diverse conditions.

Result: Extensive experiments on a benchmark simulating real-world surgical distortions demonstrate that SurgVisAgent outperforms traditional single-task models.

Conclusion: SurgVisAgent represents a promising unified approach to surgical assistance, offering enhanced flexibility and efficiency in real-world scenarios by addressing a wide range of image distortions dynamically.

Abstract: Precise surgical interventions are vital to patient safety, and advanced
enhancement algorithms have been developed to assist surgeons in
decision-making. Despite significant progress, these algorithms are typically
designed for single tasks in specific scenarios, limiting their effectiveness
in complex real-world situations. To address this limitation, we propose
SurgVisAgent, an end-to-end intelligent surgical vision agent built on
multimodal large language models (MLLMs). SurgVisAgent dynamically identifies
distortion categories and severity levels in endoscopic images, enabling it to
perform a variety of enhancement tasks such as low-light enhancement,
overexposure correction, motion blur elimination, and smoke removal.
Specifically, to achieve superior surgical scenario understanding, we design a
prior model that provides domain-specific knowledge. Additionally, through
in-context few-shot learning and chain-of-thought (CoT) reasoning, SurgVisAgent
delivers customized image enhancements tailored to a wide range of distortion
types and severity levels, thereby addressing the diverse requirements of
surgeons. Furthermore, we construct a comprehensive benchmark simulating
real-world surgical distortions, on which extensive experiments demonstrate
that SurgVisAgent surpasses traditional single-task models, highlighting its
potential as a unified solution for surgical assistance.

</details>


### [54] [Multi-Label Classification Framework for Hurricane Damage Assessment](https://arxiv.org/abs/2507.02265)
*Zhangding Liu,Neda Mohammadi,John E. Taylor*

Main category: cs.CV

TL;DR: This study introduces a multi-label classification framework for hurricane damage assessment using aerial imagery, achieving high accuracy compared to baseline methods.


<details>
  <summary>Details</summary>
Motivation: The need for accurate, timely, and comprehensive damage assessment after hurricanes, as traditional single-label classification techniques fail to capture complex damage scenarios.

Method: The framework employs ResNet for feature extraction and a class-specific attention mechanism to recognize multiple types of damage within a single aerial image, tested on the Rescuenet dataset from Hurricane Michael.

Result: The proposed methodology outperforms existing methods with a mean average precision of 90.23%, demonstrating its efficacy in identifying various damage types.

Conclusion: The study provides a robust tool for enhanced post-hurricane damage mapping, contributing to more effective disaster response and long-term resilience strategies.

Abstract: Hurricanes cause widespread destruction, resulting in diverse damage types
and severities that require timely and accurate assessment for effective
disaster response. While traditional single-label classification methods fall
short of capturing the complexity of post-hurricane damage, this study
introduces a novel multi-label classification framework for assessing damage
using aerial imagery. The proposed approach integrates a feature extraction
module based on ResNet and a class-specific attention mechanism to identify
multiple damage types within a single image. Using the Rescuenet dataset from
Hurricane Michael, the proposed method achieves a mean average precision of
90.23%, outperforming existing baseline methods. This framework enhances
post-hurricane damage assessment, enabling more targeted and efficient disaster
response and contributing to future strategies for disaster mitigation and
resilience. This paper has been accepted at the ASCE International Conference
on Computing in Civil Engineering (i3CE 2025), and the camera-ready version
will appear in the official conference proceedings.

</details>


### [55] [Cross-domain Hyperspectral Image Classification based on Bi-directional Domain Adaptation](https://arxiv.org/abs/2507.02268)
*Yuxiang Zhang,Wei Li,Wen Jia,Mengmeng Zhang,Ran Tao,Shunlin Liang*

Main category: cs.CV

TL;DR: The study proposes the Bi-directional Domain Adaptation (BiDA) framework for cross-domain hyperspectral image classification, focusing on addressing spectral shifts across regions or timeframes using a triple-branch transformer architecture.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the problem of spectral shifts in hyperspectral remote sensing images due to different regions or timeframes, which affects the accuracy of land cover classification.

Method: A Bi-directional Domain Adaptation (BiDA) framework is proposed using a triple-branch transformer architecture (source, target, and coupled branches) with a semantic tokenizer. It features Coupled Multi-head Cross-attention (CMCA) for inter-domain correlation and a bi-directional distillation loss to enhance adaptive space learning. Additionally, it introduces an Adaptive Reinforcement Strategy (ARS) for noise-resistant feature extraction.

Result: Experimental results demonstrate that BiDA outperforms state-of-the-art domain adaptation approaches, achieving 3-5% higher accuracy in cross-temporal tree species classification tasks on multiple airborne and satellite datasets.

Conclusion: The proposed BiDA framework significantly improves hyperspectral image classification across domains, effectively addressing spectral shifts. It demonstrates potential for wide applicability with its stronger adaptability and separability in various settings.

Abstract: Utilizing hyperspectral remote sensing technology enables the extraction of
fine-grained land cover classes. Typically, satellite or airborne images used
for training and testing are acquired from different regions or times, where
the same class has significant spectral shifts in different scenes. In this
paper, we propose a Bi-directional Domain Adaptation (BiDA) framework for
cross-domain hyperspectral image (HSI) classification, which focuses on
extracting both domain-invariant features and domain-specific information in
the independent adaptive space, thereby enhancing the adaptability and
separability to the target scene. In the proposed BiDA, a triple-branch
transformer architecture (the source branch, target branch, and coupled branch)
with semantic tokenizer is designed as the backbone. Specifically, the source
branch and target branch independently learn the adaptive space of source and
target domains, a Coupled Multi-head Cross-attention (CMCA) mechanism is
developed in coupled branch for feature interaction and inter-domain
correlation mining. Furthermore, a bi-directional distillation loss is designed
to guide adaptive space learning using inter-domain correlation. Finally, we
propose an Adaptive Reinforcement Strategy (ARS) to encourage the model to
focus on specific generalized feature extraction within both source and target
scenes in noise condition. Experimental results on cross-temporal/scene
airborne and satellite datasets demonstrate that the proposed BiDA performs
significantly better than some state-of-the-art domain adaptation approaches.
In the cross-temporal tree species classification task, the proposed BiDA is
more than 3\%$\sim$5\% higher than the most advanced method. The codes will be
available from the website:
https://github.com/YuxiangZhang-BIT/IEEE_TCSVT_BiDA.

</details>


### [56] [MAC-Lookup: Multi-Axis Conditional Lookup Model for Underwater Image Enhancement](https://arxiv.org/abs/2507.02270)
*Fanghai Yi,Zehong Zheng,Zexiao Liang,Yihang Dong,Xiyang Fang,Wangyu Wu,Xuhang Chen*

Main category: cs.CV

TL;DR: The paper introduces the MAC-Lookup model, which significantly improves the quality of underwater images suffering from distortions caused by various underwater conditions.


<details>
  <summary>Details</summary>
Motivation: Underwater images often suffer from visibility, color distortion, turbidity, and bubbles, yet many existing methods fail to address these issues adequately due to prior-based approaches or lack of high-quality datasets for deep learning.

Method: The authors propose the MAC-Lookup model, which includes Conditional 3D Lookup Table Color Correction (CLTCC) for initial corrections and Multi-Axis Adaptive Enhancement (MAAE) for detail refinement.

Result: Experimental results reveal that MAC-Lookup outperforms existing methods by efficiently restoring underwater image details and color accuracy without introducing over-enhancement or saturation.

Conclusion: MAC-Lookup offers a robust solution for underwater image enhancement by addressing unique underwater challenges while improving color, sharpness, and contrast. The model demonstrates superiority over prior approaches.

Abstract: Enhancing underwater images is crucial for exploration. These images face
visibility and color issues due to light changes, water turbidity, and bubbles.
Traditional prior-based methods and pixel-based methods often fail, while deep
learning lacks sufficient high-quality datasets. We introduce the Multi-Axis
Conditional Lookup (MAC-Lookup) model, which enhances visual quality by
improving color accuracy, sharpness, and contrast. It includes Conditional 3D
Lookup Table Color Correction (CLTCC) for preliminary color and quality
correction and Multi-Axis Adaptive Enhancement (MAAE) for detail refinement.
This model prevents over-enhancement and saturation while handling underwater
challenges. Extensive experiments show that MAC-Lookup excels in enhancing
underwater images by restoring details and colors better than existing methods.
The code is https://github.com/onlycatdoraemon/MAC-Lookup.

</details>


### [57] [Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation](https://arxiv.org/abs/2507.02271)
*Feizhen Huang,Yu Wu,Yutian Lin,Bo Du*

Main category: cs.CV

TL;DR: The paper introduces a self-distillation approach for Video-to-Audio generation to better handle scenarios with partial visual information, improving performance on the VGGSound dataset.


<details>
  <summary>Details</summary>
Motivation: Current Video-to-Audio generation methods struggle under scenarios where visual cues for Foley sounds are partially visible, due to their neglect of cinematic language.

Method: The paper uses a self-distillation approach where a student model learns to align video features through simulated cinematic language variations, enhancing audio-visual correspondence under partial visibility.

Result: The proposed method significantly improves performance across all metrics under scenarios of partial visibility, as well as on the large-scale VGGSound dataset.

Conclusion: The method successfully addresses limitations in Video-to-Audio generation models by factoring in cinematic language, offering impactful performance boosts in practical applications.

Abstract: Video-to-Audio (V2A) Generation achieves significant progress and plays a
crucial role in film and video post-production. However, current methods
overlook the cinematic language, a critical component of artistic expression in
filmmaking. As a result, their performance deteriorates in scenarios where
Foley targets are only partially visible. To address this challenge, we propose
a simple self-distillation approach to extend V2A models to cinematic language
scenarios. By simulating the cinematic language variations, the student model
learns to align the video features of training pairs with the same audio-visual
correspondences, enabling it to effectively capture the associations between
sounds and partial visual information. Our method not only achieves impressive
improvements under partial visibility across all evaluation metrics, but also
enhances performance on the large-scale V2A dataset, VGGSound.

</details>


### [58] [LaCo: Efficient Layer-wise Compression of Visual Tokens for Multimodal Large Language Models](https://arxiv.org/abs/2507.02279)
*Juntao Liu,Liqiang Niu,Wenchao Chen,Jie Zhou,Fandong Meng*

Main category: cs.CV

TL;DR: LaCo introduces intermediate-layer visual token compression to improve training efficiency and inference throughput while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Current compression techniques operate post-encoding, limiting efficiency improvement potential.

Method: LaCo uses a pixel-shuffle mechanism for token merging and a residual learning architecture with non-parametric shortcuts.

Result: LaCo achieves better token compression, improving training efficiency by over 20% and inference throughput by 15%.

Conclusion: LaCo effectively compresses visual tokens inside vision encoders, addressing limitations in existing methods and enhancing efficiency.

Abstract: Existing visual token compression methods for Multimodal Large Language
Models (MLLMs) predominantly operate as post-encoder modules, limiting their
potential for efficiency gains. To address this limitation, we propose LaCo
(Layer-wise Visual Token Compression), a novel framework that enables effective
token compression within the intermediate layers of the vision encoder. LaCo
introduces two core components: 1) a layer-wise pixel-shuffle mechanism that
systematically merges adjacent tokens through space-to-channel transformations,
and 2) a residual learning architecture with non-parametric shortcuts that
preserves critical visual information during compression. Extensive experiments
indicate that our LaCo outperforms all existing methods when compressing tokens
in the intermediate layers of the vision encoder, demonstrating superior
effectiveness. In addition, compared to external compression, our method
improves training efficiency beyond 20% and inference throughput over 15% while
maintaining strong performance.

</details>


### [59] [Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization](https://arxiv.org/abs/2507.02288)
*De Cheng,Zhipeng Xu,Xinyang Jiang,Dongsheng Li,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: The paper introduces a method for domain generalization by leveraging pre-trained Visual Foundation Models' text prompts to guide visual prompt tuning, incorporating advanced techniques for disentangling and aligning domain-invariant representations.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of disentangling invariant features across diverse domains in domain generalization tasks, especially leveraging the strengths of existing pre-trained Visual Foundation Models like CLIP.

Method: They propose using text feature-guided visual prompt tuning where text prompts are disentangled using a large language model, and introduce Worst Explicit Representation Alignment (WERA) to improve visual representation alignment by using augmented prompts and ensuring consistency.

Result: Experimental results on major DG datasets (PACS, VLCS, OfficeHome, DomainNet, and TerraInc) show that the proposed method outperforms state-of-the-art domain generalization approaches.

Conclusion: The proposed framework effectively enhances domain generalization capabilities by harmonizing textual and visual features and addressing the limitations of relying solely on language for feature disentanglement.

Abstract: Domain Generalization (DG) seeks to develop a versatile model capable of
performing effectively on unseen target domains. Notably, recent advances in
pre-trained Visual Foundation Models (VFMs), such as CLIP, have demonstrated
considerable potential in enhancing the generalization capabilities of deep
learning models. Despite the increasing attention toward VFM-based domain
prompt tuning within DG, the effective design of prompts capable of
disentangling invariant features across diverse domains remains a critical
challenge. In this paper, we propose addressing this challenge by leveraging
the controllable and flexible language prompt of the VFM. Noting that the text
modality of VFMs is naturally easier to disentangle, we introduce a novel
framework for text feature-guided visual prompt tuning. This framework first
automatically disentangles the text prompt using a large language model (LLM)
and then learns domain-invariant visual representation guided by the
disentangled text feature. However, relying solely on language to guide visual
feature disentanglement has limitations, as visual features can sometimes be
too complex or nuanced to be fully captured by descriptive text. To address
this, we introduce Worst Explicit Representation Alignment (WERA), which
extends text-guided visual prompts by incorporating an additional set of
abstract prompts. These prompts enhance source domain diversity through
stylized image augmentations, while alignment constraints ensure that visual
representations remain consistent across both the original and augmented
distributions. Experiments conducted on major DG datasets, including PACS,
VLCS, OfficeHome, DomainNet, and TerraInc, demonstrate that our proposed method
outperforms state-of-the-art DG methods.

</details>


### [60] [ViRefSAM: Visual Reference-Guided Segment Anything Model for Remote Sensing Segmentation](https://arxiv.org/abs/2507.02294)
*Hanbo Bi,Yulong Xu,Ya Li,Yongqiang Mao,Boyuan Tong,Chongyang Li,Chunbo Lang,Wenhui Diao,Hongqi Wang,Yingchao Feng,Xian Sun*

Main category: cs.CV

TL;DR: ViRefSAM enhances SAM, a general segmentation model, to effectively work with remote sensing (RS) images by using few-shot learning without manual prompts.


<details>
  <summary>Details</summary>
Motivation: Remote sensing image segmentation faces challenges like labor-intensive manual prompts and lack of adaptability to domain-specific semantics due to SAM's focus on natural images.

Method: ViRefSAM incorporates a Visual Contextual Prompt Encoder for extracting and generating class-specific prompts and a Dynamic Target Alignment Adapter to adapt SAM to RS domain characteristics.

Result: ViRefSAM achieves accurate segmentation of unseen RS classes using minimal reference images and outperforms other few-shot segmentation methods in diverse benchmarks.

Conclusion: ViRefSAM successfully adapts SAM for RS segmentation by leveraging class-specific references without manual prompts, showcasing its efficacy and practical advantages.

Abstract: The Segment Anything Model (SAM), with its prompt-driven paradigm, exhibits
strong generalization in generic segmentation tasks. However, applying SAM to
remote sensing (RS) images still faces two major challenges. First, manually
constructing precise prompts for each image (e.g., points or boxes) is
labor-intensive and inefficient, especially in RS scenarios with dense small
objects or spatially fragmented distributions. Second, SAM lacks domain
adaptability, as it is pre-trained primarily on natural images and struggles to
capture RS-specific semantics and spatial characteristics, especially when
segmenting novel or unseen classes. To address these issues, inspired by
few-shot learning, we propose ViRefSAM, a novel framework that guides SAM
utilizing only a few annotated reference images that contain class-specific
objects. Without requiring manual prompts, ViRefSAM enables automatic
segmentation of class-consistent objects across RS images. Specifically,
ViRefSAM introduces two key components while keeping SAM's original
architecture intact: (1) a Visual Contextual Prompt Encoder that extracts
class-specific semantic clues from reference images and generates object-aware
prompts via contextual interaction with target images; and (2) a Dynamic Target
Alignment Adapter, integrated into SAM's image encoder, which mitigates the
domain gap by injecting class-specific semantics into target image features,
enabling SAM to dynamically focus on task-relevant regions. Extensive
experiments on three few-shot segmentation benchmarks, including iSAID-5$^i$,
LoveDA-2$^i$, and COCO-20$^i$, demonstrate that ViRefSAM enables accurate and
automatic segmentation of unseen classes by leveraging only a few reference
images and consistently outperforms existing few-shot segmentation methods
across diverse datasets.

</details>


### [61] [DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation](https://arxiv.org/abs/2507.02299)
*Yunhan Yang,Shuo Chen,Yukun Huang,Xiaoyang Wu,Yuan-Chen Guo,Edmund Y. Lam,Hengshuang Zhao,Tong He,Xihui Liu*

Main category: cs.CV

TL;DR: The paper introduces DreamComposer++, a framework enhancing view-aware diffusion models to generate controllable novel views using multi-view conditions.


<details>
  <summary>Details</summary>
Motivation: Existing methods for generating novel views from a single image lack control due to insufficient multi-view information.

Method: DreamComposer++ employs a view-aware 3D lifting module to extract and aggregate 3D object representations from multiple views, incorporating them into pre-trained diffusion models.

Result: The framework seamlessly integrates with current diffusion models, improving their ability to synthesize controllable novel views and enabling robust 3D object reconstruction.

Conclusion: DreamComposer++ enhances the controllability and scalability of view-aware diffusion models, enabling practical applications in 3D reconstruction and beyond.

Abstract: Recent advancements in leveraging pre-trained 2D diffusion models achieve the
generation of high-quality novel views from a single in-the-wild image.
However, existing works face challenges in producing controllable novel views
due to the lack of information from multiple views. In this paper, we present
DreamComposer++, a flexible and scalable framework designed to improve current
view-aware diffusion models by incorporating multi-view conditions.
Specifically, DreamComposer++ utilizes a view-aware 3D lifting module to
extract 3D representations of an object from various views. These
representations are then aggregated and rendered into the latent features of
target view through the multi-view feature fusion module. Finally, the obtained
features of target view are integrated into pre-trained image or video
diffusion models for novel view synthesis. Experimental results demonstrate
that DreamComposer++ seamlessly integrates with cutting-edge view-aware
diffusion models and enhances their abilities to generate controllable novel
views from multi-view conditions. This advancement facilitates controllable 3D
object reconstruction and enables a wide range of applications.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [62] [SAKURAONE: Empowering Transparent and Open AI Platforms through Private-Sector HPC Investment in Japan](https://arxiv.org/abs/2507.02124)
*Fumikazu Konishi*

Main category: cs.DC

TL;DR: SAKURAONE, a managed HPC cluster by SAKURA Internet Research Center, ranks 49th globally in the ISC 2025 TOP500 list and highlights open networking technologies in HPC, achieving advanced benchmark results.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to showcase SAKURAONE's innovation in HPC systems by incorporating vendor-neutral, high-performance components, emphasizing open networking technologies for scalability and effectiveness in demanding workloads such as LLM training.

Method: SAKURAONE employs 100 compute nodes with NVIDIA H100 GPUs and an all-flash Lustre storage system. The Rail-Optimized topology with 800 GbE links and SONiC OS supports open, high-performance interconnects for data-intensive computing.

Result: The system sustained 33.95 PFLOP/s in HPL benchmarks, 396.295 TFLOP/s in HPCG benchmarks, and 339.86 PFLOP/s in HPL-MxP benchmarks using FP8 precision. It ranks as the 49th fastest HPC system globally and the only top-100 system using open network technology.

Conclusion: SAKURAONE demonstrates the feasibility and competitiveness of open and vendor-neutral technologies in large-scale HPC infrastructure while also addressing complex computing tasks efficiently for AI and HPC workloads.

Abstract: SAKURAONE is a managed high performance computing (HPC) cluster developed and
operated by the SAKURA Internet Research Center. It reinforces the ``KOKARYOKU
PHY'' configuration of bare-metal GPU servers and is designed as a cluster
computing resource optimized for advanced workloads, including large language
model (LLM) training.
  In the ISC 2025 edition of the TOP500 list, SAKURAONE was ranked
\textbf{49th} in the world based on its High Performance Linpack (HPL) score,
demonstrating its global competitiveness. In particular, it is the \textbf{only
system within the top 100} that employs a fully open networking stack based on
\textbf{800~GbE (Gigabit Ethernet)} and the \textbf{SONiC (Software for Open
Networking in the Cloud)} operating system, highlighting the viability of open
and vendor-neutral technologies in large-scale HPC infrastructure.
  SAKURAONE achieved a sustained performance of 33.95~PFLOP/s on the HPL
benchmark (Rmax), and 396.295~TFLOP/s on the High Performance Conjugate
Gradient (HPCG) benchmark. For the HPL-MxP benchmark, which targets
low-precision workloads representative of AI applications, SAKURAONE delivered
an impressive 339.86~PFLOP/s using FP8 precision.
  The system comprises 100 compute nodes, each equipped with eight NVIDIA H100
GPUs. It is supported by an all-flash Lustre storage subsystem with a total
physical capacity of 2~petabytes, providing high-throughput and low-latency
data access. Internode communication is enabled by a full-bisection bandwidth
interconnect based on a Rail-Optimized topology, where the Leaf and Spine
layers are interconnected via 800~GbE links. This topology, in combination with
RoCEv2 (RDMA over Converged Ethernet version 2), enables high-speed, lossless
data transfers and mitigates communication bottlenecks in large-scale parallel
workloads.

</details>


### [63] [Signalling Health for Improved Kubernetes Microservice Availability](https://arxiv.org/abs/2507.02158)
*Jacob Roberts,Blair Archibald,Phil Trinder*

Main category: cs.DC

TL;DR: The paper introduces Signal-based Container Monitoring (SCM) for Kubernetes and shows it outperforms traditional Poll-based Container Monitoring (PCM) methods in detecting container status changes faster and with fewer errors.


<details>
  <summary>Details</summary>
Motivation: To improve the detection and handling of container health changes in orchestration environments, addressing limitations of the traditional Poll-based Container Monitoring (PCM) approach.

Method: Designed, implemented, and evaluated a Signal-based Container Monitoring (SCM) system for Kubernetes. Conducted experiments using SockShop benchmark and utilized a mathematical model to compare SCM against PCM in terms of performance, speed, and accuracy.

Result: SCM detects container failures 86% faster than PCM, identifies container readiness in comparable time, and exhibits lower erroneous failure detections, improving service availability by 4%. It also avoids the need for tuning polling intervals.

Conclusion: SCM offers faster and more accurate container health monitoring over PCM without requiring complex tuning or introducing significant resource overheads. It is recommended that orchestrators adopt SCM for enhanced service availability.

Abstract: Microservices are often deployed and managed by a container orchestrator that
can detect and fix failures to maintain the service availability critical in
many applications. In Poll-based Container Monitoring (PCM), the orchestrator
periodically checks container health. While a common approach, PCM requires
careful tuning, may degrade service availability, and can be slow to detect
container health changes. An alternative is Signal-based Container Monitoring
(SCM), where the container signals the orchestrator when its status changes. We
present the design, implementation, and evaluation of an SCM approach for
Kubernetes and empirically show that it has benefits over PCM, as predicted by
a new mathematical model. We compare the service availability of SCM and PCM
over six experiments using the SockShop benchmark. SCM does not require that
polling intervals are tuned, and yet detects container failure 86\% faster than
PCM and container readiness in a comparable time with limited resource
overheads. We find PCM can erroneously detect failures, and this reduces
service availability by 4\%. We propose that orchestrators offer SCM features
for faster failure detection than PCM without erroneous detections or careful
tuning.

</details>


### [64] [Domain-Adversarial Transfer Learning for Fault Root Cause Identification in Cloud Computing Systems](https://arxiv.org/abs/2507.02233)
*Bruce Fang,Danyi Gao*

Main category: cs.DC

TL;DR: The paper proposes a transfer learning-based algorithm for fault root cause identification in cloud computing, addressing challenges like complex system structures and limited fault information. It outperforms existing methods under various realistic conditions.


<details>
  <summary>Details</summary>
Motivation: Fault root cause identification in cloud computing is challenging due to complex structures, dense service coupling, and scarce fault information. Existing methods struggle in such scenarios.

Method: The paper introduces a method based on transfer learning with shared feature extraction modules and domain adversarial mechanisms. It uses pseudo-label selection to enhance performance under limited labeled data.

Result: Experimental results demonstrate superior performance of the proposed method compared to mainstream approaches, excelling in accuracy, F1-Score, and AUC, even under extreme imbalance and structural differences.

Conclusion: The proposed method is effective and robust for fault root cause identification in complex cloud computing environments, showing practical value for real-world applications.

Abstract: This paper addresses the challenge of fault root cause identification in
cloud computing environments. The difficulty arises from complex system
structures, dense service coupling, and limited fault information. To solve
this problem, an intelligent identification algorithm based on transfer
learning is proposed. The method introduces a shared feature extraction module
and a domain adversarial mechanism to enable effective knowledge transfer from
the source domain to the target domain. This improves the model's
discriminative ability and generalization performance in the target domain. The
model incorporates a pseudo-label selection strategy. When labeled samples are
lacking in the target domain, high-confidence predictions are used in training.
This enhances the model's ability to recognize minority classes. To evaluate
the stability and adaptability of the method in real-world scenarios,
experiments are designed under three conditions: label scarcity, class
imbalance, and heterogeneous node environments. Experimental results show that
the proposed method outperforms existing mainstream approaches in several key
metrics, including accuracy, F1-Score, and AUC. The model demonstrates stronger
discriminative power and robustness. Notably, under extreme class imbalance and
significant structural differences in the target domain, the model still
maintains high performance. This validates the effectiveness and practical
value of the proposed mechanisms in complex cloud computing systems.

</details>


### [65] [Flotilla: A scalable, modular and resilient federated learning framework for heterogeneous resources](https://arxiv.org/abs/2507.02295)
*Roopkatha Banerjee,Prince Modi,Jinal Vyas,Chunduru Sri Abhijit,Tejus Chandrashekar,Harsha Varun Marisetty,Manik Gupta,Yogesh Simmhan*

Main category: cs.DC

TL;DR: This paper introduces Flotilla, a modular and scalable Federated Learning framework, designed for real-world edge deployments with support for synchronous and asynchronous FL strategies, fault tolerance, and efficient resource usage.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing FL frameworks, which focus on pseudo-distributed simulation rather than real-world edge deployment, lack support for asynchronous aggregation, and have limited fault tolerance.

Method: The authors designed Flotilla, a framework with modular architecture, stateless clients, a decoupled session state for servers, and periodic/incremental checkpointing. It supports various FL strategies and is hardware-agnostic.

Result: Flotilla was tested with five FL strategies and five DNN models, showing fault tolerance on systems with 200+ clients, rapid failover capabilities, and resource efficiency on edge devices. It also scaled efficiently for 1000+ clients compared to competitors.

Conclusion: Flotilla is a robust and efficient FL framework that supports a wide range of strategies, scales well, and is suitable for real-world deployments, making it a strong candidate for FL systems research and deployment.

Abstract: With the recent improvements in mobile and edge computing and rising concerns
of data privacy, Federated Learning(FL) has rapidly gained popularity as a
privacy-preserving, distributed machine learning methodology. Several FL
frameworks have been built for testing novel FL strategies. However, most focus
on validating the learning aspects of FL through pseudo-distributed simulation
but not for deploying on real edge hardware in a distributed manner to
meaningfully evaluate the federated aspects from a systems perspective. Current
frameworks are also inherently not designed to support asynchronous
aggregation, which is gaining popularity, and have limited resilience to client
and server failures. We introduce Flotilla, a scalable and lightweight FL
framework. It adopts a ``user-first'' modular design to help rapidly compose
various synchronous and asynchronous FL strategies while being agnostic to the
DNN architecture. It uses stateless clients and a server design that separates
out the session state, which are periodically or incrementally checkpointed. We
demonstrate the modularity of Flotilla by evaluating five different FL
strategies for training five DNN models. We also evaluate the client and
server-side fault tolerance on 200+ clients, and showcase its ability to
rapidly failover within seconds. Finally, we show that Flotilla's resource
usage on Raspberry Pis and Nvidia Jetson edge accelerators are comparable to or
better than three state-of-the-art FL frameworks, Flower, OpenFL and FedML. It
also scales significantly better compared to Flower for 1000+ clients. This
positions Flotilla as a competitive candidate to build novel FL strategies on,
compare them uniformly, rapidly deploy them, and perform systems research and
optimizations.

</details>


### [66] [Alps, a versatile research infrastructure](https://arxiv.org/abs/2507.02404)
*Maxime Martinasso,Mark Klein,Thomas C. Schulthess*

Main category: cs.DC

TL;DR: CSCS introduces Alps, a next-generation HPC infrastructure aimed at flexible and composable computing for diverse scientific needs.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of traditional HPC architectures, which fail to meet the increasingly diverse requirements of scientific computing.

Method: CSCS designed Alps with modularity and adaptability, employing a network-centric model where resources operate independently, and integrating vCluster technology to combine cloud and HPC paradigms.

Result: Alps supports diverse scientific workloads such as numerical weather prediction and AI research, demonstrating the adaptability of the system.

Conclusion: Alps provides a significant leap in flexibility and composability for high-performance computing by leveraging vCluster technology and heterogeneous hardware.

Abstract: The Swiss National Supercomputing Centre (CSCS) has a long-standing tradition
of delivering top-tier high-performance computing systems, exemplified by the
Piz Daint supercomputer. However, the increasing diversity of scientific needs
has exposed limitations in traditional vertically integrated HPC architectures,
which often lack flexibility and composability. To address these challenges,
CSCS developed Alps, a next-generation HPC infrastructure designed with a
transformative principle: resources operate as independent endpoints within a
high-speed network. This architecture enables the creation of independent
tenant-specific and platform-specific services, tailored to diverse scientific
requirements.
  Alps incorporates heterogeneous hardware, including CPUs and GPUs,
interconnected by a high-performance Slingshot network, and offers a modular
storage system. A key innovation is the versatile software-defined cluster
(vCluster) technology, which bridges cloud and HPC paradigms. By abstracting
infrastructure, service management, and user environments into distinct layers,
vClusters allow for customized platforms that support diverse workloads.
Current platforms on Alps serve various scientific domains, including numerical
weather prediction, and AI research.

</details>


### [67] [FlowSpec: Continuous Pipelined Speculative Decoding for Efficient Distributed LLM Inference](https://arxiv.org/abs/2507.02620)
*Xing Liu,Lizhuo Luo,Ming Tang,Chao Huang*

Main category: cs.DC

TL;DR: The paper introduces FlowSpec, a tree-based speculative decoding framework for distributed inference of large language models (LLMs) at the edge, achieving 1.36×-1.77× speedup over baselines.


<details>
  <summary>Details</summary>
Motivation: Current pipeline-based approaches for distributed inference at the edge struggle with low utilization when requests are sparse, leading to inefficiencies.

Method: FlowSpec leverages pipeline-parallel speculative decoding with mechanisms like score-based step-wise verification, efficient draft management, and dynamic draft expansion strategies to enhance decoding efficiency.

Result: Experimental evaluation demonstrated that FlowSpec outpaces existing baselines in inference speed across diverse setups, with speedups ranging from 1.36× to 1.77×.

Conclusion: FlowSpec represents a significant advancement in efficient distributed LLM inference, improving latency and throughput through innovative speculative decoding techniques.

Abstract: Distributed inference serves as a promising approach to enabling the
inference of large language models (LLMs) at the network edge. It distributes
the inference process to multiple devices to ensure that the LLMs can fit into
the device memory. Recent pipeline-based approaches have the potential to
parallelize communication and computation, which helps reduce inference
latency. However, the benefit diminishes when the inference request at the
network edge is sparse, where pipeline is typically at low utilization. To
enable efficient distributed LLM inference at the edge, we propose
\textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding
framework. FlowSpec incorporates three key mechanisms to improve decoding
efficiency: 1) score-based step-wise verification prioritizes more important
draft tokens to bring earlier accpeted tokens; 2) efficient draft management to
prune invalid tokens while maintaining correct causal relationship during
verification; 3) dynamic draft expansion strategies to supply high-quality
speculative inputs. These techniques work in concert to enhance both pipeline
utilization and speculative efficiency. We evaluate FlowSpec on a real-world
testbed with other baselines. Experimental results demonstrate that our
proposed framework significantly improves inference speed across diverse models
and configurations, achieving speedup ratios 1.36$\times$-1.77$\times$ compared
to baselines. Our code is publicly available at
\href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\#}

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [68] [Learnable-Differentiable Finite Volume Solver for Accelerated Simulation of Flows](https://arxiv.org/abs/2507.01975)
*Mengtao Yan,Qi Wang,Haining Wang,Ruizhi Chengze,Yi Zhang,Hongsheng Liu,Zidong Wang,Fan Yu,Qi Qi,Hao Sun*

Main category: cs.LG

TL;DR: The paper introduces LDSolver, a novel machine learning-based differentiable finite volume solver designed for efficient and accurate fluid flow simulations on coarse grids, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of classical numerical solvers for fluid simulations, which demand high computational resources, and machine learning approaches that struggle with interpretability, generalizability, and data dependency.

Method: The authors developed LDSolver, featuring a differentiable finite volume solver and a learnable module that approximates fluxes and corrects temporal errors on coarse grids for improved simulation performance.

Result: LDSolver demonstrated superior generalizability and high accuracy even with limited training data, as validated through experiments on various fluid flow systems, outperforming other baseline methods.

Conclusion: LDSolver offers an efficient, accurate, and scalable solution to fluid flow simulations on coarse grids, addressing key challenges such as computational cost and machine learning limitations.

Abstract: Simulation of fluid flows is crucial for modeling physical phenomena like
meteorology, aerodynamics, and biomedicine. Classical numerical solvers often
require fine spatiotemporal grids to satisfy stability, consistency, and
convergence conditions, leading to substantial computational costs. Although
machine learning has demonstrated better efficiency, they typically suffer from
issues of interpretability, generalizability, and data dependency. Hence, we
propose a learnable and differentiable finite volume solver, called LDSolver,
designed for efficient and accurate simulation of fluid flows on spatiotemporal
coarse grids. LDSolver comprises two key components: (1) a differentiable
finite volume solver, and (2) an learnable module providing equivalent
approximation for fluxes (derivatives and interpolations), and temporal error
correction on coarse grids. Even with limited training data (e.g., only a few
trajectories), our model could accelerate the simulation while maintaining a
high accuracy with superior generalizability. Experiments on different flow
systems (e.g., Burgers, decaying, forced and shear flows) show that LDSolver
achieves state-of-the-art performance, surpassing baseline models with notable
margins.

</details>


### [69] [DKGCM: A Spatio-Temporal Prediction Model for Traffic Flow by Fusing Spatial Node Clustering Method and Fourier Bidirectional Mamba Mechanism](https://arxiv.org/abs/2507.01982)
*Siqing Long,Xiangzhi Huang,Jiemin Xie,Ming Cai*

Main category: cs.LG

TL;DR: A novel graph convolutional network structure (DKGCM) is introduced for accurate traffic demand forecasting, leveraging techniques like Dynamic Time Warping, clustering, and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the performance limitations of traffic demand forecasting models caused by complex spatiotemporal relationships in traffic systems, enabling better resource allocation.

Method: The study introduces DK-GCN, combining Dynamic Time Warping and K-means clustering for spatial dependencies and integrates FFT with a bidirectional Mamba framework for temporal dependencies. GRPO reinforcement learning is used for optimizing training.

Result: Experiments show the proposed DKGCM model outperforms advanced methods, achieving strong results on three public datasets.

Conclusion: The DKGCM model improves accuracy in spatiotemporal traffic demand prediction, demonstrating effectiveness through robust evaluations.

Abstract: Accurate traffic demand forecasting enables transportation management
departments to allocate resources more effectively, thereby improving their
utilization efficiency. However, complex spatiotemporal relationships in
traffic systems continue to limit the performance of demand forecasting models.
To improve the accuracy of spatiotemporal traffic demand prediction, we propose
a new graph convolutional network structure called DKGCM. Specifically, we
first consider the spatial flow distribution of different traffic nodes and
propose a novel temporal similarity-based clustering graph convolution method,
DK-GCN. This method utilizes Dynamic Time Warping (DTW) and K-means clustering
to group traffic nodes and more effectively capture spatial dependencies. On
the temporal scale, we integrate the Fast Fourier Transform (FFT) within the
bidirectional Mamba deep learning framework to capture temporal dependencies in
traffic demand. To further optimize model training, we incorporate the GRPO
reinforcement learning strategy to enhance the loss function feedback
mechanism. Extensive experiments demonstrate that our model outperforms several
advanced methods and achieves strong results on three public datasets.

</details>


### [70] [Multimodal Misinformation Detection Using Early Fusion of Linguistic, Visual, and Social Features](https://arxiv.org/abs/2507.01984)
*Gautam Kishore Shahi*

Main category: cs.LG

TL;DR: The paper studies the effectiveness of multimodal approaches, incorporating text, image, and social features for misinformation detection, showing a 15% performance increase over unimodal models.


<details>
  <summary>Details</summary>
Motivation: To address the limited exploration of multimodal feature combinations in misinformation detection, especially during critical periods such as elections and crises.

Method: The researchers applied an early fusion approach to analyze 1,529 multimodal tweets during the COVID-19 pandemic and election periods, utilizing techniques like object detection, OCR, and data enrichment for additional insights.

Result: Combining supervised and unsupervised machine learning models led to a 15% performance boost over unimodal methods and a 5% improvement over bimodal models.

Conclusion: Multimodal approaches leveraging text, images, and social features enhance misinformation detection significantly while providing deeper insights into misinformation propagation patterns.

Abstract: Amid a tidal wave of misinformation flooding social media during elections
and crises, extensive research has been conducted on misinformation detection,
primarily focusing on text-based or image-based approaches. However, only a few
studies have explored multimodal feature combinations, such as integrating text
and images for building a classification model to detect misinformation. This
study investigates the effectiveness of different multimodal feature
combinations, incorporating text, images, and social features using an early
fusion approach for the classification model. This study analyzed 1,529 tweets
containing both text and images during the COVID-19 pandemic and election
periods collected from Twitter (now X). A data enrichment process was applied
to extract additional social features, as well as visual features, through
techniques such as object detection and optical character recognition (OCR).
The results show that combining unsupervised and supervised machine learning
models improves classification performance by 15% compared to unimodal models
and by 5% compared to bimodal models. Additionally, the study analyzes the
propagation patterns of misinformation based on the characteristics of
misinformation tweets and the users who disseminate them.

</details>


### [71] [Positive region preserved random sampling: an efficient feature selection method for massive data](https://arxiv.org/abs/2507.01998)
*Hexiang Bai,Deyu Li,Jiye Liang,Yanhui Zhai*

Main category: cs.LG

TL;DR: This paper introduces a new feature selection method using sampling techniques and rough set theory to efficiently handle massive datasets on limited computing resources.


<details>
  <summary>Details</summary>
Motivation: Feature selection is vital for intelligent machines optimizing success, but handling massive data with limited computing resources remains challenging.

Method: A new discriminatory measure is utilized for feature sets, coupled with constructing positive region preserved samples and finding a feature subset based on high discriminatory ability.

Result: Experiments showed that approximate reducts with high discriminatory ability were efficiently obtained on large datasets using personal computers.

Conclusion: The proposed method balances computational efficiency and discriminatory ability, making it practical for feature selection in massive datasets.

Abstract: Selecting relevant features is an important and necessary step for
intelligent machines to maximize their chances of success. However, intelligent
machines generally have no enough computing resources when faced with huge
volume of data. This paper develops a new method based on sampling techniques
and rough set theory to address the challenge of feature selection for massive
data. To this end, this paper proposes using the ratio of discernible object
pairs to all object pairs that should be distinguished to measure the
discriminatory ability of a feature set. Based on this measure, a new feature
selection method is proposed. This method constructs positive region preserved
samples from massive data to find a feature subset with high discriminatory
ability. Compared with other methods, the proposed method has two advantages.
First, it is able to select a feature subset that can preserve the
discriminatory ability of all the features of the target massive data set
within an acceptable time on a personal computer. Second, the lower boundary of
the probability of the object pairs that can be discerned using the feature
subset selected in all object pairs that should be distinguished can be
estimated before finding reducts. Furthermore, 11 data sets of different sizes
were used to validate the proposed method. The results show that approximate
reducts can be found in a very short period of time, and the discriminatory
ability of the final reduct is larger than the estimated lower boundary.
Experiments on four large-scale data sets also showed that an approximate
reduct with high discriminatory ability can be obtained in reasonable time on a
personal computer.

</details>


### [72] [Continuous Wavelet Transform and Siamese Network-Based Anomaly Detection in Multi-variate Semiconductor Process Time Series](https://arxiv.org/abs/2507.01999)
*Bappaditya Dey,Daniel Sorensen,Minjin Hwang,Sandip Halder*

Main category: cs.LG

TL;DR: This paper proposes a machine learning methodology to address challenges in semiconductor manufacturing by transforming multi-variate time-series (MTS) data into image-based representations for anomaly detection.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address key challenges in anomaly prediction within semiconductor manufacturing, such as high data dimensionality, class imbalance, and the non-stationary behavior of production systems.

Method: The methodology involves converting MTS data into Continuous Wavelet Transform (CWT) images, fine-tuning a pretrained VGG-16 model for image classification, and using a Siamese network to compare embeddings of image pairs for anomaly detection.

Result: The proposed method achieves high accuracy in identifying anomalies in real FAB process time-series datasets and demonstrates flexibility for application in supervised and semi-supervised settings.

Conclusion: The presented approach provides a generic and promising solution for anomaly detection in semiconductor fabrication, overcoming issues like complexity and delayed fault emergence.

Abstract: Semiconductor manufacturing is an extremely complex process, characterized by
thousands of interdependent parameters collected across diverse tools and
process steps. Multi-variate time-series (MTS) analysis has emerged as a
critical methodology for enabling real-time monitoring, fault detection, and
predictive maintenance in such environments. However, anomaly prediction in
semiconductor fabrication presents several critical challenges, including high
data dimensionality, severe class imbalance due to the rarity of true faults,
noisy and missing measurements, and non-stationary behavior of production
systems. Furthermore, the complex interdependencies between variables and the
delayed emergence of faults across downstream stages complicate both anomaly
detection and root-cause-analysis. This paper presents a novel and generic
approach for anomaly detection in MTS data using machine learning. The proposed
methodology consists of three main steps: a) converting MTS data into
image-based representations using the Continuous Wavelet Transform, b)
developing a multi-class image classifier by fine-tuning a pretrained VGG-16
architecture on custom CWT image datasets, and c) constructing a Siamese
network composed of two identical sub-networks, each utilizing the fine-tuned
VGG-16 as a backbone. The network takes pairs of CWT images as input -one
serving as a reference or anchor (representing a known-good signal), and the
other as a query (representing an unknown signal). The model then compares the
embeddings of both inputs to determine whether they belong to the same class at
a given time step. Our approach demonstrates high accuracy in identifying
anomalies on a real FAB process time-series dataset, offering a promising
solution for offline anomaly detection in process and tool trace data.
Moreover, the approach is flexible and can be applied in both supervised and
semi-supervised settings.

</details>


### [73] [Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames](https://arxiv.org/abs/2507.02001)
*Anurag Arnab,Ahmet Iscen,Mathilde Caron,Alireza Fathi,Cordelia Schmid*

Main category: cs.LG

TL;DR: The paper introduces Temporal Chain of Thought, an inference strategy for video question-answering, which improves model accuracy by selecting the most relevant frames from long videos.


<details>
  <summary>Details</summary>
Motivation: Long-video understanding in Vision-Language Models remains challenging due to ineffective use of long-context sequences and distractions from irrelevant frames.

Method: Temporal Chain of Thought identifies and extracts the most relevant frames iteratively using the VLM itself, optimizing context selection during inference.

Result: The method achieves state-of-the-art results across 4 video QA datasets, especially excelling on longer videos with improved performance using 32K context windows.

Conclusion: Leveraging enhanced computation and targeted context selection at inference leads to improved video QA accuracy, particularly for long videos, and demonstrates scalability of VLMs at inference-time.

Abstract: Despite recent advances in Vision-Language Models (VLMs), long-video
understanding remains a challenging problem. Although state-of-the-art
long-context VLMs can process around 1000 input frames, they still struggle to
effectively leverage this sequence length, and succumb to irrelevant
distractors within the context window. We present Temporal Chain of Thought, an
inference strategy for video question-answering that curates the model's input
context. We use the VLM itself to iteratively identify and extract the most
relevant frames from the video, which are then used for answering. We
demonstrate how leveraging more computation at inference-time to select the
most relevant context leads to improvements in accuracy, in agreement with
recent work on inference-time scaling of LLMs. Moreover, we achieve
state-of-the-art results on 4 diverse video question-answering datasets,
showing consistent improvements with 3 different VLMs. In particular, our
method shines on longer videos which would not otherwise fit within the model's
context window: On longer videos of more than 1 hour on LVBench, our approach
using a context window of 32K outperforms the same VLM using standard inference
with a 700K context window by 2.8 points.

</details>


### [74] [TFOC-Net: A Short-time Fourier Transform-based Deep Learning Approach for Enhancing Cross-Subject Motor Imagery Classification](https://arxiv.org/abs/2507.02510)
*Ahmed G. Habashi,Ahmed M. Azab,Seif Eldawlatly,Gamal M. Aly*

Main category: cs.LG

TL;DR: This paper focuses on improving cross-subject motor imagery (CS-MI) classification for brain-computer interfaces (BCIs) using optimized preprocessing, STFT-transformed EEG data, and a CNN, achieving notable accuracy improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of significant variability in EEG patterns across individuals, which hampers accuracy in calibration-free BCIs for real-world applications.

Method: The authors utilize an optimized preprocessing pipeline that includes Short-Time Fourier Transform (STFT) with parameter tuning and a balanced batching training strategy for a convolutional neural network (CNN).

Result: Their method achieved classification accuracy of 67.60%, 65.96%, and 80.22% on three benchmark BCI datasets, significantly outperforming state-of-the-art techniques.

Conclusion: This study sets a new benchmark for generalizable, calibration-free motor imagery classification, offering a pathway for developing BCIs suitable for real-world use while contributing an open-access dataset for further research.

Abstract: Cross-subject motor imagery (CS-MI) classification in brain-computer
interfaces (BCIs) is a challenging task due to the significant variability in
Electroencephalography (EEG) patterns across different individuals. This
variability often results in lower classification accuracy compared to
subject-specific models, presenting a major barrier to developing
calibration-free BCIs suitable for real-world applications. In this paper, we
introduce a novel approach that significantly enhances cross-subject MI
classification performance through optimized preprocessing and deep learning
techniques. Our approach involves direct classification of Short-Time Fourier
Transform (STFT)-transformed EEG data, optimized STFT parameters, and a
balanced batching strategy during training of a Convolutional Neural Network
(CNN). This approach is uniquely validated across four different datasets,
including three widely-used benchmark datasets leading to substantial
improvements in cross-subject classification, achieving 67.60% on the BCI
Competition IV Dataset 1 (IV-1), 65.96% on Dataset 2A (IV-2A), and 80.22% on
Dataset 2B (IV-2B), outperforming state-of-the-art techniques. Additionally, we
systematically investigate the classification performance using MI windows
ranging from the full 4-second window to 1-second windows. These results
establish a new benchmark for generalizable, calibration-free MI classification
in addition to contributing a robust open-access dataset to advance research in
this domain.

</details>


### [75] [AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design](https://arxiv.org/abs/2507.02006)
*Shakya Jayakody,Youpeng Zhao,Jun Wang*

Main category: cs.LG

TL;DR: This paper presents AIRES, an algorithm-system co-designed solution for faster out-of-core sparse matrix-matrix multiplication (SpGEMM) in graph convolutional networks (GCNs), addressing issues in data alignment and memory bottlenecks.


<details>
  <summary>Details</summary>
Motivation: Graph convolutional networks demand efficient handling of sparse matrix operations due to increasing graph data size, but traditional methods suffer from memory limitation and GPU under-utilization.

Method: AIRES introduces row block-wise alignment for sparse format optimization, along with a three-phase dynamic scheduling system that combines GPU memory, GPU Direct Storage, and host memory for improved throughput.

Result: AIRES achieves up to 1.8x lower latency compared to state-of-the-art methods in graph processing benchmarks.

Conclusion: AIRES effectively accelerates out-of-core SpGEMM for GCNs by addressing critical bottlenecks like data alignment issues and memory allocation challenges.

Abstract: Graph convolutional networks (GCNs) are fundamental in various scientific
applications, ranging from biomedical protein-protein interactions (PPI) to
large-scale recommendation systems. An essential component for modeling graph
structures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As
the size of graph data continues to scale up, SpGEMMs are often conducted in an
out-of-core fashion due to limited GPU memory space in resource-constrained
systems. Albeit recent efforts that aim to alleviate the memory constraints of
out-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory
layout, or performing the computation in sparse format, current systems suffer
from both high I/O latency and GPU under-utilization issues.
  In this paper, we first identify the problems of existing systems, where
sparse format data alignment and memory allocation are the main performance
bottlenecks, and propose AIRES, a novel algorithm-system co-design solution to
accelerate out-of-core SpGEMM computation for GCNs. Specifically, from the
algorithm angle, AIRES proposes to alleviate the data alignment issues on the
block level for matrices in sparse formats and develops a tiling algorithm to
facilitate row block-wise alignment. On the system level, AIRES employs a
three-phase dynamic scheduling that features a dual-way data transfer strategy
utilizing a tiered memory system: integrating GPU memory, GPU Direct Storage
(GDS), and host memory to reduce I/O latency and improve throughput.
Evaluations show that AIRES significantly outperforms the state-of-the-art
methods, achieving up to 1.8x lower latency in real-world graph processing
benchmarks.

</details>


### [76] [GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters](https://arxiv.org/abs/2507.02085)
*Wanjia Zhao,Jiaqi Han,Siyi Gu,Mingjian Jiang,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: This paper introduces GeoAda, an SE(3)-equivariant adapter framework that facilitates efficient fine-tuning of geometric diffusion models for controlled generative tasks, ensuring performance consistency and minimal overfitting.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of efficient fine-tuning frameworks for geometric diffusion models to tackle downstream tasks with diverse geometric controls, while preserving model consistency and mitigating overfitting.

Method: GeoAda is developed as an SE(3)-equivariant adapter framework that uses coupling operators, trainable copies of existing layers, and decoupling operators to encode and project control signals while ensuring geometric consistency. Fine-tuning is achieved by updating only these adapter components.

Result: GeoAda demonstrates state-of-the-art fine-tuning performance across diverse geometric controls and application domains such as molecular dynamics and human motion prediction. It also maintains the pretrained model's accuracy, avoiding overfitting and catastrophic forgetting seen in baseline approaches.

Conclusion: GeoAda presents a structured, parameter-efficient method for fine-tuning geometric diffusion models, proving its effectiveness across various tasks and controls by leveraging SE(3)-equivariant properties.

Abstract: Geometric diffusion models have shown remarkable success in molecular
dynamics and structure generation. However, efficiently fine-tuning them for
downstream tasks with varying geometric controls remains underexplored. In this
work, we propose an SE(3)-equivariant adapter framework ( GeoAda) that enables
flexible and parameter-efficient fine-tuning for controlled generative tasks
without modifying the original model architecture. GeoAda introduces a
structured adapter design: control signals are first encoded through coupling
operators, then processed by a trainable copy of selected pretrained model
layers, and finally projected back via decoupling operators followed by an
equivariant zero-initialized convolution. By fine-tuning only these lightweight
adapter modules, GeoAda preserves the model's geometric consistency while
mitigating overfitting and catastrophic forgetting. We theoretically prove that
the proposed adapters maintain SE(3)-equivariance, ensuring that the geometric
inductive biases of the pretrained diffusion model remain intact during
adaptation. We demonstrate the wide applicability of GeoAda across diverse
geometric control types, including frame control, global control, subgraph
control, and a broad range of application domains such as particle dynamics,
molecular dynamics, human motion prediction, and molecule generation. Empirical
results show that GeoAda achieves state-of-the-art fine-tuning performance
while preserving original task accuracy, whereas other baselines experience
significant performance degradation due to overfitting and catastrophic
forgetting.

</details>


### [77] [Sample Complexity Bounds for Linear Constrained MDPs with a Generative Model](https://arxiv.org/abs/2507.02089)
*Xingtu Liu,Lin F. Yang,Sharan Vaswani*

Main category: cs.LG

TL;DR: This paper develops a primal-dual framework for solving constrained Markov Decision Processes (CMDPs) using black-box MDP solvers, providing sample complexity results for relaxed and strict feasibility cases.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to address the challenge of efficiently solving CMDPs, where constraints on cumulative rewards must be satisfied, by leveraging generative models and unconstrained MDP solvers.

Method: The proposed method uses a primal-dual framework integrated with mirror descent value iteration (MDVI) to solve CMDPs, providing sample complexity guarantees for relaxed and strict feasibility cases.

Result: The algorithm achieves sample complexity of $\tilde{O}(\frac{d^2}{(1-\gamma)^4\epsilon^2})$ for relaxed feasibility and $\tilde{O}(\frac{d^2}{(1-\gamma)^6\epsilon^2\zeta^2})$ for strict feasibility, with near-optimal dependence on key problem parameters.

Conclusion: The paper concludes that the proposed framework can effectively solve CMDPs with rigorous sample complexity bounds and recover near-optimal results in tabular settings.

Abstract: We consider infinite-horizon $\gamma$-discounted (linear) constrained Markov
decision processes (CMDPs) where the objective is to find a policy that
maximizes the expected cumulative reward subject to expected cumulative
constraints. Given access to a generative model, we propose to solve CMDPs with
a primal-dual framework that can leverage any black-box unconstrained MDP
solver. For linear CMDPs with feature dimension $d$, we instantiate the
framework by using mirror descent value iteration
(\texttt{MDVI})~\citep{kitamura2023regularization} an example MDP solver. We
provide sample complexity bounds for the resulting CMDP algorithm in two cases:
(i) relaxed feasibility, where small constraint violations are allowed, and
(ii) strict feasibility, where the output policy is required to exactly satisfy
the constraint. For (i), we prove that the algorithm can return an
$\epsilon$-optimal policy with high probability by using
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^4\epsilon^2}\right)$ samples. We note
that these results exhibit a near-optimal dependence on both $d$ and
$\epsilon$. For (ii), we show that the algorithm requires
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^6\epsilon^2\zeta^2}\right)$ samples,
where $\zeta$ is the problem-dependent Slater constant that characterizes the
size of the feasible region. Finally, we instantiate our framework for tabular
CMDPs and show that it can be used to recover near-optimal sample complexities
in this setting.

</details>


### [78] [Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions](https://arxiv.org/abs/2507.02087)
*Eitan Anzenberg,Arunava Samajpati,Sivasankaran Chandrasekar,Varun Kacholia*

Main category: cs.LG

TL;DR: The study benchmarks general-purpose LLMs against a custom hiring model, showing the latter significantly outperforms in both accuracy and fairness for job candidate matching.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of accuracy and fairness in using LLMs for hiring, considering the risks of bias in high-stakes AI applications.

Method: The paper evaluates several LLMs and a proprietary hiring model using metrics like ROC AUC, F1-score, and fairness measures such as impact ratio across demographics.

Result: The proprietary Match Score model outperformed general-purpose LLMs in accuracy (ROC AUC 0.85 vs. 0.77) and fairness (near-parity impact ratios, better than LLMs).

Conclusion: Domain-specific AI models tailored for hiring can achieve both high accuracy and fairness, whereas general-purpose LLMs require extensive safeguards to mitigate bias.

Abstract: The use of large language models (LLMs) in hiring promises to streamline
candidate screening, but it also raises serious concerns regarding accuracy and
algorithmic bias where sufficient safeguards are not in place. In this work, we
benchmark several state-of-the-art foundational LLMs - including models from
OpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our
proprietary domain-specific hiring model (Match Score) for job candidate
matching. We evaluate each model's predictive accuracy (ROC AUC,
Precision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis
across declared gender, race, and intersectional subgroups). Our experiments on
a dataset of roughly 10,000 real-world recent candidate-job pairs show that
Match Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs
0.77) and achieves significantly more equitable outcomes across demographic
groups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957
(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the
intersectionals, respectively). We discuss why pretraining biases may cause
LLMs with insufficient safeguards to propagate societal biases in hiring
scenarios, whereas a bespoke supervised model can more effectively mitigate
these biases. Our findings highlight the importance of domain-specific modeling
and bias auditing when deploying AI in high-stakes domains such as hiring, and
caution against relying on off-the-shelf LLMs for such tasks without extensive
fairness safeguards. Furthermore, we show with empirical evidence that there
shouldn't be a dichotomy between choosing accuracy and fairness in hiring: a
well-designed algorithm can achieve both accuracy in hiring and fairness in
outcomes.

</details>


### [79] [Improving Constrained Generation in Language Models via Self-Distilled Twisted Sequential Monte Carlo](https://arxiv.org/abs/2507.02315)
*Sooyeon Kim,Giung Nam,Juho Lee*

Main category: cs.LG

TL;DR: The paper revisits constrained text generation using autoregressive language models and improves generation quality through iterative self-distillation.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in constrained text generation, particularly when the desired outputs (target distribution) are sparse or unlikely under the original language model, which hinders effective learning.

Method: The authors propose an iterative self-distillation process to refine the base model, progressively aligning it with the target distribution for achieving better outcomes.

Result: The iterative refinement approach results in marked improvements in generating high-quality constrained text outputs.

Conclusion: By leveraging self-distillation, the study demonstrates that it is possible to overcome sparse reward challenges and significantly enhance the performance of constrained text generation models.

Abstract: Recent work has framed constrained text generation with autoregressive
language models as a probabilistic inference problem. Among these, Zhao et al.
(2024) introduced a promising approach based on twisted Sequential Monte Carlo,
which incorporates learned twist functions and twist-induced proposals to guide
the generation process. However, in constrained generation settings where the
target distribution concentrates on outputs that are unlikely under the base
model, learning becomes challenging due to sparse and uninformative reward
signals. We show that iteratively refining the base model through
self-distillation alleviates this issue by making the model progressively more
aligned with the target, leading to substantial gains in generation quality.

</details>


### [80] [Energy-Based Transformers are Scalable Learners and Thinkers](https://arxiv.org/abs/2507.02092)
*Alexi Gladstone,Ganesh Nanduru,Md Mofijul Islam,Peixuan Han,Hyeonjeong Ha,Aman Chadha,Yilun Du,Heng Ji,Jundong Li,Tariq Iqbal*

Main category: cs.LG

TL;DR: This paper proposes Energy-Based Transformers (EBTs) as a general framework to improve model performance using System 2 Thinking, suitable for both text and image tasks, showing superior efficiency and generalization.


<details>
  <summary>Details</summary>
Motivation: Current inference-time System 2 Thinking approaches are limited by being modality-specific, problem-specific, or requiring additional supervision. The authors aim to generalize System 2 Thinking to develop unsupervised models capable of improved decision-making.

Method: The authors introduce Energy-Based Transformers (EBTs), which assign energy values to input-prediction pairs and use gradient descent-based energy minimization for predictions. EBTs enable better performance across discrete and continuous modalities.

Result: EBTs achieve a 35% higher scaling rate than Transformer++ models and improve System 2 Thinking performance by 29% during inference on language tasks. They outperform diffusion-based models on image denoising and generalize better on downstream tasks, even under worse pretraining conditions.

Conclusion: Energy-Based Transformers (EBTs) provide a promising and scalable solution for enhancing both learning and thinking capabilities across diverse modalities, offering a generalizable approach for unsupervised advanced reasoning.

Abstract: Inference-time computation techniques, analogous to human System 2 Thinking,
have recently become popular for improving model performances. However, most
existing approaches suffer from several limitations: they are modality-specific
(e.g., working only in text), problem-specific (e.g., verifiable domains like
math and coding), or require additional supervision/training on top of
unsupervised pretraining (e.g., verifiers or verifiable rewards). In this
paper, we ask the question "Is it possible to generalize these System 2
Thinking approaches, and develop models that learn to think solely from
unsupervised learning?" Interestingly, we find the answer is yes, by learning
to explicitly verify the compatibility between inputs and
candidate-predictions, and then re-framing prediction problems as optimization
with respect to this verifier. Specifically, we train Energy-Based Transformers
(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy
value to every input and candidate-prediction pair, enabling predictions
through gradient descent-based energy minimization until convergence. Across
both discrete (text) and continuous (visual) modalities, we find EBTs scale
faster than the dominant Transformer++ approach during training, achieving an
up to 35% higher scaling rate with respect to data, batch size, parameters,
FLOPs, and depth. During inference, EBTs improve performance with System 2
Thinking by 29% more than the Transformer++ on language tasks, and EBTs
outperform Diffusion Transformers on image denoising while using fewer forward
passes. Further, we find that EBTs achieve better results than existing models
on most downstream tasks given the same or worse pretraining performance,
suggesting that EBTs generalize better than existing approaches. Consequently,
EBTs are a promising new paradigm for scaling both the learning and thinking
capabilities of models.

</details>


### [81] [Online Conformal Prediction with Efficiency Guarantees](https://arxiv.org/abs/2507.02496)
*Vaidehi Srinivas*

Main category: cs.LG

TL;DR: The paper investigates conformal prediction in an online setting, focusing on achieving coverage while optimizing efficiency in terms of interval volume. It highlights differences between exchangeable and arbitrary sequences in achieving optimal performance, with a matching deterministic algorithm provided.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the need for efficient confidence intervals in online and sequential settings, balancing coverage accuracy (maintaining miscoverage rates) and efficiency (minimizing interval lengths).

Method: The method involves studying the problem for two settings: exchangeable and arbitrary sequences. They propose algorithms that achieve coverage targets and optimize interval lengths, with a focus on Pareto-optimal tradeoffs and deterministic algorithms robust to adversarial conditions.

Result: For exchangeable sequences, intervals can achieve near-perfect coverage efficiency, while for arbitrary sequences, achieving optimal efficiency leads to more coverage mistakes. A matching algorithm is introduced that balances efficiency and mistakes, achieving Pareto-optimal settings.

Conclusion: There is a fundamental difference between performance in exchangeable and arbitrary input sequences for this online conformal prediction problem. The paper provides theoretical guarantees and a deterministic algorithm for optimal tradeoffs between efficiency and coverage.

Abstract: We study the problem of conformal prediction in a novel online framework that
directly optimizes efficiency. In our problem, we are given a target
miscoverage rate $\alpha > 0$, and a time horizon $T$. On each day $t \le T$ an
algorithm must output an interval $I_t \subseteq [0, 1]$, then a point $y_t \in
[0, 1]$ is revealed. The goal of the algorithm is to achieve coverage, that is,
$y_t \in I_t$ on (close to) a $(1 - \alpha)$-fraction of days, while
maintaining efficiency, that is, minimizing the average volume (length) of the
intervals played. This problem is an online analogue to the problem of
constructing efficient confidence intervals.
  We study this problem over arbitrary and exchangeable (random order) input
sequences. For exchangeable sequences, we show that it is possible to construct
intervals that achieve coverage $(1 - \alpha) - o(1)$, while having length
upper bounded by the best fixed interval that achieves coverage in hindsight.
For arbitrary sequences however, we show that any algorithm that achieves a
$\mu$-approximation in average length compared to the best fixed interval
achieving coverage in hindsight, must make a multiplicative factor more
mistakes than $\alpha T$, where the multiplicative factor depends on $\mu$ and
the aspect ratio of the problem. Our main algorithmic result is a matching
algorithm that can recover all Pareto-optimal settings of $\mu$ and number of
mistakes. Furthermore, our algorithm is deterministic and therefore robust to
an adaptive adversary.
  This gap between the exchangeable and arbitrary settings is in contrast to
the classical online learning problem. In fact, we show that no single
algorithm can simultaneously be Pareto-optimal for arbitrary sequences and
optimal for exchangeable sequences. On the algorithmic side, we give an
algorithm that achieves the near-optimal tradeoff between the two cases.

</details>


### [82] [Parametric Neural Amp Modeling with Active Learning](https://arxiv.org/abs/2507.02109)
*Florian Grötschla,Luca A. Lanzendörfer,Longxiang Jiao,Roger Wattenhofer*

Main category: cs.LG

TL;DR: This paper presents PANAMA, an active learning framework to efficiently train end-to-end parametric guitar amp models using minimal data.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of developing accurate guitar amp models using fewer data samples for training.

Method: PANAMA employs a WaveNet-like architecture and utilizes gradient-based optimization for active learning, selecting the most effective data points for training.

Result: The framework shows that active learning with gradient-based optimization can produce effective models with a constrained number of samples.

Conclusion: Active learning strategies like PANAMA can efficiently create virtual guitar amps, reducing the data samples needed for high-quality modeling.

Abstract: We introduce PANAMA, an active learning framework for the training of
end-to-end parametric guitar amp models using a WaveNet-like architecture. With
\model, one can create a virtual amp by recording samples that are determined
by an active learning strategy to use a minimum amount of datapoints (i.e., amp
knob settings). We show that gradient-based optimization algorithms can be used
to determine the optimal datapoints to sample, and that the approach helps
under a constrained number of samples.

</details>


### [83] [Scaling Collapse Reveals Universal Dynamics in Compute-Optimally Trained Neural Networks](https://arxiv.org/abs/2507.02119)
*Shikai Qiu,Lechao Xiao,Andrew Gordon Wilson,Jeffrey Pennington,Atish Agarwala*

Main category: cs.LG

TL;DR: The paper introduces a universal scaling law for training neural networks, showing how training loss curves collapse into a single universal curve when training compute and loss are properly normalized.


<details>
  <summary>Details</summary>
Motivation: To understand and characterize the universal scaling behavior of neural network training dynamics as model size and training time increase.

Method: The authors demonstrate loss curve collapse by normalizing training compute and loss, derive insights from scaling laws, and utilize an analytical model of SGD noise dynamics to explain their observations.

Result: They find that training loss curves from models of different sizes exhibit tight collapse (termed supercollapse) under optimal learning rate schedules, across datasets and architectures, but fail under poor hyperparameter scaling.

Conclusion: The findings provide a practical test for hyperparameter optimization and deepen theoretical understanding of training dynamics using scaling laws and noise dynamics.

Abstract: What scaling limits govern neural network training dynamics when model size
and training time grow in tandem? We show that despite the complex interactions
between architecture, training algorithms, and data, compute-optimally trained
models exhibit a remarkably precise universality. Specifically, loss curves
from models of varying sizes collapse onto a single universal curve when
training compute and loss are normalized to unity at the end of training. With
learning rate decay, the collapse becomes so tight that differences in the
normalized curves across models fall below the noise floor of individual loss
curves across random seeds, a phenomenon we term supercollapse. We observe
supercollapse across learning rate schedules, datasets, and architectures,
including transformers trained on next-token prediction, and find it breaks
down when hyperparameters are scaled suboptimally, providing a precise and
practical indicator of good scaling. We explain these phenomena by connecting
collapse to the power-law structure in typical neural scaling laws, and
analyzing a simple yet surprisingly effective model of SGD noise dynamics that
accurately predicts loss curves across various learning rate schedules and
quantitatively explains the origin of supercollapse.

</details>


### [84] [Classification by Separating Hypersurfaces: An Entropic Approach](https://arxiv.org/abs/2507.02732)
*Argimiro Arratia,Mahmoud El Daou,Henryk Gzyl*

Main category: cs.LG

TL;DR: The paper introduces a new entropy-based method for finding classification hyperplanes in high-dimensional space, extending to polynomial decision boundaries as an alternative to traditional methods like SVMs.


<details>
  <summary>Details</summary>
Motivation: To improve upon linear and quadratic optimization techniques for data classification, providing a more robust and versatile framework.

Method: Developing an entropy-based optimization over bounded hypercubes in N-dimensional space, allowing for both linear and polynomial decision surfaces.

Result: The proposed approach efficiently handles diverse classification problems, including both linear and non-linear separable datasets.

Conclusion: The method presents a reliable alternative to existing optimization techniques with strong performance in handling a variety of classification tasks.

Abstract: We consider the following classification problem: Given a population of
individuals characterized by a set of attributes represented as a vector in
${\mathbb R}^N$, the goal is to find a hyperplane in ${\mathbb R}^N$ that
separates two sets of points corresponding to two distinct classes. This
problem, with a history dating back to the perceptron model, remains central to
machine learning. In this paper we propose a novel approach by searching for a
vector of parameters in a bounded $N$-dimensional hypercube centered at the
origin and a positive vector in ${\mathbb R}^M$, obtained through the
minimization of an entropy-based function defined over the space of unknown
variables. The method extends to polynomial surfaces, allowing the separation
of data points by more complex decision boundaries. This provides a robust
alternative to traditional linear or quadratic optimization techniques, such as
support vector machines and gradient descent. Numerical experiments demonstrate
the efficiency and versatility of the method in handling diverse classification
tasks, including linear and non-linear separability.

</details>


### [85] [CROP: Circuit Retrieval and Optimization with Parameter Guidance using LLMs](https://arxiv.org/abs/2507.02128)
*Jingyu Pan,Isaac Jacobson,Zheng Zhao,Tung-Chieh Chen,Guanglei Zhou,Chen-Chia Chang,Vineet Rashingkar,Yiran Chen*

Main category: cs.LG

TL;DR: CROP is an LLM-powered framework for automating VLSI design flow tuning, showcasing improved performance (e.g., 9.9% reduction in power consumption).


<details>
  <summary>Details</summary>
Motivation: To address the challenges of optimizing chip design in EDA due to large parameter spaces and limitations of manual tuning.

Method: CROP uses dense vector representations, embedding-based design retrieval, and an LLM-guided parameter search constrained by prior design knowledge.

Result: Experimental results show superior quality-of-results with fewer iterations, including significant power consumption improvement.

Conclusion: CROP demonstrates the capability of LLMs in optimizing VLSI designs efficiently, surpassing traditional manual and automated methods.

Abstract: Modern very large-scale integration (VLSI) design requires the implementation
of integrated circuits using electronic design automation (EDA) tools. Due to
the complexity of EDA algorithms, the vast parameter space poses a huge
challenge to chip design optimization, as the combination of even moderate
numbers of parameters creates an enormous solution space to explore. Manual
parameter selection remains industrial practice despite being excessively
laborious and limited by expert experience. To address this issue, we present
CROP, the first large language model (LLM)-powered automatic VLSI design flow
tuning framework. Our approach includes: (1) a scalable methodology for
transforming RTL source code into dense vector representations, (2) an
embedding-based retrieval system for matching designs with semantically similar
circuits, and (3) a retrieval-augmented generation (RAG)-enhanced LLM-guided
parameter search system that constrains the search process with prior knowledge
from similar designs. Experiment results demonstrate CROP's ability to achieve
superior quality-of-results (QoR) with fewer iterations than existing
approaches on industrial designs, including a 9.9% reduction in power
consumption.

</details>


### [86] [Contextual Online Pricing with (Biased) Offline Data](https://arxiv.org/abs/2507.02762)
*Yixuan Zhang,Ruihao Zhu,Qiaomin Xie*

Main category: cs.LG

TL;DR: The paper addresses online pricing using biased offline data and provides statistical complexity and regret bounds based on key parameters. Introducing Optimism-in-the-Face-of-Uncertainty (OFU) policies achieves minimax-optimal regret.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of contextual online pricing utilizing biased offline data and design strategies achieving optimal performance while handling this bias.

Method: Optimism-in-the-Face-of-Uncertainty (OFU) policies were designed and analyzed to achieve tight regret bounds in pricing models. Extensions to robust algorithms capable of handling unknown bias bounds are also constructed.

Result: The paper provides tight regret guarantees for contextual pricing in biased offline data settings and demonstrates similar results for stochastic linear bandits with biased data.

Conclusion: Optimism-in-the-Face-of-Uncertainty (OFU) policies and its robust variants deliver optimal strategies for contextual pricing, overcoming biases in offline data effectively.

Abstract: We study contextual online pricing with biased offline data. For the scalar
price elasticity case, we identify the instance-dependent quantity $\delta^2$
that measures how far the offline data lies from the (unknown) online optimum.
We show that the time length $T$, bias bound $V$, size $N$ and dispersion
$\lambda_{\min}(\hat{\Sigma})$ of the offline data, and $\delta^2$ jointly
determine the statistical complexity. An Optimism-in-the-Face-of-Uncertainty
(OFU) policy achieves a minimax-optimal, instance-dependent regret bound
$\tilde{\mathcal{O}}\big(d\sqrt{T} \wedge (V^2T +
\frac{dT}{\lambda_{\min}(\hat{\Sigma}) + (N \wedge T) \delta^2})\big)$. For
general price elasticity, we establish a worst-case, minimax-optimal rate
$\tilde{\mathcal{O}}\big(d\sqrt{T} \wedge (V^2T + \frac{dT
}{\lambda_{\min}(\hat{\Sigma})})\big)$ and provide a generalized OFU algorithm
that attains it. When the bias bound $V$ is unknown, we design a robust variant
that always guarantees sub-linear regret and strictly improves on purely online
methods whenever the exact bias is small. These results deliver the first tight
regret guarantees for contextual pricing in the presence of biased offline
data. Our techniques also transfer verbatim to stochastic linear bandits with
biased offline data, yielding analogous bounds.

</details>


### [87] [Generative Latent Diffusion for Efficient Spatiotemporal Data Reduction](https://arxiv.org/abs/2507.02129)
*Xiao Li,Liangji Zhu,Anand Rangarajan,Sanjay Ranka*

Main category: cs.LG

TL;DR: The paper introduces a latent diffusion framework combining a variational autoencoder with a conditional diffusion model for spatiotemporal data compression through generative interpolation. It achieves high compression ratios and superior performance compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in generative models related to controllability and reconstruction accuracy, particularly for data compression applications, by creating a more efficient framework.

Method: The method involves using variational autoencoder combined with conditional diffusion models to compress keyframes into latent space and reconstruct other frames using generative interpolation, reducing storage needs.

Result: The framework achieves up to 10x higher compression ratios compared to rule-based methods (like SZ3) and up to 63% better performance compared to learning-based methods, under the same reconstruction error.

Conclusion: The proposed approach effectively balances reconstructive accuracy and storage efficiency, making it a promising solution for spatiotemporal data compression.

Abstract: Generative models have demonstrated strong performance in conditional
settings and can be viewed as a form of data compression, where the condition
serves as a compact representation. However, their limited controllability and
reconstruction accuracy restrict their practical application to data
compression. In this work, we propose an efficient latent diffusion framework
that bridges this gap by combining a variational autoencoder with a conditional
diffusion model. Our method compresses only a small number of keyframes into
latent space and uses them as conditioning inputs to reconstruct the remaining
frames via generative interpolation, eliminating the need to store latent
representations for every frame. This approach enables accurate spatiotemporal
reconstruction while significantly reducing storage costs. Experimental results
across multiple datasets show that our method achieves up to 10 times higher
compression ratios than rule-based state-of-the-art compressors such as SZ3,
and up to 63 percent better performance than leading learning-based methods
under the same reconstruction error.

</details>


### [88] [Non-exchangeable Conformal Prediction for Temporal Graph Neural Networks](https://arxiv.org/abs/2507.02151)
*Tuo Wang,Jian Kang,Yujun Yan,Adithya Kulkarni,Dawei Zhou*

Main category: cs.LG

TL;DR: The paper proposes NCPNET, a framework for applying conformal prediction to temporal graphs, addressing issues of statistical coverage and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current conformal prediction methods fail to work well on temporal graphs due to the violation of exchangeability assumptions caused by dynamic graph properties.

Method: NCPNET introduces a diffusion-based non-conformity score and an efficiency-aware optimization algorithm tailored to capture topological and temporal uncertainties in evolving graphs.

Result: NCPNET ensures reliable coverage on temporal graphs, reducing prediction set size by up to 31% on the WIKI dataset and outperforming existing methods in efficiency.

Conclusion: This work presents a significant advancement in conformal prediction for temporal graphs, improving both uncertainty quantification and computational efficiency for high-stakes applications.

Abstract: Conformal prediction for graph neural networks (GNNs) offers a promising
framework for quantifying uncertainty, enhancing GNN reliability in high-stakes
applications. However, existing methods predominantly focus on static graphs,
neglecting the evolving nature of real-world graphs. Temporal dependencies in
graph structure, node attributes, and ground truth labels violate the
fundamental exchangeability assumption of standard conformal prediction
methods, limiting their applicability. To address these challenges, in this
paper, we introduce NCPNET, a novel end-to-end conformal prediction framework
tailored for temporal graphs. Our approach extends conformal prediction to
dynamic settings, mitigating statistical coverage violations induced by
temporal dependencies. To achieve this, we propose a diffusion-based
non-conformity score that captures both topological and temporal uncertainties
within evolving networks. Additionally, we develop an efficiency-aware
optimization algorithm that improves the conformal prediction process,
enhancing computational efficiency and reducing coverage violations. Extensive
experiments on diverse real-world temporal graphs, including WIKI, REDDIT,
DBLP, and IBM Anti-Money Laundering dataset, demonstrate NCPNET's capability to
ensure guaranteed coverage in temporal graphs, achieving up to a 31% reduction
in prediction set size on the WIKI dataset, significantly improving efficiency
compared to state-of-the-art methods. Our data and code are available at
https://github.com/ODYSSEYWT/NCPNET.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [89] [Quality Diversity Genetic Programming for Learning Scheduling Heuristics](https://arxiv.org/abs/2507.02235)
*Meng Xu,Frank Neumann,Aneta Neumann,Yew Soon Ong*

Main category: cs.NE

TL;DR: The paper introduces a novel Quality-Diversity (QD) approach for solving dynamic scheduling problems, emphasizing diverse and high-performing heuristic solutions instead of directly optimizing solutions.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing QD algorithms primarily designed for static problems and to tackle their theoretical underdevelopment in learning heuristics for dynamic combinatorial optimization.

Method: The authors propose a map-building strategy connecting heuristic genotypes to behaviors, enabling visualization and dynamic exploration of solution spaces on a QD map. Experiments were conducted on fixed and dynamic training instances to evaluate the map's evolution.

Result: The approach effectively discovers and maintains diverse scheduling heuristics, demonstrating adaptability to changing contexts and showcasing the evolving solution distribution over time.

Conclusion: The proposed QD framework is a step forward in applying QD optimization to dynamic combinatorial problems, broadening its scope and opening doors to further research in the domain.

Abstract: Real-world optimization often demands diverse, high-quality solutions.
Quality-Diversity (QD) optimization is a multifaceted approach in evolutionary
algorithms that aims to generate a set of solutions that are both
high-performing and diverse. QD algorithms have been successfully applied
across various domains, providing robust solutions by exploring diverse
behavioral niches. However, their application has primarily focused on static
problems, with limited exploration in the context of dynamic combinatorial
optimization problems. Furthermore, the theoretical understanding of QD
algorithms remains underdeveloped, particularly when applied to learning
heuristics instead of directly learning solutions in complex and dynamic
combinatorial optimization domains, which introduces additional challenges.
This paper introduces a novel QD framework for dynamic scheduling problems. We
propose a map-building strategy that visualizes the solution space by linking
heuristic genotypes to their behaviors, enabling their representation on a QD
map. This map facilitates the discovery and maintenance of diverse scheduling
heuristics. Additionally, we conduct experiments on both fixed and dynamically
changing training instances to demonstrate how the map evolves and how the
distribution of solutions unfolds over time. We also discuss potential future
research directions that could enhance the learning process and broaden the
applicability of QD algorithms to dynamic combinatorial optimization
challenges.

</details>


### [90] [Tracing the Interactions of Modular CMA-ES Configurations Across Problem Landscapes](https://arxiv.org/abs/2507.02331)
*Ana Nikolikj,Mario Andrés Muñoz,Eva Tuba,Tome Eftimov*

Main category: cs.NE

TL;DR: The paper examines algorithm footprints to analyze the relationship between algorithm configurations and problem properties using variants of CMA-ES on benchmark problems.


<details>
  <summary>Details</summary>
Motivation: Understand how algorithm configurations interact with problem features and improve interpretability in performance outcomes.

Method: Performance footprints are generated for six CMA-ES variants applied to benchmark problems, across dimensional settings of 5D and 30D.

Result: Shared and distinct behavioral patterns across configurations are identified, driven by interactions and features of problems.

Conclusion: Algorithm footprints are effective for understanding configurations and guiding choices for algorithm design.

Abstract: This paper leverages the recently introduced concept of algorithm footprints
to investigate the interplay between algorithm configurations and problem
characteristics. Performance footprints are calculated for six modular variants
of the CMA-ES algorithm (modCMA), evaluated on 24 benchmark problems from the
BBOB suite, across two-dimensional settings: 5-dimensional and 30-dimensional.
These footprints provide insights into why different configurations of the same
algorithm exhibit varying performance and identify the problem features
influencing these outcomes. Our analysis uncovers shared behavioral patterns
across configurations due to common interactions with problem properties, as
well as distinct behaviors on the same problem driven by differing problem
features. The results demonstrate the effectiveness of algorithm footprints in
enhancing interpretability and guiding configuration choices.

</details>


### [91] [ClustOpt: A Clustering-based Approach for Representing and Visualizing the Search Dynamics of Numerical Metaheuristic Optimization Algorithms](https://arxiv.org/abs/2507.02337)
*Gjorgjina Cenikj,Gašper Petelin,Tome Eftimov*

Main category: cs.NE

TL;DR: This paper introduces a novel methodology for visualizing and analyzing numerical metaheuristic optimization algorithms, using clustering and metrics that reveal algorithm stability and similarity.


<details>
  <summary>Details</summary>
Motivation: Existing visualization techniques for numerical metaheuristic optimization algorithms are insufficient to capture search dynamics in high-dimensional or complex solution spaces.

Method: The authors propose clustering solution candidates from the algorithms, tracking cluster memberships dynamically, and introducing metrics for algorithm stability and similarity.

Result: The methodology was applied to ten numerical metaheuristic algorithms, offering insights into their stability and comparative behaviors.

Conclusion: The approach enhances the understanding of search dynamics, providing interpretable insights for advancing algorithm development and evaluation.

Abstract: Understanding the behavior of numerical metaheuristic optimization algorithms
is critical for advancing their development and application. Traditional
visualization techniques, such as convergence plots, trajectory mapping, and
fitness landscape analysis, often fall short in illustrating the structural
dynamics of the search process, especially in high-dimensional or complex
solution spaces. To address this, we propose a novel representation and
visualization methodology that clusters solution candidates explored by the
algorithm and tracks the evolution of cluster memberships across iterations,
offering a dynamic and interpretable view of the search process. Additionally,
we introduce two metrics - algorithm stability and algorithm similarity- to
quantify the consistency of search trajectories across runs of an individual
algorithm and the similarity between different algorithms, respectively. We
apply this methodology to a set of ten numerical metaheuristic algorithms,
revealing insights into their stability and comparative behaviors, thereby
providing a deeper understanding of their search dynamics.

</details>


### [92] [An Experimental Approach for Running-Time Estimation of Multi-objective Evolutionary Algorithms in Numerical Optimization](https://arxiv.org/abs/2507.02372)
*Han Huang,Tianyu Wang,Chaoda Peng,Tongli He,Zhifeng Hao*

Main category: cs.NE

TL;DR: The paper presents a novel experimental method to estimate upper bounds on the running time of multi-objective evolutionary algorithms for numerical optimization without relying on simplifications.


<details>
  <summary>Details</summary>
Motivation: Existing running time analyses for MOEAs in numerical optimization often rely on oversimplifications, limiting their usefulness in real-world applications.

Method: An average gain model is used alongside statistical methods and adaptive sampling to accurately characterize algorithmic progress and estimate running time bounds, supported by experiments on benchmark problems.

Result: The proposed approach successfully estimates running time upper bounds for various MOEAs (e.g., NSGA-II, MOEA/D) on benchmark suites like ZDT and DTLZ without simplifications.

Conclusion: This experimentally-based methodology complements theoretical research and provides an effective tool for assessing MOEAs in numerical optimization tasks.

Abstract: Multi-objective evolutionary algorithms (MOEAs) have become essential tools
for solving multi-objective optimization problems (MOPs), making their running
time analysis crucial for assessing algorithmic efficiency and guiding
practical applications. While significant theoretical advances have been
achieved for combinatorial optimization, existing studies for numerical
optimization primarily rely on algorithmic or problem simplifications, limiting
their applicability to real-world scenarios. To address this gap, we propose an
experimental approach for estimating upper bounds on the running time of MOEAs
in numerical optimization without simplification assumptions. Our approach
employs an average gain model that characterizes algorithmic progress through
the Inverted Generational Distance metric. To handle the stochastic nature of
MOEAs, we use statistical methods to estimate the probabilistic distribution of
gains. Recognizing that gain distributions in numerical optimization exhibit
irregular patterns with varying densities across different regions, we
introduce an adaptive sampling method that dynamically adjusts sampling density
to ensure accurate surface fitting for running time estimation. We conduct
comprehensive experiments on five representative MOEAs (NSGA-II, MOEA/D,
AR-MOEA, AGEMOEA-II, and PREA) using the ZDT and DTLZ benchmark suites. The
results demonstrate the effectiveness of our approach in estimating upper
bounds on the running time without requiring algorithmic or problem
simplifications. Additionally, we provide a web-based implementation to
facilitate broader adoption of our methodology. This work provides a practical
complement to theoretical research on MOEAs in numerical optimization.

</details>


### [93] [Running-time Analysis of ($μ+λ$) Evolutionary Combinatorial Optimization Based on Multiple-gain Estimation](https://arxiv.org/abs/2507.02381)
*Min Huang,Pengxiang Chen,Han Huang,Tongli He,Yushan Zhang,Zhifeng Hao*

Main category: cs.NE

TL;DR: This paper improves running-time analysis for $(\mu+\lambda)$ evolutionary algorithms (EA) in combinatorial optimization using a multiple-gain model, achieving tighter time complexity bounds.


<details>
  <summary>Details</summary>
Motivation: Theoretical understanding of $(\mu+\lambda)$ EAs in combinatorial optimization is limited compared to simpler problems like pseudo-Boolean problems.

Method: A multiple-gain model was developed, building on the average gain model with enhancements to estimate expected first hitting time in stochastic processes.

Result: Tighter or novel time complexity upper bounds were derived for $(\mu+\lambda)$ EA in problems like knapsack, $k$-MAX-SAT, and traveling salesperson. Experimental results supported theoretical predictions.

Conclusion: The multiple-gain model is a robust approach for analyzing $(\mu+\lambda)$ EA runtimes in combinatorial optimization, contributing tighter bounds and practical validation.

Abstract: The running-time analysis of evolutionary combinatorial optimization is a
fundamental topic in evolutionary computation. However, theoretical results
regarding the $(\mu+\lambda)$ evolutionary algorithm (EA) for combinatorial
optimization problems remain relatively scarce compared to those for simple
pseudo-Boolean problems. This paper proposes a multiple-gain model to analyze
the running time of EAs for combinatorial optimization problems. The proposed
model is an improved version of the average gain model, which is a
fitness-difference drift approach under the sigma-algebra condition to estimate
the running time of evolutionary numerical optimization. The improvement yields
a framework for estimating the expected first hitting time of a stochastic
process in both average-case and worst-case scenarios. It also introduces novel
running-time results of evolutionary combinatorial optimization, including two
tighter time complexity upper bounds than the known results in the case of
($\mu+\lambda$) EA for the knapsack problem with favorably correlated weights,
a closed-form expression of time complexity upper bound in the case of
($\mu+\lambda$) EA for general $k$-MAX-SAT problems and a tighter time
complexity upper bounds than the known results in the case of ($\mu+\lambda$)
EA for the traveling salesperson problem. Experimental results indicate that
the practical running time aligns with the theoretical results, verifying that
the multiple-gain model is an effective tool for running-time analysis of
($\mu+\lambda$) EA for combinatorial optimization problems.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [94] [DecoRTL: A Run-time Decoding Framework for RTL Code Generation with LLMs](https://arxiv.org/abs/2507.02226)
*Mohammad Akyash,Kimia Azar,Hadi Kamali*

Main category: cs.PL

TL;DR: This paper addresses failures in standard large language model (LLM) decoding strategies for generating register transfer level (RTL) code, introducing DecoRTL, a novel inference-time approach to optimize correctness and diversity without model fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Standard LLM decoding strategies struggle to generate valid RTL code due to structural ambiguities and semantic complexities, leading to unusable outputs.

Method: The authors propose DecoRTL, which employs self-consistency sampling for candidate refinement and syntax-aware temperature adaptation to balance determinism and exploratory creativity during RTL code generation.

Result: DecoRTL improves syntactic validity, functional correctness, and output diversity in RTL generation across multiple LLMs, while imposing negligible inference-time overhead.

Conclusion: The paper establishes DecoRTL as an effective inference-time decoding strategy for enhancing RTL code generation, addressing key limitations in conventional methods without requiring additional model training.

Abstract: As one of their many applications, large language models (LLMs) have recently
shown promise in automating register transfer level (RTL) code generation.
However, conventional LLM decoding strategies, originally designed for natural
language, often fail to meet the structural and semantic demands of RTL,
leading to hallucinated, repetitive, or invalid code outputs. In this paper, we
first investigate the root causes of these decoding failures through an
empirical analysis of token-level entropy during RTL generation. Our findings
reveal that LLMs exhibit low confidence in regions of structural ambiguity or
semantic complexity, showing that standard decoding strategies fail to
differentiate between regions requiring determinism (syntax-critical regions)
and those that benefit from creative exploratory variability (design-critical
regions). Then, to overcome this, we introduce DecoRTL, a novel run-time
decoding strategy, that is both syntax-aware and contrastive for RTL code
generation. DecoRTL integrates two complementary components: (i)
self-consistency sampling, which generates multiple candidates and re-ranks
them based on token-level agreement to promote correctness while maintaining
diversity; and (ii) syntax-aware temperature adaptation, which classifies
tokens by their syntactical and functional roles and adjusts the sampling
temperature accordingly, enforcing low temperature for syntax-critical tokens
and higher temperature for exploratory ones. Our approach operates entirely at
inference time without requiring any additional model fine-tuning. Through
evaluations on multiple open-source LLMs using the VerilogEval benchmark, we
demonstrate significant improvements in syntactic validity, functional
correctness, and output diversity, while the execution overhead (performance
overhead) is imperceptible.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [95] [Effective Explanations for Belief-Desire-Intention Robots: When and What to Explain](https://arxiv.org/abs/2507.02016)
*Cong Wang,Roberto Calandra,Verena Klös*

Main category: cs.RO

TL;DR: The paper explores when and how robots should provide explanations for their actions to avoid user confusion and annoyance, proposing algorithms for better human-robot interaction.


<details>
  <summary>Details</summary>
Motivation: Improve human-robot interaction by addressing confusion caused by unexpected robot actions during complex tasks.

Method: Investigated user preferences for explanation timing and content; developed algorithms for identifying surprising actions and making effective explanations integrated with BDI robots.

Result: Users prefer concise explanations in surprising situations; proposed algorithms function with Belief-Desire-Intention systems for tailored explanations.

Conclusion: Effective explanations tailored to user preferences enhance understanding in human-robot interaction and can be implemented easily in BDI robot frameworks.

Abstract: When robots perform complex and context-dependent tasks in our daily lives,
deviations from expectations can confuse users. Explanations of the robot's
reasoning process can help users to understand the robot intentions. However,
when to provide explanations and what they contain are important to avoid user
annoyance. We have investigated user preferences for explanation demand and
content for a robot that helps with daily cleaning tasks in a kitchen. Our
results show that users want explanations in surprising situations and prefer
concise explanations that clearly state the intention behind the confusing
action and the contextual factors that were relevant to this decision. Based on
these findings, we propose two algorithms to identify surprising actions and to
construct effective explanations for Belief-Desire-Intention (BDI) robots. Our
algorithms can be easily integrated in the BDI reasoning process and pave the
way for better human-robot interaction with context- and user-specific
explanations.

</details>


### [96] [RoboBrain 2.0 Technical Report](https://arxiv.org/abs/2507.02029)
*BAAI RoboBrain Team,Mingyu Cao,Huajie Tan,Yuheng Ji,Minglan Lin,Zhiyu Li,Zhou Cao,Pengwei Wang,Enshen Zhou,Yi Han,Yingbo Tang,Xiangqi Xu,Wei Guo,Yaoxu Lyu,Yijie Xu,Jiayu Shi,Cheng Chi,Mengdi Zhao,Xiaoshuai Hao,Shanyu Rong,Zhengliang Cai,Bolun Zhang,Shuyi Zhang,Huaihai Lyu,Mengfei Du,Lingfeng Zhang,Xi Feng,Xiaodan Liu,Yance Jiao,Chenrui He,Mengsi Lyu,Zhuo Chen,Yulong Ao,Xue Sun,Zheqi He,Jingshu Zheng,Xi Yang,Donghai Shi,Kunchang Xie,Bochao Zhang,Shaokai Nie,Chunlei Men,Yonghua Lin,Zhongyuan Wang,Tiejun Huang,Shanghang Zhang*

Main category: cs.RO

TL;DR: The paper introduces RoboBrain 2.0, a new embodied vision-language foundation model with 7B and 32B variants, achieving strong results in spatial and temporal embodied AI tasks.


<details>
  <summary>Details</summary>
Motivation: To unify perception, reasoning, and planning for complex tasks in physical environments with an advanced embodied AI model.

Method: Developed two model variants, a 7B and a 32B architecture, combining a vision encoder and a language model, trained on custom datasets using multi-stage strategies.

Result: The 32B variant surpasses prior state-of-the-art models in spatial understanding and temporal decision-making benchmarks.

Conclusion: RoboBrain 2.0 represents progress towards generalist embodied agents, advancing embodied AI research and offering practical applications with open-access resources.

Abstract: We introduce RoboBrain 2.0, our latest generation of embodied vision-language
foundation models, designed to unify perception, reasoning, and planning for
complex embodied tasks in physical environments. It comes in two variants: a
lightweight 7B model and a full-scale 32B model, featuring a heterogeneous
architecture with a vision encoder and a language model. Despite its compact
size, RoboBrain 2.0 achieves strong performance across a wide spectrum of
embodied reasoning tasks. On both spatial and temporal benchmarks, the 32B
variant achieves leading results, surpassing prior open-source and proprietary
models. In particular, it supports key real-world embodied AI capabilities,
including spatial understanding (e.g., affordance prediction, spatial
referring, trajectory forecasting) and temporal decision-making (e.g.,
closed-loop interaction, multi-agent long-horizon planning, and scene graph
updating). This report details the model architecture, data construction,
multi-stage training strategies, infrastructure and practical applications. We
hope RoboBrain 2.0 advances embodied AI research and serves as a practical step
toward building generalist embodied agents. The code, checkpoint and benchmark
are available at https://superrobobrain.github.io.

</details>


### [97] [Towards Bio-Inspired Robotic Trajectory Planning via Self-Supervised RNN](https://arxiv.org/abs/2507.02171)
*Miroslav Cibula,Kristína Malinovská,Matthias Kerzel*

Main category: cs.RO

TL;DR: This paper introduces a cognitively inspired self-supervised learning model for robotic trajectory planning, simplifying planning for complex tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional trajectory planning relies heavily on computationally intensive sampling-based methods or supervised sequence learning techniques, which have limitations like imitation without goal validation.

Method: The authors propose a novel self-supervised learning approach utilizing a recurrent neural network to model trajectories based on paired forward and inverse kinematics.

Result: The proposed model successfully learns to generate effective trajectories for robotic arms using paired kinematic models.

Conclusion: The method shows promise in simplifying trajectory planning for complex robotic tasks, demonstrating adaptability and effectiveness.

Abstract: Trajectory planning in robotics is understood as generating a sequence of
joint configurations that will lead a robotic agent, or its manipulator, from
an initial state to the desired final state, thus completing a manipulation
task while considering constraints like robot kinematics and the environment.
Typically, this is achieved via sampling-based planners, which are
computationally intensive. Recent advances demonstrate that trajectory planning
can also be performed by supervised sequence learning of trajectories, often
requiring only a single or fixed number of passes through a neural
architecture, thus ensuring a bounded computation time. Such fully supervised
approaches, however, perform imitation learning; they do not learn based on
whether the trajectories can successfully reach a goal, but try to reproduce
observed trajectories. In our work, we build on this approach and propose a
cognitively inspired self-supervised learning scheme based on a recurrent
architecture for building a trajectory model. We evaluate the feasibility of
the proposed method on a task of kinematic planning for a robotic arm. The
results suggest that the model is able to learn to generate trajectories only
using given paired forward and inverse kinematics models, and indicate that
this novel method could facilitate planning for more complex manipulation tasks
requiring adaptive solutions.

</details>


### [98] [cVLA: Towards Efficient Camera-Space VLAs](https://arxiv.org/abs/2507.02190)
*Max Argus,Jelena Bratulic,Houman Masnavi,Maxim Velikanov,Nick Heppert,Abhinav Valada,Thomas Brox*

Main category: cs.RO

TL;DR: The paper proposes a Vision-Language-Action (VLA) model that leverages Vision Language Models (VLMs) for robotic task prediction, emphasizing efficiency and strong sim-to-real transfer abilities.


<details>
  <summary>Details</summary>
Motivation: To address the high costs and inefficiencies in training VLA models for complex robotic manipulation tasks by leveraging VLMs.

Method: Developed a next-token prediction architecture with trajectory waypoint predictions, utilizing depth images and demonstration-conditioned action generation tested in simulated environments.

Result: The proposed model exhibits strong performance on simulated to real-world evaluation, demonstrating accurate and executable robotic trajectories.

Conclusion: The approach efficiently predicts meaningful robotic actions and establishes an advance in sim-to-real transfer for robotic systems, balancing performance and lightweight architecture.

Abstract: Vision-Language-Action (VLA) models offer a compelling framework for tackling
complex robotic manipulation tasks, but they are often expensive to train. In
this paper, we propose a novel VLA approach that leverages the competitive
performance of Vision Language Models (VLMs) on 2D images to directly infer
robot end-effector poses in image frame coordinates. Unlike prior VLA models
that output low-level controls, our model predicts trajectory waypoints, making
it both more efficient to train and robot embodiment agnostic. Despite its
lightweight design, our next-token prediction architecture effectively learns
meaningful and executable robot trajectories. We further explore the
underutilized potential of incorporating depth images, inference-time
techniques such as decoding strategies, and demonstration-conditioned action
generation. Our model is trained on a simulated dataset and exhibits strong
sim-to-real transfer capabilities. We evaluate our approach using a combination
of simulated and real data, demonstrating its effectiveness on a real robotic
system.

</details>


### [99] [GPS-DRIFT: Marine Surface Robot Localization using IMU-GPS Fusion and Invariant Filtering](https://arxiv.org/abs/2507.02198)
*Surya Pratap Singh,Tsimafei Lazouski,Maani Ghaffari*

Main category: cs.RO

TL;DR: This paper extends the DRIFT framework for GPS and IMU sensor fusion enabling accurate pose and heading estimation using the Invariant Extended Kalman Filter (InEKF).


<details>
  <summary>Details</summary>
Motivation: There is a need for robust and drift-free localization in mobile systems, particularly in applications like autonomous surface vehicles, where GPS and IMU data fusion is crucial for accurate navigation.

Method: The authors develop a symmetry-preserving sensor fusion pipeline using the Invariant Extended Kalman Filter (InEKF) to fuse GPS position updates and IMU data, introducing a novel heading correction mechanism leveraging GPS course-over-ground information.

Result: The proposed system was deployed successfully on the Blue Robotics BlueBoat, demonstrating accurate yaw observation and drift-free localization, even in GPS-degraded conditions.

Conclusion: The study provides an open-source algorithmic solution for robust sensor fusion, enabling accurate localization and heading estimation for mobile systems, and sets the stage for future experimental enhancements.

Abstract: This paper presents an extension of the DRIFT invariant state estimation
framework, enabling robust fusion of GPS and IMU data for accurate pose and
heading estimation. Originally developed for testing and usage on a marine
autonomous surface vehicle (ASV), this approach can also be utilized on other
mobile systems. Building upon the original proprioceptive only DRIFT algorithm,
we develop a symmetry-preserving sensor fusion pipeline utilizing the invariant
extended Kalman filter (InEKF) to integrate global position updates from GPS
directly into the correction step. Crucially, we introduce a novel heading
correction mechanism that leverages GPS course-over-ground information in
conjunction with IMU orientation, overcoming the inherent unobservability of
yaw in dead-reckoning. The system was deployed and validated on a customized
Blue Robotics BlueBoat, but the methodological focus is on the algorithmic
approach to fusing exteroceptive and proprioceptive sensors for drift-free
localization and reliable orientation estimation. This work provides an open
source solution for accurate yaw observation and localization in challenging or
GPS-degraded conditions, and lays the groundwork for future experimental and
comparative studies.

</details>


### [100] [CoInfra: A Large-Scale Cooperative Infrastructure Perception System and Dataset in Adverse Weather](https://arxiv.org/abs/2507.02245)
*Minghao Ning,Yufeng Yang,Keqi Shu,Shucheng Huang,Jiaming Zhong,Maryam Salehi,Mahdi Rahmani,Yukun Lu,Chen Sun,Aladdin Saleh,Ehsan Hashemi,Amir Khajepour*

Main category: cs.RO

TL;DR: The paper introduces CoInfra, a multi-agent perception system and dataset for research in autonomous driving under various weather conditions. It includes 14 synchronized sensor nodes, real-world weather data, and 3D annotations for traffic participants.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of robust multi-agent perception in autonomous driving, especially under adverse weather conditions, through a cooperative infrastructure-based approach.

Method: The proposed CoInfra system uses 14 sensor nodes with cameras and LiDARs for data capture. It employs synchronization protocols and scalable architectures to ensure accurate real-time perception. The accompanying dataset was collected under diverse weather scenarios and includes aligned data with annotated 3D bounding boxes.

Result: Key dataset characteristics include 195k LiDAR frames, 390k camera images, high-definition maps, and extensive annotations. Experiments showcase the trade-offs between fusion strategies and highlight the benefits of integrating HD maps.

Conclusion: By releasing the dataset, codebase, and system documentation publicly, the paper aims to drive progress in autonomous driving research through reproducibility and a focus on challenging environments.

Abstract: We present CoInfra, a large-scale cooperative infrastructure perception
system and dataset designed to advance robust multi-agent perception under
real-world and adverse weather conditions. The CoInfra system includes 14 fully
synchronized sensor nodes, each equipped with dual RGB cameras and a LiDAR,
deployed across a shared region and operating continuously to capture all
traffic participants in real-time. A robust, delay-aware synchronization
protocol and a scalable system architecture that supports real-time data
fusion, OTA management, and remote monitoring are provided in this paper. On
the other hand, the dataset was collected in different weather scenarios,
including sunny, rainy, freezing rain, and heavy snow and includes 195k LiDAR
frames and 390k camera images from 8 infrastructure nodes that are globally
time-aligned and spatially calibrated. Furthermore, comprehensive 3D bounding
box annotations for five object classes (i.e., car, bus, truck, person, and
bicycle) are provided in both global and individual node frames, along with
high-definition maps for contextual understanding. Baseline experiments
demonstrate the trade-offs between early and late fusion strategies, the
significant benefits of HD map integration are discussed. By openly releasing
our dataset, codebase, and system documentation at
https://github.com/NingMingHao/CoInfra, we aim to enable reproducible research
and drive progress in infrastructure-supported autonomous driving, particularly
in challenging, real-world settings.

</details>


### [101] [A Vehicle-in-the-Loop Simulator with AI-Powered Digital Twins for Testing Automated Driving Controllers](https://arxiv.org/abs/2507.02313)
*Zengjie Zhang,Giannis Badakis,Michalis Galanis,Adem Bavarşi,Edwin van Hassel,Mohsen Alirezaei,Sofie Haesaert*

Main category: cs.RO

TL;DR: A practical simulator was developed featuring scaled physical cars and AI-enhanced digital twin models to save costs, increase fidelity, and test automated driving controllers effectively.


<details>
  <summary>Details</summary>
Motivation: The demand for efficient and cost-effective testing mechanisms for automated driving controllers, overcoming limitations like high expenses or reality gaps in conventional simulation techniques.

Method: The study introduced scaled physical cars and AI-based digital twins, integrating these elements with software and control algorithms, along with formal safety benchmarks.

Result: The developed simulator demonstrated high efficacy in experimental tests, validating its capabilities in testing autonomous vehicle controllers and intelligent traffic solutions.

Conclusion: The simulator presents a feasible, scalable tool for reliable testing of autonomous driving systems, paving the way for enhanced deployment and validation efficiency.

Abstract: Simulators are useful tools for testing automated driving controllers.
Vehicle-in-the-loop (ViL) tests and digital twins (DTs) are widely used
simulation technologies to facilitate the smooth deployment of controllers to
physical vehicles. However, conventional ViL tests rely on full-size vehicles,
requiring large space and high expenses. Also, physical-model-based DT suffers
from the reality gap caused by modeling imprecision. This paper develops a
comprehensive and practical simulator for testing automated driving controllers
enhanced by scaled physical cars and AI-powered DT models. The scaled cars
allow for saving space and expenses of simulation tests. The AI-powered DT
models ensure superior simulation fidelity. Moreover, the simulator integrates
well with off-the-shelf software and control algorithms, making it easy to
extend. We use a filtered control benchmark with formal safety guarantees to
showcase the capability of the simulator in validating automated driving
controllers. Experimental studies are performed to showcase the efficacy of the
simulator, implying its great potential in validating control solutions for
autonomous vehicles and intelligent traffic.

</details>


### [102] [Path Planning using a One-shot-sampling Skeleton Map](https://arxiv.org/abs/2507.02328)
*Gabriel O. Flores-Aquino,Octavio Gutierrez-Frias,Juan Irving Vasquez*

Main category: cs.RO

TL;DR: The paper presents a path-planning method using a U-Net-based autoencoder called SkelUnet to efficiently compute safe and navigable routes, especially for UAVs, achieving improved safety and reduced processing times.


<details>
  <summary>Details</summary>
Motivation: Standard path-planning algorithms focus on finding optimal distance paths, but they may not balance response time, safety, and length. There is a need for methods that address these trade-offs, especially in structured robotics environments.

Method: A Deep Denoising Auto-Encoder (DDAE) based on U-Net architecture, named SkelUnet, is used for skeletonizing navigation maps. This neural network enables one-shot sampling for path planning. The method is evaluated using 12,500 maps for training and 250 unseen maps for testing, specifically for UAV simulation environments.

Result: The proposed SkelUnet approach demonstrates significant improvements in creating roadmaps that connect all free workspace regions, enhance path safety, and reduce resource-intensive processing times.

Conclusion: The SkelUnet-based methodology is effective for path planning in structured environments, offering key advantages in processing speed and safety, and is well-suited for mobile service robots.

Abstract: Path planning algorithms aim to compute a collision-free path, and many works
focus on finding the optimal distance path. However, for some applications, a
more suitable approach is to balance response time, safety of the paths, and
path length. In this context, a skeleton map is a useful tool in graph-based
schemes, as it provides an intrinsic representation of free configuration
space. However, skeletonization algorithms are very resource-intensive, being
primarily oriented towards image processing tasks. We propose an efficient
path-planning methodology that finds safe paths within an acceptable processing
time. This methodology leverages a Deep Denoising Auto-Encoder (DDAE) based on
U-Net architecture to compute a skeletonized version of the navigation map,
which we refer to as SkelUnet. The SkelUnet network facilitates exploration of
the entire workspace through one-shot sampling (OSS), as opposed to the
iterative process used by exact algorithms or the probabilistic sampling
process. SkelUnet is trained and tested on a dataset consisting of 12,500
bi-dimensional dungeon maps. The motion planning methodology is evaluated in a
simulation environment for an Unmanned Aerial Vehicle (UAV) using 250
previously unseen maps, and assessed with various navigation metrics to
quantify the navigability of the computed paths. The results demonstrate that
using SkelUnet to construct a roadmap offers significant advantages, such as
connecting all regions of free workspace, providing safer paths, and reducing
processing times. These characteristics make this method particularly suitable
for mobile service robots in structured environments.

</details>


### [103] [DigiT4TAF -- Bridging Physical and Digital Worlds for Future Transportation Systems](https://arxiv.org/abs/2507.02400)
*Maximilian Zipfl,Pascal Zwick,Patrick Schulz,Marc Rene Zofka,Albert Schotschneider,Helen Gremmelmaier,Nikolai Polley,Ferdinand Mütsch,Kevin Simon,Fabian Gottselig,Michael Frey,Sergio Marschall,Akim Stark,Maximilian Müller,Marek Wehmer,Mihai Kocsis,Dominic Waldenmayer,Florian Schnepf,Erik Heinrich,Sabrina Pletz,Matthias Kölle,Karin Langbein-Euchner,Alexander Viehl,Raoul Zöllner,J. Marius Zöllner*

Main category: cs.RO

TL;DR: This paper discusses the development and application of a Digital Twin for the Test Area Autonomous Driving-Baden-Württemberg (TAF-BW), focusing on infrastructure, data collection, and simulation for traffic optimization and security scenarios.


<details>
  <summary>Details</summary>
Motivation: To leverage digitalization and interconnected systems for enhanced mobility and create a continuous integration of real and virtual environments through the Digital Twin concept.

Method: The method involves digital reconstruction of TAF-BW using V2X communication, camera sensors, and LiDAR-equipped test vehicles to extract data for traffic simulation. A unified interface allows real-world detections to be resimulated.

Result: The framework created enables authentic, resimulated digital Twin data and is publicly available for download. Case studies demonstrate optimization of traffic signals and simulation of security-related communication scenarios.

Conclusion: The paper successfully shows the applicability of Digital Twin technology for analyzing and optimizing traffic systems and security scenarios, while providing a robust and accessible framework for future research.

Abstract: In the future, mobility will be strongly shaped by the increasing use of
digitalization. Not only will individual road users be highly interconnected,
but also the road and associated infrastructure. At that point, a Digital Twin
becomes particularly appealing because, unlike a basic simulation, it offers a
continuous, bilateral connection linking the real and virtual environments.
This paper describes the digital reconstruction used to develop the Digital
Twin of the Test Area Autonomous Driving-Baden-W\"urttemberg (TAF-BW), Germany.
The TAF-BW offers a variety of different road sections, from high-traffic urban
intersections and tunnels to multilane motorways. The test area is equipped
with a comprehensive Vehicle-to-Everything (V2X) communication infrastructure
and multiple intelligent intersections equipped with camera sensors to
facilitate real-time traffic flow monitoring. The generation of authentic data
as input for the Digital Twin was achieved by extracting object lists at the
intersections. This process was facilitated by the combined utilization of
camera images from the intelligent infrastructure and LiDAR sensors mounted on
a test vehicle. Using a unified interface, recordings from real-world
detections of traffic participants can be resimulated. Additionally, the
simulation framework's design and the reconstruction process is discussed. The
resulting framework is made publicly available for download and utilization at:
https://digit4taf-bw.fzi.de The demonstration uses two case studies to
illustrate the application of the digital twin and its interfaces: the analysis
of traffic signal systems to optimize traffic flow and the simulation of
security-related scenarios in the communications sector.

</details>


### [104] [A Late Collaborative Perception Framework for 3D Multi-Object and Multi-Source Association and Fusion](https://arxiv.org/abs/2507.02430)
*Maryem Fadili,Mohamed Anis Ghaoui,Louis Lecrosnier,Steve Pechberti,Redouane Khemmar*

Main category: cs.RO

TL;DR: The paper introduces a late collaborative framework for 3D object fusion in autonomous driving, achieving state-of-the-art accuracy without requiring high bandwidth or access to detection model details.


<details>
  <summary>Details</summary>
Motivation: The authors aim to overcome the constraints of high communication bandwidth and proprietary model access in collaborative perception methods for autonomous driving.

Method: They propose a novel late collaborative framework that uses only shared 3D bounding box attributes (category, size, position, orientation) without accessing detection models.

Result: The framework significantly improves error metrics, reducing position error fivefold, scale error by 7.5x, and halving orientation error, all with 100% precision and recall.

Conclusion: The proposed method effectively addresses real-world challenges in collaborative perception, establishing a new benchmark for efficient and scalable multi-agent fusion.

Abstract: In autonomous driving, recent research has increasingly focused on
collaborative perception based on deep learning to overcome the limitations of
individual perception systems. Although these methods achieve high accuracy,
they rely on high communication bandwidth and require unrestricted access to
each agent's object detection model architecture and parameters. These
constraints pose challenges real-world autonomous driving scenarios, where
communication limitations and the need to safeguard proprietary models hinder
practical implementation. To address this issue, we introduce a novel late
collaborative framework for 3D multi-source and multi-object fusion, which
operates solely on shared 3D bounding box attributes-category, size, position,
and orientation-without necessitating direct access to detection models. Our
framework establishes a new state-of-the-art in late fusion, achieving up to
five times lower position error compared to existing methods. Additionally, it
reduces scale error by a factor of 7.5 and orientation error by half, all while
maintaining perfect 100% precision and recall when fusing detections from
heterogeneous perception systems. These results highlight the effectiveness of
our approach in addressing real-world collaborative perception challenges,
setting a new benchmark for efficient and scalable multi-agent fusion.

</details>


### [105] [MISC: Minimal Intervention Shared Control with Guaranteed Safety under Non-Convex Constraints](https://arxiv.org/abs/2507.02438)
*Shivam Chaubey,Francesco Verdoja,Shankar Deka,Ville Kyrki*

Main category: cs.RO

TL;DR: The paper presents a shared control framework that improves user experience and safety by combining constrained optimal control and offline-computed control invariant sets, validated through a large-scale user study.


<details>
  <summary>Details</summary>
Motivation: Existing shared control methods face challenges in feasibility, scalability, and safety, especially when dealing with unpredictable user input.

Method: They propose a framework based on Constrained Optimal Control Problem with offline-computed Control Invariant Sets, enabling real-time safety guarantees and accommodating non-convex constraints.

Result: A large-scale user study with 66 participants showed improvements in task load, trust, perceived control, and performance, without compromising safety or user intent.

Conclusion: The proposed framework enhances shared control usability and effectiveness, offering a feasible and safe solution for unpredictable user input scenarios.

Abstract: Shared control combines human intention with autonomous decision-making, from
low-level safety overrides to high-level task guidance, enabling systems that
adapt to users while ensuring safety and performance. This enhances task
effectiveness and user experience across domains such as assistive robotics,
teleoperation, and autonomous driving. However, existing shared control
methods, based on e.g. Model Predictive Control, Control Barrier Functions, or
learning-based control, struggle with feasibility, scalability, or safety
guarantees, particularly since the user input is unpredictable.
  To address these challenges, we propose an assistive controller framework
based on Constrained Optimal Control Problem that incorporates an
offline-computed Control Invariant Set, enabling online computation of control
actions that ensure feasibility, strict constraint satisfaction, and minimal
override of user intent. Moreover, the framework can accommodate structured
class of non-convex constraints, which are common in real-world scenarios. We
validate the approach through a large-scale user study with 66
participants--one of the most extensive in shared control research--using a
computer game environment to assess task load, trust, and perceived control, in
addition to performance. The results show consistent improvements across all
these aspects without compromising safety and user intent.

</details>


### [106] [HAC-LOCO: Learning Hierarchical Active Compliance Control for Quadruped Locomotion under Continuous External Disturbances](https://arxiv.org/abs/2507.02447)
*Xiang Zhou,Xinyu Zhang,Qingrui Zhang*

Main category: cs.RO

TL;DR: The study proposes a hierarchical learning framework integrating force estimation and compliance to achieve robust and energy-efficient quadruped locomotion under external disturbances.


<details>
  <summary>Details</summary>
Motivation: Current quadruped control methods struggle to balance robustness and compliance, leading to stiff locomotion and energy inefficiency.

Method: The framework consists of two stages: velocity-tracking policy training with a force estimator in stage one, and compliance action module inspired by impedance control in stage two.

Result: The proposed approach improves quadruped performance in robustness, energy efficiency, and safety, outperforming state-of-the-art RL controllers with validated results in simulations and real-world experiments.

Conclusion: The compliance action module plays a critical role in achieving a balance between robustness and compliance for quadruped locomotion, making the solution practical and highly effective.

Abstract: Despite recent remarkable achievements in quadruped control, it remains
challenging to ensure robust and compliant locomotion in the presence of
unforeseen external disturbances. Existing methods prioritize locomotion
robustness over compliance, often leading to stiff, high-frequency motions, and
energy inefficiency. This paper, therefore, presents a two-stage hierarchical
learning framework that can learn to take active reactions to external force
disturbances based on force estimation. In the first stage, a velocity-tracking
policy is trained alongside an auto-encoder to distill historical
proprioceptive features. A neural network-based estimator is learned through
supervised learning, which estimates body velocity and external forces based on
proprioceptive measurements. In the second stage, a compliance action module,
inspired by impedance control, is learned based on the pre-trained encoder and
policy. This module is employed to actively adjust velocity commands in
response to external forces based on real-time force estimates. With the
compliance action module, a quadruped robot can robustly handle minor
disturbances while appropriately yielding to significant forces, thus striking
a balance between robustness and compliance. Simulations and real-world
experiments have demonstrated that our method has superior performance in terms
of robustness, energy efficiency, and safety. Experiment comparison shows that
our method outperforms the state-of-the-art RL-based locomotion controllers.
Ablation studies are given to show the critical roles of the compliance action
module.

</details>


### [107] [Safe and Socially Aware Multi-Robot Coordination in Multi-Human Social Care Settings](https://arxiv.org/abs/2507.02521)
*Ayodeji O. Abioye,Jayati Deshmukh,Athina Georgara,Dominic Price,Tuyen Nguyen,Aleksandra Landowska,Amel Bennaceur,Joel E. Fischer,Sarvapali D. Ramchurn*

Main category: cs.RO

TL;DR: The paper explores multi-objective strategies for coordination in multi-robot and multi-human settings, focusing on path planning, navigation, and task management.


<details>
  <summary>Details</summary>
Motivation: To address challenges in coordinating multiple robots and humans, specifically in areas like path planning, task scheduling, and human-robot interaction, in dynamic environments.

Method: A learning-based, multi-objective approach is introduced to optimize coordination in multi-human, multi-robot scenarios.

Result: The paper evaluates improvements in path planning, navigation, task scheduling, allocation, and interaction in MHMR environments.

Conclusion: This study provides a framework for enhancing coordination and functionality in multi-human, multi-robot systems by addressing key operational challenges.

Abstract: This research investigates strategies for multi-robot coordination in
multi-human environments. It proposes a multi-objective learning-based
coordination approach to addressing the problem of path planning, navigation,
task scheduling, task allocation, and human-robot interaction in multi-human
multi-robot (MHMR) settings.

</details>


### [108] [Vibration of Soft, Twisted Beams for Under-Actuated Quadrupedal Locomotion](https://arxiv.org/abs/2507.02547)
*Yuhao Jiang,Fuchen Chen,Jamie Paik,Daniel M. Aukes*

Main category: cs.RO

TL;DR: Flix-Walker, a unique compliant robot that uses helix-shaped legs and minimal motors, achieves multiple mobility modes validated by experiments.


<details>
  <summary>Details</summary>
Motivation: To address challenges in actuation and control by leveraging dynamic behaviors in under-actuated compliant robotic systems.

Method: Designing and testing Flix-Walker robot with flexible helix-shaped legs, actuated by vibrations from two motors, analyzing locomotion via simulation and experiments.

Result: Flix-Walker demonstrated three mobility modes and reliable trajectory tracking and self-navigation functionalities in varied conditions.

Conclusion: Validated actuation parameters enable robust and functional motions in under-actuated compliant robotic systems, paving the way for advanced control frameworks.

Abstract: Under-actuated compliant robotic systems offer a promising approach to
mitigating actuation and control challenges by harnessing pre-designed,
embodied dynamic behaviors. This paper presents Flix-Walker, a novel,
untethered, centimeter-scale quadrupedal robot inspired by compliant
under-actuated mechanisms. Flix-Walker employs flexible, helix-shaped beams as
legs, which are actuated by vibrations from just two motors to achieve three
distinct mobility modes. We analyze the actuation parameters required to
generate various locomotion modes through both simulation and prototype
experiments. The effects of system and environmental variations on locomotion
performance are examined, and we propose a generic metric for selecting control
parameters that produce robust and functional motions. Experiments validate the
effectiveness and robustness of these actuation parameters within a closed-loop
control framework, demonstrating reliable trajectory-tracking and
self-navigation capabilities.

</details>


### [109] [ArtGS:3D Gaussian Splatting for Interactive Visual-Physical Modeling and Manipulation of Articulated Objects](https://arxiv.org/abs/2507.02600)
*Qiaojun Yu,Xibin Yuan,Yu jiang,Junting Chen,Dongzhe Zheng,Ce Hao,Yang You,Yixing Chen,Yao Mu,Liu Liu,Cewu Lu*

Main category: cs.RO

TL;DR: ArtGS is a new framework for robotic manipulation of articulated objects, combining 3D Gaussian Splatting with vision-language model reasoning to achieve improved joint estimation accuracy and manipulation success rates.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome challenges in articulated object manipulation by addressing complex kinematic constraints and the limitations in physical reasoning of current methods.

Method: The method involves multi-view RGB-D reconstruction, vision-language model reasoning to identify articulated structures, and dynamic 3D Gaussian Splatting for closed-loop optimization and physically consistent modeling.

Result: ArtGS demonstrates superior performance in joint estimation accuracy and manipulation success rates in both simulated and real-world scenarios compared to prior approaches.

Conclusion: ArtGS offers an efficient, scalable, and adaptable solution for articulated object modeling and manipulation, marking progress in robotic physical interaction.

Abstract: Articulated object manipulation remains a critical challenge in robotics due
to the complex kinematic constraints and the limited physical reasoning of
existing methods. In this work, we introduce ArtGS, a novel framework that
extends 3D Gaussian Splatting (3DGS) by integrating visual-physical modeling
for articulated object understanding and interaction. ArtGS begins with
multi-view RGB-D reconstruction, followed by reasoning with a vision-language
model (VLM) to extract semantic and structural information, particularly the
articulated bones. Through dynamic, differentiable 3DGS-based rendering, ArtGS
optimizes the parameters of the articulated bones, ensuring physically
consistent motion constraints and enhancing the manipulation policy. By
leveraging dynamic Gaussian splatting, cross-embodiment adaptability, and
closed-loop optimization, ArtGS establishes a new framework for efficient,
scalable, and generalizable articulated object modeling and manipulation.
Experiments conducted in both simulation and real-world environments
demonstrate that ArtGS significantly outperforms previous methods in joint
estimation accuracy and manipulation success rates across a variety of
articulated objects. Additional images and videos are available on the project
website: https://sites.google.com/view/artgs/home

</details>


### [110] [MISCGrasp: Leveraging Multiple Integrated Scales and Contrastive Learning for Enhanced Volumetric Grasping](https://arxiv.org/abs/2507.02672)
*Qingyu Fan,Yinghao Cai,Chao Li,Chunting Jiao,Xudong Zheng,Tao Lu,Bin Liang,Shuo Wang*

Main category: cs.RO

TL;DR: MISCGrasp introduces a volumetric grasping method combining multi-scale feature extraction and contrastive learning to improve self-adaptive grasping for diverse objects. It surpasses existing approaches in tabletop decluttering tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome the challenges robotic grasping faces when dealing with objects of varying shapes and sizes.

Method: The authors present a method that employs the Insight Transformer for query-based high and low-level feature interaction, and the Empower Transformer for focused attention on high-level features. Multi-scale contrastive learning is used to ensure feature consistency.

Result: MISCGrasp demonstrated superior performance compared to baseline and variant methods in simulated and real-world tabletop decluttering experiments.

Conclusion: The results confirm that the proposed MISCGrasp approach enhances robotic grasping capabilities and adaptability by effectively combining multi-scale feature analysis and contrastive learning techniques.

Abstract: Robotic grasping faces challenges in adapting to objects with varying shapes
and sizes. In this paper, we introduce MISCGrasp, a volumetric grasping method
that integrates multi-scale feature extraction with contrastive feature
enhancement for self-adaptive grasping. We propose a query-based interaction
between high-level and low-level features through the Insight Transformer,
while the Empower Transformer selectively attends to the highest-level
features, which synergistically strikes a balance between focusing on fine
geometric details and overall geometric structures. Furthermore, MISCGrasp
utilizes multi-scale contrastive learning to exploit similarities among
positive grasp samples, ensuring consistency across multi-scale features.
Extensive experiments in both simulated and real-world environments demonstrate
that MISCGrasp outperforms baseline and variant methods in tabletop
decluttering tasks. More details are available at https://miscgrasp.github.io/.

</details>


### [111] [Integrating path-planning and control for robotic unicycles](https://arxiv.org/abs/2507.02700)
*Máté B. Vizi,Dénes Tákács,Gábor Stépán,Gábor Orosz*

Main category: cs.RO

TL;DR: This paper explores path-planning and control integration tailored for robotic unicycles, presenting a design and method for optimized maneuvering.


<details>
  <summary>Details</summary>
Motivation: Address the unique challenges of robotic unicycles by creating an efficient path-planning and control system.

Method: Segment paths into straight and curved sections for acceleration/braking and turning, while optimizing curvature profiles considering slipping limits and control performance.

Result: Numerical simulations showcase the performance of this integrated approach.

Conclusion: The study confirms the feasibility of the proposed method for enhancing robotic unicycle navigation and maneuverability.

Abstract: This article focuses on integrating path-planning and control with
specializing on the unique needs of robotic unicycles. A unicycle design is
presented which is capable of accelerating/breaking and carrying out a variety
of maneuvers. The proposed path-planning method segments the path into straight
and curved path sections dedicated for accelerating/breaking and turning
maneuvers, respectively. The curvature profiles of the curved sections are
optimized while considering the control performance and the slipping limits of
the wheel. The performance of the proposed integrated approach is demonstrated
via numerical simulations.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [112] [How do Software Engineering Candidates Prepare for Technical Interviews?](https://arxiv.org/abs/2507.02068)
*Brian Bell,Teresa Thomas,Sang Won Lee,Chris Brown*

Main category: cs.SE

TL;DR: The study examines how software engineering candidates prepare for technical interviews, finding that current educational approaches are insufficient, causing stress and unpreparedness.


<details>
  <summary>Details</summary>
Motivation: Technical interviews are a significant hurdle for aspiring software engineers due to the lack of focused preparation within computing curricula.

Method: A survey was distributed to 131 candidates who were actively preparing for technical interviews, analyzing preparation methods and educational influences.

Result: Candidates typically do not train in authentic interview settings, and courses fail to adequately support their preparation, resulting in stress and feelings of unpreparedness.

Conclusion: Enhancing technical interview preparation efforts within educational contexts can better support candidates and reduce stress during this crucial process.

Abstract: To obtain employment, aspiring software engineers must complete technical
interviews -- a hiring process which involves candidates writing code while
communicating to an audience. However, the complexities of tech interviews are
difficult to prepare for and seldom faced in computing curricula. To this end,
we seek to understand how candidates prepare for technical interviews,
investigating the effects of preparation methods and the role of education. We
distributed a survey to candidates (n = 131) actively preparing for technical
interviews. Our results suggest candidates rarely train in authentic settings
and courses fail to support preparation efforts -- leading to stress and
unpreparedness. Based on our findings, we provide implications for stakeholders
to enhance tech interview preparation for candidates pursuing software
engineering roles.

</details>


### [113] [Structural Code Search using Natural Language Queries](https://arxiv.org/abs/2507.02107)
*Ben Limpanukorn,Yanjun Wang,Zach Patterson,Pranav Garg,Murali Krishna Ramanathan,Xiaofei Ma,Anoop Deoras,Miryung Kim*

Main category: cs.SE

TL;DR: The paper proposes using natural language to search for code structurally, combining large language models (LLMs) with existing structural search tools like Semgrep and GQL, achieving better precision and recall over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Structural code search tools allow developers to query code based on syntax but are hindered by the complexity of domain-specific languages (DSLs). This work seeks to lower the entry barrier by enabling natural language queries.

Method: A system is developed that translates natural language queries into DSL queries using the reasoning power of LLMs and integrates these with structural search tools. The approach is evaluated using a newly constructed benchmark of 400 queries across 10 Java projects.

Result: The proposed approach achieves high precision and recall between 55% - 70% and outperforms baselines like semantic search and LLM retrievals by up to 57% and 14% in F1 scores.

Conclusion: Translating natural language queries to DSL with LLMs is an effective way to make structural code search more accessible and robust, offering significant performance improvements.

Abstract: Searching code is a common task that developers perform to understand APIs,
learn common code patterns, and navigate code. Currently, developers most
commonly search using keywords and regular expressions that are easy to use and
widely available. Beyond keywords and regular expressions, structural code
search tools allow developers to search for code based on its syntactic
structure. This has numerous applications ranging from bug finding to
systematically refactoring code. However, these structural code search tools
operate on queries expressed in domain-specific languages (DSL) that can be
difficult to learn and write. We propose to allow developers to use natural
language to search for code structurally. Expressing queries in natural
language provides an intuitive way to search for code and lowers the barrier to
entry.
  In this work, we develop a novel general approach that combines the reasoning
capabilities of an LLM to interpret natural language search queries with the
power of structural search tools to efficiently and accurately retrieve
relevant code. We then instantiate this approach for two structural code search
DSLs: Semgrep and GQL. In our evaluation, we construct a new benchmark for
structural code search consisting of 400 queries over 10 Java projects. We show
that our approach for structural code search based on translating NL queries to
DSL queries using an LLM is effective and robust, achieving a high precision
and recall ranging from 55% - 70%. Further, our approach significantly
outperforms baselines based on semantic code search and LLM retrievals by up to
57% and 14% on F1 scores.

</details>


### [114] [Can Internal Software Metrics Predict App Popularity at Launch? Yeas! and Nays!](https://arxiv.org/abs/2507.02110)
*Md Nahidul Islam Opu,Fatima Islam Mouri,Rick Kazman,Yuanfang Cai,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: The study investigates if internal software metrics (from source code) can predict mobile app popularity, attaining improved results with classification models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to help developers strategize by effectively predicting app popularity using pre-release internal metrics, overcoming challenges posed by competitive markets.

Method: A dataset of 446 open-source Android apps from F-Droid was analyzed using various code and metadata metrics. Regression models and binary classification models (e.g., Multilayer Perceptron) evaluated different feature sets.

Result: Regression models yielded poor performance due to skewed data. Binary classification significantly improved results (F1-score of 0.72 with Multilayer Perceptron using Voting set features).

Conclusion: Internal code metrics can moderately predict app popularity, contradicting earlier studies that dismissed these metrics as irrelevant for software quality indicators.

Abstract: Predicting mobile app popularity before release can provide developers with a
strategic advantage in a competitive marketplace, yet it remains a challenging
problem. This study explores whether internal software metrics, measurable from
source code before deployment, can predict an app's popularity, defined by user
ratings (calculated from user reviews) and DownloadsPerYear (yearly downloads).
Using a dataset of 446 open-source Android apps from F-Droid, we extract a wide
array of features, including system-, class-, and method-level code metrics,
code smells, and app metadata. Additional information, such as user reviews,
download counts, and uses-permission, was collected from the Google Play Store.
We evaluate regression and classification models across three feature sets: a
minimal Size-only baseline, a domain-informed Handpicked set, and a Voting set
derived via feature selection algorithms. Regression models perform poorly due
to skewed data, with low $R^2$ scores. However, when reframed as binary
classification (Popular vs. Unpopular), results improve significantly. The best
model, a Multilayer Perceptron using the Voting set, achieves F1-scores of
0.72. These results suggest that internal code metrics, although limited in
their explanatory power, can serve as useful indicators of app popularity. This
challenges earlier findings that dismissed internal metrics as predictors of
software quality.

</details>


### [115] [A Multimodal Approach Combining Biometrics and Self-Report Instruments for Monitoring Stress in Programming: Methodological Insights](https://arxiv.org/abs/2507.02118)
*Cristina Martinez Montes,Daniela Grassi,Nicole Novielli,Birgit Penzenstadle*

Main category: cs.SE

TL;DR: This paper explores the relationship between self-reported stress measures and biometric indicators, finding limited biometric stress evidence during software engineering tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from concerns about biases in self-report instruments for measuring human factors like stress and well-being, prompting interest in objective alternatives such as biometrics.

Method: Participants were surveyed, completed programming tasks with biometric sensors, answered post-task surveys, and participated in exit interviews to compare psychometric and biometric stress measures.

Result: The psychometric tools found no stress, interviews noted mixed feelings, and biometric data revealed significant differences only in EDA phasic peaks.

Conclusion: The stress induction method was insufficient, and the study provides insights for improving methodologies in stress research using psychometric and biometric approaches.

Abstract: The study of well-being, stress and other human factors has traditionally
relied on self-report instruments to assess key variables. However, concerns
about potential biases in these instruments, even when thoroughly validated and
standardised, have driven growing interest in alternatives in combining these
measures with more objective methods, such as physiological measures.
  We aimed to (i) compare psychometric stress measures and biometric indicators
and (ii) identify stress-related patterns in biometric data during software
engineering tasks.
  We conducted an experiment where participants completed a pre-survey, then
programmed two tasks wearing biometric sensors, answered brief post-surveys for
each, and finally went through a short exit interview.
  Our results showed diverse outcomes; we found no stress in the psychometric
instruments. Participants in the interviews reported a mix of feeling no stress
and experiencing time pressure. Finally, the biometrics showed a significant
difference only in EDA phasic peaks.
  We conclude that our chosen way of inducing stress by imposing a stricter
time limit was insufficient. We offer methodological insights for future
studies working with stress, biometrics, and psychometric instruments.

</details>


### [116] [Towards Trustworthy Sentiment Analysis in Software Engineering: Dataset Characteristics and Tool Selection](https://arxiv.org/abs/2507.02137)
*Martin Obaidi,Marc Herrmann,Jil Klünder,Kurt Schneider*

Main category: cs.SE

TL;DR: The paper examines sentiment analysis tools in software engineering and presents a method to recommend suitable tools based on dataset characteristics.


<details>
  <summary>Details</summary>
Motivation: Understand and improve sentiment analysis in the context of software development communication to ensure trustworthy analytics.

Method: Analyzed linguistic and statistical features of 10 developer communication datasets across five platforms; evaluated 14 sentiment analysis tools; proposed a mapping approach and questionnaire to recommend tools.

Result: Transformer models like SetFit and RoBERTa achieve strong results but effectiveness varies with context. Proposed method enhances tool selection based on dataset features.

Conclusion: Dataset characteristics significantly impact sentiment analysis tool performance. A systematic selection approach helps researchers and practitioners use trustworthy tools effectively.

Abstract: Software development relies heavily on text-based communication, making
sentiment analysis a valuable tool for understanding team dynamics and
supporting trustworthy AI-driven analytics in requirements engineering.
However, existing sentiment analysis tools often perform inconsistently across
datasets from different platforms, due to variations in communication style and
content.
  In this study, we analyze linguistic and statistical features of 10 developer
communication datasets from five platforms and evaluate the performance of 14
sentiment analysis tools. Based on these results, we propose a mapping approach
and questionnaire that recommends suitable sentiment analysis tools for new
datasets, using their characteristic features as input.
  Our results show that dataset characteristics can be leveraged to improve
tool selection, as platforms differ substantially in both linguistic and
statistical properties. While transformer-based models such as SetFit and
RoBERTa consistently achieve strong results, tool effectiveness remains
context-dependent. Our approach supports researchers and practitioners in
selecting trustworthy tools for sentiment analysis in software engineering,
while highlighting the need for ongoing evaluation as communication contexts
evolve.

</details>


### [117] [Enhancing COBOL Code Explanations: A Multi-Agents Approach Using Large Language Models](https://arxiv.org/abs/2507.02182)
*Fangjian Lei,Jiawen Liu,Shayan Noei,Ying Zou,Derek Truong,William Alexander*

Main category: cs.SE

TL;DR: The paper addresses the difficulties of explaining COBOL code using LLMs due to its syntactical and architectural challenges, proposing a multi-agent LLM approach to improve code explanation at function, file, and project levels.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the challenges in maintaining COBOL systems due to a declining number of COBOL developers, the language's complexity, and the lack of documentation, making it hard for new developers to understand the code.

Method: A multi-agent approach is proposed where two LLM-based agents collaborate to provide explanations for COBOL code, leveraging contextual information to overcome token window size limitations.

Result: The approach outperforms baselines, showing significant improvements in explaining COBOL code, with metrics like METEOR, chrF, and SentenceBERT improving by 12.67%, 18.59%, and 0.62% at the function level and notable enhancements in file and project-level explanations.

Conclusion: The study concludes that the multi-agent method is effective in addressing the unique challenges of COBOL code by significantly improving explanation quality for functions, files, and projects.

Abstract: Common Business Oriented Language (COBOL) is a programming language used to
develop business applications that are widely adopted by financial, business,
and government agencies. Due to its age, complexity, and declining number of
COBOL developers, maintaining COBOL codebases is becoming increasingly
challenging. In particular, the lack of documentation makes it difficult for
new developers to effectively understand and maintain COBOL systems. Existing
research utilizes large language models (LLMs) to explain the functionality of
code snippets. However, COBOL presents unique challenges due to its
architectural and syntactical differences, which often cause its code to exceed
the token window size of LLMs. In this work, we propose a multi-agent approach
that leverages two LLM-based agents working collaboratively to generate
explanations for functions, files, and the overall project. These agents
incorporate together by utilizing contextual information from the codebase into
the code explanation prompts. We evaluate the effectiveness of our approach
using 14 open-source, real-world COBOL projects. Our results indicate that our
approach performs significantly better than the baseline in function code
explanation, with improvements of 12.67%, 18.59%, and 0.62% in terms of METEOR,
chrF, and SentenceBERT scores, respectively. At the file level, our approach
effectively explains both short and long COBOL files that exceed the token
window size of LLMs and surpass the baseline by 4.21%, 10.72%, and 14.68% in
explaining the purpose, functionality, and clarity of the generated
explanation. At the project level, our approach generates explanations that
convey the functionality and purpose of 82% of the selected projects.

</details>


### [118] [VeFIA: An Efficient Inference Auditing Framework for Vertical Federated Collaborative Software](https://arxiv.org/abs/2507.02376)
*Chung-ju Huang,Ziqi Zhang,Yinggui Wang,Binghui Wang,Tao Wei,Leye Wang*

Main category: cs.SE

TL;DR: The paper presents the VeFIA framework to audit the correctness of inference software in Vertical Federated Learning without compromising data privacy or increasing latency.


<details>
  <summary>Details</summary>
Motivation: Existing Vertical Federated Learning implementations lack mechanisms to verify proper execution of data party inference software.

Method: The VeFIA framework integrates Trusted Execution Environments (TEE) and a coordinator to validate computational correctness while preserving privacy and maintaining latency.

Result: VeFIA detects inference abnormalities above 5.4% with 99.99% accuracy, achieving perfect predictive metrics and true positive rates.

Conclusion: VeFIA successfully audits inference correctness in VFL, addressing a crucial gap in ensuring software accountability without privacy or performance trade-offs.

Abstract: Vertical Federated Learning (VFL) is a distributed AI software deployment
mechanism for cross-silo collaboration without accessing participants' data.
However, existing VFL work lacks a mechanism to audit the execution correctness
of the inference software of the data party. To address this problem, we design
a Vertical Federated Inference Auditing (VeFIA) framework. VeFIA helps the task
party to audit whether the data party's inference software is executed as
expected during large-scale inference without leaking the data privacy of the
data party or introducing additional latency to the inference system. The core
of VeFIA is that the task party can use the inference results from a framework
with Trusted Execution Environments (TEE) and the coordinator to validate the
correctness of the data party's computation results. VeFIA guarantees that, as
long as the abnormal inference exceeds 5.4%, the task party can detect
execution anomalies in the inference software with a probability of 99.99%,
without incurring any additional online inference latency. VeFIA's random
sampling validation achieves 100% positive predictive value, negative
predictive value, and true positive rate in detecting abnormal inference. To
the best of our knowledge, this is the first paper to discuss the correctness
of inference software execution in VFL.

</details>


### [119] [Precisely Detecting Python Type Errors via LLM-based Unit Test Generation](https://arxiv.org/abs/2507.02318)
*Chen Yang,Ziqi Wang,Yanjie Jiang,Lin Yang,Yuteng Zheng,Jianyi Zhou,Junjie Chen*

Main category: cs.SE

TL;DR: RTED presents a novel approach to detect Python type errors using type-aware test generation, outperforming existing techniques in accuracy and precision.


<details>
  <summary>Details</summary>
Motivation: Type errors in Python lead to runtime failures affecting software reliability and productivity; existing tools either produce high false positives or struggle to generate effective test cases.

Method: RTED utilizes step-by-step type constraint analysis coupled with reflective validation to guide test case generation, effectively suppressing false positives.

Result: RTED outperformed four state-of-the-art techniques, detecting up to 29 more type errors and reducing false positives by up to 245.9%; it also identified previously unknown errors in real-world projects.

Conclusion: RTED significantly enhances Python type error detection, improving both coverage and precision, and could aid developers in boosting software reliability.

Abstract: Type errors in Python often lead to runtime failures, posing significant
challenges to software reliability and developer productivity. Existing static
analysis tools aim to detect such errors without execution but frequently
suffer from high false positive rates. Recently, unit test generation
techniques offer great promise in achieving high test coverage, but they often
struggle to produce bug-revealing tests without tailored guidance. To address
these limitations, we present RTED, a novel type-aware test generation
technique for automatically detecting Python type errors. Specifically, RTED
combines step-by-step type constraint analysis with reflective validation to
guide the test generation process and effectively suppress false positives. We
evaluated RTED on two widely-used benchmarks, BugsInPy and TypeBugs.
Experimental results show that RTED can detect 22-29 more benchmarked type
errors than four state-of-the-art techniques. RTED is also capable of producing
fewer false positives, achieving an improvement of 173.9%-245.9% in precision.
Furthermore, RTED successfully discovered 12 previously unknown type errors
from six real-world open-source Python projects.

</details>


### [120] [Meta-Fair: AI-Assisted Fairness Testing of Large Language Models](https://arxiv.org/abs/2507.02533)
*Miguel Romero-Arjona,José A. Parejo,Juan C. Alonso,Ana B. Sánchez,Aitor Arrieta,Sergio Segura*

Main category: cs.SE

TL;DR: This paper introduces Meta-Fair, an automated method for fairness testing in large language models (LLMs), addressing bias detection through metamorphic testing and LLMs' generative and evaluative capabilities.


<details>
  <summary>Details</summary>
Motivation: Fairness in AI remains challenging to evaluate and enforce, with current methods being resource-intensive and limited by manual evaluation or curated resources. The paper seeks to automate and broaden the applicability of fairness testing in LLMs.

Method: The authors propose Meta-Fair, which uses metamorphic testing to identify bias by analyzing response changes to modified input prompts. LLMs help generate test cases and classify outputs, supported by open-source tools.

Result: Through experiments with 12 LLMs, 14 metamorphic relations, and 7.9K test cases, Meta-Fair achieved 92% average precision and detected biases in 29% of cases. Some LLMs showed reliable evaluation with up to 0.79 F1-scores.

Conclusion: Meta-Fair shows promise in automating LLM fairness testing, achieving high bias detection precision and reliability despite challenges like non-determinism, which can be minimized with careful design.

Abstract: Fairness--the absence of unjustified bias--is a core principle in the
development of Artificial Intelligence (AI) systems, yet it remains difficult
to assess and enforce. Current approaches to fairness testing in large language
models (LLMs) often rely on manual evaluation, fixed templates, deterministic
heuristics, and curated datasets, making them resource-intensive and difficult
to scale. This work aims to lay the groundwork for a novel, automated method
for testing fairness in LLMs, reducing the dependence on domain-specific
resources and broadening the applicability of current approaches. Our approach,
Meta-Fair, is based on two key ideas. First, we adopt metamorphic testing to
uncover bias by examining how model outputs vary in response to controlled
modifications of input prompts, defined by metamorphic relations (MRs). Second,
we propose exploiting the potential of LLMs for both test case generation and
output evaluation, leveraging their capability to generate diverse inputs and
classify outputs effectively. The proposal is complemented by three open-source
tools supporting LLM-driven generation, execution, and evaluation of test
cases. We report the findings of several experiments involving 12 pre-trained
LLMs, 14 MRs, 5 bias dimensions, and 7.9K automatically generated test cases.
The results show that Meta-Fair is effective in uncovering bias in LLMs,
achieving an average precision of 92% and revealing biased behaviour in 29% of
executions. Additionally, LLMs prove to be reliable and consistent evaluators,
with the best-performing models achieving F1-scores of up to 0.79. Although
non-determinism affects consistency, these effects can be mitigated through
careful MR design. While challenges remain to ensure broader applicability, the
results indicate a promising path towards an unprecedented level of automation
in LLM testing.

</details>


### [121] [LLMREI: Automating Requirements Elicitation Interviews with LLMs](https://arxiv.org/abs/2507.02564)
*Alexander Korn,Samuel Gorsch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: This paper introduces LLMREI, a chatbot leveraging Large Language Models for conducting requirements elicitation interviews, aiming to reduce errors and improve scalability compared to human interviewers.


<details>
  <summary>Details</summary>
Motivation: Reduce reliance on skilled analysts in requirements elicitation interviews, minimize human biases and errors, and leverage recent advancements in Large Language Models to automate the interview process.

Method: Developed LLMREI chatbot using zero-shot and least-to-most prompting approaches, evaluated its performance in 33 simulated interviews, and abandoned fine-tuning methods after poor preliminary results.

Result: LLMREI demonstrated error rates similar to human interviewers, extracted a substantial portion of requirements, and generated context-dependent questions effectively.

Conclusion: LLMREI shows significant potential in automating interviews, particularly in scenarios involving large numbers of stakeholders, though its error rates remain comparable to human interviewers.

Abstract: Requirements elicitation interviews are crucial for gathering system
requirements but heavily depend on skilled analysts, making them
resource-intensive, susceptible to human biases, and prone to miscommunication.
Recent advancements in Large Language Models present new opportunities for
automating parts of this process. This study introduces LLMREI, a chat bot
designed to conduct requirements elicitation interviews with minimal human
intervention, aiming to reduce common interviewer errors and improve the
scalability of requirements elicitation. We explored two main approaches,
zero-shot prompting and least-to-most prompting, to optimize LLMREI for
requirements elicitation and evaluated its performance in 33 simulated
stakeholder interviews. A third approach, fine-tuning, was initially considered
but abandoned due to poor performance in preliminary trials. Our study assesses
the chat bot's effectiveness in three key areas: minimizing common interview
errors, extracting relevant requirements, and adapting its questioning based on
interview context and user responses. Our findings indicate that LLMREI makes a
similar number of errors compared to human interviewers, is capable of
extracting a large portion of requirements, and demonstrates a notable ability
to generate highly context-dependent questions. We envision the greatest
benefit of LLMREI in automating interviews with a large number of stakeholders.

</details>


### [122] [Human-Machine Collaboration and Ethical Considerations in Adaptive Cyber-Physical Systems](https://arxiv.org/abs/2507.02578)
*Zoe Pfister*

Main category: cs.SE

TL;DR: This paper addresses the integration of Human-Machine Teaming (HMT) into Adaptive Cyber-Physical Systems (CPS) by developing methods for seamless interaction and ethical frameworks.


<details>
  <summary>Details</summary>
Motivation: Adaptive CPS increasingly involve Human-Machine Teaming, raising the need for seamless collaboration while addressing operational cadences and respecting human values and privacy.

Method: The research introduces novel methods for integrating HMT into adaptive CPS feedback loops and frameworks for incorporating ethics and human values into system lifecycles, starting at requirements engineering.

Result: The result includes improved human-machine interaction methods and processes, alongside ethical frameworks for CPS design and operation.

Conclusion: Integrating HMT into adaptive CPS is accomplished through innovative methods and ethical frameworks, ensuring privacy and human values are respected.

Abstract: Adaptive Cyber-Physical Systems (CPS) are systems that integrate both
physical and computational capabilities, which can adjust in response to
changing parameters. Furthermore, they increasingly incorporate human-machine
collaboration, allowing them to benefit from the individual strengths of humans
and machines. Human-Machine Teaming (HMT) represents the most advanced paradigm
of human-machine collaboration, envisioning seamless teamwork between humans
and machines. However, achieving effective and seamless HMT in adaptive CPS is
challenging. While adaptive CPS already benefit from feedback loops such as
MAPE-K, there is still a gap in integrating humans into these feedback loops
due to different operational cadences of humans and machines. Further, HMT
requires constant monitoring of human operators, collecting potentially
sensitive information about their actions and behavior. Respecting the privacy
and human values of the actors of the CPS is crucial for the success of
human-machine teams. This research addresses these challenges by: (1)
developing novel methods and processes for integrating HMT into adaptive CPS,
focusing on human-machine interaction principles and their incorporation into
adaptive feedback loops found in CPS, and (2) creating frameworks for
integrating, verifying, and validating ethics and human values throughout the
system lifecycle, starting from requirements engineering.

</details>


### [123] [Do Research Software Engineers and Software Engineering Researchers Speak the Same Language?](https://arxiv.org/abs/2507.02665)
*Timo Kehrer,Robert Haines,Guido Juckeland,Shurui Zhou,David E. Bernholdt*

Main category: cs.SE

TL;DR: The paper investigates the terminology differences between Research Software Engineers (RSEs) and Software Engineering Researchers (SERs), proposing a systematic approach to bridge gaps and foster collaboration.


<details>
  <summary>Details</summary>
Motivation: To address communication challenges between RSEs and SERs caused by divergent terminologies and understand how software engineering fundamentals are interpreted within the RSE community.

Method: A systematic terminology mapping methodology was employed to identify aligned concepts, knowledge gaps, and potential adaptations between the RSE and SER communities.

Result: The study revealed opportunities for mutual learning and collaboration between the two fields.

Conclusion: A foundation is provided for future crowd-sourced extension and validation to improve communication and collaboration between RSEs and SERs.

Abstract: Anecdotal evidence suggests that Research Software Engineers (RSEs) and
Software Engineering Researchers (SERs) often use different terminologies for
similar concepts, creating communication challenges. To better understand these
divergences, we have started investigating how SE fundamentals from the SER
community are interpreted within the RSE community, identifying aligned
concepts, knowledge gaps, and areas for potential adaptation. Our preliminary
findings reveal opportunities for mutual learning and collaboration, and our
systematic methodology for terminology mapping provides a foundation for a
crowd-sourced extension and validation in the future.

</details>


### [124] [RLHGNN: Reinforcement Learning-driven Heterogeneous Graph Neural Network for Next Activity Prediction in Business Processes](https://arxiv.org/abs/2507.02690)
*Jiaxing Wang,Yifeng Yu,Jiahan Song,Bin Cao,Jing Fan,Ji Zhang*

Main category: cs.SE

TL;DR: This paper introduces RLHGNN, a framework for predicting next activities in complex business processes by transforming event logs into heterogeneous process graphs, achieving superior performance and practicality.


<details>
  <summary>Details</summary>
Motivation: Existing sequence-based methods fail to capture non-sequential relationships, while graph-based methods suffer from homogeneous representations and static structures, both limiting their effectiveness in accurately modeling real-world process complexities.

Method: RLHGNN transforms event logs into heterogeneous process graphs with three edge types and selectively combines these edges into four graph structures. Reinforcement learning identifies the optimal graph structure for specific instances, and heterogeneous graph convolution is applied for prediction.

Result: RLHGNN consistently outperforms state-of-the-art approaches across six real-world datasets and achieves an inference latency of approximately 1 ms per prediction, suitable for real-time applications.

Conclusion: RLHGNN is a novel, efficient framework that enables precise modeling of complex business processes, integrating sequential and non-sequential relationships for real-time applications.

Abstract: Next activity prediction represents a fundamental challenge for optimizing
business processes in service-oriented architectures such as microservices
environments, distributed enterprise systems, and cloud-native platforms, which
enables proactive resource allocation and dynamic service composition. Despite
the prevalence of sequence-based methods, these approaches fail to capture
non-sequential relationships that arise from parallel executions and
conditional dependencies. Even though graph-based approaches address structural
preservation, they suffer from homogeneous representations and static
structures that apply uniform modeling strategies regardless of individual
process complexity characteristics. To address these limitations, we introduce
RLHGNN, a novel framework that transforms event logs into heterogeneous process
graphs with three distinct edge types grounded in established process mining
theory. Our approach creates four flexible graph structures by selectively
combining these edges to accommodate different process complexities, and
employs reinforcement learning formulated as a Markov Decision Process to
automatically determine the optimal graph structure for each specific process
instance. RLHGNN then applies heterogeneous graph convolution with
relation-specific aggregation strategies to effectively predict the next
activity. This adaptive methodology enables precise modeling of both sequential
and non-sequential relationships in service interactions. Comprehensive
evaluation on six real-world datasets demonstrates that RLHGNN consistently
outperforms state-of-the-art approaches. Furthermore, it maintains an inference
latency of approximately 1 ms per prediction, representing a highly practical
solution suitable for real-time business process monitoring applications. The
source code is available at https://github.com/Joker3993/RLHGNN.

</details>


### [125] [Sustainability Flags for the Identification of Sustainability Posts in Q&A Platforms](https://arxiv.org/abs/2507.02695)
*Sahar Ahmadisakha,Lech Bialek,Mohamed Soliman,Vasilios Andrikopoulos*

Main category: cs.SE

TL;DR: The paper proposes sustainability flags derived from cloud practices to recognize sustainability discussions in software architecture posts, achieving improved performance and usability.


<details>
  <summary>Details</summary>
Motivation: The growing emphasis on sustainability in software systems, particularly in cloud computing, necessitates methods to guide architectural decisions via sustainability identification, which currently lacks clear guidelines.

Method: Thematic analysis of cloud provider best practices was used to develop sustainability flags; these were then evaluated through a controlled experiment analyzing cloud architecture discussion posts.

Result: Using sustainability flags reduces the number of posts classified as sustainability-related but improves classification certainty, performance, and usability compared to traditional definitions.

Conclusion: Sustainability flags hold promise for more effective and understandable identification of sustainability aspects in software architecture discussions compared to generic definitions.

Abstract: In recent years, sustainability in software systems has gained significant
attention, especially with the rise of cloud computing and the shift towards
cloud-based architectures. This shift has intensified the need to identify
sustainability in architectural discussions to take informed architectural
decisions. One source to see these decisions is in online Q&A forums among
practitioners' discussions. However, recognizing sustainability concepts within
software practitioners' discussions remains challenging due to the lack of
clear and distinct guidelines for this task. To address this issue, we
introduce the notion of sustainability flags as pointers in relevant
discussions, developed through thematic analysis of multiple sustainability
best practices from cloud providers. This study further evaluates the
effectiveness of these flags in identifying sustainability within cloud
architecture posts, using a controlled experiment. Preliminary results suggest
that the use of flags results in classifying fewer posts as
sustainability-related compared to a control group, with moderately higher
certainty and significantly improved performance. Moreover, sustainability
flags are perceived as more useful and understandable than relying solely on
definitions for identifying sustainability.

</details>


### [126] [Legal Requirements Translation from Law](https://arxiv.org/abs/2507.02846)
*Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: The paper proposes an automated method to process legal text for software compliance using a Python-based representation, achieving notable accuracy and avoiding heavy reliance on manual data labeling.


<details>
  <summary>Details</summary>
Motivation: Extracting legal requirements from regulations is challenging for small organizations due to the complexity of legal text and a lack of dedicated legal expertise.

Method: The researchers use textual entailment and in-context learning to transform legal texts into a Python-executable canonical representation based on a designed Python class structure as a domain-specific metamodel.

Result: Their method achieved approximately 89.4% correctness in test cases and a precision and recall of 82.2% and 88.7%, respectively, when tested on 13 U.S. state data breach notification laws.

Conclusion: The approach minimizes the need for manual dataset labeling while providing an accurate and efficient way to process structural and semantic metadata from legal texts, ensuring better compliance for software systems.

Abstract: Software systems must comply with legal regulations, which is a
resource-intensive task, particularly for small organizations and startups
lacking dedicated legal expertise. Extracting metadata from regulations to
elicit legal requirements for software is a critical step to ensure compliance.
However, it is a cumbersome task due to the length and complex nature of legal
text. Although prior work has pursued automated methods for extracting
structural and semantic metadata from legal text, key limitations remain: they
do not consider the interplay and interrelationships among attributes
associated with these metadata types, and they rely on manual labeling or
heuristic-driven machine learning, which does not generalize well to new
documents. In this paper, we introduce an approach based on textual entailment
and in-context learning for automatically generating a canonical representation
of legal text, encodable and executable as Python code. Our representation is
instantiated from a manually designed Python class structure that serves as a
domain-specific metamodel, capturing both structural and semantic legal
metadata and their interrelationships. This design choice reduces the need for
large, manually labeled datasets and enhances applicability to unseen
legislation. We evaluate our approach on 13 U.S. state data breach notification
laws, demonstrating that our generated representations pass approximately 89.4%
of test cases and achieve a precision and recall of 82.2 and 88.7,
respectively.

</details>


### [127] [Requirements Elicitation Follow-Up Question Generation](https://arxiv.org/abs/2507.02858)
*Yuchen Shen,Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: The paper investigates using GPT-4 in generating follow-up interview questions for requirements elicitation, showing that GPT-4-generated questions rival, and sometimes exceed, human-generated questions when guided by common mistake types.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in requirements elicitation interviews, such as domain unfamiliarity, cognitive load, and information processing, by leveraging advancements in large language models (LLMs) like GPT-4.

Method: The researchers applied GPT-4 to generate follow-up interview questions based on a framework for common interviewer mistake types, compared its outputs to human-generated questions in controlled experiments, and evaluated them on clarity, relevancy, and informativeness.

Result: LLM-generated questions performed comparably to human-authored questions in unguided scenarios, and surpassed human-authored questions when guided by interviewer mistake types.

Conclusion: LLMs, specifically GPT-4, have promising potential to enhance the quality and efficiency of requirements elicitation interviews by assisting interviewers with real-time question formulation.

Abstract: Interviews are a widely used technique in eliciting requirements to gather
stakeholder needs, preferences, and expectations for a software system.
Effective interviewing requires skilled interviewers to formulate appropriate
interview questions in real time while facing multiple challenges, including
lack of familiarity with the domain, excessive cognitive load, and information
overload that hinders how humans process stakeholders' speech. Recently, large
language models (LLMs) have exhibited state-of-the-art performance in multiple
natural language processing tasks, including text summarization and entailment.
To support interviewers, we investigate the application of GPT-4o to generate
follow-up interview questions during requirements elicitation by building on a
framework of common interviewer mistake types. In addition, we describe methods
to generate questions based on interviewee speech. We report a controlled
experiment to evaluate LLM-generated and human-authored questions with minimal
guidance, and a second controlled experiment to evaluate the LLM-generated
questions when generation is guided by interviewer mistake types. Our findings
demonstrate that, for both experiments, the LLM-generated questions are no
worse than the human-authored questions with respect to clarity, relevancy, and
informativeness. In addition, LLM-generated questions outperform human-authored
questions when guided by common mistakes types. This highlights the potential
of using LLMs to help interviewers improve the quality and ease of requirements
elicitation interviews in real time.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [128] [Alignment between Brains and AI: Evidence for Convergent Evolution across Modalities, Scales and Training Trajectories](https://arxiv.org/abs/2507.01966)
*Guobin Shen,Dongcheng Zhao,Yiting Dong,Qian Zhang,Yi Zeng*

Main category: q-bio.NC

TL;DR: The study finds that higher-performing AI models naturally align more closely with human brain patterns, suggesting brain-like representations may enhance AI capabilities.


<details>
  <summary>Details</summary>
Motivation: To investigate whether artificial and biological systems evolve similar computational strategies despite different architectures and to understand the relationship between AI task performance and brain-like alignments.

Method: The researchers examined over 600 AI models (spanning language and vision domains) using large-scale analysis of 60 million brain-activity and AI alignment measurements. They also conducted longitudinal analysis observing alignment and performance changes during training.

Result: Higher-performing AI models showed greater brain alignment, with stronger correlations in language models (r=0.89) than vision models (r=0.53). Patterns emerged linking alignment with specific brain regions and layers of processing, showing a progression as model scales increased.

Conclusion: Optimizing task performance in AI systems naturally leads to brain-like computational strategies, offering insights into intelligent processing and providing a framework for advancing AI capabilities.

Abstract: Artificial and biological systems may evolve similar computational solutions
despite fundamental differences in architecture and learning mechanisms -- a
form of convergent evolution. We demonstrate this phenomenon through
large-scale analysis of alignment between human brain activity and internal
representations of over 600 AI models spanning language and vision domains,
from 1.33M to 72B parameters. Analyzing 60 million alignment measurements
reveals that higher-performing models spontaneously develop stronger brain
alignment without explicit neural constraints, with language models showing
markedly stronger correlation (r=0.89, p<7.5e-13) than vision models (r=0.53,
p<2.0e-44). Crucially, longitudinal analysis demonstrates that brain alignment
consistently precedes performance improvements during training, suggesting that
developing brain-like representations may be a necessary stepping stone toward
higher capabilities. We find systematic patterns: language models exhibit
strongest alignment with limbic and integrative regions, while vision models
show progressive alignment with visual cortices; deeper processing layers
converge across modalities; and as representational scale increases, alignment
systematically shifts from primary sensory to higher-order associative regions.
These findings provide compelling evidence that optimization for task
performance naturally drives AI systems toward brain-like computational
strategies, offering both fundamental insights into principles of intelligent
information processing and practical guidance for developing more capable AI
systems.

</details>


### [129] [Ghost in the Machine: Examining the Philosophical Implications of Recursive Algorithms in Artificial Intelligence Systems](https://arxiv.org/abs/2507.01967)
*Llewellin RG Jegels*

Main category: q-bio.NC

TL;DR: The paper assesses the possibility of machine consciousness in current AI systems utilizing recursion, meta-learning, and self-referential mechanisms, concluding that they lack subjective experience.


<details>
  <summary>Details</summary>
Motivation: The study aims to clarify whether advanced AI architectures demonstrate machine consciousness, addressing unresolved challenges like symbol grounding and affective qualia.

Method: Integrating philosophical, cognitive science, and engineering perspectives, the study uses conceptual analysis, textual review, case studies, and neuroscientific synthesis.

Result: The paper finds that while AI systems can achieve functional self-modeling, they do not exhibit phenomenal consciousness, posing ethical and legal challenges but suggesting future research directions.

Conclusion: Recursive self-referential designs improve AI capabilities but fail to equate to consciousness, leaving moral status and other related debates unresolved.

Abstract: This paper investigates whether contemporary AI architectures employing deep
recursion, meta-learning, and self-referential mechanisms provide evidence of
machine consciousness. Integrating philosophical history, cognitive science,
and AI engineering, it situates recursive algorithms within a lineage spanning
Cartesian dualism, Husserlian intentionality, Integrated Information Theory,
the Global Workspace model, and enactivist perspectives. The argument proceeds
through textual analysis, comparative architecture review, and synthesis of
neuroscience findings on integration and prediction. Methodologically, the
study combines conceptual analysis, case studies, and normative risk assessment
informed by phenomenology and embodied cognition. Technical examples, including
transformer self-attention, meta-cognitive agents, and neuromorphic chips,
illustrate how functional self-modeling can arise without subjective
experience. By distinguishing functional from phenomenal consciousness, the
paper argues that symbol grounding, embodiment, and affective qualia remain
unresolved barriers to attributing sentience to current AI. Ethical analysis
explores risks of premature anthropomorphism versus neglect of future sentient
systems; legal implications include personhood, liability, authorship, and
labor impacts. Future directions include quantum architectures, embodied
robotics, unsupervised world modeling, and empirical tests for non-biological
phenomenality. The study reframes the "hard problem" as a graded and
increasingly testable phenomenon, rather than a metaphysical impasse. It
concludes that recursive self-referential design enhances capability but does
not entail consciousness or justify moral status. Keywords: Recursive
algorithms; self-reference; machine consciousness; AI ethics; AI consciousness

</details>


### [130] [REMI: Reconstructing Episodic Memory During Intrinsic Path Planning](https://arxiv.org/abs/2507.02064)
*Zhaoze Wang,Genela Morris,Dori Derdikman,Pratik Chaudhari,Vijay Balasubramanian*

Main category: q-bio.NC

TL;DR: This paper explores how both grid cells in the medial entorhinal cortex (MEC) and place cells in the hippocampus (HC) contribute to spatial navigation and planning, proposing a unified framework.


<details>
  <summary>Details</summary>
Motivation: Although grid and place cells both support spatial memory and information processing, it remains unclear why animals use two distinct spatial representations.

Method: The authors developed a single-layer recurrent neural network (RNN) model to simulate the HC-MEC loop and planning network. This model incorporates interactions between grid and place cells for goal-directed planning.

Result: The study demonstrates that local transition rules can generalize to long-distance planning and that sequential updates in the grid cell network can assist in path forecasting. Place cell activity complements this process by reconstructing experiences along the selected path.

Conclusion: Grid and place cells synergistically support planning by leveraging their distinct yet interrelated roles in spatial representation. The proposed mechanism also makes specific, testable predictions regarding their functional interactions.

Abstract: Grid cells in the medial entorhinal cortex (MEC) are believed to path
integrate speed and direction signals to activate at triangular grids of
locations in an environment, thus implementing a population code for position.
In parallel, place cells in the hippocampus (HC) fire at spatially confined
locations, with selectivity tuned not only to allocentric position but also to
environmental contexts, such as sensory cues. Although grid and place cells
both encode spatial information and support memory for multiple locations, why
animals maintain two such representations remains unclear. Noting that place
representations seem to have other functional roles in intrinsically motivated
tasks such as recalling locations from sensory cues, we propose that animals
maintain grid and place representations together to support planning.
Specifically, we posit that place cells auto-associate not only sensory
information relayed from the MEC but also grid cell patterns, enabling recall
of goal location grid patterns from sensory and motivational cues, permitting
subsequent planning with only grid representations. We extend a previous
theoretical framework for grid-cell-based planning and show that local
transition rules can generalize to long-distance path forecasting. We further
show that a planning network can sequentially update grid cell states toward
the goal. During this process, intermediate grid activity can trigger place
cell pattern completion, reconstructing experiences along the planned path. We
demonstrate all these effects using a single-layer RNN that simultaneously
models the HC-MEC loop and the planning subnetwork. We show that such recurrent
mechanisms for grid cell-based planning, with goal recall driven by the place
system, make several characteristic, testable predictions.

</details>


### [131] [NLP4Neuro: Sequence-to-sequence learning for neural population decoding](https://arxiv.org/abs/2507.02264)
*Jacob J. Morra,Kaitlyn E. Fouke,Kexin Hang,Zichen He,Owen Traubert,Timothy W. Dunn,Eva A. Naumann*

Main category: q-bio.NC

TL;DR: This paper evaluates the use of pre-trained large language models (LLMs) for decoding animal behavior from neural activity across the brain, finding them effective, especially when using models like DeepSeek Coder-7b.


<details>
  <summary>Details</summary>
Motivation: The study aims to tackle the challenge of understanding how neural activity across densely connected, large mammalian brains leads to specific behaviors, by leveraging the capabilities of LLMs that excel in sequence-to-sequence learning and generalization.

Method: The authors systematically evaluated off-the-shelf pre-trained LLMs, termed NLP4Neuro, utilizing them to decode behavior from brain-wide neural activity recorded through calcium imaging in larval zebrafish exposed to visual stimuli.

Result: They found that pre-trained LLMs, especially DeepSeek Coder-7b, enhanced decoding accuracy of behavior, predicted long-timescale tail movements, and offered interpretable neuron salience maps with anatomical consistency.

Conclusion: LLMs, especially advanced models like DeepSeek Coder-7b, show promising capabilities in decoding complex neural activity and offer a novel approach for brain-wide neural circuit analysis.

Abstract: Delineating how animal behavior arises from neural activity is a foundational
goal of neuroscience. However, as the computations underlying behavior unfold
in networks of thousands of individual neurons across the entire brain, this
presents challenges for investigating neural roles and computational mechanisms
in large, densely wired mammalian brains during behavior. Transformers, the
backbones of modern large language models (LLMs), have become powerful tools
for neural decoding from smaller neural populations. These modern LLMs have
benefited from extensive pre-training, and their sequence-to-sequence learning
has been shown to generalize to novel tasks and data modalities, which may also
confer advantages for neural decoding from larger, brain-wide activity
recordings. Here, we present a systematic evaluation of off-the-shelf LLMs to
decode behavior from brain-wide populations, termed NLP4Neuro, which we used to
test LLMs on simultaneous calcium imaging and behavior recordings in larval
zebrafish exposed to visual motion stimuli. Through NLP4Neuro, we found that
LLMs become better at neural decoding when they use pre-trained weights learned
from textual natural language data. Moreover, we found that a recent
mixture-of-experts LLM, DeepSeek Coder-7b, significantly improved behavioral
decoding accuracy, predicted tail movements over long timescales, and provided
anatomically consistent highly interpretable readouts of neuron salience.
NLP4Neuro demonstrates that LLMs are highly capable of informing brain-wide
neural circuit dissection.

</details>


### [132] [Nonlinear Network Reconstruction by Pairwise Time-delayed Transfer Entropy](https://arxiv.org/abs/2507.02304)
*Kai Chen,Zhong-qi K. Tian,Yifei Chen,Songting Li,Douglas Zhou*

Main category: q-bio.NC

TL;DR: This study introduces a pairwise time-delayed transfer entropy (PTD-TE) method to reconstruct network structural connectivity more accurately and robustly, addressing limitations of traditional transfer entropy methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods like transfer entropy face challenges such as high-dimensional probability estimation and weak understanding of causal and structural connectivity relationships.

Method: The study proposes PTD-TE, an analytical framework that establishes a quadratic relationship between PTD-TE values and coupling strengths, improving on dimensionality issues of standard TE.

Result: Performance tests show PTD-TE consistently reconstructs networks accurately and maintains robustness against noise across biological, physical, and electrophysiological systems.

Conclusion: PTD-TE offers a scalable and versatile tool for inferring structural connectivity in nonlinear real-world networks, outperforming traditional approaches.

Abstract: Analyzing network structural connectivity is crucial for understanding
dynamics and functions of complex networks across disciplines. In many
networks, structural connectivity is not observable, which requires to be
inferred via causal inference methods. Among them, transfer entropy (TE) is one
of the most broadly applied causality measure due to its model-free property.
However, TE often faces the curse of dimensionality in high-dimensional
probability estimation, and the relation between the inferred causal
connectivity and the underlying structural connectivity remains poorly
understood. Here we address these issues by proposing a pairwise time-delayed
transfer entropy (PTD-TE) method. We theoretically establish a quadratic
relationship between PTD-TE values and node coupling strengths, and demonstrate
its immunity to dimensionality issues and broad applicability. Tests on
biological neuronal networks, nonlinear physical systems, and
electrophysiological data show PTD-TE achieves consistent, high-performance
reconstructions. Compared to a bunch of existing approaches for network
connectivity reconstruction, PTD-TE outperforms these methods across various
network systems in accuracy and robustness against noise. Our framework
provides a scalable, model-agnostic tool for structural connectivity inference
in nonlinear real-world networks.

</details>


### [133] [Network structural change point detection and reconstruction for balanced neuronal networks](https://arxiv.org/abs/2507.02450)
*Kai Chen,Mingzhang Wang,Songting Li,Douglas Zhou*

Main category: q-bio.NC

TL;DR: The paper proposes a novel method for reconstructing dynamic neuronal networks undergoing structural changes using a CPD-TDCC framework, addressing prior limitations tied to static connectivity assumptions.


<details>
  <summary>Details</summary>
Motivation: Conventional methods for inferring brain network connectivity struggle with dynamic changes and modifications in structural connectivity, limiting their applicability to real-world neuronal networks.

Method: The authors introduce a unified framework that combines connectivity-induced change point detection (CPD) with time-delayed correlation coefficient (TDCC) analysis to identify structural changes and reconstruct the network using segmented neuronal spike train recordings.

Result: Simulation experiments with various neuronal models showed the framework accurately reconstructed networks undergoing changes in topology and synaptic coupling strength, even with sparsely sampled subnetwork data.

Conclusion: The CPD-TDCC framework represents an innovative approach to address dynamic changes in brain connectivity, making it suitable for practical experimental applications in neurobiology.

Abstract: Understanding brain dynamics and functions critically depends on knowledge of
the network connectivity among neurons. However, the complexity of brain
structural connectivity, coupled with continuous modifications driven by
synaptic plasticity, makes its direct experimental measurement particularly
challenging. Conventional connectivity inference methods based on neuronal
recordings often assumes a static underlying structural connectivity and
requires stable statistical features of neural activities, making them
unsuitable for reconstructing structural connectivity that undergoes changes.
To fulfill the needs of reconstructing networks undergoing potential structural
changes, we propose a unified network reconstruction framework that combines
connectivity-induced change point detection (CPD) with pairwise time-delayed
correlation coefficient (TDCC) method. For general neuronal networks in
balanced regimes, we develop a theoretical analysis for discriminating changes
in structural connectivity based on the fluctuation of neuronal voltage time
series. We then demonstrate a pairwise TDCC method to reconstruct the network
using spike train recordings segmented at the detected change points. We show
the effectiveness of our CPD-TDCC network reconstruction using large-scale
network simulations with multiple neuronal models. Crucially, our method
accommodates networks with changes in both network topologies and synaptic
coupling strengths while retaining accuracy even with sparsely sampled
subnetwork data, achieving a critical advancement for practical applications in
real experimental situations. Our CPD-TDCC framework addresses the critical gap
in network reconstruction by accounting connectivity-induced changes points,
potentially offering a valuable tool for studying structure and dynamics in the
cortical brain.

</details>


### [134] [A Dopamine-Serotonin Theory of Consciousness](https://arxiv.org/abs/2507.02614)
*Diogo Sousa*

Main category: q-bio.NC

TL;DR: A mathematical theory of consciousness explains its neurochemical basis, focusing on dopamine and serotonin roles, validated by clinical data.


<details>
  <summary>Details</summary>
Motivation: There is a lack of comprehensive theoretical explanations for diverse conscious states across neurochemical dynamics.

Method: The study proposes a formalized neurochemical model and tests it using clinical data from Parkinson's disease patients, analyzing interactions between disease severity and dopaminergic medication.

Result: The analysis confirms the hypothesis, showing a significant interaction between dopamine input and disease severity, supporting state-dependent consciousness dynamics.

Conclusion: The proposed framework successfully accounts for varying conscious states and explains several paradoxical phenomena in neuroscience and pharmacology.

Abstract: This work presents a comprehensive theory of consciousness grounded in
mathematical formalism and supported by clinical data analysis. The framework
developed herein demonstrates that consciousness exists as a continuous,
non-monotonic function across a high-dimensional neurochemical space, with
dopamine serving as the primary intensity regulator and serotonin (5-HT2A) as
the complexity modulator. This work offers mechanistic explanations for the
full spectrum of conscious states, from deep sleep and psychosis to the
ultimate collapse in neural death. The theory explains paradoxical phenomena
such as prefrontal cortex hypoactivity during seizures, the evolutionary
persistence of psychosis-prone individuals, and why controlled administration
of classical 5-HT2A agonists shows a comparatively low incidence of serious
medical events (< 0.01 % in modern clinical trials), while dopaminergic excess
proves rapidly lethal. The framework is tested using 70,290 sleep nights from
242 Parkinson's disease patients, using disease severity (UPDRS) as a proxy for
system integrity and medication (LEDD) as a proxy for dopaminergic input. The
analysis reveals a significant LEDD x UPDRS interaction (beta=-1.7, p<.0001),
confirming the model's prediction of state-dependent, non-linear dynamics.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [135] [Adaptive Iterative Soft-Thresholding Algorithm with the Median Absolute Deviation](https://arxiv.org/abs/2507.02084)
*Yining Feng,Ivan Selesnick*

Main category: stat.ML

TL;DR: This paper examines the adaptive ISTA for solving the LASSO problem, focusing on theoretical properties such as fixed-point properties, local linear convergence, and global behavior.


<details>
  <summary>Details</summary>
Motivation: Despite the practical success of adaptive ISTA in solving LASSO problems without explicitly tuning the regularization parameter, its theoretical understanding is limited.

Method: The authors analyze adaptive ISTA using the median absolute deviation thresholding strategy, exploring fixed-point properties, local linear convergence, and global convergence.

Result: The paper proves properties of fixed points (e.g., scale equivariance, non-uniqueness, local stability), guarantees local linear convergence, and discusses its global behavior.

Conclusion: The analysis provides theoretical insights into adaptive ISTA's functionality, addressing gaps in its understanding and offering guarantees for its performance.

Abstract: The adaptive Iterative Soft-Thresholding Algorithm (ISTA) has been a popular
algorithm for finding a desirable solution to the LASSO problem without
explicitly tuning the regularization parameter $\lambda$. Despite that the
adaptive ISTA is a successful practical algorithm, few theoretical results
exist. In this paper, we present the theoretical analysis on the adaptive ISTA
with the thresholding strategy of estimating noise level by median absolute
deviation. We show properties of the fixed points of the algorithm, including
scale equivariance, non-uniqueness, and local stability, prove the local linear
convergence guarantee, and show its global convergence behavior.

</details>


### [136] [Hybrid least squares for learning functions from highly noisy data](https://arxiv.org/abs/2507.02215)
*Ben Adcock,Bernhard Hientzsch,Akil Narayan,Yiming Xu*

Main category: stat.ML

TL;DR: This paper proposes a new hybrid algorithm combining Christoffel sampling and optimal experimental design for least-squares function approximation under heavy noise, yielding improved efficiency and theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Efficiently estimating conditional expectations under heavily polluted data remains challenging, as current methods excel in small noise scenarios but falter under larger noise.

Method: A hybrid approach is developed by integrating Christoffel sampling with optimal experimental design. The algorithm is extended to convex-constrained settings and adaptive random subspaces.

Result: The proposed method improves computational efficiency and sample complexity, while theoretical findings are validated using numerical studies in synthetic and computational finance datasets.

Conclusion: The hybrid approach addresses heavy noise limitations, ensures theoretical guarantees, and demonstrates practical utility across different applications.

Abstract: Motivated by the need for efficient estimation of conditional expectations,
we consider a least-squares function approximation problem with heavily
polluted data. Existing methods that are powerful in the small noise regime are
suboptimal when large noise is present. We propose a hybrid approach that
combines Christoffel sampling with certain types of optimal experimental design
to address this issue. We show that the proposed algorithm enjoys appropriate
optimality properties for both sample point generation and noise mollification,
leading to improved computational efficiency and sample complexity compared to
existing methods. We also extend the algorithm to convex-constrained settings
with similar theoretical guarantees. When the target function is defined as the
expectation of a random field, we extend our approach to leverage adaptive
random subspaces and establish results on the approximation capacity of the
adaptive procedure. Our theoretical findings are supported by numerical studies
on both synthetic data and on a more challenging stochastic simulation problem
in computational finance.

</details>


### [137] [Transfer Learning for Matrix Completion](https://arxiv.org/abs/2507.02248)
*Dali Liu,Haolei Weng*

Main category: stat.ML

TL;DR: The paper introduces a transfer learning approach for matrix completion to improve low-rank matrix estimation using auxiliary data, proving its theoretical effectiveness and practical utility.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy of estimating low-rank matrices by effectively utilizing relevant auxiliary data in matrix completion.

Method: The paper proposes a transfer learning procedure leveraging favorable source datasets and addresses model convergence rates, using advanced sharp concentration inequalities. Additionally, a detection procedure identifies relevant source datasets when prior information is absent.

Result: The proposed method outperforms traditional single target methods when source matrices are sufficiently related to the target, achieving minimax optimality in convergence rates and selection consistency for source datasets.

Conclusion: The methodology demonstrates theoretical robustness and practical feasibility, supported by simulations and real data analysis, establishing its superiority in matrix completion tasks with relevant auxiliary data.

Abstract: In this paper, we explore the knowledge transfer under the setting of matrix
completion, which aims to enhance the estimation of a low-rank target matrix
with auxiliary data available. We propose a transfer learning procedure given
prior information on which source datasets are favorable. We study its
convergence rates and prove its minimax optimality. Our analysis reveals that
with the source matrices close enough to the target matrix, out method
outperforms the traditional method using the single target data. In particular,
we leverage the advanced sharp concentration inequalities introduced in
\cite{brailovskaya2024universality} to eliminate a logarithmic factor in the
convergence rate, which is crucial for proving the minimax optimality. When the
relevance of source datasets is unknown, we develop an efficient detection
procedure to identify informative sources and establish its selection
consistency. Simulations and real data analysis are conducted to support the
validity of our methodology.

</details>


### [138] [It's Hard to Be Normal: The Impact of Noise on Structure-agnostic Estimation](https://arxiv.org/abs/2507.02275)
*Jikai Jin,Lester Mackey,Vasilis Syrgkanis*

Main category: stat.ML

TL;DR: The paper examines the estimation of treatment effects using black-box machine learning under varying treatment noise distributions, showing improvements in robustness and proposing novel techniques.


<details>
  <summary>Details</summary>
Motivation: To understand and improve causal inference methods when treatment noise distributions vary, addressing limitations with traditional double machine learning (DML) for non-Gaussian noise.

Method: The paper utilizes partially linear models, compares DML with newly designed ACE procedures, and provides robustness results based on cumulants. Minimax guarantees are also proposed for binary treatments.

Result: DML is shown to be optimal for Gaussian noise but suboptimal for non-Gaussian noise. ACE procedures achieve higher-order robustness under specific conditions, and practical performance is demonstrated in synthetic experiments.

Conclusion: Treatment noise distribution plays a critical role in causal inference. ACE procedures provide improvements over DML for non-Gaussian noise, showcasing their practical utility with extended robustness.

Abstract: Structure-agnostic causal inference studies how well one can estimate a
treatment effect given black-box machine learning estimates of nuisance
functions (like the impact of confounders on treatment and outcomes). Here, we
find that the answer depends in a surprising way on the distribution of the
treatment noise. Focusing on the partially linear model of
\citet{robinson1988root}, we first show that the widely adopted double machine
learning (DML) estimator is minimax rate-optimal for Gaussian treatment noise,
resolving an open problem of \citet{mackey2018orthogonal}. Meanwhile, for
independent non-Gaussian treatment noise, we show that DML is always suboptimal
by constructing new practical procedures with higher-order robustness to
nuisance errors. These \emph{ACE} procedures use structure-agnostic cumulant
estimators to achieve $r$-th order insensitivity to nuisance errors whenever
the $(r+1)$-st treatment cumulant is non-zero. We complement these core results
with novel minimax guarantees for binary treatments in the partially linear
model. Finally, using synthetic demand estimation experiments, we demonstrate
the practical benefits of our higher-order robust estimators.

</details>


### [139] [Sparse Gaussian Processes: Structured Approximations and Power-EP Revisited](https://arxiv.org/abs/2507.02377)
*Thang D. Bui,Michalis K. Titsias*

Main category: stat.ML

TL;DR: This paper introduces a block-diagonal extension to sparse variational Gaussian Processes (GPs) and applies it in a revised Power Expectation Propagation (PEP) framework, achieving improved results with similar costs.


<details>
  <summary>Details</summary>
Motivation: To improve sparse variational Gaussian Processes by enhancing their posterior approximation and to offer flexible alternatives to existing methods.

Method: Introduced a block-diagonal scaling matrix for the posterior density and incorporated it into a unified PEP framework, supported by both theoretical analysis and extensive regression experiments.

Result: The block-diagonal approximation outperformed or matched diagonal approximations in accuracy while maintaining similar computational efficiency. The PEP framework with structured posteriors performed competitively across different settings.

Conclusion: The proposed block-diagonal method and its integration with PEP provide improved performance and flexible alternatives, making them valuable additions to current sparse GP approaches.

Abstract: Inducing-point-based sparse variational Gaussian processes have become the
standard workhorse for scaling up GP models. Recent advances show that these
methods can be improved by introducing a diagonal scaling matrix to the
conditional posterior density given the inducing points. This paper first
considers an extension that employs a block-diagonal structure for the scaling
matrix, provably tightening the variational lower bound. We then revisit the
unifying framework of sparse GPs based on Power Expectation Propagation (PEP)
and show that it can leverage and benefit from the new structured approximate
posteriors. Through extensive regression experiments, we show that the proposed
block-diagonal approximation consistently performs similarly to or better than
existing diagonal approximations while maintaining comparable computational
costs. Furthermore, the new PEP framework with structured posteriors provides
competitive performance across various power hyperparameter settings, offering
practitioners flexible alternatives to standard variational approaches.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [140] [ManifoldMind: Dynamic Hyperbolic Reasoning for Trustworthy Recommendations](https://arxiv.org/abs/2507.02014)
*Anoushka Harit,Zhongtian Sun,Suncica Hadzidedic*

Main category: cs.IR

TL;DR: ManifoldMind is a geometric recommender system leveraging hyperbolic space for reasoning over semantic hierarchies. It uses adaptive-curvature probabilistic spheres for personalized modeling and enables diverse semantic exploration.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve reasoning and recommendation in semantic hierarchies by introducing personalized uncertainty modeling and geometry-aware exploration of abstract or sparse domains.

Method: ManifoldMind uses adaptive-curvature probabilistic spheres to represent users, items, and tags, paired with a curvature-aware semantic kernel for soft multi-hop inference and conceptual path exploration.

Result: Experiments on four benchmarks demonstrate superior recommendation performance in terms of NDCG, calibration, and diversity compared to existing methods.

Conclusion: ManifoldMind enhances exploratory reasoning and recommendation transparency through explicit reasoning traces, making it suitable for complex semantic challenges.

Abstract: We introduce ManifoldMind, a probabilistic geometric recommender system for
exploratory reasoning over semantic hierarchies in hyperbolic space. Unlike
prior methods with fixed curvature and rigid embeddings, ManifoldMind
represents users, items, and tags as adaptive-curvature probabilistic spheres,
enabling personalised uncertainty modeling and geometry-aware semantic
exploration. A curvature-aware semantic kernel supports soft, multi-hop
inference, allowing the model to explore diverse conceptual paths instead of
overfitting to shallow or direct interactions. Experiments on four public
benchmarks show superior NDCG, calibration, and diversity compared to strong
baselines. ManifoldMind produces explicit reasoning traces, enabling
transparent, trustworthy, and exploration-driven recommendations in sparse or
abstract domains.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [141] [Neural simulation-based inference of the Higgs trilinear self-coupling via off-shell Higgs production](https://arxiv.org/abs/2507.02032)
*Aishik Ghosh,Maximilian Griese,Ulrich Haisch,Tae Hyoun Park*

Main category: hep-ph

TL;DR: The study develops a hybrid neural approach combining matrix-element-enhanced and classification-based techniques for analyzing off-shell Higgs production. It aims to provide sensitive constraints to support precision Higgs measurements.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of experimentally determining the Higgs trilinear self-coupling and to investigate off-shell Higgs production as a complementary probe to on-shell measurements.

Method: A hybrid neural simulation-based inference (NSBI) method was designed, combining matrix-element-enhanced techniques for robustness with classification-based methods for efficient background estimation.

Result: The proposed NSBI approach achieved sensitivity close to theoretical limits and provided constraints for the high-luminosity upgrade of the Large Hadron Collider.

Conclusion: The study highlights the effectiveness of the NSBI approach for analyzing Higgs behaviors, with implications for precise measurements of SMEFT operators and Higgs trilinear self-coupling.

Abstract: One of the forthcoming major challenges in particle physics is the
experimental determination of the Higgs trilinear self-coupling. While efforts
have largely focused on on-shell double- and single-Higgs production in
proton-proton collisions, off-shell Higgs production has also been proposed as
a valuable complementary probe. In this article, we design a hybrid neural
simulation-based inference (NSBI) approach to construct a likelihood of the
Higgs signal incorporating modifications from the Standard Model effective
field theory (SMEFT), relevant background processes, and quantum interference
effects. It leverages the training efficiency of matrix-element-enhanced
techniques, which are vital for robust SMEFT applications, while also
incorporating the practical advantages of classification-based methods for
effective background estimates. We demonstrate that our NSBI approach achieves
sensitivity close to the theoretical optimum and provide expected constraints
for the high-luminosity upgrade of the Large Hadron Collider. While we
primarily concentrate on the Higgs trilinear self-coupling, we also consider
constraints on other SMEFT operators that affect off-shell Higgs production.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [142] [Alleviating Attack Data Scarcity: SCANIA's Experience Towards Enhancing In-Vehicle Cyber Security Measures](https://arxiv.org/abs/2507.02607)
*Frida Sundfeldt,Bianca Widstam,Mahshid Helali Moghadam,Kuo-Yun Liang,Anders Vesterberg*

Main category: cs.CR

TL;DR: The paper introduces a context-aware attack data generator that creates simulated attack scenarios and corresponding in-vehicle network logs to address limitations in testing security on connected vehicles. It also evaluates the effectiveness of using this generated data for training intrusion detection systems.


<details>
  <summary>Details</summary>
Motivation: The increasing digitization of connected vehicles and associated security risks require effective cybersecurity measures capable of addressing unknown and evolving threats. Limited real-world data on attack scenarios creates the need for efficient methods to simulate these conditions.

Method: The paper proposes using a parameterized attack model combined with CAN message decoding and adjustable attack intensity to simulate diverse, realistic vehicle attack scenarios. The generated data is tested via deep neural network models within an intrusion detection system (IDS).

Result: The generated attack data was successfully validated against intrusion detection systems, where models showed high detection and classification capabilities. The approach proved efficient, scalable, and consistent in simulating realistic threats.

Conclusion: The data generation method effectively addresses the scarcity of real-world attack data, enabling robust IDS development and testing. The paper highlights the strategy’s practicality, fidelity to real-world conditions, and the promising application in cybersecurity for connected vehicles.

Abstract: The digital evolution of connected vehicles and the subsequent security risks
emphasize the critical need for implementing in-vehicle cyber security measures
such as intrusion detection and response systems. The continuous advancement of
attack scenarios further highlights the need for adaptive detection mechanisms
that can detect evolving, unknown, and complex threats. The effective use of
ML-driven techniques can help address this challenge. However, constraints on
implementing diverse attack scenarios on test vehicles due to safety, cost, and
ethical considerations result in a scarcity of data representing attack
scenarios. This limitation necessitates alternative efficient and effective
methods for generating high-quality attack-representing data. This paper
presents a context-aware attack data generator that generates attack inputs and
corresponding in-vehicle network log, i.e., controller area network (CAN) log,
representing various types of attack including denial of service (DoS), fuzzy,
spoofing, suspension, and replay attacks. It utilizes parameterized attack
models augmented with CAN message decoding and attack intensity adjustments to
configure the attack scenarios with high similarity to real-world scenarios and
promote variability. We evaluate the practicality of the generated
attack-representing data within an intrusion detection system (IDS) case study,
in which we develop and perform an empirical evaluation of two deep neural
network IDS models using the generated data. In addition to the efficiency and
scalability of the approach, the performance results of IDS models, high
detection and classification capabilities, validate the consistency and
effectiveness of the generated data as well. In this experience study, we also
elaborate on the aspects influencing the fidelity of the data to real-world
scenarios and provide insights into its application.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [143] [Detecting Fraud in Financial Networks: A Semi-Supervised GNN Approach with Granger-Causal Explanations](https://arxiv.org/abs/2507.01980)
*Linh Nguyen,Marcel Boersma,Erman Acar*

Main category: q-fin.ST

TL;DR: Fraud detection in finance faces challenges like sparse data and lack of explainability in machine learning models. SAGE-FIN uses a semi-supervised graph neural network with Granger causal explanations to tackle these issues effectively.


<details>
  <summary>Details</summary>
Motivation: Fraud detection is crucial in the financial sector, costing billions annually. The paper aims to address challenges in sparsely labeled datasets and regulatory need for explainability in ML models.

Method: The paper introduces SAGE-FIN, a semi-supervised graph neural network combined with Granger causal explanations, designed to evaluate fraudulent activities in financial interaction networks.

Result: SAGE-FIN demonstrates favorable performance on the real-world financial dataset, Elliptic++, detecting fraud and providing causal explanations without prior assumptions on network structure.

Conclusion: SAGE-FIN offers a promising solution to detect and explain financial fraud by leveraging semi-supervised learning and causality analysis, meeting regulatory requirements and overcoming technical limitations.

Abstract: Fraudulent activity in the financial industry costs billions annually.
Detecting fraud, therefore, is an essential yet technically challenging task
that requires carefully analyzing large volumes of data. While machine learning
(ML) approaches seem like a viable solution, applying them successfully is not
so easy due to two main challenges: (1) the sparsely labeled data, which makes
the training of such approaches challenging (with inherent labeling costs), and
(2) lack of explainability for the flagged items posed by the opacity of ML
models, that is often required by business regulations. This article proposes
SAGE-FIN, a semi-supervised graph neural network (GNN) based approach with
Granger causal explanations for Financial Interaction Networks. SAGE-FIN learns
to flag fraudulent items based on weakly labeled (or unlabelled) data points.
To adhere to regulatory requirements, the flagged items are explained by
highlighting related items in the network using Granger causality. We
empirically validate the favorable performance of SAGE-FIN on a real-world
dataset, Bipartite Edge-And-Node Attributed financial network (Elliptic++),
with Granger-causal explanations for the identified fraudulent items without
any prior assumption on the network structure.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [144] [Resolving CAP Through Automata-Theoretic Economic Design: A Unified Mathematical Framework for Real-Time Partition-Tolerant Systems](https://arxiv.org/abs/2507.02464)
*Craig S Wright*

Main category: cs.GT

TL;DR: The paper reframes the CAP theorem's trade-off among consistency, availability, and partition tolerance as a constraint optimization problem using automata-theoretic and economic frameworks, showing that availability and consistency can coexist within bounded margins.


<details>
  <summary>Details</summary>
Motivation: To address the classical CAP theorem's limiting trade-offs by leveraging automata and economic theories for stability in distributed systems.

Method: Distributed systems are modeled as partition-aware state machines with economic incentive layers and game-theoretic mechanisms to analyze behavior under adversarial partitions.

Result: The paper provides provable bounds for convergence, liveness, and correctness, showing that availability and consistency can co-exist within controlled margins.

Conclusion: The CAP theorem's limits can be extended through formal economic control, offering a new perspective for optimizing distributed systems.

Abstract: The CAP theorem asserts a trilemma between consistency, availability, and
partition tolerance. This paper introduces a rigorous automata-theoretic and
economically grounded framework that reframes the CAP trade-off as a constraint
optimization problem. We model distributed systems as partition-aware state
machines and embed economic incentive layers to stabilize consensus behavior
across adversarially partitioned networks. By incorporating game-theoretic
mechanisms into the global transition semantics, we define provable bounds on
convergence, liveness, and correctness. Our results demonstrate that
availability and consistency can be simultaneously preserved within bounded
epsilon margins, effectively extending the classical CAP limits through formal
economic control.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [145] [MULTI-SCOUT: Multistatic Integrated Sensing and Communications in 5G and Beyond for Moving Target Detection, Positioning, and Tracking](https://arxiv.org/abs/2507.02613)
*Yalin E. Sagduyu,Kemal Davaslioglu,Tugba Erpek,Sastry Kompella,Gustave Anderson,Jonathan Ashdown*

Main category: cs.NI

TL;DR: This paper outlines a system using 5G Positioning Reference Signal (PRS) for detecting, estimating, and tracking moving targets in multistatic ISAC systems.


<details>
  <summary>Details</summary>
Motivation: To leverage the capabilities of 5G PRS for high-accuracy sensing and communication requirements in multistatic systems.

Method: The paper uses a distributed architecture with multiple receivers, employing CAF for range-Doppler mapping, nonlinear least-squares trilateration for positioning, radial-speed inversion for velocity estimation, and Kalman filters for tracking.

Result: The method demonstrates effective moving-target detection, position estimation, and velocity tracking, utilizing 5G PRS for multistatic ISAC.

Conclusion: 5G PRS is highly capable for multistatic ISAC, enabling accurate geometric positioning and velocity estimation, along with robust target tracking.

Abstract: This paper presents a complete signal-processing chain for multistatic
integrated sensing and communications (ISAC) using 5G Positioning Reference
Signal (PRS). We consider a distributed architecture in which one gNB transmits
a periodic OFDM-PRS waveform while multiple spatially separated receivers
exploit the same signal for target detection, parameter estimation and
tracking. A coherent cross-ambiguity function (CAF) is evaluated to form a
range-Doppler map from which the bistatic delay and radial velocity are
extracted for every target. For a single target, the resulting bistatic delays
are fused through nonlinear least-squares trilateration, yielding a geometric
position estimate, and a regularized linear inversion of the radial-speed
equations yields a two-dimensional velocity vector, where speed and heading are
obtained. The approach is applied to 2D and 3D settings, extended to account
for time synchronization bias, and generalized to multiple targets by resolving
target association. The sequence of position-velocity estimates is then fed to
standard and extended Kalman filters to obtain smoothed tracks. Our results
show high-fidelity moving-target detection, positioning, and tracking using 5G
PRS signals for multistatic ISAC.

</details>
