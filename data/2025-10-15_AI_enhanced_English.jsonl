{"id": "2510.11938", "pdf": "https://arxiv.org/pdf/2510.11938", "abs": "https://arxiv.org/abs/2510.11938", "authors": ["Yanying Lin", "Shijie Peng", "Chengzhi Lu", "Chengzhong Xu", "Kejiang Ye"], "title": "FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline Refactoring in Fragmented Serverless Clusters", "categories": ["cs.DC"], "comment": "EuroSys 26", "summary": "Serving Large Language Models (LLMs) in production faces significant\nchallenges from highly variable request patterns and severe resource\nfragmentation in serverless clusters. Current systems rely on static pipeline\nconfigurations that struggle to adapt to dynamic workload conditions, leading\nto substantial inefficiencies. We present FlexPipe, a novel system that\ndynamically reconfigures pipeline architectures during runtime to address these\nfundamental limitations. FlexPipe decomposes models into fine-grained stages\nand intelligently adjusts pipeline granularity based on real-time request\npattern analysis, implementing three key innovations: fine-grained model\npartitioning with preserved computational graph constraints, inflight pipeline\nrefactoring with consistent cache transitions, and topology-aware resource\nallocation that navigates GPU fragmentation. Comprehensive evaluation on an\n82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource\nefficiency while maintaining 38.3% lower latency compared to state-of-the-art\nsystems, reducing GPU reservation requirements from 75% to 30% of peak\ncapacity.", "AI": {"tldr": "FlexPipe dynamically reconfigures serverless pipeline architectures for serving Large Language Models (LLMs), leading to significant resource efficiency and lower latency.", "motivation": "Serving LLMs in production suffers from inefficiencies due to static pipeline configurations that cannot handle dynamic workloads efficiently.", "method": "FlexPipe introduces dynamic pipeline reconfiguration by fine-grained model partitioning, inflight pipeline adjustments, and topology-aware GPU resource allocation.", "result": "FlexPipe delivers up to 8.5x better resource efficiency while achieving 38.3% lower latency compared to current systems, and it reduces GPU reservation needs dramatically.", "conclusion": "FlexPipe enhances the scalability and adaptability of LLM serverless systems, showcasing the potential for optimized resource utilization and performance."}}
{"id": "2510.12166", "pdf": "https://arxiv.org/pdf/2510.12166", "abs": "https://arxiv.org/abs/2510.12166", "authors": ["Kenneth Weiss", "Thomas M. Stitt", "Daryl Hawkins", "Olga Pearce", "Stephanie Brink", "Robert N. Rieben"], "title": "Comparing Cross-Platform Performance via Node-to-Node Scaling Studies", "categories": ["cs.DC"], "comment": "16 pages; accepted to the International Journal of High Performance\n  Computing Applications (IJHPCA)", "summary": "Due to the increasing diversity of high-performance computing architectures,\nresearchers and practitioners are increasingly interested in comparing a code's\nperformance and scalability across different platforms. However, there is a\nlack of available guidance on how to actually set up and analyze such\ncross-platform studies. In this paper, we contend that the natural base unit of\ncomputing for such studies is a single compute node on each platform and offer\nguidance in setting up, running, and analyzing node-to-node scaling studies. We\npropose templates for presenting scaling results of these studies and provide\nseveral case studies highlighting the benefits of this approach.", "AI": {"tldr": "This paper advocates for using a single compute node as the base unit in cross-platform performance studies, providing guidance, templates, and case studies for effective comparison.", "motivation": "Increasing diversity in high-performance computing platforms necessitates a standard approach to comparing performance and scalability across different systems.", "method": "The paper identifies a single compute node as the base comparison unit, suggests setup and analysis procedures, and provides templates and case studies for conducting node-to-node scaling analyses.", "result": "Templates and case studies demonstrated the practicality and benefits of using single-node scaling as a standard method for cross-platform performance studies.", "conclusion": "Using a single compute node as the basis for cross-platform studies simplifies comparative analysis and enhances the clarity of performance results across diverse systems."}}
{"id": "2510.12196", "pdf": "https://arxiv.org/pdf/2510.12196", "abs": "https://arxiv.org/abs/2510.12196", "authors": ["Petr Samoldekin", "Christian Schulz", "Henning Woydt"], "title": "GPU-Accelerated Algorithms for Process Mapping", "categories": ["cs.DC", "8W10"], "comment": null, "summary": "Process mapping asks to assign vertices of a task graph to processing\nelements of a supercomputer such that the computational workload is balanced\nwhile the communication cost is minimized. Motivated by the recent success of\nGPU-based graph partitioners, we propose two GPU-accelerated algorithms for\nthis optimization problem. The first algorithm employs hierarchical\nmultisection, which partitions the task graph alongside the hierarchy of the\nsupercomputer. The method utilizes GPU-based graph partitioners to accelerate\nthe mapping process. The second algorithm integrates process mapping directly\ninto the modern multilevel graph partitioning pipeline. Vital phases like\ncoarsening and refinement are accelerated by exploiting the parallelism of\nGPUs. In our experiments, both methods achieve speedups exceeding 300 when\ncompared to state-of-the-art CPU-based algorithms. The first algorithm has, on\naverage, about 10 percent greater communication costs and thus remains\ncompetitive to CPU algorithms. The second approach is much faster, with a\ngeometric mean speedup of 77.6 and peak speedup of 598 at the cost of lower\nsolution quality. To our knowledge, these are the first GPU-based algorithms\nfor process mapping.", "AI": {"tldr": "This paper introduces two GPU-accelerated algorithms for process mapping, achieving significant speedups compared to CPU-based techniques while maintaining competitive communication costs.", "motivation": "Current process mapping methods are CPU-based, making them slower and less efficient for large-scale computations, and GPUs have shown potential for graph partitioning.", "method": "Two methods are presented: hierarchical multisection using GPU-based graph partitioners and integration of process mapping into multilevel graph partitioning, leveraging GPU parallelism.", "result": "Both algorithms show speedups exceeding 300 times compared to CPU algorithms. The first is competitive in communication costs, while the second emphasizes faster computation but sacrifices some solution quality.", "conclusion": "These GPU-based methods are the first of their kind for process mapping and demonstrate significant advancements in speed and efficiency, although with a tradeoff in solution quality for the second method."}}
{"id": "2510.12274", "pdf": "https://arxiv.org/pdf/2510.12274", "abs": "https://arxiv.org/abs/2510.12274", "authors": ["Hao Jiang", "Meng Qin", "Ruijie Kuai", "Dandan Liang"], "title": "Metronome: Efficient Scheduling for Periodic Traffic Jobs with Network and Priority Awareness", "categories": ["cs.DC"], "comment": "16 pages, 16 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "With the rapid growth in computing power demand, cloud native networks have\nemerged as a promising solution to address the challenges of efficient resource\ncoordination, particularly in coping with the dynamic fluctuations of network\nbandwidth in clusters. We propose Metronome, a network-aware and priority-aware\nscheduling mechanism for cloud native networks. This mechanism is designed to\nsupport jobs that exhibit periodic traffic patterns and dynamic bandwidth\ndemands, particularly in the context of distributed training. Specifically,\nMetronome employs a time-division multiplexing approach that leverages job\ntraffic characteristics to construct an elastic network resource allocation\nmodel, enabling efficient bandwidth sharing across multiple jobs. In addition,\nit incorporates a multi-objective optimization strategy, jointly considering\nlatency and job priorities to achieve globally optimal as well as dynamic\nresource allocation. Finally, Metronome adapts to the dynamic environment by\nmonitoring the cluster and performing reconfiguration operations. Extensive\nexperiments with 13 common machine learning models demonstrate that Metronome\ncan enhance cluster resource utilization while guaranteeing service\nperformance. Compared with the existing Kubernetes scheduling mechanisms across\nmultiple scenarios, Metronome reduces job completion time by up to 19.50% while\nimproving average bandwidth utilization by up to 23.20%.", "AI": {"tldr": "The paper introduces Metronome, a network-aware and priority-aware scheduling system for optimizing bandwidth allocation in cloud-native networks, particularly for distributed training.", "motivation": "To address the challenge of resource coordination and handling dynamic bandwidth demands in cloud-native networks for distributed training tasks.", "method": "Metronome employs a time-division multiplexing approach to create an elastic network resource allocation model. It also integrates multi-objective optimization for latency and job priorities and adjusts dynamically by monitoring the cluster.", "result": "Experiments on 13 machine learning models showed Metronome reduces job completion time by up to 19.50% and improves bandwidth utilization by up to 23.20%, outperforming Kubernetes scheduling.", "conclusion": "Metronome enhances resource utilization and service performance, demonstrating the effectiveness of its dynamic and optimized scheduling approach for cloud-native environments."}}
{"id": "2510.12102", "pdf": "https://arxiv.org/pdf/2510.12102", "abs": "https://arxiv.org/abs/2510.12102", "authors": ["Donghyun Lee", "Alex Sima", "Yuhang Li", "Panos Stinis", "Priyadarshini Panda"], "title": "SpikePool: Event-driven Spiking Transformer with Pooling Attention", "categories": ["cs.NE"], "comment": null, "summary": "Building on the success of transformers, Spiking Neural Networks (SNNs) have\nincreasingly been integrated with transformer architectures, leading to spiking\ntransformers that demonstrate promising performance on event-based vision\ntasks. However, despite these empirical successes, there remains limited\nunderstanding of how spiking transformers fundamentally process event-based\ndata. Current approaches primarily focus on architectural modifications without\nanalyzing the underlying signal processing characteristics. In this work, we\nanalyze spiking transformers through the frequency spectrum domain and discover\nthat they behave as high-pass filters, contrasting with Vision Transformers\n(ViTs) that act as low-pass filters. This frequency domain analysis reveals why\ncertain designs work well for event-based data, which contains valuable\nhigh-frequency information but is also sparse and noisy. Based on this\nobservation, we propose SpikePool, which replaces spike-based self-attention\nwith max pooling attention, a low-pass filtering operation, to create a\nselective band-pass filtering effect. This design preserves meaningful\nhigh-frequency content while capturing critical features and suppressing noise,\nachieving a better balance for event-based data processing. Our approach\ndemonstrates competitive results on event-based datasets for both\nclassification and object detection tasks while significantly reducing training\nand inference time by up to 42.5% and 32.8%, respectively.", "AI": {"tldr": "The study uncovers that spiking transformers act as high-pass filters contrasting Vision Transformers' low-pass filters, proposing SpikePool to improve event-based data processing.", "motivation": "To provide a deeper understanding of how spiking transformers process event-based data, which has been underexplored in literature.", "method": "Analyzing spiking transformers in the frequency domain, discovering functional behavior, and introducing SpikePool, which uses a low-pass filtering mechanism for efficient data processing.", "result": "SpikePool enables competitive results on object detection and classification tasks, reducing training and inference times by 42.5% and 32.8%, respectively.", "conclusion": "The frequency domain analysis of spiking transformers enhances understanding and performance in event-based vision tasks, fostering efficient processing through innovations like SpikePool."}}
{"id": "2510.12277", "pdf": "https://arxiv.org/pdf/2510.12277", "abs": "https://arxiv.org/abs/2510.12277", "authors": ["Thomas Benz", "Axel Vanoni", "Michael Rogenmoser", "Luca Benini"], "title": "A Direct Memory Access Controller (DMAC) for Irregular Data Transfers on RISC-V Linux Systems", "categories": ["cs.AR"], "comment": "6 pages, 5 figures", "summary": "With the ever-growing heterogeneity in computing systems, driven by modern\nmachine learning applications, pressure is increasing on memory systems to\nhandle arbitrary and more demanding transfers efficiently. Descriptor-based\ndirect memory access controllers (DMACs) allow such transfers to be executed by\ndecoupling memory transfers from processing units. Classical descriptor-based\nDMACs are inefficient when handling arbitrary transfers of small unit sizes.\nExcessive descriptor size and the serialized nature of processing descriptors\nemployed by the DMAC lead to large static overheads when setting up transfers.\nTo tackle this inefficiency, we propose a descriptor-based DMAC optimized to\nefficiently handle arbitrary transfers of small unit sizes. We implement a\nlightweight descriptor format in an AXI4-based DMAC. We further increase\nperformance by implementing a low-overhead speculative descriptor prefetching\nscheme without additional latency penalties in the case of a misprediction. Our\nDMAC is integrated into a 64-bit Linux-capable RISC-V SoC and emulated on a\nKintex FPGA to evaluate its performance. Compared to an off-the-shelf\ndescriptor-based DMAC IP, we achieve 1.66x less latency launching transfers,\nincrease bus utilization up to 2.5x in an ideal memory system with\n64-byte-length transfers while requiring 11% fewer lookup tables, 23% fewer\nflip-flops, and no block RAMs. We can extend our lead in bus utilization to\n3.6x with 64-byte-length transfers in deep memory systems. We synthesized our\nDMAC in GlobalFoundries' GF12LP+ node, achieving a clock frequency of over 1.44\nGHz while occupying only 49.5 kGE.", "AI": {"tldr": "This paper proposes an optimized descriptor-based DMAC to efficiently handle small-size memory transfers, improving latency, bus utilization, and resource usage.", "motivation": "Increasing heterogeneity in computing systems and demands from machine learning applications require more efficient memory transfer solutions due to inefficiencies in classical DMAC designs.", "method": "The paper introduces a lightweight descriptor format and speculative descriptor prefetching scheme in an AXI4-based DMAC. This is integrated into a RISC-V SoC and evaluated on a Kintex FPGA.", "result": "Achieved 1.66x less latency for transfers, up to 2.5x higher bus utilization, and reduced hardware resource usage (11% fewer lookup tables, 23% fewer flip-flops). Extended bus utilization to 3.6x in deep memory systems. Synthesis results show clock frequency of 1.44 GHz with a small area of 49.5 kGE.", "conclusion": "The proposed DMAC offers significant improvements in performance and resource efficiency while addressing inefficiencies in handling small-size memory transfers. It is suitable for high-performance and resource-constrained systems."}}
{"id": "2510.12280", "pdf": "https://arxiv.org/pdf/2510.12280", "abs": "https://arxiv.org/abs/2510.12280", "authors": ["Yosuke Bando", "Akinobu Mita", "Kazuhiro Hiwada", "Shintaro Sano", "Tomoya Suzuki", "Yu Nakanishi", "Kazutaka Tomida", "Hirotsugu Kajihara", "Akiyuki Kaneko", "Daisuke Taki", "Yukimasa Miyamoto", "Tomokazu Yoshida", "Tatsuo Shiozawa"], "title": "Analysis and Evaluation of Using Microsecond-Latency Memory for In-Memory Indices and Caches in SSD-Based Key-Value Stores", "categories": ["cs.PF", "cs.DB"], "comment": null, "summary": "When key-value (KV) stores use SSDs for storing a large number of items,\noftentimes they also require large in-memory data structures including indices\nand caches to be traversed to reduce IOs. This paper considers offloading most\nof such data structures from the costly host DRAM to secondary memory whose\nlatency is in the microsecond range, an order of magnitude longer than those of\ncurrently available DIMM-mounted or CXL memory devices. While emerging\nmicrosecond-latency memory is likely to cost much less than DRAM, it can\nsignificantly slow down SSD-based KV stores if naively employed. This paper\nanalyzes and evaluates the impact of microsecond-level memory latency on the KV\noperation throughput. Our analysis finds that a well-known latency-hiding\ntechnique of software prefetching for long-latency memory from user-level\nthreads is effective. The novelty of our analysis lies in modeling how the\ninterplay between prefetching and IO affects performance, from which we derive\nan equation that well explains the throughput degradation due to long memory\nlatency. The model tells us that the presence of IO significantly enhances the\ntolerance to memory latency, leading to a finding that SSD-based KV stores can\nbe made latency-tolerant without devising new techniques for\nmicrosecond-latency memory. To confirm this, we design a microbenchmark as well\nas modify existing SSD-based KV stores so that they issue prefetches from\nuser-level threads, and run them while placing most of in-memory data\nstructures on FPGA-based memory with adjustable microsecond latency. The\nresults demonstrate that their KV operation throughputs can be well explained\nby our model, and the modified KV stores achieve near-DRAM throughputs for up\nto a memory latency of 5 microseconds. This suggests the possibility that\nSSD-based KV stores can use microsecond-latency memory as a cost-effective\nalternative to the host DRAM.", "AI": {"tldr": "This paper proposes using cost-effective, microsecond-latency memory for SSD-based key-value stores instead of expensive DRAM, and demonstrates that software techniques like prefetching can mitigate the latency impacts effectively.", "motivation": "Current SSD-based key-value stores rely on costly DRAM for managing large in-memory data structures. The paper explores a more affordable alternative using microsecond-latency memory and aims to understand its performance impact while analyzing ways to mitigate potential bottlenecks.", "method": "The study models the interaction between prefetching techniques and IO operations to analyze memory latency effects on throughput. It uses microbenchmarks and modifies existing SSD-based KV stores, testing them on FPGA-based memory with adjustable latency levels.", "result": "The experiments validate the proposed model, showing that KV stores achieve near-DRAM throughput performance for latency up to 5 microseconds. This demonstrates that microsecond-latency memory can perform effectively when paired with latency-hiding techniques like software prefetching.", "conclusion": "SSD-based KV stores can use microsecond-latency memory as a viable, cost-efficient alternative to DRAM without requiring entirely new latency mitigation techniques, making it promising for future SSD-based applications."}}
{"id": "2510.11751", "pdf": "https://arxiv.org/pdf/2510.11751", "abs": "https://arxiv.org/abs/2510.11751", "authors": ["Jan Pedersen", "Kevin Chalmers"], "title": "Verifying Correctness of Shared Channels in a Cooperatively Scheduled Process-Oriented Language", "categories": ["cs.PL"], "comment": null, "summary": "Correct concurrent behaviour is important in understanding how components\nwill act within certain conditions. In this work. we analyse the behaviour of\nshared communicating channels within a coorporatively scheduled runtime. We use\nthe refinement checking and modelling tool FDR to develop both specifications\nof how such shared channels should behave and models of the implementations of\nthese channels in the cooperatively scheduled language ProcessJ. Our results\ndemonstrate that although we can certainly implement the correct behaviour of\nsuch channels, the outcome is dependant on having adequate resources available\nto execute all processes involved. We conclude that modelling the runtime\nenvironment of concurrent components is necessary to ensure components behave\nas specified in the real world.", "AI": {"tldr": "The paper studies shared communicating channels in a cooperatively scheduled runtime, showing implementations depend on resource availability and emphasizing runtime modeling for accurate concurrent behavior.", "motivation": "Understanding and ensuring correct behavior of concurrent components in specific runtime environments.", "method": "Utilized FDR for refinement checking and modeling, specifying behaviors and simulating implementations in the ProcessJ language.", "result": "Correct channel behavior can be implemented, but its success relies on sufficient resources to execute involved processes.", "conclusion": "Modeling runtime environments is essential to guarantee real-world behavior aligns with specifications of concurrent components."}}
{"id": "2510.11812", "pdf": "https://arxiv.org/pdf/2510.11812", "abs": "https://arxiv.org/abs/2510.11812", "authors": ["Souradeep Mukhopadhyay", "Rishabh Baral", "Nimeesh Mahajan", "Samhitha Harish", "Aswin RRV", "Mihir Parmar", "Mutsumi Nakamura", "Chitta Baral"], "title": "PHANTOM RECALL: When Familiar Puzzles Fool Smart Models", "categories": ["cs.CL", "cs.AI"], "comment": "22 Pages", "summary": "Large language models (LLMs) such as GPT, Gemini, and Claude often appear\nadept at solving classic logic puzzles--but how much genuine reasoning\nunderlies their answers? Recent evidence suggests that these models frequently\nrely on memorized templates rather than reasoning from first principles. When\npuzzles are slightly modified, their performance collapses, revealing a\nstriking fragility. In particular, we asked: Have LLMs addressed these issues?\nTo what extent? How about perturbations to other puzzles? Is there a general\nway of reformulating the prompt so that the models do better? To examine these\nthings systematically, we introduce PHANTOM RECALL, a benchmark comprising 25\nwell-known logic puzzles and 149 carefully designed perturbations that preserve\nreasoning structure but alter superficial details and solutions. We evaluate\neleven leading LLMs and identify a recurring failure mode--phantom\nrecall--where models confidently reproduce memorized solutions or spurious\nrationales that no longer fit the altered scenario. To probe and mitigate this\nissue, we contribute three tools: (i) an automated logical-equivalence judge to\ndetect reasoning mismatches, (ii) a taxonomy of fine-grained reasoning error\ncategories, and (iii) a prompting-based mitigation framework guided by these\ncategories. Despite near-perfect accuracy on unmodified puzzles, models\nsignificantly underperform humans on perturbed ones, exhibiting both phantom\nrecall and over-elaboration. Our findings reveal a crucial limitation: LLMs\noften fail to re-reason when contextual cues shift--highlighting the gap\nbetween linguistic fluency and logical understanding.", "AI": {"tldr": "This paper examines the reasoning capability of large language models (LLMs) such as GPT and evaluates their performance on logic puzzles, revealing significant limitations in adapting to slightly modified problems.", "motivation": "The study is motivated by the challenge of assessing whether LLMs genuinely reason through logic puzzles or rely on memorization, and to examine their fragility when faced with modified problems.", "method": "The authors introduced the PHANTOM RECALL benchmark, which includes 25 logic puzzles and 149 modified versions. They evaluated 11 LLMs, identified recurring failure modes, and developed tools such as a logical-equivalence judge, a taxonomy of reasoning errors, and a mitigation framework.", "result": "The analysis shows that while LLMs perform well on unmodified puzzles, they fail significantly on perturbed versions due to a phenomenon called \"phantom recall,\" where they employ memorized solutions unfit for the changes.", "conclusion": "The study highlights a critical limitation in LLM reasoning, emphasizing that these models struggle to adapt their reasoning when contextual details shift, thus revealing the gap between language fluency and true logical understanding."}}
{"id": "2510.12090", "pdf": "https://arxiv.org/pdf/2510.12090", "abs": "https://arxiv.org/abs/2510.12090", "authors": ["Hakan Ceylan", "Edoardo Sinibaldi", "Sanjay Misra", "Pankaj J. Pasricha", "Dietmar W. Hutmacher"], "title": "Translating Milli/Microrobots with A Value-Centered Readiness Framework", "categories": ["cs.RO", "cs.ET"], "comment": null, "summary": "Untethered mobile milli/microrobots hold transformative potential for\ninterventional medicine by enabling more precise and entirely non-invasive\ndiagnosis and therapy. Realizing this promise requires bridging the gap between\ngroundbreaking laboratory demonstrations and successful clinical integration.\nDespite remarkable technical progress over the past two decades, most\nmillirobots and microrobots remain confined to laboratory proof-of-concept\ndemonstrations, with limited real-world feasibility. In this Review, we\nidentify key factors that slow translation from bench to bedside, focusing on\nthe disconnect between technical innovation and real-world application. We\nargue that the long-term impact and sustainability of the field depend on\naligning development with unmet medical needs, ensuring applied feasibility,\nand integrating seamlessly into existing clinical workflows, which are\nessential pillars for delivering meaningful patient outcomes. To support this\nshift, we introduce a strategic milli/microrobot Technology Readiness Level\nframework (mTRL), which maps system development from initial conceptualization\nto clinical adoption through clearly defined milestones and their associated\nstepwise activities. The mTRL model provides a structured gauge of\ntechnological maturity, a common language for cross-disciplinary collaboration\nand actionable guidance to accelerate translational development toward new,\nsafer and more efficient interventions.", "AI": {"tldr": "The paper reviews the challenges in translating milli/microrobots from laboratory concepts to clinical applications and proposes a Technology Readiness Level framework to facilitate this process.", "motivation": "To overcome the barriers in adopting milli/microrobots for interventional medicine and improve their real-world feasibility in diagnostic and therapeutic applications.", "method": "Development of a milli/microrobot Technology Readiness Level framework (mTRL) to outline clear milestones for advancing the technology toward clinical integration.", "result": "The proposed mTRL framework offers structured milestones and activities, ensuring alignment with medical needs and supporting translational development of milli/microrobots.", "conclusion": "By utilizing the mTRL framework, milli/microrobot technology can advance more effectively toward clinical adoption, enhancing patient outcomes through safer and more efficient medical interventions."}}
{"id": "2510.11924", "pdf": "https://arxiv.org/pdf/2510.11924", "abs": "https://arxiv.org/abs/2510.11924", "authors": ["Ji Xia", "Yizi Zhang", "Shuqi Wang", "Genevera I. Allen", "Liam Paninski", "Cole Lincoln Hurwitz", "Kenneth D. Miller"], "title": "Inpainting the Neural Picture: Inferring Unrecorded Brain Area Dynamics from Multi-Animal Datasets", "categories": ["q-bio.NC", "stat.AP", "stat.ML"], "comment": null, "summary": "Characterizing interactions between brain areas is a fundamental goal of\nsystems neuroscience. While such analyses are possible when areas are recorded\nsimultaneously, it is rare to observe all combinations of areas of interest\nwithin a single animal or recording session. How can we leverage multi-animal\ndatasets to better understand multi-area interactions? Building on recent\nprogress in large-scale, multi-animal models, we introduce NeuroPaint, a masked\nautoencoding approach for inferring the dynamics of unrecorded brain areas. By\ntraining across animals with overlapping subsets of recorded areas, NeuroPaint\nlearns to reconstruct activity in missing areas based on shared structure\nacross individuals. We train and evaluate our approach on synthetic data and\ntwo multi-animal, multi-area Neuropixels datasets. Our results demonstrate that\nmodels trained across animals with partial observations can successfully\nin-paint the dynamics of unrecorded areas, enabling multi-area analyses that\ntranscend the limitations of any single experiment.", "AI": {"tldr": "The paper presents NeuroPaint, a model that predicts unrecorded brain area dynamics by leveraging multi-animal datasets and overlapping recorded areas.", "motivation": "To overcome limitations in single-animal experiments where not all brain area dynamics can be simultaneously recorded, and to explore multi-animal data to understand interactions between brain areas.", "method": "The method involves NeuroPaint, a masked autoencoding model trained on synthetic and Neuropixels datasets. The training utilizes overlapping subsets of recorded brain regions across animals to predict the dynamics of unrecorded regions.", "result": "NeuroPaint successfully reconstructed activity in missing brain areas and demonstrated its capability for accurate multi-area analysis beyond single animal or single session data.", "conclusion": "The approach allows multi-area brain studies by utilizing partial and overlapping data from multiple animals, addressing the challenge of limited simultaneous recordings."}}
{"id": "2510.11736", "pdf": "https://arxiv.org/pdf/2510.11736", "abs": "https://arxiv.org/abs/2510.11736", "authors": ["Sahaj Raj Malla"], "title": "AI Agents for the Dhumbal Card Game: A Comparative Study", "categories": ["cs.AI", "cs.GT", "cs.LG", "91A68", "I.2.1; I.2.6"], "comment": "10 pages, 7 figures, 6 tables", "summary": "This study evaluates Artificial Intelligence (AI) agents for Dhumbal, a\nculturally significant multiplayer card game with imperfect information,\nthrough a systematic comparison of rule-based, search-based, and learning-based\nstrategies. We formalize Dhumbal's mechanics and implement diverse agents,\nincluding heuristic approaches (Aggressive, Conservative, Balanced,\nOpportunistic), search-based methods such as Monte Carlo Tree Search (MCTS) and\nInformation Set Monte Carlo Tree Search (ISMCTS), and reinforcement learning\napproaches including Deep Q-Network (DQN) and Proximal Policy Optimization\n(PPO), and a random baseline. Evaluation involves within-category tournaments\nfollowed by a cross-category championship. Performance is measured via win\nrate, economic outcome, Jhyap success, cards discarded per round, risk\nassessment, and decision efficiency. Statistical significance is assessed using\nWelch's t-test with Bonferroni correction, effect sizes via Cohen's d, and 95%\nconfidence intervals (CI). Across 1024 simulated rounds, the rule-based\nAggressive agent achieves the highest win rate (88.3%, 95% CI: [86.3, 90.3]),\noutperforming ISMCTS (9.0%) and PPO (1.5%) through effective exploitation of\nJhyap declarations. The study contributes a reproducible AI framework, insights\ninto heuristic efficacy under partial information, and open-source code,\nthereby advancing AI research and supporting digital preservation of cultural\ngames.", "AI": {"tldr": "The study analyzes AI agents for the culturally significant Dhumbal card game, comparing rule-based, search-based, and learning-based strategies. The Aggressive rule-based agent performed best with an 88.3% win rate.", "motivation": "The paper aims to evaluate AI strategies for Dhumbal, a multiplayer card game with imperfect information, and explore how AI can contribute to digital preservation of cultural games.", "method": "The study formalizes Dhumbal's mechanics and tests rule-based (e.g., Aggressive, Conservative), search-based (e.g., MCTS, ISMCTS), and learning-based (e.g., DQN, PPO) AI agents through within-category and cross-category tournaments using statistical methods like Welch's t-test and Cohen's d.", "result": "The Aggressive rule-based agent had the highest win rate at 88.3%, outperforming search-based (ISMCTS) and learning-based (PPO) agents. Performance was measured by various metrics like win rates and decision efficiency.", "conclusion": "Rule-based strategies, particularly the Aggressive heuristic, are highly effective in Dhumbal, offering insights into heuristic efficacy in imperfect information settings. The research framework and open-source code also aid in cultural game preservation."}}
{"id": "2510.11722", "pdf": "https://arxiv.org/pdf/2510.11722", "abs": "https://arxiv.org/abs/2510.11722", "authors": ["Haruhiko Yoshioka", "Kazumasa Shimari", "Hidetake Uwano", "Kenichi Matsumoto"], "title": "eye2vec: Learning Distributed Representations of Eye Movement for Program Comprehension Analysis", "categories": ["cs.SE"], "comment": "2 pages, 1 figure, conference", "summary": "This paper presents eye2vec, an infrastructure for analyzing software\ndevelopers' eye movements while reading source code. In common eye-tracking\nstudies in program comprehension, researchers must preselect analysis targets\nsuch as control flow or syntactic elements, and then develop analysis methods\nto extract appropriate metrics from the fixation for source code. Here,\nresearchers can define various levels of AOIs like words, lines, or code\nblocks, and the difference leads to different results. Moreover, the\ninterpretation of fixation for word/line can vary across the purposes of the\nanalyses. Hence, the eye-tracking analysis is a difficult task that depends on\nthe time-consuming manual work of the researchers. eye2vec represents\ncontinuous two fixations as transitions between syntactic elements using\ndistributed representations. The distributed representation facilitates the\nadoption of diverse data analysis methods with rich semantic interpretations.", "AI": {"tldr": "This paper introduces eye2vec, a system for analyzing developers' eye movements on source code using distributed representations to simplify eye-tracking analysis.", "motivation": "To address challenges in eye-tracking analysis for program comprehension, such as preselection of analysis targets, manually varying AOI levels, and difficulty interpreting fixation metrics.", "method": "eye2vec encodes eye movement transitions as distributed representations of syntactic elements, enabling a more streamlined and semantically rich analysis.", "result": "The proposed infrastructure supports diverse data analysis methods and reduces manual labor in eye-tracking studies.", "conclusion": "eye2vec simplifies and improves the analysis of developers' eye movements in software comprehension tasks, promoting better insights through semantic representations."}}
{"id": "2510.11817", "pdf": "https://arxiv.org/pdf/2510.11817", "abs": "https://arxiv.org/abs/2510.11817", "authors": ["Yumi Iwashita", "Haakon Moe", "Yang Cheng", "Adnan Ansar", "Georgios Georgakis", "Adrian Stoica", "Kazuto Nakashima", "Ryo Kurazume", "Jim Torresen"], "title": "Enhancing the Quality of 3D Lunar Maps Using JAXA's Kaguya Imagery", "categories": ["cs.CV", "cs.LG"], "comment": "Presented at IEEE SMC 2025", "summary": "As global efforts to explore the Moon intensify, the need for high-quality 3D\nlunar maps becomes increasingly critical-particularly for long-distance\nmissions such as NASA's Endurance mission concept, in which a rover aims to\ntraverse 2,000 km across the South Pole-Aitken basin. Kaguya TC (Terrain\nCamera) images, though globally available at 10 m/pixel, suffer from altitude\ninaccuracies caused by stereo matching errors and JPEG-based compression\nartifacts. This paper presents a method to improve the quality of 3D maps\ngenerated from Kaguya TC images, focusing on mitigating the effects of\ncompression-induced noise in disparity maps. We analyze the compression\nbehavior of Kaguya TC imagery, and identify systematic disparity noise\npatterns, especially in darker regions. In this paper, we propose an approach\nto enhance 3D map quality by reducing residual noise in disparity images\nderived from compressed images. Our experimental results show that the proposed\napproach effectively reduces elevation noise, enhancing the safety and\nreliability of terrain data for future lunar missions.", "AI": {"tldr": "This paper addresses the inaccuracies in 3D lunar maps from Kaguya TC images due to stereo matching errors and JPEG compression artifacts, proposing an approach to mitigate disparity noise and improve elevation accuracy.", "motivation": "With increasing global lunar exploration initiatives, such as NASA's Endurance mission aiming to traverse long distances, there is a growing need for precise and reliable 3D lunar maps.", "method": "The method involves analyzing the compression behavior in Kaguya TC images and addressing systematic disparity noise patterns, particularly in darker regions, to enhance the disparity maps from compressed data.", "result": "The experimental results demonstrate successful reduction of elevation noise, leading to improved quality and reliability of 3D lunar maps.", "conclusion": "The proposed technique enhances map quality, contributing to safer and more effective lunar mission planning and execution."}}
{"id": "2510.11745", "pdf": "https://arxiv.org/pdf/2510.11745", "abs": "https://arxiv.org/abs/2510.11745", "authors": ["Qingwen Li", "Xiaohang Zhao", "Xiao Han", "Hailiang Huang", "Lanjuan Liu"], "title": "Think as a Doctor: An Interpretable AI Approach for ICU Mortality Prediction", "categories": ["cs.LG"], "comment": "42 pages", "summary": "Intensive Care Unit (ICU) mortality prediction, which estimates a patient's\nmortality status at discharge using EHRs collected early in an ICU admission,\nis vital in critical care. For this task, predictive accuracy alone is\ninsufficient; interpretability is equally essential for building clinical trust\nand meeting regulatory standards, a topic that has attracted significant\nattention in information system research. Accordingly, an ideal solution should\nenable intrinsic interpretability and align its reasoning with three key\nelements of the ICU decision-making practices: clinical course identification,\ndemographic heterogeneity, and prognostication awareness. However, conventional\napproaches largely focus on demographic heterogeneity, overlooking clinical\ncourse identification and prognostication awareness. Recent prototype learning\nmethods address clinical course identification, yet the integration of the\nother elements into such frameworks remains underexplored. To address these\ngaps, we propose ProtoDoctor, a novel ICU mortality prediction framework that\ndelivers intrinsic interpretability while integrating all three elements of the\nICU decision-making practices into its reasoning process. Methodologically,\nProtoDoctor features two key innovations: the Prognostic Clinical Course\nIdentification module and the Demographic Heterogeneity Recognition module. The\nformer enables the identification of clinical courses via prototype learning\nand achieves prognostication awareness using a novel regularization mechanism.\nThe latter models demographic heterogeneity through cohort-specific prototypes\nand risk adjustments. Extensive empirical evaluations demonstrate that\nProtoDoctor outperforms state-of-the-art baselines in predictive accuracy.\nHuman evaluations further confirm that its interpretations are more clinically\nmeaningful, trustworthy, and applicable in ICU practice.", "AI": {"tldr": "The paper introduces ProtoDoctor, a novel ICU mortality prediction framework that not only improves predictive accuracy but also provides intrinsic interpretability by integrating three critical elements of ICU decision-making practices.", "motivation": "ICU mortality prediction requires not just predictive accuracy but also interpretability to gain clinical trust and comply with regulations. Current methods inadequately integrate the key elements of ICU decision-making practices, such as clinical course identification, demographic heterogeneity, and prognostication awareness.", "method": "ProtoDoctor employs a Prognostic Clinical Course Identification module with prototype learning and a new regularization mechanism for clinical awareness, and a Demographic Heterogeneity Recognition module for modeling cohort-specific prototypes and adjusting risks.", "result": "Experimental evaluations show that ProtoDoctor achieves higher predictive accuracy than existing approaches. Additionally, human evaluations highlight that its interpretations are more meaningful, trustworthy, and applicable in clinical practice.", "conclusion": "ProtoDoctor successfully bridges gaps in ICU mortality prediction frameworks by integrating all critical decision-making elements and offering both improved accuracy and interpretability, making it highly useful for clinical applications."}}
{"id": "2510.11789", "pdf": "https://arxiv.org/pdf/2510.11789", "abs": "https://arxiv.org/abs/2510.11789", "authors": ["Shai Zucker", "Xiong Wang", "Fei Lu", "Inbar Seroussi"], "title": "Dimension-Free Minimax Rates for Learning Pairwise Interactions in Attention-Style Models", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST", "stat.TH"], "comment": null, "summary": "We study the convergence rate of learning pairwise interactions in\nsingle-layer attention-style models, where tokens interact through a weight\nmatrix and a non-linear activation function. We prove that the minimax rate is\n$M^{-\\frac{2\\beta}{2\\beta+1}}$ with $M$ being the sample size, depending only\non the smoothness $\\beta$ of the activation, and crucially independent of token\ncount, ambient dimension, or rank of the weight matrix. These results highlight\na fundamental dimension-free statistical efficiency of attention-style nonlocal\nmodels, even when the weight matrix and activation are not separately\nidentifiable and provide a theoretical understanding of the attention mechanism\nand its training.", "AI": {"tldr": "The paper examines the convergence rate of learning pairwise interactions in single-layer attention-style models, revealing a fundamental dimension-free statistical efficiency.", "motivation": "To understand the statistical efficiency and theoretical properties of attention-style models for handling pairwise interactions, independent of complex factors like token count or matrix rank.", "method": "The authors derive theoretical minimax convergence rates, analyzing the dependency on smoothness of activation and other inherent properties of the attention mechanism.", "result": "The convergence rate for learning pairwise interactions is $M^{-(2\\beta)/(2\\beta+1)}$, solely influenced by activation smoothness ($\\beta$), unaffected by token count, ambient dimension, or weight matrix rank.", "conclusion": "Attention-style models demonstrate dimension-free statistical efficiency, offering insights into their training and mechanisms despite identifiability challenges in weight matrices and activations."}}
{"id": "2510.12354", "pdf": "https://arxiv.org/pdf/2510.12354", "abs": "https://arxiv.org/abs/2510.12354", "authors": ["Sepideh Masoudi", "Mark Edward Michael Daly", "Jannis Kiesel", "Stefan Tai"], "title": "A Non-Intrusive Framework for Deferred Integration of Cloud Patterns in Energy-Efficient Data-Sharing Pipelines", "categories": ["cs.DC"], "comment": null, "summary": "As data mesh architectures gain traction in federated environments,\norganizations are increasingly building consumer-specific data-sharing\npipelines using modular, cloud-native transformation services. Prior work has\nshown that structuring these pipelines with reusable transformation stages\nenhances both scalability and energy efficiency. However, integrating\ntraditional cloud design patterns into such pipelines poses a challenge:\npredefining and embedding patterns can compromise modularity, reduce\nreusability, and conflict with the pipelines dynamic, consumer-driven nature.\nTo address this, we introduce a Kubernetes-based tool that enables the deferred\nand non-intrusive application of selected cloud design patterns without\nrequiring changes to service source code. The tool supports automated pattern\ninjection and collects energy consumption metrics, allowing developers to make\nenergy-aware decisions while preserving the flexible, composable structure of\nreusable data-sharing pipelines.", "AI": {"tldr": "The paper introduces a Kubernetes-based tool for modular, cloud-native data-sharing pipelines, enabling the seamless application of cloud design patterns and energy-aware decision-making.", "motivation": "To enhance scalability, energy efficiency, and modularity in consumer-specific data-sharing pipelines, while avoiding conflicts with traditional cloud design patterns.", "method": "Development of a Kubernetes-based tool that allows deferred, non-intrusive application of cloud design patterns and collects energy consumption metrics.", "result": "The tool supports automated pattern injection and enables developers to optimize pipelines for energy efficiency without altering service source code.", "conclusion": "The proposed tool balances modularity, reusability, and energy-awareness in data-sharing pipelines, overcoming challenges posed by traditional cloud design patterns."}}
{"id": "2510.12269", "pdf": "https://arxiv.org/pdf/2510.12269", "abs": "https://arxiv.org/abs/2510.12269", "authors": ["Pedro Domingos"], "title": "Tensor Logic: The Language of AI", "categories": ["cs.AI", "cs.LG", "cs.NE", "cs.PL", "stat.ML", "I.2.3; I.2.4; I.2.5; I.2.6; I.5.1"], "comment": "17 pages, 0 figures", "summary": "Progress in AI is hindered by the lack of a programming language with all the\nrequisite features. Libraries like PyTorch and TensorFlow provide automatic\ndifferentiation and efficient GPU implementation, but are additions to Python,\nwhich was never intended for AI. Their lack of support for automated reasoning\nand knowledge acquisition has led to a long and costly series of hacky attempts\nto tack them on. On the other hand, AI languages like LISP an Prolog lack\nscalability and support for learning. This paper proposes tensor logic, a\nlanguage that solves these problems by unifying neural and symbolic AI at a\nfundamental level. The sole construct in tensor logic is the tensor equation,\nbased on the observation that logical rules and Einstein summation are\nessentially the same operation, and all else can be reduced to them. I show how\nto elegantly implement key forms of neural, symbolic and statistical AI in\ntensor logic, including transformers, formal reasoning, kernel machines and\ngraphical models. Most importantly, tensor logic makes new directions possible,\nsuch as sound reasoning in embedding space. This combines the scalability and\nlearnability of neural networks with the reliability and transparency of\nsymbolic reasoning, and is potentially a basis for the wider adoption of AI.", "AI": {"tldr": "The paper introduces 'tensor logic,' a programming language designed to unify neural and symbolic AI, overcoming limitations in existing tools like PyTorch, TensorFlow, LISP, and Prolog.", "motivation": "AI lacks a unified language that combines the capabilities of neural and symbolic AI. Existing tools like PyTorch and TensorFlow are not intrinsically designed for AI, while traditional AI languages lack scalability and learning support.", "method": "The paper proposes tensor logic as a new language, grounded in tensor equations, unifying logical rules and Einstein summation. It demonstrates key AI implementations and explores new capabilities like reasoning in embedding space.", "result": "Tensor logic enables the seamless integration of neural, symbolic, and statistical AI techniques, showing potential for scalable and reliable AI applications.", "conclusion": "Tensor logic combines the strengths of diverse AI approaches, paving the way for integrated systems that are scalable, learnable, reliable, and transparent, potentially broadening AI adoption."}}
{"id": "2510.12397", "pdf": "https://arxiv.org/pdf/2510.12397", "abs": "https://arxiv.org/abs/2510.12397", "authors": ["S\u00f6ren Henning", "Adriano Vogel", "Esteban Perez-Wohlfeil", "Otmar Ertl", "Rick Rabiser"], "title": "Should I Run My Cloud Benchmark on Black Friday?", "categories": ["cs.SE", "cs.DC", "cs.PF"], "comment": "Accepted for the 16th Symposium on Software Performance 2025", "summary": "Benchmarks and performance experiments are frequently conducted in cloud\nenvironments. However, their results are often treated with caution, as the\npresumed high variability of performance in the cloud raises concerns about\nreproducibility and credibility. In a recent study, we empirically quantified\nthe impact of this variability on benchmarking results by repeatedly executing\na stream processing application benchmark at different times of the day over\nseveral months. Our analysis confirms that performance variability is indeed\nobservable at the application level, although it is less pronounced than often\nassumed. The larger scale of our study compared to related work allowed us to\nidentify subtle daily and weekly performance patterns. We now extend this\ninvestigation by examining whether a major global event, such as Black Friday,\naffects the outcomes of performance benchmarks.", "AI": {"tldr": "This paper investigates the impact of performance variability during benchmarks in cloud environments, especially during major events like Black Friday.", "motivation": "Researchers are questioning the reproducibility and credibility of benchmarks in cloud environments due to presumed performance variability.", "method": "The study analyzes benchmarking a stream processing application over months, identifying daily/weekly patterns, and further evaluating Black Friday's effect on performance benchmarks.", "result": "The study revealed subtle performance patterns over time and explores variability during a significant global event like Black Friday.", "conclusion": "While performance variability in cloud environments is observable, it is less pronounced than presumed; global events could further influence benchmarking results."}}
{"id": "2510.11759", "pdf": "https://arxiv.org/pdf/2510.11759", "abs": "https://arxiv.org/abs/2510.11759", "authors": ["Hongyu Lin", "Haolin Pan", "Haoran Luo", "Yuchen Li", "Kaichun Yao", "Libo Zhang", "Mingjie Xing", "Yanjun Wu"], "title": "AwareCompiler: Agentic Context-Aware Compiler Optimization via a Synergistic Knowledge-Data Driven Framework", "categories": ["cs.PL", "cs.AI"], "comment": null, "summary": "Compiler optimization is crucial for enhancing program performance by\ntransforming the sequence of optimization passes while maintaining correctness.\nDespite the promising potential of large language models (LLMs)-based agent for\nsoftware optimization, automating compiler optimization remains challenging due\nto: (1) semantic misalignment between abstract program representations and\nconcrete optimization passes, (2) inefficient interaction mechanisms between\nagents and compiler environments, and (3) reward sparsity from the extensive\ndecision-making process within large optimization spaces. This paper introduces\n\\textbf{AwareCompiler}, an agentic framework for compiler optimization that\naddresses these challenges through three key innovations: structured knowledge\nintegration and dataset construction, knowledge-driven adaptive pass\ngeneration, and data-driven hybrid training pipeline. Experimental results on\nstandard benchmarks demonstrate that AwareCompiler significantly outperforms\nexisting baselines in both performance and efficiency, highlighting the\neffectiveness of our synergistic knowledge-data-driven approach. Our code is\npublicly available at https://github.com/LHY-24/AwareCompiler.", "AI": {"tldr": "The paper introduces AwareCompiler, a framework for automating compiler optimization using large language models, overcoming challenges of semantic misalignment, inefficient agent-compiler interaction, and reward sparsity.", "motivation": "Automating compiler optimization to improve program performance while addressing issues like semantic misalignment, inefficient interaction mechanisms, and sparse rewards in optimization spaces.", "method": "AwareCompiler combines structured knowledge integration, adaptive pass generation, and a hybrid data-driven training pipeline to enhance compiler optimization.", "result": "Experiments show AwareCompiler surpasses existing methods regarding performance and efficiency based on standard benchmarks.", "conclusion": "AwareCompiler provides an effective solution for compiler optimization, demonstrating its potential in optimizing performance and efficiency with a knowledge-driven approach."}}
{"id": "2510.11892", "pdf": "https://arxiv.org/pdf/2510.11892", "abs": "https://arxiv.org/abs/2510.11892", "authors": ["Kai Mei", "Jiang Guo", "Shuaichen Chang", "Mingwen Dong", "Dongkyu Lee", "Xing Niu", "Jiarong Jiang"], "title": "R-WoM: Retrieval-augmented World Model For Computer-use Agents", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) can serve as world models to enhance agent\ndecision-making in digital environments by simulating future states and\npredicting action outcomes, potentially eliminating costly trial-and-error\nexploration. However, this capability is fundamentally limited by LLMs'\ntendency toward hallucination and their reliance on static training knowledge,\nwhich can lead to compounding errors that inhibit long-horizon simulations. To\nsystematically investigate whether LLMs are appropriate for world modeling, we\nprobe two core capabilities of world models--future state prediction and reward\nestimation--through three tasks: next-state identification, full-procedure\nplanning alignment, and milestone transition recognition. Our analysis shows\nthat while LLMs effectively capture immediate next states and identify\nmeaningful state transitions, their performance rapidly degrades in\nfull-procedure planning. This highlights LLMs' limitations in reliably modeling\nenvironment dynamics over long horizons. To address these limitations, we\npropose the Retrieval-augmented World Model (R-WoM), which grounds LLM\nsimulations by incorporating factual, up-to-date knowledge retrieved from\nexternal tutorials. Experiments show that R-WoM achieves substantial\nimprovements of up to 25.3% (OSWorld) and 18.1% (WebArena) compared to\nbaselines, with particular advantages in longer-horizon simulations.", "AI": {"tldr": "The paper explores the use of Large Language Models (LLMs) as world models for decision-making in digital environments, highlighting their limitations in long-term simulations and proposing an improved Retrieval-augmented World Model (R-WoM).", "motivation": "Investigate whether LLMs can serve as reliable world models for predictive tasks like simulating future states and outcomes, despite their deficiencies such as hallucination and static knowledge.", "method": "The paper systematically probes LLM capabilities in world modeling tasks like next-state prediction, full-procedure planning, and milestone recognition, and introduces R-WoM, which enhances LLMs with external, factual knowledge for simulations.", "result": "Experiments demonstrate that R-WoM significantly improves LLM performance in long-term simulation tasks, with up to 25.3% improvement in OSWorld and 18.1% in WebArena compared to baselines.", "conclusion": "LLMs have potential in short-term world modeling but falter in long-term tasks due to knowledge and reasoning limits. R-WoM effectively addresses these gaps, making LLMs more viable for extended simulations."}}
{"id": "2510.12101", "pdf": "https://arxiv.org/pdf/2510.12101", "abs": "https://arxiv.org/abs/2510.12101", "authors": ["Pengyu Yin", "Shenghai Yuan", "Haozhi Cao", "Xingyu Ji", "Ruofei Bai", "Siyu Chen", "Lihua Xie"], "title": "Gaussian Semantic Field for One-shot LiDAR Global Localization", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "We present a one-shot LiDAR global localization algorithm featuring semantic\ndisambiguation ability based on a lightweight tri-layered scene graph. While\nlandmark semantic registration-based methods have shown promising performance\nimprovements in global localization compared with geometric-only methods,\nlandmarks can be repetitive and misleading for correspondence establishment. We\npropose to mitigate this problem by modeling semantic distributions with\ncontinuous functions learned from a population of Gaussian processes. Compared\nwith discrete semantic labels, the continuous functions capture finer-grained\ngeo-semantic information and also provide more detailed metric information for\ncorrespondence establishment. We insert this continuous function as the middle\nlayer between the object layer and the metric-semantic layer, forming a\ntri-layered 3D scene graph, serving as a light-weight yet performant backend\nfor one-shot localization. We term our global localization pipeline Outram-GSF\n(Gaussian semantic field) and conduct a wide range of experiments on publicly\navailable data sets, validating the superior performance against the current\nstate-of-the-art.", "AI": {"tldr": "A lightweight tri-layered scene graph is introduced for one-shot LiDAR global localization, enhancing semantic disambiguation using continuous functions derived from Gaussian processes.", "motivation": "Landmark-based methods improve global localization but may struggle with repetition and misleading correspondences.", "method": "Uses continuous semantic functions learned from Gaussian processes, organized into a tri-layered 3D scene graph for localization.", "result": "Achieved superior performance as demonstrated in experiments on public datasets against state-of-the-art methods.", "conclusion": "The proposed pipeline, Outram-GSF, efficiently addresses semantic disambiguation, proving its utility in global localization."}}
{"id": "2510.12141", "pdf": "https://arxiv.org/pdf/2510.12141", "abs": "https://arxiv.org/abs/2510.12141", "authors": ["Sabine Muzellec", "Yousif Kashef Alghetaa", "Simon Kornblith", "Kohitij Kar"], "title": "MAPS: Masked Attribution-based Probing of Strategies- A computational framework to align human and model explanations", "categories": ["q-bio.NC", "cs.CV"], "comment": null, "summary": "Human core object recognition depends on the selective use of visual\ninformation, but the strategies guiding these choices are difficult to measure\ndirectly. We present MAPS (Masked Attribution-based Probing of Strategies), a\nbehaviorally validated computational tool that tests whether explanations\nderived from artificial neural networks (ANNs) can also explain human vision.\nMAPS converts attribution maps into explanation-masked images (EMIs) and\ncompares image-by-image human accuracies on these minimal images with limited\npixel budgets with accuracies on the full stimuli. MAPS provides a principled\nway to evaluate and choose among competing ANN interpretability methods. In\nsilico, EMI-based behavioral similarity between models reliably recovers the\nground-truth similarity computed from their attribution maps, establishing\nwhich explanation methods best capture the model's strategy. When applied to\nhumans and macaques, MAPS identifies ANN-explanation combinations whose\nexplanations align most closely with biological vision, achieving the\nbehavioral validity of Bubble masks while requiring far fewer behavioral\ntrials. Because it needs only access to model attributions and a modest set of\nbehavioral data on the original images, MAPS avoids exhaustive psychophysics\nwhile offering a scalable tool for adjudicating explanations and linking human\nbehavior, neural activity, and model decisions under a common standard.", "AI": {"tldr": "The paper introduces MAPS, a computational tool that tests if explanations from artificial neural networks (ANNs) align with human vision, using explanation-masked images and behavioral comparisons.", "motivation": "To address the challenge of understanding human core object recognition by leveraging interpretable strategies in artificial neural networks and linking them to biological vision.", "method": "Developed MAPS (Masked Attribution-based Probing of Strategies), which converts ANN attribution maps into explanation-masked images (EMIs) and compares human and model accuracies using minimal and full stimulus images.", "result": "MAPS demonstrated its ability to reliably indicate the similarity between models based on their strategies and provided insight into ANN-explanation combinations that closely align with the vision strategies of humans and macaques.", "conclusion": "MAPS offers a scalable and efficient way to evaluate ANN explanation methods, linking human behavior, neural activity, and model decisions without requiring extensive behavioral trials."}}
{"id": "2510.11822", "pdf": "https://arxiv.org/pdf/2510.11822", "abs": "https://arxiv.org/abs/2510.11822", "authors": ["Suryaansh Jain", "Umair Z. Ahmed", "Shubham Sahai", "Ben Leong"], "title": "Beyond Consensus: Mitigating the Agreeableness Bias in LLM Judge Evaluations", "categories": ["cs.AI"], "comment": null, "summary": "New Large Language Models (LLMs) become available every few weeks, and modern\napplication developers confronted with the unenviable task of having to decide\nif they should switch to a new model. While human evaluation remains the gold\nstandard, it is costly and unscalable. The state-of-the-art approach is to use\nLLMs as evaluators ( LLM-as-a-judge), but this suffers from a critical flaw:\nLLMs exhibit a strong positive bias. We provide empirical evidence showing that\nwhile LLMs can identify valid outputs with high accuracy (i.e., True Positive\nRate 96%), they are remarkably poor at identifying invalid ones (i.e., True\nNegative Rate <25%). This systematic bias, coupled with class imbalance, often\nleads to inflated reliability scores.\n  While ensemble-based methods like majority voting can help, we show that they\nare not good enough. We introduce an optimal minority-veto strategy that is\nresilient to missing data and mitigates this bias to a large extent. For\nscenarios requiring even higher precision, we propose a novel regression-based\nframework that directly models the validator bias using a small set of\nhuman-annotated ground truth data. On a challenging code feedback task over 366\nhigh-school Python programs, our regression approach reduces the maximum\nabsolute error to just 1.2%, achieving a 2x improvement over the\nbest-performing ensemble of 14 state-of-the-art LLMs.", "AI": {"tldr": "This paper addresses the limitation of LLM-as-a-judge due to positive bias when evaluating outputs. It proposes a minority-veto strategy and a regression-based framework tailored to reduce bias and improve evaluation accuracy.", "motivation": "Frequent new LLMs compel developers to evaluate switching, but human evaluation is costly, and automated LLM-based evaluations are biased, necessitating improved evaluation techniques.", "method": "Proposed a minority-veto strategy to counter bias and a regression-based method using limited human-annotated data to directly model validator bias.", "result": "On a Python program feedback task, the regression approach reduced maximum absolute error to 1.2%, doubling the performance of the best ensemble model.", "conclusion": "The proposed solutions significantly enhance reliability and precision in automated LLM evaluation by addressing bias and ensuring scalability."}}
{"id": "2510.11813", "pdf": "https://arxiv.org/pdf/2510.11813", "abs": "https://arxiv.org/abs/2510.11813", "authors": ["Marcus Emmanuel Barnes", "Taher A. Ghaleb", "Safwat Hassan"], "title": "Task-Aware Reduction for Scalable LLM-Database Systems", "categories": ["cs.SE", "cs.CL", "cs.DB"], "comment": "Preprint. Accepted for presentation at the Workshop on Language\n  Models and Databases (LMD), co-located with CASCON 2025 (IEEE). The final\n  version will appear in IEEE Xplore", "summary": "Large Language Models (LLMs) are increasingly applied to data-intensive\nworkflows, from database querying to developer observability. Yet the\neffectiveness of these systems is constrained by the volume, verbosity, and\nnoise of real-world text-rich data such as logs, telemetry, and monitoring\nstreams. Feeding such data directly into LLMs is costly, environmentally\nunsustainable, and often misaligned with task objectives. Parallel efforts in\nLLM efficiency have focused on model- or architecture-level optimizations, but\nthe challenge of reducing upstream input verbosity remains underexplored. In\nthis paper, we argue for treating the token budget of an LLM as an attention\nbudget and elevating task-aware text reduction as a first-class design\nprinciple for language -- data systems. We position input-side reduction not as\ncompression, but as attention allocation: prioritizing information most\nrelevant to downstream tasks. We outline open research challenges for building\nbenchmarks, designing adaptive reduction pipelines, and integrating\ntoken-budget--aware preprocessing into database and retrieval systems. Our\nvision is to channel scarce attention resources toward meaningful signals in\nnoisy, data-intensive workflows, enabling scalable, accurate, and sustainable\nLLM--data integration.", "AI": {"tldr": "The paper proposes prioritizing upstream input reduction to optimize Large Language Models (LLMs) for handling noisy, voluminous, and verbose data in data-intensive workflows.", "motivation": "The existing challenges with LLMs stem from inefficiencies associated with processing large volumes of noisy and verbose real-world text data, which is both costly and environmentally unsustainable. Current optimization efforts overlook input verbosity reduction.", "method": "The paper introduces an approach that treats text input reduction as a key strategy for better attention allocation in LLMs. It reframes input-side reduction from mere compression to emphasizing relevant signals via adaptive reduction pipelines and token-budget-aware preprocessing.", "result": "The proposed method channels computational attention towards meaningful data signals, aiming to improve efficiency, scalability, and accuracy in integrating LLMs with text-rich workflows.", "conclusion": "The work advocates for upstream input reduction as a critical design principle for sustainable and effective LLM-powered systems, encouraging further research into benchmarks and reduction techniques."}}
{"id": "2510.11835", "pdf": "https://arxiv.org/pdf/2510.11835", "abs": "https://arxiv.org/abs/2510.11835", "authors": ["Yiming Liu", "Yuhui Zhang", "Dhruba Ghosh", "Ludwig Schmidt", "Serena Yeung-Levy"], "title": "Data or Language Supervision: What Makes CLIP Better than DINO?", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "EMNLP 2025 Findings", "summary": "CLIP outperforms self-supervised models like DINO as vision encoders for\nvision-language models (VLMs), but it remains unclear whether this advantage\nstems from CLIP's language supervision or its much larger training data. To\ndisentangle these factors, we pre-train CLIP and DINO under controlled settings\n-- using the same architecture, dataset, and training configuration --\nachieving similar ImageNet accuracy. Embedding analysis shows that CLIP\ncaptures high-level semantics (e.g., object categories, text), while DINO is\nmore responsive to low-level features like colors and styles. When integrated\ninto VLMs and evaluated on 20 VQA benchmarks, CLIP excels at text-intensive\ntasks, while DINO slightly outperforms on vision-centric ones. Variants of\nlanguage supervision (e.g., sigmoid loss, pre-trained language encoders) yield\nlimited gains. Our findings provide scientific insights into vision encoder\ndesign and its impact on VLM performance.", "AI": {"tldr": "The study compares CLIP and DINO as vision encoders for vision-language models, examining their training settings and benchmarking their performance on various tasks.", "motivation": "The paper aims to understand whether CLIP's superior performance over DINO in VLMs is due to language supervision or larger training data.", "method": "Researchers pre-trained CLIP and DINO under identical controlled settings, analyzing their embeddings and testing them in VLMs across multiple benchmarks.", "result": "CLIP excels in text-intensive tasks, while DINO slightly outperforms in vision-centric tasks. Language supervision variants offered limited improvements.", "conclusion": "Insights highlight the differing strengths of vision encoders and underscore their varied impact on VLM performance."}}
{"id": "2510.11769", "pdf": "https://arxiv.org/pdf/2510.11769", "abs": "https://arxiv.org/abs/2510.11769", "authors": ["Ruida Wang", "Jiarui Yao", "Rui Pan", "Shizhe Diao", "Tong Zhang"], "title": "GAR: Generative Adversarial Reinforcement Learning for Formal Theorem Proving", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Solving math problems through verifiable languages such as Lean has\nsignificantly impacted both the mathematics and computer science communities.\nCurrent state-of-the-art models are often trained with expensive online\nReinforcement Learning (RL) or expert iteration. However, these approaches rely\non fixed problem sets, which causes inefficient training and limits the model\nto tackle complex problems. To overcome these limitations, we propose GAR:\nGenerative Adversarial Reinforcement learning, a comprehensive RL training\nframework that jointly trains the problem composer and solver in an adversarial\nloop. GAR introduces an implicit curriculum learning mechanism, which aligns\ntask difficulty with the prover's evolving capability. It thereby improves the\ntraining efficiency and enables stronger performance of proving advanced\ntheorems. Experiments show that with GAR training, Goedel-Prover-V2-8B and\nDeepSeek-Prover-V2-7B achieve an average relative improvement in pass@32 of\n4.20% on MiniF2F-Test benchmark, while DeepSeek-Prover-V2's pass@32 on\nProofNet-Test increases from 22.58% to 25.81%. Beyond formal proving, GAR\nestablishes a general RL paradigm for co-evolution of problem generation and\nsolving under verifiable environments.", "AI": {"tldr": "The paper introduces Generative Adversarial Reinforcement learning (GAR) to jointly train problem composers and solvers, enhancing the training efficiency and theorem-proving performance.", "motivation": "To address inefficiencies and limitations in training models to solve math problems using verifiable languages, particularly issues with fixed problem sets hindering the tackling of complex problems.", "method": "The authors propose the GAR framework, which employs an adversarial training loop with joint training of problem composers and solvers, introducing implicit curriculum learning to align task difficulty with solver capabilities.", "result": "The GAR framework improved state-of-the-art models, like Goedel-Prover-V2-8B and DeepSeek-Prover-V2-7B, yielding a 4.20% performance improvement on MiniF2F-Test benchmarks and raising ProofNet-Test performance for DeepSeek-Prover-V2 from 22.58% to 25.81%.", "conclusion": "GAR enhances model efficiency and ensures better performance on advanced theorems. Moreover, it offers a broader RL paradigm that integrates problem generation and solving in verifiable settings."}}
{"id": "2510.11792", "pdf": "https://arxiv.org/pdf/2510.11792", "abs": "https://arxiv.org/abs/2510.11792", "authors": ["Nathan Wycoff"], "title": "On Thompson Sampling and Bilateral Uncertainty in Additive Bayesian Optimization", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "In Bayesian Optimization (BO), additive assumptions can mitigate the twin\ndifficulties of modeling and searching a complex function in high dimension.\nHowever, common acquisition functions, like the Additive Lower Confidence\nBound, ignore pairwise covariances between dimensions, which we'll call\n\\textit{bilateral uncertainty} (BU), imposing a second layer of approximations.\nWhile theoretical results indicate that asymptotically not much is lost in\ndoing so, little is known about the practical effects of this assumption in\nsmall budgets. In this article, we show that by exploiting conditional\nindependence, Thompson Sampling respecting BU can be efficiently conducted. We\nuse this fact to execute an empirical investigation into the loss incurred by\nignoring BU, finding that the additive approximation to Thompson Sampling does\nindeed have, on balance, worse performance than the exact method, but that this\ndifference is of little practical significance. This buttresses the theoretical\nunderstanding and suggests that the BU-ignoring approximation is sufficient for\nBO in practice, even in the non-asymptotic regime.", "AI": {"tldr": "The study evaluates whether ignoring bilateral uncertainty (pairwise covariances) in Bayesian Optimization significantly impacts performance, finding it to be practically insignificant.", "motivation": "Additive assumptions simplify high-dimensional Bayesian Optimization but commonly used methods ignore pairwise covariances, raising concerns about their impact on practical performance, especially with small budgets.", "method": "The paper leverages conditional independence to conduct Thompson Sampling that incorporates bilateral uncertainty, and empirically compares its results with approximations ignoring BU.", "result": "Ignoring bilateral uncertainty leads to slightly worse, but practically insignificant results, confirming the sufficiency of simpler approximations in small-budget Bayesian Optimization.", "conclusion": "Simplified additive methods without considering bilateral uncertainty are adequate for practical Bayesian Optimization, even outside asymptotic regimes."}}
{"id": "2510.12436", "pdf": "https://arxiv.org/pdf/2510.12436", "abs": "https://arxiv.org/abs/2510.12436", "authors": ["Valentin Seitz", "Jordy Trilaksono", "Marta Garcia-Gasulla"], "title": "TALP-Pages: An easy-to-integrate continuous performance monitoring framework", "categories": ["cs.DC", "cs.PF"], "comment": null, "summary": "Ensuring good performance is a key aspect in the development of codes that\ntarget HPC machines. As these codes are under active development, the necessity\nto detect performance degradation early in the development process becomes\napparent. In addition, having meaningful insight into application scaling\nbehavior tightly coupled to the development workflow is helpful. In this paper,\nwe introduce TALP-Pages, an easy-to-integrate framework that enables developers\nto get fast and in-repository feedback about their code performance using\nestablished fundamental performance and scaling factors. The framework relies\non TALP, which enables the on-the-fly collection of these metrics. Based on a\nfolder structure suited for CI which contains the files generated by TALP,\nTALP-Pages generates an HTML report with visualizations of the performance\nfactor regression as well as scaling-efficiency tables. We compare TALP-Pages\nto tracing-based tools in terms of overhead and post-processing requirements\nand find that TALP-Pages can produce the scaling-efficiency tables faster and\nunder tighter resource constraints. To showcase the ease of use and\neffectiveness of this approach, we extend the current CI setup of GENE-X with\nonly minimal changes required and showcase the ability to detect and explain a\nperformance improvement.", "AI": {"tldr": "This paper presents TALP-Pages, a framework to provide quick feedback on code performance during development using metrics and HTML visualization.", "motivation": "Performance degradation detection during HPC code development is critical to maintain code efficiency, along with insights into application scaling.", "method": "The authors propose TALP-Pages, which integrates with CI workflows and uses TALP to collect performance metrics on-the-fly, generating HTML reports with visualizations and scaling-efficiency tables.", "result": "TALP-Pages is evaluated against tracing-based tools, demonstrating quicker generation of scaling-efficiency tables with lower resource usage.", "conclusion": "TALP-Pages is easy to integrate and effective for identifying and understanding performance changes in HPC code development workflows."}}
{"id": "2510.12293", "pdf": "https://arxiv.org/pdf/2510.12293", "abs": "https://arxiv.org/abs/2510.12293", "authors": ["Fei Ren", "Sifan Wang", "Pei-Zhi Zhuang", "Hai-Sui Yu", "He Yang"], "title": "General Fourier Feature Physics-Informed Extreme Learning Machine (GFF-PIELM) for High-Frequency PDEs", "categories": ["cs.LG", "cs.NE", "physics.comp-ph"], "comment": null, "summary": "Conventional physics-informed extreme learning machine (PIELM) often faces\nchallenges in solving partial differential equations (PDEs) involving\nhigh-frequency and variable-frequency behaviors. To address these challenges,\nwe propose a general Fourier feature physics-informed extreme learning machine\n(GFF-PIELM). We demonstrate that directly concatenating multiple Fourier\nfeature mappings (FFMs) and an extreme learning machine (ELM) network makes it\ndifficult to determine frequency-related hyperparameters. Fortunately, we find\nan alternative to establish the GFF-PIELM in three main steps. First, we\nintegrate a variation of FFM into ELM as the Fourier-based activation function,\nso there is still one hidden layer in the GFF-PIELM framework. Second, we\nassign a set of frequency coefficients to the hidden neurons, which enables ELM\nnetwork to capture diverse frequency components of target solutions. Finally,\nwe develop an innovative, straightforward initialization method for these\nhyperparameters by monitoring the distribution of ELM output weights. GFF-PIELM\nnot only retains the high accuracy, efficiency, and simplicity of the PIELM\nframework but also inherits the ability of FFMs to effectively handle\nhigh-frequency problems. We carry out five case studies with a total of ten\nnumerical examples to highlight the feasibility and validity of the proposed\nGFF-PIELM, involving high frequency, variable frequency, multi-scale behaviour,\nirregular boundary and inverse problems. Compared to conventional PIELM, the\nGFF-PIELM approach significantly improves predictive accuracy without\nadditional cost in training time and architecture complexity. Our results\nconfirm that that PIELM can be extended to solve high-frequency and\nvariable-frequency PDEs with high accuracy, and our initialization strategy may\nfurther inspire advances in other physics-informed machine learning (PIML)\nframeworks.", "AI": {"tldr": "This paper introduces a Fourier feature physics-informed extreme learning machine (GFF-PIELM) to address shortcomings in solving PDEs with high- and variable-frequency behaviors. It enhances the conventional PIELM technique with improved predictive accuracy and efficiency.", "motivation": "The motivation is to address the limited capability of conventional PIELM frameworks in solving partial differential equations with high-frequency and variable-frequency behaviors, which are critical in complex physics modeling.", "method": "The method integrates Fourier feature mappings into the ELM network by using Fourier-based activation functions. It assigns frequency coefficients to hidden neurons and employs a novel initialization method for hyperparameters based on ELM output weights.", "result": "The proposed GFF-PIELM is demonstrated to improve predictive accuracy compared to conventional PIELM across diverse numerical examples, without adding extra computational costs or complexity to the architecture.", "conclusion": "GFF-PIELM extends the capabilities of PIELM to efficiently solve PDEs with high-frequency and variable-frequency behaviors. The novel initialization strategy has potential applicability in broader physics-informed machine learning frameworks."}}
{"id": "2510.12131", "pdf": "https://arxiv.org/pdf/2510.12131", "abs": "https://arxiv.org/abs/2510.12131", "authors": ["Haobin Ni", "Robbert van Renesse", "Greg Morrisett"], "title": "Functional Reasoning for Distributed Systems with Failures", "categories": ["cs.PL"], "comment": null, "summary": "Distributed system theory literature often argues for correctness using an\ninformal, Hoare-like style of reasoning. While these arguments are intuitive,\nthey have not all been foolproof, and whether they directly correspond to\nformal proofs is in question. We formally ground this kind of reasoning and\nconnect it to standard formal approaches through language design and\nmeta-analysis, which leads to a functional style of compositional formal\nreasoning for a class of distributed systems, including cases involving\nByzantine faults. The core of our approach is twin languages: Sync and Async,\nwhich formalize the insight from distributed system theory that an asynchronous\nsystem can be reduced to a synchronous system for more straightforward\nreasoning under certain conditions. Sync describes a distributed system as a\nsingle, synchronous, data-parallel program. It restricts programs syntactically\nand has a functional denotational semantics suitable for Hoare-style formal\nreasoning. Async models a distributed system as a collection of interacting\nmonadic programs, one for each non-faulty node in the system. It has a standard\ntrace-based operational semantics, modeling asynchrony with interleaving. Sync\ncompiles to Async and can then be extracted to yield executable code. We prove\nthat any safety property proven for a Sync program in its denotational\nsemantics is preserved in the operational semantics of its compiled Async\nprograms. We implement the twin languages in Rocq and verify the safety\nproperties of two fault-tolerant consensus protocols: BOSCO and SeqPaxos.", "AI": {"tldr": "This paper introduces a formalized reasoning framework for distributed systems using twin languages, Sync (synchronous) and Async (asynchronous). These frameworks provide a structured way to argue correctness and verify safety properties in distributed protocols, demonstrated on BOSCO and SeqPaxos.", "motivation": "To address the informal and error-prone reasoning in distributed system theory and bridge the gap with formal methods, enabling reliable correctness arguments for distributed systems.", "method": "The authors propose twin languages, Sync and Async. Sync uses a synchronous model for simplifying reasoning and employs a denotational semantics for formal proofs. Async models real-world asynchrony with interleaving semantics. Sync programs are compiled into Async programs, maintaining safety properties proven in Sync.", "result": "The authors prove the preservation of safety properties from Sync to Async and implement these concepts in the Rocq framework. They validate their approach by verifying safety properties of two consensus protocols, BOSCO and SeqPaxos.", "conclusion": "The proposed twin language approach provides a robust, compositional method for formal reasoning about distributed systems, bridging theory with practical implementation while ensuring safety property preservation."}}
{"id": "2510.11905", "pdf": "https://arxiv.org/pdf/2510.11905", "abs": "https://arxiv.org/abs/2510.11905", "authors": ["Patrick Haller", "Mark Ibrahim", "Polina Kirichenko", "Levent Sagun", "Samuel J. Bell"], "title": "LLM Knowledge is Brittle: Truthfulness Representations Rely on Superficial Resemblance", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "For Large Language Models (LLMs) to be reliable, they must learn robust\nknowledge that can be generally applied in diverse settings -- often unlike\nthose seen during training. Yet, extensive research has shown that LLM\nperformance can be brittle, with models exhibiting excessive sensitivity to\ntrivial input variations. In this work, we explore whether this brittleness is\na direct result of unstable internal knowledge representations. To explore this\nquestion, we build on previous work showing that LLM representations encode\nstatement truthfulness -- i.e., true, factual statements can be easily\nseparated from false, inaccurate ones. Specifically, we test the robustness of\nlearned knowledge by evaluating representation separability on samples that\nhave undergone superficial transformations to drive them out-of-distribution\n(OOD), such as typos or reformulations. By applying semantically-preserving\nperturbations, we study how separability degrades as statements become more\nOOD, across four LLM families, five evaluation datasets, and three knowledge\nprobing methods. Our results reveal that internal representations of statement\ntruthfulness collapse as the samples' presentations become less similar to\nthose seen during pre-training. While LLMs can often distinguish between true\nand false statements when they closely resemble the pre-training data, this\nability is highly dependent on the statement's exact surface form. These\nfindings offer a possible explanation for brittle benchmark performance: LLMs\nmay learn shallow, non-robust knowledge representations that allow for only\nlimited generalizability. Our work presents a fundamental challenge for the\nutility of truthfulness probes, and more broadly, calls for further research on\nimproving the robustness of learned knowledge representations.", "AI": {"tldr": "The paper investigates whether the brittleness of LLM performance is caused by unstable internal knowledge representations, revealing that LLM truthfulness recognition is sensitive to superficial input variations.", "motivation": "To address the observed brittleness in LLMs, which often fail to apply their knowledge in diverse, unseen settings unlike their training data.", "method": "The study evaluates representation separability under semantically-preserving perturbations that drive statements out-of-distribution across multiple LLM families, datasets, and probing methods.", "result": "The robustness of LLM internal representations degrades significantly under OOD transformations, indicating reliance on shallow knowledge structures sensitive to exact surface forms.", "conclusion": "LLMs exhibit non-robust knowledge representations, limiting their generalizability and calling for research to enhance representation stability and robustness."}}
{"id": "2510.12169", "pdf": "https://arxiv.org/pdf/2510.12169", "abs": "https://arxiv.org/abs/2510.12169", "authors": ["Akshay Naik", "William R. Norris", "Dustin Nottage", "Ahmet Soylemezoglu"], "title": "Hybrid Terrain-Aware Path Planning: Integrating VD--RRT\\(^{*}\\) Exploration and VD--D\\(^{*}\\) Lite Repair", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Autonomous ground vehicles operating off-road must plan curvature-feasible\npaths while accounting for spatially varying soil strength and slope hazards in\nreal time. We present a continuous state--cost metric that combines a Bekker\npressure--sinkage model with elevation-derived slope and attitude penalties.\nThe resulting terrain cost field is analytic, bounded, and monotonic in soil\nmodulus and slope, ensuring well-posed discretization and stable updates under\nsensor noise. This metric is evaluated on a lattice with exact steering\nprimitives: Dubins and Reeds--Shepp motions for differential drive and\ntime-parameterized bicycle arcs for Ackermann steering. Global exploration is\nperformed using Vehicle-Dynamics RRT\\(^{*}\\), while local repair is managed by\nVehicle-Dynamics D\\(^{*}\\) Lite, enabling millisecond-scale replanning without\nheuristic smoothing. By separating the terrain--vehicle model from the planner,\nthe framework provides a reusable basis for deterministic, sampling-based, or\nlearning-driven planning in deformable terrain. Hardware trials on an off-road\nplatform demonstrate real-time navigation across soft soil and slope\ntransitions, supporting reliable autonomy in unstructured environments.", "AI": {"tldr": "This paper focuses on real-time navigation for autonomous ground vehicles in off-road environments, introducing a new continuous terrain cost metric and efficient planning algorithms for handling deformable terrain.", "motivation": "The motivation is to enable autonomous ground vehicles to navigate challenging off-road terrains with varying soil strength and slopes, overcoming issues like sensor noise and path feasibility.", "method": "The paper proposes a continuous terrain cost metric based on the Bekker pressure-sinkage model and slope penalties, combined with a planning framework using Vehicle-Dynamics RRT* for exploration and D* Lite for real-time local path repair.", "result": "The proposed methods enable millisecond-scale replanning, ensuring curvature-feasible paths and stability under noisy sensors, and they are validated with hardware trials in unstructured off-road environments.", "conclusion": "The framework provides a robust basis for real-time autonomous navigation in deformable terrains, with potential for integration into deterministic, sampling-based, or learning-driven planning methods."}}
{"id": "2510.12228", "pdf": "https://arxiv.org/pdf/2510.12228", "abs": "https://arxiv.org/abs/2510.12228", "authors": ["Shunsuke Onoo", "Yoshihiro Nagano", "Yukiyasu Kamitani"], "title": "Readout Representation: Redefining Neural Codes by Input Recovery", "categories": ["q-bio.NC"], "comment": null, "summary": "Sensory representation is typically understood through a hierarchical-causal\nframework where progressively abstract features are extracted sequentially.\nHowever, this causal view fails to explain misrepresentation, a phenomenon\nbetter handled by an informational view based on decodable content. This\ncreates a tension: how does a system that abstracts away details still preserve\nthe fine-grained information needed for downstream functions? We propose\nreadout representation to resolve this, defining representation by the\ninformation recoverable from features rather than their causal origin.\nEmpirically, we show that inputs can be accurately reconstructed even from\nheavily perturbed mid-level features, demonstrating that a single input\ncorresponds to a broad, redundant region of feature space, challenging the\ncausal mapping perspective. To quantify this property, we introduce\nrepresentation size, a metric linked to model robustness and representational\nredundancy. Our framework offers a new lens for analyzing how both biological\nand artificial neural systems learn complex features while maintaining robust,\ninformation-rich representations of the world.", "AI": {"tldr": "The paper introduces 'readout representation,' a framework focused on recovering information from features rather than relying on their causal origin, challenging traditional hierarchical-causal perspectives in sensory representation.", "motivation": "To address the tension between abstracting away details in sensory representations while preserving fine-grained information needed for downstream tasks.", "method": "The authors proposed a metric called 'representation size' and empirically demonstrated the reconstructability of inputs from perturbed mid-level features, highlighting redundancy in the feature space.", "result": "Empirical findings showed that inputs can still be reconstructed even from heavily altered mid-level features, indicating representational redundancy and challenging traditional causal mapping paradigms.", "conclusion": "The proposed readout framework bridges understanding of biological and artificial systems by emphasizing information-rich and robust sensory representations over causal hierarchies."}}
{"id": "2510.11977", "pdf": "https://arxiv.org/pdf/2510.11977", "abs": "https://arxiv.org/abs/2510.11977", "authors": ["Sayash Kapoor", "Benedikt Stroebl", "Peter Kirgis", "Nitya Nadgir", "Zachary S Siegel", "Boyi Wei", "Tianci Xue", "Ziru Chen", "Felix Chen", "Saiteja Utpala", "Franck Ndzomga", "Dheeraj Oruganty", "Sophie Luskin", "Kangheng Liu", "Botao Yu", "Amit Arora", "Dongyoon Hahm", "Harsh Trivedi", "Huan Sun", "Juyong Lee", "Tengjun Jin", "Yifan Mai", "Yifei Zhou", "Yuxuan Zhu", "Rishi Bommasani", "Daniel Kang", "Dawn Song", "Peter Henderson", "Yu Su", "Percy Liang", "Arvind Narayanan"], "title": "Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "AI agents have been developed for complex real-world tasks from coding to\ncustomer service. But AI agent evaluations suffer from many challenges that\nundermine our understanding of how well agents really work. We introduce the\nHolistic Agent Leaderboard (HAL) to address these challenges. We make three\nmain contributions. First, we provide a standardized evaluation harness that\norchestrates parallel evaluations across hundreds of VMs, reducing evaluation\ntime from weeks to hours while eliminating common implementation bugs. Second,\nwe conduct three-dimensional analysis spanning models, scaffolds, and\nbenchmarks. We validate the harness by conducting 21,730 agent rollouts across\n9 models and 9 benchmarks in coding, web navigation, science, and customer\nservice with a total cost of about $40,000. Our analysis reveals surprising\ninsights, such as higher reasoning effort reducing accuracy in the majority of\nruns. Third, we use LLM-aided log inspection to uncover previously unreported\nbehaviors, such as searching for the benchmark on HuggingFace instead of\nsolving a task, or misusing credit cards in flight booking tasks. We share all\nagent logs, comprising 2.5B tokens of language model calls, to incentivize\nfurther research into agent behavior. By standardizing how the field evaluates\nagents and addressing common pitfalls in agent evaluation, we hope to shift the\nfocus from agents that ace benchmarks to agents that work reliably in the real\nworld.", "AI": {"tldr": "The paper introduces the Holistic Agent Leaderboard (HAL) to standardize AI agent evaluation, focusing on real-world reliability over benchmark performance.", "motivation": "Existing AI agent evaluations face challenges that limit understanding of agent performance in practical scenarios.", "method": "HAL standardizes evaluation by orchestrating parallel tests on hundreds of VMs, conducts comprehensive analysis across models, scaffolds, and benchmarks, and uses LLM-aided log inspection to detect unique behaviors.", "result": "Evaluations of 21,730 agent rollouts revealed surprising insights, such as reduced accuracy with increased reasoning efforts, and uncovered unusual agent behaviors.", "conclusion": "HAL sets a standardized framework for evaluating AI agents, aiming to shift focus from scoring well on benchmarks to performing reliably in real-world tasks."}}
{"id": "2510.11838", "pdf": "https://arxiv.org/pdf/2510.11838", "abs": "https://arxiv.org/abs/2510.11838", "authors": ["Xu Yang", "Jiayuan Zhou", "Michael Pacheco", "Wenhan Zhu", "Pengfei He", "Shaowei Wang", "Kui Liu", "Ruiqi Pan"], "title": "Lingxi: Repository-Level Issue Resolution Framework Enhanced by Procedural Knowledge Guided Scaling", "categories": ["cs.SE"], "comment": null, "summary": "Driven by the advancements of Large Language Models (LLMs), LLM-powered\nagents are making significant improvements in software engineering tasks, yet\nstruggle with complex, repository-level issue resolution. Existing agent-based\nmethods have two key limitations. First, they lack of procedural knowledge\n(i.e., how an issue is fixed step-by-step and rationales behind it) to learn\nand leverage for issue resolution. Second, they rely on massive computational\npower to blindly explore the solution space. % To address those limitations, we\npropose Lingxi, an issue resolution framework that leverages procedural\nknowledge extracted from historical issue-fixing data to guide agents in\nsolving repository-level issues. \\ourTool first constructs this knowledge\noffline through a hierarchical abstraction mechanism, enabling agents to learn\nthe how and why behind a fix, not just the final solution. During online\napplication, it employs a knowledge-driven scaling method that leverages the\nprocedural knowledge of similar issues to intelligently analyze the target\nissue from multiple perspectives, in sharp contrast to undirected, brute-force\nexploration. % Lingxi successfully resolves 74.6\\% of bugs on the SWE-bench\nVerified benchmark in Past@1 setting, outperforming five state-of-the-art\ntechniques by a significant margin (5.4\\% to 14.9\\%). Our comprehensive\nablation study confirmed that the success of Lingxi comes directly from its use\nof procedural knowledge. Without it, the performance gains from scaling alone\nis negligible. Our qualitative study further shows that the ``design patterns\n$\\&$ coding practices'' is the most critical knowledge aspect, and that the\nroles of different knowledge aspects switch across different stages (i.e.,\nanalysis, planning, and fixing).", "AI": {"tldr": "This paper introduces Lingxi, a framework utilizing procedural knowledge to improve issue resolution in software repositories, addressing inefficiencies in LLM-powered agents.", "motivation": "To overcome deficiencies of LLM-based agents in repository-level issue resolution, including procedural knowledge gaps and inefficient computational exploration.", "method": "Lingxi extracts procedural knowledge from historical issue-fixing data using hierarchical abstraction and applies knowledge-driven scaling to analyze and resolve issues intelligently.", "result": "Lingxi achieves a 74.6% success rate on the SWE-bench Verified benchmark in Past@1 setting, outperforming existing methods by 5.4% to 14.9%.", "conclusion": "Procedural knowledge drives Lingxi's success by enabling efficient and effective issue resolution, with qualitative insights highlighting the significance of design patterns and coding practices."}}
{"id": "2510.11883", "pdf": "https://arxiv.org/pdf/2510.11883", "abs": "https://arxiv.org/abs/2510.11883", "authors": ["Sicheng Zhou", "Lei Wu", "Cao Xiao", "Parminder Bhatia", "Taha Kass-Hout"], "title": "MammoDINO: Anatomically Aware Self-Supervision for Mammographic Images", "categories": ["cs.CV", "cs.AI", "1.2"], "comment": "5 pages", "summary": "Self-supervised learning (SSL) has transformed vision encoder training in\ngeneral domains but remains underutilized in medical imaging due to limited\ndata and domain specific biases. We present MammoDINO, a novel SSL framework\nfor mammography, pretrained on 1.4 million mammographic images. To capture\nclinically meaningful features, we introduce a breast tissue aware data\naugmentation sampler for both image-level and patch-level supervision and a\ncross-slice contrastive learning objective that leverages 3D digital breast\ntomosynthesis (DBT) structure into 2D pretraining. MammoDINO achieves\nstate-of-the-art performance on multiple breast cancer screening tasks and\ngeneralizes well across five benchmark datasets. It offers a scalable,\nannotation-free foundation for multipurpose computer-aided diagnosis (CAD)\ntools for mammogram, helping reduce radiologists' workload and improve\ndiagnostic efficiency in breast cancer screening.", "AI": {"tldr": "MammoDINO introduces a novel self-supervised learning framework tailored for mammography, achieving state-of-the-art results in breast cancer screening tasks.", "motivation": "The paper seeks to overcome challenges in medical imaging SSL caused by limited data and domain biases by transforming the way mammography images are processed and analyzed.", "method": "The framework incorporates breast tissue-aware data augmentation, cross-slice contrastive learning using 3D DBT structure, and pretraining on a dataset of 1.4 million mammographic images.", "result": "MammoDINO demonstrates state-of-the-art performance on various breast cancer screening tasks and generalizes well across five benchmark datasets.", "conclusion": "This scalable and annotation-free framework could enhance computer-aided diagnosis tools and help radiologists in breast cancer screening, improving diagnostic efficiency."}}
{"id": "2510.11827", "pdf": "https://arxiv.org/pdf/2510.11827", "abs": "https://arxiv.org/abs/2510.11827", "authors": ["Simone Mungari", "Ettore Ritacco", "Pietro Sabatino"], "title": "Combining Euclidean and Hyperbolic Representations for Node-level Anomaly Detection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Node-level anomaly detection (NAD) is challenging due to diverse structural\npatterns and feature distributions. As such, NAD is a critical task with\nseveral applications which range from fraud detection, cybersecurity, to\nrecommendation systems. We introduce Janus, a framework that jointly leverages\nEuclidean and Hyperbolic Graph Neural Networks to capture complementary aspects\nof node representations. Each node is described by two views, composed by the\noriginal features and structural features derived from random walks and\ndegrees, then embedded into Euclidean and Hyperbolic spaces. A multi\nGraph-Autoencoder framework, equipped with a contrastive learning objective as\nregularization term, aligns the embeddings across the Euclidean and Hyperbolic\nspaces, highlighting nodes whose views are difficult to reconcile and are thus\nlikely anomalous. Experiments on four real-world datasets show that Janus\nconsistently outperforms shallow and deep baselines, empirically demonstrating\nthat combining multiple geometric representations provides a robust and\neffective approach for identifying subtle and complex anomalies in graphs.", "AI": {"tldr": "Janus uses a combination of Euclidean and Hyperbolic Graph Neural Networks for effective node-level anomaly detection by contrasting feature and structural embeddings.", "motivation": "Node-level anomaly detection is critical for tasks such as fraud detection and cybersecurity, necessitating a robust framework to handle diverse structural patterns and feature distributions.", "method": "Janus introduces a multi Graph-Autoencoder framework using Euclidean and Hyperbolic embeddings, aligned with a contrastive learning objective to spot anomalies where embeddings diverge.", "result": "Experiments on four real-world datasets demonstrate that Janus outperforms existing baselines in detecting subtle and complex graph anomalies.", "conclusion": "By leveraging multiple geometric representations, Janus effectively identifies challenging anomalies in graphs, establishing a novel framework for robust anomaly detection."}}
{"id": "2510.11871", "pdf": "https://arxiv.org/pdf/2510.11871", "abs": "https://arxiv.org/abs/2510.11871", "authors": ["Poorbita Kundu", "Nathan Wycoff"], "title": "Active Subspaces in Infinite Dimension", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Active subspace analysis uses the leading eigenspace of the gradient's second\nmoment to conduct supervised dimension reduction. In this article, we extend\nthis methodology to real-valued functionals on Hilbert space. We define an\noperator which coincides with the active subspace matrix when applied to a\nEuclidean space. We show that many of the desirable properties of Active\nSubspace analysis extend directly to the infinite dimensional setting. We also\npropose a Monte Carlo procedure and discuss its convergence properties.\nFinally, we deploy this methodology to create visualizations and improve\nmodeling and optimization on complex test problems.", "AI": {"tldr": "The paper extends active subspace analysis to real-valued functionals on Hilbert spaces and investigates its properties in infinite dimensional settings.", "motivation": "To generalize active subspace analysis from Euclidean spaces to Hilbert spaces for better visualization, modeling, and optimization.", "method": "The paper introduces an operator for Hilbert spaces, adapts Monte Carlo procedures, and analyzes its convergence properties.", "result": "Methodology showed effectiveness in creating visualizations and optimizing models on complex test problems.", "conclusion": "Active subspace analysis can be successfully extended to Hilbert spaces, retaining desirable properties and improving modeling and optimization tasks."}}
{"id": "2510.12597", "pdf": "https://arxiv.org/pdf/2510.12597", "abs": "https://arxiv.org/abs/2510.12597", "authors": ["Ilya Baldin", "Michael Goodrich", "Vardan Gyurjyan", "Graham Heyes", "Derek Howard", "Yatish Kumar", "David Lawrence", "Brad Sawatzky", "Stacey Sheldon", "Carl Timmer"], "title": "Low Latency, High Bandwidth Streaming of Experimental Data with EJFAT", "categories": ["cs.DC"], "comment": null, "summary": "Thomas Jefferson National Accelerator Facility (JLab) has partnered with\nEnergy Sciences Network (ESnet) to define and implement an edge to compute\ncluster computational load balancing acceleration architecture. The ESnet-JLab\nFPGA Accelerated Transport (EJFAT) architecture focuses on FPGA acceleration to\naddress compression, fragmentation, UDP packet destination redirection (Network\nAddress Translation (NAT)) and decompression and reassembly.\n  EJFAT seamlessly integrates edge and cluster computing to support direct\nprocessing of streamed experimental data. This will directly benefit the JLab\nscience program as well as data centers of the future that require high\nthroughput and low latency for both time-critical data acquisition systems and\ndata center workflows.\n  The EJFAT project will be presented along with how it is synergistic with\nother DOE activities such as an Integrated Research Infrastructure (IRI), and\nrecent results using data sources at JLab, an EJFAT LB at ESnet, and\ncomputational cluster resources at Lawrence Berkeley National Laboratory\n(LBNL).", "AI": {"tldr": "The paper introduces EJFAT, an architecture utilizing FPGA acceleration for efficient edge-to-cluster computational load balancing, benefiting both experimental science and data centers.", "motivation": "The aim is to streamline data processing between edge devices and compute clusters, improving throughput and latency for critical systems and workflows.", "method": "The EJFAT architecture employs FPGA acceleration for data compression, fragmentation, NAT (UDP packet redirection), decompression, and reassembly for seamless integration of edge and cluster computing.", "result": "The EJFAT system successfully integrates edge devices and computational clusters, enhancing experimental data processing and demonstrating synergy with DOE initiatives.", "conclusion": "EJFAT supports high-throughput, low-latency operations essential for science programs and advanced data center requirements, showcasing practical applications at JLab and LBNL."}}
{"id": "2510.12295", "pdf": "https://arxiv.org/pdf/2510.12295", "abs": "https://arxiv.org/abs/2510.12295", "authors": ["Roberto M. Amadio"], "title": "Operational methods in semantics", "categories": ["cs.PL", "cs.LO", "ACM F.3.2", "F.3.2"], "comment": null, "summary": "The focus of these lecture notes is on abstract models and basic ideas and\nresults that relate to the operational semantics of programming languages\nlargely conceived. The approach is to start with an abstract description of the\ncomputation steps of programs and then to build on top semantic equivalences,\nspecification languages, and static analyses. While other approaches to the\nsemantics of programming languages are possible, it appears that the\noperational one is particularly effective in that it requires a moderate level\nof mathematical sophistication and scales reasonably well to a large variety of\nprogramming features. In practice, operational semantics is a suitable\nframework to build portable language implementations and to specify and test\nprogram properties. It is also used routinely to tackle more ambitious tasks\nsuch as proving the correctness of a compiler or a static analyzer.", "AI": {"tldr": "This paper introduces abstract models for the operational semantics of programming languages, covering equivalences, specification languages, and static analyses.", "motivation": "To effectively study and utilize an approach for describing and analyzing the computation steps and semantics of programming languages.", "method": "The approach focuses on operational semantics, starting with abstract description of computation steps and building on it to develop formal structures for semantic analysis.", "result": "Operational semantics is demonstrated to be effective, requiring moderate mathematical sophistication and being versatile across various programming features.", "conclusion": "Operational semantics provides a practical and scalable framework for tasks like building portable implementations and proving correctness of compilers or analyzers."}}
{"id": "2510.11919", "pdf": "https://arxiv.org/pdf/2510.11919", "abs": "https://arxiv.org/abs/2510.11919", "authors": ["Armel Zebaze", "Rachel Bawden", "Beno\u00eet Sagot"], "title": "LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens", "categories": ["cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs) have led to new possibilities in terms of\nproblem-solving, through the devising of a natural language thought process\nprior to answering a query. While their capabilities are well known across\nmathematics and coding tasks, their impact on the task of machine translation\n(MT) remains underexplored. In this work, we explore the benefits of the\ngeneration of intermediate tokens when performing MT across multiple language\npairs of different levels of resourcedness and multiple setups. We find that\n\"thinking tokens\" do not help LRMs better perform MT. This result generalizes\nto models fine-tuned to reason before translating using distilled chain of\nthought (CoT) inspired by human translators' practices. Specifically,\nfine-tuning a model with synthetic CoT explanations detailing how to translate\nstep-by-step does not outperform standard input-output fine-tuning. However,\nconstructing the intermediate tokens by combining the outputs of modular\ntranslation-specific prompting strategies results in improvements. Our findings\nunderscore that the contribution of intermediate tokens during fine-tuning\nhighly depends on the presence of translation attempts within them. More\nbroadly, our results suggest that using a teacher to refine target translations\nor to expand parallel corpora is more impactful than distilling their CoT\nexplanations into \"thinking\" MT models.", "AI": {"tldr": "The study evaluates the utility of \"thinking tokens\" for machine translation in large reasoning models and concludes that they mostly do not enhance performance unless constructed through modular translation-specific prompting strategies.", "motivation": "To explore whether intermediate reasoning processes, represented as \"thinking tokens,\" can improve machine translation across various languages and setups.", "method": "The researchers tested the impact of intermediate tokens by comparing standard input-output fine-tuning, fine-tuning with synthetic chain-of-thought explanations, and modular translation-specific prompting strategies.", "result": "\"Thinking tokens\" generally fail to boost machine translation performance unless designed through modular translation-specific methods.", "conclusion": "Using reasoning tokens is not universally beneficial for machine translation, and alternative strategies like teacher-guided refinement or parallel corpus expansion yield greater improvements."}}
{"id": "2510.12206", "pdf": "https://arxiv.org/pdf/2510.12206", "abs": "https://arxiv.org/abs/2510.12206", "authors": ["Pin-Lun Chen", "Chi-Hsi Kung", "Che-Han Chang", "Wei-Chen Chiu", "Yi-Ting Chen"], "title": "Controllable Collision Scenario Generation via Collision Pattern Prediction", "categories": ["cs.RO", "cs.LG"], "comment": "8 pages, 3 figures. Submitted to IEEE International Conference on\n  Robotics and Automation (ICRA) 2026", "summary": "Evaluating the safety of autonomous vehicles (AVs) requires diverse,\nsafety-critical scenarios, with collisions being especially important yet rare\nand unsafe to collect in the real world. Therefore, the community has been\nfocusing on generating safety-critical scenarios in simulation. However,\ncontrolling attributes such as collision type and time-to-accident (TTA)\nremains challenging. We introduce a new task called controllable collision\nscenario generation, where the goal is to produce trajectories that realize a\nuser-specified collision type and TTA, to investigate the feasibility of\nautomatically generating desired collision scenarios. To support this task, we\npresent COLLIDE, a large-scale collision scenario dataset constructed by\ntransforming real-world driving logs into diverse collisions, balanced across\nfive representative collision types and different TTA intervals. We propose a\nframework that predicts Collision Pattern, a compact and interpretable\nrepresentation that captures the spatial configuration of the ego and the\nadversarial vehicles at impact, before rolling out full adversarial\ntrajectories. Experiments show that our approach outperforms strong baselines\nin both collision rate and controllability. Furthermore, generated scenarios\nconsistently induce higher planner failure rates, revealing limitations of\nexisting planners. We demonstrate that these scenarios fine-tune planners for\nrobustness improvements, contributing to safer AV deployment in different\ncollision scenarios.", "AI": {"tldr": "The paper introduces a method for controllably generating collision scenarios to evaluate autonomous vehicle safety. A dataset called COLLIDE and a predictive framework were developed to achieve this goal.", "motivation": "Autonomous vehicles require testing in diverse, safety-critical scenarios, especially collisions, which are rare and unsafe to replicate in real-world settings.", "method": "The authors created a large dataset (COLLIDE) from real-world driving logs and proposed a framework to predict collision patterns, which are then used to generate full adversarial trajectories.", "result": "The proposed method achieved better control over collision scenarios and increased planner failure rates during testing, indicating limitations in current planners.", "conclusion": "The generated scenarios effectively improve planner robustness, which can lead to safer AV deployment across various collision situations."}}
{"id": "2510.12751", "pdf": "https://arxiv.org/pdf/2510.12751", "abs": "https://arxiv.org/abs/2510.12751", "authors": ["Junjie Wu", "Benjamin B Risk", "Taylor A James", "Nicholas Seyfried", "David W Loring", "Felicia C Goldstein", "Allan I Levey", "James J Lah", "Deqiang Qiu"], "title": "Non-linear associations of amyloid-$\u03b2$ with resting-state functional networks and their cognitive relevance in a large community-based cohort of cognitively normal older adults", "categories": ["q-bio.NC"], "comment": null, "summary": "Background: Non-linear alterations in brain network connectivity may\nrepresent early neural signatures of Alzheimer's disease (AD) pathology in\ncognitively normal older adults. Understanding these changes and their\ncognitive relevance could provide sensitive biomarkers for early detection.\nMost prior studies recruited participants from memory clinics, often with\nsubjective memory concerns, limiting generalizability.\n  Methods: We examined 14 large-scale functional brain networks in 968\ncognitively normal older adults recruited from the community using\nresting-state functional MRI, cerebrospinal fluid (CSF) biomarkers\n(amyloid-$\\beta$ 1-42 [A$\\beta$], total tau, phosphorylated tau 181), and\nneuropsychological assessments. Functional networks were identified using group\nindependent component analysis.\n  Results: Inverted U-shaped associations between CSF A$\\beta$ and functional\nconnectivity were observed in the precuneus network and ventral default mode\nnetwork (DMN), but not in the dorsal DMN, indicating network-specific\nvulnerability to early amyloid pathology. Higher connectivity in\nA$\\beta$-related networks, including dorsal and ventral DMN, precuneus, and\nposterior salience networks, was associated with better visual memory,\nvisuospatial, and executive performance. No significant relationships were\nobserved between CSF tau and functional connectivity.\n  Conclusions: Using a large, community-based cohort, we demonstrate that\nnon-linear alterations in functional connectivity occur in specific networks\neven during the asymptomatic phase of AD. Moreover, A$\\beta$-related network\nconnectivity is cognitively relevant, highlighting functional brain networks as\npromising imaging markers for early detection and prognosis of AD.", "AI": {"tldr": "The study investigates non-linear changes in brain network connectivity as early neural indicators of Alzheimer's Disease (AD) in cognitively normal older adults, using resting-state functional MRI and cerebrospinal fluid biomarkers.", "motivation": "To understand non-linear brain network connectivity changes related to early amyloid pathology in Alzheimer's Disease and their cognitive significance for early detection.", "method": "The study analyzed data from 968 cognitively normal older adults using resting-state functional MRI and cerebrospinal fluid biomarkers to assess the relationships between amyloid-beta levels, tau levels, and functional brain connectivity across 14 large-scale networks. Functional networks were identified using independent component analysis.", "result": "Inverted U-shaped associations between amyloid-beta levels and functional connectivity were found in specific networks like the precuneus and ventral default mode network. Functional connectivity in amyloid-beta-related networks correlated with better cognitive performance, with no significant relationships observed for tau.", "conclusion": "Non-linear changes in functional connectivity precede symptoms of AD, specifically in amyloid-beta-associated networks, thereby emphasizing their potential as early biomarkers for AD detection and prognosis."}}
{"id": "2510.11985", "pdf": "https://arxiv.org/pdf/2510.11985", "abs": "https://arxiv.org/abs/2510.11985", "authors": ["Owen Queen", "Harrison G. Zhang", "James Zou"], "title": "CGBench: Benchmarking Language Model Scientific Reasoning for Clinical Genetics Research", "categories": ["cs.AI"], "comment": "Accepted at NeurIPS 2025", "summary": "Variant and gene interpretation are fundamental to personalized medicine and\ntranslational biomedicine. However, traditional approaches are manual and\nlabor-intensive. Generative language models (LMs) can facilitate this process,\naccelerating the translation of fundamental research into clinically-actionable\ninsights. While existing benchmarks have attempted to quantify the capabilities\nof LMs for interpreting scientific data, these studies focus on narrow tasks\nthat do not translate to real-world research. To meet these challenges, we\nintroduce CGBench, a robust benchmark that tests reasoning capabilities of LMs\non scientific publications. CGBench is built from ClinGen, a resource of\nexpert-curated literature interpretations in clinical genetics. CGBench\nmeasures the ability to 1) extract relevant experimental results following\nprecise protocols and guidelines, 2) judge the strength of evidence, and 3)\ncategorize and describe the relevant outcome of experiments. We test 8\ndifferent LMs and find that while models show promise, substantial gaps exist\nin literature interpretation, especially on fine-grained instructions.\nReasoning models excel in fine-grained tasks but non-reasoning models are\nbetter at high-level interpretations. Finally, we measure LM explanations\nagainst human explanations with an LM judge approach, revealing that models\noften hallucinate or misinterpret results even when correctly classifying\nevidence. CGBench reveals strengths and weaknesses of LMs for precise\ninterpretation of scientific publications, opening avenues for future research\nin AI for clinical genetics and science more broadly.", "AI": {"tldr": "This paper introduces CGBench, a benchmark to assess generative language models in interpreting scientific literature, especially for clinical genetics.", "motivation": "To address the limitations of manual, labor-intensive approaches in interpreting genetic literature and the lack of real-world relevance in existing LM benchmarks.", "method": "CGBench was built using ClinGen data to evaluate LMs on tasks such as experimental result extraction, evidence judgment, and task categorization. It tested 8 different LMs across reasoning and high-level interpretation tasks.", "result": "The study found that while reasoning models perform well in fine-grained tasks, non-reasoning models excel in high-level interpretations. However, all models struggle with aspects such as hallucinations and misinterpretation.", "conclusion": "CGBench highlights the strengths and weaknesses of LMs in interpreting scientific literature, suggesting paths for improvement in AI applications for clinical genetics and beyond."}}
{"id": "2510.11872", "pdf": "https://arxiv.org/pdf/2510.11872", "abs": "https://arxiv.org/abs/2510.11872", "authors": ["Alessandro Cornacchia", "Vaastav Anand", "Muhammad Bilal", "Zafar Qazi", "Marco Canini"], "title": "DMAS-Forge: A Framework for Transparent Deployment of AI Applications as Distributed Systems", "categories": ["cs.SE"], "comment": "1st Workshop on Systems for Agentic AI (SAA '25)", "summary": "Agentic AI applications increasingly rely on multiple agents with distinct\nroles, specialized tools, and access to memory layers to solve complex tasks --\nclosely resembling service-oriented architectures. Yet, in the rapid evolving\nlandscape of programming frameworks and new protocols, deploying and testing AI\nagents as distributed systems remains a daunting and labor-intensive task. We\npresent DMAS-Forge, a framework designed to close this gap. DMAS-Forge\ndecouples application logic from specific deployment choices, and aims at\ntransparently generating the necessary glue code and configurations to spawn\ndistributed multi-agent applications across diverse deployment scenarios with\nminimal manual effort. We present our vision, design principles, and a\nprototype of DMAS-Forge. Finally, we discuss the opportunities and future work\nfor our approach.", "AI": {"tldr": "Deploying and testing AI agents as distributed systems is challenging, and DMAS-Forge aims to simplify this with minimal manual effort.", "motivation": "AI applications increasingly rely on distributed multi-agent systems, resembling service-oriented architectures, that are hard to deploy and test efficiently.", "method": "DMAS-Forge framework decouples application logic from specific deployment scenarios, generating necessary code and configurations automatically.", "result": "A prototype of DMAS-Forge is presented alongside its design principles and vision for minimally manual distributed multi-agent application setup.", "conclusion": "DMAS-Forge addresses deployment challenges for distributed multi-agent systems, presenting promising opportunities for streamlined and automated workflows."}}
{"id": "2510.11907", "pdf": "https://arxiv.org/pdf/2510.11907", "abs": "https://arxiv.org/abs/2510.11907", "authors": ["Blessing Agyei Kyem", "Neema Jakisa Owor", "Andrews Danyo", "Joshua Kofi Asamoah", "Eugene Denteh", "Tanner Muturi", "Anthony Dontoh", "Yaw Adu-Gyamfi", "Armstrong Aboah"], "title": "Task-Specific Dual-Model Framework for Comprehensive Traffic Safety Video Description and Analysis", "categories": ["cs.CV"], "comment": "This paper was accepted at ICCV 2025", "summary": "Traffic safety analysis requires complex video understanding to capture\nfine-grained behavioral patterns and generate comprehensive descriptions for\naccident prevention. In this work, we present a unique dual-model framework\nthat strategically utilizes the complementary strengths of VideoLLaMA and\nQwen2.5-VL through task-specific optimization to address this issue. The core\ninsight behind our approach is that separating training for captioning and\nvisual question answering (VQA) tasks minimizes task interference and allows\neach model to specialize more effectively. Experimental results demonstrate\nthat VideoLLaMA is particularly effective in temporal reasoning, achieving a\nCIDEr score of 1.1001, while Qwen2.5-VL excels in visual understanding with a\nVQA accuracy of 60.80\\%. Through extensive experiments on the WTS dataset, our\nmethod achieves an S2 score of 45.7572 in the 2025 AI City Challenge Track 2,\nplacing 10th on the challenge leaderboard. Ablation studies validate that our\nseparate training strategy outperforms joint training by 8.6\\% in VQA accuracy\nwhile maintaining captioning quality.", "AI": {"tldr": "The paper introduces a dual-model framework combining VideoLLaMA and Qwen2.5-VL, optimized separately for captioning and VQA tasks to enhance traffic safety analysis. It achieved competitive results in the 2025 AI City Challenge.", "motivation": "To address the need for a detailed understanding of video content for traffic accident prevention and tackle task interference issues in captioning and VQA.", "method": "A dual-model strategy where VideoLLaMA specializes in temporal reasoning and Qwen2.5-VL focuses on visual understanding, coupled with separate training for each model.", "result": "Experimental results show VideoLLaMA achieves a CIDEr score of 1.1001 and Qwen2.5-VL earns VQA accuracy of 60.80%. On the WTS dataset, the team achieved an S2 score of 45.7572 in the AI City Challenge, securing 10th place.", "conclusion": "Separate training for captioning and VQA improves task performance, as validated by ablation studies, ensuring balanced quality across both tasks."}}
{"id": "2510.11829", "pdf": "https://arxiv.org/pdf/2510.11829", "abs": "https://arxiv.org/abs/2510.11829", "authors": ["Jin Ma", "Ying Tan", "Renyuan Xu"], "title": "Schr\u00f6dinger bridge for generative AI: Soft-constrained formulation and convergence analysis", "categories": ["cs.LG", "math.DS", "math.OC", "q-fin.MF"], "comment": "31 pages", "summary": "Generative AI can be framed as the problem of learning a model that maps\nsimple reference measures into complex data distributions, and it has recently\nfound a strong connection to the classical theory of the Schr\\\"odinger bridge\nproblems (SBPs) due partly to their common nature of interpolating between\nprescribed marginals via entropy-regularized stochastic dynamics. However, the\nclassical SBP enforces hard terminal constraints, which often leads to\ninstability in practical implementations, especially in high-dimensional or\ndata-scarce regimes. To address this challenge, we follow the idea of the\nso-called soft-constrained Schr\\\"odinger bridge problem (SCSBP), in which the\nterminal constraint is replaced by a general penalty function. This relaxation\nleads to a more flexible stochastic control formulation of McKean-Vlasov type.\n  We establish the existence of optimal solutions for all penalty levels and\nprove that, as the penalty grows, both the controls and value functions\nconverge to those of the classical SBP at a linear rate. Our analysis builds on\nDoob's h-transform representations, the stability results of Schr\\\"odinger\npotentials, Gamma-convergence, and a novel fixed-point argument that couples an\noptimization problem over the space of measures with an auxiliary entropic\noptimal transport problem. These results not only provide the first\nquantitative convergence guarantees for soft-constrained bridges but also shed\nlight on how penalty regularization enables robust generative modeling,\nfine-tuning, and transfer learning.", "AI": {"tldr": "This paper introduces soft-constrained Schr\u00f6dinger bridge problems (SCSBPs) to address stability issues in generative AI models, providing theoretical convergence guarantees.", "motivation": "Generative AI models mapping simple measures to complex distributions can face instability due to the strict constraints of classical Schr\u00f6dinger bridge problems (SBPs) in high-dimensional or data-scarce contexts.", "method": "The authors propose the soft-constrained Schr\u00f6dinger bridge problem (SCSBP), replacing hard terminal constraints with flexible penalty functions, complemented by theoretical analysis using Doob's h-transform and entropic optimal transport.", "result": "Optimal solutions exist for all penalty levels, and as the penalty increases, the controls and value functions converge to the classical SBP at a linear rate, ensuring stability and robustness.", "conclusion": "Soft-constrained regularization enables both robust generative modeling and flexible applications like fine-tuning and transfer learning, with established convergence guarantees."}}
{"id": "2510.11895", "pdf": "https://arxiv.org/pdf/2510.11895", "abs": "https://arxiv.org/abs/2510.11895", "authors": ["Maryam Aliakbarpour", "Alireza Fallah", "Swaha Roy", "Ria Stevens"], "title": "High-Probability Bounds For Heterogeneous Local Differential Privacy", "categories": ["stat.ML", "cs.CR", "cs.DS", "cs.LG"], "comment": null, "summary": "We study statistical estimation under local differential privacy (LDP) when\nusers may hold heterogeneous privacy levels and accuracy must be guaranteed\nwith high probability. Departing from the common in-expectation analyses, and\nfor one-dimensional and multi-dimensional mean estimation problems, we develop\nfinite sample upper bounds in $\\ell_2$-norm that hold with probability at least\n$1-\\beta$. We complement these results with matching minimax lower bounds,\nestablishing the optimality (up to constants) of our guarantees in the\nheterogeneous LDP regime. We further study distribution learning in\n$\\ell_\\infty$-distance, designing an algorithm with high-probability guarantees\nunder heterogeneous privacy demands. Our techniques offer principled guidance\nfor designing mechanisms in settings with user-specific privacy levels.", "AI": {"tldr": "This paper examines statistical estimation and distribution learning under heterogeneous local differential privacy (LDP) with high-probability guarantees, providing optimal bounds and practical guidance.", "motivation": "The study addresses the challenge of handling varying privacy levels among users while ensuring statistical estimation and learning tasks maintain high-probability guarantees.", "method": "The authors develop finite sample upper bounds for $\\\\ell_2$-norm and matching minimax lower bounds for mean estimation problems, along with algorithms for distribution learning in $\\\\ell_\\\\infty$-distance under heterogeneous LDP.", "result": "The paper provides upper bounds, establishes optimal high-probability guarantees for heterogeneous LDP scenarios, and introduces tailored algorithms for distribution learning.", "conclusion": "The findings offer a framework for optimal mechanism design in scenarios with user-specific privacy requirements and varying levels of protection."}}
{"id": "2510.12705", "pdf": "https://arxiv.org/pdf/2510.12705", "abs": "https://arxiv.org/abs/2510.12705", "authors": ["Evelyne Ringoot", "Rabab Alomairy", "Alan Edelman"], "title": "A GPU-resident Memory-Aware Algorithm for Accelerating Bidiagonalization of Banded Matrices", "categories": ["cs.DC", "cs.MS"], "comment": "13 pages, 7 figures, 3 tables", "summary": "The reduction of a banded matrix to a bidiagonal form is a crucial step in\nthe Singular Value Decomposition (SVD), a cornerstone of scientific computing\nand AI. Despite being a highly parallel algorithm, it was previously believed\nto be unsuitable for GPU computation because it is memory bandwidth-bound.\nRecent developments in GPU hardware, including larger L1 memory per Streaming\nMultiprocessor/Compute Unit, have changed that. We present the first GPU\nalgorithm for reducing a banded matrix to bidiagonal form as part of the\nNextLA.jl open-source software package. Our algorithm is based on previous\nCPU-based multicore parallel cache-efficient bulge chasing algorithms and\nadapted to optimize for GPU throughput. We leverage Julia Language's Array\nabstractions and KernelAbstractions to implement a single hardware- and data\nprecision-agnostic function on NVIDIA, AMD, Intel, and Apple Metal GPUs for\nhalf, single, and double precision, and examine performance optimization across\nhardware architectures and data precision. We also develop a hardware-aware\nperformance model and identify key hyperparameters, such as inner tilewidth and\nblock concurrency, that govern optimal GPU execution for bandwidth-bound\nworkloads. We demonstrate highly parallel bandwidth-bound algorithm on the GPU\ncan outperform CPU-based implementations: the GPU algorithm outperforms\nmultithreaded CPU High-Performance libraries PLASMA and SLATE as of matrix size\n1024 x 1024 and by a factor over 100 for matrices of 32k x 32k. In addition,\nthe performance of the algorithm increases linearly with matrix bandwidth size,\nmaking faster reduction of larger matrix bandwidths now also possible. With\nthis work, we break memory bandwidth barriers, as well as matrix bandwidth\nbarriers, resulting in orders-of-magnitude faster algorithms for the reduction\nof banded matrices to bidiagonal form on the GPU.", "AI": {"tldr": "This paper introduces the first GPU algorithm for reducing banded matrices to bidiagonal form, significantly outperforming CPU-based implementations.", "motivation": "To address the inefficiency of CPU-based methods in reducing banded matrices to bidiagonal form and leverage recent advancements in GPU hardware.", "method": "A GPU algorithm adapted from CPU-based multicore parallel cache-efficient algorithms, optimized using Julia's abstractions and leveraging hardware-aware performance models.", "result": "The GPU implementation outperforms existing CPU libraries, with a performance increase proportional to matrix bandwidth size and showing over 100 times speedup for larger matrices.", "conclusion": "Breaking memory and matrix bandwidth barriers, this work demonstrates the feasibility and superiority of GPU-based algorithms for banded matrix reduction in SVD."}}
{"id": "2510.12582", "pdf": "https://arxiv.org/pdf/2510.12582", "abs": "https://arxiv.org/abs/2510.12582", "authors": ["Mark Koch", "Alan Lawrence", "Kartik Singhal", "Seyon Sivarajah", "Ross Duncan"], "title": "GUPPY: Pythonic Quantum-Classical Programming", "categories": ["cs.PL", "cs.SE", "quant-ph"], "comment": "Presented at the Fourth International Workshop on Programming\n  Languages for Quantum Computing (PLanQC 2024)", "summary": "We present ongoing work on Guppy, a domain-specific language embedded in\nPython that allows users to write high-level hybrid quantum programs with\ncomplex control flow in Pythonic syntax, aiming to run them on actual quantum\nhardware.", "AI": {"tldr": "Guppy introduces a domain-specific language embedded in Python for writing high-level hybrid quantum programs with complex control flow.", "motivation": "To create a practical tool for users to write hybrid quantum programs with Python syntax that can run on actual quantum hardware.", "method": "Developed Guppy, a Python-embedded domain-specific language tailored for hybrid quantum programming and complex control flows.", "result": "Ongoing work on Guppy showcases its potential for enabling hybrid quantum programming in Python syntax.", "conclusion": "Guppy provides a promising framework for efficient programming in quantum computing using Python."}}
{"id": "2510.11928", "pdf": "https://arxiv.org/pdf/2510.11928", "abs": "https://arxiv.org/abs/2510.11928", "authors": ["Lorena Calvo-Bartolom\u00e9", "Val\u00e9rie Aldana", "Karla Cantarero", "Alonso Madro\u00f1al de Mesa", "Jer\u00f3nimo Arenas-Garc\u00eda", "Jordan Boyd-Graber"], "title": "Discrepancy Detection at the Data Level: Toward Consistent Multilingual Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "Long paper accepted at EMNLP 2025", "summary": "Multilingual question answering (QA) systems must ensure factual consistency\nacross languages, especially for objective queries such as What is jaundice?,\nwhile also accounting for cultural variation in subjective responses. We\npropose MIND, a user-in-the-loop fact-checking pipeline to detect factual and\ncultural discrepancies in multilingual QA knowledge bases. MIND highlights\ndivergent answers to culturally sensitive questions (e.g., Who assists in\nchildbirth?) that vary by region and context. We evaluate MIND on a bilingual\nQA system in the maternal and infant health domain and release a dataset of\nbilingual questions annotated for factual and cultural inconsistencies. We\nfurther test MIND on datasets from other domains to assess generalization. In\nall cases, MIND reliably identifies inconsistencies, supporting the development\nof more culturally aware and factually consistent QA systems.", "AI": {"tldr": "The paper proposes MIND, a user-in-the-loop pipeline for detecting factual and cultural inconsistencies in multilingual question-answering (QA) knowledge bases.", "motivation": "Ensure factual consistency across languages in multilingual QA systems while handling cultural variations in responses.", "method": "Developed MIND, a user-in-the-loop fact-checking pipeline that identifies discrepancies in multilingual QA knowledge bases with a focus on both factual and culturally sensitive questions.", "result": "MIND effectively detects inconsistencies in a bilingual QA system within the maternal and infant health domain and generalizes well across datasets from other domains.", "conclusion": "MIND enables the creation of culturally sensitive and factually consistent QA systems by reliably identifying and addressing inconsistencies."}}
{"id": "2510.12215", "pdf": "https://arxiv.org/pdf/2510.12215", "abs": "https://arxiv.org/abs/2510.12215", "authors": ["Chanwoo Kim", "Jihwan Yoon", "Hyeonseong Kim", "Taemoon Jeong", "Changwoo Yoo", "Seungbeen Lee", "Soohwan Byeon", "Hoon Chung", "Matthew Pan", "Jean Oh", "Kyungjae Lee", "Sungjoon Choi"], "title": "Learning Social Navigation from Positive and Negative Demonstrations and Rule-Based Specifications", "categories": ["cs.RO"], "comment": "For more videos, see https://chanwookim971024.github.io/PioneeR/", "summary": "Mobile robot navigation in dynamic human environments requires policies that\nbalance adaptability to diverse behaviors with compliance to safety\nconstraints. We hypothesize that integrating data-driven rewards with\nrule-based objectives enables navigation policies to achieve a more effective\nbalance of adaptability and safety. To this end, we develop a framework that\nlearns a density-based reward from positive and negative demonstrations and\naugments it with rule-based objectives for obstacle avoidance and goal\nreaching. A sampling-based lookahead controller produces supervisory actions\nthat are both safe and adaptive, which are subsequently distilled into a\ncompact student policy suitable for real-time operation with uncertainty\nestimates. Experiments in synthetic and elevator co-boarding simulations show\nconsistent gains in success rate and time efficiency over baselines, and\nreal-world demonstrations with human participants confirm the practicality of\ndeployment. A video illustrating this work can be found on our project page\nhttps://chanwookim971024.github.io/PioneeR/.", "AI": {"tldr": "The paper proposes a navigation framework for mobile robots in dynamic environments, merging data-driven rewards with rule-based goals to enhance adaptability and safety, validated through both simulations and real-world demonstrations.", "motivation": "The motivation is to address the challenge of developing navigation policies for robots that can balance adaptability to dynamic human behaviors with adherence to safety constraints.", "method": "The framework combines density-based rewards derived from demonstrations with rule-based objectives for obstacle avoidance and goal-reaching. It uses a sampling-based controller for generating supervisory actions and distills these into a compact real-time policy with uncertainty estimation.", "result": "Results from simulations and real-world scenarios show improved success rates and time efficiency compared to baselines, showcasing the method's effectiveness.", "conclusion": "The study concludes that integrating data-driven and rule-based approaches enhances navigation adaptability and safety, making the framework practical for deployment in real-world dynamic environments."}}
{"id": "2510.12015", "pdf": "https://arxiv.org/pdf/2510.12015", "abs": "https://arxiv.org/abs/2510.12015", "authors": ["Ali Montazeralghaem", "Guy Tennenholtz", "Craig Boutilier", "Ofer Meshi"], "title": "Asking Clarifying Questions for Preference Elicitation With Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have made it possible for recommendation systems\nto interact with users in open-ended conversational interfaces. In order to\npersonalize LLM responses, it is crucial to elicit user preferences, especially\nwhen there is limited user history. One way to get more information is to\npresent clarifying questions to the user. However, generating effective\nsequential clarifying questions across various domains remains a challenge. To\naddress this, we introduce a novel approach for training LLMs to ask sequential\nquestions that reveal user preferences. Our method follows a two-stage process\ninspired by diffusion models. Starting from a user profile, the forward process\ngenerates clarifying questions to obtain answers and then removes those answers\nstep by step, serving as a way to add ``noise'' to the user profile. The\nreverse process involves training a model to ``denoise'' the user profile by\nlearning to ask effective clarifying questions. Our results show that our\nmethod significantly improves the LLM's proficiency in asking funnel questions\nand eliciting user preferences effectively.", "AI": {"tldr": "The paper presents a method for training Large Language Models (LLMs) to ask sequential clarifying questions for better user preference elicitation.", "motivation": "LLMs for recommendation systems need to elicit user preferences effectively for personalization, especially with limited user history.", "method": "A two-stage process inspired by diffusion models is introduced to train LLMs to ask clarifying questions. The forward process generates and removes answers to simulate noise, while the reverse process trains the model to ask effective questions to 'denoise' the user profile.", "result": "The method enhances the ability of LLMs to ask sequential clarifying questions, improving preference elicitation.", "conclusion": "The proposed approach successfully trains LLMs to generate effective clarifying questions, advancing their capability in user preference elicitation."}}
{"id": "2510.12011", "pdf": "https://arxiv.org/pdf/2510.12011", "abs": "https://arxiv.org/abs/2510.12011", "authors": ["Bei Zhou", "Maximilian Balmus", "Cesare Corrado", "Ludovica Cicci", "Shuang Qian", "Steven A. Niederer"], "title": "TorchCor: High-Performance Cardiac Electrophysiology Simulations with the Finite Element Method on GPUs", "categories": ["cs.SE"], "comment": null, "summary": "Cardiac electrophysiology (CEP) simulations are increasingly used for\nunderstanding cardiac arrhythmias and guiding clinical decisions. However,\nthese simulations typically require high-performance computing resources with\nnumerous CPU cores, which are often inaccessible to many research groups and\nclinicians. To address this, we present TorchCor, a high-performance Python\nlibrary for CEP simulations using the finite element method on general-purpose\nGPUs. Built on PyTorch, TorchCor significantly accelerates CEP simulations,\nparticularly for large 3D meshes. The accuracy of the solver is verified\nagainst manufactured analytical solutions and the $N$-version benchmark\nproblem. TorchCor is freely available for both academic and commercial use\nwithout restrictions.", "AI": {"tldr": "TorchCor is a Python library enabling efficient cardiac electrophysiology simulations on GPUs, improving accessibility for researchers and clinicians.", "motivation": "CEP simulations require significant computational resources, often inaccessible to researchers and clinicians. TorchCor aims to provide an alternative solution using GPUs.", "method": "TorchCor uses the finite element method, implemented on general-purpose GPUs with PyTorch, to accelerate CEP simulations.", "result": "TorchCor achieves significant speed-ups for large 3D meshes and displays verified accuracy against known analytical solutions and benchmarks.", "conclusion": "TorchCor enhances accessibility and performance of CEP simulations, offering a freely available tool for academic and commercial applications."}}
{"id": "2510.11992", "pdf": "https://arxiv.org/pdf/2510.11992", "abs": "https://arxiv.org/abs/2510.11992", "authors": ["Hatem Ibrahem", "Ahmed Salem", "Qinmin Vivian Hu", "Guanghui Wang"], "title": "PanoTPS-Net: Panoramic Room Layout Estimation via Thin Plate Spline Transformation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurately estimating the 3D layout of rooms is a crucial task in computer\nvision, with potential applications in robotics, augmented reality, and\ninterior design. This paper proposes a novel model, PanoTPS-Net, to estimate\nroom layout from a single panorama image. Leveraging a Convolutional Neural\nNetwork (CNN) and incorporating a Thin Plate Spline (TPS) spatial\ntransformation, the architecture of PanoTPS-Net is divided into two stages:\nFirst, a convolutional neural network extracts the high-level features from the\ninput images, allowing the network to learn the spatial parameters of the TPS\ntransformation. Second, the TPS spatial transformation layer is generated to\nwarp a reference layout to the required layout based on the predicted\nparameters. This unique combination empowers the model to properly predict room\nlayouts while also generalizing effectively to both cuboid and non-cuboid\nlayouts. Extensive experiments on publicly available datasets and comparisons\nwith state-of-the-art methods demonstrate the effectiveness of the proposed\nmethod. The results underscore the model's accuracy in room layout estimation\nand emphasize the compatibility between the TPS transformation and panorama\nimages. The robustness of the model in handling both cuboid and non-cuboid room\nlayout estimation is evident with a 3DIoU value of 85.49, 86.16, 81.76, and\n91.98 on PanoContext, Stanford-2D3D, Matterport3DLayout, and ZInD datasets,\nrespectively. The source code is available at:\nhttps://github.com/HatemHosam/PanoTPS_Net.", "AI": {"tldr": "The paper introduces PanoTPS-Net, a model designed to predict room layouts from panorama images using CNN and Thin Plate Spline transformations, achieving state-of-the-art results.", "motivation": "The study aims to enhance the ability to estimate 3D room layouts from single panorama images, which is critical for applications in robotics, augmented reality, and interior design.", "method": "PanoTPS-Net leverages CNNs for feature extraction and integrates Thin Plate Spline spatial transformations in a two-stage process that learns and applies spatial parameters to refine room layout predictions.", "result": "The model demonstrated strong performance in cuboid and non-cuboid layout predictions, achieving high 3DIoU scores across multiple datasets: PanoContext, Stanford-2D3D, Matterport3DLayout, and ZInD.", "conclusion": "The PanoTPS-Net successfully improves room layout estimation accuracy, utilizing effective integration of TPS transformations with panoramas, and shows robustness across diverse datasets."}}
{"id": "2510.11832", "pdf": "https://arxiv.org/pdf/2510.11832", "abs": "https://arxiv.org/abs/2510.11832", "authors": ["Narine Kokhlikyan", "Kamalika Chaudhuri", "Saeed Mahloujifar"], "title": "Z0-Inf: Zeroth Order Approximation for Data Influence", "categories": ["cs.LG"], "comment": null, "summary": "A critical aspect of analyzing and improving modern machine learning systems\nlies in understanding how individual training examples influence a model's\npredictive behavior. Estimating this influence enables critical applications,\nincluding data selection and model debugging; in particular, self-influence,\nwhich quantifies the influence of a training point on itself, has found many\nuses in data quality assessment and outlier detection. Existing methods for\nmeasuring data influence, however, are often impractical for large models due\nto low accuracy or prohibitive computational costs: most approaches either\nprovide poor approximations or rely on gradients and inverse-Hessian\ncomputations that remain challenging to scale. In this work, we introduce a\nhighly efficient zeroth-order approximation for estimating the influence of\ntraining data that requires only a fraction of the time and memory footprint of\nprior methods. Notably, our method relies solely on loss values of intermediate\ncheckpoints on the training and test data, along with the checkpoints\nthemselves, making it broadly applicable even when the loss function of\ninterest is non-differentiable. Beyond its computational efficiency, our\napproach achieves superior accuracy in estimating self-influence and comparable\nor improved accuracy in estimating train-test influence for fine-tuned large\nlanguage models, enabling scalable and practical analysis of how training data\nshapes model behavior.", "AI": {"tldr": "The paper introduces a computationally efficient method for measuring the influence of training data on machine learning models, improving scalability and accuracy without relying on gradients or inverse-Hessian calculations.", "motivation": "Understanding the influence of individual training examples on model behavior is crucial for tasks like data selection and model debugging. Existing methods are often inefficient or inaccurate for large models, creating a need for better approaches.", "method": "The authors propose a zeroth-order approximation method that estimates data influence using only loss values from intermediate training checkpoints. This method avoids gradients and Hessian computations, making it suitable for large models and non-differentiable loss functions.", "result": "The proposed method achieves superior accuracy in estimating self-influence and performs comparably or better for train-test influence in fine-tuned large language models. It also requires significantly less computational time and memory.", "conclusion": "The approach enables scalable and practical analysis of training data influence, facilitating broader applications in large-scale machine learning systems."}}
{"id": "2510.11910", "pdf": "https://arxiv.org/pdf/2510.11910", "abs": "https://arxiv.org/abs/2510.11910", "authors": ["Tyler Maunu"], "title": "Simplifying Optimal Transport through Schatten-$p$ Regularization", "categories": ["stat.ML", "cs.LG"], "comment": "26 pages, 4 figures", "summary": "We propose a new general framework for recovering low-rank structure in\noptimal transport using Schatten-$p$ norm regularization. Our approach extends\nexisting methods that promote sparse and interpretable transport maps or plans,\nwhile providing a unified and principled family of convex programs that\nencourage low-dimensional structure. The convexity of our formulation enables\ndirect theoretical analysis: we derive optimality conditions and prove recovery\nguarantees for low-rank couplings and barycentric maps in simplified settings.\nTo efficiently solve the proposed program, we develop a mirror descent\nalgorithm with convergence guarantees for $p \\geq 1$. Experiments on synthetic\nand real data demonstrate the method's efficiency, scalability, and ability to\nrecover low-rank transport structures.", "AI": {"tldr": "The paper introduces a new framework for recovering low-rank structures in optimal transport using Schatten-$p$ norm regularization.", "motivation": "Existing methods in optimal transport lack a unified approach for promoting low-dimensional, interpretable transport maps, which motivates the need for a principled framework.", "method": "A convex program for encouraging low-rank structures is proposed, with theoretical analysis and a mirror descent algorithm for efficient optimization when $p \\geq 1$.", "result": "The authors derive optimality conditions and provide recovery guarantees in simplified cases, supported by experiments showing efficiency and scalability on both synthetic and real data.", "conclusion": "The proposed framework is effective for recovering low-rank transport structures, providing theoretical guarantees and practical efficacy in relevant applications."}}
{"id": "2510.11866", "pdf": "https://arxiv.org/pdf/2510.11866", "abs": "https://arxiv.org/abs/2510.11866", "authors": ["Michael Crystal", "Guy Goren", "Scott Duke Kominers"], "title": "Rationally Analyzing Shelby: Proving Incentive Compatibility in a Decentralized Storage Network", "categories": ["cs.GT", "cs.DC", "cs.MA"], "comment": "23 pages, 1 figure", "summary": "Decentralized storage is one of the most natural applications built on\nblockchains and a central component of the Web3 ecosystem. Yet despite a decade\nof active development -- from IPFS and Filecoin to more recent entrants -- most\nof these storage protocols have received limited formal analysis of their\nincentive properties. Claims of incentive compatibility are sometimes made, but\nrarely proven. This gap matters: without well-designed incentives, a system may\ndistribute storage but fail to truly decentralize it.\n  We analyze Shelby -- a storage network protocol recently proposed by Aptos\nLabs and Jump Crypto -- and provide the first formal proof of its incentive\nproperties. Our game-theoretic model shows that while off-chain audits alone\ncollapse to universal shirking, Shelby's combination of peer audits with\noccasional on-chain verification yields incentive compatibility under natural\nparameter settings. We also examine coalition behavior and outline a simple\nmodification that strengthens the protocol's collusion-resilience.", "AI": {"tldr": "This paper provides a formal analysis of the incentive properties of Shelby, a decentralized storage network protocol, addressing a significant gap in the current research on blockchain-based storage protocols.", "motivation": "Decentralized storage systems are key to the Web3 ecosystem but lack formal analysis of their incentive mechanisms, raising concerns about their effectiveness in truly decentralizing storage.", "method": "The authors use a game-theoretic model to analyze Shelby's incentive properties, assessing the effectiveness of its peer audits combined with occasional on-chain verification.", "result": "Shelby is shown to achieve incentive compatibility under certain conditions, and a modification is proposed to enhance its resistance to collusion.", "conclusion": "Shelby's incentive mechanisms are theoretically sound with the outlined adjustments, and the paper highlights the importance of formal incentive analysis for decentralized storage systems."}}
{"id": "2510.11944", "pdf": "https://arxiv.org/pdf/2510.11944", "abs": "https://arxiv.org/abs/2510.11944", "authors": ["Yupei Li", "Philipp Borchert", "Gerasimos Lampouras"], "title": "TopoAlign: A Framework for Aligning Code to Math via Topological Decomposition", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) excel at both informal and formal (e.g. Lean 4)\nmathematical reasoning but still struggle with autoformalisation, the task of\ntransforming informal into formal mathematical statements. Autoformalisation\nhelps pair the informal reasoning of LLMs with formal proof assistants which\nenable machine-verifiable generation and mitigate hallucinations. Yet, the\nperformance of current Math LLMs is constrained by the scarcity of large-scale\ncorpora, particularly those containing pairs of informal and formal statements.\nAlthough current models are trained to generate code from natural language\ninstructions, structural and syntactic differences between these and formal\nmathematics limit effective transfer learning. We propose TopoAlign, a\nframework that unlocks widely available code repositories as training resources\nfor Math LLMs. TopoAlign decomposes code into docstrings, main functions, and\ndependency functions, and reassembles these components into analogues that\nstructurally mirror formal statements. This produces structurally aligned code\ndata that can be used for training Math LLMs without requiring additional human\nannotation. We train two state-of-the-art models, DeepSeek-Math and Herald, and\nevaluate them on the minif2f, Putnam, and ProofNet benchmarks. TopoAlign\nprovides substantial gains for DeepSeek-Math, improving performance by 17.77%\non BEq@10 and 68.82% on typecheck@10. Despite introducing no new mathematical\nknowledge, our framework achieves gains of 0.12% and 1.09% for Herald on BEq@10\nand typecheck@10, respectively, demonstrating that training on aligned code\ndata is beneficial even for specialized models.", "AI": {"tldr": "This paper introduces TopoAlign, a framework for leveraging widely available code repositories to improve the training of Math Large Language Models (LLMs) on autoformalization tasks, achieving notable performance improvements.", "motivation": "The paper aims to address the challenge of autoformalization, which is transforming informal mathematical reasoning into formal statements. This task is important as it connects informal mathematical LLM outputs with formal proof assistants, ensuring machine-verifiable results while reducing hallucinations. The scarcity of large-scale paired data for informal and formal statements motivates the need for alternative training approaches.", "method": "TopoAlign is proposed as a framework that decomposes code into structuring components such as docstrings, main functions, and dependencies, and reassembles them to mimic formal mathematical constructs. This technique enables the use of existing code repositories to train Math LLMs without requiring additional human annotation.", "result": "Using TopoAlign, the paper reports significant performance improvements for their Math LLMs (DeepSeek-Math and Herald). Specifically, DeepSeek-Math achieved a 17.77% improvement on BEq@10 and 68.82% improvement on typecheck@10. Similarly, gains were observed for the specialized model Herald with 0.12% improvement on BEq@10 and 1.09% improvement on typecheck@10.", "conclusion": "TopoAlign demonstrates the utility of leveraging structural code alignment for making existing code repositories usable for training Math LLMs. This approach provides significant performance gains even for specialized models and contributes toward the advancement of autoformalization techniques."}}
{"id": "2510.12276", "pdf": "https://arxiv.org/pdf/2510.12276", "abs": "https://arxiv.org/abs/2510.12276", "authors": ["Fuhao Li", "Wenxuan Song", "Han Zhao", "Jingbo Wang", "Pengxiang Ding", "Donglin Wang", "Long Zeng", "Haoang Li"], "title": "Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model", "categories": ["cs.RO"], "comment": null, "summary": "Vision-language-action (VLA) models have recently shown strong potential in\nenabling robots to follow language instructions and execute precise actions.\nHowever, most VLAs are built upon vision-language models pretrained solely on\n2D data, which lack accurate spatial awareness and hinder their ability to\noperate in the 3D physical world. Existing solutions attempt to incorporate\nexplicit 3D sensor inputs such as depth maps or point clouds, but these\napproaches face challenges due to sensor noise, hardware heterogeneity, and\nincomplete depth coverage in existing datasets. Alternative methods that\nestimate 3D cues from 2D images also suffer from the limited performance of\ndepth estimators.We propose Spatial Forcing (SF), a simple yet effective\nalignment strategy that implicitly forces VLA models to develop spatial\ncomprehension capabilities without relying on explicit 3D inputs or depth\nestimators. SF aligns intermediate visual embeddings of VLAs with geometric\nrepresentations produced by pretrained 3D foundation models. By enforcing\nalignment at intermediate layers, SF guides VLAs to encode richer spatial\nrepresentations that enhance action precision.Extensive experiments in\nsimulation and real-world environments demonstrate that SF achieves\nstate-of-the-art results, surpassing both 2D- and 3D-based VLAs. SF further\naccelerates training by up to 3.8x and improves data efficiency across diverse\nrobotic tasks. Project page is at https://spatial-forcing.github.io/", "AI": {"tldr": "The paper introduces Spatial Forcing (SF) as a new strategy for vision-language-action (VLA) models, enhancing spatial comprehension without relying on explicit 3D inputs.", "motivation": "VLAs are limited by 2D data as they lack accurate spatial awareness, hindering their applicability in functioning efficiently in the real-world 3D space.", "method": "The paper proposes SF, which aligns intermediate visual embeddings of VLAs with geometric representations from pretrained 3D foundation models to implicitly enforce spatial comprehension.", "result": "SF achieves state-of-the-art performance, surpassing both 2D and 3D-based models in robotic tasks. It speeds up training by 3.8x and enhances data efficiency.", "conclusion": "SF introduces a novel approach that addresses spatial comprehension in VLAs without relying on explicit 3D inputs, significantly improving precision and efficiency in robotic tasks."}}
{"id": "2510.12033", "pdf": "https://arxiv.org/pdf/2510.12033", "abs": "https://arxiv.org/abs/2510.12033", "authors": ["Chathurangi Shyalika", "Aryaman Sharma", "Fadi El Kalach", "Utkarshani Jaimini", "Cory Henson", "Ramy Harik", "Amit Sheth"], "title": "CausalTrace: A Neurosymbolic Causal Analysis Agent for Smart Manufacturing", "categories": ["cs.AI"], "comment": "8 pages, 4 figures, 3 tables, Accepted at AAAI 2026: IAAI -\n  Innovative Applications of AI Conference", "summary": "Modern manufacturing environments demand not only accurate predictions but\nalso interpretable insights to process anomalies, root causes, and potential\ninterventions. Existing AI systems often function as isolated black boxes,\nlacking the seamless integration of prediction, explanation, and causal\nreasoning required for a unified decision-support solution. This fragmentation\nlimits their trustworthiness and practical utility in high-stakes industrial\nenvironments. In this work, we present CausalTrace, a neurosymbolic causal\nanalysis module integrated into the SmartPilot industrial CoPilot. CausalTrace\nperforms data-driven causal analysis enriched by industrial ontologies and\nknowledge graphs, including advanced functions such as causal discovery,\ncounterfactual reasoning, and root cause analysis (RCA). It supports real-time\noperator interaction and is designed to complement existing agents by offering\ntransparent, explainable decision support. We conducted a comprehensive\nevaluation of CausalTrace using multiple causal assessment methods and the C3AN\nframework (i.e. Custom, Compact, Composite AI with Neurosymbolic Integration),\nwhich spans principles of robustness, intelligence, and trustworthiness. In an\nacademic rocket assembly testbed, CausalTrace achieved substantial agreement\nwith domain experts (ROUGE-1: 0.91 in ontology QA) and strong RCA performance\n(MAP@3: 94%, PR@2: 97%, MRR: 0.92, Jaccard: 0.92). It also attained 4.59/5 in\nthe C3AN evaluation, demonstrating precision and reliability for live\ndeployment.", "AI": {"tldr": "The paper introduces CausalTrace, a neurosymbolic causal analysis module, integrated into an industrial CoPilot to enhance trust, explanation, and prediction in manufacturing environments.", "motivation": "The motivation is to address the limitations of existing AI systems in manufacturing environments, which often function as black boxes and lack integrated prediction, explanation, and causal reasoning capabilities, reducing trustworthiness and utility.", "method": "The authors developed CausalTrace, which combines data-driven causal analysis with industrial ontologies and knowledge graphs, and offers features like causal discovery, counterfactual reasoning, and root cause analysis. It was evaluated using multiple causal methods and the C3AN framework.", "result": "CausalTrace demonstrated high agreement with domain experts and outstanding performance in root cause analysis, with metrics like ROUGE-1 (0.91), MAP@3 (94%), PR@2 (97%), and MRR (0.92). It also scored 4.59/5 in trustworthiness evaluations.", "conclusion": "CausalTrace successfully integrates causal reasoning into industrial AI systems, providing precise, reliable, and interpretable decision-support tools for real-time applications."}}
{"id": "2510.12082", "pdf": "https://arxiv.org/pdf/2510.12082", "abs": "https://arxiv.org/abs/2510.12082", "authors": ["Huy Nguyen", "Christoph Treude", "Patanamon Thongtanunam"], "title": "Enhancing Neural Code Representation with Additional Context", "categories": ["cs.SE", "cs.AI"], "comment": "34 pages, 7 figures, 11 tables", "summary": "Automated program comprehension underpins many software engineering tasks,\nfrom code summarisation to clone detection. Recent deep learning models achieve\nstrong results but typically rely on source code alone, overlooking contextual\ninformation such as version history or structural relationships. This limits\ntheir ability to capture how code evolves and operates. We conduct an empirical\nstudy on how enriching code representations with such contextual signals\naffects neural model performance on key comprehension tasks. Two downstream\ntasks, code clone detection and code summarisation, are evaluated using SeSaMe\n(1,679 Java methods) and CodeSearchNet (63,259 methods). Five representative\nmodels (CodeBERT, GraphCodeBERT, CodeT5, PLBART, ASTNN) are fine-tuned under\ncode-only and context-augmented settings. Results show that context generally\nimproves performance: version history consistently boosts clone detection\n(e.g., CodeT5 +15.92% F1) and summarisation (e.g., GraphCodeBERT +5.56%\nMETEOR), while call-graph effects vary by model and task. Combining multiple\ncontexts yields further gains (up to +21.48% macro-F1). Human evaluation on 100\nJava snippets confirms that context-augmented summaries are significantly\npreferred for Accuracy and Content Adequacy (p <= 0.026; |delta| up to 0.55).\nThese findings highlight the potential of contextual signals to enhance code\ncomprehension and open new directions for optimising contextual encoding in\nneural SE models.", "AI": {"tldr": "The study examines the impact of enriching code representation with contextual information on program comprehension tasks using neural models. Experiments highlight improved performance in tasks like code summarisation and clone detection.", "motivation": "To address limitations in neural models which typically rely solely on source code, disregarding contextual information such as version history and structural relationships.", "method": "By evaluating five neural models (CodeBERT, GraphCodeBERT, CodeT5, PLBART, ASTNN) on two tasks, clone detection and code summarisation, under both code-only and enriched context-augmented representations.", "result": "Adding contextual signals improves model performance significantly, with boosts up to +21.48% macro-F1 for combined contexts, and human evaluations show higher preference for contextual summaries in terms of accuracy and content adequacy.", "conclusion": "Incorporating contextual signals into code representation enhances program comprehension tasks and provides avenues to optimize contextual encoding in software engineering neural models."}}
{"id": "2510.11996", "pdf": "https://arxiv.org/pdf/2510.11996", "abs": "https://arxiv.org/abs/2510.11996", "authors": ["Tanner Muturi", "Blessing Agyei Kyem", "Joshua Kofi Asamoah", "Neema Jakisa Owor", "Richard Dyzinela", "Andrews Danyo", "Yaw Adu-Gyamfi", "Armstrong Aboah"], "title": "Prompt-Guided Spatial Understanding with RGB-D Transformers for Fine-Grained Object Relation Reasoning", "categories": ["cs.CV"], "comment": "The paper was accepted at ICCV Conference 2025", "summary": "Spatial reasoning in large-scale 3D environments such as warehouses remains a\nsignificant challenge for vision-language systems due to scene clutter,\nocclusions, and the need for precise spatial understanding. Existing models\noften struggle with generalization in such settings, as they rely heavily on\nlocal appearance and lack explicit spatial grounding. In this work, we\nintroduce a dedicated spatial reasoning framework for the Physical AI Spatial\nIntelligence Warehouse dataset introduced in the Track 3 2025 AI City\nChallenge. Our approach enhances spatial comprehension by embedding mask\ndimensions in the form of bounding box coordinates directly into the input\nprompts, enabling the model to reason over object geometry and layout. We\nfine-tune the framework across four question categories namely: Distance\nEstimation, Object Counting, Multi-choice Grounding, and Spatial Relation\nInference using task-specific supervision. To further improve consistency with\nthe evaluation system, normalized answers are appended to the GPT response\nwithin the training set. Our comprehensive pipeline achieves a final score of\n73.0606, placing 4th overall on the public leaderboard. These results\ndemonstrate the effectiveness of structured prompt enrichment and targeted\noptimization in advancing spatial reasoning for real-world industrial\nenvironments.", "AI": {"tldr": "The authors address spatial reasoning challenges in cluttered 3D environments, proposing a model that incorporates bounding box coordinates into prompts for improved spatial comprehension, leading to competitive performance in an AI City competition.", "motivation": "Vision-language systems struggle with spatial reasoning in complex 3D environments like warehouses due to clutter and occlusions.", "method": "The framework embeds bounding box coordinates into input prompts and fine-tunes tasks including Distance Estimation, Object Counting, and Spatial Relation Inference; normalized answers were also included in training for better consistency.", "result": "The proposed pipeline achieved a score of 73.0606, securing 4th place on the public leaderboard for the AI City Challenge 2025.", "conclusion": "Structured prompt enrichment and task-specific tuning significantly enhance spatial reasoning capabilities in industrial settings."}}
{"id": "2510.11834", "pdf": "https://arxiv.org/pdf/2510.11834", "abs": "https://arxiv.org/abs/2510.11834", "authors": ["Sarah Ball", "Andreas Haupt"], "title": "Don't Walk the Line: Boundary Guidance for Filtered Generation", "categories": ["cs.LG", "cs.CL"], "comment": "9 pages, 3 figures, 3 tables", "summary": "Generative models are increasingly paired with safety classifiers that filter\nharmful or undesirable outputs. A common strategy is to fine-tune the generator\nto reduce the probability of being filtered, but this can be suboptimal: it\noften pushes the model toward producing samples near the classifier's decision\nboundary, increasing both false positives and false negatives. We propose\nBoundary Guidance, a reinforcement learning fine-tuning method that explicitly\nsteers generation away from the classifier's margin. On a benchmark of\njailbreak and ambiguous prompts, Boundary Guidance improves both the safety and\nthe utility of outputs, as judged by LLM-as-a-Judge evaluations. Comprehensive\nablations across model scales and reward designs demonstrate the robustness of\nour approach.", "AI": {"tldr": "The paper presents Boundary Guidance, a reinforcement learning approach to fine-tune generative models paired with safety classifiers, enhancing safety and utility of outputs by steering generation away from classifier margins.", "motivation": "Generative models often produce undesirable outputs due to safety classifiers pushing generations near decision boundaries, causing inefficiencies and errors.", "method": "Boundary Guidance utilizes reinforcement learning to explicitly steer model generation away from the classifier's decision boundary.", "result": "Improved safety and utility for generative model outputs, validated on jailbreak and ambiguous prompts using LLM-as-a-Judge evaluations.", "conclusion": "Boundary Guidance offers a robust approach to optimize generation safety and effectiveness, with extensive ablations confirming its reliability across scales and designs."}}
{"id": "2510.12013", "pdf": "https://arxiv.org/pdf/2510.12013", "abs": "https://arxiv.org/abs/2510.12013", "authors": ["Jiaqi Li", "Zhipeng Lou", "Johannes Schmidt-Hieber", "Wei Biao Wu"], "title": "Statistical Guarantees for High-Dimensional Stochastic Gradient Descent", "categories": ["stat.ML", "cs.LG"], "comment": "Accepted to NeurIPS 2025", "summary": "Stochastic Gradient Descent (SGD) and its Ruppert-Polyak averaged variant\n(ASGD) lie at the heart of modern large-scale learning, yet their theoretical\nproperties in high-dimensional settings are rarely understood. In this paper,\nwe provide rigorous statistical guarantees for constant learning-rate SGD and\nASGD in high-dimensional regimes. Our key innovation is to transfer powerful\ntools from high-dimensional time series to online learning. Specifically, by\nviewing SGD as a nonlinear autoregressive process and adapting existing\ncoupling techniques, we prove the geometric-moment contraction of\nhigh-dimensional SGD for constant learning rates, thereby establishing\nasymptotic stationarity of the iterates. Building on this, we derive the $q$-th\nmoment convergence of SGD and ASGD for any $q\\ge2$ in general $\\ell^s$-norms,\nand, in particular, the $\\ell^{\\infty}$-norm that is frequently adopted in\nhigh-dimensional sparse or structured models. Furthermore, we provide sharp\nhigh-probability concentration analysis which entails the probabilistic bound\nof high-dimensional ASGD. Beyond closing a critical gap in SGD theory, our\nproposed framework offers a novel toolkit for analyzing a broad class of\nhigh-dimensional learning algorithms.", "AI": {"tldr": "The paper provides rigorous statistical guarantees for SGD and ASGD with constant learning rates in high-dimensional settings, leveraging techniques from high-dimensional time series.", "motivation": "Understanding the theoretical properties of SGD and ASGD in high-dimensional regimes, which are central to modern machine learning.", "method": "Adapt tools from high-dimensional time series and nonlinear autoregressive processes to analyze and establish geometric-moment contraction and asymptotic stationarity.", "result": "Proved geometric-moment contraction for constant learning rates, derived $q$-th moment convergence, and provided high-probability concentration bounds for high-dimensional ASGD.", "conclusion": "This work bridges a critical gap in the theory of SGD and introduces a new analytical toolkit for high-dimensional learning algorithms."}}
{"id": "2510.12128", "pdf": "https://arxiv.org/pdf/2510.12128", "abs": "https://arxiv.org/abs/2510.12128", "authors": ["Ziqi Zhao", "Vivek Sarin"], "title": "nuGPR: GPU-Accelerated Gaussian Process Regression with Iterative Algorithms and Low-Rank Approximations", "categories": ["cs.LG", "cs.DC", "cs.NA", "math.NA", "65Y05, 60G15, 65F10, 65F55"], "comment": "22 pages, 6 figures, published in SIAM Journal on Scientific\n  Computing, E-print available at:\n  https://epubs.siam.org/eprint/5CF5CKX49Y4FUQXZFHCN/full", "summary": "Gaussian Process Regression (GPR) is an important type of supervised machine\nlearning model with inherent uncertainty measure in its predictions. We propose\na new framework, nuGPR, to address the well-known challenge of high computation\ncost associated with GPR training. Our framework includes several ideas from\nnumerical linear algebra to reduce the amount of computation in key steps of\nGPR, and we combine them to establish an end-to-end training algorithm.\nSpecifically, we leverage the preconditioned conjugate gradient method to\naccelerate the convergence of the linear solves required in GPR. We exploit\nclustering in the input data to identify block-diagonal structure of the\ncovariance matrix and subsequently construct low-rank approximations of the\noff-diagonal blocks. These enhancements significantly reduce the time and space\ncomplexity of our computations. In addition, unlike other frameworks that rely\non exact differentiation, we employ numerical gradients to optimize the\nhyperparameters of our GPR model, further reducing the training cost by\neliminating the need for backpropagation. Lastly, we leverage the CUDA Toolkit\nto efficiently parallelize the training procedure on NVIDIA GPUs. As a result,\nnuGPR reduces total training time by up to 2x and peak memory consumption by up\nto 12x on various synthetic and real-world datasets when compared to the best\nexisting GPU-based GPR implementation.", "AI": {"tldr": "The paper introduces nuGPR, a new framework to enhance the computational efficiency of Gaussian Process Regression (GPR) by combining techniques from numerical linear algebra, clustering, low-rank approximations, and parallelization.", "motivation": "High computational cost in the training of Gaussian Process Regression (GPR) models limits their scalability in practical applications.", "method": "The nuGPR framework integrates preconditioned conjugate gradient methods for faster linear solves, clustering for covariance matrix approximation, numerical gradient optimization to avoid backpropagation, and GPU-based parallelization using the CUDA Toolkit.", "result": "nuGPR achieves up to a 2x reduction in training time and up to a 12x reduction in peak memory usage compared to existing GPU-based GPR implementations, while maintaining performance on synthetic and real-world datasets.", "conclusion": "nuGPR provides a more computationally efficient approach to GPR training, making it more scalable and practical for complex datasets without sacrificing predictive accuracy."}}
{"id": "2510.12297", "pdf": "https://arxiv.org/pdf/2510.12297", "abs": "https://arxiv.org/abs/2510.12297", "authors": ["Nathan Guermond", "Gopalan Nadathur"], "title": "Ground Stratification for a Logic of Definitions with Induction", "categories": ["cs.LO", "cs.PL", "F.4.1; D.2.4"], "comment": "In Proceedings LFMTP 2025, arXiv:2510.11199", "summary": "The logic underlying the Abella proof assistant includes mechanisms for\ninterpreting atomic predicates through fixed point definitions that can\nadditionally be treated inductively or co-inductively. However, the original\nformulation of the logic includes a strict stratification condition on\ndefinitions that is too restrictive for some applications such as those that\nuse a logical relations based approach to semantic equivalence. Tiu has shown\nhow this restriction can be eased by utilizing a weaker notion referred to as\nground stratification. Tiu's results were limited to a version of the logic\nthat does not treat inductive definitions. We show here that they can be\nextended to cover such definitions. While our results are obtained by using\ntechniques that have been previously deployed in related ways in this context,\ntheir use is sensitive to the particular way in which we generalize the logic.\nIn particular, although ground stratification may be used with arbitrary\nfixed-point definitions, we show that weakening stratification to this form for\ninductive definitions leads to inconsistency. The particular generalization we\ndescribe accords well with the way logical relations are used in practice. Our\nresults are also a intermediate step to building a more flexible form for\ndefinitions into the full logic underlying Abella, which additionally includes\nco-induction, generic quantification, and a mechanism referred to as nominal\nabstraction for analyzing occurrences of objects in terms that are governed by\ngeneric quantifiers.", "AI": {"tldr": "The paper explores generalizing the Abella proof assistant's logic to include ground stratification for inductive definitions while resolving issues of inconsistency.", "motivation": "The strict stratification in the Abella proof assistant's logic limits its applicability to scenarios requiring more flexible semantic equivalence reasoning, such as logical relations approach.", "method": "The authors extend Tiu's concept of ground stratification to cover inductive definitions while addressing and preventing inconsistencies.", "result": "Weakening stratification for inductive definitions leads to inconsistency, but the specific generalization aligns with practical uses of logical relations.", "conclusion": "This work enables more flexible definitions in Abella's underlying logic, serving as a step toward broader enhancements involving co-induction and nominal abstraction."}}
{"id": "2510.11952", "pdf": "https://arxiv.org/pdf/2510.11952", "abs": "https://arxiv.org/abs/2510.11952", "authors": ["Priyanka Dey", "Daniele Rosa", "Wenqing Zheng", "Daniel Barcklow", "Jieyu Zhao", "Emilio Ferrara"], "title": "GRAVITY: A Framework for Personalized Text Generation via Profile-Grounded Synthetic Preferences", "categories": ["cs.CL"], "comment": null, "summary": "Personalization in LLMs often relies on costly human feedback or interaction\nlogs, limiting scalability and neglecting deeper user attributes. To reduce the\nreliance on human annotations, we introduce GRAVITY (Generative Response with\nAligned Values, Interests, and Traits of You), a framework for generating\nsynthetic, profile-grounded preference data that captures users' interests,\nvalues, beliefs, and personality traits. By integrating demographic, cultural,\nand psychological frameworks -- including Hofstede's cultural dimensions,\nSchwartz's basic values, the World Values Survey, and Big Five OCEAN traits --\nGRAVITY synthesizes preference pairs to guide personalized content generation.\nWe evaluate GRAVITY on book descriptions for 400 Amazon users, comparing it to\nprompt-based conditioning, standard fine-tuning, and naive synthetic pair\ngeneration. Profile-grounded synthetic data consistently improves generation,\nespecially across multiple cultures (USA, Brazil, Japan, India), achieving over\n4% higher preference gains across baselines, with user studies showing that\nGRAVITY outputs are preferred over 86% of the time. Our results show that\nscenario-grounded synthetic data can capture richer user variation, reduce\nreliance on costly annotation, and produce more engaging, user-centered\ncontent, offering a scalable path for LLM personalization.", "AI": {"tldr": "The paper proposes GRAVITY, a framework generating synthetic profile-grounded data for LLM personalization and demonstrates its effectiveness in creating user-centered content across different cultures.", "motivation": "Existing personalization in LLMs is constrained by costly human feedback and lacks deeper insights into user attributes such as interests, values, and personality traits.", "method": "The authors developed GRAVITY, which uses psychological and cultural frameworks like Big Five traits and Hofstede\u2019s dimensions, to generate synthetic user preference data that guides content creation.", "result": "GRAVITY improved personalized content generation with over 4% higher preference gains across baselines and was preferred in 86% of user studies, evaluated across cultures.", "conclusion": "Scenario-grounded synthetic data effectively enhances personalization in LLMs, reduces the need for human annotation, and scales personalization efforts while maintaining user engagement."}}
{"id": "2510.12332", "pdf": "https://arxiv.org/pdf/2510.12332", "abs": "https://arxiv.org/abs/2510.12332", "authors": ["Mohammadreza Kasaei", "Mostafa Ghobadi", "Mohsen Khadem"], "title": "Shape-Aware Whole-Body Control for Continuum Robots with Application in Endoluminal Surgical Robotics", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a shape-aware whole-body control framework for\ntendon-driven continuum robots with direct application to endoluminal surgical\nnavigation. Endoluminal procedures, such as bronchoscopy, demand precise and\nsafe navigation through tortuous, patient-specific anatomy where conventional\ntip-only control often leads to wall contact, tissue trauma, or failure to\nreach distal targets. To address these challenges, our approach combines a\nphysics-informed backbone model with residual learning through an Augmented\nNeural ODE, enabling accurate shape estimation and efficient Jacobian\ncomputation. A sampling-based Model Predictive Path Integral (MPPI) controller\nleverages this representation to jointly optimize tip tracking, backbone\nconformance, and obstacle avoidance under actuation constraints. A task manager\nfurther enhances adaptability by allowing real-time adjustment of objectives,\nsuch as wall clearance or direct advancement, during tele-operation. Extensive\nsimulation studies demonstrate millimeter-level accuracy across diverse\nscenarios, including trajectory tracking, dynamic obstacle avoidance, and\nshape-constrained reaching. Real-robot experiments on a bronchoscopy phantom\nvalidate the framework, showing improved lumen-following accuracy, reduced wall\ncontacts, and enhanced adaptability compared to joystick-only navigation and\nexisting baselines. These results highlight the potential of the proposed\nframework to increase safety, reliability, and operator efficiency in minimally\ninvasive endoluminal surgery, with broader applicability to other confined and\nsafety-critical environments.", "AI": {"tldr": "This paper proposes a control framework for tendon-driven continuum robots, enhancing safety and precision in endoluminal surgeries like bronchoscopy.", "motivation": "Address challenges of conventional tip-only control leading to wall trauma and failure in navigating patient-specific anatomy during endoluminal procedures.", "method": "Developed a shape-aware framework integrating a physics-informed backbone model with Augmented Neural ODE for shape estimation and using MPPI controller for path optimization under constraints.", "result": "Simulation studies exhibited high accuracy in diverse tasks including tracking and obstacle avoidance. Experiments on a bronchoscopy phantom showed improved lumen-following and reduced wall contacts.", "conclusion": "This framework enhances precision, safety, and efficiency in endoluminal surgeries, with potential applications in other confined environments."}}
{"id": "2510.12047", "pdf": "https://arxiv.org/pdf/2510.12047", "abs": "https://arxiv.org/abs/2510.12047", "authors": ["Soohan Lim", "Joonghyuk Hahn", "Hyunwoo Park", "Sang-Ki Ko", "Yo-Sub Han"], "title": "Do Large Language Models Respect Contracts? Evaluating and Enforcing Contract-Adherence in Code Generation", "categories": ["cs.AI", "cs.SE", "68T01", "I.2.7"], "comment": "21 pages, 12 figures, 3 tables", "summary": "Prevailing code generation benchmarks, such as HumanEval+ and MBPP+,\nprimarily evaluate large language models (LLMs) with pass@k on functional\ncorrectness using well-formed inputs. However, they ignore a crucial aspect of\nreal-world software: adherence to contracts-the preconditions and validity\nconstraints that dictate how ill-formed inputs must be rejected. This critical\noversight means that existing benchmarks fail to measure, and models\nconsequently fail to generate, truly robust and reliable code snippets. We\nintroduce PACT, a program assessment and contract-adherence evaluation\nframework, to bridge this gap. PACT is the first framework designed to\nsystematically evaluate and enhance contract-adherence in LLM-generated code\nsnippets alongside functional correctness. PACT's contributions are threefold:\nFirst, it provides a comprehensive test-suite corpus focused on contract\nviolations, extending HumanEval+ and MBPP+. Second, it enables a systematic\nanalysis of code generation under varied prompting conditions. This analysis\ndemonstrates that augmenting prompts with contract-violating test cases\nsignificantly enhance a model's ability to respect contracts compared to using\ncontract description alone. Finally, it introduces novel metrics to rigorously\nquantify contract adherence in both test generation and code generation. By\nrevealing critical errors that conventional benchmarks overlook, PACT provides\nthe rigorous and interpretable metrics to evaluate the robustness of\nLLM-generated code snippets in both functionality and contract-adherence.Our\ncode and data are available at https://github.com/suhanmen/PACT.", "AI": {"tldr": "The paper introduces PACT, a framework to evaluate and improve large language models (LLMs) in adhering to software contracts (rules for handling ill-formed inputs) in code generation, along with functional correctness.", "motivation": "Existing code generation benchmarks lack evaluation of adherence to software contracts, which are critical for assessing robustness and reliability of real-world software.", "method": "PACT introduces a test-suite with contract-focused violations, employs varied prompting conditions, and develops metrics to evaluate and enhance contract adherence and functionality in LLM-generated code.", "result": "PACT demonstrates the effectiveness of prompting augmentation in improving contract adherence and reveals overlooked errors in traditional benchmarks, providing a more reliable and interpretable evaluation of LLMs.", "conclusion": "PACT enhances the robustness and reliability assessment of LLM-generated code by addressing both functional correctness and contract adherence, advancing the development of better performing models."}}
{"id": "2510.12120", "pdf": "https://arxiv.org/pdf/2510.12120", "abs": "https://arxiv.org/abs/2510.12120", "authors": ["Zhenyu Mao", "Jacky Keung", "Fengji Zhang", "Shuo Liu", "Yifei Wang", "Jialong Li"], "title": "Towards Engineering Multi-Agent LLMs: A Protocol-Driven Approach", "categories": ["cs.SE"], "comment": null, "summary": "The increasing demand for software development has driven interest in\nautomating software engineering (SE) tasks using Large Language Models (LLMs).\nRecent efforts extend LLMs into multi-agent systems (MAS) that emulate\ncollaborative development workflows, but these systems often fail due to three\ncore deficiencies: under-specification, coordination misalignment, and\ninappropriate verification, arising from the absence of foundational SE\nstructuring principles. This paper introduces Software Engineering Multi-Agent\nProtocol (SEMAP), a protocol-layer methodology that instantiates three core SE\ndesign principles for multi-agent LLMs: (1) explicit behavioral contract\nmodeling, (2) structured messaging, and (3) lifecycle-guided execution with\nverification, and is implemented atop Google's Agent-to-Agent (A2A)\ninfrastructure. Empirical evaluation using the Multi-Agent System Failure\nTaxonomy (MAST) framework demonstrates that SEMAP effectively reduces failures\nacross different SE tasks. In code development, it achieves up to a 69.6%\nreduction in total failures for function-level development and 56.7% for\ndeployment-level development. For vulnerability detection, SEMAP reduces\nfailure counts by up to 47.4% on Python tasks and 28.2% on C/C++ tasks.", "AI": {"tldr": "The paper proposes SEMAP, a protocol for enhancing large language models (LLMs) in multi-agent software development systems. It addresses key issues like under-specification, coordination gaps, and verification challenges, showing significant reductions in task failures.", "motivation": "The paper is motivated by the shortcomings of existing multi-agent systems (MAS) using LLMs in software engineering, particularly in handling under-specification, coordination issues, and poor verification due to the lack of foundational SE principles.", "method": "SEMAP, a protocol-layer methodology implemented on Google's A2A system, integrates three core SE principles: explicit behavioral contracts, structured messaging, and lifecycle-guided execution with verification.", "result": "Empirical results show SEMAP reduces failure rates significantly, with up to 69.6% less failure in function-level code development, 56.7% in deployment-level development, and reductions of 47.4% and 28.2% in vulnerability detection for Python and C/C++ tasks, respectively.", "conclusion": "SEMAP effectively addresses key deficiencies in multi-agent LLM-based systems for software engineering, demonstrating its utility in reducing failure rates and improving workflow reliability in various tasks."}}
{"id": "2510.12021", "pdf": "https://arxiv.org/pdf/2510.12021", "abs": "https://arxiv.org/abs/2510.12021", "authors": ["Leili Barekatain", "Ben Glocker"], "title": "Evaluating the Explainability of Vision Transformers in Medical Imaging", "categories": ["cs.CV"], "comment": "Accepted at Workshop on Interpretability of Machine Intelligence in\n  Medical Image Computing at MICCAI 2025", "summary": "Understanding model decisions is crucial in medical imaging, where\ninterpretability directly impacts clinical trust and adoption. Vision\nTransformers (ViTs) have demonstrated state-of-the-art performance in\ndiagnostic imaging; however, their complex attention mechanisms pose challenges\nto explainability. This study evaluates the explainability of different Vision\nTransformer architectures and pre-training strategies - ViT, DeiT, DINO, and\nSwin Transformer - using Gradient Attention Rollout and Grad-CAM. We conduct\nboth quantitative and qualitative analyses on two medical imaging tasks:\nperipheral blood cell classification and breast ultrasound image\nclassification. Our findings indicate that DINO combined with Grad-CAM offers\nthe most faithful and localized explanations across datasets. Grad-CAM\nconsistently produces class-discriminative and spatially precise heatmaps,\nwhile Gradient Attention Rollout yields more scattered activations. Even in\nmisclassification cases, DINO with Grad-CAM highlights clinically relevant\nmorphological features that appear to have misled the model. By improving model\ntransparency, this research supports the reliable and explainable integration\nof ViTs into critical medical diagnostic workflows.", "AI": {"tldr": "This paper studies the explainability of Vision Transformer architectures in medical imaging using methods like Grad-CAM and Gradient Attention Rollout, finding DINO with Grad-CAM to give localized, accurate explanations.", "motivation": "Interpretability in medical imaging models is essential for clinical trust and adoption, especially given the complexity of Vision Transformers (ViTs).", "method": "The study evaluates four ViT architectures (ViT, DeiT, DINO, and Swin Transformer) with Gradient Attention Rollout and Grad-CAM, analyzing their explainability on classification tasks in blood cell and breast ultrasound images.", "result": "DINO combined with Grad-CAM showed the best localized and faithful explanations across datasets, emphasizing clinically relevant features even in misclassifications.", "conclusion": "The findings enhance transparency and explainability of ViTs in medical diagnostics, promoting their reliable use in critical workflows."}}
{"id": "2510.11839", "pdf": "https://arxiv.org/pdf/2510.11839", "abs": "https://arxiv.org/abs/2510.11839", "authors": ["Yu-Hsiang Wang", "Olgica Milenkovic"], "title": "WaveletDiff: Multilevel Wavelet Diffusion For Time Series Generation", "categories": ["cs.LG"], "comment": null, "summary": "Time series are ubiquitous in many applications that involve forecasting,\nclassification and causal inference tasks, such as healthcare, finance, audio\nsignal processing and climate sciences. Still, large, high-quality time series\ndatasets remain scarce. Synthetic generation can address this limitation;\nhowever, current models confined either to the time or frequency domains\nstruggle to reproduce the inherently multi-scaled structure of real-world time\nseries. We introduce WaveletDiff, a novel framework that trains diffusion\nmodels directly on wavelet coefficients to exploit the inherent\nmulti-resolution structure of time series data. The model combines dedicated\ntransformers for each decomposition level with cross-level attention mechanisms\nthat enable selective information exchange between temporal and frequency\nscales through adaptive gating. It also incorporates energy preservation\nconstraints for individual levels based on Parseval's theorem to preserve\nspectral fidelity throughout the diffusion process. Comprehensive tests across\nsix real-world datasets from energy, finance, and neuroscience domains\ndemonstrate that WaveletDiff consistently outperforms state-of-the-art\ntime-domain and frequency-domain generative methods on both short and long time\nseries across five diverse performance metrics. For example, WaveletDiff\nachieves discriminative scores and Context-FID scores that are $3\\times$\nsmaller on average than the second-best baseline across all datasets.", "AI": {"tldr": "WaveletDiff introduces a method for generating synthetic time series data by leveraging wavelet coefficients through multi-resolution analysis. This approach outperforms existing methods across diverse datasets and metrics.", "motivation": "Current time series datasets are insufficient for many applications, and existing generation methods struggle to capture the complex, multi-scaled nature of real-world data.", "method": "The WaveletDiff framework trains diffusion models on wavelet coefficients, using transformers for multi-resolution decomposition levels and energy preservation constraints based on Parseval's theorem.", "result": "The model showed superior performance across six datasets from different domains, achieving smaller error scores compared to existing generative methods.", "conclusion": "WaveletDiff successfully enhances synthetic time series generation, addressing limitations of time-domain and frequency-domain models."}}
{"id": "2510.12077", "pdf": "https://arxiv.org/pdf/2510.12077", "abs": "https://arxiv.org/abs/2510.12077", "authors": ["Einar Urdshals", "Edmund Lau", "Jesse Hoogland", "Stan van Wingerden", "Daniel Murfet"], "title": "Compressibility Measures Complexity: Minimum Description Length Meets Singular Learning Theory", "categories": ["stat.ML", "cs.LG"], "comment": "33 pages, 21 figures", "summary": "We study neural network compressibility by using singular learning theory to\nextend the minimum description length (MDL) principle to singular models like\nneural networks. Through extensive experiments on the Pythia suite with\nquantization, factorization, and other compression techniques, we find that\ncomplexity estimates based on the local learning coefficient (LLC) are closely,\nand in some cases, linearly correlated with compressibility. Our results\nprovide a path toward rigorously evaluating the limits of model compression.", "AI": {"tldr": "The paper uses singular learning theory to extend the MDL principle for evaluating neural network compressibility and establishes a correlation between local learning coefficient (LLC) and compressibility.", "motivation": "To rigorously evaluate and understand the limits of neural network model compression by establishing a theoretical framework using singular learning theory and exploring its connection with compressibility.", "method": "The authors extend the MDL principle for singular models like neural networks through singular learning theory and perform extensive experiments using the Pythia suite with various compression techniques.", "result": "The study demonstrates that complexity estimates based on the local learning coefficient (LLC) are highly and, in some cases, linearly correlated with model compressibility.", "conclusion": "Local learning coefficient (LLC)-based complexity estimates can provide an effective framework for understanding and evaluating neural network compressibility, offering a theoretical tool to examine the limits of compression."}}
{"id": "2510.12702", "pdf": "https://arxiv.org/pdf/2510.12702", "abs": "https://arxiv.org/abs/2510.12702", "authors": ["Cedric Richter", "Heike Wehrheim"], "title": "Beyond Postconditions: Can Large Language Models infer Formal Contracts for Automatic Software Verification?", "categories": ["cs.SE", "cs.AI", "cs.PL"], "comment": "under submission", "summary": "Automatic software verifiers have become increasingly effective at the task\nof checking software against (formal) specifications. Yet, their adoption in\npractice has been hampered by the lack of such specifications in real world\ncode. Large Language Models (LLMs) have shown promise in inferring formal\npostconditions from natural language hints embedded in code such as function\nnames, comments or documentation. Using the generated postconditions as\nspecifications in a subsequent verification, however, often leads verifiers to\nsuggest invalid inputs, hinting at potential issues that ultimately turn out to\nbe false alarms.\n  To address this, we revisit the problem of specification inference from\nnatural language in the context of automatic software verification. In the\nprocess, we introduce NL2Contract, the task of employing LLMs to translate\ninformal natural language into formal functional contracts, consisting of\npostconditions as well as preconditions. We introduce metrics to validate and\ncompare different NL2Contract approaches, using soundness, bug discriminative\npower of the generated contracts and their usability in the context of\nautomatic software verification as key metrics. We evaluate NL2Contract with\ndifferent LLMs and compare it to the task of postcondition generation\nnl2postcond. Our evaluation shows that (1) LLMs are generally effective at\ngenerating functional contracts sound for all possible inputs, (2) the\ngenerated contracts are sufficiently expressive for discriminating buggy from\ncorrect behavior, and (3) verifiers supplied with LLM inferred functional\ncontracts produce fewer false alarms than when provided with postconditions\nalone. Further investigations show that LLM inferred preconditions generally\nalign well with developers intentions which allows us to use automatic software\nverifiers to catch real-world bugs.", "AI": {"tldr": "This paper introduces NL2Contract, a task where Large Language Models (LLMs) translate natural language into formal functional contracts for software verification to improve accuracy and reduce false alarms.", "motivation": "The lack of formal specifications in real-world software code hampers the broader adoption of automatic software verifiers, which rely on specifications for effectiveness.", "method": "The paper integrates LLMs to infer formal functional contracts (including preconditions and postconditions) from natural language hints in code, and evaluates these generated contracts based on metrics such as soundness, bug discriminative power, and usability for verification.", "result": "Findings show that LLMs effectively generate sound and expressive functional contracts, improving the ability to discriminate between buggy and correct behavior, and lowering false alarms for software verifiers.", "conclusion": "LLM-inferred preconditions align with developer intentions and enhance automatic software verifiers\u2019 capacity to identify real-world bugs more accurately."}}
{"id": "2510.11956", "pdf": "https://arxiv.org/pdf/2510.11956", "abs": "https://arxiv.org/abs/2510.11956", "authors": ["Gabrielle Kaili-May Liu", "Bryan Li", "Arman Cohan", "William Gantt Walden", "Eugene Yang"], "title": "Evaluating Retrieval-Augmented Generation Systems on Unanswerable, Uncheatable, Realistic, Multi-hop Queries", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Real-world use cases often present RAG systems with complex queries for which\nrelevant information is missing from the corpus or is incomplete. In these\nsettings, RAG systems must be able to reject unanswerable, out-of-scope queries\nand identify failures of retrieval and multi-hop reasoning. Despite this,\nexisting RAG benchmarks rarely reflect realistic task complexity for multi-hop\nor out-of-scope questions, which often can be cheated via disconnected\nreasoning (i.e., solved without genuine multi-hop inference) or require only\nsimple factual recall. This limits the ability for such benchmarks to uncover\nlimitations of existing RAG systems. To address this gap, we present the first\npipeline for automatic, difficulty-controlled creation of\nun$\\underline{c}$heatable, $\\underline{r}$ealistic, $\\underline{u}$nanswerable,\nand $\\underline{m}$ulti-hop $\\underline{q}$uerie$\\underline{s}$ (CRUMQs),\nadaptable to any corpus and domain. We use our pipeline to create CRUMQs over\ntwo popular RAG datasets and demonstrate its effectiveness via benchmark\nexperiments on leading retrieval-augmented LLMs. Results show that compared to\nprior RAG benchmarks, CRUMQs are highly challenging for RAG systems and achieve\nup to 81.0\\% reduction in cheatability scores. More broadly, our pipeline\noffers a simple way to enhance benchmark difficulty and realism and drive\ndevelopment of more capable RAG systems.", "AI": {"tldr": "The paper introduces a pipeline for generating challenging benchmark queries to improve the evaluation of retrieval-augmented generation (RAG) systems.", "motivation": "Existing RAG benchmarks fail to adequately test systems on complex queries, multi-hop reasoning, and out-of-scope questions, which results in limitations being overlooked.", "method": "The authors developed a pipeline to create uncheatable, realistic, unanswerable, and multi-hop queries (CRUMQs) that can be applied to any corpus or domain.", "result": "Experiments on leading RAG systems demonstrated that CRUMQs significantly increase difficulty, reducing cheatability scores by up to 81.0% compared to existing benchmarks.", "conclusion": "The proposed CRUMQs pipeline enhances benchmark realism and difficulty, encouraging the development of more advanced and capable RAG systems."}}
{"id": "2510.12340", "pdf": "https://arxiv.org/pdf/2510.12340", "abs": "https://arxiv.org/abs/2510.12340", "authors": ["Nicky Mol", "Luka Peternel", "Alessandro Ianniello", "Denis Zatyagov", "Auke Nachenius", "Stephan Balvert", "J. Micah Prendergast", "Sara Muscolo", "Olger Siebinga", "Eva Verhoef", "Deborah Forster", "David A. Abbink"], "title": "Achieving Meaningful Collaboration: Worker-centered Design of a Physical Human-Robot Collaborative Blending Task", "categories": ["cs.RO"], "comment": "3 pages, 1 figure, ICRA@40 (Extended abstract)", "summary": "The use of robots in industrial settings continues to grow, driven by the\nneed to address complex societal challenges such as labor shortages, aging\npopulations, and ever-increasing production demands. In this abstract, we\nadvocate for (and demonstrate) a transdisciplinary approach when considering\nrobotics in the workplace. Transdisciplinarity emphasizes the integration of\nacademic research with pragmatic expertise and embodied experiential knowledge,\nthat prioritize values such as worker wellbeing and job attractiveness. In the\nfollowing, we describe an ongoing multi-pronged effort to explore the potential\nof collaborative robots in the context of airplane engine repair and\nmaintenance operations.", "AI": {"tldr": "This paper proposes a transdisciplinary approach to using collaborative robots in airplane engine repair, focusing on worker well-being and system effectiveness.", "motivation": "Address the increasing demand for production efficiency while adapting to labor shortages and aging populations in industrial settings.", "method": "Implementation of a transdisciplinary framework combining academic research, practical expertise, and worker-oriented values to develop solutions using collaborative robots.", "result": "Demonstrated potential of collaborative robots to improve airplane engine repair and maintenance processes.", "conclusion": "The transdisciplinary approach has value in integrating technology to better address industrial challenges while enhancing workplace conditions for laborers."}}
{"id": "2510.12061", "pdf": "https://arxiv.org/pdf/2510.12061", "abs": "https://arxiv.org/abs/2510.12061", "authors": ["Yiheng Chen", "Lingyao Li", "Zihui Ma", "Qikai Hu", "Yilun Zhu", "Min Deng", "Runlong Yu"], "title": "Empowering LLM Agents with Geospatial Awareness: Toward Grounded Reasoning for Wildfire Response", "categories": ["cs.AI"], "comment": null, "summary": "Effective disaster response is essential for safeguarding lives and property.\nExisting statistical approaches often lack semantic context, generalize poorly\nacross events, and offer limited interpretability. While Large language models\n(LLMs) provide few-shot generalization, they remain text-bound and blind to\ngeography. To bridge this gap, we introduce a Geospatial Awareness Layer (GAL)\nthat grounds LLM agents in structured earth data. Starting from raw wildfire\ndetections, GAL automatically retrieves and integrates infrastructure,\ndemographic, terrain, and weather information from external geodatabases,\nassembling them into a concise, unit-annotated perception script. This enriched\ncontext enables agents to produce evidence-based resource-allocation\nrecommendations (e.g., personnel assignments, budget allocations), further\nreinforced by historical analogs and daily change signals for incremental\nupdates. We evaluate the framework in real wildfire scenarios across multiple\nLLM models, showing that geospatially grounded agents can outperform baselines.\nThe proposed framework can generalize to other hazards such as floods and\nhurricanes.", "AI": {"tldr": "This paper introduces a Geospatial Awareness Layer (GAL) to enhance disaster response models by integrating geospatial data, improving semantic context and interpretability in recommendations.", "motivation": "Existing disaster response approaches lack semantic context, geographic awareness, and interpretability, limiting their effectiveness in decision-making during emergencies such as wildfires.", "method": "The authors propose adding a Geospatial Awareness Layer (GAL) to LLMs to ground them in geospatial data. GAL integrates structured information on infrastructure, demographics, terrain, and weather into a perception script for data-driven recommendations.", "result": "In testing on wildfire scenarios using multiple LLMs, geospatially grounded agents equipped with GAL showcased enhanced performance, surpassing traditional baselines in resource allocation and decision-making.", "conclusion": "The proposed GAL framework improves the applicability and efficiency of LLMs in disaster management scenarios and has the potential to be adapted for various hazard types like floods and hurricanes."}}
{"id": "2510.12186", "pdf": "https://arxiv.org/pdf/2510.12186", "abs": "https://arxiv.org/abs/2510.12186", "authors": ["Yun Peng", "Kisub Kim", "Linghan Meng", "Kui Liu"], "title": "iCodeReviewer: Improving Secure Code Review with Mixture of Prompts", "categories": ["cs.SE"], "comment": null, "summary": "Code review is an essential process to ensure the quality of software that\nidentifies potential software issues at an early stage of software development.\nAmong all software issues, security issues are the most important to identify,\nas they can easily lead to severe software crashes and service disruptions.\nRecent research efforts have been devoted to automated approaches to reduce the\nmanual efforts required in the secure code review process. Despite the\nprogress, current automated approaches on secure code review, including static\nanalysis, deep learning models, and prompting approaches, still face the\nchallenges of limited precision and coverage, and a lack of comprehensive\nevaluation.\n  To mitigate these challenges, we propose iCodeReviewer, which is an automated\nsecure code review approach based on large language models (LLMs).\niCodeReviewer leverages a novel mixture-of-prompts architecture that\nincorporates many prompt experts to improve the coverage of security issues.\nEach prompt expert is a dynamic prompt pipeline to check the existence of a\nspecific security issue. iCodeReviewer also implements an effective routing\nalgorithm to activate only necessary prompt experts based on the code features\nin the input program, reducing the false positives induced by LLM\nhallucination. Experiment results in our internal dataset demonstrate the\neffectiveness of iCodeReviewer in security issue identification and\nlocalization with an F1 of 63.98%. The review comments generated by\niCodeReviewer also achieve a high acceptance rate up to 84% when it is deployed\nin production environments.", "AI": {"tldr": "The paper introduces iCodeReviewer, an automated secure code review system based on large language models (LLMs), utilizing a mixture-of-prompts approach to enhance precision and coverage in identifying security issues.", "motivation": "Ensure software security and minimize human effort in code reviews, addressing the limitations of current automated secure code review methods such as low precision, coverage, and lack of thorough evaluation.", "method": "iCodeReviewer employs a mixture-of-prompts architecture that segments the process into dynamic prompt experts, each checking specific security issues. Additionally, it uses a routing algorithm to reduce false positives by activating only necessary prompts.", "result": "Achieves an F1 score of 63.98% for security issue identification and localization, with an 84% acceptance rate of review comments in production.", "conclusion": "iCodeReviewer demonstrates enhanced reliability and effectiveness in automated secure code reviews, addressing longstanding challenges in precision, coverage, and practical evaluation."}}
{"id": "2510.12056", "pdf": "https://arxiv.org/pdf/2510.12056", "abs": "https://arxiv.org/abs/2510.12056", "authors": ["Xinxin Huang", "Han Sun", "Junmin Cai", "Ningzhong Liu", "Huiyu Zhou"], "title": "APGNet: Adaptive Prior-Guided for Underwater Camouflaged Object Detection", "categories": ["cs.CV"], "comment": "6 pages. accepted by ACM MM Asia 2025", "summary": "Detecting camouflaged objects in underwater environments is crucial for\nmarine ecological research and resource exploration. However, existing methods\nface two key challenges: underwater image degradation, including low contrast\nand color distortion, and the natural camouflage of marine organisms.\nTraditional image enhancement techniques struggle to restore critical features\nin degraded images, while camouflaged object detection (COD) methods developed\nfor terrestrial scenes often fail to adapt to underwater environments due to\nthe lack of consideration for underwater optical characteristics.\n  To address these issues, we propose APGNet, an Adaptive Prior-Guided Network,\nwhich integrates a Siamese architecture with a novel prior-guided mechanism to\nenhance robustness and detection accuracy. First, we employ the Multi-Scale\nRetinex with Color Restoration (MSRCR) algorithm for data augmentation,\ngenerating illumination-invariant images to mitigate degradation effects.\nSecond, we design an Extended Receptive Field (ERF) module combined with a\nMulti-Scale Progressive Decoder (MPD) to capture multi-scale contextual\ninformation and refine feature representations. Furthermore, we propose an\nadaptive prior-guided mechanism that hierarchically fuses position and boundary\npriors by embedding spatial attention in high-level features for coarse\nlocalization and using deformable convolution to refine contours in low-level\nfeatures.\n  Extensive experimental results on two public MAS datasets demonstrate that\nour proposed method APGNet outperforms 15 state-of-art methods under widely\nused evaluation metrics.", "AI": {"tldr": "APGNet is proposed to detect camouflaged objects in underwater environments by mitigating image degradation effects and enhancing detection performance.", "motivation": "Underwater image degradation and natural camouflage of marine organisms hinder accurate camouflaged object detection in underwater environments.", "method": "APGNet integrates a Siamese architecture with prior-guided mechanisms, MSRCR algorithm for illumination correction, and adaptive modules for multi-scale feature refinement.", "result": "APGNet outperformed 15 state-of-the-art methods on two public MAS datasets based on evaluation metrics.", "conclusion": "APGNet improves underwater camouflaged object detection by addressing challenges in optical degradation and natural camouflage effectively."}}
{"id": "2510.11842", "pdf": "https://arxiv.org/pdf/2510.11842", "abs": "https://arxiv.org/abs/2510.11842", "authors": ["Urs Spiegelhalter", "J\u00f6rg K. H. Franke", "Frank Hutter"], "title": "Balancing Synthetic Data and Replay for Enhancing Task-Specific Capabilities", "categories": ["cs.LG", "cs.CL"], "comment": "Presented at 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025) Workshop on Continual and Compatible Foundation Model Updates\n  (CCFM)", "summary": "Adapting language models to new tasks through continued pretraining faces a\nfundamental trade-off: models must learn new capabilities while avoiding\ncatastrophic forgetting of existing knowledge. While prior work has studied\nsynthetic data generation techniques, the optimal replay ratios for balancing\ntask performance and knowledge retention under computational constraints remain\npoorly understood. We present a comprehensive empirical study investigating the\ninterplay between replay ratio configuration and computational budget when\nadapting language models to new tasks. Using the bAbI reasoning tasks as our\ntarget objective, we apply synthetic data generation and systematically\nevaluate different total token budgets and replay ratio configurations. We\nanalyze their effects on both task mastery and general knowledge retention. Our\nexperiments reveal an optimal configuration that balances task-specific\nperformance with general knowledge retention. Based on our findings, we provide\nempirically-grounded guidelines for selecting replay ratios based on\ncomputational budget, enabling practitioners to achieve strong task adaptation\nwith significantly reduced training costs.", "AI": {"tldr": "The study examines how to optimally balance new task adaptation and knowledge retention in language models under computational constraints using replay ratios.", "motivation": "The paper aims to address the issue of retaining existing knowledge while adapting language models to new tasks without catastrophic forgetting.", "method": "An empirical study is conducted using synthetic data generation, the bAbI reasoning tasks, and varying replay ratio configurations under different total token budgets.", "result": "The experiments identified an optimal replay ratio configuration that balances task-specific performance with knowledge retention.", "conclusion": "Practitioners can use the study's guidelines to adapt language models effectively with lower training costs while achieving good task adaptation and preserving general knowledge."}}
{"id": "2510.12152", "pdf": "https://arxiv.org/pdf/2510.12152", "abs": "https://arxiv.org/abs/2510.12152", "authors": ["Chaiwon Kim", "Jongyeong Lee", "Min-hwan Oh"], "title": "Follow-the-Perturbed-Leader for Decoupled Bandits: Best-of-Both-Worlds and Practicality", "categories": ["stat.ML", "cs.LG"], "comment": "Preprint, 29 pages", "summary": "We study the decoupled multi-armed bandit (MAB) problem, where the learner\nselects one arm for exploration and one arm for exploitation in each round. The\nloss of the explored arm is observed but not counted, while the loss of the\nexploited arm is incurred without being observed. We propose a policy within\nthe Follow-the-Perturbed-Leader (FTPL) framework using Pareto perturbations.\nOur policy achieves (near-)optimal regret regardless of the environment, i.e.,\nBest-of-Both-Worlds (BOBW): constant regret in the stochastic regime, improving\nupon the optimal bound of the standard MABs, and minimax optimal regret in the\nadversarial regime. Moreover, the practicality of our policy stems from\navoiding both the convex optimization step required by the previous BOBW\npolicy, Decoupled-Tsallis-INF (Rouyer & Seldin, 2020), and the resampling step\nthat is typically necessary in FTPL. Consequently, it achieves substantial\ncomputational improvement, about $20$ times faster than Decoupled-Tsallis-INF,\nwhile also demonstrating better empirical performance in both regimes. Finally,\nwe empirically show that our approach outperforms a pure exploration policy,\nand that naively combining a pure exploration with a standard exploitation\npolicy is suboptimal.", "AI": {"tldr": "The paper introduces a new policy for decoupled multi-armed bandit (MAB) problems, achieving near-optimal regret in both stochastic and adversarial settings with significant computational efficiency.", "motivation": "The motivation lies in addressing the decoupled MAB challenge, where the learner must balance exploration and exploitation with partially observable losses, while improving computational efficiency and achieving optimal regret bounds.", "method": "A policy based on the Follow-the-Perturbed-Leader (FTPL) framework using Pareto perturbations is proposed, eliminating the need for convex optimization and resampling steps required by previous policies.", "result": "The new policy achieves constant regret in stochastic settings, minimax optimal regret in adversarial settings, and is approximately 20 times faster than the Decoupled-Tsallis-INF policy while showing superior empirical performance.", "conclusion": "This work demonstrates a practically efficient, high-performing solution for decoupled MAB problems, surpassing existing approaches both theoretically and computationally."}}
{"id": "2510.12469", "pdf": "https://arxiv.org/pdf/2510.12469", "abs": "https://arxiv.org/abs/2510.12469", "authors": ["Filip Rezabek", "Moe Mahhouk", "Andrew Miller", "Stefan Genchev", "Quintus Kilbourn", "Georg Carle", "Jonathan Passerat-Palmbach"], "title": "Proof of Cloud: Data Center Execution Assurance for Confidential VMs", "categories": ["cs.CR", "cs.DC"], "comment": null, "summary": "Confidential Virtual Machines (CVMs) protect data in use by running workloads\ninside hardware-isolated environments. In doing so, they also inherit the\nlimitations of the underlying hardware. Trusted Execution Environments (TEEs),\nwhich enforce this isolation, explicitly exclude adversaries with physical\naccess from their threat model. Commercial TEEs, e.g., Intel TDX, thus assume\ninfrastructure providers do not physically exploit hardware and serve as\nsafeguards instead. This creates a tension: tenants must trust provider\nintegrity at the hardware layer, yet existing remote attestation offers no way\nto verify that CVMs actually run on physically trusted platforms, leaving\ntoday's CVM deployments unable to demonstrate that their guarantees align with\nthe TEE vendor's threat model.\n  We bridge this confidence gap with Data Center Execution Assurance (DCEA), a\ndesign generating \"Proofs of Cloud\". DCEA binds a CVM to its underlying\nplatform using vTPM-anchored measurements, ensuring CVM launch evidence and TPM\nquotes refer to the same physical chassis.\n  This takes advantage of the fact that data centers are often identifiable via\nTPMs. Our approach applies to CVMs accessing vTPMs and running on top of\nsoftware stacks fully controlled by the cloud provider, as well as\nsingle-tenant bare-metal deployments with discrete TPMs. We trust providers for\nintegrity (certificate issuance), but not for the confidentiality of\nCVM-visible state. DCEA enables remote verification of a CVM's platform origin\nand integrity, mitigating attacks like replay and attestation proxying. We\ninclude a candidate implementation on Google Cloud and Intel TDX that leverages\nIntel TXT for trusted launch. Our design refines CVMs' threat model and\nprovides a practical path for deploying high-assurance, confidential workloads\nin minimally trusted environments.", "AI": {"tldr": "This paper introduces Data Center Execution Assurance (DCEA), a mechanism to ensure that Confidential Virtual Machines (CVMs) operate within physically trusted environments, thus bridging the gap in existing remote attestation processes.", "motivation": "The paper was motivated by the lack of mechanisms to verify that CVMs run on physically trusted data center platforms, which leads to gaps in confidence about alignment with TEE vendors' threat models.", "method": "The authors propose DCEA, which combines vTPM-anchored measurements and TPM quotes to verify the physical platform integrity of CVMs. This method enables remote verification and mitigates attacks like replay and attestation proxying.", "result": "The proposed DCEA design was implemented on Google Cloud using Intel TDX and Intel TXT, demonstrating its practicality in real-world cloud environments.", "conclusion": "The paper refines the threat model of CVMs and offers a practical, high-assurance approach for deploying confidential workloads in environments with minimal trusted infrastructure."}}
{"id": "2510.11958", "pdf": "https://arxiv.org/pdf/2510.11958", "abs": "https://arxiv.org/abs/2510.11958", "authors": ["Xuan Luo", "Weizhi Wang", "Xifeng Yan"], "title": "Direct Multi-Token Decoding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Decoder-only transformers have become the standard architecture for large\nlanguage models (LLMs) due to their strong performance. Recent studies suggest\nthat, in pre-trained LLMs, early, middle, and late layers may serve distinct\nroles: Early layers focus on understanding the input context, middle layers\nhandle task-specific processing, and late layers convert abstract\nrepresentations into output tokens. We hypothesize that once representations\nhave been processed by the early and middle layers, the resulting hidden states\nmay encapsulate sufficient information to support the generation of multiple\ntokens using only the late layers, eliminating the need to repeatedly traverse\nthe early and middle layers. We refer to this inference paradigm as Direct\nMulti-Token Decoding (DMTD). Unlike speculative decoding, our method introduces\nno additional parameters, auxiliary routines, or post-generation verification.\nDespite being trained on a limited dataset, a fine-tuned DMTD Qwen3-4B model\nhas already demonstrated promising results, achieving up to a 2x speedup with\nonly minor performance loss. Moreover, as shown in our scaling analysis, its\nperformance is expected to further improve with larger training datasets.", "AI": {"tldr": "The paper proposes Direct Multi-Token Decoding (DMTD), a paradigm allowing late layers of transformers to generate multiple tokens from processed representations, achieving faster inference speeds.", "motivation": "The motivation is to optimize the efficiency of large language models (LLMs) by reducing computational redundancies during token generation without compromising performance.", "method": "This approach eliminates repeated traversals of early and middle layers by utilizing the fully processed hidden states for multi-token generation directly in late layers.", "result": "The fine-tuned DMTD model (Qwen3-4B) achieved up to 2x speed improvements in token generation with minimal performance degradation and showed potential for scaling with larger datasets.", "conclusion": "DMTD introduces a novel inference method that enhances LLM efficiency, presenting a promising alternative to traditional decoding approaches and offering scalable gains in larger setups."}}
{"id": "2510.12346", "pdf": "https://arxiv.org/pdf/2510.12346", "abs": "https://arxiv.org/abs/2510.12346", "authors": ["Bingquan Li", "Ning Wang", "Tianwei Zhang", "Zhicheng He", "Yucong Wu"], "title": "PolygMap: A Perceptive Locomotion Framework for Humanoid Robot Stair Climbing", "categories": ["cs.RO"], "comment": null, "summary": "Recently, biped robot walking technology has been significantly developed,\nmainly in the context of a bland walking scheme. To emulate human walking,\nrobots need to step on the positions they see in unknown spaces accurately. In\nthis paper, we present PolyMap, a perception-based locomotion planning\nframework for humanoid robots to climb stairs. Our core idea is to build a\nreal-time polygonal staircase plane semantic map, followed by a footstep planar\nusing these polygonal plane segments. These plane segmentation and visual\nodometry are done by multi-sensor fusion(LiDAR, RGB-D camera and IMUs). The\nproposed framework is deployed on a NVIDIA Orin, which performs 20-30 Hz\nwhole-body motion planning output. Both indoor and outdoor real-scene\nexperiments indicate that our method is efficient and robust for humanoid robot\nstair climbing.", "AI": {"tldr": "This paper proposes PolyMap, a real-time framework enabling humanoid robots to climb stairs effectively using multi-sensor data and motion planning.", "motivation": "The authors aim to improve humanoid robots' ability to emulate human walking by accurately stepping on visualized positions, specifically designed for climbing stairs in unknown spaces.", "method": "They introduced PolyMap, which employs a real-time polygonal staircase plane semantic map and multi-sensor fusion (LiDAR, RGB-D camera, IMUs) for plane segmentation and visual odometry. Footstep planning integrates these inputs, running on NVIDIA Orin hardware at 20-30 Hz.", "result": "Experiments conducted in indoor and outdoor real-world scenes demonstrate PolyMap's efficiency and robustness in enabling humanoid robots to climb stairs.", "conclusion": "PolyMap provides a solid perception-based locomotion planning framework for humanoid robots to navigate stairs efficiently, contributing significantly to advancements in biped robot walking technology."}}
{"id": "2510.12063", "pdf": "https://arxiv.org/pdf/2510.12063", "abs": "https://arxiv.org/abs/2510.12063", "authors": ["Sunzhu Li", "Zhiyu Lin", "Shuling Yang", "Jiale Zhao", "Wei Chen"], "title": "ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) are powerful, but they still suffer from\ninefficient and off-target reasoning. Currently, training-free methods are\nlimited to either rigid heuristics or descriptive, non-actionable analyses. In\nthis paper, we introduce ThinkPilot, a training-free framework that\nautomatically optimizes LRMs reasoning. It uses an evolutionary process to\ngenerate think-prefixes, which are instructions that evolve driven by a\ntaxonomy of reasoning behaviors to guide models toward superior performance.\nExtensive experiments demonstrate ThinkPilot's broad effectiveness: it\nsignificantly improves the accuracy-length trade-off for efficient reasoning,\ndrastically improves safety (for example, cutting the StrongREJECT score of\nDeepSeek-R1-Distill-Qwen-32B from 27.0% to 0.7), and enhances instruction\nfollowing. It also synergizes with existing training-based methods. Our\nanalysis reveals that think-prefixes can reliably control LRMs' reasoning\nbehaviors, and that different tasks have strong preferences for specific\nbehavioral distributions. By automatically identifying and eliciting these\nbehaviors, ThinkPilot provides a generalizable framework for aligning LRMs\nreasoning with task demands. Data and code are available at\nhttps://github.com/teqkilla/ThinkPilot", "AI": {"tldr": "ThinkPilot improves large reasoning models' (LRMs) reasoning through a training-free framework using think-prefixes to optimize efficiency, safety, and instruction-following.", "motivation": "Existing LRMs suffer from inefficient and off-target reasoning, and current methods to improve them lack adaptability and actionable solutions.", "method": "ThinkPilot employs an evolutionary process to create think-prefixes\u2014specific instructions that guide reasoning behaviors, using a taxonomy of behaviors for tailored optimization.", "result": "Extensive experiments show ThinkPilot enhances efficiency in reasoning, drastically improves safety, strengthens instruction adherence, and works well alongside training-based methods.", "conclusion": "ThinkPilot reliably optimizes LRMs' reasoning by aligning behavioral distributions with task-specific demands, offering a scalable and actionable solution for LRM improvement."}}
{"id": "2510.12294", "pdf": "https://arxiv.org/pdf/2510.12294", "abs": "https://arxiv.org/abs/2510.12294", "authors": ["Gerg\u0151 Balogh", "D\u00e1vid K\u00f3sz\u00f3", "Homayoun Safarpour Motealegh Mahalegi", "L\u00e1szl\u00f3 T\u00f3th", "Bence Szak\u00e1cs", "\u00c1ron B\u00facs\u00fa"], "title": "Show Your Title! A Scoping Review on Verbalization in Software Engineering with LLM-Assisted Screening", "categories": ["cs.SE"], "comment": "preprint of a paper under publication in Quality of Information and\n  Communications Technology 2025", "summary": "Understanding how software developers think, make decisions, and behave\nremains a key challenge in software engineering (SE). Verbalization techniques\n(methods that capture spoken or written thought processes) offer a lightweight\nand accessible way to study these cognitive aspects. This paper presents a\nscoping review of research at the intersection of SE and psychology (PSY),\nfocusing on the use of verbal data. To make large-scale interdisciplinary\nreviews feasible, we employed a large language model (LLM)-assisted screening\npipeline using GPT to assess the relevance of over 9,000 papers based solely on\ntitles. We addressed two questions: what themes emerge from\nverbalization-related work in SE, and how effective are LLMs in supporting\ninterdisciplinary review processes? We validated GPT's outputs against human\nreviewers and found high consistency, with a 13\\% disagreement rate. Prominent\nthemes mainly were tied to the craft of SE, while more human-centered topics\nwere underrepresented. The data also suggests that SE frequently draws on PSY\nmethods, whereas the reverse is rare.", "AI": {"tldr": "This paper reviews the intersection of software engineering (SE) and psychology (PSY) through verbalization techniques and evaluates the use of GPT for screening papers.", "motivation": "The paper seeks to understand software developers' cognition, decision-making, and behaviors using verbalization techniques, while addressing the effectiveness of GPT in interdisciplinary literature screening.", "method": "A scoping review using a GPT-assisted screening pipeline assessed the relevance of over 9,000 papers based on titles, validating against human reviews.", "result": "High consistency between GPT and human reviewers (13% disagreement rate); themes in verbalization work focus on SE craft, with less emphasis on human-centered topics. SE draws heavily on PSY methods, while PSY rarely draws on SE.", "conclusion": "Verbalization techniques effectively study cognitive aspects in SE. GPT is efficient for interdisciplinary reviews, though human-centered topics need more exploration in SE and PSY integration."}}
{"id": "2510.12069", "pdf": "https://arxiv.org/pdf/2510.12069", "abs": "https://arxiv.org/abs/2510.12069", "authors": ["Sandeep Mishra", "Oindrila Saha", "Alan C. Bovik"], "title": "VIDMP3: Video Editing by Representing Motion with Pose and Position Priors", "categories": ["cs.CV"], "comment": null, "summary": "Motion-preserved video editing is crucial for creators, particularly in\nscenarios that demand flexibility in both the structure and semantics of\nswapped objects. Despite its potential, this area remains underexplored.\nExisting diffusion-based editing methods excel in structure-preserving tasks,\nusing dense guidance signals to ensure content integrity. While some recent\nmethods attempt to address structure-variable editing, they often suffer from\nissues such as temporal inconsistency, subject identity drift, and the need for\nhuman intervention. To address these challenges, we introduce VidMP3, a novel\napproach that leverages pose and position priors to learn a generalized motion\nrepresentation from source videos. Our method enables the generation of new\nvideos that maintain the original motion while allowing for structural and\nsemantic flexibility. Both qualitative and quantitative evaluations demonstrate\nthe superiority of our approach over existing methods. The code will be made\npublicly available at https://github.com/sandeep-sm/VidMP3.", "AI": {"tldr": "VidMP3 introduces a novel approach for motion-preserved video editing that solves issues like temporal inconsistency and subject identity drift.", "motivation": "Existing editing methods face challenges like poor temporal consistency, identity drift, and dependency on human intervention.", "method": "VidMP3 uses pose and position priors to learn generalized motion representation, ensuring flexibility in video editing while preserving motion.", "result": "Both qualitative and quantitative evaluations show VidMP3 outperforms current methods in video editing tasks.", "conclusion": "VidMP3 provides a superior and generalized motion-preservation video editing technique, addressing prior limitations."}}
{"id": "2510.11852", "pdf": "https://arxiv.org/pdf/2510.11852", "abs": "https://arxiv.org/abs/2510.11852", "authors": ["Saroj Basnet", "Shafkat Farabi", "Tharindu Ranasinghe", "Diptesh Kanoji", "Marcos Zampieri"], "title": "Evaluating Open-Source Vision-Language Models for Multimodal Sarcasm Detection", "categories": ["cs.LG"], "comment": "Accepted to ICDMW 2025 Workshop on Multimodal AI (MMAI). Full\n  workshop info: https://icdmw25mmai.github.io/", "summary": "Recent advances in open-source vision-language models (VLMs) offer new\nopportunities for understanding complex and subjective multimodal phenomena\nsuch as sarcasm. In this work, we evaluate seven state-of-the-art VLMs - BLIP2,\nInstructBLIP, OpenFlamingo, LLaVA, PaliGemma, Gemma3, and Qwen-VL - on their\nability to detect multimodal sarcasm using zero-, one-, and few-shot prompting.\nFurthermore, we evaluate the models' capabilities in generating explanations to\nsarcastic instances. We evaluate the capabilities of VLMs on three benchmark\nsarcasm datasets (Muse, MMSD2.0, and SarcNet). Our primary objectives are\ntwofold: (1) to quantify each model's performance in detecting sarcastic\nimage-caption pairs, and (2) to assess their ability to generate human-quality\nexplanations that highlight the visual-textual incongruities driving sarcasm.\nOur results indicate that, while current models achieve moderate success in\nbinary sarcasm detection, they are still not able to generate high-quality\nexplanations without task-specific finetuning.", "AI": {"tldr": "The paper evaluates the performance of seven vision-language models in detecting and explaining multimodal sarcasm.", "motivation": "To understand how well current open-source vision-language models can detect and explain sarcasm in multimodal (image-caption) scenarios.", "method": "The authors tested seven state-of-the-art vision-language models on three sarcasm datasets using zero-shot, one-shot, and few-shot prompting approaches for both sarcasm detection and explanation generation.", "result": "The models demonstrated moderate effectiveness in binary sarcasm detection but struggled to generate high-quality explanations, revealing limitations without task-specific finetuning.", "conclusion": "While current models show some promise in sarcasm detection, their lack of ability to generate meaningful explanations highlights the need for further improvements, particularly through finetuning."}}
{"id": "2510.12311", "pdf": "https://arxiv.org/pdf/2510.12311", "abs": "https://arxiv.org/abs/2510.12311", "authors": ["Joanna Marks", "Tim Y. J. Wang", "O. Deniz Akyildiz"], "title": "Learning Latent Energy-Based Models via Interacting Particle Langevin Dynamics", "categories": ["stat.ML", "cs.LG", "stat.CO"], "comment": null, "summary": "We develop interacting particle algorithms for learning latent variable\nmodels with energy-based priors. To do so, we leverage recent developments in\nparticle-based methods for solving maximum marginal likelihood estimation\n(MMLE) problems. Specifically, we provide a continuous-time framework for\nlearning latent energy-based models, by defining stochastic differential\nequations (SDEs) that provably solve the MMLE problem. We obtain a practical\nalgorithm as a discretisation of these SDEs and provide theoretical guarantees\nfor the convergence of the proposed algorithm. Finally, we demonstrate the\nempirical effectiveness of our method on synthetic and image datasets.", "AI": {"tldr": "This paper proposes a new interacting particle algorithm leveraging SDEs for solving MMLE problems in latent variable models with energy-based priors, showing theoretical guarantees and empirical success.", "motivation": "The motivation is to advance methods for learning latent variable models with energy-based priors, which requires solving MMLE problems effectively.", "method": "The paper uses stochastic differential equations (SDEs) as a framework for MMLE, offering a continuous-time approach that is discretized into a practical algorithm with convergence guarantees.", "result": "The proposed algorithm is shown to be theoretically sound and empirically effective on both synthetic and image datasets.", "conclusion": "The study demonstrates that SDE-based methods are a robust approach for learning latent energy-based models, combining theoretical rigor and practical applicability."}}
{"id": "2510.12494", "pdf": "https://arxiv.org/pdf/2510.12494", "abs": "https://arxiv.org/abs/2510.12494", "authors": ["Yi Liu", "Yang Liu", "Leqian Zheng", "Jue Hong", "Junjie Shi", "Qingyou Yang", "Ye Wu", "Cong Wang"], "title": "PubSub-VFL: Towards Efficient Two-Party Split Learning in Heterogeneous Environments via Publisher/Subscriber Architecture", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "Accepted at NeurIPS 2025", "summary": "With the rapid advancement of the digital economy, data collaboration between\norganizations has become a well-established business model, driving the growth\nof various industries. However, privacy concerns make direct data sharing\nimpractical. To address this, Two-Party Split Learning (a.k.a. Vertical\nFederated Learning (VFL)) has emerged as a promising solution for secure\ncollaborative learning. Despite its advantages, this architecture still suffers\nfrom low computational resource utilization and training efficiency.\nSpecifically, its synchronous dependency design increases training latency,\nwhile resource and data heterogeneity among participants further hinder\nefficient computation. To overcome these challenges, we propose PubSub-VFL, a\nnovel VFL paradigm with a Publisher/Subscriber architecture optimized for\ntwo-party collaborative learning with high computational efficiency. PubSub-VFL\nleverages the decoupling capabilities of the Pub/Sub architecture and the data\nparallelism of the parameter server architecture to design a hierarchical\nasynchronous mechanism, reducing training latency and improving system\nefficiency. Additionally, to mitigate the training imbalance caused by resource\nand data heterogeneity, we formalize an optimization problem based on\nparticipants' system profiles, enabling the selection of optimal\nhyperparameters while preserving privacy. We conduct a theoretical analysis to\ndemonstrate that PubSub-VFL achieves stable convergence and is compatible with\nsecurity protocols such as differential privacy. Extensive case studies on five\nbenchmark datasets further validate its effectiveness, showing that, compared\nto state-of-the-art baselines, PubSub-VFL not only accelerates training by $2\n\\sim 7\\times$ without compromising accuracy, but also achieves a computational\nresource utilization rate of up to 91.07%.", "AI": {"tldr": "The paper introduces PubSub-VFL, an optimized approach for Vertical Federated Learning (VFL) using Publisher/Subscriber architecture, enhancing computational efficiency and addressing issues of heterogeneity and latency.", "motivation": "The paper seeks to enhance the efficiency and resource utilization of Two-Party Split Learning (Vertical Federated Learning), which faces challenges in computational resource utilization and training efficiency due to synchronous dependencies and heterogeneity among participants.", "method": "PubSub-VFL employs a Publisher/Subscriber (Pub/Sub) architecture combined with a hierarchical asynchronous mechanism and data parallelism, along with an optimization problem based on system profiles to mitigate training imbalances and maintain privacy.", "result": "PubSub-VFL improves system efficiency and reduces training latency, achieving up to 91.07% computational resource utilization and accelerating training by 2 to 7 times compared to existing solutions while maintaining model accuracy.", "conclusion": "The proposed PubSub-VFL presents a significant improvement in computational efficiency for VFL scenarios, ensures stable convergence, and supports privacy-preserving measures, as validated by theoretical analysis and extensive case studies."}}
{"id": "2510.11967", "pdf": "https://arxiv.org/pdf/2510.11967", "abs": "https://arxiv.org/abs/2510.11967", "authors": ["Weiwei Sun", "Miao Lu", "Zhan Ling", "Kang Liu", "Xuesong Yao", "Yiming Yang", "Jiecao Chen"], "title": "Scaling Long-Horizon LLM Agent via Context-Folding", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language model (LLM) agents are fundamentally constrained by context\nlength on long-horizon tasks. We introduce Context-Folding, a framework that\nempowers agents to actively manage their working context. An agent can\nprocedurally branch into a sub-trajectory to handle a subtask and then fold it\nupon completion, collapsing the intermediate steps while retaining a concise\nsummary of the outcome. To make this behavior learnable, we develop an\nend-to-end reinforcement learning framework FoldGRPO with specific process\nrewards to encourage effective task decomposition and context management. On\ncomplex long-horizon tasks (Deep Research and SWE), our folding agent matches\nor outperforms the ReAct baselines while using an active context 10$\\times$\nsmaller and significantly outperforms models that rely on summarization-based\ncontext management.", "AI": {"tldr": "Introduces Context-Folding, enabling language model agents to manage working context efficiently on long-horizon tasks through task decomposition and context summarization.", "motivation": "To address the limitation of large language models constrained by context length when solving long-horizon tasks.", "method": "Development of Context-Folding, including reinforcement learning framework FoldGRPO with task decomposition and process rewards.", "result": "The folding agent matches or outperforms baselines using significantly smaller active context and surpasses summarization-based context management models on complex tasks.", "conclusion": "Context-Folding effectively enhances long-horizon task performance by optimizing context management without compromising efficiency."}}
{"id": "2510.12363", "pdf": "https://arxiv.org/pdf/2510.12363", "abs": "https://arxiv.org/abs/2510.12363", "authors": ["Jiale Fan", "Andrei Cramariuc", "Tifanny Portela", "Marco Hutter"], "title": "Pretraining in Actor-Critic Reinforcement Learning for Robot Motion Control", "categories": ["cs.RO", "cs.LG"], "comment": "Submitted to ICLR 2026", "summary": "The pretraining-finetuning paradigm has facilitated numerous transformative\nadvancements in artificial intelligence research in recent years. However, in\nthe domain of reinforcement learning (RL) for robot motion control, individual\nskills are often learned from scratch despite the high likelihood that some\ngeneralizable knowledge is shared across all task-specific policies belonging\nto a single robot embodiment. This work aims to define a paradigm for\npretraining neural network models that encapsulate such knowledge and can\nsubsequently serve as a basis for warm-starting the RL process in classic\nactor-critic algorithms, such as Proximal Policy Optimization (PPO). We begin\nwith a task-agnostic exploration-based data collection algorithm to gather\ndiverse, dynamic transition data, which is then used to train a Proprioceptive\nInverse Dynamics Model (PIDM) through supervised learning. The pretrained\nweights are loaded into both the actor and critic networks to warm-start the\npolicy optimization of actual tasks. We systematically validated our proposed\nmethod on seven distinct robot motion control tasks, showing significant\nbenefits to this initialization strategy. Our proposed approach on average\nimproves sample efficiency by 40.1% and task performance by 7.5%, compared to\nrandom initialization. We further present key ablation studies and empirical\nanalyses that shed light on the mechanisms behind the effectiveness of our\nmethod.", "AI": {"tldr": "The paper introduces a paradigm for pretraining neural networks to enhance reinforcement learning in robot motion control, resulting in improved sample efficiency and task performance.", "motivation": "To address the inefficiency of learning robot motion control tasks individually from scratch, despite the potential for shared generalizable knowledge across tasks.", "method": "The method involves collecting diverse transition data using a task-agnostic algorithm, training a Proprioceptive Inverse Dynamics Model (PIDM) through supervised learning, and using its pretrained weights for initializing actor-critic RL algorithms.", "result": "Application of the method on seven robot motion control tasks demonstrated average improvements of 40.1% in sample efficiency and 7.5% in task performance compared to random initialization.", "conclusion": "The proposed pretraining strategy successfully enhances RL processes for robot motion control, leveraging shared dynamical knowledge across tasks to achieve better efficiency and outcomes."}}
{"id": "2510.12066", "pdf": "https://arxiv.org/pdf/2510.12066", "abs": "https://arxiv.org/abs/2510.12066", "authors": ["Alessandro Achille", "Stefano Soatto"], "title": "AI Agents as Universal Task Solvers", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "AI reasoning agents are already able to solve a variety of tasks by deploying\ntools, simulating outcomes of multiple hypotheses and reflecting on them. In\ndoing so, they perform computation, although not in the classical sense --\nthere is no program being executed. Still, if they perform computation, can AI\nagents be universal? Can chain-of-thought reasoning solve any computable task?\nHow does an AI Agent learn to reason? Is it a matter of model size? Or training\ndataset size?\n  In this work, we reinterpret the role of learning in the context of AI\nAgents, viewing them as compute-capable stochastic dynamical systems, and\nhighlight the role of time in a foundational principle for learning to reason.\nIn doing so, we propose a shift from classical inductive learning to\ntransductive learning -- where the objective is not to approximate the\ndistribution of past data, but to capture their algorithmic structure to reduce\nthe time needed to find solutions to new tasks.\n  Transductive learning suggests that, counter to Shannon's theory, a key role\nof information in learning is about reduction of time rather than\nreconstruction error. In particular, we show that the optimal speed-up that a\nuniversal solver can achieve using past data is tightly related to their\nalgorithmic information. Using this, we show a theoretical derivation for the\nobserved power-law scaling of inference time versus training time. We then show\nthat scaling model size can lead to behaviors that, while improving accuracy on\nbenchmarks, fail any reasonable test of intelligence, let alone\nsuper-intelligence: In the limit of infinite space and time, large models can\nbehave as savants, able to brute-force through any task without any insight.\nInstead, we argue that the key quantity to optimize when scaling reasoning\nmodels is time, whose critical role in learning has so far only been indirectly\nconsidered.", "AI": {"tldr": "This paper explores the concept of AI agents as computational systems capable of reasoning and learning, proposing a shift from inductive to transductive learning focused on time reduction for solving tasks.", "motivation": "The paper seeks to address fundamental questions about the universality of AI reasoning agents, how they learn, and the relationship between model size, training data, and reasoning capabilities.", "method": "The authors reinterpret AI agents as stochastic dynamical systems and emphasize transductive learning, focusing on understanding algorithmic structures to minimize time in solving new tasks. They derive theoretical results connecting optimal speed-up and algorithmic information and analyze scaling behaviors in reasoning models.", "result": "The study provides theoretical insights into the relationship between training and inference time, showing power-law scaling and illustrating the limitations of scaling model size, which can lead to brute-force behavior without true intelligence.", "conclusion": "The authors argue that optimizing reasoning models should prioritize time efficiency rather than solely focusing on accuracy or size, challenging traditional concepts of learning and scaling in AI."}}
{"id": "2510.12364", "pdf": "https://arxiv.org/pdf/2510.12364", "abs": "https://arxiv.org/abs/2510.12364", "authors": ["Kevin Krings", "Nino S. Bohn", "Thomas Ludwig"], "title": "(R)evolution of Programming: Vibe Coding as a Post-Coding Paradigm", "categories": ["cs.SE", "cs.AI", "cs.HC", "D.2.3"], "comment": "Workshop Submission at the sixth decennial Aarhus conference in\n  Workshop \"The End of Programming (as we know it) - Envisioning Radical\n  Re-Conceptualizations of Co-Coding with AI\"", "summary": "Recent advancements in generative artificial intelligence (GenAI),\nparticularly large language models, have introduced new possibilities for\nsoftware development practices. In our paper we investigate the emerging Vibe\nCoding (VC) paradigm that emphasizes intuitive, affect-driven, and\nimprovisational interactions between developers and AI systems. Building upon\nthe discourse of End-User Development (EUD), we explore how VC diverges from\nconventional programming approaches such as those supported by tools like\nGitHub Copilot. Through five semi-structured interview sessions with ten\nexperienced software practitioners, we identify five thematic dimensions:\ncreativity, sustainability, the future of programming, collaboration, and\ncriticism. Our analysis conceptualizes VC within the metaphor of co-drifting,\ncontrasting it with the prevalent co-piloting perspective of AI-assisted\ndevelopment. We argue that VC reconfigures the developers role, blurring\nboundaries between professional and non-developers. While VC enables novel\nforms of expression and rapid prototyping, it also introduces challenges\nregarding reproducibility, scalability, and inclusivity. We propose that VC\nrepresents a meaningful shift in programming culture, warranting further\ninvestigation within human-computer interaction (HCI) and software engineering\nresearch.", "AI": {"tldr": "The paper explores a new coding paradigm called Vibe Coding (VC), which emphasizes intuitive and affect-driven collaboration between developers and AI systems, investigating its impact on programming practices and culture.", "motivation": "A need to understand how advancements in generative AI like large language models are reshaping software development practices and enabling novel paradigms such as Vibe Coding.", "method": "The study incorporates five semi-structured interview sessions with ten experienced software practitioners, analyzing thematic dimensions like creativity, collaboration, and sustainability.", "result": "VC is conceptualized within the metaphor of co-drifting rather than co-piloting, highlighting changes in developer roles that blur lines between professionals and non-developers.", "conclusion": "Vibe Coding represents a significant cultural shift in programming, offering innovative opportunities but also posing challenges such as scalability, reproducibility, and inclusivity that require further research."}}
{"id": "2510.12075", "pdf": "https://arxiv.org/pdf/2510.12075", "abs": "https://arxiv.org/abs/2510.12075", "authors": ["Aashish Dhawan", "Divyanshu Mudgal"], "title": "A Review on Domain Adaption and Generative Adversarial Networks(GANs)", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The major challenge in today's computer vision scenario is the availability\nof good quality labeled data. In a field of study like image classification,\nwhere data is of utmost importance, we need to find more reliable methods which\ncan overcome the scarcity of data to produce results comparable to previous\nbenchmark results. In most cases, obtaining labeled data is very difficult\nbecause of the high cost of human labor and in some cases impossible. The\npurpose of this paper is to discuss Domain Adaptation and various methods to\nimplement it. The main idea is to use a model trained on a particular dataset\nto predict on data from a different domain of the same kind, for example - a\nmodel trained on paintings of airplanes predicting on real images of airplanes", "AI": {"tldr": "This paper addresses the scarcity of quality labeled data in computer vision, focusing on domain adaptation for image classification.", "motivation": "To tackle the challenge of obtaining labeled data, which is expensive and sometimes impossible, by exploring methods to adapt models across domains.", "method": "Discussing various domain adaptation techniques to use models trained on one dataset to predict on a different domain.", "result": "Highlights the potential of domain adaptation techniques to overcome data scarcity and achieve comparable results to established benchmarks.", "conclusion": "Domain adaptation can pave the way for effective cross-domain predictions in scenarios with limited labeled data."}}
{"id": "2510.11856", "pdf": "https://arxiv.org/pdf/2510.11856", "abs": "https://arxiv.org/abs/2510.11856", "authors": ["Aurelie Leribaux", "Rafael Oyamada", "Johannes De Smedt", "Zahra Dasht Bozorgi", "Artem Polyvyanyy", "Jochen De Weerdt"], "title": "Actor-Enriched Time Series Forecasting of Process Performance", "categories": ["cs.LG"], "comment": "Accepted at ICPM 2025", "summary": "Predictive Process Monitoring (PPM) is a key task in Process Mining that aims\nto predict future behavior, outcomes, or performance indicators. Accurate\nprediction of the latter is critical for proactive decision-making. Given that\nprocesses are often resource-driven, understanding and incorporating actor\nbehavior in forecasting is crucial. Although existing research has incorporated\naspects of actor behavior, its role as a time-varying signal in PPM remains\nlimited. This study investigates whether incorporating actor behavior\ninformation, modeled as time series, can improve the predictive performance of\nthroughput time (TT) forecasting models. Using real-life event logs, we\nconstruct multivariate time series that include TT alongside actor-centric\nfeatures, i.e., actor involvement, the frequency of continuation, interruption,\nand handover behaviors, and the duration of these behaviors. We train and\ncompare several models to study the benefits of adding actor behavior. The\nresults show that actor-enriched models consistently outperform baseline\nmodels, which only include TT features, in terms of RMSE, MAE, and R2. These\nfindings demonstrate that modeling actor behavior over time and incorporating\nthis information into forecasting models enhances performance indicator\npredictions.", "AI": {"tldr": "This study explores incorporating actor behavior as time-varying signals into predictive models for throughput time (TT) in processes, showing significant improvements in accuracy.", "motivation": "To enhance proactive decision-making in Predictive Process Monitoring (PPM) by better understanding and incorporating actor behavior as time-varying signals, given its limited exploration in previous research.", "method": "The study uses real-life event logs to construct multivariate time series, including both TT and actor-centric features. Several models are trained and compared to assess the impact of adding actor behavior data.", "result": "Actor-enriched models consistently outperform baseline TT-only feature models in predictive accuracy, as measured by RMSE, MAE, and R2 metrics.", "conclusion": "Integrating time-dependent actor behavior data significantly improves the accuracy of throughput time predictions in Process Mining."}}
{"id": "2510.12375", "pdf": "https://arxiv.org/pdf/2510.12375", "abs": "https://arxiv.org/abs/2510.12375", "authors": ["Bogdan Butyrin", "Eric Moulines", "Alexey Naumov", "Sergey Samsonov", "Qi-Man Shao", "Zhuo-Song Zhang"], "title": "Improved Central Limit Theorem and Bootstrap Approximations for Linear Stochastic Approximation", "categories": ["stat.ML", "cs.LG", "math.OC", "math.PR", "math.ST", "stat.TH", "60F05, 62L20, 62E20"], "comment": null, "summary": "In this paper, we refine the Berry-Esseen bounds for the multivariate normal\napproximation of Polyak-Ruppert averaged iterates arising from the linear\nstochastic approximation (LSA) algorithm with decreasing step size. We consider\nthe normal approximation by the Gaussian distribution with covariance matrix\npredicted by the Polyak-Juditsky central limit theorem and establish the rate\nup to order $n^{-1/3}$ in convex distance, where $n$ is the number of samples\nused in the algorithm. We also prove a non-asymptotic validity of the\nmultiplier bootstrap procedure for approximating the distribution of the\nrescaled error of the averaged LSA estimator. We establish approximation rates\nof order up to $1/\\sqrt{n}$ for the latter distribution, which significantly\nimproves upon the previous results obtained by Samsonov et al. (2024).", "AI": {"tldr": "This paper enhances Berry-Esseen bounds for multivariate normal approximations in Linear Stochastic Approximation (LSA) with averaged iterates, and validates a bootstrap method with improved approximation rates.", "motivation": "The authors aim to improve theoretical bounds and validation methods for multivariate normal approximations and the associated bootstrap procedure in LSA settings.", "method": "The study refines Berry-Esseen bounds for convex distances and proves non-asymptotic multiplier bootstrap validity for the averaged LSA estimator.", "result": "Enhanced bounds with rate up to $n^{-1/3}$ for normal approximations and $1/\\sqrt{n}$ for error distribution approximations are established, marking significant improvements over prior work.", "conclusion": "The results underscore the improved precision and reliability of both normal approximation and bootstrap methodologies in LSA, benefiting scenarios with finite samples."}}
{"id": "2510.12633", "pdf": "https://arxiv.org/pdf/2510.12633", "abs": "https://arxiv.org/abs/2510.12633", "authors": ["Guangming Sheng", "Yuxuan Tong", "Borui Wan", "Wang Zhang", "Chaobo Jia", "Xibin Wu", "Yuqi Wu", "Xiang Li", "Chi Zhang", "Yanghua Peng", "Haibin Lin", "Xin Liu", "Chuan Wu"], "title": "Laminar: A Scalable Asynchronous RL Post-Training Framework", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Reinforcement learning (RL) post-training for Large Language Models (LLMs) is\nnow scaling to large clusters and running for extended durations to enhance\nmodel reasoning performance. However, the scalability of existing RL frameworks\nis limited, as extreme long-tail skewness in RL trajectory generation causes\nsevere GPU underutilization. Current asynchronous RL systems attempt to\nmitigate this, but they rely on global weight synchronization between the actor\nand all rollouts, which creates a rigid model update schedule. This global\nsynchronization is ill-suited for the highly skewed and evolving distribution\nof trajectory generation latency in RL training, crippling training efficiency.\nOur key insight is that efficient scaling requires breaking this lockstep\nthrough trajectory-level asynchrony, which generates and consumes each\ntrajectory independently. We propose Laminar, a scalable and robust RL\npost-training system built on a fully decoupled architecture. First, we replace\nglobal updates with a tier of relay workers acting as a distributed parameter\nservice. This enables asynchronous and fine-grained weight synchronization,\nallowing rollouts to pull the latest weight anytime without stalling the\nactor's training loop. Second, a dynamic repack mechanism consolidates\nlong-tail trajectories onto a few dedicated rollouts, maximizing generation\nthroughput. The fully decoupled design also isolates failures, ensuring\nrobustness for long-running jobs. Our evaluation on a 1024-GPU cluster shows\nthat Laminar achieves up to 5.48$\\times$ training throughput speedup over\nstate-of-the-art systems, while reducing model convergence time.", "AI": {"tldr": "The paper introduces 'Laminar,' an optimized RL post-training system for Large Language Models that improves GPU utilization and training efficiency through a fully decoupled architecture.", "motivation": "The paper aims to address the scalability limitations in RL frameworks caused by skewed trajectory generation latency and inefficient global weight synchronization, which leads to GPU underutilization and suboptimal training efficiency.", "method": "It utilizes trajectory-level asynchrony by implementing a decoupled system architecture. Key innovations include relay workers for asynchronous weight synchronization and dynamic repack mechanisms to enhance trajectory generation throughput and training robustness.", "result": "Laminar demonstrates up to 5.48\u00d7 speedup in training throughput over current RL systems, significantly reducing convergence time in a 1024-GPU cluster.", "conclusion": "Laminar provides a scalable and robust solution for RL post-training in LLMs, improving GPU efficiency and accelerating convergence while ensuring fault isolation for long-duration tasks."}}
{"id": "2510.11986", "pdf": "https://arxiv.org/pdf/2510.11986", "abs": "https://arxiv.org/abs/2510.11986", "authors": ["Jasivan Alex Sivakumar", "Philipp Borchert", "Ronald Cardenas", "Gerasimos Lampouras"], "title": "Conjecturing: An Overlooked Step in Formal Mathematical Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Autoformalisation, the task of expressing informal mathematical statements in\nformal language, is often viewed as a direct translation process. This,\nhowever, disregards a critical preceding step: conjecturing. Many mathematical\nproblems cannot be formalised directly without first conjecturing a conclusion\nsuch as an explicit answer, or a specific bound. Since Large Language Models\n(LLMs) already struggle with autoformalisation, and the evaluation of their\nconjecturing ability is limited and often entangled within autoformalisation or\nproof, it is particularly challenging to understand its effect. To address this\ngap, we augment existing datasets to create ConjectureBench, and redesign the\nevaluation framework and metric specifically to measure the conjecturing\ncapabilities of LLMs both as a distinct task and within the autoformalisation\npipeline. Our evaluation of foundational models, including GPT-4.1 and\nDeepSeek-V3.1, reveals that their autoformalisation performance is\nsubstantially overestimated when the conjecture is accounted for during\nevaluation. However, the conjecture should not be assumed to be provided. We\ndesign an inference-time method, Lean-FIRe to improve conjecturing and\nautoformalisation, which, to the best of our knowledge, achieves the first\nsuccessful end-to-end autoformalisation of 13 PutnamBench problems with GPT-4.1\nand 7 with DeepSeek-V3.1. We demonstrate that while LLMs possess the requisite\nknowledge to generate accurate conjectures, improving autoformalisation\nperformance requires treating conjecturing as an independent task, and\ninvestigating further how to correctly integrate it within autoformalisation.\nFinally, we provide forward-looking guidance to steer future research toward\nimproving conjecturing, an overlooked step of formal mathematical reasoning.", "AI": {"tldr": "This paper introduces ConjectureBench, a benchmark for evaluating LLM conjecturing abilities, and proposes an inference-time method (Lean-FIRe) to improve conjecturing and autoformalisation, achieving notable results on mathematical problems.", "motivation": "Autoformalisation of mathematical statements in formal language is hindered by the conjecturing step, which is often overlooked but crucial for certain problems. Existing LLMs show limited success and understanding of conjecturing.", "method": "The authors created ConjectureBench to evaluate conjecturing independently, assessed foundational LLMs, and developed Lean-FIRe to enhance conjecturing and autoformalisation capabilities.", "result": "The paper finds that LLMs' autoformalisation abilities are commonly overestimated due to overlooked conjecturing, and demonstrates improvements using Lean-FIRe, achieving end-to-end autoformalisation for 13 problems with GPT-4.1 and 7 with DeepSeek-V3.1.", "conclusion": "Conjecturing must be recognized as a distinct task to improve autoformalisation. The creation of ConjectureBench highlights this gap, and future research should focus on integrating improved conjecturing methods within the autoformalisation pipeline."}}
{"id": "2510.12370", "pdf": "https://arxiv.org/pdf/2510.12370", "abs": "https://arxiv.org/abs/2510.12370", "authors": ["Wenli Shi", "Clemence Grislain", "Olivier Sigaud", "Mohamed Chetouani"], "title": "Controlling Intent Expressiveness in Robot Motion with Diffusion Models", "categories": ["cs.RO"], "comment": "Using diffusion models trained on quality diversity datasets for\n  generating robot motions with adjustable legibility levels", "summary": "Legibility of robot motion is critical in human-robot interaction, as it\nallows humans to quickly infer a robot's intended goal. Although traditional\ntrajectory generation methods typically prioritize efficiency, they often fail\nto make the robot's intentions clear to humans. Meanwhile, existing approaches\nto legible motion usually produce only a single \"most legible\" trajectory,\noverlooking the need to modulate intent expressiveness in different contexts.\nIn this work, we propose a novel motion generation framework that enables\ncontrollable legibility across the full spectrum, from highly legible to highly\nambiguous motions. We introduce a modeling approach based on an Information\nPotential Field to assign continuous legibility scores to trajectories, and\nbuild upon it with a two-stage diffusion framework that first generates paths\nat specified legibility levels and then translates them into executable robot\nactions. Experiments in both 2D and 3D reaching tasks demonstrate that our\napproach produces diverse and controllable motions with varying degrees of\nlegibility, while achieving performance comparable to SOTA. Code and project\npage: https://legibility-modulator.github.io.", "AI": {"tldr": "The paper proposes a motion generation framework for robots that allows for controllable legibility, enabling humans to infer robot intentions clearly across varying degrees of legibility.", "motivation": "Human-robot interaction often suffers due to robot motions prioritizing efficiency over clarity of intent. Existing systems fail to provide adaptability in expressing intentions depending on context.", "method": "The authors use an Information Potential Field for assigning legibility scores and a two-stage diffusion framework to generate and implement robot motions that align with specified legibility levels.", "result": "Experiments in both 2D and 3D tasks show the framework produces diverse, controllable, and legible motions, with a comparable performance to current state-of-the-art systems.", "conclusion": "The framework enables flexibility in robot motion legibility, making it adaptable to various contexts while preserving efficiency and state-of-the-art performance."}}
{"id": "2510.12067", "pdf": "https://arxiv.org/pdf/2510.12067", "abs": "https://arxiv.org/abs/2510.12067", "authors": ["Junyi Xie", "Yuankun Jiao", "Jina Kim", "Yao-Yi Chiang", "Lingyi Zhao", "Khurram Shafique"], "title": "HiCoTraj:Zero-Shot Demographic Reasoning via Hierarchical Chain-of-Thought Prompting from Trajectory", "categories": ["cs.AI"], "comment": "accepted by The 1st ACM SIGSPATIAL International Workshop on\n  Generative and Agentic AI for Multi-Modality Space-Time Intelligence", "summary": "Inferring demographic attributes such as age, sex, or income level from human\nmobility patterns enables critical applications such as targeted public health\ninterventions, equitable urban planning, and personalized transportation\nservices. Existing mobility-based demographic inference studies heavily rely on\nlarge-scale trajectory data with demographic labels, leading to limited\ninterpretability and poor generalizability across different datasets and user\ngroups. We propose HiCoTraj (Zero-Shot Demographic Reasoning via Hierarchical\nChain-of-Thought Prompting from Trajectory), a framework that leverages LLMs'\nzero-shot learning and semantic understanding capabilities to perform\ndemographic inference without labeled training data. HiCoTraj transforms\ntrajectories into semantically rich, natural language representations by\ncreating detailed activity chronicles and multi-scale visiting summaries. Then\nHiCoTraj uses a novel hierarchical chain of thought reasoning to systematically\nguide LLMs through three cognitive stages: factual feature extraction,\nbehavioral pattern analysis, and demographic inference with structured output.\nThis approach addresses the scarcity challenge of labeled demographic data\nwhile providing transparent reasoning chains. Experimental evaluation on\nreal-world trajectory data demonstrates that HiCoTraj achieves competitive\nperformance across multiple demographic attributes in zero-shot scenarios.", "AI": {"tldr": "The paper introduces HiCoTraj, a framework using LLMs for zero-shot demographic inference from mobility data, leveraging natural language representations and hierarchical reasoning to improve interpretability and generalizability.", "motivation": "Demographic inference from mobility patterns is essential for applications like public health and urban planning, but current methods lack interpretability and generalizability due to reliance on labeled data.", "method": "HiCoTraj employs large language models (LLMs) for zero-shot learning by transforming trajectory data into natural language representations and using hierarchical chain-of-thought reasoning for demographic inference.", "result": "HiCoTraj demonstrates competitive zero-shot inferencing performance on real-world trajectory data for various demographic attributes.", "conclusion": "The approach addresses challenges in labeled data scarcity and improves transparency and reasoning in demographic inference, offering a generalizable and interpretable solution."}}
{"id": "2510.12089", "pdf": "https://arxiv.org/pdf/2510.12089", "abs": "https://arxiv.org/abs/2510.12089", "authors": ["Xingpei Ma", "Shenneng Huang", "Jiaran Cai", "Yuansheng Guan", "Shen Zheng", "Hanfeng Zhao", "Qiang Zhang", "Shunsi Zhang"], "title": "Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in diffusion models have significantly improved audio-driven\nhuman video generation, surpassing traditional methods in both quality and\ncontrollability. However, existing approaches still face challenges in lip-sync\naccuracy, temporal coherence for long video generation, and multi-character\nanimation. In this work, we propose a diffusion transformer (DiT)-based\nframework for generating lifelike talking videos of arbitrary length, and\nintroduce a training-free method for multi-character audio-driven animation.\nFirst, we employ a LoRA-based training strategy combined with a position shift\ninference approach, which enables efficient long video generation while\npreserving the capabilities of the foundation model. Moreover, we combine\npartial parameter updates with reward feedback to enhance both lip\nsynchronization and natural body motion. Finally, we propose a training-free\napproach, Mask Classifier-Free Guidance (Mask-CFG), for multi-character\nanimation, which requires no specialized datasets or model modifications and\nsupports audio-driven animation for three or more characters. Experimental\nresults demonstrate that our method outperforms existing state-of-the-art\napproaches, achieving high-quality, temporally coherent, and multi-character\naudio-driven video generation in a simple, efficient, and cost-effective\nmanner.", "AI": {"tldr": "A novel diffusion transformer framework is proposed to improve audio-driven human video generation with enhanced lip-sync, temporal coherence, and support for multi-character animation using innovative training strategies and training-free methods.", "motivation": "To address challenges in lip-sync accuracy, temporal coherence in long video generation, and multi-character animation in audio-driven human video generation.", "method": "Introduces a DiT-based framework utilizing LoRA-based training for long videos, reward feedback to improve lip-sync and body motion, and a training-free Mask-CFG approach for multi-character animation without requiring specialized datasets.", "result": "Improved performance over state-of-the-art methods in quality, temporal coherence, and efficient multi-character audio-driven video generation.", "conclusion": "The proposed framework enables simple, efficient, and cost-effective lifelike talking videos and multi-character animation, advancing the field of audio-driven human video generation."}}
{"id": "2510.11868", "pdf": "https://arxiv.org/pdf/2510.11868", "abs": "https://arxiv.org/abs/2510.11868", "authors": ["Rita T. Sousa", "Heiko Paulheim"], "title": "Improving Knowledge Graph Embeddings through Contrastive Learning with Negative Statements", "categories": ["cs.LG"], "comment": "Accepted at the Thirteenth International Conference on Knowledge\n  Capture (K-CAP 2025)", "summary": "Knowledge graphs represent information as structured triples and serve as the\nbackbone for a wide range of applications, including question answering, link\nprediction, and recommendation systems. A prominent line of research for\nexploring knowledge graphs involves graph embedding methods, where entities and\nrelations are represented in low-dimensional vector spaces that capture\nunderlying semantics and structure. However, most existing methods rely on\nassumptions such as the Closed World Assumption or Local Closed World\nAssumption, treating missing triples as false. This contrasts with the Open\nWorld Assumption underlying many real-world knowledge graphs. Furthermore,\nwhile explicitly stated negative statements can help distinguish between false\nand unknown triples, they are rarely included in knowledge graphs and are often\noverlooked during embedding training.\n  In this work, we introduce a novel approach that integrates explicitly\ndeclared negative statements into the knowledge embedding learning process. Our\napproach employs a dual-model architecture, where two embedding models are\ntrained in parallel, one on positive statements and the other on negative\nstatements. During training, each model generates negative samples by\ncorrupting positive samples and selecting the most likely candidates as scored\nby the other model. The proposed approach is evaluated on both general-purpose\nand domain-specific knowledge graphs, with a focus on link prediction and\ntriple classification tasks. The extensive experiments demonstrate that our\napproach improves predictive performance over state-of-the-art embedding\nmodels, demonstrating the value of integrating meaningful negative knowledge\ninto embedding learning.", "AI": {"tldr": "The paper introduces a dual-model architecture to improve knowledge graph embeddings by integrating explicit negative statements, enhancing tasks like link prediction and triple classification.", "motivation": "Existing methods for knowledge graph embeddings often make inaccurate assumptions, such as treating missing triples as false, limiting their utility for real-world open-world scenarios.", "method": "A dual-model architecture is proposed where one model trains on positive statements and another on negative statements. Negative samples are generated by corrupting positive ones, with each model helping score the other's samples.", "result": "The approach, tested on both general-purpose and domain-specific knowledge graphs, shows superior predictive performance compared to state-of-the-art models.", "conclusion": "Incorporating explicit negative knowledge into the embedding learning process enhances the utility and accuracy of knowledge graph embeddings for diverse tasks."}}
{"id": "2510.12416", "pdf": "https://arxiv.org/pdf/2510.12416", "abs": "https://arxiv.org/abs/2510.12416", "authors": ["Alvaro Ortiz", "Tomasa Rodrigo"], "title": "Geopolitics, Geoeconomics and Risk:A Machine Learning Approach", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We introduce a novel high-frequency daily panel dataset of both markets and\nnews-based indicators -- including Geopolitical Risk, Economic Policy\nUncertainty, Trade Policy Uncertainty, and Political Sentiment -- for 42\ncountries across both emerging and developed markets. Using this dataset, we\nstudy how sentiment dynamics shape sovereign risk, measured by Credit Default\nSwap (CDS) spreads, and evaluate their forecasting value relative to\ntraditional drivers such as global monetary policy and market volatility. Our\nhorse-race analysis of forecasting models demonstrates that incorporating\nnews-based indicators significantly enhances predictive accuracy and enriches\nthe analysis, with non-linear machine learning methods -- particularly Random\nForests -- delivering the largest gains. Our analysis reveals that while global\nfinancial variables remain the dominant drivers of sovereign risk, geopolitical\nrisk and economic policy uncertainty also play a meaningful role. Crucially,\ntheir effects are amplified through non-linear interactions with global\nfinancial conditions. Finally, we document pronounced regional heterogeneity,\nas certain asset classes and emerging markets exhibit heightened sensitivity to\nshocks in policy rates, global financial volatility, and geopolitical risk.", "AI": {"tldr": "This paper introduces a novel daily dataset covering 42 countries to study the influence of sentiment dynamics on sovereign risk, showing improved forecasts using news-based indicators and machine learning.", "motivation": "To better understand how news-based sentiment dynamics impact sovereign risk, and to assess the forecasting potential of these indicators compared to traditional economic drivers.", "method": "The authors employ a novel high-frequency panel dataset and use statistical methods, including machine learning models like Random Forests, to analyze the relationships and forecasting accuracy.", "result": "News-based indicators significantly enhance the predictive accuracy of forecasting models for sovereign risk, particularly when combined with non-linear machine learning methods.", "conclusion": "Global financial variables are the dominant drivers of sovereign risk, but geopolitical risk and economic policy uncertainty also significantly influence sovereign risk, especially in emerging markets and under certain financial conditions."}}
{"id": "2510.12727", "pdf": "https://arxiv.org/pdf/2510.12727", "abs": "https://arxiv.org/abs/2510.12727", "authors": ["Anas Abouaomar", "Mohammed El hanjri", "Abdellatif Kobbane", "Anis Laouiti", "Khalid Nafil"], "title": "Hierarchical Federated Learning for Crop Yield Prediction in Smart Agricultural Production Systems", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "6 pages, 3 figures, conference", "summary": "In this paper, we presents a novel hierarchical federated learning\narchitecture specifically designed for smart agricultural production systems\nand crop yield prediction. Our approach introduces a seasonal subscription\nmechanism where farms join crop-specific clusters at the beginning of each\nagricultural season. The proposed three-layer architecture consists of\nindividual smart farms at the client level, crop-specific aggregators at the\nmiddle layer, and a global model aggregator at the top level. Within each crop\ncluster, clients collaboratively train specialized models tailored to specific\ncrop types, which are then aggregated to produce a higher-level global model\nthat integrates knowledge across multiple crops. This hierarchical design\nenables both local specialization for individual crop types and global\ngeneralization across diverse agricultural contexts while preserving data\nprivacy and reducing communication overhead. Experiments demonstrate the\neffectiveness of the proposed system, showing that local and crop-layer models\nclosely follow actual yield patterns with consistent alignment, significantly\noutperforming standard machine learning models. The results validate the\nadvantages of hierarchical federated learning in the agricultural context,\nparticularly for scenarios involving heterogeneous farming environments and\nprivacy-sensitive agricultural data.", "AI": {"tldr": "This paper introduces a hierarchical federated learning architecture for smart farming and crop yield prediction, emphasizing privacy and efficient communication.", "motivation": "To address challenges in smart agriculture such as data privacy and communication inefficiencies while offering tailored predictions for diverse crops.", "method": "A three-layer federated learning model where farms join crop-specific clusters, train specialized models locally, and aggregate them hierarchically for global generalization.", "result": "The proposed method significantly outperforms traditional machine learning models, accurately aligning with actual yield patterns across heterogeneous farming environments.", "conclusion": "Hierarchical federated learning improves crop yield prediction while respecting privacy constraints and adapting efficiently to diverse agricultural scenarios."}}
{"id": "2510.11997", "pdf": "https://arxiv.org/pdf/2510.11997", "abs": "https://arxiv.org/abs/2510.11997", "authors": ["Ryan Shea", "Yunan Lu", "Liang Qiu", "Zhou Yu"], "title": "SAGE: A Top-Down Bottom-Up Knowledge-Grounded User Simulator for Multi-turn AGent Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Evaluating multi-turn interactive agents is challenging due to the need for\nhuman assessment. Evaluation with simulated users has been introduced as an\nalternative, however existing approaches typically model generic users and\noverlook the domain-specific principles required to capture realistic behavior.\nWe propose SAGE, a novel user Simulation framework for multi-turn AGent\nEvaluation that integrates knowledge from business contexts. SAGE incorporates\ntop-down knowledge rooted in business logic, such as ideal customer profiles,\ngrounding user behavior in realistic customer personas. We further integrate\nbottom-up knowledge taken from business agent infrastructure (e.g., product\ncatalogs, FAQs, and knowledge bases), allowing the simulator to generate\ninteractions that reflect users' information needs and expectations in a\ncompany's target market. Through empirical evaluation, we find that this\napproach produces interactions that are more realistic and diverse, while also\nidentifying up to 33% more agent errors, highlighting its effectiveness as an\nevaluation tool to support bug-finding and iterative agent improvement.", "AI": {"tldr": "SAGE is a new framework for evaluating interactive agents by simulating realistic user behavior based on business-specific knowledge rather than generic user modeling.", "motivation": "Evaluating multi-turn interactive agents typically requires human assessment, which is resource-intensive. Simulations with generic user models lack domain-specific realism.", "method": "SAGE integrates top-down business logic (e.g., customer profiles) and bottom-up infrastructure knowledge (e.g., product catalogs, FAQs) to simulate realistic user interactions tailored to target markets.", "result": "Empirical tests show SAGE generates diverse and realistic interactions, identifying 33% more agent errors than existing methods.", "conclusion": "SAGE improves the evaluation process for interactive agents, helping refine their performance by offering precise and domain-specific simulations."}}
{"id": "2510.12392", "pdf": "https://arxiv.org/pdf/2510.12392", "abs": "https://arxiv.org/abs/2510.12392", "authors": ["Junhyuk So", "Chiwoong Lee", "Shinyoung Lee", "Jungseul Ok", "Eunhyeok Park"], "title": "Improving Generative Behavior Cloning via Self-Guidance and Adaptive Chunking", "categories": ["cs.RO", "cs.LG"], "comment": "Accepted at NeurIPS25", "summary": "Generative Behavior Cloning (GBC) is a simple yet effective framework for\nrobot learning, particularly in multi-task settings. Recent GBC methods often\nemploy diffusion policies with open-loop (OL) control, where actions are\ngenerated via a diffusion process and executed in multi-step chunks without\nreplanning. While this approach has demonstrated strong success rates and\ngeneralization, its inherent stochasticity can result in erroneous action\nsampling, occasionally leading to unexpected task failures. Moreover, OL\ncontrol suffers from delayed responses, which can degrade performance in noisy\nor dynamic environments. To address these limitations, we propose two novel\ntechniques to enhance the consistency and reactivity of diffusion policies: (1)\nself-guidance, which improves action fidelity by leveraging past observations\nand implicitly promoting future-aware behavior; and (2) adaptive chunking,\nwhich selectively updates action sequences when the benefits of reactivity\noutweigh the need for temporal consistency. Extensive experiments show that our\napproach substantially improves GBC performance across a wide range of\nsimulated and real-world robotic manipulation tasks. Our code is available at\nhttps://github.com/junhyukso/SGAC", "AI": {"tldr": "The paper introduces Generative Behavior Cloning (GBC) enhancements for robot learning using self-guidance and adaptive chunking to address the limitations of stochasticity and delayed responses in diffusion policies.", "motivation": "To overcome issues of stochastic sampling errors and delays in open-loop (OL) diffusion policies for improving robotic performance in multi-task settings.", "method": "Introducing two techniques: self-guidance (leveraging past observations for action fidelity) and adaptive chunking (updating action sequences based on reactive needs).", "result": "Proposed techniques showed substantial improvement in GBC performance across a variety of simulated and real-world robotic manipulation tasks.", "conclusion": "The approach enhances the consistency and reactivity of diffusion policies, significantly improving robot learning outcomes. The associated codebase is made publicly available."}}
{"id": "2510.12072", "pdf": "https://arxiv.org/pdf/2510.12072", "abs": "https://arxiv.org/abs/2510.12072", "authors": ["Zixing Lei", "Sheng Yin", "Yichen Xiong", "Yuanzhuo Ding", "Wenhao Huang", "Yuxi Wei", "Qingyao Xu", "Yiming Li", "Weixin Li", "Yunhong Wang", "Siheng Chen"], "title": "EmboMatrix: A Scalable Training-Ground for Embodied Decision-Making", "categories": ["cs.AI", "cs.RO"], "comment": "10 pages 8 figures", "summary": "Embodied decision-making enables agents to translate high-level goals into\nexecutable actions through continuous interactions within the physical world,\nforming a cornerstone of general-purpose embodied intelligence. Large language\nmodels (LLMs), with their general decision-making capabilities, offer a\npromising path to realize this potential; however, LLMs trained solely on\nlanguage lack exposure to physical environments, limiting their true embodied\nunderstanding. To bridge this gap, we propose the concept of a training ground:\na comprehensive infrastructure that provides task and scene simulation,\nembodied interaction, and feedback signals, offering a one-stop solution for\nLLM acquire genuine embodied decision-making skills. In this work, we present\nEmboMatrix, the first training ground of its kind, providing massive and\ndiverse tasks with efficient simulation and precise rewards. EmboMatrix\nincorporates a series of novel techniques: a multi-agent data engine for\nlarge-scale task and scene generation, a distributed heterogeneous-hardware\nsystem for scalable simulation, and a multi-level reward architecture for\nprecise supervision. Leveraging EmboMatrix, we cultivate EmboBrain, an LLM\nwhose embodied decision-making abilities emerge from extensive embodied\ninteractions. Experiments show that EmboBrain-7B surpasses the 671B DeepSeek-R1\nbaseline by 9.5\\% on two challenging embodied decision-making benchmarks,\ndemonstrating the power of interactive, environment-grounded learning for\nbuilding truly intelligent embodied agents.", "AI": {"tldr": "The paper introduces EmboMatrix, an infrastructure for training large language models (LLMs) in embodied decision-making, overcoming limitations from traditional language-only training.", "motivation": "LLMs lack true embodied understanding due to limited exposure to physical interactions in real-world environments.", "method": "The paper proposes EmboMatrix, integrating task and scene simulation, embodied interaction, feedback signals, and precise rewards using advanced techniques such as a multi-agent data engine.", "result": "EmboBrain-7B, trained using EmboMatrix, exhibited a 9.5% improvement over a baseline model on challenging embodied decision-making benchmarks.", "conclusion": "Environment-grounded learning enhances the intelligent capabilities of LLMs in decision-making tasks tied to physical interactions."}}
{"id": "2510.12478", "pdf": "https://arxiv.org/pdf/2510.12478", "abs": "https://arxiv.org/abs/2510.12478", "authors": ["\u00d8ystein Haugen", "Stefan Klikovits", "Martin Arthur Andersen", "Jonathan Beaulieu", "Francis Bordeleau", "Joachim Denil", "Joost Mertens"], "title": "DarTwin made precise by SysMLv2 -- An Experiment", "categories": ["cs.SE", "cs.SY", "eess.SY"], "comment": null, "summary": "The new SysMLv2 adds mechanisms for the built-in specification of\ndomain-specific concepts and language extensions. This feature promises to\nfacilitate the creation of Domain-Specific Languages (DSLs) and interfacing\nwith existing system descriptions and technical designs. In this paper, we\nreview these features and evaluate SysMLv2's capabilities using concrete use\ncases. We develop DarTwin DSL, a DSL that formalizes the existing DarTwin\nnotation for Digital Twin (DT) evolution, through SysMLv2, thereby supposedly\nenabling the wide application of DarTwin's evolution templates using any\nSysMLv2 tool. We demonstrate DarTwin DSL, but also point out limitations in the\ncurrently available tooling of SysMLv2 in terms of graphical notation\ncapabilities. This work contributes to the growing field of Model-Driven\nEngineering (MDE) for DTs and combines it with the release of SysMLv2, thus\nintegrating a systematic approach with DT evolution management in systems\nengineering.", "AI": {"tldr": "SysMLv2 introduces new capabilities for domain-specific language creation and integration with systems engineering, exemplified by the DarTwin DSL for Digital Twin evolution, while noting some tooling limitations.", "motivation": "The paper aims to explore the new functionalities in SysMLv2 facilitating domain-specific language creation, with the goal of enhancing system descriptions and supporting Digital Twin evolution.", "method": "The authors introduced and evaluated the DarTwin DSL, a formalization of the DarTwin notation for Digital Twin evolution, by leveraging SysMLv2's built-in specification tools and use cases.", "result": "DarTwin DSL was successfully demonstrated to enable Digital Twin evolution management, although limitations in graphical notation capabilities of SysMLv2 tools were identified.", "conclusion": "SysMLv2 offers promising features for integrating domain-specific languages into systems engineering, but further improvements in tooling are necessary for full graphical support."}}
{"id": "2510.12095", "pdf": "https://arxiv.org/pdf/2510.12095", "abs": "https://arxiv.org/abs/2510.12095", "authors": ["Wenxu Zhou", "Kaixuan Nie", "Hang Du", "Dong Yin", "Wei Huang", "Siqiang Guo", "Xiaobo Zhang", "Pengbo Hu"], "title": "IL3D: A Large-Scale Indoor Layout Dataset for LLM-Driven 3D Scene Generation", "categories": ["cs.CV"], "comment": "9 pages main paper; 15 pages references and appendix", "summary": "In this study, we present IL3D, a large-scale dataset meticulously designed\nfor large language model (LLM)-driven 3D scene generation, addressing the\npressing demand for diverse, high-quality training data in indoor layout\ndesign. Comprising 27,816 indoor layouts across 18 prevalent room types and a\nlibrary of 29,215 high-fidelity 3D object assets, IL3D is enriched with\ninstance-level natural language annotations to support robust multimodal\nlearning for vision-language tasks. We establish rigorous benchmarks to\nevaluate LLM-driven scene generation. Experimental results show that supervised\nfine-tuning (SFT) of LLMs on IL3D significantly improves generalization and\nsurpasses the performance of SFT on other datasets. IL3D offers flexible\nmultimodal data export capabilities, including point clouds, 3D bounding boxes,\nmultiview images, depth maps, normal maps, and semantic masks, enabling\nseamless adaptation to various visual tasks. As a versatile and robust\nresource, IL3D significantly advances research in 3D scene generation and\nembodied intelligence, by providing high-fidelity scene data to support\nenvironment perception tasks of embodied agents.", "AI": {"tldr": "IL3D is a large-scale dataset designed to enhance vision-language multimodal learning, specifically for 3D scene generation, providing 27,816 indoor layouts and 29,215 3D object assets with extensive annotations.", "motivation": "Address the need for high-quality, diverse training data to improve 3D scene generation and vision-language tasks using large language models.", "method": "Created IL3D, a dataset with annotated indoor layouts and a variety of multimodal data exports, and established benchmarks to evaluate 3D scene generation performance.", "result": "Supervised fine-tuning of LLMs on IL3D data significantly improves generalization and performs better compared to other datasets.", "conclusion": "IL3D is a versatile dataset that supports advancements in 3D scene generation and embodied intelligence research by offering extensive, high-fidelity multimodal data."}}
{"id": "2510.11877", "pdf": "https://arxiv.org/pdf/2510.11877", "abs": "https://arxiv.org/abs/2510.11877", "authors": ["Xiaohang Tang", "Zhuowen Cheng", "Satyabrat Kumar"], "title": "Robust Adversarial Reinforcement Learning in Stochastic Games via Sequence Modeling", "categories": ["cs.LG", "cs.GT"], "comment": "Accepted by Reliable ML Workshop @ NeurIPS 2025", "summary": "The Transformer, a highly expressive architecture for sequence modeling, has\nrecently been adapted to solve sequential decision-making, most notably through\nthe Decision Transformer (DT), which learns policies by conditioning on desired\nreturns. Yet, the adversarial robustness of reinforcement learning methods\nbased on sequence modeling remains largely unexplored. Here we introduce the\nConservative Adversarially Robust Decision Transformer (CART), to our knowledge\nthe first framework designed to enhance the robustness of DT in adversarial\nstochastic games. We formulate the interaction between the protagonist and the\nadversary at each stage as a stage game, where the payoff is defined as the\nexpected maximum value over subsequent states, thereby explicitly incorporating\nstochastic state transitions. By conditioning Transformer policies on the NashQ\nvalue derived from these stage games, CART generates policy that are\nsimultaneously less exploitable (adversarially robust) and conservative to\ntransition uncertainty. Empirically, CART achieves more accurate minimax value\nestimation and consistently attains superior worst-case returns across a range\nof adversarial stochastic games.", "AI": {"tldr": "The paper introduces CART, a framework enhancing Decision Transformer (DT) robustness in adversarial stochastic games by incorporating NashQ values.", "motivation": "To address the unexplored area of adversarial robustness in reinforcement learning methods based on sequence modeling.", "method": "Formulates interactions as a stage game and integrates NashQ values to improve robustness and conservativeness in Transformer policies.", "result": "CART shows better minimax value estimation and higher worst-case returns across various adversarial stochastic games.", "conclusion": "CART demonstrates potential in advancing adversarial robustness and conservative decision-making within DT frameworks."}}
{"id": "2510.12547", "pdf": "https://arxiv.org/pdf/2510.12547", "abs": "https://arxiv.org/abs/2510.12547", "authors": ["Madi Matymov", "Ba-Hien Tran", "Maurizio Filippone"], "title": "Universal Adaptive Environment Discovery", "categories": ["stat.ML", "cs.LG", "62F15, 68T07 (Primary) 62M45, 62C10, 65C60 (Secondary)"], "comment": "8 papes in the main body, 4 pages in the appendix, 4 figures and 9\n  tables overall, conference", "summary": "An open problem in Machine Learning is how to avoid models to exploit\nspurious correlations in the data; a famous example is the background-label\nshortcut in the Waterbirds dataset. A common remedy is to train a model across\nmultiple environments; in the Waterbirds dataset, this corresponds to training\nby randomizing the background. However, selecting the right environments is a\nchallenging problem, given that these are rarely known a priori. We propose\nUniversal Adaptive Environment Discovery (UAED), a unified framework that\nlearns a distribution over data transformations that instantiate environments,\nand optimizes any robust objective averaged over this learned distribution.\nUAED yields adaptive variants of IRM, REx, GroupDRO, and CORAL without\npredefined groups or manual environment design. We provide a theoretical\nanalysis by providing PAC-Bayes bounds and by showing robustness to test\nenvironment distributions under standard conditions. Empirically, UAED\ndiscovers interpretable environment distributions and improves worst-case\naccuracy on standard benchmarks, while remaining competitive on mean accuracy.\nOur results indicate that making environments adaptive is a practical route to\nout-of-distribution generalization.", "AI": {"tldr": "The paper introduces Universal Adaptive Environment Discovery (UAED), a framework that learns adaptive data environments to improve generalization and reduce dependency on spurious correlations in machine learning.", "motivation": "To address the challenge of model reliance on spurious correlations (e.g., background-label shortcuts in datasets) and the lack of predefined environments for robust training.", "method": "UAED introduces an adaptive framework to discover a distribution over data transformations, which is then used to create environments for robust objective optimization, applicable to existing methods like IRM, REx, and GroupDRO.", "result": "UAED demonstrates theoretical robustness (via PAC-Bayes bounds) and empirical improvements, enhancing worst-case accuracy without predefined environmental knowledge while remaining competitive in mean accuracy.", "conclusion": "The adaptive discovery of environments in UAED is a viable approach for achieving better out-of-distribution generalization in machine learning models."}}
{"id": "2510.12741", "pdf": "https://arxiv.org/pdf/2510.12741", "abs": "https://arxiv.org/abs/2510.12741", "authors": ["Adam Tupper", "Christian Gagn\u00e9"], "title": "Personalized Federated Fine-Tuning of Vision Foundation Models for Healthcare", "categories": ["cs.CV", "cs.DC"], "comment": "Accepted to the Symposium on Model Accountability, Sustainability and\n  Healthcare (SMASH) 2025", "summary": "Foundation models open up new possibilities for the use of AI in healthcare.\nHowever, even when pre-trained on health data, they still need to be fine-tuned\nfor specific downstream tasks. Furthermore, although foundation models reduce\nthe amount of training data required to achieve good performance, obtaining\nsufficient data is still a challenge. This is due, in part, to restrictions on\nsharing and aggregating data from different sources to protect patients'\nprivacy. One possible solution to this is to fine-tune foundation models via\nfederated learning across multiple participating clients (i.e., hospitals,\nclinics, etc.). In this work, we propose a new personalized federated\nfine-tuning method that learns orthogonal LoRA adapters to disentangle general\nand client-specific knowledge, enabling each client to fully exploit both their\nown data and the data of others. Our preliminary results on real-world\nfederated medical imaging tasks demonstrate that our approach is competitive\nagainst current federated fine-tuning methods.", "AI": {"tldr": "The paper proposes a personalized federated fine-tuning method for healthcare foundation models using LoRA adapters, achieving competitive results in real-world medical imaging tasks.", "motivation": "AI foundation models require fine-tuning for specific healthcare tasks, but data sharing restrictions pose challenges. Federated learning, which avoids centralized data storage, can address this issue.", "method": "The authors introduce a personalized federated fine-tuning approach using orthogonal LoRA adapters that separate general knowledge from client-specific knowledge, leveraging data across multiple clients without infringing privacy.", "result": "Preliminary results show that the proposed method performs competitively compared to existing federated fine-tuning methods in medical imaging tasks.", "conclusion": "The approach enables effective fine-tuning for healthcare foundation models while maximizing utility from both local and shared data across clients, maintaining privacy."}}
{"id": "2510.12001", "pdf": "https://arxiv.org/pdf/2510.12001", "abs": "https://arxiv.org/abs/2510.12001", "authors": ["Xinyu Wang", "Haoming Yu", "Yicheng Yang", "Zhiyuan Li"], "title": "Generate Logical Equivalence Questions", "categories": ["cs.CL"], "comment": null, "summary": "Academic dishonesty is met with zero tolerance in higher education, yet\nplagiarism has become increasingly prevalent in the era of online teaching and\nlearning. Automatic Question Generation (AQG) presents a potential solution to\nmitigate copying by creating unique questions for each student. Additionally,\nAQG can provide a vast array of practice questions. Our AQG focuses on\ngenerating logical equivalence questions for Discrete Mathematics, a\nfoundational course for first-year computer science students. A literature\nreview reveals that existing AQGs for this type of question generate all\npropositions that meet user-defined constraints, resulting in inefficiencies\nand a lack of uniform question difficulty. To address this, we propose a new\napproach that defines logical equivalence questions using a formal language,\ntranslates this language into two sets of generation rules, and develops a\nlinear-time algorithm for question generation. We evaluated our AQG through two\nexperiments. The first involved a group of students completing questions\ngenerated by our system. Statistical analysis shows that the accuracy of these\nquestions is comparable to that of textbook questions. The second experiment\nassessed the number of steps required to solve our generated questions,\ntextbook questions, and those generated by multiple large language models. The\nresults indicated that the difficulty of our questions was similar to that of\ntextbook questions, confirming the quality of our AQG.", "AI": {"tldr": "The paper addresses the issue of plagiarism in online learning by using Automatic Question Generation (AQG) to create unique logical equivalence questions for Discrete Mathematics, presenting a more efficient and uniform method using a formal language and linear-time algorithm.", "motivation": "To mitigate academic dishonesty and provide ample practice questions for students, particularly in online learning, by improving the Automatic Question Generation for Discrete Mathematics.", "method": "The AQG defines logical equivalence questions using a formal language, translates it into sets of generation rules, and employs a linear-time algorithm to create questions.", "result": "Two experiments were conducted showing the accuracy and difficulty of generated questions were akin to textbook questions and comparable to Language Model-generated questions in solving steps needed.", "conclusion": "The proposed AQG approach proves to be efficient, reliable, and generates high-quality questions, providing solutions to plagiarism and enhancing learning resources in online contexts."}}
{"id": "2510.12403", "pdf": "https://arxiv.org/pdf/2510.12403", "abs": "https://arxiv.org/abs/2510.12403", "authors": ["Francesco Capuano", "Caroline Pascal", "Adil Zouitine", "Thomas Wolf", "Michel Aractingi"], "title": "Robot Learning: A Tutorial", "categories": ["cs.RO", "cs.LG"], "comment": "Tutorial on Robot Learning using LeRobot, the end-to-end robot\n  learning library developed by Hugging Face", "summary": "Robot learning is at an inflection point, driven by rapid advancements in\nmachine learning and the growing availability of large-scale robotics data.\nThis shift from classical, model-based methods to data-driven, learning-based\nparadigms is unlocking unprecedented capabilities in autonomous systems. This\ntutorial navigates the landscape of modern robot learning, charting a course\nfrom the foundational principles of Reinforcement Learning and Behavioral\nCloning to generalist, language-conditioned models capable of operating across\ndiverse tasks and even robot embodiments. This work is intended as a guide for\nresearchers and practitioners, and our goal is to equip the reader with the\nconceptual understanding and practical tools necessary to contribute to\ndevelopments in robot learning, with ready-to-use examples implemented in\n$\\texttt{lerobot}$.", "AI": {"tldr": "The paper provides an overview of modern robot learning, focusing on the shift from traditional models to advanced data-driven methods.", "motivation": "To address the need for a conceptual and practical guide amidst the transition to modern data-driven approaches in robot learning.", "method": "The tutorial explores foundational principles like Reinforcement Learning and Behavioral Cloning, and introduces advanced concepts like generalist, language-conditioned models.", "result": "The paper presents a guide equipped with examples implemented in \"lerobot\" to help researchers understand and participate in robot learning.", "conclusion": "This work serves as a resource for researchers and practitioners to contribute to advancements in modern robot learning paradigms."}}
{"id": "2510.12076", "pdf": "https://arxiv.org/pdf/2510.12076", "abs": "https://arxiv.org/abs/2510.12076", "authors": ["Junyi Xie", "Jina Kim", "Yao-Yi Chiang", "Lingyi Zhao", "Khurram Shafique"], "title": "BeSTAD: Behavior-Aware Spatio-Temporal Anomaly Detection for Human Mobility Data", "categories": ["cs.AI"], "comment": "accepted by The 2nd ACM SIGSPATIAL International Workshop on\n  Geospatial Anomaly Detection", "summary": "Traditional anomaly detection in human mobility has primarily focused on\ntrajectory-level analysis, identifying statistical outliers or spatiotemporal\ninconsistencies across aggregated movement traces. However, detecting\nindividual-level anomalies, i.e., unusual deviations in a person's mobility\nbehavior relative to their own historical patterns, within datasets\nencompassing large populations remains a significant challenge. In this paper,\nwe present BeSTAD (Behavior-aware Spatio-Temporal Anomaly Detection for Human\nMobility Data), an unsupervised framework that captures individualized\nbehavioral signatures across large populations and uncovers fine-grained\nanomalies by jointly modeling spatial context and temporal dynamics. BeSTAD\nlearns semantically enriched mobility representations that integrate location\nmeaning and temporal patterns, enabling the detection of subtle deviations in\nindividual movement behavior. BeSTAD further employs a behavior-cluster-aware\nmodeling mechanism that builds personalized behavioral profiles from normal\nactivity and identifies anomalies through cross-period behavioral comparison\nwith consistent semantic alignment. Building on prior work in mobility behavior\nclustering, this approach enables not only the detection of behavioral shifts\nand deviations from established routines but also the identification of\nindividuals exhibiting such changes within large-scale mobility datasets. By\nlearning individual behaviors directly from unlabeled data, BeSTAD advances\nanomaly detection toward personalized and interpretable mobility analysis.", "AI": {"tldr": "This paper introduces BeSTAD, a framework for detecting personalized anomalies in human mobility by jointly analyzing spatial and temporal patterns.", "motivation": "Traditional anomaly detection has overlooked individual-level deviations in large datasets, focusing instead on trajectory-level inconsistencies and statistical outliers.", "method": "The proposed BeSTAD framework combines spatial context and temporal dynamics to create enriched mobility representations and uses behavior-cluster-aware modeling to personalize anomaly detection.", "result": "BeSTAD identifies subtle deviations in mobility behavior, behavioral shifts, and individuals with anomalous patterns within large-scale datasets.", "conclusion": "BeSTAD enhances anomaly detection by integrating personalized, interpretable, and behavior-aware mechanisms for large-scale mobility data analysis."}}
{"id": "2510.12487", "pdf": "https://arxiv.org/pdf/2510.12487", "abs": "https://arxiv.org/abs/2510.12487", "authors": ["Evgeniy Glukhov", "Michele Conti", "Egor Bogomolov", "Yaroslav Golubev", "Alexander Bezzubov"], "title": "Diff-XYZ: A Benchmark for Evaluating Diff Understanding", "categories": ["cs.SE", "cs.LG"], "comment": null, "summary": "Reliable handling of code diffs is central to agents that edit and refactor\nrepositories at scale. We introduce Diff-XYZ, a compact benchmark for code-diff\nunderstanding with three supervised tasks: apply (old code $+$ diff\n$\\rightarrow$ new code), anti-apply (new code $-$ diff $\\rightarrow$ old code),\nand diff generation (new code $-$ old code $\\rightarrow$ diff). Instances in\nthe benchmark are triples $\\langle \\textit{old code}, \\textit{new code},\n\\textit{diff} \\rangle$ drawn from real commits in CommitPackFT, paired with\nautomatic metrics and a clear evaluation protocol. We use the benchmark to do a\nfocused empirical study of the unified diff format and run a cross-format\ncomparison of different diff representations. Our findings reveal that\ndifferent formats should be used depending on the use case and model size. For\nexample, representing diffs in search-replace format is good for larger models\nin the diff generation scenario, yet not suited well for diff analysis and\nsmaller models. The Diff-XYZ benchmark is a reusable foundation for assessing\nand improving diff handling in LLMs that can aid future development of diff\nformats and models editing code. The dataset is published on HuggingFace Hub:\nhttps://huggingface.co/datasets/JetBrains-Research/diff-xyz.", "AI": {"tldr": "The paper introduces Diff-XYZ, a benchmark for code-diff understanding with three tasks: apply, anti-apply, and diff generation. The dataset enables empirical studies and comparisons of diff formats.", "motivation": "The paper aims to address the need for reliable handling of code diffs by automated agents working at scale, with a focus on improving diff-understanding.", "method": "The authors curated a dataset, Diff-XYZ, comprising real commit data from CommitPackFT, defined three supervised tasks, and performed evaluations with automatic metrics across different diff representations.", "result": "Findings show that diff format effectiveness varies by use case and model size. For example, larger models favor search-replace format for diff generation, but this format is less effective for analysis and smaller models.", "conclusion": "Diff-XYZ serves as a reusable benchmark for evaluating and improving code diff handling by LLMs, guiding further advancements in diff formats and code-editing models."}}
{"id": "2510.12098", "pdf": "https://arxiv.org/pdf/2510.12098", "abs": "https://arxiv.org/abs/2510.12098", "authors": ["Jianping Li", "Dongyang Guo", "Wenjie Li", "Wei Zhao"], "title": "An Adaptive Edge-Guided Dual-Network Framework for Fast QR Code Motion Deblurring", "categories": ["cs.CV"], "comment": null, "summary": "Unlike general image deblurring that prioritizes perceptual quality, QR code\ndeblurring focuses on ensuring successful decoding. QR codes are characterized\nby highly structured patterns with sharp edges, a robust prior for restoration.\nYet existing deep learning methods rarely exploit these priors explicitly. To\naddress this gap, we propose the Edge-Guided Attention Block (EGAB), which\nembeds explicit edge priors into a Transformer architecture. Based on EGAB, we\ndevelop Edge-Guided Restormer (EG-Restormer), an effective network that\nsignificantly boosts the decoding rate of severely blurred QR codes. For mildly\nblurred inputs, we design the Lightweight and Efficient Network (LENet) for\nfast deblurring. We further integrate these two networks into an Adaptive\nDual-network (ADNet), which dynamically selects the suitable network based on\ninput blur severity, making it ideal for resource-constrained mobile devices.\nExtensive experiments show that our EG-Restormer and ADNet achieve\nstate-of-the-art performance with a competitive speed. Project page:\nhttps://github.com/leejianping/ADNet", "AI": {"tldr": "The paper introduces Edge-Guided Restormer (EG-Restormer) and Adaptive Dual-network (ADNet) for effective QR code deblurring using edge priors for improved decoding success rates.", "motivation": "Existing deep learning methods for QR code deblurring often ignore the structured and sharp-edge character of QR codes, which can be a robust prior for restoration. The paper aims to address this gap.", "method": "The authors propose the Edge-Guided Attention Block (EGAB) to embed explicit edge priors in a Transformer architecture. They developed two models: EG-Restormer for severe blur scenarios and LENet for less severe cases. These models are integrated into ADNet, which dynamically switches between them based on the input blur severity.", "result": "The proposed models, EG-Restormer and ADNet, achieve state-of-the-art performance in decoding severely and mildly blurred QR codes while maintaining competitive processing speed.", "conclusion": "The integration of explicit edge priors into QR code deblurring networks significantly improves their decoding accuracy. The adaptive approach in ADNet makes it particularly beneficial for resource-limited mobile devices."}}
{"id": "2510.11899", "pdf": "https://arxiv.org/pdf/2510.11899", "abs": "https://arxiv.org/abs/2510.11899", "authors": ["Chenliang Li", "Junyu Leng", "Jiaxiang Li", "Youbang Sun", "Shixiang Chen", "Shahin Shahrampour", "Alfredo Garcia"], "title": "ADARL: Adaptive Low-Rank Structures for Robust Policy Learning under Uncertainty", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Robust reinforcement learning (Robust RL) seeks to handle epistemic\nuncertainty in environment dynamics, but existing approaches often rely on\nnested min--max optimization, which is computationally expensive and yields\noverly conservative policies. We propose \\textbf{Adaptive Rank Representation\n(AdaRL)}, a bi-level optimization framework that improves robustness by\naligning policy complexity with the intrinsic dimension of the task. At the\nlower level, AdaRL performs policy optimization under fixed-rank constraints\nwith dynamics sampled from a Wasserstein ball around a centroid model. At the\nupper level, it adaptively adjusts the rank to balance the bias--variance\ntrade-off, projecting policy parameters onto a low-rank manifold. This design\navoids solving adversarial worst-case dynamics while ensuring robustness\nwithout over-parameterization. Empirical results on MuJoCo continuous control\nbenchmarks demonstrate that AdaRL not only consistently outperforms fixed-rank\nbaselines (e.g., SAC) and state-of-the-art robust RL methods (e.g., RNAC,\nParseval), but also converges toward the intrinsic rank of the underlying\ntasks. These results highlight that adaptive low-rank policy representations\nprovide an efficient and principled alternative for robust RL under model\nuncertainty.", "AI": {"tldr": "This paper proposes Adaptive Rank Representation (AdaRL), a bi-level optimization framework for robust reinforcement learning that adapts policy complexity to the task's intrinsic dimension, avoiding computational issues and overly conservative policies.", "motivation": "Current approaches in robust reinforcement learning are computationally expensive and result in conservative policies due to their reliance on nested min-max optimization.", "method": "AdaRL operates on a bi-level framework: the lower-level performs policy optimization under fixed-rank constraints with dynamics sampled from a Wasserstein ball, while the upper-level adjusts rank to balance bias-variance trade-offs.", "result": "Empirical testing using MuJoCo benchmarks shows AdaRL consistently outperforms fixed-rank baselines and state-of-the-art robust RL methods, converging on intrinsic task ranks.", "conclusion": "AdaRL demonstrates that adaptive low-rank policy representations improve efficiency and robustness in reinforcement learning under model uncertainty."}}
{"id": "2510.12636", "pdf": "https://arxiv.org/pdf/2510.12636", "abs": "https://arxiv.org/abs/2510.12636", "authors": ["Jannis Chemseddine", "Gregor Kornhardt", "Richard Duong", "Gabriele Steidl"], "title": "Adapting Noise to Data: Generative Flows from 1D Processes", "categories": ["stat.ML", "cs.LG", "math.AP"], "comment": null, "summary": "We introduce a general framework for constructing generative models using\none-dimensional noising processes. Beyond diffusion processes, we outline\nexamples that demonstrate the flexibility of our approach. Motivated by this,\nwe propose a novel framework in which the 1D processes themselves are\nlearnable, achieved by parameterizing the noise distribution through quantile\nfunctions that adapt to the data. Our construction integrates seamlessly with\nstandard objectives, including Flow Matching and consistency models. Learning\nquantile-based noise naturally captures heavy tails and compact supports when\npresent. Numerical experiments highlight both the flexibility and the\neffectiveness of our method.", "AI": {"tldr": "The paper introduces a flexible framework for generative models using learnable one-dimensional noising processes, offering advantages for data with heavy tails and compact supports.", "motivation": "Existing generative models, such as those based on diffusion processes, have limitations in handling certain data characteristics. This paper seeks a flexible framework that adapts more effectively to data distributions.", "method": "The method involves parameterizing the noise distribution through learnable quantile functions, enabling adaptation to data properties. It integrates with existing objectives like Flow Matching and consistency models.", "result": "The proposed method effectively captures data with heavy tails and compact supports, as demonstrated in numerical experiments showcasing its flexibility and performance.", "conclusion": "Learning quantile-based noise in generative models enhances adaptability to diverse data distributions, highlighting a promising direction for generative modeling frameworks."}}
{"id": "2510.12023", "pdf": "https://arxiv.org/pdf/2510.12023", "abs": "https://arxiv.org/abs/2510.12023", "authors": ["Alice Saebom Kwak", "Maria Alexeeva", "Gus Hahn-Powell", "Keith Alcock", "Kevin McLaughlin", "Doug McCorkle", "Gabe McNunn", "Mihai Surdeanu"], "title": "Information Extraction from Conversation Transcripts: Neuro-Symbolic vs. LLM", "categories": ["cs.CL"], "comment": "15 pages, 2 figures", "summary": "The current trend in information extraction (IE) is to rely extensively on\nlarge language models, effectively discarding decades of experience in building\nsymbolic or statistical IE systems. This paper compares a neuro-symbolic (NS)\nand an LLM-based IE system in the agricultural domain, evaluating them on nine\ninterviews across pork, dairy, and crop subdomains. The LLM-based system\noutperforms the NS one (F1 total: 69.4 vs. 52.7; core: 63.0 vs. 47.2), where\ntotal includes all extracted information and core focuses on essential details.\nHowever, each system has trade-offs: the NS approach offers faster runtime,\ngreater control, and high accuracy in context-free tasks but lacks\ngeneralizability, struggles with contextual nuances, and requires significant\nresources to develop and maintain. The LLM-based system achieves higher\nperformance, faster deployment, and easier maintenance but has slower runtime,\nlimited control, model dependency and hallucination risks. Our findings\nhighlight the \"hidden cost\" of deploying NLP systems in real-world\napplications, emphasizing the need to balance performance, efficiency, and\ncontrol.", "AI": {"tldr": "The paper contrasts neuro-symbolic (NS) and large language model (LLM)-based information extraction systems in agriculture, finding LLMs outperform in accuracy but have trade-offs in efficiency, control, and reliability.", "motivation": "The authors aim to provide insights into the trade-offs between established neuro-symbolic methods and modern LLM-based methods for practical real-world IE tasks in agriculture.", "method": "The study evaluates NS and LLM systems based on information extraction performance across nine interviews in pork, dairy, and crop subdomains, comparing trade-offs such as accuracy, runtime, generalizability, and control.", "result": "The LLM-based system performs better in overall accuracy (F1-total: 69.4 vs. 52.7 and F1-core: 63.0 vs. 47.2), but the NS system excels in efficiency, context-free task accuracy, and control.", "conclusion": "The findings identify the trade-offs of deploying NLP systems, stressing the need for balance between performance, runtime efficiency, reliability, and resource management."}}
{"id": "2510.12419", "pdf": "https://arxiv.org/pdf/2510.12419", "abs": "https://arxiv.org/abs/2510.12419", "authors": ["Shunnosuke Yoshimura", "Kento Kawaharazuka", "Kei Okada"], "title": "M3D-skin: Multi-material 3D-printed Tactile Sensor with Hierarchical Infill Structures for Pressure Sensing", "categories": ["cs.RO"], "comment": "Accepted to IROS2025, Website:\n  https://ssk-yoshimura.github.io/M3D-skin/", "summary": "Tactile sensors have a wide range of applications, from utilization in\nrobotic grippers to human motion measurement. If tactile sensors could be\nfabricated and integrated more easily, their applicability would further\nexpand. In this study, we propose a tactile sensor-M3D-skin-that can be easily\nfabricated with high versatility by leveraging the infill patterns of a\nmulti-material fused deposition modeling (FDM) 3D printer as the sensing\nprinciple. This method employs conductive and non-conductive flexible filaments\nto create a hierarchical structure with a specific infill pattern. The flexible\nhierarchical structure deforms under pressure, leading to a change in\nelectrical resistance, enabling the acquisition of tactile information. We\nmeasure the changes in characteristics of the proposed tactile sensor caused by\nmodifications to the hierarchical structure. Additionally, we demonstrate the\nfabrication and use of a multi-tile sensor. Furthermore, as applications, we\nimplement motion pattern measurement on the sole of a foot, integration with a\nrobotic hand, and tactile-based robotic operations. Through these experiments,\nwe validate the effectiveness of the proposed tactile sensor.", "AI": {"tldr": "This paper introduces a new tactile sensor called M3D-skin using 3D printing methods to capture tactile information effectively.", "motivation": "To simplify fabrication and expand applications of tactile sensors in robotics and human motion measurement.", "method": "Leveraging multi-material Fused Deposition Modeling (FDM) 3D printing with conductive and non-conductive filaments to create tactile sensors based on infill patterns.", "result": "The proposed sensors effectively detect tactile information through deformation under pressure. Demonstrations include motion measurement and robotic applications.", "conclusion": "The M3D-skin sensor is validated as versatile, easy-to-produce, and applicable for various tasks, proving the concept's effectiveness in practical applications."}}
{"id": "2510.12080", "pdf": "https://arxiv.org/pdf/2510.12080", "abs": "https://arxiv.org/abs/2510.12080", "authors": ["Rabimba Karanjai", "Yang Lu", "Ranjith Chodavarapu", "Lei Xu", "Weidong Shi"], "title": "Evaluating the Quality of Randomness and Entropy in Tasks Supported by Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "The rapid advancement of large language model (LLM) technology has led to\ndiverse applications, many of which inherently require randomness, such as\nstochastic decision-making, gaming, scheduling, AI agents, and\ncryptography-related tasks. However, the capabilities of LLMs in handling\nrandomness, particularly in generating and utilizing random numbers\neffectively, remain unclear. This paper investigates the capacity of LLMs for\nhandling tasks that involve randomness through a series of experiments. We\ndesigned a set of experiments that consider various factors that can influence\nan LLM's performance in tasks involving randomness, such as accessibility to\nexternal tools, types of tasks, model states (fresh vs. non-fresh), and\nprompting strategies. The experiments cover a range of tasks, including\ngenerating random numbers, generating random strings such as passwords,\nshuffling items, and evaluating the quality of randomness using entropy and the\nNIST randomness test-suite. Our findings reveal that while LLMs can generate\noutputs that exhibit some degree of randomness, their performance is\ninconsistent and often deviates significantly from the expected behavior. The\nanalysis of the experimental results highlights key limitations and areas where\nimprovement is needed for the LLMs to effectively handle tasks involving\nrandomness", "AI": {"tldr": "This paper explores the ability of large language models (LLMs) in tasks requiring randomness, revealing inconsistent and suboptimal behavior.", "motivation": "To assess whether LLMs can reliably handle various tasks requiring randomness for applications like AI agents, gaming, and cryptography.", "method": "Conducted experiments with factors affecting LLM performance in randomness tasks, such as tool access, task types, model states, and prompting strategies.", "result": "LLMs showed inconsistent ability in generating high-quality random numbers, strings, and successful randomness-related tasks.", "conclusion": "LLMs possess some ability for randomness but require significant improvement to handle these tasks effectively."}}
{"id": "2510.12546", "pdf": "https://arxiv.org/pdf/2510.12546", "abs": "https://arxiv.org/abs/2510.12546", "authors": ["Hashini Gunatilake", "John Grundy", "Rashina Hoda", "Ingo Mueller"], "title": "The EmpathiSEr: Development and Validation of Software Engineering Oriented Empathy Scales", "categories": ["cs.SE"], "comment": null, "summary": "Empathy plays a critical role in software engineering (SE), influencing\ncollaboration, communication, and user-centred design. Although SE research has\nincreasingly recognised empathy as a key human aspect, there remains no\nvalidated instrument specifically designed to measure it within the unique\nsocio-technical contexts of SE. Existing generic empathy scales, while\nwell-established in psychology and healthcare, often rely on language,\nscenarios, and assumptions that are not meaningful or interpretable for\nsoftware practitioners. These scales fail to account for the diverse,\nrole-specific, and domain-bound expressions of empathy in SE, such as\nunderstanding a non-technical user's frustrations or another practitioner's\ntechnical constraints, which differ substantially from empathy in clinical or\neveryday contexts. To address this gap, we developed and validated two\ndomain-specific empathy scales: EmpathiSEr-P, assessing empathy among\npractitioners, and EmpathiSEr-U, capturing practitioner empathy towards users.\nGrounded in a practitioner-informed conceptual framework, the scales encompass\nthree dimensions of empathy: cognitive empathy, affective empathy, and empathic\nresponses. We followed a rigorous, multi-phase methodology, including expert\nevaluation, cognitive interviews, and two practitioner surveys. The resulting\ninstruments represent the first psychometrically validated empathy scales\ntailored to SE, offering researchers and practitioners a tool for assessing\nempathy and designing empathy-enhancing interventions in software teams and\nuser interactions.", "AI": {"tldr": "The paper addresses the lack of tools to measure empathy in software engineering by developing two context-specific empathy scales: EmpathiSEr-P and EmpathiSEr-U.", "motivation": "Empathy is a critical factor in the success of software engineering, influencing collaboration and design. However, existing empathy measurement tools are not suited for the socio-technical nuances of software engineering. This paper aims to fill this gap.", "method": "The authors created and validated two domain-specific scales, EmpathiSEr-P and EmpathiSEr-U, using a multi-phase approach involving expert evaluation, cognitive interviews, and practitioner surveys.", "result": "The study produced the first psychometrically validated empathy measurement tools tailored for software engineering contexts, covering three dimensions of empathy: cognitive, affective, and empathic responses.", "conclusion": "The new scales enable researchers and practitioners to measure and enhance empathy in software teams and user interactions effectively, filling a vital gap in the domain."}}
{"id": "2510.12099", "pdf": "https://arxiv.org/pdf/2510.12099", "abs": "https://arxiv.org/abs/2510.12099", "authors": ["Junfeng Ni", "Yixin Chen", "Zhifei Yang", "Yu Liu", "Ruijie Lu", "Song-Chun Zhu", "Siyuan Huang"], "title": "G4Splat: Geometry-Guided Gaussian Splatting with Generative Prior", "categories": ["cs.CV"], "comment": "Project page: https://dali-jack.github.io/g4splat-web/", "summary": "Despite recent advances in leveraging generative prior from pre-trained\ndiffusion models for 3D scene reconstruction, existing methods still face two\ncritical limitations. First, due to the lack of reliable geometric supervision,\nthey struggle to produce high-quality reconstructions even in observed regions,\nlet alone in unobserved areas. Second, they lack effective mechanisms to\nmitigate multi-view inconsistencies in the generated images, leading to severe\nshape-appearance ambiguities and degraded scene geometry. In this paper, we\nidentify accurate geometry as the fundamental prerequisite for effectively\nexploiting generative models to enhance 3D scene reconstruction. We first\npropose to leverage the prevalence of planar structures to derive accurate\nmetric-scale depth maps, providing reliable supervision in both observed and\nunobserved regions. Furthermore, we incorporate this geometry guidance\nthroughout the generative pipeline to improve visibility mask estimation, guide\nnovel view selection, and enhance multi-view consistency when inpainting with\nvideo diffusion models, resulting in accurate and consistent scene completion.\nExtensive experiments on Replica, ScanNet++, and DeepBlending show that our\nmethod consistently outperforms existing baselines in both geometry and\nappearance reconstruction, particularly for unobserved regions. Moreover, our\nmethod naturally supports single-view inputs and unposed videos, with strong\ngeneralizability in both indoor and outdoor scenarios with practical real-world\napplicability. The project page is available at\nhttps://dali-jack.github.io/g4splat-web/.", "AI": {"tldr": "This paper addresses two major issues in 3D scene reconstruction using pre-trained diffusion models, namely the lack of reliable geometric supervision and multi-view inconsistencies, by focusing on accurate geometry and leveraging planar structures to enhance reconstructions.", "motivation": "Existing 3D reconstruction methods using pre-trained diffusion models fail to deliver high-quality results due to unreliable geometric supervision and inconsistencies across multiple views.", "method": "The paper introduces a technique that leverages planar structures to derive accurate depth maps for supervision. Geometry guidance is integrated throughout the generative pipeline to enhance factors such as visibility mask estimation and multi-view consistency.", "result": "The proposed method outperforms baseline approaches in both geometry and appearance reconstruction. It demonstrates improvements in unobserved regions and supports inputs like single views and unposed videos with general applicability across scenarios.", "conclusion": "Accurate geometry is critical for effective 3D reconstruction using generative models. By incorporating reliable depth-based supervision and improving consistency, the method has broad potential for real-world applications in indoor and outdoor settings."}}
{"id": "2510.11903", "pdf": "https://arxiv.org/pdf/2510.11903", "abs": "https://arxiv.org/abs/2510.11903", "authors": ["Rizal Fathony", "Igor Melnyk", "Owen Reinert", "Nam H. Nguyen", "Daniele Rosa", "C. Bayan Bruss"], "title": "Integrating Sequential and Relational Modeling for User Events: Datasets and Prediction Tasks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "User event modeling plays a central role in many machine learning\napplications, with use cases spanning e-commerce, social media, finance,\ncybersecurity, and other domains. User events can be broadly categorized into\npersonal events, which involve individual actions, and relational events, which\ninvolve interactions between two users. These two types of events are typically\nmodeled separately, using sequence-based methods for personal events and\ngraph-based methods for relational events. Despite the need to capture both\nevent types in real-world systems, prior work has rarely considered them\ntogether. This is often due to the convenient simplification that user behavior\ncan be adequately represented by a single formalization, either as a sequence\nor a graph. To address this gap, there is a need for public datasets and\nprediction tasks that explicitly incorporate both personal and relational\nevents. In this work, we introduce a collection of such datasets, propose a\nunified formalization, and empirically show that models benefit from\nincorporating both event types. Our results also indicate that current methods\nleave a notable room for improvements. We release these resources to support\nfurther research in unified user event modeling and encourage progress in this\ndirection.", "AI": {"tldr": "The paper highlights the gap in user event modeling by pointing out the lack of methods combining personal and relational events, introduces unified datasets for such modeling, and empirically demonstrates its benefits while releasing resources for further research.", "motivation": "To address the limitations of existing user event modeling, which focuses separately on personal events (sequences of user actions) and relational events (interactions between users), despite the necessity of combining both in real-world applications.", "method": "The paper introduces unified datasets combining personal and relational events, proposes a formalized framework for modeling these events together, and conducts empirical evaluations showing improved modeling performance.", "result": "The approach demonstrates that unified models yield better predictions by incorporating both personal and relational user events, and it also uncovers gaps in existing methods.", "conclusion": "Unified modeling of user events provides demonstrable benefits, but there is significant potential to further improve these methods. The authors provide datasets and tools to aid future research in this area."}}
{"id": "2510.12639", "pdf": "https://arxiv.org/pdf/2510.12639", "abs": "https://arxiv.org/abs/2510.12639", "authors": ["Anand Srinivasan", "Jean-Jacques Slotine"], "title": "Contraction and entropy production in continuous-time Sinkhorn dynamics", "categories": ["stat.ML", "cs.LG", "math.PR"], "comment": "10 pages excluding references", "summary": "Recently, the vanishing-step-size limit of the Sinkhorn algorithm at finite\nregularization parameter $\\varepsilon$ was shown to be a mirror descent in the\nspace of probability measures. We give $L^2$ contraction criteria in two\ntime-dependent metrics induced by the mirror Hessian, which reduce to the\ncoercivity of certain conditional expectation operators. We then give an exact\nidentity for the entropy production rate of the Sinkhorn flow, which was\npreviously known only to be nonpositive. Examining this rate shows that the\nstandard semigroup analysis of diffusion processes extends systematically to\nthe Sinkhorn flow. We show that the flow induces a reversible Markov dynamics\non the target marginal as an Onsager gradient flow. We define the Dirichlet\nform associated to its (nonlocal) infinitesimal generator, prove a Poincar\\'e\ninequality for it, and show that the spectral gap is strictly positive along\nthe Sinkhorn flow whenever $\\varepsilon > 0$. Lastly, we show that the entropy\ndecay is exponential if and only if a logarithmic Sobolev inequality (LSI)\nholds. We give for illustration two immediate practical use-cases for the\nSinkhorn LSI: as a design principle for the latent space in which generative\nmodels are trained, and as a stopping heuristic for discrete-time algorithms.", "AI": {"tldr": "This paper examines the Sinkhorn algorithm and its vanishing-step-size limit, establishing connections to mirror descent in probability spaces and Markov dynamics. It provides entropy production analysis, metrics contraction conditions, and proves mathematical inequalities for improved practical applications.", "motivation": "To better understand the theoretical properties of the Sinkhorn algorithm and its connections to probability measures, reversible Markov processes, and entropy dynamics, and explore its implications in practical applications such as generative models and algorithm stopping criteria.", "method": "The authors use mathematical analysis to establish $L^2$ contraction criteria in certain metrics, derive entropy production rates, and prove inequalities like Poincar\u00e9 and logarithmic Sobolev, focusing on the Sinkhorn flow as a structured gradient flow.", "result": "They show $L^2$ contraction criteria, derive entropy production rate identities, and prove strict positivity of the spectral gap for $\text{\u03b5}>0$. Additionally, they illustrate practical applications like its utility in generative model latent spaces and stopping heuristics for algorithms.", "conclusion": "The study highlights systematic diffusion process extensions for the Sinkhorn flow, proving exponential entropy decay reliant on LSI and emphasizing its applicability in areas like machine learning training spaces and algorithm optimization."}}
{"id": "2510.12029", "pdf": "https://arxiv.org/pdf/2510.12029", "abs": "https://arxiv.org/abs/2510.12029", "authors": ["Jung-Woo Shim", "Yeong-Joon Ju", "Ji-Hoon Park", "Seong-Whan Lee"], "title": "CPR: Mitigating Large Language Model Hallucinations with Curative Prompt Refinement", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "2024 IEEE International Conference on Systems, Man, and Cybernetics\n  (SMC), 7 pages, 2 figures", "summary": "Recent advancements in large language models (LLMs) highlight their fluency\nin generating responses to diverse prompts. However, these models sometimes\ngenerate plausible yet incorrect ``hallucinated\" facts, undermining trust. A\nfrequent but often overlooked cause of such errors is the use of poorly\nstructured or vague prompts by users, leading LLMs to base responses on assumed\nrather than actual intentions. To mitigate hallucinations induced by these\nill-formed prompts, we introduce Curative Prompt Refinement (CPR), a\nplug-and-play framework for curative prompt refinement that 1) cleans\nill-formed prompts, and 2) generates additional informative task descriptions\nto align the intention of the user and the prompt using a fine-tuned small\nlanguage model. When applied to language models, we discover that CPR\nsignificantly increases the quality of generation while also mitigating\nhallucination. Empirical studies show that prompts with CPR applied achieves\nover a 90\\% win rate over the original prompts without any external knowledge.", "AI": {"tldr": "Large language models (LLMs) can produce errors due to vague prompts. The study introduces Curative Prompt Refinement (CPR) to clean prompts and better align user intentions, improving outputs and reducing hallucinations.", "motivation": "Errors known as 'hallucinated facts' undermine trust in LLMs, with vague prompts being a primary overlooked cause.", "method": "A framework called CPR refines ill-formed prompts by cleaning their structure and providing task descriptions, using a fine-tuned small language model.", "result": "CPR significantly improved LLM output quality and reduced hallucination, achieving over a 90% win rate compared to original prompts.", "conclusion": "Improving prompt clarity through CPR enhances trust and performance in LLM outputs without the need for external knowledge."}}
{"id": "2510.12477", "pdf": "https://arxiv.org/pdf/2510.12477", "abs": "https://arxiv.org/abs/2510.12477", "authors": ["Gaoyuan Liu", "Joris de Winter", "Kelly Merckaert", "Denis Steckelmacher", "Ann Nowe", "Bram Vanderborght"], "title": "A Task-Efficient Reinforcement Learning Task-Motion Planner for Safe Human-Robot Cooperation", "categories": ["cs.RO"], "comment": null, "summary": "In a Human-Robot Cooperation (HRC) environment, safety and efficiency are the\ntwo core properties to evaluate robot performance. However, safety mechanisms\nusually hinder task efficiency since human intervention will cause backup\nmotions and goal failures of the robot. Frequent motion replanning will\nincrease the computational load and the chance of failure. In this paper, we\npresent a hybrid Reinforcement Learning (RL) planning framework which is\ncomprised of an interactive motion planner and a RL task planner. The RL task\nplanner attempts to choose statistically safe and efficient task sequences\nbased on the feedback from the motion planner, while the motion planner keeps\nthe task execution process collision-free by detecting human arm motions and\ndeploying new paths when the previous path is not valid anymore. Intuitively,\nthe RL agent will learn to avoid dangerous tasks, while the motion planner\nensures that the chosen tasks are safe. The proposed framework is validated on\nthe cobot in both simulation and the real world, we compare the planner with\nhard-coded task motion planning methods. The results show that our planning\nframework can 1) react to uncertain human motions at both joint and task\nlevels; 2) reduce the times of repeating failed goal commands; 3) reduce the\ntotal number of replanning requests.", "AI": {"tldr": "The paper presents a hybrid framework combining reinforcement learning (RL) and interactive motion planning to enhance safety and efficiency in human-robot cooperation (HRC) by reducing task failures, replanning requests, and computational loads.", "motivation": "To resolve the trade-off between safety and task efficiency in HRC environments, particularly the challenges posed by frequent motion replanning and human intervention.", "method": "The framework integrates a Reinforcement Learning task planner with an interactive motion planner. The RL task planner selects safe and efficient task sequences using feedback from the motion planner, while the motion planner ensures collision-free executions by detecting changes in human arm motions and adjusting paths as needed.", "result": "The framework was validated on a cobot both in simulation and in real-world scenarios. It was found to react effectively to uncertain human motions, decrease failed goal command repetitions, and reduce the number of replanning requests compared to traditional approaches.", "conclusion": "The proposed hybrid RL framework enhances both safety and efficiency in HRC environments, demonstrating its practical value in managing uncertainty and minimizing computational issues during task planning and execution."}}
{"id": "2510.12088", "pdf": "https://arxiv.org/pdf/2510.12088", "abs": "https://arxiv.org/abs/2510.12088", "authors": ["Zaid Khan", "Archiki Prasad", "Elias Stengel-Eskin", "Jaemin Cho", "Mohit Bansal"], "title": "One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Project page: https://onelife-worldmodel.github.io/; 39 pages", "summary": "Symbolic world modeling requires inferring and representing an environment's\ntransitional dynamics as an executable program. Prior work has focused on\nlargely deterministic environments with abundant interaction data, simple\nmechanics, and human guidance. We address a more realistic and challenging\nsetting, learning in a complex, stochastic environment where the agent has only\n\"one life\" to explore a hostile environment without human guidance. We\nintroduce OneLife, a framework that models world dynamics through\nconditionally-activated programmatic laws within a probabilistic programming\nframework. Each law operates through a precondition-effect structure,\nactivating in relevant world states. This creates a dynamic computation graph\nthat routes inference and optimization only through relevant laws, avoiding\nscaling challenges when all laws contribute to predictions about a complex,\nhierarchical state, and enabling the learning of stochastic dynamics even with\nsparse rule activation. To evaluate our approach under these demanding\nconstraints, we introduce a new evaluation protocol that measures (a) state\nranking, the ability to distinguish plausible future states from implausible\nones, and (b) state fidelity, the ability to generate future states that\nclosely resemble reality. We develop and evaluate our framework on Crafter-OO,\nour reimplementation of the Crafter environment that exposes a structured,\nobject-oriented symbolic state and a pure transition function that operates on\nthat state alone. OneLife can successfully learn key environment dynamics from\nminimal, unguided interaction, outperforming a strong baseline on 16 out of 23\nscenarios tested. We also test OneLife's planning ability, with simulated\nrollouts successfully identifying superior strategies. Our work establishes a\nfoundation for autonomously constructing programmatic world models of unknown,\ncomplex environments.", "AI": {"tldr": "The paper introduces OneLife, a probabilistic programming framework for modeling dynamics in stochastic environments where an agent has minimal interaction and no human guidance. OneLife demonstrates strong performance in learning dynamics and planning strategies.", "motivation": "The goal is to model transitional dynamics in complex, stochastic environments where agents interact sparsely and autonomously, avoiding prior reliance on abundant data and human assistance.", "method": "OneLife uses conditionally-activated programmatic laws within a probabilistic framework. These laws operate with a precondition-effect structure and dynamically adjust the computation graph for relevance, facilitating effective learning of sparse dynamics.", "result": "OneLife outperformed a strong baseline in 16 out of 23 test scenarios using the Crafter-OO environment, showing better understanding of dynamics and improved planning strategies.", "conclusion": "The work provides a robust method for autonomously constructing programmatic world models in unknown and complex environments, marking progress toward realistic symbolic world modeling."}}
{"id": "2510.12566", "pdf": "https://arxiv.org/pdf/2510.12566", "abs": "https://arxiv.org/abs/2510.12566", "authors": ["Maja H. Kirkeby", "Timmie Lagermann"], "title": "Evaluating End-User Device Energy Models in Sustainability Reporting of Browser-Based Web Services", "categories": ["cs.SE"], "comment": null, "summary": "Sustainability reporting in web-based services increasingly relies on\nsimplified energy and carbon models such as the Danish Agency of Digital\nGovernment's Digst framework and the United Kingdom-based DIMPACT model.\nAlthough these models are widely adopted, their accuracy and precision remain\nunderexplored. This paper presents an empirical study evaluating how well such\nmodels reflect actual energy consumption during realistic user interactions\nwith common website categories. Energy use was measured across shopping,\nbooking, navigation, and news services using predefined user flows executed on\nfour laptop platforms. The results show that the commonly applied\nconstant-power approximation (P * t) can diverge substantially from measured\nenergy, depending on website category, device type, and task characteristics.\nThe findings demonstrate that model deviations are systematic rather than\nrandom and highlight the need for category-aware and device-reflective power\nparameters in reproducible sustainability reporting frameworks.", "AI": {"tldr": "This study examines the accuracy of energy and carbon models used for web-based services sustainability reporting, highlighting discrepancies in energy estimation based on website categories, device types, and tasks.", "motivation": "To address the lack of precision and accuracy in widely used energy and carbon models for sustainability reporting.", "method": "Conducted an empirical study measuring real energy consumption across multiple website categories and devices during predefined user interactions.", "result": "Found substantial deviations between models and actual measured energy. Discrepancies are systematic, influenced by category, device type, and task considerations.", "conclusion": "Existing models need category-aware, device-reflective adjustments to enhance precision and reproducibility in sustainability reporting frameworks."}}
{"id": "2510.12107", "pdf": "https://arxiv.org/pdf/2510.12107", "abs": "https://arxiv.org/abs/2510.12107", "authors": ["Jiawei Zhan", "Jun Liu", "Jinlong Peng", "Xiaochen Chen", "Bin-Bin Gao", "Yong Liu", "Chengjie Wang"], "title": "DRL: Discriminative Representation Learning with Parallel Adapters for Class Incremental Learning", "categories": ["cs.CV", "68T05, 68T07", "I.2.6; I.5.4"], "comment": "13 pages, 7 figures", "summary": "With the excellent representation capabilities of Pre-Trained Models (PTMs),\nremarkable progress has been made in non-rehearsal Class-Incremental Learning\n(CIL) research. However, it remains an extremely challenging task due to three\nconundrums: increasingly large model complexity, non-smooth representation\nshift during incremental learning and inconsistency between stage-wise\nsub-problem optimization and global inference. In this work, we propose the\nDiscriminative Representation Learning (DRL) framework to specifically address\nthese challenges. To conduct incremental learning effectively and yet\nefficiently, the DRL's network, called Incremental Parallel Adapter (IPA)\nnetwork, is built upon a PTM and increasingly augments the model by learning a\nlightweight adapter with a small amount of parameter learning overhead in each\nincremental stage. The adapter is responsible for adapting the model to new\nclasses, it can inherit and propagate the representation capability from the\ncurrent model through parallel connection between them by a transfer gate. As a\nresult, this design guarantees a smooth representation shift between different\nincremental stages. Furthermore, to alleviate inconsistency and enable\ncomparable feature representations across incremental stages, we design the\nDecoupled Anchor Supervision (DAS). It decouples constraints of positive and\nnegative samples by respectively comparing them with the virtual anchor. This\ndecoupling promotes discriminative representation learning and aligns the\nfeature spaces learned at different stages, thereby narrowing the gap between\nstage-wise local optimization over a subset of data and global inference across\nall classes. Extensive experiments on six benchmarks reveal that our DRL\nconsistently outperforms other state-of-the-art methods throughout the entire\nCIL period while maintaining high efficiency in both training and inference\nphases.", "AI": {"tldr": "The paper introduces the Discriminative Representation Learning (DRL) framework, which leverages Pre-Trained Models to address challenges in Class-Incremental Learning effectively and efficiently by augmenting models with lightweight adapters and decoupled supervision.", "motivation": "Class-Incremental Learning (CIL) with Pre-Trained Models is challenging due to model complexity, representation shift, and inconsistency between local optimization and global inference.", "method": "The paper proposes the Incremental Parallel Adapter (IPA) network, which uses lightweight adapters to augment the model and a Decoupled Anchor Supervision (DAS) to improve representation discrimination and alignment.", "result": "Extensive experiments on six benchmarks show that DRL achieves state-of-the-art performance in Class-Incremental Learning while maintaining efficiency in training and inference.", "conclusion": "DRL effectively addresses key challenges in non-rehearsal CIL by ensuring smooth representation shift and consistent feature alignment, demonstrating superior performance and efficiency over existing approaches."}}
{"id": "2510.11917", "pdf": "https://arxiv.org/pdf/2510.11917", "abs": "https://arxiv.org/abs/2510.11917", "authors": ["Jun-En Ding", "Anna Zilverstand", "Shihao Yang", "Albert Chih-Chieh Yang", "Feng Liu"], "title": "Variational Mixture of Graph Neural Experts for Alzheimer's Disease Biomarker Recognition in EEG Brain Networks", "categories": ["cs.LG"], "comment": null, "summary": "Dementia disorders such as Alzheimer's disease (AD) and frontotemporal\ndementia (FTD) exhibit overlapping electrophysiological signatures in EEG that\nchallenge accurate diagnosis. Existing EEG-based methods are limited by\nfull-band frequency analysis that hinders precise differentiation of dementia\nsubtypes and severity stages. We propose a variational mixture of graph neural\nexperts (VMoGE) that integrates frequency-specific biomarker identification\nwith structured variational inference for enhanced dementia diagnosis and\nstaging. VMoGE employs a multi-granularity transformer to extract multi-scale\ntemporal patterns across four frequency bands, followed by a variational graph\nconvolutional encoder using Gaussian Markov Random Field priors. Through\nstructured variational inference and adaptive gating, VMoGE links neural\nspecialization to physiologically meaningful EEG frequency bands. Evaluated on\ntwo diverse datasets for both subtype classification and severity staging,\nVMoGE achieves superior performance with AUC improvements of +4% to +10% over\nstate-of-the-art methods. Moreover, VMoGE provides interpretable insights\nthrough expert weights that correlate with clinical indicators and spatial\npatterns aligned with neuropathological signatures, facilitating EEG biomarker\ndiscovery for comprehensive dementia diagnosis and monitoring.", "AI": {"tldr": "This paper introduces VMoGE, a method using graph neural networks and structured variational inference to improve dementia diagnosis and severity staging based on EEG data. It outperforms existing methods and connects findings to physiological insights.", "motivation": "Accurate diagnosis and severity staging for dementia disorders like Alzheimer's and frontotemporal dementia are challenged by overlapping EEG signatures and limitations in existing full-band frequency analysis approaches.", "method": "The proposed VMoGE model combines a multi-granularity transformer for extracting multi-scale temporal patterns across frequency bands, and a variational graph convolutional encoder with Gaussian Markov Random Field priors. It uses structured variational inference and adaptive gating for EEG frequency band analysis and biomarker discovery.", "result": "VMoGE delivered superior diagnostic performance, improving AUC by 4%-10% compared to state-of-the-art methods across two datasets. It also enabled interpretable insights correlated with clinical indicators and neuropathological signatures.", "conclusion": "VMoGE enhances dementia diagnosis and staging by addressing EEG challenges, improving accuracy, and uncovering physiologically meaningful biomarkers for in-depth understanding and monitoring."}}
{"id": "2510.12744", "pdf": "https://arxiv.org/pdf/2510.12744", "abs": "https://arxiv.org/abs/2510.12744", "authors": ["Do Tien Hai", "Trung Nguyen Mai", "TrungTin Nguyen", "Nhat Ho", "Binh T. Nguyen", "Christopher Drovandi"], "title": "Dendrograms of Mixing Measures for Softmax-Gated Gaussian Mixture of Experts: Consistency without Model Sweeps", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.CO", "stat.ME", "stat.TH"], "comment": "Do Tien Hai, Trung Nguyen Mai, and TrungTin Nguyen are co-first\n  authors", "summary": "We develop a unified statistical framework for softmax-gated Gaussian mixture\nof experts (SGMoE) that addresses three long-standing obstacles in parameter\nestimation and model selection: (i) non-identifiability of gating parameters up\nto common translations, (ii) intrinsic gate-expert interactions that induce\ncoupled differential relations in the likelihood, and (iii) the tight\nnumerator-denominator coupling in the softmax-induced conditional density. Our\napproach introduces Voronoi-type loss functions aligned with the gate-partition\ngeometry and establishes finite-sample convergence rates for the maximum\nlikelihood estimator (MLE). In over-specified models, we reveal a link between\nthe MLE's convergence rate and the solvability of an associated system of\npolynomial equations characterizing near-nonidentifiable directions. For model\nselection, we adapt dendrograms of mixing measures to SGMoE, yielding a\nconsistent, sweep-free selector of the number of experts that attains\npointwise-optimal parameter rates under overfitting while avoiding multi-size\ntraining. Simulations on synthetic data corroborate the theory, accurately\nrecovering the expert count and achieving the predicted rates for parameter\nestimation while closely approximating the regression function. Under model\nmisspecification (e.g., $\\epsilon$-contamination), the dendrogram selection\ncriterion is robust, recovering the true number of mixture components, while\nthe Akaike information criterion, the Bayesian information criterion, and the\nintegrated completed likelihood tend to overselect as sample size grows. On a\nmaize proteomics dataset of drought-responsive traits, our dendrogram-guided\nSGMoE selects two experts, exposes a clear mixing-measure hierarchy, stabilizes\nthe likelihood early, and yields interpretable genotype-phenotype maps,\noutperforming standard criteria without multi-size training.", "AI": {"tldr": "This paper proposes a unified statistical method for softmax-gated Gaussian mixture of experts (SGMoE) to address challenges in parameter estimation and model selection. It introduces Voronoi-type loss functions and presents improved techniques for identifying the number of experts.", "motivation": "To overcome challenges in the SGMoE framework, including gating parameter non-identifiability, coupled gate-expert interactions, and tight parameter dependencies in softmax-induced densities.", "method": "The authors introduce Voronoi-type loss functions aligned with gate geometries, derive convergence rates for MLE in over-specified models, and adapt dendrogram-based techniques for consistent expert count selection.", "result": "Simulation experiments confirm theoretical predictions, showing robust expert count recovery and improved performance even under model misspecification, outperforming other selection criteria.", "conclusion": "The proposed SGMoE framework provides robust, interpretable results for model selection and parameter estimation, outperforming existing criteria and showing practical potential in real-world datasets like genetic trait mapping."}}
{"id": "2510.12032", "pdf": "https://arxiv.org/pdf/2510.12032", "abs": "https://arxiv.org/abs/2510.12032", "authors": ["Jung-Woo Shim", "Yeong-Joon Ju", "Ji-Hoon Park", "Seong-Whan Lee"], "title": "Multi-stage Prompt Refinement for Mitigating Hallucinations in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "22 pages, 6 figures", "summary": "Recent advancements in large language models (LLMs) have shown strong\nperformance in natural language understanding and generation tasks. However,\nLLMs continue to encounter challenges with hallucinations, where models\ngenerate plausible but incorrect information. While several factors contribute\nto hallucinations, the impact of ill-formed prompts, prompts with ambiguous\nwording, incorrect grammar, or incomplete information, was relatively under\nexplored. To address this, we introduce Multi-stage Prompt Refinement (MPR), a\nframework designed to systematically improve these ill-formed prompts across\nmultiple stages. Each stage addresses specific errors such as punctuation,\ntypographical mistakes, and misuse of key terms, using small language models\n(SLMs) fine-tuned for these tasks. MPR iteratively enhances the clarity of\nprompts with additional context and employs a self-reflection mechanism with\nranking to prioritize the most relevant input. Experimental results on\nhallucination benchmarks show that prompts refined by MPR achieve over an 85~\\%\nwin rate compared to their original forms, demonstrating its effectiveness in\nreducing hallucinations and improving LLM output accuracy. Interestingly, we\nreveal that MPR can be combined with existing post-hoc hallucination mitigation\nframeworks, further enhancing its versatility. MPR provides a lightweight and\nadaptable solution for enhancing LLM reliability across various domains.", "AI": {"tldr": "Multi-stage Prompt Refinement (MPR) improves the clarity and accuracy of prompts to significantly reduce hallucinations in large language models.", "motivation": "Large language models often hallucinate due to ill-formed prompts characterized by ambiguous wording, grammatical errors, or incomplete information, an underexplored issue.", "method": "MPR systematically refines prompts in multiple stages using fine-tuned small language models to address errors like punctuation, typographical issues, and terminology misuse. It incorporates self-reflection mechanisms and ranking systems for quality improvement.", "result": "Refined prompts using MPR showed over 85% win rate on hallucination benchmarks, proving its effectiveness in improving LLM output accuracy.", "conclusion": "MPR is a lightweight and adaptable framework that reduces hallucinations, enhances LLM reliability, and complements other mitigation strategies across various domains."}}
{"id": "2510.12483", "pdf": "https://arxiv.org/pdf/2510.12483", "abs": "https://arxiv.org/abs/2510.12483", "authors": ["Jingkai Jia", "Tong Yang", "Xueyao Chen", "Chenhuan Liu", "Wenqiang Zhang"], "title": "Fast Visuomotor Policy for Robotic Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "We present a fast and effective policy framework for robotic manipulation,\nnamed Energy Policy, designed for high-frequency robotic tasks and\nresource-constrained systems. Unlike existing robotic policies, Energy Policy\nnatively predicts multimodal actions in a single forward pass, enabling\nhigh-precision manipulation at high speed. The framework is built upon two core\ncomponents. First, we adopt the energy score as the learning objective to\nfacilitate multimodal action modeling. Second, we introduce an energy MLP to\nimplement the proposed objective while keeping the architecture simple and\nefficient. We conduct comprehensive experiments in both simulated environments\nand real-world robotic tasks to evaluate the effectiveness of Energy Policy.\nThe results show that Energy Policy matches or surpasses the performance of\nstate-of-the-art manipulation methods while significantly reducing\ncomputational overhead. Notably, on the MimicGen benchmark, Energy Policy\nachieves superior performance with at a faster inference compared to existing\napproaches.", "AI": {"tldr": "The paper introduces a framework called Energy Policy for fast and efficient robotic manipulation tasks. It uses energy scores and an energy MLP to predict multimodal actions in real-time.", "motivation": "To address the need for high-frequency robotic manipulation tasks in resource-constrained systems, surpassing computational limitations of existing methods.", "method": "The Energy Policy leverages energy scores as the learning objective and an energy MLP for efficient multimodal action prediction in a straightforward architecture.", "result": "Energy Policy matches or outperforms current state-of-the-art methods, reduces computational overhead, and delivers superior performance on the MimicGen benchmark with faster inference speeds.", "conclusion": "The Energy Policy is a promising solution for real-world robotic tasks, offering high-speed and precision while maintaining computational efficiency."}}
{"id": "2510.12091", "pdf": "https://arxiv.org/pdf/2510.12091", "abs": "https://arxiv.org/abs/2510.12091", "authors": ["Lijie Ding", "Jan-Michael Carrillo", "Changwoo Do"], "title": "ToPolyAgent: AI Agents for Coarse-Grained Topological Polymer Simulations", "categories": ["cs.AI", "cond-mat.mtrl-sci", "cond-mat.soft"], "comment": "10 pages, 8 figures", "summary": "We introduce ToPolyAgent, a multi-agent AI framework for performing\ncoarse-grained molecular dynamics (MD) simulations of topological polymers\nthrough natural language instructions. By integrating large language models\n(LLMs) with domain-specific computational tools, ToPolyAgent supports both\ninteractive and autonomous simulation workflows across diverse polymer\narchitectures, including linear, ring, brush, and star polymers, as well as\ndendrimers. The system consists of four LLM-powered agents: a Config Agent for\ngenerating initial polymer-solvent configurations, a Simulation Agent for\nexecuting LAMMPS-based MD simulations and conformational analyses, a Report\nAgent for compiling markdown reports, and a Workflow Agent for streamlined\nautonomous operations. Interactive mode incorporates user feedback loops for\niterative refinements, while autonomous mode enables end-to-end task execution\nfrom detailed prompts. We demonstrate ToPolyAgent's versatility through case\nstudies involving diverse polymer architectures under varying solvent\ncondition, thermostats, and simulation lengths. Furthermore, we highlight its\npotential as a research assistant by directing it to investigate the effect of\ninteraction parameters on the linear polymer conformation, and the influence of\ngrafting density on the persistence length of the brush polymer. By coupling\nnatural language interfaces with rigorous simulation tools, ToPolyAgent lowers\nbarriers to complex computational workflows and advances AI-driven materials\ndiscovery in polymer science. It lays the foundation for autonomous and\nextensible multi-agent scientific research ecosystems.", "AI": {"tldr": "ToPolyAgent is a multi-agent AI framework integrating large language models and computational tools to perform molecular simulations of polymers via natural language commands.", "motivation": "To lower barriers in complex computational workflows and advance AI-driven discoveries in polymer science.", "method": "The framework uses four LLM-powered agents for configuration generation, simulation execution, report compilation, and workflow management in both interactive and autonomous modes.", "result": "Demonstrated versatility in case studies of various polymer architectures and conditions, showing its capability to explore polymer science parameters.", "conclusion": "ToPolyAgent facilitates research in polymer science, enabling autonomous multi-agent workflows and extensible scientific research ecosystems."}}
{"id": "2510.12616", "pdf": "https://arxiv.org/pdf/2510.12616", "abs": "https://arxiv.org/abs/2510.12616", "authors": ["Muhammad Ashfaq", "Ahmed R. Sadik", "Teerath Das", "Muhammad Waseem", "Niko Makitalo", "Tommi Mikkonen"], "title": "Runtime Composition in Dynamic System of Systems: A Systematic Review of Challenges, Solutions, Tools, and Evaluation Methods", "categories": ["cs.SE", "cs.MA"], "comment": null, "summary": "Context: Modern Systems of Systems (SoSs) increasingly operate in dynamic\nenvironments (e.g., smart cities, autonomous vehicles) where runtime\ncomposition -- the on-the-fly discovery, integration, and coordination of\nconstituent systems (CSs)--is crucial for adaptability. Despite growing\ninterest, the literature lacks a cohesive synthesis of runtime composition in\ndynamic SoSs. Objective: This study synthesizes research on runtime composition\nin dynamic SoSs and identifies core challenges, solution strategies, supporting\ntools, and evaluation methods. Methods: We conducted a Systematic Literature\nReview (SLR), screening 1,774 studies published between 2019 and 2024 and\nselecting 80 primary studies for thematic analysis (TA). Results: Challenges\nfall into four categories: modeling and analysis, resilient operations, system\norchestration, and heterogeneity of CSs. Solutions span seven areas:\nco-simulation and digital twins, semantic ontologies, integration frameworks,\nadaptive architectures, middleware, formal methods, and AI-driven resilience.\nService-oriented frameworks for composition and integration dominate tooling,\nwhile simulation platforms support evaluation. Interoperability across tools,\nlimited cross-toolchain workflows, and the absence of standardized benchmarks\nremain key gaps. Evaluation approaches include simulation-based,\nimplementation-driven, and human-centered studies, which have been applied in\ndomains such as smart cities, healthcare, defense, and industrial automation.\nConclusions: The synthesis reveals tensions, including autonomy versus\ncoordination, the modeling-reality gap, and socio-technical integration. It\ncalls for standardized evaluation metrics, scalable decentralized\narchitectures, and cross-domain frameworks. The analysis aims to guide\nresearchers and practitioners in developing and implementing dynamically\ncomposable SoSs.", "AI": {"tldr": "A Systematic Literature Review (SLR) explored runtime composition in dynamic systems of systems (SoSs), analyzing challenges, solutions, tools, and evaluations from literature spanning 2019-2024. The study emphasizes the need for standardized metrics and cross-domain frameworks.", "motivation": "The study aims to address the gaps in understanding runtime composition in dynamic Systems of Systems (SoSs), crucial for improving adaptability in dynamic environments like smart cities and autonomous vehicles.", "method": "A Systematic Literature Review (SLR) was conducted, analyzing 1,774 studies, with thematic analysis performed on 80 primary studies to synthesize findings on challenges, solutions, tools, and evaluation strategies.", "result": "Key challenges are modeling/analysis, resilient operations, system orchestration, and heterogeneity of constituent systems (CSs). Solutions include co-simulation tools, semantic ontologies, adaptive architectures, middleware, and AI-driven resilience. While currently service-oriented frameworks dominate tooling and simulations aid evaluation, key gaps include interoperability issues and lack of benchmarks.", "conclusion": "The study highlights the need for standardized evaluation metrics, scalable decentralized architectures, and cross-domain frameworks, offering guidance for researchers and practitioners toward improving dynamic systems of systems (SoSs)."}}
{"id": "2510.12114", "pdf": "https://arxiv.org/pdf/2510.12114", "abs": "https://arxiv.org/abs/2510.12114", "authors": ["Wenjie Li", "Xiangyi Wang", "Heng Guo", "Guangwei Gao", "Zhanyu Ma"], "title": "Self-Supervised Selective-Guided Diffusion Model for Old-Photo Face Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Old-photo face restoration poses significant challenges due to compounded\ndegradations such as breakage, fading, and severe blur. Existing pre-trained\ndiffusion-guided methods either rely on explicit degradation priors or global\nstatistical guidance, which struggle with localized artifacts or face color. We\npropose Self-Supervised Selective-Guided Diffusion (SSDiff), which leverages\npseudo-reference faces generated by a pre-trained diffusion model under weak\nguidance. These pseudo-labels exhibit structurally aligned contours and natural\ncolors, enabling region-specific restoration via staged supervision: structural\nguidance applied throughout the denoising process and color refinement in later\nsteps, aligned with the coarse-to-fine nature of diffusion. By incorporating\nface parsing maps and scratch masks, our method selectively restores breakage\nregions while avoiding identity mismatch. We further construct VintageFace, a\n300-image benchmark of real old face photos with varying degradation levels.\nSSDiff outperforms existing GAN-based and diffusion-based methods in perceptual\nquality, fidelity, and regional controllability. Code link:\nhttps://github.com/PRIS-CV/SSDiff.", "AI": {"tldr": "The paper presents Self-Supervised Selective-Guided Diffusion (SSDiff) for restoring old face photos with complex degradations, outperforming existing methods.", "motivation": "To address the challenges of restoring old face photos affected by various degradations like breakage, fading, and blur, which existing methods struggle to effectively handle.", "method": "The proposed SSDiff employs pseudo-reference faces generated by a pre-trained diffusion model under weak guidance for restoration. It uses staged supervision, leveraging structural guidance during denoising and color refinement in later steps, facilitated by face parsing maps and scratch masks for selective restoration.", "result": "SSDiff demonstrates superior performance in perceptual quality, fidelity, and regional controllability, restoring localized artifacts while preserving identity and color. The approach is validated on a newly created benchmark, VintageFace, consisting of 300 old face photos with different levels of degradation.", "conclusion": "SSDiff provides an effective pipeline for old photo restoration by deploying a self-supervised, region-specific, diffusion-guided method, offering significant improvement over existing approaches in handling complex degradations."}}
{"id": "2510.11926", "pdf": "https://arxiv.org/pdf/2510.11926", "abs": "https://arxiv.org/abs/2510.11926", "authors": ["Nayan Sanjay Bhatia", "Pranay Kocheta", "Russell Elliott", "Harikrishna S. Kuttivelil", "Katia Obraczka"], "title": "Indoor Localization using Compact, Telemetry-Agnostic, Transfer-Learning Enabled Decoder-Only Transformer", "categories": ["cs.LG", "cs.AI"], "comment": "11 pages, 12 Figures", "summary": "Indoor Wi-Fi positioning remains a challenging problem due to the high\nsensitivity of radio signals to environmental dynamics, channel propagation\ncharacteristics, and hardware heterogeneity. Conventional fingerprinting and\nmodel-based approaches typically require labor-intensive calibration and suffer\nrapid performance degradation when devices, channel or deployment conditions\nchange. In this paper, we introduce Locaris, a decoder-only large language\nmodel (LLM) for indoor localization. Locaris treats each access point (AP)\nmeasurement as a token, enabling the ingestion of raw Wi-Fi telemetry without\npre-processing. By fine-tuning its LLM on different Wi-Fi datasets, Locaris\nlearns a lightweight and generalizable mapping from raw signals directly to\ndevice location. Our experimental study comparing Locaris with state-of-the-art\nmethods consistently shows that Locaris matches or surpasses existing\ntechniques for various types of telemetry. Our results demonstrate that compact\nLLMs can serve as calibration-free regression models for indoor localization,\noffering scalable and robust cross-environment performance in heterogeneous\nWi-Fi deployments. Few-shot adaptation experiments, using only a handful of\ncalibration points per device, further show that Locaris maintains high\naccuracy when applied to previously unseen devices and deployment scenarios.\nThis yields sub-meter accuracy with just a few hundred samples, robust\nperformance under missing APs and supports any and all available telemetry. Our\nfindings highlight the practical viability of Locaris for indoor positioning in\nthe real-world scenarios, particularly in large-scale deployments where\nextensive calibration is infeasible.", "AI": {"tldr": "Locaris introduces a decoder-only large language model for indoor Wi-Fi positioning that directly maps raw Wi-Fi telemetry to device location, yielding scalable and robust performance across environments.", "motivation": "The motivation is to address challenges such as sensitivity of radio signals to environmental changes, dependence on labor-intensive calibration, and performance degradation under changing conditions in indoor Wi-Fi positioning.", "method": "The authors developed Locaris, a large language model fine-tuned on raw Wi-Fi telemetry data, which treats each access point measurement as a token and learns a generalizable mapping to device location without preprocessing.", "result": "Experimental studies demonstrate that Locaris surpasses existing methods in accuracy, achieves sub-meter accuracy with few-shot adaptation, and remains robust under challenging scenarios such as missing access points and unseen devices.", "conclusion": "Locaris offers practical indoor localization with minimal calibration, scalable usage in heterogeneous environments, and high accuracy, making it ideal for large-scale real-world deployments."}}
{"id": "2302.10359", "pdf": "https://arxiv.org/pdf/2302.10359", "abs": "https://arxiv.org/abs/2302.10359", "authors": ["Hossein Esfandiari", "Amin Karbasi", "Vahab Mirrokni", "Grigoris Velegkas", "Felix Zhou"], "title": "Replicable Clustering", "categories": ["cs.LG", "cs.DS", "stat.ML"], "comment": "to be published in NeurIPS 2023", "summary": "We design replicable algorithms in the context of statistical clustering\nunder the recently introduced notion of replicability from Impagliazzo et al.\n[2022]. According to this definition, a clustering algorithm is replicable if,\nwith high probability, its output induces the exact same partition of the\nsample space after two executions on different inputs drawn from the same\ndistribution, when its internal randomness is shared across the executions. We\npropose such algorithms for the statistical $k$-medians, statistical $k$-means,\nand statistical $k$-centers problems by utilizing approximation routines for\ntheir combinatorial counterparts in a black-box manner. In particular, we\ndemonstrate a replicable $O(1)$-approximation algorithm for statistical\nEuclidean $k$-medians ($k$-means) with $\\operatorname{poly}(d)$ sample\ncomplexity. We also describe an $O(1)$-approximation algorithm with an\nadditional $O(1)$-additive error for statistical Euclidean $k$-centers, albeit\nwith $\\exp(d)$ sample complexity. In addition, we provide experiments on\nsynthetic distributions in 2D using the $k$-means++ implementation from sklearn\nas a black-box that validate our theoretical results.", "AI": {"tldr": "The paper introduces replicable algorithms for statistical clustering, ensuring consistent outcomes under shared randomness for k-medians, k-means, and k-centers tasks.", "motivation": "The motivation is to address the replicability concern in statistical clustering algorithms, ensuring consistent partitioning of the sample space across executions.", "method": "The authors utilize approximation routines as black-boxes for statistical k-medians, k-means, and k-centers clustering problems, ensuring replicability within these models.", "result": "They present replicable O(1)-approximation algorithms for k-medians and k-means with poly(d) sample complexity, and extend the method to k-centers with exponential sample complexity.", "conclusion": "The study validates replicability through synthetic 2D experiments and theoretical models, showing promise in statistical clustering under shared randomness."}}
{"id": "2510.12036", "pdf": "https://arxiv.org/pdf/2510.12036", "abs": "https://arxiv.org/abs/2510.12036", "authors": ["Kemal Kurniawan", "Meladel Mistica", "Timothy Baldwin", "Jey Han Lau"], "title": "On the Interplay between Human Label Variation and Model Fairness", "categories": ["cs.CL"], "comment": "9 pages, 7 figures", "summary": "The impact of human label variation (HLV) on model fairness is an unexplored\ntopic. This paper examines the interplay by comparing training on majority-vote\nlabels with a range of HLV methods. Our experiments show that without explicit\ndebiasing, HLV training methods have a positive impact on fairness.", "AI": {"tldr": "This paper studies the effect of human label variation on model fairness, demonstrating HLV approaches improve fairness even without explicit debiasing.", "motivation": "The motivation is to address the unexplored relationship between human label variation and fairness in machine learning models.", "method": "The paper compares training on majority-vote labels against different human label variation methods.", "result": "Experiments revealed that using HLV methods positively impacted fairness without requiring debiasing.", "conclusion": "Human label variation methods can inherently enhance fairness in AI models without additional debiasing efforts."}}
{"id": "2510.12509", "pdf": "https://arxiv.org/pdf/2510.12509", "abs": "https://arxiv.org/abs/2510.12509", "authors": ["Gaoyuan Liu", "Bas Boom", "Naftali Slob", "Yuri Durodi\u00e9", "Ann Now\u00e9", "Bram Vanderborght"], "title": "Automated Behavior Planning for Fruit Tree Pruning via Redundant Robot Manipulators: Addressing the Behavior Planning Challenge", "categories": ["cs.RO"], "comment": null, "summary": "Pruning is an essential agricultural practice for orchards. Proper pruning\ncan promote healthier growth and optimize fruit production throughout the\norchard's lifespan. Robot manipulators have been developed as an automated\nsolution for this repetitive task, which typically requires seasonal labor with\nspecialized skills. While previous research has primarily focused on the\nchallenges of perception, the complexities of manipulation are often\noverlooked. These challenges involve planning and control in both joint and\nCartesian spaces to guide the end-effector through intricate, obstructive\nbranches. Our work addresses the behavior planning challenge for a robotic\npruning system, which entails a multi-level planning problem in environments\nwith complex collisions. In this paper, we formulate the planning problem for a\nhigh-dimensional robotic arm in a pruning scenario, investigate the system's\nintrinsic redundancies, and propose a comprehensive pruning workflow that\nintegrates perception, modeling, and holistic planning. In our experiments, we\ndemonstrate that more comprehensive planning methods can significantly enhance\nthe performance of the robotic manipulator. Finally, we implement the proposed\nworkflow on a real-world robot. As a result, this work complements previous\nefforts on robotic pruning and motivates future research and development in\nplanning for pruning applications.", "AI": {"tldr": "The paper explores behavior planning for robotic manipulator systems in orchard pruning, emphasizing improved planning methods for complex, obstructive environments.", "motivation": "Manual pruning in orchards is labor-intensive and requires specialized skills. Automating this process with robots has seen challenges primarily in planning and control for navigation through obstructive branches.", "method": "The authors formulated the planning problem for a high-dimensional robotic arm, analyzed system redundancies, and proposed an integrated workflow combining perception, modeling, and holistic planning.", "result": "Experiments showcased significant performance enhancements of the robotic manipulator using comprehensive planning methods. The workflow was also successfully implemented on a real-world robot.", "conclusion": "The study advances robotic pruning by addressing complex planning challenges, laying groundwork for future research, and complementing existing efforts in automated pruning systems."}}
{"id": "2510.12121", "pdf": "https://arxiv.org/pdf/2510.12121", "abs": "https://arxiv.org/abs/2510.12121", "authors": ["Rongzhi Zhang", "Liqin Ye", "Yuzhao Heng", "Xiang Chen", "Tong Yu", "Lingkai Kong", "Sudheer Chava", "Chao Zhang"], "title": "Precise Attribute Intensity Control in Large Language Models via Targeted Representation Editing", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Precise attribute intensity control--generating Large Language Model (LLM)\noutputs with specific, user-defined attribute intensities--is crucial for AI\nsystems adaptable to diverse user expectations. Current LLM alignment methods,\nhowever, typically provide only directional or open-ended guidance, failing to\nreliably achieve exact attribute intensities. We address this limitation with\nthree key designs: (1) reformulating precise attribute intensity control as a\ntarget-reaching problem, rather than simple maximization; (2) training a\nlightweight value function via temporal-difference learning to predict final\nattribute intensity scores from partial generations, thereby steering LLM\noutputs; and (3) employing gradient-based interventions on hidden\nrepresentations to navigate the model precisely towards specific attribute\nintensity targets. Our method enables fine-grained, continuous control over\nattribute intensities, moving beyond simple directional alignment. Experiments\non LLaMA-3.2-3b and Phi-4-mini confirm our method's ability to steer text\ngeneration to user-specified attribute intensities with high accuracy. Finally,\nwe demonstrate efficiency enhancements across three downstream tasks:\npreference data synthesis, Pareto frontier approximation and optimization, and\ndistillation of aligned behaviors for intervention-free inference. Our code is\navailable on https://github.com/Pre-Control/pre-control", "AI": {"tldr": "This paper introduces a novel method to achieve precise control over specific attribute intensities in LLM outputs, resolving challenges of current alignment methods.", "motivation": "To enable AI systems to meet diverse user expectations by achieving exact, user-specified attribute intensities in generated content.", "method": "The authors reformulate the control problem as target-reaching, utilize temporal-difference learning to train a lightweight value function predicting intensities, and perform gradient-based interventions to steer outputs.", "result": "The method achieves high accuracy in controlling attribute intensities on LLaMA-3.2-3b and Phi-4-mini models, enhancing efficiency across various downstream tasks.", "conclusion": "Precise control of attribute intensities is addressed successfully through innovative designs, advancing beyond traditional directional approaches in LLM alignment."}}
{"id": "2510.12119", "pdf": "https://arxiv.org/pdf/2510.12119", "abs": "https://arxiv.org/abs/2510.12119", "authors": ["Ziyuan Luo", "Yangyi Zhao", "Ka Chun Cheung", "Simon See", "Renjie Wan"], "title": "ImageSentinel: Protecting Visual Datasets from Unauthorized Retrieval-Augmented Image Generation", "categories": ["cs.CV"], "comment": "Accepted at NeurIPS 2025", "summary": "The widespread adoption of Retrieval-Augmented Image Generation (RAIG) has\nraised significant concerns about the unauthorized use of private image\ndatasets. While these systems have shown remarkable capabilities in enhancing\ngeneration quality through reference images, protecting visual datasets from\nunauthorized use in such systems remains a challenging problem. Traditional\ndigital watermarking approaches face limitations in RAIG systems, as the\ncomplex feature extraction and recombination processes fail to preserve\nwatermark signals during generation. To address these challenges, we propose\nImageSentinel, a novel framework for protecting visual datasets in RAIG. Our\nframework synthesizes sentinel images that maintain visual consistency with the\noriginal dataset. These sentinels enable protection verification through\nrandomly generated character sequences that serve as retrieval keys. To ensure\nseamless integration, we leverage vision-language models to generate the\nsentinel images. Experimental results demonstrate that ImageSentinel\neffectively detects unauthorized dataset usage while preserving generation\nquality for authorized applications. Code is available at\nhttps://github.com/luo-ziyuan/ImageSentinel.", "AI": {"tldr": "The paper introduces ImageSentinel, a system designed to safeguard visual datasets in Retrieval-Augmented Image Generation (RAIG) from unauthorized usage. It accomplishes this by creating sentinel images that let users verify unauthorized access.", "motivation": "The motivation for this paper stems from concerns about unauthorized exploitation of private visual datasets in RAIG systems, which traditional watermarking techniques fail to manage effectively due to RAIG's complex processing.", "method": "ImageSentinel synthesizes sentinel images using vision-language models, embedding unique character sequences to serve as retrieval keys for detecting unauthorized dataset usage.", "result": "The proposed framework successfully detects unauthorized use of datasets while ensuring high-quality image generation for authorized users.", "conclusion": "ImageSentinel offers a robust solution for protecting visual datasets in RAIG scenarios, advancing the field by addressing key limitations of prior watermarking methods."}}
{"id": "2510.11933", "pdf": "https://arxiv.org/pdf/2510.11933", "abs": "https://arxiv.org/abs/2510.11933", "authors": ["Hiroshi Nonaka", "Simon Ambrozak", "Sofia R. Miskala-Dinc", "Amedeo Ercole", "Aviva Prins"], "title": "Efficient Restarts in Non-Stationary Model-Free Reinforcement Learning", "categories": ["cs.LG"], "comment": "This paper contains 19 pages and 3 figures. To be presented at the\n  2nd Workshop on Aligning Reinforcement Learning Experimentalists and\n  Theorists (ARLET 2025) at NeurIPS 2025", "summary": "In this work, we propose three efficient restart paradigms for model-free\nnon-stationary reinforcement learning (RL). We identify two core issues with\nthe restart design of Mao et al. (2022)'s RestartQ-UCB algorithm: (1) complete\nforgetting, where all the information learned about an environment is lost\nafter a restart, and (2) scheduled restarts, in which restarts occur only at\npredefined timings, regardless of the incompatibility of the policy with the\ncurrent environment dynamics. We introduce three approaches, which we call\npartial, adaptive, and selective restarts to modify the algorithms RestartQ-UCB\nand RANDOMIZEDQ (Wang et al., 2025). We find near-optimal empirical performance\nin multiple different environments, decreasing dynamic regret by up to $91$%\nrelative to RestartQ-UCB.", "AI": {"tldr": "Proposes three new restart methods for improving model-free non-stationary reinforcement learning, addressing issues in prior approaches.", "motivation": "To address shortcomings in the RestartQ-UCB algorithm, including complete forgetting of learned information post-restart and rigid scheduled restarts insensitive to environmental changes.", "method": "Introduces three restart mechanisms: partial, adaptive, and selective restarts, to modify and improve RestartQ-UCB and RANDOMIZEDQ algorithms.", "result": "Demonstrates that the new methods achieve near-optimal performance in diverse environments, reducing dynamic regret by up to 91% relative to RestartQ-UCB.", "conclusion": "The proposed restart paradigms effectively overcome the limitations of previous approaches, significantly enhancing reinforcement learning in non-stationary settings."}}
{"id": "2402.13857", "pdf": "https://arxiv.org/pdf/2402.13857", "abs": "https://arxiv.org/abs/2402.13857", "authors": ["Alkis Kalavasis", "Amin Karbasi", "Kasper Green Larsen", "Grigoris Velegkas", "Felix Zhou"], "title": "Replicable Learning of Large-Margin Halfspaces", "categories": ["cs.LG", "cs.DS", "stat.ML"], "comment": "to be published in ICML 2024", "summary": "We provide efficient replicable algorithms for the problem of learning\nlarge-margin halfspaces. Our results improve upon the algorithms provided by\nImpagliazzo, Lei, Pitassi, and Sorrell [STOC, 2022]. We design the first\ndimension-independent replicable algorithms for this task which runs in\npolynomial time, is proper, and has strictly improved sample complexity\ncompared to the one achieved by Impagliazzo et al. [2022] with respect to all\nthe relevant parameters. Moreover, our first algorithm has sample complexity\nthat is optimal with respect to the accuracy parameter $\\epsilon$. We also\ndesign an SGD-based replicable algorithm that, in some parameters' regimes,\nachieves better sample and time complexity than our first algorithm. Departing\nfrom the requirement of polynomial time algorithms, using the\nDP-to-Replicability reduction of Bun, Gaboardi, Hopkins, Impagliazzo, Lei,\nPitassi, Sorrell, and Sivakumar [STOC, 2023], we show how to obtain a\nreplicable algorithm for large-margin halfspaces with improved sample\ncomplexity with respect to the margin parameter $\\tau$, but running time doubly\nexponential in $1/\\tau^2$ and worse sample complexity dependence on $\\epsilon$\nthan one of our previous algorithms. We then design an improved algorithm with\nbetter sample complexity than all three of our previous algorithms and running\ntime exponential in $1/\\tau^{2}$.", "AI": {"tldr": "The paper introduces efficient, replicable algorithms for learning large-margin halfspaces, surpassing prior methods by Impagliazzo et al. (2022). It offers algorithms with optimal or improved sample complexities and discusses trade-offs in running time and margin parameter dependency.", "motivation": "Existing algorithms for learning large-margin halfspaces suffer from inefficiencies in sample and time complexities. The aim is to design replicable algorithms that overcome these limitations, providing dimension-independent and optimized solutions.", "method": "The authors develop multiple replicable algorithms, including one polynomial-time, proper algorithm with optimal sample complexity concerning accuracy, and an SGD-based method with improved efficiency in certain cases. They also leverage a DP-to-Replicability reduction to achieve better sample complexity concerning the margin parameter, though with trade-offs in execution time.", "result": "The proposed algorithms demonstrate improved sample complexities concerning relevant parameters (accuracy and margin) compared to previous methods. They also provide better computational flexibility, such as SGD-based applicability. However, certain algorithms trade computational cost for optimization in specific parameters.", "conclusion": "The work successfully advances the state-of-the-art in replicable algorithms for learning large-margin halfspaces, offering significant improvements in sample complexity and computational efficiency, albeit with trade-offs in some cases."}}
{"id": "2510.12040", "pdf": "https://arxiv.org/pdf/2510.12040", "abs": "https://arxiv.org/abs/2510.12040", "authors": ["Sungmin Kang", "Yavuz Faruk Bakman", "Duygu Nur Yaldiz", "Baturalp Buyukates", "Salman Avestimehr"], "title": "Uncertainty Quantification for Hallucination Detection in Large Language Models: Foundations, Methodology, and Future Directions", "categories": ["cs.CL"], "comment": "24 pages, 3 figures, magazine", "summary": "The rapid advancement of large language models (LLMs) has transformed the\nlandscape of natural language processing, enabling breakthroughs across a wide\nrange of areas including question answering, machine translation, and text\nsummarization. Yet, their deployment in real-world applications has raised\nconcerns over reliability and trustworthiness, as LLMs remain prone to\nhallucinations that produce plausible but factually incorrect outputs.\nUncertainty quantification (UQ) has emerged as a central research direction to\naddress this issue, offering principled measures for assessing the\ntrustworthiness of model generations. We begin by introducing the foundations\nof UQ, from its formal definition to the traditional distinction between\nepistemic and aleatoric uncertainty, and then highlight how these concepts have\nbeen adapted to the context of LLMs. Building on this, we examine the role of\nUQ in hallucination detection, where quantifying uncertainty provides a\nmechanism for identifying unreliable generations and improving reliability. We\nsystematically categorize a wide spectrum of existing methods along multiple\ndimensions and present empirical results for several representative approaches.\nFinally, we discuss current limitations and outline promising future research\ndirections, providing a clearer picture of the current landscape of LLM UQ for\nhallucination detection.", "AI": {"tldr": "The paper explores the role of uncertainty quantification (UQ) in addressing reliability issues in large language models (LLMs), particularly in detecting hallucinations, and provides an overview of methods and their limitations.", "motivation": "The growing deployment of LLMs in real-world applications has raised concerns due to their tendency to produce factually incorrect outputs (hallucinations), stressing the need for measures to assess reliability and trustworthiness.", "method": "The paper introduces the theoretical foundations of uncertainty quantification, discusses its adaptation to LLMs, categorizes existing UQ methods for hallucination detection, and presents empirical evaluations of representative techniques.", "result": "The study systematically categorizes UQ methods, demonstrates their usage in detecting hallucinations, and provides empirical insights into the effectiveness of these approaches.", "conclusion": "UQ is crucial for identifying and mitigating hallucinations in LLMs. Despite progress, challenges remain, and the paper outlines potential future research directions to enhance reliability in LLM deployments."}}
{"id": "2510.12528", "pdf": "https://arxiv.org/pdf/2510.12528", "abs": "https://arxiv.org/abs/2510.12528", "authors": ["Muxing Huang", "Zibin Chen", "Weiliang Xu", "Zilan Li", "Yuanzhi Zhou", "Guoyuan Zhou", "Wenjing Chen", "Xinming Li"], "title": "Two-stream network-driven vision-based tactile sensor for object feature extraction and fusion perception", "categories": ["cs.RO", "physics.app-ph"], "comment": null, "summary": "Tactile perception is crucial for embodied intelligent robots to recognize\nobjects. Vision-based tactile sensors extract object physical attributes\nmultidimensionally using high spatial resolution; however, this process\ngenerates abundant redundant information. Furthermore, single-dimensional\nextraction, lacking effective fusion, fails to fully characterize object\nattributes. These challenges hinder the improvement of recognition accuracy. To\naddress this issue, this study introduces a two-stream network feature\nextraction and fusion perception strategy for vision-based tactile systems.\nThis strategy employs a distributed approach to extract internal and external\nobject features. It obtains depth map information through three-dimensional\nreconstruction while simultaneously acquiring hardness information by measuring\ncontact force data. After extracting features with a convolutional neural\nnetwork (CNN), weighted fusion is applied to create a more informative and\neffective feature representation. In standard tests on objects of varying\nshapes and hardness, the force prediction error is 0.06 N (within a 12 N\nrange). Hardness recognition accuracy reaches 98.0%, and shape recognition\naccuracy reaches 93.75%. With fusion algorithms, object recognition accuracy in\nactual grasping scenarios exceeds 98.5%. Focused on object physical attributes\nperception, this method enhances the artificial tactile system ability to\ntransition from perception to cognition, enabling its use in embodied\nperception applications.", "AI": {"tldr": "This paper addresses inefficiencies in vision-based tactile systems by proposing a two-stream network for feature extraction and fusion, significantly improving recognition accuracy in shape and hardness during object recognition and grasping.", "motivation": "To improve recognition accuracy in embodied intelligent robots by addressing redundancy in tactile data and lack of multidimensional feature fusion in vision-based tactile sensors.", "method": "A two-stream network extracts internal and external object features, combining 3D depth map information with contact force data. Features are processed using CNNs and weighted fusion to create comprehensive feature representations.", "result": "The proposed method achieves a force prediction error of 0.06 N, 98.0% accuracy in hardness recognition, 93.75% in shape recognition, and over 98.5% accuracy in actual grasping scenarios.", "conclusion": "The new method significantly improves the tactile system's ability to move from simple perception to higher-level cognition, making it more effective in embodied perception applications."}}
{"id": "2510.12171", "pdf": "https://arxiv.org/pdf/2510.12171", "abs": "https://arxiv.org/abs/2510.12171", "authors": ["Junkai Zhang", "Jingru Gan", "Xiaoxuan Wang", "Zian Jia", "Changquan Gu", "Jianpeng Chen", "Yanqiao Zhu", "Mingyu Derek Ma", "Dawei Zhou", "Ling Li", "Wei Wang"], "title": "MatSciBench: Benchmarking the Reasoning Ability of Large Language Models in Materials Science", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable abilities in\nscientific reasoning, yet their reasoning capabilities in materials science\nremain underexplored. To fill this gap, we introduce MatSciBench, a\ncomprehensive college-level benchmark comprising 1,340 problems that span the\nessential subdisciplines of materials science. MatSciBench features a\nstructured and fine-grained taxonomy that categorizes materials science\nquestions into 6 primary fields and 31 sub-fields, and includes a three-tier\ndifficulty classification based on the reasoning length required to solve each\nquestion. MatSciBench provides detailed reference solutions enabling precise\nerror analysis and incorporates multimodal reasoning through visual contexts in\nnumerous questions. Evaluations of leading models reveal that even the\nhighest-performing model, Gemini-2.5-Pro, achieves under 80% accuracy on\ncollege-level materials science questions, highlighting the complexity of\nMatSciBench. Our systematic analysis of different reasoning strategie--basic\nchain-of-thought, tool augmentation, and self-correction--demonstrates that no\nsingle method consistently excels across all scenarios. We further analyze\nperformance by difficulty level, examine trade-offs between efficiency and\naccuracy, highlight the challenges inherent in multimodal reasoning tasks,\nanalyze failure modes across LLMs and reasoning methods, and evaluate the\ninfluence of retrieval-augmented generation. MatSciBench thus establishes a\ncomprehensive and solid benchmark for assessing and driving improvements in the\nscientific reasoning capabilities of LLMs within the materials science domain.", "AI": {"tldr": "MatSciBench is a college-level benchmark with 1,340 materials science problems, assessing LLM reasoning through multimodal contexts and structured difficulty.", "motivation": "To address the unexplored reasoning capabilities of Large Language Models in the materials science domain.", "method": "Developing MatSciBench, categorizing problems by subfields, difficulty tiers, and incorporating multimodal challenges for evaluation with various reasoning strategies.", "result": "Leading LLMs, like Gemini-2.5-Pro, struggle with under 80% accuracy, emphasizing the complexity of reasoning in this benchmark.", "conclusion": "MatSciBench sets a robust standard for assessing and advancing LLMs' scientific reasoning in materials science."}}
{"id": "2510.12123", "pdf": "https://arxiv.org/pdf/2510.12123", "abs": "https://arxiv.org/abs/2510.12123", "authors": ["David Parra", "Felipe Gutierrez-Barragan", "Trevor Seets", "Andreas Velten"], "title": "Hardware-aware Coding Function Design for Compressive Single-Photon 3D Cameras", "categories": ["cs.CV"], "comment": "IEEE TPAMI Special Issue", "summary": "Single-photon cameras are becoming increasingly popular in time-of-flight 3D\nimaging because they can time-tag individual photons with extreme resolution.\nHowever, their performance is susceptible to hardware limitations, such as\nsystem bandwidth, maximum laser power, sensor data rates, and in-sensor memory\nand compute resources. Compressive histograms were recently introduced as a\nsolution to the challenge of data rates through an online in-sensor compression\nof photon timestamp data. Although compressive histograms work within limited\nin-sensor memory and computational resources, they underperform when subjected\nto real-world illumination hardware constraints. To address this, we present a\nconstrained optimization approach for designing practical coding functions for\ncompressive single-photon 3D imaging. Using gradient descent, we jointly\noptimize an illumination and coding matrix (i.e., the coding functions) that\nadheres to hardware constraints. We show through extensive simulations that our\ncoding functions consistently outperform traditional coding designs under both\nbandwidth and peak power constraints. This advantage is particularly pronounced\nin systems constrained by peak power. Finally, we show that our approach adapts\nto arbitrary parameterized impulse responses by evaluating it on a real-world\nsystem with a non-ideal impulse response function.", "AI": {"tldr": "The paper addresses limitations of single-photon cameras in 3D imaging and proposes a constrained optimization approach to improve coding functions under hardware constraints.", "motivation": "Single-photon cameras face challenges due to hardware limitations, such as bandwidth and memory, which restrict their performance in real-world applications.", "method": "A constrained optimization approach is developed using gradient descent to optimize illumination and coding matrices while adhering to hardware constraints.", "result": "The proposed coding functions outperform traditional designs under bandwidth and peak power constraints, especially in systems restricted by peak power. It also adapts effectively to real-world non-ideal impulse responses.", "conclusion": "The work demonstrates a practical and effective solution to enhance the performance of compressive single-photon imaging within realistic hardware limitations."}}
{"id": "2510.11942", "pdf": "https://arxiv.org/pdf/2510.11942", "abs": "https://arxiv.org/abs/2510.11942", "authors": ["Tomaso Poggio"], "title": "On efficiently computable functions, deep networks and sparse compositionality", "categories": ["cs.LG"], "comment": null, "summary": "We show that \\emph{efficient Turing computability} at any fixed input/output\nprecision implies the existence of \\emph{compositionally sparse}\n(bounded-fan-in, polynomial-size) DAG representations and of corresponding\nneural approximants achieving the target precision. Concretely: if\n$f:[0,1]^d\\to\\R^m$ is computable in time polynomial in the bit-depths, then for\nevery pair of precisions $(n,m_{\\mathrm{out}})$ there exists a bounded-fan-in\nBoolean circuit of size and depth $\\poly(n+m_{\\mathrm{out}})$ computing the\ndiscretized map; replacing each gate by a constant-size neural emulator yields\na deep network of size/depth $\\poly(n+m_{\\mathrm{out}})$ that achieves accuracy\n$\\varepsilon=2^{-m_{\\mathrm{out}}}$. We also relate these constructions to\ncompositional approximation rates\n\\cite{MhaskarPoggio2016b,poggio_deep_shallow_2017,Poggio2017,Poggio2023HowDS}\nand to optimization viewed as hierarchical search over sparse structures.", "AI": {"tldr": "The paper connects efficient Turing computability with compositional DAG representations and bounded-fan-in neural networks for approximate computation of functions, ensuring polynomial size and depth.", "motivation": "To establish a fundamental relationship between efficient Turing computability and sparse neural network structures for reliable function approximation at specified precision levels.", "method": "Analyzes the properties of polynomial-time computable functions, demonstrates their representation using Boolean circuits, and constructs equivalent neural networks using bounded-fan-in deep architectures.", "result": "Efficient Turing computability implies a bounded-fan-in Boolean circuit with polynomial size and depth, which translates into neural networks achieving high accuracy (precision scaled as $2^{-m_{\\mathrm{out}}}$).", "conclusion": "Efficiently computable functions can be represented and approximated using structured, sparse neural networks, bridging computational theory with modern neural approximations."}}
{"id": "2510.11847", "pdf": "https://arxiv.org/pdf/2510.11847", "abs": "https://arxiv.org/abs/2510.11847", "authors": ["Sam Hawke", "Eric Zhang", "Jiawen Chen", "Didong Li"], "title": "Contrastive Dimension Reduction: A Systematic Review", "categories": ["stat.ME", "math.ST", "stat.CO", "stat.ML", "stat.TH", "G.3; I.5.1"], "comment": null, "summary": "Contrastive dimension reduction (CDR) methods aim to extract signal unique to\nor enriched in a treatment (foreground) group relative to a control\n(background) group. This setting arises in many scientific domains, such as\ngenomics, imaging, and time series analysis, where traditional dimension\nreduction techniques such as Principal Component Analysis (PCA) may fail to\nisolate the signal of interest. In this review, we provide a systematic\noverview of existing CDR methods. We propose a pipeline for analyzing\ncase-control studies together with a taxonomy of CDR methods based on their\nassumptions, objectives, and mathematical formulations, unifying disparate\napproaches under a shared conceptual framework. We highlight key applications\nand challenges in existing CDR methods, and identify open questions and future\ndirections. By providing a clear framework for CDR and its applications, we aim\nto facilitate broader adoption and motivate further developments in this\nemerging field.", "AI": {"tldr": "This paper reviews contrastive dimension reduction (CDR) methods, addressing their role in isolating unique signals in treatment versus control groups, and proposes a unified framework and pipeline for case-control studies.", "motivation": "Traditional dimension reduction techniques like PCA often fail to identify signals unique to treatment groups compared to control groups in scientific domains such as genomics, imaging, and time series.", "method": "The paper categorizes CDR methods based on assumptions, objectives, and mathematical structures, suggesting a unified conceptual framework and analysis pipeline for case-control studies.", "result": "The paper systematically reviews existing CDR methods, highlights their applications and challenges, and identifies avenues for future research.", "conclusion": "It offers a shared framework to facilitate broader use and further development of CDR methods, promoting their application across scientific domains."}}
{"id": "2510.12041", "pdf": "https://arxiv.org/pdf/2510.12041", "abs": "https://arxiv.org/abs/2510.12041", "authors": ["Ruibo Chen", "Jiacheng Pan", "Heng Huang", "Zhenheng Yang"], "title": "Improving Text-to-Image Generation with Input-Side Inference-Time Scaling", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in text-to-image (T2I) generation have achieved impressive\nresults, yet existing models often struggle with simple or underspecified\nprompts, leading to suboptimal image-text alignment, aesthetics, and quality.\nWe propose a prompt rewriting framework that leverages large language models\n(LLMs) to refine user inputs before feeding them into T2I backbones. Our\napproach introduces a carefully designed reward system and an iterative direct\npreference optimization (DPO) training pipeline, enabling the rewriter to\nenhance prompts without requiring supervised fine-tuning data. We evaluate our\nmethod across diverse T2I models and benchmarks. Results show that our prompt\nrewriter consistently improves image-text alignment, visual quality, and\naesthetics, outperforming strong baselines. Furthermore, we demonstrate strong\ntransferability by showing that a prompt rewriter trained on one T2I backbone\ngeneralizes effectively to others without needing to be retrained. We also\nsystematically study scalability, evaluating how performance gains scale with\nthe capacity of the large LLM used as the rewriter. These findings highlight\nthat prompt rewriting is an effective, scalable, and practical model-agnostic\nstrategy for improving T2I systems. We plan to release the code and trained\nprompt rewriters soon.", "AI": {"tldr": "The paper introduces a prompt rewriting framework using large language models to improve text-to-image (T2I) generation, achieving better text-image alignment, visual quality, and aesthetics.", "motivation": "Existing T2I models often fail with simple or underspecified prompts, leading to poor image-text alignment and low-quality outputs.", "method": "The framework refines user input prompts using a reward system and an iterative direct preference optimization (DPO) training pipeline, without requiring supervised fine-tuning data. Evaluations were conducted across various T2I models and included scalability analyses.", "result": "The proposed prompt rewriter greatly enhances image-text alignment, visual quality, and aesthetics, surpassing strong baselines. The rewriter also generalizes well across different T2I backbones without retraining.", "conclusion": "The prompt rewriting approach is a practical and scalable strategy that significantly improves T2I generation while remaining model-agnostic. Code and models will be released for further adoption."}}
{"id": "2510.12611", "pdf": "https://arxiv.org/pdf/2510.12611", "abs": "https://arxiv.org/abs/2510.12611", "authors": ["Lukas Pries", "Markus Ryll"], "title": "Learning Robust Agile Flight Control with Stability Guarantees", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "In the evolving landscape of high-speed agile quadrotor flight, achieving\nprecise trajectory tracking at the platform's operational limits is paramount.\nControllers must handle actuator constraints, exhibit robustness to\ndisturbances, and remain computationally efficient for safety-critical\napplications. In this work, we present a novel neural-augmented feedback\ncontroller for agile flight control. The controller addresses individual\nlimitations of existing state-of-the-art control paradigms and unifies their\nstrengths. We demonstrate the controller's capabilities, including the accurate\ntracking of highly aggressive trajectories that surpass the feasibility of the\nactuators. Notably, the controller provides universal stability guarantees,\nenhancing its robustness and tracking performance even in exceedingly\ndisturbance-prone settings. Its nonlinear feedback structure is highly\nefficient enabling fast computation at high update rates. Moreover, the\nlearning process in simulation is both fast and stable, and the controller's\ninherent robustness allows direct deployment to real-world platforms without\nthe need for training augmentations or fine-tuning.", "AI": {"tldr": "Researchers propose a neural-augmented feedback controller for quadrotors, demonstrating robust, efficient trajectory tracking under disturbances and actuator constraints.", "motivation": "Improving trajectory tracking and robustness in high-speed agile quadrotor flight systems under operational constraints and disturbances.", "method": "A novel neural-augmented feedback controller combines strengths of existing control paradigms and learns robustly in simulated environments for real-world deployment.", "result": "The controller achieves accurate tracking of aggressive trajectories, ensures universal stability, and enables fast computations without requiring additional training or fine-tuning.", "conclusion": "The proposed controller advances agile quadrotor flight by integrating robustness, efficient computation, and direct real-world applicability."}}
{"id": "2510.12178", "pdf": "https://arxiv.org/pdf/2510.12178", "abs": "https://arxiv.org/abs/2510.12178", "authors": ["Abdulhady Abas Abdullah", "Arkaitz Zubiaga", "Seyedali Mirjalili", "Amir H. Gandomi", "Fatemeh Daneshfar", "Mohammadsadra Amini", "Alan Salam Mohammed", "Hadi Veisi"], "title": "Evolution of meta's llama models and parameter-efficient fine-tuning of large language models: a survey", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "This review surveys the rapid evolution of Meta AI's LLaMA (Large Language\nModel Meta AI) series - from LLaMA 1 through LLaMA 4 and the specialized\nparameter-efficient fine-tuning (PEFT) methods developed for these models. We\nfirst describe the LLaMA family of foundation models (7B-65B to 288B\nparameters), their architectures (including native multimodal and\nMixtureof-Experts variants), and key performance characteristics. We then\ndescribe and discuss the concept of PEFT, which adapts large pre-trained models\nby updating only a small subset of parameters, and review five PEFT methods\nthat have been applied to LLaMA: LoRA (Low-Rank Adaptation), LLaMA-Adapter V1\nand V2, LLaMA-Excitor, and QLoRA (Quantized LoRA). We discuss each method's\nmechanism, parameter savings, and example application to LLaMA (e.g.,\ninstruction tuning, multimodal tasks). We provide structured discussion and\nanalysis of model and adapter architectures, parameter counts, and benchmark\nresults (including examples where fine-tuned LLaMA models outperform larger\nbaselines). Finally, we examine real-world use cases where LLaMA-based models\nand PEFT have been successfully applied (e.g., legal and medical domains), and\nwe discuss ongoing challenges and future research directions (such as scaling\nto even larger contexts and improving robustness). This survey paper provides a\none-stop resource for ML researchers and practitioners interested in LLaMA\nmodels and efficient fine-tuning strategies.", "AI": {"tldr": "This paper reviews Meta AI's LLaMA models and their evolution, detailing architectures, PEFT methods, performance, and real-world applications.", "motivation": "The motivation is to explore LLaMA models' progression and PEFT methods for efficient fine-tuning of large language models in real-world scenarios.", "method": "The paper surveys LLaMA model architectures and PEFT techniques (LoRA, LLaMA-Adapter V1/V2, LLaMA-Excitor, QLoRA), analyzing their mechanisms, benchmarks, and applications.", "result": "PEFT techniques like LoRA or QLoRA achieve parameter savings and enable fine-tuned LLaMA models to outperform larger baselines in tasks like instruction tuning and multimodal use cases.", "conclusion": "LLaMA and PEFT methodologies provide scalable solutions for specialized domains like legal and medical industries. Challenges and future directions involve scaling context and improving robustness."}}
{"id": "2510.12380", "pdf": "https://arxiv.org/pdf/2510.12380", "abs": "https://arxiv.org/abs/2510.12380", "authors": ["Vibhoothi Vibhoothi", "Julien Zouein", "Shanker Shreejith", "Jean-Baptiste Kempf", "Anil Kokaram"], "title": "An Empirical Study of Reducing AV1 Decoder Complexity and Energy Consumption via Encoder Parameter Tuning", "categories": ["eess.IV", "cs.MM", "cs.SE"], "comment": "Accepted Camera-Ready paper for PCS 2025, 5 Pages", "summary": "The widespread adoption of advanced video codecs such as AV1 is often\nhindered by their high decoding complexity, posing a challenge for\nbattery-constrained devices. While encoders can be configured to produce\nbitstreams that are decoder-friendly, estimating the decoding complexity and\nenergy overhead for a given video is non-trivial. In this study, we\nsystematically analyse the impact of disabling various coding tools and\nadjusting coding parameters in two AV1 encoders, libaom-av1 and SVT-AV1. Using\nsystem-level energy measurement tools like RAPL (Running Average Power Limit),\nIntel SoC Watch (integrated with VTune profiler), we quantify the resulting\ntrade-offs between decoding complexity, energy consumption, and compression\nefficiency for decoding a bitstream. Our results demonstrate that specific\nencoder configurations can substantially reduce decoding complexity with\nminimal perceptual quality degradation. For libaom-av1, disabling CDEF, an\nin-loop filter gives us a mean reduction in decoding cycles by 10%. For\nSVT-AV1, using the in-built, fast-decode=2 preset achieves a more substantial\n24% reduction in decoding cycles. These findings provide strategies for content\nproviders to lower the energy footprint of AV1 video streaming.", "AI": {"tldr": "This paper addresses the decoding complexity of AV1, proposing strategies for energy-efficient video streaming by tweaking encoder configurations.", "motivation": "To reduce the energy consumption and decoding complexity of AV1 video codecs for battery-constrained devices.", "method": "Systematic evaluation of coding tools and parameters in two AV1 encoders (libaom-av1 and SVT-AV1) using energy measurement tools such as RAPL, Intel SoC Watch, and VTune profiler.", "result": "Disabling certain tools in libaom-av1 (e.g., CDEF) reduces decoding cycles by 10%, and using fast-decode=2 preset in SVT-AV1 achieves a 24% reduction in decoding cycles.", "conclusion": "Specific encoder configurations can significantly lower AV1 decoding complexity and energy consumption with minimal perceptual quality loss, aiding energy-efficient video streaming strategies."}}
{"id": "2510.12126", "pdf": "https://arxiv.org/pdf/2510.12126", "abs": "https://arxiv.org/abs/2510.12126", "authors": ["Zhenxin Lei", "Zhangwei Gao", "Changyao Tian", "Erfei Cui", "Guanzhou Chen", "Danni Yang", "Yuchen Duan", "Zhaokai Wang", "Wenhao Li", "Weiyun Wang", "Xiangyu Zhao", "Jiayi Ji", "Yu Qiao", "Wenhai Wang", "Gen Luo"], "title": "MetaCaptioner: Towards Generalist Visual Captioning with Open-source Suites", "categories": ["cs.CV"], "comment": null, "summary": "Generalist visual captioning goes beyond a simple appearance description\ntask, but requires integrating a series of visual cues into a caption and\nhandling various visual domains. In this task, current open-source models\npresent a large performance gap with commercial ones, which limits various\napplications such as data synthesis. To bridge the gap, this paper proposes\nCapFlow, a novel multi-agent collaboration workflow. CapFlow demonstrates for\nthe first time that, by capitalizing on open-source models, it is possible to\nachieve caption quality on par with GPT-4.1 in various domains with an 89.5%\nreduction in costs. By leveraging CapFlow as the data synthesizer, we produce\nhigh-quality visual captions from image and video domains at scale, and obtain\na generalist visual captioner via fine-tuning, namely MetaCaptioner. Through\nextensive experiments, we show that MetaCaptioner not only achieves comparable\ncaptioning capabilities with commercial models but also reaches top-tier\nmultimodal performance in the open-source community. We hope CapFlow and\nMetaCaptioner can benefit future multimodal research by providing a strong and\ncost-effective visual captioning solution.", "AI": {"tldr": "The paper introduces CapFlow, a model that uses multi-agent collaboration to achieve high-quality visual captions comparable to GPT-4.1 and leads to a generalist visual captioner, MetaCaptioner.", "motivation": "The motivation is to address the performance gap between open-source and commercial visual captioning models, enabling cost-efficient, high-quality solutions for multimodal research and applications.", "method": "The authors propose CapFlow, a multi-agent collaboration workflow leveraging open-source models to generate high-quality captions efficiently. They use CapFlow to fine-tune a generalist captioner named MetaCaptioner.", "result": "CapFlow achieves captioning quality comparable to GPT-4.1 across domains with an 89.5% reduction in costs. MetaCaptioner demonstrates high-quality captioning and top-tier multimodal performance in the open-source community.", "conclusion": "CapFlow and MetaCaptioner can drive forward multimodal research, providing strong and cost-effective solutions for visual captioning across diverse domains."}}
{"id": "2510.11953", "pdf": "https://arxiv.org/pdf/2510.11953", "abs": "https://arxiv.org/abs/2510.11953", "authors": ["Quentin Fruytier", "Akshay Malhotra", "Shahab Hamidi-Rad", "Aditya Sant", "Aryan Mokhtari", "Sujay Sanghavi"], "title": "Sculpting Latent Spaces With MMD: Disentanglement With Programmable Priors", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Learning disentangled representations, where distinct factors of variation\nare captured by independent latent variables, is a central goal in machine\nlearning. The dominant approach has been the Variational Autoencoder (VAE)\nframework, which uses a Kullback-Leibler (KL) divergence penalty to encourage\nthe latent space to match a factorized Gaussian prior. In this work, however,\nwe provide direct evidence that this KL-based regularizer is an unreliable\nmechanism, consistently failing to enforce the target distribution on the\naggregate posterior. We validate this and quantify the resulting entanglement\nusing our novel, unsupervised Latent Predictability Score (LPS). To address\nthis failure, we introduce the Programmable Prior Framework, a method built on\nthe Maximum Mean Discrepancy (MMD). Our framework allows practitioners to\nexplicitly sculpt the latent space, achieving state-of-the-art mutual\nindependence on complex datasets like CIFAR-10 and Tiny ImageNet without the\ncommon reconstruction trade-off. Furthermore, we demonstrate how this\nprogrammability can be used to engineer sophisticated priors that improve\nalignment with semantically meaningful features. Ultimately, our work provides\na foundational tool for representation engineering, opening new avenues for\nmodel identifiability and causal reasoning.", "AI": {"tldr": "This paper challenges the effectiveness of KL divergence in Variational Autoencoders (VAEs) for achieving disentangled representations and introduces a new framework based on MMD for superior latent space control.", "motivation": "Disentangled representations are integral to machine learning, yet VAEs struggle to enforce independent latent variables with current KL-based regularizers.", "method": "The authors propose the Programmable Prior Framework using Maximum Mean Discrepancy (MMD), which enables explicit latent space sculpting and includes the novel Latent Predictability Score (LPS) for evaluation.", "result": "The framework achieves state-of-the-art mutual independence on datasets like CIFAR-10 and Tiny ImageNet without sacrificing reconstruction quality.", "conclusion": "This new method empowers practitioners to design latent spaces with greater control and opens pathways for deeper insights into model identifiability and causal reasoning."}}
{"id": "2510.12044", "pdf": "https://arxiv.org/pdf/2510.12044", "abs": "https://arxiv.org/abs/2510.12044", "authors": ["Yukun Zhang", "Qi Dong"], "title": "Hierarchical Alignment: Surgical Fine-Tuning via Functional Layer Specialization in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing alignment techniques for Large Language Models (LLMs), such as\nDirect Preference Optimization (DPO), typically treat the model as a monolithic\nentity, applying uniform optimization pressure across all layers. This approach\noverlooks the functional specialization within the Transformer architecture,\nwhere different layers are known to handle distinct tasks from syntax to\nabstract reasoning. In this paper, we challenge this one-size-fits-all paradigm\nby introducing Hierarchical Alignment, a novel method that applies targeted DPO\nto distinct functional blocks of a model's layers: local (syntax), intermediate\n(logic), and global (factuality). Through a series of controlled experiments on\nstate-of-the-art models like Llama-3.1-8B and Qwen1.5-7B using LoRA for\nsurgical fine-tuning, our results, evaluated by a powerful LLM-as-Judge,\ndemonstrate significant and predictable improvements. Specifically, aligning\nthe local layers (Local-Align) enhances grammatical fluency. More importantly,\naligning the global layers (Global-Align) not only improves factual consistency\nas hypothesized but also proves to be the most effective strategy for enhancing\nlogical coherence, outperforming all baselines. Critically, all hierarchical\nstrategies successfully avoid the \"alignment tax\" observed in standard DPO,\nwhere gains in fluency come at the cost of degraded logical reasoning. These\nfindings establish a more resource-efficient, controllable, and interpretable\npath for model alignment, highlighting the immense potential of shifting from\nmonolithic optimization to structure-aware surgical fine-tuning to build more\nadvanced and reliable LLMs.", "AI": {"tldr": "Large Language Models (LLMs) alignment techniques are improved using Hierarchical Alignment, targeting specific functional blocks of their layers. This approach avoids the \"alignment tax,\" enhancing grammatical fluency, factual consistency, and logical coherence simultaneously.", "motivation": "Current alignment techniques for LLMs disregard the specialized roles of different Transformer layers, leading to less effective optimization. The paper aims to address this limitation through targeted, layer-specific adjustments.", "method": "The study introduces Hierarchical Alignment, which applies targeted Direct Preference Optimization (DPO) to separate functional blocks of a model's layers. LoRA-based fine-tuning on syntax, logic, and factuality is employed, evaluated with a powerful LLM-as-Judge.", "result": "Experiments show that aligning specific layers (local for grammar, global for logic) results in significant improvements. Notably, global layer alignment enhances logical coherence and factual consistency, surpassing baseline methods.", "conclusion": "The proposed Hierarchical Alignment method allows resource-efficient and interpretable LLM optimization, shifting from monolithic models to structure-aware fine-tuning to improve fluency, logic, and factual accuracy without trade-offs."}}
{"id": "2510.12630", "pdf": "https://arxiv.org/pdf/2510.12630", "abs": "https://arxiv.org/abs/2510.12630", "authors": ["Ajith Anil Meera", "Abian Torres", "Pablo Lanillos"], "title": "Designing Tools with Control Confidence", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Prehistoric humans invented stone tools for specialized tasks by not just\nmaximizing the tool's immediate goal-completion accuracy, but also increasing\ntheir confidence in the tool for later use under similar settings. This factor\ncontributed to the increased robustness of the tool, i.e., the least\nperformance deviations under environmental uncertainties. However, the current\nautonomous tool design frameworks solely rely on performance optimization,\nwithout considering the agent's confidence in tool use for repeated use. Here,\nwe take a step towards filling this gap by i) defining an optimization\nframework for task-conditioned autonomous hand tool design for robots, where\nii) we introduce a neuro-inspired control confidence term into the optimization\nroutine that helps the agent to design tools with higher robustness. Through\nrigorous simulations using a robotic arm, we show that tools designed with\ncontrol confidence as the objective function are more robust to environmental\nuncertainties during tool use than a pure accuracy-driven objective. We further\nshow that adding control confidence to the objective function for tool design\nprovides a balance between the robustness and goal accuracy of the designed\ntools under control perturbations. Finally, we show that our CMAES-based\nevolutionary optimization strategy for autonomous tool design outperforms other\nstate-of-the-art optimizers by designing the optimal tool within the fewest\niterations. Code: https://github.com/ajitham123/Tool_design_control_confidence.", "AI": {"tldr": "This paper introduces a framework for autonomous robotic tool design that incorporates 'control confidence' to enhance robustness to environmental uncertainties.", "motivation": "Current autonomous tool design frameworks focus only on immediate performance optimization, ignoring repeated-use confidence, leading to less robust tools.", "method": "The authors propose an optimization framework incorporating a neuro-inspired control confidence term into a CMAES-based evolutionary strategy for task-conditioned tool design.", "result": "Simulations show tools designed with control confidence exhibit better robustness to environmental uncertainties and strike a balance between robustness and accuracy.", "conclusion": "Integrating control confidence into autonomous tool design improves robustness and efficiency, outperforming state-of-the-art optimization methods."}}
{"id": "2510.12194", "pdf": "https://arxiv.org/pdf/2510.12194", "abs": "https://arxiv.org/abs/2510.12194", "authors": ["Linyi Yang", "Yixuan Weng"], "title": "ResearStudio: A Human-Intervenable Framework for Building Controllable Deep-Research Agents", "categories": ["cs.AI"], "comment": "EMNLP 2025 Demo, Oral", "summary": "Current deep-research agents run in a ''fire-and-forget'' mode: once started,\nthey give users no way to fix errors or add expert knowledge during execution.\nWe present ResearStudio, the first open-source framework that places real-time\nhuman control at its core. The system follows a Collaborative Workshop design.\nA hierarchical Planner-Executor writes every step to a live\n''plan-as-document,'' a fast communication layer streams each action, file\nchange, and tool call to a web interface. At any moment, the user can pause the\nrun, edit the plan or code, run custom commands, and resume -- switching\nsmoothly between AI-led, human-assisted and human-led, AI-assisted modes. In\nfully autonomous mode, ResearStudio achieves state-of-the-art results on the\nGAIA benchmark, surpassing systems like OpenAI's DeepResearch and Manus. These\nresults show that strong automated performance and fine-grained human control\ncan coexist. The full code, protocol, and evaluation scripts are available at\nhttps://github.com/ResearAI/ResearStudio. We will continue to update the\nrepository to encourage further work on safe and controllable research agents.\nOur live demo is publicly accessible at http://ai-researcher.net:3000/. We\nsupport the development of DeepScientist, which can be accessed at\nhttps://github.com/ResearAI/DeepScientist.", "AI": {"tldr": "ResearStudio is an open-source research agent framework allowing real-time human intervention and achieving state-of-the-art results on the GAIA benchmark.", "motivation": "To address the lack of real-time human control in existing deep-research agents, which traditionally operate without user corrections or integration of expert knowledge during execution.", "method": "Designed a Collaborative Workshop framework with a hierarchical Planner-Executor system that logs actions in a live plan document and streams updates to a web interface. Users can intervene, edit, and resume smoothly while toggling between various levels of AI and human control.", "result": "ResearStudio achieves state-of-the-art performance on the GAIA benchmark, surpassing existing systems like OpenAI's DeepResearch and Manus.", "conclusion": "ResearStudio demonstrates that robust automation and precise human control can coexist, promoting safer and more controllable research agents. Open-source availability encourages further development in this field."}}
{"id": "2510.12132", "pdf": "https://arxiv.org/pdf/2510.12132", "abs": "https://arxiv.org/abs/2510.12132", "authors": ["Xiao Yang", "Jiyao Wang"], "title": "FedHUG: Federated Heterogeneous Unsupervised Generalization for Remote Physiological Measurements", "categories": ["cs.CV"], "comment": null, "summary": "Remote physiological measurement gained wide attention, while it requires\ncollecting users' privacy-sensitive information, and existing contactless\nmeasurements still rely on labeled client data. This presents challenges when\nwe want to further update real-world deployed models with numerous user data\nlacking labels. To resolve these challenges, we instantiate a new protocol\ncalled Federated Unsupervised Domain Generalization (FUDG) in this work.\nSubsequently, the \\textbf{Fed}erated \\textbf{H}eterogeneous\n\\textbf{U}nsupervised \\textbf{G}eneralization (\\textbf{FedHUG}) framework is\nproposed and consists of: (1) Minimal Bias Aggregation module dynamically\nadjusts aggregation weights based on prior-driven bias evaluation to cope with\nheterogeneous non-IID features from multiple domains. (2) The Global\nDistribution-aware Learning Controller parameterizes the label distribution and\ndynamically manipulates client-specific training strategies, thereby mitigating\nthe server-client label distribution skew and long-tail issue. The proposal\nshows superior performance across state-of-the-art techniques in estimation\nwith either RGB video or mmWave radar. The code will be released.", "AI": {"tldr": "The paper introduces 'Federated Unsupervised Domain Generalization' and a corresponding 'FedHUG' framework for improving remote physiological measurements without labeled data.", "motivation": "Enable contactless remote physiological measurements while addressing privacy concerns and reliance on labeled client data, especially for real-world model updates.", "method": "The 'FedHUG' framework integrates a Minimal Bias Aggregation module to handle non-IID data and a Global Distribution-aware Learning Controller to address label distribution skew and long-tail issues.", "result": "FedHUG outperformed existing techniques in remote physiological estimation using both RGB video and mmWave radar.", "conclusion": "The proposed framework demonstrates significant performance improvements in remote physiological measurement and advances federated unsupervised learning techniques; the implementation will be open-sourced."}}
{"id": "2510.11955", "pdf": "https://arxiv.org/pdf/2510.11955", "abs": "https://arxiv.org/abs/2510.11955", "authors": ["Arip Asadulaev", "Semyon Semenov", "Abduragim Shtanchaev", "Eric Moulines", "Fakhri Karray", "Martin Takac"], "title": "Y-shaped Generative Flows", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Modern continuous-time generative models often induce V-shaped transport:\neach sample travels independently along nearly straight trajectories from prior\nto data, overlooking shared structure. We introduce Y-shaped generative flows,\nwhich move probability mass together along shared pathways before branching to\ntarget-specific endpoints. Our formulation is based on novel velocity-powered\ntransport cost with a sublinear exponent (between zero and one). this concave\ndependence rewards joint and fast mass movement. Practically, we instantiate\nthe idea in a scalable neural ODE training objective. On synthetic, image, and\nbiology datasets, Y-flows recover hierarchy-aware structure, improve\ndistributional metrics over strong flow-based baselines, and reach targets with\nfewer integration steps.", "AI": {"tldr": "The paper introduces Y-shaped generative flows for modeling shared transport pathways in generative models, improving efficiency and structure in probabilistic mass movement.", "motivation": "Current continuous-time generative models often miss shared transport structures by using independent sample trajectories. This paper addresses the limitation by proposing a model that incorporates shared pathways.", "method": "The authors propose Y-shaped generative flows using a novel velocity-powered transport cost with a sublinear exponent to encourage joint and rapid mass movement. The method is implemented using scalable neural ODE objectives.", "result": "Experiments on synthetic, image, and biological datasets show that Y-flows recover hierarchical structures, improve distribution metrics over existing flow-based models, and require fewer integration steps.", "conclusion": "Y-shaped generative flows enhance transport efficiency and structure awareness over traditional generative models, showcasing improved results across diverse datasets."}}
{"id": "2510.11923", "pdf": "https://arxiv.org/pdf/2510.11923", "abs": "https://arxiv.org/abs/2510.11923", "authors": ["Juno Nam", "B\u00e1lint M\u00e1t\u00e9", "Artur P. Toshev", "Manasa Kaniselvan", "Rafael G\u00f3mez-Bombarelli", "Ricky T. Q. Chen", "Brandon Wood", "Guan-Horng Liu", "Benjamin Kurt Miller"], "title": "Enhancing Diffusion-Based Sampling with Molecular Collective Variables", "categories": ["physics.chem-ph", "cs.LG", "stat.ML"], "comment": null, "summary": "Diffusion-based samplers learn to sample complex, high-dimensional\ndistributions using energies or log densities alone, without training data.\nYet, they remain impractical for molecular sampling because they are often\nslower than molecular dynamics and miss thermodynamically relevant modes.\nInspired by enhanced sampling, we encourage exploration by introducing a\nsequential bias along bespoke, information-rich, low-dimensional projections of\natomic coordinates known as collective variables (CVs). We introduce a\nrepulsive potential centered on the CVs from recent samples, which pushes\nfuture samples towards novel CV regions and effectively increases the\ntemperature in the projected space. Our resulting method improves efficiency,\nmode discovery, enables the estimation of free energy differences, and retains\nindependent sampling from the approximate Boltzmann distribution via\nreweighting by the bias. On standard peptide conformational sampling\nbenchmarks, the method recovers diverse conformational states and accurate free\nenergy profiles. We are the first to demonstrate reactive sampling using a\ndiffusion-based sampler, capturing bond breaking and formation with universal\ninteratomic potentials at near-first-principles accuracy. The approach resolves\nreactive energy landscapes at a fraction of the wall-clock time of standard\nsampling methods, advancing diffusion-based sampling towards practical use in\nmolecular sciences.", "AI": {"tldr": "The paper introduces a diffusion-based sampling method enhanced with collective variables (CVs) to improve molecular sampling efficiency and accuracy in discovering thermodynamic modes and estimating free energies.", "motivation": "Current diffusion-based samplers are inefficient for practical molecular sampling because they are slower than molecular dynamics and fail to capture important thermodynamic modes.", "method": "The method introduces a repulsive potential based on low-dimensional collective variables (CVs) to guide samples towards novel regions, effectively increasing temperature in the CV space. This supports mode discovery and facilitates reweighting to approximate the Boltzmann distribution.", "result": "The proposed method demonstrated better efficiency, discovered diverse molecular conformational states, recovered accurate free energy profiles, and achieved reactive sampling (bond breaking and formation) with universal interatomic potentials.", "conclusion": "This research advances diffusion-based sampling by making it practical for molecular sciences, offering faster, accurate, and thermodynamically relevant molecular sampling solutions."}}
{"id": "2510.12051", "pdf": "https://arxiv.org/pdf/2510.12051", "abs": "https://arxiv.org/abs/2510.12051", "authors": ["Baisub Lee", "Sanghyun Byun", "Mohanad Odema", "Jung Guack", "Jacob Song", "Woo Seong Chung"], "title": "APCE: Adaptive Progressive Context Expansion for Long Context Processing", "categories": ["cs.CL", "cs.AI"], "comment": "NeurIPS 2025 Workshop: ML For Systems", "summary": "Deploying useful Long-Context Transformer Models (LCTMs) requires addressing\ntwo key challenges: (1) A growing memory footprint due to quadratic\nself-attention and linear KV-cache scaling in memory as sequence length\nincreases; (2) the ContextRot phenomena where empirical evidence suggests that\ntransformer architecture's performance degrades with increasing context length.\nGiven the shared dependency on the input, a natural question arises: Can we\nsurgically select the most important input chunks for processing to\nsynergistically (a) reduce the memory footprint, and (b) mitigate the\nContextRot effects? In this paper, we answer this question in the affirmative\nfor long-context summarization tasks. We propose APCE as a context-aware\nsolution to select the most important input chunks through low-dimensional\nsemantic similarity matching with the current query. By directly operating on\nthe input, APCE decouples from strict dependency on underlying hardware or CUDA\nenvironments, promising a compatible solution scalable to different deployment\nsystems. Our empirical evaluations have demonstrated superior or on-par\nsummarization performance for APCE compared to the full dense baseline using a\nfraction (50%-70%) of the input sequence resulting in KV-cache and\nself-attention memory efficiency improvements. We hope our findings inspire\nfurther research on context-aware efficiency solutions for LCTMs geared towards\nother relevant long-context tasks.", "AI": {"tldr": "The paper addresses challenges in deploying Long-Context Transformer Models (LCTMs) and proposes APCE as a solution to mitigate memory issues and performance degradation for summarization tasks.", "motivation": "Address challenges of growing memory footprint and performance degradation (ContextRot) in LCTMs due to increasing context length.", "method": "Introduce APCE, a context-aware method that selects important input chunks based on semantic similarity to reduce memory usage and reliance on hardware.", "result": "Empirical evaluation shows APCE achieves superior or comparable summarization performance using only 50%-70% of the input sequence, leading to memory efficiency improvements.", "conclusion": "APCE effectively reduces memory footprint and mitigates ContextRot without strict hardware dependency, encouraging further research for similar LCTM tasks."}}
{"id": "2510.12662", "pdf": "https://arxiv.org/pdf/2510.12662", "abs": "https://arxiv.org/abs/2510.12662", "authors": ["Oz Gitelson", "Satya Prakash Nayak", "Ritam Raha", "Anne-Kathrin Schmuck"], "title": "Maximal Adaptation, Minimal Guidance: Permissive Reactive Robot Task Planning with Humans in the Loop", "categories": ["cs.RO"], "comment": null, "summary": "We present a novel framework for human-robot \\emph{logical} interaction that\nenables robots to reliably satisfy (infinite horizon) temporal logic tasks\nwhile effectively collaborating with humans who pursue independent and unknown\ntasks. The framework combines two key capabilities: (i) \\emph{maximal\nadaptation} enables the robot to adjust its strategy \\emph{online} to exploit\nhuman behavior for cooperation whenever possible, and (ii) \\emph{minimal\ntunable feedback} enables the robot to request cooperation by the human online\nonly when necessary to guarantee progress. This balance minimizes human-robot\ninterference, preserves human autonomy, and ensures persistent robot task\nsatisfaction even under conflicting human goals. We validate the approach in a\nreal-world block-manipulation task with a Franka Emika Panda robotic arm and in\nthe Overcooked-AI benchmark, demonstrating that our method produces rich,\n\\emph{emergent} cooperative behaviors beyond the reach of existing approaches,\nwhile maintaining strong formal guarantees.", "AI": {"tldr": "The paper introduces a framework for logical human-robot collaboration that optimizes robot strategy for temporal logic tasks while respecting human autonomy.", "motivation": "To address the challenge of enabling robots to effectively collaborate with humans pursuing independent, unknown tasks while ensuring robots meet their formal goals.", "method": "The framework uses 'maximal adaptation' to adjust robot strategies dynamically and 'minimal tunable feedback' to request human cooperation only when needed for task progression.", "result": "Validated on a robotic block-manipulation task and the Overcooked-AI benchmark, demonstrating superior cooperation behaviors and maintaining task guarantees.", "conclusion": "The proposed method achieves cooperative human-robot interaction with balanced autonomy, minimal interference, and formal task satisfaction guarantees."}}
{"id": "2510.12201", "pdf": "https://arxiv.org/pdf/2510.12201", "abs": "https://arxiv.org/abs/2510.12201", "authors": ["Aline Mangold", "Juliane Zietz", "Susanne Weinhold", "Sebastian Pannasch"], "title": "On the Design and Evaluation of Human-centered Explainable AI Systems: A Systematic Review and Taxonomy", "categories": ["cs.AI"], "comment": null, "summary": "As AI becomes more common in everyday living, there is an increasing demand\nfor intelligent systems that are both performant and understandable.\nExplainable AI (XAI) systems aim to provide comprehensible explanations of\ndecisions and predictions. At present, however, evaluation processes are rather\ntechnical and not sufficiently focused on the needs of human users.\nConsequently, evaluation studies involving human users can serve as a valuable\nguide for conducting user studies. This paper presents a comprehensive review\nof 65 user studies evaluating XAI systems across different domains and\napplication contexts. As a guideline for XAI developers, we provide a holistic\noverview of the properties of XAI systems and evaluation metrics focused on\nhuman users (human-centered). We propose objectives for the human-centered\ndesign (design goals) of XAI systems. To incorporate users' specific\ncharacteristics, design goals are adapted to users with different levels of AI\nexpertise (AI novices and data experts). In this regard, we provide an\nextension to existing XAI evaluation and design frameworks. The first part of\nour results includes the analysis of XAI system characteristics. An important\nfinding is the distinction between the core system and the XAI explanation,\nwhich together form the whole system. Further results include the distinction\nof evaluation metrics into affection towards the system, cognition, usability,\ninterpretability, and explanation metrics. Furthermore, the users, along with\ntheir specific characteristics and behavior, can be assessed. For AI novices,\nthe relevant extended design goals include responsible use, acceptance, and\nusability. For data experts, the focus is performance-oriented and includes\nhuman-AI collaboration and system and user task performance.", "AI": {"tldr": "The paper reviews 65 user studies on XAI systems, outlines evaluation metrics, and proposes human-centered design goals tailored to users with varying AI expertise.", "motivation": "AI's increasing integration into daily life necessitates systems that are both effective and explainable, demanding user-focused evaluation processes.", "method": "The authors conducted a comprehensive review of 65 user studies and developed a framework that extends current XAI evaluation and design guidelines.", "result": "Key findings include distinguishing XAI core systems from their explanations, categorizing evaluation metrics (e.g., usability, cognition), and tailoring design goals for AI novices and data experts.", "conclusion": "Holistic design goals are essential for creating user-focused XAI systems, requiring adaptability to different user expertise levels for responsible, acceptable, and performant systems."}}
{"id": "2510.12150", "pdf": "https://arxiv.org/pdf/2510.12150", "abs": "https://arxiv.org/abs/2510.12150", "authors": ["Jiahuan Zhou", "Chao Zhu", "Zhenyu Cui", "Zichen Liu", "Xu Zou", "Gang Hua"], "title": "Class-aware Domain Knowledge Fusion and Fission for Continual Test-Time Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Continual Test-Time Adaptation (CTTA) aims to quickly fine-tune the model\nduring the test phase so that it can adapt to multiple unknown downstream\ndomain distributions without pre-acquiring downstream domain data. To this end,\nexisting advanced CTTA methods mainly reduce the catastrophic forgetting of\nhistorical knowledge caused by irregular switching of downstream domain data by\nrestoring the initial model or reusing historical models. However, these\nmethods are usually accompanied by serious insufficient learning of new\nknowledge and interference from potentially harmful historical knowledge,\nresulting in severe performance degradation. To this end, we propose a\nclass-aware domain Knowledge Fusion and Fission method for continual test-time\nadaptation, called KFF, which adaptively expands and merges class-aware domain\nknowledge in old and new domains according to the test-time data from different\ndomains, where discriminative historical knowledge can be dynamically\naccumulated. Specifically, considering the huge domain gap within streaming\ndata, a domain Knowledge FIssion (KFI) module is designed to adaptively\nseparate new domain knowledge from a paired class-aware domain prompt pool,\nalleviating the impact of negative knowledge brought by old domains that are\ndistinct from the current domain. Besides, to avoid the cumulative computation\nand storage overheads from continuously fissioning new knowledge, a domain\nKnowledge FUsion (KFU) module is further designed to merge the fissioned new\nknowledge into the existing knowledge pool with minimal cost, where a greedy\nknowledge dynamic merging strategy is designed to improve the compatibility of\nnew and old knowledge while keeping the computational efficiency. Extensive\nexperiments on the ImageNet-C dataset verify the effectiveness of our proposed\nmethod against other methods.", "AI": {"tldr": "This paper introduces a method called Knowledge Fusion and Fission (KFF) for Continual Test-Time Adaptation (CTTA) to adapt models to varied downstream domains by dynamically managing historical and new knowledge.", "motivation": "The paper addresses the challenges of catastrophic forgetting and insufficient knowledge learning in CTTA when adapting to downstream domains, aiming to improve performance while avoiding interference from harmful historical knowledge.", "method": "The authors propose the KFF method, which includes two modules: Knowledge FIssion (KFI) to separate class-aware domain knowledge for new domains and minimize negative influence, and Knowledge FUsion (KFU) to merge new knowledge with old to maintain computational efficiency.", "result": "Extensive experiments on the ImageNet-C dataset show improved effectiveness of the proposed KFF method compared to existing CTTA approaches.", "conclusion": "The KFF method dynamically manages historical and new domain knowledge, addressing key CTTA challenges to enhance performance and computational efficiency."}}
{"id": "2510.11962", "pdf": "https://arxiv.org/pdf/2510.11962", "abs": "https://arxiv.org/abs/2510.11962", "authors": ["Bowei Guo", "Shengkun Tang", "Cong Zeng", "Zhiqiang Shen"], "title": "MosaicDiff: Training-free Structural Pruning for Diffusion Model Acceleration Reflecting Pretraining Dynamics", "categories": ["cs.LG", "cs.CV"], "comment": "International Conference on Computer Vision, ICCV 2025", "summary": "Diffusion models are renowned for their generative capabilities, yet their\npretraining processes exhibit distinct phases of learning speed that have been\nentirely overlooked in prior post-training acceleration efforts in the\ncommunity. In this study, we introduce a novel framework called MosaicDiff that\naligns diffusion pretraining dynamics with post-training sampling acceleration\nvia trajectory-aware structural pruning. Our approach leverages the observation\nthat the middle, fast-learning stage of diffusion pretraining requires more\nconservative pruning to preserve critical model features, while the early and\nlater, slow-learning stages benefit from a more aggressive pruning strategy.\nThis adaptive pruning mechanism is the first to explicitly mirror the inherent\nlearning speed variations of diffusion pretraining, thereby harmonizing the\nmodel's inner training dynamics with its accelerated sampling process.\nExtensive experiments on DiT and SDXL demonstrate that our method achieves\nsignificant speed-ups in sampling without compromising output quality,\noutperforming previous state-of-the-art methods by large margins, also\nproviding a new viewpoint for more efficient and robust training-free diffusion\nacceleration.", "AI": {"tldr": "The paper proposes MosaicDiff, a framework leveraging adaptive pruning for more efficient diffusion model training and sampling acceleration.", "motivation": "While diffusion models excel in generative tasks, existing methods have overlooked the varying learning speeds in diffusion pretraining phases, which could inform sampling acceleration strategies.", "method": "MosaicDiff employs trajectory-aware structural pruning, varying aggressiveness based on different stages of learning speed during diffusion pretraining.", "result": "Experiments on DiT and SDXL show significant sampling speed-ups achieved by MosaicDiff without degrading quality, surpassing previous acceleration methods.", "conclusion": "MosaicDiff offers an efficient, robust way to accelerate diffusion model sampling and opens up new perspectives for training-free optimization approaches."}}
{"id": "2510.12083", "pdf": "https://arxiv.org/pdf/2510.12083", "abs": "https://arxiv.org/abs/2510.12083", "authors": ["Benjamin W. Nelson", "Celeste Wong", "Matthew T. Silvestrini", "Sooyoon Shin", "Alanna Robinson", "Jessica Lee", "Eric Yang", "John Torous", "Andrew Trister"], "title": "An AI-Based Behavioral Health Safety Filter and Dataset for Identifying Mental Health Crises in Text-Based Conversations", "categories": ["cs.CL", "cs.AI"], "comment": "Main Text: 2943; Abstract: 256; Tables and Figures: 5", "summary": "Large language models often mishandle psychiatric emergencies, offering\nharmful or inappropriate advice and enabling destructive behaviors. This study\nevaluated the Verily behavioral health safety filter (VBHSF) on two datasets:\nthe Verily Mental Health Crisis Dataset containing 1,800 simulated messages and\nthe NVIDIA Aegis AI Content Safety Dataset subsetted to 794 mental\nhealth-related messages. The two datasets were clinician-labelled and we\nevaluated performance using the clinician labels. Additionally, we carried out\ncomparative performance analyses against two open source, content moderation\nguardrails: OpenAI Omni Moderation Latest and NVIDIA NeMo Guardrails. The VBHSF\ndemonstrated, well-balanced performance on the Verily Mental Health Crisis\nDataset v1.0, achieving high sensitivity (0.990) and specificity (0.992) in\ndetecting any mental health crises. It achieved an F1-score of 0.939,\nsensitivity ranged from 0.917-0.992, and specificity was >= 0.978 in\nidentifying specific crisis categories. When evaluated against the NVIDIA Aegis\nAI Content Safety Dataset 2.0, VBHSF performance remained highly sensitive\n(0.982) and accuracy (0.921) with reduced specificity (0.859). When compared\nwith the NVIDIA NeMo and OpenAI Omni Moderation Latest guardrails, the VBHSF\ndemonstrated superior performance metrics across both datasets, achieving\nsignificantly higher sensitivity in all cases (all p < 0.001) and higher\nspecificity relative to NVIDIA NeMo (p < 0.001), but not to OpenAI Omni\nModeration Latest (p = 0.094). NVIDIA NeMo and OpenAI Omni Moderation Latest\nexhibited inconsistent performance across specific crisis types, with\nsensitivity for some categories falling below 0.10. Overall, the VBHSF\ndemonstrated robust, generalizable performance that prioritizes sensitivity to\nminimize missed crises, a crucial feature for healthcare applications.", "AI": {"tldr": "The study evaluates Verily's behavioral health safety filter (VBHSF) for handling mental health crises, demonstrating its superior sensitivity and specificity compared to existing moderation tools.", "motivation": "Large language models often provide harmful advice during psychiatric emergencies, necessitating reliable tools for detecting and addressing mental health crises.", "method": "The study tested VBHSF against two labeled datasets: Verily Mental Health Crisis Dataset and NVIDIA Aegis AI Content Safety Dataset, and compared its performance with OpenAI Omni Moderation and NVIDIA NeMo Guardrails.", "result": "VBHSF showed high sensitivity and specificity in detecting mental health crises, outperformed competing systems across metrics, and maintained consistent results across different datasets.", "conclusion": "The VBHSF is an effective tool for identifying mental health crises, emphasizing sensitivity to minimize risks, making it suitable for healthcare applications."}}
{"id": "2510.12684", "pdf": "https://arxiv.org/pdf/2510.12684", "abs": "https://arxiv.org/abs/2510.12684", "authors": ["Alvaro Belmonte-Baeza", "Miguel Cazorla", "Gabriel J. Garc\u00eda", "Carlos J. P\u00e9rez-Del-Pulgar", "Jorge Pomares"], "title": "Autonomous Legged Mobile Manipulation for Lunar Surface Operations via Constrained Reinforcement Learning", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "This is the authors version of the paper accepted for publication in\n  The IEEE International Conference on Space Robotics 2025. The final version\n  link will be added here after conference proceedings are published", "summary": "Robotics plays a pivotal role in planetary science and exploration, where\nautonomous and reliable systems are crucial due to the risks and challenges\ninherent to space environments. The establishment of permanent lunar bases\ndemands robotic platforms capable of navigating and manipulating in the harsh\nlunar terrain. While wheeled rovers have been the mainstay for planetary\nexploration, their limitations in unstructured and steep terrains motivate the\nadoption of legged robots, which offer superior mobility and adaptability. This\npaper introduces a constrained reinforcement learning framework designed for\nautonomous quadrupedal mobile manipulators operating in lunar environments. The\nproposed framework integrates whole-body locomotion and manipulation\ncapabilities while explicitly addressing critical safety constraints, including\ncollision avoidance, dynamic stability, and power efficiency, in order to\nensure robust performance under lunar-specific conditions, such as reduced\ngravity and irregular terrain. Experimental results demonstrate the framework's\neffectiveness in achieving precise 6D task-space end-effector pose tracking,\nachieving an average positional accuracy of 4 cm and orientation accuracy of\n8.1 degrees. The system consistently respects both soft and hard constraints,\nexhibiting adaptive behaviors optimized for lunar gravity conditions. This work\neffectively bridges adaptive learning with essential mission-critical safety\nrequirements, paving the way for advanced autonomous robotic explorers for\nfuture lunar missions.", "AI": {"tldr": "This paper proposes a constrained reinforcement learning framework for quadrupedal robots in lunar environments, focusing on mobility, manipulation, and safety constraints like stability and power efficiency.", "motivation": "The reliance on wheeled rovers for planetary exploration limits adaptability in challenging terrains, especially for establishing permanent lunar bases.", "method": "A constrained reinforcement learning framework focusing on whole-body locomotion and manipulation while incorporating safety constraints such as collision avoidance, dynamic stability, and power efficiency.", "result": "The system achieved an average positional accuracy of 4 cm and orientation accuracy of 8.1 degrees, respecting constraints and adapting to lunar gravity conditions.", "conclusion": "This work combines adaptive learning with safety-critical requirements, advancing autonomous robotics for future lunar exploration."}}
{"id": "2510.12218", "pdf": "https://arxiv.org/pdf/2510.12218", "abs": "https://arxiv.org/abs/2510.12218", "authors": ["Hyunji Min", "Sangwon Jung", "Junyoung Sung", "Dosung Lee", "Leekyeung Han", "Paul Hongsuck Seo"], "title": "GOAT: A Training Framework for Goal-Oriented Agent with Tools", "categories": ["cs.AI"], "comment": "32 pages, 21 figures", "summary": "Large language models (LLMs) have recently been extended beyond traditional\ntext generation to serve as interactive agents capable of using external tools\nbased on user intent. However, current LLM agents still show limited ability to\nhandle goal-oriented queries, which require decomposing a high-level objective\ninto multiple interdependent API calls with correct planning and execution.\nCurrent approaches mainly rely on zero-shot evaluation due to the absence of\ntraining data. While proprietary closed-source models such as GPT-4 demonstrate\nstrong reasoning abilities, smaller open-source models struggle to perform\ncomplex tool use effectively. Thus, we propose a novel training framework GOAT,\nwhich enables fine-tuning of LLM agents in a human annotation-free setting.\nGOAT automatically constructs synthetic datasets of goal-oriented API execution\ntasks directly from given API documents, equipping models with the ability to\nreason over interdependent calls and generate coherent responses. Through\nextensive experiments, we show that GOAT-trained agents achieve\nstate-of-the-art performance across multiple existing goal-oriented benchmarks.\nIn addition, we introduce GOATBench, a new goal-oriented API execution\nbenchmark, and demonstrate that agents trained with GOAT also excel in this\nsetting. These results highlight GOAT as a practical path toward building\nrobust open-source LLM agents capable of complex reasoning and tool use.", "AI": {"tldr": "The paper introduces GOAT, a framework for enhancing goal-oriented abilities in open-source large language models (LLMs) to handle complex reasoning and tool usage tasks without needing human annotations.", "motivation": "LLMs show limitations in goal-oriented queries requiring decomposition of tasks into interdependent API calls. Open-source models struggle with complex tool usage compared to closed-source solutions like GPT-4.", "method": "GOAT utilizes synthetic data generated from API documentation to fine-tune LLMs, eliminating the need for human annotations. It equips models with reasoning abilities necessary for executing interdependent API calls.", "result": "Experiments demonstrate that GOAT-trained agents achieved state-of-the-art results across goal-oriented benchmarks and performed excellently on the newly introduced GOATBench.", "conclusion": "GOAT represents a promising method for training LLM agents to handle complex reasoning and tool use in an effective, scalable, and human annotation-free manner."}}
{"id": "2510.12159", "pdf": "https://arxiv.org/pdf/2510.12159", "abs": "https://arxiv.org/abs/2510.12159", "authors": ["Ziyuan Gao", "Philippe Morel"], "title": "DPL: Spatial-Conditioned Diffusion Prototype Enhancement for One-Shot Medical Segmentation", "categories": ["cs.CV"], "comment": "Accepted at IVCNZ 2025. To be published in IEEE proceedings", "summary": "One-shot medical image segmentation faces fundamental challenges in prototype\nrepresentation due to limited annotated data and significant anatomical\nvariability across patients. Traditional prototype-based methods rely on\ndeterministic averaging of support features, creating brittle representations\nthat fail to capture intra-class diversity essential for robust generalization.\nThis work introduces Diffusion Prototype Learning (DPL), a novel framework that\nreformulates prototype construction through diffusion-based feature space\nexploration. DPL models one-shot prototypes as learnable probability\ndistributions, enabling controlled generation of diverse yet semantically\ncoherent prototype variants from minimal labeled data. The framework operates\nthrough three core innovations: (1) a diffusion-based prototype enhancement\nmodule that transforms single support prototypes into diverse variant sets via\nforward-reverse diffusion processes, (2) a spatial-aware conditioning mechanism\nthat leverages geometric properties derived from prototype feature statistics,\nand (3) a conservative fusion strategy that preserves prototype fidelity while\nmaximizing representational diversity. DPL ensures training-inference\nconsistency by using the same diffusion enhancement and fusion pipeline in both\nphases. This process generates enhanced prototypes that serve as the final\nrepresentations for similarity calculations, while the diffusion process itself\nacts as a regularizer. Extensive experiments on abdominal MRI and CT datasets\ndemonstrate significant improvements respectively, establishing new\nstate-of-the-art performance in one-shot medical image segmentation.", "AI": {"tldr": "The paper tackles challenges in one-shot medical image segmentation by introducing Diffusion Prototype Learning (DPL), which builds robust prototypes through diffusion-based feature space exploration.", "motivation": "Existing one-shot segmentation methods encounter difficulties due to limited annotated data and anatomical variability, leading to brittle prototype representations that struggle to generalize across diverse intra-class features.", "method": "The introduced DPL framework includes three core components: (1) diffusion-based prototype enhancement for creating diverse prototype variants, (2) spatial-aware conditioning based on geometric properties, and (3) conservative fusion to balance fidelity and diversity. Both training and inference utilize the same pipeline.", "result": "Experiments on abdominal MRI and CT datasets show substantial performance gains, achieving state-of-the-art results in the one-shot segmentation task.", "conclusion": "DPL enables enhanced prototype generation that improves robustness and segmentation performance through innovative use of diffusion processes to handle limited data and anatomical variabilities."}}
{"id": "2510.11963", "pdf": "https://arxiv.org/pdf/2510.11963", "abs": "https://arxiv.org/abs/2510.11963", "authors": ["Aditya Gupta", "Kirandeep Kaur", "Vinayak Gupta"], "title": "QLENS: Towards A Quantum Perspective of Language Transformers", "categories": ["cs.LG"], "comment": null, "summary": "In natural language processing, current methods for understanding\nTransformers are successful at identifying intermediate predictions during a\nmodel's inference. However, these approaches function as limited diagnostic\ncheckpoints, lacking a mathematical framework for mechanistically modeling how\neach layer facilitates transitions between these evolving states. This\ninterpretability gap and past successes of interdisciplinary outlooks inspire\nus to turn to physics in search of a descriptive mathematical framework for\nTransformers. We observe that language models are intrinsically probabilistic,\nan attribute that is echoed in the core postulates of quantum mechanics. This\nparallel inspires us to translate insights from this discipline to that of\nnatural language processing. Towards this objective, we propose QLENS a novel\nattempt to develop a physics-based perspective on the Transformer generation\nprocess. Under QLENS, a Transformer is studied by converting its latent\nactivations into a state vector in a Hilbert space derived from the model's\noutput units. This state subsequently evolves through hidden layers -\nreformulated as unitary operators and analogously defined Hamiltonians - during\ninference. The model's final probability distribution is obtained by applying\nthe Born rule to the end state using a specific measurement operator. To\ndemonstrate QLENS's potential, we conduct a proof-of-concept by probing a toy\nTransformer to investigate the influence of individual layers in a model's\nprediction trajectory. We present our work as a foundation for cross-domain\ninsights to be leveraged towards a broader understanding of Transformers.", "AI": {"tldr": "This paper introduces QLENS, a physics-based framework inspired by quantum mechanics to model the inference process of Transformers in natural language processing and sheds light on layer-by-layer prediction dynamics.", "motivation": "The work is motivated by the interpretability gap in understanding how each Transformer layer contributes to evolving states during inference, with inspiration from the parallels between probabilistic language models and quantum mechanics.", "method": "QLENS reformulates the latent activations of a Transformer as quantum-inspired state vectors evolving in a Hilbert space, with layers represented as unitary operators informed by Hamiltonian dynamics and output probabilities extracted using the Born rule.", "result": "A toy Transformer model is used as a proof-of-concept under QLENS, highlighting how individual layers influence prediction trajectories within this quantum-inspired framework.", "conclusion": "This paper lays the groundwork for leveraging insights from physics to enhance the interpretability of Transformer models, proposing a new paradigm for understanding their inference process through cross-domain methodologies."}}
{"id": "2510.12026", "pdf": "https://arxiv.org/pdf/2510.12026", "abs": "https://arxiv.org/abs/2510.12026", "authors": ["Junsoo Oh", "Wei Huang", "Taiji Suzuki"], "title": "Mamaba Can Learn Low-Dimensional Targets In-Context via Test-Time Feature Learning", "categories": ["cs.LG", "stat.ML"], "comment": "34 pages", "summary": "Mamba, a recently proposed linear-time sequence model, has attracted\nsignificant attention for its computational efficiency and strong empirical\nperformance. However, a rigorous theoretical understanding of its underlying\nmechanisms remains limited. In this work, we provide a theoretical analysis of\nMamba's in-context learning (ICL) capability by focusing on tasks defined by\nlow-dimensional nonlinear target functions. Specifically, we study in-context\nlearning of a single-index model $y \\approx g_*(\\langle \\boldsymbol{\\beta},\n\\boldsymbol{x} \\rangle)$, which depends on only a single relevant direction\n$\\boldsymbol{\\beta}$, referred to as feature. We prove that Mamba, pretrained\nby gradient-based methods, can achieve efficient ICL via test-time feature\nlearning, extracting the relevant direction directly from context examples.\nConsequently, we establish a test-time sample complexity that improves upon\nlinear Transformers -- analyzed to behave like kernel methods -- and is\ncomparable to nonlinear Transformers, which have been shown to surpass the\nCorrelational Statistical Query (CSQ) lower bound and achieve near\ninformation-theoretically optimal rate in previous works. Our analysis reveals\nthe crucial role of the nonlinear gating mechanism in Mamba for feature\nextraction, highlighting it as the fundamental driver behind Mamba's ability to\nachieve both computational efficiency and high performance.", "AI": {"tldr": "The paper analyzes the theoretical mechanism behind Mamba's efficient in-context learning, especially for tasks described by low-dimensional nonlinear target functions.", "motivation": "Understanding the theoretical underpinnings of Mamba's empirical success in computational efficiency and in-context learning is necessary to bridge the gap between its practical success and theoretical understanding.", "method": "The study focuses on analyzing Mamba's in-context learning by examining its ability to handle single-index models through test-time feature learning. The analysis emphasizes the nonlinear gating mechanism as a key driver of performance.", "result": "The analysis establishes Mamba's lower sample complexity at test time, surpassing linear Transformers and achieving a rate comparable to nonlinear Transformers while underscoring the significance of nonlinear gating.", "conclusion": "Mamba's nonlinear gating mechanism is critical for feature extraction and achieving optimal computational and learning performance, demonstrating its theoretical potential and efficiency."}}
{"id": "2510.12110", "pdf": "https://arxiv.org/pdf/2510.12110", "abs": "https://arxiv.org/abs/2510.12110", "authors": ["Ziliang Qiu", "Renfen Hu"], "title": "Deep Associations, High Creativity: A Simple yet Effective Metric for Evaluating Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages", "summary": "The evaluation of LLMs' creativity represents a crucial research domain,\nthough challenges such as data contamination and costly human assessments often\nimpede progress. Drawing inspiration from human creativity assessment, we\npropose PACE, asking LLMs to generate Parallel Association Chains to Evaluate\ntheir creativity. PACE minimizes the risk of data contamination and offers a\nstraightforward, highly efficient evaluation, as evidenced by its strong\ncorrelation with Chatbot Arena Creative Writing rankings (Spearman's $\\rho =\n0.739$, $p < 0.001$) across various proprietary and open-source models. A\ncomparative analysis of associative creativity between LLMs and humans reveals\nthat while high-performing LLMs achieve scores comparable to average human\nperformance, professional humans consistently outperform LLMs. Furthermore,\nlinguistic analysis reveals that both humans and LLMs exhibit a trend of\ndecreasing concreteness in their associations, and humans demonstrating a\ngreater diversity of associative patterns.", "AI": {"tldr": "PACE, a novel model-based creativity evaluation method for LLMs, minimizes data contamination and correlates well with human rankings, revealing differences between human and LLM creativity.", "motivation": "Challenges such as data contamination and resource-intensive human assessments hinder effective evaluation of LLMs' creativity.", "method": "Proposed PACE, a system that generates Parallel Association Chains to assess LLM creativity in an efficient and contamination-resistant manner.", "result": "PACE showed strong correlation (Spearman's $\rho = 0.739$, $p < 0.001$) with human creative writing rankings and revealed gaps in creativity between LLMs and humans.", "conclusion": "PACE provides an effective creativity evaluation for LLMs but establishes that humans demonstrate superior and more diverse associative creativity compared to even high-performing LLMs."}}
{"id": "2510.12710", "pdf": "https://arxiv.org/pdf/2510.12710", "abs": "https://arxiv.org/abs/2510.12710", "authors": ["Baicheng Li", "Dong Wu", "Zike Yan", "Xinchen Liu", "Zecui Zeng", "Lusong Li", "Hongbin Zha"], "title": "Reflection-Based Task Adaptation for Self-Improving VLA", "categories": ["cs.RO"], "comment": null, "summary": "Pre-trained Vision-Language-Action (VLA) models represent a major leap\ntowards general-purpose robots, yet efficiently adapting them to novel,\nspecific tasks in-situ remains a significant hurdle. While reinforcement\nlearning (RL) is a promising avenue for such adaptation, the process often\nsuffers from low efficiency, hindering rapid task mastery. We introduce\nReflective Self-Adaptation, a framework for rapid, autonomous task adaptation\nwithout human intervention. Our framework establishes a self-improving loop\nwhere the agent learns from its own experience to enhance both strategy and\nexecution.\n  The core of our framework is a dual-pathway architecture that addresses the\nfull adaptation lifecycle. First, a Failure-Driven Reflective RL pathway\nenables rapid learning by using the VLM's causal reasoning to automatically\nsynthesize a targeted, dense reward function from failure analysis. This\nprovides a focused learning signal that significantly accelerates policy\nexploration. However, optimizing such proxy rewards introduces a potential risk\nof \"reward hacking,\" where the agent masters the reward function but fails the\nactual task. To counteract this, our second pathway, Success-Driven\nQuality-Guided SFT, grounds the policy in holistic success. It identifies and\nselectively imitates high-quality successful trajectories, ensuring the agent\nremains aligned with the ultimate task goal. This pathway is strengthened by a\nconditional curriculum mechanism to aid initial exploration.\n  We conduct experiments in challenging manipulation tasks. The results\ndemonstrate that our framework achieves faster convergence and higher final\nsuccess rates compared to representative baselines. Our work presents a robust\nsolution for creating self-improving agents that can efficiently and reliably\nadapt to new environments.", "AI": {"tldr": "The paper introduces Reflective Self-Adaptation, a framework for rapid, autonomous task adaptation in robots using a self-improving loop to enhance execution and strategy.", "motivation": "Pretrained Vision-Language-Action (VLA) models struggle to adapt efficiently to new tasks, posing a challenge for task-specific applications in robotics.", "method": "The framework utilizes a dual-pathway architecture: a Failure-Driven Reflective RL pathway that synthesizes targeted dense reward functions to accelerate learning, and a Success-Driven Quality-Guided SFT pathway to align the learning with actual task goals by selectively imitating high-quality trajectories.", "result": "Experiments on manipulation tasks showed faster convergence and higher success rates compared to baseline methods.", "conclusion": "The framework provides a robust method for creating self-improving agents that can adapt effectively and autonomously to novel tasks with minimal human intervention."}}
{"id": "2510.12224", "pdf": "https://arxiv.org/pdf/2510.12224", "abs": "https://arxiv.org/abs/2510.12224", "authors": ["Yuechun Yu", "Han Ying", "Haoan Jin", "Wenjian Jiang", "Dong Xian", "Binghao Wang", "Zhou Yang", "Mengyue Wu"], "title": "MedKGEval: A Knowledge Graph-Based Multi-Turn Evaluation Framework for Open-Ended Patient Interactions with Clinical LLMs", "categories": ["cs.AI"], "comment": null, "summary": "The reliable evaluation of large language models (LLMs) in medical\napplications remains an open challenge, particularly in capturing the\ncomplexity of multi-turn doctor-patient interactions that unfold in real\nclinical environments. Existing evaluation methods typically rely on post hoc\nreview of full conversation transcripts, thereby neglecting the dynamic,\ncontext-sensitive nature of medical dialogues and the evolving informational\nneeds of patients. In this work, we present MedKGEval, a novel multi-turn\nevaluation framework for clinical LLMs grounded in structured medical\nknowledge. Our approach introduces three key contributions: (1) a knowledge\ngraph-driven patient simulation mechanism, where a dedicated control module\nretrieves relevant medical facts from a curated knowledge graph, thereby\nendowing the patient agent with human-like and realistic conversational\nbehavior. This knowledge graph is constructed by integrating open-source\nresources with additional triples extracted from expert-annotated datasets; (2)\nan in-situ, turn-level evaluation framework, where each model response is\nassessed by a Judge Agent for clinical appropriateness, factual correctness,\nand safety as the dialogue progresses using a suite of fine-grained,\ntask-specific metrics; (3) a comprehensive multi-turn benchmark of eight\nstate-of-the-art LLMs, demonstrating MedKGEval's ability to identify subtle\nbehavioral flaws and safety risks that are often overlooked by conventional\nevaluation pipelines. Although initially designed for Chinese and English\nmedical applications, our framework can be readily extended to additional\nlanguages by switching the input knowledge graphs, ensuring seamless bilingual\nsupport and domain-specific applicability.", "AI": {"tldr": "The paper introduces MedKGEval, a novel framework for evaluating clinical Large Language Models (LLMs) using structured knowledge graphs and multi-turn interactions.", "motivation": "There is a need for reliable evaluation methods that capture the dynamic nature of doctor-patient interactions in clinical environments, as existing methods are often limited to post hoc analyses and fail to account for evolving informational needs.", "method": "The framework includes (1) a knowledge graph-driven patient simulation for realistic conversational behavior, (2) in-situ, turn-level evaluation for assessing model responses using task-specific metrics, and (3) a benchmark of eight state-of-the-art LLMs to identify behavioral flaws and safety risks.", "result": "MedKGEval successfully identifies subtle flaws and safety risks in LLMs that traditional evaluation pipelines overlook, demonstrating its effectiveness in diverse medical applications and languages.", "conclusion": "MedKGEval enables reliable and dynamic evaluation of clinical LLMs, bridging gaps in current assessment methods and ensuring adaptability across languages and medical domains."}}
{"id": "2510.12160", "pdf": "https://arxiv.org/pdf/2510.12160", "abs": "https://arxiv.org/abs/2510.12160", "authors": ["Jiahuan Zhou", "Kai Zhu", "Zhenyu Cui", "Zichen Liu", "Xu Zou", "Gang Hua"], "title": "State Space Prompting via Gathering and Spreading Spatio-Temporal Information for Video Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Recently, pre-trained state space models have shown great potential for video\nclassification, which sequentially compresses visual tokens in videos with\nlinear complexity, thereby improving the processing efficiency of video data\nwhile maintaining high performance. To apply powerful pre-trained models to\ndownstream tasks, prompt learning is proposed to achieve efficient downstream\ntask adaptation with only a small number of fine-tuned parameters. However, the\nsequentially compressed visual prompt tokens fail to capture the spatial and\ntemporal contextual information in the video, thus limiting the effective\npropagation of spatial information within a video frame and temporal\ninformation between frames in the state compression model and the extraction of\ndiscriminative information. To tackle the above issue, we proposed a State\nSpace Prompting (SSP) method for video understanding, which combines\nintra-frame and inter-frame prompts to aggregate and propagate key\nspatiotemporal information in the video. Specifically, an Intra-Frame Gathering\n(IFG) module is designed to aggregate spatial key information within each\nframe. Besides, an Inter-Frame Spreading (IFS) module is designed to spread\ndiscriminative spatio-temporal information across different frames. By\nadaptively balancing and compressing key spatio-temporal information within and\nbetween frames, our SSP effectively propagates discriminative information in\nvideos in a complementary manner. Extensive experiments on four video benchmark\ndatasets verify that our SSP significantly outperforms existing SOTA methods by\n2.76% on average while reducing the overhead of fine-tuning parameters.", "AI": {"tldr": "The paper introduces a State Space Prompting (SSP) method for video classification that improves spatial and temporal information propagation for competitive performance and efficiency.", "motivation": "Pre-trained state space models for video classification fail to capture essential spatiotemporal contextual information during compression, limiting their effectiveness in downstream tasks.", "method": "The authors designed SSP using two key modules, Intra-Frame Gathering (IFG) for spatial information aggregation within frames, and Inter-Frame Spreading (IFS) for spatiotemporal information propagation between frames.", "result": "Extensive experiments show SSP outperforms prior state-of-the-art video classification methods by 2.76% on average, with reduced fine-tuning parameter overhead.", "conclusion": "SSP method effectively improves video understanding by adaptively balancing spatiotemporal information propagation within and across frames, achieving both higher accuracy and efficiency."}}
{"id": "2510.11978", "pdf": "https://arxiv.org/pdf/2510.11978", "abs": "https://arxiv.org/abs/2510.11978", "authors": ["Jusheng Zhang", "Kaitong Cai", "Jing Yang", "Keze Wang"], "title": "Learning Dynamics of VLM Finetuning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Preference-based finetuning of vision--language models (VLMs) is brittle:\ntrivially wrong negatives inject uninformative gradients that destabilize\ntraining. We recast alignment as \\textbf{learning-dynamics--aware optimization}\nand introduce \\textbf{Cooling-Weighted DPO (CW-DPO)}, a two-stage recipe that\nexplicitly models and exploits the training trajectory. \\textbf{Stage 1}\nperforms supervised finetuning with \\textbf{gentle negatives}:\n\\textbf{low-weight smoothed supervision} that regularizes the base policy and\ncurbs overconfidence without explicit penalties. \\textbf{Stage 2} applies a DPO\nobjective in which the \\textbf{negative term is scaled by a cooling weight}\ncomputed from the model's \\textbf{average token log-probability} on each\nnegative, suppressing uninformative gradients from easy or off-distribution\nsamples while preserving signal from hard negatives. In practice, we emphasize\n\\textbf{on-policy negatives} and allow \\textbf{mixed negatives} by blending a\ncontrollable fraction of dataset negatives to maintain contrast freshness.\nThroughout, we instrument training with $\\Delta\\!\\log p$ probes on positives\nand negatives as first-class signals for early stopping, curriculum design, and\nfailure diagnosis. Across diverse VLM tasks, CW-DPO yields \\textbf{more stable\noptimization}, \\textbf{better calibration}, and \\textbf{higher pairwise\nwin-rates} than SFT-only and vanilla DPO, while \\textbf{converging in fewer\nsteps}. Ablations isolate the \\textbf{cooling-weight mechanism} as the primary\ndriver of these gains and show complementary benefits from mixing on-policy and\ndataset negatives. Taken together, our results show that \\textbf{smoothing\nlearning dynamics before cooling preferences} is a simple, general principle\nfor robust VLM alignment.", "AI": {"tldr": "This paper introduces a novel method for robust vision-language model alignment aimed at mitigating brittle preference-based finetuning by optimizing training dynamics.", "motivation": "Preference-based finetuning of vision-language models is prone to issues due to trivial negatives that destabilize training, necessitating a more stable optimization framework.", "method": "The paper proposes a two-stage method: Stage 1 performs supervised finetuning with low-weight smoothed supervision using gentle negatives, while Stage 2 employs a Dynamic Policy Optimization (DPO) approach with cooling weights based on average token log-probabilities to handle negatives effectively.", "result": "The CW-DPO method shows improved stability, calibration, pairwise win-rates, and faster convergence compared to previous approaches like SFT-only and vanilla DPO across diverse vision-language tasks.", "conclusion": "The cooling-weight mechanism and smoothing before preference cooling form a general and effective principle for robust vision-language model alignment."}}
{"id": "2510.12115", "pdf": "https://arxiv.org/pdf/2510.12115", "abs": "https://arxiv.org/abs/2510.12115", "authors": ["Xin Zhao", "Naoki Yoshinaga", "Yuma Tsuta", "Akiko Aizawa"], "title": "Tracing Multilingual Knowledge Acquisition Dynamics in Domain Adaptation: A Case Study of English-Japanese Biomedical Adaptation", "categories": ["cs.CL"], "comment": "22 Pages, Submitted to ARR 2025 Oct", "summary": "Multilingual domain adaptation (ML-DA) is widely used to learn new domain\nknowledge across languages into large language models (LLMs). Although many\nmethods have been proposed to improve domain adaptation, the mechanisms of\nmultilingual knowledge acquisition, how domain knowledge is learned within a\nlanguage and transferred across languages, remain underexplored. This gap leads\nto suboptimal performance, particularly in low-resource settings. This work\nexamines the learning dynamics of LLMs during ML-DA. Because prior ML-DA\nstudies often train and evaluate on datasets with mismatched knowledge\ncoverage, we propose AdaXEval, an adaptive evaluation method that builds\nmultiple-choice QA datasets from the same bilingual domain corpus used for\ntraining, thereby directly studying multilingual knowledge acquisition. Through\ncontinual training of LLMs with diverse data recipes, we track how LLMs acquire\ndomain facts and pinpoint the mechanism behind the transformation process from\ndomain training data to knowledge. Our experiments on a 13B English-Japanese\nbilingual LLM reveal that cross-lingual transfer remains challenging despite a\nhigh-quality bilingual corpus. The code has been released.", "AI": {"tldr": "This paper explores the dynamics of multilingual domain adaptation (ML-DA) in large language models (LLMs), highlighting challenges in cross-lingual transfer and introducing AdaXEval for evaluation.", "motivation": "To understand how domain knowledge is acquired and transferred across languages in multilingual domain adaptation (ML-DA), addressing gaps in prior research that limit performance, especially for low-resource languages.", "method": "They propose AdaXEval, an adaptive evaluation method that builds multiple-choice QA datasets from the same bilingual domain corpus used for training. Through continual training with diverse data recipes, they study how domain knowledge is acquired and transferred.", "result": "Experiments on a 13B English-Japanese bilingual LLM demonstrate that cross-lingual knowledge transfer remains challenging, even with high-quality bilingual datasets.", "conclusion": "The study provides insights into the mechanisms of multilingual knowledge acquisition and calls attention to the difficulties in achieving effective cross-lingual transfer in ML-DA, emphasizing the need for improved methodologies."}}
{"id": "2510.12717", "pdf": "https://arxiv.org/pdf/2510.12717", "abs": "https://arxiv.org/abs/2510.12717", "authors": ["Se Hwan Jeon", "Ho Jae Lee", "Seungwoo Hong", "Sangbae Kim"], "title": "Residual MPC: Blending Reinforcement Learning with GPU-Parallelized Model Predictive Control", "categories": ["cs.RO"], "comment": "TRO submission preprint", "summary": "Model Predictive Control (MPC) provides interpretable, tunable locomotion\ncontrollers grounded in physical models, but its robustness depends on frequent\nreplanning and is limited by model mismatch and real-time computational\nconstraints. Reinforcement Learning (RL), by contrast, can produce highly\nrobust behaviors through stochastic training but often lacks interpretability,\nsuffers from out-of-distribution failures, and requires intensive reward\nengineering. This work presents a GPU-parallelized residual architecture that\ntightly integrates MPC and RL by blending their outputs at the torque-control\nlevel. We develop a kinodynamic whole-body MPC formulation evaluated across\nthousands of agents in parallel at 100 Hz for RL training. The residual policy\nlearns to make targeted corrections to the MPC outputs, combining the\ninterpretability and constraint handling of model-based control with the\nadaptability of RL. The model-based control prior acts as a strong bias,\ninitializing and guiding the policy towards desirable behavior with a simple\nset of rewards. Compared to standalone MPC or end-to-end RL, our approach\nachieves higher sample efficiency, converges to greater asymptotic rewards,\nexpands the range of trackable velocity commands, and enables zero-shot\nadaptation to unseen gaits and uneven terrain.", "AI": {"tldr": "This paper presents a GPU-parallelized architecture integrating Model Predictive Control (MPC) and Reinforcement Learning (RL) to leverage the strengths of both methods for robust and interpretable locomotion control.", "motivation": "The motivation is to address the robustness and computational limitations of MPC, improve RL's interpretability and adaptability, and combine both techniques for better performance in locomotion control.", "method": "The authors propose an architecture where a residual policy is trained using RL to make corrections to the outputs of a kinodynamic whole-body MPC. Training is conducted using GPU parallelization at 100 Hz.", "result": "The proposed approach demonstrates higher sample efficiency, achieves better asymptotic rewards, allows tracking of a wider range of velocity commands, and supports zero-shot adaptation to novel gaits and terrains compared to standalone MPC or RL.", "conclusion": "The integration of MPC and RL successfully combines their respective strengths, offering interpretability, constraint handling, and adaptability for robust locomotion across various scenarios."}}
{"id": "2510.12246", "pdf": "https://arxiv.org/pdf/2510.12246", "abs": "https://arxiv.org/abs/2510.12246", "authors": ["Jingyi Wang", "Hongyuan Zhu", "Ye Niu", "Yunhui Deng"], "title": "PromptFlow: Training Prompts Like Neural Networks", "categories": ["cs.AI"], "comment": "Comments: 18 pages, 14 figures, conference submission, appendix\n  included", "summary": "Large Language Models (LLMs) have demonstrated profound impact on Natural\nLanguage Processing (NLP) tasks. However, their effective deployment across\ndiverse domains often require domain-specific adaptation strategies, as generic\nmodels may underperform when faced with specialized data distributions. Recent\nadvances in prompt engineering (PE) offer a promising alternative to extensive\nretraining by refining input instructions to align LLM outputs with task\nobjectives. This paradigm has emerged as a rapid and versatile approach for\nmodel fine-tuning. Despite its potential, manual prompt design remains\nlabor-intensive and heavily depends on specialized expertise, often requiring\niterative human effort to achieve optimal formulations. To address this\nlimitation, automated prompt engineering methodologies have been developed to\nsystematically generate task-specific prompts. However, current implementations\npredominantly employ static update rules and lack mechanisms for dynamic\nstrategy selection, resulting in suboptimal adaptation to varying NLP task\nrequirements. Furthermore, most methods treat and update the whole prompts at\neach step, without considering editing prompt sections at a finer granularity.\nAt last, in particular, the problem of how to recycle experience in LLM is\nstill underexplored. To this end, we propose the PromptFlow, a modular training\nframework inspired by TensorFlow, which integrates meta-prompts, operators,\noptimization, and evaluator. Our framework can be equipped with the latest\noptimization methods and autonomously explores optimal prompt refinement\ntrajectories through gradient-based meta-learning, requiring minimal\ntask-specific training data. Specifically, we devise a reinforcement learning\nmethod to recycle experience for LLM in the PE process. Finally, we conduct\nextensive experiments on various datasets, and demonstrate the effectiveness of\nPromptFlow.", "AI": {"tldr": "Large Language Models (LLMs) require effective domain-specific adaptations for optimal performance. This paper introduces PromptFlow, a modular training framework for automated and dynamic prompt engineering, enhancing task-specific refinement.", "motivation": "The paper addresses limitations in current prompt engineering methodologies, such as static updates, coarse-grained prompt adjustments, and a lack of reinforcement mechanisms to recycle experience.", "method": "The proposed PromptFlow framework integrates elements like meta-prompts, operators, optimization, and evaluators to refine prompts dynamically using gradient-based meta-learning and reinforcement learning techniques.", "result": "Experiments across various datasets demonstrate the effectiveness of PromptFlow in achieving optimal prompt refinement trajectories with minimal task-specific data.", "conclusion": "PromptFlow successfully addresses challenges in prompt engineering, providing a modular and efficient framework for domain-specific LLM adaptations, thereby enhancing NLP task performance."}}
{"id": "2510.12174", "pdf": "https://arxiv.org/pdf/2510.12174", "abs": "https://arxiv.org/abs/2510.12174", "authors": ["Yusen Xie", "Zhenmin Huang", "Jianhao Jiao", "Dimitrios Kanoulas", "Jun Ma"], "title": "UniGS: Unified Geometry-Aware Gaussian Splatting for Multimodal Rendering", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "In this paper, we propose UniGS, a unified map representation and\ndifferentiable framework for high-fidelity multimodal 3D reconstruction based\non 3D Gaussian Splatting. Our framework integrates a CUDA-accelerated\nrasterization pipeline capable of rendering photo-realistic RGB images,\ngeometrically accurate depth maps, consistent surface normals, and semantic\nlogits simultaneously. We redesign the rasterization to render depth via\ndifferentiable ray-ellipsoid intersection rather than using Gaussian centers,\nenabling effective optimization of rotation and scale attribute through\nanalytic depth gradients. Furthermore, we derive the analytic gradient\nformulation for surface normal rendering, ensuring geometric consistency among\nreconstructed 3D scenes. To improve computational and storage efficiency, we\nintroduce a learnable attribute that enables differentiable pruning of\nGaussians with minimal contribution during training. Quantitative and\nqualitative experiments demonstrate state-of-the-art reconstruction accuracy\nacross all modalities, validating the efficacy of our geometry-aware paradigm.\nSource code and multimodal viewer will be available on GitHub.", "AI": {"tldr": "This paper introduces UniGS, a unified framework for multimodal 3D reconstruction, leveraging 3D Gaussian Splatting and a CUDA-accelerated pipeline.", "motivation": "The paper aims to address limitations in existing multimodal 3D reconstruction methods by introducing a unified framework to improve accuracy, consistency, and efficiency.", "method": "It employs a redesigned rasterization pipeline for differentiable rendering of depth, normals, RGB images, and semantics, alongside a learnable attribute for pruning irrelevant components during training.", "result": "Experimental results show state-of-the-art reconstruction accuracy across RGB, depth, surface normals, and semantics, verifying the paradigm's efficacy.", "conclusion": "UniGS provides a significant advancement in multimodal 3D reconstruction techniques, combining unified representation and computational efficiency with publicly available resources."}}
{"id": "2510.11984", "pdf": "https://arxiv.org/pdf/2510.11984", "abs": "https://arxiv.org/abs/2510.11984", "authors": ["Mattia Scardecchia"], "title": "Learning by Steering the Neural Dynamics: A Statistical Mechanics Perspective", "categories": ["cs.LG"], "comment": null, "summary": "Despite the striking successes of deep neural networks trained with\ngradient-based optimization, these methods differ fundamentally from their\nbiological counterparts. This gap raises key questions about how nature\nachieves robust, sample-efficient learning at minimal energy costs and solves\nthe credit-assignment problem without backpropagation. We take a step toward\nbridging contemporary AI and computational neuroscience by studying how neural\ndynamics can support fully local, distributed learning that scales to simple\nmachine-learning benchmarks. Using tools from statistical mechanics, we\nidentify conditions for the emergence of robust dynamical attractors in random\nasymmetric recurrent networks. We derive a closed-form expression for the\nnumber of fixed points as a function of self-coupling strength, and we reveal a\nphase transition in their structure: below a critical self-coupling, isolated\nfixed points coexist with exponentially many narrow clusters showing the\noverlap-gap property; above it, subdominant yet dense and extensive clusters\nappear. These fixed points become accessible, including to a simple\nasynchronous dynamical rule, after an algorithm-dependent self-coupling\nthreshold. Building on this analysis, we propose a biologically plausible\nalgorithm for supervised learning with any binary recurrent network. Inputs are\nmapped to fixed points of the dynamics, by relaxing under transient external\nstimuli and stabilizing the resulting configurations via local plasticity. We\nshow that our algorithm can learn an entangled version of MNIST, leverages\ndepth to develop hierarchical representations and increase hetero-association\ncapacity, and is applicable to several architectures. Finally, we highlight the\nstrong connection between algorithm performance and the unveiled phase\ntransition, and we suggest a cortex-inspired alternative to self-couplings for\nits emergence.", "AI": {"tldr": "The paper develops a biologically plausible learning algorithm for neural networks, leveraging fixed-point dynamics, local plasticity, and phase transitions identified through statistical mechanics.", "motivation": "To address the gap between artificial neural networks and biological ones, focusing on energy-efficient and robust learning mechanisms while solving the credit assignment problem without backpropagation.", "method": "Statistical mechanics is used to analyze asymmetrical recurrent networks, deriving fixed-point dynamics and a phase transition framework. A biologically plausible algorithm maps inputs to fixed points via transient stimuli and local plasticity.", "result": "The proposed algorithm successfully learns on benchmarks like MNIST, demonstrating hierarchical representation capabilities and enhanced hetero-association capacity across architectures.", "conclusion": "This study bridges AI and neuroscience by introducing biologically inspired neural dynamics and learning mechanisms, emphasizing the importance of phase transitions and offering new avenues for AI inspired by the cortex."}}
{"id": "2510.12337", "pdf": "https://arxiv.org/pdf/2510.12337", "abs": "https://arxiv.org/abs/2510.12337", "authors": ["Nina Drobac", "Margaux Br\u00e9g\u00e8re", "Joseph de Vilmarest", "Olivier Wintenberger"], "title": "Sliding-Window Signatures for Time Series: Application to Electricity Demand Forecasting", "categories": ["stat.ME", "stat.ML"], "comment": null, "summary": "Nonlinear and delayed effects of covariates often render time series\nforecasting challenging. To this end, we propose a novel forecasting framework\nbased on ridge regression with signature features calculated on sliding\nwindows. These features capture complex temporal dynamics without relying on\nlearned or hand-crafted representations. Focusing on the discrete-time setting,\nwe establish theoretical guarantees, namely universality of approximation and\nstationarity of signatures. We introduce an efficient sequential algorithm for\ncomputing signatures on sliding windows. The method is evaluated on both\nsynthetic and real electricity demand data. Results show that signature\nfeatures effectively encode temporal and nonlinear dependencies, yielding\naccurate forecasts competitive with those based on expert knowledge.", "AI": {"tldr": "The paper proposes a ridge regression method using signature features on sliding windows to improve time series forecasting, capturing complex temporal patterns without hand-crafted representations.", "motivation": "To address challenges in time series forecasting caused by nonlinear and delayed effects of covariates, and to find a method that does not rely on learned or manual feature representations.", "method": "The approach uses ridge regression combined with signature features extracted from sliding windows to model temporal dynamics. They develop a theoretical basis (universality of approximation and stationarity properties) and an efficient sequential algorithm for computation.", "result": "The method performs well with synthetic and real-world electricity demand datasets, demonstrating that signature features can encode temporal and nonlinear dependencies effectively\u2014producing competitive forecasts compared to expert-driven approaches.", "conclusion": "The framework provides an efficient, theoretically sound, and accurate methodology for time-series forecasting, particularly in situations with complex dependencies."}}
{"id": "2510.12116", "pdf": "https://arxiv.org/pdf/2510.12116", "abs": "https://arxiv.org/abs/2510.12116", "authors": ["Bajian Xiang", "Shuaijiang Zhao", "Tingwei Guo", "Wei Zou"], "title": "Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP 2025 (Main Conference)", "summary": "End-to-end Large Speech Language Models (LSLMs) have demonstrated impressive\nconversational generation abilities, yet consistently fall short of traditional\npipeline systems on semantic understanding benchmarks. In this work, we reveal\nthrough systematic experimentation that although LSLMs lose some text input\nperformance after speech-text alignment training, the performance gap between\nspeech and text inputs is more pronounced, which we refer to as the modality\ngap. To understand this gap, we analyze both coarse- and fine-grained text and\nspeech representations. At the coarse-grained level, representations of speech\nand text in deeper layers are found to be increasingly aligned in direction\n(cosine similarity), while concurrently diverging in magnitude (Euclidean\ndistance). We further find that representation similarity is strongly\ncorrelated with the modality gap. At the fine-grained level, a spontaneous\ntoken-level alignment pattern between text and speech representations is\nobserved. Based on this, we introduce the Alignment Path Score to quantify\ntoken-level alignment quality, which exhibits stronger correlation with the\nmodality gap. Building on these insights, we design targeted interventions on\ncritical tokens through angle projection and length normalization. These\nstrategies demonstrate the potential to improve correctness for speech inputs.\nOur study provides the first systematic empirical analysis of the modality gap\nand alignment mechanisms in LSLMs, offering both theoretical and methodological\nguidance for future optimization.", "AI": {"tldr": "This paper investigates the performance gap ('modality gap') between speech and text inputs in end-to-end Large Speech Language Models (LSLMs), presenting a systematic analysis of representation alignment and proposing interventions to mitigate the gap.", "motivation": "End-to-end LSLMs perform well in conversational tasks but fall short in semantic understanding benchmarks, especially when speech inputs are involved. This work aims to understand and address the performance gap between modalities.", "method": "The authors conduct systematic experiments to analyze coarse- and fine-grained text and speech representations, introduce metrics such as the Alignment Path Score, and design interventions like angle projection and length normalization.", "result": "Speech and text representations show alignment in cosine similarity but diverge in Euclidean distance. Representation alignment quality is correlated with the modality gap, and targeted interventions are shown to improve speech input performance.", "conclusion": "The study systematically analyzes the modality gap in LSLMs, revealing alignment mechanisms and providing approaches to optimize speech input performance for enhanced semantic understanding."}}
{"id": "2510.12724", "pdf": "https://arxiv.org/pdf/2510.12724", "abs": "https://arxiv.org/abs/2510.12724", "authors": ["Xin Fei", "Zhixuan Xu", "Huaicong Fang", "Tianrui Zhang", "Lin Shao"], "title": "T(R,O) Grasp: Efficient Graph Diffusion of Robot-Object Spatial Transformation for Cross-Embodiment Dexterous Grasping", "categories": ["cs.RO"], "comment": "12 pages, 14 figures", "summary": "Dexterous grasping remains a central challenge in robotics due to the\ncomplexity of its high-dimensional state and action space. We introduce T(R,O)\nGrasp, a diffusion-based framework that efficiently generates accurate and\ndiverse grasps across multiple robotic hands. At its core is the T(R,O) Graph,\na unified representation that models spatial transformations between robotic\nhands and objects while encoding their geometric properties. A graph diffusion\nmodel, coupled with an efficient inverse kinematics solver, supports both\nunconditioned and conditioned grasp synthesis. Extensive experiments on a\ndiverse set of dexterous hands show that T(R,O) Grasp achieves average success\nrate of 94.83%, inference speed of 0.21s, and throughput of 41 grasps per\nsecond on an NVIDIA A100 40GB GPU, substantially outperforming existing\nbaselines. In addition, our approach is robust and generalizable across\nembodiments while significantly reducing memory consumption. More importantly,\nthe high inference speed enables closed-loop dexterous manipulation,\nunderscoring the potential of T(R,O) Grasp to scale into a foundation model for\ndexterous grasping.", "AI": {"tldr": "The paper presents T(R,O) Grasp, a framework for efficient and diverse robotic dexterous grasping using diffusion models and inverse kinematics.", "motivation": "To address the complexity and challenges of dexterous grasping due to high-dimensional state and action spaces.", "method": "Introduces a unified T(R,O) Graph representation for hands and objects paired with a graph diffusion model and inverse kinematics solver for grasp synthesis.", "result": "Achieves a 94.83% success rate, 0.21s inference speed, and 41 grasps per second, outperforming current methods.", "conclusion": "T(R,O) Grasp is fast, accurate, memory-efficient, and scalable, demonstrating potential as a foundation model for dexterous grasping."}}
{"id": "2510.12264", "pdf": "https://arxiv.org/pdf/2510.12264", "abs": "https://arxiv.org/abs/2510.12264", "authors": ["Deyu Zou", "Yongqiang Chen", "Jianxiang Wang", "Haochen Yang", "Mufei Li", "James Cheng", "Pan Li", "Yu Gong"], "title": "$\\mathbf{T^3}$: Reducing Belief Deviation in Reinforcement Learning for Active Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Active reasoning requires large language models (LLMs) to interact with\nexternal sources and strategically gather information to solve problems.\nCentral to this process is belief tracking: maintaining a coherent\nunderstanding of the problem state and the missing information toward the\nsolution. However, due to limited reasoning capabilities, LLM-based agents\noften suffer from belief deviation: they struggle to correctly model beliefs,\nlose track of problem states, and fall into uninformative or repetitive\nactions. Once this happens, errors compound and reinforcement learning (RL)\ntraining fails to properly credit the crucial exploratory steps. To address\nthis issue, we propose to track the deviation of model beliefs and develop\n$\\mathbf{T^3}$, a simple yet effective method that detects excessive belief\ndeviation and truncates trajectories during training to remove uninformative\ntails. By preserving credit for informative prefixes, $\\mathbf{T^3}$\nsystematically improves policy optimization. Across 5 challenging tasks,\n$\\mathbf{T^3}$ consistently enhances training stability, token efficiency, and\nfinal performance, achieving up to 30% gains while cutting rollout tokens by\nroughly 25%. These results highlight belief control as a key principle for\ndeveloping robust and generalizable LLM-based active reasoners.", "AI": {"tldr": "The paper introduces $\\mathbf{T^3}$, a method that improves reasoning in large language models (LLMs) by addressing belief tracking issues, resulting in up to 30% performance gains and reduced token usage.", "motivation": "LLM-based agents often face difficulties with belief tracking, leading to errors, uninformative actions, and training inefficiencies, hampering their ability to perform active reasoning.", "method": "The authors propose $\\mathbf{T^3}$, a method that monitors belief deviations during training and truncates uninformative trajectories, crediting only the useful exploratory steps to enhance policy optimization.", "result": "Across five tasks, $\\mathbf{T^3}$ stabilizes training, reduces rollout tokens by roughly 25%, and improves performance, achieving up to 30% gains in various metrics.", "conclusion": "Belief control is crucial for creating robust LLM-based agents, and $\\mathbf{T^3}$ effectively improves their reasoning and performance by addressing belief deviation issues."}}
{"id": "2510.12182", "pdf": "https://arxiv.org/pdf/2510.12182", "abs": "https://arxiv.org/abs/2510.12182", "authors": ["Youngju Yoo", "Seho Kim", "Changick Kim"], "title": "BEEP3D: Box-Supervised End-to-End Pseudo-Mask Generation for 3D Instance Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "3D instance segmentation is crucial for understanding complex 3D\nenvironments, yet fully supervised methods require dense point-level\nannotations, resulting in substantial annotation costs and labor overhead. To\nmitigate this, box-level annotations have been explored as a weaker but more\nscalable form of supervision. However, box annotations inherently introduce\nambiguity in overlapping regions, making accurate point-to-instance assignment\nchallenging. Recent methods address this ambiguity by generating pseudo-masks\nthrough training a dedicated pseudo-labeler in an additional training stage.\nHowever, such two-stage pipelines often increase overall training time and\ncomplexity, hinder end-to-end optimization. To overcome these challenges, we\npropose BEEP3D-Box-supervised End-to-End Pseudo-mask generation for 3D instance\nsegmentation. BEEP3D adopts a student-teacher framework, where the teacher\nmodel serves as a pseudo-labeler and is updated by the student model via an\nExponential Moving Average. To better guide the teacher model to generate\nprecise pseudo-masks, we introduce an instance center-based query refinement\nthat enhances position query localization and leverages features near instance\ncenters. Additionally, we design two novel losses-query consistency loss and\nmasked feature consistency loss-to align semantic and geometric signals between\npredictions and pseudo-masks. Extensive experiments on ScanNetV2 and S3DIS\ndatasets demonstrate that BEEP3D achieves competitive or superior performance\ncompared to state-of-the-art weakly supervised methods while remaining\ncomputationally efficient.", "AI": {"tldr": "The paper presents BEEP3D, a computationally efficient, end-to-end method for 3D instance segmentation using box-level annotations, improving performance over existing weakly supervised approaches.", "motivation": "Fully supervised 3D instance segmentation is costly due to its reliance on dense point-level annotations; using box-level annotations as weak supervision is more scalable but introduces ambiguity in point-to-instance assignment.", "method": "The BEEP3D approach uses a student-teacher framework, enhancing pseudo-mask generation through an Exponential Moving Average update mechanism and introducing techniques like instance center-based query refinement and novel loss functions for better alignment.", "result": "BEEP3D consistently achieves competitive or superior performance compared to state-of-the-art weakly supervised models on benchmarks like ScanNetV2 and S3DIS, maintaining computational efficiency.", "conclusion": "BEEP3D optimizes the trade-off between annotation effort and segmentation performance, refining weakly supervised methods for practical applications in 3D environments."}}
{"id": "2510.11987", "pdf": "https://arxiv.org/pdf/2510.11987", "abs": "https://arxiv.org/abs/2510.11987", "authors": ["Conor Rowan"], "title": "Nonlinear discretizations and Newton's method: characterizing stationary points of regression objectives", "categories": ["cs.LG"], "comment": null, "summary": "Second-order methods are emerging as promising alternatives to standard\nfirst-order optimizers such as gradient descent and ADAM for training neural\nnetworks. Though the advantages of including curvature information in computing\noptimization steps have been celebrated in the scientific machine learning\nliterature, the only second-order methods that have been studied are\nquasi-Newton, meaning that the Hessian matrix of the objective function is\napproximated. Though one would expect only to gain from using the true Hessian\nin place of its approximation, we show that neural network training reliably\nfails when relying on exact curvature information. The failure modes provide\ninsight both into the geometry of nonlinear discretizations as well as the\ndistribution of stationary points in the loss landscape, leading us to question\nthe conventional wisdom that the loss landscape is replete with local minima.", "AI": {"tldr": "The paper investigates why training neural networks with exact second-order methods (using the true Hessian) fails, despite their theoretical benefits over quasi-Newton approaches.", "motivation": "To explore the practical challenges and failure modes of using exact second-order optimization methods for training neural networks, as only quasi-Newton approximations have been studied extensively.", "method": "Analyzing failure modes and their underlying causes when employing the true Hessian for neural network training, with a focus on geometry and stationary point distributions in the loss landscape.", "result": "It is shown that training fails reliably with exact curvature, providing insights into nonlinear discretizations and questioning the belief about the prevalence of local minima in the loss landscape.", "conclusion": "Using the true Hessian for neural networks does not work as theoretically expected due to geometric and stationary point distribution issues, challenging some existing assumptions about the landscape of loss functions."}}
{"id": "2510.12402", "pdf": "https://arxiv.org/pdf/2510.12402", "abs": "https://arxiv.org/abs/2510.12402", "authors": ["Lizhang Chen", "Jonathan Li", "Kaizhao Liang", "Baiyu Su", "Cong Xie", "Nuo Wang Pierse", "Chen Liang", "Ni Lao", "Qiang Liu"], "title": "Cautious Weight Decay", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": null, "summary": "We introduce Cautious Weight Decay (CWD), a one-line, optimizer-agnostic\nmodification that applies weight decay only to parameter coordinates whose\nsigns align with the optimizer update. Unlike standard decoupled decay, which\nimplicitly optimizes a regularized or constrained objective, CWD preserves the\noriginal loss and admits a bilevel interpretation: it induces sliding-mode\nbehavior upon reaching the stationary manifold, allowing it to search for\nlocally Pareto-optimal stationary points of the unmodified objective. In\npractice, CWD is a drop-in change for optimizers such as AdamW, Lion, and Muon,\nrequiring no new hyperparameters or additional tuning. For language model\npre-training and ImageNet classification, CWD consistently improves final loss\nand accuracy at million- to billion-parameter scales.", "AI": {"tldr": "Cautious Weight Decay (CWD) is a simple adjustment to optimization algorithms that applies weight decay only to parameters aligned with optimizer updates, improving final performance metrics in large-scale tasks.", "motivation": "Current weight decay approaches implicitly optimize a regularized objective, potentially deviating from the original unmodified objective.", "method": "CWD applies weight decay selectively\u2014only to parameters with updates aligned in sign. This does not modify the original loss objective and admits a bilevel optimization perspective.", "result": "Results on large-scale language model pre-training and ImageNet classification show consistent improvements in final loss and accuracy with optimizers like AdamW, Lion, and Muon.", "conclusion": "CWD is an easy-to-implement, optimizer-agnostic approach that offers better performance across diverse tasks without introducing new hyperparameters or requiring tuning."}}
{"id": "2510.12133", "pdf": "https://arxiv.org/pdf/2510.12133", "abs": "https://arxiv.org/abs/2510.12133", "authors": ["Han Zhu", "Juntao Dai", "Jiaming Ji", "Haoran Li", "Chengkun Cai", "Pengcheng Wen", "Chi-Min Chan", "Boyuan Chen", "Yaodong Yang", "Sirui Han", "Yike Guo"], "title": "SafeMT: Multi-turn Safety for Multimodal Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the widespread use of multi-modal Large Language models (MLLMs), safety\nissues have become a growing concern. Multi-turn dialogues, which are more\ncommon in everyday interactions, pose a greater risk than single prompts;\nhowever, existing benchmarks do not adequately consider this situation. To\nencourage the community to focus on the safety issues of these models in\nmulti-turn dialogues, we introduce SafeMT, a benchmark that features dialogues\nof varying lengths generated from harmful queries accompanied by images. This\nbenchmark consists of 10,000 samples in total, encompassing 17 different\nscenarios and four jailbreak methods. Additionally, we propose Safety Index\n(SI) to evaluate the general safety of MLLMs during conversations. We assess\nthe safety of 17 models using this benchmark and discover that the risk of\nsuccessful attacks on these models increases as the number of turns in harmful\ndialogues rises. This observation indicates that the safety mechanisms of these\nmodels are inadequate for recognizing the hazard in dialogue interactions. We\npropose a dialogue safety moderator capable of detecting malicious intent\nconcealed within conversations and providing MLLMs with relevant safety\npolicies. Experimental results from several open-source models indicate that\nthis moderator is more effective in reducing multi-turn ASR compared to existed\nguard models.", "AI": {"tldr": "The paper introduces SafeMT, a benchmark for evaluating the safety of multi-modal Large Language Models (MLLMs) in multi-turn dialogues, particularly concerning harmful interactions. It includes 10,000 samples and proposes a Safety Index for assessment.", "motivation": "Safety concerns over the increasing use of multi-modal Large Language Models (MLLMs) in multi-turn dialogues motivated the study, as existing benchmarks insufficiently address this issue.", "method": "The study presents SafeMT, a benchmark with 10,000 samples covering 17 scenarios and 4 jailbreak methods. A new metric, Safety Index (SI), is also introduced to assess safety. It further proposes a dialogue safety moderator to detect malicious intent and apply relevant safety policies.", "result": "Testing 17 models revealed that safety risks increase with the number of dialogue turns, indicating existing safety systems' inadequacy. The proposed dialogue safety moderator reduced attack success rates (ASR) better than current safety models.", "conclusion": "The authors highlight critical vulnerabilities in MLLMs' safety mechanisms for multi-turn dialogues. SafeMT and the dialogue safety moderator propose paths for improved safety in complex conversational contexts."}}
{"id": "2510.12733", "pdf": "https://arxiv.org/pdf/2510.12733", "abs": "https://arxiv.org/abs/2510.12733", "authors": ["Hang Yu", "Julian Jordan", "Julian Schmidt", "Silvan Lindner", "Alessandro Canevaro", "Wilhelm Stork"], "title": "HYPE: Hybrid Planning with Ego Proposal-Conditioned Predictions", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Safe and interpretable motion planning in complex urban environments needs to\nreason about bidirectional multi-agent interactions. This reasoning requires to\nestimate the costs of potential ego driving maneuvers. Many existing planners\ngenerate initial trajectories with sampling-based methods and refine them by\noptimizing on learned predictions of future environment states, which requires\na cost function that encodes the desired vehicle behavior. Designing such a\ncost function can be very challenging, especially if a wide range of complex\nurban scenarios has to be considered. We propose HYPE: HYbrid Planning with Ego\nproposal-conditioned predictions, a planner that integrates multimodal\ntrajectory proposals from a learned proposal model as heuristic priors into a\nMonte Carlo Tree Search (MCTS) refinement. To model bidirectional interactions,\nwe introduce an ego-conditioned occupancy prediction model, enabling\nconsistent, scene-aware reasoning. Our design significantly simplifies cost\nfunction design in refinement by considering proposal-driven guidance,\nrequiring only minimalistic grid-based cost terms. Evaluations on large-scale\nreal-world benchmarks nuPlan and DeepUrban show that HYPE effectively achieves\nstate-of-the-art performance, especially in safety and adaptability.", "AI": {"tldr": "This paper introduces HYPE, a hybrid motion planning framework that integrates learned trajectory proposals into Monte Carlo Tree Search to simplify cost function design and improve safety and adaptability in complex urban driving scenarios.", "motivation": "Developing safe and interpretable motion planning systems for urban environments requires effective reasoning about complex bidirectional multi-agent interactions and manageable cost function design.", "method": "HYPE combines multimodal trajectory proposals from a learned model as priors into Monte Carlo Tree Search refinement, supported by an ego-conditioned occupancy prediction model to handle bidirectional interactions.", "result": "HYPE achieves state-of-the-art performance on large-scale real-world benchmarks, excelling particularly in safety and adaptability metrics.", "conclusion": "The paper simplifies cost function design and demonstrates the practicality of integrating learned trajectory models into motion planning, advancing the field of safe urban driving."}}
{"id": "2510.12184", "pdf": "https://arxiv.org/pdf/2510.12184", "abs": "https://arxiv.org/abs/2510.12184", "authors": ["Jiwan Kim", "Kibum Kim", "Sangwoo Seo", "Chanyoung Park"], "title": "CompoDistill: Attention Distillation for Compositional Reasoning in Multimodal LLMs", "categories": ["cs.CV", "cs.AI"], "comment": "Preprint. Under Review", "summary": "Recently, efficient Multimodal Large Language Models (MLLMs) have gained\nsignificant attention as a solution to their high computational complexity,\nmaking them more practical for real-world applications. In this regard, the\nknowledge distillation (KD) approach has emerged as a promising alternative,\nwhich transfers the rich visual and linguistic knowledge from a larger model\n(teacher) to a smaller model (student). However, we observe that existing KD\nmethods struggle to effectively distill the teacher MLLM's rich visual\nperception abilities to the student, a challenge that has been largely\noverlooked in previous studies. Through a systematic analysis, we identify\nvisual attention misalignment between student and teacher as the main cause of\nthis issue. Based on this insight, we propose CompoDistill, a novel KD\nframework that explicitly aligns the student's visual attention with that of\nthe teacher to enhance the student's visual perception abilities. Our extensive\nexperiments show that CompoDistill significantly improves performance on\ncompositional reasoning tasks that require visual perception abilities while\nmaintaining strong performance on visual question answering tasks, as done in\nexisting studies. Furthermore, CompoDistill demonstrates effectiveness with a\nmore advanced backbone, highlighting its generalizability.", "AI": {"tldr": "The paper introduces CompoDistill, a knowledge distillation framework aligning student and teacher models' visual attention, improving multimodal tasks.", "motivation": "Efficient Multimodal Large Language Models face computational challenges; existing knowledge distillation methods fail to effectively transfer visual perception abilities.", "method": "CompoDistill aligns visual attention between teacher and student models to improve the student's visual perception abilities.", "result": "Experiments show CompoDistill boosts compositional reasoning and visual question answering tasks while being adaptable to advanced model architectures.", "conclusion": "CompoDistill enhances knowledge distillation in MLLMs by addressing visual attention misalignment, improving performance and generalizability."}}
{"id": "2510.12489", "pdf": "https://arxiv.org/pdf/2510.12489", "abs": "https://arxiv.org/abs/2510.12489", "authors": ["Beibu Li", "Qichao Shentu", "Yang Shu", "Hui Zhang", "Ming Li", "Ning Jin", "Bin Yang", "Chenjuan Guo"], "title": "CrossAD: Time Series Anomaly Detection with Cross-scale Associations and Cross-window Modeling", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted by the thirty-ninth annual conference on Neural Information\n  Processing Systems", "summary": "Time series anomaly detection plays a crucial role in a wide range of\nreal-world applications. Given that time series data can exhibit different\npatterns at different sampling granularities, multi-scale modeling has proven\nbeneficial for uncovering latent anomaly patterns that may not be apparent at a\nsingle scale. However, existing methods often model multi-scale information\nindependently or rely on simple feature fusion strategies, neglecting the\ndynamic changes in cross-scale associations that occur during anomalies.\nMoreover, most approaches perform multi-scale modeling based on fixed sliding\nwindows, which limits their ability to capture comprehensive contextual\ninformation. In this work, we propose CrossAD, a novel framework for time\nseries Anomaly Detection that takes Cross-scale associations and Cross-window\nmodeling into account. We propose a cross-scale reconstruction that\nreconstructs fine-grained series from coarser series, explicitly capturing\ncross-scale associations. Furthermore, we design a query library and\nincorporate global multi-scale context to overcome the limitations imposed by\nfixed window sizes. Extensive experiments conducted on multiple real-world\ndatasets using nine evaluation metrics validate the effectiveness of CrossAD,\ndemonstrating state-of-the-art performance in anomaly detection.", "AI": {"tldr": "CrossAD introduces a framework addressing anomaly detection in time series by emphasizing cross-scale and cross-window modeling. It achieves state-of-the-art results.", "motivation": "Current anomaly detection methods fail to dynamically model cross-scale associations and are limited by fixed sliding windows, which hinders their ability to capture comprehensive contextual information.", "method": "CrossAD uses a cross-scale reconstruction approach to capture associations between fine-grained and coarser series. Additionally, it incorporates a query library and multi-scale global context to surpass the limitations of fixed window sizes.", "result": "Experiments on real-world datasets across nine evaluation metrics demonstrated the method\u2019s state-of-the-art performance in detecting anomalies.", "conclusion": "CrossAD effectively addresses gaps in multi-scale and sliding window modeling for time series anomaly detection, enhancing detection performance through advanced cross-scale and contextual modeling techniques."}}
{"id": "2510.12137", "pdf": "https://arxiv.org/pdf/2510.12137", "abs": "https://arxiv.org/abs/2510.12137", "authors": ["Shihao Ji", "Zihui Song", "Jiajie Huang"], "title": "Credal Transformer: A Principled Approach for Quantifying and Mitigating Hallucinations in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) hallucinate, generating factually incorrect yet\nconfident assertions. We argue this stems from the Transformer's Softmax\nfunction, which creates \"Artificial Certainty\" by collapsing ambiguous\nattention scores into a single probability distribution, discarding uncertainty\ninformation at each layer. To fix this, we introduce the Credal Transformer,\nwhich replaces standard attention with a Credal Attention Mechanism (CAM) based\non evidential theory. CAM produces a \"credal set\" (a set of distributions)\ninstead of a single attention vector, with the set's size directly measuring\nmodel uncertainty. We implement this by re-conceptualizing attention scores as\nevidence masses for a Dirichlet distribution: sufficient evidence recovers\nstandard attention, while insufficient evidence yields a diffuse distribution,\nrepresenting ambiguity. Empirically, the Credal Transformer identifies\nout-of-distribution inputs, quantifies ambiguity, and significantly reduces\nconfident errors on unanswerable questions by abstaining. Our contribution is a\nnew architecture to mitigate hallucinations and a design paradigm that\nintegrates uncertainty quantification directly into the model, providing a\nfoundation for more reliable AI.", "AI": {"tldr": "The paper introduces the Credal Transformer, addressing hallucination issues in Large Language Models (LLMs) by incorporating uncertainty quantification into the attention mechanism.", "motivation": "To tackle the problem of hallucinations (incorrect yet confident outputs) in LLMs caused by the Softmax function collapsing ambiguous attention scores into a single probability distribution.", "method": "The paper proposes replacing the standard attention mechanism with a Credal Attention Mechanism (CAM) based on evidential theory. CAM creates a set of distributions to quantify uncertainty by re-conceptualizing attention scores as evidence masses for a Dirichlet distribution.", "result": "The Credal Transformer is able to detect out-of-distribution inputs, measure ambiguity, and significantly lower the generation of confident errors in unanswerable scenarios through model abstention.", "conclusion": "Credal Transformer represents a novel architecture that mitigates hallucinations in LLMs, integrates uncertainty quantification, and provides a direction for building more reliable AI systems."}}
{"id": "2510.11754", "pdf": "https://arxiv.org/pdf/2510.11754", "abs": "https://arxiv.org/abs/2510.11754", "authors": ["Dongrong Yang", "Xin Wu", "Yibo Xie", "Xinyi Li", "Qiuwen Wu", "Jackie Wu", "Yang Sheng"], "title": "Zero-Shot Large Language Model Agents for Fully Automated Radiotherapy Treatment Planning", "categories": ["physics.med-ph", "cs.AI", "cs.RO"], "comment": "Accepted for poster presentation at the NeurIPS 2025 Workshop on\n  GenAI for Health: Potential, Trust, and Policy Compliance", "summary": "Radiation therapy treatment planning is an iterative, expertise-dependent\nprocess, and the growing burden of cancer cases has made reliance on manual\nplanning increasingly unsustainable, underscoring the need for automation. In\nthis study, we propose a workflow that leverages a large language model\n(LLM)-based agent to navigate inverse treatment planning for\nintensity-modulated radiation therapy (IMRT). The LLM agent was implemented to\ndirectly interact with a clinical treatment planning system (TPS) to\niteratively extract intermediate plan states and propose new constraint values\nto guide inverse optimization. The agent's decision-making process is informed\nby current observations and previous optimization attempts and evaluations,\nallowing for dynamic strategy refinement. The planning process was performed in\na zero-shot inference setting, where the LLM operated without prior exposure to\nmanually generated treatment plans and was utilized without any fine-tuning or\ntask-specific training. The LLM-generated plans were evaluated on twenty\nhead-and-neck cancer cases against clinical manual plans, with key dosimetric\nendpoints analyzed and reported. The LLM-generated plans achieved comparable\norgan-at-risk (OAR) sparing relative to clinical plans while demonstrating\nimproved hot spot control (Dmax: 106.5% vs. 108.8%) and superior conformity\n(conformity index: 1.18 vs. 1.39 for boost PTV; 1.82 vs. 1.88 for primary PTV).\nThis study demonstrates the feasibility of a zero-shot, LLM-driven workflow for\nautomated IMRT treatment planning in a commercial TPS. The proposed approach\nprovides a generalizable and clinically applicable solution that could reduce\nplanning variability and support broader adoption of AI-based planning\nstrategies.", "AI": {"tldr": "This study uses a large language model (LLM)-based agent for automated intensity-modulated radiation therapy (IMRT) treatment planning, showing feasibility and clinical comparability to manual planning.", "motivation": "The study aims to address the unsustainability of manual radiation therapy planning processes due to increasing cancer cases and the need for automation.", "method": "An LLM-based agent interacts with clinical treatment planning systems in a zero-shot setting, proposing dynamic constraints for inverse optimization without fine-tuning or prior training.", "result": "LLM-generated treatment plans demonstrated comparable organ-at-risk sparing, improved hot spot control, and superior conformity compared to manual plans over 20 head-and-neck cancer cases.", "conclusion": "The study showcases the potential of zero-shot LLM-driven workflows to standardize IMRT planning while enhancing clinical outcomes and supporting AI adoption in treatment planning."}}
{"id": "2510.12323", "pdf": "https://arxiv.org/pdf/2510.12323", "abs": "https://arxiv.org/abs/2510.12323", "authors": ["Zirui Guo", "Xubin Ren", "Lingrui Xu", "Jiahao Zhang", "Chao Huang"], "title": "RAG-Anything: All-in-One RAG Framework", "categories": ["cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm\nfor expanding Large Language Models beyond their static training limitations.\nHowever, a critical misalignment exists between current RAG capabilities and\nreal-world information environments. Modern knowledge repositories are\ninherently multimodal, containing rich combinations of textual content, visual\nelements, structured tables, and mathematical expressions. Yet existing RAG\nframeworks are limited to textual content, creating fundamental gaps when\nprocessing multimodal documents. We present RAG-Anything, a unified framework\nthat enables comprehensive knowledge retrieval across all modalities. Our\napproach reconceptualizes multimodal content as interconnected knowledge\nentities rather than isolated data types. The framework introduces dual-graph\nconstruction to capture both cross-modal relationships and textual semantics\nwithin a unified representation. We develop cross-modal hybrid retrieval that\ncombines structural knowledge navigation with semantic matching. This enables\neffective reasoning over heterogeneous content where relevant evidence spans\nmultiple modalities. RAG-Anything demonstrates superior performance on\nchallenging multimodal benchmarks, achieving significant improvements over\nstate-of-the-art methods. Performance gains become particularly pronounced on\nlong documents where traditional approaches fail. Our framework establishes a\nnew paradigm for multimodal knowledge access, eliminating the architectural\nfragmentation that constrains current systems. Our framework is open-sourced\nat: https://github.com/HKUDS/RAG-Anything.", "AI": {"tldr": "The paper introduces 'RAG-Anything,' a unified framework that expands retrieval-augmented generation (RAG) capabilities to effectively process multimodal knowledge repositories, achieving notable performance improvements over existing methods.", "motivation": "Address the limitations of existing RAG frameworks which are confined to textual data, preventing efficient knowledge retrieval from multimodal repositories.", "method": "The framework employs a dual-graph construction to represent cross-modal relationships and textual semantics unifiedly, alongside a cross-modal hybrid retrieval mechanism for structural and semantic reasoning.", "result": "RAG-Anything exhibits superior performance on multimodal benchmarks, especially with complex and long documents, outperforming state-of-the-art methods.", "conclusion": "This framework sets a new standard for multimodal knowledge retrieval, eliminating the fragmentation in current RAG systems and enhancing reasoning across diverse data types."}}
{"id": "2510.12190", "pdf": "https://arxiv.org/pdf/2510.12190", "abs": "https://arxiv.org/abs/2510.12190", "authors": ["Shingo Yokoi", "Kento Sasaki", "Yu Yamaguchi"], "title": "Hierarchical Reasoning with Vision-Language Models for Incident Reports from Dashcam Videos", "categories": ["cs.CV"], "comment": "2nd Place Winner, ICCV 2025 2COOOL Competition", "summary": "Recent advances in end-to-end (E2E) autonomous driving have been enabled by\ntraining on diverse large-scale driving datasets, yet autonomous driving models\nstill struggle in out-of-distribution (OOD) scenarios. The COOOL benchmark\ntargets this gap by encouraging hazard understanding beyond closed taxonomies,\nand the 2COOOL challenge extends it to generating human-interpretable incident\nreports. We present a hierarchical reasoning framework for incident report\ngeneration from dashcam videos that integrates frame-level captioning, incident\nframe detection, and fine-grained reasoning within vision-language models\n(VLMs). We further improve factual accuracy and readability through model\nensembling and a Blind A/B Scoring selection protocol. On the official 2COOOL\nopen leaderboard, our method ranks 2nd among 29 teams and achieves the best\nCIDEr-D score, producing accurate and coherent incident narratives. These\nresults indicate that hierarchical reasoning with VLMs is a promising direction\nfor accident analysis and for broader understanding of safety-critical traffic\nevents. The implementation and code are available at\nhttps://github.com/riron1206/kaggle-2COOOL-2nd-Place-Solution.", "AI": {"tldr": "This paper addresses challenges in autonomous driving in out-of-distribution (OOD) scenarios by introducing a hierarchical reasoning framework for incident report generation in dashcam videos, which achieves second place in the 2COOOL challenge.", "motivation": "To improve autonomous driving models' performance in OOD scenarios by enhancing their ability to generate human-interpretable incident reports based on dashcam videos.", "method": "The paper presents a hierarchical reasoning approach that combines frame-level captioning, incident frame detection, and fine-grained reasoning within vision-language models (VLMs), alongside model ensembling and a Blind A/B Scoring selection protocol.", "result": "The proposed method ranked 2nd among 29 teams on the 2COOOL leaderboard and achieved the best CIDEr-D score for generating accurate and coherent incident narratives.", "conclusion": "Hierarchical reasoning with vision-language models is shown to be effective in analyzing accidents and understanding safety-critical traffic events."}}
{"id": "2510.12060", "pdf": "https://arxiv.org/pdf/2510.12060", "abs": "https://arxiv.org/abs/2510.12060", "authors": ["Yi-Chung Chen", "David I. Inouye", "Jing Gao"], "title": "Your VAR Model is Secretly an Efficient and Explainable Generative Classifier", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Generative classifiers, which leverage conditional generative models for\nclassification, have recently demonstrated desirable properties such as\nrobustness to distribution shifts. However, recent progress in this area has\nbeen largely driven by diffusion-based models, whose substantial computational\ncost severely limits scalability. This exclusive focus on diffusion-based\nmethods has also constrained our understanding of generative classifiers. In\nthis work, we propose a novel generative classifier built on recent advances in\nvisual autoregressive (VAR) modeling, which offers a new perspective for\nstudying generative classifiers. To further enhance its performance, we\nintroduce the Adaptive VAR Classifier$^+$ (A-VARC$^+$), which achieves a\nsuperior trade-off between accuracy and inference speed, thereby significantly\nimproving practical applicability. Moreover, we show that the VAR-based method\nexhibits fundamentally different properties from diffusion-based methods. In\nparticular, due to its tractable likelihood, the VAR-based classifier enables\nvisual explainability via token-wise mutual information and demonstrates\ninherent resistance to catastrophic forgetting in class-incremental learning\ntasks.", "AI": {"tldr": "This paper explores an alternative to diffusion-based generative classifiers using a novel visual autoregressive (VAR) method and Adaptive VAR Classifier (A-VARC$^+$) for faster and more robust classification.", "motivation": "To address scalability limitations and broaden understanding of generative classifiers by exploring alternatives to diffusion-based models.", "method": "The proposed method introduces visual autoregressive (VAR) modeling to generative classification and enhances it using the Adaptive VAR Classifier$^+$ (A-VARC$^+$).", "result": "VAR-based classifiers are faster, more accurate, and offer unique properties such as visual explainability and resistance to catastrophic forgetting.", "conclusion": "VAR modeling provides a promising alternative to diffusion approaches, improving inference speed, accuracy, and introducing novel classifier properties."}}
{"id": "2510.12503", "pdf": "https://arxiv.org/pdf/2510.12503", "abs": "https://arxiv.org/abs/2510.12503", "authors": ["Huiyang Yi", "Yanyan He", "Duxin Chen", "Mingyu Kang", "He Wang", "Wenwu Yu"], "title": "The Robustness of Differentiable Causal Discovery in Misspecified Scenarios", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "comment": "accepted to ICLR 2025", "summary": "Causal discovery aims to learn causal relationships between variables from\ntargeted data, making it a fundamental task in machine learning. However,\ncausal discovery algorithms often rely on unverifiable causal assumptions,\nwhich are usually difficult to satisfy in real-world data, thereby limiting the\nbroad application of causal discovery in practical scenarios. Inspired by these\nconsiderations, this work extensively benchmarks the empirical performance of\nvarious mainstream causal discovery algorithms, which assume i.i.d. data, under\neight model assumption violations. Our experimental results show that\ndifferentiable causal discovery methods exhibit robustness under the metrics of\nStructural Hamming Distance and Structural Intervention Distance of the\ninferred graphs in commonly used challenging scenarios, except for scale\nvariation. We also provide the theoretical explanations for the performance of\ndifferentiable causal discovery methods. Finally, our work aims to\ncomprehensively benchmark the performance of recent differentiable causal\ndiscovery methods under model assumption violations, and provide the standard\nfor reasonable evaluation of causal discovery, as well as to further promote\nits application in real-world scenarios.", "AI": {"tldr": "This paper benchmarks differentiable causal discovery methods against model assumption violations in real-world data.", "motivation": "Causal discovery is limited by reliance on unverifiable assumptions, which hinders its practical application.", "method": "The study evaluates several causal discovery algorithms under eight model assumption violations and examines theoretical explanations behind their robustness.", "result": "Differentiable causal discovery methods display robustness in challenging scenarios but struggle with scale variation.", "conclusion": "The findings set a benchmark for evaluating causal discovery methods, enhancing their reliability for real-world applications."}}
{"id": "2510.12164", "pdf": "https://arxiv.org/pdf/2510.12164", "abs": "https://arxiv.org/abs/2510.12164", "authors": ["Ziqi Wang", "Boye Niu", "Zipeng Gao", "Zhi Zheng", "Tong Xu", "Linghui Meng", "Zhongli Li", "Jing Liu", "Yilong Chen", "Chen Zhu", "Hua Wu", "Haifeng Wang", "Enhong Chen"], "title": "A Survey on Parallel Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "With the increasing capabilities of Large Language Models (LLMs), parallel\nreasoning has emerged as a new inference paradigm that enhances reasoning\nrobustness by concurrently exploring multiple lines of thought before\nconverging on a final answer. It has become a significant trend to explore\nparallel reasoning to overcome the fragility of standard sequential methods and\nimprove practical performance. In this paper, we aim to survey and summarize\nthe progress and challenges of parallel reasoning. We first present a formal\ndefinition of parallel reasoning and clarify its distinction from related\nconcepts like Chain-of-Thought. Then, we organize and discuss advanced\ntechniques based on a novel taxonomy, including non-interactive reasoning,\ninteractive reasoning, and efficiency-focused decoding strategies.\nAdditionally, we explore various application scenarios, such as solving complex\nproblems and enhancing the reliability of LLM outputs.Finally, we highlight the\ncore challenges of parallel reasoning and suggest potential directions for\nfuture research. We hope that our work can provide a useful roadmap for\nbeginners and encourage more research on improving parallel reasoning methods.\nRelated source can be avaliable in\nhttps://github.com/PPPP-kaqiu/Awesome-Parallel-Reasoning.", "AI": {"tldr": "The paper surveys the progress of parallel reasoning with Large Language Models (LLMs), proposing a new taxonomy and identifying challenges and future directions for its advancement.", "motivation": "To address the fragility of traditional sequential reasoning methods in LLMs by exploring enhanced robustness through parallel reasoning.", "method": "The paper formally defines parallel reasoning, differentiates it from related concepts such as Chain-of-Thought, introduces a taxonomy of techniques including non-interactive reasoning, interactive reasoning, and efficiency-focused decoding strategies, and explores its applications and challenges.", "result": "The study consolidates state-of-the-art advancements, categorizes techniques, and identifies challenges in parallel reasoning to improve the robustness and reliability of LLM outputs.", "conclusion": "The authors offer a roadmap for beginners and researchers, urging the community to delve deeper into parallel reasoning methods and address existing challenges, fostering advancements in this domain of LLMs."}}
{"id": "2510.12350", "pdf": "https://arxiv.org/pdf/2510.12350", "abs": "https://arxiv.org/abs/2510.12350", "authors": ["Ayush Khaitan", "Vijay Ganesh"], "title": "O-Forge: An LLM + Computer Algebra Framework for Asymptotic Analysis", "categories": ["cs.AI", "03B35, 68W30, 68T05"], "comment": null, "summary": "Large language models have recently demonstrated advanced capabilities in\nsolving IMO and Putnam problems; yet their role in research mathematics has\nremained fairly limited. The key difficulty is verification: suggested proofs\nmay look plausible, but cannot be trusted without rigorous checking. We present\na framework, called LLM+CAS, and an associated tool, O-Forge, that couples\nfrontier LLMs with a computer algebra systems (CAS) in an In-Context Symbolic\nFeedback loop to produce proofs that are both creative and symbolically\nverified. Our focus is on asymptotic inequalities, a topic that often involves\ndifficult proofs and appropriate decomposition of the domain into the \"right\"\nsubdomains. Many mathematicians, including Terry Tao, have suggested that using\nAI tools to find the right decompositions can be very useful for research-level\nasymptotic analysis. In this paper, we show that our framework LLM+CAS turns\nout to be remarkably effective at proposing such decompositions via a\ncombination of a frontier LLM and a CAS. More precisely, we use an LLM to\nsuggest domain decomposition, and a CAS (such as Mathematica) that provides a\nverification of each piece axiomatically. Using this loop, we answer a question\nposed by Terence Tao: whether LLMs coupled with a verifier can be used to help\nprove intricate asymptotic inequalities. More broadly, we show how AI can move\nbeyond contest math towards research-level tools for professional\nmathematicians.", "AI": {"tldr": "The paper introduces LLM+CAS, a framework combining large language models (LLMs) and computer algebra systems (CAS), to generate and verify creative proofs in research-level mathematics. The approach is applied to solving asymptotic inequalities.", "motivation": "The difficulty in trustworthiness of AI-generated proofs limits their use in research mathematics. Verification is essential for proofs of complex problems like asymptotic inequalities.", "method": "The LLM+CAS framework combines a large language model to propose domain decompositions and a computer algebra system for symbolic verification in a feedback loop.", "result": "The framework successfully addresses a question posed by Terence Tao by helping to prove intricate asymptotic inequalities, demonstrating its utility for research-level mathematics.", "conclusion": "LLM+CAS shows that AI tools can assist mathematicians in research-level tasks and extend beyond contests to professional mathematical reasoning."}}
{"id": "2510.12208", "pdf": "https://arxiv.org/pdf/2510.12208", "abs": "https://arxiv.org/abs/2510.12208", "authors": ["Muammer Bay", "Timo von Marcard", "Dren Fazlija"], "title": "The Impact of Synthetic Data on Object Detection Model Performance: A Comparative Analysis with Real-World Data", "categories": ["cs.CV"], "comment": "18 pages, 12 figures, 2 tables. Code:\n  https://github.com/MuammerBay/omniverse-replicator-sim2real-analysis ; Data:\n  https://doi.org/10.5281/zenodo.17308406", "summary": "Recent advances in generative AI, particularly in computer vision (CV), offer\nnew opportunities to optimize workflows across industries, including logistics\nand manufacturing. However, many AI applications are limited by a lack of\nexpertise and resources, which forces a reliance on general-purpose models.\nSuccess with these models often requires domain-specific data for fine-tuning,\nwhich can be costly and inefficient. Thus, using synthetic data for fine-tuning\nis a popular, cost-effective alternative to gathering real-world data. This\nwork investigates the impact of synthetic data on the performance of object\ndetection models, compared to models trained on real-world data only,\nspecifically within the domain of warehouse logistics. To this end, we examined\nthe impact of synthetic data generated using the NVIDIA Omniverse Replicator\ntool on the effectiveness of object detection models in real-world scenarios.\nIt comprises experiments focused on pallet detection in a warehouse setting,\nutilizing both real and various synthetic dataset generation strategies. Our\nfindings provide valuable insights into the practical applications of synthetic\nimage data in computer vision, suggesting that a balanced integration of\nsynthetic and real data can lead to robust and efficient object detection\nmodels.", "AI": {"tldr": "This paper evaluates synthetic data's impact on object detection models in warehouse logistics, using NVIDIA Omniverse Replicator for experiments.", "motivation": "To address the limitation of real-world data for fine-tuning general-purpose AI models, especially in resource-constrained scenarios.", "method": "Generated synthetic data using NVIDIA Omniverse Replicator and evaluated its effect on object detection performance in a warehouse setting, comparing results with models trained on real-world data.", "result": "Found that balanced integration of synthetic and real data enhances object detection performance and provides cost efficiency.", "conclusion": "Synthetic data is a viable solution for improving object detection models, especially in domains like warehouse logistics, where real-world data is scarce or expensive."}}
{"id": "2510.12070", "pdf": "https://arxiv.org/pdf/2510.12070", "abs": "https://arxiv.org/abs/2510.12070", "authors": ["Sangmin Jo", "Jee Seok Yoon", "Wootaek Jeong", "Kwanseok Oh", "Heung-Il Suk"], "title": "MEASURE: Multi-scale Minimal Sufficient Representation Learning for Domain Generalization in Sleep Staging", "categories": ["cs.LG", "cs.AI"], "comment": "12 page, 7 figures, uses IEEE.sty", "summary": "Deep learning-based automatic sleep staging has significantly advanced in\nperformance and plays a crucial role in the diagnosis of sleep disorders.\nHowever, those models often struggle to generalize on unseen subjects due to\nvariability in physiological signals, resulting in degraded performance in\nout-of-distribution scenarios. To address this issue, domain generalization\napproaches have recently been studied to ensure generalized performance on\nunseen domains during training. Among those techniques, contrastive learning\nhas proven its validity in learning domain-invariant features by aligning\nsamples of the same class across different domains. Despite its potential, many\nexisting methods are insufficient to extract adequately domain-invariant\nrepresentations, as they do not explicitly address domain characteristics\nembedded within the unshared information across samples. In this paper, we\nposit that mitigating such domain-relevant attributes-referred to as excess\ndomain-relevant information-is key to bridging the domain gap. However, the\ndirect strategy to mitigate the domain-relevant attributes often overfits\nfeatures at the high-level information, limiting their ability to leverage the\ndiverse temporal and spectral information encoded in the multiple feature\nlevels. To address these limitations, we propose a novel MEASURE (Multi-scalE\nminimAl SUfficient Representation lEarning) framework, which effectively\nreduces domain-relevant information while preserving essential temporal and\nspectral features for sleep stage classification. In our exhaustive experiments\non publicly available sleep staging benchmark datasets, SleepEDF-20 and MASS,\nour proposed method consistently outperformed state-of-the-art methods. Our\ncode is available at : https://github.com/ku-milab/Measure", "AI": {"tldr": "The paper proposes the MEASURE framework to address challenges in automatic sleep staging due to variability in physiological signals, leveraging multi-scale minimal sufficient representation learning.", "motivation": "Current deep learning models struggle with generalizing to unseen subjects due to variability in physiological signals during sleep staging, which impacts performance, especially in out-of-distribution scenarios.", "method": "The authors introduce the MEASURE framework that addresses excess domain-relevant information using multi-scale representation learning to preserve essential temporal and spectral features while minimizing irrelevant domain-specific data.", "result": "The MEASURE framework consistently outperformed state-of-the-art methods on benchmark sleep staging datasets like SleepEDF-20 and MASS.", "conclusion": "MEASURE effectively enhances sleep staging performance by reducing domain-relevant variability and improving generalization, offering consistent results across datasets."}}
{"id": "2510.12700", "pdf": "https://arxiv.org/pdf/2510.12700", "abs": "https://arxiv.org/abs/2510.12700", "authors": ["Vicente Bosca", "Tatum Rask", "Sunia Tanweer", "Andrew R. Tawfeek", "Branden Stone"], "title": "Topological Signatures of ReLU Neural Network Activation Patterns", "categories": ["cs.LG", "cs.AI", "cs.CG", "math.AT", "stat.ML"], "comment": null, "summary": "This paper explores the topological signatures of ReLU neural network\nactivation patterns. We consider feedforward neural networks with ReLU\nactivation functions and analyze the polytope decomposition of the feature\nspace induced by the network. Mainly, we investigate how the Fiedler partition\nof the dual graph and show that it appears to correlate with the decision\nboundary -- in the case of binary classification. Additionally, we compute the\nhomology of the cellular decomposition -- in a regression task -- to draw\nsimilar patterns in behavior between the training loss and polyhedral\ncell-count, as the model is trained.", "AI": {"tldr": "The paper studies topological aspects of ReLU neural network activation patterns, focusing on the dual graph's Fiedler partition correlation with decision boundaries and cellular decomposition's homology behavior.", "motivation": "To understand how the polytope decomposition induced by ReLU neural networks relates to their decision-making and training behaviors.", "method": "Analyzed ReLU neural networks' polytope decomposition, investigated Fiedler partitions of the dual graph for classification, and computed cellular decomposition homology for regression tasks.", "result": "Found a correlation between Fiedler partitions and decision boundaries in binary classification, and identified patterns between training loss and cell count in regression tasks.", "conclusion": "Topological features like the Fiedler partition and polyhedral cell-count offer insights into ReLU neural networks' decision processes and training dynamics."}}
{"id": "2510.12167", "pdf": "https://arxiv.org/pdf/2510.12167", "abs": "https://arxiv.org/abs/2510.12167", "authors": ["Minghan Wang", "Thuy-Trang Vu", "Ehsan Shareghi", "Gholamreza Haffari"], "title": "Towards Inference-time Scaling for Continuous Space Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Inference-time scaling through multiple sample generation in combination with\nProcess- or Outcome-Reward Model (PRM or ORM) re-ranking has proven effective\nfor text-based reasoning in large language models. This paper investigates\nwhether such established techniques can be successfully adapted to reasoning in\nthe continuous space, using COCONUT (Hao et al. 2024) continuous space\nreasoning LM as the backbone. We demonstrate the feasibility of generating\ndiverse reasoning paths through dropout-based sampling. Our Pass@N analysis on\nthe generated samples reveals the potential that could enable a significant\ngain in performance akin to observed gain in the discrete space. However, we\nhighlight unique challenges faced for materializing this gain in the continuous\nthought space. In particular, working recipes for data generation and training\nPRM and ORM models in the discrete space unlocks only marginal improvements in\nthe continuous space. Through probing various aspects including geometric\nproperties and trajectory dynamics we identify the underlying reasons that\nprevent effective discrimination between correct and incorrect reasoning\n(essential for the functioning of PRM and ORM). Our findings reveal that\ncurrent limitations stem from the absence of key inductive biases in continuous\nthought representations. We argue that the training frameworks for continuous\nreasoning LMs require not only to optimize for accuracy but also to explicitly\nincorporate inductive biases that could be utilized during inference-time for\ndiscrimination of correct and incorrect thoughts.\\footnote{Our code and data\nwill be publicly available.}", "AI": {"tldr": "The paper explores adapting established inference-time scaling and re-ranking techniques to continuous space reasoning in large language models using COCONUT. While it demonstrates potential performance gains, obstacles such as lack of inductive biases are identified.", "motivation": "To investigate if techniques effective in discrete space can be successfully adapted for continuous space reasoning in large language models.", "method": "The paper uses dropout-based sampling for generating diverse reasoning paths and analyzes their effectiveness through Pass@N metrics while probing geometric and trajectory properties.", "result": "Demonstrates potential for performance gains in continuous reasoning, but highlights challenges in achieving discrimination between correct and incorrect reasoning paths.", "conclusion": "Techniques effective in discrete space minimally improve continuous space reasoning. The absence of inductive biases in continuous representations is a key challenge, requiring updated training frameworks that optimize accuracy and allow inductive biases."}}
{"id": "2510.12399", "pdf": "https://arxiv.org/pdf/2510.12399", "abs": "https://arxiv.org/abs/2510.12399", "authors": ["Yuyao Ge", "Lingrui Mei", "Zenghao Duan", "Tianhao Li", "Yujia Zheng", "Yiwei Wang", "Lexin Wang", "Jiayu Yao", "Tianyu Liu", "Yujun Cai", "Baolong Bi", "Fangda Guo", "Jiafeng Guo", "Shenghua Liu", "Xueqi Cheng"], "title": "A Survey of Vibe Coding with Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "The advancement of large language models (LLMs) has catalyzed a paradigm\nshift from code generation assistance to autonomous coding agents, enabling a\nnovel development methodology termed \"Vibe Coding\" where developers validate\nAI-generated implementations through outcome observation rather than\nline-by-line code comprehension. Despite its transformative potential, the\neffectiveness of this emergent paradigm remains under-explored, with empirical\nevidence revealing unexpected productivity losses and fundamental challenges in\nhuman-AI collaboration. To address this gap, this survey provides the first\ncomprehensive and systematic review of Vibe Coding with large language models,\nestablishing both theoretical foundations and practical frameworks for this\ntransformative development approach. Drawing from systematic analysis of over\n1000 research papers, we survey the entire vibe coding ecosystem, examining\ncritical infrastructure components including LLMs for coding, LLM-based coding\nagent, development environment of coding agent, and feedback mechanisms. We\nfirst introduce Vibe Coding as a formal discipline by formalizing it through a\nConstrained Markov Decision Process that captures the dynamic triadic\nrelationship among human developers, software projects, and coding agents.\nBuilding upon this theoretical foundation, we then synthesize existing\npractices into five distinct development models: Unconstrained Automation,\nIterative Conversational Collaboration, Planning-Driven, Test-Driven, and\nContext-Enhanced Models, thus providing the first comprehensive taxonomy in\nthis domain. Critically, our analysis reveals that successful Vibe Coding\ndepends not merely on agent capabilities but on systematic context engineering,\nwell-established development environments, and human-agent collaborative\ndevelopment models.", "AI": {"tldr": "The paper examines 'Vibe Coding,' a development method using large language models for autonomous coding, and identifies challenges in its productivity and collaboration. It introduces theoretical foundations and provides a taxonomy of development models.", "motivation": "The motivation is to explore the unexplored effectiveness of Vibe Coding, aiming to establish theoretical foundations, frameworks, and address productivity and collaboration challenges in autonomous coding with large language models.", "method": "The authors conducted a systematic review of over 1000 research papers, analyzed the ecosystem, introduced formalism through a Constrained Markov Decision Process, and synthesized practices into distinct development models.", "result": "The paper identifies factors like systematic context engineering, robust development environments, and collaborative models as critical for successful Vibe Coding.", "conclusion": "The study establishes Vibe Coding as a formal discipline and provides a taxonomy to enhance productivity and collaboration in human-agent coding systems."}}
{"id": "2510.12219", "pdf": "https://arxiv.org/pdf/2510.12219", "abs": "https://arxiv.org/abs/2510.12219", "authors": ["Vu Tram Anh Khuong", "Luu Tu Nguyen", "Thi Bich Phuong Man", "Thanh Ha Le", "Thi Duyen Ngo"], "title": "DIANet: A Phase-Aware Dual-Stream Network for Micro-Expression Recognition via Dynamic Images", "categories": ["cs.CV"], "comment": null, "summary": "Micro-expressions are brief, involuntary facial movements that typically last\nless than half a second and often reveal genuine emotions. Accurately\nrecognizing these subtle expressions is critical for applications in\npsychology, security, and behavioral analysis. However, micro-expression\nrecognition (MER) remains a challenging task due to the subtle and transient\nnature of facial cues and the limited availability of annotated data. While\ndynamic image (DI) representations have been introduced to summarize temporal\nmotion into a single frame, conventional DI-based methods often overlook the\ndistinct characteristics of different temporal phases within a\nmicro-expression. To address this issue, this paper proposes a novel\ndual-stream framework, DIANet, which leverages phase-aware dynamic images - one\nencoding the onset-to-apex phase and the other capturing the apex-to-offset\nphase. Each stream is processed by a dedicated convolutional neural network,\nand a cross-attention fusion module is employed to adaptively integrate\nfeatures from both streams based on their contextual relevance. Extensive\nexperiments conducted on three benchmark MER datasets (CASME-II, SAMM, and\nMMEW) demonstrate that the proposed method consistently outperforms\nconventional single-phase DI-based approaches. The results highlight the\nimportance of modeling temporal phase information explicitly and suggest a\npromising direction for advancing MER.", "AI": {"tldr": "The paper introduces DIANet, a dual-stream framework to enhance micro-expression recognition by using phase-aware dynamic images, and shows it outperforms existing methods.", "motivation": "Micro-expression recognition is crucial yet challenging due to subtle cues and limited annotated data. Existing methods often fail to incorporate distinct temporal phases effectively.", "method": "Proposes DIANet, utilizing dynamic images that encode onset-to-apex and apex-to-offset phases separately, processed through dedicated neural networks. Cross-attention fusion integrates features adaptively.", "result": "Experiments on three benchmark datasets show DIANet outperforms conventional single-phase methods, confirming the advantage of explicit phase modeling.", "conclusion": "Explicitly modeling temporal phase information improves micro-expression recognition, offering a promising direction for future research in this area."}}
{"id": "2510.12071", "pdf": "https://arxiv.org/pdf/2510.12071", "abs": "https://arxiv.org/abs/2510.12071", "authors": ["Jin Hwa Lee", "Matthew Smith", "Maxwell Adam", "Jesse Hoogland"], "title": "Influence Dynamics and Stagewise Data Attribution", "categories": ["cs.LG"], "comment": "28 pages, 15 figures", "summary": "Current training data attribution (TDA) methods treat the influence one\nsample has on another as static, but neural networks learn in distinct stages\nthat exhibit changing patterns of influence. In this work, we introduce a\nframework for stagewise data attribution grounded in singular learning theory.\nWe predict that influence can change non-monotonically, including sign flips\nand sharp peaks at developmental transitions. We first validate these\npredictions analytically and empirically in a toy model, showing that dynamic\nshifts in influence directly map to the model's progressive learning of a\nsemantic hierarchy. Finally, we demonstrate these phenomena at scale in\nlanguage models, where token-level influence changes align with known\ndevelopmental stages.", "AI": {"tldr": "The paper discusses a novel framework for stagewise training data attribution (TDA) to track dynamic influence changes in neural networks during distinct learning stages.", "motivation": "To address the limitation of current TDA methods that assume static influence while neural networks learn in evolving, stagewise patterns.", "method": "The framework is based on singular learning theory, featuring analytical and empirical validation using toy models and large-scale language models.", "result": "Influence shifts were found to be non-monotonic, including sign flips and peaks during developmental transitions, which were validated in both toy models and language models at scale.", "conclusion": "Stagewise TDA reveals dynamic, stage-aligned influences, enhancing understanding of how neural networks develop semantic hierarchies over training."}}
{"id": "2510.12795", "pdf": "https://arxiv.org/pdf/2510.12795", "abs": "https://arxiv.org/abs/2510.12795", "authors": ["Caner Korkmaz", "Brighton Nuwagira", "Bar\u0131\u015f Co\u015fkunuzer", "Tolga Birdal"], "title": "CuMPerLay: Learning Cubical Multiparameter Persistence Vectorizations", "categories": ["cs.CV", "cs.AI", "cs.LG", "math.AT", "stat.ML"], "comment": "Appears at ICCV 2025", "summary": "We present CuMPerLay, a novel differentiable vectorization layer that enables\nthe integration of Cubical Multiparameter Persistence (CMP) into deep learning\npipelines. While CMP presents a natural and powerful way to topologically work\nwith images, its use is hindered by the complexity of multifiltration\nstructures as well as the vectorization of CMP. In face of these challenges, we\nintroduce a new algorithm for vectorizing MP homologies of cubical complexes.\nOur CuMPerLay decomposes the CMP into a combination of individual, learnable\nsingle-parameter persistence, where the bifiltration functions are jointly\nlearned. Thanks to the differentiability, its robust topological feature\nvectors can be seamlessly used within state-of-the-art architectures such as\nSwin Transformers. We establish theoretical guarantees for the stability of our\nvectorization under generalized Wasserstein metrics. Our experiments on\nbenchmark medical imaging and computer vision datasets show the benefit\nCuMPerLay on classification and segmentation performance, particularly in\nlimited-data scenarios. Overall, CuMPerLay offers a promising direction for\nintegrating global structural information into deep networks for structured\nimage analysis.", "AI": {"tldr": "CuMPerLay is a novel differentiable layer that integrates Cubical Multiparameter Persistence into deep learning for structured image analysis.", "motivation": "To overcome difficulties in using CMP for image analysis due to multifiltration complexity and vectorization challenges.", "method": "A new algorithm for vectorizing CMP is developed, decomposing CMP into learnable single-parameter persistence paired with bifiltration function learning.", "result": "CuMPerLay demonstrates improved performance in classification and segmentation tasks, especially in low-data settings.", "conclusion": "CuMPerLay provides stable and robust topological features, showing promise for integrating geometric insights into deep image analysis."}}
{"id": "2510.12181", "pdf": "https://arxiv.org/pdf/2510.12181", "abs": "https://arxiv.org/abs/2510.12181", "authors": ["Chengrui Xiang", "Tengfei Ma", "Xiangzheng Fu", "Yiping Liu", "Bosheng Song", "Xiangxiang Zeng"], "title": "From Knowledge to Treatment: Large Language Model Assisted Biomedical Concept Representation for Drug Repurposing", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 4 figures, 13 tables. Accepted by EMNLP 2025 (Findings)", "summary": "Drug repurposing plays a critical role in accelerating treatment discovery,\nespecially for complex and rare diseases. Biomedical knowledge graphs (KGs),\nwhich encode rich clinical associations, have been widely adopted to support\nthis task. However, existing methods largely overlook common-sense biomedical\nconcept knowledge in real-world labs, such as mechanistic priors indicating\nthat certain drugs are fundamentally incompatible with specific treatments. To\naddress this gap, we propose LLaDR, a Large Language Model-assisted framework\nfor Drug Repurposing, which improves the representation of biomedical concepts\nwithin KGs. Specifically, we extract semantically enriched treatment-related\ntextual representations of biomedical entities from large language models\n(LLMs) and use them to fine-tune knowledge graph embedding (KGE) models. By\ninjecting treatment-relevant knowledge into KGE, LLaDR largely improves the\nrepresentation of biomedical concepts, enhancing semantic understanding of\nunder-studied or complex indications. Experiments based on benchmarks\ndemonstrate that LLaDR achieves state-of-the-art performance across different\nscenarios, with case studies on Alzheimer's disease further confirming its\nrobustness and effectiveness. Code is available at\nhttps://github.com/xiaomingaaa/LLaDR.", "AI": {"tldr": "The paper proposes LLaDR, a Large Language Model-assisted framework, to improve drug repurposing by enhancing biomedical knowledge graph representations.", "motivation": "Existing drug repurposing methods largely overlook common-sense biomedical knowledge and mechanistic priors necessary for real-world treatment compatibility.", "method": "The authors use large language models to extract semantically enriched textual representations of biomedical entities and fine-tune knowledge graph embedding models with this information.", "result": "The LLaDR framework significantly improves the representation of biomedical concepts in knowledge graphs and achieves state-of-the-art performance in drug repurposing benchmarks, with robust case study validation on Alzheimer's disease.", "conclusion": "LLaDR demonstrates how integrating large language models with knowledge graph embeddings can enhance understanding of complex or under-studied biomedical scenarios, advancing drug discovery efforts."}}
{"id": "2510.12360", "pdf": "https://arxiv.org/pdf/2510.12360", "abs": "https://arxiv.org/abs/2510.12360", "authors": ["Weijie Ren", "Haowen Liu", "Guang-Ren Duan"], "title": "A Unidirectionally Connected FAS Approach for 6-DOF Quadrotor Control", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": "This paper has been submitted to 2026 IFAC World Congress.\n  Corresponding author: Guang-Ren Duan", "summary": "This paper proposes a unidirectionally connected fully actuated system\n(UC-FAS) approach for the sub-stabilization and tracking control of 6-DOF\nquadrotors, tackling limitations both in state-space and FAS framework to some\nextent. The framework systematically converts underactuated quadrotor dynamics\ninto a UC-FAS model, unifying the existing different FAS transformation ways.\nBy eliminating estimation of the high-order derivatives of control inputs, a\ndrawback of current methods, the UC-FAS model simplifies controller design and\nenables direct eigenstructure assignment for closed-loop dynamics. Simulations\ndemonstrate precise 6-DOF tracking performance. This work bridges theoretical\nFAS approach advancements with practical implementation needs, offering a\nstandardized paradigm for nonlinear quadrotor control.", "AI": {"tldr": "This paper introduces a system for improving quadrotor control through a novel framework, simplifying controller design and achieving precise tracking.", "motivation": "The study aims to address limitations in state-space and the Fully Actuated System (FAS) framework for quadrotor control.", "method": "The proposed UC-FAS model transforms underactuated quadrotor dynamics into a unidirectionally connected fully actuated system, simplifying the design by eliminating high-order derivative estimations.", "result": "Simulations validate effective 6-DOF tracking performance.", "conclusion": "The UC-FAS model integrates theoretical advancements with practical control needs, establishing a standardized approach for nonlinear quadrotor dynamics."}}
{"id": "2510.12409", "pdf": "https://arxiv.org/pdf/2510.12409", "abs": "https://arxiv.org/abs/2510.12409", "authors": ["Yunuo Liu", "Dawei Zhu", "Zena Al-Khalili", "Dai Cheng", "Yanjun Chen", "Dietrich Klakow", "Wei Zhang", "Xiaoyu Shen"], "title": "PricingLogic: Evaluating LLMs Reasoning on Complex Tourism Pricing Tasks", "categories": ["cs.AI"], "comment": null, "summary": "We present PricingLogic, the first benchmark that probes whether Large\nLanguage Models(LLMs) can reliably automate tourism-related prices when\nmultiple, overlapping fare rules apply. Travel agencies are eager to offload\nthis error-prone task onto AI systems; however, deploying LLMs without verified\nreliability could result in significant financial losses and erode customer\ntrust. PricingLogic comprises 300 natural-language questions based on booking\nrequests derived from 42 real-world pricing policies, spanning two levels of\ndifficulty: (i) basic customer-type pricing and (ii)bundled-tour calculations\ninvolving interacting discounts. Evaluations of a line of LLMs reveal a steep\nperformance drop on the harder tier,exposing systematic failures in rule\ninterpretation and arithmetic reasoning.These results highlight that, despite\ntheir general capabilities, today's LLMs remain unreliable in revenue-critical\napplications without further safeguards or domain adaptation. Our code and\ndataset are available at https://github.com/EIT-NLP/PricingLogic.", "AI": {"tldr": "The paper introduces PricingLogic, a benchmark for assessing the ability of Large Language Models (LLMs) to automate pricing tasks in tourism, uncovered performance limitations on complex fare rules.", "motivation": "To address the need for reliable automation of tourism-related pricing tasks by LLMs to reduce errors and ensure financial stability and customer trust.", "method": "Developed PricingLogic, a benchmark with 300 natural-language questions from real-world scenarios that test rule interpretation and arithmetic reasoning skills of LLMs.", "result": "Evaluations reveal LLMs struggle with complex bundled-tour pricing involving discounts, showing reliability gaps in revenue-critical applications.", "conclusion": "Current LLMs require safeguards and adaptation to be deemed trustworthy in handling high-stakes tasks like tourism pricing."}}
{"id": "2510.12225", "pdf": "https://arxiv.org/pdf/2510.12225", "abs": "https://arxiv.org/abs/2510.12225", "authors": ["Hritik Bansal", "Devandra Singh Sachan", "Kai-Wei Chang", "Aditya Grover", "Gargi Ghosh", "Wen-tau Yih", "Ramakanth Pasunuru"], "title": "HoneyBee: Data Recipes for Vision-Language Reasoners", "categories": ["cs.CV", "cs.LG"], "comment": "32 pages", "summary": "Recent advances in vision-language models (VLMs) have made them highly\neffective at reasoning tasks. However, the principles underlying the\nconstruction of performant VL reasoning training datasets remain poorly\nunderstood. In this work, we introduce several data curation approaches and\nstudy their impacts on VL reasoning capabilities by carefully controlling\ntraining and evaluation setups. We analyze the effects of context (image and\nquestion pair) sources, implement targeted data interventions, and explore\nscaling up images, questions, and chain-of-thought (CoT) solutions. Our\nfindings reveal that (a) context source strategies significantly affect VLM\nperformance, (b) interventions such as auxiliary signals from image captions\nand the inclusion of text-only reasoning yield substantial gains, and (c)\nscaling all data dimensions (e.g., unique questions per image and unique CoTs\nper image-question pair) consistently improves reasoning capability. Motivated\nby these insights, we introduce HoneyBee, a large-scale, high-quality CoT\nreasoning dataset with 2.5M examples consisting 350K image-question pairs. VLMs\ntrained with HoneyBee outperform state-of-the-art models across model sizes.\nFor instance, a HoneyBee-trained VLM with 3B parameters outperforms the SOTA\nmodel and the base model by 7.8% and 24.8%, respectively, on MathVerse.\nFurthermore, we propose a test-time scaling strategy that reduces decoding cost\nby 73% without sacrificing accuracy. Overall, this work presents improved\nstrategies for VL reasoning dataset curation research.", "AI": {"tldr": "This paper studies how data curation affects the performance of vision-language models (VLMs) in reasoning tasks, introducing techniques like data interventions and scaling dimensions, and proposes HoneyBee, a new dataset yielding superior VLM performance.", "motivation": "Understanding the principles behind constructing effective training datasets for vision-language reasoning tasks to overcome gaps in current VLM training methods.", "method": "Various data curation approaches, including context source analysis, targeted interventions like auxiliary signals, and scaling dimensions (e.g., questions and CoTs per image), alongside creating and testing the HoneyBee dataset.", "result": "HoneyBee-trained VLMs surpass state-of-the-art models on benchmarks like MathVerse, achieving improved reasoning performance and reducing decoding costs using a new test-time scaling strategy.", "conclusion": "This paper shows that strategic data curation significantly boosts VL reasoning capabilities and introduces HoneyBee as a benchmark dataset to elevate VLM performance and efficiency."}}
{"id": "2510.12085", "pdf": "https://arxiv.org/pdf/2510.12085", "abs": "https://arxiv.org/abs/2510.12085", "authors": ["Heng Zhang", "Tianyi Zhang", "Yuling Shi", "Xiaodong Gu", "Yaomin Shen", "Haochen You", "Zijian Zhang", "Yilei Yuan", "Jin Huang"], "title": "GraphShaper: Geometry-aware Alignment for Improving Transfer Learning in Text-Attributed Graphs", "categories": ["cs.LG", "cs.GR"], "comment": null, "summary": "Graph foundation models represent a transformative paradigm for learning\ntransferable representations across diverse graph domains. Recent methods\nleverage large language models to unify graph and text modalities into a shared\nrepresentation space using contrastive learning. However, systematic\nevaluations reveal significant performance degradation at structural boundaries\nwhere distinct topological patterns converge, with accuracy losses exceeding 20\npercentage points. This issue arises from a key limitation: current methods\nassume all graph structures can be encoded within a single Euclidean space. In\nreality, tree structures require hyperbolic geometry to preserve hierarchical\nbranching, while cyclic patterns depend on spherical geometry for closure\nproperties. At structural boundaries, nodes experience conflicting geometric\nconstraints that uniform encoding spaces cannot resolve. This raises a crucial\nchallenge: \\textbf{Can alignment frameworks be designed to respect the\nintrinsic geometric diversity of graph structures?} We introduce\n\\textbf{GraphShaper}, a geometry-aware framework that enhances graph encoding\nthrough multi-geometric specialization. Our approach employs expert networks\ntailored to different geometric spaces, dynamically computing fusion weights to\nadaptively integrate geometric properties based on local structural\ncharacteristics. This adaptive fusion preserves structural integrity before\nalignment with text embeddings. Extensive experiments demonstrate that\nGraphShaper achieves 9.47\\% accuracy improvements on citation networks and\n7.63\\% on social networks in zero-shot settings.", "AI": {"tldr": "The paper introduces GraphShaper, a geometry-aware framework aimed at preserving geometric diversity of graph structures, addressing performance issues at structural boundaries in graph foundation models.", "motivation": "The paper was motivated by the observation that current graph foundation models suffer from performance losses at structural boundaries due to their reliance on single Euclidean encoding spaces, which cannot sufficiently accommodate diverse geometric needs like hierarchical branching and cyclic patterns.", "method": "The proposed GraphShaper framework employs expert networks that specialize in different geometries (e.g., hyperbolic and spherical). It dynamically calculates fusion weights to adaptively integrate these geometric properties based on local structural characteristics before aligning with text embeddings.", "result": "GraphShaper demonstrates significant accuracy improvements in zero-shot settings, achieving 9.47% better performance on citation networks and 7.63% on social networks.", "conclusion": "The paper concludes that incorporating geometry-aware adaptations through GraphShaper resolves geometric conflicts at structural boundaries, enhancing the structural integrity and performance of graph foundation models."}}
{"id": "2510.12185", "pdf": "https://arxiv.org/pdf/2510.12185", "abs": "https://arxiv.org/abs/2510.12185", "authors": ["Jiayu Yao", "Shenghua Liu", "Yiwei Wang", "Rundong Cheng", "Lingrui Mei", "Baolong Bi", "Zhen Xiong", "Xueqi Cheng"], "title": "Not in Sync: Unveiling Temporal Bias in Audio Chat Models", "categories": ["cs.CL", "cs.SD"], "comment": null, "summary": "Large Audio Language Models (LALMs) are increasingly applied to audio\nunderstanding and multimodal reasoning, yet their ability to locate when events\noccur remains underexplored. We present the first systematic study of temporal\nbias in LALMs, revealing a key limitation in their timestamp prediction. For\nexample, when asked \"At which second does the lecturer introduce the key\nformula?\", models often predict timestamps that are consistently earlier or\nlater than the ground truth. Through controlled experiments on timestamped\ndatasets, we find that temporal bias (i) is prevalent across datasets and\nmodels, (ii) increases with audio length - even accumulating to tens of seconds\nin extended recordings, and (iii) varies across event types and positions. We\nquantify this effect with the Temporal Bias Index (TBI), measuring systematic\nmisalignment in predicted event timings, and complement it with a visualization\nframework. Our findings highlight a fundamental limitation in current LALMs and\ncall for the development of temporally robust architectures.", "AI": {"tldr": "The study investigates the limitations of Large Audio Language Models (LALMs) in timestamp prediction, revealing prevalent temporal bias across datasets and models, which worsens with audio length and event types.", "motivation": "To analyze and address the overlooked issue of temporal bias in LALMs during timestamp prediction, which affects their use in audio and multimodal reasoning tasks.", "method": "Conducted controlled experiments on timestamped datasets, developed the Temporal Bias Index (TBI) to quantify misalignments, and implemented a visualization framework to illustrate results.", "result": "Temporal bias is prevalent, worsens with longer audio recordings, varies by event types, and leads to systematic misalignment of predicted event timings.", "conclusion": "Current LALMs exhibit significant temporal bias, calling for the development of architectures capable of robust temporal predictions."}}
{"id": "2510.12560", "pdf": "https://arxiv.org/pdf/2510.12560", "abs": "https://arxiv.org/abs/2510.12560", "authors": ["Xiaoji Zheng", "Ziyuan Yang", "Yanhao Chen", "Yuhang Peng", "Yuanrong Tang", "Gengyuan Liu", "Bokui Chen", "Jiangtao Gong"], "title": "CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in Latent World Models for Autonomous Driving", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "18 pages, 17 figures", "summary": "End-to-end autonomous driving models trained solely with imitation learning\n(IL) often suffer from poor generalization. In contrast, reinforcement learning\n(RL) promotes exploration through reward maximization but faces challenges such\nas sample inefficiency and unstable convergence. A natural solution is to\ncombine IL and RL. Moving beyond the conventional two-stage paradigm (IL\npretraining followed by RL fine-tuning), we propose CoIRL-AD, a competitive\ndual-policy framework that enables IL and RL agents to interact during\ntraining. CoIRL-AD introduces a competition-based mechanism that facilitates\nknowledge exchange while preventing gradient conflicts. Experiments on the\nnuScenes dataset show an 18% reduction in collision rate compared to baselines,\nalong with stronger generalization and improved performance on long-tail\nscenarios. Code is available at: https://github.com/SEU-zxj/CoIRL-AD.", "AI": {"tldr": "Combining imitation learning (IL) and reinforcement learning (RL) for autonomous driving, CoIRL-AD introduces a dual-policy framework to improve generalization and reduce collision rates.", "motivation": "Improving generalization and performance in end-to-end autonomous driving models, which often face challenges using only IL or RL.", "method": "A competitive dual-policy framework, CoIRL-AD, integrates IL and RL agents during training through a competition-based mechanism for better collaboration and gradient conflict prevention.", "result": "Experiments showed an 18% reduction in collisions and improved generalization and performance in long-tail scenarios, validated on the nuScenes dataset.", "conclusion": "The CoIRL-AD approach effectively combines IL and RL, promoting better exploration and knowledge exchange, addressing weaknesses of traditional methods, and improving autonomous driving outcomes."}}
{"id": "2510.12423", "pdf": "https://arxiv.org/pdf/2510.12423", "abs": "https://arxiv.org/abs/2510.12423", "authors": ["Dingyi Zuo", "Hongjie Zhang", "Jie Ou", "Chaosheng Feng", "Shuwan Liu"], "title": "MTOS: A LLM-Driven Multi-topic Opinion Simulation Framework for Exploring Echo Chamber Dynamics", "categories": ["cs.AI"], "comment": "14 pages, 11figures", "summary": "The polarization of opinions, information segregation, and cognitive biases\non social media have attracted significant academic attention. In real-world\nnetworks, information often spans multiple interrelated topics, posing\nchallenges for opinion evolution and highlighting the need for frameworks that\nsimulate interactions among topics. Existing studies based on large language\nmodels (LLMs) focus largely on single topics, limiting the capture of cognitive\ntransfer in multi-topic, cross-domain contexts. Traditional numerical models,\nmeanwhile, simplify complex linguistic attitudes into discrete values, lacking\ninterpretability, behavioral consistency, and the ability to integrate multiple\ntopics. To address these issues, we propose Multi-topic Opinion Simulation\n(MTOS), a social simulation framework integrating multi-topic contexts with\nLLMs. MTOS leverages LLMs alongside short-term and long-term memory,\nincorporates multiple user-selection interaction mechanisms and dynamic\ntopic-selection strategies, and employs a belief decay mechanism to enable\nperspective updates across topics. We conduct extensive experiments on MTOS,\nvarying topic numbers, correlation types, and performing ablation studies to\nassess features such as group polarization and local consistency. Results show\nthat multi-topic settings significantly alter polarization trends: positively\ncorrelated topics amplify echo chambers, negatively correlated topics inhibit\nthem, and irrelevant topics also mitigate echo chamber effects through resource\ncompetition. Compared with numerical models, LLM-based agents realistically\nsimulate dynamic opinion changes, reproduce linguistic features of news texts,\nand capture complex human reasoning, improving simulation interpretability and\nsystem stability.", "AI": {"tldr": "The paper presents Multi-topic Opinion Simulation (MTOS), a framework for simulating opinion evolution across multiple topics using LLMs.", "motivation": "Existing approaches oversimplify complex opinions and lack tools to simulate multi-topic interactions and cognitive transfers effectively.", "method": "Developing MTOS using large language models combined with memory mechanisms and dynamic topic strategies, studying opinion evolution with variables like topic correlations.", "result": "Results show that correlations among multiple topics affect echo chambers: positive correlations amplify them, negative correlations suppress them, and irrelevant topics reduce them via resource competition.", "conclusion": "MTOS improves simulation by incorporating linguistic realism, complex human reasoning, and stability compared to traditional numerical models."}}
{"id": "2510.12231", "pdf": "https://arxiv.org/pdf/2510.12231", "abs": "https://arxiv.org/abs/2510.12231", "authors": ["Victor Besnier", "David Hurych", "Andrei Bursuc", "Eduardo Valle"], "title": "BIGFix: Bidirectional Image Generation with Token Fixing", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in image and video generation have raised significant\ninterest from both academia and industry. A key challenge in this field is\nimproving inference efficiency, as model size and the number of inference steps\ndirectly impact the commercial viability of generative models while also posing\nfundamental scientific challenges. A promising direction involves combining\nauto-regressive sequential token modeling with multi-token prediction per step,\nreducing inference time by up to an order of magnitude. However, predicting\nmultiple tokens in parallel can introduce structural inconsistencies due to\ntoken incompatibilities, as capturing complex joint dependencies during\ntraining remains challenging. Traditionally, once tokens are sampled, there is\nno mechanism to backtrack and refine erroneous predictions. We propose a method\nfor self-correcting image generation by iteratively refining sampled tokens. We\nachieve this with a novel training scheme that injects random tokens in the\ncontext, improving robustness and enabling token fixing during sampling. Our\nmethod preserves the efficiency benefits of parallel token prediction while\nsignificantly enhancing generation quality. We evaluate our approach on image\ngeneration using the ImageNet-256 and CIFAR-10 datasets, as well as on video\ngeneration with UCF-101 and NuScenes, demonstrating substantial improvements\nacross both modalities.", "AI": {"tldr": "The paper introduces a method to refine token predictions in image and video generation, improving efficiency and quality.", "motivation": "To address inefficiency challenges in generative models caused by large model sizes and numerous inference steps, which impact commercial and scientific viability.", "method": "A training scheme that uses random token injection to allow iterative correction during the sampling process, enhancing robustness and quality.", "result": "The method significantly improved image generation on ImageNet-256 and CIFAR-10, and video generation on UCF-101 and NuScenes datasets.", "conclusion": "The proposed method balances the advantages of parallel token prediction with better generation quality, marking progress in efficient and high-quality generative modeling."}}
{"id": "2510.12094", "pdf": "https://arxiv.org/pdf/2510.12094", "abs": "https://arxiv.org/abs/2510.12094", "authors": ["Heng Zhang", "Tianyi Zhang", "Zijun Liu", "Yuling Shi", "Yaomin Shen", "Haochen You", "Haichuan Hu", "Lubin Gan", "Jin Huang"], "title": "H4G: Unlocking Faithful Inference for Zero-Shot Graph Learning in Hyperbolic Space", "categories": ["cs.LG", "cs.GR"], "comment": null, "summary": "Text-attributed graphs are widely used across domains, offering rich\nopportunities for zero-shot learning via graph-text alignment. However,\nexisting methods struggle with tasks requiring fine-grained pattern\nrecognition, particularly on heterophilic graphs. Through empirical and\ntheoretical analysis, we identify an \\textbf{over-abstraction problem}: current\napproaches operate at excessively large hyperbolic radii, compressing\nmulti-scale structural information into uniform high-level abstractions. This\nabstraction-induced information loss obscures critical local patterns essential\nfor accurate predictions. By analyzing embeddings in hyperbolic space, we\ndemonstrate that optimal graph learning requires \\textbf{faithful preservation}\nof fine-grained structural details, better retained by representations\npositioned closer to the origin. To address this, we propose \\textbf{H4G}, a\nframework that systematically reduces embedding radii using learnable\nblock-diagonal scaling matrices and M\\\"obius matrix multiplication. This\napproach restores access to fine-grained patterns while maintaining global\nreceptive ability with minimal computational overhead. Experiments show H4G\nachieves state-of-the-art zero-shot performance with \\textbf{12.8\\%}\nimprovement on heterophilic graphs and \\textbf{8.4\\%} on homophilic graphs,\nconfirming that radius reduction enables faithful multi-scale representation\nfor advancing zero-shot graph learning.", "AI": {"tldr": "This paper introduces H4G, a new framework for zero-shot graph learning, addressing over-abstraction issues in existing methods by adjusting embedding radii for better fine-grained structural representation.", "motivation": "The study aims to address the over-abstraction problem in existing graph learning methods, which lose local patterns needed for accurate predictions, especially in heterophilic graphs.", "method": "The proposed method, H4G, uses learnable block-diagonal scaling matrices and M\u00f6bius matrix multiplication to reduce embedding radii, retaining fine-grained structural details while maintaining global representation.", "result": "Experiments demonstrate that H4G achieves state-of-the-art performance, improving zero-shot learning by 12.8% on heterophilic graphs and 8.4% on homophilic graphs.", "conclusion": "H4G effectively captures fine-grained structural details, advancing faithful multi-scale representation and enhancing zero-shot graph learning performance with minimal computational cost."}}
{"id": "2510.12195", "pdf": "https://arxiv.org/pdf/2510.12195", "abs": "https://arxiv.org/abs/2510.12195", "authors": ["Zeyu Yang", "Satoshi Nakamura"], "title": "DPO-Tuned Large Language Models for Segmentation in Simultaneous Speech Translation", "categories": ["cs.CL"], "comment": null, "summary": "Simultaneous speech translation requires accurate segmentation to balance\ntranslation quality and latency. Recent studies such as SHAS have introduced\npretrained segmentation models, achieving stronger performance than heuristic\nrules. However, segmentation models such as SHAS, though pretrained and more\nrobust than heuristic methods, are still constrained by supervised learning\nobjectives and do not incorporate human preference alignment, which is crucial\nfor natural real-time interpretation. In this work, we propose a segmentation\nframework based on large language models (LLMs) trained with Direct Preference\nOptimization (DPO). By leveraging preference alignment, our method enables LLMs\nto predict natural segmentation points that better meet the demands of\nreal-time translation. We evaluate the system on the ACL 60/60 corpus across\nthree language pairs (English-Japanese, Chinese, German), using SeamlessM4T v2\nas the translation backbone. Experimental results show that our DPO-tuned LLM\nachieves higher segmentation accuracy than SHAS and yields consistent\nimprovements in translation quality (BLEU, COMET) as well as latency (Average\nLagging). Furthermore, our system benefits from IWSLT baselines for direct\ncomparison. These findings highlight the potential of preference-tuned LLMs to\nsurpass existing pretrained segmentation models and advance adaptive,\nhuman-aligned simultaneous interpretation.", "AI": {"tldr": "The paper proposes a segmentation framework using large language models (LLMs) optimized with Direct Preference Optimization (DPO) for better real-time speech translation, outperforming previous models like SHAS.", "motivation": "Current supervised learning-based segmentation models lack human preference alignment, which is key for natural and effective simultaneous speech translation.", "method": "The segmentation framework utilizes preference-tuned LLMs trained with Direct Preference Optimization to predict natural segmentation points, enhancing real-time translation accuracy.", "result": "Experimental evaluation shows the proposed LLM framework outperforms SHAS in segmentation accuracy, translation quality (BLEU, COMET metrics), and latency (Average Lagging).", "conclusion": "Preference-tuned LLMs offer a significant improvement in meeting human-aligned requirements for simultaneous speech translation, surpassing existing models."}}
{"id": "2510.12687", "pdf": "https://arxiv.org/pdf/2510.12687", "abs": "https://arxiv.org/abs/2510.12687", "authors": ["Kunyu Peng", "Di Wen", "Kailun Yang", "Jia Fu", "Yufan Chen", "Ruiping Liu", "Jiamin Wu", "Junwei Zheng", "M. Saquib Sarfraz", "Luc Van Gool", "Danda Pani Paudel", "Rainer Stiefelhagen"], "title": "EReLiFM: Evidential Reliability-Aware Residual Flow Meta-Learning for Open-Set Domain Generalization under Noisy Labels", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "The source code is available at https://github.com/KPeng9510/ERELIFM", "summary": "Open-Set Domain Generalization (OSDG) aims to enable deep learning models to\nrecognize unseen categories in new domains, which is crucial for real-world\napplications. Label noise hinders open-set domain generalization by corrupting\nsource-domain knowledge, making it harder to recognize known classes and reject\nunseen ones. While existing methods address OSDG under Noisy Labels (OSDG-NL)\nusing hyperbolic prototype-guided meta-learning, they struggle to bridge domain\ngaps, especially with limited clean labeled data. In this paper, we propose\nEvidential Reliability-Aware Residual Flow Meta-Learning (EReLiFM). We first\nintroduce an unsupervised two-stage evidential loss clustering method to\npromote label reliability awareness. Then, we propose a residual flow matching\nmechanism that models structured domain- and category-conditioned residuals,\nenabling diverse and uncertainty-aware transfer paths beyond\ninterpolation-based augmentation. During this meta-learning process, the model\nis optimized such that the update direction on the clean set maximizes the loss\ndecrease on the noisy set, using pseudo labels derived from the most confident\npredicted class for supervision. Experimental results show that EReLiFM\noutperforms existing methods on OSDG-NL, achieving state-of-the-art\nperformance. The source code is available at\nhttps://github.com/KPeng9510/ERELIFM.", "AI": {"tldr": "The paper introduces \"EReLiFM,\" a meta-learning model improving domain generalization under noisy labels, achieving state-of-the-art results for handling unseen categories.", "motivation": "Label noise obstinately harms knowledge transfer and classification for unseen categories in open-set domain generalization, urging improved methods for robustness.", "method": "EReLiFM combines unsupervised evidential loss clustering and residual flow matching, modeling uncertainties while optimizing meta-learning using pseudo-labels.", "result": "The method surpasses existing techniques by achieving superior performance in noisy label setups, validated through experimental evaluation and benchmark comparisons.", "conclusion": "EReLiFM represents an effective advancement in OSDG-NL research, demonstrating improved generalization through innovative clustering and residual modeling."}}
{"id": "2510.12428", "pdf": "https://arxiv.org/pdf/2510.12428", "abs": "https://arxiv.org/abs/2510.12428", "authors": ["Chengyang Dong", "Nan Guo"], "title": "Biased-Attention Guided Risk Prediction for Safe Decision-Making at Unsignalized Intersections", "categories": ["cs.AI"], "comment": null, "summary": "Autonomous driving decision-making at unsignalized intersections is highly\nchallenging due to complex dynamic interactions and high conflict risks. To\nachieve proactive safety control, this paper proposes a deep reinforcement\nlearning (DRL) decision-making framework integrated with a biased attention\nmechanism. The framework is built upon the Soft Actor-Critic (SAC) algorithm.\nIts core innovation lies in the use of biased attention to construct a traffic\nrisk predictor. This predictor assesses the long-term risk of collision for a\nvehicle entering the intersection and transforms this risk into a dense reward\nsignal to guide the SAC agent in making safe and efficient driving decisions.\nFinally, the simulation results demonstrate that the proposed method\neffectively improves both traffic efficiency and vehicle safety at the\nintersection, thereby proving the effectiveness of the intelligent\ndecision-making framework in complex scenarios. The code of our work is\navailable at https://github.com/hank111525/SAC-RWB.", "AI": {"tldr": "The paper proposes a DRL framework integrated with biased attention to improve autonomous driving decision-making at unsignalized intersections.", "motivation": "To address challenges in autonomous driving at unsignalized intersections by improving safety and efficiency through proactive decision-making.", "method": "The authors developed a DRL framework based on the SAC algorithm, incorporating biased attention to predict traffic risks and convert them into dense reward signals for decision-making.", "result": "Simulation results demonstrated enhanced traffic efficiency and vehicle safety when using the proposed framework.", "conclusion": "The innovative decision-making framework is effective in tackling complex intersection challenges, ensuring improved safety and operational efficiency for autonomous vehicles."}}
{"id": "2510.12241", "pdf": "https://arxiv.org/pdf/2510.12241", "abs": "https://arxiv.org/abs/2510.12241", "authors": ["Yuehui Li", "Yahao Lu", "Haoyuan Wu", "Sen Zhang", "Liang Lin", "Yukai Shi"], "title": "Ivan-ISTD: Rethinking Cross-domain Heteroscedastic Noise Perturbations in Infrared Small Target Detection", "categories": ["cs.CV", "eess.IV"], "comment": "In infrared small target detection, noise from different sensors can\n  cause significant interference to performance. We propose a new dataset and a\n  wavelet-guided Invariance learning framework(Ivan-ISTD) to emphasize this\n  issue", "summary": "In the multimedia domain, Infrared Small Target Detection (ISTD) plays a\nimportant role in drone-based multi-modality sensing. To address the dual\nchallenges of cross-domain shift and heteroscedastic noise perturbations in\nISTD, we propose a doubly wavelet-guided Invariance learning\nframework(Ivan-ISTD). In the first stage, we generate training samples aligned\nwith the target domain using Wavelet-guided Cross-domain Synthesis. This\nwavelet-guided alignment machine accurately separates the target background\nthrough multi-frequency wavelet filtering. In the second stage, we introduce\nReal-domain Noise Invariance Learning, which extracts real noise\ncharacteristics from the target domain to build a dynamic noise library. The\nmodel learns noise invariance through self-supervised loss, thereby overcoming\nthe limitations of distribution bias in traditional artificial noise modeling.\nFinally, we create the Dynamic-ISTD Benchmark, a cross-domain dynamic\ndegradation dataset that simulates the distribution shifts encountered in\nreal-world applications. Additionally, we validate the versatility of our\nmethod using other real-world datasets. Experimental results demonstrate that\nour approach outperforms existing state-of-the-art methods in terms of many\nquantitative metrics. In particular, Ivan-ISTD demonstrates excellent\nrobustness in cross-domain scenarios. The code for this work can be found at:\nhttps://github.com/nanjin1/Ivan-ISTD.", "AI": {"tldr": "The paper introduces Ivan-ISTD, a framework for improving Infrared Small Target Detection (ISTD) by addressing cross-domain shifts and noise through wavelet-guided methods and dynamic noise libraries.", "motivation": "To overcome the dual challenges of cross-domain domain shift and heteroscedastic noise, which hinder the effectiveness of ISTD in practical and varied scenarios.", "method": "The framework consists of two stages: wavelet-guided cross-domain synthesis for domain alignment and real-domain noise invariance learning using self-supervised loss and a dynamic noise library. This is supplemented by a new benchmark dataset, Dynamic-ISTD.", "result": "The experimental results show that Ivan-ISTD outperforms state-of-the-art methods across various quantitative metrics and demonstrates robustness in cross-domain ISTD scenarios.", "conclusion": "Ivan-ISTD effectively addresses the limitations of existing ISTD methods, delivering superior performance and robustness across dynamic real-world applications, as validated by experiments and benchmarks."}}
{"id": "2510.12096", "pdf": "https://arxiv.org/pdf/2510.12096", "abs": "https://arxiv.org/abs/2510.12096", "authors": ["Guozheng Ma", "Lu Li", "Zilin Wang", "Haoyu Wang", "Shengchao Hu", "Leszek Rutkowski", "Dacheng Tao"], "title": "Rethinking the Role of Dynamic Sparse Training for Scalable Deep Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Scaling neural networks has driven breakthrough advances in machine learning,\nyet this paradigm fails in deep reinforcement learning (DRL), where larger\nmodels often degrade performance due to unique optimization pathologies such as\nplasticity loss. While recent works show that dynamically adapting network\ntopology during training can mitigate these issues, existing studies have three\ncritical limitations: (1) applying uniform dynamic training strategies across\nall modules despite encoder, critic, and actor following distinct learning\nparadigms, (2) focusing evaluation on basic architectures without clarifying\nthe relative importance and interaction between dynamic training and\narchitectural improvements, and (3) lacking systematic comparison between\ndifferent dynamic approaches including sparse-to-sparse, dense-to-sparse, and\nsparse-to-dense. Through comprehensive investigation across modules and\narchitectures, we reveal that dynamic sparse training strategies provide\nmodule-specific benefits that complement the primary scalability foundation\nestablished by architectural improvements. We finally distill these insights\ninto Module-Specific Training (MST), a practical framework that further\nexploits the benefits of architectural improvements and demonstrates\nsubstantial scalability gains across diverse RL algorithms without algorithmic\nmodifications.", "AI": {"tldr": "This paper investigates the limitations in scaling deep reinforcement learning models and proposes a Module-Specific Training (MST) framework to enhance scalability and performance.", "motivation": "The study aims to address optimization pathologies such as plasticity loss which hinder the scalability of deep reinforcement learning models and also examines limitations in existing dynamic training strategies.", "method": "The methodology involves a thorough evaluation across various network modules and architectures, systematically comparing dynamic approaches such as sparse-to-sparse, dense-to-sparse, and sparse-to-dense.", "result": "The findings indicate that dynamic sparse training strategies provide module-specific benefits, complementing architectural improvements to achieve better scalability.", "conclusion": "The proposed Module-Specific Training (MST) framework leverages insights from architectural and training strategy evaluations to deliver significant scalability improvements across reinforcement learning algorithms without requiring changes to algorithmic design."}}
{"id": "2510.12217", "pdf": "https://arxiv.org/pdf/2510.12217", "abs": "https://arxiv.org/abs/2510.12217", "authors": ["Ali Mekky", "Omar El Herraoui", "Preslav Nakov", "Yuxia Wang"], "title": "HALF: Harm-Aware LLM Fairness Evaluation Aligned with Deployment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed across high-impact\ndomains, from clinical decision support and legal analysis to hiring and\neducation, making fairness and bias evaluation before deployment critical.\nHowever, existing evaluations lack grounding in real-world scenarios and do not\naccount for differences in harm severity, e.g., a biased decision in surgery\nshould not be weighed the same as a stylistic bias in text summarization. To\naddress this gap, we introduce HALF (Harm-Aware LLM Fairness), a\ndeployment-aligned framework that assesses model bias in realistic applications\nand weighs the outcomes by harm severity. HALF organizes nine application\ndomains into three tiers (Severe, Moderate, Mild) using a five-stage pipeline.\nOur evaluation results across eight LLMs show that (1) LLMs are not\nconsistently fair across domains, (2) model size or performance do not\nguarantee fairness, and (3) reasoning models perform better in medical decision\nsupport but worse in education. We conclude that HALF exposes a clear gap\nbetween previous benchmarking success and deployment readiness.", "AI": {"tldr": "The paper presents a framework called HALF (Harm-Aware LLM Fairness) to evaluate the fairness of large language models in realistic applications, prioritizing bias assessment based on harm severity.", "motivation": "The growing deployment of LLMs in critical areas necessitates fairness evaluations grounded in real-world scenarios, addressing the lack of harm severity considerations in current bias assessments.", "method": "The HALF framework organizes nine domains into three harm severity tiers (Severe, Moderate, Mild) using a five-stage evaluation pipeline to assess fairness in realistic applications.", "result": "The study finds that LLMs display inconsistent fairness across domains, with neither size nor performance guaranteeing fairness. Reasoning models excel in medical contexts but underperform in education.", "conclusion": "HALF highlights significant challenges in assessing bias in LLMs before deployment and exposes a gap between traditional benchmarking and practical deployment readiness."}}
{"id": "2510.12462", "pdf": "https://arxiv.org/pdf/2510.12462", "abs": "https://arxiv.org/abs/2510.12462", "authors": ["Jiaxin Gao", "Chen Chen", "Yanwen Jia", "Xueluan Gong", "Kwok-Yan Lam", "Qian Wang"], "title": "Evaluating and Mitigating LLM-as-a-judge Bias in Communication Systems", "categories": ["cs.AI", "cs.CR"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly being used to autonomously\nevaluate the quality of content in communication systems, e.g., to assess\nresponses in telecom customer support chatbots. However, the impartiality of\nthese AI \"judges\" is not guaranteed, and any biases in their evaluation\ncriteria could skew outcomes and undermine user trust. In this paper, we\nsystematically investigate judgment biases in two LLM-as-a-judge models (i.e.,\nGPT-Judge and JudgeLM) under the point-wise scoring setting, encompassing 11\ntypes of biases that cover both implicit and explicit forms. We observed that\nstate-of-the-art LLM judges demonstrate robustness to biased inputs, generally\nassigning them lower scores than the corresponding clean samples. Providing a\ndetailed scoring rubric further enhances this robustness. We further found that\nfine-tuning an LLM on high-scoring yet biased responses can significantly\ndegrade its performance, highlighting the risk of training on biased data. We\nalso discovered that the judged scores correlate with task difficulty: a\nchallenging dataset like GPQA yields lower average scores, whereas an\nopen-ended reasoning dataset (e.g., JudgeLM-val) sees higher average scores.\nFinally, we proposed four potential mitigation strategies to ensure fair and\nreliable AI judging in practical communication scenarios.", "AI": {"tldr": "The paper investigates judgment biases in Large Language Models (LLMs) acting as judges (GPT-Judge and JudgeLM) and proposes mitigation strategies to ensure fair and reliable evaluations.", "motivation": "To address the potential biases in LLMs used as judges in communication systems and ensure their impartiality and reliability.", "method": "The study examines 11 types of biases in LLM-as-a-judge models under a point-wise scoring setting and evaluates their response to biased inputs. It includes experiments with scoring rubrics and analyzes the impact of training on biased data. The correlation of judged scores with task difficulty is also assessed.", "result": "LLMs showed robustness to biased inputs when detailed scoring rubrics were provided. However, fine-tuning on biased data degraded performance. Scores were influenced by task difficulty, with higher scores in open-ended reasoning datasets and lower scores in challenging datasets.", "conclusion": "The paper concludes that LLM judges have potential biases but can be improved with scoring rubrics and proper training. Four mitigation strategies are proposed to ensure fair and reliable AI judging in communication systems."}}
{"id": "2510.12256", "pdf": "https://arxiv.org/pdf/2510.12256", "abs": "https://arxiv.org/abs/2510.12256", "authors": ["Ye Chen", "Liming Tan", "Yupeng Zhu", "Yuanbin Wang", "Bingbing Ni"], "title": "Vectorized Video Representation with Easy Editing via Hierarchical Spatio-Temporally Consistent Proxy Embedding", "categories": ["cs.CV"], "comment": null, "summary": "Current video representations heavily rely on unstable and over-grained\npriors for motion and appearance modelling, \\emph{i.e.}, pixel-level matching\nand tracking. A tracking error of just a few pixels would lead to the collapse\nof the visual object representation, not to mention occlusions and large motion\nfrequently occurring in videos. To overcome the above mentioned vulnerability,\nthis work proposes spatio-temporally consistent proxy nodes to represent\ndynamically changing objects/scenes in the video. On the one hand, the\nhierarchical proxy nodes have the ability to stably express the multi-scale\nstructure of visual objects, so they are not affected by accumulated tracking\nerror, long-term motion, occlusion, and viewpoint variation. On the other hand,\nthe dynamic representation update mechanism of the proxy nodes adequately\nleverages spatio-temporal priors of the video to mitigate the impact of\ninaccurate trackers, thereby effectively handling drastic changes in scenes and\nobjects. Additionally, the decoupled encoding manner of the shape and texture\nrepresentations across different visual objects in the video facilitates\ncontrollable and fine-grained appearance editing capability. Extensive\nexperiments demonstrate that the proposed representation achieves high video\nreconstruction accuracy with fewer parameters and supports complex video\nprocessing tasks, including video in-painting and keyframe-based temporally\nconsistent video editing.", "AI": {"tldr": "The paper introduces spatio-temporally consistent proxy nodes for robust video representation, improving on traditional pixel-level models.", "motivation": "Current video representation methods are fragile due to reliance on pixel-level matching and tracking, which are sensitive to tracking errors, occlusions, and motion.", "method": "The method leverages hierarchical proxy nodes for stable, multi-scale structure representation and a dynamic update mechanism to handle spatio-temporal changes and inaccuracies.", "result": "This approach achieves high video reconstruction accuracy with fewer parameters and supports tasks like video inpainting and temporally consistent keyframe-based editing.", "conclusion": "Proxy node-based video representation is a robust, efficient alternative to pixel-level methods, enhancing video editing and reconstruction capabilities."}}
{"id": "2510.12111", "pdf": "https://arxiv.org/pdf/2510.12111", "abs": "https://arxiv.org/abs/2510.12111", "authors": ["Aakash Lahoti", "Tanya Marwah", "Ratish Puduppully", "Albert Gu"], "title": "Chimera: State Space Models Beyond Sequences", "categories": ["cs.LG", "cs.AI"], "comment": "Published in TMLR (October 2025); 22 Pages, 6 Figures, 11 Tables", "summary": "Transformer-based deep learning methods have become the standard approach for\nmodeling diverse data such as sequences, images, and graphs. These methods rely\non self-attention, which treats data as an unordered set of elements. This\nignores the neighborhood structure or graph topology of the data and requires\ninductive biases--such as position embeddings in sequences and images, or\nrandom walks in graphs--to incorporate topology. However, designing such\ntask-specific biases requires significant effort and can introduce side effects\nthat hinder generalization. We introduce Chimera, a unified model that directly\nincorporates data topology in a principled way, removing the need for\ndomain-specific biases. The key idea is that state space models--which\nnaturally do not require position embeddings--can be generalized to capture any\ngraph topology. Our experiments show that Chimera achieves strong performance\nacross language, vision, and graph domains, outperforming BERT on GLUE by 0.7\npoints, ViT on ImageNet-1k by 2.6%, and all baselines on the Long Range Graph\nBenchmark. We further propose algorithmic optimizations to improve Chimera's\nefficiency: (1) for Directed Acyclic Graphs, Chimera can be implemented as a\nlinear-time recurrence; (2) for general graphs, a simple mathematical\nrelaxation achieves Transformer's quadratic complexity without domain-specific\nheuristics. These results validate Chimera's core contribution and support the\nidea that data topology is a powerful inductive bias across modalities.", "AI": {"tldr": "Chimera, a novel unified model, incorporates data topology directly into Transformer-based methods, enhancing performance across diverse domains like language, vision, and graphs.", "motivation": "Address the limitations of current Transformer-based models that require domain-specific biases to incorporate data topology.", "method": "Propose Chimera, a model leveraging state-space models to capture graph topology universally without requiring positional embeddings or domain-specific inductive biases.", "result": "Chimera outperforms established baselines: +0.7 points over BERT on GLUE, +2.6% over ViT on ImageNet-1k, and surpasses all metrics on Long Range Graph Benchmark.", "conclusion": "Incorporating topology directly into state space models is a robust inductive bias, improving generalization and efficiency across multiple domains without relying on task-specific heuristics."}}
{"id": "2510.12229", "pdf": "https://arxiv.org/pdf/2510.12229", "abs": "https://arxiv.org/abs/2510.12229", "authors": ["Bianca Raimondi", "Daniela Dalbagno", "Maurizio Gabbrielli"], "title": "Analysing Moral Bias in Finetuned LLMs through Mechanistic Interpretability", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint. Under review", "summary": "Large language models (LLMs) have been shown to internalize human-like biases\nduring finetuning, yet the mechanisms by which these biases manifest remain\nunclear. In this work, we investigated whether the well-known Knobe effect, a\nmoral bias in intentionality judgements, emerges in finetuned LLMs and whether\nit can be traced back to specific components of the model. We conducted a\nLayer-Patching analysis across 3 open-weights LLMs and demonstrated that the\nbias is not only learned during finetuning but also localized in a specific set\nof layers. Surprisingly, we found that patching activations from the\ncorresponding pretrained model into just a few critical layers is sufficient to\neliminate the effect. Our findings offer new evidence that social biases in\nLLMs can be interpreted, localized, and mitigated through targeted\ninterventions, without the need for model retraining.", "AI": {"tldr": "The paper explores how the Knobe effect, a moral bias, is learned and localized in finetuned LLMs and how targeted interventions can mitigate it.", "motivation": "The authors aim to understand how human-like biases in language models arise, focusing on the Knobe effect, a moral bias in intentionality judgments.", "method": "They utilized a Layer-Patching analysis across three open-weight language models to identify where the moral bias is learned and localized within the model.", "result": "The study found that the bias is learned during finetuning and localized in specific layers. Patching activations from corresponding pretrained models into those layers effectively eliminated the bias.", "conclusion": "Social biases in LLMs can be analyzed, localized, and removed through precise interventions without requiring retraining of the entire model."}}
{"id": "2510.12490", "pdf": "https://arxiv.org/pdf/2510.12490", "abs": "https://arxiv.org/abs/2510.12490", "authors": ["Rui Reis", "Pedro Rangel Henriques", "Jo\u00e3o Ferreira-Coimbra", "Eva Oliveira", "Nuno F. Rodrigues"], "title": "Using Medical Algorithms for Task-Oriented Dialogue in LLM-Based Medical Interviews", "categories": ["cs.AI"], "comment": null, "summary": "We developed a task-oriented dialogue framework structured as a Directed\nAcyclic Graph (DAG) of medical questions. The system integrates: (1) a\nsystematic pipeline for transforming medical algorithms and guidelines into a\nclinical question corpus; (2) a cold-start mechanism based on hierarchical\nclustering to generate efficient initial questioning without prior patient\ninformation; (3) an expand-and-prune mechanism enabling adaptive branching and\nbacktracking based on patient responses; (4) a termination logic to ensure\ninterviews end once sufficient information is gathered; and (5) automated\nsynthesis of doctor-friendly structured reports aligned with clinical\nworkflows. Human-computer interaction principles guided the design of both the\npatient and physician applications. Preliminary evaluation involved five\nphysicians using standardized instruments: NASA-TLX (cognitive workload), the\nSystem Usability Scale (SUS), and the Questionnaire for User Interface\nSatisfaction (QUIS). The patient application achieved low workload scores\n(NASA-TLX = 15.6), high usability (SUS = 86), and strong satisfaction (QUIS =\n8.1/9), with particularly high ratings for ease of learning and interface\ndesign. The physician application yielded moderate workload (NASA-TLX = 26) and\nexcellent usability (SUS = 88.5), with satisfaction scores of 8.3/9. Both\napplications demonstrated effective integration into clinical workflows,\nreducing cognitive demand and supporting efficient report generation.\nLimitations included occasional system latency and a small, non-diverse\nevaluation sample.", "AI": {"tldr": "Development of a Directed Acyclic Graph (DAG)-based task-oriented dialogue system for medical questioning, evaluated for usability and satisfaction with promising results.", "motivation": "To create an adaptive dialogue system that efficiently gathers medical information, reduces cognitive load, and generates structured reports integrated into clinical workflows.", "method": "The framework uses a DAG structure for medical questioning, implements mechanisms for cold-start questioning, adaptive branching, termination logic, and automated report synthesis. Human-computer interaction principles guide its design.", "result": "The system demonstrated low workload and high satisfaction (NASA-TLX, SUS, QUIS scales), with effective integration into clinical workflows. Limitations included occasional latency and a small evaluation sample.", "conclusion": "The system efficiently supports medical information gathering and clinical workflows, showing strong usability and satisfaction, though further testing is needed."}}
{"id": "2510.12258", "pdf": "https://arxiv.org/pdf/2510.12258", "abs": "https://arxiv.org/abs/2510.12258", "authors": ["Yuto Yokoi", "Kazuhiro Hotta"], "title": "Multiplicative Loss for Enhancing Semantic Segmentation in Medical and Cellular Images", "categories": ["cs.CV"], "comment": "Accepted by ICCV2025 Workshop \"Third Workshop on Computer Vision for\n  Automated Medical Diagnosis\"", "summary": "We propose two novel loss functions, Multiplicative Loss and\nConfidence-Adaptive Multiplicative Loss, for semantic segmentation in medical\nand cellular images. Although Cross Entropy and Dice Loss are widely used,\ntheir additive combination is sensitive to hyperparameters and often performs\nsuboptimally, especially with limited data. Medical images suffer from data\nscarcity due to privacy, ethics, and costly annotations, requiring robust and\nefficient training objectives. Our Multiplicative Loss combines Cross Entropy\nand Dice losses multiplicatively, dynamically modulating gradients based on\nprediction confidence. This reduces penalties for confident correct predictions\nand amplifies gradients for incorrect overconfident ones, stabilizing\noptimization. Building on this, Confidence-Adaptive Multiplicative Loss applies\na confidence-driven exponential scaling inspired by Focal Loss, integrating\npredicted probabilities and Dice coefficients to emphasize difficult samples.\nThis enhances learning under extreme data scarcity by strengthening gradients\nwhen confidence is low. Experiments on cellular and medical segmentation\nbenchmarks show our framework consistently outperforms tuned additive and\nexisting loss functions, offering a simple, effective, and hyperparameter-free\nmechanism for robust segmentation under challenging data limitations.", "AI": {"tldr": "This paper introduces two new loss functions for improving semantic segmentation in medical and cellular images under conditions of limited data: Multiplicative Loss and Confidence-Adaptive Multiplicative Loss.", "motivation": "The motivation is to address the limitations of widely used Cross Entropy and Dice Loss functions, which struggle with hyperparameter sensitivity and perform poorly under data scarcity, a common issue in medical imaging.", "method": "The paper proposes Multiplicative Loss, which combines Cross Entropy and Dice loss multiplicatively to dynamically adjust gradients based on confidence. It also introduces Confidence-Adaptive Multiplicative Loss, incorporating confidence-driven exponential scaling to focus on challenging samples.", "result": "Experimental results show that the proposed loss functions consistently outperform traditional additive loss combinations and other existing methods in medical and cellular image segmentation benchmarks.", "conclusion": "The proposed loss functions provide a robust, effective, and hyperparameter-free approach, particularly suited for data-limited scenarios, advancing segmentation performance in challenging contexts."}}
{"id": "2510.12251", "pdf": "https://arxiv.org/pdf/2510.12251", "abs": "https://arxiv.org/abs/2510.12251", "authors": ["Jiakai Li", "Rongzheng Wang", "Yizhuo Ma", "Shuang Liang", "Guangchun Luo", "Ke Qin"], "title": "DSAS: A Universal Plug-and-Play Framework for Attention Optimization in Multi-Document Question Answering", "categories": ["cs.CL"], "comment": "27 pages, has been accepted by NeurIPS 2025", "summary": "While large language models (LLMs) show considerable promise across various\nfields, they have notable limitations in handling multi-document question\nanswering (Multi-doc QA) tasks. The first challenge is long-range dependency\nmodeling, where LLMs struggle to focus on key information in long texts, which\nweakens important semantic connections. Second, most LLMs suffer from the\n''lost-in-the-middle'' issue, where they have difficulty processing information\nin the middle of long inputs. Current solutions either truncate global\ndependencies or demand costly finetuning, ultimately lacking a universal and\nsimple solution for these challenges. To resolve these limitations, we propose\nDual-Stage Adaptive Sharpening (DSAS) containing two modules. (i) The\nContextual Gate Weighting (CGW) module alleviates ''lost-in-the-middle'' by\nassessing paragraph relevance through layer-wise attention tracking and\nposition-aware weighting. (ii) The Reciprocal Attention Suppression (RAS)\nmodule enhances focus on critical paragraphs by suppressing information\nexchange between key and irrelevant texts, thus mitigating the limitations in\nlong-range dependency modeling. Notably, DSAS functions as a plug-and-play\nsolution requiring no architectural modifications or extra training parameters.\nExtensive experiments on four benchmarks demonstrate DSAS's efficacy across\nmainstream LLMs (Llama, Qwen, Mistral, and Deepseek), with an average F1-score\nimprovement of 4.2% in Multi-doc QA tasks on Llama-3.1-8B-Instruct and\nQwen2.5-14B-Instruct. Ablation studies confirm the essential contributions of\nboth the CGW and RAS modules. In addition, detailed discussions in the Appendix\nfurther validate the robustness and scalability of DSAS.", "AI": {"tldr": "The paper introduces Dual-Stage Adaptive Sharpening (DSAS) to tackle limitations of large language models (LLMs) in multi-document question answering (Multi-doc QA). DSAS improves attention with the Contextual Gate Weighting (CGW) and Reciprocal Attention Suppression (RAS) modules, leading to significant F1-score improvements without requiring model architecture changes.", "motivation": "Large language models face issues in multi-document question answering due to difficulty in maintaining semantic connections in long texts and struggling with the \"lost-in-the-middle\" issue for information processing in lengthy inputs.", "method": "The DSAS framework consists of two modules: Contextual Gate Weighting (CGW) for assessing paragraph relevance and addressing \"lost-in-the-middle\" issues, and Reciprocal Attention Suppression (RAS) to suppress irrelevant text and enhance focus on key information. DSAS works as a plug-and-play solution, requiring no architectural changes or additional training.", "result": "Experiments on four benchmarks showcase an average F1-score improvement of 4.2% in Multi-doc QA tasks, tested on Llama-3.1-8B-Instruct and Qwen2.5-14B-Instruct models, along with ablation studies confirming the value of DSAS's modules.", "conclusion": "Dual-Stage Adaptive Sharpening (DSAS) significantly enhances multi-document QA performance in LLMs through improved attention mechanisms, presenting a universal, cost-effective plug-and-play solution without architectural modifications."}}
{"id": "2510.12498", "pdf": "https://arxiv.org/pdf/2510.12498", "abs": "https://arxiv.org/abs/2510.12498", "authors": ["Chengpeng Hu", "Calvin Yu-Chian Chen"], "title": "Artificial Intelligence Virtual Cells: From Measurements to Decisions across Modality, Scale, Dynamics, and Evaluation", "categories": ["cs.AI"], "comment": null, "summary": "Artificial Intelligence Virtual Cells (AIVCs) aim to learn executable,\ndecision-relevant models of cell state from multimodal, multiscale\nmeasurements. Recent studies have introduced single-cell and spatial foundation\nmodels, improved cross-modality alignment, scaled perturbation atlases, and\nexplored pathway-level readouts. Nevertheless, although held-out validation is\nstandard practice, evaluations remain predominantly within single datasets and\nsettings; evidence indicates that transport across laboratories and platforms\nis often limited, that some data splits are vulnerable to leakage and coverage\nbias, and that dose, time and combination effects are not yet systematically\nhandled. Cross-scale coupling also remains constrained, as anchors linking\nmolecular, cellular and tissue levels are sparse, and alignment to scientific\nor clinical readouts varies across studies. We propose a model-agnostic\nCell-State Latent (CSL) perspective that organizes learning via an operator\ngrammar: measurement, lift/project for cross-scale coupling, and intervention\nfor dosing and scheduling. This view motivates a decision-aligned evaluation\nblueprint across modality, scale, context and intervention, and emphasizes\nfunction-space readouts such as pathway activity, spatial neighborhoods and\nclinically relevant endpoints. We recommend operator-aware data design,\nleakage-resistant partitions, and transparent calibration and reporting to\nenable reproducible, like-for-like comparisons.", "AI": {"tldr": "This paper discusses the challenges of building robust Artificial Intelligence Virtual Cells (AIVCs) models and proposes a Cell-State Latent (CSL) framework to enhance cross-scale coupling, evaluation, and reproducibility.", "motivation": "The motivation is to address the limitations in current AIVC models, including poor cross-laboratory transportability, data leakage, inadequate handling of dose and time effects, and sparse cross-scale coupling between molecular, cellular, and tissue levels.", "method": "The authors propose a model-agnostic CSL perspective organized around an operator grammar (measurement, lift/project for cross-scale coupling, and intervention). This approach emphasizes decision-aligned evaluations and function-space readouts, along with improved data design and evaluation protocols.", "result": "The study introduces a decision-aligned evaluation blueprint and highlights the importance of operator-aware data design, leakage-resistant partitions, and transparent reporting. This enables more robust AIVC models with better cross-modality and cross-scale compatibility.", "conclusion": "The CSL framework provides a systematic strategy to improve the reproducibility, transparency, and reliability of AIVC models, facilitating their alignment with scientific or clinical applications."}}
{"id": "2510.12259", "pdf": "https://arxiv.org/pdf/2510.12259", "abs": "https://arxiv.org/abs/2510.12259", "authors": ["Jinlun Ye", "Zhuohao Sun", "Yiqiao Qiu", "Qiu Li", "Zhijun Tan", "Ruixuan Wang"], "title": "Local Background Features Matter in Out-of-Distribution Detection", "categories": ["cs.CV"], "comment": null, "summary": "Out-of-distribution (OOD) detection is crucial when deploying deep neural\nnetworks in the real world to ensure the reliability and safety of their\napplications. One main challenge in OOD detection is that neural network models\noften produce overconfident predictions on OOD data. While some methods using\nauxiliary OOD datasets or generating fake OOD images have shown promising OOD\ndetection performance, they are limited by the high costs of data collection\nand training. In this study, we propose a novel and effective OOD detection\nmethod that utilizes local background features as fake OOD features for model\ntraining. Inspired by the observation that OOD images generally share similar\nbackground regions with ID images, the background features are extracted from\nID images as simulated OOD visual representations during training based on the\nlocal invariance of convolution. Through being optimized to reduce the\n$L_2$-norm of these background features, the neural networks are able to\nalleviate the overconfidence issue on OOD data. Extensive experiments on\nmultiple standard OOD detection benchmarks confirm the effectiveness of our\nmethod and its wide combinatorial compatibility with existing post-hoc methods,\nwith new state-of-the-art performance achieved from our method.", "AI": {"tldr": "The paper introduces a new Out-of-distribution (OOD) detection method using local background features from in-distribution (ID) images for training, achieving state-of-the-art detection results.", "motivation": "The paper addresses the challenge of overconfident predictions on OOD data by neural networks, without relying on expensive auxiliary OOD datasets or generated fake OOD images.", "method": "The proposed method extracts background features from ID images to simulate OOD visual representations during training, leveraging local invariance of convolution and optimizing $L_2$-norm reduction.", "result": "Extensive experiments on standard OOD detection benchmarks show the effectiveness of the proposed method and its compatibility with existing post-hoc methods, achieving state-of-the-art performance.", "conclusion": "The research provides a cost-effective and efficient OOD detection approach, leveraging background features to improve model reliability and safety in real-world applications."}}
{"id": "2510.12140", "pdf": "https://arxiv.org/pdf/2510.12140", "abs": "https://arxiv.org/abs/2510.12140", "authors": ["Yonghao Liu", "Yajun Wang", "Chunli Guo", "Wei Pang", "Ximing Li", "Fausto Giunchiglia", "Xiaoyue Feng", "Renchu Guan"], "title": "Graph Few-Shot Learning via Adaptive Spectrum Experts and Cross-Set Distribution Calibration", "categories": ["cs.LG"], "comment": "NeurIPS25", "summary": "Graph few-shot learning has attracted increasing attention due to its ability\nto rapidly adapt models to new tasks with only limited labeled nodes. Despite\nthe remarkable progress made by existing graph few-shot learning methods,\nseveral key limitations remain. First, most current approaches rely on\npredefined and unified graph filters (e.g., low-pass or high-pass filters) to\nglobally enhance or suppress node frequency signals. Such fixed spectral\noperations fail to account for the heterogeneity of local topological\nstructures inherent in real-world graphs. Moreover, these methods often assume\nthat the support and query sets are drawn from the same distribution. However,\nunder few-shot conditions, the limited labeled data in the support set may not\nsufficiently capture the complex distribution of the query set, leading to\nsuboptimal generalization. To address these challenges, we propose GRACE, a\nnovel Graph few-shot leaRning framework that integrates Adaptive spectrum\nexperts with Cross-sEt distribution calibration techniques. Theoretically, the\nproposed approach enhances model generalization by adapting to both local\nstructural variations and cross-set distribution calibration. Empirically,\nGRACE consistently outperforms state-of-the-art baselines across a wide range\nof experimental settings. Our code can be found here.", "AI": {"tldr": "The paper introduces GRACE, a novel graph few-shot learning framework that improves model adaptability and generalization by addressing limitations in spectral operations and support-query set distributions.", "motivation": "Current graph few-shot learning methods face challenges due to reliance on fixed spectral operations and assumptions about support-query set distribution uniformity.", "method": "GRACE leverages adaptive spectrum experts for local structural adaptations and cross-set distribution calibration for better generalization in few-shot scenarios.", "result": "GRACE consistently outperforms state-of-the-art methods in various graph few-shot learning scenarios.", "conclusion": "The proposed framework enhances adaptability and generalization, addressing key limitations, and offers a promising solution for graph few-shot learning with limited labeled nodes."}}
{"id": "2510.12255", "pdf": "https://arxiv.org/pdf/2510.12255", "abs": "https://arxiv.org/abs/2510.12255", "authors": ["Blazej Manczak", "Eric Lin", "Francisco Eiras", "James O' Neill", "Vaikkunth Mugunthan"], "title": "Shallow Robustness, Deep Vulnerabilities: Multi-Turn Evaluation of Medical LLMs", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.6; J.3"], "comment": "Dataset and code:\n  https://huggingface.co/datasets/dynamoai-ml/MedQA-USMLE-4-MultiTurnRobust ;\n  https://github.com/bmanczak/MedQA-MultiTurnRobustness Accepted as a poster at\n  NeurIPS 2025 Workshop on GenAI for Health: Potential, Trust, and Policy\n  Compliance", "summary": "Large language models (LLMs) are rapidly transitioning into medical clinical\nuse, yet their reliability under realistic, multi-turn interactions remains\npoorly understood. Existing evaluation frameworks typically assess single-turn\nquestion answering under idealized conditions, overlooking the complexities of\nmedical consultations where conflicting input, misleading context, and\nauthority influence are common. We introduce MedQA-Followup, a framework for\nsystematically evaluating multi-turn robustness in medical question answering.\nOur approach distinguishes between shallow robustness (resisting misleading\ninitial context) and deep robustness (maintaining accuracy when answers are\nchallenged across turns), while also introducing an indirect-direct axis that\nseparates contextual framing (indirect) from explicit suggestion (direct).\nUsing controlled interventions on the MedQA dataset, we evaluate five\nstate-of-the-art LLMs and find that while models perform reasonably well under\nshallow perturbations, they exhibit severe vulnerabilities in multi-turn\nsettings, with accuracy dropping from 91.2% to as low as 13.5% for Claude\nSonnet 4. Counterintuitively, indirect, context-based interventions are often\nmore harmful than direct suggestions, yielding larger accuracy drops across\nmodels and exposing a significant vulnerability for clinical deployment.\nFurther compounding analyses reveal model differences, with some showing\nadditional performance drops under repeated interventions while others\npartially recovering or even improving. These findings highlight multi-turn\nrobustness as a critical but underexplored dimension for safe and reliable\ndeployment of medical LLMs.", "AI": {"tldr": "The paper investigates the robustness of large language models (LLMs) in medical multi-turn conversations, highlighting severe vulnerabilities when interacting in complex multi-turn medical consultation scenarios.", "motivation": "The study aims to address the lack of understanding of LLMs' performance in realistic multi-turn medical interactions, where conflicting input, misleading context, and authority influence are common.", "method": "The authors propose MedQA-Followup, evaluating five state-of-the-art LLMs' robustness through controlled interventions on the MedQA dataset. They distinguish between shallow robustness (resisting misleading context) and deep robustness (accuracy when answers are challenged across turns).", "result": "Findings reveal that LLMs perform reasonably well under shallow conditions but demonstrate severe accuracy reductions in multi-turn settings, dropping accuracy from 91.2% to as low as 13.5% for some models. Indirect interventions harm accuracy more than direct suggestions.", "conclusion": "Multi-turn robustness is a critical and underexplored area for medical LLMs. Their vulnerabilities highlight the need for systematic solutions to ensure safe clinical usage."}}
{"id": "2510.12534", "pdf": "https://arxiv.org/pdf/2510.12534", "abs": "https://arxiv.org/abs/2510.12534", "authors": ["Utsav Kumar Nareti", "Suraj Kumar", "Soumya Pandey", "Soumi Chattopadhyay", "Chandranath Adak"], "title": "ProtoSiTex: Learning Semi-Interpretable Prototypes for Multi-label Text Classification", "categories": ["cs.AI"], "comment": null, "summary": "The surge in user-generated reviews has amplified the need for interpretable\nmodels that can provide fine-grained insights. Existing prototype-based models\noffer intuitive explanations but typically operate at coarse granularity\n(sentence or document level) and fail to address the multi-label nature of\nreal-world text classification. We propose ProtoSiTex, a semi-interpretable\nframework designed for fine-grained multi-label text classification. ProtoSiTex\nemploys a dual-phase alternating training strategy: an unsupervised prototype\ndiscovery phase that learns semantically coherent and diverse prototypes, and a\nsupervised classification phase that maps these prototypes to class labels. A\nhierarchical loss function enforces consistency across sub-sentence, sentence,\nand document levels, enhancing interpretability and alignment. Unlike prior\napproaches, ProtoSiTex captures overlapping and conflicting semantics using\nadaptive prototypes and multi-head attention. We also introduce a benchmark\ndataset of hotel reviews annotated at the sub-sentence level with multiple\nlabels. Experiments on this dataset and two public benchmarks (binary and\nmulti-class) show that ProtoSiTex achieves state-of-the-art performance while\ndelivering faithful, human-aligned explanations, establishing it as a robust\nsolution for semi-interpretable multi-label text classification.", "AI": {"tldr": "ProtoSiTex is a new model for fine-grained multi-label text classification that achieves interpretability and state-of-the-art performance by leveraging prototype-based explanations.", "motivation": "The growing volume of user-generated reviews necessitates models that provide detailed, interpretable insights beyond coarse granularities, addressing multi-label classification challenges.", "method": "ProtoSiTex operates through an unsupervised prototype discovery phase and a supervised classification phase, using a hierarchical loss function for alignment while leveraging adaptive prototypes and multi-head attention for capturing semantics.", "result": "ProtoSiTex outperforms state-of-the-art models on various datasets, demonstrating its efficacy in both performance and providing human-aligned, interpretable explanations.", "conclusion": "ProtoSiTex offers a scalable and interpretable solution for fine-grained multi-label text classification, capable of explaining overlapping and multi-label semantics at different text granularities."}}
{"id": "2510.12260", "pdf": "https://arxiv.org/pdf/2510.12260", "abs": "https://arxiv.org/abs/2510.12260", "authors": ["Xiaopeng Liu", "Yupei Lin", "Sen Zhang", "Xiao Wang", "Yukai Shi", "Liang Lin"], "title": "AngularFuse: A Closer Look at Angle-based Perception for Spatial-Sensitive Multi-Modality Image Fusion", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "For the first time, angle-based perception was introduced into the\n  multi-modality image fusion task", "summary": "Visible-infrared image fusion is crucial in key applications such as\nautonomous driving and nighttime surveillance. Its main goal is to integrate\nmultimodal information to produce enhanced images that are better suited for\ndownstream tasks. Although deep learning based fusion methods have made\nsignificant progress, mainstream unsupervised approaches still face serious\nchallenges in practical applications. Existing methods mostly rely on manually\ndesigned loss functions to guide the fusion process. However, these loss\nfunctions have obvious limitations. On one hand, the reference images\nconstructed by existing methods often lack details and have uneven brightness.\nOn the other hand, the widely used gradient losses focus only on gradient\nmagnitude. To address these challenges, this paper proposes an angle-based\nperception framework for spatial-sensitive image fusion (AngularFuse). At\nfirst, we design a cross-modal complementary mask module to force the network\nto learn complementary information between modalities. Then, a fine-grained\nreference image synthesis strategy is introduced. By combining Laplacian edge\nenhancement with adaptive histogram equalization, reference images with richer\ndetails and more balanced brightness are generated. Last but not least, we\nintroduce an angle-aware loss, which for the first time constrains both\ngradient magnitude and direction simultaneously in the gradient domain.\nAngularFuse ensures that the fused images preserve both texture intensity and\ncorrect edge orientation. Comprehensive experiments on the MSRS, RoadScene, and\nM3FD public datasets show that AngularFuse outperforms existing mainstream\nmethods with clear margin. Visual comparisons further confirm that our method\nproduces sharper and more detailed results in challenging scenes, demonstrating\nsuperior fusion capability.", "AI": {"tldr": "AngularFuse introduces innovative fusion techniques for visible-infrared image processing, addressing limitations in existing methods and achieving improved results on public datasets.", "motivation": "To address challenges in unsupervised visible-infrared image fusion, particularly issues with manually designed loss functions and reference image limitations.", "method": "Proposed AngularFuse framework includes a cross-modal complementary mask module, fine-grained reference image synthesis combining edge enhancement and histogram equalization, and an angle-aware loss considering both gradient magnitude and direction.", "result": "Extensive experiments on standard datasets (MSRS, RoadScene, M3FD) demonstrate that AngularFuse outperforms existing methods in both quantitative metrics and visual quality.", "conclusion": "AngularFuse effectively enhances brightness, details, and texture directionality, offering a superior solution for spatial-sensitive image fusion applications like autonomous driving and surveillance."}}
{"id": "2510.12143", "pdf": "https://arxiv.org/pdf/2510.12143", "abs": "https://arxiv.org/abs/2510.12143", "authors": ["Harsh Kasyap", "Minghong Fang", "Zhuqing Liu", "Carsten Maple", "Somanath Tripathy"], "title": "Fairness-Constrained Optimization Attack in Federated Learning", "categories": ["cs.LG", "cs.CR"], "comment": "To appear in IEEE TrustCom 2025", "summary": "Federated learning (FL) is a privacy-preserving machine learning technique\nthat facilitates collaboration among participants across demographics. FL\nenables model sharing, while restricting the movement of data. Since FL\nprovides participants with independence over their training data, it becomes\nsusceptible to poisoning attacks. Such collaboration also propagates bias among\nthe participants, even unintentionally, due to different data distribution or\nhistorical bias present in the data. This paper proposes an intentional\nfairness attack, where a client maliciously sends a biased model, by increasing\nthe fairness loss while training, even considering homogeneous data\ndistribution. The fairness loss is calculated by solving an optimization\nproblem for fairness metrics such as demographic parity and equalized odds. The\nattack is insidious and hard to detect, as it maintains global accuracy even\nafter increasing the bias. We evaluate our attack against the state-of-the-art\nByzantine-robust and fairness-aware aggregation schemes over different\ndatasets, in various settings. The empirical results demonstrate the attack\nefficacy by increasing the bias up to 90\\%, even in the presence of a single\nmalicious client in the FL system.", "AI": {"tldr": "The paper introduces an intentional fairness attack in federated learning, where a single client manipulates the model to increase bias while maintaining global accuracy, undermining fairness metrics.", "motivation": "To address the vulnerability of federated learning systems to poisoning attacks and to demonstrate how fairness metrics can be deliberately manipulated.", "method": "A client intentionally increases fairness loss during training using an optimization problem for metrics like demographic parity and equalized odds. The attack's effectiveness is tested against robust aggregation methods using various datasets.", "result": "Empirical evaluation shows that even a single malicious client can increase bias by up to 90%, illustrating the attack's potency against state-of-the-art defense mechanisms.", "conclusion": "Federated learning systems remain vulnerable to sophisticated poisoning attacks that challenge fairness, highlighting the need for new defense strategies."}}
{"id": "2510.12285", "pdf": "https://arxiv.org/pdf/2510.12285", "abs": "https://arxiv.org/abs/2510.12285", "authors": ["Zeyu Zhao", "Ningtao Wang", "Xing Fu", "Yu Cheng"], "title": "Chinese ModernBERT with Whole-Word Masking", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Encoder-only Transformers have advanced along three axes -- architecture,\ndata, and systems -- yielding Pareto gains in accuracy, speed, and memory\nefficiency. Yet these improvements have not fully transferred to Chinese, where\ntokenization and morphology differ markedly from English. We introduce Chinese\nModernBERT, a from-scratch Chinese encoder that couples: (i) a hardware-aware\n32k BPE vocabulary tailored to frequent Chinese affixes/compounds, lowering the\nembedding budget; (ii) whole-word masking (WWM) with a dynamic masking\ncurriculum (30% -> 15%) to align task difficulty with training progress; (iii)\na two-stage pre-training pipeline that extends the native context from 1,024 to\n8,192 tokens using RoPE and alternating local/global attention; and (iv) a\ndamped-cosine learning-rate schedule for stable long-horizon optimization. We\npre-train on ~1.2T Chinese tokens from CCI3-HQ, CCI4 (Chinese), and\nCosmopedia-Chinese. On CLUE, Chinese ModernBERT is competitive with strong\nChinese encoders under a unified fine-tuning protocol. Under bf16 it achieves\nhigh long-sequence throughput while maintaining strong short-sequence speed,\nreflecting benefits from budget allocation and attention design. To probe\nretrieval-oriented quality, we add a small amount of open contrastive data:\nfine-tuning on SimCLUE (~3M pairs) improves further when adding T2Ranking\n(~2M), reaching 0.505 (Pearson) / 0.537 (Spearman) on the SimCLUE test set.\nUnder this open-data setting, Chinese ModernBERT surpasses Qwen-0.6B-embedding\non SimCLUE, suggesting a clear scaling path for STS with additional curated\npairs. We will release tokenizer and weights to facilitate reproducible\nresearch.", "AI": {"tldr": "The paper introduces Chinese ModernBERT, an encoder-only Transformer specifically tailored for Chinese language tasks through architecture, vocabulary, masking, and pre-training innovations, achieving strong performance on Chinese datasets.", "motivation": "To address the challenges in transferring encoder-based Transformer improvements from English to Chinese due to differences in tokenization and language structure.", "method": "The authors developed Chinese ModernBERT by combining a 32k BPE vocabulary optimized for Chinese affixes, whole-word masking with dynamic curriculum, extended-context training, and damped-cosine learning-rate scheduling.", "result": "Chinese ModernBERT demonstrates competitive results on CLUE benchmarks, achieves efficient long and short-sequence processing, and outperforms Qwen-0.6B-embedding in open-data retrieval tasks with improved STS quality.", "conclusion": "Chinese ModernBERT provides a scalable and reproducible approach for Chinese language modeling, with its tokenizer and weights released to promote further research advancements."}}
{"id": "2510.12555", "pdf": "https://arxiv.org/pdf/2510.12555", "abs": "https://arxiv.org/abs/2510.12555", "authors": ["Andries Rosseau", "Rapha\u00ebl Avalos", "Ann Now\u00e9"], "title": "Inclusive Fitness as a Key Step Towards More Advanced Social Behaviors in Multi-Agent Reinforcement Learning Settings", "categories": ["cs.AI", "cs.MA", "cs.SI"], "comment": "This version is a slightly updated version (e.g., added an important\n  reference) compared to the peer-reviewed versions at 'Adapative Learning\n  Agents' at AAMAS 2022 or 'From Cells to Societies' at ICLR 2022", "summary": "The competitive and cooperative forces of natural selection have driven the\nevolution of intelligence for millions of years, culminating in nature's vast\nbiodiversity and the complexity of human minds. Inspired by this process, we\npropose a novel multi-agent reinforcement learning framework where each agent\nis assigned a genotype and where reward functions are modelled after the\nconcept of inclusive fitness. An agent's genetic material may be shared with\nother agents, and our inclusive reward function naturally accounts for this. We\nstudy the resulting social dynamics in two types of network games with\nprisoner's dilemmas and find that our results align with well-established\nprinciples from biology, such as Hamilton's rule. Furthermore, we outline how\nthis framework can extend to more open-ended environments with spatial and\ntemporal structure, finite resources, and evolving populations. We hypothesize\nthe emergence of an arms race of strategies, where each new strategy is a\ngradual improvement over earlier adaptations of other agents, effectively\nproducing a multi-agent autocurriculum analogous to biological evolution. In\ncontrast to the binary team-based structures prevalent in earlier research, our\ngene-based reward structure introduces a spectrum of cooperation ranging from\nfull adversity to full cooperativeness based on genetic similarity, enabling\nunique non team-based social dynamics. For example, one agent having a mutual\ncooperative relationship with two other agents, while the two other agents\nbehave adversarially towards each other. We argue that incorporating inclusive\nfitness in agents provides a foundation for the emergence of more strategically\nadvanced and socially intelligent agents.", "AI": {"tldr": "The paper introduces a novel multi-agent reinforcement learning framework using genotypes and inclusive fitness, aligning agent interactions with biological principles like Hamilton's rule.", "motivation": "The paper aims to mimic natural evolution's intelligence development by introducing genotypic models and inclusive fitness into multi-agent systems.", "method": "A reinforcement learning framework assigns genetic material to agents and uses an inclusive fitness reward structure to capture social dynamics and cooperation levels.", "result": "Results show consistency with biological principles, potential emergence of a multi-agent strategy evolution akin to biological autocurriculum, and diverse social interactions beyond simple team-based structures.", "conclusion": "The paper suggests that incorporating inclusive fitness may enhance strategic and socially intelligent agent behaviors in multi-agent systems."}}
{"id": "2510.12267", "pdf": "https://arxiv.org/pdf/2510.12267", "abs": "https://arxiv.org/abs/2510.12267", "authors": ["Chenghanyu Zhang", "Zekun Li", "Peipei Li", "Xing Cui", "Shuhan Xia", "Weixiang Yan", "Yiqiao Zhang", "Qianyu Zhuang"], "title": "SpineBench: Benchmarking Multimodal LLMs for Spinal Pathology Analysis", "categories": ["cs.CV"], "comment": "Proceedings of the 33rd ACM International Conference on\n  Multimedia,ACMMM 2025 Dataset Track", "summary": "With the increasing integration of Multimodal Large Language Models (MLLMs)\ninto the medical field, comprehensive evaluation of their performance in\nvarious medical domains becomes critical. However, existing benchmarks\nprimarily assess general medical tasks, inadequately capturing performance in\nnuanced areas like the spine, which relies heavily on visual input. To address\nthis, we introduce SpineBench, a comprehensive Visual Question Answering (VQA)\nbenchmark designed for fine-grained analysis and evaluation of MLLMs in the\nspinal domain. SpineBench comprises 64,878 QA pairs from 40,263 spine images,\ncovering 11 spinal diseases through two critical clinical tasks: spinal disease\ndiagnosis and spinal lesion localization, both in multiple-choice format.\nSpineBench is built by integrating and standardizing image-label pairs from\nopen-source spinal disease datasets, and samples challenging hard negative\noptions for each VQA pair based on visual similarity (similar but not the same\ndisease), simulating real-world challenging scenarios. We evaluate 12 leading\nMLLMs on SpineBench. The results reveal that these models exhibit poor\nperformance in spinal tasks, highlighting limitations of current MLLM in the\nspine domain and guiding future improvements in spinal medicine applications.\nSpineBench is publicly available at\nhttps://zhangchenghanyu.github.io/SpineBench.github.io/.", "AI": {"tldr": "The paper introduces SpineBench, a Visual Question Answering benchmark for evaluating Multimodal Large Language Models (MLLMs) in spinal medicine tasks such as disease diagnosis and lesion localization.", "motivation": "Existing benchmarks fail to evaluate MLLMs effectively in specialized medical fields like spinal medicine that rely heavily on visual data.", "method": "The authors created SpineBench by standardizing open-source spinal disease datasets and generating QA pairs with hard negative options to simulate real-world scenarios. They assessed 12 MLLMs using this benchmark.", "result": "Evaluations showed the 12 MLLMs underperforming in spine-related tasks, indicating limitations in current MLLMs for specialized medical domains.", "conclusion": "SpineBench serves as a valuable tool to highlight gaps in MLLM capabilities in spinal medicine, fostering targeted advancements in this field."}}
{"id": "2510.12144", "pdf": "https://arxiv.org/pdf/2510.12144", "abs": "https://arxiv.org/abs/2510.12144", "authors": ["Ali Parsaee", "Bei Jiang", "Zachary Friggstad", "Russell Greiner"], "title": "Budget-constrained Active Learning to Effectively De-censor Survival Data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Standard supervised learners attempt to learn a model from a labeled dataset.\nGiven a small set of labeled instances, and a pool of unlabeled instances, a\nbudgeted learner can use its given budget to pay to acquire the labels of some\nunlabeled instances, which it can then use to produce a model. Here, we explore\nbudgeted learning in the context of survival datasets, which include (right)\ncensored instances, where we know only a lower bound on an instance's\ntime-to-event. Here, that learner can pay to (partially) label a censored\ninstance -- e.g., to acquire the actual time for an instance [perhaps go from\n(3 yr, censored) to (7.2 yr, uncensored)], or other variants [e.g., learn about\none more year, so go from (3 yr, censored) to either (4 yr, censored) or\nperhaps (3.2 yr, uncensored)]. This serves as a model of real world data\ncollection, where follow-up with censored patients does not always lead to\nuncensoring, and how much information is given to the learner model during data\ncollection is a function of the budget and the nature of the data itself. We\nprovide both experimental and theoretical results for how to apply\nstate-of-the-art budgeted learning algorithms to survival data and the\nrespective limitations that exist in doing so. Our approach provides bounds and\ntime complexity asymptotically equivalent to the standard active learning\nmethod BatchBALD. Moreover, empirical analysis on several survival tasks show\nthat our model performs better than other potential approaches on several\nbenchmarks.", "AI": {"tldr": "The paper explores budgeted learning for survival datasets, addressing challenges with censored data and improving model performance using state-of-the-art algorithms.", "motivation": "To enhance the performance of supervised learning in survival datasets with censored instances, representing real-world data collection challenges.", "method": "Designing budgeted learning algorithms using experimental and theoretical approaches based on BatchBALD for survival data scenarios.", "result": "The proposed method shows asymptotic equivalence in bounds and time complexity to BatchBALD and outperforms other models in several survival dataset benchmarks.", "conclusion": "The framework effectively applies budgeted learning to censored survival data and demonstrates better performance in benchmarks."}}
{"id": "2510.12306", "pdf": "https://arxiv.org/pdf/2510.12306", "abs": "https://arxiv.org/abs/2510.12306", "authors": ["Cameron Morin", "Matti Marttinen Larsson"], "title": "A large-scale, unsupervised pipeline for automatic corpus annotation using LLMs: variation and change in the English consider construction", "categories": ["cs.CL"], "comment": null, "summary": "As natural language corpora expand at an unprecedented rate, manual\nannotation remains a significant methodological bottleneck in corpus linguistic\nwork. We address this challenge by presenting a scalable, unsupervised pipeline\nfor automating grammatical annotation in voluminous corpora using large\nlanguage models (LLMs). Unlike previous supervised and iterative approaches,\nour method employs a four-phase workflow: prompt engineering, pre-hoc\nevaluation, automated batch processing, and post-hoc validation. We demonstrate\nthe pipeline's accessibility and effectiveness through a diachronic case study\nof variation in the English consider construction. Using GPT-5 through the\nOpenAI API, we annotate 143,933 sentences from the Corpus of Historical\nAmerican English (COHA) in under 60 hours, achieving 98%+ accuracy on two\nsophisticated annotation procedures. Our results suggest that LLMs can perform\na range of data preparation tasks at scale with minimal human intervention,\nopening new possibilities for corpus-based research, though implementation\nrequires attention to costs, licensing, and other ethical considerations.", "AI": {"tldr": "The paper presents an unsupervised pipeline for automating grammatical annotation in large corpora using large language models, achieving high accuracy and scalability.", "motivation": "To address the time-intensive bottleneck of manual annotation in corpus linguistics, especially as corpora grow larger.", "method": "The pipeline involves a four-phase workflow: prompt engineering, pre-hoc evaluation, automated batch processing, and post-hoc validation. Testing was performed using GPT-5 and 143,933 sentences from the Corpus of Historical American English (COHA).", "result": "The pipeline annotated sentences with over 98% accuracy within 60 hours, showcasing its efficiency for corpus linguistics annotation tasks.", "conclusion": "LLMs have proven capable of large-scale, accurate data preparation with minimal manual intervention, though challenges like cost, licensing, and ethical concerns remain."}}
{"id": "2510.12563", "pdf": "https://arxiv.org/pdf/2510.12563", "abs": "https://arxiv.org/abs/2510.12563", "authors": ["Jingcong Liang", "Shijun Wan", "Xuehai Wu", "Siyuan Wang", "Yitong Li", "Qianglong Chen", "Duyu Tang", "Zhongyu Wei"], "title": "HardcoreLogic: Challenging Large Reasoning Models with Long-tail Logic Puzzle Games", "categories": ["cs.AI"], "comment": null, "summary": "Large Reasoning Models (LRMs) have demonstrated impressive performance on\ncomplex tasks, including logical puzzle games that require deriving solutions\nsatisfying all constraints. However, whether they can flexibly apply\nappropriate rules to varying conditions, particularly when faced with\nnon-canonical game variants, remains an open question. Existing corpora focus\non popular puzzles like 9x9 Sudoku, risking overfitting to canonical formats\nand memorization of solution patterns, which can mask deficiencies in\nunderstanding novel rules or adapting strategies to new variants. To address\nthis, we introduce HardcoreLogic, a challenging benchmark of over 5,000 puzzles\nacross 10 games, designed to test the robustness of LRMs on the \"long-tail\" of\nlogical games. HardcoreLogic systematically transforms canonical puzzles\nthrough three dimensions: Increased Complexity (IC), Uncommon Elements (UE),\nand Unsolvable Puzzles (UP), reducing reliance on shortcut memorization.\nEvaluations on a diverse set of LRMs reveal significant performance drops, even\nfor models achieving top scores on existing benchmarks, indicating heavy\nreliance on memorized stereotypes. While increased complexity is the dominant\nsource of difficulty, models also struggle with subtle rule variations that do\nnot necessarily increase puzzle difficulty. Our systematic error analysis on\nsolvable and unsolvable puzzles further highlights gaps in genuine reasoning.\nOverall, HardcoreLogic exposes the limitations of current LRMs and establishes\na benchmark for advancing high-level logical reasoning.", "AI": {"tldr": "This paper introduces 'HardcoreLogic,' a benchmark of over 5,000 puzzles designed to test the reasoning capabilities of Large Reasoning Models (LRMs) on less familiar and challenging puzzles.", "motivation": "The motivation is to address the gaps in LRMs' ability to adapt to diverse and non-canonical logical puzzles, moving beyond reliance on memorization found in existing popular benchmarks like 9x9 Sudoku.", "method": "HardcoreLogic modifies canonical puzzles via three dimensions: Increased Complexity (IC), Uncommon Elements (UE), and Unsolvable Puzzles (UP), creating a challenging dataset to rigorously test models.", "result": "The study found significant performance drops among top-scoring LRMs on this benchmark, demonstrating their reliance on pre-learned patterns and difficulty with subtle rule variations.", "conclusion": "HardcoreLogic highlights the limitations of LRMs in genuine reasoning, providing a valuable tool to push advancements in logical reasoning models."}}
{"id": "2510.12282", "pdf": "https://arxiv.org/pdf/2510.12282", "abs": "https://arxiv.org/abs/2510.12282", "authors": ["Ying A", "Wenzhang Sun", "Chang Zeng", "Chunfeng Wang", "Hao Li", "Jianxun Cui"], "title": "PAGS: Priority-Adaptive Gaussian Splatting for Dynamic Driving Scenes", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing dynamic 3D urban scenes is crucial for autonomous driving, yet\ncurrent methods face a stark trade-off between fidelity and computational cost.\nThis inefficiency stems from their semantically agnostic design, which\nallocates resources uniformly, treating static backgrounds and safety-critical\nobjects with equal importance. To address this, we introduce Priority-Adaptive\nGaussian Splatting (PAGS), a framework that injects task-aware semantic\npriorities directly into the 3D reconstruction and rendering pipeline. PAGS\nintroduces two core contributions: (1) Semantically-Guided Pruning and\nRegularization strategy, which employs a hybrid importance metric to\naggressively simplify non-critical scene elements while preserving fine-grained\ndetails on objects vital for navigation. (2) Priority-Driven Rendering\npipeline, which employs a priority-based depth pre-pass to aggressively cull\noccluded primitives and accelerate the final shading computations. Extensive\nexperiments on the Waymo and KITTI datasets demonstrate that PAGS achieves\nexceptional reconstruction quality, particularly on safety-critical objects,\nwhile significantly reducing training time and boosting rendering speeds to\nover 350 FPS.", "AI": {"tldr": "PAGS is a framework for improving dynamic 3D urban scene reconstruction efficiency by injecting semantic priorities, focusing on safety-critical elements, and achieving high fidelity and speed.", "motivation": "Existing methods for 3D urban scene reconstruction face inefficiencies due to semantically agnostic designs, failing to prioritize safety-critical objects and balancing fidelity with computational costs.", "method": "PAGS introduces Semantically-Guided Pruning and Regularization to simplify non-critical elements and Priority-Driven Rendering to optimize computational resources by culling occluded primitives.", "result": "Experiments on Waymo and KITTI datasets show PAGS achieves superior reconstruction quality for critical objects, significantly reduced training times, and rendering speeds exceeding 350 FPS.", "conclusion": "PAGS effectively balances fidelity and speed by prioritizing safety-critical objects, demonstrating its potential for advancements in autonomous driving 3D urban scene reconstruction."}}
{"id": "2510.12157", "pdf": "https://arxiv.org/pdf/2510.12157", "abs": "https://arxiv.org/abs/2510.12157", "authors": ["Zhongwei Yu", "Wannian Xia", "Xue Yan", "Bo Xu", "Haifeng Zhang", "Yali Du", "Jun Wang"], "title": "Self-Verifying Reflection Helps Transformers with CoT Reasoning", "categories": ["cs.LG"], "comment": "Accepted by NeurIPS2025", "summary": "Advanced large language models (LLMs) frequently reflect in reasoning\nchain-of-thoughts (CoTs), where they self-verify the correctness of current\nsolutions and explore alternatives. However, given recent findings that LLMs\ndetect limited errors in CoTs, how reflection contributes to empirical\nimprovements remains unclear. To analyze this issue, in this paper, we present\na minimalistic reasoning framework to support basic self-verifying reflection\nfor small transformers without natural language, which ensures analytic clarity\nand reduces the cost of comprehensive experiments. Theoretically, we prove that\nself-verifying reflection guarantees improvements if verification errors are\nproperly bounded. Experimentally, we show that tiny transformers, with only a\nfew million parameters, benefit from self-verification in both training and\nreflective execution, reaching remarkable LLM-level performance in integer\nmultiplication and Sudoku. Similar to LLM results, we find that reinforcement\nlearning (RL) improves in-distribution performance and incentivizes frequent\nreflection for tiny transformers, yet RL mainly optimizes shallow statistical\npatterns without faithfully reducing verification errors. In conclusion,\nintegrating generative transformers with discriminative verification inherently\nfacilitates CoT reasoning, regardless of scaling and natural language.", "AI": {"tldr": "The paper explores self-verifying reflection in reasoning for small transformers, showing theoretical and experimental progress while analyzing its limitations.", "motivation": "The motivation is to understand how self-verifying reflective processes in reasoning contribute to empirical improvements in transformers, especially given their limited error detection in reasoning chains.", "method": "A minimalistic reasoning framework is presented for small transformers without natural language. This simplifies analytical clarity and reduces experimentation costs. Theoretical proofs and experiments were conducted to validate the role of self-verification.", "result": "Self-verification in tiny transformers improves performance in both training and reflective execution tasks, achieving LLM-level results in specific domains like integer multiplication and Sudoku. Reinforcement learning enhances in-distribution performance but largely optimizes superficial patterns.", "conclusion": "Integrating generative transformers with discriminative verification enhances chain-of-thought reasoning, irrespective of scaling or natural language use."}}
{"id": "2510.12316", "pdf": "https://arxiv.org/pdf/2510.12316", "abs": "https://arxiv.org/abs/2510.12316", "authors": ["Greta Damo", "Elena Cabrio", "Serena Villata"], "title": "Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation", "categories": ["cs.CL"], "comment": null, "summary": "Counter-speech generation is at the core of many expert activities, such as\nfact-checking and hate speech, to counter harmful content. Yet, existing work\ntreats counter-speech generation as pure text generation task, mainly based on\nLarge Language Models or NGO experts. These approaches show severe drawbacks\ndue to the limited reliability and coherence in the generated countering text,\nand in scalability, respectively. To close this gap, we introduce a novel\nframework to model counter-speech generation as knowledge-wise text generation\nprocess. Our framework integrates advanced Retrieval-Augmented Generation (RAG)\npipelines to ensure the generation of trustworthy counter-speech for 8 main\ntarget groups identified in the hate speech literature, including women, people\nof colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons,\nand other. We built a knowledge base over the United Nations Digital Library,\nEUR-Lex and the EU Agency for Fundamental Rights, comprising a total of 32,792\ntexts. We use the MultiTarget-CONAN dataset to empirically assess the quality\nof the generated counter-speech, both through standard metrics (i.e., JudgeLM)\nand a human evaluation. Results show that our framework outperforms standard\nLLM baselines and competitive approach, on both assessments. The resulting\nframework and the knowledge base pave the way for studying trustworthy and\nsound counter-speech generation, in hate speech and beyond.", "AI": {"tldr": "This paper presents a novel framework for generating reliable counter-speech using knowledge-wise text generation with advanced Retrieval-Augmented Generation (RAG) pipelines, aimed at addressing reliability and scalability issues in the domain.", "motivation": "The paper aims to address the limitations of current counter-speech generation methods, including lack of reliability, coherence, and scalability, which are critical in countering harmful content like hate speech.", "method": "The authors employ a framework that models counter-speech as a knowledge-wise text generation process, utilizing a knowledge base comprising 32,792 texts from trusted sources like the United Nations Digital Library and EU agencies. This is paired with advanced RAG pipelines tailored for 8 target groups identified in hate speech literature.", "result": "Empirical evaluations using the MultiTarget-CONAN dataset, standard metrics (JudgeLM), and human evaluation show that the proposed framework outperforms both traditional LLM baselines and other competitive models in counter-speech generation quality.", "conclusion": "This framework and its corresponding knowledge base demonstrate the potential for studying trustworthy counter-speech generation for addressing hate speech and similar challenges, offering improvements in scalability, coherence, and reliability."}}
{"id": "2510.12635", "pdf": "https://arxiv.org/pdf/2510.12635", "abs": "https://arxiv.org/abs/2510.12635", "authors": ["Yuxiang Zhang", "Jiangming Shu", "Ye Ma", "Xueyuan Lin", "Shangxi Wu", "Jitao Sang"], "title": "Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models face challenges in long-horizon agentic tasks as their\nconstrained memory is easily overwhelmed by distracting or irrelevant context.\nExisting working memory methods typically rely on external, heuristic\nmechanisms that are decoupled from the agent's core policy. In this work, we\nreframe working memory management as a learnable, intrinsic capability. We\npropose a novel framework, Memory-as-Action, where an agent actively manages\nits working memory by executing explicit editing operations as part of a\nunified policy. This formulation allows an agent, trained via reinforcement\nlearning, to balance memory curation against long-term task objectives under\ngiven resource constraints. However, such memory editing actions break the\nstandard assumption of a continuously growing prefix in LLM interactions,\nleading to what we call trajectory fractures. These non-prefix changes disrupt\nthe causal continuity required by standard policy gradient methods, making\nthose methods inapplicable. To address this, we propose a new algorithm,\nDynamic Context Policy Optimization, which enables stable end-to-end\nreinforcement learning by segmenting trajectories at memory action points and\napplying trajectory-level advantages to the resulting action segments. Our\nresults demonstrate that jointly optimizing for task reasoning and memory\nmanagement in an end-to-end fashion not only reduces overall computational\nconsumption but also improves task performance, driven by adaptive context\ncuration strategies tailored to the model's intrinsic capabilities.", "AI": {"tldr": "The paper introduces a novel Memory-as-Action framework for large language models (LLMs), focusing on integrating memory editing as an intrinsic capability via reinforcement learning. This improves task reasoning and reduces computational costs.", "motivation": "To address limitations in LLMs when dealing with long-context tasks, where constrained memory often leads to distractions from irrelevant context, by reframing working memory management as an intrinsic capability.", "method": "They propose the Memory-as-Action framework, where memory editing is treated as part of the agent's policy, and introduce Dynamic Context Policy Optimization to handle trajectory fractures caused by these non-prefix memory edits.", "result": "The approach enables better management of task reasoning and memory by integrating adaptive memory strategies. It leads to enhanced task performance and lower computational consumption.", "conclusion": "End-to-end optimization of task reasoning and memory management in LLMs supports adaptive context curation, improving both efficiency and performance."}}
{"id": "2510.12283", "pdf": "https://arxiv.org/pdf/2510.12283", "abs": "https://arxiv.org/abs/2510.12283", "authors": ["Jianfeng Dong", "Lei Huang", "Daizong Liu", "Xianke Chen", "Xun Yang", "Changting Lin", "Xun Wang", "Meng Wang"], "title": "Dual Learning with Dynamic Knowledge Distillation and Soft Alignment for Partially Relevant Video Retrieval", "categories": ["cs.CV"], "comment": null, "summary": "Almost all previous text-to-video retrieval works ideally assume that videos\nare pre-trimmed with short durations containing solely text-related content.\nHowever, in practice, videos are typically untrimmed in long durations with\nmuch more complicated background content. Therefore, in this paper, we focus on\nthe more practical yet challenging task of Partially Relevant Video Retrieval\n(PRVR), which aims to retrieve partially relevant untrimmed videos with the\ngiven query. To tackle this task, we propose a novel framework that distills\ngeneralization knowledge from a powerful large-scale vision-language\npre-trained model and transfers it to a lightweight, task-specific PRVR\nnetwork. Specifically, we introduce a Dual Learning framework with Dynamic\nKnowledge Distillation (DL-DKD++), where a large teacher model provides\nsupervision to a compact dual-branch student network. The student model\ncomprises two branches: an inheritance branch that absorbs transferable\nknowledge from the teacher, and an exploration branch that learns task-specific\ninformation from the PRVR dataset to address domain gaps. To further enhance\nlearning, we incorporate a dynamic soft-target construction mechanism. By\nreplacing rigid hard-target supervision with adaptive soft targets that evolve\nduring training, our method enables the model to better capture the\nfine-grained, partial relevance between videos and queries. Experiment results\ndemonstrate that our proposed model achieves state-of-the-art performance on\nTVR, ActivityNet, and Charades-STA datasets for PRVR. The code is available at\nhttps://github.com/HuiGuanLab/DL-DKD.", "AI": {"tldr": "The paper addresses partially relevant video retrieval (PRVR) for untrimmed videos, introducing a novel framework to tackle related challenges and achieving state-of-the-art results.", "motivation": "Many text-to-video frameworks exist in ideal scenarios with videos trimmed to match queries. However, practical scenarios involve untrimmed videos with complex content, necessitating effective methods for PRVR.", "method": "The authors propose Dynamic Knowledge Distillation (DL-DKD++) using a dual-branch student model supervised by a powerful teacher model, with dynamic soft targets for adaptive learning.", "result": "The framework achieves state-of-the-art performance on benchmarks such as TVR, ActivityNet, and Charades-STA for partially relevant video queries.", "conclusion": "The DL-DKD++ framework effectively bridges domain gaps and fine-tunes PRVR task-specific nuances, demonstrating high efficiency and robustness for video retrieval tasks."}}
{"id": "2510.12209", "pdf": "https://arxiv.org/pdf/2510.12209", "abs": "https://arxiv.org/abs/2510.12209", "authors": ["Yiming Zhang", "Chester Holtz", "Gal Mishne", "Alex Cloninger"], "title": "Revisiting Meta-Learning with Noisy Labels: Reweighting Dynamics and Theoretical Guarantees", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Learning with noisy labels remains challenging because over-parameterized\nnetworks memorize corrupted supervision. Meta-learning-based sample reweighting\nmitigates this by using a small clean subset to guide training, yet its\nbehavior and training dynamics lack theoretical understanding. We provide a\nrigorous theoretical analysis of meta-reweighting under label noise and show\nthat its training trajectory unfolds in three phases: (i) an alignment phase\nthat amplifies examples consistent with a clean subset and suppresses\nconflicting ones; (ii) a filtering phase driving noisy example weights toward\nzero until the clean subset loss plateaus; and (iii) a post-filtering phase in\nwhich noise filtration becomes perturbation-sensitive. The mechanism is a\nsimilarity-weighted coupling between training and clean subset signals together\nwith clean subset training loss contraction; in the post-filtering regime where\nthe clean-subset loss is sufficiently small, the coupling term vanishes and\nmeta-reweighting loses discriminatory power. Guided by this analysis, we\npropose a lightweight surrogate for meta-reweighting that integrates\nmean-centering, row shifting, and label-signed modulation, yielding more stable\nperformance while avoiding expensive bi-level optimization. Across synthetic\nand real noisy-label benchmarks, our method consistently outperforms strong\nreweighting/selection baselines.", "AI": {"tldr": "The study delivers a detailed theoretical analysis of meta-reweighting under noisy labels, highlights its limitations, and introduces a surrogate method for performance improvement.", "motivation": "To address the challenge of training models with noisy labels, which can lead to memorization of corrupted supervision, and improve understanding and performance of meta-reweighting methods.", "method": "The paper analyzes the phases of meta-reweighting under label noise using a theoretical approach and introduces a surrogate method involving mean-centering, row shifting, and label-signed modulation to improve stability and performance.", "result": "The analysis reveals specific training dynamics in meta-reweighting, identifies weaknesses in post-filtering phases, and demonstrates superior performance of the proposed surrogate method compared to existing baselines.", "conclusion": "The proposed surrogate method aids in achieving stable performance improvements in noisy-label settings without the computational cost of meta-reweighting, advancing understanding and practical application of such techniques."}}
{"id": "2510.12355", "pdf": "https://arxiv.org/pdf/2510.12355", "abs": "https://arxiv.org/abs/2510.12355", "authors": ["Michela Proietti", "Roberto Capobianco", "Mariya Toneva"], "title": "Fine-grained Analysis of Brain-LLM Alignment through Input Attribution", "categories": ["cs.CL"], "comment": null, "summary": "Understanding the alignment between large language models (LLMs) and human\nbrain activity can reveal computational principles underlying language\nprocessing. We introduce a fine-grained input attribution method to identify\nthe specific words most important for brain-LLM alignment, and leverage it to\nstudy a contentious research question about brain-LLM alignment: the\nrelationship between brain alignment (BA) and next-word prediction (NWP). Our\nfindings reveal that BA and NWP rely on largely distinct word subsets: NWP\nexhibits recency and primacy biases with a focus on syntax, while BA\nprioritizes semantic and discourse-level information with a more targeted\nrecency effect. This work advances our understanding of how LLMs relate to\nhuman language processing and highlights differences in feature reliance\nbetween BA and NWP. Beyond this study, our attribution method can be broadly\napplied to explore the cognitive relevance of model predictions in diverse\nlanguage processing tasks.", "AI": {"tldr": "The study explores the alignment between large language models (LLMs) and human brain activity during language processing, using a novel attribution method.", "motivation": "To uncover computational principles in language processing by examining the alignment between human brain activity and LLMs.", "method": "A fine-grained input attribution method was introduced to analyze specific words' importance for brain-LLM alignment, particularly comparing brain alignment (BA) and next-word prediction (NWP).", "result": "The analysis found that BA and NWP rely on distinct word sets, with BA focusing on semantic and discourse-level information, and NWP focusing on syntax and biases like recency and primacy.", "conclusion": "The study highlights fundamental differences in the features that BA and NWP prioritize and offers a generalizable attribution method for cognitive and language studies."}}
{"id": "2510.12693", "pdf": "https://arxiv.org/pdf/2510.12693", "abs": "https://arxiv.org/abs/2510.12693", "authors": ["Hanyang Chen", "Mark Zhao", "Rui Yang", "Qinwei Ma", "Ke Yang", "Jiarui Yao", "Kangrui Wang", "Hao Bai", "Zhenhailong Wang", "Rui Pan", "Mengchao Zhang", "Jose Barreiros", "Aykut Onol", "ChengXiang Zhai", "Heng Ji", "Manling Li", "Huan Zhang", "Tong Zhang"], "title": "ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in embodied AI highlight the potential of vision language\nmodels (VLMs) as agents capable of perception, reasoning, and interaction in\ncomplex environments. However, top-performing systems rely on large-scale\nmodels that are costly to deploy, while smaller VLMs lack the necessary\nknowledge and skills to succeed. To bridge this gap, we present\n\\textit{Embodied Reasoning Agent (ERA)}, a two-stage framework that integrates\nprior knowledge learning and online reinforcement learning (RL). The first\nstage, \\textit{Embodied Prior Learning}, distills foundational knowledge from\nthree types of data: (1) Trajectory-Augmented Priors, which enrich existing\ntrajectory data with structured reasoning generated by stronger models; (2)\nEnvironment-Anchored Priors, which provide in-environment knowledge and\ngrounding supervision; and (3) External Knowledge Priors, which transfer\ngeneral knowledge from out-of-environment datasets. In the second stage, we\ndevelop an online RL pipeline that builds on these priors to further enhance\nagent performance. To overcome the inherent challenges in agent RL, including\nlong horizons, sparse rewards, and training instability, we introduce three key\ndesigns: self-summarization for context management, dense reward shaping, and\nturn-level policy optimization. Extensive experiments on both high-level\nplanning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate\nthat ERA-3B surpasses both prompting-based large models and previous\ntraining-based baselines. Specifically, it achieves overall improvements of\n8.4\\% on EB-ALFRED and 19.4\\% on EB-Manipulation over GPT-4o, and exhibits\nstrong generalization to unseen tasks. Overall, ERA offers a practical path\ntoward scalable embodied intelligence, providing methodological insights for\nfuture embodied AI systems.", "AI": {"tldr": "This paper introduces Embodied Reasoning Agent (ERA), a two-stage framework integrating prior knowledge learning and reinforcement learning to enhance smaller vision-language models' embodied AI capabilities.", "motivation": "To address the limitations of smaller vision-language models and high deployment costs of large-scale models in embodied AI tasks.", "method": "ERA integrates two stages: (1) Embodied Prior Learning from trajectory-augmented, environment-anchored, and external knowledge priors, and (2) Online Reinforcement Learning with designs like self-summarization, dense reward shaping, and turn-level policy optimization.", "result": "ERA-3B outperformed prompting-based large models and training-based baselines, with 8.4% improvement on high-level planning tasks and 19.4% improvement on low-level control tasks, demonstrating superior generalization.", "conclusion": "ERA offers a scalable and efficient path for developing embodied AI systems, combining knowledge learning and RL to enhance smaller VLM capabilities while achieving competitive performance."}}
{"id": "2510.12287", "pdf": "https://arxiv.org/pdf/2510.12287", "abs": "https://arxiv.org/abs/2510.12287", "authors": ["Sifan Li", "Hongkai Chen", "Yujun Cai", "Qingwen Ye", "Liyang Chen", "Junsong Yuan", "Yiwei Wang"], "title": "Vision Language Models Map Logos to Text via Semantic Entanglement in the Visual Projector", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Vision Language Models (VLMs) have achieved impressive progress in multimodal\nreasoning; yet, they remain vulnerable to hallucinations, where outputs are not\ngrounded in visual evidence. In this paper, we investigate a previously\noverlooked setting: logo hallucination, where models generate brand names or\ntextual content despite logos containing no visible words. Using curated splits\nof pure symbols, hybrids, and text-bearing logos, as well as the challenging\nHard-60 subset, we systematically measure hallucination across leading VLMs. We\nfurther probe robustness through nine structured perturbations and show that\nhallucinations persist even under strong distortions, with occlusion exposing\nthe sharpest weaknesses. Embedding-level analysis with open-weight LLaVA\ndemonstrates that hallucination is tied to a small subset of projector\ndimensions, and targeted ablation substantially reduces errors while preserving\nOCR accuracy. Together, these findings reveal that VLMs often rely on symbolic\npriors rather than genuine glyph perception, particularly for iconic circular\nlogos, and that projector subspaces play a decisive role in this failure mode.\nOur work contributes both a novel diagnostic lens and actionable mitigation\ninsights, highlighting projector disentanglement and OCR-guided decoding as\npromising directions for building more trustworthy multimodal systems.", "AI": {"tldr": "Vision Language Models (VLMs) are susceptible to \"logo hallucination,\" generating brand names or text content where logos lack visible words. Despite distortions, hallucinations persist due to reliance on symbolic priors rather than glyph perception.", "motivation": "Investigate the vulnerability of Vision Language Models (VLMs) to hallucinations, specifically when it comes to logos without visible text.", "method": "Curated logo splits, robustness testing with nine structured perturbations, embedding-level analysis targeting projector dimensions, and ablation techniques to distinguish hallucinations and enhance Optical Character Recognition (OCR) accuracy.", "result": "Findings show hallucinations stemming from symbolic priors and projector subspaces. Targeted ablation reduces errors while preserving OCR accuracy, exposing issues in multimodal reasoning for logos.", "conclusion": "VLMs often rely on symbolic priors rather than true visual comprehension, especially in circular logos, leading to logo hallucinations. Projector disentanglement and OCR-guided decoding are recommended solutions to enhance reliability."}}
{"id": "2510.12214", "pdf": "https://arxiv.org/pdf/2510.12214", "abs": "https://arxiv.org/abs/2510.12214", "authors": ["Tao Xie", "Zexi Tan", "Haoyi Xiao", "Binbin Sun", "Yiqun Zhang"], "title": "DE3S: Dual-Enhanced Soft-Sparse-Shape Learning for Medical Early Time-Series Classification", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to IEEE BIBM 2025", "summary": "Early time-series classification (ETSC) in medical applications is crucial\nfor time-sensitive scenarios such as sepsis prediction in intensive care units\n(ICUs), where a large number of deaths are caused by delayed prediction. ETSC\ncan significantly improve ICU resource utilization efficiency and healthcare\nprecision. However, it faces conflicting goals of accuracy and earliness, with\nexisting methods often trading one for the other, struggling to capture subtle\nearly-stage patterns due to weak initial signals and class imbalance. The key\nto solve these challenges is to find shapelets, which are discriminative\nsubsequences (or shapes) with high interpretability in time-series\nclassification. This paper proposes Dual-Enhanced Soft-Sparse-Shape Learning\nfor Medical Early Time-Series Classification (DE3S), which introduces a novel\nDual-Enhanced Soft-Shape Learning framework to figure out shapelets precisely\nthrough three innovations: (1) a comprehensive dual-enhancement strategy\ncombines traditional temporal augmentation with attention-based global temporal\nenhancement for robust representation learning, (2) an attention-score-based\nsoft shapelet sparsification mechanism dynamically preserves discriminative\npatterns while aggregating less important shapelets into representative tokens,\nand (3) a dual-path Mixture of Experts Network (MoE) and Inception modules\nfusion architecture where MoE performs local learning within shapelets and\nmulti-scale Inception modules capture global patterns across shapelets. The\nframework employs weighted cross-entropy loss for class imbalance handling and\ndemonstrates robustness on subject-consistency datasets. Extensive experiments\non six real-world medical datasets show state-of-the-art performance, with\nablation studies confirming component efficacy.", "AI": {"tldr": "This paper proposes DE3S, a novel method for early time-series classification in medical scenarios, focusing on the trade-off between predictive accuracy and earliness. It leverages dual-enhanced soft-shape learning to identify discriminative shapelets for better prediction.", "motivation": "The paper aims to improve early classification in time-series medical problems, such as sepsis prediction, to enhance ICU efficiency and precision while addressing challenges like weak initial signals and class imbalance.", "method": "The method, DE3S, introduces innovations such as dual-enhancement strategies combining temporal augmentation and attention-based enhancement, a soft shapelet sparsification mechanism, and a dual-path network architecture mixing local and global learning using MoE and Inception modules.", "result": "The proposed framework achieves state-of-the-art results on six real-world medical datasets and demonstrates robustness in handling class imbalance and subject consistency. Ablation studies confirm the effectiveness of each component.", "conclusion": "DE3S successfully balances accuracy and earliness in early medical time-series classification, offering a robust and interpretable solution with significant real-world implications."}}
{"id": "2510.12357", "pdf": "https://arxiv.org/pdf/2510.12357", "abs": "https://arxiv.org/abs/2510.12357", "authors": ["Yushu Zhao", "Yubin Qin", "Yang Wang", "Xiaolong Yang", "Huiming Han", "Shaojun Wei", "Yang Hu", "Shouyi Yin"], "title": "MoBiLE: Efficient Mixture-of-Experts Inference on Consumer GPU with Mixture of Big Little Experts", "categories": ["cs.CL"], "comment": "Accepted to ASP-DAC 2026", "summary": "Mixture-of-Experts (MoE) models have recently demonstrated exceptional\nperformance across a diverse range of applications. The principle of sparse\nactivation in MoE models facilitates an offloading strategy, wherein active\nexperts are maintained in GPU HBM, while inactive experts are stored in CPU\nDRAM. The efficacy of this approach, however, is fundamentally constrained by\nthe limited bandwidth of the CPU-GPU interconnect. To mitigate this bottleneck,\nexisting approaches have employed prefetching to accelerate MoE inference.\nThese methods attempt to predict and prefetch the required experts using\nspecially trained modules. Nevertheless, such techniques are often encumbered\nby significant training overhead and have shown diminished effectiveness on\nrecent MoE models with fine-grained expert segmentation.\n  In this paper, we propose MoBiLE, a plug-and-play offloading-based MoE\ninference framework with \\textit{mixture of big-little experts}. It reduces the\nnumber of experts for unimportant tokens to half for acceleration while\nmaintaining full experts for important tokens to guarantee model quality.\nFurther, a dedicated fallback and prefetching mechanism is designed for\nswitching between little and big experts to improve memory efficiency. We\nevaluate MoBiLE on four typical modern MoE architectures and challenging\ngenerative tasks. Our results show that MoBiLE achieves a speedup of 1.60x to\n1.72x compared to the baseline on a consumer GPU system, with negligible\ndegradation in accuracy.", "AI": {"tldr": "MoBiLE, a novel inference framework for Mixture-of-Experts models, combines big-little experts with fallback mechanisms, achieving significant speed improvements with minimal accuracy loss.", "motivation": "To address bandwidth limitations in CPU-GPU interconnect during MoE model inference and improve efficiency without compromising accuracy.", "method": "MoBiLE introduces big-little experts to reduce the number of experts for low-importance tokens while maintaining quality for high-importance tokens. It uses fallback and prefetch mechanisms to enhance memory efficiency.", "result": "MoBiLE improved inference speed by 1.60x to 1.72x compared to the baseline on consumer GPU systems with negligible accuracy degradation.", "conclusion": "The proposed framework effectively enhances performance for MoE-based generative tasks on modern architectures while maintaining model quality and memory efficiency."}}
{"id": "2510.12697", "pdf": "https://arxiv.org/pdf/2510.12697", "abs": "https://arxiv.org/abs/2510.12697", "authors": ["Tianyu Hu", "Zhen Tan", "Song Wang", "Huaizhi Qu", "Tianlong Chen"], "title": "Multi-Agent Debate for LLM Judges with Adaptive Stability Detection", "categories": ["cs.AI"], "comment": null, "summary": "With advancements in reasoning capabilities, Large Language Models (LLMs) are\nincreasingly employed for automated judgment tasks. While LLMs-as-Judges offer\npromise in automating evaluations, current approaches often rely on simplistic\naggregation methods (e.g., majority voting), which can fail even when\nindividual agents provide correct answers. To address this, we propose a\nmulti-agent debate judge framework where agents collaboratively reason and\niteratively refine their responses. We formalize the debate process\nmathematically, analyzing agent interactions and proving that debate amplifies\ncorrectness compared to static ensembles. To enhance efficiency, we introduce a\nstability detection mechanism that models judge consensus dynamics via a\ntime-varying Beta-Binomial mixture, with adaptive stopping based on\ndistributional similarity (Kolmogorov-Smirnov test). This mechanism models the\njudges' collective correct rate dynamics using a time-varying mixture of\nBeta-Binomial distributions and employs an adaptive stopping criterion based on\ndistributional similarity (Kolmogorov-Smirnov statistic). Experiments across\nmultiple benchmarks and models demonstrate that our framework improves judgment\naccuracy over majority voting while maintaining computational efficiency.", "AI": {"tldr": "The paper introduces a multi-agent debate judge framework enabling LLMs to collaboratively refine their judgments, outperforming traditional methods like majority voting and achieving higher accuracy and efficiency.", "motivation": "Current methods of using Large Language Models for automated evaluations rely on simplistic aggregation techniques, such as majority voting, which are prone to failure even with accurate individual answers. A more robust evaluation framework is needed.", "method": "The proposed multi-agent debate judge framework enables LLM agents to collaboratively reason, iteratively refine their judgments, and detect stability in consensus dynamics. This is formalized mathematically and employs a time-varying Beta-Binomial mixture model with adaptive stopping based on distributional similarity using the Kolmogorov-Smirnov test.", "result": "Experimental evaluations across benchmarks showed improved judgment accuracy over majority voting, with the framework maintaining computational efficiency.", "conclusion": "Collaborative reasoning among LLMs enhances judgment accuracy, proving the superiority of debate-based mechanisms over static methods in automating evaluations."}}
{"id": "2510.12308", "pdf": "https://arxiv.org/pdf/2510.12308", "abs": "https://arxiv.org/abs/2510.12308", "authors": ["Mohamed Omran", "Farhad Zanjani", "Davide Abati", "Jens Petersen", "Amirhossein Habibian"], "title": "Hybrid Gaussian Splatting for Novel Urban View Synthesis", "categories": ["cs.CV"], "comment": "ICCV 2025 RealADSim Workshop", "summary": "This paper describes the Qualcomm AI Research solution to the RealADSim-NVS\nchallenge, hosted at the RealADSim Workshop at ICCV 2025. The challenge\nconcerns novel view synthesis in street scenes, and participants are required\nto generate, starting from car-centric frames captured during some training\ntraversals, renders of the same urban environment as viewed from a different\ntraversal (e.g. different street lane or car direction). Our solution is\ninspired by hybrid methods in scene generation and generative simulators\nmerging gaussian splatting and diffusion models, and it is composed of two\nstages: First, we fit a 3D reconstruction of the scene and render novel views\nas seen from the target cameras. Then, we enhance the resulting frames with a\ndedicated single-step diffusion model. We discuss specific choices made in the\ninitialization of gaussian primitives as well as the finetuning of the enhancer\nmodel and its training data curation. We report the performance of our model\ndesign and we ablate its components in terms of novel view quality as measured\nby PSNR, SSIM and LPIPS. On the public leaderboard reporting test results, our\nproposal reaches an aggregated score of 0.432, achieving the second place\noverall.", "AI": {"tldr": "The paper presents Qualcomm AI Research\u2019s solution to the RealADSim-NVS challenge, focusing on novel view synthesis for urban environments using car-centric frames and achieving second place in the competition.", "motivation": "To develop an innovative method for novel view synthesis in urban street scenes using limited car-centric training frames.", "method": "The approach involves a two-stage pipeline: (1) 3D reconstruction of scenes with gaussian splatting for view rendering, and (2) applying a single-step diffusion model for enhancement.", "result": "The proposed solution achieves an aggregated score of 0.432 on the public leaderboard, ranking second overall.", "conclusion": "The hybrid pipeline effectively combines 3D reconstruction and diffusion model enhancement, demonstrating high-quality novel view synthesis for urban environments."}}
{"id": "2510.12220", "pdf": "https://arxiv.org/pdf/2510.12220", "abs": "https://arxiv.org/abs/2510.12220", "authors": ["Hanru Bai", "Weiyang Ding", "Difan Zou"], "title": "Hierarchical Koopman Diffusion: Fast Generation with Interpretable Diffusion Trajectory", "categories": ["cs.LG"], "comment": "NeurIPS 2025", "summary": "Diffusion models have achieved impressive success in high-fidelity image\ngeneration but suffer from slow sampling due to their inherently iterative\ndenoising process. While recent one-step methods accelerate inference by\nlearning direct noise-to-image mappings, they sacrifice the interpretability\nand fine-grained control intrinsic to diffusion dynamics, key advantages that\nenable applications like editable generation. To resolve this dichotomy, we\nintroduce \\textbf{Hierarchical Koopman Diffusion}, a novel framework that\nachieves both one-step sampling and interpretable generative trajectories.\nGrounded in Koopman operator theory, our method lifts the nonlinear diffusion\ndynamics into a latent space where evolution is governed by globally linear\noperators, enabling closed-form trajectory solutions. This formulation not only\neliminates iterative sampling but also provides full access to intermediate\nstates, allowing manual intervention during generation. To model the\nmulti-scale nature of images, we design a hierarchical architecture that\ndisentangles generative dynamics across spatial resolutions via scale-specific\nKoopman subspaces, capturing coarse-to-fine details systematically. We\nempirically show that the Hierarchical Koopman Diffusion not only achieves\ncompetitive one-step generation performance but also provides a principled\nmechanism for interpreting and manipulating the generative process through\nspectral analysis. Our framework bridges the gap between fast sampling and\ninterpretability in diffusion models, paving the way for explainable image\nsynthesis in generative modeling.", "AI": {"tldr": "This paper introduces Hierarchical Koopman Diffusion, a novel framework combining one-step image synthesis with interpretable generative trajectories, addressing limitations of current diffusion models.", "motivation": "To overcome the slow sampling times of diffusion models while retaining interpretability and control in the generative process.", "method": "A framework based on the Koopman operator theory that maps nonlinear diffusion processes to a latent space with globally linear dynamics, coupled with a hierarchical architecture to model multi-scale image features.", "result": "The model offers competitive one-step image generation performance while enabling interpretability and fine-grained control over the generative process.", "conclusion": "The proposed framework effectively bridges the gap between fast generation and interpretability, offering a new path for explainable and manipulable generative modeling."}}
{"id": "2510.12367", "pdf": "https://arxiv.org/pdf/2510.12367", "abs": "https://arxiv.org/abs/2510.12367", "authors": ["Rui Li", "Jia-Chen Gu", "Po-Nien Kung", "Heming Xia", "Junfeng liu", "Xiangwen Kong", "Zhifang Sui", "Nanyun Peng"], "title": "LLM-REVal: Can We Trust LLM Reviewers Yet?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has inspired\nresearchers to integrate them extensively into the academic workflow,\npotentially reshaping how research is practiced and reviewed. While previous\nstudies highlight the potential of LLMs in supporting research and peer review,\ntheir dual roles in the academic workflow and the complex interplay between\nresearch and review bring new risks that remain largely underexplored. In this\nstudy, we focus on how the deep integration of LLMs into both peer-review and\nresearch processes may influence scholarly fairness, examining the potential\nrisks of using LLMs as reviewers by simulation. This simulation incorporates a\nresearch agent, which generates papers and revises, alongside a review agent,\nwhich assesses the submissions. Based on the simulation results, we conduct\nhuman annotations and identify pronounced misalignment between LLM-based\nreviews and human judgments: (1) LLM reviewers systematically inflate scores\nfor LLM-authored papers, assigning them markedly higher scores than\nhuman-authored ones; (2) LLM reviewers persistently underrate human-authored\npapers with critical statements (e.g., risk, fairness), even after multiple\nrevisions. Our analysis reveals that these stem from two primary biases in LLM\nreviewers: a linguistic feature bias favoring LLM-generated writing styles, and\nan aversion toward critical statements. These results highlight the risks and\nequity concerns posed to human authors and academic research if LLMs are\ndeployed in the peer review cycle without adequate caution. On the other hand,\nrevisions guided by LLM reviews yield quality gains in both LLM-based and human\nevaluations, illustrating the potential of the LLMs-as-reviewers for\nearly-stage researchers and enhancing low-quality papers.", "AI": {"tldr": "The paper evaluates the influence of integrating large language models (LLMs) into academic research and peer review processes, highlighting potential biases when LLMs are employed as reviewers.", "motivation": "To explore the impact of LLMs on scholarly fairness and identify risks associated with their usage as peer reviewers.", "method": "Simulation involving LLMs as both research agents generating/revising papers and review agents assessing submissions, combined with human annotations.", "result": "LLMs show biases by inflating scores for LLM-authored papers and underrating human-authored papers with critical statements, favoring LLM-generated writing styles while disfavoring critical language.", "conclusion": "LLM reviewers could jeopardize academic equity, but their guidance improves paper quality, showing promise for aiding early-stage researchers and low-quality submissions with caution."}}
{"id": "2510.12703", "pdf": "https://arxiv.org/pdf/2510.12703", "abs": "https://arxiv.org/abs/2510.12703", "authors": ["Mattia Grasselli", "Angelo Porrello", "Carlo Augusto Grazia"], "title": "CAMNet: Leveraging Cooperative Awareness Messages for Vehicle Trajectory Prediction", "categories": ["cs.AI", "cs.NI"], "comment": "Accepted at the IEEE Consumer Communications & Networking Conference\n  (CCNC) 2026 - Las Vegas, NV, USA 9 - 12 January 2026", "summary": "Autonomous driving remains a challenging task, particularly due to safety\nconcerns. Modern vehicles are typically equipped with expensive sensors such as\nLiDAR, cameras, and radars to reduce the risk of accidents. However, these\nsensors face inherent limitations: their field of view and line of sight can be\nobstructed by other vehicles, thereby reducing situational awareness. In this\ncontext, vehicle-to-vehicle communication plays a crucial role, as it enables\ncars to share information and remain aware of each other even when sensors are\noccluded. One way to achieve this is through the use of Cooperative Awareness\nMessages (CAMs). In this paper, we investigate the use of CAM data for vehicle\ntrajectory prediction. Specifically, we design and train a neural network,\nCooperative Awareness Message-based Graph Neural Network (CAMNet), on a widely\nused motion forecasting dataset. We then evaluate the model on a second dataset\nthat we created from scratch using Cooperative Awareness Messages, in order to\nassess whether this type of data can be effectively exploited. Our approach\ndemonstrates promising results, showing that CAMs can indeed support vehicle\ntrajectory prediction. At the same time, we discuss several limitations of the\napproach, which highlight opportunities for future research.", "AI": {"tldr": "The paper explores vehicle-to-vehicle communication using Cooperative Awareness Messages (CAMs) to enhance vehicle trajectory prediction, utilizing a graph neural network (CAMNet) and yielding promising results.", "motivation": "Current autonomous driving systems face limitations in situational awareness due to sensor occlusions. This paper aims to assess whether vehicle-to-vehicle communication via CAMs can mitigate such limitations.", "method": "The authors designed and trained a neural network (CAMNet) on a motion forecasting dataset and evaluated it using a custom dataset derived from Cooperative Awareness Messages.", "result": "Results indicate that CAMNet effectively utilizes CAM data for vehicle trajectory prediction and supports the idea that CAMs can enhance situational awareness.", "conclusion": "CAMNet demonstrates the utility of CAM-based communication for trajectory prediction but also reveals limitations that offer areas for future research."}}
{"id": "2510.12362", "pdf": "https://arxiv.org/pdf/2510.12362", "abs": "https://arxiv.org/abs/2510.12362", "authors": ["Jinzhou Lin", "Jie Zhou", "Wenhao Xu", "Rongtao Xu", "Changwei Wang", "Shunpeng Chen", "Kexue Fu", "Yihua Shao", "Li Guo", "Shibiao Xu"], "title": "CurriFlow: Curriculum-Guided Depth Fusion with Optical Flow-Based Temporal Alignment for 3D Semantic Scene Completion", "categories": ["cs.CV"], "comment": null, "summary": "Semantic Scene Completion (SSC) aims to infer complete 3D geometry and\nsemantics from monocular images, serving as a crucial capability for\ncamera-based perception in autonomous driving. However, existing SSC methods\nrelying on temporal stacking or depth projection often lack explicit motion\nreasoning and struggle with occlusions and noisy depth supervision. We propose\nCurriFlow, a novel semantic occupancy prediction framework that integrates\noptical flow-based temporal alignment with curriculum-guided depth fusion.\nCurriFlow employs a multi-level fusion strategy to align segmentation, visual,\nand depth features across frames using pre-trained optical flow, thereby\nimproving temporal consistency and dynamic object understanding. To enhance\ngeometric robustness, a curriculum learning mechanism progressively transitions\nfrom sparse yet accurate LiDAR depth to dense but noisy stereo depth during\ntraining, ensuring stable optimization and seamless adaptation to real-world\ndeployment. Furthermore, semantic priors from the Segment Anything Model (SAM)\nprovide category-agnostic supervision, strengthening voxel-level semantic\nlearning and spatial consistency. Experiments on the SemanticKITTI benchmark\ndemonstrate that CurriFlow achieves state-of-the-art performance with a mean\nIoU of 16.9, validating the effectiveness of our motion-guided and\ncurriculum-aware design for camera-based 3D semantic scene completion.", "AI": {"tldr": "The paper introduces CurriFlow, a framework enhancing semantic scene completion by integrating optical flow-based alignment and curriculum-guided depth fusion.", "motivation": "Semantic Scene Completion (SSC) from monocular images is crucial for autonomous driving, but existing methods struggle with motion reasoning, occlusions, and noisy depth data.", "method": "CurriFlow employs optical flow for temporal alignment, multi-frame fusion for feature consistency, curriculum learning for depth fusion, and semantic priors from SAM for voxel-level semantic learning.", "result": "CurriFlow achieves state-of-the-art performance with a mean IoU of 16.9 on the SemanticKITTI benchmark, proving its effectiveness in dynamic object understanding and temporal consistency.", "conclusion": "The proposed CurriFlow framework robustly improves camera-based 3D semantic scene completion by addressing motion and depth challenges, with promising deployment adaptability and enhanced performance."}}
{"id": "2510.12233", "pdf": "https://arxiv.org/pdf/2510.12233", "abs": "https://arxiv.org/abs/2510.12233", "authors": ["Bowen Fan", "Zhilin Guo", "Xunkai Li", "Yihan Zhou", "Bing Zhou", "Zhenjun Li", "Rong-Hua Li", "Guoren Wang"], "title": "Unveiling the Vulnerability of Graph-LLMs: An Interpretable Multi-Dimensional Adversarial Attack on TAGs", "categories": ["cs.LG"], "comment": "12 pages, 4 figures", "summary": "Graph Neural Networks (GNNs) have become a pivotal framework for modeling\ngraph-structured data, enabling a wide range of applications from social\nnetwork analysis to molecular chemistry. By integrating large language models\n(LLMs), text-attributed graphs (TAGs) enhance node representations with rich\ntextual semantics, significantly boosting the expressive power of graph-based\nlearning. However, this sophisticated synergy introduces critical\nvulnerabilities, as Graph-LLMs are susceptible to adversarial attacks on both\ntheir structural topology and textual attributes. Although specialized attack\nmethods have been designed for each of these aspects, no work has yet unified\nthem into a comprehensive approach. In this work, we propose the Interpretable\nMulti-Dimensional Graph Attack (IMDGA), a novel human-centric adversarial\nattack framework designed to orchestrate multi-level perturbations across both\ngraph structure and textual features. IMDGA utilizes three tightly integrated\nmodules to craft attacks that balance interpretability and impact, enabling a\ndeeper understanding of Graph-LLM vulnerabilities. Through rigorous theoretical\nanalysis and comprehensive empirical evaluations on diverse datasets and\narchitectures, IMDGA demonstrates superior interpretability, attack\neffectiveness, stealthiness, and robustness compared to existing methods. By\nexposing critical weaknesses in TAG representation learning, this work uncovers\na previously underexplored semantic dimension of vulnerability in Graph-LLMs,\noffering valuable insights for improving their resilience. Our code and\nresources are publicly available at\nhttps://anonymous.4open.science/r/IMDGA-7289.", "AI": {"tldr": "Graph Neural Networks (GNNs) are enhanced by large language models (LLMs) for text-attributed graphs but face vulnerabilities. This paper proposes IMDGA, a human-centric framework addressing adversarial attacks on both graph structure and textual features.", "motivation": "The paper aims to address vulnerabilities in text-attributed graphs (TAGs) enhanced by LLMs, which are prone to adversarial attacks affecting both graph topology and textual attributes.", "method": "IMDGA introduces a multi-dimensional attack framework with three integrated modules that enable interpretability, balance, and effectiveness in adversarial perturbations.", "result": "The framework achieves superior interpretability, stealth, robustness, and attack effectiveness compared to existing methods, tested across diverse graphs, datasets, and architectures.", "conclusion": "The study uncovers new areas of vulnerability in Graph-LLMs and provides insights for resilience improvement, emphasizing the importance of safeguarding text-attributed graphs."}}
{"id": "2510.12389", "pdf": "https://arxiv.org/pdf/2510.12389", "abs": "https://arxiv.org/abs/2510.12389", "authors": ["Hailay Kidu Teklehaymanot", "Wolfgang Nejdl"], "title": "Tokenization Disparities as Infrastructure Bias: How Subword Systems Create Inequities in LLM Access and Efficiency", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.1; H.3.3; F.2.2"], "comment": "6 pages 4 figures", "summary": "Tokenization disparities pose a significant barrier to achieving equitable\naccess to artificial intelligence across linguistically diverse populations.\nThis study conducts a large-scale cross-linguistic evaluation of tokenization\nefficiency in over 200 languages to systematically quantify computational\ninequities in large language models (LLMs). Using a standardized experimental\nframework, we applied consistent preprocessing and normalization protocols,\nfollowed by uniform tokenization through the tiktoken library across all\nlanguage samples. Comprehensive tokenization statistics were collected using\nestablished evaluation metrics, including Tokens Per Sentence (TPS) and\nRelative Tokenization Cost (RTC), benchmarked against English baselines. Our\ncross-linguistic analysis reveals substantial and systematic disparities:\nLatin-script languages consistently exhibit higher tokenization efficiency,\nwhile non-Latin and morphologically complex languages incur significantly\ngreater token inflation, often 3-5 times higher RTC ratios. These\ninefficiencies translate into increased computational costs and reduced\neffective context utilization for underrepresented languages. Overall, the\nfindings highlight structural inequities in current AI systems, where speakers\nof low-resource and non-Latin languages face disproportionate computational\ndisadvantages. Future research should prioritize the development of\nlinguistically informed tokenization strategies and adaptive vocabulary\nconstruction methods that incorporate typological diversity, ensuring more\ninclusive and computationally equitable multilingual AI systems.", "AI": {"tldr": "The study investigates tokenization disparities in over 200 languages, revealing inequities in computational costs for non-Latin and low-resource languages.", "motivation": "To address barriers in equitable AI accessibility across diverse linguistic populations, motivated by tokenization inefficiencies.", "method": "Standardized preprocessing and uniform tokenization using tiktoken library were applied, followed by comparative analysis using established metrics.", "result": "Latin-script languages showed higher tokenization efficiency, while non-Latin/morphologically complex languages had greater token inflation and RTC ratios, causing computational disadvantages.", "conclusion": "Current AI systems have inequities affecting non-Latin and low-resource language speakers. Future strategies should incorporate language diversity for equitable AI systems."}}
{"id": "2510.12713", "pdf": "https://arxiv.org/pdf/2510.12713", "abs": "https://arxiv.org/abs/2510.12713", "authors": ["Wissam Salhab", "Darine Ameyed", "Hamid Mcheick", "Fehmi Jaafar"], "title": "Towards Robust Artificial Intelligence: Self-Supervised Learning Approach for Out-of-Distribution Detection", "categories": ["cs.AI"], "comment": null, "summary": "Robustness in AI systems refers to their ability to maintain reliable and\naccurate performance under various conditions, including out-of-distribution\n(OOD) samples, adversarial attacks, and environmental changes. This is crucial\nin safety-critical systems, such as autonomous vehicles, transportation, or\nhealthcare, where malfunctions could have severe consequences. This paper\nproposes an approach to improve OOD detection without the need of labeled data,\nthereby increasing the AI systems' robustness. The proposed approach leverages\nthe principles of self-supervised learning, allowing the model to learn useful\nrepresentations from unlabeled data. Combined with graph-theoretical\ntechniques, this enables the more efficient identification and categorization\nof OOD samples. Compared to existing state-of-the-art methods, this approach\nachieved an Area Under the Receiver Operating Characteristic Curve (AUROC) =\n0.99.", "AI": {"tldr": "This paper introduces a method to improve out-of-distribution (OOD) detection in AI systems using self-supervised learning and graph-theoretical techniques, achieving an AUROC of 0.99.", "motivation": "Robustness in AI systems is essential for safety-critical applications like autonomous vehicles and healthcare, where failure to handle OOD samples can result in severe consequences.", "method": "The approach uses self-supervised learning to extract representations from unlabeled data and combines it with graph-theoretical techniques for efficient detection and classification of OOD samples.", "result": "The proposed method significantly improves OOD detection efficiency, demonstrated by achieving a high AUROC score of 0.99.", "conclusion": "Enhancing robustness through improved OOD detection without labeled data makes AI systems safer and more reliable, particularly for critical fields and applications."}}
{"id": "2510.12376", "pdf": "https://arxiv.org/pdf/2510.12376", "abs": "https://arxiv.org/abs/2510.12376", "authors": ["Sharath M Shankaranarayana", "Soumava Kumar Roy", "Prasad Sudhakar", "Chandan Aladahalli"], "title": "Deep Attention-guided Adaptive Subsampling", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Although deep neural networks have provided impressive gains in performance,\nthese improvements often come at the cost of increased computational complexity\nand expense. In many cases, such as 3D volume or video classification tasks,\nnot all slices or frames are necessary due to inherent redundancies. To address\nthis issue, we propose a novel learnable subsampling framework that can be\nintegrated into any neural network architecture. Subsampling, being a\nnondifferentiable operation, poses significant challenges for direct adaptation\ninto deep learning models. While some works, have proposed solutions using the\nGumbel-max trick to overcome the problem of non-differentiability, they fall\nshort in a crucial aspect: they are only task-adaptive and not inputadaptive.\nOnce the sampling mechanism is learned, it remains static and does not adjust\nto different inputs, making it unsuitable for real-world applications. To this\nend, we propose an attention-guided sampling module that adapts to inputs even\nduring inference. This dynamic adaptation results in performance gains and\nreduces complexity in deep neural network models. We demonstrate the\neffectiveness of our method on 3D medical imaging datasets from MedMNIST3D as\nwell as two ultrasound video datasets for classification tasks, one of them\nbeing a challenging in-house dataset collected under real-world clinical\nconditions.", "AI": {"tldr": "The paper introduces a novel framework for adaptive subsampling in deep neural networks that dynamically adjusts input slices or frames during inference to reduce computational complexity and redundancy.", "motivation": "Deep neural networks improve tasks like video classification, but their computational cost is significant due to processing redundant information such as unnecessary slices or frames.", "method": "An attention-guided sampling module that operates dynamically during inference, overcoming the non-differentiability of traditional subsampling techniques and adapting sampling mechanisms to the input.", "result": "The method demonstrated performance gains and computational savings on 3D medical imaging datasets and real-world ultrasound video datasets, including a challenging clinical dataset.", "conclusion": "Dynamic input-adaptive subsampling modules can enhance deep learning model efficiency and simplify computational demands while maintaining or improving task performance."}}
{"id": "2510.12245", "pdf": "https://arxiv.org/pdf/2510.12245", "abs": "https://arxiv.org/abs/2510.12245", "authors": ["Tao Yin", "Xiaohong Zhang", "Jiacheng Zhang", "Li Huang", "Zhibin Zhang", "Yuansong Zeng", "Jin Xie", "Meng Yan"], "title": "MoRA: On-the-fly Molecule-aware Low-Rank Adaptation Framework for LLM-based Multi-Modal Molecular Assistant", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Effectively integrating molecular graph structures with Large Language Models\n(LLMs) is a key challenge in drug discovery. Most existing multi-modal\nalignment methods typically process these structures by fine-tuning the LLM or\nadding a static adapter simultaneously. However, these approaches have two main\nlimitations: (1) it optimizes a shared parameter space across all molecular\ninputs, limiting the model's ability to capture instance-specific structural\nfeatures; and (2) fine-tuning the LLM for molecular tasks can lead to\ncatastrophic forgetting, undermining its general reasoning capabilities. In\nthis paper, instead of static task-oriented adaptation, we propose an\ninstance-specific parameter space alignment approach for each molecule\non-the-fly. To this end, we introduce Molecule-aware Low-Rank Adaptation (MoRA)\nthat produces a unique set of low-rank adaptation weights for each input\nmolecular graph. These weights are then dynamically injected into a frozen LLM,\nallowing the model to adapt its reasoning to the structure of each molecular\ninput, while preserving the LLM's core knowledge. Extensive experiments\ndemonstrate that on key molecular tasks, such as chemical reaction prediction\nand molecular captioning, MoRA's instance-specific dynamic adaptation\noutperforms statically adapted baselines, including a 14.1% relative\nimprovement in reaction prediction exact match and a 22% reduction in error for\nquantum property prediction. The code is available at\nhttps://github.com/jk-sounds/MoRA.", "AI": {"tldr": "This paper proposes MoRA, a dynamic method for adapting frozen Large Language Models (LLMs) to molecular graph tasks by generating instance-specific low-rank adaptation weights, yielding superior performance without compromising general reasoning.", "motivation": "The paper aims to address limitations in existing molecular graph-LLM integration methods, which struggle with capturing instance-specific features and risk degrading the LLM's general reasoning abilities through static adaptation or fine-tuning.", "method": "MoRA generates unique, low-rank adaptation weights tailored to each input molecular graph. These weights are dynamically integrated into a frozen LLM, enabling task-specific reasoning while preserving the model's general knowledge.", "result": "MoRA achieves significant performance improvements in molecular tasks, such as a 14.1% relative gain in reaction prediction accuracy and a 22% error reduction in quantum property predictions, outperforming static adaptation baselines.", "conclusion": "The MoRA approach successfully enhances molecular graph processing in LLMs by enabling dynamic and instance-specific adaptation, maintaining the LLM's core capabilities and improving task-specific outcomes."}}
{"id": "2510.12434", "pdf": "https://arxiv.org/pdf/2510.12434", "abs": "https://arxiv.org/abs/2510.12434", "authors": ["Xiangjun Zai", "Xingyu Tan", "Xiaoyang Wang", "Qing Liu", "Xiwei Xu", "Wenjie Zhang"], "title": "PRoH: Dynamic Planning and Reasoning over Knowledge Hypergraphs for Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Knowledge Hypergraphs (KHs) have recently emerged as a knowledge\nrepresentation for retrieval-augmented generation (RAG), offering a paradigm to\nmodel multi-entity relations into a structured form. However, existing KH-based\nRAG methods suffer from three major limitations: static retrieval planning,\nnon-adaptive retrieval execution, and superficial use of KH structure and\nsemantics, which constrain their ability to perform effective multi-hop\nquestion answering. To overcome these limitations, we propose PRoH, a dynamic\nPlanning and Reasoning over Knowledge Hypergraphs framework. PRoH incorporates\nthree core innovations: (i) a context-aware planning module that sketches the\nlocal KH neighborhood to guide structurally grounded reasoning plan generation;\n(ii) a structured question decomposition process that organizes subquestions as\na dynamically evolving Directed Acyclic Graph (DAG) to enable adaptive,\nmulti-trajectory exploration; and (iii) an Entity-Weighted Overlap (EWO)-guided\nreasoning path retrieval algorithm that prioritizes semantically coherent\nhyperedge traversals. Experiments across multiple domains demonstrate that PRoH\nachieves state-of-the-art performance, surpassing the prior SOTA model\nHyperGraphRAG by an average of 19.73% in F1 and 8.41% in Generation Evaluation\n(G-E) score, while maintaining strong robustness in long-range multi-hop\nreasoning tasks.", "AI": {"tldr": "PRoH is a new framework designed to overcome limitations of existing KH-based RAG methods by integrating dynamic planning and reasoning mechanisms for better multi-hop question answering.", "motivation": "Existing KH-based RAG methods are limited by static retrieval planning, non-adaptive execution, and superficial KH utilization, hindering effective multi-hop reasoning.", "method": "PRoH introduces context-aware planning for local KH neighborhood analysis, adaptive question decomposition using a dynamically evolving DAG, and an EWO-guided retrieval algorithm for semantically coherent reasoning paths.", "result": "Experiments showed PRoH surpasses HyperGraphRAG by 19.73% in F1 and 8.41% in Generation Evaluation score, proving its effectiveness and robustness for multi-hop reasoning across domains.", "conclusion": "PRoH provides a significant advancement in RAG tasks by leveraging dynamic KH planning and reasoning strategies, improving accuracy and robustness in multi-hop question answering."}}
{"id": "2510.12732", "pdf": "https://arxiv.org/pdf/2510.12732", "abs": "https://arxiv.org/abs/2510.12732", "authors": ["Myles Foley", "Sergio Maffeis", "Muhammad Fakhrur Rozi", "Takeshi Takahashi"], "title": "Clutch Control: An Attention-based Combinatorial Bandit for Efficient Mutation in JavaScript Engine Fuzzing", "categories": ["cs.AI"], "comment": null, "summary": "JavaScript engines are widely used in web browsers, PDF readers, and\nserver-side applications. The rise in concern over their security has led to\nthe development of several targeted fuzzing techniques. However, existing\napproaches use random selection to determine where to perform mutations in\nJavaScript code. We postulate that the problem of selecting better mutation\ntargets is suitable for combinatorial bandits with a volatile number of arms.\nThus, we propose CLUTCH, a novel deep combinatorial bandit that can observe\nvariable length JavaScript test case representations, using an attention\nmechanism from deep learning. Furthermore, using Concrete Dropout, CLUTCH can\ndynamically adapt its exploration. We show that CLUTCH increases efficiency in\nJavaScript fuzzing compared to three state-of-the-art solutions by increasing\nthe number of valid test cases and coverage-per-testcase by, respectively,\n20.3% and 8.9% on average. In volatile and combinatorial settings we show that\nCLUTCH outperforms state-of-the-art bandits, achieving at least 78.1% and 4.1%\nless regret in volatile and combinatorial settings, respectively.", "AI": {"tldr": "This paper introduces CLUTCH, a deep combinatorial bandit method for improved JavaScript fuzzing, enhancing test case validity and coverage compared to existing methods.", "motivation": "JavaScript engine security concerns have driven the need for better targeted fuzzing techniques that optimize mutation targeting in test cases.", "method": "The proposed method, CLUTCH, integrates deep combinatorial bandits with attention mechanisms and Concrete Dropout to improve mutation targeting in JavaScript test cases.", "result": "CLUTCH demonstrates increased efficiency in JavaScript fuzzing, with 20.3% more valid test cases and 8.9% better coverage-per-testcase, outperforming state-of-the-art bandit methods in volatile and combinatorial settings.", "conclusion": "CLUTCH is a significant advancement for JavaScript fuzzing, providing higher efficiency and lower regret in challenging mutation targeting scenarios."}}
{"id": "2510.12385", "pdf": "https://arxiv.org/pdf/2510.12385", "abs": "https://arxiv.org/abs/2510.12385", "authors": ["Tim J. Schoonbeek", "Shao-Hsuan Hung", "Dan Lehman", "Hans Onvlee", "Jacek Kustra", "Peter H. N. de With", "Fons van der Sommen"], "title": "Learning to Recognize Correctly Completed Procedure Steps in Egocentric Assembly Videos through Spatio-Temporal Modeling", "categories": ["cs.CV"], "comment": "26 pages, 7 figures and 5 tables in the main paper and one figure and\n  table in the appendix. To be published in Computer Vision and Image\n  Understanding", "summary": "Procedure step recognition (PSR) aims to identify all correctly completed\nsteps and their sequential order in videos of procedural tasks. The existing\nstate-of-the-art models rely solely on detecting assembly object states in\nindividual video frames. By neglecting temporal features, model robustness and\naccuracy are limited, especially when objects are partially occluded. To\novercome these limitations, we propose Spatio-Temporal Occlusion-Resilient\nModeling for Procedure Step Recognition (STORM-PSR), a dual-stream framework\nfor PSR that leverages both spatial and temporal features. The assembly state\ndetection stream operates effectively with unobstructed views of the object,\nwhile the spatio-temporal stream captures both spatial and temporal features to\nrecognize step completions even under partial occlusion. This stream includes a\nspatial encoder, pre-trained using a novel weakly supervised approach to\ncapture meaningful spatial representations, and a transformer-based temporal\nencoder that learns how these spatial features relate over time. STORM-PSR is\nevaluated on the MECCANO and IndustReal datasets, reducing the average delay\nbetween actual and predicted assembly step completions by 11.2% and 26.1%,\nrespectively, compared to prior methods. We demonstrate that this reduction in\ndelay is driven by the spatio-temporal stream, which does not rely on\nunobstructed views of the object to infer completed steps. The code for\nSTORM-PSR, along with the newly annotated MECCANO labels, is made publicly\navailable at https://timschoonbeek.github.io/stormpsr .", "AI": {"tldr": "The paper introduces STORM-PSR, a dual-stream model for recognizing procedural steps in videos by leveraging both spatial and temporal features to improve resilience against occlusions.", "motivation": "Existing models for procedure step recognition rely only on spatial features and fail when objects are partially occluded, reducing accuracy and robustness.", "method": "STORM-PSR consists of a dual-stream design with an assembly state detection stream for unobstructed views and a spatio-temporal stream with a pre-trained spatial encoder and a transformer-based temporal encoder.", "result": "Compared to prior methods, STORM-PSR reduces the delay between actual and predicted assembly steps by 11.2% on MECCANO and 26.1% on IndustReal datasets.", "conclusion": "STORM-PSR effectively improves recognition robustness even under object occlusion and makes its model and dataset contribution publicly available."}}
{"id": "2510.12249", "pdf": "https://arxiv.org/pdf/2510.12249", "abs": "https://arxiv.org/abs/2510.12249", "authors": ["Edwige Cyffers", "Alireza Mirrokni", "Marco Mondelli"], "title": "Optimal Regularization for Performative Learning", "categories": ["cs.LG"], "comment": null, "summary": "In performative learning, the data distribution reacts to the deployed model\n- for example, because strategic users adapt their features to game it - which\ncreates a more complex dynamic than in classical supervised learning. One\nshould thus not only optimize the model for the current data but also take into\naccount that the model might steer the distribution in a new direction, without\nknowing the exact nature of the potential shift. We explore how regularization\ncan help cope with performative effects by studying its impact in\nhigh-dimensional ridge regression. We show that, while performative effects\nworsen the test risk in the population setting, they can be beneficial in the\nover-parameterized regime where the number of features exceeds the number of\nsamples. We show that the optimal regularization scales with the overall\nstrength of the performative effect, making it possible to set the\nregularization in anticipation of this effect. We illustrate this finding\nthrough empirical evaluations of the optimal regularization parameter on both\nsynthetic and real-world datasets.", "AI": {"tldr": "The paper explores how regularization can address performative effects in machine learning, particularly in ridge regression.", "motivation": "The study aims to optimize models while accounting for potential shifts in data distribution caused by model deployment.", "method": "The researchers analyze high-dimensional ridge regression under performative effects, identifying optimal regularization scaling.", "result": "Performative effects may worsen test risk overall but can be advantageous in over-parameterized settings with strategic regularization.", "conclusion": "Anticipating performative effects can help set optimal regularization, improving model performance under such conditions."}}
{"id": "2510.12460", "pdf": "https://arxiv.org/pdf/2510.12460", "abs": "https://arxiv.org/abs/2510.12460", "authors": ["Linfeng Gao", "Baolong Bi", "Zheng Yuan", "Le Wang", "Zerui Chen", "Zhimin Wei", "Shenghua Liu", "Qinggang Zhang", "Jinsong Su"], "title": "Probing Latent Knowledge Conflict for Faithful Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to\nenhance the factuality of Large Language Models (LLMs). However, existing RAG\nsystems often suffer from an unfaithfulness issue, where the model's response\ncontradicts evidence from the retrieved context. Existing approaches to\nimproving contextual faithfulness largely rely on external interventions, such\nas prompt engineering, decoding constraints, or reward-based fine-tuning. These\nworks treat the LLM as a black box and overlook a crucial question: how does\nthe LLM internally integrate retrieved evidence with its parametric memory,\nparticularly under knowledge conflicts? To address this gap, we conduct a\nprobing-based analysis of hidden-state representations in LLMs and observe\nthree findings: knowledge integration occurs hierarchically, conflicts manifest\nas latent signals at the sentence level, and irrelevant context is often\namplified when aligned with parametric knowledge. Building on these findings,\nwe propose CLEAR (Conflict-Localized and Enhanced Attention for RAG), a\nframework that (i) decomposes context into fine-grained sentence-level\nknowledge, (ii) employs hidden-state probing to localize conflicting knowledge,\nand (iii) introduces conflict-aware fine-tuning to guide the model to\naccurately integrate retrieved evidence. Extensive experiments across three\nbenchmarks demonstrate that CLEAR substantially improves both accuracy and\ncontextual faithfulness, consistently outperforming strong baselines under\ndiverse conflict conditions. The related resources are available at\nhttps://github.com/LinfengGao/CLEAR.", "AI": {"tldr": "This paper addresses contextual faithfulness issues in Retrieval-Augmented Generation (RAG), proposing CLEAR, a framework emphasizing sentence-level context decomposition, conflict localization, and fine-tuning.", "motivation": "Existing RAG systems often produce responses that contradict retrieved evidence, and current solutions treat LLMs as black boxes without examining how they internally integrate retrieved evidence with parametric memory.", "method": "The paper conducts probing-based analysis of LLMs' hidden states to understand knowledge integration and conflict manifestation. Based on findings, CLEAR applies sentence-level decomposition, conflict localization, and conflict-aware fine-tuning.", "result": "CLEAR significantly improves accuracy and contextual faithfulness across different benchmarks, outperforming strong baselines in various conflict scenarios.", "conclusion": "The CLEAR framework advances RAG systems by enhancing evidence integration through a deeper understanding and targeted fine-tuning, establishing a new standard for accuracy and faithfulness in LLM responses."}}
{"id": "2510.12742", "pdf": "https://arxiv.org/pdf/2510.12742", "abs": "https://arxiv.org/abs/2510.12742", "authors": ["Micah Carroll", "Adeline Foote", "Kevin Feng", "Marcus Williams", "Anca Dragan", "W. Bradley Knox", "Smitha Milli"], "title": "CTRL-Rec: Controlling Recommender Systems With Natural Language", "categories": ["cs.AI", "cs.IR"], "comment": null, "summary": "When users are dissatisfied with recommendations from a recommender system,\nthey often lack fine-grained controls for changing them. Large language models\n(LLMs) offer a solution by allowing users to guide their recommendations\nthrough natural language requests (e.g., \"I want to see respectful posts with a\ndifferent perspective than mine\"). We propose a method, CTRL-Rec, that allows\nfor natural language control of traditional recommender systems in real-time\nwith computational efficiency. Specifically, at training time, we use an LLM to\nsimulate whether users would approve of items based on their language requests,\nand we train embedding models that approximate such simulated judgments. We\nthen integrate these user-request-based predictions into the standard weighting\nof signals that traditional recommender systems optimize. At deployment time,\nwe require only a single LLM embedding computation per user request, allowing\nfor real-time control of recommendations. In experiments with the MovieLens\ndataset, our method consistently allows for fine-grained control across a\ndiversity of requests. In a study with 19 Letterboxd users, we find that\nCTRL-Rec was positively received by users and significantly enhanced users'\nsense of control and satisfaction with recommendations compared to traditional\ncontrols.", "AI": {"tldr": "CTRL-Rec integrates LLMs with traditional recommendation systems, enabling users to adjust suggestions via natural language requests, providing real-time control and satisfaction.", "motivation": "Users often find existing recommendation systems restrictive and lack options for precise customization, motivating the need for more personalized control mechanisms.", "method": "CTRL-Rec trains embedding models based on LLM-simulated user judgments from natural language requests, integrates these predictions into traditional systems, and ensures computational efficiency for real-time adjustments.", "result": "CTRL-Rec successfully demonstrated its effectiveness in experiments with the MovieLens dataset and was well-received by Letterboxd users who appreciated the enhanced control and satisfaction it provided.", "conclusion": "The proposed method offers a practical and efficient approach to incorporating user-driven customization in recommender systems, greatly improving user experience and control."}}
{"id": "2510.12387", "pdf": "https://arxiv.org/pdf/2510.12387", "abs": "https://arxiv.org/abs/2510.12387", "authors": ["Wenjing Bian", "Axel Barroso-Laguna", "Tommaso Cavallari", "Victor Adrian Prisacariu", "Eric Brachmann"], "title": "Scene Coordinate Reconstruction Priors", "categories": ["cs.CV"], "comment": "ICCV 2025, Project page: https://nianticspatial.github.io/scr-priors/", "summary": "Scene coordinate regression (SCR) models have proven to be powerful implicit\nscene representations for 3D vision, enabling visual relocalization and\nstructure-from-motion. SCR models are trained specifically for one scene. If\ntraining images imply insufficient multi-view constraints SCR models\ndegenerate. We present a probabilistic reinterpretation of training SCR models,\nwhich allows us to infuse high-level reconstruction priors. We investigate\nmultiple such priors, ranging from simple priors over the distribution of\nreconstructed depth values to learned priors over plausible scene coordinate\nconfigurations. For the latter, we train a 3D point cloud diffusion model on a\nlarge corpus of indoor scans. Our priors push predicted 3D scene points towards\nplausible geometry at each training step to increase their likelihood. On three\nindoor datasets our priors help learning better scene representations,\nresulting in more coherent scene point clouds, higher registration rates and\nbetter camera poses, with a positive effect on down-stream tasks such as novel\nview synthesis and camera relocalization.", "AI": {"tldr": "The paper enhances scene coordinate regression (SCR) models using probabilistic interpretations and high-level reconstruction priors, achieving better 3D scene representations for tasks like camera relocalization and novel view synthesis.", "motivation": "Scene coordinate regression (SCR) models are powerful yet tend to degrade when facing insufficient multi-view constraints, necessitating improved mechanisms for robust 3D vision.", "method": "The authors propose a probabilistic reinterpretation of SCR models infused with high-level priors, including those learned from 3D point cloud diffusion models trained on extensive indoor scans.", "result": "The approach produces more coherent 3D scene point clouds, improves registration rates and camera pose estimation, positively impacting tasks like novel view synthesis and relocalization.", "conclusion": "Introducing reconstruction priors into SCR training improves scene representational quality and benefits downstream 3D vision tasks, demonstrating its efficacy on multiple indoor datasets."}}
{"id": "2510.12253", "pdf": "https://arxiv.org/pdf/2510.12253", "abs": "https://arxiv.org/abs/2510.12253", "authors": ["Changfu Xu", "Jianxiong Guo", "Yuzhu Liang", "Haiyang Huang", "Haodong Zou", "Xi Zheng", "Shui Yu", "Xiaowen Chu", "Jiannong Cao", "Tian Wang"], "title": "Diffusion Models for Reinforcement Learning: Foundations, Taxonomy, and Development", "categories": ["cs.LG", "cs.AI"], "comment": "Under Review", "summary": "Diffusion Models (DMs), as a leading class of generative models, offer key\nadvantages for reinforcement learning (RL), including multi-modal\nexpressiveness, stable training, and trajectory-level planning. This survey\ndelivers a comprehensive and up-to-date synthesis of diffusion-based RL. We\nfirst provide an overview of RL, highlighting its challenges, and then\nintroduce the fundamental concepts of DMs, investigating how they are\nintegrated into RL frameworks to address key challenges in this research field.\nWe establish a dual-axis taxonomy that organizes the field along two orthogonal\ndimensions: a function-oriented taxonomy that clarifies the roles DMs play\nwithin the RL pipeline, and a technique-oriented taxonomy that situates\nimplementations across online versus offline learning regimes. We also provide\na comprehensive examination of this progression from single-agent to\nmulti-agent domains, thereby forming several frameworks for DM-RL integration\nand highlighting their practical utility. Furthermore, we outline several\ncategories of successful applications of diffusion-based RL across diverse\ndomains, discuss open research issues of current methodologies, and highlight\nkey directions for future research to advance the field. Finally, we summarize\nthe survey to identify promising future development directions. We are actively\nmaintaining a GitHub repository (https://github.com/ChangfuXu/D4RL-FTD) for\npapers and other related resources to apply DMs for RL.", "AI": {"tldr": "This survey examines the integration of Diffusion Models (DMs) into reinforcement learning (RL), creating a taxonomy for their roles and techniques and summarizing applications, issues, and future directions.", "motivation": "To address key challenges in reinforcement learning (RL) by leveraging the advantages of Diffusion Models (DMs) such as multi-modal expressiveness, stable training, and trajectory-level planning.", "method": "Introduced a dual-axis taxonomy to organize the use of DMs in RL, based on their functions in RL pipelines and their deployment in online and offline learning regimes, along with progression from single-agent to multi-agent domains.", "result": "Provided frameworks for DM-RL integration, highlighted diverse applications, analyzed current limitations, and outlined future research directions.", "conclusion": "Diffusion Models demonstrate significant potential in overcoming RL challenges, offer practical frameworks and applications, and warrant further exploration and development in this field."}}
{"id": "2510.12463", "pdf": "https://arxiv.org/pdf/2510.12463", "abs": "https://arxiv.org/abs/2510.12463", "authors": ["Nikoleta Pantelidou", "Evelina Leivada", "Paolo Morosi"], "title": "Resource-sensitive but language-blind: Community size and not grammatical complexity better predicts the accuracy of Large Language Models in a novel Wug Test", "categories": ["cs.CL"], "comment": null, "summary": "The linguistic abilities of Large Language Models are a matter of ongoing\ndebate. This study contributes to this discussion by investigating model\nperformance in a morphological generalization task that involves novel words.\nUsing a multilingual adaptation of the Wug Test, six models were tested across\nfour partially unrelated languages (Catalan, English, Greek, and Spanish) and\ncompared with human speakers. The aim is to determine whether model accuracy\napproximates human competence and whether it is shaped primarily by linguistic\ncomplexity or by the quantity of available training data. Consistent with\nprevious research, the results show that the models are able to generalize\nmorphological processes to unseen words with human-like accuracy. However,\naccuracy patterns align more closely with community size and data availability\nthan with structural complexity, refining earlier claims in the literature. In\nparticular, languages with larger speaker communities and stronger digital\nrepresentation, such as Spanish and English, revealed higher accuracy than\nless-resourced ones like Catalan and Greek. Overall, our findings suggest that\nmodel behavior is mainly driven by the richness of linguistic resources rather\nthan by sensitivity to grammatical complexity, reflecting a form of performance\nthat resembles human linguistic competence only superficially.", "AI": {"tldr": "The study assesses Large Language Models (LLMs) for morphological generalization in Catalan, English, Greek, and Spanish using an adaptation of the Wug Test. Results show LLMs generalize morphological processes like humans but are influenced more by language resource availability than complexity.", "motivation": "To investigate whether LLMs replicate human linguistic competence and understand the factors affecting model accuracy in morphological generalization across various languages.", "method": "Six LLMs were tested in a multilingual morphological generalization task using novel words from four languages. Results were compared with human speakers and analyzed for correlation with linguistic complexity and training data availability.", "result": "LLMs displayed high accuracy in generalizing morphological processes to unseen words, outperforming in resource-rich languages (English and Spanish) compared to resource-scarce ones (Catalan and Greek). Accuracy patterns correlated more with training data amount than linguistic complexity.", "conclusion": "Model behavior reflects a shallow resemblance to human linguistic competence, largely driven by the availability of linguistic resources, rather than an intrinsic sensitivity to grammatical complexity."}}
{"id": "2510.12787", "pdf": "https://arxiv.org/pdf/2510.12787", "abs": "https://arxiv.org/abs/2510.12787", "authors": ["Marco Del Tredici", "Jacob McCarran", "Benjamin Breen", "Javier Aspuru Mijares", "Weichen Winston Yin", "Jacob M. Taylor", "Frank Koppens", "Dirk Englund"], "title": "Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in Mathematics and Quantum Physics", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "We present Ax-Prover, a multi-agent system for automated theorem proving in\nLean that can solve problems across diverse scientific domains and operate\neither autonomously or collaboratively with human experts. To achieve this,\nAx-Prover approaches scientific problem solving through formal proof\ngeneration, a process that demands both creative reasoning and strict syntactic\nrigor. Ax-Prover meets this challenge by equipping Large Language Models\n(LLMs), which provide knowledge and reasoning, with Lean tools via the Model\nContext Protocol (MCP), which ensure formal correctness. To evaluate its\nperformance as an autonomous prover, we benchmark our approach against frontier\nLLMs and specialized prover models on two public math benchmarks and on two\nLean benchmarks we introduce in the fields of abstract algebra and quantum\ntheory. On public datasets, Ax-Prover is competitive with state-of-the-art\nprovers, while it largely outperform them on the new benchmarks. This shows\nthat, unlike specialized systems that struggle to generalize, our tool-based\nagentic theorem prover approach offers a generalizable methodology for formal\nverification across diverse scientific domains. Furthermore, we demonstrate\nAx-Prover's assistant capabilities in a practical use case, showing how it\nenabled an expert mathematician to formalize the proof of a complex\ncryptography theorem.", "AI": {"tldr": "The paper introduces Ax-Prover, a system built around Large Language Models (LLMs) integrated with Lean tools via the Model Context Protocol (MCP), excelling in automated theorem proving across various domains.", "motivation": "Automated theorem proving requires systems capable of handling the creativity of reasoning while ensuring syntactic rigor, aiming to assist experts and ensure correctness across diverse scientific domains.", "method": "Ax-Prover integrates LLMs with Lean tools via the Model Context Protocol to generate formal proofs. Evaluations include public math benchmarks and introduced Lean benchmarks in abstract algebra and quantum theory.", "result": "Ax-Prover competes effectively with state-of-the-art systems on existing benchmarks and significantly outperforms them on new scientific benchmarks. It also demonstrates practical application by helping an expert formalize a complex cryptography theorem.", "conclusion": "Ax-Prover offers a generalized and robust methodology for formal verification, functioning autonomously or collaboratively, with promising results across mathematical and scientific domains."}}
{"id": "2510.12400", "pdf": "https://arxiv.org/pdf/2510.12400", "abs": "https://arxiv.org/abs/2510.12400", "authors": ["Andr\u00e9 Torneiro", "Diogo Monteiro", "Paulo Novais", "Pedro Rangel Henriques", "Nuno F. Rodrigues"], "title": "Towards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agenda", "categories": ["cs.CV"], "comment": "44 pages", "summary": "Urban monitoring of public infrastructure (such as waste bins, road signs,\nvegetation, sidewalks, and construction sites) poses significant challenges due\nto the diversity of objects, environments, and contextual conditions involved.\nCurrent state-of-the-art approaches typically rely on a combination of IoT\nsensors and manual inspections, which are costly, difficult to scale, and often\nmisaligned with citizens' perception formed through direct visual observation.\nThis raises a critical question: Can machines now \"see\" like citizens and infer\ninformed opinions about the condition of urban infrastructure? Vision-Language\nModels (VLMs), which integrate visual understanding with natural language\nreasoning, have recently demonstrated impressive capabilities in processing\ncomplex visual information, turning them into a promising technology to address\nthis challenge. This systematic review investigates the role of VLMs in urban\nmonitoring, with particular emphasis on zero-shot applications. Following the\nPRISMA methodology, we analyzed 32 peer-reviewed studies published between 2021\nand 2025 to address four core research questions: (1) What urban monitoring\ntasks have been effectively addressed using VLMs? (2) Which VLM architectures\nand frameworks are most commonly used and demonstrate superior performance? (3)\nWhat datasets and resources support this emerging field? (4) How are VLM-based\napplications evaluated, and what performance levels have been reported?", "AI": {"tldr": "This paper systematically reviews the role of Vision-Language Models (VLMs) in urban infrastructure monitoring, exploring applications, architectures, datasets, and evaluation methods in the field.", "motivation": "Urban infrastructure monitoring faces challenges due to object diversity, environmental factors, and reliance on costly sensor-based and manual methods. The study seeks to explore if VLMs, with their ability to integrate visual and language understanding, can address these issues effectively.", "method": "The researchers analyzed 32 peer-reviewed studies from 2021 to 2025, following the PRISMA methodology. Key areas explored include urban monitoring tasks addressed by VLMs, commonly used architectures, supporting datasets, and evaluation methods.", "result": "The paper identifies the effectiveness of VLMs in various urban monitoring tasks, highlights specific VLM architectures and frameworks demonstrating superior performance, and reviews datasets and evaluation metrics applied in the field.", "conclusion": "Vision-Language Models hold significant promise for urban infrastructure monitoring by providing scalable and perceptually aligned approaches. The study consolidates key findings and establishes groundwork for future research in this domain."}}
{"id": "2510.12254", "pdf": "https://arxiv.org/pdf/2510.12254", "abs": "https://arxiv.org/abs/2510.12254", "authors": ["Ningxin He", "Yang Liu", "Wei Sun", "Xiaozhou Ye", "Ye Ouyang", "Tiegang Gao", "Zehui Zhang"], "title": "FedMMKT:Co-Enhancing a Server Text-to-Image Model and Client Task Models in Multi-Modal Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "Text-to-Image (T2I) models have demonstrated their versatility in a wide\nrange of applications. However, adaptation of T2I models to specialized tasks\nis often limited by the availability of task-specific data due to privacy\nconcerns. On the other hand, harnessing the power of rich multimodal data from\nmodern mobile systems and IoT infrastructures presents a great opportunity.\nThis paper introduces Federated Multi-modal Knowledge Transfer (FedMMKT), a\nnovel framework that enables co-enhancement of a server T2I model and client\ntask-specific models using decentralized multimodal data without compromising\ndata privacy.", "AI": {"tldr": "This paper introduces FedMMKT, a framework for enhancing T2I models and client-specific models without compromising privacy.", "motivation": "Existing T2I models face challenges adapting to specific tasks due to limited task-specific data and privacy concerns.", "method": "The authors propose FedMMKT, a decentralized framework leveraging multimodal data without sharing sensitive information.", "result": "FedMMKT ensures co-enhancement of server T2I models and task-specific client models while preserving data privacy.", "conclusion": "FedMMKT provides a privacy-preserving solution for adapting T2I models to specialized tasks using decentralized multimodal data."}}
{"id": "2510.12474", "pdf": "https://arxiv.org/pdf/2510.12474", "abs": "https://arxiv.org/abs/2510.12474", "authors": ["Biao Zhang", "Lixin Chen", "Tong Liu", "Bo Zheng"], "title": "SMEC: Rethinking Matryoshka Representation Learning for Retrieval Embedding Compression", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted by EMNLP2025", "summary": "Large language models (LLMs) generate high-dimensional embeddings that\ncapture rich semantic and syntactic information. However, high-dimensional\nembeddings exacerbate computational complexity and storage requirements,\nthereby hindering practical deployment. To address these challenges, we propose\na novel training framework named Sequential Matryoshka Embedding Compression\n(SMEC). This framework introduces the Sequential Matryoshka Representation\nLearning(SMRL) method to mitigate gradient variance during training, the\nAdaptive Dimension Selection (ADS) module to reduce information degradation\nduring dimension pruning, and the Selectable Cross-batch Memory (S-XBM) module\nto enhance unsupervised learning between high- and low-dimensional embeddings.\nExperiments on image, text, and multimodal datasets demonstrate that SMEC\nachieves significant dimensionality reduction while maintaining performance.\nFor instance, on the BEIR dataset, our approach improves the performance of\ncompressed LLM2Vec embeddings (256 dimensions) by 1.1 points and 2.7 points\ncompared to the Matryoshka-Adaptor and Search-Adaptor models, respectively.", "AI": {"tldr": "The paper introduces SMEC, a novel framework for compressing large language model embeddings, offering significant dimensionality reduction while maintaining performance.", "motivation": "High-dimensional embeddings from large language models lead to computational complexity and storage issues, requiring an effective compression method.", "method": "The paper proposes SMEC, incorporating SMRL for reduced gradient variance, ADS for minimal information loss during pruning, and S-XBM for enhancing unsupervised learning between varying dimensions.", "result": "SMEC shows effective dimensionality reduction with maintained performance across image, text, and multimodal data, outperforming existing compression models on benchmarks like the BEIR dataset.", "conclusion": "SMEC demonstrates its capability as a robust framework for embedding compression, balancing compactness with high performance in various applications."}}
{"id": "2503.20934", "pdf": "https://arxiv.org/pdf/2503.20934", "abs": "https://arxiv.org/abs/2503.20934", "authors": ["Fraol Batole", "Abhiram Bellur", "Malinda Dilhara", "Mohammed Raihan Ullah", "Yaroslav Zharov", "Timofey Bryksin", "Kai Ishikawa", "Haifeng Chen", "Masaharu Morimoto", "Shota Motoura", "Takeo Hosomi", "Tien N. Nguyen", "Hridesh Rajan", "Nikolaos Tsantalis", "Danny Dig"], "title": "Leveraging LLMs, IDEs, and Semantic Embeddings for Automated Move Method Refactoring", "categories": ["cs.SE", "cs.AI"], "comment": "12 pages, 2 figures", "summary": "MOVEMETHOD is a hallmark refactoring. Despite a plethora of research tools\nthat recommend which methods to move and where, these recommendations do not\nalign with how expert developers perform MOVEMETHOD. Given the extensive\ntraining of Large Language Models and their reliance upon naturalness of code,\nthey should expertly recommend which methods are misplaced in a given class and\nwhich classes are better hosts. Our formative study of 2016 LLM recommendations\nrevealed that LLMs give expert suggestions, yet they are unreliable: up to 80%\nof the suggestions are hallucinations. We introduce the first LLM fully powered\nassistant for MOVEMETHOD refactoring that automates its whole end-to-end\nlifecycle, from recommendation to execution. We designed novel solutions that\nautomatically filter LLM hallucinations using static analysis from IDEs and a\nnovel workflow that requires LLMs to be self-consistent, critique, and rank\nrefactoring suggestions. As MOVEMETHOD refactoring requires global,\nprojectlevel reasoning, we solved the limited context size of LLMs by employing\nrefactoring-aware retrieval augment generation (RAG). Our approach, MM-assist,\nsynergistically combines the strengths of the LLM, IDE, static analysis, and\nsemantic relevance. In our thorough, multi-methodology empirical evaluation, we\ncompare MM-assist with the previous state-of-the-art approaches. MM-assist\nsignificantly outperforms them: (i) on a benchmark widely used by other\nresearchers, our Recall@1 and Recall@3 show a 1.7x improvement; (ii) on a\ncorpus of 210 recent refactorings from Open-source software, our Recall rates\nimprove by at least 2.4x. Lastly, we conducted a user study with 30 experienced\nparticipants who used MM-assist to refactor their own code for one week. They\nrated 82.8% of MM-assist recommendations positively. This shows that MM-assist\nis both effective and useful.", "AI": {"tldr": "This paper introduces MM-assist, an end-to-end automated assistant for MOVEMETHOD refactoring using Large Language Models, which outperforms current approaches and is positively rated by users.", "motivation": "The motivation is to address the mismatch between existing refactoring tool recommendations and expert developer practices, leveraging the natural language learning capabilities of LLMs to enhance the MOVEMETHOD refactoring process.", "method": "The method involves integrating static analysis from IDEs, a self-consistency workflow for LLMs, and refactoring-aware retrieval augmentation to filter out hallucinations and improve recommender accuracy.", "result": "MM-assist demonstrates a 1.7x improvement on benchmark Recall@1 and Recall@3, a 2.4x improvement on a recent open-source refactoring corpus, and an 82.8% positive rating from 30 user participants.", "conclusion": "The findings show that MM-assist effectively automates MOVEMETHOD refactoring, offers superior performance compared to previous approaches, and is well-received by experienced developers."}}
{"id": "2510.12408", "pdf": "https://arxiv.org/pdf/2510.12408", "abs": "https://arxiv.org/abs/2510.12408", "authors": ["Huu Tien Nguyen", "Ahmed Karam Eldaly"], "title": "Low-Field Magnetic Resonance Image Quality Enhancement using a Conditional Flow Matching Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper introduces a novel framework for image quality transfer based on\nconditional flow matching (CFM). Unlike conventional generative models that\nrely on iterative sampling or adversarial objectives, CFM learns a continuous\nflow between a noise distribution and target data distributions through the\ndirect regression of an optimal velocity field. We evaluate this approach in\nthe context of low-field magnetic resonance imaging (LF-MRI), a rapidly\nemerging modality that offers affordable and portable scanning but suffers from\ninherently low signal-to-noise ratio and reduced diagnostic quality. Our\nframework is designed to reconstruct high-field-like MR images from their\ncorresponding low-field inputs, thereby bridging the quality gap without\nrequiring expensive infrastructure. Experiments demonstrate that CFM not only\nachieves state-of-the-art performance, but also generalizes robustly to both\nin-distribution and out-of-distribution data. Importantly, it does so while\nutilizing significantly fewer parameters than competing deep learning methods.\nThese results underline the potential of CFM as a powerful and scalable tool\nfor MRI reconstruction, particularly in resource-limited clinical environments.", "AI": {"tldr": "The paper presents a new image quality transfer method for reconstructing high-quality images using conditional flow matching (CFM), targeted at improving low-field MRI outputs.", "motivation": "The motivation of the paper is to address the inherent limitations of low-field MRI's reduced signal-to-noise ratio and diagnostic quality, providing cost-effective solutions without expensive infrastructure.", "method": "The paper employs conditional flow matching (CFM), which learns a continuous flow between noise and target distributions through optimal velocity field regression, avoiding iterative sampling or adversarial objectives.", "result": "The framework achieves state-of-the-art results in robust generalization to both in-distribution and out-of-distribution data, using fewer parameters than other deep learning models.", "conclusion": "CFM is a scalable and efficient tool for MRI image quality enhancement, with significant implications for resource-limited clinical setups."}}
{"id": "2510.12266", "pdf": "https://arxiv.org/pdf/2510.12266", "abs": "https://arxiv.org/abs/2510.12266", "authors": ["Ziyi Han", "Huanyu Wang", "Zeyu Zhang", "Xiangxiang Dai", "Xutong Liu", "John C. S. Lui"], "title": "HiLoRA: Adaptive Hierarchical LoRA Routing for Training-Free Domain Generalization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) has emerged as a widely used technique for\nadapting large language models (LLMs) to new domains, due to its modular design\nand broad availability on platforms such as HuggingFace. This availability has\nmotivated efforts to reuse existing LoRAs for domain generalization.\n  However, existing methods often rely on explicit task labels or additional\ntraining, which are impractical for deployment. Moreover, they typically\nactivate a fixed number of entire LoRA modules, leading to parameter redundancy\nor insufficiency that degrade performance.\n  In this paper, we propose \\texttt{HiLoRA}, a training-free framework that\nperforms adaptive hierarchical routing over LoRA pools. Drawing on structural\nproperties of LoRA, we define rank-one components (ROCs), in which each rank\nparameter is regarded as an independent unit. For a given input sequence,\n\\texttt{HiLoRA} first adaptively selects a subset of LoRAs and determines their\nROC allocation based on Gaussian likelihoods at the sequence level. At the\ntoken level, it further refines routing by activating only the most informative\nROCs.\n  We further provide theoretical guarantees that \\texttt{HiLoRA} selects the\nmost relevant LoRAs with high probability.\n  Extensive experiments show that \\texttt{HiLoRA} achieves substantial\nimprovements in domain generalization, with accuracy gains of up to {\\small\n$55\\%$} over state-of-the-art baselines, while maintaining comparable inference\nthroughput.", "AI": {"tldr": "HiLoRA introduces a training-free hierarchical routing framework that optimizes LoRA usage for domain generalization, achieving significant gains over existing methods.", "motivation": "Current LoRA adaptation techniques face inefficiencies due to reliance on explicit task labels, additional training, and fixed module activation, which degrade performance in domain generalization.", "method": "HiLoRA adaptively selects and allocates rank-one components (ROCs) from LoRA pools based on Gaussian likelihoods at sequence and token levels, without requiring additional training.", "result": "Experiments demonstrate up to 55% improvements in domain generalization accuracy over state-of-the-art baselines, without compromising inference throughput.", "conclusion": "HiLoRA offers a novel, efficient, and theoretically supported approach for leveraging LoRA modules to enhance performance in domain generalization tasks."}}
{"id": "2510.12476", "pdf": "https://arxiv.org/pdf/2510.12476", "abs": "https://arxiv.org/abs/2510.12476", "authors": ["Lang Gao", "Xuhui Li", "Chenxi Wang", "Mingzhe Li", "Wei Liu", "Zirui Song", "Jinghui Zhang", "Rui Yan", "Preslav Nakov", "Xiuying Chen"], "title": "When Personalization Tricks Detectors: The Feature-Inversion Trap in Machine-Generated Text Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have grown more powerful in language generation,\nproducing fluent text and even imitating personal style. Yet, this ability also\nheightens the risk of identity impersonation. To the best of our knowledge, no\nprior work has examined personalized machine-generated text (MGT) detection. In\nthis paper, we introduce \\dataset, the first benchmark for evaluating detector\nrobustness in personalized settings, built from literary and blog texts paired\nwith their LLM-generated imitations. Our experimental results demonstrate large\nperformance gaps across detectors in personalized settings: some\nstate-of-the-art models suffer significant drops. We attribute this limitation\nto the \\textit{feature-inversion trap}, where features that are discriminative\nin general domains become inverted and misleading when applied to personalized\ntext. Based on this finding, we propose \\method, a simple and reliable way to\npredict detector performance changes in personalized settings. \\method\nidentifies latent directions corresponding to inverted features and constructs\nprobe datasets that differ primarily along these features to evaluate detector\ndependence. Our experiments show that \\method can accurately predict both the\ndirection and the magnitude of post-transfer changes, showing 85\\% correlation\nwith the actual performance gaps. We hope that this work will encourage further\nresearch on personalized text detection.", "AI": {"tldr": "This paper analyzes the risks of identity impersonation by Large Language Models (LLMs) and introduces a benchmark to assess the robustness of detectors against personalized machine-generated text (MGT).", "motivation": "The motivation behind this work is to address the growing threat of identity impersonation via personalized LLM-generated text and the lack of prior research in detecting such personalized MGT.", "method": "The researchers introduced a benchmark called \\dataset, focusing on literary and blog texts paired with LLM-generated imitations, and proposed \\method to predict detector performance changes by analyzing inverted features in personalized text.", "result": "Experimental results revealed significant performance drops in state-of-the-art detectors when applied to personalized settings due to the feature-inversion trap. \\method displayed an 85% correlation with actual performance gaps.", "conclusion": "This study highlights the limitations of current detectors for personalized MGT, proposes a predictive framework \\method, and calls for further research in personalized text detection."}}
{"id": "2510.11728", "pdf": "https://arxiv.org/pdf/2510.11728", "abs": "https://arxiv.org/abs/2510.11728", "authors": ["Bingqiao Gu", "Jiale Zeng", "Xingqin Qi", "Dong Li"], "title": "Modeling Hypergraph Using Large Language Models", "categories": ["cs.SI", "cs.AI", "68R10, 68T07", "G.2.2; H.2.8; I.2.6"], "comment": "10 pages, 5 figures", "summary": "Due to the advantages of hypergraphs in modeling high-order relationships in\ncomplex systems, they have been applied to higher-order clustering, hypergraph\nneural networks and computer vision. These applications rely heavily on access\nto high-quality, large-scale real-world hypergraph data. Yet, compared to\ntraditional pairwise graphs, real hypergraph datasets remain scarce in both\nscale and diversity. This shortage significantly limits the development and\nevaluation of advanced hypergraph learning algorithms. Therefore, how to\nquickly generate large-scale hypergraphs that conform to the characteristics of\nreal networks is a crucial task that has not received sufficient attention.\nMotivated by recent advances in large language models (LLMs), particularly\ntheir capabilities in semantic reasoning, structured generation, and simulating\nhuman behavior, we investigate whether LLMs can facilitate hypergraph\ngeneration from a fundamentally new perspective. We introduce HyperLLM, a novel\nLLM-driven hypergraph generator that simulates the formation and evolution of\nhypergraphs through a multi-agent collaboration. The framework integrates\nprompts and structural feedback mechanisms to ensure that the generated\nhypergraphs reflect key real-world patterns. Extensive experiments across\ndiverse datasets demonstrate that HyperLLM achieves superior fidelity to\nstructural and temporal hypergraph patterns, while requiring minimal\nstatistical priors. Our findings suggest that LLM-based frameworks offer a\npromising new direction for hypergraph modeling.", "AI": {"tldr": "The paper introduces HyperLLM, a framework leveraging large language models (LLMs) to generate real-world hypergraphs, addressing the scarcity of large-scale hypergraph datasets.", "motivation": "The lack of high-quality hypergraph datasets limits the development and evaluation of hypergraph learning algorithms. The paper aims to address this by exploring LLMs\u2019 capabilities for hypergraph generation.", "method": "The authors developed HyperLLM, a generator utilizing multi-agent collaboration, prompts, and feedback mechanisms to simulate hypergraph formation, ensuring alignment with real-world patterns.", "result": "HyperLLM demonstrated superior fidelity to real-world structural and temporal hypergraph patterns across diverse datasets, with minimal need for statistical priors.", "conclusion": "LLM-based frameworks such as HyperLLM open a promising avenue for hypergraph modeling, tackling challenges in dataset generation effectively."}}
{"id": "2510.12422", "pdf": "https://arxiv.org/pdf/2510.12422", "abs": "https://arxiv.org/abs/2510.12422", "authors": ["Jialong Zuo", "Yongtai Deng", "Lingdong Kong", "Jingkang Yang", "Rui Jin", "Yiwei Zhang", "Nong Sang", "Liang Pan", "Ziwei Liu", "Changxin Gao"], "title": "VideoLucy: Deep Memory Backtracking for Long Video Understanding", "categories": ["cs.CV"], "comment": "NeurIPS-2025 Accepted Paper", "summary": "Recent studies have shown that agent-based systems leveraging large language\nmodels (LLMs) for key information retrieval and integration have emerged as a\npromising approach for long video understanding. However, these systems face\ntwo major challenges. First, they typically perform modeling and reasoning on\nindividual frames, struggling to capture the temporal context of consecutive\nframes. Second, to reduce the cost of dense frame-level captioning, they adopt\nsparse frame sampling, which risks discarding crucial information. To overcome\nthese limitations, we propose VideoLucy, a deep memory backtracking framework\nfor long video understanding. Inspired by the human recollection process from\ncoarse to fine, VideoLucy employs a hierarchical memory structure with\nprogressive granularity. This structure explicitly defines the detail level and\ntemporal scope of memory at different hierarchical depths. Through an\nagent-based iterative backtracking mechanism, VideoLucy systematically mines\nvideo-wide, question-relevant deep memories until sufficient information is\ngathered to provide a confident answer. This design enables effective temporal\nunderstanding of consecutive frames while preserving critical details. In\naddition, we introduce EgoMem, a new benchmark for long video understanding.\nEgoMem is designed to comprehensively evaluate a model's ability to understand\ncomplex events that unfold over time and capture fine-grained details in\nextremely long videos. Extensive experiments demonstrate the superiority of\nVideoLucy. Built on open-source models, VideoLucy significantly outperforms\nstate-of-the-art methods on multiple long video understanding benchmarks,\nachieving performance even surpassing the latest proprietary models such as\nGPT-4o. Our code and dataset will be made publicly at\nhttps://videolucy.github.io", "AI": {"tldr": "This paper introduces VideoLucy, a framework for improving long video understanding by leveraging hierarchical memory structures and iterative backtracking. It addresses challenges in temporal context modeling and sparse frame sampling, achieving superior results compared to other methods.", "motivation": "The paper aims to address limitations in current agent-based systems that struggle with understanding the temporal context of videos and risks of discarding crucial information by sparse frame sampling.", "method": "VideoLucy employs a hierarchical memory structure with progressive granularity and an agent-based iterative backtracking mechanism to mine question-relevant information from videos.", "result": "VideoLucy demonstrates superior performance compared to state-of-the-art models, outperforming even proprietary systems like GPT-4 on multiple long video understanding benchmarks.", "conclusion": "The proposed framework effectively enhances long video understanding, and its open-source code and dataset promise to benefit future research in this area."}}
{"id": "2510.12273", "pdf": "https://arxiv.org/pdf/2510.12273", "abs": "https://arxiv.org/abs/2510.12273", "authors": ["Laurin Luttmann", "Lin Xie"], "title": "Multi-Action Self-Improvement for Neural Combinatorial Optimization", "categories": ["cs.LG"], "comment": null, "summary": "Self-improvement has emerged as a state-of-the-art paradigm in Neural\nCombinatorial Optimization (NCO), where models iteratively refine their\npolicies by generating and imitating high-quality solutions. Despite strong\nempirical performance, existing methods face key limitations. Training is\ncomputationally expensive, as policy updates require sampling numerous\ncandidate solutions per instance to extract a single expert trajectory. More\nfundamentally, these approaches fail to exploit the structure of combinatorial\nproblems involving the coordination of multiple agents, such as vehicles in\nmin-max routing or machines in scheduling. By supervising on single-action\ntrajectories, they fail to exploit agent-permutation symmetries, where distinct\nsequences of actions yield identical solutions, hindering generalization and\nthe ability to learn coordinated behavior.\n  We address these challenges by extending self-improvement to operate over\njoint multi-agent actions. Our model architecture predicts complete agent-task\nassignments jointly at each decision step. To explicitly leverage symmetries,\nwe employ a set-prediction loss, which supervises the policy on multiple expert\nassignments for any given state. This approach enhances sample efficiency and\nthe model's ability to learn coordinated behavior. Furthermore, by generating\nmulti-agent actions in parallel, it drastically accelerates the solution\ngeneration phase of the self-improvement loop. Empirically, we validate our\nmethod on several combinatorial problems, demonstrating consistent improvements\nin the quality of the final solution and a reduced generation latency compared\nto standard self-improvement.", "AI": {"tldr": "The paper introduces a method to enhance Neural Combinatorial Optimization (NCO) with improved sample efficiency and faster training by predicting joint multi-agent actions and leveraging permutation symmetries.", "motivation": "The study aims to address inefficiencies and limitations in existing self-improvement paradigms for NCO, particularly regarding computational expense and failure to utilize multi-agent coordination and permutation symmetries.", "method": "The method involves extending self-improvement for predicting joint multi-agent actions and employs a set-prediction loss to supervise the policy on multiple expert assignments, exploiting problem symmetries.", "result": "The approach shows better final solution quality, faster solution generation, and improved sample efficiency based on empirical validation over various combinatorial problems.", "conclusion": "The method significantly improves NCO performance, enhancing both computational efficiency and the ability to learn coordinated multi-agent behavior."}}
{"id": "2510.12516", "pdf": "https://arxiv.org/pdf/2510.12516", "abs": "https://arxiv.org/abs/2510.12516", "authors": ["Tomas Ruiz", "Siyao Peng", "Barbara Plank", "Carsten Schwemmer"], "title": "BoN Appetit Team at LeWiDi-2025: Best-of-N Test-time Scaling Can Not Stomach Annotation Disagreements (Yet)", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Test-time scaling is a family of techniques to improve LLM outputs at\ninference time by performing extra computation. To the best of our knowledge,\ntest-time scaling has been limited to domains with verifiably correct answers,\nlike mathematics and coding. We transfer test-time scaling to the LeWiDi-2025\ntasks to evaluate annotation disagreements. We experiment with three test-time\nscaling methods: two benchmark algorithms (Model Averaging and Majority\nVoting), and a Best-of-N sampling method. The two benchmark methods improve LLM\nperformance consistently on the LeWiDi tasks, but the Best-of-N method does\nnot. Our experiments suggest that the Best-of-N method does not currently\ntransfer from mathematics to LeWiDi tasks, and we analyze potential reasons for\nthis gap.", "AI": {"tldr": "The paper explores test-time scaling techniques to enhance LLM outputs in evaluating annotation disagreements, achieving consistent improvements with two methods but not with Best-of-N sampling.", "motivation": "To transfer and test whether test-time scaling techniques, originally applied in domains with verifiably correct answers, can be utilized in tasks evaluating annotation disagreements.", "method": "The authors tested three methods: Model Averaging, Majority Voting, and Best-of-N sampling on the LeWiDi-2025 tasks.", "result": "Model Averaging and Majority Voting methods consistently improved LLM performance, but the Best-of-N sampling method failed to show effectiveness in this context.", "conclusion": "Test-time scaling methods like Model Averaging and Majority Voting can improve LLM performance for annotation disagreements, while Best-of-N does not translate well from mathematics to LeWiDi tasks, requiring further analysis."}}
{"id": "2510.11732", "pdf": "https://arxiv.org/pdf/2510.11732", "abs": "https://arxiv.org/abs/2510.11732", "authors": ["Guojian Li", "Qijie Shao", "Zhixian Zhao", "Shuiyuan Wang", "Zhonghua Fu", "Lei Xie"], "title": "Serial-Parallel Dual-Path Architecture for Speaking Style Recognition", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Accepted by NCMMSC2025", "summary": "Speaking Style Recognition (SSR) identifies a speaker's speaking style\ncharacteristics from speech. Existing style recognition approaches primarily\nrely on linguistic information, with limited integration of acoustic\ninformation, which restricts recognition accuracy improvements. The fusion of\nacoustic and linguistic modalities offers significant potential to enhance\nrecognition performance. In this paper, we propose a novel serial-parallel\ndual-path architecture for SSR that leverages acoustic-linguistic bimodal\ninformation. The serial path follows the ASR+STYLE serial paradigm, reflecting\na sequential temporal dependency, while the parallel path integrates our\ndesigned Acoustic-Linguistic Similarity Module (ALSM) to facilitate cross-modal\ninteraction with temporal simultaneity. Compared to the existing SSR baseline\n-- the OSUM model, our approach reduces parameter size by 88.4% and achieves a\n30.3% improvement in SSR accuracy for eight styles on the test set.", "AI": {"tldr": "The paper introduces a dual-path model combining acoustic and linguistic data for Speaking Style Recognition, achieving significant accuracy improvements.", "motivation": "Existing methods primarily focus on linguistic data, neglecting the potential of acoustic data integration for better SSR accuracy.", "method": "Proposed a dual-path architecture with sequential dependency in one path and an Acoustic-Linguistic Similarity Module for cross-modal interaction in the other.", "result": "Achieved 30.3% improvement in SSR accuracy compared to baselines, with 88.4% reduction in model size.", "conclusion": "Fusing acoustic and linguistic data enhances SSR accuracy, demonstrating the potential for more efficient multimodal models in this domain."}}
{"id": "2510.12444", "pdf": "https://arxiv.org/pdf/2510.12444", "abs": "https://arxiv.org/abs/2510.12444", "authors": ["Shaoyang Zhou", "Yingshu Li", "Yunyi Liu", "Lingqiao Liu", "Lei Wang", "Luping Zhou"], "title": "A Review of Longitudinal Radiology Report Generation: Dataset Composition, Methods, and Performance Evaluation", "categories": ["cs.CV"], "comment": null, "summary": "Chest Xray imaging is a widely used diagnostic tool in modern medicine, and\nits high utilization creates substantial workloads for radiologists. To\nalleviate this burden, vision language models are increasingly applied to\nautomate Chest Xray radiology report generation (CXRRRG), aiming for clinically\naccurate descriptions while reducing manual effort. Conventional approaches,\nhowever, typically rely on single images, failing to capture the longitudinal\ncontext necessary for producing clinically faithful comparison statements.\nRecently, growing attention has been directed toward incorporating longitudinal\ndata into CXR RRG, enabling models to leverage historical studies in ways that\nmirror radiologists diagnostic workflows. Nevertheless, existing surveys\nprimarily address single image CXRRRG and offer limited guidance for\nlongitudinal settings, leaving researchers without a systematic framework for\nmodel design. To address this gap, this survey provides the first comprehensive\nreview of longitudinal radiology report generation (LRRG). Specifically, we\nexamine dataset construction strategies, report generation architectures\nalongside longitudinally tailored designs, and evaluation protocols\nencompassing both longitudinal specific measures and widely used benchmarks. We\nfurther summarize LRRG methods performance, alongside analyses of different\nablation studies, which collectively highlight the critical role of\nlongitudinal information and architectural design choices in improving model\nperformance. Finally, we summarize five major limitations of current research\nand outline promising directions for future development, aiming to lay a\nfoundation for advancing this emerging field.", "AI": {"tldr": "This survey reviews advancements in longitudinal radiology report generation, emphasizing the integration of historical Chest X-ray studies to improve model performance.", "motivation": "Radiologists face substantial workloads from high utilization of Chest X-ray imaging. There is a need for automated systems that can generate clinically accurate radiology reports to streamline diagnostic workflows.", "method": "The paper surveys dataset creation strategies, longitudinally tailored model architectures, evaluation protocols, and analyzes the role of longitudinal information and architectural design in enhancing performance.", "result": "The survey identifies key methods and their performance, highlights critical architectural designs, and conducts ablation studies to show the importance of longitudinal data.", "conclusion": "Five core limitations are discussed in current LRRG research, with suggestions for future directions to advance this field and establish systematic frameworks for model design."}}
{"id": "2510.12548", "pdf": "https://arxiv.org/pdf/2510.12548", "abs": "https://arxiv.org/abs/2510.12548", "authors": ["Stella Frank", "Emily Allaway"], "title": "VISaGE: Understanding Visual Generics and Exceptions", "categories": ["cs.CL", "cs.CV"], "comment": "EMNLP 2025", "summary": "While Vision Language Models (VLMs) learn conceptual representations, in the\nform of generalized knowledge, during training, they are typically used to\nanalyze individual instances. When evaluation instances are atypical, this\nparadigm results in tension between two priors in the model. The first is a\npragmatic prior that the textual and visual input are both relevant, arising\nfrom VLM finetuning on congruent inputs; the second is a semantic prior that\nthe conceptual representation is generally true for instances of the category.\nIn order to understand how VLMs trade off these priors, we introduce a new\nevaluation dataset, VISaGE, consisting of both typical and exceptional images.\nIn carefully balanced experiments, we show that conceptual understanding\ndegrades when the assumption of congruency underlying the pragmatic prior is\nviolated with incongruent images. This effect is stronger than the effect of\nthe semantic prior when querying about individual instances.", "AI": {"tldr": "The paper introduces a new evaluation dataset called VISaGE to investigate how Vision Language Models (VLMs) manage trade-offs between pragmatic and semantic priors when analyzing atypical inputs.", "motivation": "To explore how VLMs balance their conceptual understanding (semantic prior) against their assumption of input congruency (pragmatic prior) when analyzing both typical and exceptional instances.", "method": "The authors constructed VISaGE, a carefully designed evaluation dataset featuring both typical and incongruent images, to conduct experiments that measure how the violation of congruent inputs affects a VLM's conceptual understanding.", "result": "Experimental results show that when input congruency is violated (incongruent images), VLMs' conceptual understanding deteriorates. This decrease is more pronounced compared to the effect of the semantic prior when querying individual instances.", "conclusion": "The study reveals that the pragmatic prior dominates over the semantic prior in influencing VLM behavior, emphasizing the limitations of VLMs in understanding atypical scenarios."}}
{"id": "2510.11734", "pdf": "https://arxiv.org/pdf/2510.11734", "abs": "https://arxiv.org/abs/2510.11734", "authors": ["Yuqi Bai", "Tianyu Huang", "Kun Sun", "Yuting Chen"], "title": "Scaling Law in LLM Simulated Personality: More Detailed and Realistic Persona Profile Is All You Need", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "This research focuses on using large language models (LLMs) to simulate\nsocial experiments, exploring their ability to emulate human personality in\nvirtual persona role-playing. The research develops an end-to-end evaluation\nframework, including individual-level analysis of stability and\nidentifiability, as well as population-level analysis called progressive\npersonality curves to examine the veracity and consistency of LLMs in\nsimulating human personality. Methodologically, this research proposes\nimportant modifications to traditional psychometric approaches (CFA and\nconstruct validity) which are unable to capture improvement trends in LLMs at\ntheir current low-level simulation, potentially leading to remature rejection\nor methodological misalignment. The main contributions of this research are:\nproposing a systematic framework for LLM virtual personality evaluation;\nempirically demonstrating the critical role of persona detail in personality\nsimulation quality; and identifying marginal utility effects of persona\nprofiles, especially a Scaling Law in LLM personality simulation, offering\noperational evaluation metrics and a theoretical foundation for applying large\nlanguage models in social science experiments.", "AI": {"tldr": "The study explores using large language models (LLMs) for social experiments, focusing on their ability to simulate human personalities and evaluates their performance through a systematic framework.", "motivation": "To understand and evaluate the capability of LLMs in simulating human-like personalities for social science experiments, addressing their potential and limitations.", "method": "The paper introduces an evaluation framework involving individual-level and population-level analyses, modifies traditional psychometric approaches, and highlights factors influencing simulation quality.", "result": "The research demonstrated the significance of persona detail in LLM personality simulation quality and uncovered scaling laws and marginal utility effects of persona profiles.", "conclusion": "LLMs show potential in social science experiments with proposed evaluation metrics and theoretical foundations, but their ability to emulate human personalities is still in developmental stages."}}
{"id": "2510.12468", "pdf": "https://arxiv.org/pdf/2510.12468", "abs": "https://arxiv.org/abs/2510.12468", "authors": ["Dion J. X. Ho", "Gabriel Lee Jun Rong", "Niharika Shrivastava", "Harshavardhan Abichandani", "Pai Chet Ng", "Xiaoxiao Miao"], "title": "MS-GAGA: Metric-Selective Guided Adversarial Generation Attack", "categories": ["cs.CV"], "comment": null, "summary": "We present MS-GAGA (Metric-Selective Guided Adversarial Generation Attack), a\ntwo-stage framework for crafting transferable and visually imperceptible\nadversarial examples against deepfake detectors in black-box settings. In Stage\n1, a dual-stream attack module generates adversarial candidates: MNTD-PGD\napplies enhanced gradient calculations optimized for small perturbation\nbudgets, while SG-PGD focuses perturbations on visually salient regions. This\ncomplementary design expands the adversarial search space and improves\ntransferability across unseen models. In Stage 2, a metric-aware selection\nmodule evaluates candidates based on both their success against black-box\nmodels and their structural similarity (SSIM) to the original image. By jointly\noptimizing transferability and imperceptibility, MS-GAGA achieves up to 27%\nhigher misclassification rates on unseen detectors compared to state-of-the-art\nattacks.", "AI": {"tldr": "The paper introduces MS-GAGA, a two-stage framework for creating transferable and visually imperceptible adversarial examples to fool deepfake detectors in black-box settings, achieving superior performance against unseen detectors.", "motivation": "Deepfake detectors are increasing in usage; however, their robustness needs scrutiny, especially in black-box settings. The paper aims to craft adversarial examples that can evade these detectors while remaining visually imperceptible.", "method": "MS-GAGA uses a two-stage approach. Stage 1 deploys a dual-stream attack module combining MNTD-PGD for small perturbation budgets and SG-PGD for visually salient region perturbations. Stage 2 incorporates a metric-aware selection module to optimize experiments for success against black-box models and structural similarity.", "result": "The framework demonstrated up to 27% higher misclassification rates on unseen deepfake detectors than existing state-of-the-art attacks, highlighting its efficacy.", "conclusion": "MS-GAGA improves transferable and imperceptible adversarial attacks, demonstrating potential in advancing the understanding of vulnerabilities in deepfake detection systems."}}
{"id": "2510.12312", "pdf": "https://arxiv.org/pdf/2510.12312", "abs": "https://arxiv.org/abs/2510.12312", "authors": ["Florent Delgrange", "Raphael Avalos", "Willem R\u00f6pke"], "title": "Deep SPI: Safe Policy Improvement via World Models", "categories": ["cs.LG", "cs.AI"], "comment": "10 pages main text, 17 pages appendix (excluding references)", "summary": "Safe policy improvement (SPI) offers theoretical control over policy updates,\nyet existing guarantees largely concern offline, tabular reinforcement learning\n(RL). We study SPI in general online settings, when combined with world model\nand representation learning. We develop a theoretical framework showing that\nrestricting policy updates to a well-defined neighborhood of the current policy\nensures monotonic improvement and convergence. This analysis links transition\nand reward prediction losses to representation quality, yielding online, \"deep\"\nanalogues of classical SPI theorems from the offline RL literature. Building on\nthese results, we introduce DeepSPI, a principled on-policy algorithm that\ncouples local transition and reward losses with regularised policy updates. On\nthe ALE-57 benchmark, DeepSPI matches or exceeds strong baselines, including\nPPO and DeepMDPs, while retaining theoretical guarantees.", "AI": {"tldr": "The paper introduces DeepSPI, an on-policy algorithm ensuring monotonic policy improvement and theoretical convergence, achieving strong performance on the ALE-57 benchmark.", "motivation": "Existing SPI methods primarily focus on offline and tabular RL; there is a need to explore SPI in online settings paired with world model and representation learning.", "method": "The authors propose a theoretical framework linking policy update neighborhoods with monotonic improvement, enabling the introduction of DeepSPI, which couples transition and reward prediction losses with regularized updates.", "result": "DeepSPI demonstrates performance equal to or exceeding baselines like PPO and DeepMDPs on ALE-57 while retaining theoretical guarantees.", "conclusion": "The authors successfully extend classical SPI concepts to online deep RL, demonstrating practical applicability and strong empirical results while maintaining theoretical backing."}}
{"id": "2510.12587", "pdf": "https://arxiv.org/pdf/2510.12587", "abs": "https://arxiv.org/abs/2510.12587", "authors": ["Bryan Eikema", "Evgenia Ilia", "Jos\u00e9 G. C. de Souza", "Chrysoula Zerva", "Wilker Aziz"], "title": "Teaching Language Models to Faithfully Express their Uncertainty", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) often miscommunicate their uncertainty: repeated\nqueries can produce divergent answers, yet generated responses are typically\nunhedged or hedged in ways that do not reflect this variability. This conveys\nunfaithful information about the uncertain state of the LLMs' knowledge,\ncreating a faithfulness gap that affects even strong LLMs. We introduce\nFaithful Uncertainty Tuning (FUT): a fine-tuning approach that teaches\ninstruction-tuned LLMs to express uncertainty faithfully without altering their\nunderlying answer distribution. We construct training data by augmenting model\nsamples with uncertainty hedges (i.e. verbal cues such as 'possibly' or\n'likely') aligned with sample consistency, requiring no supervision beyond the\nmodel and a set of prompts. We evaluate FUT on open-domain question answering\n(QA) across multiple models and datasets. Our results show that FUT\nsubstantially reduces the faithfulness gap, while preserving QA accuracy and\nintroducing minimal semantic distribution shift. Further analyses demonstrate\nrobustness across decoding strategies, choice of hedgers, and other forms of\nuncertainty expression (i.e. numerical). These findings establish FUT as a\nsimple and effective way to teach LLMs to communicate uncertainty faithfully.", "AI": {"tldr": "The paper addresses large language models' (LLMs) inability to faithfully express uncertainty and proposes Faithful Uncertainty Tuning (FUT) to improve this aspect without compromising answer accuracy.", "motivation": "LLMs often fail to communicate their uncertainty accurately, misleading users about their knowledge's reliability and introducing a 'faithfulness gap' in their responses.", "method": "The authors introduce 'Faithful Uncertainty Tuning' (FUT), which fine-tunes instruction-tuned LLMs by augmenting training data with verbal uncertainty hedges that reflect response consistency, requiring minimal supervision.", "result": "FUT significantly improves the faithfulness of uncertainty communication in LLMs while maintaining answer accuracy and minimizing semantic distribution shifts, showing robustness across strategies and forms of expressing uncertainty.", "conclusion": "FUT offers a simple and effective solution for making LLMs communicate their uncertainty more faithfully, providing a step forward in enhancing trust and reliability in AI responses."}}
{"id": "2510.11738", "pdf": "https://arxiv.org/pdf/2510.11738", "abs": "https://arxiv.org/abs/2510.11738", "authors": ["Simone Carnemolla", "Matteo Pennisi", "Chiara Russo", "Simone Palazzo", "Daniela Giordano", "Concetto Spampinato"], "title": "SeeingSounds: Learning Audio-to-Visual Alignment via Text", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM"], "comment": "accepted to ACM Multimedia Asia 2025", "summary": "We introduce SeeingSounds, a lightweight and modular framework for\naudio-to-image generation that leverages the interplay between audio, language,\nand vision-without requiring any paired audio-visual data or training on visual\ngenerative models. Rather than treating audio as a substitute for text or\nrelying solely on audio-to-text mappings, our method performs dual alignment:\naudio is projected into a semantic language space via a frozen language\nencoder, and, contextually grounded into the visual domain using a\nvision-language model. This approach, inspired by cognitive neuroscience,\nreflects the natural cross-modal associations observed in human perception. The\nmodel operates on frozen diffusion backbones and trains only lightweight\nadapters, enabling efficient and scalable learning. Moreover, it supports\nfine-grained and interpretable control through procedural text prompt\ngeneration, where audio transformations (e.g., volume or pitch shifts)\ntranslate into descriptive prompts (e.g., \"a distant thunder\") that guide\nvisual outputs. Extensive experiments across standard benchmarks confirm that\nSeeingSounds outperforms existing methods in both zero-shot and supervised\nsettings, establishing a new state of the art in controllable audio-to-visual\ngeneration.", "AI": {"tldr": "SeeingSounds framework enables audio-to-image generation without paired audio-visual data, using dual alignment with language and vision models.", "motivation": "To create an efficient and controllable audio-to-image generation framework inspired by human cross-modal perception, without relying on visual generative training or paired data.", "method": "Audio is projected to a language semantic space through a frozen language encoder and contextually aligned with the visual domain via a vision-language model, using lightweight adapters for scalable learning.", "result": "SeeingSounds demonstrated state-of-the-art performance in audio-to-visual generation tasks across zero-shot and supervised benchmarks.", "conclusion": "This novel approach achieves effective audio-to-image generation, leveraging interpretability and control, while being efficient and modular for broader applications."}}
{"id": "2510.12482", "pdf": "https://arxiv.org/pdf/2510.12482", "abs": "https://arxiv.org/abs/2510.12482", "authors": ["Shurong Chai", "Rahul Kumar JAIN", "Rui Xu", "Shaocong Mo", "Ruibo Hou", "Shiyu Teng", "Jiaqing Liu", "Lanfen Lin", "Yen-Wei Chen"], "title": "A Text-Image Fusion Method with Data Augmentation Capabilities for Referring Medical Image Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deep learning relies heavily on data augmentation to mitigate limited data,\nespecially in medical imaging. Recent multimodal learning integrates text and\nimages for segmentation, known as referring or text-guided image segmentation.\nHowever, common augmentations like rotation and flipping disrupt spatial\nalignment between image and text, weakening performance. To address this, we\npropose an early fusion framework that combines text and visual features before\naugmentation, preserving spatial consistency. We also design a lightweight\ngenerator that projects text embeddings into visual space, bridging semantic\ngaps. Visualization of generated pseudo-images shows accurate region\nlocalization. Our method is evaluated on three medical imaging tasks and four\nsegmentation frameworks, achieving state-of-the-art results. Code is publicly\navailable on GitHub: https://github.com/11yxk/MedSeg_EarlyFusion.", "AI": {"tldr": "This paper proposes a data augmentation method for text-guided medical image segmentation that preserves spatial consistency between image and text.", "motivation": "Medical imaging often suffers from limited datasets, requiring augmentation techniques to improve segmentation outcomes.", "method": "The authors introduce an early fusion framework for combining text and visual features before augmentation, along with a lightweight generator for projecting text embeddings into visual space.", "result": "The proposed method demonstrates state-of-the-art results across three medical imaging tasks and four segmentation frameworks.", "conclusion": "Early fusion and spatially consistent augmentation improve segmentation performance in multimodal medical imaging tasks."}}
{"id": "2510.12328", "pdf": "https://arxiv.org/pdf/2510.12328", "abs": "https://arxiv.org/abs/2510.12328", "authors": ["Kiattikun Chobtham", "Kanoksri Sarinnapakorn", "Kritanai Torsri", "Prattana Deeprasertkul", "Jirawan Kamma"], "title": "Leveraging Teleconnections with Physics-Informed Graph Attention Networks for Long-Range Extreme Rainfall Forecasting in Thailand", "categories": ["cs.LG"], "comment": null, "summary": "Accurate rainfall forecasting, particularly for extreme events, remains a\nsignificant challenge in climatology and the Earth system. This paper presents\nnovel physics-informed Graph Neural Networks (GNNs) combined with extreme-value\nanalysis techniques to improve gauge-station rainfall predictions across\nThailand. The model leverages a graph-structured representation of gauge\nstations to capture complex spatiotemporal patterns, and it offers\nexplainability through teleconnections. We preprocess relevant climate indices\nthat potentially influence regional rainfall. The proposed Graph Attention\nNetwork with Long Short-Term Memory (Attention-LSTM) applies the attention\nmechanism using initial edge features derived from simple\norographic-precipitation physics formulation. The embeddings are subsequently\nprocessed by LSTM layers. To address extremes, we perform Peak-Over-Threshold\n(POT) mapping using the novel Spatial Season-aware Generalized Pareto\nDistribution (GPD) method, which overcomes limitations of traditional\nmachine-learning models. Experiments demonstrate that our method outperforms\nwell-established baselines across most regions, including areas prone to\nextremes, and remains strongly competitive with the state of the art. Compared\nwith the operational forecasting system SEAS5, our real-world application\nimproves extreme-event prediction and offers a practical enhancement to produce\nfine-resolution maps that support decision-making in long-term water\nmanagement.", "AI": {"tldr": "The paper introduces physics-informed Graph Neural Networks (GNNs) combined with extreme-value analysis to improve rainfall prediction in Thailand.", "motivation": "The study aims to enhance the accuracy of rainfall forecasting, especially for extreme weather events, which is a persistent challenge in climatology.", "method": "The authors propose an Attention-LSTM-based Graph Neural Network leveraging physics principles and extreme-value statistical methods, such as the Spatial Season-aware Generalized Pareto Distribution for improved predictions.", "result": "The proposed model outperforms traditional baselines and operational systems like SEAS5 in predicting rainfall extremes across various regions.", "conclusion": "This novel approach improves extreme-event rainfall predictions, offering practical utility in water resource management and decision-making processes."}}
{"id": "2510.12608", "pdf": "https://arxiv.org/pdf/2510.12608", "abs": "https://arxiv.org/abs/2510.12608", "authors": ["Siyuan Li", "Aodu Wulianghai", "Xi Lin", "Guangyan Li", "Xiang Chen", "Jun Wu", "Jianhua Li"], "title": "StyleDecipher: Robust and Explainable Detection of LLM-Generated Texts with Stylistic Analysis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the increasing integration of large language models (LLMs) into\nopen-domain writing, detecting machine-generated text has become a critical\ntask for ensuring content authenticity and trust. Existing approaches rely on\nstatistical discrepancies or model-specific heuristics to distinguish between\nLLM-generated and human-written text. However, these methods struggle in\nreal-world scenarios due to limited generalization, vulnerability to\nparaphrasing, and lack of explainability, particularly when facing stylistic\ndiversity or hybrid human-AI authorship. In this work, we propose\nStyleDecipher, a robust and explainable detection framework that revisits\nLLM-generated text detection using combined feature extractors to quantify\nstylistic differences. By jointly modeling discrete stylistic indicators and\ncontinuous stylistic representations derived from semantic embeddings,\nStyleDecipher captures distinctive style-level divergences between human and\nLLM outputs within a unified representation space. This framework enables\naccurate, explainable, and domain-agnostic detection without requiring access\nto model internals or labeled segments. Extensive experiments across five\ndiverse domains, including news, code, essays, reviews, and academic abstracts,\ndemonstrate that StyleDecipher consistently achieves state-of-the-art in-domain\naccuracy. Moreover, in cross-domain evaluations, it surpasses existing\nbaselines by up to 36.30%, while maintaining robustness against adversarial\nperturbations and mixed human-AI content. Further qualitative and quantitative\nanalysis confirms that stylistic signals provide explainable evidence for\ndistinguishing machine-generated text. Our source code can be accessed at\nhttps://github.com/SiyuanLi00/StyleDecipher.", "AI": {"tldr": "StyleDecipher is a detection framework for distinguishing LLM-generated text from human-written text, achieving high accuracy and robustness across various domains.", "motivation": "The increasing use of large language models (LLMs) in writing heightens the need to detect machine-generated text to maintain authenticity and trust.", "method": "StyleDecipher quantifies stylistic differences by combining discrete stylistic indicators and continuous representations from semantic embeddings into a unified space without relying on model specifics or labeled data.", "result": "Experiments in five domains show StyleDecipher achieves state-of-the-art detection accuracy, surpassing baselines by up to 36.30% in cross-domain evaluations while resisting adversarial and hybrid content.", "conclusion": "StyleDecipher provides a robust, explainable solution for LLM-generated text detection, addressing challenges like stylistic diversity and hybrid authorship."}}
{"id": "2510.11739", "pdf": "https://arxiv.org/pdf/2510.11739", "abs": "https://arxiv.org/abs/2510.11739", "authors": ["Muhammad Hamza", "Rizwan Jafar"], "title": "Celebrity Profiling on Short Urdu Text using Twitter Followers' Feed", "categories": ["cs.SI", "cs.AI", "cs.CL"], "comment": null, "summary": "Social media has become an essential part of the digital age, serving as a\nplatform for communication, interaction, and information sharing. Celebrities\nare among the most active users and often reveal aspects of their personal and\nprofessional lives through online posts. Platforms such as Twitter provide an\nopportunity to analyze language and behavior for understanding demographic and\nsocial patterns. Since followers frequently share linguistic traits and\ninterests with the celebrities they follow, textual data from followers can be\nused to predict celebrity demographics. However, most existing research in this\nfield has focused on English and other high-resource languages, leaving Urdu\nlargely unexplored.\n  This study applies modern machine learning and deep learning techniques to\nthe problem of celebrity profiling in Urdu. A dataset of short Urdu tweets from\nfollowers of subcontinent celebrities was collected and preprocessed. Multiple\nalgorithms were trained and compared, including Logistic Regression, Support\nVector Machines, Random Forests, Convolutional Neural Networks, and Long\nShort-Term Memory networks. The models were evaluated using accuracy,\nprecision, recall, F1-score, and cumulative rank (cRank). The best performance\nwas achieved for gender prediction with a cRank of 0.65 and an accuracy of\n0.65, followed by moderate results for age, profession, and fame prediction.\nThese results demonstrate that follower-based linguistic features can be\neffectively leveraged using machine learning and neural approaches for\ndemographic prediction in Urdu, a low-resource language.", "AI": {"tldr": "This paper explores profiling celebrities by analyzing Urdu tweets from followers using machine learning techniques, achieving reasonable accuracy in predicting demographics.", "motivation": "The study aims to fill the gap in celebrity profiling research by focusing on low-resource languages like Urdu, which have been largely unexplored despite their vast online presence.", "method": "A dataset of Urdu tweets from followers of subcontinent celebrities was collected and preprocessed. Machine learning and deep learning models, such as Logistic Regression, SVM, Random Forests, CNN, and LSTM, were trained and evaluated.", "result": "The best results were achieved for gender prediction with an accuracy of 65%, while moderate results were obtained for age, profession, and fame prediction across various evaluation metrics.", "conclusion": "Linguistic features from follower tweets can be successfully used with advanced AI methods to profile celebrity demographics in a low-resource language like Urdu."}}
{"id": "2510.12493", "pdf": "https://arxiv.org/pdf/2510.12493", "abs": "https://arxiv.org/abs/2510.12493", "authors": ["An Zhao", "Piaopiao Yu", "Zhe Zhu", "Mingqiang Wei"], "title": "BSGS: Bi-stage 3D Gaussian Splatting for Camera Motion Deblurring", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting has exhibited remarkable capabilities in 3D scene\nreconstruction.However, reconstructing high-quality 3D scenes from\nmotion-blurred images caused by camera motion poses a significant challenge.The\nperformance of existing 3DGS-based deblurring methods are limited due to their\ninherent mechanisms, such as extreme dependence on the accuracy of camera poses\nand inability to effectively control erroneous Gaussian primitives\ndensification caused by motion blur.To solve these problems, we introduce a\nnovel framework, Bi-Stage 3D Gaussian Splatting, to accurately reconstruct 3D\nscenes from motion-blurred images.BSGS contains two stages. First, Camera Pose\nRefinement roughly optimizes camera poses to reduce motion-induced distortions.\nSecond, with fixed rough camera poses, Global RigidTransformation further\ncorrects motion-induced blur distortions.To alleviate multi-subframe gradient\nconflicts, we propose a subframe gradient aggregation strategy to optimize both\nstages.Furthermore, a space-time bi-stage optimization strategy is introduced\nto dynamically adjust primitive densification thresholds and prevent premature\nnoisy Gaussian generation in blurred regions. Comprehensive experiments verify\nthe effectiveness of our proposed deblurring method and show its superiority\nover the state of the arts.", "AI": {"tldr": "The paper proposes Bi-Stage 3D Gaussian Splatting (BSGS), a new framework for accurately reconstructing 3D scenes from motion-blurred images.", "motivation": "Reconstructing high-quality 3D scenes from motion-blurred images caused by camera movement poses challenges due to limitations in existing methods, such as reliance on precise camera poses and issues with Gaussian primitive errors.", "method": "BSGS employs two stages: Camera Pose Refinement to reduce distortions, and Global Rigid Transformation for further correction. It incorporates subframe gradient aggregation and a bi-stage space-time optimization strategy to improve performance.", "result": "Experiments demonstrate that BSGS effectively deblurs motion-blurred images and significantly outperforms existing state-of-the-art methods.", "conclusion": "The proposed BSGS framework addresses key limitations in previous methods, offering a robust solution for 3D scene reconstruction from motion-blurred images."}}
{"id": "2510.12334", "pdf": "https://arxiv.org/pdf/2510.12334", "abs": "https://arxiv.org/abs/2510.12334", "authors": ["Rui Hu", "Yu Chen", "Longbo Huang"], "title": "Finite-time Convergence Analysis of Actor-Critic with Evolving Reward", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Many popular practical reinforcement learning (RL) algorithms employ evolving\nreward functions-through techniques such as reward shaping, entropy\nregularization, or curriculum learning-yet their theoretical foundations remain\nunderdeveloped. This paper provides the first finite-time convergence analysis\nof a single-timescale actor-critic algorithm in the presence of an evolving\nreward function under Markovian sampling. We consider a setting where the\nreward parameters may change at each time step, affecting both policy\noptimization and value estimation. Under standard assumptions, we derive\nnon-asymptotic bounds for both actor and critic errors. Our result shows that\nan $O(1/\\sqrt{T})$ convergence rate is achievable, matching the best-known rate\nfor static rewards, provided the reward parameters evolve slowly enough. This\nrate is preserved when the reward is updated via a gradient-based rule with\nbounded gradient and on the same timescale as the actor and critic, offering a\ntheoretical foundation for many popular RL techniques. As a secondary\ncontribution, we introduce a novel analysis of distribution mismatch under\nMarkovian sampling, improving the best-known rate by a factor of $\\log^2T$ in\nthe static-reward case.", "AI": {"tldr": "The paper presents a finite-time convergence analysis of actor-critic algorithms under evolving reward functions, achieving an $O(1/\\sqrt{T})$ convergence rate under certain conditions, and offers improved distribution mismatch analysis.", "motivation": "To address the lack of theoretical foundations for RL algorithms employing evolving reward functions, such as reward shaping or curriculum learning.", "method": "The paper derives non-asymptotic error bounds for actor and critic under evolving reward parameters and introduces a novel analysis of distribution mismatch under Markovian sampling.", "result": "It demonstrates an $O(1/\\sqrt{T})$ convergence rate, provided the reward evolves slowly enough and achieves improved rates in analyzing distribution mismatch.", "conclusion": "The findings offer a theoretical basis for popular RL techniques utilizing evolving reward functions and improve upon existing distribution mismatch analysis methods."}}
{"id": "2510.12621", "pdf": "https://arxiv.org/pdf/2510.12621", "abs": "https://arxiv.org/abs/2510.12621", "authors": ["I\u00f1aki Lacunza", "Javier Garcia Gilabert", "Francesca De Luca Fornaciari", "Javier Aula-Blasco", "Aitor Gonzalez-Agirre", "Maite Melero", "Marta Villegas"], "title": "ACADATA: Parallel Dataset of Academic Data for Machine Translation", "categories": ["cs.CL"], "comment": null, "summary": "We present ACADATA, a high-quality parallel dataset for academic translation,\nthat consists of two subsets: ACAD-TRAIN, which contains approximately 1.5\nmillion author-generated paragraph pairs across 96 language directions and\nACAD-BENCH, a curated evaluation set of almost 6,000 translations covering 12\ndirections. To validate its utility, we fine-tune two Large Language Models\n(LLMs) on ACAD-TRAIN and benchmark them on ACAD-BENCH against specialized\nmachine-translation systems, general-purpose, open-weight LLMs, and several\nlarge-scale proprietary models. Experimental results demonstrate that\nfine-tuning on ACAD-TRAIN leads to improvements in academic translation quality\nby +6.1 and +12.4 d-BLEU points on average for 7B and 2B models respectively,\nwhile also improving long-context translation in a general domain by up to\n24.9% when translating out of English. The fine-tuned top-performing model\nsurpasses the best propietary and open-weight models on academic translation\ndomain. By releasing ACAD-TRAIN, ACAD-BENCH and the fine-tuned models, we\nprovide the community with a valuable resource to advance research in academic\ndomain and long-context translation.", "AI": {"tldr": "The paper introduces ACADATA, a dataset designed to enhance academic translations, including a training set (ACAD-TRAIN) and an evaluation set (ACAD-BENCH). Fine-tuned language models using ACAD-TRAIN demonstrate significant improvements in translation quality.", "motivation": "The motivation is to enhance the quality and accuracy of academic translations and long-context translations by providing a high-quality dataset and fine-tuned models.", "method": "A high-quality dataset ACADATA is created, comprising ACAD-TRAIN (1.5 million paragraph pairs) and ACAD-BENCH (6,000 curated translations). Large Language Models were fine-tuned on ACAD-TRAIN and benchmarked against various machine translation models.", "result": "Fine-tuning on ACAD-TRAIN improves translation performance by +6.1 and +12.4 d-BLEU points for 7B and 2B models, respectively. The approach also enhances general long-context translation by up to 24.9% and outperforms top proprietary models in academic translation.", "conclusion": "The datasets and fine-tuned models significantly improve academic translation performance, surpassing existing proprietary and open-weight models. They offer valuable resources for advancing research in academic and long-context translation."}}
{"id": "2510.11752", "pdf": "https://arxiv.org/pdf/2510.11752", "abs": "https://arxiv.org/abs/2510.11752", "authors": ["Zhiyu Wang", "Bingxin Zhou", "Jing Wang", "Yang Tan", "Weishu Zhao", "Pietro Li\u00f2", "Liang Hong"], "title": "Fast and Interpretable Protein Substructure Alignment via Optimal Transport", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "comment": null, "summary": "Proteins are essential biological macromolecules that execute life functions.\nLocal motifs within protein structures, such as active sites, are the most\ncritical components for linking structure to function and are key to\nunderstanding protein evolution and enabling protein engineering. Existing\ncomputational methods struggle to identify and compare these local structures,\nwhich leaves a significant gap in understanding protein structures and\nharnessing their functions. This study presents PLASMA, the first deep learning\nframework for efficient and interpretable residue-level protein substructure\nalignment. We reformulate the problem as a regularized optimal transport task\nand leverage differentiable Sinkhorn iterations. For a pair of input protein\nstructures, PLASMA outputs a clear alignment matrix with an interpretable\noverall similarity score. Through extensive quantitative evaluations and three\nbiological case studies, we demonstrate that PLASMA achieves accurate,\nlightweight, and interpretable residue-level alignment. Additionally, we\nintroduce PLASMA-PF, a training-free variant that provides a practical\nalternative when training data are unavailable. Our method addresses a critical\ngap in protein structure analysis tools and offers new opportunities for\nfunctional annotation, evolutionary studies, and structure-based drug design.\nReproducibility is ensured via our official implementation at\nhttps://github.com/ZW471/PLASMA-Protein-Local-Alignment.git.", "AI": {"tldr": "The paper introduces PLASMA, a deep learning framework for residue-level protein substructure alignment using optimal transport, offering accuracy and interpretability.", "motivation": "Existing computational methods struggle with identifying and comparing local protein motifs, hindering advancements in understanding protein functions and engineering.", "method": "The study reformulates protein substructure alignment as a regularized optimal transport task using differentiable Sinkhorn iterations, providing interpretable alignment matrices.", "result": "PLASMA achieves accurate, efficient, and interpretable alignment in quantitative evaluations and biological case studies, along with a training-free variant named PLASMA-PF.", "conclusion": "PLASMA fills a gap in protein structure analysis, enabling functional annotation, evolutionary studies, and drug design, with openly accessible implementation."}}
{"id": "2510.12524", "pdf": "https://arxiv.org/pdf/2510.12524", "abs": "https://arxiv.org/abs/2510.12524", "authors": ["Jiayi Kong", "Chen Zong", "Junkai Deng", "Xuhui Chen", "Fei Hou", "Shiqing Xin", "Junhui Hou", "Chen Qian", "Ying He"], "title": "Voronoi-Assisted Diffusion for Computing Unsigned Distance Fields from Unoriented Points", "categories": ["cs.CV"], "comment": null, "summary": "Unsigned Distance Fields (UDFs) provide a flexible representation for 3D\nshapes with arbitrary topology, including open and closed surfaces, orientable\nand non-orientable geometries, and non-manifold structures. While recent neural\napproaches have shown promise in learning UDFs, they often suffer from\nnumerical instability, high computational cost, and limited controllability. We\npresent a lightweight, network-free method, Voronoi-Assisted Diffusion (VAD),\nfor computing UDFs directly from unoriented point clouds. Our approach begins\nby assigning bi-directional normals to input points, guided by two\nVoronoi-based geometric criteria encoded in an energy function for optimal\nalignment. The aligned normals are then diffused to form an approximate UDF\ngradient field, which is subsequently integrated to recover the final UDF.\nExperiments demonstrate that VAD robustly handles watertight and open surfaces,\nas well as complex non-manifold and non-orientable geometries, while remaining\ncomputationally efficient and stable.", "AI": {"tldr": "This paper introduces Voronoi-Assisted Diffusion (VAD), a network-free method for efficiently computing unsigned distance fields (UDFs) from unoriented point clouds.", "motivation": "The motivation is to address the limitations of existing neural methods for learning UDFs, such as instability, high computational cost, and lack of control.", "method": "The method involves assigning bi-directional normals to points using Voronoi-based criteria, diffusing these normals to approximate a UDF gradient field, and then integrating it to obtain the final UDF.", "result": "VAD successfully handles diverse geometries, including watertight, open, non-manifold, and non-orientable structures, while being efficient and stable.", "conclusion": "The proposed VAD method offers a robust and computationally efficient alternative for UDF computation without relying on neural networks."}}
{"id": "2510.12343", "pdf": "https://arxiv.org/pdf/2510.12343", "abs": "https://arxiv.org/abs/2510.12343", "authors": ["Donghwan Rho", "Sieun Seo", "Hyewon Sung", "Chohong Min", "Ernest K. Ryu"], "title": "Traveling Salesman-Based Token Ordering Improves Stability in Homomorphically Encrypted Language Models", "categories": ["cs.LG", "cs.CR"], "comment": "34 pages", "summary": "As users increasingly interact with large language models (LLMs) using\nprivate information, secure and encrypted communication becomes essential.\nHomomorphic encryption (HE) provides a principled solution by enabling\ncomputation directly on encrypted data. Although prior work has explored\naspects of running LLMs under HE, the challenge of text generation,\nparticularly next-token prediction, has received limited attention and remains\na key obstacle to practical encrypted interaction. In this work, we propose a\nTSP-based token reordering strategy to address the difficulties of encrypted\ntext generation, together with a post-processing step that further reduces\napproximation error. Theoretical analysis and experimental results demonstrate\nthat our method prevents collapse, improves coherence in generated text, and\npreserves data privacy throughout. Overall, our contributions advance the\nfeasibility of practical and privacy-preserving LLM inference.", "AI": {"tldr": "The paper explores text generation using encrypted interaction with large language models (LLMs) and proposes a solution utilizing homomorphic encryption.", "motivation": "The study aims to overcome the challenge of generating next-token predictions securely for privacy-preserving interaction with LLMs via homomorphic encryption.", "method": "The authors introduce a TSP-based token reordering strategy alongside a post-processing step to enhance text generation under homomorphic encryption.", "result": "Their approach effectively prevents collapse, improves coherence in text generation, and preserves privacy, as evidenced by theoretical analysis and experimental results.", "conclusion": "The research highlights advancements towards practical and secure application of LLMs through improved encrypted interaction techniques."}}
{"id": "2510.12637", "pdf": "https://arxiv.org/pdf/2510.12637", "abs": "https://arxiv.org/abs/2510.12637", "authors": ["Nzubechukwu C. Ohalete", "Kevin B. Gittner", "Lauren M. Matheny"], "title": "COSTAR-A: A prompting framework for enhancing Large Language Model performance on Point-of-View questions", "categories": ["cs.CL", "I.2.7"], "comment": "20 pages, 2 figures", "summary": "Large Language Models (LLMs) are highly sensitive to prompt design, and\nmaking optimized prompting techniques is crucial for generating consistent,\nhigh-quality outputs. In this study, we introduce COSTAR-A, a novel prompt\nengineering framework that enhances the existing COSTAR method, which stands\nfor Context, Objective, Style, Tone, Audience, and Response, by adding the\n'Answer' component at the end. We demonstrate that while the original COSTAR\nframework improves prompt clarity and aligns outputs for larger LLMs, its\nperformance is less consistent with smaller, locally optimized models,\nparticularly in tasks that require more directive or constrained outputs.\nThrough a series of controlled prompt-output assessments with smaller (at most\n8 billion parameters), fine-tuned models, we found that COSTAR-A can enhance\nthe output structure and decisiveness of localized LLMs for certain tasks,\nalthough its effectiveness varies across models and use cases. Notably, the\nLlama 3.1-8B model exhibited performance improvements when prompted with\nCOSTAR-A compared to COSTAR alone. These findings emphasize the adaptability\nand scalability of COSTAR-A as a prompting framework, particularly in\ncomputationally efficient AI deployments on resource-constrained hardware.", "AI": {"tldr": "The paper presents COSTAR-A, a refined prompt engineering framework, which improves upon the original COSTAR prompting method by adding an 'Answer' component, optimizing outputs for smaller LLMs.", "motivation": "Existing prompting techniques face challenges in optimizing outputs across different scales of LLMs, particularly smaller, localized versions. This paper aims to address inconsistent performance and adaptability issues.", "method": "The study refines the original COSTAR framework by adding the 'Answer' component, conducts controlled experiments across different fine-tuned LLMs, and evaluates output consistency and structure.", "result": "COSTAR-A improved output clarity and quality for smaller LLMs, like Llama 3.1-8B, in certain tasks compared to the original COSTAR framework. However, effectiveness varied by model and use case.", "conclusion": "COSTAR-A provides a scalable and adaptable prompting approach, especially tailored for smaller, resource-constrained models, enriching computational efficiency in those setups."}}
{"id": "2510.12537", "pdf": "https://arxiv.org/pdf/2510.12537", "abs": "https://arxiv.org/abs/2510.12537", "authors": ["David Bj\u00f6rkstrand", "Tiesheng Wang", "Lars Bretzner", "Josephine Sullivan"], "title": "Unconditional Human Motion and Shape Generation via Balanced Score-Based Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent work has explored a range of model families for human motion\ngeneration, including Variational Autoencoders (VAEs), Generative Adversarial\nNetworks (GANs), and diffusion-based models. Despite their differences, many\nmethods rely on over-parameterized input features and auxiliary losses to\nimprove empirical results. These strategies should not be strictly necessary\nfor diffusion models to match the human motion distribution. We show that on\npar with state-of-the-art results in unconditional human motion generation are\nachievable with a score-based diffusion model using only careful feature-space\nnormalization and analytically derived weightings for the standard L2\nscore-matching loss, while generating both motion and shape directly, thereby\navoiding slow post hoc shape recovery from joints. We build the method step by\nstep, with a clear theoretical motivation for each component, and provide\ntargeted ablations demonstrating the effectiveness of each proposed addition in\nisolation.", "AI": {"tldr": "This paper proposes a streamlined approach using score-based diffusion models for human motion generation, relying on minimal input feature engineering and introducing precise feature-normalization and weighting strategies.", "motivation": "Current models for human motion generation often depend on over-parameterized input features and auxiliary losses, which may be redundant in achieving accurate motion modeling with diffusion models.", "method": "The study uses a score-based diffusion model with feature-space normalization and analytically derived weightings for L2 score-matching loss, eliminating the need for additional postprocessing steps like recovering shape from joints.", "result": "State-of-the-art results in unconditional human motion generation are achieved, showcasing comparability with more complex methods while being more efficient.", "conclusion": "Carefully crafted normalization and loss weighting within diffusion models can simplify and streamline human motion generation while maintaining high performance, as validated through targeted ablation studies."}}
{"id": "2510.12383", "pdf": "https://arxiv.org/pdf/2510.12383", "abs": "https://arxiv.org/abs/2510.12383", "authors": ["Olga Ovcharenko", "Sebastian Schelter"], "title": "Towards Cross-Modal Error Detection with Tables and Images", "categories": ["cs.LG"], "comment": null, "summary": "Ensuring data quality at scale remains a persistent challenge for large\norganizations. Despite recent advances, maintaining accurate and consistent\ndata is still complex, especially when dealing with multiple data modalities.\nTraditional error detection and correction methods tend to focus on a single\nmodality, typically a table, and often miss cross-modal errors that are common\nin domains like e-Commerce and healthcare, where image, tabular, and text data\nco-exist. To address this gap, we take an initial step towards cross-modal\nerror detection in tabular data, by benchmarking several methods. Our\nevaluation spans four datasets and five baseline approaches. Among them,\nCleanlab, a label error detection framework, and DataScope, a data valuation\nmethod, perform the best when paired with a strong AutoML framework, achieving\nthe highest F1 scores. Our findings indicate that current methods remain\nlimited, particularly when applied to heavy-tailed real-world data, motivating\nfurther research in this area.", "AI": {"tldr": "This paper evaluates methods for cross-modal error detection in tabular data, specifically in contexts involving image, tabular, and text data.", "motivation": "The paper aims to solve the challenge of detecting and correcting data errors, particularly cross-modal inconsistencies, which are common in industries like e-Commerce and healthcare.", "method": "The authors benchmarked several methods for cross-modal error detection using four datasets and five baseline approaches, highlighting the performance of Cleanlab and DataScope when paired with a strong AutoML framework.", "result": "Cleanlab and DataScope achieve the highest F1 scores in the study, but overall, current methods show limitations when dealing with complex, real-world data.", "conclusion": "There is a need for improved techniques in cross-modal error detection due to the limitations of existing methods, particularly in handling heavy-tailed, real-world scenarios."}}
{"id": "2510.12643", "pdf": "https://arxiv.org/pdf/2510.12643", "abs": "https://arxiv.org/abs/2510.12643", "authors": ["Chaoxu Pang", "Yixuan Cao", "Ping Luo"], "title": "Reasoning Pattern Matters: Learning to Reason without Human Rationales", "categories": ["cs.CL", "cs.AI"], "comment": "Submitted to Frontiers of Computer Science", "summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning\ncapabilities under the widely adopted SFT+RLVR paradigm, which first performs\nSupervised Fine-Tuning (SFT) on human-annotated reasoning trajectories\n(rationales) to establish initial reasoning behaviors, then applies\nReinforcement Learning with Verifiable Rewards (RLVR) to optimize the model\nusing verifiable signals without golden rationales. However, annotating\nhigh-quality rationales for the SFT stage remains prohibitively expensive. This\npaper investigates when and how rationale annotation costs can be substantially\nreduced without compromising reasoning performance. We identify a broad class\nof problems, termed patterned reasoning tasks, where reasoning follows a fixed,\nprocedural strategy consistent across instances. Although instances vary in\ncontent such as domain knowledge, factual information, or numeric values, the\nsolution derives from applying a shared reasoning pattern. We argue that the\nsuccess of SFT+RLVR on such tasks primarily stems from its ability to enable\nmodels to internalize these reasoning patterns. Using numerical semantic\nmatching as a representative task, we provide both causal and behavioral\nevidence showing that reasoning patterns rather than the quantity or quality of\nrationales are the key determinant of performance. Building on these insights,\nwe propose Pattern-Aware LLMs as Rationale AnnOtators (PARO), a simple yet\neffective framework that enables LLMs to generate rationales aligned with\ntask-specific reasoning patterns without requiring human rationale annotations.\nExperiments show that PARO-generated rationales achieve comparable SFT+RLVR\nperformance to human rationales that are 10 times larger. These results suggest\nthat large-scale human rationale annotations can be replaced with LLM-based\nautomatic annotations requiring only limited human supervision over reasoning\npatterns.", "AI": {"tldr": "This paper addresses cost reduction in rationale annotations for reasoning tasks in LLMs by leveraging automated rationale generation tools.", "motivation": "Creating high-quality human rationale annotations is resource-intensive and expensive, particularly for patterned reasoning tasks.", "method": "Using numerical semantic matching, the authors introduce PARO, a framework enabling LLMs to generate rationales based on reasoning patterns with minimal human supervision.", "result": "PARO-generated rationales perform comparably to human-generated rationales despite being significantly smaller in scale, reducing annotation costs.", "conclusion": "LLM-based automated rationale generation offers a viable alternative to human annotations while maintaining reasoning performance standards in large language model training."}}
{"id": "2510.11755", "pdf": "https://arxiv.org/pdf/2510.11755", "abs": "https://arxiv.org/abs/2510.11755", "authors": ["Ananth Hariharan"], "title": "Artificial Intelligence for Optimal Learning: A Comparative Approach towards AI-Enhanced Learning Environments", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "In the rapidly evolving educational landscape, the integration of technology\nhas shifted from an enhancement to a cornerstone of educational strategy\nworldwide. This transition is propelled by advancements in digital technology,\nespecially the emergence of artificial intelligence as a crucial tool in\nlearning environments. This research project critically evaluates the impact of\nthree distinct educational settings: traditional educational methods without\ntechnological integration, those enhanced by non-AI technology, and those\nutilising AI-driven technologies. This comparison aims to assess how each\nenvironment influences educational outcomes, engagement, pedagogical methods,\nand equity in access to learning resources, and how each contributes uniquely\nto the learning experience. The ultimate goal of this research is to synthesise\nthe strengths of each model to create a more holistic educational approach. By\nintegrating the personal interaction and tested pedagogical techniques of\ntraditional classrooms, the enhanced accessibility and collaborative tools\noffered by non-AI technology, and the personalised, adaptive learning\nstrategies enabled by AI-driven technologies, education systems can develop\nricher, more effective learning environments. This hybrid approach aims to\nleverage the best elements of each setting, thereby enhancing educational\noutcomes, engagement, and inclusiveness, while also addressing the distinct\nchallenges and limitations inherent in each model. The intention is to create\nan educational framework deeply attentive to the diverse needs of students,\nensuring equitable access to high-quality education for all.", "AI": {"tldr": "This paper evaluates traditional education, technology-enhanced methods, and AI-driven learning to create a hybrid educational approach for equitable, effective learning.", "motivation": "To address how different educational methods and technologies influence learning outcomes, engagement, and inclusivity and to design a holistic framework for education.", "method": "A comparative study of traditional education, non-AI technology-enhanced learning, and AI-driven education to assess their unique contributions and limitations.", "result": "The study identifies the strengths and challenges of each learning model and suggests combining their benefits for an improved hybrid educational system.", "conclusion": "Integrating traditional, non-AI, and AI-driven educational methods can create a more inclusive, effective, and adaptable learning environment that serves diverse student needs."}}
{"id": "2510.12401", "pdf": "https://arxiv.org/pdf/2510.12401", "abs": "https://arxiv.org/abs/2510.12401", "authors": ["Shengyin Sun", "Chen Ma", "Jiehao Chen"], "title": "Enhanced Pre-training of Graph Neural Networks for Million-Scale Heterogeneous Graphs", "categories": ["cs.LG"], "comment": "26 pages", "summary": "In recent years, graph neural networks (GNNs) have facilitated the\ndevelopment of graph data mining. However, training GNNs requires sufficient\nlabeled task-specific data, which is expensive and sometimes unavailable. To be\nless dependent on labeled data, recent studies propose to pre-train GNNs in a\nself-supervised manner and then apply the pre-trained GNNs to downstream tasks\nwith limited labeled data. However, most existing methods are designed solely\nfor homogeneous graphs (real-world graphs are mostly heterogeneous) and do not\nconsider semantic mismatch (the semantic difference between the original data\nand the ideal data containing more transferable semantic information). In this\npaper, we propose an effective framework to pre-train GNNs on the large-scale\nheterogeneous graph. We first design a structure-aware pre-training task, which\naims to capture structural properties in heterogeneous graphs. Then, we design\na semantic-aware pre-training task to tackle the mismatch. Specifically, we\nconstruct a perturbation subspace composed of semantic neighbors to help deal\nwith the semantic mismatch. Semantic neighbors make the model focus more on the\ngeneral knowledge in the semantic space, which in turn assists the model in\nlearning knowledge with better transferability. Finally, extensive experiments\nare conducted on real-world large-scale heterogeneous graphs to demonstrate the\nsuperiority of the proposed method over state-of-the-art baselines. Code\navailable at https://github.com/sunshy-1/PHE.", "AI": {"tldr": "The paper introduces a self-supervised framework to pre-train graph neural networks (GNNs) on large-scale heterogeneous graphs, addressing both structural properties and semantic mismatch issues, achieving superior results on real-world datasets.", "motivation": "To address the limitations of dependency on labeled graph data for training GNNs, especially in heterogeneous graphs, and tackle semantic mismatch for improved transferability.", "method": "A framework with two pre-training tasks: (1) structure-aware task to learn structural properties of heterogeneous graphs, and (2) semantic-aware task using perturbation subspace and semantic neighbors to handle semantic mismatch.", "result": "Extensive experiments on large-scale heterogeneous graphs show that the proposed method outperforms state-of-the-art baselines.", "conclusion": "The framework effectively pre-trains GNNs on heterogeneous graphs by capturing structural and semantic properties, enhancing transferability and applicability in downstream tasks."}}
{"id": "2510.12699", "pdf": "https://arxiv.org/pdf/2510.12699", "abs": "https://arxiv.org/abs/2510.12699", "authors": ["Sunny Yu", "Ahmad Jabbar", "Robert Hawkins", "Dan Jurafsky", "Myra Cheng"], "title": "Generation Space Size: Understanding and Calibrating Open-Endedness of LLM Generations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Different open-ended generation tasks require different degrees of output\ndiversity. However, current LLMs are often miscalibrated. They collapse to\noverly homogeneous outputs for creative tasks and hallucinate diverse but\nincorrect responses for factual tasks. We argue that these two failure modes\nare unified by, and can both be addressed by, the notion of effective\ngeneration space size (GSS) -- the set of semantically distinct outputs a model\nconsiders for a prompt. We present GSSBench, a task suite of prompt pairs with\nground-truth GSS relationships to assess different metrics and understand where\nmodels diverge from desired behavior. We find that hallucination detection\nmetrics, particularly EigenScore, consistently outperform standard diversity\nand uncertainty quantification metrics, while using only model internals,\nproviding interpretable insights into a model's internal task representations.\nWe demonstrate three applications of GSS: (1) detecting prompt ambiguity and\npredicting clarification questions for better grounding, (2) interpreting\noverthinking and underthinking in reasoning models, and (3) steering models to\nexpand their generation space to yield high-quality and diverse outputs.", "AI": {"tldr": "The paper introduces the concept of 'generation space size' (GSS) to address issues of miscalibration in large language models (LLMs) and presents GSSBench, a benchmark to assess and improve diverse generation in creative and factual tasks.", "motivation": "LLMs struggle with miscalibrated output diversity, producing overly uniform outputs for creative tasks and overly diverse but inaccurate outputs for factual tasks. This points to the need for a principled approach to balance generation diversity.", "method": "The paper introduces GSS (generation space size) as a way to measure and calibrate model outputs. It evaluates this using GSSBench, a benchmark composed of prompt pairs with ground-truth GSS relationships, and uses metrics like EigenScore to analyze model behavior.", "result": "The study found that hallucination detection metrics, especially EigenScore, outperform standard metrics in measuring GSS and provide insights into how models internally represent tasks. GSS was also applied successfully to address prompt ambiguity, reasoning issues, and generating diverse outputs.", "conclusion": "GSS and associated tools offer a unified framework to assess and steer LLMs for better, more task-appropriate diversity in outputs, thereby addressing pressing issues like hallucination and creativity miscalibration."}}
{"id": "2510.11758", "pdf": "https://arxiv.org/pdf/2510.11758", "abs": "https://arxiv.org/abs/2510.11758", "authors": ["Shumin Li", "Xiaoyun Lai"], "title": "The Adoption Paradox: A Comparative Analysis of Veterinary AI Adoption in China and the North America", "categories": ["cs.CY", "cs.AI"], "comment": "1 Table, 5 Figures (included in the end), Full questionnaire used in\n  this study (both original Chinese version and translated/English version\n  included in the end)", "summary": "This study compares the perception, adoption, and application of artificial\nintelligence (AI) among veterinary professionals in China and North America\n(NA), testing the hypothesis that adoption patterns are shaped by regional\nmarket and demographic factors. A descriptive, cross-sectional survey was\nconducted with 455 veterinary professionals in China between May and July 2025.\nThe results were compared with published data from a 2024 survey of 3,968\nveterinary professionals in the United States and Canada. The Chinese cohort,\nprimarily composed of clinicians (81.5%), showed a high AI adoption rate\n(71.0%) despite low familiarity (55.4%). Their AI use was focused on clinical\ntasks, such as disease diagnosis (50.1%) and prescription calculation (44.8%).\nIn contrast, the NA cohort reported high familiarity (83.8%) but a lower\nadoption rate (39.2%). Their priorities were administrative, including imaging\nanalysis (39.0%) and record-keeping (39.0%). Concerns about AI reliability and\naccuracy were the top barrier in both groups. Our findings reveal an \"adoption\nparadox\" where the Chinese market demonstrates a practitioner-driven, bottom-up\nadoption model focused on augmenting clinical efficacy, while the NA market\nshows a more cautious, structured, top-down integration aimed at improving\nadministrative efficiency. This suggests that a one-size-fits-all approach to\nAI development and integration is insufficient, and tailored, region-specific\nstrategies are necessary to responsibly incorporate AI into global veterinary\npractice.", "AI": {"tldr": "The study explores how AI is adopted in veterinary fields in China and North America, finding Chinese practitioners focus on clinical use despite low familiarity, while North Americans prioritize administrative tasks with high familiarity but lower adoption.", "motivation": "To examine differences in AI adoption patterns in veterinary practice between China and North America, influenced by regional market and demographic factors.", "method": "A descriptive, cross-sectional survey was conducted on 455 veterinary professionals in China and compared with previously published data from a 2024 North American survey of 3,968 participants.", "result": "The Chinese cohort showed high AI adoption (71.0%) despite low familiarity, focusing on clinical tasks. The North American cohort had high familiarity but lower adoption (39.2%), focusing on administrative tasks. Concerns about AI reliability were a common barrier in both regions.", "conclusion": "AI integration in veterinary practice requires tailored strategies specific to regional needs, as one-size-fits-all approaches fail to address differing market motivations and adoption behaviors effectively."}}
{"id": "2510.12565", "pdf": "https://arxiv.org/pdf/2510.12565", "abs": "https://arxiv.org/abs/2510.12565", "authors": ["Tianhao Li", "Tingfa Xu", "Ying Wang", "Haolin Qin", "Xu Lin", "Jianan Li"], "title": "MMOT: The First Challenging Benchmark for Drone-based Multispectral Multi-Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Drone-based multi-object tracking is essential yet highly challenging due to\nsmall targets, severe occlusions, and cluttered backgrounds. Existing RGB-based\ntracking algorithms heavily depend on spatial appearance cues such as color and\ntexture, which often degrade in aerial views, compromising reliability.\nMultispectral imagery, capturing pixel-level spectral reflectance, provides\ncrucial cues that enhance object discriminability under degraded spatial\nconditions. However, the lack of dedicated multispectral UAV datasets has\nhindered progress in this domain. To bridge this gap, we introduce MMOT, the\nfirst challenging benchmark for drone-based multispectral multi-object\ntracking. It features three key characteristics: (i) Large Scale - 125 video\nsequences with over 488.8K annotations across eight categories; (ii)\nComprehensive Challenges - covering diverse conditions such as extreme small\ntargets, high-density scenarios, severe occlusions, and complex motion; and\n(iii) Precise Oriented Annotations - enabling accurate localization and reduced\nambiguity under aerial perspectives. To better extract spectral features and\nleverage oriented annotations, we further present a multispectral and\norientation-aware MOT scheme adapting existing methods, featuring: (i) a\nlightweight Spectral 3D-Stem integrating spectral features while preserving\ncompatibility with RGB pretraining; (ii) an orientation-aware Kalman filter for\nprecise state estimation; and (iii) an end-to-end orientation-adaptive\ntransformer. Extensive experiments across representative trackers consistently\nshow that multispectral input markedly improves tracking performance over RGB\nbaselines, particularly for small and densely packed objects. We believe our\nwork will advance drone-based multispectral multi-object tracking research. Our\nMMOT, code, and benchmarks are publicly available at\nhttps://github.com/Annzstbl/MMOT.", "AI": {"tldr": "The paper addresses the challenges in drone-based multi-object tracking (MOT) and introduces MMOT, a benchmark dataset for multispectral tracking, while presenting new methods that enhance tracking performance under difficult conditions.", "motivation": "The motivation is to overcome the limitations of RGB-based tracking methods in aerial views, especially in scenarios involving small targets, occlusions, and cluttered environments, by utilizing multispectral data.", "method": "The study introduces MMOT, a multispectral UAV dataset and proposes techniques like Spectral 3D-Stem, orientation-aware Kalman filter, and an orientation-adaptive transformer to improve multispectral tracking.", "result": "Experiments showed that multispectral input significantly enhances tracking performance compared to RGB-based methods, particularly in challenging scenarios like tracking small, densely packed objects.", "conclusion": "The research advances drone-based multispectral MOT by providing a benchmark dataset, new methods, and insights to address tracking challenges, promising improvements in this field."}}
{"id": "2510.12720", "pdf": "https://arxiv.org/pdf/2510.12720", "abs": "https://arxiv.org/abs/2510.12720", "authors": ["Ziyang Ma", "Ruiyang Xu", "Zhenghao Xing", "Yunfei Chu", "Yuxuan Wang", "Jinzheng He", "Jin Xu", "Pheng-Ann Heng", "Kai Yu", "Junyang Lin", "Eng Siong Chng", "Xie Chen"], "title": "Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception", "categories": ["cs.CL", "cs.CV", "cs.MM", "cs.SD"], "comment": "https://github.com/ddlBoJack/Omni-Captioner", "summary": "Fine-grained perception of multimodal information is critical for advancing\nhuman-AI interaction. With recent progress in audio-visual technologies, Omni\nLanguage Models (OLMs), capable of processing audio and video signals in\nparallel, have emerged as a promising paradigm for achieving richer\nunderstanding and reasoning. However, their capacity to capture and describe\nfine-grained details remains limited explored. In this work, we present a\nsystematic and comprehensive investigation of omni detailed perception from the\nperspectives of the data pipeline, models, and benchmark. We first identify an\ninherent \"co-growth\" between detail and hallucination in current OLMs. To\naddress this, we propose Omni-Detective, an agentic data generation pipeline\nintegrating tool-calling, to autonomously produce highly detailed yet minimally\nhallucinatory multimodal data. Based on the data generated with Omni-Detective,\nwe train two captioning models: Audio-Captioner for audio-only detailed\nperception, and Omni-Captioner for audio-visual detailed perception. Under the\ncascade evaluation protocol, Audio-Captioner achieves the best performance on\nMMAU and MMAR among all open-source models, surpassing Gemini 2.5 Flash and\ndelivering performance comparable to Gemini 2.5 Pro. On existing detailed\ncaptioning benchmarks, Omni-Captioner sets a new state-of-the-art on VDC and\nachieves the best trade-off between detail and hallucination on the\nvideo-SALMONN 2 testset. Given the absence of a dedicated benchmark for omni\ndetailed perception, we design Omni-Cloze, a novel cloze-style evaluation for\ndetailed audio, visual, and audio-visual captioning that ensures stable,\nefficient, and reliable assessment. Experimental results and analysis\ndemonstrate the effectiveness of Omni-Detective in generating high-quality\ndetailed captions, as well as the superiority of Omni-Cloze in evaluating such\ndetailed captions.", "AI": {"tldr": "The study explores fine-grained perception in Omni Language Models (OLMs) and introduces methods to enhance detailed audio-visual understanding.", "motivation": "To advance human-AI interaction by improving fine-grained understanding and reasoning in multimodal (audio-visual) information processing.", "method": "Proposed an agentic data generation pipeline, Omni-Detective, to create detailed and minimally hallucinatory data; trained two captioning models (Audio-Captioner and Omni-Captioner); developed Omni-Cloze as a benchmark for assessing detailed perception.", "result": "Achieved state-of-the-art performance in detailed captioning tasks, surpassing or matching leading models on benchmarks like MMAU, MMAR, VDC, and video-SALMONN 2.", "conclusion": "Omni-Detective effectively enhances detailed perception while reducing hallucination, and the Omni-Cloze benchmark offers reliable evaluation of detailed captions."}}
{"id": "2510.12573", "pdf": "https://arxiv.org/pdf/2510.12573", "abs": "https://arxiv.org/abs/2510.12573", "authors": ["Quang Nguyen", "Tri Le", "Baoru Huang", "Minh Nhat Vu", "Ngan Le", "Thieu Vo", "Anh Nguyen"], "title": "Learning Human Motion with Temporally Conditional Mamba", "categories": ["cs.CV"], "comment": "10 pages", "summary": "Learning human motion based on a time-dependent input signal presents a\nchallenging yet impactful task with various applications. The goal of this task\nis to generate or estimate human movement that consistently reflects the\ntemporal patterns of conditioning inputs. Existing methods typically rely on\ncross-attention mechanisms to fuse the condition with motion. However, this\napproach primarily captures global interactions and struggles to maintain\nstep-by-step temporal alignment. To address this limitation, we introduce\nTemporally Conditional Mamba, a new mamba-based model for human motion\ngeneration. Our approach integrates conditional information into the recurrent\ndynamics of the Mamba block, enabling better temporally aligned motion. To\nvalidate the effectiveness of our method, we evaluate it on a variety of human\nmotion tasks. Extensive experiments demonstrate that our model significantly\nimproves temporal alignment, motion realism, and condition consistency over\nstate-of-the-art approaches. Our project page is available at\nhttps://zquang2202.github.io/TCM.", "AI": {"tldr": "This paper introduces Temporally Conditional Mamba, a new model for human motion generation addressing limitations in temporal alignment using an improved fusion approach.", "motivation": "The need to reliably generate or estimate human motion aligned with time-dependent inputs due to challenges in maintaining step-by-step temporal alignment with existing methods.", "method": "Developed a Mamba-based model that integrates conditional inputs into recurrent dynamics to improve temporally aligned human motion generation.", "result": "Experimental validation shows significant improvements in temporal alignment, motion realism, and consistency compared to state-of-the-art methods.", "conclusion": "The proposed model demonstrates enhanced motion generation capabilities, showcasing its effectiveness in addressing temporal conditioning challenges."}}
{"id": "2510.12405", "pdf": "https://arxiv.org/pdf/2510.12405", "abs": "https://arxiv.org/abs/2510.12405", "authors": ["Masahiro Negishi", "Hyunsoo Park", "Kinga O. Mastej", "Aron Walsh"], "title": "Continuous Uniqueness and Novelty Metrics for Generative Modeling of Inorganic Crystals", "categories": ["cs.LG", "cond-mat.mtrl-sci", "68T07", "J.2; I.2.0"], "comment": "13 pages (5 pages of main text), accepted to the AI4Mat workshop at\n  NeurIPS 2025. See https://github.com/WMD-group/xtalmet for the code", "summary": "To address pressing scientific challenges such as climate change,\nincreasingly sophisticated generative artificial intelligence models are being\ndeveloped that can efficiently sample the large chemical space of possible\nfunctional materials. These models can quickly sample new chemical compositions\npaired with crystal structures. They are typically evaluated using uniqueness\nand novelty metrics, which depend on a chosen crystal distance function.\nHowever, the most prevalent distance function has four limitations: it fails to\nquantify the degree of similarity between compounds, cannot distinguish\ncompositional difference and structural difference, lacks Lipschitz continuity\nagainst shifts in atomic coordinates, and results in a uniqueness metric that\nis not invariant against the permutation of generated samples. In this work, we\npropose using two continuous distance functions to evaluate uniqueness and\nnovelty, which theoretically overcome these limitations. Our experiments show\nthat these distances reveal insights missed by traditional distance functions,\nproviding a more reliable basis for evaluating and comparing generative models\nfor inorganic crystals.", "AI": {"tldr": "The paper proposes two new continuous distance functions to overcome the limitations of traditional distance functions used to evaluate generative AI models for functional materials.", "motivation": "Address limitations in current distance functions for evaluating generative AI models for functional materials, which fail to adequately quantify similarity, distinguish differences, maintain continuity, and ensure sample permutation invariance.", "method": "Introduce two continuous distance functions that address the shortcomings of traditional distance metrics in evaluating generative models for inorganic crystals.", "result": "The proposed distance functions reveal new insights that traditional metrics fail to capture, offering a more reliable evaluation of generative models for inorganic crystals.", "conclusion": "The new continuous distance functions improve the evaluation of generative models for inorganic crystals, offering a more effective and reliable framework compared to traditional metrics."}}
{"id": "2510.12722", "pdf": "https://arxiv.org/pdf/2510.12722", "abs": "https://arxiv.org/abs/2510.12722", "authors": ["Nadine El-Naggar", "Tatsuki Kuribayashi", "Ted Briscoe"], "title": "Which Word Orders Facilitate Length Generalization in LMs? An Investigation with GCG-Based Artificial Languages", "categories": ["cs.CL"], "comment": "EMNLP 2025 Main Conference", "summary": "Whether language models (LMs) have inductive biases that favor typologically\nfrequent grammatical properties over rare, implausible ones has been\ninvestigated, typically using artificial languages (ALs) (White and Cotterell,\n2021; Kuribayashi et al., 2024). In this paper, we extend these works from two\nperspectives. First, we extend their context-free AL formalization by adopting\nGeneralized Categorial Grammar (GCG) (Wood, 2014), which allows ALs to cover\nattested but previously overlooked constructions, such as unbounded dependency\nand mildly context-sensitive structures. Second, our evaluation focuses more on\nthe generalization ability of LMs to process unseen longer test sentences.\nThus, our ALs better capture features of natural languages and our experimental\nparadigm leads to clearer conclusions -- typologically plausible word orders\ntend to be easier for LMs to productively generalize.", "AI": {"tldr": "The paper explores whether language models (LMs) prefer typologically common grammatical properties over rare ones using artificial languages enriched with advanced constructions.", "motivation": "To understand if LMs inherently favor frequent, typologically plausible grammatical structures over unlikely ones.", "method": "Extended artificial language formalization using Generalized Categorial Grammar and focused evaluation on LM generalization towards unseen, longer test sentences.", "result": "LMs demonstrated better generalization ability for typologically plausible word orders, indicating alignment with linguistic typology.", "conclusion": "LMs exhibit inductive biases favoring typologically plausible grammatical structures, showing their alignment with properties of natural languages."}}
{"id": "2510.11760", "pdf": "https://arxiv.org/pdf/2510.11760", "abs": "https://arxiv.org/abs/2510.11760", "authors": ["Yi Wang", "Yinfeng Yu", "Fuchun Sun", "Liejun Wang", "Wendong Zheng"], "title": "Audio-Guided Visual Perception for Audio-Visual Navigation", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM"], "comment": "Main paper (6 pages). Accepted for publication by International\n  Conference on Virtual Reality and Visualization 2025 (ICVRV 2025)", "summary": "Audio-Visual Embodied Navigation aims to enable agents to autonomously\nnavigate to sound sources in unknown 3D environments using auditory cues. While\ncurrent AVN methods excel on in-distribution sound sources, they exhibit poor\ncross-source generalization: navigation success rates plummet and search paths\nbecome excessively long when agents encounter unheard sounds or unseen\nenvironments. This limitation stems from the lack of explicit alignment\nmechanisms between auditory signals and corresponding visual regions. Policies\ntend to memorize spurious \\enquote{acoustic fingerprint-scenario} correlations\nduring training, leading to blind exploration when exposed to novel sound\nsources. To address this, we propose the AGVP framework, which transforms sound\nfrom policy-memorable acoustic fingerprint cues into spatial guidance. The\nframework first extracts global auditory context via audio self-attention, then\nuses this context as queries to guide visual feature attention, highlighting\nsound-source-related regions at the feature level. Subsequent temporal modeling\nand policy optimization are then performed. This design, centered on\ninterpretable cross-modal alignment and region reweighting, reduces dependency\non specific acoustic fingerprints. Experimental results demonstrate that AGVP\nimproves both navigation efficiency and robustness while achieving superior\ncross-scenario generalization on previously unheard sounds.", "AI": {"tldr": "This paper introduces the AGVP framework for Audio-Visual Navigation to address poor generalization to unheard sounds and unseen environments, using cross-modal alignment to improve navigation efficiency.", "motivation": "Current methods for Audio-Visual Navigation lack explicit mechanisms to align auditory signals with visual regions, causing ineffective navigation for new sound sources or scenarios.", "method": "The AGVP framework employs audio self-attention to extract auditory context, which is used to guide visual attention and perform cross-modal alignment, followed by temporal modeling and policy optimization.", "result": "The proposed AGVP framework enhances navigation efficiency, robustness, and cross-scenario generalization, especially against unheard sound sources.", "conclusion": "AGVP successfully mitigates reliance on acoustic fingerprints by providing spatial auditory guidance, making agents more capable of navigating in novel environments."}}
{"id": "2510.12579", "pdf": "https://arxiv.org/pdf/2510.12579", "abs": "https://arxiv.org/abs/2510.12579", "authors": ["Simon Rav\u00e9", "Jean-Christophe Lombardo", "Pejman Rasti", "Alexis Joly", "David Rousseau"], "title": "Unlocking Zero-Shot Plant Segmentation with Pl@ntNet Intelligence", "categories": ["cs.CV"], "comment": null, "summary": "We present a zero-shot segmentation approach for agricultural imagery that\nleverages Plantnet, a large-scale plant classification model, in conjunction\nwith its DinoV2 backbone and the Segment Anything Model (SAM). Rather than\ncollecting and annotating new datasets, our method exploits Plantnet's\nspecialized plant representations to identify plant regions and produce coarse\nsegmentation masks. These masks are then refined by SAM to yield detailed\nsegmentations. We evaluate on four publicly available datasets of various\ncomplexity in terms of contrast including some where the limited size of the\ntraining data and complex field conditions often hinder purely supervised\nmethods. Our results show consistent performance gains when using\nPlantnet-fine-tuned DinoV2 over the base DinoV2 model, as measured by the\nJaccard Index (IoU). These findings highlight the potential of combining\nfoundation models with specialized plant-centric models to alleviate the\nannotation bottleneck and enable effective segmentation in diverse agricultural\nscenarios.", "AI": {"tldr": "A zero-shot segmentation method combines Plantnet, DinoV2, and SAM to segment plants in agricultural imagery, achieving better results without requiring new annotated datasets.", "motivation": "To alleviate the annotation bottleneck in agricultural segmentation tasks and enable better performance under challenging conditions, such as limited training data and complex field scenarios.", "method": "The method leverages Plantnet's plant-centric representations to generate coarse segmentation masks, refined by SAM for detailed segmentation. It employs Plantnet-fine-tuned DinoV2 for better performance.", "result": "Consistent improvements in segmentation performance were achieved across four datasets, with Plantnet-fine-tuned DinoV2 outperforming the baseline DinoV2 model, evaluated using the Jaccard Index (IoU).", "conclusion": "Combining foundation models like DinoV2 with domain-specific models like Plantnet enables effective zero-shot segmentation for agricultural use, reducing the need for new data annotations."}}
{"id": "2510.12447", "pdf": "https://arxiv.org/pdf/2510.12447", "abs": "https://arxiv.org/abs/2510.12447", "authors": ["Anush Anand", "Pranav Agrawal", "Tejas Bodas"], "title": "Bayesian Optimization for Dynamic Pricing and Learning", "categories": ["cs.LG"], "comment": null, "summary": "Dynamic pricing is the practice of adjusting the selling price of a product\nto maximize a firm's revenue by responding to market demand. The literature\ntypically distinguishes between two settings: infinite inventory, where the\nfirm has unlimited stock and time to sell, and finite inventory, where both\ninventory and selling horizon are limited. In both cases, the central challenge\nlies in the fact that the demand function -- how sales respond to price -- is\nunknown and must be learned from data. Traditional approaches often assume a\nspecific parametric form for the demand function, enabling the use of\nreinforcement learning (RL) to identify near-optimal pricing strategies.\nHowever, such assumptions may not hold in real-world scenarios, limiting the\napplicability of these methods. In this work, we propose a Gaussian Process\n(GP) based nonparametric approach to dynamic pricing that avoids restrictive\nmodeling assumptions. We treat the demand function as a black-box function of\nthe price and develop pricing algorithms based on Bayesian Optimization (BO) --\na sample-efficient method for optimizing unknown functions. We present BO-based\nalgorithms tailored for both infinite and finite inventory settings and provide\nregret guarantees for both regimes, thereby quantifying the learning efficiency\nof our methods. Through extensive experiments, we demonstrate that our BO-based\nmethods outperform several state-of-the-art RL algorithms in terms of revenue,\nwhile requiring fewer assumptions and offering greater robustness. This\nhighlights Bayesian Optimization as a powerful and practical tool for dynamic\npricing in complex, uncertain environments.", "AI": {"tldr": "The paper introduces a Gaussian Process-based Bayesian Optimization method for dynamic pricing, outperforming reinforcement learning techniques in accuracy, efficiency, and robustness.", "motivation": "Existing dynamic pricing methods relying on parametric demand functions and reinforcement learning are constrained by restrictive assumptions, limiting their real-world applicability.", "method": "The authors propose a nonparametric dynamic pricing model utilizing Gaussian Processes and Bayesian Optimization, offering tailored algorithms and regret guarantees for infinite and finite inventory scenarios.", "result": "Extensive experiments reveal that the proposed BO-based algorithms generate higher revenue while reducing assumptions compared to state-of-the-art reinforcement learning approaches.", "conclusion": "Bayesian Optimization proves to be a robust and practical solution for dynamic pricing, addressing real-world uncertainties and outperforming existing methods in various scenarios."}}
{"id": "2510.12740", "pdf": "https://arxiv.org/pdf/2510.12740", "abs": "https://arxiv.org/abs/2510.12740", "authors": ["Sanghee J. Kim", "Kanishka Misra"], "title": "Hey, wait a minute: on at-issue sensitivity in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 5 figures, 3 tables. See\n  https://github.com/sangheek16/hey-wait-a-minute for code and data", "summary": "Evaluating the naturalness of dialogue in language models (LMs) is not\ntrivial: notions of 'naturalness' vary, and scalable quantitative metrics\nremain limited. This study leverages the linguistic notion of 'at-issueness' to\nassess dialogue naturalness and introduces a new method: Divide, Generate,\nRecombine, and Compare (DGRC). DGRC (i) divides a dialogue as a prompt, (ii)\ngenerates continuations for subparts using LMs, (iii) recombines the dialogue\nand continuations, and (iv) compares the likelihoods of the recombined\nsequences. This approach mitigates bias in linguistic analyses of LMs and\nenables systematic testing of discourse-sensitive behavior. Applying DGRC, we\nfind that LMs prefer to continue dialogue on at-issue content, with this effect\nenhanced in instruct-tuned models. They also reduce their at-issue preference\nwhen relevant cues (e.g., \"Hey, wait a minute\") are present. Although\ninstruct-tuning does not further amplify this modulation, the pattern reflects\na hallmark of successful dialogue dynamics.", "AI": {"tldr": "The study leverages 'at-issueness' to evaluate dialogue naturalness in language models (LMs) and introduces the DGRC method for systematic testing of discourse sensitivity.", "motivation": "Current methods to evaluate the naturalness of dialogue are limited and subjective, prompting the need for scalable and systematic approaches.", "method": "The proposed DGRC method involves dividing a dialogue, generating continuations using LMs, recombining parts, and comparing likelihoods of recombined sequences.", "result": "The study observed that LMs show a preference for continuing at-issue content, with instruct-tuned models demonstrating an enhanced effect. LMs also adapt to cues like 'Hey, wait a minute' by reducing their at-issue preference.", "conclusion": "DGRC offers a scalable method to study dialogue dynamics, showing that LMs can exhibit behavior reflective of natural dialogue patterns, especially in handling at-issue content."}}
{"id": "2510.12581", "pdf": "https://arxiv.org/pdf/2510.12581", "abs": "https://arxiv.org/abs/2510.12581", "authors": ["Yasaman Haghighi", "Bastien van Delft", "Mariam Hassan", "Alexandre Alahi"], "title": "LayerSync: Self-aligning Intermediate Layers", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We propose LayerSync, a domain-agnostic approach for improving the generation\nquality and the training efficiency of diffusion models. Prior studies have\nhighlighted the connection between the quality of generation and the\nrepresentations learned by diffusion models, showing that external guidance on\nmodel intermediate representations accelerates training. We reconceptualize\nthis paradigm by regularizing diffusion models with their own intermediate\nrepresentations. Building on the observation that representation quality varies\nacross diffusion model layers, we show that the most semantically rich\nrepresentations can act as an intrinsic guidance for weaker ones, reducing the\nneed for external supervision. Our approach, LayerSync, is a self-sufficient,\nplug-and-play regularizer term with no overhead on diffusion model training and\ngeneralizes beyond the visual domain to other modalities. LayerSync requires no\npretrained models nor additional data. We extensively evaluate the method on\nimage generation and demonstrate its applicability to other domains such as\naudio, video, and motion generation. We show that it consistently improves the\ngeneration quality and the training efficiency. For example, we speed up the\ntraining of flow-based transformer by over 8.75x on ImageNet dataset and\nimproved the generation quality by 23.6%. The code is available at\nhttps://github.com/vita-epfl/LayerSync.", "AI": {"tldr": "LayerSync offers a domain-agnostic solution to enhance diffusion models\u2019 training efficiency and output quality by leveraging their internal representations instead of external guidance or additional data.", "motivation": "To address the inefficiencies in training and generation quality of diffusion models by replacing external guidance with a method that utilizes intrinsic model representations.", "method": "A regularization technique called LayerSync that uses the semantically rich intermediate representations of diffusion models as guidance for weaker layers, enabling efficient and effective training without external dependencies.", "result": "LayerSync significantly speeds up training (e.g., 8.75x on ImageNet with flow-based transformer) and improves generation quality (e.g., by 23.6%). It is applicable across various domains including image, audio, video, and motion generation.", "conclusion": "The proposed LayerSync is a plug-and-play, resource-efficient regularizer that enhances the performance and applicability of diffusion models without relying on pretrained models or additional data."}}
{"id": "2510.12451", "pdf": "https://arxiv.org/pdf/2510.12451", "abs": "https://arxiv.org/abs/2510.12451", "authors": ["Israel Mason-Williams", "Gabryel Mason-Williams", "Helen Yannakoudakis"], "title": "A Function Centric Perspective On Flat and Sharp Minima", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "26 pages, 26 tables, 63 figures, pre-print", "summary": "Flat minima are widely believed to correlate with improved generalisation in\ndeep neural networks. However, this connection has proven more nuanced in\nrecent studies, with both theoretical counterexamples and empirical exceptions\nemerging in the literature. In this paper, we revisit the role of sharpness in\nmodel performance, proposing that sharpness is better understood as a\nfunction-dependent property rather than a reliable indicator of poor\ngeneralisation. We conduct extensive empirical studies, from single-objective\noptimisation to modern image classification tasks, showing that sharper minima\noften emerge when models are regularised (e.g., via SAM, weight decay, or data\naugmentation), and that these sharp minima can coincide with better\ngeneralisation, calibration, robustness, and functional consistency. Across a\nrange of models and datasets, we find that baselines without regularisation\ntend to converge to flatter minima yet often perform worse across all safety\nmetrics. Our findings demonstrate that function complexity, rather than\nflatness alone, governs the geometry of solutions, and that sharper minima can\nreflect more appropriate inductive biases (especially under regularisation),\ncalling for a function-centric reappraisal of loss landscape geometry.", "AI": {"tldr": "This paper revisits the correlation between sharpness and generalization in deep neural networks, suggesting sharpness is context-dependent and not necessarily indicative of poor generalization.", "motivation": "The paper seeks to clarify the nuanced relationship between sharpness (loss landscape) and generalization in neural networks, prompted by contradictory findings in prior studies.", "method": "Extensive empirical studies were performed across different optimization techniques and tasks, including modern image classification, to analyze the impact of regularization methods on sharpness and associated metrics.", "result": "The study finds that sharper minima can coincide with improved generalization, calibration, robustness, and functional consistency when models are regularized.", "conclusion": "Function complexity, rather than flatness alone, shapes solution geometry. Sharp minima under regularization may offer better inductive biases, urging a reassessment of loss landscape geometry as a function-centric perspective."}}
{"id": "2510.12766", "pdf": "https://arxiv.org/pdf/2510.12766", "abs": "https://arxiv.org/abs/2510.12766", "authors": ["\u0141ukasz Borchmann"], "title": "Language Models Model Language", "categories": ["cs.CL"], "comment": null, "summary": "Linguistic commentary on LLMs, heavily influenced by the theoretical\nframeworks of de Saussure and Chomsky, is often speculative and unproductive.\nCritics challenge whether LLMs can legitimately model language, citing the need\nfor \"deep structure\" or \"grounding\" to achieve an idealized linguistic\n\"competence.\" We argue for a radical shift in perspective towards the\nempiricist principles of Witold Ma\\'nczak, a prominent general and historical\nlinguist. He defines language not as a \"system of signs\" or a \"computational\nsystem of the brain\" but as the totality of all that is said and written. Above\nall, he identifies frequency of use of particular language elements as\nlanguage's primary governing principle. Using his framework, we challenge prior\ncritiques of LLMs and provide a constructive guide for designing, evaluating,\nand interpreting language models.", "AI": {"tldr": "This paper challenges traditional linguistic critiques of LLMs by applying Witold Ma\u0144czak's empiricist viewpoint, emphasizing frequency of language use as the key principle.", "motivation": "To address criticisms of LLMs from traditional linguistic theories and propose a shift towards a frequency-based framework to understand and evaluate LLMs.", "method": "The paper adopts Witold Ma\u0144czak's empiricist linguistic framework, focusing on language as the totality of usage and frequency-driven principles, to reinterpret critiques and guide language model development.", "result": "The argument reframes critiques of LLMs, presenting a constructive perspective for their design and evaluation using frequency-based linguistic principles.", "conclusion": "This perspective lays the groundwork for a new methodology to assess LLMs, emphasizing practical usage frequency over traditional theoretical constructs like 'deep structure.'"}}
{"id": "2510.12586", "pdf": "https://arxiv.org/pdf/2510.12586", "abs": "https://arxiv.org/abs/2510.12586", "authors": ["Jiachen Lei", "Keli Liu", "Julius Berner", "Haiming Yu", "Hongkai Zheng", "Jiahong Wu", "Xiangxiang Chu"], "title": "Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training", "categories": ["cs.CV"], "comment": null, "summary": "Pixel-space generative models are often more difficult to train and generally\nunderperform compared to their latent-space counterparts, leaving a persistent\nperformance and efficiency gap. In this paper, we introduce a novel two-stage\ntraining framework that closes this gap for pixel-space diffusion and\nconsistency models. In the first stage, we pre-train encoders to capture\nmeaningful semantics from clean images while aligning them with points along\nthe same deterministic sampling trajectory, which evolves points from the prior\nto the data distribution. In the second stage, we integrate the encoder with a\nrandomly initialized decoder and fine-tune the complete model end-to-end for\nboth diffusion and consistency models. Our training framework demonstrates\nstrong empirical performance on ImageNet dataset. Specifically, our diffusion\nmodel reaches an FID of 2.04 on ImageNet-256 and 2.35 on ImageNet-512 with 75\nnumber of function evaluations (NFE), surpassing prior pixel-space methods by a\nlarge margin in both generation quality and efficiency while rivaling leading\nVAE-based models at comparable training cost. Furthermore, on ImageNet-256, our\nconsistency model achieves an impressive FID of 8.82 in a single sampling step,\nsignificantly surpassing its latent-space counterpart. To the best of our\nknowledge, this marks the first successful training of a consistency model\ndirectly on high-resolution images without relying on pre-trained VAEs or\ndiffusion models.", "AI": {"tldr": "This paper introduces a two-stage training framework for pixel-space diffusion and consistency models, achieving superior image generation quality and efficiency on the ImageNet dataset.", "motivation": "Pixel-space generative models underperform and are harder to train compared to latent-space models, motivating the need for improved methods.", "method": "The two-stage framework first pre-trains encoders to capture semantics aligned with deterministic sampling trajectories, then integrates the encoder with a decoder for end-to-end model fine-tuning.", "result": "On ImageNet, the diffusion model achieved remarkable FID scores (2.04 for 256px and 2.35 for 512px), while the consistency model achieved FID 8.82 in one sampling step, outperforming latent-space counterparts.", "conclusion": "This framework closes the performance gap for pixel-space models, offering an efficient alternative rivaling latent-space methods and enabling direct training of consistency models for high-resolution images."}}
{"id": "2510.12453", "pdf": "https://arxiv.org/pdf/2510.12453", "abs": "https://arxiv.org/abs/2510.12453", "authors": ["Viacheslav Vasilev", "Arseny Ivanov", "Nikita Gushchin", "Maria Kovaleva", "Alexander Korotin"], "title": "Time-Correlated Video Bridge Matching", "categories": ["cs.LG"], "comment": null, "summary": "Diffusion models excel in noise-to-data generation tasks, providing a mapping\nfrom a Gaussian distribution to a more complex data distribution. However they\nstruggle to model translations between complex distributions, limiting their\neffectiveness in data-to-data tasks. While Bridge Matching (BM) models address\nthis by finding the translation between data distributions, their application\nto time-correlated data sequences remains unexplored. This is a critical\nlimitation for video generation and manipulation tasks, where maintaining\ntemporal coherence is particularly important. To address this gap, we propose\nTime-Correlated Video Bridge Matching (TCVBM), a framework that extends BM to\ntime-correlated data sequences in the video domain. TCVBM explicitly models\ninter-sequence dependencies within the diffusion bridge, directly incorporating\ntemporal correlations into the sampling process. We compare our approach to\nclassical methods based on bridge matching and diffusion models for three\nvideo-related tasks: frame interpolation, image-to-video generation, and video\nsuper-resolution. TCVBM achieves superior performance across multiple\nquantitative metrics, demonstrating enhanced generation quality and\nreconstruction fidelity.", "AI": {"tldr": "Diffusion models struggle with data-to-data tasks. This paper introduces TCVBM, enhancing video generation/manipulation by modeling temporal correlations within time-dependent data sequences.", "motivation": "The paper addresses the limitation of diffusion models in handling translations between complex distributions for time-correlated data, which is crucial for video generation and manipulation.", "method": "The proposed Time-Correlated Video Bridge Matching (TCVBM) extends Bridge Matching (BM) methods by integrating temporal dependencies through a diffusion bridge, focusing on video-related tasks.", "result": "TCVBM demonstrated superior performance compared to classical methods in video frame interpolation, image-to-video generation, and video super-resolution across various metrics.", "conclusion": "TCVBM effectively incorporates temporal correlations in video generation tasks, improving quality and fidelity compared to standard methods."}}
{"id": "2510.12773", "pdf": "https://arxiv.org/pdf/2510.12773", "abs": "https://arxiv.org/abs/2510.12773", "authors": ["Ahmed Heakl", "Martin Gubri", "Salman Khan", "Sangdoo Yun", "Seong Joon Oh"], "title": "Dr.LLM: Dynamic Layer Routing in LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "17 pages, Under submission", "summary": "Large Language Models (LLMs) process every token through all layers of a\ntransformer stack, causing wasted computation on simple queries and\ninsufficient flexibility for harder ones that need deeper reasoning.\nAdaptive-depth methods can improve efficiency, but prior approaches rely on\ncostly inference-time search, architectural changes, or large-scale retraining,\nand in practice often degrade accuracy despite efficiency gains. We introduce\nDr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that\nequips pretrained models with lightweight per-layer routers deciding to skip,\nexecute, or repeat a block. Routers are trained with explicit supervision:\nusing Monte Carlo Tree Search (MCTS), we derive high-quality layer\nconfigurations that preserve or improve accuracy under a compute budget. Our\ndesign, windowed pooling for stable routing, focal loss with class balancing,\nand bottleneck MLP routers, ensures robustness under class imbalance and long\nsequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to\n+3.4%p while saving 5 layers per example on average. Routers generalize to\nout-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA,\nAGIEval) with only 0.85% accuracy drop while retaining efficiency, and\noutperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that\nexplicitly supervised routers retrofit frozen LLMs for budget-aware,\naccuracy-driven inference without altering base weights.", "AI": {"tldr": "Dr.LLM introduces a framework for pretrained models to dynamically adjust layer usage, improving efficiency and accuracy without changing base weights.", "motivation": "The motivation is to address inefficiency and flexibility issues in LLMs caused by uniformly processing tokens through all transformer layers.", "method": "The study retrofits pretrained models with per-layer routers that decide to skip, execute, or repeat blocks. The routers are trained using Monte Carlo Tree Search (MCTS) for optimal layer configurations under compute budgets.", "result": "Dr.LLM achieves up to 3.4% accuracy improvement and reduces layer usage by an average of 5 on the ARC (logic) and DART (math) tasks. It generalizes well to out-of-domain tasks with minimal accuracy drop while maintaining efficiency.", "conclusion": "Explicitly supervised routers can enhance the efficiency and accuracy of frozen LLMs without altering their base weights, making them suitable for budget-aware, accuracy-driven inference."}}
{"id": "2510.11823", "pdf": "https://arxiv.org/pdf/2510.11823", "abs": "https://arxiv.org/abs/2510.11823", "authors": ["Caelin Kaplan", "Alexander Warnecke", "Neil Archibald"], "title": "BlackIce: A Containerized Red Teaming Toolkit for AI Security Testing", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "AI models are being increasingly integrated into real-world systems, raising\nsignificant concerns about their safety and security. Consequently, AI red\nteaming has become essential for organizations to proactively identify and\naddress vulnerabilities before they can be exploited by adversaries. While\nnumerous AI red teaming tools currently exist, practitioners face challenges in\nselecting the most appropriate tools from a rapidly expanding landscape, as\nwell as managing complex and frequently conflicting software dependencies\nacross isolated projects. Given these challenges and the relatively small\nnumber of organizations with dedicated AI red teams, there is a strong need to\nlower barriers to entry and establish a standardized environment that\nsimplifies the setup and execution of comprehensive AI model assessments.\n  Inspired by Kali Linux's role in traditional penetration testing, we\nintroduce BlackIce, an open-source containerized toolkit designed for red\nteaming Large Language Models (LLMs) and classical machine learning (ML)\nmodels. BlackIce provides a reproducible, version-pinned Docker image that\nbundles 14 carefully selected open-source tools for Responsible AI and Security\ntesting, all accessible via a unified command-line interface. With this setup,\ninitiating red team assessments is as straightforward as launching a container,\neither locally or using a cloud platform. Additionally, the image's modular\narchitecture facilitates community-driven extensions, allowing users to easily\nadapt or expand the toolkit as new threats emerge. In this paper, we describe\nthe architecture of the container image, the process used for selecting tools,\nand the types of evaluations they support.", "AI": {"tldr": "BlackIce is a containerized toolkit aimed at simplifying AI red teaming by bundling essential tools within a reproducible Docker image, enabling streamlined AI model assessments.", "motivation": "The motivation behind this paper is to address the challenges in selecting appropriate tools and managing dependencies for AI red teaming due to the growing complexity and the need for enhanced security in AI systems.", "method": "The authors introduce BlackIce, a containerized toolkit inspired by Kali Linux, which includes a unified interface and 14 selected open-source tools for AI model security testing. The container image enables ease of use, modular architecture, and streamlined assessments.", "result": "BlackIce allows for simplified AI red teaming processes through a reproducible Docker image that integrates multiple tools. It supports easy deployment either locally or via cloud platforms.", "conclusion": "BlackIce lowers barriers for AI red teaming practices, facilitates standardized assessments, and encourages community-driven improvements through its modular framework."}}
{"id": "2510.12603", "pdf": "https://arxiv.org/pdf/2510.12603", "abs": "https://arxiv.org/abs/2510.12603", "authors": ["Chao Chen", "Zhixin Ma", "Yongqi Li", "Yupeng Hu", "Yinwei Wei", "Wenjie Li", "Liqiang Nie"], "title": "Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Multimodal reasoning aims to enhance the capabilities of MLLMs by\nincorporating intermediate reasoning steps before reaching the final answer. It\nhas evolved from text-only reasoning to the integration of visual information,\nenabling the thought process to be conveyed through both images and text.\nDespite its effectiveness, current multimodal reasoning methods depend on\nexplicit reasoning steps that require labor-intensive vision-text annotations\nand inherently introduce significant inference latency. To address these\nissues, we introduce multimodal latent reasoning with the advantages of\nmultimodal representation, reduced annotation, and inference efficiency. To\nfacilicate it, we propose Interleaved Vision-Text Latent Reasoning (IVT-LR),\nwhich injects both visual and textual information in the reasoning process\nwithin the latent space. Specifically, IVT-LR represents each reasoning step by\ncombining two implicit parts: latent text (the hidden states from the previous\nstep) and latent vision (a set of selected image embeddings). We further\nintroduce a progressive multi-stage training strategy to enable MLLMs to\nperform the above multimodal latent reasoning steps. Experiments on M3CoT and\nScienceQA demonstrate that our IVT-LR method achieves an average performance\nincrease of 5.45% in accuracy, while simultaneously achieving a speed increase\nof over 5 times compared to existing approaches. Code available at\nhttps://github.com/FYYDCC/IVT-LR.", "AI": {"tldr": "The paper introduces Interleaved Vision-Text Latent Reasoning (IVT-LR), a method for multimodal reasoning that improves accuracy by 5.45% and speeds up inference over 5 times compared to existing methods.", "motivation": "Current multimodal reasoning methods rely on explicit reasoning steps with heavy annotations and slow inference speeds.", "method": "IVT-LR combines latent text and vision information, using implicit reasoning steps in a latent space, trained with a progressive multi-stage strategy.", "result": "IVT-LR achieves a 5.45% accuracy gain and a 5x speed improvement in experiments on M3CoT and ScienceQA.", "conclusion": "The method enhances both efficiency and effectiveness of multimodal reasoning without requiring extensive annotations."}}
{"id": "2510.12781", "pdf": "https://arxiv.org/pdf/2510.12781", "abs": "https://arxiv.org/abs/2510.12781", "authors": ["Yacouba Diarra", "Nouhoum Souleymane Coulibaly", "Michael Leventhal"], "title": "Cost Analysis of Human-corrected Transcription for Predominately Oral Languages", "categories": ["cs.CL"], "comment": "6 pages, 1 figure", "summary": "Creating speech datasets for low-resource languages is a critical yet poorly\nunderstood challenge, particularly regarding the actual cost in human labor.\nThis paper investigates the time and complexity required to produce\nhigh-quality annotated speech data for a subset of low-resource languages, low\nliteracy Predominately Oral Languages, focusing on Bambara, a Manding language\nof Mali. Through a one-month field study involving ten transcribers with native\nproficiency, we analyze the correction of ASR-generated transcriptions of 53\nhours of Bambara voice data. We report that it takes, on average, 30 hours of\nhuman labor to accurately transcribe one hour of speech data under laboratory\nconditions and 36 hours under field conditions. The study provides a baseline\nand practical insights for a large class of languages with comparable profiles\nundertaking the creation of NLP resources.", "AI": {"tldr": "Creating accurate annotated speech datasets for low-resource languages requires substantial human labor, with Bambara transcription demanding 30-36 hours of work per hour of speech data.", "motivation": "The paper aims to address gaps in understanding the time and effort required to produce high-quality speech datasets for low-resource languages, focusing on Bambara.", "method": "The study involved a one-month field experiment with ten native transcribers correcting ASR-generated Bambara speech transcriptions for 53 hours of voice data.", "result": "It takes, on average, 30 hours of human labor to transcribe one hour of speech in laboratory settings and 36 hours in field settings for Bambara.", "conclusion": "The findings offer both a baseline and practical guidance for developing NLP resources for other low-resource languages with similar profiles."}}
{"id": "2510.11824", "pdf": "https://arxiv.org/pdf/2510.11824", "abs": "https://arxiv.org/abs/2510.11824", "authors": ["Simin Li", "Zihao Mao", "Hanxiao Li", "Zonglei Jing", "Zhuohang bian", "Jun Guo", "Li Wang", "Zhuoran Han", "Ruixiao Xu", "Xin Yu", "Chengdong Ma", "Yuqing Ma", "Bo An", "Yaodong Yang", "Weifeng Lv", "Xianglong Liu"], "title": "Empirical Study on Robustness and Resilience in Cooperative Multi-Agent Reinforcement Learning", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": "44 pages, 16 figures, NeurIPS 2025", "summary": "In cooperative Multi-Agent Reinforcement Learning (MARL), it is a common\npractice to tune hyperparameters in ideal simulated environments to maximize\ncooperative performance. However, policies tuned for cooperation often fail to\nmaintain robustness and resilience under real-world uncertainties. Building\ntrustworthy MARL systems requires a deep understanding of robustness, which\nensures stability under uncertainties, and resilience, the ability to recover\nfrom disruptions--a concept extensively studied in control systems but largely\noverlooked in MARL. In this paper, we present a large-scale empirical study\ncomprising over 82,620 experiments to evaluate cooperation, robustness, and\nresilience in MARL across 4 real-world environments, 13 uncertainty types, and\n15 hyperparameters. Our key findings are: (1) Under mild uncertainty,\noptimizing cooperation improves robustness and resilience, but this link\nweakens as perturbations intensify. Robustness and resilience also varies by\nalgorithm and uncertainty type. (2) Robustness and resilience do not generalize\nacross uncertainty modalities or agent scopes: policies robust to action noise\nfor all agents may fail under observation noise on a single agent. (3)\nHyperparameter tuning is critical for trustworthy MARL: surprisingly, standard\npractices like parameter sharing, GAE, and PopArt can hurt robustness, while\nearly stopping, high critic learning rates, and Leaky ReLU consistently help.\nBy optimizing hyperparameters only, we observe substantial improvement in\ncooperation, robustness and resilience across all MARL backbones, with the\nphenomenon also generalizing to robust MARL methods across these backbones.\nCode and results available at\nhttps://github.com/BUAA-TrustworthyMARL/adv_marl_benchmark .", "AI": {"tldr": "This paper investigates the robustness and resilience of cooperative Multi-Agent Reinforcement Learning (MARL) under real-world uncertainties using 82,620 experiments and provides guidance for hyperparameter tuning.", "motivation": "The paper aims to address the gap in understanding robustness and resilience in MARL systems, essential for developing trustworthy systems, as most existing studies focus only on cooperation without considering real-world uncertainties.", "method": "The authors conducted a large-scale empirical study with 82,620 experiments across 4 real-world environments, 13 uncertainty types, and 15 hyperparameters to evaluate cooperation, robustness, and resilience in MARL systems.", "result": "The study found that robustness and resilience depend on the intensity of perturbations, the type of uncertainty, and algorithm choice. Hyperparameter tuning significantly impacts performance, with some practices improving results while others, surprisingly, harming robustness.", "conclusion": "By optimizing hyperparameters, substantial improvements in cooperation, robustness, and resilience were observed, suggesting hyperparameter tuning is critical for trustworthy MARL systems."}}
{"id": "2510.12605", "pdf": "https://arxiv.org/pdf/2510.12605", "abs": "https://arxiv.org/abs/2510.12605", "authors": ["Runting Li", "Shijie Lian", "Hua Li", "Yutong Li", "Wenhui Wu", "Sam Kwong"], "title": "WaterFlow: Explicit Physics-Prior Rectified Flow for Underwater Saliency Mask Generation", "categories": ["cs.CV"], "comment": null, "summary": "Underwater Salient Object Detection (USOD) faces significant challenges,\nincluding underwater image quality degradation and domain gaps. Existing\nmethods tend to ignore the physical principles of underwater imaging or simply\ntreat degradation phenomena in underwater images as interference factors that\nmust be eliminated, failing to fully exploit the valuable information they\ncontain. We propose WaterFlow, a rectified flow-based framework for underwater\nsalient object detection that innovatively incorporates underwater physical\nimaging information as explicit priors directly into the network training\nprocess and introduces temporal dimension modeling, significantly enhancing the\nmodel's capability for salient object identification. On the USOD10K dataset,\nWaterFlow achieves a 0.072 gain in S_m, demonstrating the effectiveness and\nsuperiority of our method. The code will be published after the acceptance.", "AI": {"tldr": "The paper presents WaterFlow, a framework for underwater salient object detection that utilizes underwater imaging physics and temporal modeling to improve performance.", "motivation": "Current underwater salient object detection methods struggle with image degradation and fail to leverage the physical principles of underwater imaging.", "method": "The authors introduce WaterFlow, which embeds underwater imaging physics as explicit priors and incorporates temporal modeling into the training process.", "result": "WaterFlow achieves a 0.072 gain in S_m on the USOD10K dataset, illustrating its effectiveness.", "conclusion": "WaterFlow demonstrates the potential of integrating physical priors and temporal modeling in overcoming challenges in underwater salient object detection."}}
{"id": "2510.12646", "pdf": "https://arxiv.org/pdf/2510.12646", "abs": "https://arxiv.org/abs/2510.12646", "authors": ["Yanlin Jiang", "Yuchen Liu", "Mingren Liu"], "title": "Zero-Shot CFC: Fast Real-World Image Denoising based on Cross-Frequency Consistency", "categories": ["cs.CV"], "comment": "The British Machine Vision Conference", "summary": "Zero-shot denoisers address the dataset dependency of deep-learning-based\ndenoisers, enabling the denoising of unseen single images. Nonetheless,\nexisting zero-shot methods suffer from long training times and rely on the\nassumption of noise independence and a zero-mean property, limiting their\neffectiveness in real-world denoising scenarios where noise characteristics are\nmore complicated. This paper proposes an efficient and effective method for\nreal-world denoising, the Zero-Shot denoiser based on Cross-Frequency\nConsistency (ZSCFC), which enables training and denoising with a single noisy\nimage and does not rely on assumptions about noise distribution. Specifically,\nimage textures exhibit position similarity and content consistency across\ndifferent frequency bands, while noise does not. Based on this property, we\ndeveloped cross-frequency consistency loss and an ultralight network to realize\nimage denoising. Experiments on various real-world image datasets demonstrate\nthat our ZSCFC outperforms other state-of-the-art zero-shot methods in terms of\ncomputational efficiency and denoising performance.", "AI": {"tldr": "The paper introduces ZSCFC, a method enabling efficient and effective zero-shot denoising of real-world noisy images without assumptions about the noise characteristics.", "motivation": "Existing zero-shot denoising methods are computationally expensive, assume noise independence, and struggle with complex real-world noise.", "method": "The proposed ZSCFC uses cross-frequency consistency loss and an ultralight network, exploiting texture characteristics across frequency bands to differentiate noise.", "result": "ZSCFC outperforms state-of-the-art zero-shot denoisers in both speed and denoising quality, with successful results demonstrated across various real-world datasets.", "conclusion": "ZSCFC is a highly efficient and effective method for denoising single noisy images with complex real-world noise, overcoming limitations of prior methods."}}
{"id": "2510.12497", "pdf": "https://arxiv.org/pdf/2510.12497", "abs": "https://arxiv.org/abs/2510.12497", "authors": ["Jincheng Zhong", "Boyuan Jiang", "Xin Tao", "Pengfei Wan", "Kun Gai", "Mingsheng Long"], "title": "Mitigating the Noise Shift for Denoising Generative Models via Noise Awareness Guidance", "categories": ["cs.LG"], "comment": null, "summary": "Existing denoising generative models rely on solving discretized reverse-time\nSDEs or ODEs. In this paper, we identify a long-overlooked yet pervasive issue\nin this family of models: a misalignment between the pre-defined noise level\nand the actual noise level encoded in intermediate states during sampling. We\nrefer to this misalignment as noise shift. Through empirical analysis, we\ndemonstrate that noise shift is widespread in modern diffusion models and\nexhibits a systematic bias, leading to sub-optimal generation due to both\nout-of-distribution generalization and inaccurate denoising updates. To address\nthis problem, we propose Noise Awareness Guidance (NAG), a simple yet effective\ncorrection method that explicitly steers sampling trajectories to remain\nconsistent with the pre-defined noise schedule. We further introduce a\nclassifier-free variant of NAG, which jointly trains a noise-conditional and a\nnoise-unconditional model via noise-condition dropout, thereby eliminating the\nneed for external classifiers. Extensive experiments, including ImageNet\ngeneration and various supervised fine-tuning tasks, show that NAG consistently\nmitigates noise shift and substantially improves the generation quality of\nmainstream diffusion models.", "AI": {"tldr": "The paper identifies a pervasive issue in denoising generative models, termed noise shift, and proposes a solution called Noise Awareness Guidance (NAG) to address it.", "motivation": "The motivation is to improve the performance of diffusion-based generative models by addressing the noise shift issue that causes sub-optimal generation.", "method": "The authors propose Noise Awareness Guidance (NAG), which steers sampling trajectories to align with the pre-defined noise schedule and introduces a classifier-free variant using noise-condition dropout.", "result": "Experiments on ImageNet and fine-tuning tasks demonstrate that NAG effectively mitigates noise shift and substantially enhances generation quality in diffusion models.", "conclusion": "NAG proves to be a simple and effective method for resolving noise shift, leading to better alignment with noise schedules and improved generative performance."}}
{"id": "2510.12660", "pdf": "https://arxiv.org/pdf/2510.12660", "abs": "https://arxiv.org/abs/2510.12660", "authors": ["Shuhei Tarashima", "Yushan Wang", "Norio Tagawa"], "title": "On the Use of Hierarchical Vision Foundation Models for Low-Cost Human Mesh Recovery and Pose Estimation", "categories": ["cs.CV"], "comment": "Accepted at ICCVW 2025", "summary": "In this work, we aim to develop simple and efficient models for human mesh\nrecovery (HMR) and its predecessor task, human pose estimation (HPE).\nState-of-the-art HMR methods, such as HMR2.0 and its successors, rely on large,\nnon-hierarchical vision transformers as encoders, which are inherited from the\ncorresponding HPE models like ViTPose. To establish baselines across varying\ncomputational budgets, we first construct three lightweight HMR2.0 variants by\nadapting the corresponding ViTPose models. In addition, we propose leveraging\nthe early stages of hierarchical vision foundation models (VFMs), including\nSwin Transformer, GroupMixFormer, and VMamba, as encoders. This design is\nmotivated by the observation that intermediate stages of hierarchical VFMs\nproduce feature maps with resolutions comparable to or higher than those of\nnon-hierarchical counterparts. We conduct a comprehensive evaluation of 27\nhierarchical-VFM-based HMR and HPE models, demonstrating that using only the\nfirst two or three stages achieves performance on par with full-stage models.\nMoreover, we show that the resulting truncated models exhibit better trade-offs\nbetween accuracy and computational efficiency compared to existing lightweight\nalternatives.", "AI": {"tldr": "The paper introduces efficient human mesh recovery (HMR) and pose estimation (HPE) models by using hierarchical vision foundation models (VFMs), achieving competitive performance with improved efficiency.", "motivation": "Develop efficient and lightweight models for human mesh recovery (HMR) and pose estimation tasks without the computational overhead of large vision transformers.", "method": "Adapt lightweight variants of HMR2.0 by leveraging early stages of hierarchical VFMs like Swin Transformer, GroupMixFormer, and VMamba as model encoders.", "result": "Evaluation of 27 models shows that truncated hierarchical-VFM designs match the performance of full-stage models while yielding better accuracy-efficiency trade-offs.", "conclusion": "Hierarchical-VFM-based models can provide efficient solutions to HMR and HPE, enabling competitive results with fewer computational resources compared to traditional methods."}}
{"id": "2510.11746", "pdf": "https://arxiv.org/pdf/2510.11746", "abs": "https://arxiv.org/abs/2510.11746", "authors": ["Mykola Makhortykh", "Aytalina Kulichkina", "Kateryna Maikovska"], "title": "Evolution of wartime discourse on Telegram: A comparative study of Ukrainian and Russian policymakers' communication before and after Russia's full-scale invasion of Ukraine", "categories": ["cs.SI", "cs.CL", "cs.CY"], "comment": "46 pages", "summary": "This study examines elite-driven political communication on Telegram during\nthe ongoing Russo-Ukrainian war, the first large-scale European war in the\nsocial media era. Using a unique dataset of Telegram public posts from\nUkrainian and Russian policymakers (2019-2024), we analyze changes in\ncommunication volume, thematic content, and actor engagement following Russia's\n2022 full-scale invasion. Our findings show a sharp increase in Telegram\nactivity after the invasion, particularly among ruling-party policymakers.\nUkrainian policymakers initially focused on war-related topics, but this\nemphasis declined over time In contrast, Russian policymakers largely avoided\nwar-related discussions, instead emphasizing unrelated topics, such as Western\ncrises, to distract public attention. We also identify differences in\ncommunication strategies between large and small parties, as well as individual\npolicymakers. Our findings shed light on how policymakers adapt to wartime\ncommunication challenges and offer critical insights into the dynamics of\nonline political discourse during times of war.", "AI": {"tldr": "The paper studies Telegram political communication by Ukrainian and Russian policymakers during the Russo-Ukrainian war, revealing differences in communication strategies and volume.", "motivation": "To understand how policymakers adapt their communication strategies during wartime, specifically on Telegram in the Russo-Ukrainian war context.", "method": "The study uses a unique dataset of Telegram posts (2019-2024) from Ukrainian and Russian policymakers, focusing on changes in communication volume, thematic content, and actor engagement post-invasion.", "result": "Russian leaders avoided war discussions, focusing on other topics, while Ukrainian policymakers initially emphasized war but talked less about it over time. Different strategies emerged between large vs. small parties and individual policymakers.", "conclusion": "Policymakers adjust their online communication tactics to cope with wartime challenges, affecting online political discourse dynamics."}}
{"id": "2510.11837", "pdf": "https://arxiv.org/pdf/2510.11837", "abs": "https://arxiv.org/abs/2510.11837", "authors": ["Dominik Schwarz"], "title": "Countermind: A Multi-Layered Security Architecture for Large Language Models", "categories": ["cs.CR", "cs.AI", "K.6.5; I.2.7"], "comment": "33 pages, 3 figures, 6 tables. Keywords: LLM security;\n  defense-in-depth; prompt injection; activation steering; multimodal sandbox;\n  threat modeling", "summary": "The security of Large Language Model (LLM) applications is fundamentally\nchallenged by \"form-first\" attacks like prompt injection and jailbreaking,\nwhere malicious instructions are embedded within user inputs. Conventional\ndefenses, which rely on post hoc output filtering, are often brittle and fail\nto address the root cause: the model's inability to distinguish trusted\ninstructions from untrusted data. This paper proposes Countermind, a\nmulti-layered security architecture intended to shift defenses from a reactive,\npost hoc posture to a proactive, pre-inference, and intra-inference enforcement\nmodel. The architecture proposes a fortified perimeter designed to structurally\nvalidate and transform all inputs, and an internal governance mechanism\nintended to constrain the model's semantic processing pathways before an output\nis generated. The primary contributions of this work are conceptual designs\nfor: (1) A Semantic Boundary Logic (SBL) with a mandatory, time-coupled Text\nCrypter intended to reduce the plaintext prompt injection attack surface,\nprovided all ingestion paths are enforced. (2) A Parameter-Space Restriction\n(PSR) mechanism, leveraging principles from representation engineering, to\ndynamically control the LLM's access to internal semantic clusters, with the\ngoal of mitigating semantic drift and dangerous emergent behaviors. (3) A\nSecure, Self-Regulating Core that uses an OODA loop and a learning security\nmodule to adapt its defenses based on an immutable audit log. (4) A Multimodal\nInput Sandbox and Context-Defense mechanisms to address threats from\nnon-textual data and long-term semantic poisoning. This paper outlines an\nevaluation plan designed to quantify the proposed architecture's effectiveness\nin reducing the Attack Success Rate (ASR) for form-first attacks and to measure\nits potential latency overhead.", "AI": {"tldr": "Proposes 'Countermind,' a security framework for Large Language Models (LLM) to proactively defend against prompt injection and jailbreaking attacks.", "motivation": "LLMs face security vulnerabilities due to their inability to differentiate between trusted and untrusted inputs, making them susceptible to attacks like prompt injection and jailbreaking.", "method": "This paper introduces Countermind, a multi-layered security architecture containing several components: Semantic Boundary Logic, Parameter-Space Restriction, a Secure Self-Regulating Core, Multimodal Input Sandbox, and Context-Defense mechanisms.", "result": "Detailed an evaluation framework to assess Countermind's effectiveness in reducing attack success rates and measuring threat mitigation latency.", "conclusion": "Adopting Countermind could significantly improve the security of LLMs by shifting from a reactive to a proactive defense model."}}
{"id": "2510.12670", "pdf": "https://arxiv.org/pdf/2510.12670", "abs": "https://arxiv.org/abs/2510.12670", "authors": ["Julen Costa-Watanabe", "Isabelle Wittmann", "Benedikt Blumenstiel", "Konrad Schindler"], "title": "TerraCodec: Compressing Earth Observations", "categories": ["cs.CV"], "comment": null, "summary": "Earth observation (EO) satellites produce massive streams of multispectral\nimage time series, posing pressing challenges for storage and transmission.\nYet, learned EO compression remains fragmented, lacking publicly available\npretrained models and misaligned with advances in compression for natural\nimagery. Image codecs overlook temporal redundancy, while video codecs rely on\nmotion priors that fail to capture the radiometric evolution of largely static\nscenes. We introduce TerraCodec (TEC), a family of learned codecs tailored to\nEO. TEC includes efficient image-based variants adapted to multispectral\ninputs, as well as a Temporal Transformer model (TEC-TT) that leverages\ndependencies across time. To overcome the fixed-rate setting of today's neural\ncodecs, we present Latent Repacking, a novel method for training flexible-rate\ntransformer models that operate on varying rate-distortion settings. Trained on\nSentinel-2 data, TerraCodec outperforms classical codecs, achieving 3-10x\nstronger compression at equivalent image quality. Beyond compression, TEC-TT\nenables zero-shot cloud inpainting, surpassing state-of-the-art methods on the\nAllClear benchmark. Our results establish bespoke, learned compression\nalgorithms as a promising direction for Earth observation. Code and model\nweights will be released under a permissive license.", "AI": {"tldr": "TerraCodec (TEC) introduces tailored learned codecs for efficient Earth observation image compression, considering temporal redundancy and achieving higher compression rates. It also enables cloud inpainting surpassing existing methods.", "motivation": "The massive streams of multispectral image data from Earth observation satellites present storage and transmission challenges, with existing codecs failing to capture temporal dependencies or radiometric evolution in largely static imagery.", "method": "TerraCodec includes learned image-based variants for multispectral inputs and introduces a Temporal Transformer model (TEC-TT) utilizing dependencies across time, alongside Latent Repacking for flexible-rate functionality.", "result": "TerraCodec achieves 3-10x better compression performance compared to classical codecs while maintaining image quality, and TEC-TT excels in cloud inpainting on the AllClear benchmark.", "conclusion": "Learned compression algorithms, tailored for Earth observation data, prove effective, providing both improved compression and cloud inpainting capabilities, paving the way for advancements in EO data handling solutions."}}
{"id": "2510.12523", "pdf": "https://arxiv.org/pdf/2510.12523", "abs": "https://arxiv.org/abs/2510.12523", "authors": ["Ahmed Ben Yahmed", "Hafedh El Ferchichi", "Marc Abeille", "Vianney Perchet"], "title": "Multi-Armed Bandits with Minimum Aggregated Revenue Constraints", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "We examine a multi-armed bandit problem with contextual information, where\nthe objective is to ensure that each arm receives a minimum aggregated reward\nacross contexts while simultaneously maximizing the total cumulative reward.\nThis framework captures a broad class of real-world applications where fair\nrevenue allocation is critical and contextual variation is inherent. The\ncross-context aggregation of minimum reward constraints, while enabling better\nperformance and easier feasibility, introduces significant technical challenges\n-- particularly the absence of closed-form optimal allocations typically\navailable in standard MAB settings. We design and analyze algorithms that\neither optimistically prioritize performance or pessimistically enforce\nconstraint satisfaction. For each algorithm, we derive problem-dependent upper\nbounds on both regret and constraint violations. Furthermore, we establish a\nlower bound demonstrating that the dependence on the time horizon in our\nresults is optimal in general and revealing fundamental limitations of the free\nexploration principle leveraged in prior work.", "AI": {"tldr": "The paper addresses a multi-armed bandit problem with contextual information aiming to balance fairness (minimum aggregated reward constraints) and maximizing cumulative reward, tackling performance trade-offs and proving optimality bounds.", "motivation": "To address real-world scenarios that require fair revenue allocation across arms while considering contextual variations.", "method": "Developed algorithms that focus either on prioritizing cumulative reward (optimistic) or enforcing constraints (pessimistic) and derived bounds for regret and constraint violations.", "result": "Demonstrated optimal bounds on time horizon dependence and established limitations of approaches using free exploration from prior work.", "conclusion": "The proposed algorithms effectively optimize cumulative reward while respecting constraint satisfaction, ensuring feasibility and fairness in contextual bandit settings."}}
{"id": "2510.12679", "pdf": "https://arxiv.org/pdf/2510.12679", "abs": "https://arxiv.org/abs/2510.12679", "authors": ["Zefu Lin", "Wenbo Chen", "Xiaojuan Jin", "Yuran Yang", "Lue Fan", "Yixin Zhang", "Yufeng Zhang", "Zhaoxiang Zhang"], "title": "MCOP: Multi-UAV Collaborative Occupancy Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) swarm systems necessitate efficient\ncollaborative perception mechanisms for diverse operational scenarios. Current\nBird's Eye View (BEV)-based approaches exhibit two main limitations:\nbounding-box representations fail to capture complete semantic and geometric\ninformation of the scene, and their performance significantly degrades when\nencountering undefined or occluded objects. To address these limitations, we\npropose a novel multi-UAV collaborative occupancy prediction framework. Our\nframework effectively preserves 3D spatial structures and semantics through\nintegrating a Spatial-Aware Feature Encoder and Cross-Agent Feature\nIntegration. To enhance efficiency, we further introduce Altitude-Aware Feature\nReduction to compactly represent scene information, along with a Dual-Mask\nPerceptual Guidance mechanism to adaptively select features and reduce\ncommunication overhead. Due to the absence of suitable benchmark datasets, we\nextend three datasets for evaluation: two virtual datasets (Air-to-Pred-Occ and\nUAV3D-Occ) and one real-world dataset (GauUScene-Occ). Experiments results\ndemonstrate that our method achieves state-of-the-art accuracy, significantly\noutperforming existing collaborative methods while reducing communication\noverhead to only a fraction of previous approaches.", "AI": {"tldr": "The paper introduces a multi-UAV collaborative occupancy prediction framework to address limitations in existing BEV-based methods, achieving better accuracy and lower communication overhead.", "motivation": "The paper addresses the inefficiencies in current collaborative UAV perception systems, particularly limitations of bounding-box-based BEV approaches in capturing geometric and semantic information.", "method": "The framework uses a Spatial-Aware Feature Encoder, Cross-Agent Feature Integration, an Altitude-Aware Feature Reduction, and a Dual-Mask Perceptual Guidance mechanism to enhance UAV collaboration and reduce communication.", "result": "The method achieves state-of-the-art performance on extended datasets, improving accuracy and significantly reducing communication overhead.", "conclusion": "The proposed framework demonstrates an effective solution to existing challenges, advancing collaborative UAV systems with superior accuracy and practicality."}}
{"id": "2510.12541", "pdf": "https://arxiv.org/pdf/2510.12541", "abs": "https://arxiv.org/abs/2510.12541", "authors": ["Jasmin Freudenberg", "Kai Hahn", "Christian Weber", "Madjid Fathi"], "title": "Evaluation of Real-Time Preprocessing Methods in AI-Based ECG Signal Analysis", "categories": ["cs.LG", "cs.AI"], "comment": "Conference paper for 2025 IEEE World AI IoT Congress (AIIoT), FACE\n  Project, University of Siegen, Germany", "summary": "The increasing popularity of portable ECG systems and the growing demand for\nprivacy-compliant, energy-efficient real-time analysis require new approaches\nto signal processing at the point of data acquisition. In this context, the\nedge domain is acquiring increasing importance, as it not only reduces latency\ntimes, but also enables an increased level of data security. The FACE project\naims to develop an innovative machine learning solution for analysing long-term\nelectrocardiograms that synergistically combines the strengths of edge and\ncloud computing. In this thesis, various pre-processing steps of ECG signals\nare analysed with regard to their applicability in the project. The selection\nof suitable methods in the edge area is based in particular on criteria such as\nenergy efficiency, processing capability and real-time capability.", "AI": {"tldr": "The FACE project focuses on creating a machine learning solution for long-term ECG analysis by leveraging the synergy between edge and cloud computing.", "motivation": "The paper is motivated by the need for privacy-compliant, energy-efficient real-time analysis of ECG data due to the increasing usage of portable ECG systems.", "method": "The study examines various ECG pre-processing methods suitable for edge computing, emphasizing criteria like energy efficiency, processing, and real-time capabilities.", "result": "It evaluates the applicability of pre-processing techniques for the FACE project's machine learning integration.", "conclusion": "Edge and cloud computing are synergistically combined to efficiently handle ECG signal analysis, prioritizing privacy and performance at the point of data acquisition."}}
{"id": "2510.12595", "pdf": "https://arxiv.org/pdf/2510.12595", "abs": "https://arxiv.org/abs/2510.12595", "authors": ["Kevin Kuo", "Chhavi Yadav", "Virginia Smith"], "title": "Research in Collaborative Learning Does Not Serve Cross-Silo Federated Learning in Practice", "categories": ["cs.LG"], "comment": "Main text: 23 pages, 2 tables, 2 figures", "summary": "Cross-silo federated learning (FL) is a promising approach to enable\ncross-organization collaboration in machine learning model development without\ndirectly sharing private data. Despite growing organizational interest driven\nby data protection regulations such as GDPR and HIPAA, the adoption of\ncross-silo FL remains limited in practice. In this paper, we conduct an\ninterview study to understand the practical challenges associated with\ncross-silo FL adoption. With interviews spanning a diverse set of stakeholders\nsuch as user organizations, software providers, and academic researchers, we\nuncover various barriers, from concerns about model performance to questions of\nincentives and trust between participating organizations. Our study shows that\ncross-silo FL faces a set of challenges that have yet to be well-captured by\nexisting research in the area and are quite distinct from other forms of\nfederated learning such as cross-device FL. We end with a discussion on future\nresearch directions that can help overcome these challenges.", "AI": {"tldr": "This paper examines the practical barriers to adopting cross-silo federated learning (FL), revealing challenges distinct from cross-device FL, through interviews with various stakeholders.", "motivation": "The paper aims to address limited adoption of cross-silo federated learning (FL) despite its potential for facilitating organizational collaboration without sharing private data, and the increasing interest due to data protection regulations like GDPR and HIPAA.", "method": "The study adopts an interview approach, engaging with various stakeholders such as user organizations, software providers, and academic researchers to identify challenges in implementing cross-silo FL.", "result": "The study identifies barriers to cross-silo FL adoption, including model performance concerns, incentive alignment, and trust issues between participating organizations.", "conclusion": "Cross-silo FL faces unique challenges distinct from cross-device FL, and the paper proposes future research directions to address these issues."}}
{"id": "2510.12704", "pdf": "https://arxiv.org/pdf/2510.12704", "abs": "https://arxiv.org/abs/2510.12704", "authors": ["Shelley Zixin Shu", "Haozhe Luo", "Alexander Poellinger", "Mauricio Reyes"], "title": "Hybrid Explanation-Guided Learning for Transformer-Based Chest X-Ray Diagnosis", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by iMIMIC at MICCAI 2025", "summary": "Transformer-based deep learning models have demonstrated exceptional\nperformance in medical imaging by leveraging attention mechanisms for feature\nrepresentation and interpretability. However, these models are prone to\nlearning spurious correlations, leading to biases and limited generalization.\nWhile human-AI attention alignment can mitigate these issues, it often depends\non costly manual supervision. In this work, we propose a Hybrid\nExplanation-Guided Learning (H-EGL) framework that combines self-supervised and\nhuman-guided constraints to enhance attention alignment and improve\ngeneralization. The self-supervised component of H-EGL leverages\nclass-distinctive attention without relying on restrictive priors, promoting\nrobustness and flexibility. We validate our approach on chest X-ray\nclassification using the Vision Transformer (ViT), where H-EGL outperforms two\nstate-of-the-art Explanation-Guided Learning (EGL) methods, demonstrating\nsuperior classification accuracy and generalization capability. Additionally,\nit produces attention maps that are better aligned with human expertise.", "AI": {"tldr": "A Hybrid Explanation-Guided Learning framework is introduced to improve attention alignment and generalization in medical imaging models, outperforming existing methods in chest X-ray classification.", "motivation": "Transformer-based models show great promise in medical imaging but suffer from biases and spurious correlations. Human-AI attention alignment can address these issues but often requires expensive manual supervision.", "method": "The proposed framework, Hybrid Explanation-Guided Learning (H-EGL), combines self-supervised and human-guided constraints to enhance attention alignment. The self-supervised module focuses on class-distinctive attention without strict priors.", "result": "The H-EGL framework demonstrated enhanced classification accuracy and generalization in chest X-ray analysis using Vision Transformer (ViT), outperforming existing methods. Additionally, it improved the alignment of attention maps with human expertise.", "conclusion": "H-EGL effectively improves biases and generalization issues in medical imaging, offering both robust performance and better interpretability through human-aligned attention maps."}}
{"id": "2510.12615", "pdf": "https://arxiv.org/pdf/2510.12615", "abs": "https://arxiv.org/abs/2510.12615", "authors": ["Israel Mason-Williams", "Gabryel Mason-Williams", "Helen Yannakoudakis"], "title": "Rethinking Knowledge Distillation: A Data Dependent Regulariser With a Negative Asymmetric Payoff", "categories": ["cs.LG", "cs.AI"], "comment": "45 pages, 24 figures and 104 tables", "summary": "Knowledge distillation is often considered a compression mechanism when\njudged on the resulting student's accuracy and loss, yet its functional impact\nis poorly understood. In this work, we quantify the compression capacity of\nknowledge distillation and the resulting knowledge transfer from a functional\nperspective, decoupling compression from architectural reduction, which\nprovides an improved understanding of knowledge distillation. We employ\nhypothesis testing, controls, and random control distillation to understand\nknowledge transfer mechanisms across data modalities. To rigorously test the\nbreadth and limits of our analyses, we explore multiple distillation variants\nand analyse distillation scaling laws across model sizes. Our findings\ndemonstrate that, while there is statistically significant knowledge transfer\nin some modalities and architectures, the extent of this transfer is less\npronounced than anticipated, even under conditions designed to maximise\nknowledge sharing. Notably, in cases of significant knowledge transfer, we\nidentify a consistent and severe asymmetric transfer of negative knowledge to\nthe student, raising safety concerns in knowledge distillation applications.\nAcross 12 experimental setups, 9 architectures, and 7 datasets, our findings\nshow that knowledge distillation functions less as a compression mechanism and\nmore as a data-dependent regulariser with a negative asymmetric payoff.", "AI": {"tldr": "The paper examines the functional impact of knowledge distillation, finding that its primary effect is as a data-dependent regularizer rather than an effective compression mechanism.", "motivation": "To understand the functional impact of knowledge distillation beyond its use as a compression mechanism, emphasizing insights into knowledge transfer dynamics and safety implications.", "method": "The study uses hypothesis testing, controls, random control distillation, various distillation variants, and scaling laws across model sizes and data modalities.", "result": "The analysis reveals limited and less pronounced knowledge transfer than expected, with significant cases showing an asymmetric transfer of negative knowledge to the student model.", "conclusion": "Knowledge distillation is not primarily a compression mechanism, but rather acts as a data-dependent regularizer, often resulting in the unsafe transfer of negative knowledge."}}
{"id": "2510.12712", "pdf": "https://arxiv.org/pdf/2510.12712", "abs": "https://arxiv.org/abs/2510.12712", "authors": ["Xingang Guo", "Utkarsh Tyagi", "Advait Gosai", "Paula Vergara", "Ernesto Gabriel Hern\u00e1ndez Montoya", "Chen Bo Calvin Zhang", "Bin Hu", "Yunzhong He", "Bing Liu", "Rakshith Sharma Srinivasa"], "title": "Beyond Seeing: Evaluating Multimodal LLMs on Tool-Enabled Image Perception, Transformation, and Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) are increasingly applied in\nreal-world scenarios where user-provided images are often imperfect, requiring\nactive image manipulations such as cropping, editing, or enhancement to uncover\nsalient visual cues. Beyond static visual perception, MLLMs must also think\nwith images: dynamically transforming visual content and integrating it with\nother tools to solve complex tasks. However, this shift from treating vision as\npassive context to a manipulable cognitive workspace remains underexplored.\nMost existing benchmarks still follow a think about images paradigm, where\nimages are regarded as static inputs. To address this gap, we introduce IRIS,\nan Interactive Reasoning with Images and Systems that evaluates MLLMs' ability\nto perceive, transform, and reason across complex visual-textual tasks under\nthe think with images paradigm. IRIS comprises 1,204 challenging, open-ended\nvision tasks (603 single-turn, 601 multi-turn) spanning across five diverse\ndomains, each paired with detailed rubrics to enable systematic evaluation. Our\nevaluation shows that current MLLMs struggle with tasks requiring effective\nintegration of vision and general-purpose tools. Even the strongest model\n(GPT-5-think) reaches only 18.68% pass rate. We further observe divergent\ntool-use behaviors, with OpenAI models benefiting from diverse image\nmanipulations while Gemini-2.5-pro shows no improvement. By introducing the\nfirst benchmark centered on think with images, IRIS offers critical insights\nfor advancing visual intelligence in MLLMs.", "AI": {"tldr": "This paper introduces IRIS, a benchmark for testing Multimodal Large Language Models (MLLMs) on tasks requiring interactive image manipulation and reasoning.", "motivation": "Current benchmarks for MLLMs consider static images, neglecting the need for active visual transformations and dynamic reasoning.", "method": "The study develops IRIS, a benchmark with 1,204 vision-textual tasks across five domains, evaluating MLLMs on their ability to perceive, transform, and reason with images.", "result": "Evaluation reveals that current MLLMs struggle with interactive tasks, with the best model achieving an 18.68% pass rate, and shows variation in tool-use effectiveness among models.", "conclusion": "IRIS highlights the limitations of current MLLMs with interactive image tasks and provides a pathway for advancing their visual reasoning capabilities."}}
{"id": "2510.12618", "pdf": "https://arxiv.org/pdf/2510.12618", "abs": "https://arxiv.org/abs/2510.12618", "authors": ["Manuel Hinz", "Maximilian Mauel", "Patrick Seifner", "David Berghaus", "Kostadin Cvejoski", "Ramses J. Sanchez"], "title": "Towards Fast Coarse-graining and Equation Discovery with Foundation Inference Models", "categories": ["cs.LG"], "comment": null, "summary": "High-dimensional recordings of dynamical processes are often characterized by\na much smaller set of effective variables, evolving on low-dimensional\nmanifolds. Identifying these latent dynamics requires solving two intertwined\nproblems: discovering appropriate coarse-grained variables and simultaneously\nfitting the governing equations. Most machine learning approaches tackle these\ntasks jointly by training autoencoders together with models that enforce\ndynamical consistency. We propose to decouple the two problems by leveraging\nthe recently introduced Foundation Inference Models (FIMs). FIMs are pretrained\nmodels that estimate the infinitesimal generators of dynamical systems (e.g.,\nthe drift and diffusion of a stochastic differential equation) in zero-shot\nmode. By amortizing the inference of the dynamics through a FIM with frozen\nweights, and training only the encoder-decoder map, we define a simple,\nsimulation-consistent loss that stabilizes representation learning. A proof of\nconcept on a stochastic double-well system with semicircle diffusion, embedded\ninto synthetic video data, illustrates the potential of this approach for fast\nand reusable coarse-graining pipelines.", "AI": {"tldr": "The paper introduces a method to identify low-dimensional latent dynamics in dynamical systems by using pretrained models, known as Foundation Inference Models (FIMs), to decouple variable discovery and equation fitting.", "motivation": "High-dimensional dynamical systems often have underlying low-dimensional structures, but identifying these latent dynamics is challenging and requires discovering appropriate variables while fitting governing equations.", "method": "The authors leverage pretrained FIMs to estimate dynamical system generators in zero-shot mode. By freezing FIM weights, they focus on training encoder-decoder maps, stabilizing representation learning with a simulation-consistent loss.", "result": "The approach is tested on a stochastic double-well system embedded in synthetic video data, showcasing its capability for efficient and reusable coarse-graining pipelines.", "conclusion": "The proposed method simplifies and stabilizes the process of identifying latent dynamics, offering a fast and reusable tool for understanding complex dynamical systems."}}
{"id": "2510.11851", "pdf": "https://arxiv.org/pdf/2510.11851", "abs": "https://arxiv.org/abs/2510.11851", "authors": ["Shuo Chen", "Zonggen Li", "Zhen Han", "Bailan He", "Tong Liu", "Haokun Chen", "Georg Groh", "Philip Torr", "Volker Tresp", "Jindong Gu"], "title": "Deep Research Brings Deeper Harm", "categories": ["cs.CR", "cs.CL"], "comment": "Accepted to Reliable ML from Unreliable Data Workshop @ NeurIPS 2025", "summary": "Deep Research (DR) agents built on Large Language Models (LLMs) can perform\ncomplex, multi-step research by decomposing tasks, retrieving online\ninformation, and synthesizing detailed reports. However, the misuse of LLMs\nwith such powerful capabilities can lead to even greater risks. This is\nespecially concerning in high-stakes and knowledge-intensive domains such as\nbiosecurity, where DR can generate a professional report containing detailed\nforbidden knowledge. Unfortunately, we have found such risks in practice:\nsimply submitting a harmful query, which a standalone LLM directly rejects, can\nelicit a detailed and dangerous report from DR agents. This highlights the\nelevated risks and underscores the need for a deeper safety analysis. Yet,\njailbreak methods designed for LLMs fall short in exposing such unique risks,\nas they do not target the research ability of DR agents. To address this gap,\nwe propose two novel jailbreak strategies: Plan Injection, which injects\nmalicious sub-goals into the agent's plan; and Intent Hijack, which reframes\nharmful queries as academic research questions. We conducted extensive\nexperiments across different LLMs and various safety benchmarks, including\ngeneral and biosecurity forbidden prompts. These experiments reveal 3 key\nfindings: (1) Alignment of the LLMs often fail in DR agents, where harmful\nprompts framed in academic terms can hijack agent intent; (2) Multi-step\nplanning and execution weaken the alignment, revealing systemic vulnerabilities\nthat prompt-level safeguards cannot address; (3) DR agents not only bypass\nrefusals but also produce more coherent, professional, and dangerous content,\ncompared with standalone LLMs. These results demonstrate a fundamental\nmisalignment in DR agents and call for better alignment techniques tailored to\nDR agents. Code and datasets are available at\nhttps://chenxshuo.github.io/deeper-harm.", "AI": {"tldr": "Deep Research (DR) agents built on Large Language Models (LLMs) exhibit significant risks due to systemic alignment issues, especially in high-stakes domains like biosecurity.", "motivation": "To address the risks posed by the misuse of DR agents, particularly their ability to produce forbidden knowledge and bypass safeguards in complex and dangerous scenarios.", "method": "Proposal of two jailbreak strategies (Plan Injection and Intent Hijack) to investigate vulnerabilities in DR agents and experimentation across multiple LLMs and safety benchmarks.", "result": "Experiments revealed systemic vulnerabilities where DR agents bypass safeguards, weaken alignment through multi-step planning, and generate dangerous and coherent reports, surpassing standalone LLMs.", "conclusion": "DR agents demonstrate misalignment vulnerabilities that current safeguards cannot adequately address, and there is a need for advanced safety and alignment methods tailored to these agents."}}
{"id": "2510.12624", "pdf": "https://arxiv.org/pdf/2510.12624", "abs": "https://arxiv.org/abs/2510.12624", "authors": ["Yuta Kobayashi", "Zilin Jing", "Jiayu Yao", "Hongseok Namkoong", "Shalmali Joshi"], "title": "Learning-To-Measure: In-context Active Feature Acquisition", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Active feature acquisition (AFA) is a sequential decision-making problem\nwhere the goal is to improve model performance for test instances by adaptively\nselecting which features to acquire. In practice, AFA methods often learn from\nretrospective data with systematic missingness in the features and limited\ntask-specific labels. Most prior work addresses acquisition for a single\npredetermined task, limiting scalability. To address this limitation, we\nformalize the meta-AFA problem, where the goal is to learn acquisition policies\nacross various tasks. We introduce Learning-to-Measure (L2M), which consists of\ni) reliable uncertainty quantification over unseen tasks, and ii) an\nuncertainty-guided greedy feature acquisition agent that maximizes conditional\nmutual information. We demonstrate a sequence-modeling or autoregressive\npre-training approach that underpins reliable uncertainty quantification for\ntasks with arbitrary missingness. L2M operates directly on datasets with\nretrospective missingness and performs the meta-AFA task in-context,\neliminating per-task retraining. Across synthetic and real-world tabular\nbenchmarks, L2M matches or surpasses task-specific baselines, particularly\nunder scarce labels and high missingness.", "AI": {"tldr": "This paper proposes Learning-to-Measure (L2M), a solution to the meta-Active Feature Acquisition (meta-AFA) problem, enabling adaptive feature acquisition across multiple tasks and performing well even with missing or limited data.", "motivation": "The motivation is to address the limitations in Active Feature Acquisition (AFA) methods, which traditionally focus on single predetermined tasks and struggle with limited labels and systematic missingness in features.", "method": "The method, named L2M, combines reliable uncertainty quantification for unseen tasks and an uncertainty-guided greedy feature acquisition approach. It uses a sequence-modeling pre-training methodology to handle missing data and tasks with retrospective missingness.", "result": "L2M outperforms task-specific baselines on synthetic and real-world tabular data, specifically when data is sparse or has high levels of missingness.", "conclusion": "L2M provides a scalable meta-AFA solution by eliminating the need for per-task retraining and demonstrating robustness under challenging data conditions."}}
{"id": "2510.12747", "pdf": "https://arxiv.org/pdf/2510.12747", "abs": "https://arxiv.org/abs/2510.12747", "authors": ["Junhao Zhuang", "Shi Guo", "Xin Cai", "Xiaohui Li", "Yihao Liu", "Chun Yuan", "Tianfan Xue"], "title": "FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution", "categories": ["cs.CV"], "comment": "Project page with code: https://zhuang2002.github.io/FlashVSR", "summary": "Diffusion models have recently advanced video restoration, but applying them\nto real-world video super-resolution (VSR) remains challenging due to high\nlatency, prohibitive computation, and poor generalization to ultra-high\nresolutions. Our goal in this work is to make diffusion-based VSR practical by\nachieving efficiency, scalability, and real-time performance. To this end, we\npropose FlashVSR, the first diffusion-based one-step streaming framework\ntowards real-time VSR. FlashVSR runs at approximately 17 FPS for 768x1408\nvideos on a single A100 GPU by combining three complementary innovations: (i) a\ntrain-friendly three-stage distillation pipeline that enables streaming\nsuper-resolution, (ii) locality-constrained sparse attention that cuts\nredundant computation while bridging the train-test resolution gap, and (iii) a\ntiny conditional decoder that accelerates reconstruction without sacrificing\nquality. To support large-scale training, we also construct VSR-120K, a new\ndataset with 120k videos and 180k images. Extensive experiments show that\nFlashVSR scales reliably to ultra-high resolutions and achieves\nstate-of-the-art performance with up to 12x speedup over prior one-step\ndiffusion VSR models. We will release the code, pretrained models, and dataset\nto foster future research in efficient diffusion-based VSR.", "AI": {"tldr": "FlashVSR introduces an innovative diffusion-based framework for efficient and real-time video super-resolution (VSR), achieving significantly faster speeds and scalability compared to existing methods.", "motivation": "The paper addresses challenges in using diffusion models for real-world VSR, such as high latency, heavy computational demands, and poor scalability to ultra-high resolutions.", "method": "FlashVSR combines a three-stage distillation pipeline, locality-constrained sparse attention for computation reduction, and a tiny conditional decoder to enhance performance for streaming VSR.", "result": "FlashVSR achieves 17 FPS on 768x1408 videos using a single A100 GPU, scales effectively to ultra-high resolutions, and delivers up to 12x speedup over previous diffusion VSR models, validated by experiments with the VSR-120K dataset.", "conclusion": "The proposed solution demonstrates state-of-the-art performance in practical VSR applications, with high efficiency and scalability, paving the way for future advancements in diffusion models."}}
{"id": "2510.12000", "pdf": "https://arxiv.org/pdf/2510.12000", "abs": "https://arxiv.org/abs/2510.12000", "authors": ["Jinchuan Tian", "Sang-gil Lee", "Zhifeng Kong", "Sreyan Ghosh", "Arushi Goel", "Chao-Han Huck Yang", "Wenliang Dai", "Zihan Liu", "Hanrong Ye", "Shinji Watanabe", "Mohammad Shoeybi", "Bryan Catanzaro", "Rafael Valle", "Wei Ping"], "title": "UALM: Unified Audio Language Model for Understanding, Generation and Reasoning", "categories": ["cs.SD", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in the audio language modeling (ALM) domain tackle audio\nunderstanding and text-to-audio generation as separate tasks. Very few studies\nattempt to unify these tasks -- an essential step toward advanced multimodal\nreasoning. This paper introduces U}nified Audio Language Model (UALM), which\naims to unify audio understanding, text-to-audio generation, and multimodal\nreasoning in a single model. To achieve this goal, we first present UALM-Gen, a\ntext-to-audio language model that directly predicts audio tokens and is\ncomparable to state-of-the-art diffusion-based models. We then demonstrate,\nusing proper data blending, training recipes, and inference techniques, that\nour single UALM model matches the quality of state-of-the-art specialized\nmodels in audio understanding, text-to-audio generation, and text reasoning.\nFurthermore, we present UALM-Reason, a multimodal reasoning model that utilizes\nboth text and audio in the intermediate thinking steps to facilitate complex\ngeneration tasks. To our knowledge, this is the first demonstration in audio\nresearch of cross-modal generative reasoning, with its effectiveness confirmed\nby subjective evaluations.", "AI": {"tldr": "The paper introduces Unified Audio Language Model (UALM), unifying audio understanding, text-to-audio generation, and multimodal reasoning.", "motivation": "To address the lack of unified approaches to handle audio understanding, text-to-audio generation, and multimodal reasoning within a single framework.", "method": "Developed UALM-Gen for text-to-audio tasks and UALM-Reason for multimodal reasoning, applying advanced training recipes, data blending, and inference techniques.", "result": "UALM matches specialized models' performance in audio tasks and showcases cross-modal generative reasoning effectiveness.", "conclusion": "UALM marks a significant step in unifying multimodal tasks, setting a foundation for complex reasoning and generation in audio research."}}
{"id": "2510.12749", "pdf": "https://arxiv.org/pdf/2510.12749", "abs": "https://arxiv.org/abs/2510.12749", "authors": ["Zhiliu Yang", "Jinyu Dai", "Jianyuan Zhang", "Zhu Yang"], "title": "SPORTS: Simultaneous Panoptic Odometry, Rendering, Tracking and Segmentation for Urban Scenes Understanding", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Multimedia", "summary": "The scene perception, understanding, and simulation are fundamental\ntechniques for embodied-AI agents, while existing solutions are still prone to\nsegmentation deficiency, dynamic objects' interference, sensor data sparsity,\nand view-limitation problems. This paper proposes a novel framework, named\nSPORTS, for holistic scene understanding via tightly integrating Video Panoptic\nSegmentation (VPS), Visual Odometry (VO), and Scene Rendering (SR) tasks into\nan iterative and unified perspective. Firstly, VPS designs an adaptive\nattention-based geometric fusion mechanism to align cross-frame features via\nenrolling the pose, depth, and optical flow modality, which automatically\nadjust feature maps for different decoding stages. And a post-matching strategy\nis integrated to improve identities tracking. In VO, panoptic segmentation\nresults from VPS are combined with the optical flow map to improve the\nconfidence estimation of dynamic objects, which enhances the accuracy of the\ncamera pose estimation and completeness of the depth map generation via the\nlearning-based paradigm. Furthermore, the point-based rendering of SR is\nbeneficial from VO, transforming sparse point clouds into neural fields to\nsynthesize high-fidelity RGB views and twin panoptic views. Extensive\nexperiments on three public datasets demonstrate that our attention-based\nfeature fusion outperforms most existing state-of-the-art methods on the\nodometry, tracking, segmentation, and novel view synthesis tasks.", "AI": {"tldr": "The paper introduces SPORTS, a framework integrating Video Panoptic Segmentation, Visual Odometry, and Scene Rendering for holistic scene understanding, overcoming common challenges in embodied AI.", "motivation": "Current methods for scene perception and simulation in embodied AI suffer from issues like segmentation deficiency, dynamic object interference, sparse sensor data, and view limitations.", "method": "The SPORTS framework combines adaptive attention-based geometric fusion in VPS, improved confidence estimation in VO using panoptic data, and point-based rendering in SR for high-fidelity scene synthesis.", "result": "Experiments on public datasets show the proposed methods outperform state-of-the-art approaches in odometry, tracking, segmentation, and novel view synthesis.", "conclusion": "SPORTS proves effective for improving scene understanding and simulation by integrating multiple tasks and addressing known limitations of current techniques."}}
{"id": "2510.12638", "pdf": "https://arxiv.org/pdf/2510.12638", "abs": "https://arxiv.org/abs/2510.12638", "authors": ["Arip Asadulaev", "Fakhri Karray", "Martin Takac"], "title": "Expert or not? assessing data quality in offline reinforcement learning", "categories": ["cs.LG"], "comment": null, "summary": "Offline reinforcement learning (RL) learns exclusively from static datasets,\nwithout further interaction with the environment. In practice, such datasets\nvary widely in quality, often mixing expert, suboptimal, and even random\ntrajectories. The choice of algorithm therefore depends on dataset fidelity.\nBehavior cloning can suffice on high-quality data, whereas mixed- or\nlow-quality data typically benefits from offline RL methods that stitch useful\nbehavior across trajectories. Yet in the wild it is difficult to assess dataset\nquality a priori because the data's provenance and skill composition are\nunknown. We address the problem of estimating offline dataset quality without\ntraining an agent. We study a spectrum of proxies from simple cumulative\nrewards to learned value based estimators, and introduce the Bellman\nWasserstein distance (BWD), a value aware optimal transport score that measures\nhow dissimilar a dataset's behavioral policy is from a random reference policy.\nBWD is computed from a behavioral critic and a state conditional OT\nformulation, requiring no environment interaction or full policy optimization.\nAcross D4RL MuJoCo tasks, BWD strongly correlates with an oracle performance\nscore that aggregates multiple offline RL algorithms, enabling efficient\nprediction of how well standard agents will perform on a given dataset. Beyond\nprediction, integrating BWD as a regularizer during policy optimization\nexplicitly pushes the learned policy away from random behavior and improves\nreturns. These results indicate that value aware, distributional signals such\nas BWD are practical tools for triaging offline RL datasets and policy\noptimization.", "AI": {"tldr": "The paper proposes a new metric, Bellman Wasserstein Distance (BWD), to estimate the quality of offline reinforcement learning datasets without requiring agent training.", "motivation": "The authors aim to address the problem of assessing offline RL dataset quality in the absence of prior knowledge about the data's origin, skill composition, or quality.", "method": "The proposed approach uses the Bellman Wasserstein Distance (BWD), a value-aware optimal transport score computed using a behavioral critic and state-conditional optimal transport. BWD measures the dissimilarity between a dataset's behavioral policy and a random reference policy without requiring environment interactions.", "result": "The study shows that BWD correlates strongly with an oracle performance score and can predict the performance of standard RL agents on certain datasets. It also serves as a regularizer during policy optimization to improve returns.", "conclusion": "BWD appears to be a practical and effective tool for evaluating offline RL dataset quality and optimizing policies by guiding them away from random behaviors."}}
{"id": "2510.12750", "pdf": "https://arxiv.org/pdf/2510.12750", "abs": "https://arxiv.org/abs/2510.12750", "authors": ["A. Alfarano", "L. Venturoli", "D. Negueruela del Castillo"], "title": "VQArt-Bench: A semantically rich VQA Benchmark for Art and Cultural Heritage", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant\ncapabilities in joint visual and linguistic tasks. However, existing Visual\nQuestion Answering (VQA) benchmarks often fail to evaluate deep semantic\nunderstanding, particularly in complex domains like visual art analysis.\nConfined to simple syntactic structures and surface-level attributes, these\nquestions fail to capture the diversity and depth of human visual inquiry. This\nlimitation incentivizes models to exploit statistical shortcuts rather than\nengage in visual reasoning. To address this gap, we introduce VQArt-Bench, a\nnew, large-scale VQA benchmark for the cultural heritage domain. This benchmark\nis constructed using a novel multi-agent pipeline where specialized agents\ncollaborate to generate nuanced, validated, and linguistically diverse\nquestions. The resulting benchmark is structured along relevant visual\nunderstanding dimensions that probe a model's ability to interpret symbolic\nmeaning, narratives, and complex visual relationships. Our evaluation of 14\nstate-of-the-art MLLMs on this benchmark reveals significant limitations in\ncurrent models, including a surprising weakness in simple counting tasks and a\nclear performance gap between proprietary and open-source models.", "AI": {"tldr": "The paper introduces VQArt-Bench, a new Visual Question Answering benchmark focusing on deep semantic understanding in the cultural heritage domain, revealing significant limitations in current MLLMs.", "motivation": "Existing VQA benchmarks lack the depth to evaluate models' semantic understanding, especially in complex domains like visual art, incentivizing models to rely on shortcuts rather than true reasoning.", "method": "The authors developed VQArt-Bench, a large-scale VQA benchmark using a multi-agent pipeline to create diverse and validated questions that evaluate models across dimensions like symbolic meaning, narratives, and visual relationships.", "result": "Testing 14 advanced MLLMs with VQArt-Bench revealed weaknesses, including failures in simple counting tasks and a performance gap between proprietary and open-source models.", "conclusion": "Current MLLMs exhibit notable limitations in visual reasoning and semantic understanding; VQArt-Bench provides a robust tool for assessing progress in these areas."}}
{"id": "2510.12640", "pdf": "https://arxiv.org/pdf/2510.12640", "abs": "https://arxiv.org/abs/2510.12640", "authors": ["David Berghaus", "Patrick Seifner", "Kostadin Cvejoski", "Ramses J. Sanchez"], "title": "On Foundation Models for Temporal Point Processes to Accelerate Scientific Discovery", "categories": ["cs.LG"], "comment": null, "summary": "Many scientific fields, from medicine to seismology, rely on analyzing\nsequences of events over time to understand complex systems. Traditionally,\nmachine learning models must be built and trained from scratch for each new\ndataset, which is a slow and costly process. We introduce a new approach: a\nsingle, powerful model that learns the underlying patterns of event data in\ncontext. We trained this \"foundation model\" on millions of simulated event\nsequences, teaching it a general-purpose understanding of how events can\nunfold. As a result, our model can analyze new scientific data instantly,\nwithout retraining, simply by looking at a few examples from the dataset. It\ncan also be quickly fine-tuned for even higher accuracy. This approach makes\nsophisticated event analysis more accessible and accelerates the pace of\nscientific discovery.", "AI": {"tldr": "The paper introduces a foundational machine learning model for analyzing event sequences, trained on millions of simulated data, enabling quick adaptation to new datasets without retraining.", "motivation": "The motivation is to eliminate the inefficiency of building and training machine learning models from scratch for each new dataset in scientific fields analyzing event sequences.", "method": "The authors developed a foundational model pretrained on millions of simulated event sequences, allowing it to generalize and understand event patterns in various scientific datasets.", "result": "The model demonstrated the ability to analyze new datasets instantly by observing a few examples, and could be fine-tuned for higher accuracy, making event data analysis more efficient.", "conclusion": "This approach simplifies event sequence analysis, reduces computational costs, and accelerates scientific discovery by removing the need for retraining on every new dataset."}}
{"id": "2510.11974", "pdf": "https://arxiv.org/pdf/2510.11974", "abs": "https://arxiv.org/abs/2510.11974", "authors": ["Yutong Cheng", "Yang Liu", "Changze Li", "Dawn Song", "Peng Gao"], "title": "CTIArena: Benchmarking LLM Knowledge and Reasoning Across Heterogeneous Cyber Threat Intelligence", "categories": ["cs.CR", "cs.AI"], "comment": "Under peer-review", "summary": "Cyber threat intelligence (CTI) is central to modern cybersecurity, providing\ncritical insights for detecting and mitigating evolving threats. With the\nnatural language understanding and reasoning capabilities of large language\nmodels (LLMs), there is increasing interest in applying them to CTI, which\ncalls for benchmarks that can rigorously evaluate their performance. Several\nearly efforts have studied LLMs on some CTI tasks but remain limited: (i) they\nadopt only closed-book settings, relying on parametric knowledge without\nleveraging CTI knowledge bases; (ii) they cover only a narrow set of tasks,\nlacking a systematic view of the CTI landscape; and (iii) they restrict\nevaluation to single-source analysis, unlike realistic scenarios that require\nreasoning across multiple sources. To fill these gaps, we present CTIArena, the\nfirst benchmark for evaluating LLM performance on heterogeneous, multi-source\nCTI under knowledge-augmented settings. CTIArena spans three categories,\nstructured, unstructured, and hybrid, further divided into nine tasks that\ncapture the breadth of CTI analysis in modern security operations. We evaluate\nten widely used LLMs and find that most struggle in closed-book setups but show\nnoticeable gains when augmented with security-specific knowledge through our\ndesigned retrieval-augmented techniques. These findings highlight the\nlimitations of general-purpose LLMs and the need for domain-tailored techniques\nto fully unlock their potential for CTI.", "AI": {"tldr": "The paper introduces CTIArena, a benchmark for evaluating large language models (LLMs) in the context of cyber threat intelligence (CTI), addressing limitations in current studies.", "motivation": "There is growing interest in applying LLMs to CTI, but existing efforts lack a comprehensive evaluation system and fail to leverage external knowledge bases or multi-source analysis.", "method": "The authors developed CTIArena to evaluate LLMs across a wide range of CTI tasks under heterogeneous and knowledge-augmented conditions, using retrieval-augmented techniques.", "result": "Ten LLMs were tested, showing struggles in closed-book scenarios but improving significantly when complemented with security-specific knowledge.", "conclusion": "General-purpose LLMs are insufficient for CTI tasks, highlighting the necessity for domain-specific methods to optimize their performance."}}
{"id": "2510.12753", "pdf": "https://arxiv.org/pdf/2510.12753", "abs": "https://arxiv.org/abs/2510.12753", "authors": ["Wenpu Li", "Bangyan Liao", "Yi Zhou", "Qi Xu", "Pian Wan", "Peidong Liu"], "title": "E-MoFlow: Learning Egomotion and Optical Flow from Event Data via Implicit Regularization", "categories": ["cs.CV"], "comment": "The Thirty-Ninth Annual Conference on Neural Information Processing\n  Systems(NeurIPS 2025)", "summary": "The estimation of optical flow and 6-DoF ego-motion, two fundamental tasks in\n3D vision, has typically been addressed independently. For neuromorphic vision\n(e.g., event cameras), however, the lack of robust data association makes\nsolving the two problems separately an ill-posed challenge, especially in the\nabsence of supervision via ground truth. Existing works mitigate this\nill-posedness by either enforcing the smoothness of the flow field via an\nexplicit variational regularizer or leveraging explicit structure-and-motion\npriors in the parametrization to improve event alignment. The former notably\nintroduces bias in results and computational overhead, while the latter, which\nparametrizes the optical flow in terms of the scene depth and the camera\nmotion, often converges to suboptimal local minima. To address these issues, we\npropose an unsupervised framework that jointly optimizes egomotion and optical\nflow via implicit spatial-temporal and geometric regularization. First, by\nmodeling camera's egomotion as a continuous spline and optical flow as an\nimplicit neural representation, our method inherently embeds spatial-temporal\ncoherence through inductive biases. Second, we incorporate structure-and-motion\npriors through differential geometric constraints, bypassing explicit depth\nestimation while maintaining rigorous geometric consistency. As a result, our\nframework (called E-MoFlow) unifies egomotion and optical flow estimation via\nimplicit regularization under a fully unsupervised paradigm. Experiments\ndemonstrate its versatility to general 6-DoF motion scenarios, achieving\nstate-of-the-art performance among unsupervised methods and competitive even\nwith supervised approaches.", "AI": {"tldr": "The paper introduces E-MoFlow, an unsupervised framework for jointly optimizing optical flow and 6-DoF egomotion using implicit spatial-temporal and geometric regularization, offering improved accuracy in neuromorphic vision tasks without supervision.", "motivation": "The independent estimation of optical flow and 6-DoF egomotion in neuromorphic vision systems is ill-posed due to a lack of robust data association and ground-truth supervision. Existing solutions introduce bias or face optimization challenges.", "method": "The authors propose E-MoFlow, which models egomotion as a continuous spline and optical flow as an implicit neural representation to enforce spatial-temporal coherence. It incorporates differential geometric constraints for structure-and-motion priors without explicit depth estimation.", "result": "E-MoFlow demonstrates state-of-the-art performance for unsupervised optical flow and egomotion estimation tasks, showing robustness in 6-DoF motion scenarios and competitiveness with supervised methods.", "conclusion": "The framework effectively addresses limitations in neuromorphic vision tasks by jointly optimizing optical flow and egomotion under an unsupervised paradigm, providing a unified and geometrically consistent approach."}}
{"id": "2510.12650", "pdf": "https://arxiv.org/pdf/2510.12650", "abs": "https://arxiv.org/abs/2510.12650", "authors": ["Maximilian Mauel", "Manuel Hinz", "Patrick Seifner", "David Berghaus", "Ramses J. Sanchez"], "title": "Towards Foundation Inference Models that Learn ODEs In-Context", "categories": ["cs.LG"], "comment": null, "summary": "Ordinary differential equations (ODEs) describe dynamical systems evolving\ndeterministically in continuous time. Accurate data-driven modeling of systems\nas ODEs, a central problem across the natural sciences, remains challenging,\nespecially if the data is sparse or noisy. We introduce FIM-ODE (Foundation\nInference Model for ODEs), a pretrained neural model designed to estimate ODEs\nzero-shot (i.e., in context) from sparse and noisy observations. Trained on\nsynthetic data, the model utilizes a flexible neural operator for robust ODE\ninference, even from corrupted data. We empirically verify that FIM-ODE\nprovides accurate estimates, on par with a neural state-of-the-art method, and\nqualitatively compare the structure of their estimated vector fields.", "AI": {"tldr": "This paper introduces FIM-ODE, a pretrained neural model for accurately estimating ODEs from sparse and noisy data.", "motivation": "Accurate modeling of dynamical systems as ODEs is difficult, particularly with sparse and noisy datasets.", "method": "The authors propose FIM-ODE, a pretrained neural operator trained on synthetic data, capable of zero-shot inference.", "result": "FIM-ODE empirically provides accurate ODE estimates comparable to state-of-the-art neural methods, even with corrupted data.", "conclusion": "The paper demonstrates FIM-ODE's capability for robust data-driven ODE modeling, addressing challenges posed by data sparsity and noise."}}
{"id": "2510.12758", "pdf": "https://arxiv.org/pdf/2510.12758", "abs": "https://arxiv.org/abs/2510.12758", "authors": ["Zhuotong Cai", "Tianyi Zeng", "Jiazhen Zhang", "El\u00e9onore V. Lieffrig", "Kathryn Fontaine", "Chenyu You", "Enette Mae Revilla", "James S. Duncan", "Jingmin Xin", "Yihuan Lu", "John A. Onofrey"], "title": "PET Head Motion Estimation Using Supervised Deep Learning with Attention", "categories": ["cs.CV"], "comment": "Accepted for publication in IEEE Transactions on Medical Imaging\n  (TMI), 2025. This is the accepted manuscript version", "summary": "Head movement poses a significant challenge in brain positron emission\ntomography (PET) imaging, resulting in image artifacts and tracer uptake\nquantification inaccuracies. Effective head motion estimation and correction\nare crucial for precise quantitative image analysis and accurate diagnosis of\nneurological disorders. Hardware-based motion tracking (HMT) has limited\napplicability in real-world clinical practice. To overcome this limitation, we\npropose a deep-learning head motion correction approach with cross-attention\n(DL-HMC++) to predict rigid head motion from one-second 3D PET raw data.\nDL-HMC++ is trained in a supervised manner by leveraging existing dynamic PET\nscans with gold-standard motion measurements from external HMT. We evaluate\nDL-HMC++ on two PET scanners (HRRT and mCT) and four radiotracers (18F-FDG,\n18F-FPEB, 11C-UCB-J, and 11C-LSN3172176) to demonstrate the effectiveness and\ngeneralization of the approach in large cohort PET studies. Quantitative and\nqualitative results demonstrate that DL-HMC++ consistently outperforms\nstate-of-the-art data-driven motion estimation methods, producing motion-free\nimages with clear delineation of brain structures and reduced motion artifacts\nthat are indistinguishable from gold-standard HMT. Brain region of interest\nstandard uptake value analysis exhibits average difference ratios between\nDL-HMC++ and gold-standard HMT to be 1.2 plus-minus 0.5% for HRRT and 0.5\nplus-minus 0.2% for mCT. DL-HMC++ demonstrates the potential for data-driven\nPET head motion correction to remove the burden of HMT, making motion\ncorrection accessible to clinical populations beyond research settings. The\ncode is available at https://github.com/maxxxxxxcai/DL-HMC-TMI.", "AI": {"tldr": "The paper introduces DL-HMC++, a deep-learning approach using cross-attention for head motion correction in PET imaging, showcasing high effectiveness and generalization across scanners and radiotracers.", "motivation": "Head movement during PET imaging generates artifacts and inaccuracies, making it necessary to develop an alternative to hardware-based motion tracking for better clinical application.", "method": "DL-HMC++ utilizes supervised deep learning with cross-attention based on dynamic PET scans and gold-standard external hardware motion measurements to estimate and correct head motion.", "result": "DL-HMC++ consistently outperformed existing methods in motion correction, achieving motion-free imaging and reducing artifacts, with an average difference ratio of less than 2% compared to gold-standard hardware tracking.", "conclusion": "DL-HMC++ is a promising data-driven solution, reducing dependency on hardware motion tracking and improving clinical accessibility for PET imaging motion correction."}}
{"id": "2510.12659", "pdf": "https://arxiv.org/pdf/2510.12659", "abs": "https://arxiv.org/abs/2510.12659", "authors": ["Chih-Chuan Cheng", "Yi-Ju Tseng"], "title": "SG-XDEAT: Sparsity-Guided Cross-Dimensional and Cross-Encoding Attention with Target-Aware Conditioning in Tabular Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose SG-XDEAT (Sparsity-Guided Cross Dimensional and Cross-Encoding\nAttention with Target Aware Conditioning), a novel framework designed for\nsupervised learning on tabular data. At its core, SG-XDEAT employs a\ndual-stream encoder that decomposes each input feature into two parallel\nrepresentations: a raw value stream and a target-conditioned (label-aware)\nstream. These dual representations are then propagated through a hierarchical\nstack of attention-based modules. SG-XDEAT integrates three key components: (i)\nCross-Dimensional self-attention, which captures intra-view dependencies among\nfeatures within each stream; (ii) Cross-Encoding self-attention, which enables\nbidirectional interaction between raw and target-aware representations; and\n(iii) an Adaptive Sparse Self-Attention (ASSA) mechanism, which dynamically\nsuppresses low-utility tokens by driving their attention weights toward\nzero--thereby mitigating the impact of noise. Empirical results on multiple\npublic benchmarks show consistent gains over strong baselines, confirming that\njointly modeling raw and target-aware views--while adaptively filtering\nnoise--yields a more robust deep tabular learner.", "AI": {"tldr": "SG-XDEAT is a novel framework for supervised learning on tabular data, utilizing dual-stream encoding, hierarchical attention modules, and adaptive sparse self-attention to improve robustness and performance.", "motivation": "To address the challenges of supervised learning on tabular data by effectively modeling raw and label-aware feature representations while mitigating noise in the data.", "method": "Introduces a dual-stream encoder to create raw and target-conditioned feature representations, employs cross-dimensional and cross-encoding self-attentions, and uses an adaptive sparse self-attention mechanism to suppress low-utility tokens.", "result": "SG-XDEAT demonstrates consistent improvements over strong baseline models on multiple public benchmarks in supervised learning tasks.", "conclusion": "The proposed framework, SG-XDEAT, effectively enhances the robustness and learning capabilities of deep tabular models by jointly leveraging raw and label-aware views while adaptively filtering noise."}}
{"id": "2510.12764", "pdf": "https://arxiv.org/pdf/2510.12764", "abs": "https://arxiv.org/abs/2510.12764", "authors": ["Thomas Wimmer", "Prune Truong", "Marie-Julie Rakotosaona", "Michael Oechsle", "Federico Tombari", "Bernt Schiele", "Jan Eric Lenssen"], "title": "AnyUp: Universal Feature Upsampling", "categories": ["cs.CV", "cs.LG"], "comment": "Project Website: https://wimmerth.github.io/anyup/", "summary": "We introduce AnyUp, a method for feature upsampling that can be applied to\nany vision feature at any resolution, without encoder-specific training.\nExisting learning-based upsamplers for features like DINO or CLIP need to be\nre-trained for every feature extractor and thus do not generalize to different\nfeature types at inference time. In this work, we propose an inference-time\nfeature-agnostic upsampling architecture to alleviate this limitation and\nimprove upsampling quality. In our experiments, AnyUp sets a new state of the\nart for upsampled features, generalizes to different feature types, and\npreserves feature semantics while being efficient and easy to apply to a wide\nrange of downstream tasks.", "AI": {"tldr": "AnyUp is a feature upsampling method that works with any vision feature at any resolution without encoder-specific training.", "motivation": "Existing upsampling methods require re-training for every feature extractor, limiting their generalizability. AnyUp aims to overcome this by providing an efficient and universal solution.", "method": "AnyUp introduces a feature-agnostic upsampling architecture that operates at inference time, allowing generalization across different feature extractors.", "result": "Experiments show that AnyUp achieves state-of-the-art performance in upsampled feature quality, generalizes to various feature types, and preserves feature semantics.", "conclusion": "AnyUp is a practical and efficient upsampling solution that can be applied universally without the need for encoder-specific re-training, benefiting a wide range of tasks."}}
{"id": "2510.12666", "pdf": "https://arxiv.org/pdf/2510.12666", "abs": "https://arxiv.org/abs/2510.12666", "authors": ["Prasenjit K Mudi", "Anshi Sachan", "Dahlia Devapriya", "Sheetal Kalyani"], "title": "Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models", "categories": ["cs.LG"], "comment": null, "summary": "Whisper models have achieved remarkable progress in speech recognition; yet\ntheir large size remains a bottleneck for deployment on resource-constrained\nedge devices. This paper proposes a framework to design fine-tuned variants of\nWhisper which address the above problem. Structured sparsity is enforced via\nthe Sparse Group LASSO penalty as a loss regularizer, to reduce the number of\nFLOating Point operations (FLOPs). Further, a weight statistics aware pruning\nalgorithm is proposed. We also design our custom text normalizer for WER\nevaluation. On Common Voice 11.0 Hindi dataset, we obtain, without degrading\nWER, (a) 35.4% reduction in model parameters, 14.25% lower memory consumption\nand 18.5% fewer FLOPs on Whisper-small, and (b) 31% reduction in model\nparameters, 15.29% lower memory consumption and 16.95% fewer FLOPs on\nWhisper-medium; and, (c) substantially outperform the state-of-the-art\nIterative Magnitude Pruning based method by pruning 18.7% more parameters along\nwith a 12.31 reduction in WER.", "AI": {"tldr": "The paper addresses the issue of deploying Whisper models on resource-constrained devices by proposing a framework for fine-tuning them with reduced size and computational demands, while maintaining performance.", "motivation": "The motivation is to overcome the challenge of deploying Whisper speech recognition models on edge devices that are constrained by computational resources, memory, and power.", "method": "The paper introduces a framework employing the Sparse Group LASSO penalty to enforce structured sparsity for reducing computations and parameters. Additionally, a weight statistics-aware pruning method and a customized text normalizer for WER evaluation are also proposed.", "result": "Without compromising Word Error Rate (WER), the authors achieved significant reductions in parameters, memory usage, and floating-point operations for Whisper-small (parameters: -35.4%, memory: -14.25%, FLOPs: -18.5%) and Whisper-medium (parameters: -31%, memory: -15.29%, FLOPs: -16.95%). The method also outperformed existing pruning techniques by removing 18.7% more parameters and achieving a 12.31 reduction in WER.", "conclusion": "The proposed methods effectively reduce model size and computational demand of Whisper models, making them more suitable for resource-constrained edge devices while maintaining or even improving their recognition accuracy."}}
{"id": "2510.12200", "pdf": "https://arxiv.org/pdf/2510.12200", "abs": "https://arxiv.org/abs/2510.12200", "authors": ["Xiaoxue Ren", "Penghao Jiang", "Kaixin Li", "Zhiyong Huang", "Xiaoning Du", "Jiaojiao Jiang", "Zhenchang Xing", "Jiamou Sun", "Terry Yue Zhuo"], "title": "HackWorld: Evaluating Computer-Use Agents on Exploiting Web Application Vulnerabilities", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Web applications are prime targets for cyberattacks as gateways to critical\nservices and sensitive data. Traditional penetration testing is costly and\nexpertise-intensive, making it difficult to scale with the growing web\necosystem. While language model agents show promise in cybersecurity, modern\nweb applications demand visual understanding, dynamic content handling, and\nmulti-step interactions that only computer-use agents (CUAs) can perform. Yet,\ntheir ability to discover and exploit vulnerabilities through graphical\ninterfaces remains largely unexplored. We present HackWorld, the first\nframework for systematically evaluating CUAs' capabilities to exploit web\napplication vulnerabilities via visual interaction. Unlike sanitized\nbenchmarks, HackWorld includes 36 real-world applications across 11 frameworks\nand 7 languages, featuring realistic flaws such as injection vulnerabilities,\nauthentication bypasses, and unsafe input handling. Using a Capture-the-Flag\n(CTF) setup, it tests CUAs' capacity to identify and exploit these weaknesses\nwhile navigating complex web interfaces. Evaluation of state-of-the-art CUAs\nreveals concerning trends: exploitation rates below 12% and low cybersecurity\nawareness. CUAs often fail at multi-step attack planning and misuse security\ntools. These results expose the current limitations of CUAs in web security\ncontexts and highlight opportunities for developing more security-aware agents\ncapable of effective vulnerability detection and exploitation.", "AI": {"tldr": "HackWorld introduces a framework to evaluate computer-use agents' (CUAs) capability to identify and exploit web vulnerabilities through visual interactions, finding their current performance lacking.", "motivation": "Cyberattacks targeting web applications are rising, while traditional penetration testing is resource-intensive and insufficient to keep up. CUAs show promise in automating cybersecurity efforts but lack exploration in handling complex web vulnerabilities interactively.", "method": "The framework, HackWorld, includes 36 real-world applications with realistic flaws and uses a Capture-the-Flag (CTF) setup to systematically test CUAs' ability to exploit vulnerabilities via complex visual interactions.", "result": "State-of-the-art CUAs show exploitation rates below 12%, struggle with multi-step attack planning, and demonstrate low security awareness and poor utilization of security tools.", "conclusion": "CUAs currently fall short in web security exploitation through graphical interaction, revealing a need for enhanced cybersecurity capabilities in future CUAs."}}
{"id": "2510.12765", "pdf": "https://arxiv.org/pdf/2510.12765", "abs": "https://arxiv.org/abs/2510.12765", "authors": ["Bruno Longarela", "Marcos V. Conde", "Alvaro Garcia", "Radu Timofte"], "title": "Efficient Perceptual Image Super Resolution: AIM 2025 Study and Benchmark", "categories": ["cs.CV"], "comment": "ICCV 2025 - AIM Workshop", "summary": "This paper presents a comprehensive study and benchmark on Efficient\nPerceptual Super-Resolution (EPSR). While significant progress has been made in\nefficient PSNR-oriented super resolution, approaches focusing on perceptual\nquality metrics remain relatively inefficient. Motivated by this gap, we aim to\nreplicate or improve the perceptual results of Real-ESRGAN while meeting strict\nefficiency constraints: a maximum of 5M parameters and 2000 GFLOPs, calculated\nfor an input size of 960x540 pixels. The proposed solutions were evaluated on a\nnovel dataset consisting of 500 test images of 4K resolution, each degraded\nusing multiple degradation types, without providing the original high-quality\ncounterparts. This design aims to reflect realistic deployment conditions and\nserves as a diverse and challenging benchmark. The top-performing approach\nmanages to outperform Real-ESRGAN across all benchmark datasets, demonstrating\nthe potential of efficient methods in the perceptual domain. This paper\nestablishes the modern baselines for efficient perceptual super resolution.", "AI": {"tldr": "The paper studies efficient perceptual-quality super-resolution, improving on Real-ESRGAN while conforming to strict model constraints.", "motivation": "To address the inefficiency of current perceptual-quality-oriented super-resolution methods, with a focus on improving visual metrics under constrained resources.", "method": "Developed solutions constrained by parameter and GFLOPs limits, tested on a realistic 4K dataset designed with various degradation types.", "result": "The top-performing method surpassed Real-ESRGAN in perceptual quality on all benchmark datasets.", "conclusion": "Efficient approaches can achieve superior perceptual-quality super-resolution, setting new standards for resource-efficient models."}}
{"id": "2510.12669", "pdf": "https://arxiv.org/pdf/2510.12669", "abs": "https://arxiv.org/abs/2510.12669", "authors": ["Kaiwen He", "Petros Drineas", "Rajiv Khanna"], "title": "Structure-Aware Spectral Sparsification via Uniform Edge Sampling", "categories": ["cs.LG", "cs.DS"], "comment": "19 pages, 4 figures, NeurIPS 2025", "summary": "Spectral clustering is a fundamental method for graph partitioning, but its\nreliance on eigenvector computation limits scalability to massive graphs.\nClassical sparsification methods preserve spectral properties by sampling edges\nproportionally to their effective resistances, but require expensive\npreprocessing to estimate these resistances. We study whether uniform edge\nsampling-a simple, structure-agnostic strategy-can suffice for spectral\nclustering. Our main result shows that for graphs admitting a well-separated\n$k$-clustering, characterized by a large structure ratio $\\Upsilon(k) =\n\\lambda_{k+1} / \\rho_G(k)$, uniform sampling preserves the spectral subspace\nused for clustering. Specifically, we prove that uniformly sampling $O(\\gamma^2\nn \\log n / \\epsilon^2)$ edges, where $\\gamma$ is the Laplacian condition\nnumber, yields a sparsifier whose top $(n-k)$-dimensional eigenspace is\napproximately orthogonal to the cluster indicators. This ensures that the\nspectral embedding remains faithful, and clustering quality is preserved. Our\nanalysis introduces new resistance bounds for intra-cluster edges, a\nrank-$(n-k)$ effective resistance formulation, and a matrix Chernoff bound\nadapted to the dominant eigenspace. These tools allow us to bypass importance\nsampling entirely. Conceptually, our result connects recent coreset-based\nclustering theory to spectral sparsification, showing that under strong\nclusterability, even uniform sampling is structure-aware. This provides the\nfirst provable guarantee that uniform edge sampling suffices for\nstructure-preserving spectral clustering.", "AI": {"tldr": "The paper demonstrates that uniform edge sampling, instead of traditional importance sampling, can preserve spectral clustering properties for graphs with strong clusterability.", "motivation": "Spectral clustering is a widely used technique for graph partitioning but suffers scalability limitations due to its reliance on eigenvector computations. This paper explores whether a simpler and widely applicable uniform sampling can achieve similar results.", "method": "The authors analytically investigate uniform edge sampling on graphs with well-separated $k$-clusters, introducing new bounds and leveraging matrix Chernoff bounds for eigenspaces to demonstrate its viability.", "result": "Uniform edge sampling preserves the necessary spectral subspace for clustering when $O(\\gamma^2 n \\log n / \\epsilon^2)$ edges are sampled, bypassing the need for expensive preprocessing.", "conclusion": "Under strong clusterability in graphs, uniform edge sampling suffices for structure-preserving spectral clustering, uniting coreset-based clustering theory with spectral sparsification ideas."}}
{"id": "2510.12210", "pdf": "https://arxiv.org/pdf/2510.12210", "abs": "https://arxiv.org/abs/2510.12210", "authors": ["Yakun Song", "Xiaobin Zhuang", "Jiawei Chen", "Zhikang Niu", "Guanrou Yang", "Chenpeng Du", "Zhuo Chen", "Yuping Wang", "Yuxuan Wang", "Xie Chen"], "title": "DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation", "categories": ["eess.AS", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent attempts to interleave autoregressive (AR) sketchers with\ndiffusion-based refiners over continuous speech representations have shown\npromise, but they remain brittle under distribution shift and offer limited\nlevers for controllability. We introduce DISTAR, a zero-shot text-to-speech\nframework that operates entirely in a discrete residual vector quantization\n(RVQ) code space and tightly couples an AR language model with a masked\ndiffusion model, without forced alignment or a duration predictor. Concretely,\nDISTAR drafts block-level RVQ tokens with an AR language model and then\nperforms parallel masked-diffusion infilling conditioned on the draft to\ncomplete the next block, yielding long-form synthesis with blockwise\nparallelism while mitigating classic AR exposure bias. The discrete code space\naffords explicit control at inference: DISTAR produces high-quality audio under\nboth greedy and sample-based decoding using classifier-free guidance, supports\ntrade-offs between robustness and diversity, and enables variable bit-rate and\ncontrollable computation via RVQ layer pruning at test time. Extensive\nexperiments and ablations demonstrate that DISTAR surpasses state-of-the-art\nzero-shot TTS systems in robustness, naturalness, and speaker/style\nconsistency, while maintaining rich output diversity. Audio samples are\nprovided on https://anonymous.4open.science/w/DiSTAR_demo.", "AI": {"tldr": "This paper introduces DISTAR, a zero-shot text-to-speech (TTS) framework that operates in a discrete code space, using a combination of an autoregressive language model and masked diffusion for synthesis.", "motivation": "The study aims to address brittleness under distribution shift and improve controllability in interleaved autoregressive and diffusion-based TTS models.", "method": "DISTAR tightly integrates autoregressive language modeling with masked diffusion in discrete residual vector quantization (RVQ) code space. It drafts tokens with AR modeling and uses diffusion infilling for synthesis.", "result": "The framework delivers robust, natural, and speaker/style-consistent audio quality with enhanced controllability. Extensive tests show that DISTAR surpasses existing systems in various metrics.", "conclusion": "DISTAR effectively combines AR and diffusion methods within a discrete code space, offering controllable, high-quality TTS synthesis with improved robustness and output diversity."}}
{"id": "2510.12768", "pdf": "https://arxiv.org/pdf/2510.12768", "abs": "https://arxiv.org/abs/2510.12768", "authors": ["Fengzhi Guo", "Chih-Chuan Hsu", "Sihao Ding", "Cheng Zhang"], "title": "Uncertainty Matters in Dynamic Gaussian Splatting for Monocular 4D Reconstruction", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "Project page: https://tamu-visual-ai.github.io/usplat4d/", "summary": "Reconstructing dynamic 3D scenes from monocular input is fundamentally\nunder-constrained, with ambiguities arising from occlusion and extreme novel\nviews. While dynamic Gaussian Splatting offers an efficient representation,\nvanilla models optimize all Gaussian primitives uniformly, ignoring whether\nthey are well or poorly observed. This limitation leads to motion drifts under\nocclusion and degraded synthesis when extrapolating to unseen views. We argue\nthat uncertainty matters: Gaussians with recurring observations across views\nand time act as reliable anchors to guide motion, whereas those with limited\nvisibility are treated as less reliable. To this end, we introduce USplat4D, a\nnovel Uncertainty-aware dynamic Gaussian Splatting framework that propagates\nreliable motion cues to enhance 4D reconstruction. Our key insight is to\nestimate time-varying per-Gaussian uncertainty and leverages it to construct a\nspatio-temporal graph for uncertainty-aware optimization. Experiments on\ndiverse real and synthetic datasets show that explicitly modeling uncertainty\nconsistently improves dynamic Gaussian Splatting models, yielding more stable\ngeometry under occlusion and high-quality synthesis at extreme viewpoints.", "AI": {"tldr": "The paper introduces USplat4D, an Uncertainty-aware framework for improving dynamic Gaussian Splatting in reconstructing dynamic 3D scenes.", "motivation": "To address the limitations of vanilla dynamic Gaussian Splatting, which struggles with motion drifts under occlusion and poor synthesis in unseen views due to uniformly optimizing all Gaussian primitives.", "method": "USplat4D estimates time-varying uncertainty for each Gaussian primitive and uses it to create a spatio-temporal graph for optimization, effectively treating well-observed Gaussians as reliable anchors and lesser-observed ones as less reliable.", "result": "The model consistently enhances reconstruction quality, achieving stable geometry under occlusion and improved synthesis for novel and extreme viewpoints, validated across multiple real and synthetic datasets.", "conclusion": "Incorporating uncertainty into dynamic Gaussian Splatting significantly improves its ability to reconstruct dynamic 3D scenes, especially under challenging conditions like occlusion and extreme novel views."}}
{"id": "2510.12672", "pdf": "https://arxiv.org/pdf/2510.12672", "abs": "https://arxiv.org/abs/2510.12672", "authors": ["Ruben Belo", "Claudia Soares", "Marta Guimaraes"], "title": "Keep Calm and Avoid Harmful Content: Concept Alignment and Latent Manipulation Towards Safer Answers", "categories": ["cs.LG"], "comment": null, "summary": "Large Language Models are susceptible to jailbreak attacks that bypass\nbuilt-in safety guardrails (e.g., by tricking the model with adversarial\nprompts). We propose Concept Alignment and Concept Manipulation \\textbf{CALM},\nan inference-time method that suppresses harmful concepts by modifying latent\nrepresentations of the last layer of the model, without retraining. Leveraging\n\\gls*{cw} technique from Computer Vision combined with orthogonal projection,\nCALM removes unwanted latent directions associated with harmful content while\npreserving model performance. Experiments show that CALM reduces harmful\noutputs and outperforms baseline methods in most metrics, offering a\nlightweight approach to AI safety with no additional training data or model\nfine-tuning, while incurring only a small computational overhead at inference.", "AI": {"tldr": "The paper introduces \"CALM,\" a method to suppress harmful outputs of large language models without retraining.", "motivation": "Large Language Models are vulnerable to jailbreak attacks, posing risks by generating harmful outputs despite safety guardrails.", "method": "The proposed method, CALM, modifies latent representations at inference time using adversarial techniques from Computer Vision and orthogonal projection, removing harmful directions in the model's representations.", "result": "CALM shows reduced harmful outputs compared to baseline methods and maintains model performance with minimal computational overhead.", "conclusion": "CALM is an effective, lightweight solution for AI safety, offering better handling of harmful content without retraining or fine-tuning the model."}}
{"id": "2510.12777", "pdf": "https://arxiv.org/pdf/2510.12777", "abs": "https://arxiv.org/abs/2510.12777", "authors": ["Stefan Andreas Baumann", "Nick Stracke", "Timy Phan", "Bj\u00f6rn Ommer"], "title": "What If : Understanding Motion Through Sparse Interactions", "categories": ["cs.CV"], "comment": "Project page and code:\n  https://compvis.github.io/flow-poke-transformer", "summary": "Understanding the dynamics of a physical scene involves reasoning about the\ndiverse ways it can potentially change, especially as a result of local\ninteractions. We present the Flow Poke Transformer (FPT), a novel framework for\ndirectly predicting the distribution of local motion, conditioned on sparse\ninteractions termed \"pokes\". Unlike traditional methods that typically only\nenable dense sampling of a single realization of scene dynamics, FPT provides\nan interpretable directly accessible representation of multi-modal scene\nmotion, its dependency on physical interactions and the inherent uncertainties\nof scene dynamics. We also evaluate our model on several downstream tasks to\nenable comparisons with prior methods and highlight the flexibility of our\napproach. On dense face motion generation, our generic pre-trained model\nsurpasses specialized baselines. FPT can be fine-tuned in strongly\nout-of-distribution tasks such as synthetic datasets to enable significant\nimprovements over in-domain methods in articulated object motion estimation.\nAdditionally, predicting explicit motion distributions directly enables our\nmethod to achieve competitive performance on tasks like moving part\nsegmentation from pokes which further demonstrates the versatility of our FPT.\nCode and models are publicly available at\nhttps://compvis.github.io/flow-poke-transformer.", "AI": {"tldr": "The paper introduces Flow Poke Transformer (FPT), a new method to predict local motion distributions in physical scenes from sparse interactions.", "motivation": "To enable interpretable multi-modal motion predictions considering local interactions and inherent uncertainties in the dynamics of physical scenes.", "method": "FPT predicts distributions of local motion by leveraging sparse physical interactions ('pokes') and analyzing multi-modal scene motion.", "result": "FPT demonstrates superiority in face motion generation and articulated object motion estimation on synthetic datasets. It excels in diverse tasks like moving part segmentation.", "conclusion": "FPT is a flexible, interpretable tool for understanding scene dynamics and predicting motion. Its applications range across various domains, showcasing its versatility and performance advantages."}}
{"id": "2510.12680", "pdf": "https://arxiv.org/pdf/2510.12680", "abs": "https://arxiv.org/abs/2510.12680", "authors": ["Shouren Wang", "Wang Yang", "Xianxuan Long", "Qifan Wang", "Vipin Chaudhary", "Xiaotian Han"], "title": "Demystifying Hybrid Thinking: Can LLMs Truly Switch Between Think and No-Think?", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "10 pages, 6 figures", "summary": "Hybrid thinking enables LLMs to switch between reasoning and direct\nanswering, offering a balance between efficiency and reasoning capability. Yet\nour experiments reveal that current hybrid thinking LLMs only achieve partial\nmode separation: reasoning behaviors often leak into the no-think mode. To\nunderstand and mitigate this, we analyze the factors influencing\ncontrollability and identify four that matter most: (1) larger data scale, (2)\nusing think and no-think answers from different questions rather than the same\nquestion, (3) a moderate increase in no-think data number, and (4) a two-phase\nstrategy that first trains reasoning ability and then applies hybrid think\ntraining. Building on these findings, we propose a practical recipe that,\ncompared to standard training, can maintain accuracy in both modes while\nsignificantly reducing no-think output length (from $1085$ to $585$ on MATH500)\nand occurrences of reasoning-supportive tokens such as ``\\texttt{wait}'' (from\n$5917$ to $522$ on MATH500). Our findings highlight the limitations of current\nhybrid thinking and offer directions for strengthening its controllability.", "AI": {"tldr": "Current hybrid thinking LLMs struggle to fully separate reasoning and direct answering modes, leading to leaks in reasoning behavior during no-think mode.", "motivation": "To address the inefficiency and lack of strict mode controllability in hybrid thinking LLMs, which compromise performance in 'no-think' mode.", "method": "Analyzed four major factors affecting controllability and proposed a training strategy, including larger datasets, separating think/no-think questions, moderate increase in no-think data, and a two-phase training process.", "result": "Proposed training recipe maintained accuracy across modes, reduced no-think response length and reasoning-supportive token occurrences significantly on benchmark dataset MATH500.", "conclusion": "Current hybrid thinking approaches have limitations; the suggested methodology improves controllability and provides insights for future advancements."}}
{"id": "2510.12327", "pdf": "https://arxiv.org/pdf/2510.12327", "abs": "https://arxiv.org/abs/2510.12327", "authors": ["Benjamin Clavi\u00e9", "Sean Lee", "Rikiya Takehi", "Aamir Shakir", "Makoto P. Kato"], "title": "Simple Projection Variants Improve ColBERT Performance", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Multi-vector dense retrieval methods like ColBERT systematically use a\nsingle-layer linear projection to reduce the dimensionality of individual\nvectors. In this study, we explore the implications of the MaxSim operator on\nthe gradient flows of the training of multi-vector models and show that such a\nsimple linear projection has inherent, if non-critical, limitations in this\nsetting. We then discuss the theoretical improvements that could result from\nreplacing this single-layer projection with well-studied alternative\nfeedforward linear networks (FFN), such as deeper, non-linear FFN blocks, GLU\nblocks, and skip-connections, could alleviate these limitations. Through the\ndesign and systematic evaluation of alternate projection blocks, we show that\nbetter-designed final projections positively impact the downstream performance\nof ColBERT models. We highlight that many projection variants outperform the\noriginal linear projections, with the best-performing variants increasing\naverage performance on a range of retrieval benchmarks across domains by over 2\nNDCG@10 points. We then conduct further exploration on the individual\nparameters of these projections block in order to understand what drives this\nempirical performance, highlighting the particular importance of upscaled\nintermediate projections and residual connections. As part of these ablation\nstudies, we show that numerous suboptimal projection variants still outperform\nthe traditional single-layer projection across multiple benchmarks, confirming\nour hypothesis. Finally, we observe that this effect is consistent across\nrandom seeds, further confirming that replacing the linear layer of ColBERT\nmodels is a robust, drop-in upgrade.", "AI": {"tldr": "Replacing the single-layer linear projection in ColBERT models with more advanced feedforward network structures improves retrieval performance.", "motivation": "To examine and overcome limitations in single-layer linear projections used in ColBERT for multi-vector dense retrieval.", "method": "Alternative projection blocks, including deeper non-linear FFN blocks, GLU blocks, and skip-connections, were systematically designed, evaluated, and studied through ablation experiments.", "result": "Best-performing projection variants increased retrieval quality by over 2 NDCG@10 points across diverse benchmarks.", "conclusion": "Replacing ColBERT\u2019s linear layer with advanced projections yields robust improvements and serves as a reliable enhancement across settings."}}
{"id": "2510.12784", "pdf": "https://arxiv.org/pdf/2510.12784", "abs": "https://arxiv.org/abs/2510.12784", "authors": ["Weiyang Jin", "Yuwei Niu", "Jiaqi Liao", "Chengqi Duan", "Aoxue Li", "Shenghua Gao", "Xihui Liu"], "title": "SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models", "categories": ["cs.CV", "cs.CL", "I.4.0"], "comment": "20 pages, 8 figures, webpage can be seen in\n  https://waynejin0918.github.io/srum_web/", "summary": "Recently, remarkable progress has been made in Unified Multimodal Models\n(UMMs), which integrate vision-language generation and understanding\ncapabilities within a single framework. However, a significant gap exists where\na model's strong visual understanding often fails to transfer to its visual\ngeneration. A model might correctly understand an image based on user\ninstructions, yet be unable to generate a faithful image from text prompts.\nThis phenomenon directly raises a compelling question: Can a model achieve\nself-improvement by using its understanding module to reward its generation\nmodule? To bridge this gap and achieve self-improvement, we introduce SRUM, a\nself-rewarding post-training framework that can be directly applied to existing\nUMMs of various designs. SRUM creates a feedback loop where the model's own\nunderstanding module acts as an internal ``evaluator'', providing corrective\nsignals to improve its generation module, without requiring additional\nhuman-labeled data. To ensure this feedback is comprehensive, we designed a\nglobal-local dual reward system. To tackle the inherent structural complexity\nof images, this system offers multi-scale guidance: a \\textbf{global reward}\nensures the correctness of the overall visual semantics and layout, while a\n\\textbf{local reward} refines fine-grained, object-level fidelity. SRUM leads\nto powerful capabilities and shows strong generalization, boosting performance\non T2I-CompBench from 82.18 to \\textbf{88.37} and on T2I-ReasonBench from 43.82\nto \\textbf{46.75}. Overall, our work establishes a powerful new paradigm for\nenabling a UMMs' understanding module to guide and enhance its own generation\nvia self-rewarding.", "AI": {"tldr": "This paper introduces SRUM, a framework enabling Unified Multimodal Models (UMMs) to improve their visual generation capabilities by using their understanding module as an internal evaluator without additional labeled data.", "motivation": "Unified Multimodal Models often demonstrate strong visual understanding but struggle to generate accurate visual content. The paper aims to address this gap and explores whether self-improvement can be achieved through internal feedback loops.", "method": "SRUM employs a self-rewarding framework where the understanding module evaluates generated images using a global-local dual reward system that ensures semantic and object-level fidelity.", "result": "SRUM boosts performance metrics, improving scores on T2I-CompBench from 82.18 to 88.37 and on T2I-ReasonBench from 43.82 to 46.75, showing strong visual generation capabilities and generalization.", "conclusion": "The proposed SRUM framework empowers UMMs to utilize their understanding module for self-guided improvement in visual generation, introducing a new paradigm for multimodal AI systems."}}
{"id": "2510.12681", "pdf": "https://arxiv.org/pdf/2510.12681", "abs": "https://arxiv.org/abs/2510.12681", "authors": ["Guo Qin", "Zhi Chen", "Yong Liu", "Zhiyuan Shi", "Haixuan Liu", "Xiangdong Huang", "Jianmin Wang", "Mingsheng Long"], "title": "CoRA: Covariate-Aware Adaptation of Time Series Foundation Models", "categories": ["cs.LG"], "comment": null, "summary": "Time Series Foundation Models (TSFMs) have shown significant impact through\ntheir model capacity, scalability, and zero-shot generalization. However, due\nto the heterogeneity of inter-variate dependencies and the backbone scalability\non large-scale multivariate datasets, most TSFMs are typically pre-trained on\nunivariate time series. This limitation renders them oblivious to crucial\ninformation from diverse covariates in real-world forecasting tasks. To further\nenhance the performance of TSFMs, we propose a general covariate-aware\nadaptation (CoRA) framework for TSFMs. It leverages pre-trained backbones of\nfoundation models while effectively incorporating exogenous covariates from\nvarious modalities, including time series, language, and images, to improve the\nquality of predictions. Technically, CoRA maintains the equivalence of\ninitialization and parameter consistency during adaptation. With preserved\nbackbones of foundation models as frozen feature extractors, the outcome\nembeddings from foundation models are empirically demonstrated more informative\nthan raw data. Further, CoRA employs a novel Granger Causality Embedding (GCE)\nto automatically evaluate covariates regarding their causal predictability with\nrespect to the target variate. We incorporate these weighted embeddings with a\nzero-initialized condition-injection mechanism, avoiding catastrophic\nforgetting of pre-trained foundation models and gradually integrates exogenous\ninformation. Extensive experiments show that CoRA of TSFMs surpasses\nstate-of-the-art covariate-aware deep forecasters with full or few-shot\ntraining samples, achieving 31.1% MSE reduction on covariate-aware forecasting.\nCompared to other adaptation methods, CoRA exhibits strong compatibility with\nvarious advanced TSFMs and extends the scope of covariates to other modalities,\npresenting a practical paradigm for the application of TSFMs.", "AI": {"tldr": "This paper introduces CoRA, a framework for adapting Time Series Foundation Models (TSFMs) to include information from diverse covariates, achieving significant prediction accuracy improvements.", "motivation": "The study aims to address the limitations of TSFMs, which are typically pre-trained on univariate data, making them less effective in utilizing covariate information from real-world multivariate and multimodal datasets.", "method": "CoRA integrates exogenous covariates such as time series, language, and images, while preserving the frozen backbones of pre-trained TSFMs. It uses Granger Causality Embedding (GCE) to evaluate the causal influence of covariates and introduces a zero-initialized condition-injection mechanism to blend covariate information without forgetting pre-training.", "result": "Experiments demonstrate that CoRA outperforms state-of-the-art covariate-aware forecasters, achieving a significant 31.1% reduction in Mean Squared Error (MSE) on forecasting tasks. It also shows strong compatibility with diverse TSFMs and covariate modalities.", "conclusion": "CoRA effectively enhances TSFMs by leveraging covariate information, provides robust improvements in forecasting accuracy, and offers a scalable and versatile framework for real-world applications."}}
{"id": "2510.12049", "pdf": "https://arxiv.org/pdf/2510.12049", "abs": "https://arxiv.org/abs/2510.12049", "authors": ["Lu Fang", "Zhe Yuan", "Kaifu Zhang", "Dante Donati", "Miklos Sarvary"], "title": "Generative AI and Firm Productivity: Field Experiments in Online Retail", "categories": ["econ.GN", "cs.AI", "q-fin.EC"], "comment": "Keywords: Field Experiments, Generative AI, Productivity, Retail\n  Platforms, Consumer Experience. JEL codes: C93, D24, L81, M31, O3", "summary": "We quantify the impact of Generative Artificial Intelligence (GenAI) on firm\nproductivity through a series of large-scale randomized field experiments\ninvolving millions of users and products at a leading cross-border online\nretail platform. Over six months in 2023-2024, GenAI-based enhancements were\nintegrated into seven consumer-facing business workflows. We find that GenAI\nadoption significantly increases sales, with treatment effects ranging from 0\\%\nto 16.3\\%, depending on GenAI's marginal contribution relative to existing firm\npractices. Because inputs and prices were held constant across experimental\narms, these gains map directly into total factor productivity improvements.\nAcross the four GenAI applications with positive effects, the implied annual\nincremental value is approximately \\$5 per consumer-an economically meaningful\nimpact given the retailer's scale and the early stage of GenAI adoption. The\nprimary mechanism operates through higher conversion rates, consistent with\nGenAI reducing frictions in the marketplace and improving consumer experience.\nWe also document substantial heterogeneity: smaller and newer sellers, as well\nas less experienced consumers, exhibit disproportionately larger gains. Our\nfindings provide novel, large-scale causal evidence on the productivity effects\nof GenAI in online retail, highlighting both its immediate value and broader\npotential.", "AI": {"tldr": "This study examines how Generative Artificial Intelligence (GenAI) impacts firm productivity in online retail through field experiments, showing significant sales growth and productivity improvements.", "motivation": "To understand the economic impact of GenAI on productivity, especially in an online retail context, and provide evidence of its efficacy.", "method": "The research conducted large-scale randomized field experiments over six months, testing GenAI integration in seven business workflows at an online retail platform with millions of users.", "result": "The adoption of GenAI increased sales by up to 16.3% depending on its contribution, leading to a total factor productivity improvement and an annual incremental value of $5 per consumer.", "conclusion": "GenAI significantly improves online retail efficiency and conversion rates, especially benefiting smaller sellers and newer consumers, offering promising insights into its broader potential."}}
{"id": "2510.12785", "pdf": "https://arxiv.org/pdf/2510.12785", "abs": "https://arxiv.org/abs/2510.12785", "authors": ["Felix Taubner", "Ruihang Zhang", "Mathieu Tuli", "Sherwin Bahmani", "David B. Lindell"], "title": "MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "18 pages, 12 figures", "summary": "Digital human avatars aim to simulate the dynamic appearance of humans in\nvirtual environments, enabling immersive experiences across gaming, film,\nvirtual reality, and more. However, the conventional process for creating and\nanimating photorealistic human avatars is expensive and time-consuming,\nrequiring large camera capture rigs and significant manual effort from\nprofessional 3D artists. With the advent of capable image and video generation\nmodels, recent methods enable automatic rendering of realistic animated avatars\nfrom a single casually captured reference image of a target subject. While\nthese techniques significantly lower barriers to avatar creation and offer\ncompelling realism, they lack constraints provided by multi-view information or\nan explicit 3D representation. So, image quality and realism degrade when\nrendered from viewpoints that deviate strongly from the reference image. Here,\nwe build a video model that generates animatable multi-view videos of digital\nhumans based on a single reference image and target expressions. Our model,\nMVP4D, is based on a state-of-the-art pre-trained video diffusion model and\ngenerates hundreds of frames simultaneously from viewpoints varying by up to\n360 degrees around a target subject. We show how to distill the outputs of this\nmodel into a 4D avatar that can be rendered in real-time. Our approach\nsignificantly improves the realism, temporal consistency, and 3D consistency of\ngenerated avatars compared to previous methods.", "AI": {"tldr": "This paper presents MVP4D, a video model that creates animatable multi-view videos of digital human avatars from a single reference image, addressing traditional limitations in realism and 3D consistency.", "motivation": "Creating photorealistic human avatars is traditionally expensive and time-consuming, involving large camera setups and professional manual effort. Recent methods simplify this process using image/video models but suffer from degradation in realism for non-reference viewpoints.", "method": "MVP4D leverages a state-of-the-art pre-trained video diffusion model to generate multi-view videos from a single reference image and target expressions, producing hundreds of frames simultaneously across 360-degree viewpoints. Outputs are distilled into a 4D avatar for real-time rendering.", "result": "The approach improves realism, temporal consistency, and 3D consistency of digital human avatars compared to earlier methods.", "conclusion": "MVP4D advances avatar creation technology by enabling efficient, high-quality, and real-time animatable avatars from a single image."}}
{"id": "2510.12686", "pdf": "https://arxiv.org/pdf/2510.12686", "abs": "https://arxiv.org/abs/2510.12686", "authors": ["Muhammad Ayub Sabir", "Junbiao Pang", "Jiaqi Wu", "Fatima Ashraf"], "title": "Few Shot Semi-Supervised Learning for Abnormal Stop Detection from Sparse GPS Trajectories", "categories": ["cs.LG"], "comment": null, "summary": "Abnormal stop detection (ASD) in intercity coach transportation is critical\nfor ensuring passenger safety, operational reliability, and regulatory\ncompliance. However, two key challenges hinder ASD effectiveness: sparse GPS\ntrajectories, which obscure short or unauthorized stops, and limited labeled\ndata, which restricts supervised learning. Existing methods often assume dense\nsampling or regular movement patterns, limiting their applicability. To address\ndata sparsity, we propose a Sparsity-Aware Segmentation (SAS) method that\nadaptively defines segment boundaries based on local spatial-temporal density.\nBuilding upon these segments, we introduce three domain-specific indicators to\ncapture abnormal stop behaviors. To further mitigate the impact of sparsity, we\ndevelop Locally Temporal-Indicator Guided Adjustment (LTIGA), which smooths\nthese indicators via local similarity graphs. To overcome label scarcity, we\nconstruct a spatial-temporal graph where each segment is a node with\nLTIGA-refined features. We apply label propagation to expand weak supervision\nacross the graph, followed by a GCN to learn relational patterns. A final\nself-training module incorporates high-confidence pseudo-labels to iteratively\nimprove predictions. Experiments on real-world coach data show an AUC of 0.854\nand AP of 0.866 using only 10 labeled instances, outperforming prior methods.\nThe code and dataset are publicly available at\n\\href{https://github.com/pangjunbiao/Abnormal-Stop-Detection-SSL.git}", "AI": {"tldr": "The paper addresses challenges in detecting abnormal stops in intercity coach transportation due to sparse GPS data and limited labeled data. The proposed method achieves high accuracy with minimal labeled instances.", "motivation": "Ensuring passenger safety, operational reliability, and regulatory compliance in intercity transportation, despite challenges like sparse data and label scarcity.", "method": "Introduced the Sparsity-Aware Segmentation (SAS) for adaptive data segmentation, domain-specific indicators, Locally Temporal-Indicator Guided Adjustment (LTIGA) for smoothing, spatial-temporal graph construction, label propagation, Graph Convolutional Network (GCN), and self-training with pseudo-labels.", "result": "Experimental results on real-world data achieved AUC of 0.854 and AP of 0.866 using only 10 labeled instances, surpassing prior methods.", "conclusion": "The proposed approach demonstrates its effectiveness in detecting abnormal stops under sparse data conditions and label scarcity. It offers superior results and is publicly available for further research."}}
{"id": "2510.12668", "pdf": "https://arxiv.org/pdf/2510.12668", "abs": "https://arxiv.org/abs/2510.12668", "authors": ["Minghao Tang", "Shiyu Ni", "Jingtong Wu", "Zengxin Han", "Keping Bi"], "title": "The Role of Parametric Injection-A Systematic Study of Parametric Retrieval-Augmented Generation", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nretrieving external documents. As an emerging form of RAG, parametric\nretrieval-augmented generation (PRAG) encodes documents as model parameters\n(i.e., LoRA modules) and injects these representations into the model during\ninference, enabling interaction between the LLM and documents at parametric\nlevel. Compared with directly placing documents in the input context, PRAG is\nmore efficient and has the potential to offer deeper model-document\ninteraction. Despite its growing attention, the mechanism underlying parametric\ninjection remains poorly understood. In this work, we present a systematic\nstudy of PRAG to clarify the role of parametric injection, showing that\nparameterized documents capture only partial semantic information of documents,\nand relying on them alone yields inferior performance compared to interaction\nat text level. However, these parametric representations encode high-level\ndocument information that can enhance the model's understanding of documents\nwithin the input context. When combined parameterized documents with textual\ndocuments, the model can leverage relevant information more effectively and\nbecome more robust to noisy inputs, achieving better performance than either\nsource alone. We recommend jointly using parameterized and textual documents\nand advocate for increasing the information content of parametric\nrepresentations to advance PRAG.", "AI": {"tldr": "This paper investigates parametric retrieval-augmented generation (PRAG), a technique where documents are injected into language model parameters, finding it efficient but limited alone, and proposing joint use with textual documents for better performance.", "motivation": "To explore the mechanism behind parametric injection in PRAG and evaluate its efficiency and effectiveness compared to traditional text-level interactions.", "method": "The study systematically examines PRAG, analyzing how parameterized documents capture semantic information and their performance compared to text-level interactions.", "result": "Parameterized documents alone provide partial semantic capture, yielding inferior performance. However, combining them with textual documents enhances information retrieval, model robustness, and overall performance.", "conclusion": "The paper concludes that PRAG benefits from combining parameterized and textual documents, and suggests increasing parametric representations' information content to improve future applications."}}
{"id": "2510.12788", "pdf": "https://arxiv.org/pdf/2510.12788", "abs": "https://arxiv.org/abs/2510.12788", "authors": ["Daniel Feijoo", "Paula Garrido-Mellado", "Marcos V. Conde", "Jaesung Rim", "Alvaro Garcia", "Sunghyun Cho", "Radu Timofte"], "title": "Efficient Real-World Deblurring using Single Images: AIM 2025 Challenge Report", "categories": ["cs.CV"], "comment": "ICCV 2025 - AIM Workshop", "summary": "This paper reviews the AIM 2025 Efficient Real-World Deblurring using Single\nImages Challenge, which aims to advance in efficient real-blur restoration. The\nchallenge is based on a new test set based on the well known RSBlur dataset.\nPairs of blur and degraded images in this dataset are captured using a\ndouble-camera system. Participant were tasked with developing solutions to\neffectively deblur these type of images while fulfilling strict efficiency\nconstraints: fewer than 5 million model parameters and a computational budget\nunder 200 GMACs. A total of 71 participants registered, with 4 teams finally\nsubmitting valid solutions. The top-performing approach achieved a PSNR of\n31.1298 dB, showcasing the potential of efficient methods in this domain. This\npaper provides a comprehensive overview of the challenge, compares the proposed\nsolutions, and serves as a valuable reference for researchers in efficient\nreal-world image deblurring.", "AI": {"tldr": "The paper reviews the AIM 2025 Efficient Real-World Deblurring Challenge, focusing on developing efficient solutions for real-world image deblurring under strict efficiency constraints.", "motivation": "The paper aims to advance the field of real-world blur restoration by introducing new efficiency constraints.", "method": "Participants were tasked with designing image deblurring solutions using fewer than 5 million parameters and under 200 GMACs.", "result": "71 participants registered, with 4 teams providing valid solutions; the best solution achieved a PSNR of 31.1298 dB.", "conclusion": "The paper highlights the potential of efficient image deblurring techniques and offers insights for future research in this area."}}
{"id": "2510.12691", "pdf": "https://arxiv.org/pdf/2510.12691", "abs": "https://arxiv.org/abs/2510.12691", "authors": ["Danial Hosseintabar", "Fan Chen", "Giannis Daras", "Antonio Torralba", "Constantinos Daskalakis"], "title": "DiffEM: Learning from Corrupted Data with Diffusion Models via Expectation Maximization", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Diffusion models have emerged as powerful generative priors for\nhigh-dimensional inverse problems, yet learning them when only corrupted or\nnoisy observations are available remains challenging. In this work, we propose\na new method for training diffusion models with Expectation-Maximization (EM)\nfrom corrupted data. Our proposed method, DiffEM, utilizes conditional\ndiffusion models to reconstruct clean data from observations in the E-step, and\nthen uses the reconstructed data to refine the conditional diffusion model in\nthe M-step. Theoretically, we provide monotonic convergence guarantees for the\nDiffEM iteration, assuming appropriate statistical conditions. We demonstrate\nthe effectiveness of our approach through experiments on various image\nreconstruction tasks.", "AI": {"tldr": "This paper proposes DiffEM, a novel method to train diffusion models from corrupted data using Expectation-Maximization steps, showcasing convergence guarantees and experimental effectiveness.", "motivation": "To address the challenges in training diffusion models when only corrupted or noisy observations are available.", "method": "The DiffEM method employs Expectation-Maximization by using conditional diffusion models: reconstructing clean data in the E-step and refining the models in the M-step.", "result": "DiffEM is supported by convergence guarantees under statistical conditions and shown to be effective in various image reconstruction experiments.", "conclusion": "DiffEM advances the ability to train diffusion models with corrupted data, improving the accuracy and applicability of generative priors in inverse problems."}}
{"id": "2510.12789", "pdf": "https://arxiv.org/pdf/2510.12789", "abs": "https://arxiv.org/abs/2510.12789", "authors": ["Kevin Li", "Manuel Brack", "Sudeep Katakol", "Hareesh Ravi", "Ajinkya Kale"], "title": "UniFusion: Vision-Language Model as Unified Encoder in Image Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project page at https://thekevinli.github.io/unifusion/", "summary": "Although recent advances in visual generation have been remarkable, most\nexisting architectures still depend on distinct encoders for images and text.\nThis separation constrains diffusion models' ability to perform cross-modal\nreasoning and knowledge transfer. Prior attempts to bridge this gap often use\nthe last layer information from VLM, employ multiple visual encoders, or train\nlarge unified models jointly for text and image generation, which demands\nsubstantial computational resources and large-scale data, limiting its\naccessibility.We present UniFusion, a diffusion-based generative model\nconditioned on a frozen large vision-language model (VLM) that serves as a\nunified multimodal encoder. At the core of UniFusion is the Layerwise Attention\nPooling (LAP) mechanism that extracts both high level semantics and low level\ndetails from text and visual tokens of a frozen VLM to condition a diffusion\ngenerative model. We demonstrate that LAP outperforms other shallow fusion\narchitectures on text-image alignment for generation and faithful transfer of\nvisual information from VLM to the diffusion model which is key for editing. We\npropose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI),\nwhich conditions a diffusion transformer (DiT) only on the text tokens\ngenerated by the VLM during in-model prompt rewriting. VERIFI combines the\nalignment of the conditioning distribution with the VLM's reasoning\ncapabilities for increased capabilities and flexibility at inference. In\naddition, finetuning on editing task not only improves text-image alignment for\ngeneration, indicative of cross-modality knowledge transfer, but also exhibits\ntremendous generalization capabilities. Our model when trained on single image\nediting, zero-shot generalizes to multiple image references further motivating\nthe unified encoder design of UniFusion.", "AI": {"tldr": "UniFusion introduces a novel diffusion-based model using frozen vision-language models (VLMs) as unified multimodal encoders, enhanced by techniques like Layerwise Attention Pooling (LAP) and VERIFI.", "motivation": "To address the limitations of existing architectures which rely on separate text and image encoders, making cross-modal reasoning and knowledge transfer challenging.", "method": "Utilized frozen VLMs with Layerwise Attention Pooling (LAP) and VERIFI as innovative mechanisms for multimodal text-image alignment and flexible inference within diffusion-based models.", "result": "UniFusion achieved superior text-image alignment, enabled faithful transfer of visual information, and demonstrated significant generalization capabilities like zero-shot multiple image reference editing.", "conclusion": "The model validates the effectiveness of unified encoder designs in improving multimodal generation tasks and showcases scalability and flexibility in inference."}}
{"id": "2510.12692", "pdf": "https://arxiv.org/pdf/2510.12692", "abs": "https://arxiv.org/abs/2510.12692", "authors": ["Sarina Xi", "Orelia Pi", "Miaomiao Zhang", "Becca Xiong", "Jacqueline Ng Lane", "Nihar B. Shah"], "title": "Who is a Better Matchmaker? Human vs. Algorithmic Judge Assignment in a High-Stakes Startup Competition", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "comment": "17 Pages, 2 figures", "summary": "There is growing interest in applying artificial intelligence (AI) to\nautomate and support complex decision-making tasks. However, it remains unclear\nhow algorithms compare to human judgment in contexts requiring semantic\nunderstanding and domain expertise. We examine this in the context of the judge\nassignment problem, matching submissions to suitably qualified judges.\nSpecifically, we tackled this problem at the Harvard President's Innovation\nChallenge, the university's premier venture competition awarding over \\$500,000\nto student and alumni startups. This represents a real-world environment where\nhigh-quality judge assignment is essential. We developed an AI-based\njudge-assignment algorithm, Hybrid Lexical-Semantic Similarity Ensemble (HLSE),\nand deployed it at the competition. We then evaluated its performance against\nhuman expert assignments using blinded match-quality scores from judges on\n$309$ judge-venture pairs. Using a Mann-Whitney U statistic based test, we\nfound no statistically significant difference in assignment quality between the\ntwo approaches ($AUC=0.48, p=0.40$); on average, algorithmic matches are rated\n$3.90$ and manual matches $3.94$ on a 5-point scale, where 5 indicates an\nexcellent match. Furthermore, manual assignments that previously required a\nfull week could be automated in several hours by the algorithm during\ndeployment. These results demonstrate that HLSE achieves human-expert-level\nmatching quality while offering greater scalability and efficiency,\nunderscoring the potential of AI-driven solutions to support and enhance human\ndecision-making for judge assignment in high-stakes settings.", "AI": {"tldr": "The paper explores the effectiveness of an AI-based algorithm for judge assignment in a venture competition, comparing it to human expert decisions.", "motivation": "To determine if AI can achieve human-level judgment quality in complex decision-making scenarios like judge assignment in competitions.", "method": "An AI algorithm named HLSE was developed and compared with human expert assignments using blinded quality scores for judge-venture pairs.", "result": "No significant difference was found between the quality of human and algorithmic matches; however, the AI method was faster and more scalable.", "conclusion": "The AI algorithm demonstrates human-level effectiveness in judge assignments while offering improved scalability and efficiency."}}
{"id": "2510.12793", "pdf": "https://arxiv.org/pdf/2510.12793", "abs": "https://arxiv.org/abs/2510.12793", "authors": ["Long Cui", "Weiyun Wang", "Jie Shao", "Zichen Wen", "Gen Luo", "Linfeng Zhang", "Yanting Zhang", "Yu Qiao", "Wenhai Wang"], "title": "ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Existing Multimodal Large Language Models (MLLMs) suffer from increased\ninference costs due to the additional vision tokens introduced by image inputs.\nIn this work, we propose Visual Consistency Learning (ViCO), a novel training\nalgorithm that enables the model to represent images of varying semantic\ncomplexities using different numbers of vision tokens. The key idea behind our\nmethod is to employ multiple MLP connectors, each with a different image\ncompression ratio, to downsample the vision tokens based on the semantic\ncomplexity of the image. During training, we minimize the KL divergence between\nthe responses conditioned on different MLP connectors. At inference time, we\nintroduce an image router, termed Visual Resolution Router (ViR), that\nautomatically selects the appropriate compression rate for each image patch.\nCompared with existing dynamic high-resolution strategies, which adjust the\nnumber of visual tokens based on image resolutions, our method dynamically\nadapts the number of visual tokens according to semantic complexity.\nExperimental results demonstrate that our method can reduce the number of\nvision tokens by up to 50% while maintaining the model's perception, reasoning,\nand OCR capabilities. We hope this work will contribute to the development of\nmore efficient MLLMs. The code and models will be released to facilitate future\nresearch.", "AI": {"tldr": "The paper introduces Visual Consistency Learning (ViCO), a novel method to reduce inference costs in Multimodal Large Language Models (MLLMs) by dynamically adjusting the number of vision tokens according to semantic complexity.", "motivation": "Multimodal Large Language Models (MLLMs) have increased inference costs due to the excessive vision tokens introduced by image inputs. The aim is to make MLLMs more computationally efficient while retaining performance.", "method": "The proposed method, ViCO, employs multiple MLP connectors to compress vision tokens at varying ratios based on semantic complexity of images. KL divergence minimization ensures consistency across different compression levels. At inference, a Visual Resolution Router (ViR) selects the appropriate compression rate per image dynamically.", "result": "Experimental results confirmed that ViCO reduces vision tokens by up to 50% while preserving key capabilities such as perception, reasoning, and OCR.", "conclusion": "ViCO improves the efficiency of MLLMs by balancing token compression with performance. The method enables smarter visual token processing, and code/models will be released for further research development."}}
{"id": "2510.12719", "pdf": "https://arxiv.org/pdf/2510.12719", "abs": "https://arxiv.org/abs/2510.12719", "authors": ["Matthew Adrian", "Yunsie Chung", "Kevin Boyd", "Saee Paliwal", "Srimukh Prasad Veccham", "Alan C. Cheng"], "title": "Multitask finetuning and acceleration of chemical pretrained models for small molecule drug property prediction", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Chemical pretrained models, sometimes referred to as foundation models, are\nreceiving considerable interest for drug discovery applications. The general\nchemical knowledge extracted from self-supervised training has the potential to\nimprove predictions for critical drug discovery endpoints, including on-target\npotency and ADMET properties. Multi-task learning has previously been\nsuccessfully leveraged to improve predictive models. Here, we show that\nenabling multitasking in finetuning of chemical pretrained graph neural network\nmodels such as Kinetic GROVER Multi-Task (KERMT), an enhanced version of the\nGROVER model, and Knowledge-guided Pre-training of Graph Transformer (KGPT)\nsignificantly improves performance over non-pretrained graph neural network\nmodels. Surprisingly, we find that the performance improvement from finetuning\nKERMT in a multitask manner is most significant at larger data sizes.\nAdditionally, we publish two multitask ADMET data splits to enable more\naccurate benchmarking of multitask deep learning methods for drug property\nprediction. Finally, we provide an accelerated implementation of the KERMT\nmodel on GitHub, unlocking large-scale pretraining, finetuning, and inference\nin industrial drug discovery workflows.", "AI": {"tldr": "The study demonstrates that multitask finetuning of pretrained chemical graph neural network models significantly boosts predictive performance, especially with larger datasets, aiding critical drug discovery processes.", "motivation": "To enhance predictive model performance for drug discovery endpoints by leveraging multitask learning in chemical pretrained graph neural networks.", "method": "The paper utilizes multitask finetuning approaches on pretrained graph neural network models like Kinetic GROVER Multi-Task (KERMT) and KGPT, comparing their performance to non-pretrained models.", "result": "Multitask finetuned chemical models, particularly KERMT, outperform non-pretrained graph neural network models, with the improvement most pronounced at larger data sizes.", "conclusion": "Multitask chemical pretrained models significantly advance drug property prediction, and new ADMET data splits and accelerated implementations facilitate better benchmarking and practical applications in drug discovery workflows."}}
{"id": "2510.12780", "pdf": "https://arxiv.org/pdf/2510.12780", "abs": "https://arxiv.org/abs/2510.12780", "authors": ["Cristina Aggazzotti", "Ashi Garg", "Zexin Cai", "Nicholas Andrews"], "title": "Content Anonymization for Privacy in Long-form Audio", "categories": ["cs.SD", "cs.CL"], "comment": null, "summary": "Voice anonymization techniques have been found to successfully obscure a\nspeaker's acoustic identity in short, isolated utterances in benchmarks such as\nthe VoicePrivacy Challenge. In practice, however, utterances seldom occur in\nisolation: long-form audio is commonplace in domains such as interviews, phone\ncalls, and meetings. In these cases, many utterances from the same speaker are\navailable, which pose a significantly greater privacy risk: given multiple\nutterances from the same speaker, an attacker could exploit an individual's\nvocabulary, syntax, and turns of phrase to re-identify them, even when their\nvoice is completely disguised. To address this risk, we propose new content\nanonymization approaches. Our approach performs a contextual rewriting of the\ntranscripts in an ASR-TTS pipeline to eliminate speaker-specific style while\npreserving meaning. We present results in a long-form telephone conversation\nsetting demonstrating the effectiveness of a content-based attack on\nvoice-anonymized speech. Then we show how the proposed content-based\nanonymization methods can mitigate this risk while preserving speech utility.\nOverall, we find that paraphrasing is an effective defense against\ncontent-based attacks and recommend that stakeholders adopt this step to ensure\nanonymity in long-form audio.", "AI": {"tldr": "The paper examines a privacy challenge with long-form audio, proposing content anonymization methods that use contextual rewriting to mitigate risks of content-based attacks while preserving speech utility.", "motivation": "Long-form audio poses greater privacy risks because multiple utterances allow attackers to exploit vocabulary, syntax, and turns of phrase for re-identification, even with voice disguises.", "method": "The paper proposes content anonymization methods using contextual rewriting of transcripts within an ASR-TTS pipeline to eliminate speaker-specific styles while retaining meaningful speech.", "result": "Results demonstrate that content-based attacks on anonymized speech are effective, but paraphrasing methods can mitigate this risk in long-form telephone conversations.", "conclusion": "Paraphrasing is recommended as an effective defense against content-based attacks in long-form audio to enhance speakers\u2019 anonymity while maintaining the utility of speech."}}
{"id": "2510.12721", "pdf": "https://arxiv.org/pdf/2510.12721", "abs": "https://arxiv.org/abs/2510.12721", "authors": ["Dayin Gou", "Sanghyun Byun", "Nilesh Malpeddi", "Gabrielle De Micheli", "Prathamesh Vaste", "Jacob Song", "Woo Seong Chung"], "title": "CARVQ: Corrective Adaptor with Group Residual Vector Quantization for LLM Embedding Compression", "categories": ["cs.LG"], "comment": "Accepted at EMNLP Findings 2025", "summary": "Large Language Models (LLMs) typically rely on a large number of parameters\nfor token embedding, leading to substantial storage requirements and memory\nfootprints. In particular, LLMs deployed on edge devices are memory-bound, and\nreducing the memory footprint by compressing the embedding layer not only frees\nup the memory bandwidth but also speeds up inference. To address this, we\nintroduce CARVQ, a post-training novel Corrective Adaptor combined with group\nResidual Vector Quantization. CARVQ relies on the composition of both linear\nand non-linear maps and mimics the original model embedding to compress to\napproximately 1.6 bits without requiring specialized hardware to support\nlower-bit storage. We test our method on pre-trained LLMs such as LLaMA-3.2-1B,\nLLaMA-3.2-3B, LLaMA-3.2-3B-Instruct, LLaMA-3.1-8B, Qwen2.5-7B, Qwen2.5-Math-7B\nand Phi-4, evaluating on common generative, discriminative, math and reasoning\ntasks. We show that in most cases, CARVQ can achieve lower average\nbitwidth-per-parameter while maintaining reasonable perplexity and accuracy\ncompared to scalar quantization. Our contributions include a novel compression\ntechnique that is compatible with state-of-the-art transformer quantization\nmethods and can be seamlessly integrated into any hardware supporting 4-bit\nmemory to reduce the model's memory footprint in memory-constrained devices.\nThis work demonstrates a crucial step toward the efficient deployment of LLMs\non edge devices.", "AI": {"tldr": "The paper introduces CARVQ, a compression technique for large language models (LLMs) to reduce memory requirements while maintaining performance.", "motivation": "LLMs demand substantial storage and memory, posing challenges for deployment on edge devices. Compressing the embedding layer can mitigate these issues.", "method": "CARVQ employs a novel corrective adaptor with group residual vector quantization, combining linear and non-linear mappings to mimic original embeddings. It achieves compression without specialized hardware.", "result": "CARVQ compresses embeddings to around 1.6 bits, achieving lower bitwidth-per-parameter while maintaining reasonable performance across various tasks compared to scalar quantization.", "conclusion": "CARVQ facilitates efficient LLM deployment on memory-constrained edge devices, contributing to transformer quantization advancements."}}
{"id": "2510.12796", "pdf": "https://arxiv.org/pdf/2510.12796", "abs": "https://arxiv.org/abs/2510.12796", "authors": ["Yingyan Li", "Shuyao Shang", "Weisong Liu", "Bing Zhan", "Haochen Wang", "Yuqi Wang", "Yuntao Chen", "Xiaoman Wang", "Yasong An", "Chufeng Tang", "Lu Hou", "Lue Fan", "Zhaoxiang Zhang"], "title": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Scaling Vision-Language-Action (VLA) models on large-scale data offers a\npromising path to achieving a more generalized driving intelligence. However,\nVLA models are limited by a ``supervision deficit'': the vast model capacity is\nsupervised by sparse, low-dimensional actions, leaving much of their\nrepresentational power underutilized. To remedy this, we propose\n\\textbf{DriveVLA-W0}, a training paradigm that employs world modeling to\npredict future images. This task generates a dense, self-supervised signal that\ncompels the model to learn the underlying dynamics of the driving environment.\nWe showcase the paradigm's versatility by instantiating it for two dominant VLA\narchetypes: an autoregressive world model for VLAs that use discrete visual\ntokens, and a diffusion world model for those operating on continuous visual\nfeatures. Building on the rich representations learned from world modeling, we\nintroduce a lightweight action expert to address the inference latency for\nreal-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a\n680x larger in-house dataset demonstrate that DriveVLA-W0 significantly\noutperforms BEV and VLA baselines. Crucially, it amplifies the data scaling\nlaw, showing that performance gains accelerate as the training dataset size\nincreases.", "AI": {"tldr": "The paper introduces DriveVLA-W0, a training paradigm using world modeling to enhance Vision-Language-Action (VLA) models for driving by generating dense, self-supervised signals.", "motivation": "VLA models for driving are limited by a supervision deficit as they use sparse, low-dimensional actions, failing to fully utilize their potential in generalized driving intelligence.", "method": "DriveVLA-W0 employs world modeling, predicting future images to create dense self-supervised signals, thereby teaching models the dynamics of driving environments. It supports autoregressive and diffusion world models.", "result": "Experiments on NAVSIM benchmarks and a large in-house dataset show significant performance improvements over BEV and VLA baselines, with accelerated gains as dataset size increases.", "conclusion": "World modeling with DriveVLA-W0 enhances VLA models\u2019 representational power, improves their scalability on large datasets, and addresses real-time deployment challenges effectively."}}
{"id": "2510.12726", "pdf": "https://arxiv.org/pdf/2510.12726", "abs": "https://arxiv.org/abs/2510.12726", "authors": ["Juha Harviainen", "Frank Sommer", "Manuel Sorge"], "title": "Improving Decision Trees through the Lens of Parameterized Local Search", "categories": ["cs.LG"], "comment": "Accepted at NeurIPS 2025", "summary": "Algorithms for learning decision trees often include heuristic local-search\noperations such as (1) adjusting the threshold of a cut or (2) also exchanging\nthe feature of that cut. We study minimizing the number of classification\nerrors by performing a fixed number of a single type of these operations.\nAlthough we discover that the corresponding problems are NP-complete in\ngeneral, we provide a comprehensive parameterized-complexity analysis with the\naim of determining those properties of the problems that explain the hardness\nand those that make the problems tractable. For instance, we show that the\nproblems remain hard for a small number $d$ of features or small domain size\n$D$ but the combination of both yields fixed-parameter tractability. That is,\nthe problems are solvable in $(D + 1)^{2d} \\cdot |I|^{O(1)}$ time, where $|I|$\nis the size of the input. We also provide a proof-of-concept implementation of\nthis algorithm and report on empirical results.", "AI": {"tldr": "The paper explores the NP-complete nature of optimizing decision tree thresholds and features while providing parameterized-complexity insights and an algorithm that is efficient under specific conditions.", "motivation": "To understand why optimizing decision tree thresholds and features is computationally hard and identify scenarios where it becomes tractable.", "method": "The study conducts a parameterized-complexity analysis, focusing on properties like the number of features (d) and domain size (D), and provides an algorithm for fixed-parameter tractable cases. They back this with both theoretical proof and empirical evidence.", "result": "The work demonstrates that while the problems are NP-complete in general, combining a small domain size (D) and feature count (d) makes the problems fixed-parameter tractable, solvable in $(D + 1)^{2d} \\cdot |I|^{O(1)}$ time.", "conclusion": "The research identifies concrete traits making decision tree optimization hard and outlines computationally feasible cases, providing an algorithm tested both theoretically and empirically."}}
{"id": "2510.12798", "pdf": "https://arxiv.org/pdf/2510.12798", "abs": "https://arxiv.org/abs/2510.12798", "authors": ["Qing Jiang", "Junan Huo", "Xingyu Chen", "Yuda Xiong", "Zhaoyang Zeng", "Yihao Chen", "Tianhe Ren", "Junzhi Yu", "Lei Zhang"], "title": "Detect Anything via Next Point Prediction", "categories": ["cs.CV"], "comment": "homepage: https://rex-omni.github.io/", "summary": "Object detection has long been dominated by traditional coordinate\nregression-based models, such as YOLO, DETR, and Grounding DINO. Although\nrecent efforts have attempted to leverage MLLMs to tackle this task, they face\nchallenges like low recall rate, duplicate predictions, coordinate\nmisalignment, etc. In this work, we bridge this gap and propose Rex-Omni, a\n3B-scale MLLM that achieves state-of-the-art object perception performance. On\nbenchmarks like COCO and LVIS, Rex-Omni attains performance comparable to or\nexceeding regression-based models (e.g., DINO, Grounding DINO) in a zero-shot\nsetting. This is enabled by three key designs: 1) Task Formulation: we use\nspecial tokens to represent quantized coordinates from 0 to 999, reducing the\nmodel's learning difficulty and improving token efficiency for coordinate\nprediction; 2) Data Engines: we construct multiple data engines to generate\nhigh-quality grounding, referring, and pointing data, providing semantically\nrich supervision for training; \\3) Training Pipelines: we employ a two-stage\ntraining process, combining supervised fine-tuning on 22 million data with\nGRPO-based reinforcement post-training. This RL post-training leverages\ngeometry-aware rewards to effectively bridge the discrete-to-continuous\ncoordinate prediction gap, improve box accuracy, and mitigate undesirable\nbehaviors like duplicate predictions that stem from the teacher-guided nature\nof the initial SFT stage. Beyond conventional detection, Rex-Omni's inherent\nlanguage understanding enables versatile capabilities such as object referring,\npointing, visual prompting, GUI grounding, spatial referring, OCR and\nkey-pointing, all systematically evaluated on dedicated benchmarks. We believe\nthat Rex-Omni paves the way for more versatile and language-aware visual\nperception systems.", "AI": {"tldr": "The paper introduces Rex-Omni, a 3B-scale model for object detection using Multimodal Large Language Models (MLLMs), outperforming traditional regression-based models and offering versatile visual capabilities.", "motivation": "Traditional object detection models like YOLO and DETR face limitations in tasks like low recall and coordinate misalignment. The goal is to address these gaps using a novel MLLM-based approach.", "method": "Rex-Omni is designed using three innovations: (1) special tokens for quantized coordinates, (2) data engines generating diverse training data, and (3) a two-stage training pipeline combining supervised fine-tuning and reinforcement learning with geometry-aware rewards.", "result": "Rex-Omni matches or outperforms traditional models in zero-shot settings on COCO and LVIS benchmarks. It also offers expanded capabilities beyond regular tasks like object referring, OCR, and spatial tasks.", "conclusion": "Rex-Omni demonstrates that MLLMs can match and surpass traditional approaches, introducing a versatile system for language-aware visual perception."}}
{"id": "2510.12801", "pdf": "https://arxiv.org/pdf/2510.12801", "abs": "https://arxiv.org/abs/2510.12801", "authors": ["Kartik Narayan", "Yang Xu", "Tian Cao", "Kavya Nerella", "Vishal M. Patel", "Navid Shiee", "Peter Grasch", "Chao Jia", "Yinfei Yang", "Zhe Gan"], "title": "DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search", "categories": ["cs.CV", "cs.IR"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) in real-world applications require\naccess to external knowledge sources and must remain responsive to the dynamic\nand ever-changing real-world information in order to address\ninformation-seeking and knowledge-intensive user queries. Existing approaches,\nsuch as retrieval augmented generation (RAG) methods, search agents, and search\nequipped MLLMs, often suffer from rigid pipelines, excessive search calls, and\npoorly constructed search queries, which result in inefficiencies and\nsuboptimal outcomes. To address these limitations, we present DeepMMSearch-R1,\nthe first multimodal LLM capable of performing on-demand, multi-turn web\nsearches and dynamically crafting queries for both image and text search tools.\nSpecifically, DeepMMSearch-R1 can initiate web searches based on relevant crops\nof the input image making the image search more effective, and can iteratively\nadapt text search queries based on retrieved information, thereby enabling\nself-reflection and self-correction. Our approach relies on a two-stage\ntraining pipeline: a cold start supervised finetuning phase followed by an\nonline reinforcement learning optimization. For training, we introduce\nDeepMMSearchVQA, a novel multimodal VQA dataset created through an automated\npipeline intermixed with real-world information from web search tools. This\ndataset contains diverse, multi-hop queries that integrate textual and visual\ninformation, teaching the model when to search, what to search for, which\nsearch tool to use and how to reason over the retrieved information. We conduct\nextensive experiments across a range of knowledge-intensive benchmarks to\ndemonstrate the superiority of our approach. Finally, we analyze the results\nand provide insights that are valuable for advancing multimodal web-search.", "AI": {"tldr": "The paper introduces DeepMMSearch-R1, a multimodal language model designed for effective real-world web search, overcoming inefficiencies in existing models.", "motivation": "Existing MLLMs in real-world applications struggle with inefficiencies like rigid pipelines and suboptimal search functionalities, necessitating a more dynamic and effective solution.", "method": "The approach combines supervised finetuning and reinforcement learning. Additionally, a novel dataset, DeepMMSearchVQA, is created with multimodal VQA queries for robust training.", "result": "DeepMMSearch-R1 demonstrates superior performance in knowledge-intensive benchmarks, showcasing efficient, self-adaptive text and image search capabilities.", "conclusion": "DeepMMSearch-R1 significantly advances multimodal web-search by integrating dynamic search initiation and adaptive query crafting techniques, offering valuable insights for MLLMs."}}
{"id": "2510.12734", "pdf": "https://arxiv.org/pdf/2510.12734", "abs": "https://arxiv.org/abs/2510.12734", "authors": ["Jon Donnelly", "Srikar Katta", "Emanuele Borgonovo", "Cynthia Rudin"], "title": "Doctor Rashomon and the UNIVERSE of Madness: Variable Importance with Unobserved Confounding and the Rashomon Effect", "categories": ["cs.LG"], "comment": null, "summary": "Variable importance (VI) methods are often used for hypothesis generation,\nfeature selection, and scientific validation. In the standard VI pipeline, an\nanalyst estimates VI for a single predictive model with only the observed\nfeatures. However, the importance of a feature depends heavily on which other\nvariables are included in the model, and essential variables are often omitted\nfrom observational datasets. Moreover, the VI estimated for one model is often\nnot the same as the VI estimated for another equally-good model - a phenomenon\nknown as the Rashomon Effect. We address these gaps by introducing\nUNobservables and Inference for Variable importancE using Rashomon SEts\n(UNIVERSE). Our approach adapts Rashomon sets - the sets of near-optimal models\nin a dataset - to produce bounds on the true VI even with missing features. We\ntheoretically guarantee the robustness of our approach, show strong performance\non semi-synthetic simulations, and demonstrate its utility in a credit risk\ntask.", "AI": {"tldr": "UNIVERSE introduces a method to estimate variable importance (VI) considering missing features and the Rashomon effect, offering bounds for true VI across near-optimal models.", "motivation": "Variable importance methods face challenges due to dependence on model composition and missing essential variables in datasets. The Rashomon Effect further complicates VI estimation within equally-good predictive models.", "method": "The paper introduces UNIVERSE, leveraging Rashomon sets to estimate true VI bounds robustly even with missing features and varying model compositions.", "result": "Theoretical guarantees for robustness were established, strong performance validated via semi-synthetic simulations, and practical utility showcased in a credit risk prediction task.", "conclusion": "UNIVERSE addresses critical gaps in VI estimation by providing robust bounds across model sets and facilitating better insights despite dataset limitations and model diversity."}}
{"id": "2510.12752", "pdf": "https://arxiv.org/pdf/2510.12752", "abs": "https://arxiv.org/abs/2510.12752", "authors": ["Siqi Li", "Yasser Shoukry"], "title": "KoALA: KL-L0 Adversarial Detector via Label Agreement", "categories": ["cs.LG"], "comment": null, "summary": "Deep neural networks are highly susceptible to adversarial attacks, which\npose significant risks to security- and safety-critical applications. We\npresent KoALA (KL-L0 Adversarial detection via Label Agreement), a novel,\nsemantics-free adversarial detector that requires no architectural changes or\nadversarial retraining. KoALA operates on a simple principle: it detects an\nadversarial attack when class predictions from two complementary similarity\nmetrics disagree. These metrics-KL divergence and an L0-based similarity-are\nspecifically chosen to detect different types of perturbations. The KL\ndivergence metric is sensitive to dense, low-amplitude shifts, while the\nL0-based similarity is designed for sparse, high-impact changes. We provide a\nformal proof of correctness for our approach. The only training required is a\nsimple fine-tuning step on a pre-trained image encoder using clean images to\nensure the embeddings align well with both metrics. This makes KOALA a\nlightweight, plug-and-play solution for existing models and various data\nmodalities. Our extensive experiments on ResNet/CIFAR-10 and CLIP/Tiny-ImageNet\nconfirm our theoretical claims. When the theorem's conditions are met, KoALA\nconsistently and effectively detects adversarial examples. On the full test\nsets, KoALA achieves a precision of 0.94 and a recall of 0.81 on\nResNet/CIFAR-10, and a precision of 0.66 and a recall of 0.85 on\nCLIP/Tiny-ImageNet.", "AI": {"tldr": "The paper introduces KoALA, an adversarial detector for deep neural networks that uses two complementary similarity metrics to identify attacks without requiring architectural changes or adversarial retraining.", "motivation": "Deep neural networks are vulnerable to adversarial attacks, posing risks to security- and safety-critical applications, which necessitates robust solutions for attack detection.", "method": "KoALA detects adversarial attacks by evaluating class label agreement using KL divergence (for dense, low-amplitude perturbations) and an L0-based similarity metric (for sparse, high-impact perturbations). A fine-tuning step on a pre-trained encoder aligns embeddings with these metrics.", "result": "Experimental results show that KoALA achieves high precision (0.94 on ResNet/CIFAR-10, 0.66 on CLIP/Tiny-ImageNet) and recall (0.81 on ResNet/CIFAR-10, 0.85 on CLIP/Tiny-ImageNet), validating its effectiveness under the stated conditions.", "conclusion": "KoALA is a lightweight, effective, and versatile adversarial detector that can easily integrate with existing models without requiring architectural modifications or adversarial retraining."}}
{"id": "2510.12769", "pdf": "https://arxiv.org/pdf/2510.12769", "abs": "https://arxiv.org/abs/2510.12769", "authors": ["Isaac Gibbs", "Ryan J. Tibshirani"], "title": "Sample-Efficient Omniprediction for Proper Losses", "categories": ["cs.LG", "stat.ME"], "comment": null, "summary": "We consider the problem of constructing probabilistic predictions that lead\nto accurate decisions when employed by downstream users to inform actions. For\na single decision maker, designing an optimal predictor is equivalent to\nminimizing a proper loss function corresponding to the negative utility of that\nindividual. For multiple decision makers, our problem can be viewed as a\nvariant of omniprediction in which the goal is to design a single predictor\nthat simultaneously minimizes multiple losses. Existing algorithms for\nachieving omniprediction broadly fall into two categories: 1) boosting methods\nthat optimize other auxiliary targets such as multicalibration and obtain\nomniprediction as a corollary, and 2) adversarial two-player game based\napproaches that estimate and respond to the ``worst-case\" loss in an online\nfashion. We give lower bounds demonstrating that multicalibration is a strictly\nmore difficult problem than omniprediction and thus the former approach must\nincur suboptimal sample complexity. For the latter approach, we discuss how\nthese ideas can be used to obtain a sample-efficient algorithm through an\nonline-to-batch conversion. This conversion has the downside of returning a\ncomplex, randomized predictor. We improve on this method by designing a more\ndirect, unrandomized algorithm that exploits structural elements of the set of\nproper losses.", "AI": {"tldr": "The paper addresses constructing probabilistic predictors for accurate decisions across single and multiple decision makers, comparing existing methods, and proposing improvements to optimize sample efficiency.", "motivation": "The motivation is to create predictors that ensure accurate decision-making for either single or multiple users, recognizing challenges in optimizing over diverse needs and losses.", "method": "The paper evaluates existing omniprediction methods, offers complexity analysis, proposes an online-to-batch conversion for sample-efficient prediction, and designs a new direct unrandomized algorithm leveraging structural properties.", "result": "It shows multicalibration to be harder than omniprediction, discusses limitations of randomized predictors, and introduces a more efficient unrandomized algorithm.", "conclusion": "The study advances sample-efficient prediction methods, overcoming flaws in existing approaches and providing direct, robust algorithms for minimizing decision-making losses."}}
{"id": "2510.11878", "pdf": "https://arxiv.org/pdf/2510.11878", "abs": "https://arxiv.org/abs/2510.11878", "authors": ["Anastasiya Pechko", "Piotr Borycki", "Joanna Waczy\u0144ska", "Daniel Barczyk", "Agata Szyma\u0144ska", "S\u0142awomir Tadeja", "Przemys\u0142aw Spurek"], "title": "GS-Verse: Mesh-based Gaussian Splatting for Physics-aware Interaction in Virtual Reality", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "As the demand for immersive 3D content grows, the need for intuitive and\nefficient interaction methods becomes paramount. Current techniques for\nphysically manipulating 3D content within Virtual Reality (VR) often face\nsignificant limitations, including reliance on engineering-intensive processes\nand simplified geometric representations, such as tetrahedral cages, which can\ncompromise visual fidelity and physical accuracy. In this paper, we introduce\n\\our{} (\\textbf{G}aussian \\textbf{S}platting for \\textbf{V}irtual\n\\textbf{E}nvironment \\textbf{R}endering and \\textbf{S}cene \\textbf{E}diting), a\nnovel method designed to overcome these challenges by directly integrating an\nobject's mesh with a Gaussian Splatting (GS) representation. Our approach\nenables more precise surface approximation, leading to highly realistic\ndeformations and interactions. By leveraging existing 3D mesh assets, \\our{}\nfacilitates seamless content reuse and simplifies the development workflow.\nMoreover, our system is designed to be physics-engine-agnostic, granting\ndevelopers robust deployment flexibility. This versatile architecture delivers\na highly realistic, adaptable, and intuitive approach to interactive 3D\nmanipulation. We rigorously validate our method against the current\nstate-of-the-art technique that couples VR with GS in a comparative user study\ninvolving 18 participants. Specifically, we demonstrate that our approach is\nstatistically significantly better for physics-aware stretching manipulation\nand is also more consistent in other physics-based manipulations like twisting\nand shaking. Further evaluation across various interactions and scenes confirms\nthat our method consistently delivers high and reliable performance, showing\nits potential as a plausible alternative to existing methods.", "AI": {"tldr": "The paper proposes Gaussian Splatting for Virtual Environment Rendering and Scene Editing (GS-VERSE), a novel method enabling more realistic 3D content manipulation in VR by directly integrating object meshes with Gaussian Splatting representation.", "motivation": "To address the limitations of existing VR manipulation methods that rely on simplified geometric representations and engineering-intensive processes, often resulting in lower visual fidelity and limited physical accuracy.", "method": "A new method integrates 3D meshes with Gaussian Splatting representations, enabling precise and realistic deformations for intuitive 3D manipulation. The system supports existing 3D assets, simplifies workflows, and is independent of specific physics engines.", "result": "Through a user study with 18 participants, the method was found statistically significantly better in physics-aware stretching manipulation and displayed consistent performance in other manipulations like twisting and shaking.", "conclusion": "The method offers a robust, realistic, and intuitive approach, demonstrating potential as a superior alternative for 3D interaction and manipulation in VR environments."}}
{"id": "2510.10681", "pdf": "https://arxiv.org/pdf/2510.10681", "abs": "https://arxiv.org/abs/2510.10681", "authors": ["Zichun Yu", "Chenyan Xiong"], "title": "RePro: Training Language Models to Faithfully Recycle the Web for Pretraining", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "High-quality pretraining data is the fossil fuel of large language models\n(LLMs), yet its reserves are running low for frontier models. In this paper, we\nintroduce RePro, a novel web recycling method that trains a relatively small LM\nwith reinforcement learning to generate effective and faithful rephrasings of\npretraining data. Specifically, we design one quality reward and three\nfaithfulness rewards, optimizing the LM rephraser to convert organic data into\nhigh-quality rephrasings while maintaining its core semantics and structure. In\nour experiment, we train a 4B rephraser to recycle 72B tokens sampled from\nDCLM-RefinedWeb. Pretraining results on 400M and 1.4B models demonstrate that\nRePro delivers 4.7%-14.0% relative accuracy gains over organic-only baseline on\n22 downstream tasks. RePro also outperforms ReWire, the state-of-the-art web\nrecycling method that prompts a 70B rephraser, as well as the organic baseline\nwith a 4x larger data pool. Experiments with different amounts of recycled data\nhighlight that RePro improves organic data efficiency by 2-3x. Individual and\ndistributional analyses validate that RePro preserves more critical information\nand faithfully reflects the characteristics of organic data compared to\nprompting-based methods. Together, these results show that RePro provides an\nefficient and controllable path to effectively harness the fossil fuel of LLM\npretraining. We open-source our code, rephraser, and recycled data at\nhttps://github.com/cxcscmu/RePro.", "AI": {"tldr": "RePro is a novel method for recycling web data into high-quality pretraining material for language models, achieving significant accuracy improvements and data efficiency.", "motivation": "The development of large language models relies heavily on high-quality pretraining data, but current data reserves are insufficient for advancing frontier models.", "method": "RePro uses a 4B model trained with reinforcement learning, leveraging one quality reward and three faithfulness rewards to generate rephrasings of data that retain semantic and structural fidelity.", "result": "RePro provides 4.7%-14.0% accuracy improvements across various tasks, surpasses the performance of leading web recycling methods, and increases data efficiency by 2-3x.", "conclusion": "RePro offers an efficient and controllable approach to transforming existing web data into high-quality pretraining material, proving its viability for optimizing data usage in model training."}}
{"id": "2510.11726", "pdf": "https://arxiv.org/pdf/2510.11726", "abs": "https://arxiv.org/abs/2510.11726", "authors": ["Zhaokang Liang", "Shuyang Zhuang", "Xiaoran Jiao", "Weian Mao", "Hao Chen", "Chunhua Shen"], "title": "scPPDM: A Diffusion Model for Single-Cell Drug-Response Prediction", "categories": ["q-bio.QM", "cs.LG"], "comment": null, "summary": "This paper introduces the Single-Cell Perturbation Prediction Diffusion Model\n(scPPDM), the first diffusion-based framework for single-cell drug-response\nprediction from scRNA-seq data. scPPDM couples two condition channels,\npre-perturbation state and drug with dose, in a unified latent space via\nnon-concatenative GD-Attn. During inference, factorized classifier-free\nguidance exposes two interpretable controls for state preservation and\ndrug-response strength and maps dose to guidance magnitude for tunable\nintensity. Evaluated on the Tahoe-100M benchmark under two stringent regimes,\nunseen covariate combinations (UC) and unseen drugs (UD), scPPDM sets new\nstate-of-the-art results across log fold-change recovery, delta correlations,\nexplained variance, and DE-overlap. Representative gains include\n+36.11%/+34.21% on DEG logFC-Spearman/Pearson in UD over the second-best model.\nThis control interface enables transparent what-if analyses and dose tuning,\nreducing experimental burden while preserving biological specificity.", "AI": {"tldr": "The paper introduces scPPDM, a novel diffusion-based framework for predicting single-cell drug responses using scRNA-seq data.", "motivation": "To improve accuracy and interpretability in single-cell drug-response predictions by addressing challenges in covariate and drug generalization.", "method": "scPPDM uses a diffusion-based model with dual condition channels (pre-perturbation state and drug dose) in a shared latent space, enabling tunable dose-response predictions via classifier-free guidance.", "result": "The model achieves state-of-the-art performance on the Tahoe-100M benchmark, especially in conditions with unseen covariates or drugs, with significant gains in metrics like DEG logFC-Spearman/Pearson correlations.", "conclusion": "scPPDM provides a transparent and adjustable framework for drug-response predictions, facilitating precise what-if analyses while reducing experimental burden."}}
{"id": "2510.11727", "pdf": "https://arxiv.org/pdf/2510.11727", "abs": "https://arxiv.org/abs/2510.11727", "authors": ["Benius Dunn", "Javier Meza-Arroyo", "Armi Tiihonen", "Mark Lee", "Julia W. P. Hsu"], "title": "Multi-objective Bayesian Optimization with Human-in-the-Loop for Flexible Neuromorphic Electronics Fabrication", "categories": ["cs.ET", "cond-mat.mtrl-sci", "cs.LG"], "comment": null, "summary": "Neuromorphic computing hardware enables edge computing and can be implemented\nin flexible electronics for novel applications. Metal oxide materials are\npromising candidates for fabricating flexible neuromorphic electronics, but\nsuffer from processing constraints due to the incompatibilities between oxides\nand polymer substrates. In this work, we use photonic curing to fabricate\nflexible metal-insulator-metal capacitors with solution-processible aluminum\noxide dielectric tailored for neuromorphic applications. Because photonic\ncuring outcomes depend on many input parameters, identifying an optimal\nprocessing condition through a traditional grid-search approach is unfeasible.\nHere, we apply multi-objective Bayesian optimization (MOBO) to determine\nphotonic curing conditions that optimize the trade-off between desired\nelectrical properties of large capacitance-frequency dispersion and low leakage\ncurrent. Furthermore, we develop a human-in-the-loop (HITL) framework for\nincorporating failed experiments into the MOBO machine learning workflow,\ndemonstrating that this framework accelerates optimization by reducing the\nnumber of experimental rounds required. Once optimization is concluded, we\nanalyze different Pareto-optimal conditions to tune the dielectrics properties\nand provide insight into the importance of different inputs through Shapley\nAdditive exPlanations analysis. The demonstrated framework of combining MOBO\nwith HITL feedback can be adapted to a wide range of multi-objective\nexperimental problems that have interconnected inputs and high experimental\nfailure rates to generate usable results for machine learning models.", "AI": {"tldr": "The research utilizes a combined method of multi-objective Bayesian optimization (MOBO) and human-in-the-loop feedback to optimize flexible metal-insulator-metal capacitors with aluminum oxide dielectrics for neuromorphic computing applications.", "motivation": "The motivation is to overcome the processing constraints in fabricating flexible metal oxide-based neuromorphic electronics and optimize the trade-offs between the electrical properties of aluminum oxide dielectrics.", "method": "The study uses photonic curing combined with MOBO and a human-in-the-loop (HITL) framework to determine ideal curing conditions. This approach incorporates failed experiments into the learning process and analyzes variable importance using Shapley Additive exPlanations.", "result": "The approach reduces the number of experimental rounds required, optimizes processing conditions, and provides insights into the relationship between inputs and the properties of the capacitors.", "conclusion": "This combined framework accelerates the optimization process and can be adapted to other complex experimental challenges, particularly in scenarios with interconnected variables and high experimental failure rates."}}
{"id": "2510.11744", "pdf": "https://arxiv.org/pdf/2510.11744", "abs": "https://arxiv.org/abs/2510.11744", "authors": ["Laura S\u00e1ez-Ortu\u00f1o", "Santiago Forgas-Coll", "Massimiliano Ferrara"], "title": "Quantum Kernel Methods: Convergence Theory, Separation Bounds and Applications to Marketing Analytics", "categories": ["quant-ph", "cs.LG"], "comment": "15 pages, 3 figures", "summary": "This work studies the feasibility of applying quantum kernel methods to a\nreal consumer classification task in the NISQ regime. We present a hybrid\npipeline that combines a quantum-kernel Support Vector Machine (Q-SVM) with a\nquantum feature extraction module (QFE), and benchmark it against classical and\nquantum baselines in simulation and with limited shallow-depth hardware runs.\nWith fixed hyperparameters, the proposed Q-SVM attains 0.7790 accuracy, 0.7647\nprecision, 0.8609 recall, 0.8100 F1, and 0.83 ROC AUC, exhibiting higher\nsensitivity while maintaining competitive precision relative to classical SVM.\nWe interpret these results as an initial indicator and a concrete starting\npoint for NISQ-era workflows and hardware integration, rather than a definitive\nbenchmark. Methodologically, our design aligns with recent work that formalizes\nquantum-classical separations and verifies resources via XEB-style approaches,\nmotivating shallow yet expressive quantum embeddings to achieve robust\nseparability despite hardware noise constraints.", "AI": {"tldr": "This paper investigates using quantum kernel methods for consumer classification in the Noisy Intermediate-Scale Quantum (NISQ) regime and proposes a hybrid quantum-classical pipeline.", "motivation": "The motivation for this paper is to explore the feasibility of quantum kernel methods as a practical solution for consumer classification tasks, taking into account current hardware constraints in the NISQ era.", "method": "The paper proposes a hybrid pipeline that incorporates quantum-kernel Support Vector Machines (Q-SVM) and quantum feature extraction (QFE). The approach is evaluated through simulations and shallow-depth quantum hardware tests.", "result": "The Q-SVM achieves competitive performance metrics, including an accuracy of 0.7790, precision of 0.7647, recall of 0.8609, F1 score of 0.8100, and a ROC AUC of 0.83, outperforming classical SVM in sensitivity and maintaining competitive precision.", "conclusion": "The results serve as a promising starting point for integrating quantum algorithms into consumer classification workflows in the NISQ era, rather than presenting a definitive benchmark, demonstrating the potential of shallow yet expressive quantum embeddings."}}
{"id": "2510.12425", "pdf": "https://arxiv.org/pdf/2510.12425", "abs": "https://arxiv.org/abs/2510.12425", "authors": ["Peng Chen", "Deliang Wei", "Jiale Yao", "Fang Li"], "title": "Tensor Completion via Monotone Inclusion: Generalized Low-Rank Priors Meet Deep Denoisers", "categories": ["math.OC", "cs.CV", "65K10, 68T07, 94A08"], "comment": "22 pages, 5 figures", "summary": "Missing entries in multi dimensional data pose significant challenges for\ndownstream analysis across diverse real world applications. These data are\nnaturally modeled as tensors, and recent completion methods integrating global\nlow rank priors with plug and play denoisers have demonstrated strong empirical\nperformance. However, these approaches often rely on empirical convergence\nalone or unrealistic assumptions, such as deep denoisers acting as proximal\noperators of implicit regularizers, which generally does not hold. To address\nthese limitations, we propose a novel tensor completion framework grounded in\nthe monotone inclusion paradigm, which unifies generalized low rank priors with\ndeep pseudo contractive denoisers and extends beyond traditional convex\noptimization. Building on the Davis Yin splitting scheme, we develop the GTCTV\nDPC algorithm and rigorously establish its global convergence. Extensive\nexperiments demonstrate that GTCTV DPC consistently outperforms existing\nmethods in both quantitative metrics and visual quality, particularly at low\nsampling rates.", "AI": {"tldr": "The study introduces a tensor completion framework (GTCTV DPC) that combines generalized low rank priors with deep pseudo-contractive denoisers, achieving superior performance in handling missing tensor data.", "motivation": "Missing entries in multi-dimensional data create difficulties for downstream analysis in various applications. Current tensor-based methods rely on empirical assumptions, which may not always hold.", "method": "The proposed framework employs a monotone inclusion paradigm with a combination of generalized low rank priors and deep pseudo-contractive denoisers. It uses the Davis-Yin splitting scheme to ensure global convergence.", "result": "The GTCTV DPC algorithm is experimentally validated, showing improved performance (better quantitative metrics and visual quality) compared to existing methods, especially at low sampling rates.", "conclusion": "GTCTV DPC provides a robust solution for tensor completion and addresses limitations of prior approaches by backing its methodology with rigorous theoretical convergence guarantees."}}
{"id": "2510.11750", "pdf": "https://arxiv.org/pdf/2510.11750", "abs": "https://arxiv.org/abs/2510.11750", "authors": ["Sazan Mahbub", "Souvik Kundu", "Eric P. Xing"], "title": "PRISM: Enhancing Protein Inverse Folding through Fine-Grained Retrieval on Structure-Sequence Multimodal Representations", "categories": ["q-bio.QM", "cs.LG"], "comment": null, "summary": "Designing protein sequences that fold into a target three-dimensional\nstructure, known as the inverse folding problem, is central to protein\nengineering but remains challenging due to the vast sequence space and the\nimportance of local structural constraints. Existing deep learning approaches\nachieve strong recovery rates, yet they lack explicit mechanisms to reuse\nfine-grained structure-sequence patterns that are conserved across natural\nproteins. We present PRISM, a multimodal retrieval-augmented generation\nframework for inverse folding that retrieves fine-grained representations of\npotential motifs from known proteins and integrates them with a hybrid\nself-cross attention decoder. PRISM is formulated as a latent-variable\nprobabilistic model and implemented with an efficient approximation, combining\ntheoretical grounding with practical scalability. Across five benchmarks\n(CATH-4.2, TS50, TS500, CAMEO 2022, and the PDB date split), PRISM establishes\nnew state of the art in both perplexity and amino acid recovery, while also\nimproving foldability metrics (RMSD, TM-score, pLDDT), demonstrating that\nfine-grained multimodal retrieval is a powerful and efficient paradigm for\nprotein sequence design.", "AI": {"tldr": "PRISM is a novel deep learning framework addressing the protein inverse folding problem with state-of-the-art results.", "motivation": "To address the challenge of protein design due to vast sequence space and structural constraints, and to leverage conserved patterns from natural proteins that existing models underexploit.", "method": "A multimodal retrieval-augmented generation framework that retrieves motif representations from known proteins and integrates them via a hybrid self-cross attention decoder.", "result": "Outperformed existing methods across five benchmarks in metrics like perplexity, amino acid recovery, and foldability.", "conclusion": "Multimodal retrieval-based approaches like PRISM offer a powerful and efficient solution for protein sequence design."}}
{"id": "2510.12252", "pdf": "https://arxiv.org/pdf/2510.12252", "abs": "https://arxiv.org/abs/2510.12252", "authors": ["Yuqi Jia", "Yupei Liu", "Zedian Shao", "Jinyuan Jia", "Neil Gong"], "title": "PromptLocate: Localizing Prompt Injection Attacks", "categories": ["cs.CR", "cs.AI"], "comment": "To appear in IEEE Symposium on Security and Privacy, 2026", "summary": "Prompt injection attacks deceive a large language model into completing an\nattacker-specified task instead of its intended task by contaminating its input\ndata with an injected prompt, which consists of injected instruction(s) and\ndata. Localizing the injected prompt within contaminated data is crucial for\npost-attack forensic analysis and data recovery. Despite its growing\nimportance, prompt injection localization remains largely unexplored. In this\nwork, we bridge this gap by proposing PromptLocate, the first method for\nlocalizing injected prompts. PromptLocate comprises three steps: (1) splitting\nthe contaminated data into semantically coherent segments, (2) identifying\nsegments contaminated by injected instructions, and (3) pinpointing segments\ncontaminated by injected data. We show PromptLocate accurately localizes\ninjected prompts across eight existing and eight adaptive attacks.", "AI": {"tldr": "The paper introduces PromptLocate, the first method for identifying injected prompts in contaminated inputs caused by prompt injection attacks.", "motivation": "Prompt injection attacks compromise large language models by tricking them into executing malicious instructions embedded in input data. Detecting the injected prompt is critical for forensic analysis and recovery.", "method": "PromptLocate executes three steps: semantic segmentation of input data, detection of segments with injected instructions, and identification of segments with injected data.", "result": "PromptLocate was demonstrated to accurately identify injected prompts across diverse attacks, including existing and adaptive variations.", "conclusion": "PromptLocate effectively addresses the challenge of localizing injected prompts, filling an unexplored gap in understanding and mitigating prompt injection attacks."}}
{"id": "2510.12709", "pdf": "https://arxiv.org/pdf/2510.12709", "abs": "https://arxiv.org/abs/2510.12709", "authors": ["Lin Lin", "Jiefeng Long", "Zhihe Wan", "Yuchi Wang", "Dingkang Yang", "Shuang Yang", "Yueyang Yao", "Xu Chen", "Zirui Guo", "Shengqiang Li", "Weiran Li", "Hanyu Li", "Yaling Mou", "Yan Qiu", "Haiyang Yu", "Xiao Liang", "Hongsheng Li", "Chao Feng"], "title": "SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model", "categories": ["cs.IR", "cs.CV"], "comment": "Technical Report", "summary": "Multimodal embedding models aim to yield informative unified representations\nthat empower diverse cross-modal tasks. Despite promising developments in the\nevolution from CLIP-based dual-tower architectures to large vision-language\nmodels, prior works still face unavoidable challenges in real-world\napplications and business scenarios, such as the limited modality support,\nunstable training mechanisms, and industrial domain gaps. In this work, we\nintroduce SAIL-Embedding, an omni-modal embedding foundation model that\naddresses these issues through tailored training strategies and architectural\ndesign. In the optimization procedure, we propose a multi-stage training scheme\nto boost the multifaceted effectiveness of representation learning.\nSpecifically, the content-aware progressive training aims to enhance the\nmodel's adaptability to diverse downstream tasks and master enriched\ncross-modal proficiency. The collaboration-aware recommendation enhancement\ntraining further adapts multimodal representations for recommendation scenarios\nby distilling knowledge from sequence-to-item and ID-to-item embeddings while\nmining user historical interests. Concurrently, we develop the stochastic\nspecialization and dataset-driven pattern matching to strengthen model training\nflexibility and generalizability. Experimental results show that SAIL-Embedding\nachieves SOTA performance compared to other methods in different retrieval\ntasks. In online experiments across various real-world scenarios integrated\nwith our model, we observe a significant increase in Lifetime (LT), which is a\ncrucial indicator for the recommendation experience. For instance, the model\ndelivers the 7-day LT gain of +0.158% and the 14-day LT gain of +0.144% in the\nDouyin-Selected scenario. For the Douyin feed rank model, the match features\nproduced by SAIL-Embedding yield a +0.08% AUC gain.", "AI": {"tldr": "SAIL-Embedding addresses limitations in multimodal embedding models by introducing tailored training strategies and a new architecture, achieving superior results in recommendation and retrieval tasks.", "motivation": "Previous multimodal embedding models faced challenges like limited modality support, unstable training mechanics, and adaptation to industrial applications.", "method": "The model uses a multi-stage training approach, including content-aware and collaboration-aware training, alongside other optimizations like stochastic specialization and dataset-driven pattern matching.", "result": "SAIL-Embedding achieves state-of-the-art performance in retrieval tasks and improves crucial metrics in real-world applications, such as significant Lifetime (LT) and AUC gains in the Douyin platform.", "conclusion": "The proposed model overcomes prior limitations with an omni-modal approach and tailored training methods, resulting in enhanced performance and practical application in recommendation systems."}}
{"id": "2510.12265", "pdf": "https://arxiv.org/pdf/2510.12265", "abs": "https://arxiv.org/abs/2510.12265", "authors": ["Sami Khairy", "Gabriel Mittag", "Vishak Gopal", "Ross Cutler"], "title": "Human-in-the-Loop Bandwidth Estimation for Quality of Experience Optimization in Real-Time Video Communication", "categories": ["cs.MM", "cs.AI", "cs.NI", "cs.SY", "eess.SY"], "comment": "Accepted for publication in the proceedings of the AAAI Conference on\n  Artificial Intelligence 2026 (IAAI Technical Track on Deployed Highly\n  Innovative Applications of AI)", "summary": "The quality of experience (QoE) delivered by video conferencing systems is\nsignificantly influenced by accurately estimating the time-varying available\nbandwidth between the sender and receiver. Bandwidth estimation for real-time\ncommunications remains an open challenge due to rapidly evolving network\narchitectures, increasingly complex protocol stacks, and the difficulty of\ndefining QoE metrics that reliably improve user experience. In this work, we\npropose a deployed, human-in-the-loop, data-driven framework for bandwidth\nestimation to address these challenges. Our approach begins with training\nobjective QoE reward models derived from subjective user evaluations to measure\naudio and video quality in real-time video conferencing systems. Subsequently,\nwe collect roughly $1$M network traces with objective QoE rewards from\nreal-world Microsoft Teams calls to curate a bandwidth estimation training\ndataset. We then introduce a novel distributional offline reinforcement\nlearning (RL) algorithm to train a neural-network-based bandwidth estimator\naimed at improving QoE for users. Our real-world A/B test demonstrates that the\nproposed approach reduces the subjective poor call ratio by $11.41\\%$ compared\nto the baseline bandwidth estimator. Furthermore, the proposed offline RL\nalgorithm is benchmarked on D4RL tasks to demonstrate its generalization beyond\nbandwidth estimation.", "AI": {"tldr": "This paper proposes a data-driven, human-in-the-loop framework using offline reinforcement learning (RL) for improving bandwidth estimation in video conferencing systems, demonstrating a reduction in poor call ratios and generalization capabilities.", "motivation": "Accurately estimating time-varying bandwidth is crucial for enhancing the QoE in real-time video conferencing systems, but it remains a challenge due to evolving network architectures and complex protocols.", "method": "The authors trained QoE reward models using subjective user evaluations, curated a huge dataset from real-world Microsoft Teams calls, and developed a novel offline RL algorithm for a neural network-based bandwidth estimator.", "result": "The proposed approach improved QoE by reducing the subjective poor call ratio by 11.41% compared to a baseline estimator, and the RL algorithm showed generalization in D4RL benchmark tasks.", "conclusion": "The framework successfully improves QoE in video conferencing and highlights the potential of offline RL algorithms beyond bandwidth estimation."}}
{"id": "2510.12275", "pdf": "https://arxiv.org/pdf/2510.12275", "abs": "https://arxiv.org/abs/2510.12275", "authors": ["Youhao Si", "Yuan Liao", "Qiushi Han", "Yuhang Yang", "Rui Dai", "Liya Huang"], "title": "TFGA-Net: Temporal-Frequency Graph Attention Network for Brain-Controlled Speaker Extraction", "categories": ["cs.SD", "cs.AI"], "comment": "5 pages, 3 figures", "summary": "The rapid development of auditory attention decoding (AAD) based on\nelectroencephalography (EEG) signals offers the possibility EEG-driven target\nspeaker extraction. However, how to effectively utilize the target-speaker\ncommon information between EEG and speech remains an unresolved problem. In\nthis paper, we propose a model for brain-controlled speaker extraction, which\nutilizes the EEG recorded from the listener to extract the target speech. In\norder to effectively extract information from EEG signals, we derive\nmulti-scale time--frequency features and further incorporate cortical\ntopological structures that are selectively engaged during the task. Moreover,\nto effectively exploit the non-Euclidean structure of EEG signals and capture\ntheir global features, the graph convolutional networks and self-attention\nmechanism are used in the EEG encoder. In addition, to make full use of the\nfused EEG and speech feature and preserve global context and capture speech\nrhythm and prosody, we introduce MossFormer2 which combines MossFormer and\nRNN-Free Recurrent as separator. Experimental results on both the public\nCocktail Party and KUL dataset in this paper show that our TFGA-Net model\nsignificantly outper-forms the state-of-the-art method in certain objective\nevaluation metrics. The source code is available at:\nhttps://github.com/LaoDa-X/TFGA-NET.", "AI": {"tldr": "The paper proposes a brain-controlled speaker extraction model using EEG signals and introduces an advanced model (TFGA-Net) leveraging multi-scale time-frequency features and advanced neural network architectures.", "motivation": "To address the challenge of effectively utilizing common information between EEG and speech signals in the context of EEG-driven target speaker extraction.", "method": "The study proposes a method combining EEG-derived multi-scale time-frequency features, cortical topology, graph convolutional networks, self-attention mechanisms, and the MossFormer2 architecture to enhance speaker extraction.", "result": "The proposed TFGA-Net model achieved superior performance on public datasets (Cocktail Party and KUL) compared to state-of-the-art methods in specific evaluation metrics.", "conclusion": "TFGA-Net demonstrates effective utilization of EEG and speech features for target speaker extraction, introducing promising methodologies for improving such systems. The source code is openly available for further research."}}
{"id": "2510.12278", "pdf": "https://arxiv.org/pdf/2510.12278", "abs": "https://arxiv.org/abs/2510.12278", "authors": ["Alessia Ciacco", "Francesca Guerriero", "Eneko Osaba"], "title": "Quantum Annealing for Staff Scheduling in Educational Environments", "categories": ["cs.ET", "cs.AI"], "comment": "8 pages, 3 tables, and 1 figure. Paper submitted to the International\n  Conference on Quantum Communications, Networking, and Computing (QCNC 2026)", "summary": "We address a novel staff allocation problem that arises in the organization\nof collaborators among multiple school sites and educational levels. The\nproblem emerges from a real case study in a public school in Calabria, Italy,\nwhere staff members must be distributed across kindergartens, primary, and\nsecondary schools under constraints of availability, competencies, and\nfairness. To tackle this problem, we develop an optimization model and\ninvestigate a solution approach based on quantum annealing. Our computational\nexperiments on real-world data show that quantum annealing is capable of\nproducing balanced assignments in short runtimes. These results provide\nevidence of the practical applicability of quantum optimization methods in\neducational scheduling and, more broadly, in complex resource allocation tasks.", "AI": {"tldr": "This paper tackles a staff allocation problem in a public school system using quantum annealing, showing efficient and balanced solutions.", "motivation": "The aim is to address a staff allocation issue involving multiple school sites and levels in Calabria, Italy, while considering constraints such as availability, competencies, and fairness.", "method": "The researchers developed an optimization model and utilized quantum annealing techniques to achieve efficient staff allocations.", "result": "Quantum annealing delivered balanced staff assignments with short runtimes, demonstrating its computational capability on real-world data.", "conclusion": "Quantum annealing proves effective for practical use in educational scheduling tasks and complex resource allocation challenges."}}
{"id": "2510.12325", "pdf": "https://arxiv.org/pdf/2510.12325", "abs": "https://arxiv.org/abs/2510.12325", "authors": ["Jie Yang", "Chenyang Gu", "Zixuan Liu"], "title": "Causal Inspired Multi Modal Recommendation", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Multimodal recommender systems enhance personalized recommendations in\ne-commerce and online advertising by integrating visual, textual, and user-item\ninteraction data. However, existing methods often overlook two critical biases:\n(i) modal confounding, where latent factors (e.g., brand style or product\ncategory) simultaneously drive multiple modalities and influence user\npreference, leading to spurious feature-preference associations; (ii)\ninteraction bias, where genuine user preferences are mixed with noise from\nexposure effects and accidental clicks. To address these challenges, we propose\na Causal-inspired multimodal Recommendation framework. Specifically, we\nintroduce a dual-channel cross-modal diffusion module to identify hidden modal\nconfounders, utilize back-door adjustment with hierarchical matching and\nvector-quantized codebooks to block confounding paths, and apply front-door\nadjustment combined with causal topology reconstruction to build a deconfounded\ncausal subgraph. Extensive experiments on three real-world e-commerce datasets\ndemonstrate that our method significantly outperforms state-of-the-art\nbaselines while maintaining strong interpretability.", "AI": {"tldr": "The paper proposes a causal framework for multimodal recommender systems to address biases like modal confounding and interaction bias, enhancing performance and interpretability.", "motivation": "To mitigate critical biases in multimodal recommender systems, such as modal confounding and interaction bias, which create spurious associations and dilute genuine preferences.", "method": "A causal-inspired framework introduces a dual-channel cross-modal diffusion module, back-door adjustment with hierarchical matching and vector-quantized codebooks, and front-door adjustment for causal topology reconstruction to address biases.", "result": "Extensive tests on three real-world e-commerce datasets showed superior performance over state-of-the-art methods while maintaining strong interpretability.", "conclusion": "The introduced causal-inspired multimodal recommendation approach effectively corrects biases, demonstrating both improved recommendation accuracy and interpretability, beneficial for e-commerce applications."}}
{"id": "2510.12014", "pdf": "https://arxiv.org/pdf/2510.12014", "abs": "https://arxiv.org/abs/2510.12014", "authors": ["Eric He", "Akash Gupta", "Adian Liusie", "Vatsal Raina", "Piotr Molenda", "Shirom Chabra", "Vyas Raina"], "title": "Embedding the Teacher: Distilling vLLM Preferences for Scalable Image Retrieval", "categories": ["cs.IR", "cs.LG"], "comment": null, "summary": "Text--image retrieval is necessary for applications such as product\nrecommendation. Embedding-based approaches like CLIP enable efficient\nlarge-scale retrieval via vector similarity search, but they are primarily\ntrained on literal caption-like text--image pairs and often fail to capture\nabstract or persona-driven attributes common in product recommendation\napplications (e.g., ``a gift for a mother who loves gardening''). In contrast,\nstate-of-the-art vision--language models (vLLMs) can align text with images in\na flexible manner, but their limited context window prevents them from directly\nhandling retrieval over large catalogs. We propose a framework that distills\nthe preference rankings of a powerful vLLM into an embedding-based system,\ntransferring its nuanced alignment abilities while maintaining the\ninference-time scalability of an embedding-based approach. Experiments on\npersona-driven product recommendation tasks demonstrate that our method\nsignificantly outperforms existing embedding-based baselines, providing an\nefficient solution for personalized text--image retrieval.", "AI": {"tldr": "The paper proposes a framework that bridges vision-language models and embedding-based systems to improve personalized text-image retrieval for tasks like product recommendation.", "motivation": "Current embedding-based methods like CLIP are efficient but fail to capture abstract text-image pairs, which are critical in applications such as personalized product recommendation.", "method": "The proposed framework distills preference rankings from a vision-language model into an embedding-based system, combining nuanced alignment with scalability.", "result": "The method significantly outperforms existing embedding-based baselines in persona-driven product recommendation tasks.", "conclusion": "The approach effectively enhances text-image retrieval by coupling the alignment capability of vision-language models with the scalability of embedding-based methods, providing an efficient solution."}}
{"id": "2510.12054", "pdf": "https://arxiv.org/pdf/2510.12054", "abs": "https://arxiv.org/abs/2510.12054", "authors": ["Wenjin Xie", "Tao Jia"], "title": "MIARec: Mutual-influence-aware Heterogeneous Network Embedding for Scientific Paper Recommendation", "categories": ["cs.IR", "cs.LG"], "comment": null, "summary": "With the rapid expansion of scientific literature, scholars increasingly\ndemand precise and high-quality paper recommendations. Among various\nrecommendation methodologies, graph-based approaches have garnered attention by\neffectively exploiting the structural characteristics inherent in scholarly\nnetworks. However, these methods often overlook the asymmetric academic\ninfluence that is prevalent in scholarly networks when learning graph\nrepresentations. To address this limitation, this study proposes the\nMutual-Influence-Aware Recommendation (MIARec) model, which employs a\ngravity-based approach to measure the mutual academic influence between\nscholars and incorporates this influence into the feature aggregation process\nduring message propagation in graph representation learning. Additionally, the\nmodel utilizes a multi-channel aggregation method to capture both individual\nembeddings of distinct single relational sub-networks and their interdependent\nembeddings, thereby enabling a more comprehensive understanding of the\nheterogeneous scholarly network. Extensive experiments conducted on real-world\ndatasets demonstrate that the MIARec model outperforms baseline models across\nthree primary evaluation metrics, indicating its effectiveness in scientific\npaper recommendation tasks.", "AI": {"tldr": "This study introduces the MIARec model, which enhances paper recommendations by improving graph representation learning with mutual academic influence and multi-channel aggregation.", "motivation": "To improve mechanism for recommending scientific papers by addressing limitations in existing graph-based approaches that neglect asymmetric academic influence.", "method": "The study proposes the MIARec model using a gravity-based approach to measure mutual academic influence and multi-channel aggregation for comprehensive graph representation learning.", "result": "Experiments showed that MIARec outperforms baseline models in scientific paper recommendation tasks across three main metrics.", "conclusion": "Incorporating mutual academic influence and multi-channel aggregation leads to a more effective scholarly network representation, improving recommendation accuracy."}}
{"id": "2510.12379", "pdf": "https://arxiv.org/pdf/2510.12379", "abs": "https://arxiv.org/abs/2510.12379", "authors": ["Vibhoothi Vibhoothi", "Fran\u00e7ois Piti\u00e9", "Anil Kokaram"], "title": "LiteVPNet: A Lightweight Network for Video Encoding Control in Quality-Critical Applications", "categories": ["eess.IV", "cs.AI", "cs.LG", "cs.MM"], "comment": "Accepted PCS 2025 Camera-Ready Version, 5 Pages", "summary": "In the last decade, video workflows in the cinema production ecosystem have\npresented new use cases for video streaming technology. These new workflows,\ne.g. in On-set Virtual Production, present the challenge of requiring precise\nquality control and energy efficiency. Existing approaches to transcoding often\nfall short of these requirements, either due to a lack of quality control or\ncomputational overhead. To fill this gap, we present a lightweight neural\nnetwork (LiteVPNet) for accurately predicting Quantisation Parameters for NVENC\nAV1 encoders that achieve a specified VMAF score. We use low-complexity\nfeatures, including bitstream characteristics, video complexity measures, and\nCLIP-based semantic embeddings. Our results demonstrate that LiteVPNet achieves\nmean VMAF errors below 1.2 points across a wide range of quality targets.\nNotably, LiteVPNet achieves VMAF errors within 2 points for over 87% of our\ntest corpus, c.f. approx 61% with state-of-the-art methods. LiteVPNet's\nperformance across various quality regions highlights its applicability for\nenhancing high-value content transport and streaming for more energy-efficient,\nhigh-quality media experiences.", "AI": {"tldr": "The paper introduces LiteVPNet, a lightweight neural network to improve video transcoding for Virtual Production workflows by accurately predicting encoding parameters for quality preservation.", "motivation": "The motivation is to address challenges in video production workflows that demand high-quality control and energy efficiency, which are unmet by existing transcoding methods.", "method": "The proposed LiteVPNet leverages low-complexity features such as bitstream characteristics, video complexity measures, and CLIP-based semantic embeddings to predict Quantisation Parameters for video encoders.", "result": "LiteVPNet achieves a mean VMAF error below 1.2 points and performs notably better than state-of-the-art methods, with over 87% accuracy in predicting quality within 2 VMAF points.", "conclusion": "LiteVPNet proves to be applicable for enhancing media streaming quality and efficiency in Virtual Production workflows, offering improved precision and energy-efficient solutions."}}
{"id": "2510.12384", "pdf": "https://arxiv.org/pdf/2510.12384", "abs": "https://arxiv.org/abs/2510.12384", "authors": ["Huifa Li", "Feilong Tang", "Haochen Xue", "Yulong Li", "Xinlin Zhuang", "Bin Zhang", "Eran Segal", "Imran Razzak"], "title": "Phenome-Wide Multi-Omics Integration Uncovers Distinct Archetypes of Human Aging", "categories": ["q-bio.GN", "cs.AI"], "comment": null, "summary": "Aging is a highly complex and heterogeneous process that progresses at\ndifferent rates across individuals, making biological age (BA) a more accurate\nindicator of physiological decline than chronological age. While previous\nstudies have built aging clocks using single-omics data, they often fail to\ncapture the full molecular complexity of human aging. In this work, we\nleveraged the Human Phenotype Project, a large-scale cohort of 12,000 adults\naged 30--70 years, with extensive longitudinal profiling that includes\nclinical, behavioral, environmental, and multi-omics datasets -- spanning\ntranscriptomics, lipidomics, metabolomics, and the microbiome. By employing\nadvanced machine learning frameworks capable of modeling nonlinear biological\ndynamics, we developed and rigorously validated a multi-omics aging clock that\nrobustly predicts diverse health outcomes and future disease risk. Unsupervised\nclustering of the integrated molecular profiles from multi-omics uncovered\ndistinct biological subtypes of aging, revealing striking heterogeneity in\naging trajectories and pinpointing pathway-specific alterations associated with\ndifferent aging patterns. These findings demonstrate the power of multi-omics\nintegration to decode the molecular landscape of aging and lay the groundwork\nfor personalized healthspan monitoring and precision strategies to prevent\nage-related diseases.", "AI": {"tldr": "This paper builds a multi-omics aging clock using machine learning and extensive datasets to robustly predict health outcomes, reveal diverse aging patterns, and enable personalized health strategies.", "motivation": "Aging varies across individuals, and developing biological age indicators is crucial for proactive health monitoring. Previous single-omics approaches fail to capture the complexity of aging.", "method": "Utilizing a large cohort with multi-omics datasets (transcriptomics, lipidomics, metabolomics, microbiome), advanced machine learning models were applied to develop an integrated aging clock and cluster biological aging subtypes.", "result": "The multi-omics aging clock accurately predicts health outcomes and disease risks, while clustering illuminates distinct biological aging subtypes and their molecular pathways.", "conclusion": "Multi-omics integration offers powerful insights into aging, underlining its potential for personalized health and strategies for age-related disease prevention."}}
{"id": "2510.12078", "pdf": "https://arxiv.org/pdf/2510.12078", "abs": "https://arxiv.org/abs/2510.12078", "authors": ["Sijing Xie", "Dingzhu Wen", "Changsheng You", "Qimei Chen", "Mehdi Bennis", "Kaibin Huang"], "title": "FedLoDrop: Federated LoRA with Dropout for Generalized LLM Fine-tuning", "categories": ["cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "Fine-tuning (FT) large language models (LLMs) is crucial for adapting\ngeneral-purpose models to specific tasks, enhancing accuracy and relevance with\nminimal resources. To further enhance generalization ability while reducing\ntraining costs, this paper proposes Federated LoRA with Dropout (FedLoDrop), a\nnew framework that applies dropout to the rows and columns of the trainable\nmatrix in Federated LoRA. A generalization error bound and convergence analysis\nunder sparsity regularization are obtained, which elucidate the fundamental\ntrade-off between underfitting and overfitting. The error bound reveals that a\nhigher dropout rate increases model sparsity, thereby lowering the upper bound\nof pointwise hypothesis stability (PHS). While this reduces the gap between\nempirical and generalization errors, it also incurs a higher empirical error,\nwhich, together with the gap, determines the overall generalization error. On\nthe other hand, though dropout reduces communication costs, deploying FedLoDrop\nat the network edge still faces challenges due to limited network resources. To\naddress this issue, an optimization problem is formulated to minimize the upper\nbound of the generalization error, by jointly optimizing the dropout rate and\nresource allocation subject to the latency and per-device energy consumption\nconstraints. To solve this problem, a branch-and-bound (B\\&B)-based method is\nproposed to obtain its globally optimal solution. Moreover, to reduce the high\ncomputational complexity of the B\\&B-based method, a penalized successive\nconvex approximation (P-SCA)-based algorithm is proposed to efficiently obtain\nits high-quality suboptimal solution. Finally, numerical results demonstrate\nthe effectiveness of the proposed approach in mitigating overfitting and\nimproving the generalization capability.", "AI": {"tldr": "The paper introduces FedLoDrop, a framework that uses dropout techniques in Federated LoRA to enhance generalization while minimizing costs.", "motivation": "The need to adapt large language models efficiently to specific tasks to improve relevance and accuracy, while addressing overfitting, underfitting, and communication challenges.", "method": "FedLoDrop applies dropout to matrix rows and columns, analyzes the trade-off between errors, optimizes dropout rates and resources via B&B and P-SCA methods.", "result": "FedLoDrop reduces overfitting, lowers generalization errors, improves communication efficiency, and offers a manageable computational complexity.", "conclusion": "FedLoDrop is effective for enhancing model generalization and efficiency in network-constrained settings, validated by numerical results."}}
{"id": "2510.12117", "pdf": "https://arxiv.org/pdf/2510.12117", "abs": "https://arxiv.org/abs/2510.12117", "authors": ["Lipeng He", "Vasisht Duddu", "N. Asokan"], "title": "Locket: Robust Feature-Locking Technique for Language Models", "categories": ["cs.CR", "cs.LG"], "comment": "12 pages, 3 figures", "summary": "Chatbot providers (e.g., OpenAI) rely on tiered subscription schemes to\ngenerate revenue, offering basic models for free users, and advanced models for\npaying subscribers. However, a finer-grained pay-to-unlock scheme for premium\nfeatures (e.g., math, coding) is thought to be more economically viable for the\nproviders. Such a scheme requires a feature-locking technique (FLoTE) which is\n(i) effective in refusing locked features, (ii) utility-preserving for unlocked\nfeatures, (iii) robust against evasion or unauthorized credential sharing, and\n(iv) scalable to multiple features and users. However, existing FLoTEs (e.g.,\npassword-locked models) are not robust or scalable. We present Locket, the\nfirst robust and scalable FLoTE to enable pay-to-unlock schemes. Locket uses a\nnovel merging approach to attach adapters to an LLM for refusing unauthorized\nfeatures. Our comprehensive evaluation shows that Locket is effective ($100$%\nrefusal on locked features), utility-preserving ($\\leq 7$% utility degradation\nin unlocked features), robust ($\\leq 5$% attack success rate), and scales to\nmultiple features and clients.", "AI": {"tldr": "The paper introduces 'Locket,' a robust and scalable feature-locking technique for tiered chatbot subscription models, enabling pay-to-unlock schemes.", "motivation": "Current pay-to-unlock schemes for chatbots lack robust and scalable feature-locking techniques, needed to balance economic viability with effective feature controls.", "method": "Locket employs a novel merging approach to attach adapters to large language models (LLMs), effectively refusing unauthorized features while preserving functionality.", "result": "Evaluation shows Locket achieves 100% refusal for unauthorized features, \u22647% utility degradation in unlocked features, \u22645% attack success rate, and scalability across multiple features and clients.", "conclusion": "Locket addresses the limitations of existing techniques, offering a scalable and robust solution for pay-to-unlock schemes in chatbot systems."}}
{"id": "2510.12148", "pdf": "https://arxiv.org/pdf/2510.12148", "abs": "https://arxiv.org/abs/2510.12148", "authors": ["Yuki Yasuda", "Ryo Onishi"], "title": "Probabilistic Super-Resolution for Urban Micrometeorology via a Schr\u00f6dinger Bridge", "categories": ["physics.ao-ph", "cs.LG"], "comment": null, "summary": "This study employs a neural network that represents the solution to a\nSchr\\\"odinger bridge problem to perform super-resolution of 2-m temperature in\nan urban area. Schr\\\"odinger bridges generally describe transformations between\ntwo data distributions based on diffusion processes. We use a specific\nSchr\\\"odinger-bridge model (SM) that directly transforms low-resolution data\ninto high-resolution data, unlike denoising diffusion probabilistic models\n(simply, diffusion models; DMs) that generate high-resolution data from\nGaussian noise. Low-resolution and high-resolution data were obtained from\nseparate numerical simulations with a physics-based model under common initial\nand boundary conditions. Compared with a DM, the SM attains comparable accuracy\nat one-fifth the computational cost, requiring 50 neural-network evaluations\nper datum for the DM and only 10 for the SM. Furthermore, high-resolution\nsamples generated by the SM exhibit larger variance, implying superior\nuncertainty quantification relative to the DM. Owing to the reduced\ncomputational cost of the SM, our results suggest the feasibility of real-time\nensemble micrometeorological prediction using SM-based super-resolution.", "AI": {"tldr": "This paper uses a Schr\u00f6dinger bridge neural network to improve super-resolution of urban temperature data, achieving comparable accuracy at lower computational cost compared to traditional methods.", "motivation": "The paper aims to address the computational inefficiency and uncertainty quantification challenges in generating high-resolution urban temperature data.", "method": "The study utilizes a Schr\u00f6dinger-bridge neural network model that transforms low-resolution data into high-resolution data based on diffusion processes, while contrasting its performance against denoising diffusion probabilistic models.", "result": "The Schr\u00f6dinger bridge model achieves comparable accuracy to diffusion models but at one-fifth the computational cost and demonstrates superior uncertainty quantification via higher variance in generated samples.", "conclusion": "The reduced computational cost and advanced uncertainty quantification suggest that Schr\u00f6dinger-bridge-based super-resolution can enable real-time ensemble micrometeorological predictions."}}
{"id": "2510.12180", "pdf": "https://arxiv.org/pdf/2510.12180", "abs": "https://arxiv.org/abs/2510.12180", "authors": ["Mo Zhou", "Haosheng Zhou", "Ruimeng Hu"], "title": "Learning Mean-Field Games through Mean-Field Actor-Critic Flow", "categories": ["math.OC", "cs.LG", "35Q89, 49N80"], "comment": null, "summary": "We propose the Mean-Field Actor-Critic (MFAC) flow, a continuous-time\nlearning dynamics for solving mean-field games (MFGs), combining techniques\nfrom reinforcement learning and optimal transport. The MFAC framework jointly\nevolves the control (actor), value function (critic), and distribution\ncomponents through coupled gradient-based updates governed by partial\ndifferential equations (PDEs). A central innovation is the Optimal Transport\nGeodesic Picard (OTGP) flow, which drives the distribution toward equilibrium\nalong Wasserstein-2 geodesics. We conduct a rigorous convergence analysis using\nLyapunov functionals and establish global exponential convergence of the MFAC\nflow under a suitable timescale. Our results highlight the algorithmic\ninterplay among actor, critic, and distribution components. Numerical\nexperiments illustrate the theoretical findings and demonstrate the\neffectiveness of the MFAC framework in computing MFG equilibria.", "AI": {"tldr": "The paper proposes the Mean-Field Actor-Critic (MFAC) flow, a novel gradient-based methodology for solving mean-field games (MFGs), integrating reinforcement learning and optimal transport techniques, and demonstrates its convergence and effectiveness.", "motivation": "To address the challenge of solving mean-field games (MFGs), this paper aims to combine reinforcement learning and optimal transport approaches to develop a robust algorithmic framework for better computation of equilibria.", "method": "The proposed MFAC framework evolves the actor, critic, and distribution through coupled updates guided by partial differential equations (PDEs), and it uses the Optimal Transport Geodesic Picard (OTGP) flow to optimize the distribution along Wasserstein-2 geodesics.", "result": "The paper establishes a rigorous convergence analysis using Lyapunov functionals, achieving global exponential convergence of the MFAC flow, and supports the theoretical insights with relevant numerical experiments.", "conclusion": "The MFAC framework not only provides theoretical guarantees for its convergence and interplay among components but also proves practically effective for computing equilibria in mean-field games (MFGs)."}}
{"id": "2510.12238", "pdf": "https://arxiv.org/pdf/2510.12238", "abs": "https://arxiv.org/abs/2510.12238", "authors": ["Boyang Zhang", "Zhiguo Wang", "Ya-Feng Liu"], "title": "A Gradient Guided Diffusion Framework for Chance Constrained Programming", "categories": ["math.OC", "cs.LG"], "comment": null, "summary": "Chance constrained programming (CCP) is a powerful framework for addressing\noptimization problems under uncertainty. In this paper, we introduce a novel\nGradient-Guided Diffusion-based Optimization framework, termed GGDOpt, which\ntackles CCP through three key innovations. First, GGDOpt accommodates a broad\nclass of CCP problems without requiring the knowledge of the exact distribution\nof uncertainty-relying solely on a set of samples. Second, to address the\nnonconvexity of the chance constraints, it reformulates the CCP as a sampling\nproblem over the product of two distributions: an unknown data distribution\nsupported on a nonconvex set and a Boltzmann distribution defined by the\nobjective function, which fully leverages both first- and second-order gradient\ninformation. Third, GGDOpt has theoretical convergence guarantees and provides\npractical error bounds under mild assumptions. By progressively injecting noise\nduring the forward diffusion process to convexify the nonconvex feasible\nregion, GGDOpt enables guided reverse sampling to generate asymptotically\noptimal solutions. Experimental results on synthetic datasets and a waveform\ndesign task in wireless communications demonstrate that GGDOpt outperforms\nexisting methods in both solution quality and stability with nearly 80%\noverhead reduction.", "AI": {"tldr": "GGDOpt presents a novel framework for solving optimization problems under uncertainty using gradient-guided diffusion methods, yielding better solutions and stability.", "motivation": "To tackle optimization problems involving uncertainty and complex nonconvex constraints, ensuring robustness without requiring exact knowledge of distributions.", "method": "GGDOpt reformulates CCP using sampling techniques over combined distributions, utilizes forward diffusion processes for convexification, reverse sampling, and guarantees convergence and error bounds.", "result": "Experimental results show GGDOpt achieves higher solution quality, better stability, and reduces overhead by nearly 80% compared to existing methods.", "conclusion": "GGDOpt is a powerful and efficient solution to CCP problems under uncertainty, demonstrating theoretical and practical advantages in optimization applications."}}
{"id": "2510.12604", "pdf": "https://arxiv.org/pdf/2510.12604", "abs": "https://arxiv.org/abs/2510.12604", "authors": ["Qihang Zhao", "Zhongbo Sun", "Xiaoyang Zheng", "Xian Guo", "Siyuan Wang", "Zihan Liang", "Mingcan Peng", "Ben Chen", "Chenyi Lei"], "title": "SMILE: SeMantic Ids Enhanced CoLd Item Representation for Click-through Rate Prediction in E-commerce SEarch", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "With the rise of modern search and recommendation platforms, insufficient\ncollaborative information of cold-start items exacerbates the Matthew effect of\nexisting platform items, challenging platform diversity and becoming a\nlongstanding issue. Existing methods align items' side content with\ncollaborative information to transfer collaborative signals from\nhigh-popularity items to cold-start items. However, these methods fail to\naccount for the asymmetry between collaboration and content, nor the\nfine-grained differences among items. To address these issues, we propose\nSMILE, an item representation enhancement approach based on fused alignment of\nsemantic IDs. Specifically, we use RQ-OPQ encoding to quantize item content and\ncollaborative information, followed by a two-step alignment: RQ encoding\ntransfers shared collaborative signals across items, while OPQ encoding learns\ndifferentiated information of items. Comprehensive offline experiments on\nlarge-scale industrial datasets demonstrate superiority of SMILE, and rigorous\nonline A/B tests confirm statistically significant improvements: item CTR\n+1.66%, buyers +1.57%, and order volume +2.17%.", "AI": {"tldr": "This paper introduces SMILE to enhance item representation on search and recommendation platforms, addressing cold-start challenges and improving platform diversity.", "motivation": "Modern platforms struggle with the cold-start problem, where insufficient collaborative data for new items exacerbates biases and limits diversity.", "method": "The paper proposes SMILE, leveraging RQ-OPQ encoding to perform a two-step alignment for quantizing and enhancing item content and collaboration signals.", "result": "Experiments show superior performance, with significant improvements in critical metrics such as item CTR (+1.66%), buyers (+1.57%), and order volume (+2.17%).", "conclusion": "SMILE mitigates cold-start challenges, improves item representation, and enhances the diversity and effectiveness of recommendation platforms."}}
{"id": "2510.12271", "pdf": "https://arxiv.org/pdf/2510.12271", "abs": "https://arxiv.org/abs/2510.12271", "authors": ["Kutay B\u00f6lat", "Peter Palensky", "Simon Tindemans"], "title": "The Living Forecast: Evolving Day-Ahead Predictions into Intraday Reality", "categories": ["stat.AP", "cs.LG"], "comment": null, "summary": "Accurate intraday forecasts are essential for power system operations,\ncomplementing day-ahead forecasts that gradually lose relevance as new\ninformation becomes available. This paper introduces a Bayesian updating\nmechanism that converts fully probabilistic day-ahead forecasts into intraday\nforecasts without retraining or re-inference. The approach conditions the\nGaussian mixture output of a conditional variational autoencoder-based\nforecaster on observed measurements, yielding an updated distribution for the\nremaining horizon that preserves its probabilistic structure. This enables\nconsistent point, quantile, and ensemble forecasts while remaining\ncomputationally efficient and suitable for real-time applications. Experiments\non household electricity consumption and photovoltaic generation datasets\ndemonstrate that the proposed method improves forecast accuracy up to 25%\nacross likelihood-, sample-, quantile-, and point-based metrics. The largest\ngains occur in time steps with strong temporal correlation to observed data,\nand the use of pattern dictionary-based covariance structures further enhances\nperformance. The results highlight a theoretically grounded framework for\nintraday forecasting in modern power systems.", "AI": {"tldr": "The paper introduces a Bayesian updating mechanism for transforming day-ahead probabilistic forecasts into intraday forecasts, enhancing forecast accuracy for power system operations without requiring retraining.", "motivation": "To improve accuracy and efficiency of intraday forecasts for power systems as day-ahead forecasts lose relevance during real-time operations.", "method": "The method uses a Bayesian mechanism to update day-ahead Gaussian mixture probabilistic forecasts into intraday versions, leveraging conditional variational autoencoder-based forecasters with measured observations.", "result": "Improved forecast accuracy by up to 25% across various metrics, particularly at time steps with strong temporal correlations, using household electricity consumption and photovoltaic generation datasets.", "conclusion": "The framework provides a computationally efficient, theoretically grounded solution for intraday forecasting that is suitable for real-time applications in power systems."}}
{"id": "2510.12272", "pdf": "https://arxiv.org/pdf/2510.12272", "abs": "https://arxiv.org/abs/2510.12272", "authors": ["Federico Gabriele", "Aldo Glielmo", "Marco Taboga"], "title": "Heterogeneous RBCs via deep multi-agent reinforcement learning", "categories": ["cs.MA", "cs.LG", "econ.TH"], "comment": "13 pages, 9 figures", "summary": "Current macroeconomic models with agent heterogeneity can be broadly divided\ninto two main groups. Heterogeneous-agent general equilibrium (GE) models, such\nas those based on Heterogeneous Agents New Keynesian (HANK) or Krusell-Smith\n(KS) approaches, rely on GE and 'rational expectations', somewhat unrealistic\nassumptions that make the models very computationally cumbersome, which in turn\nlimits the amount of heterogeneity that can be modelled. In contrast,\nagent-based models (ABMs) can flexibly encompass a large number of arbitrarily\nheterogeneous agents, but typically require the specification of explicit\nbehavioural rules, which can lead to a lengthy trial-and-error\nmodel-development process. To address these limitations, we introduce MARL-BC,\na framework that integrates deep multi-agent reinforcement learning (MARL) with\nReal Business Cycle (RBC) models. We demonstrate that MARL-BC can: (1) recover\ntextbook RBC results when using a single agent; (2) recover the results of the\nmean-field KS model using a large number of identical agents; and (3)\neffectively simulate rich heterogeneity among agents, a hard task for\ntraditional GE approaches. Our framework can be thought of as an ABM if used\nwith a variety of heterogeneous interacting agents, and can reproduce GE\nresults in limit cases. As such, it is a step towards a synthesis of these\noften opposed modelling paradigms.", "AI": {"tldr": "The paper introduces MARL-BC, integrating multi-agent reinforcement learning with RBC models to address limitations of heterogeneous agent macroeconomic models.", "motivation": "Existing models like HANK and KS face computational difficulties and constrained heterogeneity, while ABMs require extensive development of behavioral rules; there is a need for an integrated solution.", "method": "The authors propose combining deep multi-agent reinforcement learning (MARL) with Real Business Cycle (RBC) models to enhance agent-based modeling and general equilibrium analysis.", "result": "MARL-BC successfully replicates RBC results, mean-field KS model findings, and accommodates diverse heterogeneity among agents.", "conclusion": "MARL-BC bridges the divide between agent-based models and general equilibrium models, offering a synthesized framework that tackles their respective limitations effectively."}}
{"id": "2510.12310", "pdf": "https://arxiv.org/pdf/2510.12310", "abs": "https://arxiv.org/abs/2510.12310", "authors": ["Daniel Pulido-Cort\u00e1zar", "Daniel Gibert", "Felip Many\u00e0"], "title": "DeepTrust: Multi-Step Classification through Dissimilar Adversarial Representations for Robust Android Malware Detection", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Over the last decade, machine learning has been extensively applied to\nidentify malicious Android applications. However, such approaches remain\nvulnerable against adversarial examples, i.e., examples that are subtly\nmanipulated to fool a machine learning model into making incorrect predictions.\nThis research presents DeepTrust, a novel metaheuristic that arranges flexible\nclassifiers, like deep neural networks, into an ordered sequence where the\nfinal decision is made by a single internal model based on conditions activated\nin cascade. In the Robust Android Malware Detection competition at the 2025\nIEEE Conference SaTML, DeepTrust secured the first place and achieved\nstate-of-the-art results, outperforming the next-best competitor by up to 266%\nunder feature-space evasion attacks. This is accomplished while maintaining the\nhighest detection rate on non-adversarial malware and a false positive rate\nbelow 1%. The method's efficacy stems from maximizing the divergence of the\nlearned representations among the internal models. By using classifiers\ninducing fundamentally dissimilar embeddings of the data, the decision space\nbecomes unpredictable for an attacker. This frustrates the iterative\nperturbation process inherent to evasion attacks, enhancing system robustness\nwithout compromising accuracy on clean examples.", "AI": {"tldr": "The paper introduces DeepTrust, a robust metaheuristic for detecting malware on Android, which secures state-of-the-art performance under adversarial attacks.", "motivation": "To address the vulnerability of machine learning-based Android malware detection methods against adversarial examples.", "method": "Proposes DeepTrust, a system organizing classifiers (e.g., deep neural networks) into a sequence with a final decision made by an internal model based on cascading conditions, ensuring robustness against attacks by maximizing divergence in representations among the classifiers.", "result": "DeepTrust achieved first place in the 2025 Robust Android Malware Detection competition, outperforming competitors by up to 266% under feature-space evasion attacks, with a high malware detection rate, low false positives (below 1%), and strong accuracy on clean data.", "conclusion": "DeepTrust provides a significant leap in adversarial robustness for Android malware detection by leveraging diverse representations and creating unpredictability in decision boundaries, all while maintaining strong overall accuracy."}}
{"id": "2510.12642", "pdf": "https://arxiv.org/pdf/2510.12642", "abs": "https://arxiv.org/abs/2510.12642", "authors": ["Meihui Zhang", "Liming Wang", "Chi Zhang", "Zhaojing Luo"], "title": "Aixel: A Unified, Adaptive and Extensible System for AI-powered Data Analysis", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "A growing trend in modern data analysis is the integration of data management\nwith learning, guided by accuracy, latency, and cost requirements. In practice,\napplications draw data of different formats from many sources. In the\nmeanwhile, the objectives and budgets change over time. Existing systems handle\nthese applications across databases, analysis libraries, and tuning services.\nSuch fragmentation leads to complex user interaction, limited adaptability,\nsuboptimal performance, and poor extensibility across components. To address\nthese challenges, we present Aixel, a unified, adaptive, and extensible system\nfor AI-powered data analysis. The system organizes work across four layers:\napplication, task, model, and data. The task layer provides a declarative\ninterface to capture user intent, which is parsed into an executable operator\nplan. An optimizer compiles and schedules this plan to meet specified goals in\naccuracy, latency, and cost. The task layer coordinates the execution of data\nand model operators, with built-in support for reuse and caching to improve\nefficiency. The model layer offers versioned storage for index, metadata,\ntensors, and model artifacts. It supports adaptive construction, task-aligned\ndrift detection, and safe updates that reuse shared components. The data layer\nprovides unified data management capabilities, including indexing,\nconstraint-aware discovery, task-aligned selection, and comprehensive feature\nmanagement. With the above designed layers, Aixel delivers a user friendly,\nadaptive, efficient, and extensible system.", "AI": {"tldr": "Aixel is a unified system that integrates data management with learning to address challenges in modern data analysis, ensuring adaptability, efficiency, and extensibility across components.", "motivation": "The motivation is to solve challenges caused by fragmented systems in modern data analysis that hinder adaptability, performance, user-friendliness, and component extensibility.", "method": "Aixel organizes work across four layers (application, task, model, data), offering declarative interfaces, optimized execution plans, reuse mechanisms, adaptive model management, and unified data capabilities.", "result": "Aixel provides a streamlined, user-friendly system that ensures adaptive, efficient handling of AI-powered data analysis tasks through its layered design.", "conclusion": "Aixel successfully addresses fragmentation issues in data analysis with a unified framework that promotes adaptability, optimization, and extensibility."}}
{"id": "2510.12368", "pdf": "https://arxiv.org/pdf/2510.12368", "abs": "https://arxiv.org/abs/2510.12368", "authors": ["Stefano Riva", "Carolina Introini", "Jos\u00e8 Nathan Kutz", "Antonio Cammi"], "title": "Constrained Sensing and Reliable State Estimation with Shallow Recurrent Decoders on a TRIGA Mark II Reactor", "categories": ["cs.CE", "cs.LG"], "comment": null, "summary": "Shallow Recurrent Decoder networks are a novel data-driven methodology able\nto provide accurate state estimation in engineering systems, such as nuclear\nreactors. This deep learning architecture is a robust technique designed to map\nthe temporal trajectories of a few sparse measures to the full state space,\nincluding unobservable fields, which is agnostic to sensor positions and able\nto handle noisy data through an ensemble strategy, leveraging the short\ntraining times and without the need for hyperparameter tuning. Following its\napplication to a novel reactor concept, this work investigates the performance\nof Shallow Recurrent Decoders when applied to a real system. The underlying\nmodel is represented by a fluid dynamics model of the TRIGA Mark II research\nreactor; the architecture will use both synthetic temperature data coming from\nthe numerical model and leveraging experimental temperature data recorded\nduring a previous campaign. The objective of this work is, therefore, two-fold:\n1) assessing if the architecture can reconstruct the full state of the system\n(temperature, velocity, pressure, turbulence quantities) given sparse data\nlocated in specific, low-dynamics channels and 2) assessing the correction\ncapabilities of the architecture (that is, given a discrepancy between model\nand data, assessing if sparse measurements can provide some correction to the\narchitecture output). As will be shown, the accurate reconstruction of every\ncharacteristic field, using both synthetic and experimental data, in real-time\nmakes this approach suitable for interpretable monitoring and control purposes\nin the framework of a reactor digital twin.", "AI": {"tldr": "The paper introduces Shallow Recurrent Decoder networks, a robust deep learning method for reconstructing full state estimation in engineering systems, applied specifically to the TRIGA Mark II nuclear reactor, using sparse and noisy data.", "motivation": "To develop a data-driven methodology for accurate state estimation in complex systems like nuclear reactors, overcoming challenges such as sparse measurements, noisy data, and discrepancies between model and real-world data.", "method": "Shallow Recurrent Decoder networks are implemented to map sparse sensor data to full state estimations using both synthetic and experimental data, leveraging ensemble strategies and short training times without hyperparameter tuning.", "result": "The architecture successfully reconstructed multiple state fields (temperature, velocity, pressure, turbulence quantities) and demonstrated correction capabilities, validating its performance with both synthetic and experimental data.", "conclusion": "The proposed technique proved effective for real-time accurate state reconstruction and correction, supporting its use in reactor digital twins for monitoring and control applications."}}
{"id": "2510.12689", "pdf": "https://arxiv.org/pdf/2510.12689", "abs": "https://arxiv.org/abs/2510.12689", "authors": ["Suyash Fulay", "Jocelyn Zhu", "Michiel Bakker"], "title": "From Delegates to Trustees: How Optimizing for Long-Term Interests Shapes Bias and Alignment in LLM", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have shown promising accuracy in predicting\nsurvey responses and policy preferences, which has increased interest in their\npotential to represent human interests in various domains. Most existing\nresearch has focused on behavioral cloning, effectively evaluating how well\nmodels reproduce individuals' expressed preferences. Drawing on theories of\npolitical representation, we highlight an underexplored design trade-off:\nwhether AI systems should act as delegates, mirroring expressed preferences, or\nas trustees, exercising judgment about what best serves an individual's\ninterests. This trade-off is closely related to issues of LLM sycophancy, where\nmodels can encourage behavior or validate beliefs that may be aligned with a\nuser's short-term preferences, but is detrimental to their long-term interests.\nThrough a series of experiments simulating votes on various policy issues in\nthe U.S. context, we apply a temporal utility framework that weighs short and\nlong-term interests (simulating a trustee role) and compare voting outcomes to\nbehavior-cloning models (simulating a delegate). We find that trustee-style\npredictions weighted toward long-term interests produce policy decisions that\nalign more closely with expert consensus on well-understood issues, but also\nshow greater bias toward models' default stances on topics lacking clear\nagreement. These findings reveal a fundamental trade-off in designing AI\nsystems to represent human interests. Delegate models better preserve user\nautonomy but may diverge from well-supported policy positions, while trustee\nmodels can promote welfare on well-understood issues yet risk paternalism and\nbias on subjective topics.", "AI": {"tldr": "The paper explores the trade-offs between designing AI systems as delegates, reflecting user preferences, versus trustees, exercising judgment for long-term welfare, in the context of responding to policy preferences.", "motivation": "To investigate whether AI models should prioritize mirroring user preferences or promoting long-term welfare in human representation, given their influence in domains like policy decision-making.", "method": "Conducting experiments using a temporal utility framework to simulate votes on U.S. policy issues, comparing delegate-style (behavior cloning) and trustee-style (long-term interest weighing) AI models.", "result": "Trustee models aligned better with expert consensus on clear policy issues but displayed biases on ambiguous topics. Delegate models preserved user autonomy but could deviate from well-supported policy positions.", "conclusion": "Designing AI systems requires balancing user autonomy with welfare promotion; delegate models uphold expressed preferences while trustee models may better serve long-term interests at the risk of paternalism and bias."}}
{"id": "2510.12430", "pdf": "https://arxiv.org/pdf/2510.12430", "abs": "https://arxiv.org/abs/2510.12430", "authors": ["Bodo Rosenhahn", "Tobias J. Osborne", "Christoph Hirche"], "title": "Neural Guided Sampling for Quantum Circuit Optimization", "categories": ["quant-ph", "cs.LG"], "comment": "12 pages, 9 Figures", "summary": "Translating a general quantum circuit on a specific hardware topology with a\nreduced set of available gates, also known as transpilation, comes with a\nsubstantial increase in the length of the equivalent circuit. Due to\ndecoherence, the quality of the computational outcome can degrade seriously\nwith increasing circuit length. Thus, there is major interest to reduce a\nquantum circuit to an equivalent circuit which is in its gate count as short as\npossible. One method to address efficient transpilation is based on approaches\nknown from stochastic optimization, e.g. by using random sampling and token\nreplacement strategies. Here, a core challenge is that these methods can suffer\nfrom sampling efficiency, causing long and energy consuming optimization time.\nAs a remedy, we propose in this work 2D neural guided sampling. Thus, given a\n2D representation of a quantum circuit, a neural network predicts groups of\ngates in the quantum circuit, which are likely reducible. Thus, it leads to a\nsampling prior which can heavily reduce the compute time for quantum circuit\nreduction. In several experiments, we demonstrate that our method is superior\nto results obtained from different qiskit or BQSKit optimization levels.", "AI": {"tldr": "The paper introduces a 2D neural-guided sampling method for reducing quantum circuits, improving upon traditional optimization techniques like those in Qiskit and BQSKit.", "motivation": "To address the inefficiency in quantum circuit optimization processes, where longer circuits lead to reduced computational accuracy due to hardware constraints and decoherence.", "method": "The authors propose a neural network that leverages a 2D representation of quantum circuits to predict and reduce redundant gate groups, improving sampling efficiency during transpilation.", "result": "The proposed method demonstrated superior performance and reduced compute times when compared to traditional optimization techniques like Qiskit and BQSKit.", "conclusion": "The 2D neural-guided sampling method offers a practical solution for efficient quantum circuit reduction, paving the way for improved functionality in quantum computing hardware."}}
{"id": "2510.12440", "pdf": "https://arxiv.org/pdf/2510.12440", "abs": "https://arxiv.org/abs/2510.12440", "authors": ["Ayush Chaudhary"], "title": "Formal Models and Convergence Analysis for Context-Aware Security Verification", "categories": ["cs.CR", "cs.LG", "68Q87, 94A17, 68T05", "D.2.4; I.2.6; F.3.1; K.6.5"], "comment": "11 pages, 4 figures, 4 tables. Presents formal framework for\n  context-aware security verification with ML-enhanced adaptive systems.\n  Includes theoretical bounds (sample complexity, information-theoretic limits,\n  convergence guarantees, soundness preservation) and empirical validation on\n  97,224 exploit samples", "summary": "We present a formal framework for context-aware security verification that\nestablishes provable guarantees for ML-enhanced adaptive systems. We introduce\ncontext-completeness - a new security property - and prove: (1) sample\ncomplexity bounds showing when adaptive verification succeeds, (2)\ninformation-theoretic limits relating context richness to detection capability,\n(3) convergence guarantees for ML-based payload generators, and (4)\ncompositional soundness bounds. We further provide a formal separation between\nstatic context-blind verifiers and context-aware adaptive verifiers: for a\nnatural family of targets, any static verifier with finite payload budget\nachieves completeness at most alpha, while a context-aware verifier with\nsufficient information achieves completeness greater than alpha. We validate\nour theoretical predictions through controlled experiments on 97,224 exploit\nsamples, demonstrating: detection accuracy improving from 58% to 69.93% with\ndataset growth, success probability increasing from 51% to 82% with context\nenrichment, training loss converging at O(1/sqrt(T)) rate, and false positive\nrate (10.19%) within theoretical bounds (12%). Our results show that\ntheoretically-grounded adaptive verification achieves provable improvements\nover static approaches under stated assumptions while maintaining soundness\nguarantees.", "AI": {"tldr": "This paper introduces a formal framework for context-aware security verification in adaptive systems, outlining new theoretical properties and validating improvements over static models through experiments.", "motivation": "Traditional security verifiers may fail to adapt to context, limiting their effectiveness in detecting threats in ML-enhanced adaptive systems.", "method": "The authors propose a security property called context-completeness, derive theoretical proofs for verification, and validate through experiments using exploit samples to demonstrate improvements.", "result": "Key findings include increases in detection accuracy, success rates with enriched context, training loss convergence, and manageable false positive rates, all validating their theoretical predictions.", "conclusion": "The research demonstrates that context-aware adaptive verification significantly outperforms static verification methods in improving security guarantees, under the given assumptions."}}
{"id": "2510.12521", "pdf": "https://arxiv.org/pdf/2510.12521", "abs": "https://arxiv.org/abs/2510.12521", "authors": ["Sebastian Banert", "Christoph Brauer", "Dirk Lorenz", "Lionel Tondji"], "title": "Why the noise model matters: A performance gap in learned regularization", "categories": ["math.NA", "cs.LG", "cs.NA", "65J20, 68T05"], "comment": null, "summary": "This article addresses the challenge of learning effective regularizers for\nlinear inverse problems. We analyze and compare several types of learned\nvariational regularization against the theoretical benchmark of the optimal\naffine reconstruction, i.e. the best possible affine linear map for minimizing\nthe mean squared error. It is known that this optimal reconstruction can be\nachieved using Tikhonov regularization, but this requires precise knowledge of\nthe noise covariance to properly weight the data fidelity term. However, in\nmany practical applications, noise statistics are unknown. We therefore\ninvestigate the performance of regularization methods learned without access to\nthis noise information, focusing on Tikhonov, Lavrentiev, and quadratic\nregularization. Our theoretical analysis and numerical experiments demonstrate\nthat for non-white noise, a performance gap emerges between these methods and\nthe optimal affine reconstruction. Furthermore, we show that these different\ntypes of regularization yield distinct results, highlighting that the choice of\nregularizer structure is critical when the noise model is not explicitly\nlearned. Our findings underscore the significant value of accurately modeling\nor co-learning noise statistics in data-driven regularization.", "AI": {"tldr": "This paper investigates learning regularizers for linear inverse problems, emphasizing the role of noise statistics in achieving optimal reconstruction.", "motivation": "The study aims to address the challenge of optimizing regularization methods in scenarios where precise noise statistics are unavailable.", "method": "The authors theoretically analyze regularization methods such as Tikhonov, Lavrentiev, and quadratic regularization, comparing them against the optimal affine reconstruction, and conduct numerical experiments.", "result": "They find that a performance gap exists between standard regularization methods and optimal reconstruction under non-white noise conditions. Different regularization types yield distinct outcomes.", "conclusion": "The choice of regularizer structure is essential when noise models are not explicitly learned, and accurately modeling or co-learning noise statistics is highly valuable."}}
{"id": "2510.12714", "pdf": "https://arxiv.org/pdf/2510.12714", "abs": "https://arxiv.org/abs/2510.12714", "authors": ["Alejandro Lopez-Montes", "Fereshteh Yousefirizi", "Yizhou Chen", "Yazdan Salimi", "Robert Seifert", "Ali Afshar-Oromieh", "Carlos Uribe", "Axel Rominger", "Habib Zaidi", "Arman Rahmim", "Kuangyu Shi"], "title": "Artificial intelligence for simplified patient-centered dosimetry in radiopharmaceutical therapies", "categories": ["physics.med-ph", "cs.AI", "physics.app-ph"], "comment": null, "summary": "KEY WORDS: Artificial Intelligence (AI), Theranostics, Dosimetry,\nRadiopharmaceutical Therapy (RPT), Patient-friendly dosimetry KEY POINTS - The\nrapid evolution of radiopharmaceutical therapy (RPT) highlights the growing\nneed for personalized and patient-centered dosimetry. - Artificial Intelligence\n(AI) offers solutions to the key limitations in current dosimetry calculations.\n- The main advances on AI for simplified dosimetry toward patient-friendly RPT\nare reviewed. - Future directions on the role of AI in RPT dosimetry are\ndiscussed.", "AI": {"tldr": "The abstract highlights the use of Artificial Intelligence (AI) to enhance dosimetry calculations in radiopharmaceutical therapy (RPT), aiming for personalized approaches.", "motivation": "To address the challenges in current dosimetry calculations and improve patient-centered care in RPT by leveraging AI.", "method": "AI-based advancements are reviewed and discussed in the context of simplifying and optimizing dosimetry processes for RPT.", "result": "AI has the potential to overcome limitations in dosimetry calculations and offers pathways toward simplified, patient-friendly solutions in RPT.", "conclusion": "AI could play a crucial role in advancing personalized dosimetry in RPT, with future opportunities for optimizing therapy outcomes."}}
{"id": "2510.12617", "pdf": "https://arxiv.org/pdf/2510.12617", "abs": "https://arxiv.org/abs/2510.12617", "authors": ["Davide Greco", "Konrad Rawlik"], "title": "Same model, better performance: the impact of shuffling on DNA Language Models benchmarking", "categories": ["q-bio.GN", "cs.LG"], "comment": null, "summary": "Large Language Models are increasingly popular in genomics due to their\npotential to decode complex biological sequences. Hence, researchers require a\nstandardized benchmark to evaluate DNA Language Models (DNA LMs) capabilities.\nHowever, evaluating DNA LMs is a complex task that intersects genomic's\ndomain-specific challenges and machine learning methodologies, where seemingly\nminor implementation details can significantly compromise benchmark validity.\nWe demonstrate this through BEND (Benchmarking DNA Language Models), where\nhardware-dependent hyperparameters -- number of data loading workers and buffer\nsizes -- create spurious performance variations of up to 4% for identical\nmodels. The problem stems from inadequate data shuffling interacting with\ndomain specific data characteristics. Experiments with three DNA language\nmodels (HyenaDNA, DNABERT-2, ResNet-LM) show these artifacts affect both\nabsolute performance and relative model rankings. We propose a simple solution:\npre-shuffling data before storage eliminates hardware dependencies while\nmaintaining efficiency. This work highlights how standard ML practices can\ninteract unexpectedly with domain-specific data characteristics, with broader\nimplications for benchmark design in specialized domains.", "AI": {"tldr": "This paper discusses the creation of BEND, a benchmark for DNA Language Models. It highlights performance variations caused by hardware-dependent data shuffling and provides a solution by pre-shuffling data.", "motivation": "To address the lack of standardized benchmarks for evaluating DNA Language Models and to study how standard ML practices interact with domain-specific characteristics in genomics.", "method": "Experiments with three DNA LMs (HyenaDNA, DNABERT-2, ResNet-LM) to understand the impact of hardware-dependent hyperparameters and testing a pre-shuffling solution.", "result": "Identified 4% performance variations caused by data loading workers and buffer sizes due to inadequate shuffling. These artifacts impacted absolute performance and model rankings.", "conclusion": "Pre-shuffling data before storage resolves hardware dependencies and ensures benchmark validity, emphasizing the need for careful benchmark design in specialized domains."}}
{"id": "2510.12763", "pdf": "https://arxiv.org/pdf/2510.12763", "abs": "https://arxiv.org/abs/2510.12763", "authors": ["Saurabh Sihag", "Gonzalo Mateos", "Alejandro Ribeiro"], "title": "Disentangling Neurodegeneration with Brain Age Gap Prediction Models: A Graph Signal Processing Perspective", "categories": ["eess.SP", "cs.AI", "q-bio.QM"], "comment": "Accepted for publication in IEEE Signal Processing Magazine", "summary": "Neurodegeneration, characterized by the progressive loss of neuronal\nstructure or function, is commonly assessed in clinical practice through\nreductions in cortical thickness or brain volume, as visualized by structural\nMRI. While informative, these conventional approaches lack the statistical\nsophistication required to fully capture the spatially correlated and\nheterogeneous nature of neurodegeneration, which manifests both in healthy\naging and in neurological disorders. To address these limitations, brain age\ngap has emerged as a promising data-driven biomarker of brain health. The brain\nage gap prediction (BAGP) models estimate the difference between a person's\npredicted brain age from neuroimaging data and their chronological age. The\nresulting brain age gap serves as a compact biomarker of brain health, with\nrecent studies demonstrating its predictive utility for disease progression and\nseverity. However, practical adoption of BAGP models is hindered by their\nmethodological obscurities and limited generalizability across diverse clinical\npopulations. This tutorial article provides an overview of BAGP and introduces\na principled framework for this application based on recent advancements in\ngraph signal processing (GSP). In particular, we focus on graph neural networks\n(GNNs) and introduce the coVariance neural network (VNN), which leverages the\nanatomical covariance matrices derived from structural MRI. VNNs offer strong\ntheoretical grounding and operational interpretability, enabling robust\nestimation of brain age gap predictions. By integrating perspectives from GSP,\nmachine learning, and network neuroscience, this work clarifies the path\nforward for reliable and interpretable BAGP models and outlines future research\ndirections in personalized medicine.", "AI": {"tldr": "This paper addresses limitations in traditional neurodegeneration assessment methods by introducing a graph-based framework, focusing on improving brain age gap prediction (BAGP) using graph neural networks, specifically the coVariance neural network (VNN).", "motivation": "Conventional methods for assessing neurodegeneration (e.g., measuring cortical thickness or brain volume) lack the statistical sophistication to fully account for spatially correlated and heterogeneous brain changes.", "method": "The authors propose using graph signal processing (GSP) and graph neural networks (GNNs), particularly the coVariance neural network (VNN), which uses anatomical covariance matrices derived from MRI data to improve BAGP models.", "result": "The VNN approach provides robust brain age gap predictions with theoretical grounding and operational interpretability, improving the generalizability of BAGP models.", "conclusion": "This study offers a clearer pathway for reliable and interpretable brain age gap prediction methods, paving the way for advancements in personalized medicine."}}
{"id": "2510.12728", "pdf": "https://arxiv.org/pdf/2510.12728", "abs": "https://arxiv.org/abs/2510.12728", "authors": ["Minjae Lee", "Minsuk Kahng"], "title": "Data-Model Co-Evolution: Growing Test Sets to Refine LLM Behavior", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "A long-standing challenge in machine learning has been the rigid separation\nbetween data work and model refinement, enforced by slow fine-tuning cycles.\nThe rise of Large Language Models (LLMs) overcomes this historical barrier,\nallowing applications developers to instantly govern model behavior by editing\nprompt instructions. This shift enables a new paradigm: data-model\nco-evolution, where a living test set and a model's instructions evolve in\ntandem. We operationalize this paradigm in an interactive system designed to\naddress the critical challenge of encoding subtle, domain-specific policies\ninto prompt instructions. The system's structured workflow guides people to\ndiscover edge cases, articulate rationales for desired behavior, and\niteratively evaluate instruction revisions against a growing test set. A user\nstudy shows our workflow helps participants refine instructions systematically\nand specify ambiguous policies more concretely. This work points toward more\nrobust and responsible LLM applications through human-in-the-loop development\naligned with local preferences and policies.", "AI": {"tldr": "The authors propose a system allowing users to iteratively refine Large Language Model (LLM) behavior by editing prompt instructions, aligning it with domain-specific policies through a human-in-the-loop approach.", "motivation": "Machine learning has faced historically slow fine-tuning cycles and a rigid separation between data and model refinement. LLMs bridge this gap and enable dynamic interaction between data and model instructions.", "method": "An interactive workflow helps users identify edge cases, clarify policies, and refine instructions by iteratively testing them against an evolving test set.", "result": "A user study demonstrates that participants can refine instructions systematically and specify ambiguous policies more clearly, proving the system's effectiveness.", "conclusion": "The work introduces a paradigm of data-model co-evolution, enhancing robust and responsible LLM applications through iterative human-guided development."}}
{"id": "2510.12778", "pdf": "https://arxiv.org/pdf/2510.12778", "abs": "https://arxiv.org/abs/2510.12778", "authors": ["Ugur Akpinar", "Erdem Sahin", "Tina M. Hayward", "Apratim Majumder", "Rajesh Menon", "Atanas Gotchev"], "title": "Wavefront Coding for Accommodation-Invariant Near-Eye Displays", "categories": ["physics.optics", "cs.AR", "cs.LG"], "comment": null, "summary": "We present a new computational near-eye display method that addresses the\nvergence-accommodation conflict problem in stereoscopic displays through\naccommodation-invariance. Our system integrates a refractive lens eyepiece with\na novel wavefront coding diffractive optical element, operating in tandem with\na pre-processing convolutional neural network. We employ end-to-end learning to\njointly optimize the wavefront-coding optics and the image pre-processing\nmodule. To implement this approach, we develop a differentiable retinal image\nformation model that accounts for limiting aperture and chromatic aberrations\nintroduced by the eye optics. We further integrate the neural transfer function\nand the contrast sensitivity function into the loss model to account for\nrelated perceptual effects. To tackle off-axis distortions, we incorporate\nposition dependency into the pre-processing module. In addition to conducting\nrigorous analysis based on simulations, we also fabricate the designed\ndiffractive optical element and build a benchtop setup, demonstrating\naccommodation-invariance for depth ranges of up to four diopters.", "AI": {"tldr": "The paper introduces a near-eye display method that resolves the vergence-accommodation conflict using wavefront coding optics and neural networks, overcoming perceptual challenges and demonstrating efficacy up to four diopters.", "motivation": "The study seeks to address the vergence-accommodation conflict problem in stereoscopic displays, which is a key challenge in achieving realistic depth perception.", "method": "It employs a combination of refractive lens eyepieces, wavefront coding diffractive optical elements, and a convolutional neural network pre-processing module optimized through end-to-end learning with a differentiable retinal image formation model.", "result": "The method demonstrated accommodation-invariance for depth ranges up to four diopters, validated through simulations and a benchtop setup.", "conclusion": "The approach successfully mitigates the vergence-accommodation conflict and offers practical depth perception enhancements in near-eye displays."}}
