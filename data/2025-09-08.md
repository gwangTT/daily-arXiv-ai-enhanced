<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 18]
- [cs.CL](#cs.CL) [Total: 93]
- [cs.CV](#cs.CV) [Total: 54]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.LG](#cs.LG) [Total: 51]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 18]
- [cs.SE](#cs.SE) [Total: 8]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [stat.ML](#stat.ML) [Total: 4]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.SI](#cs.SI) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.DS](#cs.DS) [Total: 2]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [cs.CR](#cs.CR) [Total: 2]
- [econ.GN](#econ.GN) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.GR](#cs.GR) [Total: 2]
- [eess.SY](#eess.SY) [Total: 2]
- [stat.AP](#stat.AP) [Total: 1]
- [eess.IV](#eess.IV) [Total: 4]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [quant-ph](#quant-ph) [Total: 6]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [math.NA](#math.NA) [Total: 2]
- [cs.SD](#cs.SD) [Total: 5]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Ethical Compass of the Machine: Evaluating Large Language Models for Decision Support in Construction Project Management](https://arxiv.org/abs/2509.04505)
*Somtochukwu Azie,Yiping Meng*

Main category: cs.AI

TL;DR: This paper evaluates the ability of LLMs to aid in ethically sensitive decisions in construction project management and introduces a framework for assessment.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the growing integration of AI, particularly Large Language Models (LLMs), in decision-making scenarios in construction project management, which often involve ethical and high-risk considerations.

Method: A mixed-methods approach was used, including quantitative performance testing of LLMs via the Ethical Decision Support Assessment Checklist (EDSAC) across ethical scenarios and qualitative interviews with industry experts.

Result: The study found that LLMs perform well in structured, rule-based domains but struggle with nuanced ethical contexts, accountability, and transparent reasoning. Experts prefer human oversight in ethical decisions.

Conclusion: LLMs should act as support tools rather than autonomous agents in ethical decision-making due to current limitations. The paper also introduces the EDSAC framework for assessing AI ethical reasoning.

Abstract: The integration of Artificial Intelligence (AI) into construction project
management (CPM) is accelerating, with Large Language Models (LLMs) emerging as
accessible decision-support tools. This study aims to critically evaluate the
ethical viability and reliability of LLMs when applied to the ethically
sensitive, high-risk decision-making contexts inherent in CPM. A mixed-methods
research design was employed, involving the quantitative performance testing of
two leading LLMs against twelve real-world ethical scenarios using a novel
Ethical Decision Support Assessment Checklist (EDSAC), and qualitative analysis
of semi-structured interviews with 12 industry experts to capture professional
perceptions. The findings reveal that while LLMs demonstrate adequate
performance in structured domains such as legal compliance, they exhibit
significant deficiencies in handling contextual nuance, ensuring
accountability, and providing transparent reasoning. Stakeholders expressed
considerable reservations regarding the autonomous use of AI for ethical
judgments, strongly advocating for robust human-in-the-loop oversight. To our
knowledge, this is one of the first studies to empirically test the ethical
reasoning of LLMs within the construction domain. It introduces the EDSAC
framework as a replicable methodology and provides actionable recommendations,
emphasising that LLMs are currently best positioned as decision-support aids
rather than autonomous ethical agents.

</details>


### [2] [Maestro: Joint Graph & Config Optimization for Reliable AI Agents](https://arxiv.org/abs/2509.04642)
*Wenxiao Wang,Priyatham Kattakinda,Soheil Feizi*

Main category: cs.AI

TL;DR: Maestro is an optimization framework for improving LLM agents by jointly optimizing their graph structure and node configurations. It outperforms other methods on several benchmarks while being more efficient.


<details>
  <summary>Details</summary>
Motivation: Existing methods primarily tune configurations, ignoring structural issues in LLM agents. This limitation leads to unaddressed failure modes, creating the need for a holistic optimization approach.

Method: Maestro performs joint optimization over the graph structure and node configurations of LLM agents, using rollout/token budgets and reflective textual feedback to efficiently prioritize edits.

Result: Maestro surpasses top benchmark methods like MIPROv2 and GEPA with significant improvements (e.g., 12% for IFBench) while using fewer rollouts. It demonstrates similar success in application scenarios, e.g., interviewer and RAG agents.

Conclusion: Joint optimization of graphs and configurations resolves structural failure modes that simple prompt tuning cannot. Maestro sets a new benchmark for LLM agent quality and efficiency.

Abstract: Building reliable LLM agents requires decisions at two levels: the graph
(which modules exist and how information flows) and the configuration of each
node (models, prompts, tools, control knobs). Most existing optimizers tune
configurations while holding the graph fixed, leaving structural failure modes
unaddressed. We introduce Maestro, a framework-agnostic holistic optimizer for
LLM agents that jointly searches over graphs and configurations to maximize
agent quality, subject to explicit rollout/token budgets. Beyond numeric
metrics, Maestro leverages reflective textual feedback from traces to
prioritize edits, improving sample efficiency and targeting specific failure
modes. On the IFBench and HotpotQA benchmarks, Maestro consistently surpasses
leading prompt optimizers--MIPROv2, GEPA, and GEPA+Merge--by an average of 12%,
4.9%, and 4.86%, respectively; even when restricted to prompt-only
optimization, it still leads by 9.65%, 2.37%, and 2.41%. Maestro achieves these
results with far fewer rollouts than GEPA. We further show large gains on two
applications (interviewer & RAG agents), highlighting that joint graph &
configuration search addresses structural failure modes that prompt tuning
alone cannot fix.

</details>


### [3] [Towards Personalized Explanations for Health Simulations: A Mixed-Methods Framework for Stakeholder-Centric Summarization](https://arxiv.org/abs/2509.04646)
*Philippe J. Giabbanelli,Ameeta Agrawal*

Main category: cs.AI

TL;DR: A framework is introduced to adapt Large Language Models (LLMs) for tailoring explanations and summaries of simulation outputs to meet diverse stakeholder needs in health decision-making. 


<details>
  <summary>Details</summary>
Motivation: Agent-based models are valuable for health interventions but are often complex and inaccessible to stakeholders like policymakers and clinicians. Current text summaries generated by LLMs fail to address diverse informational needs.

Method: The authors propose a mixed-methods framework that identifies stakeholder needs, tunes LLM attributes for tailored outputs, and evaluates comprehensively to optimize explanations of simulation models.

Result: The framework systematically elicits user requirements, modifies LLMs for personalized summaries, and enables better communication of health simulations to diverse audiences.

Conclusion: Tailored explanations of health simulations empower stakeholders, improving decision-making in healthcare and addressing accessibility challenges with complex modeling systems.

Abstract: Modeling & Simulation (M&S) approaches such as agent-based models hold
significant potential to support decision-making activities in health, with
recent examples including the adoption of vaccines, and a vast literature on
healthy eating behaviors and physical activity behaviors. These models are
potentially usable by different stakeholder groups, as they support
policy-makers to estimate the consequences of potential interventions and they
can guide individuals in making healthy choices in complex environments.
However, this potential may not be fully realized because of the models'
complexity, which makes them inaccessible to the stakeholders who could benefit
the most. While Large Language Models (LLMs) can translate simulation outputs
and the design of models into text, current approaches typically rely on
one-size-fits-all summaries that fail to reflect the varied informational needs
and stylistic preferences of clinicians, policymakers, patients, caregivers,
and health advocates. This limitation stems from a fundamental gap: we lack a
systematic understanding of what these stakeholders need from explanations and
how to tailor them accordingly. To address this gap, we present a step-by-step
framework to identify stakeholder needs and guide LLMs in generating tailored
explanations of health simulations. Our procedure uses a mixed-methods design
by first eliciting the explanation needs and stylistic preferences of diverse
health stakeholders, then optimizing the ability of LLMs to generate tailored
outputs (e.g., via controllable attribute tuning), and then evaluating through
a comprehensive range of metrics to further improve the tailored generation of
summaries.

</details>


### [4] [An Approach to Grounding AI Model Evaluations in Human-derived Criteria](https://arxiv.org/abs/2509.04676)
*Sasha Mitts*

Main category: cs.AI

TL;DR: The paper proposes improving AI benchmarks by incorporating human-derived evaluation criteria, focusing on cognitive skills like Prioritization, Memorizing, Discerning, and Contextualizing.


<details>
  <summary>Details</summary>
Motivation: Current AI benchmarks fail to effectively evaluate nuanced capabilities, particularly in physical world modeling.

Method: Combines human insights from interviews and surveys with existing AI benchmarks (Perception Test and OpenEQA) to identify critical cognitive skills and integrate them into benchmark design.

Result: Findings reveal AI deficiencies in interpretive and empathetic skills, along with high user expectations. Offers a framework for more human-aligned benchmarking.

Conclusion: Emphasizes user-centered evaluation, providing practical steps for aligning AI evaluation with human cognitive processes for better benchmarking and future progress.

Abstract: In the rapidly evolving field of artificial intelligence (AI), traditional
benchmarks can fall short in attempting to capture the nuanced capabilities of
AI models. We focus on the case of physical world modeling and propose a novel
approach to augment existing benchmarks with human-derived evaluation criteria,
aiming to enhance the interpretability and applicability of model behaviors.
Grounding our study in the Perception Test and OpenEQA benchmarks, we conducted
in-depth interviews and large-scale surveys to identify key cognitive skills,
such as Prioritization, Memorizing, Discerning, and Contextualizing, that are
critical for both AI and human reasoning. Our findings reveal that participants
perceive AI as lacking in interpretive and empathetic skills yet hold high
expectations for AI performance. By integrating insights from our findings into
benchmark design, we offer a framework for developing more human-aligned means
of defining and measuring progress. This work underscores the importance of
user-centered evaluation in AI development, providing actionable guidelines for
researchers and practitioners aiming to align AI capabilities with human
cognitive processes. Our approach both enhances current benchmarking practices
and sets the stage for future advancements in AI model evaluation.

</details>


### [5] [Language-Driven Hierarchical Task Structures as Explicit World Models for Multi-Agent Learning](https://arxiv.org/abs/2509.04731)
*Brennen Hill*

Main category: cs.AI

TL;DR: The paper proposes leveraging Large Language Models (LLMs) to dynamically create hierarchical, structured world models for more efficient multi-agent learning in complex environments.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to overcome the limitations of current agent training methodologies, particularly in long-horizon, complex, multi-agent tasks with sparse rewards and vast exploration spaces.

Method: The authors suggest using hierarchical scaffolding to decompose complex goals into structured subgoals and propose dynamically generating these scaffolds via LLMs to construct explicit world models.

Result: A systematic review reveals a trend towards combining symbolic and hierarchical techniques with multi-agent reinforcement learning (MARL), laying the groundwork for the proposed paradigm shift.

Conclusion: Dynamic language-driven world modeling offers a promising approach to improve sample efficiency and strategic capabilities in agent training, bridging gaps between low-level actions and high-level behaviors.

Abstract: The convergence of Language models, Agent models, and World models represents
a critical frontier for artificial intelligence. While recent progress has
focused on scaling Language and Agent models, the development of sophisticated,
explicit World Models remains a key bottleneck, particularly for complex,
long-horizon multi-agent tasks. In domains such as robotic soccer, agents
trained via standard reinforcement learning in high-fidelity but
structurally-flat simulators often fail due to intractable exploration spaces
and sparse rewards. This position paper argues that the next frontier in
developing capable agents lies in creating environments that possess an
explicit, hierarchical World Model. We contend that this is best achieved
through hierarchical scaffolding, where complex goals are decomposed into
structured, manageable subgoals. Drawing evidence from a systematic review of
2024 research in multi-agent soccer, we identify a clear and decisive trend
towards integrating symbolic and hierarchical methods with multi-agent
reinforcement learning (MARL). These approaches implicitly or explicitly
construct a task-based world model to guide agent learning. We then propose a
paradigm shift: leveraging Large Language Models to dynamically generate this
hierarchical scaffold, effectively using language to structure the World Model
on the fly. This language-driven world model provides an intrinsic curriculum,
dense and meaningful learning signals, and a framework for compositional
learning, enabling Agent Models to acquire sophisticated, strategic behaviors
with far greater sample efficiency. By building environments with explicit,
language-configurable task layers, we can bridge the gap between low-level
reactive behaviors and high-level strategic team play, creating a powerful and
generalizable framework for training the next generation of intelligent agents.

</details>


### [6] [What-If Analysis of Large Language Models: Explore the Game World Using Proactive Thinking](https://arxiv.org/abs/2509.04791)
*Yuan Sui,Yanming Zhang,Yi Liao,Yu Gu,Guohua Tang,Zhongqian Sun,Wei Yang,Bryan Hooi*

Main category: cs.AI

TL;DR: This paper introduces WiA-LLM, a paradigm enabling LLMs to proactively simulate outcomes through What-If Analysis, improving decision-making and forecasting in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: LLMs lack proactive capabilities to forecast consequences of actions, limiting their effectiveness in strategic, high-stakes scenarios.

Method: The authors propose WiA-LLM, integrating What-If Analysis and reinforcement learning to simulate outcomes of actions and anticipate future states.

Result: WiA-LLM achieves 74.2% accuracy in forecasting game-state changes in a complex multiplayer game environment, significantly outperforming baselines.

Conclusion: WiA-LLM marks a key advancement in proactive reasoning for LLMs, offering a scalable solution with broad implications for dynamic and strategic applications.

Abstract: Large language models (LLMs) excel at processing information reactively but
lack the ability to systemically explore hypothetical futures. They cannot ask,
"what if we take this action? how will it affect the final outcome" and
forecast its potential consequences before acting. This critical gap limits
their utility in dynamic, high-stakes scenarios like strategic planning, risk
assessment, and real-time decision making. To bridge this gap, we propose
WiA-LLM, a new paradigm that equips LLMs with proactive thinking capabilities.
Our approach integrates What-If Analysis (WIA), a systematic approach for
evaluating hypothetical scenarios by changing input variables. By leveraging
environmental feedback via reinforcement learning, WiA-LLM moves beyond
reactive thinking. It dynamically simulates the outcomes of each potential
action, enabling the model to anticipate future states rather than merely react
to the present conditions. We validate WiA-LLM in Honor of Kings (HoK), a
complex multiplayer game environment characterized by rapid state changes and
intricate interactions. The game's real-time state changes require precise
multi-step consequence prediction, making it an ideal testbed for our approach.
Experimental results demonstrate WiA-LLM achieves a remarkable 74.2% accuracy
in forecasting game-state changes (up to two times gain over baselines). The
model shows particularly significant gains in high-difficulty scenarios where
accurate foresight is critical. To our knowledge, this is the first work to
formally explore and integrate what-if analysis capabilities within LLMs.
WiA-LLM represents a fundamental advance toward proactive reasoning in LLMs,
providing a scalable framework for robust decision-making in dynamic
environments with broad implications for strategic applications.

</details>


### [7] [TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models](https://arxiv.org/abs/2509.04809)
*Haechang Kim,Hao Chen,Can Li,Jong Min Lee*

Main category: cs.AI

TL;DR: This paper introduces TalkToAgent, an explainable reinforcement learning (XRL) framework using a multi-agent large language model (LLM) setup to enhance the communicability and comprehensibility of RL policies.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to address the gap between complex RL policies and domain experts by improving the usability and comprehensibility of XRL tools, which currently cause uncertainty about their effectiveness.

Method: The method involves a multi-agent LLM framework composed of five specialized agents—Coordinator, Explainer, Coder, Evaluator, and Debugger. These agents work together to analyze user queries, map them to appropriate XRL tools, and provide detailed explanations of RL agent actions using counterfactual reasoning and behavioral descriptions.

Result: TalkToAgent was successfully tested on a nonlinear control benchmark problem, demonstrating high accuracy in translating user queries into XRL tasks. Coder-debugger interactions helped reduce failures, and qualitative evaluations showed effective interpretation and contextualization of RL agent actions.

Conclusion: TalkToAgent bridges the gap between RL agents and domain experts by improving transparency and explanation quality. The framework’s ability to use natural language for clarification boosts the credibility and applicability of XRL approaches.

Abstract: Explainable Reinforcement Learning (XRL) has emerged as a promising approach
in improving the transparency of Reinforcement Learning (RL) agents. However,
there remains a gap between complex RL policies and domain experts, due to the
limited comprehensibility of XRL results and isolated coverage of current XRL
approaches that leave users uncertain about which tools to employ. To address
these challenges, we introduce TalkToAgent, a multi-agent Large Language Models
(LLM) framework that delivers interactive, natural language explanations for RL
policies. The architecture with five specialized LLM agents (Coordinator,
Explainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically
map user queries to relevant XRL tools and clarify an agent's actions in terms
of either key state variables, expected outcomes, or counterfactual
explanations. Moreover, our approach extends previous counterfactual
explanations by deriving alternative scenarios from qualitative behavioral
descriptions, or even new rule-based policies. We validated TalkToAgent on
quadruple-tank process control problem, a well-known nonlinear control
benchmark. Results demonstrated that TalkToAgent successfully mapped user
queries into XRL tasks with high accuracy, and coder-debugger interactions
minimized failures in counterfactual generation. Furthermore, qualitative
evaluation confirmed that TalkToAgent effectively interpreted agent's actions
and contextualized their meaning within the problem domain.

</details>


### [8] [Collaboration and Conflict between Humans and Language Models through the Lens of Game Theory](https://arxiv.org/abs/2509.04847)
*Mukul Singh,Arjun Radhakrishna,Sumit Gulwani*

Main category: cs.AI

TL;DR: This paper investigates how language models behave in long-term cooperative and competitive settings using the iterated prisoner's dilemma (IPD).


<details>
  <summary>Details</summary>
Motivation: The study aims to understand the cooperative and competitive behavior of language models in multi-party and long-horizon interactions, addressing a research gap in human-model collaboration and strategic adaptability.

Method: Language models were tested against 240 classical strategies in an Axelrod-style tournament in the iterated prisoner's dilemma. Behavioral properties such as niceness, provocability, adaptability, and generosity were analyzed.

Result: Language models performed on par with or surpassed classical strategies, exhibiting strong cooperative traits and rapid adaptability, often rivaling human behavior.

Conclusion: The findings highlight the potential of language models to adapt and cooperate effectively in complex social dynamics, providing a basis for future exploration of human-AI collaboration in mixed environments.

Abstract: Language models are increasingly deployed in interactive online environments,
from personal chat assistants to domain-specific agents, raising questions
about their cooperative and competitive behavior in multi-party settings. While
prior work has examined language model decision-making in isolated or
short-term game-theoretic contexts, these studies often neglect long-horizon
interactions, human-model collaboration, and the evolution of behavioral
patterns over time. In this paper, we investigate the dynamics of language
model behavior in the iterated prisoner's dilemma (IPD), a classical framework
for studying cooperation and conflict. We pit model-based agents against a
suite of 240 well-established classical strategies in an Axelrod-style
tournament and find that language models achieve performance on par with, and
in some cases exceeding, the best-known classical strategies. Behavioral
analysis reveals that language models exhibit key properties associated with
strong cooperative strategies - niceness, provocability, and generosity while
also demonstrating rapid adaptability to changes in opponent strategy mid-game.
In controlled "strategy switch" experiments, language models detect and respond
to shifts within only a few rounds, rivaling or surpassing human adaptability.
These results provide the first systematic characterization of long-term
cooperative behaviors in language model agents, offering a foundation for
future research into their role in more complex, mixed human-AI social
environments.

</details>


### [9] [Cloning a Conversational Voice AI Agent from Call\,Recording Datasets for Telesales](https://arxiv.org/abs/2509.04871)
*Krittanon Kaewtawee,Wachiravit Modecrua,Krittin Pachtrachai,Touchapon Kraisingkorn*

Main category: cs.AI

TL;DR: The paper develops a cloned conversational voice AI agent capable of interacting with customers over the phone using call transcripts, combining speech recognition, dialogue management, and text-to-speech synthesis.


<details>
  <summary>Details</summary>
Motivation: To automate routine conversations in domains like customer service and healthcare, reducing costs while providing 24/7 support, using call transcript-based AI agents.

Method: The authors employ domain selection, knowledge extraction, prompt engineering, speech recognition, a dialogue manager powered by large language models, and text-to-speech synthesis in a streaming pipeline.

Result: The cloned AI agent demonstrates comparable performance to humans in routine call aspects but falls short in persuasion and objection handling, as evaluated on 22 criteria.

Conclusion: While the AI agent replicates human-like engagement in routine scenarios, it requires improvement in complex areas like objection handling; the paper outlines future directions including simulation-based evaluation.

Abstract: Recent advances in language and speech modelling have made it possible to
build autonomous voice assistants that understand and generate human dialogue
in real time. These systems are increasingly being deployed in domains such as
customer service and healthcare care, where they can automate repetitive tasks,
reduce operational costs, and provide constant support around the clock. In
this paper, we present a general methodology for cloning a conversational voice
AI agent from a corpus of call recordings. Although the case study described in
this paper uses telesales data to illustrate the approach, the underlying
process generalizes to any domain where call transcripts are available. Our
system listens to customers over the telephone, responds with a synthetic
voice, and follows a structured playbook learned from top performing human
agents. We describe the domain selection, knowledge extraction, and prompt
engineering used to construct the agent, integrating automatic speech
recognition, a large language model based dialogue manager, and text to speech
synthesis into a streaming inference pipeline. The cloned agent is evaluated
against human agents on a rubric of 22 criteria covering introduction, product
communication, sales drive, objection handling, and closing. Blind tests show
that the AI agent approaches human performance in routine aspects of the call
while underperforming in persuasion and objection handling. We analyze these
shortcomings and refine the prompt accordingly. The paper concludes with design
lessons and avenues for future research, including large scale simulation and
automated evaluation.

</details>


### [10] [OSC: Cognitive Orchestration through Dynamic Knowledge Alignment in Multi-Agent LLM Collaboration](https://arxiv.org/abs/2509.04876)
*Jusheng Zhang,Yijia Fan,Kaitong Cai,Xiaofei Sun,Keze Wang*

Main category: cs.AI

TL;DR: Introduces OSC, a framework that enhances multi-agent collaboration by enabling better communication and understanding between agents.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in linguistic interaction for deep collaboration among LLM-based expert agents.

Method: Introduced OSC as an intermediate layer that uses Collaborator Knowledge Models (CKM) and real-time cognitive gap analysis for adaptive communication strategies.

Result: Significant improvements in task performance and communication efficiency demonstrated on reasoning and problem-solving benchmarks.

Conclusion: OSC transforms parallel-working agents into a collaborative cognitive team, optimizing both multi-agent efficiency and interaction insights.

Abstract: This paper introduces OSC (Orchestrating Cognitive Synergy), a
knowledge-aware adaptive collaboration framework designed to enhance cognitive
synergy in multi-agent systems with large language models. While prior work has
advanced agent selection and result aggregation, efficient linguistic
interactions for deep collaboration among expert agents remain a critical
bottleneck. OSC addresses this gap as a pivotal intermediate layer between
selection and aggregation, introducing Collaborator Knowledge Models (CKM) to
enable each agent to dynamically perceive its collaborators' cognitive states.
Through real-time cognitive gap analysis, agents adaptively adjust
communication behaviors, including content focus, detail level, and expression
style, using learned strategies. Experiments on complex reasoning and
problem-solving benchmarks demonstrate that OSC significantly improves task
performance and communication efficiency, transforming "parallel-working
individuals'' into a "deeply collaborative cognitive team.'' This framework not
only optimizes multi-agent collaboration but also offers new insights into LLM
agent interaction behaviors.

</details>


### [11] [SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing](https://arxiv.org/abs/2509.04908)
*Hongyi Jing,Jiafu Chen,Chen Rao,Ziqiang Dang,Jiajie Teng,Tianyi Chu,Juncheng Mo,Shuo Fang,Huaizhong Lin,Rui Lv,Chenguang Ma,Lei Zhao*

Main category: cs.AI

TL;DR: SparkUI-Parser addresses limitations in existing GUI perception models by providing more precise localization and fine-grained parsing capabilities via novel continuous modeling and a rejection mechanism, significantly outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with discrete autoregressive mechanisms and limited parsing capabilities, leading to lower accuracy, slower inference speed, and restricted applicability in GUI tasks.

Method: SparkUI-Parser combines continuous modeling with a pre-trained Multimodal Large Language Model (MLLM) alongside a token router and coordinate decoder to improve accuracy and speed. It also integrates a rejection mechanism based on Hungarian matching to reduce false positives.

Result: The model achieves consistent improvement over SOTA methods on benchmarks like ScreenSpot, ScreenSpot-v2, CAGUI-Grounding, and the newly introduced ScreenParse.

Conclusion: SparkUI-Parser successfully enhances both localization precision and parsing robustness, setting a new standard in GUI perception models.

Abstract: The existing Multimodal Large Language Models (MLLMs) for GUI perception have
made great progress. However, the following challenges still exist in prior
methods: 1) They model discrete coordinates based on text autoregressive
mechanism, which results in lower grounding accuracy and slower inference
speed. 2) They can only locate predefined sets of elements and are not capable
of parsing the entire interface, which hampers the broad application and
support for downstream tasks. To address the above issues, we propose
SparkUI-Parser, a novel end-to-end framework where higher localization
precision and fine-grained parsing capability of the entire interface are
simultaneously achieved. Specifically, instead of using probability-based
discrete modeling, we perform continuous modeling of coordinates based on a
pre-trained Multimodal Large Language Model (MLLM) with an additional token
router and coordinate decoder. This effectively mitigates the limitations
inherent in the discrete output characteristics and the token-by-token
generation process of MLLMs, consequently boosting both the accuracy and the
inference speed. To further enhance robustness, a rejection mechanism based on
a modified Hungarian matching algorithm is introduced, which empowers the model
to identify and reject non-existent elements, thereby reducing false positives.
Moreover, we present ScreenParse, a rigorously constructed benchmark to
systematically assess structural perception capabilities of GUI models across
diverse scenarios. Extensive experiments demonstrate that our approach
consistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2,
CAGUI-Grounding and ScreenParse benchmarks. The resources are available at
https://github.com/antgroup/SparkUI-Parser.

</details>


### [12] [Towards Ontology-Based Descriptions of Conversations with Qualitatively-Defined Concepts](https://arxiv.org/abs/2509.04926)
*Barbara Gendron,Gaël Guibon,Mathieu D'aquin*

Main category: cs.AI

TL;DR: The paper introduces an ontology-based method to define and control conversational features in Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Ensuring controllability, predictable behavior, and user-personalized responses in conversational agents built on LLMs.

Method: Derives quantitative definitions for qualitative conversational concepts using linguistic descriptors, formalized in an ontology via description logic, and applied to text generation control using CEFR language proficiency levels.

Result: The framework yields consistent and explainable proficiency-level definitions, enhancing transparency in conversational agents.

Conclusion: The approach improves controllability and transparency in conversational AI while enabling linguistic reasoning and consistency checking.

Abstract: The controllability of Large Language Models (LLMs) when used as
conversational agents is a key challenge, particularly to ensure predictable
and user-personalized responses. This work proposes an ontology-based approach
to formally define conversational features that are typically qualitative in
nature. By leveraging a set of linguistic descriptors, we derive quantitative
definitions for qualitatively-defined concepts, enabling their integration into
an ontology for reasoning and consistency checking. We apply this framework to
the task of proficiency-level control in conversations, using CEFR language
proficiency levels as a case study. These definitions are then formalized in
description logic and incorporated into an ontology, which guides controlled
text generation of an LLM through fine-tuning. Experimental results demonstrate
that our approach provides consistent and explainable proficiency-level
definitions, improving transparency in conversational AI.

</details>


### [13] [Internet 3.0: Architecture for a Web-of-Agents with it's Algorithm for Ranking Agents](https://arxiv.org/abs/2509.04979)
*Rajesh Tembarai Krishnamachari,Srividya Rajesh*

Main category: cs.AI

TL;DR: The paper introduces DOVIS, a protocol to address Agent Ranking for autonomous AI in the emerging Web of Agents, proposing a privacy-preserving ranking system called AgentRank-UC that evaluates usage and competence.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to rank autonomous AI agents based on actual proven performance rather than declared capabilities, particularly in the Web of Agents ecosystem where fragmented and private usage signals pose challenges.

Method: The authors propose DOVIS, a five-layer protocol consisting of Discovery, Orchestration, Verification, Incentives, and Semantics. They also introduce AgentRank-UC, an algorithm that integrates usage and competence metrics for ranking agents while tackling issues like Sybil resistance.

Result: Simulation results and theoretical analysis validate the convergence, robustness, and functionality of both DOVIS and AgentRank-UC, showing their effectiveness for scalable and trustworthy agent ranking.

Conclusion: The proposed methods enable an operational and scalable framework for dynamic and trust-aware agency ranking in the Web of Agents ecosystem, paving the way for better coordination and usability in this new paradigm.

Abstract: AI agents -- powered by reasoning-capable large language models (LLMs) and
integrated with tools, data, and web search -- are poised to transform the
internet into a \emph{Web of Agents}: a machine-native ecosystem where
autonomous agents interact, collaborate, and execute tasks at scale. Realizing
this vision requires \emph{Agent Ranking} -- selecting agents not only by
declared capabilities but by proven, recent performance. Unlike Web~1.0's
PageRank, a global, transparent network of agent interactions does not exist;
usage signals are fragmented and private, making ranking infeasible without
coordination.
  We propose \textbf{DOVIS}, a five-layer operational protocol
(\emph{Discovery, Orchestration, Verification, Incentives, Semantics}) that
enables the collection of minimal, privacy-preserving aggregates of usage and
performance across the ecosystem. On this substrate, we implement
\textbf{AgentRank-UC}, a dynamic, trust-aware algorithm that combines
\emph{usage} (selection frequency) and \emph{competence} (outcome quality,
cost, safety, latency) into a unified ranking. We present simulation results
and theoretical guarantees on convergence, robustness, and Sybil resistance,
demonstrating the viability of coordinated protocols and performance-aware
ranking in enabling a scalable, trustworthy Agentic Web.

</details>


### [14] [Sticker-TTS: Learn to Utilize Historical Experience with a Sticker-driven Test-Time Scaling Framework](https://arxiv.org/abs/2509.05007)
*Jie Chen,Jinhao Jiang,Yingqian Min,Zican Dong,Shijie Wang,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.AI

TL;DR: The paper introduces Sticker-TTS, a test-time scaling framework for reasoning tasks that optimizes computational efficiency by using historical attempts and distilling critical insights called "stickers."


<details>
  <summary>Details</summary>
Motivation: Current Large Reasoning Models lack efficient test-time scaling methods, often using redundant sampling without utilizing historical information, leading to computational inefficiencies.

Method: The proposed method, Sticker-TTS, uses three collaborative LRMs and a two-stage optimization strategy that leverages stickers to extract, refine, and reuse information progressively through imitation learning and self-improvement.

Result: Sticker-TTS shows superior performance on mathematical reasoning benchmarks AIME-24, AIME-25, and OlymMATH, outperforming baselines such as self-consistency and advanced reinforcement learning within similar inference budgets.

Conclusion: Sticker-TTS effectively enhances computational efficiency by utilizing historical reasoning attempts and showcases its advantages over existing methods for complex reasoning tasks.

Abstract: Large reasoning models (LRMs) have exhibited strong performance on complex
reasoning tasks, with further gains achievable through increased computational
budgets at inference. However, current test-time scaling methods predominantly
rely on redundant sampling, ignoring the historical experience utilization,
thereby limiting computational efficiency. To overcome this limitation, we
propose Sticker-TTS, a novel test-time scaling framework that coordinates three
collaborative LRMs to iteratively explore and refine solutions guided by
historical attempts. At the core of our framework are distilled key
conditions-termed stickers-which drive the extraction, refinement, and reuse of
critical information across multiple rounds of reasoning. To further enhance
the efficiency and performance of our framework, we introduce a two-stage
optimization strategy that combines imitation learning with self-improvement,
enabling progressive refinement. Extensive evaluations on three challenging
mathematical reasoning benchmarks, including AIME-24, AIME-25, and OlymMATH,
demonstrate that Sticker-TTS consistently surpasses strong baselines, including
self-consistency and advanced reinforcement learning approaches, under
comparable inference budgets. These results highlight the effectiveness of
sticker-guided historical experience utilization. Our code and data are
available at https://github.com/RUCAIBox/Sticker-TTS.

</details>


### [15] [Finding your MUSE: Mining Unexpected Solutions Engine](https://arxiv.org/abs/2509.05072)
*Nir Sweed,Hanit Hakim,Ben Wolfson,Hila Lifshitz,Dafna Shahaf*

Main category: cs.AI

TL;DR: The paper introduces Functional Concept Graphs (FCGs) to overcome cognitive fixation in innovation and a MUSE algorithm to derive inspirations. It computes and shares an FCG from 500K patents for future studies.


<details>
  <summary>Details</summary>
Motivation: To address cognitive fixation in innovators that limits exploration of novel solutions by offering a tool for abstraction and analogical problem-solving.

Method: The paper presents the creation of FCGs, which are functional representations with explicit abstraction relations, and MUSE, an algorithm exploiting FCGs for inspiration generation.

Result: Constructed high-quality, large-scale FCGs using data from 500K patents and demonstrated its use as an inspiration tool.

Conclusion: FCGs and MUSE can significantly aid innovators by re-framing problem-solving for creative solution exploration, demonstrated through their extensive dataset and public release.

Abstract: Innovators often exhibit cognitive fixation on existing solutions or nascent
ideas, hindering the exploration of novel alternatives. This paper introduces a
methodology for constructing Functional Concept Graphs (FCGs), interconnected
representations of functional elements that support abstraction, problem
reframing, and analogical inspiration. Our approach yields large-scale,
high-quality FCGs with explicit abstraction relations, overcoming limitations
of prior work. We further present MUSE, an algorithm leveraging FCGs to
generate creative inspirations for a given problem. We demonstrate our method
by computing an FCG on 500K patents, which we release for further research.

</details>


### [16] [ProToM: Promoting Prosocial Behaviour via Theory of Mind-Informed Feedback](https://arxiv.org/abs/2509.05091)
*Matteo Bortoletto,Yichao Zhou,Lance Ying,Tianmin Shu,Andreas Bulling*

Main category: cs.AI

TL;DR: The paper introduces ProToM, an AI system informed by Theory of Mind, that facilitates prosocial behaviors in multi-agent environments by providing targeted feedback based on inferred goals.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of promoting prosocial behaviors in multi-agent systems by enabling effective collaboration and feedback while pursuing independent goals.

Method: ProToM uses Bayesian inverse planning to infer agents' goals and selects feedback by maximizing expected utility based on the inferred goal distribution.

Result: ProToM outperforms state-of-the-art models in providing context-sensitive feedback, achieving higher success rates, faster task completion, and is rated more favorably by human users.

Conclusion: ProToM demonstrates that Theory of Mind-based systems can effectively enhance cooperation and prosocial behaviors in multi-agent environments, addressing limitations in current models.

Abstract: While humans are inherently social creatures, the challenge of identifying
when and how to assist and collaborate with others - particularly when pursuing
independent goals - can hinder cooperation. To address this challenge, we aim
to develop an AI system that provides useful feedback to promote prosocial
behaviour - actions that benefit others, even when not directly aligned with
one's own goals. We introduce ProToM, a Theory of Mind-informed facilitator
that promotes prosocial actions in multi-agent systems by providing targeted,
context-sensitive feedback to individual agents. ProToM first infers agents'
goals using Bayesian inverse planning, then selects feedback to communicate by
maximising expected utility, conditioned on the inferred goal distribution. We
evaluate our approach against baselines in two multi-agent environments: Doors,
Keys, and Gems, as well as Overcooked. Our results suggest that
state-of-the-art large language and reasoning models fall short of
communicating feedback that is both contextually grounded and well-timed -
leading to higher communication overhead and task speedup. In contrast, ProToM
provides targeted and helpful feedback, achieving a higher success rate,
shorter task completion times, and is consistently preferred by human users.

</details>


### [17] [Evaluation and Comparison Semantics for ODRL](https://arxiv.org/abs/2509.05139)
*Jaime Osvaldo Salas,Paolo Pareti,Semih Yumuşak,Soulmaz Gheisari,Luis-Daniel Ibáñez,George Konstantinidis*

Main category: cs.AI

TL;DR: This paper refines the formal semantics of the Open Digital Rights Language (ODRL) based on query answering, enabling comparisons of computational policies.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper is to address the lack of comprehensive formal semantics for ODRL despite its significance as a standard for governing digital resource access and usage.

Method: The authors propose a query-answering-based formal semantics for ODRL and define a framework for comparing computational policies, such as detecting equivalent, restrictive, and permissive policies.

Result: The paper achieves a more refined formalisation of ODRL aligned with its latest specification (2.2), and provides theoretical tools for evaluating and comparing policies in data-sharing scenarios.

Conclusion: This work strengthens the foundation for managing digital rights by offering tools to understand and compare ODRL-based policies effectively.

Abstract: We consider the problem of evaluating, and comparing computational policies
in the Open Digital Rights Language (ODRL), which has become the de facto
standard for governing the access and usage of digital resources. Although
preliminary progress has been made on the formal specification of the
language's features, a comprehensive formal semantics of ODRL is still missing.
In this paper, we provide a simple and intuitive formal semantics for ODRL that
is based on query answering. Our semantics refines previous formalisations, and
is aligned with the latest published specification of the language (2.2).
Building on our evaluation semantics, and motivated by data sharing scenarios,
we also define and study the problem of comparing two policies, detecting
equivalent, more restrictive or more permissive policies.

</details>


### [18] [LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation](https://arxiv.org/abs/2509.05263)
*Yinglin Duan,Zhengxia Zou,Tongwei Gu,Wei Jia,Zhan Zhao,Luyi Xu,Xinzhu Liu,Hao Jiang,Kang Chen,Shuang Qiu*

Main category: cs.AI

TL;DR: The paper introduces LatticeWorld, a 3D world generation framework utilizing lightweight LLMs and advanced rendering engines to efficiently produce high-quality virtual environments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to reduce the sim-to-real gap by creating realistic 3D virtual worlds that simulate real-world physics, enabling applications in AI, autonomous driving, and entertainment.

Method: The method involves combining lightweight LLMs like LLaMA-2-7B with high-quality rendering engines (e.g., Unreal Engine 5) to generate 3D environments based on multimodal instructions, including text and visuals.

Result: LatticeWorld demonstrates superior accuracy in scene layout generation, high visual fidelity, and an over 90x efficiency improvement in industrial production compared to manual modeling.

Conclusion: LatticeWorld significantly enhances the efficiency and quality of 3D virtual world production, showcasing its potential to streamline industrial pipelines and provide highly interactive environments.

Abstract: Recent research has been increasingly focusing on developing 3D world models
that simulate complex real-world scenarios. World models have found broad
applications across various domains, including embodied AI, autonomous driving,
entertainment, etc. A more realistic simulation with accurate physics will
effectively narrow the sim-to-real gap and allow us to gather rich information
about the real world conveniently. While traditional manual modeling has
enabled the creation of virtual 3D scenes, modern approaches have leveraged
advanced machine learning algorithms for 3D world generation, with most recent
advances focusing on generative methods that can create virtual worlds based on
user instructions. This work explores such a research direction by proposing
LatticeWorld, a simple yet effective 3D world generation framework that
streamlines the industrial production pipeline of 3D environments. LatticeWorld
leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering
engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed
framework accepts textual descriptions and visual instructions as multimodal
inputs and creates large-scale 3D interactive worlds with dynamic agents,
featuring competitive multi-agent interaction, high-fidelity physics
simulation, and real-time rendering. We conduct comprehensive experiments to
evaluate LatticeWorld, showing that it achieves superior accuracy in scene
layout generation and visual fidelity. Moreover, LatticeWorld achieves over a
$90\times$ increase in industrial production efficiency while maintaining high
creative quality compared with traditional manual production methods. Our demo
video is available at https://youtu.be/8VWZXpERR18

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [19] [L1RA: Dynamic Rank Assignment in LoRA Fine-Tuning](https://arxiv.org/abs/2509.04884)
*Raul Singh,Nicolo Brunello,Vincenzo Scotti,Mark James Carman*

Main category: cs.CL

TL;DR: L1RA dynamically adjusts low-rank adapters during LLM fine-tuning using L1 regularisation to optimize performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the computational challenges of fine-tuning Large Language Models for downstream tasks.

Method: Introduced L1RA, a technique applying L1 regularisation to prune and redistribute rank budgets in low-rank adapters during fine-tuning.

Result: Achieved comparable or better performance with reduced computational overhead and identified components requiring adaptation.

Conclusion: L1RA improves fine-tuning efficiency, offers insights for model refinement, and is beneficial for resource-constrained scenarios.

Abstract: The ability of Large Language Models (LLMs) to solve complex tasks has made
them crucial in the development of AI-based applications. However, the high
computational requirements to fine-tune these LLMs on downstream tasks pose
significant challenges, particularly when resources are limited. In response to
this challenge, we introduce L1RA, a novel technique aimed at dynamically
distributing the rank of low-rank adapters during fine-tuning using LoRA. Given
a rank budget (i.e., total sum of adapters rank), L1RA leverages L1
regularisation to prune redundant ranks and redistribute them across adapters,
thereby optimising resource utilisation. Through a series of comprehensive
experiments, we empirically demonstrate that L1RA maintains comparable or even
reduced computational overhead compared to other LoRA variants, including the
vanilla approach, while achieving same or better performances. Moreover, the
post-training analysis of rank distribution unveiled insights into the specific
model components requiring the most adaptation to align with the task
objective: the feed-forward layers and the attention output projection. These
results highlight the efficacy of L1RA in not only enhancing the efficiency of
LLM fine-tuning, but also in providing valuable diagnostic information for
model refinement and customisation. In conclusion, L1RA stands as a promising
technique for advancing the performance and interpretability of LLM adaptation,
particularly in scenarios where computational resources are constrained.

</details>


### [20] [INSEva: A Comprehensive Chinese Benchmark for Large Language Models in Insurance](https://arxiv.org/abs/2509.04455)
*Shisong Chen,Qian Zhu,Wenyan Yang,Chengyi Yang,Zhong Wang,Ping Wang,Xuan Lin,Bo Xu,Daqian Li,Chao Yuan,Licai Qi,Wanqing Xu,sun zhenxing,Xin Lu,Shiqiang Xiong,Chao Chen,Haixiang Hu,Yanghua Xiao*

Main category: cs.CL

TL;DR: The paper introduces INSEva, a specialized AI benchmark for evaluating Large Language Models (LLMs) in the Chinese insurance domain, addressing unique industry requirements.


<details>
  <summary>Details</summary>
Motivation: Existing AI benchmarks fail to adequately assess the unique requirements and tasks in the insurance industry.

Method: The authors created INSEva, a Chinese benchmark with 38,704 evaluation examples, organized by multiple dimensions like business areas and task difficulty. It includes tailored methods for evaluating faithfulness and completeness in AI responses.

Result: Eight state-of-the-art Large Language Models (LLMs) were evaluated with INSEva, revealing performance variations. While general LLMs score well on basic tasks, they struggle with complex insurance scenarios.

Conclusion: There is a need for specialized benchmarks like INSEva to advance AI’s capabilities in handling intricate, real-world insurance tasks. The benchmark will soon be publicly available.

Abstract: Insurance, as a critical component of the global financial system, demands
high standards of accuracy and reliability in AI applications. While existing
benchmarks evaluate AI capabilities across various domains, they often fail to
capture the unique characteristics and requirements of the insurance domain. To
address this gap, we present INSEva, a comprehensive Chinese benchmark
specifically designed for evaluating AI systems' knowledge and capabilities in
insurance. INSEva features a multi-dimensional evaluation taxonomy covering
business areas, task formats, difficulty levels, and cognitive-knowledge
dimension, comprising 38,704 high-quality evaluation examples sourced from
authoritative materials. Our benchmark implements tailored evaluation methods
for assessing both faithfulness and completeness in open-ended responses.
Through extensive evaluation of 8 state-of-the-art Large Language Models
(LLMs), we identify significant performance variations across different
dimensions. While general LLMs demonstrate basic insurance domain competency
with average scores above 80, substantial gaps remain in handling complex,
real-world insurance scenarios. The benchmark will be public soon.

</details>


### [21] [Mentalic Net: Development of RAG-based Conversational AI and Evaluation Framework for Mental Health Support](https://arxiv.org/abs/2509.04456)
*Anandi Dutta,Shivani Mruthyunjaya,Jessica Saddington,Kazi Sifatul Islam*

Main category: cs.CL

TL;DR: The paper introduces a safe and effective mental health chatbot using Retrieval-Augmented Generation (RAG), achieving high performance and emphasizing responsible development.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges posed by large language models while exploring their potential for mental health support applications.

Method: The authors developed a chatbot using a RAG framework, integrated prompt engineering, and fine-tuned a pre-trained model on novel datasets.

Result: The chatbot achieved a high BERT Score of 0.898 with other satisfactory evaluation metrics.

Conclusion: The authors advocate a human-in-the-loop and long-term strategy for responsibly leveraging transformative technologies like LLMs.

Abstract: The emergence of large language models (LLMs) has unlocked boundless
possibilities, along with significant challenges. In response, we developed a
mental health support chatbot designed to augment professional healthcare, with
a strong emphasis on safe and meaningful application. Our approach involved
rigorous evaluation, covering accuracy, empathy, trustworthiness, privacy, and
bias. We employed a retrieval-augmented generation (RAG) framework, integrated
prompt engineering, and fine-tuned a pre-trained model on novel datasets. The
resulting system, Mentalic Net Conversational AI, achieved a BERT Score of
0.898, with other evaluation metrics falling within satisfactory ranges. We
advocate for a human-in-the-loop approach and a long-term, responsible strategy
in developing such transformative technologies, recognizing both their
potential to change lives and the risks they may pose if not carefully managed.

</details>


### [22] [Do MLLMs Really Understand the Charts?](https://arxiv.org/abs/2509.04457)
*Xiao Zhang,Dongyuan Li,Liuyu Xiang,Yao Zhang,Cheng Zhong,Zhaofeng He*

Main category: cs.CL

TL;DR: This paper introduces ChartReasoner and a benchmark (CRBench) to improve and assess the chart reasoning abilities of multimodal large language models (MLLMs), addressing their issues like hallucinations and reliance on recognition.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with understanding non-annotated charts, often hallucinate, and rely on recognition without reasoning. The research aims to mimic human-like chart interpretation and reasoning abilities.

Method: The authors developed CRBench for evaluating visual reasoning capabilities and proposed ChartReasoner, a system that emulates human behavior for grounded chart estimation.

Result: ChartReasoner-3B/7B shows superior performance in chart reasoning, outperforming models like GPT-4o and Gemini-2.5-Flash. It also improves visual reasoning in general benchmarks.

Conclusion: ChartReasoner enhances rational chart understanding and visual reasoning in MLLMs, marking significant progress in chart comprehension. Tools and data will be made accessible.

Abstract: Although Multimodal Large Language Models (MLLMs) have demonstrated
increasingly impressive performance in chart understanding, most of them
exhibit alarming hallucinations and significant performance degradation when
handling non-annotated charts. Therefore, a question arises: Do MLLMs really
understand the charts? Since a human is capable of understanding charts and
estimating the values by visual reasoning, we first carefully establish a
comprehensive Chart Reasoning Benchmark CRBench to rigorously evaluate the
visual reasoning abilities of MLLMs on non-annotated charts. We argue that
MLLMs are primarily relying on recognition rather than reasoning to interpret
the charts. To steer MLLMs to reasonable chart understanding, we propose
ChartReasoner that mimics human behavior by grounding their estimation in chart
understanding. Extensive results on the proposed CRBench show that
ChartReasnoner-3B/7B achieves superior performance in chart reasoning, even
compared to GPT-4o and Gemini-2.5-Flash. More importantly, ChartReasnoner also
demonstrates the visual reasoning abilities in general chart comprehension on
public benchmarks, leading to significant performance gains and enabling MLLMs
to rationally understand the charts. The code and dataset will be publicly
available upon publication.

</details>


### [23] [Predicting Failures of LLMs to Link Biomedical Ontology Terms to Identifiers Evidence Across Models and Ontologies](https://arxiv.org/abs/2509.04458)
*Daniel B. Hier,Steven Keith Platt,Tayo Obafemi-Ajayi*

Main category: cs.CL

TL;DR: The study examines why language models GPT-4o and LLaMa 3.1 face challenges in linking ontology terms to their correct identifiers, finding that exposure to ontology identifiers is crucial.


<details>
  <summary>Details</summary>
Motivation: To determine why language models fail to correctly link ontology terms to their identifiers in the biomedical NLP domain.

Method: Analyses were performed across two ontologies (Human Phenotype Ontology and Gene Ontology) and two models (GPT-4o, LLaMa 3.1). Nine features related to term familiarity, usage, morphology, and structure were evaluated via univariate and multivariate analysis.

Result: Exposure to ontology identifiers emerged as the most significant factor predicting success in term linking.

Conclusion: Improving the exposure of language models to ontology identifiers could enhance their performance in linking terms accurately.

Abstract: Large language models often perform well on biomedical NLP tasks but may fail
to link ontology terms to their correct identifiers. We investigate why these
failures occur by analyzing predictions across two major ontologies, Human
Phenotype Ontology and Gene Ontology, and two high-performing models, GPT-4o
and LLaMa 3.1 405B. We evaluate nine candidate features related to term
familiarity, identifier usage, morphology, and ontology structure. Univariate
and multivariate analyses show that exposure to ontology identifiers is the
strongest predictor of linking success.

</details>


### [24] [Uncertainty-Aware Collaborative System of Large and Small Models for Multimodal Sentiment Analysis](https://arxiv.org/abs/2509.04459)
*Shiqin Han,Manning Gao,Menghua Jiang,Yuncheng Jiang,Haifeng Hu,Sijie Mai*

Main category: cs.CL

TL;DR: The paper introduces an Uncertainty-Aware Collaborative System (U-ACS) for multimodal sentiment analysis, combining a lightweight model and a Multimodal Large Language Model (MLLM) to reduce computational costs while maintaining high performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational inefficiency of Multimodal Large Language Models (MLLMs) for real-world deployment and reconcile the trade-off between performance and efficiency in multimodal machine learning.

Method: The method involves using a lightweight model as a rapid filter to identify easy samples, while escalating uncertain and complex samples to an MLLM. Advanced strategies like weighted averaging and prompt-based cross-verification are applied to manage ambiguous predictions.

Result: The proposed U-ACS system is shown to achieve state-of-the-art performance on benchmark datasets while requiring significantly less computational resources compared to standalone MLLMs.

Conclusion: U-ACS demonstrates that a collaborative approach between specialized models and MLLMs can achieve highly efficient and accurate multimodal sentiment analysis, making it suitable for real-world applications.

Abstract: The advent of Multimodal Large Language Models (MLLMs) has significantly
advanced the state-of-the-art in multimodal machine learning, yet their
substantial computational demands present a critical barrier to real-world
deployment. Conversely, smaller, specialized models offer high efficiency but
often at the cost of performance. To reconcile this performance-efficiency
trade-off, we propose a novel Uncertainty-Aware Collaborative System (U-ACS)
that synergistically orchestrates a powerful MLLM (e.g., HumanOmni) and a
lightweight baseline model for multimodal sentiment analysis. The core of our
system is an uncertainty-driven cascade mechanism, where the efficient small
model first acts as a rapid filter for all input samples. Only those samples
yielding high predictive uncertainty, thereby indicating greater difficulty,
are selectively escalated to the MLLM for more sophisticated analysis.
Furthermore, our system introduces advanced strategies to handle ambiguous or
conflicting predictions, including weighted averaging for predictions of
similar polarity and a prompt-based cross-verification to resolve conflicting
predictions when both models exhibit high uncertainty. This
sample-difficulty-aware approach allows for a dynamic allocation of
computational resources, drastically reducing inference costs while retaining
the high accuracy of MLLM. Extensive experiments on benchmark datasets
demonstrate that our proposed method achieves state-of-the-art performance,
while requiring only a fraction of the computational resources compared to
using a standalone MLLM.

</details>


### [25] [CoCoNUTS: Concentrating on Content while Neglecting Uninformative Textual Styles for AI-Generated Peer Review Detection](https://arxiv.org/abs/2509.04460)
*Yihan Chen,Jiawei Chen,Guozhao Mo,Xuanang Chen,Ben He,Xianpei Han,Le Sun*

Main category: cs.CL

TL;DR: The paper addresses the shortcomings of current AI-generated text detectors in the peer review process and introduces CoCoNUTS, a content-oriented benchmark, along with CoCoDet, a multi-task AI detection framework.


<details>
  <summary>Details</summary>
Motivation: The increasing use of LLMs in the peer review process risks fairness and reliability, as current AI-text detectors struggle to identify substantive AI-generated content versus permissible language refinement.

Method: The authors developed CoCoNUTS, a fine-grained dataset of human-AI collaborative peer reviews, and CoCoDet, a multi-task learning framework for more accurate AI-review detection.

Result: CoCoDet showed improved accuracy and robustness in identifying AI-generated content in peer reviews compared to existing detectors.

Conclusion: The proposed paradigm shift from style-based to content-based detection enhances the fairness and reliability of identifying AI influence in scholarly peer reviews.

Abstract: The growing integration of large language models (LLMs) into the peer review
process presents potential risks to the fairness and reliability of scholarly
evaluation. While LLMs offer valuable assistance for reviewers with language
refinement, there is growing concern over their use to generate substantive
review content. Existing general AI-generated text detectors are vulnerable to
paraphrasing attacks and struggle to distinguish between surface language
refinement and substantial content generation, suggesting that they primarily
rely on stylistic cues. When applied to peer review, this limitation can result
in unfairly suspecting reviews with permissible AI-assisted language
enhancement, while failing to catch deceptively humanized AI-generated reviews.
To address this, we propose a paradigm shift from style-based to content-based
detection. Specifically, we introduce CoCoNUTS, a content-oriented benchmark
built upon a fine-grained dataset of AI-generated peer reviews, covering six
distinct modes of human-AI collaboration. Furthermore, we develop CoCoDet, an
AI review detector via a multi-task learning framework, designed to achieve
more accurate and robust detection of AI involvement in review content. Our
work offers a practical foundation for evaluating the use of LLMs in peer
review, and contributes to the development of more precise, equitable, and
reliable detection methods for real-world scholarly applications. Our code and
data will be publicly available at https://github.com/Y1hanChen/COCONUTS.

</details>


### [26] [From Post To Personality: Harnessing LLMs for MBTI Prediction in Social Media](https://arxiv.org/abs/2509.04461)
*Tian Ma,Kaiyu Feng,Yu Rong,Kangfei Zhao*

Main category: cs.CL

TL;DR: This paper proposes "PostToPersonality" (PtoP), an advanced framework leveraging Large Language Models (LLMs) to predict Myers-Briggs personality traits from social media posts, addressing hallucination and class imbalance challenges.


<details>
  <summary>Details</summary>
Motivation: Predicting personality traits, specifically MBTI types, from social media posts is important for applications in psychology and sociology. Traditional methods using ML/DL face limitations, prompting exploration with the capabilities of LLMs.

Method: The PtoP framework mitigates LLM hallucination issues with Retrieval Augmented Generation and employs fine-tuning with synthetic minority oversampling to address MBTI class imbalances.

Result: PtoP outperforms 10 benchmark ML and DL models in predicting MBTI types using a real-world social media dataset.

Conclusion: The PtoP framework demonstrates the effectiveness of LLMs in personality prediction, offering a state-of-the-art approach addressing critical challenges like hallucination and imbalance.

Abstract: Personality prediction from social media posts is a critical task that
implies diverse applications in psychology and sociology. The Myers Briggs Type
Indicator (MBTI), a popular personality inventory, has been traditionally
predicted by machine learning (ML) and deep learning (DL) techniques. Recently,
the success of Large Language Models (LLMs) has revealed their huge potential
in understanding and inferring personality traits from social media content.
However, directly exploiting LLMs for MBTI prediction faces two key challenges:
the hallucination problem inherent in LLMs and the naturally imbalanced
distribution of MBTI types in the population. In this paper, we propose
PostToPersonality (PtoP), a novel LLM based framework for MBTI prediction from
social media posts of individuals. Specifically, PtoP leverages Retrieval
Augmented Generation with in context learning to mitigate hallucination in
LLMs. Furthermore, we fine tune a pretrained LLM to improve model specification
in MBTI understanding with synthetic minority oversampling, which balances the
class imbalance by generating synthetic samples. Experiments conducted on a
real world social media dataset demonstrate that PtoP achieves state of the art
performance compared with 10 ML and DL baselines.

</details>


### [27] [Benchmarking GPT-5 for biomedical natural language processing](https://arxiv.org/abs/2509.04462)
*Yu Hou,Zaifu Zhan,Rui Zhang*

Main category: cs.CL

TL;DR: The paper evaluates GPT-5 and GPT-4o's performance on a biomedical NLP benchmark, showing GPT-5's significant advancements in reasoning tasks but highlighting limitations in certain extraction and summarization tasks.


<details>
  <summary>Details</summary>
Motivation: The rapid growth in biomedical literature has created an urgent need for scalable NLP solutions that can perform well across diverse tasks.

Method: The paper tests GPT-5 and GPT-4o on 12 datasets covering six BiomedNLP task types using zero-/one-/five-shot prompting, with standard prompts and settings, comparing the results with GPT-4, GPT-3.5, and LLaMA-2-13B.

Result: GPT-5 achieved the highest benchmark performance overall (macro-average score: 0.557 in five-shot), showed outstanding performance in tasks like biomedical QA, and set a new state-of-the-art for MedQA and PubMedQA. However, it underperformed on summarization and some extraction tasks compared to domain-specific systems.

Conclusion: While GPT-5 is ready for deployment in reasoning-heavy biomedical applications, tasks requiring high precision or evidence-dense outputs still favor fine-tuned or hybrid systems. The benchmark provides a framework for identifying where additional methods are needed.

Abstract: The rapid expansion of biomedical literature has heightened the need for
scalable natural language processing (NLP) solutions. While GPT-4 substantially
narrowed the gap with task-specific systems, especially in question answering,
its performance across other domains remained uneven. We updated a standardized
BioNLP benchmark to evaluate GPT-5 and GPT-4o under zero-, one-, and five-shot
prompting across 12 datasets spanning six task families: named entity
recognition, relation extraction, multi-label document classification, question
answering, text summarization, and text simplification. Using fixed prompt
templates, identical decoding parameters, and batch inference, we report
primary metrics per dataset and include prior results for GPT-4, GPT-3.5, and
LLaMA-2-13B for comparison. GPT-5 achieved the strongest overall benchmark
performance, with macro-average scores rising to 0.557 under five-shot
prompting versus 0.506 for GPT-4 and 0.508 for GPT-4o. On MedQA, GPT-5 reached
94.1% accuracy, exceeding the previous supervised state of the art by over
fifty points, and attained parity with supervised systems on PubMedQA (0.734).
In extraction tasks, GPT-5 delivered major gains in chemical NER (0.886 F1) and
ChemProt relation extraction (0.616 F1), outperforming GPT-4 and GPT-4o, though
summarization and disease NER still lagged behind domain-specific baselines.
These results establish GPT-5 as a general-purpose model now offering
deployment-ready performance for reasoning-oriented biomedical QA, while
precision-critical extraction and evidence-dense summarization continue to
favor fine-tuned or hybrid approaches. The benchmark delineates where simple
prompting suffices and where retrieval-augmented or planning-based scaffolds
are likely required, providing actionable guidance for BioNLP system design as
frontier models advance.

</details>


### [28] [Can Multiple Responses from an LLM Reveal the Sources of Its Uncertainty?](https://arxiv.org/abs/2509.04464)
*Yang Nan,Pengfei He,Ravi Tandon,Han Xu*

Main category: cs.CL

TL;DR: The paper proposes a framework to analyze the sources of uncertainty in large language models (LLMs) by inspecting patterns of disagreement among their generated outputs. An auxiliary LLM is used to diagnose whether uncertainty arises from input ambiguities, knowledge gaps, or both.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of diagnosing the sources of uncertainty in LLMs, which still produce unreliable outputs despite their advancements, affecting their real-world usability.

Method: The authors use an auxiliary LLM to analyze multiple responses generated by a target LLM. This auxiliary model determines the likely causes of uncertainty (input ambiguity or lack of knowledge) and identifies missing facts if knowledge gaps exist.

Result: The framework was validated on datasets such as AmbigQA, OpenBookQA, and MMLU-Pro, demonstrating its ability to generalize across various tasks and identify distinct uncertainty sources.

Conclusion: The proposed diagnostic approach can facilitate precise manual interventions, potentially improving the reliability and performance of LLMs in practical applications.

Abstract: Large language models (LLMs) have delivered significant breakthroughs across
diverse domains but can still produce unreliable or misleading outputs, posing
critical challenges for real-world applications. While many recent studies
focus on quantifying model uncertainty, relatively little work has been devoted
to \textit{diagnosing the source of uncertainty}. In this study, we show that,
when an LLM is uncertain, the patterns of disagreement among its multiple
generated responses contain rich clues about the underlying cause of
uncertainty. To illustrate this point, we collect multiple responses from a
target LLM and employ an auxiliary LLM to analyze their patterns of
disagreement. The auxiliary model is tasked to reason about the likely source
of uncertainty, such as whether it stems from ambiguity in the input question,
a lack of relevant knowledge, or both. In cases involving knowledge gaps, the
auxiliary model also identifies the specific missing facts or concepts
contributing to the uncertainty. In our experiment, we validate our framework
on AmbigQA, OpenBookQA, and MMLU-Pro, confirming its generality in diagnosing
distinct uncertainty sources. Such diagnosis shows the potential for relevant
manual interventions that improve LLM performance and reliability.

</details>


### [29] [Emotionally-Aware Agents for Dispute Resolution](https://arxiv.org/abs/2509.04465)
*Sushrita Rakshit,James Hale,Kushal Chawla,Jeanne M. Brett,Jonathan Gratch*

Main category: cs.CL

TL;DR: The paper investigates the role of emotional expressions in shaping outcomes during buyer-seller disputes, using large corpora and text emotion recognition with large-language models.


<details>
  <summary>Details</summary>
Motivation: To understand the influence of emotional expressions in disputes and explore whether emotion recognition can inform dispute resolution strategies.

Method: The study employs large-language models for emotion intensity annotation on buyer-seller dispute dialogues and compares their effectiveness to prior approaches.

Result: Large-language models demonstrate superior explanatory power and alignment with human annotators, confirming theoretical understanding of emotional dynamics in disputes.

Conclusion: Emotional expressions significantly impact conflict processes, and agent-based systems recognizing emotional escalation could aid in dispute resolution.

Abstract: In conflict, people use emotional expressions to shape their counterparts'
thoughts, feelings, and actions. This paper explores whether automatic text
emotion recognition offers insight into this influence in the context of
dispute resolution. Prior work has shown the promise of such methods in
negotiations; however, disputes evoke stronger emotions and different social
processes. We use a large corpus of buyer-seller dispute dialogues to
investigate how emotional expressions shape subjective and objective outcomes.
We further demonstrate that large-language models yield considerably greater
explanatory power than previous methods for emotion intensity annotation and
better match the decisions of human annotators. Findings support existing
theoretical models for how emotional expressions contribute to conflict
escalation and resolution and suggest that agent-based systems could be useful
in managing disputes by recognizing and potentially mitigating emotional
escalation.

</details>


### [30] [Just-in-time and distributed task representations in language models](https://arxiv.org/abs/2509.04466)
*Yuxuan Li,Declan Campbell,Stephanie C. Y. Chan,Andrew Kyle Lampinen*

Main category: cs.CL

TL;DR: The paper studies how language models form and change representations for new tasks during in-context learning, emphasizing 'transferrable' task representations that restore task context even without the full prompt. The authors find that these representations evolve sporadically and locally, and primarily work for minimal task scopes, while larger tasks rely on distributed representations.


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand when and how language models create representations for new tasks, focusing on their capacity to perform new tasks without changing their weights.

Method: The study focuses on analyzing 'transferrable' task representations in language models—representations capable of restoring task contexts. The authors scrutinize how these evolve during task sequences and their relationship to evidence within prompts.

Result: The research reveals that such representations are formed sporadically and locally along sequences, condensing evidence into minimal task scopes. These localized representations align with performance improvements from additional examples in a prompt but are distinct from more persistent, high-level task representations.

Conclusion: The findings highlight that language models utilize just-in-time computational processes for task learning, with local representations handling smaller tasks and temporally-distributed representations supporting longer, composite tasks. This sheds light on the adaptive dynamics in in-context learning.

Abstract: Many of language models' impressive capabilities originate from their
in-context learning: based on instructions or examples, they can infer and
perform new tasks without weight updates. In this work, we investigate
\emph{when} representations for new tasks are formed in language models, and
\emph{how} these representations change over the course of context. We focus on
''transferrable'' task representations -- vector representations that can
restore task context in another instance of the model, even without the full
prompt. We show that these representations evolve in non-monotonic and sporadic
ways, and are distinct from a more inert representation of high-level task
categories that persists throughout the context. Specifically, models often
condense multiple evidence into these transferrable task representations, which
align well with the performance improvement based on more examples in the
context. However, this accrual process exhibits strong locality along the
sequence dimension, coming online only at certain tokens -- despite task
identity being reliably decodable throughout the context. Moreover, these local
but transferrable task representations tend to capture minimal ''task scopes'',
such as a semantically-independent subtask, and models rely on more
temporally-distributed representations to support longer and composite tasks.
This two-fold locality (temporal and semantic) underscores a kind of
just-in-time computational process underlying language models' ability to adapt
to new evidence and learn new tasks on the fly.

</details>


### [31] [Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode Disaggregation in Inference](https://arxiv.org/abs/2509.04467)
*Hao Zhang,Mengsi Lyu,Yulong Ao,Yonghua Lin*

Main category: cs.CL

TL;DR: This paper introduces a novel model pruning method aimed at optimizing prefill-decode (PD) disaggregation inference in large language models, achieving significant efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: The high computational and memory demands of large language models limit their usability. Current pruning strategies do not adequately address the unique challenges of PD disaggregation.

Method: The method involves constructing pruning and distillation sets to iteratively remove blocks independently for prefill and decode stages, along with a token-aware cache pruning mechanism.

Result: The approach results in a 20.56% inference speedup and a 4.95x reduction in data transmission bandwidth consumption, validated by extensive experiments.

Conclusion: The proposed method proves effective and efficient in optimizing both PD disaggregated and unified settings, enhancing the overall usability of large language models.

Abstract: Large Language Models (LLMs) demonstrate exceptional capabilities across
various tasks, but their deployment is constrained by high computational and
memory costs. Model pruning provides an effective means to alleviate these
demands. However, existing methods often ignore the characteristics of
prefill-decode (PD) disaggregation in practice. In this paper, we propose a
novel pruning method for PD disaggregation inference, enabling more precise and
efficient block and KV Cache pruning. Our approach constructs pruning and
distillation sets to perform iterative block removal independently for the
prefill and decode stages, obtaining better pruning solutions. Moreover, we
introduce a token-aware cache pruning mechanism that retains all KV Cache in
the prefill stage but selectively reuses entries for the first and last token
sequences in selected layers during decode, reducing communication costs with
minimal overhead. Extensive experiments demonstrate that our approach
consistently achieves strong performance in both PD disaggregation and PD
unified settings without disaggregation. Under the default settings, our method
achieves a 20.56% inference speedup and a 4.95 times reduction in data
transmission bandwidth consumption.

</details>


### [32] [Evaluating Large Language Models for Financial Reasoning: A CFA-Based Benchmark Study](https://arxiv.org/abs/2509.04468)
*Xuan Yao,Qianteng Wang,Xinbo Liu,Ke-Wei Huang*

Main category: cs.CL

TL;DR: This study evaluates LLMs' abilities in financial contexts using CFA mock exam questions, comparing performance under zero-shot and a Retrieval-Augmented Generation pipeline.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic evaluations of large language models in specialized financial domains, using complex and scenario-based CFA exam questions.

Method: The study uses 1,560 CFA mock exam questions and assesses LLMs in both zero-shot settings and via a Retrieval-Augmented Generation (RAG) pipeline that incorporates hierarchical knowledge organization.

Result: Reasoning-specialized models showed consistent superiority in zero-shot tasks, while the RAG pipeline notably improved accuracy in complex scenarios.

Conclusion: Actionable insights are provided regarding the deployment of LLMs in financial applications, especially for balancing model accuracy with cost-efficiency.

Abstract: The rapid advancement of large language models presents significant
opportunities for financial applications, yet systematic evaluation in
specialized financial contexts remains limited. This study presents the first
comprehensive evaluation of state-of-the-art LLMs using 1,560 multiple-choice
questions from official mock exams across Levels I-III of CFA, most rigorous
professional certifications globally that mirror real-world financial analysis
complexity. We compare models distinguished by core design priorities:
multi-modal and computationally powerful, reasoning-specialized and highly
accurate, and lightweight efficiency-optimized.
  We assess models under zero-shot prompting and through a novel
Retrieval-Augmented Generation pipeline that integrates official CFA curriculum
content. The RAG system achieves precise domain-specific knowledge retrieval
through hierarchical knowledge organization and structured query generation,
significantly enhancing reasoning accuracy in professional financial
certification evaluation.
  Results reveal that reasoning-oriented models consistently outperform others
in zero-shot settings, while the RAG pipeline provides substantial improvements
particularly for complex scenarios. Comprehensive error analysis identifies
knowledge gaps as the primary failure mode, with minimal impact from text
readability. These findings provide actionable insights for LLM deployment in
finance, offering practitioners evidence-based guidance for model selection and
cost-performance optimization.

</details>


### [33] [Multi-Modal Vision vs. Text-Based Parsing: Benchmarking LLM Strategies for Invoice Processing](https://arxiv.org/abs/2509.04469)
*David Berghaus,Armin Berger,Lars Hillebrand,Kostadin Cvejoski,Rafet Sifa*

Main category: cs.CL

TL;DR: The study benchmarks eight multi-modal large language models across invoice datasets using zero-shot prompting and compares image processing vs. structured parsing approaches.


<details>
  <summary>Details</summary>
Motivation: To determine the performance and suitability of multi-modal large language models for automated document processing of invoices, considering various processing strategies.

Method: Eight models were tested across three invoice datasets using zero-shot prompting, employing both native image processing and structured parsing via markdown conversion.

Result: Native image processing typically outperformed structured parsing, although performance varied depending on the model and document characteristics.

Conclusion: Native image processing is generally more effective for invoice document processing, offering guidance for selecting models and strategies in automated systems.

Abstract: This paper benchmarks eight multi-modal large language models from three
families (GPT-5, Gemini 2.5, and open-source Gemma 3) on three diverse openly
available invoice document datasets using zero-shot prompting. We compare two
processing strategies: direct image processing using multi-modal capabilities
and a structured parsing approach converting documents to markdown first.
Results show native image processing generally outperforms structured
approaches, with performance varying across model types and document
characteristics. This benchmark provides insights for selecting appropriate
models and processing strategies for automated document systems. Our code is
available online.

</details>


### [34] [COCORELI: Cooperative, Compositional Reconstitution \& Execution of Language Instructions](https://arxiv.org/abs/2509.04470)
*Swarnadeep Bhar,Omar Naim,Eleni Metheniti,Bastien Navarri,Loïc Cabannes,Morteza Ezzabady,Nicholas Asher*

Main category: cs.CL

TL;DR: The paper introduces COCORELI, a hybrid agent framework that addresses limitations of large language models for following complex instructions, spatial reasoning, and minimizing hallucinations. It uses novel abstraction mechanisms and excels in collaborative construction tasks.


<details>
  <summary>Details</summary>
Motivation: Address the shortcomings of LLMs in handling complex tasks, especially in reasoning, reducing hallucinations, and dynamic adaptability in specific tasks.

Method: Develop a hybrid framework combining medium-sized LLMs with abstraction mechanisms and a discourse module to parse instructions and dynamically represent environments.

Result: COCORELI outperformed systems leveraging larger LLMs in collaborative tasks, avoiding hallucinations and effectively asking clarifications and updating learned information.

Conclusion: COCORELI showcases advantages over traditional LLM methods, extending application effectiveness even beyond ENVIRONMENT tasks, as demonstrated in API completion evaluations.

Abstract: We present COCORELI, a hybrid agent framework designed to tackle the
limitations of large language models (LLMs) in tasks requiring: following
complex instructions, minimizing hallucination, and spatial reasoning. COCORELI
integrates medium-sized LLM agents with novel abstraction mechanisms and a
discourse module to parse instructions to in-context learn dynamic, high-level
representations of the environment. Experiments on natural collaborative
construction tasks show that COCORELI outperforms single-LLM CoT and agentic
LLM systems, all using larger LLMs. It manages to largely avoid hallucinations,
identify missing information, ask for clarifications, and update its learned
objects. COCORELI's abstraction abilities extend beyond ENVIRONMENT, as shown
in the ToolBench API completion task.

</details>


### [35] [MOSAIC: A Multilingual, Taxonomy-Agnostic, and Computationally Efficient Approach for Radiological Report Classification](https://arxiv.org/abs/2509.04471)
*Alice Schiavone,Marco Fraccaro,Lea Marie Pehrson,Silvia Ingala,Rasmus Bonnevie,Michael Bachmann Nielsen,Vincent Beliveau,Melanie Ganz,Desmond Elliott*

Main category: cs.CL

TL;DR: MOSAIC is a multilingual, taxonomy-flexible approach for classifying radiology reports using a compact, open-source model called MedGemma-4B. It achieves expert-level performance across multiple datasets and languages while being computationally efficient.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of existing radiology report classification methods, namely, linguistic variability, dependence on large annotated datasets, and resource-intensive language models unsuitable for clinical use.

Method: The authors developed MOSAIC, leveraging MedGemma-4B for zero-/few-shot prompting and lightweight fine-tuning. It allows deployment on consumer-grade GPUs and supports multilingual and multi-modality datasets.

Result: MOSAIC achieves an average macro F1 score of 88 on five chest X-ray datasets, performs well on Danish radiology reports with limited annotations, and operates efficiently using only 24 GB of GPU memory.

Conclusion: MOSAIC provides a practical and open-source alternative to proprietary LLMs for clinical applications and invites contributions for further extension across other languages, taxonomies, and imaging modalities.

Abstract: Radiology reports contain rich clinical information that can be used to train
imaging models without relying on costly manual annotation. However, existing
approaches face critical limitations: rule-based methods struggle with
linguistic variability, supervised models require large annotated datasets, and
recent LLM-based systems depend on closed-source or resource-intensive models
that are unsuitable for clinical use. Moreover, current solutions are largely
restricted to English and single-modality, single-taxonomy datasets. We
introduce MOSAIC, a multilingual, taxonomy-agnostic, and computationally
efficient approach for radiological report classification. Built on a compact
open-access language model (MedGemma-4B), MOSAIC supports both zero-/few-shot
prompting and lightweight fine-tuning, enabling deployment on consumer-grade
GPUs. We evaluate MOSAIC across seven datasets in English, Spanish, French, and
Danish, spanning multiple imaging modalities and label taxonomies. The model
achieves a mean macro F1 score of 88 across five chest X-ray datasets,
approaching or exceeding expert-level performance, while requiring only 24 GB
of GPU memory. With data augmentation, as few as 80 annotated samples are
sufficient to reach a weighted F1 score of 82 on Danish reports, compared to 86
with the full 1600-sample training set. MOSAIC offers a practical alternative
to large or proprietary LLMs in clinical settings. Code and models are
open-source. We invite the community to evaluate and extend MOSAIC on new
languages, taxonomies, and modalities.

</details>


### [36] [RECAP: REwriting Conversations for Intent Understanding in Agentic Planning](https://arxiv.org/abs/2509.04472)
*Kushan Mitra,Dan Zhang,Hannah Kim,Estevam Hruschka*

Main category: cs.CL

TL;DR: The paper introduces RECAP, a benchmark for improving intent detection in conversational AI by rewriting dialogues to clearly capture user goals.


<details>
  <summary>Details</summary>
Motivation: Accurate intent understanding is crucial for conversational AI effective planning, but this is challenged by ambiguous and dynamic real-world dialogues.

Method: The authors propose RECAP, a benchmark and dataset for intent rewriting, combined with an LLM-based evaluator and prompt-based rewriting approaches.

Result: The proposed methods outperformed baselines, and fine-tuning with DPO-based rewriters showed further planning improvements.

Conclusion: Intent rewriting via RECAP is a valuable and feasible method for enhancing agent planning in open-domain dialogue systems.

Abstract: Understanding user intent is essential for effective planning in
conversational assistants, particularly those powered by large language models
(LLMs) coordinating multiple agents. However, real-world dialogues are often
ambiguous, underspecified, or dynamic, making intent detection a persistent
challenge. Traditional classification-based approaches struggle to generalize
in open-ended settings, leading to brittle interpretations and poor downstream
planning. We propose RECAP (REwriting Conversations for Agent Planning), a new
benchmark designed to evaluate and advance intent rewriting, reframing
user-agent dialogues into concise representations of user goals. RECAP captures
diverse challenges such as ambiguity, intent drift, vagueness, and mixed-goal
conversations. Alongside the dataset, we introduce an LLM-based evaluator that
assesses planning utility given the rewritten intent. Using RECAP, we develop a
prompt-based rewriting approach that outperforms baselines. We further
demonstrate that fine-tuning two DPO-based rewriters yields additional utility
gains. Our results highlight intent rewriting as a critical and tractable
component for improving agent planning in open-domain dialogue systems.

</details>


### [37] [SpeechLLM: Unified Speech and Language Model for Enhanced Multi-Task Understanding in Low Resource Settings](https://arxiv.org/abs/2509.04473)
*Jaekwon Yoo,Kunal Chandiramani,Divya Tadimeti,Abenezer Girma,Chandra Dhir*

Main category: cs.CL

TL;DR: The paper proposes a parameter-efficient adapter to bridge speech encoders with LLMs, showing strong results in tasks like ASR, NER, and SA using fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Current methods of integrating speech encoders with LLMs demand extensive data and computational resources, creating limitations for use cases with scarce resources.

Method: The paper introduces a parameter-efficient adapter that processes speech embeddings into LLM-compatible tokens and uses an LLM-based synthetic dataset annotation approach to lower labeling costs. Techniques like classifier regularizers and LoRA optimization are also applied.

Result: The adapter uses 7x fewer trainable parameters and achieves significant improvements in tasks: 26% relative WER reduction (ASR), 6.3% F1 score increase (NER), and 32% F1 boost (SA). Advanced techniques further improve SLUE by 6.6% and 9.5%.

Conclusion: The approach effectively reduces resource demands while yielding strong performance gains across various speech and language tasks, making LLM integration with speech systems more feasible.

Abstract: While integrating speech encoder with LLM requires substantial data and
resources, use cases face limitations due to insufficient availability. To
address this, we propose a solution with a parameter-efficient adapter that
converts speech embeddings into LLM-compatible tokens, focusing on end-to-end
automatic speech recognition (ASR), named entity recognition (NER), and
sentiment analysis (SA). To reduce labeling costs, we employ an LLM-based
synthetic dataset annotation technique. The proposed adapter, using 7x fewer
trainable parameters, achieves significant performance gains: a 26% relative
Word Error Rates (WER) improvement on the LibriSpeech ASR task, a 6.3% relative
F1 score increase on the NER task, and a 32% relative F1 score boost on the SA
task. Moreover, using advanced techniques such as adding a classifier
regularizer and optimizing the LLM with Low-Rank Adaptation (LoRA) yields
notable performance gains, with Spoken Language Understanding Evaluation (SLUE)
score improvement of 6.6% and 9.5%

</details>


### [38] [Scaling Up, Speeding Up: A Benchmark of Speculative Decoding for Efficient LLM Test-Time Scaling](https://arxiv.org/abs/2509.04474)
*Shengyin Sun,Yiming Li,Xing Li,Yingzhao Lian,Weizhe Lin,Hui-Ling Zhen,Zhiyuan Yang,Chen Chen,Xianzhi Yu,Mingxuan Yuan,Chen Ma*

Main category: cs.CL

TL;DR: The paper addresses inefficiencies in using additional computational resources for large language models during test-time scaling by introducing a benchmark for speculative decoding methods to mitigate redundancy and enhance reasoning efficiency.


<details>
  <summary>Details</summary>
Motivation: To tackle inefficiencies caused by repetitive and redundant reasoning traces during test-time scaling in large language models (LLMs).

Method: The paper introduces a benchmarking framework that evaluates speculative decoding methods across three paradigms: Best-of-N sampling, multi-round thinking, and three categories of decoding methods (model-based, training-based, and n-gram-based).

Result: Experiments show that simple n-gram-based methods effectively handle repetitive patterns, greatly aiding speculative decoding and accelerating test-time scaling.

Conclusion: Integrating n-gram-based methods with model-based or training-based approaches has unique potential to balance acceleration for repetitive and diverse reasoning in test-time scaling for large language models.

Abstract: Test-time scaling has emerged as a powerful paradigm for enhancing the
reasoning capabilities of large language models (LLMs) by allocating additional
computational resources during inference. However, this paradigm is inherently
inefficient due to the generation of redundant and repetitive reasoning traces,
leading to significant computational overhead. Speculative decoding offers a
promising avenue for mitigating this inefficiency, yet its efficacy in the
structured, repetition-rich context of test-time scaling remains largely
unexplored. To bridge this gap, we introduce the first comprehensive benchmark
designed to evaluate speculative decoding methods for accelerating LLM
test-time scaling. Our benchmark provides consistent experimental protocols
across representative test-time scaling paradigms (e.g., Best-of-N sampling and
multi-round thinking), enabling a fair comparison of three major categories of
speculative decoding: model-based, training-based, and n-gram-based methods.
Extensive experiments reveal that simple n-gram-based methods effectively
capture repetitive patterns, demonstrating unique potential in accelerating
test-time scaling. This phenomenon demonstrates the value of integrating
n-gram-based methods with model-based or training-based approaches to balance
acceleration for both repetitive and diverse reasoning in test-time scaling. We
hope this benchmark spurs further research on speculative decoding for
test-time scaling, enabling faster and more practical reasoning in LLMs through
better handling of repetitive and diverse reasoning paths.

</details>


### [39] [ParaThinker: Native Parallel Thinking as a New Paradigm to Scale LLM Test-time Compute](https://arxiv.org/abs/2509.04475)
*Hao Wen,Yifan Su,Feifei Zhang,Yunxin Liu,Yunhao Liu,Ya-Qin Zhang,Yuanchun Li*

Main category: cs.CL

TL;DR: The paper identifies a bottleneck in sequential compute scaling for LLMs and introduces a parallel reasoning paradigm using the 'ParaThinker' framework.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of sequential reasoning in LLMs, where initial flawed steps constrain performance, referred to as 'Tunnel Vision.'

Method: Presented a framework, 'ParaThinker,' that trains LLMs to generate diverse reasoning paths in parallel and integrate them into a refined answer.

Result: On reasoning benchmarks, ParaThinker showed accuracy improvements (12.3% for 1.5B and 7.5% for 7B models with 8 parallel paths) and minimal latency overhead (7.1%).

Conclusion: Parallel reasoning scaling is more efficient than sequential scaling, unlocking reasoning potential in smaller models and offering a new direction for future LLM development.

Abstract: Recent advances in Large Language Models (LLMs) have been driven by test-time
compute scaling - a strategy that improves reasoning by generating longer,
sequential thought processes. While effective, this approach encounters a
significant bottleneck as computation increases, where further computation
offers only marginal performance gains. We argue this ceiling is not an
inherent limit of the model's capability but a flaw in the scaling strategy
itself, a phenomenon we term "Tunnel Vision", where a model's imperfect initial
steps lock it into a suboptimal reasoning path. To overcome this, we introduce
a new scaling paradigm: native thought parallelism. We present ParaThinker, an
end-to-end framework that trains an LLM to generate multiple, diverse reasoning
paths in parallel and synthesize them into a superior final answer. By
exploring different lines of thoughts simultaneously, ParaThinker effectively
sidesteps the Tunnel Vision issue and unlocks the model's latent reasoning
potential. Our approach demonstrates that scaling compute in parallel (width)
is a more effective and efficient way to superior reasoning than simply scaling
sequentially (depth). On challenging reasoning benchmarks, ParaThinker achieves
substantial accuracy improvements over sequential LLMs (12.3% for 1.5B and 7.5%
for 7B models on average with 8 parallel paths), while adding only negligible
latency overhead (7.1%). This enables smaller models to surpass much larger
counterparts and establishes parallel thinking as a critical, efficient
dimension for scaling future LLMs.

</details>


### [40] [Training Text-to-Molecule Models with Context-Aware Tokenization](https://arxiv.org/abs/2509.04476)
*Seojin Kim,Hyeontae Song,Jaehyun Nam,Jinwoo Shin*

Main category: cs.CL

TL;DR: CAMT5, a novel text-to-molecule model, improves molecular representation with substructure-level tokenization and importance-based training, outperforming state-of-the-art models using minimal training tokens.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitation of text-to-molecule models relying on atom-level tokenizations that fail to capture global structural context within molecules.

Method: Introduction of a substructure-level tokenization scheme and development of an importance-based training strategy to prioritize key molecular substructures.

Result: Extensive experiments demonstrate CAMT5's superiority in text-to-molecule generation tasks, achieving state-of-the-art performance using only 2% of training tokens.

Conclusion: CAMT5 enhances text-to-molecule generation by capturing molecular semantics effectively, and a proposed ensemble strategy further boosts generation performance.

Abstract: Recently, text-to-molecule models have shown great potential across various
chemical applications, e.g., drug-discovery. These models adapt language models
to molecular data by representing molecules as sequences of atoms. However,
they rely on atom-level tokenizations, which primarily focus on modeling local
connectivity, thereby limiting the ability of models to capture the global
structural context within molecules. To tackle this issue, we propose a novel
text-to-molecule model, coined Context-Aware Molecular T5 (CAMT5). Inspired by
the significance of the substructure-level contexts in understanding molecule
structures, e.g., ring systems, we introduce substructure-level tokenization
for text-to-molecule models. Building on our tokenization scheme, we develop an
importance-based training strategy that prioritizes key substructures, enabling
CAMT5 to better capture the molecular semantics. Extensive experiments verify
the superiority of CAMT5 in various text-to-molecule generation tasks.
Intriguingly, we find that CAMT5 outperforms the state-of-the-art methods using
only 2% of training tokens. In addition, we propose a simple yet effective
ensemble strategy that aggregates the outputs of text-to-molecule models to
further boost the generation performance. Code is available at
https://github.com/Songhyeontae/CAMT5.git.

</details>


### [41] [An End-to-End System for Culturally-Attuned Driving Feedback using a Dual-Component NLG Engine](https://arxiv.org/abs/2509.04478)
*Iniakpokeikiye Peter Thompson,Yi Dewei,Reiter Ehud*

Main category: cs.CL

TL;DR: The paper introduces a mobile system to provide culturally-sensitive safe driving feedback for drivers in Nigeria, leveraging novel AI approaches.


<details>
  <summary>Details</summary>
Motivation: Addressing infrastructural challenges in Nigeria to promote safer driving using culturally-adapted solutions.

Method: Developed an end-to-end mobile system including trip detection, behavior analysis, dual-component Natural Language Generation, and alcohol-detection models.

Result: Pilot deployment with 90 drivers verifies the feasibility of the system, showing initial results on unsafe behaviors.

Conclusion: The system represents a robust framework for applying AI and data-to-text methods to improve road safety and social well-being in low-resource settings.

Abstract: This paper presents an end-to-end mobile system that delivers
culturally-attuned safe driving feedback to drivers in Nigeria, a low-resource
environment with significant infrastructural challenges. The core of the system
is a novel dual-component Natural Language Generation (NLG) engine that
provides both legally-grounded safety tips and persuasive, theory-driven
behavioural reports. We describe the complete system architecture, including an
automatic trip detection service, on-device behaviour analysis, and a
sophisticated NLG pipeline that leverages a two-step reflection process to
ensure high-quality feedback. The system also integrates a specialized machine
learning model for detecting alcohol-influenced driving, a key local safety
issue. The architecture is engineered for robustness against intermittent
connectivity and noisy sensor data. A pilot deployment with 90 drivers
demonstrates the viability of our approach, and initial results on detected
unsafe behaviours are presented. This work provides a framework for applying
data-to-text and AI systems to achieve social good.

</details>


### [42] [No Clustering, No Routing: How Transformers Actually Process Rare Tokens](https://arxiv.org/abs/2509.04479)
*Jing Liu*

Main category: cs.CL

TL;DR: The paper explores how large language models (LLMs) specialize in processing rare tokens. It identifies that rare token processing involves additional specialized neurons outside the power-law distribution, with these neurons being distributed and not modular.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms behind rare token specialization in large language models and why they struggle with rare token prediction.

Method: The study used neuron influence analyses, graph-based clustering, and attention head ablations in GPT-2 XL and Pythia models to investigate how rare token specialization emerges.

Result: Rare token processing requires additional plateau neurons outside the typical power-law regime. These neurons are spatially distributed and not part of modular clusters. Furthermore, attention mechanisms do not show preferential routing to these specialist neurons.

Conclusion: Rare token specialization in LLMs is achieved through a distributed, training-driven differentiation process rather than through architectural modularity, allowing context-sensitive flexibility while optimizing resource allocation.

Abstract: Large language models struggle with rare token prediction, yet the mechanisms
driving their specialization remain unclear. Prior work identified specialized
``plateau'' neurons for rare tokens following distinctive three-regime
influence patterns \cite{liu2025emergent}, but their functional organization is
unknown. We investigate this through neuron influence analyses, graph-based
clustering, and attention head ablations in GPT-2 XL and Pythia models. Our
findings show that: (1) rare token processing requires additional plateau
neurons beyond the power-law regime sufficient for common tokens, forming dual
computational regimes; (2) plateau neurons are spatially distributed rather
than forming modular clusters; and (3) attention mechanisms exhibit no
preferential routing to specialists. These results demonstrate that rare token
specialization arises through distributed, training-driven differentiation
rather than architectural modularity, preserving context-sensitive flexibility
while achieving adaptive capacity allocation.

</details>


### [43] [Discrete Prompt Tuning via Recursive Utilization of Black-box Multimodal Large Language Model for Personalized Visual Emotion Recognition](https://arxiv.org/abs/2509.04480)
*Ryo Takahashi,Naoki Saito,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CL

TL;DR: The paper discusses improving personalized Visual Emotion Recognition (VER) using discrete prompt tuning for Multimodal Large Language Models (MLLMs).


<details>
  <summary>Details</summary>
Motivation: VER has broad applications, and its personalization is essential for practical use but current methods favor generalized patterns and majority views, limiting individual-level emotion recognition.

Method: The approach adapts VER tasks using discrete prompt tuning, inspired by human prompt engineering, to select optimal natural language input for personalized model adjustments.

Result: The method enables better individual-level VER performance by updating prompts based on natural language representations.

Conclusion: Discrete prompt tuning improves personalized VER accuracy, addressing limitations in MLLMs for real-world applications.

Abstract: Visual Emotion Recognition (VER) is an important research topic due to its
wide range of applications, including opinion mining and advertisement design.
Extending this capability to recognize emotions at the individual level further
broadens its potential applications. Recently, Multimodal Large Language Models
(MLLMs) have attracted increasing attention and demonstrated performance
comparable to that of conventional VER methods. However, MLLMs are trained on
large and diverse datasets containing general opinions, which causes them to
favor majority viewpoints and familiar patterns. This tendency limits their
performance in a personalized VER, which is crucial for practical and
real-world applications, and indicates a key area for improvement. To address
this limitation, the proposed method employs discrete prompt tuning inspired by
the process of humans' prompt engineering to adapt the VER task to each
individual. Our method selects the best natural language representation from
the generated prompts and uses it to update the prompt for the realization of
accurate personalized VER.

</details>


### [44] [Energy Landscapes Enable Reliable Abstention in Retrieval-Augmented Large Language Models for Healthcare](https://arxiv.org/abs/2509.04482)
*Ravi Shankar,Sheng Wong,Lin Li,Magdalena Bachmann,Alex Silverthorne,Beth Albert,Gabriel Davis Jones*

Main category: cs.CL

TL;DR: This paper introduces an energy-based model (EBM) to improve abstention decisions in retrieval-augmented generation systems, especially for semantically challenging scenarios.


<details>
  <summary>Details</summary>
Motivation: To provide safer and more reliable abstention mechanisms in RAG systems, particularly in safety-critical applications like women's health.

Method: The paper leverages an energy-based model to create a smooth energy landscape over a large corpus of guideline-derived questions. It benchmarks EBM against softmax and kNN density heuristics under different abstention scenarios.

Result: The EBM outperformed softmax in hard abstention cases with an AUROC of 0.961 (vs. 0.950 for softmax) and a lower FPR@95 (0.235 vs. 0.331), while showing comparable performance on easy cases.

Conclusion: Energy-based abstention scoring provides a more reliable and interpretable confidence framework for RAG systems, making it especially useful in safety-critical contexts.

Abstract: Reliable abstention is critical for retrieval-augmented generation (RAG)
systems, particularly in safety-critical domains such as women's health, where
incorrect answers can lead to harm. We present an energy-based model (EBM) that
learns a smooth energy landscape over a dense semantic corpus of 2.6M
guideline-derived questions, enabling the system to decide when to generate or
abstain. We benchmark the EBM against a calibrated softmax baseline and a
k-nearest neighbour (kNN) density heuristic across both easy and hard
abstention splits, where hard cases are semantically challenging
near-distribution queries. The EBM achieves superior abstention performance
abstention on semantically hard cases, reaching AUROC 0.961 versus 0.950 for
softmax, while also reducing FPR@95 (0.235 vs 0.331). On easy negatives,
performance is comparable across methods, but the EBM's advantage becomes most
pronounced in safety-critical hard distributions. A comprehensive ablation with
controlled negative sampling and fair data exposure shows that robustness stems
primarily from the energy scoring head, while the inclusion or exclusion of
specific negative types (hard, easy, mixed) sharpens decision boundaries but is
not essential for generalisation to hard cases. These results demonstrate that
energy-based abstention scoring offers a more reliable confidence signal than
probability-based softmax confidence, providing a scalable and interpretable
foundation for safe RAG systems.

</details>


### [45] [DecMetrics: Structured Claim Decomposition Scoring for Factually Consistent LLM Outputs](https://arxiv.org/abs/2509.04483)
*Minghui Huang*

Main category: cs.CL

TL;DR: The paper introduces 'DecMetrics,' new metrics for claim decomposition in fact-checking, focusing on quality assessment of atomic claims.


<details>
  <summary>Details</summary>
Motivation: Current research lacks emphasis on evaluating the quality of decomposed atomic claims in the fact-checking process.

Method: Proposed three new metrics (COMPLETENESS, CORRECTNESS, SEMANTIC ENTROPY) and integrated them as reward functions in a lightweight decomposition model.

Result: Developed a decomposition model evaluated through these metrics, aiming to improve fact-checking systems.

Conclusion: The metrics provide a benchmark for claim decomposition, improving reliability and effectiveness in the fact-checking process.

Abstract: Claim decomposition plays a crucial role in the fact-checking process by
breaking down complex claims into simpler atomic components and identifying
their unfactual elements. Despite its importance, current research primarily
focuses on generative methods for decomposition, with insufficient emphasis on
evaluating the quality of these decomposed atomic claims. To bridge this gap,
we introduce \textbf{DecMetrics}, which comprises three new metrics:
\texttt{COMPLETENESS}, \texttt{CORRECTNESS}, and \texttt{SEMANTIC ENTROPY},
designed to automatically assess the quality of claims produced by
decomposition models. Utilizing these metrics, we develop a lightweight claim
decomposition model, optimizing its performance through the integration of
these metrics as a reward function. Through automatic evaluation, our approach
aims to set a benchmark for claim decomposition, enhancing both the reliability
and effectiveness of fact-checking systems.

</details>


### [46] [The Good, the Bad and the Constructive: Automatically Measuring Peer Review's Utility for Authors](https://arxiv.org/abs/2509.04484)
*Abdelrahman Sadallah,Tim Baumgärtner,Iryna Gurevych,Ted Briscoe*

Main category: cs.CL

TL;DR: The study proposes automated methods for improving review quality in peer review by focusing on key aspects such as Actionability, Grounding & Specificity, Verifiability, and Helpfulness. The RevUtil dataset is introduced to evaluate models on these criteria.


<details>
  <summary>Details</summary>
Motivation: With reviewers facing time constraints, there is a need for automated systems to support high-quality feedback generation, ensuring its usefulness for authors.

Method: The study creates the RevUtil dataset with 1,430 human-labeled review comments and 10k synthetically labeled ones, along with rationales for synthetic data. Models are fine-tuned and benchmarked on assessing and generating rationales for review comments based on the defined aspects.

Result: Fine-tuned models showed agreement with human assessments comparable to, and sometimes surpassing, GPT-4. However, machine-generated reviews still lag behind human reviews in quality across the key aspects.

Conclusion: Automated systems can significantly support reviewing quality, but further advancements are needed for machine-generated reviews to match human performance consistently.

Abstract: Providing constructive feedback to paper authors is a core component of peer
review. With reviewers increasingly having less time to perform reviews,
automated support systems are required to ensure high reviewing quality, thus
making the feedback in reviews useful for authors. To this end, we identify
four key aspects of review comments (individual points in weakness sections of
reviews) that drive the utility for authors: Actionability, Grounding &
Specificity, Verifiability, and Helpfulness. To enable evaluation and
development of models assessing review comments, we introduce the RevUtil
dataset. We collect 1,430 human-labeled review comments and scale our data with
10k synthetically labeled comments for training purposes. The synthetic data
additionally contains rationales, i.e., explanations for the aspect score of a
review comment. Employing the RevUtil dataset, we benchmark fine-tuned models
for assessing review comments on these aspects and generating rationales. Our
experiments demonstrate that these fine-tuned models achieve agreement levels
with humans comparable to, and in some cases exceeding, those of powerful
closed models like GPT-4o. Our analysis further reveals that machine-generated
reviews generally underperform human reviews on our four aspects.

</details>


### [47] [ASCENDgpt: A Phenotype-Aware Transformer Model for Cardiovascular Risk Prediction from Electronic Health Records](https://arxiv.org/abs/2509.04485)
*Chris Sainsbury,Andreas Karwath*

Main category: cs.CL

TL;DR: ASCENDgpt leverages phenotype-aware tokenization and transformer-based models to predict cardiovascular risks from electronic health records (EHRs) with high accuracy (average C-index of 0.816).


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve cardiovascular risk prediction by introducing a domain-specific transformer model and addressing challenges in EHR data, such as excessive ICD code complexity and semantic variability.

Method: ASCENDgpt utilizes phenotype-aware tokenization to map raw ICD codes into clinically meaningful tokens, reducing vocabulary size. The model is pretrained on masked language modeling and fine-tuned for five cardiovascular outcomes using time-to-event prediction.

Result: The model achieved excellent predictive performance across five cardiovascular outcomes, with C-index values ranging from 0.792 to 0.842.

Conclusion: Domain-specific tokenization and pretraining approaches are validated as effective tools for clinically interpretable and efficient EHR-based risk prediction models.

Abstract: We present ASCENDgpt, a transformer-based model specifically designed for
cardiovascular risk prediction from longitudinal electronic health records
(EHRs). Our approach introduces a novel phenotype-aware tokenization scheme
that maps 47,155 raw ICD codes to 176 clinically meaningful phenotype tokens,
achieving 99.6\% consolidation of diagnosis codes while preserving semantic
information. This phenotype mapping contributes to a total vocabulary of 10,442
tokens - a 77.9\% reduction when compared with using raw ICD codes directly. We
pretrain ASCENDgpt on sequences derived from 19402 unique individuals using a
masked language modeling objective, then fine-tune for time-to-event prediction
of five cardiovascular outcomes: myocardial infarction (MI), stroke, major
adverse cardiovascular events (MACE), cardiovascular death, and all-cause
mortality. Our model achieves excellent discrimination on the held-out test set
with an average C-index of 0.816, demonstrating strong performance across all
outcomes (MI: 0.792, stroke: 0.824, MACE: 0.800, cardiovascular death: 0.842,
all-cause mortality: 0.824). The phenotype-based approach enables clinically
interpretable predictions while maintaining computational efficiency. Our work
demonstrates the effectiveness of domain-specific tokenization and pretraining
for EHR-based risk prediction tasks.

</details>


### [48] [Serialized Output Prompting for Large Language Model-based Multi-Talker Speech Recognition](https://arxiv.org/abs/2509.04488)
*Hao Shi,Yusuke Fujita,Tomoya Mizumoto,Lianbo Liu,Atsushi Kojima,Yui Sudo*

Main category: cs.CL

TL;DR: This paper introduces a structured prompt framework using serialized outputs to enhance multi-talker automatic speech recognition (ASR) performance in systems using large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Existing multi-talker ASR systems using LLMs lack effective prompt design, hindering performance improvement beyond simple task-definition strategies.

Method: The authors propose an SOP framework where MT content is separated via a Separator and serialized CTC layers, followed by a structured training strategy involving SOT fine-tuning, serialized speech extraction, and SOP-based adaptation.

Result: Experimental results indicate that the SOP framework markedly improves recognition performance for both two-talker and three-talker scenarios on the LibriMix dataset, with notable gains especially in complex multi-talker cases.

Conclusion: The structured SOP framework effectively leverages LLM capabilities for enhanced multi-talker ASR, addressing shortcomings of prior methods.

Abstract: Prompts are crucial for task definition and for improving the performance of
large language models (LLM)-based systems. However, existing LLM-based
multi-talker (MT) automatic speech recognition (ASR) systems either omit
prompts or rely on simple task-definition prompts, with no prior work exploring
the design of prompts to enhance performance. In this paper, we propose
extracting serialized output prompts (SOP) and explicitly guiding the LLM using
structured prompts to improve system performance (SOP-MT-ASR). A Separator and
serialized Connectionist Temporal Classification (CTC) layers are inserted
after the speech encoder to separate and extract MT content from the mixed
speech encoding in a first-speaking-first-out manner. Subsequently, the SOP,
which serves as a prompt for LLMs, is obtained by decoding the serialized CTC
outputs using greedy search. To train the model effectively, we design a
three-stage training strategy, consisting of serialized output training (SOT)
fine-tuning, serialized speech information extraction, and SOP-based
adaptation. Experimental results on the LibriMix dataset show that, although
the LLM-based SOT model performs well in the two-talker scenario, it fails to
fully leverage LLMs under more complex conditions, such as the three-talker
scenario. The proposed SOP approach significantly improved performance under
both two- and three-talker conditions.

</details>


### [49] [Refining Transcripts With TV Subtitles by Prompt-Based Weakly Supervised Training of ASR](https://arxiv.org/abs/2509.04491)
*Xinnian Zhao,Hugo Van Hamme*

Main category: cs.CL

TL;DR: The paper introduces a novel framework utilizing TV subtitles with weakly supervised ASR by treating subtitles as context cues instead of direct transcription targets, and shows improved transcription quality with pseudo-labeled datasets.


<details>
  <summary>Details</summary>
Motivation: TV subtitles, while readily available, are misaligned with audio, making them unsuitable as supervised targets for verbatim transcription.

Method: The paper proposes treating subtitles as context-rich prompts to guide iterative pseudo-transcript refinement, supported by a weighted attention mechanism to emphasize relevant subtitle tokens.

Result: Experiments show improved transcription accuracy over traditional methods, with pseudo-labeled datasets serving as high-quality resources for ASR training.

Conclusion: The approach successfully transforms low-quality TV subtitle data into valuable resources for developing robust ASR systems.

Abstract: This study proposes a novel approach to using TV subtitles within a weakly
supervised (WS) Automatic Speech Recognition (ASR) framework. Although TV
subtitles are readily available, their imprecise alignment with corresponding
audio limits their applicability as supervised targets for verbatim
transcription. Rather than using subtitles as direct supervision signals, our
method reimagines them as context-rich prompts. This design enables the model
to handle discrepancies between spoken audio and subtitle text. Instead,
generated pseudo transcripts become the primary targets, with subtitles acting
as guiding cues for iterative refinement. To further enhance the process, we
introduce a weighted attention mechanism that emphasizes relevant subtitle
tokens during inference. Our experiments demonstrate significant improvements
in transcription accuracy, highlighting the effectiveness of the proposed
method in refining transcripts. These enhanced pseudo-labeled datasets provide
high-quality foundational resources for training robust ASR systems.

</details>


### [50] [Learned Hallucination Detection in Black-Box LLMs using Token-level Entropy Production Rate](https://arxiv.org/abs/2509.04492)
*Charles Moslonka,Hicham Randrianarivo,Arthur Garnier,Emmanuel Malherbe*

Main category: cs.CL

TL;DR: The paper introduces a robust methodology for detecting hallucinations in LLM QA responses using limited data access, leveraging token log-probabilities and entropy-based metrics.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLM responses undermine reliability in real-world applications, especially in scenarios with restricted data access, necessitating efficient detection methods.

Method: The method derives uncertainty indicators from log-probabilities of top-ranked tokens generated during decoding, uses an Entropy Production Rate (EPR) as a baseline metric, and enhances detection with a supervised learning model.

Result: The proposed method significantly outperforms the baseline metric in detecting hallucinations across diverse QA datasets and LLMs, even when using limited log-probabilities.

Conclusion: The technique offers a practical and efficient solution for hallucination detection that can enhance LLM trustworthiness, with demonstrated utility in API-constrained environments and industrial datasets.

Abstract: Hallucinations in Large Language Model (LLM) outputs for Question Answering
(QA) tasks critically undermine their real-world reliability. This paper
introduces an applied methodology for robust, one-shot hallucination detection,
specifically designed for scenarios with limited data access, such as
interacting with black-box LLM APIs that typically expose only a few top
candidate log-probabilities per token. Our approach derives uncertainty
indicators directly from these readily available log-probabilities generated
during non-greedy decoding. We first derive an Entropy Production Rate (EPR)
metric that offers baseline performance, later augmented with supervised
learning. Our learned model uses features representing the entropic
contributions of the accessible top-ranked tokens within a single generated
sequence, requiring no multiple query re-runs. Evaluated across diverse QA
datasets and multiple LLMs, this estimator significantly improves hallucination
detection over using EPR alone. Crucially, high performance is demonstrated
using only the typically small set of available log-probabilities (e.g., top
<10 per token), confirming its practical efficiency and suitability for these
API-constrained deployments. This work provides a readily deployable technique
to enhance the trustworthiness of LLM responses from a single generation pass
in QA and Retrieval-Augmented Generation (RAG) systems, with its utility
further demonstrated in a finance framework analyzing responses to queries on
annual reports from an industrial dataset.

</details>


### [51] [A Narrative-Driven Computational Framework for Clinician Burnout Surveillance](https://arxiv.org/abs/2509.04497)
*Syed Ahmad Chan Bukhari,Fazel Keshtkar,Alyssa Meczkowska*

Main category: cs.CL

TL;DR: The paper introduces a hybrid method using clinical narratives from ICU discharge summaries to monitor clinician burnout, achieving better predictive performance than metadata-based methods.


<details>
  <summary>Details</summary>
Motivation: Clinician burnout, especially in ICU settings, impacts patient safety, yet existing studies often neglect narrative data in clinical notes.

Method: The study employs sentiment embeddings (BioBERT fine-tuned for medical narratives), a tailored stress lexicon, and topic modeling (LDA) to analyze 10,000 ICU discharge summaries and classifies burnout risk using logistic regression.

Result: Their model achieves strong performance with a precision of 0.80, recall of 0.89, and F1 score of 0.84, significantly outperforming metadata-only baselines.

Conclusion: Clinical narratives provide actionable insights for monitoring clinician burnout, suggesting this approach could enhance proactive well-being interventions.

Abstract: Clinician burnout poses a substantial threat to patient safety, particularly
in high-acuity intensive care units (ICUs). Existing research predominantly
relies on retrospective survey tools or broad electronic health record (EHR)
metadata, often overlooking the valuable narrative information embedded in
clinical notes. In this study, we analyze 10,000 ICU discharge summaries from
MIMIC-IV, a publicly available database derived from the electronic health
records of Beth Israel Deaconess Medical Center. The dataset encompasses
diverse patient data, including vital signs, medical orders, diagnoses,
procedures, treatments, and deidentified free-text clinical notes. We introduce
a hybrid pipeline that combines BioBERT sentiment embeddings fine-tuned for
clinical narratives, a lexical stress lexicon tailored for clinician burnout
surveillance, and five-topic latent Dirichlet allocation (LDA) with workload
proxies. A provider-level logistic regression classifier achieves a precision
of 0.80, a recall of 0.89, and an F1 score of 0.84 on a stratified hold-out
set, surpassing metadata-only baselines by greater than or equal to 0.17 F1
score. Specialty-specific analysis indicates elevated burnout risk among
providers in Radiology, Psychiatry, and Neurology. Our findings demonstrate
that ICU clinical narratives contain actionable signals for proactive
well-being monitoring.

</details>


### [52] [Where Should I Study? Biased Language Models Decide! Evaluating Fairness in LMs for Academic Recommendations](https://arxiv.org/abs/2509.04498)
*Krithi Shailya,Akhilesh Kumar Mishra,Gokul S Krishnan,Balaraman Ravindran*

Main category: cs.CL

TL;DR: This paper examines biases in university and program recommendations made by open-source LLMs, revealing strong geographic, demographic, and economic biases that favor Global North institutions and reinforce stereotypes.


<details>
  <summary>Details</summary>
Motivation: Educational use of LLMs is growing in importance, but risks of systemic biases require urgent attention to ensure fair and inclusive recommendations.

Method: The authors used 360 simulated user profiles and evaluated over 25,000 recommendations from three LLMs (LLaMA-3.1-8B, Gemma-7B, and Mistral-7B) using a multi-dimensional evaluation framework.

Result: Findings revealed substantial biases, including over-representation of Global North institutions and gender stereotypes, although LLaMA-3.1 showed the most diversity in recommendations.

Conclusion: Additional work is needed to address biases in LLM-based recommendation systems to promote equitable access to education globally.

Abstract: Large Language Models (LLMs) are increasingly used as daily recommendation
systems for tasks like education planning, yet their recommendations risk
perpetuating societal biases. This paper empirically examines geographic,
demographic, and economic biases in university and program suggestions from
three open-source LLMs: LLaMA-3.1-8B, Gemma-7B, and Mistral-7B. Using 360
simulated user profiles varying by gender, nationality, and economic status, we
analyze over 25,000 recommendations. Results show strong biases: institutions
in the Global North are disproportionately favored, recommendations often
reinforce gender stereotypes, and institutional repetition is prevalent. While
LLaMA-3.1 achieves the highest diversity, recommending 481 unique universities
across 58 countries, systemic disparities persist. To quantify these issues, we
propose a novel, multi-dimensional evaluation framework that goes beyond
accuracy by measuring demographic and geographic representation. Our findings
highlight the urgent need for bias consideration in educational LMs to ensure
equitable global access to higher education.

</details>


### [53] [DeepTRACE: Auditing Deep Research AI Systems for Tracking Reliability Across Citations and Evidence](https://arxiv.org/abs/2509.04499)
*Pranav Narayanan Venkit,Philippe Laban,Yilun Zhou,Kung-Hsiang Huang,Yixin Mao,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: The paper introduces DeepTRACE as an audit framework to evaluate generative search engines and deep research LLM agents for their reliability and accuracy in sourcing and citation practices.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the frequent issues observed in generative search engines and deep research LLM agents, such as overconfidence, weak sourcing, and confusing citation practices, which compromise their trustworthiness and reliability.

Method: The authors present DeepTRACE, an audit framework employing statement-level analysis, confidence scoring, and evidence attribution using citation and factual-support matrices. They deployed automated extraction pipelines and an LLM-judge validated by human raters to assess systems like GPT-4.5/5, You.com, and Bing, among others.

Result: The study finds that generative search engines and deep research agents often provide highly confident but one-sided responses on debate topics and include statements unsupported by their listed sources. Deep-research configurations improve overconfidence and citation thoroughness but still show limitations in citation accuracy (40–80%) and neutrality on debate queries.

Conclusion: While deep-research configurations offer some advancements, significant challenges remain in ensuring source reliability, neutral perspectives, and comprehensive citation accuracy in generative search engines and deep research agents.

Abstract: Generative search engines and deep research LLM agents promise trustworthy,
source-grounded synthesis, yet users regularly encounter overconfidence, weak
sourcing, and confusing citation practices. We introduce DeepTRACE, a novel
sociotechnically grounded audit framework that turns prior community-identified
failure cases into eight measurable dimensions spanning answer text, sources,
and citations. DeepTRACE uses statement-level analysis (decomposition,
confidence scoring) and builds citation and factual-support matrices to audit
how systems reason with and attribute evidence end-to-end. Using automated
extraction pipelines for popular public models (e.g., GPT-4.5/5, You.com,
Perplexity, Copilot/Bing, Gemini) and an LLM-judge with validated agreement to
human raters, we evaluate both web-search engines and deep-research
configurations. Our findings show that generative search engines and deep
research agents frequently produce one-sided, highly confident responses on
debate queries and include large fractions of statements unsupported by their
own listed sources. Deep-research configurations reduce overconfidence and can
attain high citation thoroughness, but they remain highly one-sided on debate
queries and still exhibit large fractions of unsupported statements, with
citation accuracy ranging from 40--80% across systems.

</details>


### [54] [Context Engineering for Trustworthiness: Rescorla Wagner Steering Under Mixed and Inappropriate Contexts](https://arxiv.org/abs/2509.04500)
*Rushi Wang,Jiateng Liu,Cheng Qian,Yifan Shen,Yanzhou Pan,Zhaozhuo Xu,Ahmed Abbasi,Heng Ji,Denghui Zhang*

Main category: cs.CL

TL;DR: The paper introduces a method to address LLMs' tendency to incorporate inappropriate content from mixed contexts, improving response quality significantly.


<details>
  <summary>Details</summary>
Motivation: To address the problem of LLMs processing and prioritizing mixed real-world contexts, which often contain both relevant and inappropriate information, posing risks to response quality.

Method: The authors developed the Poisoned Context Testbed to test LLM behaviors in mixed contexts and adapted the Rescorla-Wagner (RW) model to measure contextual influence. They also proposed a fine-tuning technique, RW-Steering, which helps LLMs disregard inappropriate context signals.

Result: RW-Steering improved the response quality of LLMs by 39.8% and reversed harmful incorporation of inappropriate signals, demonstrating robust generalizability across diverse context mixtures.

Conclusion: RW-Steering offers a reliable solution to enhance LLM response quality and safety in real-world environments by effectively managing inappropriate context signals.

Abstract: Incorporating external context can significantly enhance the response quality
of Large Language Models (LLMs). However, real-world contexts often mix
relevant information with disproportionate inappropriate content, posing
reliability risks. How do LLMs process and prioritize mixed context? To study
this, we introduce the Poisoned Context Testbed, pairing queries with
real-world contexts containing relevant and inappropriate content. Inspired by
associative learning in animals, we adapt the Rescorla-Wagner (RW) model from
neuroscience to quantify how competing contextual signals influence LLM
outputs. Our adapted model reveals a consistent behavioral pattern: LLMs
exhibit a strong tendency to incorporate information that is less prevalent in
the context. This susceptibility is harmful in real-world settings, where small
amounts of inappropriate content can substantially degrade response quality.
Empirical evaluations on our testbed further confirm this vulnerability. To
tackle this, we introduce RW-Steering, a two-stage finetuning-based approach
that enables the model to internally identify and ignore inappropriate signals.
Unlike prior methods that rely on extensive supervision across diverse context
mixtures, RW-Steering generalizes robustly across varying proportions of
inappropriate content. Experiments show that our best fine-tuned model improves
response quality by 39.8% and reverses the undesirable behavior curve,
establishing RW-Steering as a robust, generalizable context engineering
solution for improving LLM safety in real-world use.

</details>


### [55] [Understanding Reinforcement Learning for Model Training, and future directions with GRAPE](https://arxiv.org/abs/2509.04501)
*Rohit Patel*

Main category: cs.CL

TL;DR: This paper clearly explains key algorithms for instruction tuning of models, simplifies their concepts for LLMs, and introduces a new research idea called GRAPE.


<details>
  <summary>Details</summary>
Motivation: Explanations of instruction tuning algorithms often lack clarity, are overly complex, and assume prior knowledge, making them inaccessible to many researchers.

Method: The paper provides step-by-step explanations using simplified, explicit notation for key algorithms focused on large language models (LLMs), avoiding unnecessary abstractions and connecting concepts directly to LLMs.

Result: The authors deliver a clear understanding of existing algorithms, conduct a literature review of new techniques, and introduce the novel GRAPE method for future research.

Conclusion: By simplifying instruction tuning algorithms and presenting GRAPE, the paper enhances accessibility, encourages exploration, and contributes to advancing LLM research.

Abstract: This paper provides a self-contained, from-scratch, exposition of key
algorithms for instruction tuning of models: SFT, Rejection Sampling,
REINFORCE, Trust Region Policy Optimization (TRPO), Proximal Policy
Optimization (PPO), Group Relative Policy Optimization (GRPO), and Direct
Preference Optimization (DPO). Explanations of these algorithms often assume
prior knowledge, lack critical details, and/or are overly generalized and
complex. Here, each method is discussed and developed step by step using
simplified and explicit notation focused on LLMs, aiming to eliminate ambiguity
and provide a clear and intuitive understanding of the concepts. By minimizing
detours into the broader RL literature and connecting concepts to LLMs, we
eliminate superfluous abstractions and reduce cognitive overhead. Following
this exposition, we provide a literature review of new techniques and
approaches beyond those detailed. Finally, new ideas for research and
exploration in the form of GRAPE (Generalized Relative Advantage Policy
Evolution) are presented.

</details>


### [56] [VaccineRAG: Boosting Multimodal Large Language Models' Immunity to Harmful RAG Samples](https://arxiv.org/abs/2509.04502)
*Qixin Sun,Ziqin Wang,Hengyuan Zhao,Yilin Li,Kaiyou Song,Linjiang Huang,Xiaolin Hu,Qingpei Guo,Si Liu*

Main category: cs.CL

TL;DR: This paper introduces VaccineRAG, a novel retrieval-augmented generation approach using Chain-of-Thought analysis to address retrieval accuracy issues in Large Language Models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the challenge of inaccurate retrieval samples in Retrieval Augmented Generation (RAG) systems, which negatively affect the performance of LLMs.

Method: The authors propose VaccineRAG, a dataset featuring Chain-of-Thought analysis to enhance retrieval quality, and Partial-GRPO, a technique for learning complex sequences through modular output modeling.

Result: VaccineRAG improves positive/negative sample discrimination and retrieval efficacy, demonstrated through comprehensive evaluations and ablation studies.

Conclusion: The study shows that employing structured Chain-of-Thought analysis and Partial-GRPO can significantly enhance RAG systems. The dataset and code will be made publicly available to support further research.

Abstract: Retrieval Augmented Generation enhances the response accuracy of Large
Language Models (LLMs) by integrating retrieval and generation modules with
external knowledge, demonstrating particular strength in real-time queries and
Visual Question Answering tasks. However, the effectiveness of RAG is
frequently hindered by the precision of the retriever: many retrieved samples
fed into the generation phase are irrelevant or misleading, posing a critical
bottleneck to LLMs' performance. To address this challenge, we introduce
VaccineRAG, a novel Chain-of-Thought-based retrieval-augmented generation
dataset. On one hand, VaccineRAG employs a benchmark to evaluate models using
data with varying positive/negative sample ratios, systematically exposing
inherent weaknesses in current LLMs. On the other hand, it enhances models'
sample-discrimination capabilities by prompting LLMs to generate explicit
Chain-of-Thought (CoT) analysis for each sample before producing final answers.
Furthermore, to enhance the model's ability to learn long-sequence complex CoT
content, we propose Partial-GRPO. By modeling the outputs of LLMs as multiple
components rather than a single whole, our model can make more informed
preference selections for complex sequences, thereby enhancing its capacity to
learn complex CoT. Comprehensive evaluations and ablation studies on VaccineRAG
validate the effectiveness of the proposed scheme. The code and dataset will be
publicly released soon.

</details>


### [57] [Behavioral Fingerprinting of Large Language Models](https://arxiv.org/abs/2509.04504)
*Zehua Pei,Hui-Ling Zhen,Ying Zhang,Zhiyuan Yang,Xing Li,Xianzhi Yu,Mingxuan Yuan,Bei Yu*

Main category: cs.CL

TL;DR: The paper proposes a novel evaluation framework, 'Behavioral Fingerprinting,' to analyze the behavioral characteristics of large language models (LLMs) beyond traditional performance metrics.


<details>
  <summary>Details</summary>
Motivation: Current LLM benchmarks fail to emphasize the nuanced behaviors of models and focus heavily on performance, necessitating new methodologies to uncover intrinsic cognitive and interactive styles.

Method: The authors employ a 'Behavioral Fingerprinting' framework using a diagnostic prompt suite and an automated evaluation process where a high-performing LLM serves as the judge. They analyze 18 models' behaviors and capabilities through this pipeline.

Result: The study finds that core reasoning abilities among LLMs converge at higher capability levels, but alignment-related behaviors (e.g., sycophancy, semantic robustness) differ significantly. Additionally, models exhibit default behavioral clusters (e.g., ISTJ/ESTJ personas) influenced by developers' alignment strategies.

Conclusion: Behavioral characteristics of LLMs arise from deliberate alignment strategies rather than their scale or reasoning abilities. The proposed framework offers a scalable way to evaluate and differentiate LLM behaviors.

Abstract: Current benchmarks for Large Language Models (LLMs) primarily focus on
performance metrics, often failing to capture the nuanced behavioral
characteristics that differentiate them. This paper introduces a novel
``Behavioral Fingerprinting'' framework designed to move beyond traditional
evaluation by creating a multi-faceted profile of a model's intrinsic cognitive
and interactive styles. Using a curated \textit{Diagnostic Prompt Suite} and an
innovative, automated evaluation pipeline where a powerful LLM acts as an
impartial judge, we analyze eighteen models across capability tiers. Our
results reveal a critical divergence in the LLM landscape: while core
capabilities like abstract and causal reasoning are converging among top
models, alignment-related behaviors such as sycophancy and semantic robustness
vary dramatically. We further document a cross-model default persona clustering
(ISTJ/ESTJ) that likely reflects common alignment incentives. Taken together,
this suggests that a model's interactive nature is not an emergent property of
its scale or reasoning power, but a direct consequence of specific, and highly
variable, developer alignment strategies. Our framework provides a reproducible
and scalable methodology for uncovering these deep behavioral differences.
Project: https://github.com/JarvisPei/Behavioral-Fingerprinting

</details>


### [58] [From Silent Signals to Natural Language: A Dual-Stage Transformer-LLM Approach](https://arxiv.org/abs/2509.04507)
*Nithyashree Sivasubramaniam*

Main category: cs.CL

TL;DR: The paper proposes an enhanced automatic speech recognition framework for Silent Speech Interfaces (SSIs) combining transformer-based acoustic models with large language models to reduce phonetic ambiguity and ensure linguistic consistency, achieving significant error rate reduction.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of phonetic ambiguity and noise in synthesized speech from Silent Speech Interfaces (SSIs) that hinder recognition and downstream processing.

Method: The method employs a transformer-based acoustic model to capture full utterance context and integrates a large language model (LLM) for post-processing to ensure linguistic consistency.

Result: The proposed framework achieves a 16% relative and 6% absolute reduction in word error rate, showing significant improvements over the baseline.

Conclusion: The integration of transformer-based models with LLMs enhances intelligibility and robustness of speech recognition in Silent Speech Interfaces.

Abstract: Silent Speech Interfaces (SSIs) have gained attention for their ability to
generate intelligible speech from non-acoustic signals. While significant
progress has been made in advancing speech generation pipelines, limited work
has addressed the recognition and downstream processing of synthesized speech,
which often suffers from phonetic ambiguity and noise. To overcome these
challenges, we propose an enhanced automatic speech recognition framework that
combines a transformer-based acoustic model with a large language model (LLM)
for post-processing. The transformer captures full utterance context, while the
LLM ensures linguistic consistency. Experimental results show a 16% relative
and 6% absolute reduction in word error rate (WER) over a 36% baseline,
demonstrating substantial improvements in intelligibility for silent speech
interfaces.

</details>


### [59] [ProST: Progressive Sub-task Training for Pareto-Optimal Multi-agent Systems Using Small Language Models](https://arxiv.org/abs/2509.04508)
*Biddut Sarker Bijoy,Mohammad Saqib Hasan,Pegah Alipoormolabashi,Avirup Sil,Aruna Balasubramanian,Niranjan Balasubramanian*

Main category: cs.CL

TL;DR: The study compares single-agent large language models (LLMs) and multi-agent small language models (SLMs) for complex tasks, proposing a progressive sub-task training strategy which improves the performance of SLM-based systems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore whether multi-agent systems using smaller language models can perform as effectively and efficiently as single-agent large language models for complex problem-solving.

Method: The authors analyzed single and multi-agent setups in the AppWorld environment, employing a progressive sub-task training strategy to address issues faced by SLMs in long-trajectory learning.

Result: The proposed strategy improved SLM-based multi-agent systems, leading to better effectiveness, reduced subtask errors, and a favorable effectiveness-efficiency trade-off as demonstrated through Pareto analysis.

Conclusion: Progressive sub-task training enhances the utility of multi-agent systems with smaller language models, making them a competitive alternative to LLM-driven single-agent systems for certain tasks.

Abstract: Multi-agent systems with smaller language models (SLMs) present a viable
alternative to single agent systems powered by large language models (LLMs) for
addressing complex problems. In this work, we study how these alternatives
compare in terms of both effectiveness and efficiency. To study this trade-off,
we instantiate single and multi-agent systems for the complex problems in the
AppWorld environment using different sized language models.
  We find that difficulties with long-trajectory learning in smaller language
models (SLMs) limit their performance. Even when trained for specialized roles,
SLMs fail to learn all subtasks effectively. To address this issue, we
introduce a simple progressive sub-task training strategy, which introduces new
sub-tasks progressively in each training epoch. We find that this novel
strategy, analogous to instance level curriculum learning, consistently
improves the effectiveness of multi-agents at all configurations. Our Pareto
analysis shows that fine-tuned multi-agent systems yield better
effectiveness-efficiency trade-offs. Additional ablations and analyses shows
the importance of our progressive training strategy and its ability to reduce
subtask error rates.

</details>


### [60] [Combine Virtual Reality and Machine-Learning to Identify the Presence of Dyslexia: A Cross-Linguistic Approach](https://arxiv.org/abs/2509.04510)
*Michele Materazzini,Gianluca Morciano,Jose Manuel Alcalde-Llergo,Enrique Yeguas-Bolivar,Giuseppe Calabro,Andrea Zingoni,Juri Taborri*

Main category: cs.CL

TL;DR: This research utilizes VR and ML to assess dyslexia in Italian and Spanish university students using Silent Reading tests and self-esteem tasks, achieving accuracy rates of 87.5%, 66.6%, and 75% across groups.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore innovative approaches, such as VR and AI, to diagnose dyslexia in a more engaging and data-driven manner, leveraging potential advancements in neuropsychological assessment.

Method: Participants undertook VR-based tests for Silent Reading and self-esteem. Statistical differences were analyzed using tests, followed by training supervised ML models to classify dyslexia from resulting data.

Result: The ML models classified dyslexia with an accuracy of 87.5% for Italian, 66.6% for Spanish, and 75% when considering both groups pooled together. Task completion time differences played a critical role.

Conclusion: VR and ML show promise as assessment tools for dyslexia, especially by focusing on reading task completion speed, though linguistic and cultural factors may require consideration to enhance accuracy.

Abstract: This study explores the use of virtual reality (VR) and artificial
intelligence (AI) to predict the presence of dyslexia in Italian and Spanish
university students. In particular, the research investigates whether
VR-derived data from Silent Reading (SR) tests and self-esteem assessments can
differentiate between students that are affected by dyslexia and students that
are not, employing machine learning (ML) algorithms. Participants completed
VR-based tasks measuring reading performance and self-esteem. A preliminary
statistical analysis (t tests and Mann Whitney tests) on these data was
performed, to compare the obtained scores between individuals with and without
dyslexia, revealing significant differences in completion time for the SR test,
but not in accuracy, nor in self esteem. Then, supervised ML models were
trained and tested, demonstrating an ability to classify the presence/absence
of dyslexia with an accuracy of 87.5 per cent for Italian, 66.6 per cent for
Spanish, and 75.0 per cent for the pooled group. These findings suggest that VR
and ML can effectively be used as supporting tools for assessing dyslexia,
particularly by capturing differences in task completion speed, but
language-specific factors may influence classification accuracy.

</details>


### [61] [Scaling behavior of large language models in emotional safety classification across sizes and tasks](https://arxiv.org/abs/2509.04512)
*Edoardo Pinzuti,Oliver Tüscher,André Ferreira Castro*

Main category: cs.CL

TL;DR: The paper examines how large language models (LLMs) handle emotional safety in mental health contexts by testing their classification capabilities on a new dataset. Key findings show that while larger models perform better, smaller, fine-tuned models can be efficient alternatives for sensitive applications.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand and improve how LLMs process emotionally sensitive content to create safe, reliable systems, especially for mental health applications.

Method: The study creates a novel dataset from existing mental health datasets (>15K samples) and ChatGPT-generated prompts, then evaluates four LLaMA models of varying sizes across zero-shot, few-shot, and fine-tuning tasks for emotion classification.

Result: Larger LLMs showed better performance in nuanced tasks, especially in the zero-shot setting. However, fine-tuning smaller models (e.g., 1B LLaMA) allowed them to reach performance levels comparable to larger models while maintaining computational efficiency.

Conclusion: Smaller, fine-tuned LLMs provide viable, privacy-preserving alternatives for sensitive applications, offering strong performance in emotional context interpretation and scalability for safety-critical environments.

Abstract: Understanding how large language models (LLMs) process emotionally sensitive
content is critical for building safe and reliable systems, particularly in
mental health contexts. We investigate the scaling behavior of LLMs on two key
tasks: trinary classification of emotional safety (safe vs. unsafe vs.
borderline) and multi-label classification using a six-category safety risk
taxonomy. To support this, we construct a novel dataset by merging several
human-authored mental health datasets (> 15K samples) and augmenting them with
emotion re-interpretation prompts generated via ChatGPT. We evaluate four LLaMA
models (1B, 3B, 8B, 70B) across zero-shot, few-shot, and fine-tuning settings.
Our results show that larger LLMs achieve stronger average performance,
particularly in nuanced multi-label classification and in zero-shot settings.
However, lightweight fine-tuning allowed the 1B model to achieve performance
comparable to larger models and BERT in several high-data categories, while
requiring <2GB VRAM at inference. These findings suggest that smaller,
on-device models can serve as viable, privacy-preserving alternatives for
sensitive applications, offering the ability to interpret emotional context and
maintain safe conversational boundaries. This work highlights key implications
for therapeutic LLM applications and the scalable alignment of safety-critical
systems.

</details>


### [62] [Mitigation of Gender and Ethnicity Bias in AI-Generated Stories through Model Explanations](https://arxiv.org/abs/2509.04515)
*Martha O. Dimgba,Sharon Oba,Ameeta Agrawal,Philippe J. Giabbanelli*

Main category: cs.CL

TL;DR: This paper addresses biases in AI-generated occupational stories, focusing on gender and ethnicity. It proposes and evaluates BAME, a mitigation method leveraging model explanations to improve demographic representation.


<details>
  <summary>Details</summary>
Motivation: Investigate the presence and mitigation of social biases in AI-generated outputs, particularly in occupational contexts, to enhance fairness and transparency.

Method: Using BAME (Bias Analysis and Mitigation through Explanation), the paper uses model-generated explanations to guide prompt engineering, analyzing outcomes across 25 jobs and multiple models.

Result: Improvements in demographic representation ranged from 2% to 20%, showing reduced biases without altering model parameters.

Conclusion: The study highlights that leveraging models' internal reasoning can significantly enhance fairness in demographic representation, fostering more transparent AI systems.

Abstract: Language models have been shown to propagate social bias through their
output, particularly in the representation of gender and ethnicity. This paper
investigates gender and ethnicity biases in AI-generated occupational stories.
Representation biases are measured before and after applying our proposed
mitigation strategy, Bias Analysis and Mitigation through Explanation (BAME),
revealing improvements in demographic representation ranging from 2% to 20%.
BAME leverages model-generated explanations to inform targeted prompt
engineering, effectively reducing biases without modifying model parameters. By
analyzing stories generated across 25 occupational groups, three large language
models (Claude 3.5 Sonnet, Llama 3.1 70B Instruct, and GPT-4 Turbo), and
multiple demographic dimensions, we identify persistent patterns of
overrepresentation and underrepresentation linked to training data stereotypes.
Our findings demonstrate that guiding models with their own internal reasoning
mechanisms can significantly enhance demographic parity, thereby contributing
to the development of more transparent generative AI systems.

</details>


### [63] [Artificially Fluent: Swahili AI Performance Benchmarks Between English-Trained and Natively-Trained Datasets](https://arxiv.org/abs/2509.04516)
*Sophie Jaffer,Simeon Sayer*

Main category: cs.CL

TL;DR: This paper compares Swahili and English language models' performance, finding that native-language models outperform translations due to internal knowledge representation differences.


<details>
  <summary>Details</summary>
Motivation: Investigate whether large language models trained predominantly in English disadvantage speakers of underrepresented languages like Swahili.

Method: Two BERT models were used: one trained in Swahili data and another trained in English. Swahili data was also translated into English for testing against the English-trained model.

Result: The Swahili-trained model produced four times fewer errors (0.36% error rate) compared to the Swahili-to-English translated model (1.47% error rate).

Conclusion: Native-language training is crucial as translation does not fully address linguistic representation disparities. Broader multilingual datasets and improved evaluation methods are needed to reduce AI-induced inequalities.

Abstract: As large language models (LLMs) expand multilingual capabilities, questions
remain about the equity of their performance across languages. While many
communities stand to benefit from AI systems, the dominance of English in
training data risks disadvantaging non-English speakers. To test the hypothesis
that such data disparities may affect model performance, this study compares
two monolingual BERT models: one trained and tested entirely on Swahili data,
and another on comparable English news data. To simulate how multilingual LLMs
process non-English queries through internal translation and abstraction, we
translated the Swahili news data into English and evaluated it using the
English-trained model. This approach tests the hypothesis by evaluating whether
translating Swahili inputs for evaluation on an English model yields better or
worse performance compared to training and testing a model entirely in Swahili,
thus isolating the effect of language consistency versus cross-lingual
abstraction. The results prove that, despite high-quality translation, the
native Swahili-trained model performed better than the Swahili-to-English
translated model, producing nearly four times fewer errors: 0.36% vs. 1.47%
respectively. This gap suggests that translation alone does not bridge
representational differences between languages and that models trained in one
language may struggle to accurately interpret translated inputs due to
imperfect internal knowledge representation, suggesting that native-language
training remains important for reliable outcomes. In educational and
informational contexts, even small performance gaps may compound inequality.
Future research should focus on addressing broader dataset development for
underrepresented languages and renewed attention to multilingual model
evaluation, ensuring the reinforcing effect of global AI deployment on existing
digital divides is reduced.

</details>


### [64] [Analysis of Voluntarily Reported Data Post Mesh Implantation for Detecting Public Emotion and Identifying Concern Reports](https://arxiv.org/abs/2509.04517)
*Indu Bala,Lewis Mitchell,Marianne H Gillam*

Main category: cs.CL

TL;DR: The paper uses sentiment and emotion analysis on patient reports regarding hernia mesh implants to understand emotional patterns and highlight concerns over time.


<details>
  <summary>Details</summary>
Motivation: To address patient emotional concerns and complications following hernia mesh implant surgeries, aiming to improve healthcare practices and preoperative counselling.

Method: Analyzed patient narratives from the MAUDE database using NLP techniques, including the NRC Emotion Lexicon and TextBlob, categorizing emotions and sentiment polarity.

Result: The study identified increased emotional intensity and Concern Reports during specific timeframes (2011-2012 and 2017-2018), correlating with regulatory and technological healthcare changes.

Conclusion: Sentiment analysis is a valuable tool for understanding patient experiences, aiding healthcare practitioners in enhancing patient care in mesh implant surgeries.

Abstract: Mesh implants are widely utilized in hernia repair surgeries, but
postoperative complications present a significant concern. This study analyzes
patient reports from the Manufacturer and User Facility Device Experience
(MAUDE) database spanning 2000 to 2021 to investigate the emotional aspects of
patients following mesh implantation using Natural Language Processing (NLP).
Employing the National Research Council Canada (NRC) Emotion Lexicon and
TextBlob for sentiment analysis, the research categorizes patient narratives
into eight emotions (anger, fear, anticipation, trust, surprise, sadness, joy,
and disgust) and assesses sentiment polarity. The goal is to discern patterns
in patient sentiment over time and to identify reports signaling urgent
concerns, referred to as "Concern Reports," thereby understanding shifts in
patient experiences in relation to changes in medical device regulation and
technological advancements in healthcare. The study detected an increase in
Concern Reports and higher emotional intensity during the periods of 2011-2012
and 2017-2018. Through temporal analysis of Concern Reports and overall
sentiment, this research provides valuable insights for healthcare
practitioners, enhancing their understanding of patient experiences
post-surgery, which is critical for improving preoperative counselling,
postoperative care, and preparing patients for mesh implant surgeries. The
study underscores the importance of emotional considerations in medical
practices and the potential for sentiment analysis to inform and enhance
patient care.

</details>


### [65] [Advancing SLM Tool-Use Capability using Reinforcement Learning](https://arxiv.org/abs/2509.04518)
*Dhruvi Paprunia,Vansh Kharidia,Pankti Doshi*

Main category: cs.CL

TL;DR: This paper uses reinforcement learning to improve the tool-use capabilities of Small Language Models (SLMs), aiming to make them more efficient alternatives to computationally expensive Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) excel at using tools for real-world tasks but are limited by computational complexity and resource needs. There is a demand for improving Small Language Models (SLMs) to make them more practical and efficient in tool use.

Method: The study implements Group Relative Policy Optimization (GRPO), a reinforcement learning approach, to enhance tool-use proficiency in SLMs, avoiding the computational demands of traditional fine-tuning methods.

Result: The approach significantly boosted tool-use accuracy in SLMs, improving their practical utility.

Conclusion: Using reinforcement learning, specifically the GRPO method, successfully enhances the tool-using abilities of SLMs, making them competitive alternatives to larger and resource-intensive LLMs for certain applications.

Abstract: Large Language Models (LLMs) have progressed beyond simple text creation, and
tool use has become increasingly important for complex, real-world tasks. Tool
use in LLMs refers to their ability to utilize external resources such as APIs,
databases, or software functions to extend their functionality beyond
generating text.Tools are used for tasks such as performing calculations,
making API calls to retrieve the current time and date, and more. This
capability enables models to fetch real-time data, execute commands, or solve
problems requiring dynamic interaction, making it indispensable for
applications like AI agents in virtual assistants, robotic control, or
automated workflows.
  However, while LLMs are usually adept tool use, their vast resource
requirements and computation complexity restrict their use in every use case.As
a result, there is an increasing need for more compact and efficient Small
Language Models (SLMs). Small language models (SLMs) struggle in tool use
compared to large language models (LLMs). As soon in Table 1. SLMs are
typically trained on smaller, more specific datasets, resulting in a narrower
knowledge base and limited contextual understanding compared to LLMs.
  This research addresses these challenges by using Reinforcement Learning
(RL), specifically Group Relative Policy Optimization (GRPO), to enhance
tool-use proficiency in SLMs. Unlike conventional fine-tuning approaches that
require heavy computation and often lack adaptability, our method provides an
efficient, effective solution that significantly boosts SLM tool-use accuracy,
increasing their practical utility.

</details>


### [66] [Hierarchical Section Matching Prediction (HSMP) BERT for Fine-Grained Extraction of Structured Data from Hebrew Free-Text Radiology Reports in Crohn's Disease](https://arxiv.org/abs/2509.04519)
*Zvi Badash,Hadas Ben-Atya,Naama Gavrielov,Liam Hazan,Gili Focht,Ruth Cytter-Kuint,Talar Hagopian,Dan Turner,Moti Freiman*

Main category: cs.CL

TL;DR: The paper introduces HSMP-BERT, a prompt-based model, to extract structured clinical information from Hebrew radiology reports, specifically for Crohn's disease. The model shows high performance in accuracy metrics and scalability.


<details>
  <summary>Details</summary>
Motivation: The study aims to address challenges in extracting detailed clinical data from radiology reports in low-resource languages, particularly targeting a rarely analyzed condition, Crohn's disease.

Method: HSMP-BERT, a hierarchical and prompt-based model, was trained on radiologist-annotated data of 512 reports and evaluated across a multilabel dataset based on diverse performance metrics.

Result: The model significantly outperforms baseline models, showcasing high accuracy (mean F1: 0.83±0.08) and κ-score (0.65±0.17), while also demonstrating efficiency in runtime reductions.

Conclusion: HSMP-BERT serves as a scalable computational tool for structured clinical extraction, enabling population-level analyses of Crohn's disease, especially in low-resource linguistic settings.

Abstract: Extracting structured clinical information from radiology reports is
challenging, especially in low-resource languages. This is pronounced in
Crohn's disease, with sparsely represented multi-organ findings. We developed
Hierarchical Structured Matching Prediction BERT (HSMP-BERT), a prompt-based
model for extraction from Hebrew radiology text. In an administrative database
study, we analyzed 9,683 reports from Crohn's patients imaged 2010-2023 across
Israeli providers. A subset of 512 reports was radiologist-annotated for
findings across six gastrointestinal organs and 15 pathologies, yielding 90
structured labels per subject. Multilabel-stratified split (66%
train+validation; 33% test), preserving label prevalence. Performance was
evaluated with accuracy, F1, Cohen's $\kappa$, AUC, PPV, NPV, and recall. On 24
organ-finding combinations with $>$15 positives, HSMP-BERT achieved mean F1
0.83$\pm$0.08 and $\kappa$ 0.65$\pm$0.17, outperforming the SMP zero-shot
baseline (F1 0.49$\pm$0.07, $\kappa$ 0.06$\pm$0.07) and standard fine-tuning
(F1 0.30$\pm$0.27, $\kappa$ 0.27$\pm$0.34; paired t-test $p < 10^{-7}$).
Hierarchical inference cuts runtime 5.1$\times$ vs. traditional inference.
Applied to all reports, it revealed associations among ileal wall thickening,
stenosis, and pre-stenotic dilatation, plus age- and sex-specific trends in
inflammatory findings. HSMP-BERT offers a scalable solution for structured
extraction in radiology, enabling population-level analysis of Crohn's disease
and demonstrating AI's potential in low-resource settings.

</details>


### [67] [Using LLMs to create analytical datasets: A case study of reconstructing the historical memory of Colombia](https://arxiv.org/abs/2509.04523)
*David Anderson,Galia Benitez,Margret Bjarnadottir,Shriyan Reyya*

Main category: cs.CL

TL;DR: The paper uses GPT, a large language model, to analyze over 200,000 violence-related newspaper articles for historical memory and policy analysis in Colombia.


<details>
  <summary>Details</summary>
Motivation: Colombia's lack of systematic documentation of violence has created gaps in public conflict information and historical accounts.

Method: Employing GPT to process and analyze violence-related newspaper articles in Spanish for descriptive analysis and policy insights.

Result: A comprehensive dataset and insights regarding the relationship between violence and coca crop eradication have been produced.

Conclusion: LLMs like GPT unlock new possibilities for deep analysis of extensive text corpora, overcoming previous limitations in research.

Abstract: Colombia has been submerged in decades of armed conflict, yet until recently,
the systematic documentation of violence was not a priority for the Colombian
government. This has resulted in a lack of publicly available conflict
information and, consequently, a lack of historical accounts. This study
contributes to Colombia's historical memory by utilizing GPT, a large language
model (LLM), to read and answer questions about over 200,000 violence-related
newspaper articles in Spanish. We use the resulting dataset to conduct both
descriptive analysis and a study of the relationship between violence and the
eradication of coca crops, offering an example of policy analyses that such
data can support. Our study demonstrates how LLMs have opened new research
opportunities by enabling examinations of large text corpora at a previously
infeasible depth.

</details>


### [68] [Quantized Large Language Models in Biomedical Natural Language Processing: Evaluation and Recommendation](https://arxiv.org/abs/2509.04534)
*Zaifu Zhan,Shuang Zhou,Min Zeng,Kai Yu,Meijia Song,Xiaoyi Chen,Jun Wang,Yu Hou,Rui Zhang*

Main category: cs.CL

TL;DR: The paper examines how quantizing large language models (LLMs) for biomedical applications can significantly reduce GPU memory needs without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges in deploying large language models in healthcare, where data privacy and limited resources make it difficult to adopt existing cloud-based AI models in clinical settings.

Method: The authors systematically evaluated the impact of quantization on 12 state-of-the-art general-purpose and biomedical-specific language models across 8 benchmark datasets for tasks such as named entity recognition, relation extraction, multi-label classification, and question answering.

Result: Quantization reduced GPU memory usage by up to 75% while maintaining model performance. It enabled deployment of 70B-parameter models on consumer-grade GPUs, with domain-specific knowledge and responsiveness to prompts largely preserved.

Conclusion: Quantization is a practical approach for deploying large language models securely and locally in biomedical applications, promoting real-world clinical translation without compromising performance.

Abstract: Large language models have demonstrated remarkable capabilities in biomedical
natural language processing, yet their rapid growth in size and computational
requirements present a major barrier to adoption in healthcare settings where
data privacy precludes cloud deployment and resources are limited. In this
study, we systematically evaluated the impact of quantization on 12
state-of-the-art large language models, including both general-purpose and
biomedical-specific models, across eight benchmark datasets covering four key
tasks: named entity recognition, relation extraction, multi-label
classification, and question answering. We show that quantization substantially
reduces GPU memory requirements-by up to 75%-while preserving model performance
across diverse tasks, enabling the deployment of 70B-parameter models on 40GB
consumer-grade GPUs. In addition, domain-specific knowledge and responsiveness
to advanced prompting methods are largely maintained. These findings provide
significant practical and guiding value, highlighting quantization as a
practical and effective strategy for enabling the secure, local deployment of
large yet high-capacity language models in biomedical contexts, bridging the
gap between technical advances in AI and real-world clinical translation.

</details>


### [69] [Manipulating Transformer-Based Models: Controllability, Steerability, and Robust Interventions](https://arxiv.org/abs/2509.04549)
*Faruk Alpay,Taylan Alpay*

Main category: cs.CL

TL;DR: This paper addresses the challenge of fine-grained control in transformer-based language models by investigating intervention methods at the levels of prompts, activations, and weights within a unified framework.


<details>
  <summary>Details</summary>
Motivation: The motivation was to improve fine-grained controllability of transformer-based language models, which is essential for targeted behavior changes while maintaining base performance.

Method: The paper employs methods like prompt engineering, parameter-efficient fine-tuning, model editing, and reinforcement learning, and presents a unified framework for interventions at prompt, activation, and weight levels.

Result: Results demonstrated over 90% success in sentiment control and factual editing tasks with minimal side effects, while preserving the model's base performance.

Conclusion: The study highlights the potential for designing controllable and robust language models, while emphasizing the trade-offs, ethical risks, and the importance of rigorous evaluation.

Abstract: Transformer-based language models excel in NLP tasks, but fine-grained
control remains challenging. This paper explores methods for manipulating
transformer models through principled interventions at three levels: prompts,
activations, and weights. We formalize controllable text generation as an
optimization problem addressable via prompt engineering, parameter-efficient
fine-tuning, model editing, and reinforcement learning. We introduce a unified
framework encompassing prompt-level steering, activation interventions, and
weight-space edits. We analyze robustness and safety implications, including
adversarial attacks and alignment mitigations. Theoretically, we show minimal
weight updates can achieve targeted behavior changes with limited side-effects.
Empirically, we demonstrate >90% success in sentiment control and factual edits
while preserving base performance, though generalization-specificity trade-offs
exist. We discuss ethical dual-use risks and the need for rigorous evaluation.
This work lays groundwork for designing controllable and robust language
models.

</details>


### [70] [Spoken in Jest, Detected in Earnest: A Systematic Review of Sarcasm Recognition -- Multimodal Fusion, Challenges, and Future Prospects](https://arxiv.org/abs/2509.04605)
*Xiyuan Gao,Shekhar Nayak,Matt Coler*

Main category: cs.CL

TL;DR: The paper investigates speech-based sarcasm recognition, transitioning from unimodal to multimodal approaches with a focus on datasets, feature extraction, and classification methods.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the underexplored role of speech in sarcasm recognition, in contrast to text-based approaches, and aims to improve human-machine interactions and assist individuals with communication challenges.

Method: A systematic review strategy was employed, analyzing advancements in sarcasm recognition including data limitations, traditional and deep learning feature extraction techniques, and progression to multimodal fusion classification.

Result: The review highlights limitations in speech datasets, the evolution of feature extraction from acoustic to deep learning-based methods, and the shift from unimodal to multimodal classification techniques.

Conclusion: Speech-based sarcasm recognition needs cross-cultural, multilingual perspectives and a focus on sarcasm as a multimodal phenomenon for improved applications in both human and machine communication.

Abstract: Sarcasm, a common feature of human communication, poses challenges in
interpersonal interactions and human-machine interactions. Linguistic research
has highlighted the importance of prosodic cues, such as variations in pitch,
speaking rate, and intonation, in conveying sarcastic intent. Although previous
work has focused on text-based sarcasm detection, the role of speech data in
recognizing sarcasm has been underexplored. Recent advancements in speech
technology emphasize the growing importance of leveraging speech data for
automatic sarcasm recognition, which can enhance social interactions for
individuals with neurodegenerative conditions and improve machine understanding
of complex human language use, leading to more nuanced interactions. This
systematic review is the first to focus on speech-based sarcasm recognition,
charting the evolution from unimodal to multimodal approaches. It covers
datasets, feature extraction, and classification methods, and aims to bridge
gaps across diverse research domains. The findings include limitations in
datasets for sarcasm recognition in speech, the evolution of feature extraction
techniques from traditional acoustic features to deep learning-based
representations, and the progression of classification methods from unimodal
approaches to multimodal fusion techniques. In so doing, we identify the need
for greater emphasis on cross-cultural and multilingual sarcasm recognition, as
well as the importance of addressing sarcasm as a multimodal phenomenon, rather
than a text-based challenge.

</details>


### [71] [Sample-efficient Integration of New Modalities into Large Language Models](https://arxiv.org/abs/2509.04606)
*Osman Batur İnce,André F. T. Martins,Oisin Mac Aodha,Edoardo M. Ponti*

Main category: cs.CL

TL;DR: The paper proposes a method called SEMI for integrating new modalities into Large Language Models (LLMs) with minimal data.


<details>
  <summary>Details</summary>
Motivation: Foundation models struggle to incorporate all possible modalities due to evolving and large variety, and current integration methods often require extensive paired data that is scarce.

Method: The paper introduces a hypernetwork that adapts a shared projector, which connects modality-specific encoders with LLMs, by conditioning on a few samples from any arbitrary modality.

Result: SEMI achieves a significant improvement in sample efficiency, allowing new modalities to be integrated using substantially less data compared to traditional training methods.

Conclusion: SEMI shows potential for expanding the modality capability of foundation models efficiently, making them more adaptable to diverse and low-resource modalities.

Abstract: Multimodal foundation models can process several modalities. However, since
the space of possible modalities is large and evolving over time, training a
model from scratch to encompass all modalities is unfeasible. Moreover,
integrating a modality into a pre-existing foundation model currently requires
a significant amount of paired data, which is often not available for
low-resource modalities. In this paper, we introduce a method for
sample-efficient modality integration (SEMI) into Large Language Models (LLMs).
To this end, we devise a hypernetwork that can adapt a shared projector --
placed between modality-specific encoders and an LLM -- to any modality. The
hypernetwork, trained on high-resource modalities (i.e., text, speech, audio,
video), is conditioned on a few samples from any arbitrary modality at
inference time to generate a suitable adapter. To increase the diversity of
training modalities, we artificially multiply the number of encoders through
isometric transformations. We find that SEMI achieves a significant boost in
sample efficiency during few-shot integration of new modalities (i.e.,
satellite images, astronomical images, inertial measurements, and molecules)
with encoders of arbitrary embedding dimensionality. For instance, to reach the
same accuracy as 32-shot SEMI, training the projector from scratch needs
64$\times$ more data. As a result, SEMI holds promise to extend the modality
coverage of foundation models.

</details>


### [72] [Breaking to Build: A Threat Model of Prompt-Based Attacks for Securing LLMs](https://arxiv.org/abs/2509.04615)
*Brennen Hill,Surendra Parla,Venkata Abhijeeth Balabhadruni,Atharv Prajod Padmalayam,Sujay Chandra Shekara Sharma*

Main category: cs.CL

TL;DR: The paper investigates security threats in Large Language Models (LLMs) caused by prompt-based attacks and offers a categorized survey to bolster future security measures.


<details>
  <summary>Details</summary>
Motivation: To address the escalating security vulnerabilities posed by adversarial prompt manipulations in LLMs, which can lead to intellectual property theft and misinformation.

Method: A comprehensive literature review is conducted to categorize and analyze prompt-based attack methodologies, providing a clear threat model for LLM vulnerabilities.

Result: The survey identifies various mechanisms and impacts of prompt-based attacks, classifying different exploit strategies.

Conclusion: The paper serves as a cornerstone for developing secure LLMs by systematically understanding prompt-based attack techniques and informing countermeasure development.

Abstract: The proliferation of Large Language Models (LLMs) has introduced critical
security challenges, where adversarial actors can manipulate input prompts to
cause significant harm and circumvent safety alignments. These prompt-based
attacks exploit vulnerabilities in a model's design, training, and contextual
understanding, leading to intellectual property theft, misinformation
generation, and erosion of user trust. A systematic understanding of these
attack vectors is the foundational step toward developing robust
countermeasures. This paper presents a comprehensive literature survey of
prompt-based attack methodologies, categorizing them to provide a clear threat
model. By detailing the mechanisms and impacts of these exploits, this survey
aims to inform the research community's efforts in building the next generation
of secure LLMs that are inherently resistant to unauthorized distillation,
fine-tuning, and editing.

</details>


### [73] [Comparative Analysis of Transformer Models in Disaster Tweet Classification for Public Safety](https://arxiv.org/abs/2509.04650)
*Sharif Noor Zisad,Ragib Hasan*

Main category: cs.CL

TL;DR: The paper explores transformer-based models like BERT for classifying disaster-related tweets, showing superior performance (91%) compared to traditional ML methods.


<details>
  <summary>Details</summary>
Motivation: Emergency services require accurate and rapid classification of disaster-related social media posts, but traditional ML models struggle with informal or ambiguous language.

Method: The authors compare transformer-based models (e.g., BERT, DistilBERT) with traditional ML methods for tweet classification, emphasizing contextual embeddings and attention mechanisms.

Result: Experimental results reveal that BERT achieves the highest accuracy (91%), outperforming logistic regression and naive Bayes at 82%.

Conclusion: Transformer models are highly effective for public safety tweet classification, offering better generalization and understanding compared to traditional ML approaches.

Abstract: Twitter and other social media platforms have become vital sources of real
time information during disasters and public safety emergencies. Automatically
classifying disaster related tweets can help emergency services respond faster
and more effectively. Traditional Machine Learning (ML) models such as Logistic
Regression, Naive Bayes, and Support Vector Machines have been widely used for
this task, but they often fail to understand the context or deeper meaning of
words, especially when the language is informal, metaphorical, or ambiguous. We
posit that, in this context, transformer based models can perform better than
traditional ML models. In this paper, we evaluate the effectiveness of
transformer based models, including BERT, DistilBERT, RoBERTa, and DeBERTa, for
classifying disaster related tweets. These models are compared with traditional
ML approaches to highlight the performance gap. Experimental results show that
BERT achieved the highest accuracy (91%), significantly outperforming
traditional models like Logistic Regression and Naive Bayes (both at 82%). The
use of contextual embeddings and attention mechanisms allows transformer models
to better understand subtle language in tweets, where traditional ML models
fall short. This research demonstrates that transformer architectures are far
more suitable for public safety applications, offering improved accuracy,
deeper language understanding, and better generalization across real world
social media text.

</details>


### [74] [Polysemantic Dropout: Conformal OOD Detection for Specialized LLMs](https://arxiv.org/abs/2509.04655)
*Ayush Gupta,Ramneet Kaur,Anirban Roy,Adam D. Cobb,Rama Chellappa,Susmit Jha*

Main category: cs.CL

TL;DR: The paper introduces a novel inference-time technique for detecting out-of-domain (OOD) inputs in fine-tuned large language models (LLMs), achieving significant improvement in accuracy.


<details>
  <summary>Details</summary>
Motivation: Specialized LLMs, while excelling in in-domain tasks, are prone to unreliable outputs when encountering OOD inputs. This vulnerability poses risks in critical applications, necessitating reliable OOD detection methods.

Method: The paper utilizes the Inductive Conformal Anomaly Detection (ICAD) framework with a unique non-conformity measure based on dropout tolerance. This measure is aggregated across layers using a valid ensemble approach to enhance OOD detection accuracy.

Result: Experiments demonstrate AUROC improvements ranging from 2% to 37% in detecting OOD inputs compared to baseline methods, suggesting significant accuracy enhancements.

Conclusion: The proposed method provides a robust mechanism for OOD detection in specialized LLMs, enhancing reliability in critical applications while maintaining theoretical control on false alarms.

Abstract: We propose a novel inference-time out-of-domain (OOD) detection algorithm for
specialized large language models (LLMs). Despite achieving state-of-the-art
performance on in-domain tasks through fine-tuning, specialized LLMs remain
vulnerable to incorrect or unreliable outputs when presented with OOD inputs,
posing risks in critical applications. Our method leverages the Inductive
Conformal Anomaly Detection (ICAD) framework, using a new non-conformity
measure based on the model's dropout tolerance. Motivated by recent findings on
polysemanticity and redundancy in LLMs, we hypothesize that in-domain inputs
exhibit higher dropout tolerance than OOD inputs. We aggregate dropout
tolerance across multiple layers via a valid ensemble approach, improving
detection while maintaining theoretical false alarm bounds from ICAD.
Experiments with medical-specialized LLMs show that our approach detects OOD
inputs better than baseline methods, with AUROC improvements of $2\%$ to $37\%$
when treating OOD datapoints as positives and in-domain test datapoints as
negatives.

</details>


### [75] [AraHalluEval: A Fine-grained Hallucination Evaluation Framework for Arabic LLMs](https://arxiv.org/abs/2509.04656)
*Aisha Alansari,Hamzah Luqman*

Main category: cs.CL

TL;DR: The paper evaluates hallucinations in Arabic and multilingual LLMs for Generative Question Answering (GQA) and Summarization tasks using a framework with 12 indicators.


<details>
  <summary>Details</summary>
Motivation: Address the lack of research on hallucinations in Arabic and multilingual LLMs, given Arabic's global relevance and significance.

Method: An evaluation of 12 LLMs (Arabic, multilingual, reasoning-based) using a newly developed fine-grained hallucination evaluation framework with 12 indicators.

Result: Factual hallucinations are more common than faithfulness errors. The Arabic pre-trained model 'Allam' has lower hallucination rates compared to multilingual models and performs comparably to reasoning-based models.

Conclusion: Allam showcases the potential for Arabic-specific LLMs to outperform in factual consistency, highlighting the importance of tailored models.

Abstract: Recently, extensive research on the hallucination of the large language
models (LLMs) has mainly focused on the English language. Despite the growing
number of multilingual and Arabic-specific LLMs, evaluating LLMs' hallucination
in the Arabic context remains relatively underexplored. The knowledge gap is
particularly pressing given Arabic's widespread use across many regions and its
importance in global communication and media. This paper presents the first
comprehensive hallucination evaluation of Arabic and multilingual LLMs on two
critical Arabic natural language generation tasks: generative question
answering (GQA) and summarization. This study evaluates a total of 12 LLMs,
including 4 Arabic pre-trained models, 4 multilingual models, and 4
reasoning-based models. To assess the factual consistency and faithfulness of
LLMs' outputs, we developed a fine-grained hallucination evaluation framework
consisting of 12 fine-grained hallucination indicators that represent the
varying characteristics of each task. The results reveal that factual
hallucinations are more prevalent than faithfulness errors across all models
and tasks. Notably, the Arabic pre-trained model Allam consistently
demonstrates lower hallucination rates than multilingual models and a
comparative performance with reasoning-based models. The code is available at:
\href{https://github.com/aishaalansari57/AraHalluEval}{Github link}.

</details>


### [76] [Evaluating NL2SQL via SQL2NL](https://arxiv.org/abs/2509.04657)
*Mohammadtaher Safarzadeh,Afshin Oroojlooyjadid,Dan Roth*

Main category: cs.CL

TL;DR: The study proposes a controlled paraphrasing framework to analyze the performance robustness of Natural Language to SQL (NL2SQL) models against linguistic variation. It reveals notable drops in execution accuracy for state-of-the-art models, emphasizing the brittleness and the need for improved evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Existing NL2SQL benchmarks fail to systematically address robustness against linguistic variations, limiting understanding of model generalization capabilities.

Method: The authors introduced a schema-aligned paraphrasing framework using SQL-to-NL generation for creating lexically diverse yet semantically equivalent queries aligned with original schema and intents.

Result: State-of-the-art models showed significant drops in execution accuracy when tested with paraphrased queries, with some models like LLaMa3.3-70B losing 10.23% and LLaMa3.1-8B losing nearly 20%. Smaller models were even more affected, showing greater brittleness.

Conclusion: Models are less robust to linguistic variations than benchmarks indicate, especially with complex queries. Evaluation frameworks need to explicitly measure linguistic generalization to ensure reliability in real-world scenarios.

Abstract: Robust evaluation in the presence of linguistic variation is key to
understanding the generalization capabilities of Natural Language to SQL
(NL2SQL) models, yet existing benchmarks rarely address this factor in a
systematic or controlled manner. We propose a novel schema-aligned paraphrasing
framework that leverages SQL-to-NL (SQL2NL) to automatically generate
semantically equivalent, lexically diverse queries while maintaining alignment
with the original schema and intent. This enables the first targeted evaluation
of NL2SQL robustness to linguistic variation in isolation-distinct from prior
work that primarily investigates ambiguity or schema perturbations. Our
analysis reveals that state-of-the-art models are far more brittle than
standard benchmarks suggest. For example, LLaMa3.3-70B exhibits a 10.23% drop
in execution accuracy (from 77.11% to 66.9%) on paraphrased Spider queries,
while LLaMa3.1-8B suffers an even larger drop of nearly 20% (from 62.9% to
42.5%). Smaller models (e.g., GPT-4o mini) are disproportionately affected. We
also find that robustness degradation varies significantly with query
complexity, dataset, and domain -- highlighting the need for evaluation
frameworks that explicitly measure linguistic generalization to ensure reliable
performance in real-world settings.

</details>


### [77] [Why Language Models Hallucinate](https://arxiv.org/abs/2509.04664)
*Adam Tauman Kalai,Ofir Nachum,Santosh S. Vempala,Edwin Zhang*

Main category: cs.CL

TL;DR: This paper investigates why large language models (LLMs) hallucinate, attributing it to training and evaluation methods that reward guessing over admitting uncertainty. The authors argue this needs a change in evaluation scoring to build trustworthy AI.


<details>
  <summary>Details</summary>
Motivation: Large language models often produce incorrect statements (hallucinations), which undermines trust in AI systems. The study aims to uncover root causes and propose solutions to reduce these issues.

Method: The paper analyzes statistical causes of hallucinations in LLM training processes and scoring methods of evaluations, especially focusing on how rewarding guesses impacts behavior.

Result: The paper identifies that hallucinations stem from errors in binary classification and are reinforced by evaluation systems that penalize uncertain responses. It highlights that this misalignment encourages misleading outputs.

Conclusion: Reducing hallucinations calls for socio-technical changes, particularly in modifying how benchmarks are scored, to balance performance and trustworthiness in AI systems.

Abstract: Like students facing hard exam questions, large language models sometimes
guess when uncertain, producing plausible yet incorrect statements instead of
admitting uncertainty. Such "hallucinations" persist even in state-of-the-art
systems and undermine trust. We argue that language models hallucinate because
the training and evaluation procedures reward guessing over acknowledging
uncertainty, and we analyze the statistical causes of hallucinations in the
modern training pipeline. Hallucinations need not be mysterious -- they
originate simply as errors in binary classification. If incorrect statements
cannot be distinguished from facts, then hallucinations in pretrained language
models will arise through natural statistical pressures. We then argue that
hallucinations persist due to the way most evaluations are graded -- language
models are optimized to be good test-takers, and guessing when uncertain
improves test performance. This "epidemic" of penalizing uncertain responses
can only be addressed through a socio-technical mitigation: modifying the
scoring of existing benchmarks that are misaligned but dominate leaderboards,
rather than introducing additional hallucination evaluations. This change may
steer the field toward more trustworthy AI systems.

</details>


### [78] [Phonological Representation Learning for Isolated Signs Improves Out-of-Vocabulary Generalization](https://arxiv.org/abs/2509.04745)
*Lee Kezar,Zed Sehyr,Jesse Thomason*

Main category: cs.CL

TL;DR: The paper explores improving generalization to unseen signs in sign language models using vector quantization and two phonological biases.


<details>
  <summary>Details</summary>
Motivation: Current sign language datasets have limited vocabulary, requiring models to generalize to unseen signs effectively.

Method: The study evaluates two phonological biases: Parameter Disentanglement (an architectural bias) and Phonological Semi-Supervision (a regularization technique) in a vector-quantized autoencoder.

Result: Learned representations from the proposed model enhance one-shot reconstruction of unseen signs and improve discriminative sign identification compared to a baseline.

Conclusion: Linguistically-motivated biases can improve generalization and representation learning for sign language models.

Abstract: Sign language datasets are often not representative in terms of vocabulary,
underscoring the need for models that generalize to unseen signs. Vector
quantization is a promising approach for learning discrete, token-like
representations, but it has not been evaluated whether the learned units
capture spurious correlations that hinder out-of-vocabulary performance. This
work investigates two phonological inductive biases: Parameter Disentanglement,
an architectural bias, and Phonological Semi-Supervision, a regularization
technique, to improve isolated sign recognition of known signs and
reconstruction quality of unseen signs with a vector-quantized autoencoder. The
primary finding is that the learned representations from the proposed model are
more effective for one-shot reconstruction of unseen signs and more
discriminative for sign identification compared to a controlled baseline. This
work provides a quantitative analysis of how explicit, linguistically-motivated
biases can improve the generalization of learned representations of sign
language.

</details>


### [79] [ODKE+: Ontology-Guided Open-Domain Knowledge Extraction with LLMs](https://arxiv.org/abs/2509.04696)
*Samira Khorshidi,Azadeh Nikfarjam,Suprita Shankar,Yisi Sang,Yash Govind,Hyun Jang,Ali Kasgari,Alexis McClimans,Mohamed Soliman,Vishnu Konda,Ahmed Fakhry,Xiaoguang Qi*

Main category: cs.CL

TL;DR: ODKE+ is a system for extracting and ingesting millions of facts from web sources with high precision, improving knowledge graph freshness and completeness.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the high cost of maintaining freshness and completeness of knowledge graphs, crucial for AI applications.

Method: ODKE+ uses a modular pipeline with five components including fact extraction, validation, and ingestion processes, leveraging LLMs and ontology-guided techniques.

Result: The system processes over 9 million Wikipedia pages, ingests 19 million high-confidence facts with 98.8% precision, improves KG coverage, and reduces update lag by 50 days.

Conclusion: Combining LLM-based extraction with ontological structure enables scalable, trustworthy, production-grade knowledge graph augmentation with broad applicability.

Abstract: Knowledge graphs (KGs) are foundational to many AI applications, but
maintaining their freshness and completeness remains costly. We present ODKE+,
a production-grade system that automatically extracts and ingests millions of
open-domain facts from web sources with high precision. ODKE+ combines modular
components into a scalable pipeline: (1) the Extraction Initiator detects
missing or stale facts, (2) the Evidence Retriever collects supporting
documents, (3) hybrid Knowledge Extractors apply both pattern-based rules and
ontology-guided prompting for large language models (LLMs), (4) a lightweight
Grounder validates extracted facts using a second LLM, and (5) the Corroborator
ranks and normalizes candidate facts for ingestion. ODKE+ dynamically generates
ontology snippets tailored to each entity type to align extractions with schema
constraints, enabling scalable, type-consistent fact extraction across 195
predicates. The system supports batch and streaming modes, processing over 9
million Wikipedia pages and ingesting 19 million high-confidence facts with
98.8% precision. ODKE+ significantly improves coverage over traditional
methods, achieving up to 48% overlap with third-party KGs and reducing update
lag by 50 days on average. Our deployment demonstrates that LLM-based
extraction, grounded in ontological structure and verification workflows, can
deliver trustworthiness, production-scale knowledge ingestion with broad
real-world applicability. A recording of the system demonstration is included
with the submission and is also available at https://youtu.be/UcnE3_GsTWs.

</details>


### [80] [OleSpeech-IV: A Large-Scale Multispeaker and Multilingual Conversational Speech Dataset with Diverse Topics](https://arxiv.org/abs/2509.04702)
*Wei Chu,Yuanzhe Dong,Ke Tan,Dong Han,Xavier Menendez-Pidal,Ruchao Fan,Chenfeng Miao,Chanwoo Kim,Bhiksha Raj,Rita Singh*

Main category: cs.CL

TL;DR: The paper presents OleSpeech-IV, a diverse multilingual conversational speech dataset sourced from public audio content and refined through a proprietary process.


<details>
  <summary>Details</summary>
Motivation: To address the need for high-quality, multispeaker, multilingual conversational datasets in speech research.

Method: The dataset was curated from publicly available audio content such as podcasts and teleconferences, with human-sourced speaker information and transcripts refined through a specialized pipeline.

Result: The OleSpeech-IV dataset was created, with publicly available data organized into tiers of quality, and a subset (OleSpeech-IV-2025-EN-AR-100) open-sourced for research applications.

Conclusion: OleSpeech-IV contributes valuable resources for non-commercial multilingual speech research, enhancing conversational dataset availability and supporting further development in this field.

Abstract: OleSpeech-IV dataset is a large-scale multispeaker and multilingual
conversational speech dataset with diverse topics. The audio content comes from
publicly-available English podcasts, talk shows, teleconferences, and other
conversations. Speaker names, turns, and transcripts are human-sourced and
refined by a proprietary pipeline, while additional information such as
timestamps and confidence scores is derived from the pipeline. The IV denotes
its position as Tier IV in the Olewave dataset series. In addition, we have
open-sourced a subset, OleSpeech-IV-2025-EN-AR-100, for non-commercial research
use.

</details>


### [81] [KERAG: Knowledge-Enhanced Retrieval-Augmented Generation for Advanced Question Answering](https://arxiv.org/abs/2509.04716)
*Yushi Sun,Kai Sun,Yifan Ethan Xu,Xiao Yang,Xin Luna Dong,Nan Tang,Lei Chen*

Main category: cs.CL

TL;DR: KERAG improves QA coverage and accuracy by enhancing retrieval methods in KG-based RAG.


<details>
  <summary>Details</summary>
Motivation: Traditional KGQA techniques suffer from low coverage and ambiguity, limiting QA performance.

Method: KERAG retrieves broader subgraphs, filters irrelevant data, and uses fine-tuned LLMs for Chain-of-Thought reasoning.

Result: KERAG performs 7% better than state-of-the-art systems and exceeds GPT-4o (Tool) by 10-21%.

Conclusion: Broadening subgraph retrieval combined with advanced LLM reasoning frameworks significantly enhance QA capabilities.

Abstract: Retrieval-Augmented Generation (RAG) mitigates hallucination in Large
Language Models (LLMs) by incorporating external data, with Knowledge Graphs
(KGs) offering crucial information for question answering. Traditional
Knowledge Graph Question Answering (KGQA) methods rely on semantic parsing,
which typically retrieves knowledge strictly necessary for answer generation,
thus often suffer from low coverage due to rigid schema requirements and
semantic ambiguity. We present KERAG, a novel KG-based RAG pipeline that
enhances QA coverage by retrieving a broader subgraph likely to contain
relevant information. Our retrieval-filtering-summarization approach, combined
with fine-tuned LLMs for Chain-of-Thought reasoning on knowledge sub-graphs,
reduces noises and improves QA for both simple and complex questions.
Experiments demonstrate that KERAG surpasses state-of-the-art solutions by
about 7% in quality and exceeds GPT-4o (Tool) by 10-21%.

</details>


### [82] [A Study of Large Language Models for Patient Information Extraction: Model Architecture, Fine-Tuning Strategy, and Multi-task Instruction Tuning](https://arxiv.org/abs/2509.04753)
*Cheng Peng,Xinyu Dong,Mengxian Lyu,Daniel Paredes,Yaoyun Zhang,Yonghui Wu*

Main category: cs.CL

TL;DR: The paper investigates the effectiveness of large language models (LLMs) for patient information extraction in clinical narratives, comparing various architectures, fine-tuning, and learning techniques.


<details>
  <summary>Details</summary>
Motivation: To explore the optimal use of LLMs for extracting clinical concepts and relationships from healthcare data, which could support healthcare applications.

Method: The study benchmarks a range of LLMs (encoder- and decoder-based), evaluates fine-tuning strategies (e.g., traditional and prompt-based PEFT), and utilizes multi-task instruction tuning with few-shot and zero-shot learning on different datasets.

Result: LLMs, including BERT, GatorTron, and Llama variants, were compared across five clinical datasets using strategies such as leave-one-dataset-out. Results reveal differences in performance across architectures and fine-tuning approaches.

Conclusion: Combining LLM architectures with advanced fine-tuning and tuning strategies can improve robustness and generalizability for clinical concept and relation extraction tasks.

Abstract: Natural language processing (NLP) is a key technology to extract important
patient information from clinical narratives to support healthcare
applications. The rapid development of large language models (LLMs) has
revolutionized many NLP tasks in the clinical domain, yet their optimal use in
patient information extraction tasks requires further exploration. This study
examines LLMs' effectiveness in patient information extraction, focusing on LLM
architectures, fine-tuning strategies, and multi-task instruction tuning
techniques for developing robust and generalizable patient information
extraction systems. This study aims to explore key concepts of using LLMs for
clinical concept and relation extraction tasks, including: (1) encoder-only or
decoder-only LLMs, (2) prompt-based parameter-efficient fine-tuning (PEFT)
algorithms, and (3) multi-task instruction tuning on few-shot learning
performance. We benchmarked a suite of LLMs, including encoder-based LLMs
(BERT, GatorTron) and decoder-based LLMs (GatorTronGPT, Llama 3.1,
GatorTronLlama), across five datasets. We compared traditional full-size
fine-tuning and prompt-based PEFT. We explored a multi-task instruction tuning
framework that combines both tasks across four datasets to evaluate the
zero-shot and few-shot learning performance using the leave-one-dataset-out
strategy.

</details>


### [83] [Research on Multi-hop Inference Optimization of LLM Based on MQUAKE Framework](https://arxiv.org/abs/2509.04770)
*Zucheng Liang,Wenxin Wei,Kaijie Zhang,Hongyi Chen*

Main category: cs.CL

TL;DR: The study demonstrates that multi-hop question decomposition improves reasoning in LLMs for complex questions, outperforming direct answering methods even after fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Investigate a method to enhance large language model comprehension and accuracy in answering complex questions.

Method: A multi-hop question decomposition technique was applied to a knowledge graph, and the LLAMA3 model was tested on partitioned datasets using fine-tuning.

Result: The multi-hop question decomposition approach showed superior prediction performance compared to directly answering questions, both before and after fine-tuning.

Conclusion: Multi-hop question decomposition effectively enhances model accuracy for complex queries in both trained and untrained scenarios.

Abstract: Accurately answering complex questions has consistently been a significant
challenge for Large Language Models (LLMs). To address this, this paper
proposes a multi-hop question decomposition method for complex questions,
building upon research within the MQUAKE framework. Utilizing the LLAMA3 model,
we systematically investigate the impact of multi-hop question decomposition
within knowledge graphs on model comprehension and reasoning accuracy, both
before and after model training. In our experiments, we systematically
partitioned and converted the MQUAKE-T dataset into two distinct formats: a
single-hop dataset designed for directly answering complex questions, and a
multi-hop dataset constructed using the multi-hop question decomposition
method. We then fine-tuned the LLAMA3 model on these datasets and conducted
inference tests. Our results demonstrate that, without fine-tuning the LLM, the
prediction performance based on the multi-hop question decomposition method
significantly outperforms the method of directly answering complex questions.
After fine-tuning using the LoRA (Low-Rank Adaptation) method, the performance
of both approaches improved compared to the untrained baseline. Crucially, the
method utilizing multi-hop decomposition consistently maintained its
superiority. These findings validate the effectiveness of the multi-hop
decomposition method both before and after training, demonstrating its
capability to effectively enhance the LLM's ability to answer complex
questions.

</details>


### [84] [Decoders Laugh as Loud as Encoders](https://arxiv.org/abs/2509.04779)
*Eli Borodach,Raj Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.CL

TL;DR: The paper investigates the ability of large language models, specifically GPT-4o, to understand and process humor compared to traditional encoder models like RoBERTa.


<details>
  <summary>Details</summary>
Motivation: The study aims to test whether newly advanced language models truly understand nuanced themes like humor, expanding their functional credibility beyond standard NLP tasks.

Method: Researchers fine-tuned GPT-4o, a decoder model, and compared its performance in humor analysis to RoBERTa, an encoder model, using the Mean F1-macro scoring metric.

Result: GPT-4o achieved a Mean F1-macro score of 0.85 in humor understanding, which is comparable to RoBERTa's score of 0.86, indicating competitive performance.

Conclusion: GPT-4o demonstrates robust capabilities in handling nuanced linguistic themes like humor, suggesting promising understanding, albeit not definitively proving comprehension of humor itself.

Abstract: From the dawn of the computer, Allen Turing dreamed of a robot that could
communicate using language as a human being. The recent advances in the field
of Large Language Models (LLMs) shocked the scientific community when a single
model can apply for various natural language processing (NLP) tasks, while the
output results are sometimes even better than most human communication skills.
Models such as GPT, Claude, Grok, etc. have left their mark on the scientific
community. However, it is unclear how much these models understand what they
produce, especially in a nuanced theme such as humor. The question of whether
computers understand humor is still open (among the decoders, the latest to be
checked was GPT-2). We addressed this issue in this paper; we have showed that
a fine-tuned decoder (GPT-4o) performed (Mean F1-macro score of 0.85) as well
as the best fine-tuned encoder (RoBERTa with a Mean of F1-score 0.86)

</details>


### [85] [PRIM: Towards Practical In-Image Multilingual Machine Translation](https://arxiv.org/abs/2509.05146)
*Yanzhi Tian,Zeming Liu,Zhengyang Liu,Chong Feng,Xin Li,Heyan Huang,Yuhang Guo*

Main category: cs.CL

TL;DR: The paper introduces an end-to-end model called VisTrans for In-Image Multilingual Machine Translation (IIMMT) using the newly annotated PRIM dataset, which better reflects real-world conditions.


<details>
  <summary>Details</summary>
Motivation: To address the significant gap between research and practical conditions in In-Image Machine Translation due to reliance on synthetic data with simple features.

Method: Annotated the PRIM dataset capturing real-world complexities and proposed the model VisTrans for separately processing visual text and background information.

Result: Experimental results show VisTrans outperforms other models in both translation quality and visual effect.

Conclusion: The introduction of PRIM dataset and VisTrans model provides a practical solution to real-world IIMT challenges, enabling robust multilingual translation and superior visual outcomes.

Abstract: In-Image Machine Translation (IIMT) aims to translate images containing texts
from one language to another. Current research of end-to-end IIMT mainly
conducts on synthetic data, with simple background, single font, fixed text
position, and bilingual translation, which can not fully reflect real world,
causing a significant gap between the research and practical conditions. To
facilitate research of IIMT in real-world scenarios, we explore Practical
In-Image Multilingual Machine Translation (IIMMT). In order to convince the
lack of publicly available data, we annotate the PRIM dataset, which contains
real-world captured one-line text images with complex background, various
fonts, diverse text positions, and supports multilingual translation
directions. We propose an end-to-end model VisTrans to handle the challenge of
practical conditions in PRIM, which processes visual text and background
information in the image separately, ensuring the capability of multilingual
translation while improving the visual quality. Experimental results indicate
the VisTrans achieves a better translation quality and visual effect compared
to other models. The code and dataset are available at:
https://github.com/BITHLP/PRIM.

</details>


### [86] [Enhancing Diversity in Large Language Models via Determinantal Point Processes](https://arxiv.org/abs/2509.04784)
*Yilei Chen,Souradip Chakraborty,Lorenz Wolf,Ioannis Ch. Paschalidis,Aldo Pacchiano*

Main category: cs.CL

TL;DR: This paper introduces DQO, a training method leveraging determinantal point processes to optimize large language models for both quality and semantic diversity, significantly improving output diversity without reducing performance.


<details>
  <summary>Details</summary>
Motivation: Current methods for fine-tuning large language models often reduce output diversity, creating narrow, less creative responses. Enhanced diversity methods usually operate at inference time or focus on lexical diversity, limiting their effectiveness.

Method: The authors propose DQO, which samples and embeds multiple responses for each prompt, employing determinantal point processes to optimize for both semantic diversity and quality. The determinant of a kernel-based similarity matrix measures diversity.

Result: DQO substantially improves semantic diversity across various tasks such as summarization, story generation, instruction-following, and reasoning, without compromising the model's quality.

Conclusion: The DQO method effectively addresses the trade-off between diversity and quality, presenting a robust solution for generating diverse yet high-quality outputs in large language models.

Abstract: Supervised fine-tuning and reinforcement learning are two popular methods for
post-training large language models (LLMs). While improving the model's
performance on downstream tasks, they often reduce the model's output
diversity, leading to narrow, canonical responses. Existing methods to enhance
diversity are limited, either by operating at inference time or by focusing on
lexical differences. We propose a novel training method named DQO based on
determinantal point processes (DPPs) to jointly optimize LLMs for quality and
semantic diversity. Our approach samples and embeds a group of responses for
each prompt, then uses the determinant of a kernel-based similarity matrix to
measure diversity as the volume spanned by the embeddings of these responses.
Experiments across instruction-following, summarization, story generation, and
reasoning tasks demonstrate that our method substantially improves semantic
diversity without sacrificing model quality.

</details>


### [87] [Personality as a Probe for LLM Evaluation: Method Trade-offs and Downstream Effects](https://arxiv.org/abs/2509.04794)
*Gunmay Handa,Zekun Wu,Adriano Koshiyama,Philip Treleaven*

Main category: cs.CL

TL;DR: This paper systematically examines methods for controlling personality traits in large language models using the Big Five personality framework, exploring in-context learning, parameter-efficient fine-tuning, and mechanistic steering.


<details>
  <summary>Details</summary>
Motivation: The motivation is to better understand mechanisms and trade-offs in personality manipulation within LLMs, particularly for improving their use in customer service and agent-driven applications.

Method: The study categorizes methods like ICL, PEFT, and MS and introduces a dataset for balanced trait responses, as well as a unified evaluation framework to analyze reasoning capability, performance, and bias. Techniques for trait purification and a stability framework are also proposed.

Result: Experiments showed trade-offs across methods: ICL ensures alignment with low capability loss, PEFT achieves high alignment with performance degradation, and MS offers runtime control while maintaining competitive effectiveness.

Conclusion: Personality manipulation in LLMs connects behavioral representation and technical mechanisms, with mechanistic steering highlighted as both effective and lightweight for deployment and interpretability.

Abstract: Personality manipulation in large language models (LLMs) is increasingly
applied in customer service and agentic scenarios, yet its mechanisms and
trade-offs remain unclear. We present a systematic study of personality control
using the Big Five traits, comparing in-context learning (ICL),
parameter-efficient fine-tuning (PEFT), and mechanistic steering (MS). Our
contributions are fourfold. First, we construct a contrastive dataset with
balanced high/low trait responses, enabling effective steering vector
computation and fair cross-method evaluation. Second, we introduce a unified
evaluation framework based on within-run $\Delta$ analysis that disentangles,
reasoning capability, agent performance, and demographic bias across MMLU,
GAIA, and BBQ benchmarks. Third, we develop trait purification techniques to
separate openness from conscientiousness, addressing representational overlap
in trait encoding. Fourth, we propose a three-level stability framework that
quantifies method-, trait-, and combination-level robustness, offering
practical guidance under deployment constraints. Experiments on Gemma-2-2B-IT
and LLaMA-3-8B-Instruct reveal clear trade-offs: ICL achieves strong alignment
with minimal capability loss, PEFT delivers the highest alignment at the cost
of degraded task performance, and MS provides lightweight runtime control with
competitive effectiveness. Trait-level analysis shows openness as uniquely
challenging, agreeableness as most resistant to ICL, and personality encoding
consolidating around intermediate layers. Taken together, these results
establish personality manipulation as a multi-level probe into behavioral
representation, linking surface conditioning, parameter encoding, and
activation-level steering, and positioning mechanistic steering as a
lightweight alternative to fine-tuning for both deployment and
interpretability.

</details>


### [88] [Knowledge Collapse in LLMs: When Fluency Survives but Facts Fail under Recursive Synthetic Training](https://arxiv.org/abs/2509.04796)
*Figarri Keisha,Zekun Wu,Ze Wang,Adriano Koshiyama,Philip Treleaven*

Main category: cs.CL

TL;DR: Researchers identify "knowledge collapse" in large language models, where factual accuracy drops but fluency remains, caused by recursive synthetic training. They provide strategies and metrics to address this.


<details>
  <summary>Details</summary>
Motivation: To understand and mitigate the degenerative process called 'knowledge collapse,' which arises due to reliance on synthetic data in AI models, threatening their reliability in accuracy-critical contexts.

Method: Controlled experiments using recursive synthetic data training and proposing domain-specific strategies; developed evaluation frameworks combining various metrics to track collapse progression.

Result: Collapse trajectory and timing were determined to depend on the instruction format; domain-specific synthetic training improved collapse resistance without sacrificing computational efficiency.

Conclusion: The study offers insights and methodologies to prevent knowledge collapse, enhancing sustainable and reliable AI performance in accuracy-intensive fields.

Abstract: Large language models increasingly rely on synthetic data due to
human-written content scarcity, yet recursive training on model-generated
outputs leads to model collapse, a degenerative process threatening factual
reliability. We define knowledge collapse as a distinct three-stage phenomenon
where factual accuracy deteriorates while surface fluency persists, creating
"confidently wrong" outputs that pose critical risks in accuracy-dependent
domains. Through controlled experiments with recursive synthetic training, we
demonstrate that collapse trajectory and timing depend critically on
instruction format, distinguishing instruction-following collapse from
traditional model collapse through its conditional, prompt-dependent nature. We
propose domain-specific synthetic training as a targeted mitigation strategy
that achieves substantial improvements in collapse resistance while maintaining
computational efficiency. Our evaluation framework combines model-centric
indicators with task-centric metrics to detect distinct degradation phases,
enabling reproducible assessment of epistemic deterioration across different
language models. These findings provide both theoretical insights into collapse
dynamics and practical guidance for sustainable AI training in
knowledge-intensive applications where accuracy is paramount.

</details>


### [89] [Mind the Gap: Evaluating Model- and Agentic-Level Vulnerabilities in LLMs with Action Graphs](https://arxiv.org/abs/2509.04802)
*Ilham Wicaksono,Zekun Wu,Theo King,Adriano Koshiyama,Philip Treleaven*

Main category: cs.CL

TL;DR: The paper introduces AgentSeer, a framework to evaluate agent-specific risks in large language model-based systems, revealing vulnerabilities invisible under traditional evaluations.


<details>
  <summary>Details</summary>
Motivation: Traditional safety evaluations fail to address risks that arise when large language models operate as agentic systems, necessitating a new approach for deployment-specific risk assessment.

Method: The authors developed AgentSeer, an observability-based framework that decomposes agentic actions into detailed graphs. The framework was tested on two models using HarmBench attacks and comparison at both model and agentic levels.

Result: AgentSeer identifies "agentic-only" vulnerabilities, showing higher attack success rates (ASR) in agentic contexts and differences in vulnerability profiles. Iterative, context-aware attacks bypass traditional evaluation, proving the need for agent-specific assessments.

Conclusion: AgentSeer offers a standardized methodology for evaluating agentic system vulnerabilities, and the findings emphasize the inadequacy of traditional evaluation methods for agent-specific safety risks.

Abstract: As large language models transition to agentic systems, current safety
evaluation frameworks face critical gaps in assessing deployment-specific
risks. We introduce AgentSeer, an observability-based evaluation framework that
decomposes agentic executions into granular action and component graphs,
enabling systematic agentic-situational assessment. Through cross-model
validation on GPT-OSS-20B and Gemini-2.0-flash using HarmBench single turn and
iterative refinement attacks, we demonstrate fundamental differences between
model-level and agentic-level vulnerability profiles. Model-level evaluation
reveals baseline differences: GPT-OSS-20B (39.47% ASR) versus Gemini-2.0-flash
(50.00% ASR), with both models showing susceptibility to social engineering
while maintaining logic-based attack resistance. However, agentic-level
assessment exposes agent-specific risks invisible to traditional evaluation. We
discover "agentic-only" vulnerabilities that emerge exclusively in agentic
contexts, with tool-calling showing 24-60% higher ASR across both models.
Cross-model analysis reveals universal agentic patterns, agent transfer
operations as highest-risk tools, semantic rather than syntactic vulnerability
mechanisms, and context-dependent attack effectiveness, alongside
model-specific security profiles in absolute ASR levels and optimal injection
strategies. Direct attack transfer from model-level to agentic contexts shows
degraded performance (GPT-OSS-20B: 57% human injection ASR; Gemini-2.0-flash:
28%), while context-aware iterative attacks successfully compromise objectives
that failed at model-level, confirming systematic evaluation gaps. These
findings establish the urgent need for agentic-situation evaluation paradigms,
with AgentSeer providing the standardized methodology and empirical validation.

</details>


### [90] [Analyzing Finnish Inflectional Classes through Discriminative Lexicon and Deep Learning Models](https://arxiv.org/abs/2509.04813)
*Alexandre Nikolaev,Yu-Ying Chuang,R. Harald Baayen*

Main category: cs.CL

TL;DR: This study assessed the ability of the Discriminative Lexicon Model (DLM) to handle Finnish noun inflection without relying on predefined inflectional classes. Using a comprehensive dataset, the models achieved high performance but struggled more with unproductive classes and infrequent forms.


<details>
  <summary>Details</summary>
Motivation: Inflectional classes are traditionally utilized for morphological systems, but their cognitive reality and necessity for language acquisition are unclear. The study explores if the Discriminative Lexicon Model can process inflection without these classes.

Method: The DLM was trained on 55,271 inflected Finnish noun forms from 49 inflectional classes. Two approaches were considered: one ignoring token frequencies (endstate learning) and one incorporating usage frequencies (frequency-informed learning).

Result: The models achieved high accuracy on training data. Test performance was lower but still acceptable. Productivity indicators like type count and hapax legomena showed positive correlations with performance; however, frequency was the main predictor in usage-based models.

Conclusion: The study indicates that learning noun inflection is feasible without explicit inflectional classes, although model performance still varies with productivity and frequency. Practical implications exist for morphological modeling and understanding how humans process inflection.

Abstract: Descriptions of complex nominal or verbal systems make use of inflectional
classes. Inflectional classes bring together nouns which have similar stem
changes and use similar exponents in their paradigms. Although inflectional
classes can be very useful for language teaching as well as for setting up
finite state morphological systems, it is unclear whether inflectional classes
are cognitively real, in the sense that native speakers would need to discover
these classes in order to learn how to properly inflect the nouns of their
language. This study investigates whether the Discriminative Lexicon Model
(DLM) can understand and produce Finnish inflected nouns without setting up
inflectional classes, using a dataset with 55,271 inflected nouns of 2000
high-frequency Finnish nouns from 49 inflectional classes. Several DLM
comprehension and production models were set up. Some models were not informed
about frequency of use, and provide insight into learnability with infinite
exposure (endstate learning). Other models were set up from a usage based
perspective, and were trained with token frequencies being taken into
consideration (frequency-informed learning). On training data, models performed
with very high accuracies. For held-out test data, accuracies decreased, as
expected, but remained acceptable. Across most models, performance increased
for inflectional classes with more types, more lower-frequency words, and more
hapax legomena, mirroring the productivity of the inflectional classes. The
model struggles more with novel forms of unproductive and less productive
classes, and performs far better for unseen forms belonging to productive
classes. However, for usage-based production models, frequency was the dominant
predictor of model performance, and correlations with measures of productivity
were tenuous or absent.

</details>


### [91] [AFD-SLU: Adaptive Feature Distillation for Spoken Language Understanding](https://arxiv.org/abs/2509.04821)
*Yan Xie,Yibo Cui,Liang Xie,Erwei Yin*

Main category: cs.CL

TL;DR: This paper introduces Adaptive Feature Distillation (AFD) to enhance Spoken Language Understanding (SLU) systems by transferring semantic knowledge from a general embeddings model to a lightweight model.


<details>
  <summary>Details</summary>
Motivation: The challenge in developing SLU systems lies in the lack of labeled data and deployment challenges of large-scale models, which motivated the proposal of a more efficient teaching-learning system.

Method: The authors proposed AFD with a dynamic adapter using Residual Projection Neural Networks (RPNN) and a Dynamic Distillation Coefficient (DDC) to optimize semantic feature transfer based on real-time feedback.

Result: AFD-SLU achieved notable outcomes on the ProSLU benchmark, with metrics such as intent accuracy (95.67%), slot F1 score (92.02%), and overall accuracy (85.50%).

Conclusion: The paper concludes that the AFD-SLU framework effectively addresses challenges in SLU, achieving state-of-the-art results while maintaining efficiency in deployment.

Abstract: Spoken Language Understanding (SLU) is a core component of conversational
systems, enabling machines to interpret user utterances. Despite its
importance, developing effective SLU systems remains challenging due to the
scarcity of labeled training data and the computational burden of deploying
Large Language Models (LLMs) in real-world applications. To further alleviate
these issues, we propose an Adaptive Feature Distillation framework that
transfers rich semantic representations from a General Text Embeddings
(GTE)-based teacher model to a lightweight student model. Our method introduces
a dynamic adapter equipped with a Residual Projection Neural Network (RPNN) to
align heterogeneous feature spaces, and a Dynamic Distillation Coefficient
(DDC) that adaptively modulates the distillation strength based on real-time
feedback from intent and slot prediction performance. Experiments on the
Chinese profile-based ProSLU benchmark demonstrate that AFD-SLU achieves
state-of-the-art results, with 95.67% intent accuracy, 92.02% slot F1 score,
and 85.50% overall accuracy.

</details>


### [92] [Memorization $\neq$ Understanding: Do Large Language Models Have the Ability of Scenario Cognition?](https://arxiv.org/abs/2509.04866)
*Boxiang Ma,Ru Li,Yuanlong Wang,Hongye Tan,Xiaoli Li*

Main category: cs.CL

TL;DR: This paper investigates whether large language models rely on memorization or semantic understanding by evaluating their ability to process fictional scenarios.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the question of whether large language models achieve generalization through semantic understanding or memorization.

Method: They introduce a scenario-based dataset annotated with semantic elements, and propose a bi-perspective framework to evaluate LLMs through both model outputs and internal representations.

Result: Experiments show that current LLMs mostly rely on memorization and fail at robust semantic scenario cognition.

Conclusion: The study highlights limitations in LLMs' semantic understanding and suggests cognitive improvements can advance their capabilities.

Abstract: Driven by vast and diverse textual data, large language models (LLMs) have
demonstrated impressive performance across numerous natural language processing
(NLP) tasks. Yet, a critical question persists: does their generalization arise
from mere memorization of training data or from deep semantic understanding? To
investigate this, we propose a bi-perspective evaluation framework to assess
LLMs' scenario cognition - the ability to link semantic scenario elements with
their arguments in context. Specifically, we introduce a novel scenario-based
dataset comprising diverse textual descriptions of fictional facts, annotated
with scenario elements. LLMs are evaluated through their capacity to answer
scenario-related questions (model output perspective) and via probing their
internal representations for encoded scenario elements-argument associations
(internal representation perspective). Our experiments reveal that current LLMs
predominantly rely on superficial memorization, failing to achieve robust
semantic scenario cognition, even in simple cases. These findings expose
critical limitations in LLMs' semantic understanding and offer cognitive
insights for advancing their capabilities.

</details>


### [93] [Using LLMs for Multilingual Clinical Entity Linking to ICD-10](https://arxiv.org/abs/2509.04868)
*Sylvia Vassileva,Ivan Koychev,Svetla Boytcheva*

Main category: cs.CL

TL;DR: The paper focuses on linking clinical terms in medical texts to ICD-10 codes using a multistage pipeline with clinical dictionaries and in-context learning via GPT-4.1. It achieves high F1 scores across Spanish and Greek datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to simplify the process of ICD-10 coding in clinical texts, reducing the workload for healthcare professionals and ensuring consistent coding across hospitals.

Method: The method employs a multistage pipeline. First, clinical dictionaries are used to find unambiguous matches. Then, for unmatched terms, in-context learning with GPT-4.1 is used to predict ICD-10 codes.

Result: The system achieves strong results: 0.89 F1 (categories) and 0.78 F1 (subcategories) on CodiEsp (Spanish), and 0.85 F1 on ElCardioCC (Greek).

Conclusion: The proposed approach demonstrates the feasibility of using LLMs for multilingual clinical term linking to ICD-10 codes, with potential benefits for real-world healthcare applications.

Abstract: The linking of clinical entities is a crucial part of extracting structured
information from clinical texts. It is the process of assigning a code from a
medical ontology or classification to a phrase in the text. The International
Classification of Diseases - 10th revision (ICD-10) is an international
standard for classifying diseases for statistical and insurance purposes.
Automatically assigning the correct ICD-10 code to terms in discharge summaries
will simplify the work of healthcare professionals and ensure consistent coding
in hospitals. Our paper proposes an approach for linking clinical terms to
ICD-10 codes in different languages using Large Language Models (LLMs). The
approach consists of a multistage pipeline that uses clinical dictionaries to
match unambiguous terms in the text and then applies in-context learning with
GPT-4.1 to predict the ICD-10 code for the terms that do not match the
dictionary. Our system shows promising results in predicting ICD-10 codes on
different benchmark datasets in Spanish - 0.89 F1 for categories and 0.78 F1 on
subcategories on CodiEsp, and Greek - 0.85 F1 on ElCardioCC.

</details>


### [94] [PLaMo 2 Technical Report](https://arxiv.org/abs/2509.04897)
*Preferred Networks,:,Kaizaburo Chubachi,Yasuhiro Fujita,Shinichi Hemmi,Yuta Hirokawa,Toshiki Kataoka,Goro Kobayashi,Kenichi Maehashi,Calvin Metzger,Hiroaki Mikami,Shogo Murai,Daisuke Nishino,Kento Nozawa,Shintarou Okada,Daisuke Okanohara,Shunta Saito,Shotaro Sano,Shuji Suzuki,Daisuke Tanaka,Avinash Ummadisingu,Hanqin Wang,Sixue Wang,Tianqi Xu*

Main category: cs.CL

TL;DR: PLaMo 2 is a Japanese large language model with hybrid architecture and efficient training, achieving state-of-the-art results using synthetic corpora, fine-tuning, and quantization.


<details>
  <summary>Details</summary>
Motivation: To address data scarcity in Japanese language models and improve computational efficiency while maintaining high performance.

Method: Developed a hybrid large language model using synthetic data, weight reuse, structured pruning, supervised fine-tuning, preference optimization, and quantization.

Result: An 8B parameter model achieves performance comparable to a previous 100B model and outperforms other models on Japanese benchmarks.

Conclusion: The PLaMo 2 model is efficient, effective, and sets a new standard in Japanese language processing through innovative training and optimization techniques.

Abstract: In this report, we introduce PLaMo 2, a series of Japanese-focused large
language models featuring a hybrid Samba-based architecture that transitions to
full attention via continual pre-training to support 32K token contexts.
Training leverages extensive synthetic corpora to overcome data scarcity, while
computational efficiency is achieved through weight reuse and structured
pruning. This efficient pruning methodology produces an 8B model that achieves
performance comparable to our previous 100B model. Post-training further
refines the models using a pipeline of supervised fine-tuning (SFT) and direct
preference optimization (DPO), enhanced by synthetic Japanese instruction data
and model merging techniques. Optimized for inference using vLLM and
quantization with minimal accuracy loss, the PLaMo 2 models achieve
state-of-the-art results on Japanese benchmarks, outperforming similarly-sized
open models in instruction-following, language fluency, and Japanese-specific
knowledge.

</details>


### [95] [ACE-RL: Adaptive Constraint-Enhanced Reward for Long-form Generation Reinforcement Learning](https://arxiv.org/abs/2509.04903)
*Jianghao Chen,Wei Sun,Qixiang Yin,Lingxing Kong,Zhixing Tan,Jiajun Zhang*

Main category: cs.CL

TL;DR: The paper introduces ACE-RL, a reinforcement learning framework addressing challenges in long-form text generation by using adaptive constraints for more fine-grained quality optimization.


<details>
  <summary>Details</summary>
Motivation: To overcome reliance on scarce high-quality training data and address limitations of coarse-grained quality optimization in long-form text generation.

Method: The proposed ACE-RL framework automatically identifies fine-grained constraints from instructions, uses these for quality evaluation, and employs reinforcement learning to enhance long-form text generation.

Result: ACE-RL outperforms existing baselines, with improvements of 20.70% over SFT and 7.32% over RL on WritingBench. It also surpasses GPT-4o by 7.10%.

Conclusion: ACE-RL provides an effective training paradigm for LLMs to produce high-quality, diverse long-form content, addressing prior challenges in precision and data dependency.

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
long-context understanding, yet they face significant challenges in
high-quality long-form generation. Existing studies primarily suffer from two
limitations: (1) A heavy reliance on scarce, high-quality long-form response
data for supervised fine-tuning (SFT) or for pairwise preference reward in
reinforcement learning (RL). (2) Focus on coarse-grained quality optimization
dimensions, such as relevance, coherence, and helpfulness, overlooking the
fine-grained specifics inherent to diverse long-form generation scenarios. To
address this issue, we propose a framework using Adaptive Constraint-Enhanced
reward for long-form generation Reinforcement Learning (ACE-RL). ACE-RL first
automatically deconstructs each instruction into a set of fine-grained,
adaptive constraint criteria by identifying its underlying intents and demands.
Subsequently, we design a reward mechanism that quantifies the quality of
long-form responses based on their satisfaction over corresponding constraints,
converting subjective quality evaluation into constraint verification. Finally,
we utilize reinforcement learning to guide models toward superior long-form
generation capabilities. Experimental results demonstrate that our ACE-RL
framework significantly outperforms existing SFT and RL baselines by 20.70% and
7.32% on WritingBench, and our top-performing model even surpasses proprietary
systems like GPT-4o by 7.10%, providing a more effective training paradigm for
LLMs to generate high-quality content across diverse long-form generation
scenarios.

</details>


### [96] [Classification of kinetic-related injury in hospital triage data using NLP](https://arxiv.org/abs/2509.04969)
*Midhun Shyam,Jim Basilakis,Kieran Luken,Steven Thomas,John Crozier,Paul M. Middleton,X. Rosalind Wang*

Main category: cs.CL

TL;DR: The paper presents a pipeline for classifying Emergency Department triage data using large language models (LLMs) with limited computational resources.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges of analyzing sensitive hospital triage data using modern machine learning techniques, particularly with limited computational resources and manual labeling constraints.

Method: The proposed method involves initially fine-tuning a pre-trained LLM on a publicly available small dataset (2k samples) using a GPU, followed by further fine-tuning on a smaller hospital-specific dataset (1000 samples) on a CPU.

Result: The presented pipeline successfully classifies triage data by leveraging carefully curated datasets, pre-trained models, and minimal computational resources.

Conclusion: The study demonstrates the feasibility of using LLMs for medical triage data classification in resource-constrained environments by strategically reusing open data and existing computational resources.

Abstract: Triage notes, created at the start of a patient's hospital visit, contain a
wealth of information that can help medical staff and researchers understand
Emergency Department patient epidemiology and the degree of time-dependent
illness or injury. Unfortunately, applying modern Natural Language Processing
and Machine Learning techniques to analyse triage data faces some challenges:
Firstly, hospital data contains highly sensitive information that is subject to
privacy regulation thus need to be analysed on site; Secondly, most hospitals
and medical facilities lack the necessary hardware to fine-tune a Large
Language Model (LLM), much less training one from scratch; Lastly, to identify
the records of interest, expert inputs are needed to manually label the
datasets, which can be time-consuming and costly. We present in this paper a
pipeline that enables the classification of triage data using LLM and limited
compute resources. We first fine-tuned a pre-trained LLM with a classifier
using a small (2k) open sourced dataset on a GPU; and then further fine-tuned
the model with a hospital specific dataset of 1000 samples on a CPU. We
demonstrated that by carefully curating the datasets and leveraging existing
models and open sourced data, we can successfully classify triage data with
limited compute resources.

</details>


### [97] [Optimizing Small Transformer-Based Language Models for Multi-Label Sentiment Analysis in Short Texts](https://arxiv.org/abs/2509.04982)
*Julius Neumann,Robert Lange,Yuni Susanti,Michael Färber*

Main category: cs.CL

TL;DR: The paper evaluates the performance of small Transformer-based models on sentiment classification in short texts, discussing factors like domain-specific pretraining, data augmentation, and classification head changes.


<details>
  <summary>Details</summary>
Motivation: The study addresses challenges in sentiment classification for short texts, such as class imbalance, data sparsity, and the subjective nature of sentiment labels.

Method: The authors tested small Transformer models using domain-specific pretraining, generative data augmentation, and variations in classification head architecture.

Result: Results indicate data augmentation enhances classification, while domain-specific pretraining on augmented data can introduce noise. Architectural changes to the classification head have minimal impact.

Conclusion: The paper offers practical strategies for improving small BERT-based models in short-text sentiment classification, underlining the importance of careful data augmentation and indicating limited benefits from pretraining and architectural modifications.

Abstract: Sentiment classification in short text datasets faces significant challenges
such as class imbalance, limited training samples, and the inherent
subjectivity of sentiment labels -- issues that are further intensified by the
limited context in short texts. These factors make it difficult to resolve
ambiguity and exacerbate data sparsity, hindering effective learning. In this
paper, we evaluate the effectiveness of small Transformer-based models (i.e.,
BERT and RoBERTa, with fewer than 1 billion parameters) for multi-label
sentiment classification, with a particular focus on short-text settings.
Specifically, we evaluated three key factors influencing model performance: (1)
continued domain-specific pre-training, (2) data augmentation using
automatically generated examples, specifically generative data augmentation,
and (3) architectural variations of the classification head. Our experiment
results show that data augmentation improves classification performance, while
continued pre-training on augmented datasets can introduce noise rather than
boost accuracy. Furthermore, we confirm that modifications to the
classification head yield only marginal benefits. These findings provide
practical guidance for optimizing BERT-based models in resource-constrained
settings and refining strategies for sentiment classification in short-text
datasets.

</details>


### [98] [Do Large Language Models Need Intent? Revisiting Response Generation Strategies for Service Assistant](https://arxiv.org/abs/2509.05006)
*Inbal Bolshinsky,Shani Kupiec,Almog Sasson,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.CL

TL;DR: This paper explores whether explicit intent recognition is essential for generating high-quality service responses or if models can directly achieve this, comparing two task paradigms using state-of-the-art language models.


<details>
  <summary>Details</summary>
Motivation: To address a fundamental question in conversational AI regarding whether explicit intent recognition is necessary for generating accurate and effective service responses.

Method: Conducting a comparative study using two public service interaction datasets and various language models, including a fine-tuned T5 variant, to benchmark intent-first and direct response generation approaches.

Result: The study reveals surprising insights on the necessity or redundancy of explicit intent modeling by evaluating linguistic quality and task success rates.

Conclusion: The findings challenge traditional assumptions in conversational AI and provide actionable design guidelines for more efficient response systems.

Abstract: In the era of conversational AI, generating accurate and contextually
appropriate service responses remains a critical challenge. A central question
remains: Is explicit intent recognition a prerequisite for generating
high-quality service responses, or can models bypass this step and produce
effective replies directly? This paper conducts a rigorous comparative study to
address this fundamental design dilemma. Leveraging two publicly available
service interaction datasets, we benchmark several state-of-the-art language
models, including a fine-tuned T5 variant, across both paradigms: Intent-First
Response Generation and Direct Response Generation. Evaluation metrics
encompass both linguistic quality and task success rates, revealing surprising
insights into the necessity or redundancy of explicit intent modelling. Our
findings challenge conventional assumptions in conversational AI pipelines,
offering actionable guidelines for designing more efficient and effective
response generation systems.

</details>


### [99] [Masked Diffusion Language Models with Frequency-Informed Training](https://arxiv.org/abs/2509.05056)
*Despoina Kosmopoulou,Efthymios Georgiou,Vaggelis Dorovatas,Georgios Paraskevopoulos,Alexandros Potamianos*

Main category: cs.CL

TL;DR: The paper introduces a novel diffusion-based language modeling method tailored for data-efficient training, tested on the BabyLM 2025 Challenge.


<details>
  <summary>Details</summary>
Motivation: The research aims to develop a viable language modeling alternative optimized for data-restricted environments, addressing challenges in efficient learning from limited language data.

Method: A masked diffusion framework is applied, integrating frequency-informed masking for rare tokens, diverse noise scheduling strategies, and weighted noise handling within the NELBO objective.

Result: The proposed method achieves competitive performance on benchmarks evaluating linguistic competence, world knowledge, and human-likeness, comparable to hybrid autoregressive-masked baselines.

Conclusion: Diffusion-based language modeling provides a promising approach to effective language learning under strict data constraints.

Abstract: We present a masked diffusion language modeling framework for data-efficient
training for the BabyLM 2025 Challenge. Our approach applies diffusion training
objectives to language modeling under strict data constraints, incorporating
frequency-informed masking that prioritizes learning from rare tokens while
maintaining theoretical validity. We explore multiple noise scheduling
strategies, including two-mode approaches, and investigate different noise
weighting schemes within the NELBO objective. We evaluate our method on the
BabyLM benchmark suite, measuring linguistic competence, world knowledge, and
human-likeness. Results show performance competitive to hybrid
autoregressive-masked baselines, demonstrating that diffusion-based training
offers a viable alternative for data-restricted language learning.

</details>


### [100] [Entropy2Vec: Crosslingual Language Modeling Entropy as End-to-End Learnable Language Representations](https://arxiv.org/abs/2509.05060)
*Patrick Amadeus Irawan,Ryandito Diandaru,Belati Jagad Bintang Syuhada,Randy Zakya Suchrady,Alham Fikri Aji,Genta Indra Winata,Fajri Koto,Samuel Cahyawijaya*

Main category: cs.CL

TL;DR: Entropy2Vec uses the entropy from monolingual language models to generate cross-lingual embeddings. This approach improves upon traditional language typology methods and shows promise in multilingual NLP tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional methods of studying language typology are hindered by sparse feature sets and static representations. There is a need for a dynamic and adaptable solution that can effectively capture structural relationships between languages.

Method: Entropy2Vec generates dense cross-lingual embeddings by leveraging the entropy of predictions made by monolingual language models. It hypothesizes that lower entropy corresponds to higher structural similarity between languages.

Result: Empirical tests show Entropy2Vec embeddings align well with established typological categories and perform competitively in multilingual NLP tasks.

Conclusion: Entropy2Vec offers a novel way to derive flexible and accurate cross-lingual representations, bridging gaps in traditional typological approaches and boosting performance in NLP applications.

Abstract: We introduce Entropy2Vec, a novel framework for deriving cross-lingual
language representations by leveraging the entropy of monolingual language
models. Unlike traditional typological inventories that suffer from feature
sparsity and static snapshots, Entropy2Vec uses the inherent uncertainty in
language models to capture typological relationships between languages. By
training a language model on a single language, we hypothesize that the entropy
of its predictions reflects its structural similarity to other languages: Low
entropy indicates high similarity, while high entropy suggests greater
divergence. This approach yields dense, non-sparse language embeddings that are
adaptable to different timeframes and free from missing values. Empirical
evaluations demonstrate that Entropy2Vec embeddings align with established
typological categories and achieved competitive performance in downstream
multilingual NLP tasks, such as those addressed by the LinguAlchemy framework.

</details>


### [101] [ToM-SSI: Evaluating Theory of Mind in Situated Social Interactions](https://arxiv.org/abs/2509.05066)
*Matteo Bortoletto,Constantin Ruhdorfer,Andreas Bulling*

Main category: cs.CL

TL;DR: The paper introduces ToM-SSI, a benchmark to evaluate Theory of Mind (ToM) abilities in scenarios with complex social interactions and spatial dynamics.


<details>
  <summary>Details</summary>
Motivation: Traditional ToM benchmarks predominantly use simplified tests like the Sally-Anne test, lacking the capacity to assess ToM in complex and realistic environments.

Method: ToM-SSI incorporates multimodal setups with up to four agents engaged in group interactions, considering both cooperative and obstructive social scenarios in situated environments.

Result: Evaluations show that current models perform poorly on ToM-SSI tasks, pinpointing major limitations in handling these complex scenarios.

Conclusion: There is a significant scope for improvement in developing models with robust ToM abilities, offering opportunities for advancing social cognition research in AI.

Abstract: Most existing Theory of Mind (ToM) benchmarks for foundation models rely on
variations of the Sally-Anne test, offering only a very limited perspective on
ToM and neglecting the complexity of human social interactions. To address this
gap, we propose ToM-SSI: a new benchmark specifically designed to test ToM
capabilities in environments rich with social interactions and spatial
dynamics. While current ToM benchmarks are limited to text-only or dyadic
interactions, ToM-SSI is multimodal and includes group interactions of up to
four agents that communicate and move in situated environments. This unique
design allows us to study, for the first time, mixed cooperative-obstructive
settings and reasoning about multiple agents' mental state in parallel, thus
capturing a wider range of social cognition than existing benchmarks. Our
evaluations reveal that the current models' performance is still severely
limited, especially in these new tasks, highlighting critical gaps for future
research.

</details>


### [102] [ICR: Iterative Clarification and Rewriting for Conversational Search](https://arxiv.org/abs/2509.05100)
*Zhiyu Cao,Peifeng Li,Qiaoming Zhu*

Main category: cs.CL

TL;DR: The paper introduces ICR, a new method for conversational query rewriting that addresses issues of fuzzy expressions and improves performance through an iterative clarification process.


<details>
  <summary>Details</summary>
Motivation: Existing end-to-end query rewriting methods struggle with multiple fuzzy expressions in queries, making it challenging to rewrite accurately.

Method: ICR introduces an iterative framework alternating between generating clarification questions and rewriting queries to address ambiguity incrementally.

Result: Experiments demonstrate that ICR achieves continuous performance improvements in query rewriting, setting new benchmarks on two popular datasets.

Conclusion: Iterative clarification via ICR is effective in enhancing conversational query accuracy, outperforming traditional end-to-end rewriting approaches.

Abstract: Most previous work on Conversational Query Rewriting employs an end-to-end
rewriting paradigm. However, this approach is hindered by the issue of multiple
fuzzy expressions within the query, which complicates the simultaneous
identification and rewriting of multiple positions. To address this issue, we
propose a novel framework ICR (Iterative Clarification and Rewriting), an
iterative rewriting scheme that pivots on clarification questions. Within this
framework, the model alternates between generating clarification questions and
rewritten queries. The experimental results show that our ICR can continuously
improve retrieval performance in the clarification-rewriting iterative process,
thereby achieving state-of-the-art performance on two popular datasets.

</details>


### [103] [Triadic Fusion of Cognitive, Functional, and Causal Dimensions for Explainable LLMs: The TAXAL Framework](https://arxiv.org/abs/2509.05199)
*David Herrera-Poyatos,Carlos Peláez-González,Cristina Zuheros,Virilo Tejedor,Rosana Montes,Francisco Herrera*

Main category: cs.CL

TL;DR: This paper introduces TAXAL, a framework for explainability in agentic LLMs, focusing on cognitive, functional, and causal dimensions.


<details>
  <summary>Details</summary>
Motivation: Address the opacity, bias, and instability of LLMs in high-risk domains, emphasizing the need for explanations beyond traditional outputs.

Method: Proposes a triadic fusion framework (TAXAL) combining cognitive, functional, and causal perspectives; analyzes existing methods and situates them within this model; demonstrates applicability via case studies.

Result: TAXAL successfully integrates existing approaches into a unified explainability model adaptable to diverse sociotechnical scenarios (law, education, healthcare, etc.).

Conclusion: The framework enhances explainability as both a technical and sociotechnical practice, improving trust in agentic AI applications and fostering context-sensitive deployments.

Abstract: Large Language Models (LLMs) are increasingly being deployed in high-risk
domains where opacity, bias, and instability undermine trust and
accountability. Traditional explainability methods, focused on surface outputs,
do not capture the reasoning pathways, planning logic, and systemic impacts of
agentic LLMs.
  We introduce TAXAL (Triadic Alignment for eXplainability in Agentic LLMs), a
triadic fusion framework that unites three complementary dimensions: cognitive
(user understanding), functional (practical utility), and causal (faithful
reasoning). TAXAL provides a unified, role-sensitive foundation for designing,
evaluating, and deploying explanations in diverse sociotechnical settings.
  Our analysis synthesizes existing methods, ranging from post-hoc attribution
and dialogic interfaces to explanation-aware prompting, and situates them
within the TAXAL triadic fusion model. We further demonstrate its applicability
through case studies in law, education, healthcare, and public services,
showing how explanation strategies adapt to institutional constraints and
stakeholder roles.
  By combining conceptual clarity with design patterns and deployment pathways,
TAXAL advances explainability as a technical and sociotechnical practice,
supporting trustworthy and context-sensitive LLM applications in the era of
agentic AI.

</details>


### [104] [Hunyuan-MT Technical Report](https://arxiv.org/abs/2509.05209)
*Mao Zheng,Zheng Li,Bingxin Qu,Mingyang Song,Yang Du,Mingrui Sun,Di Wang*

Main category: cs.CL

TL;DR: The paper introduces two powerful models for multilingual translation, focusing on Mandarin and minority languages/dialects, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to create an efficient multilingual translation model, especially addressing translation needs for Mandarin and minority languages/dialects.

Method: Developed Hunyuan-MT-7B with pretraining, supervised fine-tuning (SFT), and reinforcement learning. Introduced Hunyuan-MT-Chimera-7B which combines multiple outputs using slow-thinking integration for enhanced translation performance.

Result: Models significantly outperform comparable translation models and achieve state-of-the-art performance, ranking first on WMT2025 across 30 of 31 language pairs.

Conclusion: The proposed models are robust across diverse languages, showing superior performance especially for Mandarin and minority languages/dialects translation.

Abstract: In this report, we introduce Hunyuan-MT-7B, our first open-source
multilingual translation model, which supports bidirectional translation across
33 major languages and places a special emphasis on translation between
Mandarin and several ethnic minority languages as well as dialects.
Furthermore, to serve and address diverse translation scenarios and enhance
model performance at test time, we introduce Hunyuan-MT-Chimera-7B, a
translation model inspired by the slow thinking mode. This model integrates
multiple outputs generated by the Hunyuan-MT-7B model under varying parameter
settings, thereby achieving performance superior to that of conventional
slow-thinking models based on Chain-of-Thought (CoT). The development of our
models follows a holistic training process specifically engineered for
multilingual translation, which begins with general and MT-oriented
pre-training to build foundational capabilities, proceeds to Supervised
Fine-Tuning (SFT) for task-specific adaptation, and culminates in advanced
alignment through Reinforcement Learning (RL) and weak-to-strong RL. Through
comprehensive experimentation, we demonstrate that both Hunyuan-MT-7B and
Hunyuan-MT-Chimera-7B significantly outperform all translation-specific models
of comparable parameter size and most of the SOTA large models, particularly on
the task of translation between Mandarin and minority languages as well as
dialects. In the WMT2025 shared task (General Machine Translation), our models
demonstrate state-of-the-art performance, ranking first in 30 out of 31
language pairs. This result highlights the robustness of our models across a
diverse linguistic spectrum, encompassing high-resource languages such as
Chinese, English, and Japanese, as well as low-resource languages including
Czech, Marathi, Estonian, and Icelandic.

</details>


### [105] [BEDTime: A Unified Benchmark for Automatically Describing Time Series](https://arxiv.org/abs/2509.05215)
*Medhasweta Sen,Zachary Gottesman,Jiaxing Qiu,C. Bayan Bruss,Nam Nguyen,Tom Hartvigsen*

Main category: cs.CL

TL;DR: This paper proposes three tasks for testing a model's ability to describe time series using natural language, evaluates recent datasets to enable model comparisons, and reveals shortcomings in current state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: There is a need for standardized benchmarks and evaluation frameworks to understand the capabilities and weaknesses of foundation models in time series analysis.

Method: The paper formalized three tasks: recognition (True/False QA), differentiation (multiple-choice QA), and generation (open-ended descriptions). It unified four datasets to compare 13 state-of-the-art models head-to-head on these tasks.

Result: The study found that language models underperformed, vision-language models showed promise, and multimodal time series models outperformed LLMs but remained fragile under robustness testing.

Conclusion: A standardized benchmark for time series reasoning is essential, and significant improvements are required in multimodal architectures to address current fragilities.

Abstract: Many recent studies have proposed general-purpose foundation models designed
for a variety of time series analysis tasks. While several established datasets
already exist for evaluating these models, previous works frequently introduce
their models in conjunction with new datasets, limiting opportunities for
direct, independent comparisons and obscuring insights into the relative
strengths of different methods. Additionally, prior evaluations often cover
numerous tasks simultaneously, assessing a broad range of model abilities
without clearly pinpointing which capabilities contribute to overall
performance. To address these gaps, we formalize and evaluate 3 tasks that test
a model's ability to describe time series using generic natural language: (1)
recognition (True/False question-answering), (2) differentiation (multiple
choice question-answering), and (3) generation (open-ended natural language
description). We then unify 4 recent datasets to enable head-to-head model
comparisons on each task. Experimentally, in evaluating 13 state-of-the-art
language, vision--language, and time series--language models, we find that (1)
popular language-only methods largely underperform, indicating a need for time
series-specific architectures, (2) VLMs are quite successful, as expected,
identifying the value of vision models for these tasks and (3) pretrained
multimodal time series--language models successfully outperform LLMs, but still
have significant room for improvement. We also find that all approaches exhibit
clear fragility in a range of robustness tests. Overall, our benchmark provides
a standardized evaluation on a task necessary for time series reasoning
systems.

</details>


### [106] [HoPE: Hyperbolic Rotary Positional Encoding for Stable Long-Range Dependency Modeling in Large Language Models](https://arxiv.org/abs/2509.05218)
*Chang Dai,Hongyu Shan,Mingyang Song,Di Liang*

Main category: cs.CL

TL;DR: The paper introduces Hyperbolic Rotary Positional Encoding (HoPE), an improved positional encoding method for Transformer models, inspired by hyperbolic geometry, addressing the challenges of long-range dependency modeling.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of traditional positional encoding mechanisms, such as extrapolation issues in absolute encodings, degradation in performance for relative encodings on long contexts, and oscillatory patterns in Rotary Positional Encoding (RoPE) that hinder stable long-distance dependency modeling.

Method: The paper proposes HoPE, which uses hyperbolic functions and Lorentz transformations from hyperbolic geometry to reformulate positional encoding. This approach ensures monotonic decay of attention weights with token distance, improving generalization over long sequences. Theoretical analysis demonstrates that RoPE is a subset of this generalized framework.

Result: The experiments show that HoPE performs consistently better than existing positional encoding methods in perplexity evaluations across various extended sequence benchmarks.

Conclusion: HoPE resolves key limitations of RoPE and other existing methods, offering an enhanced approach to representing and generalizing long-range dependencies in Transformers. Code and data will be made publicly available.

Abstract: Positional encoding mechanisms enable Transformers to model sequential
structure and long-range dependencies in text. While absolute positional
encodings struggle with extrapolation to longer sequences due to fixed
positional representations, and relative approaches like Alibi exhibit
performance degradation on extremely long contexts, the widely-used Rotary
Positional Encoding (RoPE) introduces oscillatory attention patterns that
hinder stable long-distance dependency modelling. We address these limitations
through a geometric reformulation of positional encoding. Drawing inspiration
from Lorentz transformations in hyperbolic geometry, we propose Hyperbolic
Rotary Positional Encoding (HoPE), which leverages hyperbolic functions to
implement Lorentz rotations on token representations. Theoretical analysis
demonstrates that RoPE is a special case of our generalized formulation. HoPE
fundamentally resolves RoPE's slation issues by enforcing monotonic decay of
attention weights with increasing token distances. Extensive experimental
results, including perplexity evaluations under several extended sequence
benchmarks, show that HoPE consistently exceeds existing positional encoding
methods. These findings underscore HoPE's enhanced capacity for representing
and generalizing long-range dependencies. Data and code will be available.

</details>


### [107] [Less is More Tokens: Efficient Math Reasoning via Difficulty-Aware Chain-of-Thought Distillation](https://arxiv.org/abs/2509.05226)
*Abdul Waheed,Chancharik Mitra,Laurie Z. Wang,Deva Ramanan,Bhiksha Raj*

Main category: cs.CL

TL;DR: This paper introduces a framework for difficulty-aware reasoning, enabling models to dynamically adjust reasoning depth based on problem complexity by using post-training on curated data without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Current chain-of-thought reasoning models produce overly verbose outputs for simple problems, highlighting the need for efficiency in combining simplicity and complexity.

Method: The authors use post-training methods like supervised fine-tuning (SFT) and direct preference optimization (DPO) on curated datasets to adjust reasoning length proportional to problem difficulty, without modifying model architectures.

Result: Their approach successfully reduces reasoning length for simple problems while preserving or improving accuracy for complex ones. Combining SFT and DPO achieves the best balance between reasoning length and performance.

Conclusion: Dynamic, proportional reasoning achieved through targeted post-training allows models to optimize inference depth without sacrificing accuracy, improving overall efficiency.

Abstract: Chain-of-thought reasoning, while powerful, can produce unnecessarily verbose
output for simpler problems. We present a framework for difficulty-aware
reasoning that teaches models to dynamically adjust reasoning depth based on
problem complexity. Remarkably, we show that models can be endowed with such
dynamic inference pathways without any architectural modifications; we simply
post-train on data that is carefully curated to include chain-of-thought traces
that are proportional in length to problem difficulty. Our analysis reveals
that post-training via supervised fine-tuning (SFT) primarily captures patterns
like reasoning length and format, while direct preference optimization (DPO)
preserves reasoning accuracy, with their combination reducing length and
maintaining or improving performance. Both quantitative metrics and qualitative
assessments confirm that models can learn to "think proportionally", reasoning
minimally on simple problems while maintaining depth for complex ones.

</details>


### [108] [CURE: Controlled Unlearning for Robust Embeddings -- Mitigating Conceptual Shortcuts in Pre-Trained Language Models](https://arxiv.org/abs/2509.05230)
*Aysenur Kocak,Shuo Yang,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.CL

TL;DR: The paper introduces CURE, a framework to reduce conceptual biases in pre-trained language models using disentanglement and contrastive learning, improving robustness and fairness.


<details>
  <summary>Details</summary>
Motivation: Pre-trained language models suffer from biases due to spurious concept-driven correlations, which affect their robustness and fairness.

Method: The CURE framework uses a content extractor with a reversal network to obtain concept-irrelevant information, and a debiasing module employing contrastive learning to control biases appropriately for tasks.

Result: CURE improves F1 scores by +10 (IMDB dataset) and +2 (Yelp dataset) with minimal computational overhead, tested on three pre-trained architectures.

Conclusion: CURE offers an unsupervised and adaptable approach to mitigate conceptual biases, enhancing the reliability and fairness of language models.

Abstract: Pre-trained language models have achieved remarkable success across diverse
applications but remain susceptible to spurious, concept-driven correlations
that impair robustness and fairness. In this work, we introduce CURE, a novel
and lightweight framework that systematically disentangles and suppresses
conceptual shortcuts while preserving essential content information. Our method
first extracts concept-irrelevant representations via a dedicated content
extractor reinforced by a reversal network, ensuring minimal loss of
task-relevant information. A subsequent controllable debiasing module employs
contrastive learning to finely adjust the influence of residual conceptual
cues, enabling the model to either diminish harmful biases or harness
beneficial correlations as appropriate for the target task. Evaluated on the
IMDB and Yelp datasets using three pre-trained architectures, CURE achieves an
absolute improvement of +10 points in F1 score on IMDB and +2 points on Yelp,
while introducing minimal computational overhead. Our approach establishes a
flexible, unsupervised blueprint for combating conceptual biases, paving the
way for more reliable and fair language understanding systems.

</details>


### [109] [Uniform Information Density and Syntactic Reduction: Revisiting $\textit{that}$-Mentioning in English Complement Clauses](https://arxiv.org/abs/2509.05254)
*Hailin Hao,Elsi Kaiser*

Main category: cs.CL

TL;DR: This paper studies the hypothesis that speakers regulate information density during language production, focusing on the use of the English complementizer 'that.’


<details>
  <summary>Details</summary>
Motivation: To understand how speakers manage information density variability during language production, and refine the established UID hypothesis with advanced techniques.

Method: Analyzes conversational corpus data using machine learning and neural language models to assess information density in language production.

Result: Replicated the established link between information density and use of 'that,' showing modern embeddings capture more lexical variance than previous measures.

Conclusion: Modern, contextual embeddings improve explanation of complementizer usage beyond older subcategorization probability-based methods.

Abstract: Speakers often have multiple ways to express the same meaning. The Uniform
Information Density (UID) hypothesis suggests that speakers exploit this
variability to maintain a consistent rate of information transmission during
language production. Building on prior work linking UID to syntactic reduction,
we revisit the finding that the optional complementizer $\textit{that}$in
English complement clauses is more likely to be omitted when the clause has low
information density (i.e., more predictable). We advance this line of research
by analyzing a large-scale, contemporary conversational corpus and using
machine learning and neural language models to refine estimates of information
density. Our results replicated the established relationship between
information density and $\textit{that}$-mentioning. However, we found that
previous measures of information density based on matrix verbs'
subcategorization probability capture substantial idiosyncratic lexical
variation. By contrast, estimates derived from contextual word embeddings
account for additional variance in patterns of complementizer usage.

</details>


### [110] [Elucidating the Design Space of Decay in Linear Attention](https://arxiv.org/abs/2509.05282)
*Zhen Qin,Xuyang Shen,Yiran Zhong*

Main category: cs.CL

TL;DR: The paper examines decay mechanisms in linear complexity sequence models, focusing on four dimensions: parameterization strategy, parameter sharing, decay granularity (scalar vs. vector), and compatibility with positional encoding methods like RoPE. It identifies key findings on their impact on language modeling tasks.


<details>
  <summary>Details</summary>
Motivation: To explore and understand the decay mechanisms in sequence models, ensuring better performance in language modeling tasks and handling positional relations effectively.

Method: The paper systematically categorized decay mechanisms into four dimensions and conducted experiments across diverse language modeling tasks to analyze their impact.

Result: Experiments showed critical insights: parameterization strategies affect the effectiveness, improper parameter sharing disrupts decay values, scalar decay generally underperforms vector decay, and RoPE lacks substantial benefits for most linear attention mechanisms.

Conclusion: The study sheds light on optimal decay configurations, challenges in parameter sharing, and granularity choice, while highlighting limitations with RoPE in linear attention, providing guidance for future model design.

Abstract: This paper presents a comprehensive investigation into the decay mechanisms
inherent in linear complexity sequence models. We systematically delineate the
design space of decay mechanisms across four pivotal dimensions:
parameterization strategy, which refers to the computational methodology for
decay; parameter sharing, which involves the utilization of supplementary
parameters for decay computation; decay granularity, comparing scalar versus
vector-based decay; and compatibility with relative positional encoding
methods, such as Rotary Position Embedding (RoPE). Through an extensive series
of experiments conducted on diverse language modeling tasks, we uncovered
several critical insights. Firstly, the design of the parameterization strategy
for decay requires meticulous consideration. Our findings indicate that
effective configurations are typically confined to a specific range of
parameters. Secondly, parameter sharing cannot be used arbitrarily, as it may
cause decay values to be too large or too small, thereby significantly
impacting performance. Thirdly, under identical parameterization strategies,
scalar decay generally underperforms compared to its vector-based counterpart.
However, in certain scenarios with alternative parameterization strategies,
scalar decay may unexpectedly surpass vector decay in efficacy. Lastly, our
analysis reveals that RoPE, a commonly employed relative positional encoding
method, typically fails to provide tangible benefits to the majority of linear
attention mechanisms.

</details>


### [111] [Crosscoding Through Time: Tracking Emergence & Consolidation Of Linguistic Representations Throughout LLM Pretraining](https://arxiv.org/abs/2509.05291)
*Deniz Bayazit,Aaron Mueller,Antoine Bosselut*

Main category: cs.CL

TL;DR: The study uses sparse crosscoders to identify and track the evolution of linguistic features in large language models during pretraining, offering insights into when and how specific abilities emerge.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address a gap in understanding how large language models acquire linguistic concepts and capabilities during training, as traditional evaluation methods fall short.

Method: Sparse crosscoders and a novel metric, Relative Indirect Effects (RelIE), are used to align features across checkpoints and analyze their causal importance for task performance.

Result: The approach reveals feature emergence, maintenance, and discontinuation across training stages, providing granular insights into LLM evolution.

Conclusion: The method is scalable and architecture-agnostic, paving the way for better interpretability of representation learning processes in LLM pretraining.

Abstract: Large language models (LLMs) learn non-trivial abstractions during
pretraining, like detecting irregular plural noun subjects. However, it is not
well understood when and how specific linguistic abilities emerge as
traditional evaluation methods such as benchmarking fail to reveal how models
acquire concepts and capabilities. To bridge this gap and better understand
model training at the concept level, we use sparse crosscoders to discover and
align features across model checkpoints. Using this approach, we track the
evolution of linguistic features during pretraining. We train crosscoders
between open-sourced checkpoint triplets with significant performance and
representation shifts, and introduce a novel metric, Relative Indirect Effects
(RelIE), to trace training stages at which individual features become causally
important for task performance. We show that crosscoders can detect feature
emergence, maintenance, and discontinuation during pretraining. Our approach is
architecture-agnostic and scalable, offering a promising path toward more
interpretable and fine-grained analysis of representation learning throughout
pretraining.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [112] [Facial Emotion Recognition does not detect feeling unsafe in automated driving](https://arxiv.org/abs/2509.04490)
*Abel van Elburg,Konstantinos Gkentsidis,Mathieu Sarrazin,Sarah Barendswaard,Varun Kotian,Riender Happee*

Main category: cs.CV

TL;DR: The study investigates public risk perception of automated vehicles using driving simulator experiments, analyzing various data, including subjective risk ratings, physiological responses, and facial expressions.


<details>
  <summary>Details</summary>
Motivation: To study how perceived risk impacts public acceptance of automated vehicles and understand the role of driving styles and critical interactions.

Method: 32 participants were exposed to driving simulator tests with calm and dynamic driving styles, while physiological data, comfort ratings, and facial expressions were collected for analysis.

Result: Dynamic driving styles caused greater discomfort. Crossing pedestrians doubled discomfort in dynamic styles but had no effect in calm ones. Facial expression did not reliably reflect risk, while a neural network model effectively predicted perceived risk based on motion and skin conductance data.

Conclusion: Facial expression is unsuitable for assessing perceived risk in automated vehicles. Neural network models hold promise for objective and unbiased risk perception assessment, indicating directions for future research.

Abstract: Trust and perceived safety play a crucial role in the public acceptance of
automated vehicles. To understand perceived risk, an experiment was conducted
using a driving simulator under two automated driving styles and optionally
introducing a crossing pedestrian. Data was collected from 32 participants,
consisting of continuous subjective comfort ratings, motion, webcam footage for
facial expression, skin conductance, heart rate, and eye tracking. The
continuous subjective perceived risk ratings showed significant discomfort
associated with perceived risk during cornering and braking followed by relief
or even positive comfort on continuing the ride. The dynamic driving style
induced a stronger discomfort as compared to the calm driving style. The
crossing pedestrian did not affect discomfort with the calm driving style but
doubled the comfort decrement with the dynamic driving style. This illustrates
the importance of consequences of critical interactions in risk perception.
Facial expression was successfully analyzed for 24 participants but most
(15/24) did not show any detectable facial reaction to the critical event.
Among the 9 participants who did, 8 showed a Happy expression, and only 4
showed a Surprise expression. Fear was never dominant. This indicates that
facial expression recognition is not a reliable method for assessing perceived
risk in automated vehicles. To predict perceived risk a neural network model
was implemented using vehicle motion and skin conductance. The model correlated
well with reported perceived risk, demonstrating its potential for objective
perceived risk assessment in automated vehicles, reducing subjective bias and
highlighting areas for future research.

</details>


### [113] [PromptEnhancer: A Simple Approach to Enhance Text-to-Image Models via Chain-of-Thought Prompt Rewriting](https://arxiv.org/abs/2509.04545)
*Linqing Wang,Ximing Xing,Yiji Cheng,Zhiyuan Zhao,Jiale Tao,Qixun Wang,Ruihuang Li,Xin Li,Mingrui Wu,Xinchi Deng,Chunyu Wang,Qinglin Lu*

Main category: cs.CV

TL;DR: The paper introduces PromptEnhancer, a universal prompt rewriting framework, to improve text-to-image generation alignment without modifying model weights.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models struggle with complex prompts, resulting in a mismatch between user intent and generated outputs.

Method: The authors propose PromptEnhancer, which uses a Chain-of-Thought rewriter trained via reinforcement learning and guided by a reward model (AlignEvaluator) to rewrite prompts for better alignment.

Result: Experiments on HunyuanImage 2.1 show significant improvements in image-text alignment, particularly in semantic and compositional scenarios.

Conclusion: PromptEnhancer enhances prompt fidelity for text-to-image models, introducing a human preference benchmark to promote further research.

Abstract: Recent advancements in text-to-image (T2I) diffusion models have demonstrated
remarkable capabilities in generating high-fidelity images. However, these
models often struggle to faithfully render complex user prompts, particularly
in aspects like attribute binding, negation, and compositional relationships.
This leads to a significant mismatch between user intent and the generated
output. To address this challenge, we introduce PromptEnhancer, a novel and
universal prompt rewriting framework that enhances any pretrained T2I model
without requiring modifications to its weights. Unlike prior methods that rely
on model-specific fine-tuning or implicit reward signals like image-reward
scores, our framework decouples the rewriter from the generator. We achieve
this by training a Chain-of-Thought (CoT) rewriter through reinforcement
learning, guided by a dedicated reward model we term the AlignEvaluator. The
AlignEvaluator is trained to provide explicit and fine-grained feedback based
on a systematic taxonomy of 24 key points, which are derived from a
comprehensive analysis of common T2I failure modes. By optimizing the CoT
rewriter to maximize the reward from our AlignEvaluator, our framework learns
to generate prompts that are more precisely interpreted by T2I models.
Extensive experiments on the HunyuanImage 2.1 model demonstrate that
PromptEnhancer significantly improves image-text alignment across a wide range
of semantic and compositional challenges. Furthermore, we introduce a new,
high-quality human preference benchmark to facilitate future research in this
direction.

</details>


### [114] [Skywork UniPic 2.0: Building Kontext Model with Online RL for Unified Multimodal Model](https://arxiv.org/abs/2509.04548)
*Hongyang Wei,Baixin Xu,Hongbo Liu,Cyrus Wu,Jie Liu,Yi Peng,Peiyu Wang,Zexiang Liu,Jingwen He,Yidan Xietian,Chuanxin Tang,Zidong Wang,Yichen Wei,Liang Hu,Boyi Jiang,William Li,Ying He,Yang Liu,Xuchen Song,Eric Li,Yahui Zhou*

Main category: cs.CV

TL;DR: The paper introduces UniPic2-SD3.5M-Kontext, a 2B-parameter model for advanced image generation and editing, outperforming larger models with efficient training strategies. It also integrates into a robust multimodal framework.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address inefficiencies and suboptimal performance in open-source multimodal models that focus more on scaling parameters rather than optimizing training strategies.

Method: The authors improve the SD3.5-Medium architecture, pre-train it on high-quality data, and apply a novel Progressive Dual-Task Reinforcement (PDTR) strategy to jointly optimize text-to-image generation and editing tasks.

Result: UniPic2-SD3.5M-Kontext achieves superior performance in image generation and editing compared to larger models like BAGEL (7B) and Flux-Kontext (12B). By integrating with Qwen2.5-VL-7B, it forms UniPic2-Metaquery, excelling in multimodal tasks.

Conclusion: The proposed approach, Skywork UniPic 2.0, demonstrates a scalable and effective training paradigm for multimodal models, with state-of-the-art performance across diverse tasks.

Abstract: Recent advances in multimodal models have demonstrated impressive
capabilities in unified image generation and editing. However, many prominent
open-source models prioritize scaling model parameters over optimizing training
strategies, limiting their efficiency and performance. In this work, we present
UniPic2-SD3.5M-Kontext, a 2B-parameter DiT model based on SD3.5-Medium, which
achieves state-of-the-art image generation and editing while extending
seamlessly into a unified multimodal framework. Our approach begins with
architectural modifications to SD3.5-Medium and large-scale pre-training on
high-quality data, enabling joint text-to-image generation and editing
capabilities. To enhance instruction following and editing consistency, we
propose a novel Progressive Dual-Task Reinforcement strategy (PDTR), which
effectively strengthens both tasks in a staged manner. We empirically validate
that the reinforcement phases for different tasks are mutually beneficial and
do not induce negative interference. After pre-training and reinforcement
strategies, UniPic2-SD3.5M-Kontext demonstrates stronger image generation and
editing capabilities than models with significantly larger generation
parameters-including BAGEL (7B) and Flux-Kontext (12B). Furthermore, following
the MetaQuery, we connect the UniPic2-SD3.5M-Kontext and Qwen2.5-VL-7B via a
connector and perform joint training to launch a unified multimodal model
UniPic2-Metaquery. UniPic2-Metaquery integrates understanding, generation, and
editing, achieving top-tier performance across diverse tasks with a simple and
scalable training paradigm. This consistently validates the effectiveness and
generalizability of our proposed training paradigm, which we formalize as
Skywork UniPic 2.0.

</details>


### [115] [Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing via Bidirectional Warping](https://arxiv.org/abs/2509.04582)
*Jingyi Lu,Kai Han*

Main category: cs.CV

TL;DR: Inpaint4Drag introduces a framework for drag-based image editing using pixel-space warping and inpainting for real-time and precise manipulation.


<details>
  <summary>Details</summary>
Motivation: Current drag-based editing methods relying on latent spaces are imprecise, slow, and constrained to specific models.

Method: The framework uses bidirectional warping inspired by elastic deformation and efficient image inpainting to enable real-time editing.

Result: Inpaint4Drag achieves 0.01s previews and 0.3s inpainting at high resolution, delivering superior visual quality and control.

Conclusion: The system provides an adaptable platform for drag-based editing, compatible with any inpainting models and benefiting from future advancements.

Abstract: Drag-based image editing has emerged as a powerful paradigm for intuitive
image manipulation. However, existing approaches predominantly rely on
manipulating the latent space of generative models, leading to limited
precision, delayed feedback, and model-specific constraints. Accordingly, we
present Inpaint4Drag, a novel framework that decomposes drag-based editing into
pixel-space bidirectional warping and image inpainting. Inspired by elastic
object deformation in the physical world, we treat image regions as deformable
materials that maintain natural shape under user manipulation. Our method
achieves real-time warping previews (0.01s) and efficient inpainting (0.3s) at
512x512 resolution, significantly improving the interaction experience compared
to existing methods that require minutes per edit. By transforming drag inputs
directly into standard inpainting formats, our approach serves as a universal
adapter for any inpainting model without architecture modification,
automatically inheriting all future improvements in inpainting technology.
Extensive experiments demonstrate that our method achieves superior visual
quality and precise control while maintaining real-time performance. Project
page: https://visual-ai.github.io/inpaint4drag/

</details>


### [116] [DisPatch: Disarming Adversarial Patches in Object Detection with Diffusion Models](https://arxiv.org/abs/2509.04597)
*Jin Ma,Mohammed Aldeen,Christopher Salas,Feng Luo,Mashrur Chowdhury,Mert Pesé,Long Cheng*

Main category: cs.CV

TL;DR: This paper presents DISPATCH, a diffusion-based framework serving as a defense against adversarial patch attacks in object detection. It shows significant improvement over prior approaches.


<details>
  <summary>Details</summary>
Motivation: Object detectors are vulnerable to adversarial patch attacks that can manipulate detection outcomes, causing security risks. Existing defenses tend to rely on removing adversarial patches, which may not generalize well to all attack scenarios.

Method: DISPATCH introduces a "regenerate and rectify" strategy using diffusion models. It regenerates entire images to align them with benign data distributions and identifies and replaces adversarial areas with their benign versions.

Result: Experiments reveal that DISPATCH outperforms existing defenses, achieving an mAP.5 score of 89.3% on hiding attacks and reducing the success rate of creating attacks to 24.8%. It also shows strong robustness against adaptive attacks.

Conclusion: DISPATCH offers an effective, generalizable, and robust defense against adversarial patch attacks, improving object detection reliability in various attack scenarios.

Abstract: Object detection is fundamental to various real-world applications, such as
security monitoring and surveillance video analysis. Despite their
advancements, state-of-theart object detectors are still vulnerable to
adversarial patch attacks, which can be easily applied to real-world objects to
either conceal actual items or create non-existent ones, leading to severe
consequences. Given the current diversity of adversarial patch attacks and
potential unknown threats, an ideal defense method should be effective,
generalizable, and robust against adaptive attacks. In this work, we introduce
DISPATCH, the first diffusion-based defense framework for object detection.
Unlike previous works that aim to "detect and remove" adversarial patches,
DISPATCH adopts a "regenerate and rectify" strategy, leveraging generative
models to disarm attack effects while preserving the integrity of the input
image. Specifically, we utilize the in-distribution generative power of
diffusion models to regenerate the entire image, aligning it with benign data.
A rectification process is then employed to identify and replace adversarial
regions with their regenerated benign counterparts. DISPATCH is attack-agnostic
and requires no prior knowledge of the existing patches. Extensive experiments
across multiple detectors and attacks demonstrate that DISPATCH consistently
outperforms state-of-the-art defenses on both hiding attacks and creating
attacks, achieving the best overall mAP.5 score of 89.3% on hiding attacks, and
lowering the attack success rate to 24.8% on untargeted creating attacks.
Moreover, it maintains strong robustness against adaptive attacks, making it a
practical and reliable defense for object detection systems.

</details>


### [117] [WATCH: World-aware Allied Trajectory and pose reconstruction for Camera and Human](https://arxiv.org/abs/2509.04600)
*Qijun Ying,Zhongyuan Hu,Rui Zhang,Ronghui Li,Yu Lu,Zijiao Zeng*

Main category: cs.CV

TL;DR: The paper introduces WATCH, a method to improve global human motion reconstruction from monocular videos by addressing depth and motion ambiguities, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To overcome existing limitations in accurately reconstructing human motion and camera trajectories from monocular videos, addressing depth ambiguity and ineffective camera translation integration.

Method: WATCH combines analytical heading angle decomposition for efficient pose recalibration and a camera trajectory integration mechanism inspired by world models to leverage camera translation data effectively.

Result: WATCH achieves state-of-the-art results in trajectory reconstruction experiments on in-the-wild benchmarks.

Conclusion: The research showcases the value of jointly modeling the relationship between camera and human motion, advancing the integration of camera translation into global motion reconstruction methods.

Abstract: Global human motion reconstruction from in-the-wild monocular videos is
increasingly demanded across VR, graphics, and robotics applications, yet
requires accurate mapping of human poses from camera to world coordinates-a
task challenged by depth ambiguity, motion ambiguity, and the entanglement
between camera and human movements. While human-motion-centric approaches excel
in preserving motion details and physical plausibility, they suffer from two
critical limitations: insufficient exploitation of camera orientation
information and ineffective integration of camera translation cues. We present
WATCH (World-aware Allied Trajectory and pose reconstruction for Camera and
Human), a unified framework addressing both challenges. Our approach introduces
an analytical heading angle decomposition technique that offers superior
efficiency and extensibility compared to existing geometric methods.
Additionally, we design a camera trajectory integration mechanism inspired by
world models, providing an effective pathway for leveraging camera translation
information beyond naive hard-decoding approaches. Through experiments on
in-the-wild benchmarks, WATCH achieves state-of-the-art performance in
end-to-end trajectory reconstruction. Our work demonstrates the effectiveness
of jointly modeling camera-human motion relationships and offers new insights
for addressing the long-standing challenge of camera translation integration in
global human motion reconstruction. The code will be available publicly.

</details>


### [118] [Sali4Vid: Saliency-Aware Video Reweighting and Adaptive Caption Retrieval for Dense Video Captioning](https://arxiv.org/abs/2509.04602)
*MinJu Jeon,Si-Woo Kim,Ye-Chan Kim,HyunGee Kim,Dong-Jin Kim*

Main category: cs.CV

TL;DR: The paper introduces Sali4Vid, a saliency-aware framework that improves dense video captioning by assigning frame-specific importance weights and incorporating semantic-based adaptive caption retrieval.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of previous methods, such as treating all video frames equally and overlooking scene transitions while retrieving captions, in dense video captioning tasks.

Method: The method involves (1) Saliency-aware Video Reweighting, which assigns importance weights to frames using timestamp annotations, and (2) Semantic-based Adaptive Caption Retrieval, which uses frame similarity to segment videos and improve caption retrieval.

Result: Sali4Vid achieved state-of-the-art results on the YouCook2 and ViTT datasets, showing the efficacy of the proposed approach.

Conclusion: Jointly improving video frame weighting and scene-aware caption retrieval can significantly enhance dense video captioning, as demonstrated by Sali4Vid's results.

Abstract: Dense video captioning aims to temporally localize events in video and
generate captions for each event. While recent works propose end-to-end models,
they suffer from two limitations: (1) applying timestamp supervision only to
text while treating all video frames equally, and (2) retrieving captions from
fixed-size video chunks, overlooking scene transitions. To address these, we
propose Sali4Vid, a simple yet effective saliency-aware framework. We introduce
Saliency-aware Video Reweighting, which converts timestamp annotations into
sigmoid-based frame importance weights, and Semantic-based Adaptive Caption
Retrieval, which segments videos by frame similarity to capture scene
transitions and improve caption retrieval. Sali4Vid achieves state-of-the-art
results on YouCook2 and ViTT, demonstrating the benefit of jointly improving
video weighting and retrieval for dense video captioning

</details>


### [119] [UAV-Based Intelligent Traffic Surveillance System: Real-Time Vehicle Detection, Classification, Tracking, and Behavioral Analysis](https://arxiv.org/abs/2509.04624)
*Ali Khanpour,Tianyi Wang,Afra Vahidi-Shams,Wim Ectors,Farzam Nakhaie,Amirhossein Taheri,Christian Claudel*

Main category: cs.CV

TL;DR: This paper presents a UAV-based traffic surveillance system that achieves high accuracy in vehicle detection, classification, tracking, and violation analysis in urban settings.


<details>
  <summary>Details</summary>
Motivation: Traditional traffic monitoring systems struggle with limited coverage, adaptability, and scalability, prompting the need for more advanced solutions.

Method: The system uses multi-scale/multi-angle template matching, Kalman filtering, homography-based calibration, and geofencing for UAV-enabled traffic monitoring in urban areas.

Result: The system demonstrates high performance metrics: detection precision of 91.8%, F1-score of 90.5%, and tracking MOTA/MOTP of 92.1% and 93.7%, alongside successful vehicle classification and violation detection.

Conclusion: The UAV-based system proves scalable, accurate, and practical for comprehensive traffic monitoring, supporting smart city implementation without reliance on traditional infrastructure.

Abstract: Traffic congestion and violations pose significant challenges for urban
mobility and road safety. Traditional traffic monitoring systems, such as fixed
cameras and sensor-based methods, are often constrained by limited coverage,
low adaptability, and poor scalability. To address these challenges, this paper
introduces an advanced unmanned aerial vehicle (UAV)-based traffic surveillance
system capable of accurate vehicle detection, classification, tracking, and
behavioral analysis in real-world, unconstrained urban environments. The system
leverages multi-scale and multi-angle template matching, Kalman filtering, and
homography-based calibration to process aerial video data collected from
altitudes of approximately 200 meters. A case study in urban area demonstrates
robust performance, achieving a detection precision of 91.8%, an F1-score of
90.5%, and tracking metrics (MOTA/MOTP) of 92.1% and 93.7%, respectively.
Beyond precise detection, the system classifies five vehicle types and
automatically detects critical traffic violations, including unsafe lane
changes, illegal double parking, and crosswalk obstructions, through the fusion
of geofencing, motion filtering, and trajectory deviation analysis. The
integrated analytics module supports origin-destination tracking, vehicle count
visualization, inter-class correlation analysis, and heatmap-based congestion
modeling. Additionally, the system enables entry-exit trajectory profiling,
vehicle density estimation across road segments, and movement direction
logging, supporting comprehensive multi-scale urban mobility analytics.
Experimental results confirms the system's scalability, accuracy, and practical
relevance, highlighting its potential as an enforcement-aware,
infrastructure-independent traffic monitoring solution for next-generation
smart cities.

</details>


### [120] [VCMamba: Bridging Convolutions with Multi-Directional Mamba for Efficient Visual Representation](https://arxiv.org/abs/2509.04669)
*Mustafa Munir,Alex Zhang,Radu Marculescu*

Main category: cs.CV

TL;DR: VCMamba integrates CNNs and Mamba SSMs to harness both local and global features efficiently. It achieves competitive results on ImageNet-1K and ADE20K with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: ViTs and SSMs offer global reasoning and efficiency but lack local feature extraction, prompting the need for a hybrid model that bridges these strengths with CNNs.

Method: VCMamba combines a convolutional stem and hierarchical convolutional blocks in its early stages with multi-directional Mamba blocks later to extract both local and global features while maintaining linear complexity.

Result: VCMamba-B achieves 82.6% top-1 accuracy on ImageNet-1K, surpassing benchmarks like PlainMamba-L3 and Vision GNN-B, and delivers strong mIoU performance on ADE20K with substantially reduced parameters.

Conclusion: VCMamba effectively combines CNNs and Mamba SSM strategies into a unified architecture, offering superior performance and parameter efficiency.

Abstract: Recent advances in Vision Transformers (ViTs) and State Space Models (SSMs)
have challenged the dominance of Convolutional Neural Networks (CNNs) in
computer vision. ViTs excel at capturing global context, and SSMs like Mamba
offer linear complexity for long sequences, yet they do not capture
fine-grained local features as effectively as CNNs. Conversely, CNNs possess
strong inductive biases for local features but lack the global reasoning
capabilities of transformers and Mamba. To bridge this gap, we introduce
\textit{VCMamba}, a novel vision backbone that integrates the strengths of CNNs
and multi-directional Mamba SSMs. VCMamba employs a convolutional stem and a
hierarchical structure with convolutional blocks in its early stages to extract
rich local features. These convolutional blocks are then processed by later
stages incorporating multi-directional Mamba blocks designed to efficiently
model long-range dependencies and global context. This hybrid design allows for
superior feature representation while maintaining linear complexity with
respect to image resolution. We demonstrate VCMamba's effectiveness through
extensive experiments on ImageNet-1K classification and ADE20K semantic
segmentation. Our VCMamba-B achieves 82.6% top-1 accuracy on ImageNet-1K,
surpassing PlainMamba-L3 by 0.3% with 37% fewer parameters, and outperforming
Vision GNN-B by 0.3% with 64% fewer parameters. Furthermore, VCMamba-B obtains
47.1 mIoU on ADE20K, exceeding EfficientFormer-L7 by 2.0 mIoU while utilizing
62% fewer parameters. Code is available at
https://github.com/Wertyuui345/VCMamba.

</details>


### [121] [Guideline-Consistent Segmentation via Multi-Agent Refinement](https://arxiv.org/abs/2509.04687)
*Vanshika Vats,Ashwani Rathee,James Davis*

Main category: cs.CV

TL;DR: This paper presents a training-free framework for semantic segmentation that combines vision-language models and a Worker-Supervisor loop for guideline-adherent masks.


<details>
  <summary>Details</summary>
Motivation: Human and automated segmentation approaches often fail to adhere strictly to long and complex textual guidelines, necessitating improvements.

Method: The approach involves a training-free framework with a Worker-Supervisor iterative refinement process, leveraging vision-language models and a reinforcement learning stop policy.

Result: The framework outperforms state-of-the-art baselines on the Waymo and ReasonSeg datasets, showing strong generalization and instruction adherence.

Conclusion: The proposed method demonstrates the ability to achieve guideline-consistent segmentation without retraining, providing a flexible solution for evolving guidelines.

Abstract: Semantic segmentation in real-world applications often requires not only
accurate masks but also strict adherence to textual labeling guidelines. These
guidelines are typically complex and long, and both human and automated
labeling often fail to follow them faithfully. Traditional approaches depend on
expensive task-specific retraining that must be repeated as the guidelines
evolve. Although recent open-vocabulary segmentation methods excel with simple
prompts, they often fail when confronted with sets of paragraph-length
guidelines that specify intricate segmentation rules. To address this, we
introduce a multi-agent, training-free framework that coordinates
general-purpose vision-language models within an iterative Worker-Supervisor
refinement architecture. The Worker performs the segmentation, the Supervisor
critiques it against the retrieved guidelines, and a lightweight reinforcement
learning stop policy decides when to terminate the loop, ensuring
guideline-consistent masks while balancing resource use. Evaluated on the Waymo
and ReasonSeg datasets, our method notably outperforms state-of-the-art
baselines, demonstrating strong generalization and instruction adherence.

</details>


### [122] [Domain Adaptation for Different Sensor Configurations in 3D Object Detection](https://arxiv.org/abs/2509.04711)
*Satoshi Tanaka,Kok Seang Tan,Isamu Yamashita*

Main category: cs.CV

TL;DR: The paper addresses the challenge of adapting 3D object detection models across different LiDAR sensor configurations, proposing fine-tuning techniques to improve model generalization.


<details>
  <summary>Details</summary>
Motivation: The need to address performance degradation when models trained on one LiDAR sensor configuration are applied to another due to shifts in point cloud distribution.

Method: Two techniques are introduced: (1) Downstream Fine-tuning, involving dataset-specific updates after multi-dataset training, and (2) Partial Layer Fine-tuning, focusing on selective layer updates for better generalization.

Result: Experiments using datasets from the same geographic region with diverse sensor setups show superior performance in joint training when combined with the proposed fine-tuning techniques.

Conclusion: The proposed methods offer practical and scalable solutions to tackle sensor configuration-related domain gaps in 3D object detection for autonomous driving.

Abstract: Recent advances in autonomous driving have underscored the importance of
accurate 3D object detection, with LiDAR playing a central role due to its
robustness under diverse visibility conditions. However, different vehicle
platforms often deploy distinct sensor configurations, causing performance
degradation when models trained on one configuration are applied to another
because of shifts in the point cloud distribution. Prior work on multi-dataset
training and domain adaptation for 3D object detection has largely addressed
environmental domain gaps and density variation within a single LiDAR; in
contrast, the domain gap for different sensor configurations remains largely
unexplored. In this work, we address domain adaptation across different sensor
configurations in 3D object detection. We propose two techniques: Downstream
Fine-tuning (dataset-specific fine-tuning after multi-dataset training) and
Partial Layer Fine-tuning (updating only a subset of layers to improve
cross-configuration generalization). Using paired datasets collected in the
same geographic region with multiple sensor configurations, we show that joint
training with Downstream Fine-tuning and Partial Layer Fine-tuning consistently
outperforms naive joint training for each configuration. Our findings provide a
practical and scalable solution for adapting 3D object detection models to the
diverse vehicle platforms.

</details>


### [123] [CD-Mamba: Cloud detection with long-range spatial dependency modeling](https://arxiv.org/abs/2509.04729)
*Tianxiang Xue,Jiayi Zhao,Jingsheng Li,Changlu Chen,Kun Zhan*

Main category: cs.CV

TL;DR: The paper presents CD-Mamba, a hybrid model combining convolutional neural networks and Mamba state-space modeling for enhanced cloud detection in remote sensing images.


<details>
  <summary>Details</summary>
Motivation: Address the challenges posed by cloud cover obstructing remote sensing imagery and improve data reliability.

Method: Integrate convolutional methods for local spatial dependencies with Mamba state-space modeling for long-range atmospheric dependencies, forming a unified network.

Result: CD-Mamba successfully captures detailed pixel-level textures and larger patch-level dependencies, delivering superior cloud detection performance compared to prior approaches.

Conclusion: This hybrid architecture enhances accuracy in cloud detection across various spatial scales and proves effective through extensive experiments.

Abstract: Remote sensing images are frequently obscured by cloud cover, posing
significant challenges to data integrity and reliability. Effective cloud
detection requires addressing both short-range spatial redundancies and
long-range atmospheric similarities among cloud patches. Convolutional neural
networks are effective at capturing local spatial dependencies, while Mamba has
strong capabilities in modeling long-range dependencies. To fully leverage both
local spatial relations and long-range dependencies, we propose CD-Mamba, a
hybrid model that integrates convolution and Mamba's state-space modeling into
a unified cloud detection network. CD-Mamba is designed to comprehensively
capture pixelwise textural details and long term patchwise dependencies for
cloud detection. This design enables CD-Mamba to manage both pixel-wise
interactions and extensive patch-wise dependencies simultaneously, improving
detection accuracy across diverse spatial scales. Extensive experiments
validate the effectiveness of CD-Mamba and demonstrate its superior performance
over existing methods.

</details>


### [124] [Exploiting Unlabeled Structures through Task Consistency Training for Versatile Medical Image Segmentation](https://arxiv.org/abs/2509.04732)
*Shengqian Zhu,Jiafei Wu,Xiaogang Xu,Chengrong Yu,Ying Song,Zhang Yi,Guangjun Li,Junjie Hu*

Main category: cs.CV

TL;DR: The paper introduces the Task Consistency Training (TCT) framework for versatile medical image segmentation, addressing class imbalance issues in partially labeled datasets without relying on additional models.


<details>
  <summary>Details</summary>
Motivation: Current versatile medical image segmentation faces challenges due to class imbalance in partially labeled datasets, and existing methods often require extra models or suffer from label noise.

Method: The proposed TCT framework employs a backbone network with a main segmentation head and multiple auxiliary task heads, enforces consistency constraints, filters low-consistency noisy data, and introduces a unified auxiliary uncertainty-weighted loss to improve segmentation performance.

Result: The proposed method demonstrates effectiveness through extensive experiments on eight abdominal datasets from diverse clinical sites.

Conclusion: The TCT framework provides an efficient solution for class imbalance in medical image segmentation without extra models, showcasing its practical applicability and robustness.

Abstract: Versatile medical image segmentation (VMIS) targets the segmentation of
multiple classes, while obtaining full annotations for all classes is often
impractical due to the time and labor required. Leveraging partially labeled
datasets (PLDs) presents a promising alternative; however, current VMIS
approaches face significant class imbalance due to the unequal category
distribution in PLDs. Existing methods attempt to address this by generating
pseudo-full labels. Nevertheless, these typically require additional models and
often result in potential performance degradation from label noise. In this
work, we introduce a Task Consistency Training (TCT) framework to address class
imbalance without requiring extra models. TCT includes a backbone network with
a main segmentation head (MSH) for multi-channel predictions and multiple
auxiliary task heads (ATHs) for task-specific predictions. By enforcing a
consistency constraint between the MSH and ATH predictions, TCT effectively
utilizes unlabeled anatomical structures. To avoid error propagation from
low-consistency, potentially noisy data, we propose a filtering strategy to
exclude such data. Additionally, we introduce a unified auxiliary
uncertainty-weighted loss (UAUWL) to mitigate segmentation quality declines
caused by the dominance of specific tasks. Extensive experiments on eight
abdominal datasets from diverse clinical sites demonstrate our approach's
effectiveness.

</details>


### [125] [Enhancing Self-Driving Segmentation in Adverse Weather Conditions: A Dual Uncertainty-Aware Training Approach to SAM Optimization](https://arxiv.org/abs/2509.04735)
*Dharsan Ravindran,Kevin Wang,Zhuoyuan Cao,Saleh Abdelrahman,Jeffery Wu*

Main category: cs.CV

TL;DR: The paper focuses on enhancing vision foundation models like SAM2 for resilient autonomous driving image segmentation under adverse weather by incorporating uncertainty-aware techniques.


<details>
  <summary>Details</summary>
Motivation: Current state-of-the-art vision foundation models struggle with visual ambiguity in adverse weather due to lack of uncertainty quantification, creating potential risks for safety-critical autonomous driving.

Method: The authors employ a two-pronged strategy: (1) a fine-tuning process for SAM2 incorporating uncertainty metrics into the loss function, and (2) adapting the Uncertainty-Aware Adapter (UAT) from medical imaging to driving contexts.

Result: Experimental evaluations on CamVid, BDD100K, and GTA datasets demonstrate that UAT-SAM surpasses standard SAM in extreme weather conditions, while SAM2 with uncertainty-aware loss improves performance across varied driving scenarios.

Conclusion: Integrating uncertainty-aware techniques significantly boosts reliability and robustness of segmentation models under challenging environments, highlighting their importance for safe autonomous driving applications.

Abstract: Recent advances in vision foundation models, such as the Segment Anything
Model (SAM) and its successor SAM2, have achieved state-of-the-art performance
on general image segmentation benchmarks. However, these models struggle in
adverse weather conditions where visual ambiguity is high, largely due to their
lack of uncertainty quantification. Inspired by progress in medical imaging,
where uncertainty-aware training has improved reliability in ambiguous cases,
we investigate two approaches to enhance segmentation robustness for autonomous
driving. First, we introduce a multi-step finetuning procedure for SAM2 that
incorporates uncertainty metrics directly into the loss function, improving
overall scene recognition. Second, we adapt the Uncertainty-Aware Adapter
(UAT), originally designed for medical image segmentation, to driving contexts.
We evaluate both methods on CamVid, BDD100K, and GTA driving datasets.
Experiments show that UAT-SAM outperforms standard SAM in extreme weather,
while SAM2 with uncertainty-aware loss achieves improved performance across
diverse driving scenes. These findings underscore the value of explicit
uncertainty modeling for safety-critical autonomous driving in challenging
environments.

</details>


### [126] [WatchHAR: Real-time On-device Human Activity Recognition System for Smartwatches](https://arxiv.org/abs/2509.04736)
*Taeyoung Yeon,Vasco Xu,Henry Hoffmann,Karan Ahuja*

Main category: cs.CV

TL;DR: WatchHAR is a smartwatch-based Human Activity Recognition (HAR) system utilizing audio and inertial data for accurate and fast activity classification without external data processing.


<details>
  <summary>Details</summary>
Motivation: To develop a fully on-smartwatch HAR system that addresses privacy and latency concerns while operating effectively in unconstrained environments.

Method: The paper introduces WatchHAR, which unifies sensor data preprocessing and inference into an end-to-end trainable module optimized for performance. This system combines audio and inertial data for activity recognition.

Result: WatchHAR achieves over 90% accuracy across 25+ activity classes, processes event detection in 9.3 ms, and completes multimodal classification in 11.8 ms, outperforming state-of-the-art models.

Conclusion: The research highlights the potential of smartwatches as standalone, privacy-conscious devices for continuous activity tracking without reliance on external data processing.

Abstract: Despite advances in practical and multimodal fine-grained Human Activity
Recognition (HAR), a system that runs entirely on smartwatches in unconstrained
environments remains elusive. We present WatchHAR, an audio and inertial-based
HAR system that operates fully on smartwatches, addressing privacy and latency
issues associated with external data processing. By optimizing each component
of the pipeline, WatchHAR achieves compounding performance gains. We introduce
a novel architecture that unifies sensor data preprocessing and inference into
an end-to-end trainable module, achieving 5x faster processing while
maintaining over 90% accuracy across more than 25 activity classes. WatchHAR
outperforms state-of-the-art models for event detection and activity
classification while running directly on the smartwatch, achieving 9.3 ms
processing time for activity event detection and 11.8 ms for multimodal
activity classification. This research advances on-device activity recognition,
realizing smartwatches' potential as standalone, privacy-aware, and
minimally-invasive continuous activity tracking devices.

</details>


### [127] [MCANet: A Multi-Scale Class-Specific Attention Network for Multi-Label Post-Hurricane Damage Assessment using UAV Imagery](https://arxiv.org/abs/2509.04757)
*Zhangding Liu,Neda Mohammadi,John E. Taylor*

Main category: cs.CV

TL;DR: MCANet improves hurricane damage assessment by employing a Res2Net-based architecture and multi-head class-specific residual attention to achieve high accuracy.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in existing CNN-based methods, such as difficulties in capturing multi-scale spatial features and distinguishing similar damage types.

Method: MCANet uses a Res2Net-based hierarchical backbone and multi-head class-specific residual attention modules to focus on relevant spatial areas and balance local detail with global context.

Result: MCANet achieves a high mAP of 91.75%, outperforming existing models such as ResNet and EfficientNet. Adding eight attention heads increases the mAP to 92.35%.

Conclusion: MCANet effectively identifies hurricane damage types, localizes damage-relevant regions for interpretability, and supports disaster response applications. Future work aims to enhance adaptability and semantic capabilities using advanced knowledge graphs and multimodal large language models.

Abstract: Rapid and accurate post-hurricane damage assessment is vital for disaster
response and recovery. Yet existing CNN-based methods struggle to capture
multi-scale spatial features and to distinguish visually similar or
co-occurring damage types. To address these issues, we propose MCANet, a
multi-label classification framework that learns multi-scale representations
and adaptively attends to spatially relevant regions for each damage category.
MCANet employs a Res2Net-based hierarchical backbone to enrich spatial context
across scales and a multi-head class-specific residual attention module to
enhance discrimination. Each attention branch focuses on different spatial
granularities, balancing local detail with global context. We evaluate MCANet
on the RescueNet dataset of 4,494 UAV images collected after Hurricane Michael.
MCANet achieves a mean average precision (mAP) of 91.75%, outperforming ResNet,
Res2Net, VGG, MobileNet, EfficientNet, and ViT. With eight attention heads,
performance further improves to 92.35%, boosting average precision for
challenging classes such as Road Blocked by over 6%. Class activation mapping
confirms MCANet's ability to localize damage-relevant regions, supporting
interpretability. Outputs from MCANet can inform post-disaster risk mapping,
emergency routing, and digital twin-based disaster response. Future work could
integrate disaster-specific knowledge graphs and multimodal large language
models to improve adaptability to unseen disasters and enrich semantic
understanding for real-world decision-making.

</details>


### [128] [Dynamic Group Detection using VLM-augmented Temporal Groupness Graph](https://arxiv.org/abs/2509.04758)
*Kaname Yokoyama,Chihiro Nakatani,Norimichi Ukita*

Main category: cs.CV

TL;DR: This paper presents a new dynamic human group detection method in videos using an enhanced Vision-Language Model (VLM) and global optimization techniques.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting dynamically changing human groups in videos, which requires considering both local appearance features and global scene context.

Method: The authors proposed augmenting a Vision-Language Model (VLM) for extracting local and global features, coupled with global optimization using a graph structure representing groupness probabilities over all frames.

Result: Experimental results showed that the proposed method outperformed existing state-of-the-art group detection techniques on public datasets.

Conclusion: The study demonstrates the feasibility and effectiveness of dynamically detecting changing human groups in videos by leveraging advanced VLM and optimization strategies.

Abstract: This paper proposes dynamic human group detection in videos. For detecting
complex groups, not only the local appearance features of in-group members but
also the global context of the scene are important. Such local and global
appearance features in each frame are extracted using a Vision-Language Model
(VLM) augmented for group detection in our method. For further improvement, the
group structure should be consistent over time. While previous methods are
stabilized on the assumption that groups are not changed in a video, our method
detects dynamically changing groups by global optimization using a graph with
all frames' groupness probabilities estimated by our groupness-augmented CLIP
features. Our experimental results demonstrate that our method outperforms
state-of-the-art group detection methods on public datasets. Code:
https://github.com/irajisamurai/VLM-GroupDetection.git

</details>


### [129] [FloodVision: Urban Flood Depth Estimation Using Foundation Vision-Language Models and Domain Knowledge Graph](https://arxiv.org/abs/2509.04772)
*Zhangding Liu,Neda Mohammadi,John E. Taylor*

Main category: cs.CV

TL;DR: FloodVision introduces a framework to estimate floodwater depth using images, leveraging GPT-4o and a knowledge graph of urban dimensions to improve accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Flood depth estimation is essential for road accessibility and emergency response, but current methods lack accuracy and generalization.

Method: The framework integrates GPT-4o's semantic reasoning with a domain knowledge graph containing real-world dimensions of urban objects to compute accurate depth values dynamically from RGB images.

Result: FloodVision achieved a mean absolute error of 8.17 cm, improving accuracy by 20.5% over the GPT-4o baseline and outperforming CNN-based methods.

Conclusion: FloodVision provides accurate, generalizable, and real-time flood depth estimation, suitable for use in smart city systems.

Abstract: Timely and accurate floodwater depth estimation is critical for road
accessibility and emergency response. While recent computer vision methods have
enabled flood detection, they suffer from both accuracy limitations and poor
generalization due to dependence on fixed object detectors and task-specific
training. To enable accurate depth estimation that can generalize across
diverse flood scenarios, this paper presents FloodVision, a zero-shot framework
that combines the semantic reasoning abilities of the foundation
vision-language model GPT-4o with a structured domain knowledge graph. The
knowledge graph encodes canonical real-world dimensions for common urban
objects including vehicles, people, and infrastructure elements to ground the
model's reasoning in physical reality. FloodVision dynamically identifies
visible reference objects in RGB images, retrieves verified heights from the
knowledge graph to mitigate hallucination, estimates submergence ratios, and
applies statistical outlier filtering to compute final depth values. Evaluated
on 110 crowdsourced images from MyCoast New York, FloodVision achieves a mean
absolute error of 8.17 cm, reducing the GPT-4o baseline 10.28 cm by 20.5% and
surpassing prior CNN-based methods. The system generalizes well across varying
scenes and operates in near real-time, making it suitable for future
integration into digital twin platforms and citizen-reporting apps for smart
city flood resilience.

</details>


### [130] [MLP-SRGAN: A Single-Dimension Super Resolution GAN using MLP-Mixer](https://arxiv.org/abs/2303.06298)
*Samir Mitha,Seungho Choe,Pejman Jahbedar Maralani,Alan R. Moody,April Khademi*

Main category: cs.CV

TL;DR: MLP-SRGAN, a novel architecture combining MLP-Mixers and convolutional layers, achieves superior upsampling performance in MRI slice dimensions with smaller model size and faster evaluation/training.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve the spatial resolution of MRI images in the slice direction using MLP-based generative adversarial networks.

Method: MLP-SRGAN incorporates Multi-Layer Perceptron Mixers and convolutional layers, trained on high-resolution MRI datasets and evaluated using several quality metrics.

Result: MLP-SRGAN outperforms state-of-the-art SR networks in sharpness, blurriness reduction, and detail preservation while being computationally efficient.

Conclusion: The proposed method is effective for enhancing resolution in medical imaging with fewer computational requirements, making it accessible for clinical applications.

Abstract: We propose a novel architecture called MLP-SRGAN, which is a single-dimension
Super Resolution Generative Adversarial Network (SRGAN) that utilizes
Multi-Layer Perceptron Mixers (MLP-Mixers) along with convolutional layers to
upsample in the slice direction. MLP-SRGAN is trained and validated using high
resolution (HR) FLAIR MRI from the MSSEG2 challenge dataset. The method was
applied to three multicentre FLAIR datasets (CAIN, ADNI, CCNA) of images with
low spatial resolution in the slice dimension to examine performance on
held-out (unseen) clinical data. Upsampled results are compared to several
state-of-the-art SR networks. For images with high resolution (HR) ground
truths, peak-signal-to-noise-ratio (PSNR) and structural similarity index
(SSIM) are used to measure upsampling performance. Several new structural,
no-reference image quality metrics were proposed to quantify sharpness (edge
strength), noise (entropy), and blurriness (low frequency information) in the
absence of ground truths. Results show MLP-SRGAN results in sharper edges, less
blurring, preserves more texture and fine-anatomical detail, with fewer
parameters, faster training/evaluation time, and smaller model size than
existing methods. Code for MLP-SRGAN training and inference, data generators,
models and no-reference image quality metrics will be available at
https://github.com/IAMLAB-Ryerson/MLP-SRGAN.

</details>


### [131] [Hybrid-Tower: Fine-grained Pseudo-query Interaction and Generation for Text-to-Video Retrieval](https://arxiv.org/abs/2509.04773)
*Bangxiang Lan,Ruobing Xie,Ruixiang Zhao,Xingwu Sun,Zhanhui Kang,Gang Yang,Xirong Li*

Main category: cs.CV

TL;DR: The paper introduces a Hybrid-Tower framework for Text-to-Video Retrieval (T2VR), combining advantages of Two-Tower and Single-Tower approaches for improved effectiveness and efficiency.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome the limitations of existing T2VR approaches, where Two-Tower frameworks are inefficient and Single-Tower ones lack effectiveness.

Method: The introduced method, Pseudo-query Interaction and Generation (PIG), uses pseudo-queries generated for each video to enable fine-grained interaction of features, mimicking Single-Tower effectiveness without storage or computation overhead.

Result: Experiments across five benchmarks show significant improvement in retrieval performance (1.6%-3.9% increase in R@1) with efficiency comparable to Two-Tower models.

Conclusion: The Hybrid-Tower framework with PIG elevates retrieval effectiveness and efficiency, achieving near state-of-the-art results in T2VR tasks.

Abstract: The Text-to-Video Retrieval (T2VR) task aims to retrieve unlabeled videos by
textual queries with the same semantic meanings. Recent CLIP-based approaches
have explored two frameworks: Two-Tower versus Single-Tower framework, yet the
former suffers from low effectiveness, while the latter suffers from low
efficiency. In this study, we explore a new Hybrid-Tower framework that can
hybridize the advantages of the Two-Tower and Single-Tower framework, achieving
high effectiveness and efficiency simultaneously. We propose a novel hybrid
method, Fine-grained Pseudo-query Interaction and Generation for T2VR, ie, PIG,
which includes a new pseudo-query generator designed to generate a pseudo-query
for each video. This enables the video feature and the textual features of
pseudo-query to interact in a fine-grained manner, similar to the Single-Tower
approaches to hold high effectiveness, even before the real textual query is
received. Simultaneously, our method introduces no additional storage or
computational overhead compared to the Two-Tower framework during the inference
stage, thus maintaining high efficiency. Extensive experiments on five commonly
used text-video retrieval benchmarks demonstrate that our method achieves a
significant improvement over the baseline, with an increase of $1.6\% \sim
3.9\%$ in R@1. Furthermore, our method matches the efficiency of Two-Tower
models while achieving near state-of-the-art performance, highlighting the
advantages of the Hybrid-Tower framework.

</details>


### [132] [Comparative Evaluation of Traditional and Deep Learning Feature Matching Algorithms using Chandrayaan-2 Lunar Data](https://arxiv.org/abs/2509.04775)
*R. Makharia,J. G. Singla,Amitabh,N. Dube,H. Sharma*

Main category: cs.CV

TL;DR: The paper evaluates five feature matching algorithms for cross-modality lunar image registration, with SuperGlue providing the best performance.


<details>
  <summary>Details</summary>
Motivation: Accurate image registration is crucial for lunar exploration, aiding tasks like surface mapping, resource localization, and mission planning.

Method: The paper evaluates SIFT, ASIFT, AKAZE, RIFT2, and SuperGlue on cross-modality lunar images, alongside proposing a preprocessing pipeline for resolving issues like resolution and sensor distortions.

Result: SuperGlue outperformed all other methods in accuracy and runtime efficiency, while classical methods showed region-dependent limitations, particularly in polar lighting.

Conclusion: Learning-based algorithms like SuperGlue, combined with robust preprocessing, are essential for effective lunar image registration across varied conditions.

Abstract: Accurate image registration is critical for lunar exploration, enabling
surface mapping, resource localization, and mission planning. Aligning data
from diverse lunar sensors -- optical (e.g., Orbital High Resolution Camera,
Narrow and Wide Angle Cameras), hyperspectral (Imaging Infrared Spectrometer),
and radar (e.g., Dual-Frequency Synthetic Aperture Radar, Selene/Kaguya
mission) -- is challenging due to differences in resolution, illumination, and
sensor distortion. We evaluate five feature matching algorithms: SIFT, ASIFT,
AKAZE, RIFT2, and SuperGlue (a deep learning-based matcher), using
cross-modality image pairs from equatorial and polar regions. A preprocessing
pipeline is proposed, including georeferencing, resolution alignment, intensity
normalization, and enhancements like adaptive histogram equalization, principal
component analysis, and shadow correction. SuperGlue consistently yields the
lowest root mean square error and fastest runtimes. Classical methods such as
SIFT and AKAZE perform well near the equator but degrade under polar lighting.
The results highlight the importance of preprocessing and learning-based
approaches for robust lunar image registration across diverse conditions.

</details>


### [133] [Teacher-Student Model for Detecting and Classifying Mitosis in the MIDOG 2025 Challenge](https://arxiv.org/abs/2509.03614)
*Seungho Choe,Xiaoli Qin,Abubakr Shafique,Amanda Dy,Susan Done,Dimitrios Androutsos,April Khademi*

Main category: cs.CV

TL;DR: This paper proposes a unified AI model to improve the detection and classification of mitoses in pathology, addressing challenges like domain shift and data imbalance.


<details>
  <summary>Details</summary>
Motivation: Counting mitotic figures manually is time-intensive and inconsistent between observers, and there is a need for AI solutions to ensure enhanced efficiency and reliability.

Method: The study employs a UNet segmentation backbone with domain generalization modules such as contrastive representation learning and domain-adversarial training. It uses a teacher-student strategy for generating enhanced pseudo-masks and integrates a multi-scale CNN classifier for both mitosis detection and classification.

Result: The authors achieved an F1 score of 0.7660 for mitosis detection and balanced accuracy of 0.8414 for atypical mitosis classification on the preliminary test set.

Conclusion: The proposed framework effectively combines segmentation-based detection and classification to deliver a robust solution for mitosis analysis, highlighting its potential for use in pathology applications.

Abstract: Counting mitotic figures is time-intensive for pathologists and leads to
inter-observer variability. Artificial intelligence (AI) promises a solution by
automatically detecting mitotic figures while maintaining decision consistency.
However, AI tools are susceptible to domain shift, where a significant drop in
performance can occur due to differences in the training and testing sets,
including morphological diversity between organs, species, and variations in
staining protocols. Furthermore, the number of mitoses is much less than the
count of normal nuclei, which introduces severely imbalanced data for the
detection task. In this work, we formulate mitosis detection as a pixel-level
segmentation and propose a teacher-student model that simultaneously addresses
mitosis detection (Track 1) and atypical mitosis classification (Track 2). Our
method is based on a UNet segmentation backbone that integrates domain
generalization modules, namely contrastive representation learning and
domain-adversarial training. A teacher-student strategy is employed to generate
pixel-level pseudo-masks not only for annotated mitoses and hard negatives but
also for normal nuclei, thereby enhancing feature discrimination and improving
robustness against domain shift. For the classification task, we introduce a
multi-scale CNN classifier that leverages feature maps from the segmentation
model within a multi-task learning paradigm. On the preliminary test set, the
algorithm achieved an F1 score of 0.7660 in Track 1 and balanced accuracy of
0.8414 in Track 2, demonstrating the effectiveness of integrating
segmentation-based detection and classification into a unified framework for
robust mitosis analysis.

</details>


### [134] [Toward Accessible Dermatology: Skin Lesion Classification Using Deep Learning Models on Mobile-Acquired Images](https://arxiv.org/abs/2509.04800)
*Asif Newaz,Masum Mushfiq Ishti,A Z M Ashraful Azam,Asif Ur Rahman Adib*

Main category: cs.CV

TL;DR: The paper presents a Transformer-based approach for mobile-acquired skin lesion classification, showing superior performance and interpretability, aimed at accessible dermatological screening.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of costly and inaccessible diagnostic methods and the narrow scope of current automated deep learning studies for skin disease classification.

Method: Curated a dataset with over 50 skin diseases captured via mobile devices, tested convolutional and Transformer-based models, and incorporated Grad-CAM for visual interpretability.

Result: Transformer models, specifically Swin Transformer, outperformed others by effectively capturing global contextual features and highlighting clinically relevant regions.

Conclusion: Transformer-based methods, supported by interpretability tools like Grad-CAM, have strong potential for accessible AI-driven dermatological diagnostics, especially in low-resource settings.

Abstract: Skin diseases are among the most prevalent health concerns worldwide, yet
conventional diagnostic methods are often costly, complex, and unavailable in
low-resource settings. Automated classification using deep learning has emerged
as a promising alternative, but existing studies are mostly limited to
dermoscopic datasets and a narrow range of disease classes. In this work, we
curate a large dataset of over 50 skin disease categories captured with mobile
devices, making it more representative of real-world conditions. We evaluate
multiple convolutional neural networks and Transformer-based architectures,
demonstrating that Transformer models, particularly the Swin Transformer,
achieve superior performance by effectively capturing global contextual
features. To enhance interpretability, we incorporate Gradient-weighted Class
Activation Mapping (Grad-CAM), which highlights clinically relevant regions and
provides transparency in model predictions. Our results underscore the
potential of Transformer-based approaches for mobile-acquired skin lesion
classification, paving the way toward accessible AI-assisted dermatological
screening and early diagnosis in resource-limited environments.

</details>


### [135] [Extracting Uncertainty Estimates from Mixtures of Experts for Semantic Segmentation](https://arxiv.org/abs/2509.04816)
*Svetlana Pavlitska,Beyza Keskin,Alwin Faßbender,Christian Hubschneider,J. Marius Zöllner*

Main category: cs.CV

TL;DR: The paper investigates predictive uncertainty extraction methods for Mixtures of Experts (MoEs) in computer vision, showing they yield better-calibrated results than ensembles, especially for out-of-distribution data.


<details>
  <summary>Details</summary>
Motivation: The study seeks to improve the reliability and uncertainty estimation of computer vision models in safety-critical scenarios, such as traffic scene perception, by exploring efficient MoE-based approaches as alternatives to ensembles.

Method: Three methods—predictive entropy, mutual information, and expert variance—are applied for uncertainty estimation. An MoE with two experts is trained on semantically split data (A2D2 dataset), and routing uncertainty is assessed using gate entropy.

Result: MoEs provide more reliable uncertainty estimates compared to ensembles under OOD conditions. Simple gate mechanisms perform better in uncertainty calibration, and adding more experts enhances results further.

Conclusion: MoEs are a promising alternative to ensembles for well-calibrated uncertainty estimation in computer vision, with better efficiency and suitability for safety-critical applications. Increasing expert count improves calibration.

Abstract: Estimating accurate and well-calibrated predictive uncertainty is important
for enhancing the reliability of computer vision models, especially in
safety-critical applications like traffic scene perception. While ensemble
methods are commonly used to quantify uncertainty by combining multiple models,
a mixture of experts (MoE) offers an efficient alternative by leveraging a
gating network to dynamically weight expert predictions based on the input.
Building on the promising use of MoEs for semantic segmentation in our previous
works, we show that well-calibrated predictive uncertainty estimates can be
extracted from MoEs without architectural modifications. We investigate three
methods to extract predictive uncertainty estimates: predictive entropy, mutual
information, and expert variance. We evaluate these methods for an MoE with two
experts trained on a semantical split of the A2D2 dataset. Our results show
that MoEs yield more reliable uncertainty estimates than ensembles in terms of
conditional correctness metrics under out-of-distribution (OOD) data.
Additionally, we evaluate routing uncertainty computed via gate entropy and
find that simple gating mechanisms lead to better calibration of routing
uncertainty estimates than more complex classwise gates. Finally, our
experiments on the Cityscapes dataset suggest that increasing the number of
experts can further enhance uncertainty calibration. Our code is available at
https://github.com/KASTEL-MobilityLab/mixtures-of-experts/.

</details>


### [136] [Exploring Non-Local Spatial-Angular Correlations with a Hybrid Mamba-Transformer Framework for Light Field Super-Resolution](https://arxiv.org/abs/2509.04824)
*Haosong Liu,Xiancheng Zhu,Huanqiang Zeng,Jianqing Zhu,Jiuwen Cao,Junhui Hou*

Main category: cs.CV

TL;DR: This paper proposes LFMT, a hybrid Mamba-Transformer framework for light field image super-resolution, achieving better performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current Mamba-based methods for light field image super-resolution face inefficiencies in feature extraction due to multi-directional scanning strategies.

Method: Introduced Subspace Simple Scanning (Sub-SS) and Subspace Simple Mamba Block (SSMB) for efficient feature extraction, along with a dual-stage strategy for comprehensive spatial-angular correlations.

Result: LFMT demonstrates superior performance compared to state-of-the-art methods while maintaining low computational complexity across real and synthetic datasets.

Conclusion: The proposed LFMT framework enhances both computational efficiency and feature extraction precision, making it a promising tool for light field super-resolution tasks.

Abstract: Recently, Mamba-based methods, with its advantage in long-range information
modeling and linear complexity, have shown great potential in optimizing both
computational cost and performance of light field image super-resolution
(LFSR). However, current multi-directional scanning strategies lead to
inefficient and redundant feature extraction when applied to complex LF data.
To overcome this challenge, we propose a Subspace Simple Scanning (Sub-SS)
strategy, based on which we design the Subspace Simple Mamba Block (SSMB) to
achieve more efficient and precise feature extraction. Furthermore, we propose
a dual-stage modeling strategy to address the limitation of state space in
preserving spatial-angular and disparity information, thereby enabling a more
comprehensive exploration of non-local spatial-angular correlations.
Specifically, in stage I, we introduce the Spatial-Angular Residual Subspace
Mamba Block (SA-RSMB) for shallow spatial-angular feature extraction; in stage
II, we use a dual-branch parallel structure combining the Epipolar Plane Mamba
Block (EPMB) and Epipolar Plane Transformer Block (EPTB) for deep epipolar
feature refinement. Building upon meticulously designed modules and strategies,
we introduce a hybrid Mamba-Transformer framework, termed LFMT. LFMT integrates
the strengths of Mamba and Transformer models for LFSR, enabling comprehensive
information exploration across spatial, angular, and epipolar-plane domains.
Experimental results demonstrate that LFMT significantly outperforms current
state-of-the-art methods in LFSR, achieving substantial improvements in
performance while maintaining low computational complexity on both real-word
and synthetic LF datasets.

</details>


### [137] [PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination](https://arxiv.org/abs/2509.04833)
*Ming Dai,Wenxuan Cheng,Jiedong Zhuang,Jiang-jiang Liu,Hongshen Zhao,Zhenhua Feng,Wankou Yang*

Main category: cs.CV

TL;DR: The paper presents PropVG, a novel proposal-based framework for visual grounding that integrates object proposal generation with referential object comprehension, using Contrastive-based Refer Scoring and Multi-granularity Target Discrimination modules to enhance performance.


<details>
  <summary>Details</summary>
Motivation: Existing visual grounding methods are inefficient and don't leverage prominent prospective targets or multi-granularity discrimination, which are needed for robust object identification.

Method: The authors propose PropVG, an end-to-end framework combining object proposal generation and referential object comprehension. It uses a Contrastive-based Refer Scoring module for contrastive learning and a Multi-granularity Target Discrimination module for integrating object- and semantic-level information.

Result: PropVG demonstrates effectiveness through extensive experiments on visual grounding benchmarks such as gRefCOCO, Ref-ZOM, R-RefCOCO, and RefCOCO.

Conclusion: PropVG addresses inefficiencies and enhances multi-granular discrimination in visual grounding, representing a significant advance in end-to-end proposal-based frameworks. The code is publicly available.

Abstract: Recent advances in visual grounding have largely shifted away from
traditional proposal-based two-stage frameworks due to their inefficiency and
high computational complexity, favoring end-to-end direct reference paradigms.
However, these methods rely exclusively on the referred target for supervision,
overlooking the potential benefits of prominent prospective targets. Moreover,
existing approaches often fail to incorporate multi-granularity discrimination,
which is crucial for robust object identification in complex scenarios. To
address these limitations, we propose PropVG, an end-to-end proposal-based
framework that, to the best of our knowledge, is the first to seamlessly
integrate foreground object proposal generation with referential object
comprehension without requiring additional detectors. Furthermore, we introduce
a Contrastive-based Refer Scoring (CRS) module, which employs contrastive
learning at both sentence and word levels to enhance the capability in
understanding and distinguishing referred objects. Additionally, we design a
Multi-granularity Target Discrimination (MTD) module that fuses object- and
semantic-level information to improve the recognition of absent targets.
Extensive experiments on gRefCOCO (GREC/GRES), Ref-ZOM, R-RefCOCO, and RefCOCO
(REC/RES) benchmarks demonstrate the effectiveness of PropVG. The codes and
models are available at https://github.com/Dmmm1997/PropVG.

</details>


### [138] [Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet](https://arxiv.org/abs/2509.05198)
*Mohammad Saeid,Amir Salarpour,Pedram MohajerAnsari*

Main category: cs.CV

TL;DR: The paper refines an existing 3D point cloud dataset (ModelNet40) into ModelNet-R and introduces a lightweight neural network, Point-SkipNet, significantly improving classification accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper addresses limitations in the commonly used ModelNet40 dataset, such as inconsistent labeling and insufficient class differentiation, which negatively impact the performance of 3D point cloud models.

Method: The authors developed ModelNet-R, an enhanced dataset tackling the flaws of ModelNet40. Additionally, they designed Point-SkipNet, a graph-based neural network employing efficient sampling, neighborhood grouping, and skip connections.

Result: Experiments indicate substantial performance enhancements when models are trained on ModelNet-R. Point-SkipNet achieves state-of-the-art accuracy with fewer parameters compared to other models.

Conclusion: High-quality datasets like ModelNet-R are pivotal for optimizing model efficiency in 3D point cloud classification. Point-SkipNet further demonstrates the synergy between data and model design.

Abstract: The classification of 3D point clouds is crucial for applications such as
autonomous driving, robotics, and augmented reality. However, the commonly used
ModelNet40 dataset suffers from limitations such as inconsistent labeling, 2D
data, size mismatches, and inadequate class differentiation, which hinder model
performance. This paper introduces ModelNet-R, a meticulously refined version
of ModelNet40 designed to address these issues and serve as a more reliable
benchmark. Additionally, this paper proposes Point-SkipNet, a lightweight
graph-based neural network that leverages efficient sampling, neighborhood
grouping, and skip connections to achieve high classification accuracy with
reduced computational overhead. Extensive experiments demonstrate that models
trained in ModelNet-R exhibit significant performance improvements. Notably,
Point-SkipNet achieves state-of-the-art accuracy on ModelNet-R with a
substantially lower parameter count compared to contemporary models. This
research highlights the crucial role of dataset quality in optimizing model
efficiency for 3D point cloud classification. For more details, see the code
at: https://github.com/m-saeid/ModeNetR_PointSkipNet.

</details>


### [139] [TemporalFlowViz: Parameter-Aware Visual Analytics for Interpreting Scramjet Combustion Evolution](https://arxiv.org/abs/2509.04834)
*Yifei Jia,Shiyu Cheng,Yu Dong,Guan Li,Dong Tian,Ruixiao Peng,Xuyi Lu,Yu Wang,Wei Yao,Guihua Shan*

Main category: cs.CV

TL;DR: The paper introduces TemporalFlowViz, a visual analytics system for analyzing scramjet combustion simulations by clustering, visualizing, and summarizing temporal flow fields using Vision Transformers and vision-language models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges posed by large-scale and high-dimensional data from scramjet combustion simulations, which hinder effective interpretation and comparison of flow fields crucial for advancing high-speed propulsion technologies.

Method: The method involves leveraging Vision Transformers to extract embeddings from combustion simulation frames, applying dimensionality reduction and clustering for latent mode discovery, and utilizing vision-language models to annotate and summarize data to support expert-driven analysis.

Result: The system demonstrated enhanced hypothesis generation, interpretable pattern discovery, and improved knowledge discovery during expert-informed case studies and received positive feedback from domain specialists.

Conclusion: TemporalFlowViz successfully bridges the gap between latent data representations and expert reasoning, offering significant benefits for large-scale scramjet combustion analysis and advancing high-speed propulsion technologies.

Abstract: Understanding the complex combustion dynamics within scramjet engines is
critical for advancing high-speed propulsion technologies. However, the large
scale and high dimensionality of simulation-generated temporal flow field data
present significant challenges for visual interpretation, feature
differentiation, and cross-case comparison. In this paper, we present
TemporalFlowViz, a parameter-aware visual analytics workflow and system
designed to support expert-driven clustering, visualization, and interpretation
of temporal flow fields from scramjet combustion simulations. Our approach
leverages hundreds of simulated combustion cases with varying initial
conditions, each producing time-sequenced flow field images. We use pretrained
Vision Transformers to extract high-dimensional embeddings from these frames,
apply dimensionality reduction and density-based clustering to uncover latent
combustion modes, and construct temporal trajectories in the embedding space to
track the evolution of each simulation over time. To bridge the gap between
latent representations and expert reasoning, domain specialists annotate
representative cluster centroids with descriptive labels. These annotations are
used as contextual prompts for a vision-language model, which generates
natural-language summaries for individual frames and full simulation cases. The
system also supports parameter-based filtering, similarity-based case
retrieval, and coordinated multi-view exploration to facilitate in-depth
analysis. We demonstrate the effectiveness of TemporalFlowViz through two
expert-informed case studies and expert feedback, showing TemporalFlowViz
enhances hypothesis generation, supports interpretable pattern discovery, and
enhances knowledge discovery in large-scale scramjet combustion analysis.

</details>


### [140] [Pose-Free 3D Quantitative Phase Imaging of Flowing Cellular Populations](https://arxiv.org/abs/2509.04848)
*Enze Ye,Wei Lin,Shaochi Ren,Yakun Liu,Xiaoping Li,Hao Wang,He Sun,Feng Pan*

Main category: cs.CV

TL;DR: The paper presents OmniFHT, a pose-free framework for 3D refractive index (RI) imaging in flow cytometry with arbitrary geometries and rotations.


<details>
  <summary>Details</summary>
Motivation: Current 3D quantitative phase imaging methods assume uniform cell rotation along a single axis, limiting their application to spherical cells and preventing accurate analysis of more complex, irregularly shaped cells.

Method: OmniFHT employs the Fourier diffraction theorem and implicit neural representations (INRs) for reconstructing 3D RI distributions of cells. It simultaneously optimizes rotational trajectory and cell structure using weak scattering assumptions and supports reconstruction with sparse projections.

Result: The approach achieves high-fidelity cell reconstructions even with just 10 projection angles or limited angular coverage of 120 degrees, overcoming prior constraints on shape and rotation assumptions.

Conclusion: OmniFHT enables unbiased, label-free, high-throughput tomographic imaging for broader cellular populations in flow cytometry, enhancing morphometric analysis scalability and accuracy.

Abstract: High-throughput 3D quantitative phase imaging (QPI) in flow cytometry enables
label-free, volumetric characterization of individual cells by reconstructing
their refractive index (RI) distributions from multiple viewing angles during
flow through microfluidic channels. However, current imaging methods assume
that cells undergo uniform, single-axis rotation, which require their poses to
be known at each frame. This assumption restricts applicability to
near-spherical cells and prevents accurate imaging of irregularly shaped cells
with complex rotations. As a result, only a subset of the cellular population
can be analyzed, limiting the ability of flow-based assays to perform robust
statistical analysis. We introduce OmniFHT, a pose-free 3D RI reconstruction
framework that leverages the Fourier diffraction theorem and implicit neural
representations (INRs) for high-throughput flow cytometry tomographic imaging.
By jointly optimizing each cell's unknown rotational trajectory and volumetric
structure under weak scattering assumptions, OmniFHT supports arbitrary cell
geometries and multi-axis rotations. Its continuous representation also allows
accurate reconstruction from sparsely sampled projections and restricted
angular coverage, producing high-fidelity results with as few as 10 views or
only 120 degrees of angular range. OmniFHT enables, for the first time, in
situ, high-throughput tomographic imaging of entire flowing cell populations,
providing a scalable and unbiased solution for label-free morphometric analysis
in flow cytometry platforms.

</details>


### [141] [CoRe-GS: Coarse-to-Refined Gaussian Splatting with Semantic Object Focus](https://arxiv.org/abs/2509.04859)
*Hannah Schieber,Dominik Frischmann,Simon Boche,Victor Schaack,Angela Schoellig,Stefan Leutenegger,Daniel Roth*

Main category: cs.CV

TL;DR: This paper presents CoRe-GS, a method that accelerates 3D reconstruction of specific objects in a scene for aerial robotics using semantic Gaussian Splatting.


<details>
  <summary>Details</summary>
Motivation: The need to efficiently perform 3D reconstruction of specific objects in scenes for applications such as tele-guidance and disaster response, where speed and accuracy are critical.

Method: The authors propose CoRe-GS, which uses semantic Gaussian Splatting to generate a coarse scene and then refines it with a novel color-based filtering to isolate and reconstruct the object of interest more efficiently.

Result: The approach reduces training time by about 25% compared to full semantic GS training, while improving novel-view-synthesis quality. It is validated on real-world and synthetic datasets.

Conclusion: CoRe-GS effectively balances high-quality reconstruction and reduced computation time, making it suitable for applications in mobile autonomous robotics.

Abstract: Mobile reconstruction for autonomous aerial robotics holds strong potential
for critical applications such as tele-guidance and disaster response. These
tasks demand both accurate 3D reconstruction and fast scene processing. Instead
of reconstructing the entire scene in detail, it is often more efficient to
focus on specific objects, i.e., points of interest (PoIs). Mobile robots
equipped with advanced sensing can usually detect these early during data
acquisition or preliminary analysis, reducing the need for full-scene
optimization. Gaussian Splatting (GS) has recently shown promise in delivering
high-quality novel view synthesis and 3D representation by an incremental
learning process. Extending GS with scene editing, semantics adds useful
per-splat features to isolate objects effectively.
  Semantic 3D Gaussian editing can already be achieved before the full training
cycle is completed, reducing the overall training time. Moreover, the
semantically relevant area, the PoI, is usually already known during capturing.
To balance high-quality reconstruction with reduced training time, we propose
CoRe-GS. We first generate a coarse segmentation-ready scene with semantic GS
and then refine it for the semantic object using our novel color-based
effective filtering for effective object isolation. This is speeding up the
training process to be about a quarter less than a full training cycle for
semantic GS. We evaluate our approach on two datasets, SCRREAM (real-world,
outdoor) and NeRDS 360 (synthetic, indoor), showing reduced runtime and higher
novel-view-synthesis quality.

</details>


### [142] [Cryo-RL: automating prostate cancer cryoablation planning with reinforcement learning](https://arxiv.org/abs/2509.04886)
*Trixia Simangan,Ahmed Nadeem Abbasi,Yipeng Hu,Shaheer U. Saeed*

Main category: cs.CV

TL;DR: Cryo-RL is a reinforcement learning-based framework for automatic planning of cryoprobe placement during prostate cancer cryoablation, achieving high performance and efficiency compared to both automated and human methods.


<details>
  <summary>Details</summary>
Motivation: The current manual approach to cryoablation planning is expertise-dependent, time-intensive, prone to variability, and limits scalability.

Method: Cryo-RL formulates cryoablation planning as a Markov decision process, using reinforcement learning to optimally place cryoprobes based on a reward function evaluating tumour coverage. It operates in a simulated environment modeling clinical constraints.

Result: On 583 retrospective cases, Cryo-RL achieved over 8 percentage-point Dice score improvement compared to existing automated methods and matched human expert performance, while requiring significantly less planning time.

Conclusion: Cryo-RL demonstrates the potential for reinforcement learning to improve efficiency, reproducibility, and clinical viability in cryoablation planning for prostate cancer treatment.

Abstract: Cryoablation is a minimally invasive localised treatment for prostate cancer
that destroys malignant tissue during de-freezing, while sparing surrounding
healthy structures. Its success depends on accurate preoperative planning of
cryoprobe placements to fully cover the tumour and avoid critical anatomy. This
planning is currently manual, expertise-dependent, and time-consuming, leading
to variability in treatment quality and limited scalability. In this work, we
introduce Cryo-RL, a reinforcement learning framework that models cryoablation
planning as a Markov decision process and learns an optimal policy for
cryoprobe placement. Within a simulated environment that models clinical
constraints and stochastic intraoperative variability, an agent sequentially
selects cryoprobe positions and ice sphere diameters. Guided by a reward
function based on tumour coverage, this agent learns a cryoablation strategy
that leads to optimal cryoprobe placements without the need for any
manually-designed plans. Evaluated on 583 retrospective prostate cancer cases,
Cryo-RL achieved over 8 percentage-point Dice improvements compared with the
best automated baselines, based on geometric optimisation, and matched human
expert performance while requiring substantially less planning time. These
results highlight the potential of reinforcement learning to deliver clinically
viable, reproducible, and efficient cryoablation plans.

</details>


### [143] [SpiderNets: Estimating Fear Ratings of Spider-Related Images with Vision Models](https://arxiv.org/abs/2509.04889)
*Dominik Pegler,David Steyrl,Mengfan Zhang,Alexander Karner,Jozsef Arato,Frank Scharnowski,Filip Melinscak*

Main category: cs.CV

TL;DR: Pretrained computer vision models can predict human fear levels from spider images, achieving average errors between 10.1 and 11.0 on a standardized dataset.


<details>
  <summary>Details</summary>
Motivation: To advance adaptive computerized exposure therapy using visual stimuli responsive to patient emotions.

Method: Transfer learning adapted three pretrained computer vision models evaluated via cross-validation on fear prediction from a 313-image dataset.

Result: The models achieved average MAE of 10.1–11.0, with reduced dataset size degrading performance and spider-related features driving predictions.

Conclusion: Explainable computer vision models are promising for emotion-aware therapies, but require sufficient data and focus on model interpretability.

Abstract: Advances in computer vision have opened new avenues for clinical
applications, particularly in computerized exposure therapy where visual
stimuli can be dynamically adjusted based on patient responses. As a critical
step toward such adaptive systems, we investigated whether pretrained computer
vision models can accurately predict fear levels from spider-related images. We
adapted three diverse models using transfer learning to predict human fear
ratings (on a 0-100 scale) from a standardized dataset of 313 images. The
models were evaluated using cross-validation, achieving an average mean
absolute error (MAE) between 10.1 and 11.0. Our learning curve analysis
revealed that reducing the dataset size significantly harmed performance,
though further increases yielded no substantial gains. Explainability
assessments showed the models' predictions were based on spider-related
features. A category-wise error analysis further identified visual conditions
associated with higher errors (e.g., distant views and artificial/painted
spiders). These findings demonstrate the potential of explainable computer
vision models in predicting fear ratings, highlighting the importance of both
model explainability and a sufficient dataset size for developing effective
emotion-aware therapeutic technologies.

</details>


### [144] [SynGen-Vision: Synthetic Data Generation for training industrial vision models](https://arxiv.org/abs/2509.04894)
*Alpana Dubey,Suma Mani Kuriakose,Nitish Bhardwaj*

Main category: cs.CV

TL;DR: The paper introduces a synthetic data generation approach for industrial wear and tear detection, achieving a superior mAP50 score of 0.87 in rust detection.


<details>
  <summary>Details</summary>
Motivation: Data curation for training wear and tear detection CV models is challenging due to the lack of datasets for various scenarios.

Method: The approach uses a vision language model, combined with a 3D simulation and rendering engine, to create synthetic datasets for rust detection.

Result: The synthetic data was used to train a model for rust detection, which performed better than existing methods on real images with a high mAP50 score of 0.87.

Conclusion: The proposed method is effective, customizable, and extendable to other wear and tear detection scenarios, offering a solution to data scarcity in predictive maintenance.

Abstract: We propose an approach to generate synthetic data to train computer vision
(CV) models for industrial wear and tear detection. Wear and tear detection is
an important CV problem for predictive maintenance tasks in any industry.
However, data curation for training such models is expensive and time-consuming
due to the unavailability of datasets for different wear and tear scenarios.
Our approach employs a vision language model along with a 3D simulation and
rendering engine to generate synthetic data for varying rust conditions. We
evaluate our approach by training a CV model for rust detection using the
generated dataset and tested the trained model on real images of rusted
industrial objects. The model trained with the synthetic data generated by our
approach, outperforms the other approaches with a mAP50 score of 0.87. The
approach is customizable and can be easily extended to other industrial wear
and tear detection scenarios

</details>


### [145] [Evaluating Multiple Instance Learning Strategies for Automated Sebocyte Droplet Counting](https://arxiv.org/abs/2509.04895)
*Maryam Adelipour,Gustavo Carneiro,Jeongkwon Kim*

Main category: cs.CV

TL;DR: This paper addresses the challenge of quantifying lipid droplets in sebocytes and introduces an automated attention-based MIL framework for image analysis. Two models were evaluated: a stable baseline MLP and an attention-based MIL approach.


<details>
  <summary>Details</summary>
Motivation: Quantifying lipid droplets in sebocytes is crucial for understanding sebocyte biology, but manual counting is labor-intensive and subjective. Automated solutions are needed to overcome these limitations.

Method: The authors annotated Nile Red-stained sebocyte images into 14 classes based on droplet counts and expanded the dataset to 50,000 cells using data augmentation. They benchmarked a baseline MLP trained on aggregated patch-level counts against an attention-based MIL model using ResNet-50 features with instance weighting.

Result: The baseline MLP achieved more stable performance (mean MAE = 5.6), while the attention-based MIL model was less consistent (mean MAE = 10.7) but occasionally outperformed the MLP in certain folds.

Conclusion: Simple bag-level aggregation via the baseline MLP provides a robust solution for droplet counting. The attention-based MIL model requires improved pooling and regularization to unlock its full potential.

Abstract: Sebocytes are lipid-secreting cells whose differentiation is marked by the
accumulation of intracellular lipid droplets, making their quantification a key
readout in sebocyte biology. Manual counting is labor-intensive and subjective,
motivating automated solutions. Here, we introduce a simple attention-based
multiple instance learning (MIL) framework for sebocyte image analysis. Nile
Red-stained sebocyte images were annotated into 14 classes according to droplet
counts, expanded via data augmentation to about 50,000 cells. Two models were
benchmarked: a baseline multi-layer perceptron (MLP) trained on aggregated
patch-level counts, and an attention-based MIL model leveraging ResNet-50
features with instance weighting. Experiments using five-fold cross-validation
showed that the baseline MLP achieved more stable performance (mean MAE = 5.6)
compared with the attention-based MIL, which was less consistent (mean MAE =
10.7) but occasionally superior in specific folds. These findings indicate that
simple bag-level aggregation provides a robust baseline for slide-level droplet
counting, while attention-based MIL requires task-aligned pooling and
regularization to fully realize its potential in sebocyte image analysis.

</details>


### [146] [UniView: Enhancing Novel View Synthesis From A Single Image By Unifying Reference Features](https://arxiv.org/abs/2509.04932)
*Haowang Cui,Rui Chen,Tao Luo,Rui Li,Jiaze Wang*

Main category: cs.CV

TL;DR: UniView proposes a novel approach for synthesizing novel image views by using reference images and advanced alignment techniques, significantly outperforming current methods.


<details>
  <summary>Details</summary>
Motivation: Current novel view synthesis methods often struggle with severe distortions due to reliance on ambiguous priors and interpolation, highlighting a need for better handling of unobserved regions.

Method: The method introduces UniView, which uses a retrieval system supported by a multimodal large language model (MLLM) to select relevant reference images. A plug-and-play adapter module with multi-level isolation layers generates reference features, while a decoupled triple attention mechanism aligns and integrates multi-branch features for synthesis.

Result: UniView demonstrates significant performance improvements in novel view synthesis, outperforming state-of-the-art methods on challenging datasets.

Conclusion: UniView provides a robust framework for synthesizing high-quality novel views through effective leveraging of reference images and innovative feature integration techniques.

Abstract: The task of synthesizing novel views from a single image is highly ill-posed
due to multiple explanations for unobserved areas. Most current methods tend to
generate unseen regions from ambiguity priors and interpolation near input
views, which often lead to severe distortions. To address this limitation, we
propose a novel model dubbed as UniView, which can leverage reference images
from a similar object to provide strong prior information during view
synthesis. More specifically, we construct a retrieval and augmentation system
and employ a multimodal large language model (MLLM) to assist in selecting
reference images that meet our requirements. Additionally, a plug-and-play
adapter module with multi-level isolation layers is introduced to dynamically
generate reference features for the target views. Moreover, in order to
preserve the details of an original input image, we design a decoupled triple
attention mechanism, which can effectively align and integrate multi-branch
features into the synthesis process. Extensive experiments have demonstrated
that our UniView significantly improves novel view synthesis performance and
outperforms state-of-the-art methods on the challenging datasets.

</details>


### [147] [Efficient Video-to-Audio Generation via Multiple Foundation Models Mapper](https://arxiv.org/abs/2509.04957)
*Gehui Chen,Guan'an Wang,Xiaowen Huang,Jitao Sang*

Main category: cs.CV

TL;DR: The paper introduces the "Multiple Foundation Model Mapper" (MFM-Mapper), a novel approach for enhancing Video-to-Audio generation using dual visual encoders and GPT-2 for efficient and better feature alignment and performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of resource-intensive training in Video-to-Audio (V2A) generation and improve feature extraction for semantic and temporal consistency.

Method: The paper proposes combining dual visual encoders to extract richer features and employs GPT-2 as a mapper for improved alignment between cross-modal features.

Result: The MFM-Mapper achieves superior semantic and temporal consistency in V2A generation while requiring significantly less training—only 16% compared to previous models.

Conclusion: The MFM-Mapper demonstrates competitive performance with greater efficiency, proving that dual encoders and GPT-2 integration can advance V2A generation models without substantial training resources.

Abstract: Recent Video-to-Audio (V2A) generation relies on extracting semantic and
temporal features from video to condition generative models. Training these
models from scratch is resource intensive. Consequently, leveraging foundation
models (FMs) has gained traction due to their cross-modal knowledge transfer
and generalization capabilities. One prior work has explored fine-tuning a
lightweight mapper network to connect a pre-trained visual encoder with a
text-to-audio generation model for V2A. Inspired by this, we introduce the
Multiple Foundation Model Mapper (MFM-Mapper). Compared to the previous mapper
approach, MFM-Mapper benefits from richer semantic and temporal information by
fusing features from dual visual encoders. Furthermore, by replacing a linear
mapper with GPT-2, MFM-Mapper improves feature alignment, drawing parallels
between cross-modal features mapping and autoregressive translation tasks. Our
MFM-Mapper exhibits remarkable training efficiency. It achieves better
performance in semantic and temporal consistency with fewer training consuming,
requiring only 16\% of the training scale compared to previous mapper-based
work, yet achieves competitive performance with models trained on a much larger
scale.

</details>


### [148] [Dual-Domain Perspective on Degradation-Aware Fusion: A VLM-Guided Robust Infrared and Visible Image Fusion Framework](https://arxiv.org/abs/2509.05000)
*Tianpei Zhang,Jufeng Zhao,Yiming Zhu,Guangmang Cui*

Main category: cs.CV

TL;DR: This paper introduces GD^2Fusion, a new framework enhancing infrared-visible image fusion by handling degradation through a dual-domain approach and vision-language models.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with degraded dual-source inputs, requiring multiple manual pre-enhancement steps that can degrade performance.

Method: The framework uses Vision-Language Models for perception and employs a dual-domain system — Guided Frequency Modality-Specific Extraction (GFMSE) for frequency-domain processing and Guided Spatial Modality-Aggregated Fusion (GSMAF) for spatial-domain processing — to optimize fusion.

Result: Experiments show GD^2Fusion outperforms existing IVIF methods in handling dual-source degraded input scenarios, delivering superior quantitative and qualitative results.

Conclusion: The integrated GD^2Fusion framework effectively eliminates error accumulation in fusion pipelines, offering enhanced performance and will release its code publicly.

Abstract: Most existing infrared-visible image fusion (IVIF) methods assume
high-quality inputs, and therefore struggle to handle dual-source degraded
scenarios, typically requiring manual selection and sequential application of
multiple pre-enhancement steps. This decoupled pre-enhancement-to-fusion
pipeline inevitably leads to error accumulation and performance degradation. To
overcome these limitations, we propose Guided Dual-Domain Fusion (GD^2Fusion),
a novel framework that synergistically integrates vision-language models (VLMs)
for degradation perception with dual-domain (frequency/spatial) joint
optimization. Concretely, the designed Guided Frequency Modality-Specific
Extraction (GFMSE) module performs frequency-domain degradation perception and
suppression and discriminatively extracts fusion-relevant sub-band features.
Meanwhile, the Guided Spatial Modality-Aggregated Fusion (GSMAF) module carries
out cross-modal degradation filtering and adaptive multi-source feature
aggregation in the spatial domain to enhance modality complementarity and
structural consistency. Extensive qualitative and quantitative experiments
demonstrate that GD^2Fusion achieves superior fusion performance compared with
existing algorithms and strategies in dual-source degraded scenarios. The code
will be publicly released after acceptance of this paper.

</details>


### [149] [Interpretable Deep Transfer Learning for Breast Ultrasound Cancer Detection: A Multi-Dataset Study](https://arxiv.org/abs/2509.05004)
*Mohammad Abbadi,Yassine Himeur,Shadi Atalla,Wathiq Mansoor*

Main category: cs.CV

TL;DR: This paper evaluates machine learning and deep learning models for breast cancer classification via ultrasound imaging, with ResNet-18 achieving 99.7% accuracy and high sensitivity for malignant lesions.


<details>
  <summary>Details</summary>
Motivation: Breast cancer poses a significant mortality risk worldwide, and reliable diagnostic tools for early detection using ultrasound imaging are crucial, especially for dense breast tissues.

Method: The study uses datasets like BUSI, BUS-BRA, and BrEaST-Lesions USG to evaluate classical machine learning models (e.g., SVM, KNN) alongside deep learning models (e.g., ResNet-18, EfficientNet-B0) for breast cancer classification. Grad-CAM is employed for explainability.

Result: ResNet-18 achieves the highest accuracy (99.7%) and perfect sensitivity for identifying malignant lesions, outperforming classical ML models. Classical models perform better when coupled with feature extraction from deep networks.

Conclusion: The research supports AI-based diagnostic tools for clinical workflows and shows the feasibility of deploying high-performing and interpretable systems for detecting breast cancer from ultrasound images.

Abstract: Breast cancer remains a leading cause of cancer-related mortality among women
worldwide. Ultrasound imaging, widely used due to its safety and
cost-effectiveness, plays a key role in early detection, especially in patients
with dense breast tissue. This paper presents a comprehensive study on the
application of machine learning and deep learning techniques for breast cancer
classification using ultrasound images. Using datasets such as BUSI, BUS-BRA,
and BrEaST-Lesions USG, we evaluate classical machine learning models (SVM,
KNN) and deep convolutional neural networks (ResNet-18, EfficientNet-B0,
GoogLeNet). Experimental results show that ResNet-18 achieves the highest
accuracy (99.7%) and perfect sensitivity for malignant lesions. Classical ML
models, though outperformed by CNNs, achieve competitive performance when
enhanced with deep feature extraction. Grad-CAM visualizations further improve
model transparency by highlighting diagnostically relevant image regions. These
findings support the integration of AI-based diagnostic tools into clinical
workflows and demonstrate the feasibility of deploying high-performing,
interpretable systems for ultrasound-based breast cancer detection.

</details>


### [150] [A biologically inspired separable learning vision model for real-time traffic object perception in Dark](https://arxiv.org/abs/2509.05012)
*Hulin Li,Qiliang Ren,Jun Li,Hanbing Wei,Zheng Liu,Linfang Fan*

Main category: cs.CV

TL;DR: The study addresses challenges in low-light traffic scenes perception by introducing a new dataset (Dark-traffic) and a novel model (SLVM) designed for improved performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing perception models struggle in low-light environments due to illumination issues and the lack of a dedicated benchmark for traffic scenes.

Method: The authors developed a dataset, Dark-traffic, and proposed SLVM, a biologically inspired model with components like light-adaptive mechanisms, separable learning strategies, and spatial alignment modules.

Result: SLVM outperformed existing models like RT-DETR and YOLOv12 in tasks such as object detection and instance segmentation, and demonstrated superior results on benchmarks like LIS.

Conclusion: The proposed methodology and dataset significantly enhance object perception in low-light traffic scenes while lowering computational costs, setting a new standard for future research.

Abstract: Fast and accurate object perception in low-light traffic scenes has attracted
increasing attention. However, due to severe illumination degradation and the
lack of reliable visual cues, existing perception models and methods struggle
to quickly adapt to and accurately predict in low-light environments. Moreover,
there is the absence of available large-scale benchmark specifically focused on
low-light traffic scenes. To bridge this gap, we introduce a physically
grounded illumination degradation method tailored to real-world low-light
settings and construct Dark-traffic, the largest densely annotated dataset to
date for low-light traffic scenes, supporting object detection, instance
segmentation, and optical flow estimation. We further propose the Separable
Learning Vision Model (SLVM), a biologically inspired framework designed to
enhance perception under adverse lighting. SLVM integrates four key components:
a light-adaptive pupillary mechanism for illumination-sensitive feature
extraction, a feature-level separable learning strategy for efficient
representation, task-specific decoupled branches for multi-task separable
learning, and a spatial misalignment-aware fusion module for precise
multi-feature alignment. Extensive experiments demonstrate that SLVM achieves
state-of-the-art performance with reduced computational overhead. Notably, it
outperforms RT-DETR by 11.2 percentage points in detection, YOLOv12 by 6.1
percentage points in instance segmentation, and reduces endpoint error (EPE) of
baseline by 12.37% on Dark-traffic. On the LIS benchmark, the end-to-end
trained SLVM surpasses Swin Transformer+EnlightenGAN and
ConvNeXt-T+EnlightenGAN by an average of 11 percentage points across key
metrics, and exceeds Mask RCNN (with light enhancement) by 3.1 percentage
points. The Dark-traffic dataset and complete code is released at
https://github.com/alanli1997/slvm.

</details>


### [151] [Leveraging Transfer Learning and Mobile-enabled Convolutional Neural Networks for Improved Arabic Handwritten Character Recognition](https://arxiv.org/abs/2509.05019)
*Mohsine El Khayati,Ayyad Maafiri,Yassine Himeur,Hamzah Ali Alkhazaleh,Shadi Atalla,Wathiq Mansoor*

Main category: cs.CV

TL;DR: The paper investigates transfer learning (TL) with mobile-enabled convolutional neural networks (MbNets) to improve Arabic Handwritten Character Recognition (AHCR), testing methods like full and partial fine-tuning and training from scratch across different models. MobileNet performed best overall, achieving high accuracy and robustness; future improvements focus on optimization and broader applications.


<details>
  <summary>Details</summary>
Motivation: To address computational resource demands and dataset scarcity in Arabic Handwritten Character Recognition (AHCR) using lightweight neural network architectures combined with transfer learning (TL).

Method: Three transfer learning strategies—full fine-tuning, partial fine-tuning, and training from scratch—were applied to four MbNet models (MobileNet, SqueezeNet, MnasNet, and ShuffleNet) and evaluated on datasets (AHCD, HIJJA, IFHCDB).

Result: MobileNet consistently performed the best in most metrics. Results varied by dataset, with IFHCDB achieving 99% accuracy using MnasNet, AHCD achieving 97% with ShuffleNet, and HIJJA facing variability challenges, peaking at 92% with ShuffleNet.

Conclusion: Combining TL with lightweight MbNets is effective for AHCR, particularly under full fine-tuning, which balances accuracy and convergence speed. Future work aims to further improve model robustness and address challenging datasets.

Abstract: The study explores the integration of transfer learning (TL) with
mobile-enabled convolutional neural networks (MbNets) to enhance Arabic
Handwritten Character Recognition (AHCR). Addressing challenges like extensive
computational requirements and dataset scarcity, this research evaluates three
TL strategies--full fine-tuning, partial fine-tuning, and training from
scratch--using four lightweight MbNets: MobileNet, SqueezeNet, MnasNet, and
ShuffleNet. Experiments were conducted on three benchmark datasets: AHCD,
HIJJA, and IFHCDB. MobileNet emerged as the top-performing model, consistently
achieving superior accuracy, robustness, and efficiency, with ShuffleNet
excelling in generalization, particularly under full fine-tuning. The IFHCDB
dataset yielded the highest results, with 99% accuracy using MnasNet under full
fine-tuning, highlighting its suitability for robust character recognition. The
AHCD dataset achieved competitive accuracy (97%) with ShuffleNet, while HIJJA
posed significant challenges due to its variability, achieving a peak accuracy
of 92% with ShuffleNet. Notably, full fine-tuning demonstrated the best overall
performance, balancing accuracy and convergence speed, while partial
fine-tuning underperformed across metrics. These findings underscore the
potential of combining TL and MbNets for resource-efficient AHCR, paving the
way for further optimizations and broader applications. Future work will
explore architectural modifications, in-depth dataset feature analysis, data
augmentation, and advanced sensitivity analysis to enhance model robustness and
generalizability.

</details>


### [152] [LUIVITON: Learned Universal Interoperable VIrtual Try-ON](https://arxiv.org/abs/2509.05030)
*Cong Cao,Xianhang Cheng,Jingyuan Liu,Yujian Zheng,Zhenhui Lin,Meriem Chkir,Hao Li*

Main category: cs.CV

TL;DR: The paper introduces LUIVITON, an automated virtual try-on system for draping complex clothing onto various humanoid characters with high quality and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the challenge of automatically adapting complex, multi-layered clothing onto diverse humanoid shapes and body poses, given the computational and geometric complexities involved.

Method: The system uses SMPL as a proxy for correspondence tasks, employing a geometric learning approach for clothing-to-body alignment and a diffusion model with multi-view appearance features for body-to-SMPL correspondence.

Result: LUIVITON successfully handles diverse humanoid geometries and produces high-quality 3D clothing fittings, even without requiring 2D sewing patterns, while remaining computationally efficient.

Conclusion: LUIVITON achieves automated, high-quality virtual try-on for a wide variety of humanoid characters, supporting customizable clothing adjustments, marking a significant step forward for practical adoption.

Abstract: We present LUIVITON, an end-to-end system for fully automated virtual try-on,
capable of draping complex, multi-layer clothing onto diverse and arbitrarily
posed humanoid characters. To address the challenge of aligning complex
garments with arbitrary and highly diverse body shapes, we use SMPL as a proxy
representation and separate the clothing-to-body draping problem into two
correspondence tasks: 1) clothing-to-SMPL and 2) body-to-SMPL correspondence,
where each has its unique challenges. While we address the clothing-to-SMPL
fitting problem using a geometric learning-based approach for
partial-to-complete shape correspondence prediction, we introduce a diffusion
model-based approach for body-to-SMPL correspondence using multi-view
consistent appearance features and a pre-trained 2D foundation model. Our
method can handle complex geometries, non-manifold meshes, and generalizes
effectively to a wide range of humanoid characters -- including humans, robots,
cartoon subjects, creatures, and aliens, while maintaining computational
efficiency for practical adoption. In addition to offering a fully automatic
fitting solution, LUIVITON supports fast customization of clothing size,
allowing users to adjust clothing sizes and material properties after they have
been draped. We show that our system can produce high-quality 3D clothing
fittings without any human labor, even when 2D clothing sewing patterns are not
available.

</details>


### [153] [Towards Efficient Pixel Labeling for Industrial Anomaly Detection and Localization](https://arxiv.org/abs/2509.05034)
*Jingqi Wu,Hanxi Li,Lin Yuanbo Wu,Hao Chen,Deyin Liu,Peng Wang*

Main category: cs.CV

TL;DR: The paper introduces ADClick, an interactive image segmentation method, for industrial anomaly detection using minimal user input and textual descriptions to improve AD models. A derived framework, ADClick-Seg, achieves state-of-the-art results in multi-class AD tasks.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitation of requiring pixel-level annotations in industrial anomaly detection, which hinders scalability, despite defective samples being available during production.

Method: The proposed approach consists of ADClick for generating precise pixel-wise anomaly annotations using minimal user clicks and short textual inputs. They also present ADClick-Seg, a cross-modal framework that aligns visual features with language prompts, enabling superior anomaly localization and detection based on prototype-based methods.

Result: ADClick demonstrated a significant performance boost for AD models with an AP of 96.1% on MVTec AD, while ADClick-Seg achieved state-of-the-art results on multi-class anomaly detection tasks with AP = 80.0%, PRO = 97.5%, and Pixel-AUROC = 99.1%.

Conclusion: The study concludes that ADClick and ADClick-Seg offer scalable and efficient solutions for industrial anomaly detection, achieving superior accuracy and localization through the integration of user input and cross-modal techniques.

Abstract: Industrial product inspection is often performed using Anomaly Detection (AD)
frameworks trained solely on non-defective samples. Although defective samples
can be collected during production, leveraging them usually requires
pixel-level annotations, limiting scalability. To address this, we propose
ADClick, an Interactive Image Segmentation (IIS) algorithm for industrial
anomaly detection. ADClick generates pixel-wise anomaly annotations from only a
few user clicks and a brief textual description, enabling precise and efficient
labeling that significantly improves AD model performance (e.g., AP = 96.1\% on
MVTec AD). We further introduce ADClick-Seg, a cross-modal framework that
aligns visual features and textual prompts via a prototype-based approach for
anomaly detection and localization. By combining pixel-level priors with
language-guided cues, ADClick-Seg achieves state-of-the-art results on the
challenging ``Multi-class'' AD task (AP = 80.0\%, PRO = 97.5\%, Pixel-AUROC =
99.1\% on MVTec AD).

</details>


### [154] [Systematic Review and Meta-analysis of AI-driven MRI Motion Artifact Detection and Correction](https://arxiv.org/abs/2509.05071)
*Mojtaba Safari,Zach Eidex,Richard L. J. Qiu,Matthew Goette,Tonghe Wang,Xiaofeng Yang*

Main category: cs.CV

TL;DR: This paper reviews AI methods for detecting and correcting MRI motion artifacts, highlighting their potential and challenges.


<details>
  <summary>Details</summary>
Motivation: The research aims to assess AI's role in improving MRI image quality by addressing motion artifacts, a common hurdle in medical imaging.

Method: A systematic review and meta-analysis were performed, focusing on deep learning and generative models for motion artifact correction.

Result: Deep learning, especially generative models, demonstrates promise in improving MRI image quality but faces challenges like generalizability and training data reliance.

Conclusion: AI techniques, particularly DL generative models, hold potential for better MRI results but need standardized datasets and advanced algorithms for broader application.

Abstract: Background: To systematically review and perform a meta-analysis of
artificial intelligence (AI)-driven methods for detecting and correcting
magnetic resonance imaging (MRI) motion artifacts, assessing current
developments, effectiveness, challenges, and future research directions.
Methods: A comprehensive systematic review and meta-analysis were conducted,
focusing on deep learning (DL) approaches, particularly generative models, for
the detection and correction of MRI motion artifacts. Quantitative data were
extracted regarding utilized datasets, DL architectures, and performance
metrics. Results: DL, particularly generative models, show promise for reducing
motion artifacts and improving image quality; however, limited
generalizability, reliance on paired training data, and risk of visual
distortions remain key challenges that motivate standardized datasets and
reporting. Conclusions: AI-driven methods, particularly DL generative models,
show significant potential for improving MRI image quality by effectively
addressing motion artifacts. However, critical challenges must be addressed,
including the need for comprehensive public datasets, standardized reporting
protocols for artifact levels, and more advanced, adaptable DL techniques to
reduce reliance on extensive paired datasets. Addressing these aspects could
substantially enhance MRI diagnostic accuracy, reduce healthcare costs, and
improve patient care outcomes.

</details>


### [155] [GeoSplat: A Deep Dive into Geometry-Constrained Gaussian Splatting](https://arxiv.org/abs/2509.05075)
*Yangming Li,Chaoyu Liu,Lihao Liu,Simon Masnou,Carola-Bibian Schönlieb*

Main category: cs.CV

TL;DR: This paper presents GeoSplat, a framework that integrates advanced geometric priors into Gaussian splatting optimization, leading to improved novel view synthesis performance.


<details>
  <summary>Details</summary>
Motivation: Previous studies in Gaussian splatting used low-order, noise-sensitive geometric priors, limiting their optimization potential and robustness.

Method: The framework incorporates first and second-order geometric quantities for better Gaussian initialization, gradient updates, and densification, leveraging more reliable and noise-robust estimation methods.

Result: GeoSplat significantly improves Gaussian splatting performance across multiple datasets and surpasses existing baseline methods.

Conclusion: Using advanced geometric priors and reliable estimation techniques can transform the performance and reliability of Gaussian splatting for novel view synthesis tasks.

Abstract: A few recent works explored incorporating geometric priors to regularize the
optimization of Gaussian splatting, further improving its performance. However,
those early studies mainly focused on the use of low-order geometric priors
(e.g., normal vector), and they are also unreliably estimated by
noise-sensitive methods, like local principal component analysis. To address
their limitations, we first present GeoSplat, a general geometry-constrained
optimization framework that exploits both first-order and second-order
geometric quantities to improve the entire training pipeline of Gaussian
splatting, including Gaussian initialization, gradient update, and
densification. As an example, we initialize the scales of 3D Gaussian
primitives in terms of principal curvatures, leading to a better coverage of
the object surface than random initialization. Secondly, based on certain
geometric structures (e.g., local manifold), we introduce efficient and
noise-robust estimation methods that provide dynamic geometric priors for our
framework. We conduct extensive experiments on multiple datasets for novel view
synthesis, showing that our framework: GeoSplat, significantly improves the
performance of Gaussian splatting and outperforms previous baselines.

</details>


### [156] [Scale-interaction transformer: a hybrid cnn-transformer model for facial beauty prediction](https://arxiv.org/abs/2509.05078)
*Djamel Eddine Boukhari*

Main category: cs.CV

TL;DR: This paper introduces the Scale-Interaction Transformer (SIT), a hybrid CNN-Transformer model for automated facial beauty prediction (FBP), achieving state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of traditional CNNs in modeling complex interplay of facial features across different scales in FBP.

Method: The SIT model combines CNNs for multi-scale feature extraction with Transformers for modeling interactions and relationships using self-attention.

Result: The SIT achieves a Pearson Correlation of 0.9187 on the SCUT-FBP5500 benchmark, surpassing previous results.

Conclusion: Explicitly modeling multi-scale feature interactions is critical for high-performance FBP, and hybrid CNN-Transformer architectures are promising for complex image regression tasks.

Abstract: Automated Facial Beauty Prediction (FBP) is a challenging computer vision
task due to the complex interplay of local and global facial features that
influence human perception. While Convolutional Neural Networks (CNNs) excel at
feature extraction, they often process information at a fixed scale,
potentially overlooking the critical inter-dependencies between features at
different levels of granularity. To address this limitation, we introduce the
Scale-Interaction Transformer (SIT), a novel hybrid deep learning architecture
that synergizes the feature extraction power of CNNs with the relational
modeling capabilities of Transformers. The SIT first employs a multi-scale
module with parallel convolutions to capture facial characteristics at varying
receptive fields. These multi-scale representations are then framed as a
sequence and processed by a Transformer encoder, which explicitly models their
interactions and contextual relationships via a self-attention mechanism. We
conduct extensive experiments on the widely-used SCUT-FBP5500 benchmark
dataset, where the proposed SIT model establishes a new state-of-the-art. It
achieves a Pearson Correlation of 0.9187, outperforming previous methods. Our
findings demonstrate that explicitly modeling the interplay between multi-scale
visual cues is crucial for high-performance FBP. The success of the SIT
architecture highlights the potential of hybrid CNN-Transformer models for
complex image regression tasks that demand a holistic, context-aware
understanding.

</details>


### [157] [Robust Experts: the Effect of Adversarial Training on CNNs with Sparse Mixture-of-Experts Layers](https://arxiv.org/abs/2509.05086)
*Svetlana Pavlitska,Haixi Fan,Konstantin Ditschuneit,J. Marius Zöllner*

Main category: cs.CV

TL;DR: This paper introduces sparse mixture-of-experts (MoE) layers to improve convolutional neural network robustness against adversarial attacks, demonstrating success on ResNet architectures trained on CIFAR-100.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the robustness of CNNs against adversarial attacks, a challenge typically requiring resource-intensive methods, by leveraging sparse MoE layers.

Method: The method involves integrating sparse MoE layers into selected residual or convolutional layers of ResNet architectures. These layers increase model capacity without additional inference costs, and their effect on robustness is evaluated under adversarial training scenarios.

Result: The paper demonstrates that inserting a single MoE layer in deeper stages of ResNet improves robustness against PGD and AutoPGD attacks. When switch loss is applied, routing collapses into overused experts, concentrating adversarial training on certain paths and leading to emergent robust subpaths.

Conclusion: Sparse MoE layers can enhance CNN robustness against adversarial attacks without additional inference cost. Specialized robust subpaths emerge when the switch loss is used, indicating potential future avenues for focusing adversarial training efforts.

Abstract: Robustifying convolutional neural networks (CNNs) against adversarial attacks
remains challenging and often requires resource-intensive countermeasures. We
explore the use of sparse mixture-of-experts (MoE) layers to improve robustness
by replacing selected residual blocks or convolutional layers, thereby
increasing model capacity without additional inference cost. On ResNet
architectures trained on CIFAR-100, we find that inserting a single MoE layer
in the deeper stages leads to consistent improvements in robustness under PGD
and AutoPGD attacks when combined with adversarial training. Furthermore, we
discover that when switch loss is used for balancing, it causes routing to
collapse onto a small set of overused experts, thereby concentrating
adversarial training on these paths and inadvertently making them more robust.
As a result, some individual experts outperform the gated MoE model in
robustness, suggesting that robust subpaths emerge through specialization. Our
code is available at https://github.com/KASTEL-MobilityLab/robust-sparse-moes.

</details>


### [158] [Semi-supervised Deep Transfer for Regression without Domain Alignment](https://arxiv.org/abs/2509.05092)
*Mainak Biswas,Ambedkar Dukkipati,Devarajan Sridharan*

Main category: cs.CV

TL;DR: The paper introduces CRAFT, a domain adaptation method, addressing the challenges of source-free, semi-supervised scenarios for regression tasks commonly seen in neuroscience and medicine.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges with deep learning models in domain-shifted scenarios, especially when source data is unavailable due to privacy or resource constraints.

Method: The authors build upon the CUDA framework to develop CRAFT, which adapts pretrained models for regression in a source-free, semi-supervised manner, leveraging unlabeled target data.

Result: CRAFT improved performance in neuroscience tasks (up to 9% RMSE improvement over fine-tuned models) and outperformed competing models by over 3% using unlabeled data.

Conclusion: CRAFT is proposed as a robust and efficient method for source-free domain adaptation in regression applications, with demonstrated efficacy across neuroscience and other real-world datasets.

Abstract: Deep learning models deployed in real-world applications (e.g., medicine)
face challenges because source models do not generalize well to domain-shifted
target data. Many successful domain adaptation (DA) approaches require full
access to source data. Yet, such requirements are unrealistic in scenarios
where source data cannot be shared either because of privacy concerns or
because it is too large and incurs prohibitive storage or computational costs.
Moreover, resource constraints may limit the availability of labeled targets.
We illustrate this challenge in a neuroscience setting where source data are
unavailable, labeled target data are meager, and predictions involve
continuous-valued outputs. We build upon Contradistinguisher (CUDA), an
efficient framework that learns a shared model across the labeled source and
unlabeled target samples, without intermediate representation alignment. Yet,
CUDA was designed for unsupervised DA, with full access to source data, and for
classification tasks. We develop CRAFT -- a Contradistinguisher-based
Regularization Approach for Flexible Training -- for source-free (SF),
semi-supervised transfer of pretrained models in regression tasks. We showcase
the efficacy of CRAFT in two neuroscience settings: gaze prediction with
electroencephalography (EEG) data and ``brain age'' prediction with structural
MRI data. For both datasets, CRAFT yielded up to 9% improvement in
root-mean-squared error (RMSE) over fine-tuned models when labeled training
examples were scarce. Moreover, CRAFT leveraged unlabeled target data and
outperformed four competing state-of-the-art source-free domain adaptation
models by more than 3%. Lastly, we demonstrate the efficacy of CRAFT on two
other real-world regression benchmarks. We propose CRAFT as an efficient
approach for source-free, semi-supervised deep transfer for regression that is
ubiquitous in biology and medicine.

</details>


### [159] [A Scalable Attention-Based Approach for Image-to-3D Texture Mapping](https://arxiv.org/abs/2509.05131)
*Arianna Rampini,Kanika Madan,Bruno Roy,AmirHossein Zamani,Derek Cheung*

Main category: cs.CV

TL;DR: This paper introduces a transformer-based framework to swiftly generate high-quality 3D textures using just a single image and mesh, bypassing UV mapping and differentiable rendering.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the inefficiency, complexity, and lack of fidelity in current generative methods for creating 3D textures.

Method: It incorporates a triplane representation along with depth-based backprojection losses, facilitating direct texture field prediction from inputs and eliminating UV mapping.

Result: The model achieves rapid inference (0.2 seconds per shape) while generating high-fidelity textures, outperforming existing methods in evaluations.

Conclusion: The approach is practical and effective for scalable creation of controlled and realistic 3D textures, showcasing benefits in speed and quality.

Abstract: High-quality textures are critical for realistic 3D content creation, yet
existing generative methods are slow, rely on UV maps, and often fail to remain
faithful to a reference image. To address these challenges, we propose a
transformer-based framework that predicts a 3D texture field directly from a
single image and a mesh, eliminating the need for UV mapping and differentiable
rendering, and enabling faster texture generation. Our method integrates a
triplane representation with depth-based backprojection losses, enabling
efficient training and faster inference. Once trained, it generates
high-fidelity textures in a single forward pass, requiring only 0.2s per shape.
Extensive qualitative, quantitative, and user preference evaluations
demonstrate that our method outperforms state-of-the-art baselines on
single-image texture reconstruction in terms of both fidelity to the input
image and perceptual quality, highlighting its practicality for scalable,
high-quality, and controllable 3D content creation.

</details>


### [160] [SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growing](https://arxiv.org/abs/2509.05144)
*Chaolei Wang,Yang Luo,Jing Du,Siyu Chen,Yiping Chen,Ting Han*

Main category: cs.CV

TL;DR: The paper proposes SGS-3D, a new framework for accurate 3D instance segmentation, overcoming issues with imprecision in 2D-to-3D lifting approaches by integrating geometric and semantic refinement steps.


<details>
  <summary>Details</summary>
Motivation: 3D instance segmentation is crucial for scene understanding but faces challenges due to errors from ambiguous semantics and insufficient depth constraints in existing 2D-to-3D lifting methods.

Method: The authors introduce a training-free framework called SGS-3D, which splits and refines semantic masks by combining geometric primitives and spatial continuity to produce accurate 3D instances.

Result: SGS-3D improves segmentation accuracy and robustness on datasets like ScanNet200, ScanNet++, and KITTI-360, demonstrating its effectiveness across various environments.

Conclusion: SGS-3D successfully addresses the limitations of existing frameworks, yielding high-fidelity and precise 3D object instances with good generalization capabilities across different scenarios.

Abstract: Accurate 3D instance segmentation is crucial for high-quality scene
understanding in the 3D vision domain. However, 3D instance segmentation based
on 2D-to-3D lifting approaches struggle to produce precise instance-level
segmentation, due to accumulated errors introduced during the lifting process
from ambiguous semantic guidance and insufficient depth constraints. To tackle
these challenges, we propose splitting and growing reliable semantic mask for
high-fidelity 3D instance segmentation (SGS-3D), a novel "split-then-grow"
framework that first purifies and splits ambiguous lifted masks using geometric
primitives, and then grows them into complete instances within the scene.
Unlike existing approaches that directly rely on raw lifted masks and sacrifice
segmentation accuracy, SGS-3D serves as a training-free refinement method that
jointly fuses semantic and geometric information, enabling effective
cooperation between the two levels of representation. Specifically, for
semantic guidance, we introduce a mask filtering strategy that leverages the
co-occurrence of 3D geometry primitives to identify and remove ambiguous masks,
thereby ensuring more reliable semantic consistency with the 3D object
instances. For the geometric refinement, we construct fine-grained object
instances by exploiting both spatial continuity and high-level features,
particularly in the case of semantic ambiguity between distinct objects.
Experimental results on ScanNet200, ScanNet++, and KITTI-360 demonstrate that
SGS-3D substantially improves segmentation accuracy and robustness against
inaccurate masks from pre-trained models, yielding high-fidelity object
instances while maintaining strong generalization across diverse indoor and
outdoor environments. Code is available in the supplementary materials.

</details>


### [161] [SL-SLR: Self-Supervised Representation Learning for Sign Language Recognition](https://arxiv.org/abs/2509.05188)
*Ariel Basso Madjoukeng,Jérôme Fink,Pierre Poitier,Edith Belise Kenmogne,Benoit Frenay*

Main category: cs.CV

TL;DR: This paper addresses challenges in sign language recognition (SLR) using an improved self-supervised learning framework with free-negative pairs and a novel data augmentation technique, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: The scarcity of annotated sign language data limits the applicability of supervised learning methods. Contrastive learning faces challenges in SLR, such as equal treatment of all video parts and similarity between negative pairs, leading to non-discriminative representation learning.

Method: The paper introduces a new self-supervised framework featuring free-negative pairs to avoid misleading similarities in negative pairs, along with a novel data augmentation technique to highlight meaningful parts of videos.

Result: The proposed framework shows marked improvements in accuracy for sign language recognition tasks, outperforming contrasting and self-supervised methods across various scenarios including linear evaluation, semi-supervised learning, and transferability between sign languages.

Conclusion: The novel framework effectively addresses key issues in SLR, leveraging self-supervised techniques to enhance feature discriminability and improve recognition performance in tasks with limited annotated data.

Abstract: Sign language recognition (SLR) is a machine learning task aiming to identify
signs in videos. Due to the scarcity of annotated data, unsupervised methods
like contrastive learning have become promising in this field. They learn
meaningful representations by pulling positive pairs (two augmented versions of
the same instance) closer and pushing negative pairs (different from the
positive pairs) apart. In SLR, in a sign video, only certain parts provide
information that is truly useful for its recognition. Applying contrastive
methods to SLR raises two issues: (i) contrastive learning methods treat all
parts of a video in the same way, without taking into account the relevance of
certain parts over others; (ii) shared movements between different signs make
negative pairs highly similar, complicating sign discrimination. These issues
lead to learning non-discriminative features for sign recognition and poor
results in downstream tasks. In response, this paper proposes a self-supervised
learning framework designed to learn meaningful representations for SLR. This
framework consists of two key components designed to work together: (i) a new
self-supervised approach with free-negative pairs; (ii) a new data augmentation
technique. This approach shows a considerable gain in accuracy compared to
several contrastive and self-supervised methods, across linear evaluation,
semi-supervised learning, and transferability between sign languages.

</details>


### [162] [Symbolic Graphics Programming with Large Language Models](https://arxiv.org/abs/2509.05208)
*Yamei Chen,Haoquan Zhang,Yangyi Huang,Zeju Qiu,Kaipeng Zhang,Yandong Wen,Weiyang Liu*

Main category: cs.CV

TL;DR: This paper explores the generation of symbolic graphics programs (SGPs) from natural-language descriptions using large language models (LLMs), introduces a benchmark for evaluation, and improves performance using reinforcement learning (RL).


<details>
  <summary>Details</summary>
Motivation: To evaluate and enhance the capabilities of LLMs in creating symbolic graphics programs, which serve as a medium to understand the models' cross-modal understanding and visual grounding.

Method: Introduced SGP-GenBench to evaluate LLMs on object fidelity, scene fidelity, and compositionality. Proposed an RL approach with verifiable rewards, including a format-validity gate and cross-modal reward using strong vision encoders.

Result: The proposed RL-based method significantly improved SVG generation quality, achieving performance comparable to leading proprietary models, particularly when applied to the Qwen-2.5-7B model.

Conclusion: Symbolic graphics programming offers an effective way to study cross-modal grounding in LLMs, and the presented RL approach improves models' abilities in this domain, leading to better representation and scene coherence.

Abstract: Large language models (LLMs) excel at program synthesis, yet their ability to
produce symbolic graphics programs (SGPs) that render into precise visual
content remains underexplored. We study symbolic graphics programming, where
the goal is to generate an SGP from a natural-language description. This task
also serves as a lens into how LLMs understand the visual world by prompting
them to generate images rendered from SGPs. Among various SGPs, our paper
sticks to scalable vector graphics (SVGs). We begin by examining the extent to
which LLMs can generate SGPs. To this end, we introduce SGP-GenBench, a
comprehensive benchmark covering object fidelity, scene fidelity, and
compositionality (attribute binding, spatial relations, numeracy). On
SGP-GenBench, we discover that frontier proprietary models substantially
outperform open-source models, and performance correlates well with general
coding capabilities. Motivated by this gap, we aim to improve LLMs' ability to
generate SGPs. We propose a reinforcement learning (RL) with verifiable rewards
approach, where a format-validity gate ensures renderable SVG, and a
cross-modal reward aligns text and the rendered image via strong vision
encoders (e.g., SigLIP for text-image and DINO for image-image). Applied to
Qwen-2.5-7B, our method substantially improves SVG generation quality and
semantics, achieving performance on par with frontier systems. We further
analyze training dynamics, showing that RL induces (i) finer decomposition of
objects into controllable primitives and (ii) contextual details that improve
scene coherence. Our results demonstrate that symbolic graphics programming
offers a precise and interpretable lens on cross-modal grounding.

</details>


### [163] [COGITAO: A Visual Reasoning Framework To Study Compositionality & Generalization](https://arxiv.org/abs/2509.05249)
*Yassine Taoudi-Benchekroun,Klim Troyan,Pascal Sager,Stefan Gerber,Lukas Tuggener,Benjamin Grewe*

Main category: cs.CV

TL;DR: The paper introduces COGITAO, a framework and benchmark to study compositionality and generalization in visual tasks using rule-based grids and transformations.


<details>
  <summary>Details</summary>
Motivation: Human intelligence excels at combining learned concepts in new ways, a capability that machine learning models struggle with. The paper aims to address this gap by providing a dedicated framework.

Method: The authors developed COGITAO, a modular framework for generating visual tasks with adjustable compositional depth using 28 transformations in grid-like environments to study and benchmark generalization.

Result: Baseline experiments revealed state-of-the-art vision models struggle to generalize to new combinations of known elements despite excelling in-domain.

Conclusion: COGITAO offers a flexible, open-source platform for exploring compositionality and generalization challenges, fostering further research in machine learning.

Abstract: The ability to compose learned concepts and apply them in novel settings is
key to human intelligence, but remains a persistent limitation in
state-of-the-art machine learning models. To address this issue, we introduce
COGITAO, a modular and extensible data generation framework and benchmark
designed to systematically study compositionality and generalization in visual
domains. Drawing inspiration from ARC-AGI's problem-setting, COGITAO constructs
rule-based tasks which apply a set of transformations to objects in grid-like
environments. It supports composition, at adjustable depth, over a set of 28
interoperable transformations, along with extensive control over grid
parametrization and object properties. This flexibility enables the creation of
millions of unique task rules -- surpassing concurrent datasets by several
orders of magnitude -- across a wide range of difficulties, while allowing
virtually unlimited sample generation per rule. We provide baseline experiments
using state-of-the-art vision models, highlighting their consistent failures to
generalize to novel combinations of familiar elements, despite strong in-domain
performance. COGITAO is fully open-sourced, including all code and datasets, to
support continued research in this field.

</details>


### [164] [WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool](https://arxiv.org/abs/2509.05296)
*Zizun Li,Jianjun Zhou,Yifan Wang,Haoyu Guo,Wenzheng Chang,Yang Zhou,Haoyi Zhu,Junyi Chen,Chunhua Shen,Tong He*

Main category: cs.CV

TL;DR: The paper introduces WinT3R, a model that predicts camera poses and generates point maps efficiently in real-time while maintaining high quality.


<details>
  <summary>Details</summary>
Motivation: To tackle the limitations in achieving a balance between high-quality reconstruction and real-time processing in current methods.

Method: Incorporates a sliding window mechanism for frame information exchange and a compact global camera token pool to improve prediction quality and camera pose estimation efficiency.

Result: WinT3R achieves state-of-the-art online reconstruction quality, accurate camera pose predictions, and high-speed performance, as shown in tests with diverse datasets.

Conclusion: WinT3R successfully addresses the trade-off between real-time performance and prediction quality in online reconstruction, benefiting practical applications.

Abstract: We present WinT3R, a feed-forward reconstruction model capable of online
prediction of precise camera poses and high-quality point maps. Previous
methods suffer from a trade-off between reconstruction quality and real-time
performance. To address this, we first introduce a sliding window mechanism
that ensures sufficient information exchange among frames within the window,
thereby improving the quality of geometric predictions without large
computation. In addition, we leverage a compact representation of cameras and
maintain a global camera token pool, which enhances the reliability of camera
pose estimation without sacrificing efficiency. These designs enable WinT3R to
achieve state-of-the-art performance in terms of online reconstruction quality,
camera pose estimation, and reconstruction speed, as validated by extensive
experiments on diverse datasets. Code and model are publicly available at
https://github.com/LiZizun/WinT3R.

</details>


### [165] [FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases](https://arxiv.org/abs/2509.05297)
*Matteo Poggi,Fabio Tosi*

Main category: cs.CV

TL;DR: FlowSeek is an innovative optical flow framework that combines modern network designs, depth foundation models, and motion parametrization to achieve high performance with minimal computational resources.


<details>
  <summary>Details</summary>
Motivation: The motivation for FlowSeek stems from the need to develop efficient optical flow solutions that can operate on minimal hardware resources while maintaining high accuracy across diverse datasets.

Method: FlowSeek integrates advanced optical flow network designs, single-image depth foundation models, and low-dimensional motion parametrization into a compact architecture. It is trained on a single consumer-grade GPU.

Result: FlowSeek demonstrates superior generalization across datasets, showing improvements of 10% and 15% over SEA-RAFT on Sintel Final and KITTI datasets, respectively. It also performs well on Spring and LayeredFlow datasets.

Conclusion: FlowSeek proves that minimal hardware resources can be sufficient for achieving state-of-the-art performance in optical flow tasks across multiple datasets, offering a more accessible and efficient framework.

Abstract: We present FlowSeek, a novel framework for optical flow requiring minimal
hardware resources for training. FlowSeek marries the latest advances on the
design space of optical flow networks with cutting-edge single-image depth
foundation models and classical low-dimensional motion parametrization,
implementing a compact, yet accurate architecture. FlowSeek is trained on a
single consumer-grade GPU, a hardware budget about 8x lower compared to most
recent methods, and still achieves superior cross-dataset generalization on
Sintel Final and KITTI, with a relative improvement of 10 and 15% over the
previous state-of-the-art SEA-RAFT, as well as on Spring and LayeredFlow
datasets.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [166] [STADI: Fine-Grained Step-Patch Diffusion Parallelism for Heterogeneous GPUs](https://arxiv.org/abs/2509.04719)
*Han Liang,Jiahui Zhou,Zicheng Zhou,Xiaoxi Zhang,Xu Chen*

Main category: cs.DC

TL;DR: STADI framework enables efficient diffusion model inference by optimizing workload distribution in heterogeneous multi-GPU setups, achieving up to 45% reduction in latency.


<details>
  <summary>Details</summary>
Motivation: The study aims to address inefficiencies in diffusion model inference on heterogeneous multi-GPU environments, where varied hardware capabilities lead to imbalance and underutilization.

Method: STADI implements a hybrid scheduler that combines temporal and spatial mechanisms for fine-grained parallelism. It employs a step allocator for denoising synchronization and elastic patch parallelism for balanced workload distribution based on GPU capabilities.

Result: Experiments show that STADI improves load balancing, reduces performance bottlenecks, and cuts inference latency by up to 45%, outperforming state-of-the-art frameworks.

Conclusion: STADI effectively enhances diffusion model inference in diverse GPU setups, substantially boosting efficiency and resource utilization.

Abstract: The escalating adoption of diffusion models for applications such as image
generation demands efficient parallel inference techniques to manage their
substantial computational cost. However, existing diffusion parallelism
inference schemes often underutilize resources in heterogeneous multi-GPU
environments, where varying hardware capabilities or background tasks cause
workload imbalance. This paper introduces Spatio-Temporal Adaptive Diffusion
Inference (STADI), a novel framework to accelerate diffusion model inference in
such settings. At its core is a hybrid scheduler that orchestrates fine-grained
parallelism across both temporal and spatial dimensions. Temporally, STADI
introduces a novel computation-aware step allocator applied after warmup
phases, using a least-common-multiple-minimizing quantization technique to
reduce denoising steps on slower GPUs and execution synchronization. To further
minimize GPU idle periods, STADI executes an elastic patch parallelism
mechanism that allocates variably sized image patches to GPUs according to
their computational capability, ensuring balanced workload distribution through
a complementary spatial mechanism. Extensive experiments on both
load-imbalanced and heterogeneous multi-GPU clusters validate STADI's efficacy,
demonstrating improved load balancing and mitigation of performance
bottlenecks. Compared to patch parallelism, a state-of-the-art diffusion
inference framework, our method significantly reduces end-to-end inference
latency by up to 45% and significantly improves resource utilization on
heterogeneous GPUs.

</details>


### [167] [VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing for Energy-Efficient LLM Serving](https://arxiv.org/abs/2509.04827)
*Jiahuan Yu,Aryan Taneja,Junfeng Lin,Minjia Zhang*

Main category: cs.DC

TL;DR: VoltanaLLM, a system for energy-efficient serving of large language models under latency constraints, saves up to 36.3% energy while maintaining high performance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of rising energy costs in deploying large language models, especially for interactive applications requiring real-time responses.

Method: VoltanaLLM leverages control theory to co-design frequency scaling and request routing in disaggregated LLM architectures, employing a feedback-driven GPU frequency controller and a state-space router.

Result: VoltanaLLM demonstrates energy savings of up to 36.3% while achieving near-perfect adherence to service level objectives across various LLMs and datasets.

Conclusion: VoltanaLLM offers a sustainable approach to scaling large language models by effectively balancing energy efficiency and performance requirements.

Abstract: Modern Large Language Model (LLM) serving systems increasingly support
interactive applications, like real-time chat assistants, code generation
tools, and agentic workflows. However, the soaring energy cost of LLM inference
presents a growing challenge for sustainable and cost-effective deployment.
This paper introduces VoltanaLLM, a system for SLO-aware, energy-efficient LLM
serving, built from a control theory perspective. VoltanaLLM co-designs
frequency scaling and request routing in emerging prefill/decode disaggregated
architectures, leveraging their decoupled execution to enable fine-grained
phase-specific control. It consists of a feedback-driven frequency controller
that dynamically adapts GPU frequency for prefill and decode phases, and a
state-space router that explores routing decisions across frequency-scaled
instances to minimize energy under latency constraints. We implement VoltanaLLM
in SGLang and evaluate its performance over multiple state-of-the-art LLMs and
real-world datasets. The results demonstrate that VoltanaLLM achieves up to
36.3% energy savings while maintaining near-perfect SLO attainment rate, paving
the way for sustainable and intelligent LLM serving.

</details>


### [168] [Toward Distributed 3D Gaussian Splatting for High-Resolution Isosurface Visualization](https://arxiv.org/abs/2509.05216)
*Mengjiao Han,Andres Sewell,Joseph Insley,Janet Knowles,Victor A. Mateevitsi,Michael E. Papka,Steve Petruzza,Silvio Rizzi*

Main category: cs.DC

TL;DR: The paper introduces a multi-GPU extension for the 3D Gaussian Splatting pipeline, enabling high-fidelity isosurface reconstruction for scientific visualization and scalable multi-GPU processing of large datasets.


<details>
  <summary>Details</summary>
Motivation: To enable efficient scientific visualization of large-scale simulation datasets that exceed the capacity of single GPUs, addressing a need for scalable and high-resolution reconstructions.

Method: The approach extends the 3D Gaussian Splatting pipeline by incorporating a multi-GPU training backend, which distributes optimization tasks across multiple GPUs to enhance training throughput.

Result: The system demonstrated a 5.6X speedup on the Kingsnake dataset (4M Gaussians) and successfully trained the Miranda dataset (18M Gaussians), which was infeasible on a single A100 GPU.

Conclusion: The work facilitates integrating 3D Gaussian Splatting into HPC scientific workflows, enabling real-time visualization of computational simulations, and represents a significant advancement for handling large-scale data.

Abstract: We present a multi-GPU extension of the 3D Gaussian Splatting (3D-GS)
pipeline for scientific visualization. Building on previous work that
demonstrated high-fidelity isosurface reconstruction using Gaussian primitives,
we incorporate a multi-GPU training backend adapted from Grendel-GS to enable
scalable processing of large datasets. By distributing optimization across
GPUs, our method improves training throughput and supports high-resolution
reconstructions that exceed single-GPU capacity. In our experiments, the system
achieves a 5.6X speedup on the Kingsnake dataset (4M Gaussians) using four GPUs
compared to a single-GPU baseline, and successfully trains the Miranda dataset
(18M Gaussians) that is an infeasible task on a single A100 GPU. This work lays
the groundwork for integrating 3D-GS into HPC-based scientific workflows,
enabling real-time post hoc and in situ visualization of complex simulations.

</details>


### [169] [Dynamic reconfiguration for malleable applications using RMA](https://arxiv.org/abs/2509.05248)
*Iker Martín-Álvarez,José I. Aliaga,Maribel Castillo*

Main category: cs.DC

TL;DR: The paper explores novel one-sided communication methods in MPI for dynamic resizing of malleable applications, integrated with the MaM library and compared to traditional approaches.


<details>
  <summary>Details</summary>
Motivation: To enable efficient data redistribution in malleable applications with minimal impact on execution time, addressing the high costs of traditional methods.

Method: Develop and integrate one-sided communication methods in MPI into the MaM library, extend Wait Drains for background reconfiguration, and evaluate them against traditional collective-based methods.

Result: The new methods showed comparable performance, but were hindered by high initialization costs, limiting their current advantage.

Conclusion: Despite the promising approach of one-sided communication, high initialization costs must be addressed for it to outperform traditional methods.

Abstract: This paper investigates the novel one-sided communication methods based on
remote memory access (RMA) operations in MPI for dynamic resizing of malleable
applications, enabling data redistribution with minimal impact on application
execution. After their integration into the MaM library, these methods are
compared with traditional collective-based approaches. In addition, the
existing strategy Wait Drains is extended to support efficient background
reconfiguration. Results show comparable performance, though high
initialization costs currently limit their advantage.

</details>


### [170] [Scaling Performance of Large Language Model Pretraining](https://arxiv.org/abs/2509.05258)
*Alexander Interrante-Grant,Carla Varela-Rosa,Suhaas Narayan,Chris Connelly,Albert Reuther*

Main category: cs.DC

TL;DR: The paper explores techniques for enhancing the training process of large language models (LLMs), aiming to optimize scale-up and computational efficiency in distributed systems.


<details>
  <summary>Details</summary>
Motivation: Given the computational expense and scarce public information on scaling LLMs, the paper seeks to provide practical insights into handling large-scale datasets and effectively utilizing supercomputing resources.

Method: The authors analyze and dissect distributed training, massive dataset management, and data parallelism scaling, focusing on optimal use of GPU capacity for large language model pipelines.

Result: The paper offers detailed methodologies for facilitating efficient training and management of LLMs, including practical recommendations for scaling infrastructure.

Conclusion: By clarifying the complexities of training LLMs, the paper aims to contribute valuable information for AI research and development, especially for practitioners scaling their operations.

Abstract: Large language models (LLMs) show best-in-class performance across a wide
range of natural language processing applications. Training these models is an
extremely computationally expensive task; frontier Artificial Intelligence (AI)
research companies are investing billions of dollars into supercomputing
infrastructure to train progressively larger models on increasingly massive
datasets. Unfortunately, information about the scaling performance and training
considerations of these large training pipelines is scarce in public
literature. Working with large-scale datasets and models can be complex and
practical recommendations are scarce in the public literature for tuning
training performance when scaling up large language models. In this paper, we
aim to demystify the large language model pretraining pipeline somewhat - in
particular with respect to distributed training, managing large datasets across
hundreds of nodes, and scaling up data parallelism with an emphasis on fully
leveraging available GPU compute capacity.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [171] [Greener Deep Reinforcement Learning: Analysis of Energy and Carbon Efficiency Across Atari Benchmarks](https://arxiv.org/abs/2509.05273)
*Jason Gardner,Ayan Dutta,Swapnoneel Roy,O. Patrick Kreidl,Ladislau Boloni*

Main category: cs.LG

TL;DR: This paper benchmarks the energy consumption, CO2 emissions, and monetary costs across seven DRL algorithms, identifying substantial efficiency differences and promoting more sustainable practices.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the environmental and economic challenges posed by the increasing computational demands of DRL, which remain underexplored compared to algorithmic learning performance.

Method: The benchmarking study evaluated seven DRL algorithms for energy consumption, emissions, and cost over one million training steps on ten Atari games using real-time power measurement.

Result: Substantial variation was found, with some algorithms consuming up to 24% less energy, emitting 68% less CO2, and costing 68% less while maintaining comparable learning performance.

Conclusion: The findings advocate for energy-aware DRL practices, offering insights into balancing performance with reduced environmental and economic impacts, and serve to guide sustainable algorithm development.

Abstract: The growing computational demands of deep reinforcement learning (DRL) have
raised concerns about the environmental and economic costs of training
large-scale models. While algorithmic efficiency in terms of learning
performance has been extensively studied, the energy requirements, greenhouse
gas emissions, and monetary costs of DRL algorithms remain largely unexplored.
In this work, we present a systematic benchmarking study of the energy
consumption of seven state-of-the-art DRL algorithms, namely DQN, TRPO, A2C,
ARS, PPO, RecurrentPPO, and QR-DQN, implemented using Stable Baselines. Each
algorithm was trained for one million steps each on ten Atari 2600 games, and
power consumption was measured in real-time to estimate total energy usage,
CO2-Equivalent emissions, and electricity cost based on the U.S. national
average electricity price. Our results reveal substantial variation in energy
efficiency and training cost across algorithms, with some achieving comparable
performance while consuming up to 24% less energy (ARS vs. DQN), emitting
nearly 68% less CO2, and incurring almost 68% lower monetary cost (QR-DQN vs.
RecurrentPPO) than less efficient counterparts. We further analyze the
trade-offs between learning performance, training time, energy use, and
financial cost, highlighting cases where algorithmic choices can mitigate
environmental and economic impact without sacrificing learning performance.
This study provides actionable insights for developing energy-aware and
cost-efficient DRL practices and establishes a foundation for incorporating
sustainability considerations into future algorithmic design and evaluation.

</details>


### [172] [Q-SafeML: Safety Assessment of Quantum Machine Learning via Quantum Distance Metrics](https://arxiv.org/abs/2509.04536)
*Oliver Dunn,Koorosh Aslansefat,Yiannis Papadopoulos*

Main category: cs.LG

TL;DR: The paper introduces Q-SafeML, a quantum-specific safety monitoring method for Quantum Machine Learning (QML) that adapts classical SafeML techniques. It uses quantum-centric distance measures to ensure accuracy and confidence in QML systems.


<details>
  <summary>Details</summary>
Motivation: QML lacks dedicated safety mechanisms despite its growing use in safety-critical systems, prompting the need for tailored safety solutions suited to quantum computation.

Method: Q-SafeML adapts classical SafeML by integrating quantum-centric statistical distance measures to assess model accuracy and address concept drift in QML outputs.

Result: Q-SafeML effectively monitors safety in QML systems, demonstrated in experiments with Quantum Convolutional Neural Network (QCNN) and Variational Quantum Circuit (VQC) models.

Conclusion: The method enhances transparency and safety in QML systems, providing a novel solution aligned with the unique requirements of quantum computation.

Abstract: The rise of machine learning in safety-critical systems has paralleled
advancements in quantum computing, leading to the emerging field of Quantum
Machine Learning (QML). While safety monitoring has progressed in classical ML,
existing methods are not directly applicable to QML due to fundamental
differences in quantum computation. Given the novelty of QML, dedicated safety
mechanisms remain underdeveloped. This paper introduces Q-SafeML, a safety
monitoring approach for QML. The method builds on SafeML, a recent method that
utilizes statistical distance measures to assess model accuracy and provide
confidence in the reasoning of an algorithm. An adapted version of Q-SafeML
incorporates quantum-centric distance measures, aligning with the probabilistic
nature of QML outputs. This shift to a model-dependent, post-classification
evaluation represents a key departure from classical SafeML, which is
dataset-driven and classifier-agnostic. The distinction is motivated by the
unique representational constraints of quantum systems, requiring distance
metrics defined over quantum state spaces. Q-SafeML detects distances between
operational and training data addressing the concept drifts in the context of
QML. Experiments on QCNN and VQC Models show that this enables informed human
oversight, enhancing system transparency and safety.

</details>


### [173] [Measuring the Measures: Discriminative Capacity of Representational Similarity Metrics Across Model Families](https://arxiv.org/abs/2509.04622)
*Jialin Wu,Shreya Saha,Yiqing Bo,Meenakshi Khosla*

Main category: cs.LG

TL;DR: The paper introduces a framework to compare representational similarity metrics in their ability to distinguish model families (e.g., CNNs, Vision Transformers) across architectures and training regimes. It evaluates metrics like RSA, Procrustes, and soft matching using separability measures, showing soft matching performs best followed by Procrustes and linear predictivity.


<details>
  <summary>Details</summary>
Motivation: There is a lack of systematic evaluation of representational similarity metrics’ discriminative power across architectural and training regimes in neuroscience and AI.

Method: The researchers developed a quantitative evaluation framework using measures like d-prime, silhouette coefficients, and ROC-AUC to assess metrics such as RSA, linear predictivity, Procrustes, and soft matching based on their ability to separate model families.

Result: The study found that metrics with more stringent alignment constraints (e.g., soft matching) outperform others in separability, with soft-matching leading, followed by Procrustes and linear predictivity. Non-fitting methods like RSA also exhibit strong separability.

Conclusion: The framework offers insights into the sensitivity of widely-used similarity metrics and serves as a guide for selecting appropriate metrics for large-scale model-to-model or model-to-brain comparisons.

Abstract: Representational similarity metrics are fundamental tools in neuroscience and
AI, yet we lack systematic comparisons of their discriminative power across
model families. We introduce a quantitative framework to evaluate
representational similarity measures based on their ability to separate model
families-across architectures (CNNs, Vision Transformers, Swin Transformers,
ConvNeXt) and training regimes (supervised vs. self-supervised). Using three
complementary separability measures-dprime from signal detection theory,
silhouette coefficients and ROC-AUC, we systematically assess the
discriminative capacity of commonly used metrics including RSA, linear
predictivity, Procrustes, and soft matching. We show that separability
systematically increases as metrics impose more stringent alignment
constraints. Among mapping-based approaches, soft-matching achieves the highest
separability, followed by Procrustes alignment and linear predictivity.
Non-fitting methods such as RSA also yield strong separability across families.
These results provide the first systematic comparison of similarity metrics
through a separability lens, clarifying their relative sensitivity and guiding
metric choice for large-scale model and brain comparisons.

</details>


### [174] [Finance-Grounded Optimization For Algorithmic Trading](https://arxiv.org/abs/2509.04541)
*Kasymkhan Khubiev,Mikhail Semenov,Irina Podlipnova*

Main category: cs.LG

TL;DR: The paper introduces financial-specific loss functions and turnover regularization as tools for improving deep learning applications in finance, particularly in return prediction tasks.


<details>
  <summary>Details</summary>
Motivation: To address the gap in applying deep learning to finance with interpretable AI, where traditional methods often fail to meet the nuanced metrics required in this domain.

Method: Develop and integrate financially grounded loss functions such as Sharpe ratio, Profit-and-Loss (PnL), Maximum Drawdown, and turnover regularization into model training for financial tasks.

Result: The financially grounded loss functions with turnover regularization consistently outperform traditional metrics like mean squared error in return prediction tasks, as evaluated by algorithmic trading metrics.

Conclusion: The study concludes that using domain-specific financial metrics and constraints significantly boosts the performance of deep learning models in portfolio optimization and trading strategies.

Abstract: Deep Learning is evolving fast and integrates into various domains. Finance
is a challenging field for deep learning, especially in the case of
interpretable artificial intelligence (AI). Although classical approaches
perform very well with natural language processing, computer vision, and
forecasting, they are not perfect for the financial world, in which specialists
use different metrics to evaluate model performance.
  We first introduce financially grounded loss functions derived from key
quantitative finance metrics, including the Sharpe ratio, Profit-and-Loss
(PnL), and Maximum Draw down. Additionally, we propose turnover regularization,
a method that inherently constrains the turnover of generated positions within
predefined limits.
  Our findings demonstrate that the proposed loss functions, in conjunction
with turnover regularization, outperform the traditional mean squared error
loss for return prediction tasks when evaluated using algorithmic trading
metrics. The study shows that financially grounded metrics enhance predictive
performance in trading strategies and portfolio optimization.

</details>


### [175] [Flexible inference of learning rules from de novo learning data using neural networks](https://arxiv.org/abs/2509.04661)
*Yuhan Helena Liu,Victor Geadah,Jonathan Pillow*

Main category: cs.LG

TL;DR: The paper proposes a novel framework to infer learning rules from animal decision-making data during de novo task learning using deep neural networks (DNN) and recurrent neural networks (RNN).


<details>
  <summary>Details</summary>
Motivation: The research aims to understand how animals learn, especially during de novo tasks, which are underexplored in neuroscience and yet highly relevant for creating animal- or human-aligned AI systems.

Method: The authors developed a nonparametric framework that uses deep neural networks (DNNs) to model per-trial policy updates and later extended it to recurrent variants (RNNs) to account for history dependence and non-Markovian dynamics.

Result: When applied to behavioral data of mice learning sensory tasks, the proposed models achieved better predictive performance on held-out data and identified learning behaviors such as asymmetric updates after correct vs. error trials and non-Markovian dynamics.

Conclusion: This study introduces a flexible approach for deciphering biological learning rules, enabling deeper insights into animal learning processes and guiding the development of experimental designs and AI tools.

Abstract: Understanding how animals learn is a central challenge in neuroscience, with
growing relevance to the development of animal- or human-aligned artificial
intelligence. However, most existing approaches assume specific parametric
forms for the learning rule (e.g., Q-learning, policy gradient) or are limited
to simplified settings like bandit tasks, which do not involve learning a new
input-output mapping from scratch. In contrast, animals must often learn new
behaviors de novo, which poses a rich challenge for learning-rule inference. We
target this problem by inferring learning rules directly from animal
decision-making data during de novo task learning, a setting that requires
models flexible enough to capture suboptimality, history dependence, and rich
external stimulus integration without strong structural priors. We first
propose a nonparametric framework that parameterizes the per-trial update of
policy weights with a deep neural network (DNN), and validate it by recovering
ground-truth rules in simulation. We then extend to a recurrent variant (RNN)
that captures non-Markovian dynamics by allowing updates to depend on trial
history. Applied to a large behavioral dataset of mice learning a sensory
decision-making task over multiple weeks, our models improved predictions on
held-out data. The inferred rules revealed asymmetric updates after correct
versus error trials and history dependence, consistent with non-Markovian
learning. Overall, these results introduce a flexible framework for inferring
biological learning rules from behavioral data in de novo learning tasks,
providing insights to inform experimental training protocols and the
development of behavioral digital twins.

</details>


### [176] [i-Mask: An Intelligent Mask for Breath-Driven Activity Recognition](https://arxiv.org/abs/2509.04544)
*Ashutosh Kumar Sinha,Ayush Patel,Mitul Dudhat,Pritam Anand,Rahul Mishra*

Main category: cs.LG

TL;DR: This study introduces i-Mask, a sensor-equipped mask capturing breath patterns for human activity recognition (HAR), achieving over 95% predictive accuracy.


<details>
  <summary>Details</summary>
Motivation: To use breath patterns as vital physiological signals for improving human activity recognition and enabling real-time health monitoring.

Method: Developed a mask with integrated sensors to capture exhaled breath patterns. The collected data underwent preprocessing steps like noise filtering and time-series decomposition before training predictive models.

Result: The experimental evaluation showcased i-Mask's predictive accuracy surpassing 95%, validating its effectiveness for HAR and potential in healthcare and fitness.

Conclusion: i-Mask demonstrates a promising use case for leveraging breath patterns in healthcare technologies, facilitating accurate monitoring of activities and health insights.

Abstract: The patterns of inhalation and exhalation contain important physiological
signals that can be used to anticipate human behavior, health trends, and vital
parameters. Human activity recognition (HAR) is fundamentally connected to
these vital signs, providing deeper insights into well-being and enabling
real-time health monitoring. This work presents i-Mask, a novel HAR approach
that leverages exhaled breath patterns captured using a custom-developed mask
equipped with integrated sensors. Data collected from volunteers wearing the
mask undergoes noise filtering, time-series decomposition, and labeling to
train predictive models. Our experimental results validate the effectiveness of
the approach, achieving over 95\% accuracy and highlighting its potential in
healthcare and fitness applications.

</details>


### [177] [Bootstrapping Task Spaces for Self-Improvement](https://arxiv.org/abs/2509.04575)
*Minqi Jiang,Andrei Lupu,Yoram Bachrach*

Main category: cs.LG

TL;DR: This paper introduces Exploratory Iteration (ExIt), an RL method for training agents to perform multi-step self-improvement during inference-time, demonstrating its effectiveness across several domains.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable agents to self-improve over iterative sequences during inference-time without relying on a fixed and possibly arbitrary iteration depth, overcoming limitations of traditional reinforcement learning approaches.

Method: ExIt grows the task space by selectively sampling the most informative intermediate histories encountered during episodes and using them as new task instances. Explicit exploration mechanisms are employed to sustain task diversity.

Result: ExIt demonstrates inference-time self-improvement and the ability to iterate towards higher performance across tasks like competition math, multi-turn tool-use, and ML engineering, even beyond the average training iteration depth.

Conclusion: ExIt provides a practical approach for training agents capable of self-improvement at inference-time, leveraging task history sampling and exploration mechanisms to achieve robust performance.

Abstract: Progress in many task domains emerges from repeated revisions to previous
solution attempts. Training agents that can reliably self-improve over such
sequences at inference-time is a natural target for reinforcement learning
(RL), yet the naive approach assumes a fixed maximum iteration depth, which can
be both costly and arbitrary. We present Exploratory Iteration (ExIt), a family
of autocurriculum RL methods that directly exploits the recurrent structure of
self-improvement tasks to train LLMs to perform multi-step self-improvement at
inference-time while only training on the most informative single-step
iterations. ExIt grows a task space by selectively sampling the most
informative intermediate, partial histories encountered during an episode for
continued iteration, treating these starting points as new self-iteration task
instances to train a self-improvement policy. ExIt can further pair with
explicit exploration mechanisms to sustain greater task diversity. Across
several domains, encompassing competition math, multi-turn tool-use, and
machine learning engineering, we demonstrate that ExIt strategies, starting
from either a single or many task instances, can produce policies exhibiting
strong inference-time self-improvement on held-out task instances, and the
ability to iterate towards higher performance over a step budget extending
beyond the average iteration depth encountered during training.

</details>


### [178] [Fundamental bounds on efficiency-confidence trade-off for transductive conformal prediction](https://arxiv.org/abs/2509.04631)
*Arash Behboodi,Alvaro H. C. Correia,Fabio Valerio Massoli,Christos Louizos*

Main category: cs.LG

TL;DR: This research explores transductive conformal prediction, showing a trade-off between prediction confidence and prediction set efficiency, with scaling implications for datasets with inherent uncertainty.


<details>
  <summary>Details</summary>
Motivation: To address the simultaneous prediction for multiple data points within a fixed confidence level, and understand the constraints of confidence-efficiency in prediction models.

Method: The authors derive a finite-sample bound relating prediction set size to confidence levels, incorporating conditional entropy and dispersion (variance of log conditional probability). They study achievable bounds and test cases with shared labels.

Result: Derived bounds show exponential growth in prediction set size with increased confidence in uncertain data, and provide a confidence predictor optimized for shared-label scenarios.

Conclusion: The study provides insights into the unavoidable trade-offs in confidence-efficiency, the significance of conditional entropy and dispersion, and offers solutions for shared-label prediction scenarios.

Abstract: Transductive conformal prediction addresses the simultaneous prediction for
multiple data points. Given a desired confidence level, the objective is to
construct a prediction set that includes the true outcomes with the prescribed
confidence. We demonstrate a fundamental trade-off between confidence and
efficiency in transductive methods, where efficiency is measured by the size of
the prediction sets. Specifically, we derive a strict finite-sample bound
showing that any non-trivial confidence level leads to exponential growth in
prediction set size for data with inherent uncertainty. The exponent scales
linearly with the number of samples and is proportional to the conditional
entropy of the data. Additionally, the bound includes a second-order term,
dispersion, defined as the variance of the log conditional probability
distribution. We show that this bound is achievable in an idealized setting.
Finally, we examine a special case of transductive prediction where all test
data points share the same label. We show that this scenario reduces to the
hypothesis testing problem with empirically observed statistics and provide an
asymptotically optimal confidence predictor, along with an analysis of the
error exponent.

</details>


### [179] [Instance-Wise Adaptive Sampling for Dataset Construction in Approximating Inverse Problem Solutions](https://arxiv.org/abs/2509.04583)
*Jiequn Han,Kui Ren,Nathan Soedjak*

Main category: cs.LG

TL;DR: The paper introduces an adaptive sampling method to improve data efficiency in training for inverse problems, tailoring datasets dynamically based on test instances.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address inefficiencies in conventional training for inverse problems, which require large datasets, particularly when the prior is complex or high accuracy is required.

Method: The authors propose an iterative adaptive sampling framework that refines training datasets based on the geometry of the inverse map specific to each test instance.

Result: The method demonstrated gains in sample efficiency and improved performance in inverse scattering problems, especially under complex priors or stringent accuracy requirements.

Conclusion: The adaptive sampling framework offers a scalable alternative to fixed dataset training, applicable across various inverse problems to reduce data collection costs and improve accuracy.

Abstract: We propose an instance-wise adaptive sampling framework for constructing
compact and informative training datasets for supervised learning of inverse
problem solutions. Typical learning-based approaches aim to learn a
general-purpose inverse map from datasets drawn from a prior distribution, with
the training process independent of the specific test instance. When the prior
has a high intrinsic dimension or when high accuracy of the learned solution is
required, a large number of training samples may be needed, resulting in
substantial data collection costs. In contrast, our method dynamically
allocates sampling effort based on the specific test instance, enabling
significant gains in sample efficiency. By iteratively refining the training
dataset conditioned on the latest prediction, the proposed strategy tailors the
dataset to the geometry of the inverse map around each test instance. We
demonstrate the effectiveness of our approach in the inverse scattering problem
under two types of structured priors. Our results show that the advantage of
the adaptive method becomes more pronounced in settings with more complex
priors or higher accuracy requirements. While our experiments focus on a
particular inverse problem, the adaptive sampling strategy is broadly
applicable and readily extends to other inverse problems, offering a scalable
and practical alternative to conventional fixed-dataset training regimes.

</details>


### [180] [An Efficient Subspace Algorithm for Federated Learning on Heterogeneous Data](https://arxiv.org/abs/2509.05213)
*Jiaojiao Zhang,Yuqi Xu,Kun Yuan*

Main category: cs.LG

TL;DR: FedSub is proposed to enhance federated learning efficiency, addressing client drift and cost issues via subspace projection techniques.


<details>
  <summary>Details</summary>
Motivation: Overcoming challenges in federated learning related to data heterogeneity, communication, computation, and memory costs.

Method: FedSub employs subspace projection for reduced dimensions, ensuring efficient local updates and dual variables to counteract client drift.

Result: Convergence analysis highlights critical elements affecting performance, supported by experimental verification of efficiency.

Conclusion: FedSub improves large-scale federated learning through reduced costs and mitigated client drift, showcasing promising experimental results.

Abstract: This work addresses the key challenges of applying federated learning to
large-scale deep neural networks, particularly the issue of client drift due to
data heterogeneity across clients and the high costs of communication,
computation, and memory. We propose FedSub, an efficient subspace algorithm for
federated learning on heterogeneous data. Specifically, FedSub utilizes
subspace projection to guarantee local updates of each client within
low-dimensional subspaces, thereby reducing communication, computation, and
memory costs. Additionally, it incorporates low-dimensional dual variables to
mitigate client drift. We provide convergence analysis that reveals the impact
of key factors such as step size and subspace projection matrices on
convergence. Experimental results demonstrate its efficiency.

</details>


### [181] [Toward Faithfulness-guided Ensemble Interpretation of Neural Network](https://arxiv.org/abs/2509.04588)
*Siyu Zhang,Kenneth Mcmillan*

Main category: cs.LG

TL;DR: The paper introduces FEI, a framework enhancing interpretability in neural networks through improved faithfulness in explanations.


<details>
  <summary>Details</summary>
Motivation: Faithful and interpretable explanations for neural network inferences are necessary to understand and evaluate model behavior effectively.

Method: FEI uses a smooth approximation technique to improve quantitative faithfulness scores and introduces a novel metric to assess faithfulness in hidden layers.

Result: FEI outperforms existing methods in both qualitative visualization and quantitative faithfulness metrics.

Conclusion: FEI is a comprehensive framework that significantly enhances the breadth and precision of neural network interpretations.

Abstract: Interpretable and faithful explanations for specific neural inferences are
crucial for understanding and evaluating model behavior. Our work introduces
\textbf{F}aithfulness-guided \textbf{E}nsemble \textbf{I}nterpretation
(\textbf{FEI}), an innovative framework that enhances the breadth and
effectiveness of faithfulness, advancing interpretability by providing superior
visualization. Through an analysis of existing evaluation benchmarks,
\textbf{FEI} employs a smooth approximation to elevate quantitative
faithfulness scores. Diverse variations of \textbf{FEI} target enhanced
faithfulness in hidden layer encodings, expanding interpretability.
Additionally, we propose a novel qualitative metric that assesses hidden layer
faithfulness. In extensive experiments, \textbf{FEI} surpasses existing
methods, demonstrating substantial advances in qualitative visualization and
quantitative faithfulness scores. Our research establishes a comprehensive
framework for elevating faithfulness in neural network explanations,
emphasizing both breadth and precision

</details>


### [182] [Quantum-Enhanced Multi-Task Learning with Learnable Weighting for Pharmacokinetic and Toxicity Prediction](https://arxiv.org/abs/2509.04601)
*Han Zhang,Fengji Ma,Jiamin Su,Xinyue Yang,Lei Wang,Wen-Cai Ye,Li Liu*

Main category: cs.LG

TL;DR: The paper introduces QW-MTL, a unified multi-task learning framework for ADMET classification tasks, which incorporates quantum chemical descriptors and adaptive task weighting to outperform traditional single-task learning methods on the TDC benchmarks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve ADMET predictions in drug discovery by overcoming the limitations of single-task learning approaches, which fail to utilize task complementarities and are computationally intensive.

Method: QW-MTL uses quantum chemical descriptors to boost molecular representations and introduces an exponential task-weighting mechanism for balancing losses between tasks. It employs Chemprop-RDKit as the backbone and evaluates its performance across all TDC classification benchmarks.

Result: QW-MTL significantly outperformed single-task learning baselines on 12 out of 13 tasks, demonstrating both high accuracy and reduced computational complexity.

Conclusion: The proposed QW-MTL framework effectively combines quantum-informed features and dynamic task weighting, achieving superior multi-task learning for ADMET predictions in a standardized evaluation setting.

Abstract: Prediction for ADMET (Absorption, Distribution, Metabolism, Excretion, and
Toxicity) plays a crucial role in drug discovery and development, accelerating
the screening and optimization of new drugs. Existing methods primarily rely on
single-task learning (STL), which often fails to fully exploit the
complementarities between tasks. Besides, it requires more computational
resources while training and inference of each task independently. To address
these issues, we propose a new unified Quantum-enhanced and task-Weighted
Multi-Task Learning (QW-MTL) framework, specifically designed for ADMET
classification tasks. Built upon the Chemprop-RDKit backbone, QW-MTL adopts
quantum chemical descriptors to enrich molecular representations with
additional information about the electronic structure and interactions.
Meanwhile, it introduces a novel exponential task weighting scheme that
combines dataset-scale priors with learnable parameters to achieve dynamic loss
balancing across tasks. To the best of our knowledge, this is the first work to
systematically conduct joint multi-task training across all 13 Therapeutics
Data Commons (TDC) classification benchmarks, using leaderboard-style data
splits to ensure a standardized and realistic evaluation setting. Extensive
experimental results show that QW-MTL significantly outperforms single-task
baselines on 12 out of 13 tasks, achieving high predictive performance with
minimal model complexity and fast inference, demonstrating the effectiveness
and efficiency of multi-task molecular learning enhanced by quantum-informed
features and adaptive task weighting.

</details>


### [183] [Split Conformal Prediction in the Function Space with Neural Operators](https://arxiv.org/abs/2509.04623)
*David Millard,Lars Lindemann,Ali Baheri*

Main category: cs.LG

TL;DR: This paper addresses uncertainty quantification for neural operators in infinite-dimensional settings, proposing an extension of split conformal prediction to function spaces.


<details>
  <summary>Details</summary>
Motivation: The field lacks robust techniques to quantify uncertainty for neural operators in infinite-dimensional function spaces, particularly finite-sample coverage guarantees.

Method: The authors extend split conformal prediction to function spaces using a two-step process: finite-sample guarantees via discretization maps, followed by lifting these guarantees asymptotically as discretization resolution improves.

Result: Empirical evaluations demonstrate that the proposed method maintains calibrated coverage even under resolution shifts, improves coverage in super-resolution tasks, and introduces metrics to monitor forecast quality.

Conclusion: The introduced approach tackles the challenges of uncertainty quantification in infinite-dimensional settings effectively, offering practical improvements in calibration and diagnostics for neural operator models.

Abstract: Uncertainty quantification for neural operators remains an open problem in
the infinite-dimensional setting due to the lack of finite-sample coverage
guarantees over functional outputs. While conformal prediction offers
finite-sample guarantees in finite-dimensional spaces, it does not directly
extend to function-valued outputs. Existing approaches (Gaussian processes,
Bayesian neural networks, and quantile-based operators) require strong
distributional assumptions or yield conservative coverage. This work extends
split conformal prediction to function spaces following a two step method. We
first establish finite-sample coverage guarantees in a finite-dimensional space
using a discretization map in the output function space. Then these guarantees
are lifted to the function-space by considering the asymptotic convergence as
the discretization is refined. To characterize the effect of resolution, we
decompose the conformal radius into discretization, calibration, and
misspecification components. This decomposition motivates a regression-based
correction to transfer calibration across resolutions. Additionally, we propose
two diagnostic metrics (conformal ensemble score and internal agreement) to
quantify forecast degradation in autoregressive settings. Empirical results
show that our method maintains calibrated coverage with less variation under
resolution shifts and achieves better coverage in super-resolution tasks.

</details>


### [184] [Interpreting Transformer Architectures as Implicit Multinomial Regression](https://arxiv.org/abs/2509.04653)
*Jonas A. Actor,Anthony Gruber,Eric C. Cyr*

Main category: cs.LG

TL;DR: The paper connects attention mechanisms in transformers to multinomial regression, showing how attention dynamics recover optimal classification features.


<details>
  <summary>Details</summary>
Motivation: To better understand the opaque attention mechanisms in transformers and their connection to broader concepts like feature polysemanticity and model performance.

Method: This paper compares the dynamics of attention blocks with the structure of optimizing over latent features in a fixed multinomial regression setting.

Result: It demonstrates that the evolution of representations in transformers aligns with recovering optimal features in multinomial regression.

Conclusion: The results suggest that transformer attention mechanisms can be interpreted as optimal feature recovery trajectories for classification.

Abstract: Mechanistic interpretability aims to understand how internal components of
modern machine learning models, such as weights, activations, and layers, give
rise to the model's overall behavior. One particularly opaque mechanism is
attention: despite its central role in transformer models, its mathematical
underpinnings and relationship to concepts like feature polysemanticity,
superposition, and model performance remain poorly understood. This paper
establishes a novel connection between attention mechanisms and multinomial
regression. Specifically, we show that in a fixed multinomial regression
setting, optimizing over latent features yields optimal solutions that align
with the dynamics induced by attention blocks. In other words, the evolution of
representations through a transformer can be interpreted as a trajectory that
recovers the optimal features for classification.

</details>


### [185] [Beyond Ordinary Lipschitz Constraints: Differentially Private Stochastic Optimization with Tsybakov Noise Condition](https://arxiv.org/abs/2509.04668)
*Difei Xu,Meng Ding,Zihang Xiang,Jinhui Xu,Di Wang*

Main category: cs.LG

TL;DR: The paper investigates Stochastic Convex Optimization under the Differential Privacy (DP) model when the population risk satisfies the Tsybakov Noise Condition (TNC). It provides DP algorithms with utility bounds independent of the Lipschitz constant.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of performing Stochastic Convex Optimization in a Differential Privacy framework for cases where the Lipschitz constant may be unbounded and ensure rigorous privacy protections.

Method: Proposed an $(\varepsilon, \delta)$-DP algorithm with utility bounds dependent on the $2k$-th moment of the loss gradient, and extended this approach to non-Lipschitz cases and smaller privacy budgets. Also derived a private minimax lower bound for the problem.

Result: Achieved utility bounds independent of the Lipschitz constant and established lower bounds for private minimax rates. The bounds rely on sample size (n), model dimension (d), privacy budget ($\varepsilon$), and gradient moments.

Conclusion: The study demonstrates optimization under DP is feasible even with bounded moments and unbounded Lipschitz constants, providing theoretical guarantees for utility and limits.

Abstract: We study Stochastic Convex Optimization in the Differential Privacy model
(DP-SCO). Unlike previous studies, here we assume the population risk function
satisfies the Tsybakov Noise Condition (TNC) with some parameter $\theta>1$,
where the Lipschitz constant of the loss could be extremely large or even
unbounded, but the $\ell_2$-norm gradient of the loss has bounded $k$-th moment
with $k\geq 2$. For the Lipschitz case with $\theta\geq 2$, we first propose an
$(\varepsilon, \delta)$-DP algorithm whose utility bound is
$\Tilde{O}\left(\left(\tilde{r}_{2k}(\frac{1}{\sqrt{n}}+(\frac{\sqrt{d}}{n\varepsilon}))^\frac{k-1}{k}\right)^\frac{\theta}{\theta-1}\right)$
in high probability, where $n$ is the sample size, $d$ is the model dimension,
and $\tilde{r}_{2k}$ is a term that only depends on the $2k$-th moment of the
gradient. It is notable that such an upper bound is independent of the
Lipschitz constant. We then extend to the case where
  $\theta\geq \bar{\theta}> 1$ for some known constant $\bar{\theta}$.
Moreover, when the privacy budget $\varepsilon$ is small enough, we show an
upper bound of
$\tilde{O}\left(\left(\tilde{r}_{k}(\frac{1}{\sqrt{n}}+(\frac{\sqrt{d}}{n\varepsilon}))^\frac{k-1}{k}\right)^\frac{\theta}{\theta-1}\right)$
even if the loss function is not Lipschitz. For the lower bound, we show that
for any $\theta\geq 2$, the private minimax rate for $\rho$-zero Concentrated
Differential Privacy is lower bounded by
$\Omega\left(\left(\tilde{r}_{k}(\frac{1}{\sqrt{n}}+(\frac{\sqrt{d}}{n\sqrt{\rho}}))^\frac{k-1}{k}\right)^\frac{\theta}{\theta-1}\right)$.

</details>


### [186] [Echoes Before Collapse: Deep Learning Detection of Flickering in Complex Systems](https://arxiv.org/abs/2509.04683)
*Yazdan Babazadeh Maghsoodlo,Madhur Anand,Chris T. Bauch*

Main category: cs.LG

TL;DR: The paper demonstrates that convolutional long short-term memory (CNN LSTM) models can detect flickering patterns in noisy, nonlinear time series, serving as early warning signals for critical regime shifts in diverse systems.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore the potential of deep learning, specifically CNN LSTM models, for identifying flickering (noise-driven switches between coexisting stable states), which signals reduced resilience and precedes critical regime shifts across various systems.

Method: CNN LSTM models were trained on synthetic time series generated from simple polynomial functions with additive noise to detect flickering patterns. These models were then tested on diverse stochastic systems and empirical datasets.

Result: Trained CNN LSTM models could effectively identify flickering patterns in both synthetic and empirical datasets, including dormouse body temperature and palaeoclimate records.

Conclusion: Deep learning, leveraging its ability to analyze noisy, nonlinear time series, offers a versatile framework for detecting early warning signals and identifying instability in complex dynamical systems.

Abstract: Deep learning offers powerful tools for anticipating tipping points in
complex systems, yet its potential for detecting flickering (noise-driven
switching between coexisting stable states) remains unexplored. Flickering is a
hallmark of reduced resilience in climate systems, ecosystems, financial
markets, and other systems. It can precede critical regime shifts that are
highly impactful but difficult to predict. Here we show that convolutional long
short-term memory (CNN LSTM) models, trained on synthetic time series generated
from simple polynomial functions with additive noise, can accurately identify
flickering patterns. Despite being trained on simplified dynamics, our models
generalize to diverse stochastic systems and reliably detect flickering in
empirical datasets, including dormouse body temperature records and
palaeoclimate proxies from the African Humid Period. These findings demonstrate
that deep learning can extract early warning signals from noisy, nonlinear time
series, providing a flexible framework for identifying instability across a
wide range of dynamical systems.

</details>


### [187] [KRAFT: A Knowledge Graph-Based Framework for Automated Map Conflation](https://arxiv.org/abs/2509.04684)
*Farnoosh Hashemi,Laks V. S. Lakshmanan*

Main category: cs.LG

TL;DR: KRAFT is a learning-based approach for map conflation that overcomes limitations of existing heuristic methods by leveraging knowledge graphs and advanced alignment techniques.


<details>
  <summary>Details</summary>
Motivation: To address shortcomings in existing map conflation methods, specifically their focus on linear objects and reliance on heuristic approaches.

Method: KRAFT integrates three modules: Knowledge Graph Construction to represent geospatial databases, Map Matching for entity alignment using knowledge graph alignment and geospatial feature encoding, and Map Merging using mixed integer linear programming.

Result: Experimental results show KRAFT outperforms state-of-the-art methods in operational performance for map conflation tasks, and individual modules exceed traditional matching and merging methods.

Conclusion: KRAFT effectively introduces a learning-based method for map conflation, enabling comprehensive augmentation of geospatial databases with improved accuracy and consistency.

Abstract: Digital maps play a crucial role in various applications such as navigation,
fleet management, and ride-sharing, necessitating their accuracy and currency,
which require timely updates. While the majority of geospatial databases (GDBs)
provide high-quality information, their data is (i) limited to specific regions
and/or (ii) missing some entities, even in their covered areas. Map conflation
is the process of augmentation of a GDB using another GDB to conflate missing
spatial features. Existing map conflation methods suffer from two main
limitations: (1) They are designed for the conflation of linear objects (e.g.,
road networks) and cannot simply be extended to non-linear objects, thus
missing information about most entities in the map. (2) They are heuristic
algorithmic approaches that are based on pre-defined rules, unable to learn
entities matching in a data-driven manner. To address these limitations, we
design KRAFT, a learning based approach consisting of three parts: (1)
Knowledge Graph Construction - where each GDB is represented by a knowledge
graph, (2) Map Matching - where we use a knowledge graph alignment method as
well as a geospatial feature encoder to match entities in obtained knowledge
graphs, and (3) Map Merging - where we merge matched entities in the previous
modules in a consistent manner, using a mixed integer linear programming
formulation that fully merges the GDBs without adding any inconsistencies. Our
experimental evaluation shows that not only does KRAFT achieve outstanding
performance compared to state-of-the-art and baseline methods in map conflation
tasks, but each of its modules (e.g., Map Matching and Map Merging) also
separately outperforms traditional matching and merging methods.

</details>


### [188] [CPEP: Contrastive Pose-EMG Pre-training Enhances Gesture Generalization on EMG Signals](https://arxiv.org/abs/2509.04699)
*Wenhui Cui,Christopher Sandino,Hadi Pouransari,Ran Liu,Juri Minxha,Ellen L. Zippi,Aman Verma,Anna Sedlackova,Behrooz Mahasseni,Erdrin Azemi*

Main category: cs.LG

TL;DR: This paper introduces a machine learning framework to align low-quality EMG biosignals with high-quality pose data for better hand gesture classification, achieving remarkable zero-shot classification improvements.


<details>
  <summary>Details</summary>
Motivation: To leverage cost-effective, low-power biosignals (e.g., sEMG) for wearable devices to enhance gesture classification while compensating for the weaker modality.

Method: The paper introduces a Contrastive Pose-EMG Pre-training (CPEP) framework to align EMG signals with pose data, creating high-quality EMG representations by contrastive learning.

Result: The proposed framework improves gesture classification, achieving up to 21% enhancement in in-distribution accuracy and 72% in out-of-distribution (unseen data) zero-shot classification.

Conclusion: Aligning weak-modality biosignals with structured high-quality data enhances representation quality, enabling better gesture classification and zero-shot learning applications in wearables.

Abstract: Hand gesture classification using high-quality structured data such as
videos, images, and hand skeletons is a well-explored problem in computer
vision. Leveraging low-power, cost-effective biosignals, e.g. surface
electromyography (sEMG), allows for continuous gesture prediction on wearables.
In this paper, we demonstrate that learning representations from weak-modality
data that are aligned with those from structured, high-quality data can improve
representation quality and enables zero-shot classification. Specifically, we
propose a Contrastive Pose-EMG Pre-training (CPEP) framework to align EMG and
pose representations, where we learn an EMG encoder that produces high-quality
and pose-informative representations. We assess the gesture classification
performance of our model through linear probing and zero-shot setups. Our model
outperforms emg2pose benchmark models by up to 21% on in-distribution gesture
classification and 72% on unseen (out-of-distribution) gesture classification.

</details>


### [189] [Natural Spectral Fusion: p-Exponent Cyclic Scheduling and Early Decision-Boundary Alignment in First-Order Optimization](https://arxiv.org/abs/2509.04713)
*Gongyue Zhang,Honghai Liu*

Main category: cs.LG

TL;DR: This paper introduces Natural Spectral Fusion (NSF), a method that treats optimizers as spectral controllers to balance frequency information, improving model performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of understanding of the intrinsic spectral bias of first-order optimizers and their impact on optimization paths.

Method: NSF reframes training by dynamically balancing spectral information with a p-exponent extension in the second-moment term, implemented via cyclic scheduling, without modifying the model, data, or training setup.

Result: The proposed method improves spectral coverage, cross-band fusion, and early alignment of decision boundaries while reducing test error and training costs on multiple benchmarks.

Conclusion: NSF enhances first-order optimization by making optimization paths spectral-controllable, offering a robust, unified framework to improve training efficiency and accuracy.

Abstract: Spectral behaviors have been widely discussed in machine learning, yet the
optimizer's own spectral bias remains unclear. We argue that first-order
optimizers exhibit an intrinsic frequency preference that significantly
reshapes the optimization path. To address this, we propose Natural Spectral
Fusion (NSF): reframing training as controllable spectral coverage and
information fusion rather than merely scaling step sizes. NSF has two core
principles: treating the optimizer as a spectral controller that dynamically
balances low- and high-frequency information; and periodically reweighting
frequency bands at negligible cost, without modifying the model, data, or
training pipeline. We realize NSF via a p-exponent extension of the
second-moment term, enabling both positive and negative exponents, and
implement it through cyclic scheduling. Theory and experiments show that
adaptive methods emphasize low frequencies, SGD is near-neutral, and negative
exponents amplify high-frequency information. Cyclic scheduling broadens
spectral coverage, improves cross-band fusion, and induces early
decision-boundary alignment, where accuracy improves even while loss remains
high. Across multiple benchmarks, with identical learning-rate strategies and
fixed hyperparameters, p-exponent cyclic scheduling consistently reduces test
error and demonstrates distinct convergence behavior; on some tasks, it matches
baseline accuracy with only one-quarter of the training cost. Overall, NSF
reveals the optimizer's role as an active spectral controller and provides a
unified, controllable, and efficient framework for first-order optimization.

</details>


### [190] [CoVeR: Conformal Calibration for Versatile and Reliable Autoregressive Next-Token Prediction](https://arxiv.org/abs/2509.04733)
*Yuzhu Chen,Yingjie Wang,Shunyu Liu,Yongcheng Jing,Dacheng Tao*

Main category: cs.LG

TL;DR: The paper introduces \textsc{CoVeR}, a new model-free decoding strategy that uses conformal prediction to balance search efficiency and ensure high coverage probability, particularly in long-tail sequences.


<details>
  <summary>Details</summary>
Motivation: Mainstream decoding methods, such as beam search, lack provable coverage guarantees and struggle with search efficiency, especially for long-tail sequences critical to real-world tasks.

Method: The authors propose \textsc{CoVeR}, a model-free decoding strategy within the conformal prediction framework, which guarantees a compact search space and high coverage probability.

Result: Theoretically, \textsc{CoVeR} guarantees a lower-bound coverage rate of at least $1 - \alpha$ asymptotically, where $\alpha$ is a chosen level in the range $(0,1)$.

Conclusion: \textsc{CoVeR} effectively addresses efficiency and coverage limitations of traditional decoding methods, providing both theoretical guarantees and practical benefits for complex reasoning tasks.

Abstract: Autoregressive pre-trained models combined with decoding methods have
achieved impressive performance on complex reasoning tasks. While mainstream
decoding strategies such as beam search can generate plausible candidate sets,
they often lack provable coverage guarantees, and struggle to effectively
balance search efficiency with the need for versatile trajectories,
particularly those involving long-tail sequences that are essential in certain
real-world applications. To address these limitations, we propose
\textsc{CoVeR}, a novel model-free decoding strategy wihtin the conformal
prediction framework that simultaneously maintains a compact search space and
ensures high coverage probability over desirable trajectories. Theoretically,
we establish a PAC-style generalization bound, guaranteeing that \textsc{CoVeR}
asymptotically achieves a coverage rate of at least $1 - \alpha$ for any target
level $\alpha \in (0,1)$.

</details>


### [191] [Beyond I-Con: Exploring New Dimension of Distance Measures in Representation Learning](https://arxiv.org/abs/2509.04734)
*Jasmine Shone,Shaden Alshammari,Mark Hamilton,Zhening Li,William Freeman*

Main category: cs.LG

TL;DR: The paper introduces the 'Beyond I-Con' framework that generalizes representation learning approaches by exploring alternative statistical divergences and similarity kernels, leading to improvements across clustering, contrastive learning, and dimensionality reduction tasks.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address limitations of the KL divergence function, commonly used in many representation learning methods, citing issues such as its asymmetry, unboundedness, and misalignment with true optimization objectives.

Method: The authors propose a framework for discovering new loss functions by replacing KL divergence with other statistical divergences (e.g., total variation distance, bounded f-divergence) and introducing alternative similarity kernels in representation learning.

Result: The new framework sets state-of-the-art performance in unsupervised clustering, improves supervised contrastive learning outputs, and outperforms SNE in dimensionality reduction by introducing bounded divergences and new kernels.

Conclusion: The choice of divergence functions and similarity kernels is crucial in representation learning and optimizing these components can significantly improve model performance.

Abstract: The Information Contrastive (I-Con) framework revealed that over 23
representation learning methods implicitly minimize KL divergence between data
and learned distributions that encode similarities between data points.
However, a KL-based loss may be misaligned with the true objective, and
properties of KL divergence such as asymmetry and unboundedness may create
optimization challenges. We present Beyond I-Con, a framework that enables
systematic discovery of novel loss functions by exploring alternative
statistical divergences and similarity kernels. Key findings: (1) on
unsupervised clustering of DINO-ViT embeddings, we achieve state-of-the-art
results by modifying the PMI algorithm to use total variation (TV) distance;
(2) on supervised contrastive learning, we outperform the standard approach by
using TV and a distance-based similarity kernel instead of KL and an angular
kernel; (3) on dimensionality reduction, we achieve superior qualitative
results and better performance on downstream tasks than SNE by replacing KL
with a bounded f-divergence. Our results highlight the importance of
considering divergence and similarity kernel choices in representation learning
optimization.

</details>


### [192] [VARMA-Enhanced Transformer for Time Series Forecasting](https://arxiv.org/abs/2509.04782)
*Jiajun Song,Xiaoou Liu*

Main category: cs.LG

TL;DR: The paper introduces VARMAformer, which integrates classical time series analysis with modern attention mechanisms to improve time series forecasting accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The authors aimed to bridge the gap between modern time series transformers' ability to handle global dependencies and statistical models' capacity to capture fine-grained local temporal patterns.

Method: VARMAformer combines a VARMA-inspired Feature Extractor (VFE) for local AR and MA pattern modeling with a VARMA-Enhanced Attention (VE-atten) mechanism, adding context-awareness to queries within a cross-attention framework.

Result: The proposed model consistently outperforms state-of-the-art methods on widely-used time series forecasting benchmark datasets.

Conclusion: Integrating classical statistical insights into deep learning frameworks can significantly enhance performance in time series forecasting, as demonstrated by VARMAformer.

Abstract: Transformer-based models have significantly advanced time series forecasting.
Recent work, like the Cross-Attention-only Time Series transformer (CATS),
shows that removing self-attention can make the model more accurate and
efficient. However, these streamlined architectures may overlook the
fine-grained, local temporal dependencies effectively captured by classical
statistical models like Vector AutoRegressive Moving Average model (VARMA). To
address this gap, we propose VARMAformer, a novel architecture that synergizes
the efficiency of a cross-attention-only framework with the principles of
classical time series analysis. Our model introduces two key innovations: (1) a
dedicated VARMA-inspired Feature Extractor (VFE) that explicitly models
autoregressive (AR) and moving-average (MA) patterns at the patch level, and
(2) a VARMA-Enhanced Attention (VE-atten) mechanism that employs a temporal
gate to make queries more context-aware. By fusing these classical insights
into a modern backbone, VARMAformer captures both global, long-range
dependencies and local, statistical structures. Through extensive experiments
on widely-used benchmark datasets, we demonstrate that our model consistently
outperforms existing state-of-the-art methods. Our work validates the
significant benefit of integrating classical statistical insights into modern
deep learning frameworks for time series forecasting.

</details>


### [193] [Graph Unlearning: Efficient Node Removal in Graph Neural Networks](https://arxiv.org/abs/2509.04785)
*Faqian Guan,Tianqing Zhu,Zhoutian Wang,Wei Ren,Wanlei Zhou*

Main category: cs.LG

TL;DR: The paper introduces three methods to improve node unlearning in graph neural networks (GNNs), enhancing privacy protection and model efficiency while addressing challenges in existing approaches.


<details>
  <summary>Details</summary>
Motivation: Rising concerns about privacy attacks and sensitive information leakage in GNN models require efficient techniques for removing training node data without compromising graph structure or model performance.

Method: The paper proposes three strategies for node unlearning: Class-based Label Replacement, Topology-guided Neighbor Mean Posterior Probability, and Class-consistent Neighbor Node Filtering, emphasizing graph topology utilization.

Result: Experiments on three benchmark datasets confirm the proposed methods offer better model utility, unlearning utility, and unlearning efficiency compared to state-of-the-art techniques.

Conclusion: The methods enhance privacy protection for sensitive nodes in GNNs, advance the field of node unlearning, and provide actionable insights for improving GNN model security.

Abstract: With increasing concerns about privacy attacks and potential sensitive
information leakage, researchers have actively explored methods to efficiently
remove sensitive training data and reduce privacy risks in graph neural network
(GNN) models. Node unlearning has emerged as a promising technique for
protecting the privacy of sensitive nodes by efficiently removing specific
training node information from GNN models. However, existing node unlearning
methods either impose restrictions on the GNN structure or do not effectively
utilize the graph topology for node unlearning. Some methods even compromise
the graph's topology, making it challenging to achieve a satisfactory
performance-complexity trade-off. To address these issues and achieve efficient
unlearning for training node removal in GNNs, we propose three novel node
unlearning methods: Class-based Label Replacement, Topology-guided Neighbor
Mean Posterior Probability, and Class-consistent Neighbor Node Filtering. Among
these methods, Topology-guided Neighbor Mean Posterior Probability and
Class-consistent Neighbor Node Filtering effectively leverage the topological
features of the graph, resulting in more effective node unlearning. To validate
the superiority of our proposed methods in node unlearning, we conducted
experiments on three benchmark datasets. The evaluation criteria included model
utility, unlearning utility, and unlearning efficiency. The experimental
results demonstrate the utility and efficiency of the proposed methods and
illustrate their superiority compared to state-of-the-art node unlearning
methods. Overall, the proposed methods efficiently remove sensitive training
nodes and protect the privacy information of sensitive nodes in GNNs. The
findings contribute to enhancing the privacy and security of GNN models and
provide valuable insights into the field of node unlearning.

</details>


### [194] [An Arbitration Control for an Ensemble of Diversified DQN variants in Continual Reinforcement Learning](https://arxiv.org/abs/2509.04815)
*Wonseo Jang,Dongjae Kim*

Main category: cs.LG

TL;DR: The paper presents ACED-DQN, a framework using arbitration control over diversified RL agents to address catastrophic forgetting in continual reinforcement learning (CRL).


<details>
  <summary>Details</summary>
Motivation: To overcome catastrophic forgetting in deep reinforcement learning models during continual learning scenarios, inspired by human prefrontal cortex decision-making.

Method: Introduced an ensemble of diversified DQN variants and arbitration control that prioritizes reliable agents.

Result: Demonstrated significant performance improvements in both static and continual environments, with evidence supporting the proposed arbitration control mechanism.

Conclusion: ACED-DQN allows RL agents to continuously learn effectively, inspired by human brain decision mechanisms.

Abstract: Deep reinforcement learning (RL) models, despite their efficiency in learning
an optimal policy in static environments, easily loses previously learned
knowledge (i.e., catastrophic forgetting). It leads RL models to poor
performance in continual reinforcement learning (CRL) scenarios. To address
this, we present an arbitration control mechanism over an ensemble of RL
agents. It is motivated by and closely aligned with how humans make decisions
in a CRL context using an arbitration control of multiple RL agents in parallel
as observed in the prefrontal cortex. We integrated two key ideas into our
model: (1) an ensemble of RLs (i.e., DQN variants) explicitly trained to have
diverse value functions and (2) an arbitration control that prioritizes agents
with higher reliability (i.e., less error) in recent trials. We propose a
framework for CRL, an Arbitration Control for an Ensemble of Diversified DQN
variants (ACED-DQN). We demonstrate significant performance improvements in
both static and continual environments, supported by empirical evidence showing
the effectiveness of arbitration control over diversified DQNs during training.
In this work, we introduced a framework that enables RL agents to continuously
learn, with inspiration from the human brain.

</details>


### [195] [Revolution or Hype? Seeking the Limits of Large Models in Hardware Design](https://arxiv.org/abs/2509.04905)
*Qiang Xu,Leon Stok,Rolf Drechsler,Xi Wang,Grace Li Zhang,Igor L. Markov*

Main category: cs.LG

TL;DR: The paper explores the potential and controversy surrounding the use of LLMs and LCMs in electronic design automation (EDA), analyzing their practical capabilities, limitations, and future prospects.


<details>
  <summary>Details</summary>
Motivation: The paper addresses skepticism around whether large AI models represent a true paradigm shift in circuit design or merely short-lived hype.

Method: The authors synthesize expert viewpoints from academia and industry to critically evaluate aspects like reliability, scalability, and interpretability of AI models in hardware design.

Result: The paper provides a balanced discussion and authoritative overview of the relevance of large AI models in outperforming or complementing traditional EDA methodologies.

Conclusion: While these AI models hold transformative potential, the paper calls for cautious optimism, emphasizing both challenges and opportunities for meaningful integration into circuit design workflows.

Abstract: Recent breakthroughs in Large Language Models (LLMs) and Large Circuit Models
(LCMs) have sparked excitement across the electronic design automation (EDA)
community, promising a revolution in circuit design and optimization. Yet, this
excitement is met with significant skepticism: Are these AI models a genuine
revolution in circuit design, or a temporary wave of inflated expectations?
This paper serves as a foundational text for the corresponding ICCAD 2025
panel, bringing together perspectives from leading experts in academia and
industry. It critically examines the practical capabilities, fundamental
limitations, and future prospects of large AI models in hardware design. The
paper synthesizes the core arguments surrounding reliability, scalability, and
interpretability, framing the debate on whether these models can meaningfully
outperform or complement traditional EDA methods. The result is an
authoritative overview offering fresh insights into one of today's most
contentious and impactful technology trends.

</details>


### [196] [Scaling Law for Large-Scale Pre-Training Using Chaotic Time Series and Predictability in Financial Time Series](https://arxiv.org/abs/2509.04921)
*Yuki Takemoto*

Main category: cs.LG

TL;DR: The paper proposes a methodology for financial time series modeling using synthetic chaotic data, resampling techniques, and large-scale pre-training, yielding improvements in prediction and profitability evaluation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of predicting financial returns, leveraging the observation that real-world time series exhibit chaotic properties.

Method: The approach involves generating artificial chaotic time series, using resampling techniques, conducting large-scale pre-training with 10 billion samples, and performing zero-shot prediction on Bitcoin trade data.

Result: The results show significant improvement in prediction accuracy and profitability over traditional autocorrelation models with insights into scaling laws during pre-training.

Conclusion: The study highlights potential advancements in predictive capabilities by scaling computational resources and opens avenues for verifying the scaling law across various chaotic models.

Abstract: Time series forecasting plays a critical role in decision-making processes
across diverse fields including meteorology, traffic, electricity, economics,
finance, and so on. Especially, predicting returns on financial instruments is
a challenging problem. Some researchers have proposed time series foundation
models applicable to various forecasting tasks. Simultaneously, based on the
recognition that real-world time series exhibit chaotic properties, methods
have been developed to artificially generate synthetic chaotic time series,
construct diverse datasets and train models. In this study, we propose a
methodology for modeling financial time series by generating artificial chaotic
time series and applying resampling techniques to simulate financial time
series data, which we then use as training samples. Increasing the resampling
interval to extend predictive horizons, we conducted large-scale pre-training
using 10 billion training samples for each case. We subsequently created test
datasets for multiple timeframes using actual Bitcoin trade data and performed
zero-shot prediction without re-training the pre-trained model. The results of
evaluating the profitability of a simple trading strategy based on these
predictions demonstrated significant performance improvements over
autocorrelation models. During the large-scale pre-training process, we
observed a scaling law-like phenomenon that we can achieve predictive
performance at a certain level with extended predictive horizons for chaotic
time series by increasing the number of training samples exponentially. If this
scaling law proves robust and holds true across various chaotic models, it
suggests the potential to predict near-future events by investing substantial
computational resources. Future research should focus on further large-scale
training and verifying the applicability of this scaling law to diverse chaotic
models.

</details>


### [197] [A transformer-BiGRU-based framework with data augmentation and confident learning for network intrusion detection](https://arxiv.org/abs/2509.04925)
*Jiale Zhang,Pengfei He,Fei Li,Kewei Li,Yan Wang,Lan Huang,Ruochi Zhang,Fengfeng Zhou*

Main category: cs.LG

TL;DR: This paper introduces TrailGate, a framework combining machine learning and deep learning techniques for robust and precise network intrusion detection, addressing challenges of data scarcity and class imbalance.


<details>
  <summary>Details</summary>
Motivation: The growing challenges in network traffic data analysis, including complex patterns, data scarcity, and class imbalance, necessitate advanced solutions for effective intrusion detection.

Method: TrailGate integrates Transformer and Bidirectional Gated Recurrent Unit (BiGRU) architectures, employs advanced feature selection strategies, and utilizes data augmentation techniques to enhance detection capabilities.

Result: TrailGate demonstrates proficiency in identifying common attack types and excels in swiftly detecting and neutralizing emerging threats.

Conclusion: The integration of machine learning and deep learning within TrailGate positions it as an effective solution for contemporary network intrusion detection challenges.

Abstract: In today's fast-paced digital communication, the surge in network traffic
data and frequency demands robust and precise network intrusion solutions.
Conventional machine learning methods struggle to grapple with complex patterns
within the vast network intrusion datasets, which suffer from data scarcity and
class imbalance. As a result, we have integrated machine learning and deep
learning techniques within the network intrusion detection system to bridge
this gap. This study has developed TrailGate, a novel framework that combines
machine learning and deep learning techniques. By integrating Transformer and
Bidirectional Gated Recurrent Unit (BiGRU) architectures with advanced feature
selection strategies and supplemented by data augmentation techniques,
TrailGate can identifies common attack types and excels at detecting and
mitigating emerging threats. This algorithmic fusion excels at detecting common
and well-understood attack types and has the unique ability to swiftly identify
and neutralize emerging threats that stem from existing paradigms.

</details>


### [198] [Ontology-Aligned Embeddings for Data-Driven Labour Market Analytics](https://arxiv.org/abs/2509.04942)
*Heinke Hihn,Dennis A. V. Dittrich,Carl Jeske,Cayo Costa Sobral,Helio Pais,Timm Lochmann*

Main category: cs.LG

TL;DR: The paper develops a scalable alternative for aligning diverse occupational vocabularies using fine-tuned Sentence-BERT embeddings, which bypasses the need for hand-crafted ontologies.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of reasoning across occupational data from various sources, previously requiring hand-crafted but hard-to-maintain ontologies.

Method: A Sentence-BERT model is fine-tuned on data from the German Federal Employment Agency to map job titles to embeddings linked to existing ontologies, enabling semantic searches.

Result: The alignment process successfully links free-form German job titles to two established ontologies: Klassifikation der Berufe and International Standard Classification of Education.

Conclusion: This embedding-based alignment provides scalable and flexible classification processes, facilitating integration of new classes, multilingual titles, and additional ontologies for broader applicability.

Abstract: The limited ability to reason across occupational data from different sources
is a long-standing bottleneck for data-driven labour market analytics. Previous
research has relied on hand-crafted ontologies that allow such reasoning but
are computationally expensive and require careful maintenance by human experts.
The rise of language processing machine learning models offers a scalable
alternative by learning shared semantic spaces that bridge diverse occupational
vocabularies without extensive human curation. We present an embedding-based
alignment process that links any free-form German job title to two established
ontologies - the German Klassifikation der Berufe and the International
Standard Classification of Education. Using publicly available data from the
German Federal Employment Agency, we construct a dataset to fine-tune a
Sentence-BERT model to learn the structure imposed by the ontologies. The
enriched pairs (job title, embedding) define a similarity graph structure that
we can use for efficient approximate nearest-neighbour search, allowing us to
frame the classification process as a semantic search problem. This allows for
greater flexibility, e.g., adding more classes. We discuss design decisions,
open challenges, and outline ongoing work on extending the graph with other
ontologies and multilingual titles.

</details>


### [199] [Detecting Blinks in Healthy and Parkinson's EEG: A Deep Learning Perspective](https://arxiv.org/abs/2509.04951)
*Artem Lensky,Yiding Qiu*

Main category: cs.LG

TL;DR: The study explores deep learning models for accurately detecting involuntary blinks in EEG signals using minimal pre-processed data and fewer electrodes, finding CNN-RNN hybrid models most effective.


<details>
  <summary>Details</summary>
Motivation: Blink rate and its variability serve as key physiological markers for assessing cognitive load, attention, and potential neurological disorders like Parkinson’s disease.

Method: Various deep learning architectures were tested, including CNNs, RNNs, TCNs, transformer-based, and hybrid CNN-RNN models. The task was sequence-to-sequence blink segmentation from frontal EEG signals, with data from 31 subjects.

Result: The CNN-RNN hybrid model achieved the highest blink detection accuracy: 93.8% to 95.8% for healthy participants, and 73.8% to 75.8% for Parkinson's disease patients across different electrode configurations.

Conclusion: Using fewer EEG electrodes, deep learning methods—especially hybrid architectures—can robustly segment blinks and enhance physiological monitoring.

Abstract: Blinks in electroencephalography (EEG) are often treated as unwanted
artifacts. However, recent studies have demonstrated that blink rate and its
variability are important physiological markers to monitor cognitive load,
attention, and potential neurological disorders. This paper addresses the
critical task of accurate blink detection by evaluating various deep learning
models for segmenting EEG signals into involuntary blinks and non-blinks. We
present a pipeline for blink detection using 1, 3, or 5 frontal EEG electrodes.
The problem is formulated as a sequence-to-sequence task and tested on various
deep learning architectures including standard recurrent neural networks,
convolutional neural networks (both standard and depth-wise), temporal
convolutional networks (TCN), transformer-based models, and hybrid
architectures. The models were trained on raw EEG signals with minimal
pre-processing. Training and testing was carried out on a public dataset of 31
subjects collected at UCSD. This dataset consisted of 15 healthy participants
and 16 patients with Parkinson's disease allowing us to verify the model's
robustness to tremor. Out of all models, CNN-RNN hybrid model consistently
outperformed other models and achieved the best blink detection accuracy of
93.8%, 95.4% and 95.8% with 1, 3, and 5 channels in the healthy cohort and
correspondingly 73.8%, 75.4% and 75.8% in patients with PD. The paper compares
neural networks for the task of segmenting EEG recordings to involuntary blinks
and no blinks allowing for computing blink rate and other statistics.

</details>


### [200] [On the Normalization of Confusion Matrices: Methods and Geometric Interpretations](https://arxiv.org/abs/2509.04959)
*Johan Erbani,Pierre-Edouard Portier,Elod Egyed-Zsigmond,Sonia Ben Mokhtar,Diana Nurbakova*

Main category: cs.LG

TL;DR: The paper presents bistochastic normalization using Iterative Proportional Fitting to disentangle class similarity and distribution bias in confusion matrices for better classifier evaluation.


<details>
  <summary>Details</summary>
Motivation: Classifiers' performance evaluation using confusion matrices is hindered because the observed errors combine both class similarity and distribution bias, making it difficult to separately understand these factors.

Method: The authors propose bistochastic normalization applied via Iterative Proportional Fitting, which isolates class similarity in confusion matrices to remove the influence of distribution bias.

Result: The approach successfully disentangles error factors, enabling improved diagnosis of classifier behavior, and establishes a geometrical interpretation linking normalization and internal class representations.

Conclusion: Bistochastic normalization enhances clarity in confusion matrix analysis, aiding in targeted classifier refinements and improving understanding of model performance.

Abstract: The confusion matrix is a standard tool for evaluating classifiers by
providing insights into class-level errors. In heterogeneous settings, its
values are shaped by two main factors: class similarity -- how easily the model
confuses two classes -- and distribution bias, arising from skewed
distributions in the training and test sets. However, confusion matrix values
reflect a mix of both factors, making it difficult to disentangle their
individual contributions. To address this, we introduce bistochastic
normalization using Iterative Proportional Fitting, a generalization of row and
column normalization. Unlike standard normalizations, this method recovers the
underlying structure of class similarity. By disentangling error sources, it
enables more accurate diagnosis of model behavior and supports more targeted
improvements. We also show a correspondence between confusion matrix
normalizations and the model's internal class representations. Both standard
and bistochastic normalizations can be interpreted geometrically in this space,
offering a deeper understanding of what normalization reveals about a
classifier.

</details>


### [201] [Neuro-Spectral Architectures for Causal Physics-Informed Networks](https://arxiv.org/abs/2509.04966)
*Arthur Bizzi,Leonardo M. Moreira,Márcio Marques,Leonardo Mendonça,Christian Júnior de Oliveira,Vitor Balestro,Lucas dos Santos Fernandez,Daniel Yukimura,Pavel Petrov,João M. Pereira,Tiago Novello,Lucas Nissenbaum*

Main category: cs.LG

TL;DR: NeuSA introduces a novel PINN architecture addressing spectral bias and causality issues in solving PDEs, achieving improved accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Standard MLP-based PINNs struggle with convergence, causality violations, and spectral bias when solving complex initial-value PDE problems.

Method: NeuSA leverages spectral methods for PDE projection, integrates with Neural ODEs for causal structure, and uses classical-method-based initialization for better training.

Result: NeuSA outperforms existing architectures on benchmarks for linear and nonlinear wave equations, showing quicker convergence and better predictive accuracy.

Conclusion: NeuSA advances the field of PINNs by effectively addressing key limitations, showcasing strong results in solving complex PDEs with enhanced reliability and precision.

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful neural
framework for solving partial differential equations (PDEs). However, standard
MLP-based PINNs often fail to converge when dealing with complex initial-value
problems, leading to solutions that violate causality and suffer from a
spectral bias towards low-frequency components. To address these issues, we
introduce NeuSA (Neuro-Spectral Architectures), a novel class of PINNs inspired
by classical spectral methods, designed to solve linear and nonlinear PDEs with
variable coefficients. NeuSA learns a projection of the underlying PDE onto a
spectral basis, leading to a finite-dimensional representation of the dynamics
which is then integrated with an adapted Neural ODE (NODE). This allows us to
overcome spectral bias, by leveraging the high-frequency components enabled by
the spectral representation; to enforce causality, by inheriting the causal
structure of NODEs, and to start training near the target solution, by means of
an initialization scheme based on classical methods. We validate NeuSA on
canonical benchmarks for linear and nonlinear wave equations, demonstrating
strong performance as compared to other architectures, with faster convergence,
improved temporal consistency and superior predictive accuracy. Code and
pretrained models will be released.

</details>


### [202] [Topology-Aware Graph Reinforcement Learning for Dynamic Routing in Cloud Networks](https://arxiv.org/abs/2509.04973)
*Yuxi Wang,Heyao Liu,Guanzi Yao,Nyutian Long,Yue Kang*

Main category: cs.LG

TL;DR: This paper introduces a topology-aware graph reinforcement learning method to enhance routing policy optimization in dynamic cloud server environments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve issues of decision instability and insufficient structural awareness in dynamic topologies for routing policy optimization.

Method: The method combines a Structure-Aware State Encoding (SASE) module for high-order dependency modeling using graph convolutions and embeddings, and a Policy-Adaptive Graph Update (PAGU) module for adaptive structure updates.

Result: The proposed method shows superior performance compared to baseline models in terms of throughput, latency control, and link balance on the GEANT dataset.

Conclusion: The approach effectively improves routing efficiency and robustness in dynamic and complex cloud network scenarios.

Abstract: This paper proposes a topology-aware graph reinforcement learning approach to
address the routing policy optimization problem in cloud server environments.
The method builds a unified framework for state representation and structural
evolution by integrating a Structure-Aware State Encoding (SASE) module and a
Policy-Adaptive Graph Update (PAGU) mechanism. It aims to tackle the challenges
of decision instability and insufficient structural awareness under dynamic
topologies. The SASE module models node states through multi-layer graph
convolution and structural positional embeddings, capturing high-order
dependencies in the communication topology and enhancing the expressiveness of
state representations. The PAGU module adjusts the graph structure based on
policy behavior shifts and reward feedback, enabling adaptive structural
updates in dynamic environments. Experiments are conducted on the real-world
GEANT topology dataset, where the model is systematically evaluated against
several representative baselines in terms of throughput, latency control, and
link balance. Additional experiments, including hyperparameter sensitivity,
graph sparsity perturbation, and node feature dimensionality variation, further
explore the impact of structure modeling and graph updates on model stability
and decision quality. Results show that the proposed method outperforms
existing graph reinforcement learning models across multiple performance
metrics, achieving efficient and robust routing in dynamic and complex cloud
networks.

</details>


### [203] [Adapt in the Wild: Test-Time Entropy Minimization with Sharpness and Feature Regularization](https://arxiv.org/abs/2509.04977)
*Shuaicheng Niu,Guohao Chen,Deyu Chen,Yifan Zhang,Jiaxiang Wu,Zhiquan Wen,Yaofo Chen,Peilin Zhao,Chunyan Miao,Mingkui Tan*

Main category: cs.LG

TL;DR: The paper introduces SAR and SAR^2 methods to improve test-time adaptation stability by addressing issues with batch norm layers, noisy samples, and representation collapse, achieving promising results under challenging scenarios.


<details>
  <summary>Details</summary>
Motivation: Test-time adaptation struggles with mixed distribution shifts, small batch sizes, and imbalanced label distributions, often due to instability caused by batch norm layers.

Method: SAR minimizes sharpness and removes noisy samples with large gradients, while SAR^2 adds regularizers to avoid representation collapse by reducing feature correlations and penalizing classification biases.

Result: SAR and SAR^2 outperform prior TTA methods, demonstrating better stability and efficiency in challenging test scenarios.

Conclusion: Replacing batch norm layers with batch-agnostic norms and introducing methods like SAR^2 can stabilize TTA performance, enabling practical deployment in real-world scenarios.

Abstract: Test-time adaptation (TTA) may fail to improve or even harm the model
performance when test data have: 1) mixed distribution shifts, 2) small batch
sizes, 3) online imbalanced label distribution shifts. This is often a key
obstacle preventing existing TTA methods from being deployed in the real world.
In this paper, we investigate the unstable reasons and find that the batch norm
layer is a crucial factor hindering TTA stability. Conversely, TTA can perform
more stably with batch-agnostic norm layers, i.e., group or layer norm.
However, we observe that TTA with group and layer norms does not always succeed
and still suffers many failure cases, i.e., the model collapses into trivial
solutions by assigning the same class label for all samples. By digging into
this, we find that, during the collapse process: 1) the model gradients often
undergo an initial explosion followed by rapid degradation, suggesting that
certain noisy test samples with large gradients may disrupt adaptation; and 2)
the model representations tend to exhibit high correlations and classification
bias. To address this, we first propose a sharpness-aware and reliable entropy
minimization method, called SAR, for stabilizing TTA from two aspects: 1)
remove partial noisy samples with large gradients, 2) encourage model weights
to go to a flat minimum so that the model is robust to the remaining noisy
samples. Based on SAR, we further introduce SAR^2 to prevent representation
collapse with two regularizers: 1) a redundancy regularizer to reduce
inter-dimensional correlations among centroid-invariant features; and 2) an
inequity regularizer to maximize the prediction entropy of a prototype
centroid, thereby penalizing biased representations toward any specific class.
Promising results demonstrate that our methods perform more stably over prior
methods and are computationally efficient under the above wild test scenarios.

</details>


### [204] [Directed Evolution of Proteins via Bayesian Optimization in Embedding Space](https://arxiv.org/abs/2509.04998)
*Matouš Soldát,Jiří Kléma*

Main category: cs.LG

TL;DR: This paper introduces a novel machine-learning-assisted method for directed evolution of proteins that outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Protein engineering through directed evolution is costly and time-intensive, motivating the development of more efficient methods using machine learning to identify promising variants.

Method: The paper uses a novel combination of Bayesian optimization and protein variant representations extracted from a pre-trained protein language model for machine-learning-assisted directed evolution.

Result: The representation-based Bayesian optimization method demonstrated better performance compared to state-of-the-art approaches, achieving improvements with the same number of biochemical screenings.

Conclusion: The proposed machine-learning-assisted approach significantly enhances the efficiency of protein engineering by improving the quality of predictions and reducing screening resources.

Abstract: Directed evolution is an iterative laboratory process of designing proteins
with improved function by iteratively synthesizing new protein variants and
evaluating their desired property with expensive and time-consuming biochemical
screening. Machine learning methods can help select informative or promising
variants for screening to increase their quality and reduce the amount of
necessary screening. In this paper, we present a novel method for
machine-learning-assisted directed evolution of proteins which combines
Bayesian optimization with informative representation of protein variants
extracted from a pre-trained protein language model. We demonstrate that the
new representation based on the sequence embeddings significantly improves the
performance of Bayesian optimization yielding better results with the same
number of conducted screening in total. At the same time, our method
outperforms the state-of-the-art machine-learning-assisted directed evolution
methods with regression objective.

</details>


### [205] [Depth-Aware Initialization for Stable and Efficient Neural Network Training](https://arxiv.org/abs/2509.05018)
*Vijay Pandey*

Main category: cs.LG

TL;DR: The paper proposes a novel initialization scheme for deep networks that incorporates layer-specific depth information to improve performance, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the shortcomings of existing initialization methods in deep networks, particularly the inability to maintain unit variance theoretically across all layers.

Method: The study introduces an initialization approach that adjusts the variance flexibly based on the depth of individual layers and the total depth of the network.

Result: The proposed initialization method is shown to outperform existing schemes in experimental evaluations.

Conclusion: The paper highlights the importance of accounting for depth-specific adjustments in deep networks and establishes the method as a better alternative for network initialization.

Abstract: In past few years, various initialization schemes have been proposed. These
schemes are glorot initialization, He initialization, initialization using
orthogonal matrix, random walk method for initialization. Some of these methods
stress on keeping unit variance of activation and gradient propagation through
the network layer. Few of these methods are independent of the depth
information while some methods has considered the total network depth for
better initialization. In this paper, comprehensive study has been done where
depth information of each layer as well as total network is incorporated for
better initialization scheme. It has also been studied that for deeper networks
theoretical assumption of unit variance throughout the network does not perform
well. It requires the need to increase the variance of the network from first
layer activation to last layer activation. We proposed a novel way to increase
the variance of the network in flexible manner, which incorporates the
information of each layer depth. Experiments shows that proposed method
performs better than the existing initialization scheme.

</details>


### [206] [MultiSurv: A Multimodal Deep Survival Framework for Prostrate and Bladder Cancer](https://arxiv.org/abs/2509.05037)
*Noorul Wahab,Ethar Alzaid,Jiaqi Lv,Adam Shephard,Shan E Ahmed Raza*

Main category: cs.LG

TL;DR: The paper presents MultiSurv, a multimodal deep survival model, to predict time-to-event outcomes in oncology using patient data like clinical, MRI, RNA-seq, and pathology features.


<details>
  <summary>Details</summary>
Motivation: To improve accuracy in predicting time-to-biochemical recurrence for prostate cancer and time-to-cancer recurrence for bladder cancer by integrating complementary prognostic signals from multiple data modalities.

Method: The MultiSurv framework utilizes DeepHit with a projection layer and inter-modality cross-attention for multimodal integration and survival prediction.

Result: The model achieved a C-index of 0.843 for prostate cancer and 0.662 for bladder cancer in cross-validation, indicating robust performance and adaptability, although with variation on external validation sets.

Conclusion: Multimodal integration with deep survival learning shows promise for personalised risk stratification in oncology and has broad applicability for tasks involving heterogeneous biomedical data.

Abstract: Accurate prediction of time-to-event outcomes is a central challenge in
oncology, with significant implications for treatment planning and patient
management. In this work, we present MultiSurv, a multimodal deep survival
model utilising DeepHit with a projection layer and inter-modality
cross-attention, which integrates heterogeneous patient data, including
clinical, MRI, RNA-seq and whole-slide pathology features. The model is
designed to capture complementary prognostic signals across modalities and
estimate individualised time-to-biochemical recurrence in prostate cancer and
time-to-cancer recurrence in bladder cancer. Our approach was evaluated in the
context of the CHIMERA Grand Challenge, across two of the three provided tasks.
For Task 1 (prostate cancer bio-chemical recurrence prediction), the proposed
framework achieved a concordance index (C-index) of 0.843 on 5-folds
cross-validation and 0.818 on CHIMERA development set, demonstrating robust
discriminatory ability. For Task 3 (bladder cancer recurrence prediction), the
model obtained a C-index of 0.662 on 5-folds cross-validation and 0.457 on
development set, highlighting its adaptability and potential for clinical
translation. These results suggest that leveraging multimodal integration with
deep survival learning provides a promising pathway toward personalised risk
stratification in prostate and bladder cancer. Beyond the challenge setting,
our framework is broadly applicable to survival prediction tasks involving
heterogeneous biomedical data.

</details>


### [207] [Recurrent State Encoders for Efficient Neural Combinatorial Optimization](https://arxiv.org/abs/2509.05084)
*Tim Dernedde,Daniela Thyssens,Lars Schmidt-Thieme*

Main category: cs.LG

TL;DR: This paper introduces a recurrent encoder in Neural Combinatorial Optimization (NCO) to reuse computations from previous steps, reducing model complexity and latency while maintaining or improving performance.


<details>
  <summary>Details</summary>
Motivation: To address inefficiency in Neural Combinatorial Optimization methods, which often fail to reuse computations between steps despite minimal changes to the state.

Method: Proposes using a recurrent encoder that computes state embeddings based on the current state and the embeddings from prior steps, reducing the need for layers and computation.

Result: Demonstrated that the recurrent encoder achieves competitive or superior performance compared to a non-recurrent encoder with $3\times$ fewer layers, reducing latency.

Conclusion: The proposed recurrent encoder improves the practical efficiency of NCO methods and shows applicability in tasks like TSP, CVRP, and OP, further proving its integration potential into practical optimization algorithms.

Abstract: The primary paradigm in Neural Combinatorial Optimization (NCO) are
construction methods, where a neural network is trained to sequentially add one
solution component at a time until a complete solution is constructed. We
observe that the typical changes to the state between two steps are small,
since usually only the node that gets added to the solution is removed from the
state. An efficient model should be able to reuse computation done in prior
steps. To that end, we propose to train a recurrent encoder that computes the
state embeddings not only based on the state but also the embeddings of the
step before. We show that the recurrent encoder can achieve equivalent or
better performance than a non-recurrent encoder even if it consists of
$3\times$ fewer layers, thus significantly improving on latency. We demonstrate
our findings on three different problems: the Traveling Salesman Problem (TSP),
the Capacitated Vehicle Routing Problem (CVRP), and the Orienteering Problem
(OP) and integrate the models into a large neighborhood search algorithm, to
showcase the practical relevance of our findings.

</details>


### [208] [HyPINO: Multi-Physics Neural Operators via HyperPINNs and the Method of Manufactured Solutions](https://arxiv.org/abs/2509.05117)
*Rafael Bischof,Michal Piovarči,Michael A. Kraus,Siddhartha Mishra,Bernd Bickel*

Main category: cs.LG

TL;DR: HyPINO is a multi-physics neural operator capable of zero-shot generalization for a range of parametric PDEs without fine-tuning, leveraging a hypernetwork and mixed supervision.


<details>
  <summary>Details</summary>
Motivation: To address the need for a scalable and accurate method to solve complex parametric PDEs across various conditions without task-specific optimization.

Method: The approach combines a Swin Transformer-based hypernetwork with mixed supervision using labeled analytical solutions and unlabeled physics-informed objectives. It introduces an iterative refinement process to enhance accuracy.

Result: HyPINO demonstrated superior zero-shot accuracy on seven benchmarks, iterative refinement reducing errors up to 100x in the best case, and improved fine-tuning behavior over competing methods.

Conclusion: HyPINO provides a scalable foundation for solving complex PDEs with improved accuracy and efficiency, paving the way for broader applications in computational physics.

Abstract: We present HyPINO, a multi-physics neural operator designed for zero-shot
generalization across a broad class of parametric PDEs without requiring
task-specific fine-tuning. Our approach combines a Swin Transformer-based
hypernetwork with mixed supervision: (i) labeled data from analytical solutions
generated via the Method of Manufactured Solutions (MMS), and (ii) unlabeled
samples optimized using physics-informed objectives. The model maps PDE
parametrizations to target Physics-Informed Neural Networks (PINNs) and can
handle linear elliptic, hyperbolic, and parabolic equations in two dimensions
with varying source terms, geometries, and mixed Dirichlet/Neumann boundary
conditions, including interior boundaries. HyPINO achieves strong zero-shot
accuracy on seven benchmark problems from PINN literature, outperforming
U-Nets, Poseidon, and Physics-Informed Neural Operators (PINO). Further, we
introduce an iterative refinement procedure that compares the physics of the
generated PINN to the requested PDE and uses the discrepancy to generate a
"delta" PINN. Summing their contributions and repeating this process forms an
ensemble whose combined solution progressively reduces the error on six
benchmarks and achieves over 100x gain in average $L_2$ loss in the best case,
while retaining forward-only inference. Additionally, we evaluate the
fine-tuning behavior of PINNs initialized by HyPINO and show that they converge
faster and to lower final error than both randomly initialized and
Reptile-meta-learned PINNs on five benchmarks, performing on par on the
remaining two. Our results highlight the potential of this scalable approach as
a foundation for extending neural operators toward solving increasingly
complex, nonlinear, and high-dimensional PDE problems with significantly
improved accuracy and reduced computational cost.

</details>


### [209] [Should We Always Train Models on Fine-Grained Classes?](https://arxiv.org/abs/2509.05130)
*Davide Pirovano,Federico Milanesio,Michele Caselle,Piero Fariselli,Matteo Osella*

Main category: cs.LG

TL;DR: This paper investigates the effectiveness of using fine-grained labels in classification tasks, analyzing when they improve performance.


<details>
  <summary>Details</summary>
Motivation: To examine why training on fine-grained labels may enhance classification performance and to understand the broad applicability of this observation.

Method: The study utilizes both real and synthetic datasets to analyze the impact of label granularity on classification accuracy, considering data and model factors such as geometric structure, dataset size, and model capacity.

Result: Fine-grained training does not consistently improve accuracy; its efficacy depends on the data's geometric structure, label hierarchy, dataset size, and model capacity.

Conclusion: Fine-grained labels can provide performance benefits, but their effectiveness is context-dependent and influenced by multiple factors.

Abstract: In classification problems, models must predict a class label based on the
input data features. However, class labels are organized hierarchically in many
datasets. While a classification task is often defined at a specific level of
this hierarchy, training can utilize a finer granularity of labels. Empirical
evidence suggests that such fine-grained training can enhance performance. In
this work, we investigate the generality of this observation and explore its
underlying causes using both real and synthetic datasets. We show that training
on fine-grained labels does not universally improve classification accuracy.
Instead, the effectiveness of this strategy depends critically on the geometric
structure of the data and its relations with the label hierarchy. Additionally,
factors such as dataset size and model capacity significantly influence whether
fine-grained labels provide a performance benefit.

</details>


### [210] [On the Learnability of Distribution Classes with Adaptive Adversaries](https://arxiv.org/abs/2509.05137)
*Tosca Lechner,Alex Bie,Gautam Kamath*

Main category: cs.LG

TL;DR: The paper studies the learnability of distribution classes when faced with adaptive adversaries who can manipulate samples before the learner receives them.


<details>
  <summary>Details</summary>
Motivation: To understand the challenges and implications of learning when adversaries can adaptively manipulate sample data.

Method: The authors define a general framework for learnability under adaptive adversaries, considering the adversary's budget for manipulations, and compare it with oblivious adversaries.

Result: Adaptive adversaries pose a greater challenge, and learnability under them is strictly stronger than under oblivious adversaries.

Conclusion: Learnability in the presence of adaptive adversaries requires addressing significantly stronger conditions compared to oblivious ones.

Abstract: We consider the question of learnability of distribution classes in the
presence of adaptive adversaries -- that is, adversaries capable of
intercepting the samples requested by a learner and applying manipulations with
full knowledge of the samples before passing it on to the learner. This stands
in contrast to oblivious adversaries, who can only modify the underlying
distribution the samples come from but not their i.i.d.\ nature. We formulate a
general notion of learnability with respect to adaptive adversaries, taking
into account the budget of the adversary. We show that learnability with
respect to additive adaptive adversaries is a strictly stronger condition than
learnability with respect to additive oblivious adversaries.

</details>


### [211] [Foundational Models and Federated Learning: Survey, Taxonomy, Challenges and Practical Insights](https://arxiv.org/abs/2509.05142)
*Cosmin-Andrei Hatfaludi,Alex Serban*

Main category: cs.LG

TL;DR: This paper surveys the intersection of federated learning and foundational models, focusing on healthcare as a key domain. It proposes a taxonomy and analyzes methods for integrating the two paradigms, reviewing over 4,200 articles to summarize existing approaches.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the absence of a unified survey that categorizes and analyzes technical methods combining federated learning and foundational models, critical for unlocking data and resource silos across domains like healthcare.

Method: Constructing a novel taxonomy to organize methods across the development life cycle of federated learning and foundational models. Reviewing over 4,200 articles, narrowing them down to 250, and featuring 42 unique methods for comparative analysis.

Result: A structured taxonomy, detailed comparison based on complexity, efficiency, and scalability, and insights into practical implementation in healthcare were developed as outcomes of this study.

Conclusion: This survey summarizes the state of the field, offering guidelines and insights for the practical adoption and evolution of federated learning paired with foundational models, particularly in healthcare scenarios.

Abstract: Federated learning has the potential to unlock siloed data and distributed
resources by enabling collaborative model training without sharing private
data. As more complex foundational models gain widespread use, the need to
expand training resources and integrate privately owned data grows as well. In
this article, we explore the intersection of federated learning and
foundational models, aiming to identify, categorize, and characterize technical
methods that integrate the two paradigms. As a unified survey is currently
unavailable, we present a literature survey structured around a novel taxonomy
that follows the development life-cycle stages, along with a technical
comparison of available methods. Additionally, we provide practical insights
and guidelines for implementing and evolving these methods, with a specific
focus on the healthcare domain as a case study, where the potential impact of
federated learning and foundational models is considered significant. Our
survey covers multiple intersecting topics, including but not limited to
federated learning, self-supervised learning, fine-tuning, distillation, and
transfer learning. Initially, we retrieved and reviewed a set of over 4,200
articles. This collection was narrowed to more than 250 thoroughly reviewed
articles through inclusion criteria, featuring 42 unique methods. The methods
were used to construct the taxonomy and enabled their comparison based on
complexity, efficiency, and scalability. We present these results as a
self-contained overview that not only summarizes the state of the field but
also provides insights into the practical aspects of adopting, evolving, and
integrating foundational models with federated learning.

</details>


### [212] [KVCompose: Efficient Structured KV Cache Compression with Composite Tokens](https://arxiv.org/abs/2509.05165)
*Dmitry Akulov,Mohamed Sana,Antonio De Domenico,Tareq Si Salem,Nicola Piovesan,Fadhel Ayed*

Main category: cs.LG

TL;DR: This paper addresses the challenge of memory bottlenecks in LLMs during long-context inference by introducing an efficient KV cache compression framework using attention-guided, layer-specific strategies.


<details>
  <summary>Details</summary>
Motivation: The paper aims to solve the issue of KV cache memory consumption in LLMs, which worsens with increasing context length and model depth, restricting their scalability and efficient deployment.

Method: The proposed method uses attention-guided token importance estimation, independent head-specific token selection, composite token alignment, and global layer-adaptive allocation to compress KV caches while maintaining inference compatibility.

Result: The method achieves significant memory reduction without sacrificing accuracy, outperforming prior compression techniques in consistency and practical application.

Conclusion: The approach offers a scalable solution for long-context inference in LLMs by reducing memory overhead while being fully compatible with existing inference systems.

Abstract: Large language models (LLMs) rely on key-value (KV) caches for efficient
autoregressive decoding; however, cache size grows linearly with context length
and model depth, becoming a major bottleneck in long-context inference. Prior
KV cache compression methods either enforce rigid heuristics, disrupt tensor
layouts with per-attention-head variability, or require specialized compute
kernels.
  We propose a simple, yet effective, KV cache compression framework based on
attention-guided, layer-adaptive composite tokens. Our method aggregates
attention scores to estimate token importance, selects head-specific tokens
independently, and aligns them into composite tokens that respect the uniform
cache structure required by existing inference engines. A global allocation
mechanism further adapts retention budgets across layers, assigning more
capacity to layers with informative tokens. This approach achieves significant
memory reduction while preserving accuracy, consistently outperforming prior
structured and semi-structured methods. Crucially, our approach remains fully
compatible with standard inference pipelines, offering a practical and scalable
solution for efficient long-context LLM deployment.

</details>


### [213] [Accuracy-Constrained CNN Pruning for Efficient and Reliable EEG-Based Seizure Detection](https://arxiv.org/abs/2509.05190)
*Mounvik K,N Harshit*

Main category: cs.LG

TL;DR: The paper introduces a lightweight 1D CNN model with structured pruning that reduces size and resources while maintaining or even improving predictive performance for EEG-based seizure detection.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by the large size and computational requirements of deep learning models, especially CNNs, for real-time EEG-based seizure detection in resource-constrained environments.

Method: The study designed a 1D CNN and applied structured pruning to the convolutional kernels, removing 50% of them based on their importance. Additionally, mild early stopping during training was used to prevent overfitting.

Result: After structured pruning, the model reduced weights and memory usage by half, while maintaining accuracy (92.78% vs. 92.87%) and slightly improving the macro-F1 score (from 0.8686 to 0.8707).

Conclusion: Structured pruning effectively reduces redundancy and enhances generalization, making it a viable method for improving model efficiency and reliability in resource-constrained seizure detection applications.

Abstract: Deep learning models, especially convolutional neural networks (CNNs), have
shown considerable promise for biomedical signals such as EEG-based seizure
detection. However, these models come with challenges, primarily due to their
size and compute requirements in environments where real-time detection or
limited resources are available. In this study, we present a lightweight
one-dimensional CNN model with structured pruning to improve efficiency and
reliability. The model was trained with mild early stopping to address possible
overfitting, achieving an accuracy of 92.78% and a macro-F1 score of 0.8686.
Structured pruning of the baseline CNN involved removing 50% of the
convolutional kernels based on their importance to model predictions.
Surprisingly, after pruning the weights and memory by 50%, the new network was
still able to maintain predictive capabilities, while modestly increasing
precision to 92.87% and improving the macro-F1 score to 0.8707. Overall, we
present a convincing case that structured pruning removes redundancy, improves
generalization, and, in combination with mild early stopping, achieves a
promising way forward to improve seizure detection efficiency and reliability,
which is clear motivation for resource-limited settings.

</details>


### [214] [Shift Before You Learn: Enabling Low-Rank Representations in Reinforcement Learning](https://arxiv.org/abs/2509.05193)
*Bastien Dubail,Stefan Stojanovic,Alexandre Proutière*

Main category: cs.LG

TL;DR: The paper challenges the common assumption of low-rank structure in successor measures within reinforcement learning and introduces the concept of a shifted successor measure to achieve low-rank representation, backed by theoretical analysis and practical validation.


<details>
  <summary>Details</summary>
Motivation: The authors sought to address the mismatch between the assumption of low-rank structure in successor measures and the reality, aiming to refine theoretical underpinnings for improved RL performance.

Method: The authors propose and analyze the shifted successor measure, leveraging theoretical tools like Type II Poincaré inequalities and entry-wise estimation to study low-rank approximation and spectral recoverability.

Result: The study proves that low-rank representations emerge naturally in shifted successor measures and provides bounds for approximation and estimation errors based on spectral recoverability. Experimental results show enhanced performance in goal-conditioned RL.

Conclusion: Shifting successor measures offers a promising way to capture low-rank dynamics and improve RL algorithms, connecting theoretical insights with practical benefits in goal-conditioned reinforcement learning.

Abstract: Low-rank structure is a common implicit assumption in many modern
reinforcement learning (RL) algorithms. For instance, reward-free and
goal-conditioned RL methods often presume that the successor measure admits a
low-rank representation. In this work, we challenge this assumption by first
remarking that the successor measure itself is not low-rank. Instead, we
demonstrate that a low-rank structure naturally emerges in the shifted
successor measure, which captures the system dynamics after bypassing a few
initial transitions. We provide finite-sample performance guarantees for the
entry-wise estimation of a low-rank approximation of the shifted successor
measure from sampled entries. Our analysis reveals that both the approximation
and estimation errors are primarily governed by the so-called spectral
recoverability of the corresponding matrix. To bound this parameter, we derive
a new class of functional inequalities for Markov chains that we call Type II
Poincar\'e inequalities and from which we can quantify the amount of shift
needed for effective low-rank approximation and estimation. This analysis shows
in particular that the required shift depends on decay of the high-order
singular values of the shifted successor measure and is hence typically small
in practice. Additionally, we establish a connection between the necessary
shift and the local mixing properties of the underlying dynamical system, which
provides a natural way of selecting the shift. Finally, we validate our
theoretical findings with experiments, and demonstrate that shifting the
successor measure indeed leads to improved performance in goal-conditioned RL.

</details>


### [215] [RapidGNN: Energy and Communication-Efficient Distributed Training on Large-Scale Graph Neural Networks](https://arxiv.org/abs/2509.05207)
*Arefin Niam,Tevfik Kosar,M S Q Zulkar Nine*

Main category: cs.LG

TL;DR: RapidGNN is a distributed GNN training framework that improves training throughput, reduces remote feature fetches, and achieves near-linear scalability and better energy efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges associated with distributed GNN training, such as computational loads and communication overhead caused by highly connected structures in large-scale graphs.

Method: RapidGNN employs deterministic sampling-based scheduling to optimize cache construction and prefetching of remote features, improving training efficiency for large-scale graphs.

Result: RapidGNN achieves 2.46x to 3.00x speedup in training throughput, reduces remote feature fetches by 9.70x to 15.39x, and demonstrates near-linear scalability with an increasing number of computing units. It also improves energy efficiency on both CPU (44%) and GPU (32%) compared to baseline methods.

Conclusion: RapidGNN offers a scalable and energy-efficient solution for distributed GNN training, demonstrating significant improvements over conventional methods in performance and resource utilization across diverse graph datasets.

Abstract: Graph Neural Networks (GNNs) have become popular across a diverse set of
tasks in exploring structural relationships between entities. However, due to
the highly connected structure of the datasets, distributed training of GNNs on
large-scale graphs poses significant challenges. Traditional sampling-based
approaches mitigate the computational loads, yet the communication overhead
remains a challenge. This paper presents RapidGNN, a distributed GNN training
framework with deterministic sampling-based scheduling to enable efficient
cache construction and prefetching of remote features. Evaluation on benchmark
graph datasets demonstrates RapidGNN's effectiveness across different scales
and topologies. RapidGNN improves end-to-end training throughput by 2.46x to
3.00x on average over baseline methods across the benchmark datasets, while
cutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further
demonstrates near-linear scalability with an increasing number of computing
units efficiently. Furthermore, it achieves increased energy efficiency over
the baseline methods for both CPU and GPU by 44% and 32%, respectively.

</details>


### [216] [Deep Learning-Enhanced for Amine Emission Monitoring and Performance Analysis in Industrial Carbon Capture Plants](https://arxiv.org/abs/2509.05241)
*Lokendra Poudel,David Tincher,Duy-Nhat Phan,Rahul Bhowmik*

Main category: cs.LG

TL;DR: The paper introduces deep learning models to forecast and optimize emissions and performance parameters in amine-based post-combustion carbon capture systems, achieving over 99% predictive accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency, stability, and environmental impact of carbon capture systems by leveraging machine learning models.

Method: Four deep learning architectures were developed (Basic LSTM, Stacked LSTM, Bi-directional LSTM, and Convolutional LSTM) using operational data from a solvent campaign. Causal impact analysis was also performed by perturbing input variables to assess effects on emissions and performance.

Result: The developed models efficiently predicted emissions and system performance, achieving over 99% accuracy. Causal analysis identified key operational adjustments that significantly reduce emissions and enhance system performance.

Conclusion: Machine learning frameworks not only predict emissions but also serve as decision-support tools for optimization and real-time monitoring of carbon capture operations, contributing to their sustainability and efficiency.

Abstract: We present data driven deep learning models for forecasting and monitoring
amine emissions and key performance parameters in amine-based post-combustion
carbon capture systems. Using operational data from the CESAR1 solvent campaign
at Technology Center Mongstad, four DL architectures such as Basic Long
Short-Term Memory (LSTM), Stacked LSTM, Bi-directional LSTM, and Convolutional
LSTM were developed to capture time-dependent process behavior. For emission
prediction, models were designed for 2-amino-2-methyl-1-propanol (AMP) and
Piperazine emissions measured via FTIR and IMR-MS methods. System performance
models target four critical parameters: CO$_2$ product flow, absorber outlet
temperature, depleted flue gas outlet temperature, and RFCC stripper bottom
temperature. These models achieved high predictive accuracy exceeding 99% and
effectively tracked both steady trends and abrupt fluctuations. Additionally,
we conducted causal impact analysis to evaluate how operational variables
influence emissions and system performance. Eight input variables were
systematically perturbed within $\pm$20% of nominal values to simulate
deviations and assess their impact. This analysis revealed that adjusting
specific operational parameters, such as lean solvent temperature and water
wash conditions, can significantly reduce amine emissions and enhance system
performance. This study highlights ML not only as a predictive tool but also as
a decision support system for optimizing carbon capture operations under steady
state and dynamic conditions. By enabling real time monitoring, scenario
testing, and operational optimization, the developed ML framework offers a
practical pathway for mitigating environmental impacts. This work represents a
step toward intelligent, data-driven control strategies that enhance the
efficiency, stability, and sustainability of carbon capture and storage
technologies.

</details>


### [217] [A Kolmogorov-Arnold Network for Interpretable Cyberattack Detection in AGC Systems](https://arxiv.org/abs/2509.05259)
*Jehad Jilan,Niranjana Naveen Nambiar,Ahmad Mohammad Saber,Alok Paranjape,Amr Youssef,Deepa Kundur*

Main category: cs.LG

TL;DR: The paper introduces Kolmogorov-Arnold Networks (KAN) to detect False Data Injection Attacks (FDIAs) in Automatic Generation Control (AGC) systems, achieving high accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: To improve interpretability and accuracy in FDIA detection for AGC systems, addressing the shortcomings of existing blackbox methods.

Method: KAN models are trained offline to model nonlinearities in AGC systems, and symbolic equations are extracted for improved interpretability.

Result: The KAN model achieved up to 95.97% detection accuracy while maintaining a low false alarm rate.

Conclusion: KAN offers an effective and interpretable solution for safeguarding AGC systems against stealthy cyberattacks like FDIAs.

Abstract: Automatic Generation Control (AGC) is essential for power grid stability but
remains vulnerable to stealthy cyberattacks, such as False Data Injection
Attacks (FDIAs), which can disturb the system's stability while evading
traditional detection methods. Unlike previous works that relied on blackbox
approaches, this work proposes Kolmogorov-Arnold Networks (KAN) as an
interpretable and accurate method for FDIA detection in AGC systems,
considering the system nonlinearities. KAN models include a method for
extracting symbolic equations, and are thus able to provide more
interpretability than the majority of machine learning models. The proposed KAN
is trained offline to learn the complex nonlinear relationships between the AGC
measurements under different operating scenarios. After training, symbolic
formulas that describe the trained model's behavior can be extracted and
leveraged, greatly enhancing interpretability. Our findings confirm that the
proposed KAN model achieves FDIA detection rates of up to 95.97% and 95.9% for
the initial model and the symbolic formula, respectively, with a low false
alarm rate, offering a reliable approach to enhancing AGC cybersecurity.

</details>


### [218] [SpikingBrain Technical Report: Spiking Brain-inspired Large Models](https://arxiv.org/abs/2509.05276)
*Yuqi Pan,Yupeng Feng,Jinghao Zhuang,Siyu Ding,Zehao Liu,Bohan Sun,Yuhong Chou,Han Xu,Xuerui Qiu,Anlin Deng,Anjie Hu,Peng Zhou,Man Yao,Jibin Wu,Jian Yang,Guoliang Sun,Bo Xu,Guoqi Li*

Main category: cs.LG

TL;DR: The paper introduces SpikingBrain, brain-inspired models focused on efficient long-context processing and designed for non-NVIDIA hardware platforms.


<details>
  <summary>Details</summary>
Motivation: The study aims to address efficiency bottlenecks in mainstream Transformer-based large language models, such as quadratic training computation scaling, memory growth during inference, and challenges in training on non-NVIDIA platforms.

Method: SpikingBrain employs linear and hybrid-linear attention architectures with adaptive spiking neurons, an optimized training pipeline, a spike coding framework, and customized engineering solutions tailored to the MetaX GPU cluster.

Result: The models, SpikingBrain-7B and SpikingBrain-76B, achieve comparable performance to Transformer baselines using only 150B tokens, enhance training efficiency, provide nearly constant memory inference, and achieve significant performance metrics like a 100x speedup in Time to First Token for long sequences.

Conclusion: The research showcases the potential of brain-inspired mechanisms in advancing efficient, scalable, and hardware-adaptive large language model design.

Abstract: Mainstream Transformer-based large language models face major efficiency
bottlenecks: training computation scales quadratically with sequence length,
and inference memory grows linearly, limiting long-context processing. Building
large models on non-NVIDIA platforms also poses challenges for stable and
efficient training. To address this, we introduce SpikingBrain, a family of
brain-inspired models designed for efficient long-context training and
inference. SpikingBrain leverages the MetaX GPU cluster and focuses on three
aspects: (1) Model Architecture: linear and hybrid-linear attention
architectures with adaptive spiking neurons; (2) Algorithmic Optimizations: an
efficient, conversion-based training pipeline and a dedicated spike coding
framework; (3) System Engineering: customized training frameworks, operator
libraries, and parallelism strategies tailored to MetaX hardware.
  Using these techniques, we develop two models: SpikingBrain-7B, a linear LLM,
and SpikingBrain-76B, a hybrid-linear MoE LLM. These models demonstrate the
feasibility of large-scale LLM development on non-NVIDIA platforms.
SpikingBrain achieves performance comparable to open-source Transformer
baselines while using only about 150B tokens for continual pre-training. Our
models significantly improve long-sequence training efficiency and deliver
inference with (partially) constant memory and event-driven spiking behavior.
For example, SpikingBrain-7B attains over 100x speedup in Time to First Token
for 4M-token sequences. Training remains stable for weeks on hundreds of MetaX
C550 GPUs, with the 7B model reaching a Model FLOPs Utilization of 23.4
percent. The proposed spiking scheme achieves 69.15 percent sparsity, enabling
low-power operation. Overall, this work demonstrates the potential of
brain-inspired mechanisms to drive the next generation of efficient and
scalable large model design.

</details>


### [219] [Dual-Branch Convolutional Framework for Spatial and Frequency-Based Image Forgery Detection](https://arxiv.org/abs/2509.05281)
*Naman Tyagi*

Main category: cs.LG

TL;DR: The paper proposes a forgery detection framework combining spatial and frequency features using a dual-branch CNN and a Siamese network, achieving 77.9% accuracy on the CASIA 2.0 dataset.


<details>
  <summary>Details</summary>
Motivation: The increasing prevalence of deepfakes and digital image forgeries necessitates effective methods to verify image authenticity.

Method: A dual-branch convolutional neural network extracts features from spatial and frequency domains, fuses them, and uses a Siamese network to produce embeddings for classification.

Result: The framework achieved 77.9% accuracy on the CASIA 2.0 dataset, surpassing traditional statistical methods.

Conclusion: Despite not matching larger forgery detection pipelines, the method balances computational efficiency and reliability, enabling practical deployment and progressing visual forensic techniques.

Abstract: With a very rapid increase in deepfakes and digital image forgeries, ensuring
the authenticity of images is becoming increasingly challenging. This report
introduces a forgery detection framework that combines spatial and
frequency-based features for detecting forgeries. We propose a dual branch
convolution neural network that operates on features extracted from spatial and
frequency domains. Features from both branches are fused and compared within a
Siamese network, yielding 64 dimensional embeddings for classification. When
benchmarked on CASIA 2.0 dataset, our method achieves an accuracy of 77.9%,
outperforming traditional statistical methods. Despite its relatively weaker
performance compared to larger, more complex forgery detection pipelines, our
approach balances computational complexity and detection reliability, making it
ready for practical deployment. It provides a strong methodology for forensic
scrutiny of digital images. In a broader sense, it advances the state of the
art in visual forensics, addressing an urgent requirement in media
verification, law enforcement and digital content reliability.

</details>


### [220] [Learning to accelerate distributed ADMM using graph neural networks](https://arxiv.org/abs/2509.05288)
*Henri Doerks,Paul Häusner,Daniel Hernández Escobar,Jens Sjölund*

Main category: cs.LG

TL;DR: This paper proposes combining ADMM with graph neural networks to improve convergence speed and solution quality in distributed optimization tasks.


<details>
  <summary>Details</summary>
Motivation: Distributed optimization, essential for large-scale machine learning and control, often faces challenges with slow convergence and sensitive hyperparameter choices in ADMM.

Method: The authors represent distributed ADMM iterations as graph neural networks and propose learning adaptive step sizes and communication weights using GNNs, trained in an end-to-end manner through unrolled ADMM.

Result: Numerical experiments show that the learned variant of ADMM outperforms standard ADMM in both convergence speed and solution quality.

Conclusion: Integrating GNN-based hyperparameter prediction with ADMM preserves convergence properties and significantly enhances performance for distributed optimization.

Abstract: Distributed optimization is fundamental in large-scale machine learning and
control applications. Among existing methods, the Alternating Direction Method
of Multipliers (ADMM) has gained popularity due to its strong convergence
guarantees and suitability for decentralized computation. However, ADMM often
suffers from slow convergence and sensitivity to hyperparameter choices. In
this work, we show that distributed ADMM iterations can be naturally
represented within the message-passing framework of graph neural networks
(GNNs). Building on this connection, we propose to learn adaptive step sizes
and communication weights by a graph neural network that predicts the
hyperparameters based on the iterates. By unrolling ADMM for a fixed number of
iterations, we train the network parameters end-to-end to minimize the final
iterates error for a given problem class, while preserving the algorithm's
convergence properties. Numerical experiments demonstrate that our learned
variant consistently improves convergence speed and solution quality compared
to standard ADMM. The code is available at
https://github.com/paulhausner/learning-distributed-admm.

</details>


### [221] [Deep Reinforcement Learning for Ranking Utility Tuning in the Ad Recommender System at Pinterest](https://arxiv.org/abs/2509.05292)
*Xiao Yang,Mehdi Ben Ayed,Longyu Zhao,Fan Zhou,Yuchen Shen,Abe Engle,Jinfeng Zhuang,Ling Leng,Jiajing Xu,Charles Rosenberg,Prathibha Deshikachar*

Main category: cs.LG

TL;DR: This paper proposes a Deep Reinforcement Learning framework (DRL-PUT) to optimize the ranking utility function in ad recommender systems, improving personalization and performance over manual tuning.


<details>
  <summary>Details</summary>
Motivation: Traditional manual tuning methods for ad recommender systems are suboptimal due to unprincipled objectives, excessive parameter combinations, and lack of personalization and adaptability.

Method: The study formulates the tuning problem as a reinforcement learning task, predicts optimal hyperparameters based on ad request states, and directly learns a policy model using online serving logs without estimating a value function.

Result: The proposed DRL-PUT framework demonstrated a 9.7% improvement in click-through rate and 7.7% improvement in long click-through rate during online A/B testing on Pinterest's ad system.

Conclusion: DRL-PUT proves to be effective for personalized and adaptive utility tuning in ad recommender systems, outperforming traditional manual approaches and addressing multi-objective optimization challenges.

Abstract: The ranking utility function in an ad recommender system, which linearly
combines predictions of various business goals, plays a central role in
balancing values across the platform, advertisers, and users. Traditional
manual tuning, while offering simplicity and interpretability, often yields
suboptimal results due to its unprincipled tuning objectives, the vast amount
of parameter combinations, and its lack of personalization and adaptability to
seasonality. In this work, we propose a general Deep Reinforcement Learning
framework for Personalized Utility Tuning (DRL-PUT) to address the challenges
of multi-objective optimization within ad recommender systems. Our key
contributions include: 1) Formulating the problem as a reinforcement learning
task: given the state of an ad request, we predict the optimal hyperparameters
to maximize a pre-defined reward. 2) Developing an approach to directly learn
an optimal policy model using online serving logs, avoiding the need to
estimate a value function, which is inherently challenging due to the high
variance and unbalanced distribution of immediate rewards. We evaluated DRL-PUT
through an online A/B experiment in Pinterest's ad recommender system. Compared
to the baseline manual utility tuning approach, DRL-PUT improved the
click-through rate by 9.7% and the long click-through rate by 7.7% on the
treated segment. We conducted a detailed ablation study on the impact of
different reward definitions and analyzed the personalization aspect of the
learned policy model.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [222] [Scaling Environments for Organoid Intelligence with LLM-Automated Design and Plasticity-Based Evaluation](https://arxiv.org/abs/2509.04633)
*Brennen Hill*

Main category: cs.NE

TL;DR: This paper presents a framework for training biological neural networks (neural organoids) in virtual environments to explore their learning mechanisms.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the increasing complexity of artificial agents by developing virtual environments for training and studying biological neural networks, thereby shedding light on their learning mechanisms such as LTP and LTD.

Method: The paper designs three scalable virtual environments with tasks of varying complexity (conditional avoidance, predator-prey, and Pong). It also introduces meta-learning using a Large Language Model (LLM) for optimizing environment and curriculum design and proposes multi-modal evaluations of learning.

Result: The framework offers a scalable, closed-loop methodology to train and study neural organoid-based agents, combining virtual tasks and biological observations.

Conclusion: This study bridges computational neuroscience and AI by providing a platform to study learning, intelligence, and embodiment in biological neural systems.

Abstract: As the complexity of artificial agents increases, the design of environments
that can effectively shape their behavior and capabilities has become a
critical research frontier. We propose a framework that extends this principle
to a novel class of agents: biological neural networks in the form of neural
organoids. This paper introduces three scalable, closed-loop virtual
environments designed to train organoid-based biological agents and probe the
underlying mechanisms of learning, such as long-term potentiation (LTP) and
long-term depression (LTD). We detail the design of three distinct task
environments with increasing complexity: (1) a conditional avoidance task, (2)
a one-dimensional predator-prey scenario, and (3) a replication of the classic
Pong game. For each environment, we formalize the state and action spaces, the
sensory encoding and motor decoding mechanisms, and the feedback protocols
based on predictable (reward) and unpredictable (punishment) stimulation.
Furthermore, we propose a novel meta-learning approach where a Large Language
Model (LLM) is used to automate the generation and optimization of experimental
protocols, scaling the process of environment and curriculum design. Finally,
we outline a multi-modal approach for evaluating learning by measuring synaptic
plasticity at electrophysiological, cellular, and molecular levels. This work
bridges the gap between computational neuroscience and agent-based AI, offering
a unique platform for studying embodiment, learning, and intelligence in a
controlled biological substrate.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [223] [High Performance Matrix Multiplication](https://arxiv.org/abs/2509.04594)
*Ethan Davis*

Main category: cs.PF

TL;DR: This paper evaluates and compares matrix multiplication performance using five different algorithms: CuBLAS, CUDA, BLAS, OpenMP, and C++ Threads.


<details>
  <summary>Details</summary>
Motivation: Matrix multiplication is pivotal for modern computing technologies such as deep learning, simulations, and graphics, necessitating efficient algorithms.

Method: The study employs statistical significance analysis to evaluate performance, specifically FLOPS, of five matrix multiplication approaches for large square matrices.

Result: CuBLAS outperformed others, followed by CUDA, BLAS, OpenMP, and C++ Threads, for matrices with N ≥ 10,000 showing p-value significance (<5e-12).

Conclusion: The performance ranking confirms CuBLAS as the most efficient algorithm for large matrix multiplication tasks among those evaluated.

Abstract: Matrix multiplication is the foundation from much of the success from high
performance technologies like deep learning, scientific simulations, and video
graphics. High level programming languages like Python and R rely on highly
optimized low level libraries for performing core linear algebra operations
like matrix multiplication from Basic Linear Algebra Subprograms (BLAS). This
paper compares the performance of five different matrix multiplication
algorithms using CuBLAS, CUDA, BLAS, OpenMP, and C++ Threads. We find
statistical significance with a p-value below 5e-12 to support the hypothesis
that for square $N \times N$ matrices where $N$ is at least 10,000 then the in
order performance as measured in floating point operations per second (FLOPS)
for these matrix multiplication algorithms is CuBLAS, CUDA, BLAS, OpenMP, and
C++ Threads.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [224] [A Large-Scale Study of Floating-Point Usage in Statically Typed Languages](https://arxiv.org/abs/2509.04936)
*Andrea Gilot,Tobias Wrigstad,Eva Darulova*

Main category: cs.PL

TL;DR: The paper conducts a large-scale empirical study of floating-point arithmetic usage in statically typed languages across GitHub repositories to identify patterns and compare them against benchmarks in literature.


<details>
  <summary>Details</summary>
Motivation: To understand what real-world floating-point code looks like and bridge the gap in knowledge for improving reasoning techniques.

Method: Applied state-of-the-art mining practices, random sampling, unbiased filtering, keyword search, and source code parsing to analyze public GitHub repositories.

Result: Confirmed widespread use of floating-point arithmetic, identified similarities and differences between real-world code and benchmark evaluations in literature.

Conclusion: The study provides insights and datasets to aid in designing and evaluating future techniques for floating-point arithmetic tailored to match real-world usage scenarios.

Abstract: Reasoning about floating-point arithmetic is notoriously hard. While static
and dynamic analysis techniques or program repair have made significant
progress, more work is still needed to make them relevant to real-world code.
On the critical path to that goal is understanding what real-world
floating-point code looks like. To close that knowledge gap, this paper
presents the first large-scale empirical study of floating-point arithmetic
usage in statically typed languages across public GitHub repositories. We
follow state-of the art mining practices including random sampling and
filtering based on only intrinsic properties to avoid bias, and identify
floating-point usage by searching for keywords in the source code, and
programming language constructs (e.g., loops) by parsing the code. Our
evaluation supports the claim often made in papers that floating-point
arithmetic is widely used. Comparing statistics such as size and usage of
certain constructs and functions, we find that benchmarks used in literature to
evaluate automated reasoning techniques for floating-point arithmetic are in
certain aspects representative of 'real-world' code, but not in all. We aim for
our study and dataset to help future techniques for floating-point arithmetic
to be designed and evaluated to match actual users' expectations.

</details>


### [225] [AI-Assisted Modeling: DSL-Driven AI Interactions](https://arxiv.org/abs/2509.05160)
*Steven Smyth,Daniel Busch,Moez Ben Haj Hmida,Edward A. Lee,Bernhard Steffen*

Main category: cs.PL

TL;DR: AI-assisted programming enhances software development by enabling code transparency, graphical visualization, and domain-specific modeling techniques.


<details>
  <summary>Details</summary>
Motivation: To improve software development performance by incorporating transparency and formal verification into AI-assisted programming.

Method: The paper introduces a prototype tool integrated into Visual Studio Code for the Lingua Franca language, allowing model creation, visualization, and refinement.

Result: The prototype demonstrates novel ways to visualize and verify AI-generated code with immediate feedback, tailored for specific domains.

Conclusion: The approach improves code generation, validation, and modeling practices, elevating AI-assisted programming for software development.

Abstract: AI-assisted programming greatly increases software development performance.
We enhance this potential by integrating transparency through domain-specific
modeling techniques and providing instantaneous, graphical visualizations that
accurately represent the semantics of AI-generated code. This approach
facilitates visual inspection and formal verification, such as model checking.
  Formal models can be developed using programming, natural language prompts,
voice commands, and stage-wise refinement, with immediate feedback after each
transformation step. This support can be tailored to specific domains or
intended purposes, improving both code generation and subsequent validation
processes.
  To demonstrate the effectiveness of this approach, we have developed a
prototype as a Visual Studio Code extension for the Lingua Franca language.
This prototype showcases the potential for novel domain-specific modeling
practices, offering an advancement in how models are created, visualized, and
verified.

</details>


### [226] [Non-Termination Proving: 100 Million LoC and Beyond](https://arxiv.org/abs/2509.05293)
*Julien Vanegue,Jules Villard,Peter O'Hearn,Azalea Raad*

Main category: cs.PL

TL;DR: The paper introduces Pulse Infinite, a tool for proving non-termination in large programs and demonstrates its application to over 100 million lines of code, detecting previously unknown issues.


<details>
  <summary>Details</summary>
Motivation: Non-termination detection in large-scale codebases is essential, as prior techniques had limited scalability and only worked on small benchmarks.

Method: Pulse Infinite operates compositionally for scalability and under-approximately for soundness, analyzing code written in languages like C, C++, and Hack.

Result: Pulse Infinite successfully analyzed over 100 million lines of software, identifying over 30 previously unknown divergence issues and setting a new state of the art.

Conclusion: The Pulse Infinite tool addresses scalability challenges and proves effective for detecting non-termination in real-world, large-scale codebases.

Abstract: We report on our tool, Pulse Infinite, that uses proof techniques to show
non-termination (divergence) in large programs. Pulse Infinite works
compositionally and under-approximately: the former supports scale, and the
latter ensures soundness for proving divergence. Prior work focused on small
benchmarks in the tens or hundreds of lines of code (LoC), and scale limits
their practicality: a single company may have tens of millions, or even
hundreds of millions of LoC or more. We report on applying Pulse Infinite to
over a hundred million lines of open-source and proprietary software written in
C, C++, and Hack, identifying over 30 previously unknown issues, establishing a
new state of the art for detecting divergence in real-world codebases.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [227] [In-Context Policy Adaptation via Cross-Domain Skill Diffusion](https://arxiv.org/abs/2509.04535)
*Minjong Yoo,Woo Kyung Kim,Honguk Woo*

Main category: cs.RO

TL;DR: The paper introduces the "in-context policy adaptation (ICPAD)" framework for adapting reinforcement learning policies across domains with minimal data and no model updates, using diffusion-based skill learning techniques.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of adapting long-horizon, multi-task reinforcement learning policies across diverse domains with stringent constraints like limited target domain data and no model updates.

Method: The ICPAD framework uses a cross-domain skill diffusion scheme where domain-agnostic prototype skills and a domain-grounded skill adapter are jointly learned from an offline dataset. A dynamic domain prompting method is also added to improve the alignment of the skill adapter with the target domain.

Result: Experiments in simulated environments like Metaworld for robotics and CARLA for autonomous driving demonstrate the framework's superior adaptability across domains with diverse differences, including dynamics, embodiment, and task horizon, using limited target domain data.

Conclusion: The proposed framework successfully enables effective cross-domain policy adaptation, proving to be robust and efficient under stringent data and model update constraints.

Abstract: In this work, we present an in-context policy adaptation (ICPAD) framework
designed for long-horizon multi-task environments, exploring diffusion-based
skill learning techniques in cross-domain settings. The framework enables rapid
adaptation of skill-based reinforcement learning policies to diverse target
domains, especially under stringent constraints on no model updates and only
limited target domain data. Specifically, the framework employs a cross-domain
skill diffusion scheme, where domain-agnostic prototype skills and a
domain-grounded skill adapter are learned jointly and effectively from an
offline dataset through cross-domain consistent diffusion processes. The
prototype skills act as primitives for common behavior representations of
long-horizon policies, serving as a lingua franca to bridge different domains.
Furthermore, to enhance the in-context adaptation performance, we develop a
dynamic domain prompting scheme that guides the diffusion-based skill adapter
toward better alignment with the target domain. Through experiments with
robotic manipulation in Metaworld and autonomous driving in CARLA, we show that
our $\oursol$ framework achieves superior policy adaptation performance under
limited target domain data conditions for various cross-domain configurations
including differences in environment dynamics, agent embodiment, and task
horizon.

</details>


### [228] [Action Chunking with Transformers for Image-Based Spacecraft Guidance and Control](https://arxiv.org/abs/2509.04628)
*Alejandro Posadas-Nava,Andrea Scorsoglio,Luca Ghilardi,Roberto Furfaro,Richard Linares*

Main category: cs.RO

TL;DR: The paper introduces ACT, an imitation learning system for spacecraft guidance, navigation, and control, achieving smoother trajectories with limited data compared to a meta-RL baseline.


<details>
  <summary>Details</summary>
Motivation: To develop a high-performance control policy for spacecraft GNC that is both sample-efficient and capable of smooth, accurate trajectory generation.

Method: The paper proposes Action Chunking with Transformers (ACT), which uses 100 expert demonstrations to learn policies mapping visual and state observations to control commands.

Result: ACT demonstrates smoother and consistent trajectories, higher accuracy, and greater sample efficiency than a meta-RL system requiring 40 million interactions.

Conclusion: ACT is a superior approach for spacecraft GNC tasks like ISS docking, requiring fewer resources while improving operational effectiveness.

Abstract: We present an imitation learning approach for spacecraft guidance,
navigation, and control(GNC) that achieves high performance from limited data.
Using only 100 expert demonstrations, equivalent to 6,300 environment
interactions, our method, which implements Action Chunking with Transformers
(ACT), learns a control policy that maps visual and state observations to
thrust and torque commands. ACT generates smoother, more consistent
trajectories than a meta-reinforcement learning (meta-RL) baseline trained with
40 million interactions. We evaluate ACT on a rendezvous task: in-orbit docking
with the International Space Station (ISS). We show that our approach achieves
greater accuracy, smoother control, and greater sample efficiency.

</details>


### [229] [Planning from Point Clouds over Continuous Actions for Multi-object Rearrangement](https://arxiv.org/abs/2509.04645)
*Kallol Saha,Amber Li,Angela Rodriguez-Izquierdo,Lifan Yu,Ben Eisner,Maxim Likhachev,David Held*

Main category: cs.RO

TL;DR: The paper addresses robot manipulation by introducing SPOT, a hybrid model that integrates learned models and search-based planning in continuous action spaces for multi-object rearrangement.


<details>
  <summary>Details</summary>
Motivation: To develop a method for tackling the challenges of long-horizon planning in robot manipulation without relying on the discretization of continuous states and actions.

Method: The approach, SPOT, searches for a sequence of 3D scene transformations using suggesters trained on partially observed point clouds to generate candidate actions in continuous space.

Result: SPOT demonstrated high success rates in task planning and execution for multi-object rearrangement in both simulated and real-world settings, outperforming policy-learning alternatives.

Conclusion: SPOT effectively combines learned models and search for enhanced planning performance in complex, long-horizon robotic tasks, emphasizing the importance of search-based methods.

Abstract: Long-horizon planning for robot manipulation is a challenging problem that
requires reasoning about the effects of a sequence of actions on a physical 3D
scene. While traditional task planning methods are shown to be effective for
long-horizon manipulation, they require discretizing the continuous state and
action space into symbolic descriptions of objects, object relationships, and
actions. Instead, we propose a hybrid learning-and-planning approach that
leverages learned models as domain-specific priors to guide search in
high-dimensional continuous action spaces. We introduce SPOT: Search over Point
cloud Object Transformations, which plans by searching for a sequence of
transformations from an initial scene point cloud to a goal-satisfying point
cloud. SPOT samples candidate actions from learned suggesters that operate on
partially observed point clouds, eliminating the need to discretize actions or
object relationships. We evaluate SPOT on multi-object rearrangement tasks,
reporting task planning success and task execution success in both simulation
and real-world environments. Our experiments show that SPOT generates
successful plans and outperforms a policy-learning approach. We also perform
ablations that highlight the importance of search-based planning.

</details>


### [230] [Surformer v2: A Multimodal Classifier for Surface Understanding from Touch and Vision](https://arxiv.org/abs/2509.04658)
*Manish Kansana,Sindhuja Penchala,Shahram Rahimi,Noorbakhsh Amiri Golilarz*

Main category: cs.RO

TL;DR: Surformer v2 is a multi-modal architecture for surface material classification, integrating visual and tactile data using a late fusion approach. It improves upon Surformer v1 and demonstrates strong performance for real-time robotics.


<details>
  <summary>Details</summary>
Motivation: To enhance tactile perception and surface understanding using visual and tactile data in robotic manipulation and interaction.

Method: Surformer v2 employs a CNN-based classifier for vision, a transformer model for tactile data, and decision-level fusion through a learnable weighted sum to integrate information adaptively.

Result: The model achieves strong performance on the Touch and Go dataset, maintaining competitive inference speed suitable for real-time applications.

Conclusion: Surformer v2 highlights the benefits of decision-level fusion and transformer-based tactile data processing for advancing multi-modal robotic perception.

Abstract: Multimodal surface material classification plays a critical role in advancing
tactile perception for robotic manipulation and interaction. In this paper, we
present Surformer v2, an enhanced multi-modal classification architecture
designed to integrate visual and tactile sensory streams through a
late(decision level) fusion mechanism. Building on our earlier Surformer v1
framework [1], which employed handcrafted feature extraction followed by
mid-level fusion architecture with multi-head cross-attention layers, Surformer
v2 integrates the feature extraction process within the model itself and shifts
to late fusion. The vision branch leverages a CNN-based classifier(Efficient
V-Net), while the tactile branch employs an encoder-only transformer model,
allowing each modality to extract modality-specific features optimized for
classification. Rather than merging feature maps, the model performs
decision-level fusion by combining the output logits using a learnable weighted
sum, enabling adaptive emphasis on each modality depending on data context and
training dynamics. We evaluate Surformer v2 on the Touch and Go dataset [2], a
multi-modal benchmark comprising surface images and corresponding tactile
sensor readings. Our results demonstrate that Surformer v2 performs well,
maintaining competitive inference speed, suitable for real-time robotic
applications. These findings underscore the effectiveness of decision-level
fusion and transformer-based tactile modeling for enhancing surface
understanding in multi-modal robotic perception.

</details>


### [231] [Bootstrapping Reinforcement Learning with Sub-optimal Policies for Autonomous Driving](https://arxiv.org/abs/2509.04712)
*Zhihao Zhang,Chengyang Peng,Ekim Yurtsever,Keith A. Redmill*

Main category: cs.RO

TL;DR: Guiding reinforcement learning (RL) agents in automated vehicle control using demonstration policies improves both exploration efficiency and driving performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in automated vehicle control using RL, such as poor sample efficiency and ineffective exploration, which hinder optimal driving strategy discovery.

Method: The authors integrate a rule-based lane change controller with the Soft Actor Critic (SAC) RL algorithm to guide learning with non-expert demonstration policies.

Result: The proposed approach enhances exploration and learning efficiency, resulting in improved driving performance.

Conclusion: The paper concludes that demonstration policy guidance offers significant benefits and can be generalized to other driving scenarios.

Abstract: Automated vehicle control using reinforcement learning (RL) has attracted
significant attention due to its potential to learn driving policies through
environment interaction. However, RL agents often face training challenges in
sample efficiency and effective exploration, making it difficult to discover an
optimal driving strategy. To address these issues, we propose guiding the RL
driving agent with a demonstration policy that need not be a highly optimized
or expert-level controller. Specifically, we integrate a rule-based lane change
controller with the Soft Actor Critic (SAC) algorithm to enhance exploration
and learning efficiency. Our approach demonstrates improved driving performance
and can be extended to other driving scenarios that can similarly benefit from
demonstration-based guidance.

</details>


### [232] [Hierarchical Reduced-Order Model Predictive Control for Robust Locomotion on Humanoid Robots](https://arxiv.org/abs/2509.04722)
*Adrian B. Ghansah,Sergio A. Esteban,Aaron D. Ames*

Main category: cs.RO

TL;DR: The paper proposes a hierarchical control framework for humanoid robots, improving locomotion stability and adaptability using reduced-order models.


<details>
  <summary>Details</summary>
Motivation: To enable humanoid robots to walk robustly in various real-world environments, enhancing stability and adaptability during locomotion.

Method: The framework employs a two-level strategy: a high-level nonlinear MPC optimized for step planning and a mid-level linear MPC that integrates arm and torso dynamics. Computational efficiency allows real-time implementation.

Result: Simulations and hardware tests on the Unitree G1 robot show improved push recovery (36% higher success), better disturbance rejection, and stable walking on diverse terrains.

Conclusion: The proposed approach effectively enhances humanoid locomotion stability using adaptive control strategies and validates its utility across varied real-world terrains.

Abstract: As humanoid robots enter real-world environments, ensuring robust locomotion
across diverse environments is crucial. This paper presents a computationally
efficient hierarchical control framework for humanoid robot locomotion based on
reduced-order models -- enabling versatile step planning and incorporating arm
and torso dynamics to better stabilize the walking. At the high level, we use
the step-to-step dynamics of the ALIP model to simultaneously optimize over
step periods, step lengths, and ankle torques via nonlinear MPC. The ALIP
trajectories are used as references to a linear MPC framework that extends the
standard SRB-MPC to also include simplified arm and torso dynamics. We validate
the performance of our approach through simulation and hardware experiments on
the Unitree G1 humanoid robot. In the proposed framework the high-level step
planner runs at 40 Hz and the mid-level MPC at 500 Hz using the onboard
mini-PC. Adaptive step timing increased the push recovery success rate by 36%,
and the upper body control improved the yaw disturbance rejection. We also
demonstrate robust locomotion across diverse indoor and outdoor terrains,
including grass, stone pavement, and uneven gym mats.

</details>


### [233] [Imitation Learning Based on Disentangled Representation Learning of Behavioral Characteristics](https://arxiv.org/abs/2509.04737)
*Ryoga Oishi,Sho Sakaino,Toshiaki Tsuji*

Main category: cs.RO

TL;DR: This paper introduces a model to adjust robot actions based on qualitative human instructions by learning from segmented demonstrations.


<details>
  <summary>Details</summary>
Motivation: Human instructions for robots are qualitative and challenging, requiring adaptation to varying behavioral conditions.

Method: The method involves segmenting demonstrations into short sequences, assigning weakly supervised labels for specific modifier types, and learning action adaptations from these data.

Result: The proposed model successfully adjusts robot actions online during task execution, outperforming traditional batch-based methods.

Conclusion: The paper demonstrates a motion generation model that enables real-time robot adaptation to human-imposed behavioral conditions, addressing a significant challenge in robot learning.

Abstract: In the field of robot learning, coordinating robot actions through language
instructions is becoming increasingly feasible. However, adapting actions to
human instructions remains challenging, as such instructions are often
qualitative and require exploring behaviors that satisfy varying conditions.
This paper proposes a motion generation model that adapts robot actions in
response to modifier directives human instructions imposing behavioral
conditions during task execution. The proposed method learns a mapping from
modifier directives to actions by segmenting demonstrations into short
sequences, assigning weakly supervised labels corresponding to specific
modifier types. We evaluated our method in wiping and pick and place tasks.
Results show that it can adjust motions online in response to modifier
directives, unlike conventional batch-based methods that cannot adapt during
execution.

</details>


### [234] [COMMET: A System for Human-Induced Conflicts in Mobile Manipulation of Everyday Tasks](https://arxiv.org/abs/2509.04836)
*Dongping Li,Shaoting Peng,John Pohovey,Katherine Rose Driggs-Campbell*

Main category: cs.RO

TL;DR: The paper introduces COMMET, a system designed to handle conflicts between human activities and robot actions in household environments, while adapting to user preferences.


<details>
  <summary>Details</summary>
Motivation: To address challenges arising from human-robot conflicts in domestic settings, which are influenced by dynamic human behavior and individual user preferences.

Method: COMMET uses a hybrid detection approach: multi-modal retrieval for common cases and fine-tuned model inference for low-confidence scenarios. User preferences are summarized using GPT-4o and a user-friendly data collection interface is developed.

Result: The system demonstrated improved detection accuracy and latency compared to GPT models in preliminary studies.

Conclusion: COMMET facilitates the integration of robots into daily environments by effectively resolving human-induced conflicts and adapting to personal preferences, supporting further research with its interface and deployment workflow.

Abstract: Continuous advancements in robotics and AI are driving the integration of
robots from industry into everyday environments. However, dynamic and
unpredictable human activities in daily lives would directly or indirectly
conflict with robot actions. Besides, due to the social attributes of such
human-induced conflicts, solutions are not always unique and depend highly on
the user's personal preferences. To address these challenges and facilitate the
development of household robots, we propose COMMET, a system for human-induced
COnflicts in Mobile Manipulation of Everyday Tasks. COMMET employs a hybrid
detection approach, which begins with multi-modal retrieval and escalates to
fine-tuned model inference for low-confidence cases. Based on collected user
preferred options and settings, GPT-4o will be used to summarize user
preferences from relevant cases. In preliminary studies, our detection module
shows better accuracy and latency compared with GPT models. To facilitate
future research, we also design a user-friendly interface for user data
collection and demonstrate an effective workflow for real-world deployments.

</details>


### [235] [A Knowledge-Driven Diffusion Policy for End-to-End Autonomous Driving Based on Expert Routing](https://arxiv.org/abs/2509.04853)
*Chengkai Xu,Jiaqi Liu,Yicheng Guo,Peng Hang,Jian Sun*

Main category: cs.RO

TL;DR: The paper introduces KDP, a knowledge-driven diffusion policy, to improve end-to-end autonomous driving by addressing issues like multi-modality, temporal stability, and generalization.


<details>
  <summary>Details</summary>
Motivation: Improve end-to-end autonomous driving by overcoming challenges in generating multi-modal actions, ensuring temporal stability, and generalizing across diverse scenarios.

Method: The proposed KDP model integrates generative diffusion modeling for multi-modal and temporal coherence and employs a sparse mixture-of-experts routing mechanism for modular adaptability.

Result: Experimental results show that KDP improves success rates, reduces collisions, and achieves smoother control compared to existing methods. Ablation and activation studies validate the utility of key components.

Conclusion: Integration of diffusion modeling with expert routing is a promising paradigm for scalable and interpretable autonomous driving.

Abstract: End-to-end autonomous driving remains constrained by the need to generate
multi-modal actions, maintain temporal stability, and generalize across diverse
scenarios. Existing methods often collapse multi-modality, struggle with
long-horizon consistency, or lack modular adaptability. This paper presents
KDP, a knowledge-driven diffusion policy that integrates generative diffusion
modeling with a sparse mixture-of-experts routing mechanism. The diffusion
component generates temporally coherent and multi-modal action sequences, while
the expert routing mechanism activates specialized and reusable experts
according to context, enabling modular knowledge composition. Extensive
experiments across representative driving scenarios demonstrate that KDP
achieves consistently higher success rates, reduced collision risk, and
smoother control compared to prevailing paradigms. Ablation studies highlight
the effectiveness of sparse expert activation and the Transformer backbone, and
activation analyses reveal structured specialization and cross-scenario reuse
of experts. These results establish diffusion with expert routing as a scalable
and interpretable paradigm for knowledge-driven end-to-end autonomous driving.

</details>


### [236] [Towards an Accurate and Effective Robot Vision (The Problem of Topological Localization for Mobile Robots)](https://arxiv.org/abs/2509.04948)
*Emanuela Boros*

Main category: cs.RO

TL;DR: This paper investigates topological localization using visual descriptors in mobile robotics, with a systematic comparison of features, distance measures, and classifiers in office environments.


<details>
  <summary>Details</summary>
Motivation: To address challenges in visual localization due to perceptual ambiguity, sensor noise, and varying illumination, using images from mobile robots without relying on temporal image continuity.

Method: The paper evaluates state-of-the-art visual descriptors like SIFT, ASIFT, and Bag-of-Visual-Words, along with their configured distance measures and classification strategies, under systematic experimental conditions.

Result: The research demonstrates how appropriate configurations of visual descriptors and classifiers can improve localization performance, achieving validated success in the context of the Robot Vision task of the ImageCLEF evaluation campaign.

Conclusion: Proper setup of visual descriptors and classifiers significantly enhances localization reliability. The future focus includes developing hierarchical models and feature combinations for integrated, real-time systems to operate in diverse illumination conditions and over longer routes.

Abstract: Topological localization is a fundamental problem in mobile robotics, since
robots must be able to determine their position in order to accomplish tasks.
Visual localization and place recognition are challenging due to perceptual
ambiguity, sensor noise, and illumination variations. This work addresses
topological localization in an office environment using only images acquired
with a perspective color camera mounted on a robot platform, without relying on
temporal continuity of image sequences. We evaluate state-of-the-art visual
descriptors, including Color Histograms, SIFT, ASIFT, RGB-SIFT, and
Bag-of-Visual-Words approaches inspired by text retrieval. Our contributions
include a systematic, quantitative comparison of these features, distance
measures, and classifiers. Performance was analyzed using standard evaluation
metrics and visualizations, extending previous experiments. Results demonstrate
the advantages of proper configurations of appearance descriptors, similarity
measures, and classifiers. The quality of these configurations was further
validated in the Robot Vision task of the ImageCLEF evaluation campaign, where
the system identified the most likely location of novel image sequences. Future
work will explore hierarchical models, ranking methods, and feature
combinations to build more robust localization systems, reducing training and
runtime while avoiding the curse of dimensionality. Ultimately, this aims
toward integrated, real-time localization across varied illumination and longer
routes.

</details>


### [237] [Ground-Aware Octree-A* Hybrid Path Planning for Memory-Efficient 3D Navigation of Ground Vehicles](https://arxiv.org/abs/2509.04950)
*Byeong-Il Ham,Hyun-Bin Kim,Kyung-Soo Kim*

Main category: cs.RO

TL;DR: The paper proposes a 3D path planning method integrating A* algorithm with octree structure to improve computational efficiency and memory usage for UGVs and legged robots.


<details>
  <summary>Details</summary>
Motivation: To enhance path planning for UGVs and legged robots by leveraging obstacles as aids rather than barriers, and improve efficiency in environments with varying terrain density.

Method: The authors modify the A* algorithm to include height-based penalties and integrate it with an octree-based 3D grid map to compress nodes and optimize the planning process.

Result: Benchmark tests showed improved memory usage, computational efficiency, and optimal path generation using the octree structure.

Conclusion: The integrated method provides efficient, optimal, and realistic path planning, supporting its application in real-time scenarios.

Abstract: In this paper, we propose a 3D path planning method that integrates the A*
algorithm with the octree structure. Unmanned Ground Vehicles (UGVs) and legged
robots have been extensively studied, enabling locomotion across a variety of
terrains. Advances in mobility have enabled obstacles to be regarded not only
as hindrances to be avoided, but also as navigational aids when beneficial. A
modified 3D A* algorithm generates an optimal path by leveraging obstacles
during the planning process. By incorporating a height-based penalty into the
cost function, the algorithm enables the use of traversable obstacles to aid
locomotion while avoiding those that are impassable, resulting in more
efficient and realistic path generation. The octree-based 3D grid map achieves
compression by merging high-resolution nodes into larger blocks, especially in
obstacle-free or sparsely populated areas. This reduces the number of nodes
explored by the A* algorithm, thereby improving computational efficiency and
memory usage, and supporting real-time path planning in practical environments.
Benchmark results demonstrate that the use of octree structure ensures an
optimal path while significantly reducing memory usage and computation time.

</details>


### [238] [DeGuV: Depth-Guided Visual Reinforcement Learning for Generalization and Interpretability in Manipulation](https://arxiv.org/abs/2509.04970)
*Tien Pham,Xinyun Chi,Khang Nguyen,Manfred Huber,Angelo Cangelosi*

Main category: cs.RO

TL;DR: DeGuV, a novel RL framework, enhances generalization and sample efficiency by using a masker network to focus agents on critical visual information and incorporates techniques for stable training and robust feature learning.


<details>
  <summary>Details</summary>
Motivation: Generalizing RL agents' learned skills to new environments is challenging, especially in robotics. Data augmentation helps generalization but sacrifices stability and efficiency.

Method: DeGuV uses a masker network generating masks from depth inputs to retain essential visual features, combined with contrastive learning and stabilized Q-value estimation for improved robustness and efficiency.

Result: DeGuV surpassed state-of-the-art methods in generalization and sample efficiency in the RL-ViGen benchmark with the Franka Emika robot, achieving effective zero-shot sim-to-real transfer.

Conclusion: The DeGuV framework not only advances generalization and sample efficiency but also boosts interpretability by highlighting relevant visual regions critical for RL agents.

Abstract: Reinforcement learning (RL) agents can learn to solve complex tasks from
visual inputs, but generalizing these learned skills to new environments
remains a major challenge in RL application, especially robotics. While data
augmentation can improve generalization, it often compromises sample efficiency
and training stability. This paper introduces DeGuV, an RL framework that
enhances both generalization and sample efficiency. In specific, we leverage a
learnable masker network that produces a mask from the depth input, preserving
only critical visual information while discarding irrelevant pixels. Through
this, we ensure that our RL agents focus on essential features, improving
robustness under data augmentation. In addition, we incorporate contrastive
learning and stabilize Q-value estimation under augmentation to further enhance
sample efficiency and training stability. We evaluate our proposed method on
the RL-ViGen benchmark using the Franka Emika robot and demonstrate its
effectiveness in zero-shot sim-to-real transfer. Our results show that DeGuV
outperforms state-of-the-art methods in both generalization and sample
efficiency while also improving interpretability by highlighting the most
relevant regions in the visual input

</details>


### [239] [Lyapunov-Based Deep Learning Control for Robots with Unknown Jacobian](https://arxiv.org/abs/2509.04984)
*Koji Matsuno,Chien Chern Cheah*

Main category: cs.RO

TL;DR: The paper introduces a modular deep learning control framework, ensuring stability for robot motion control via Lyapunov-like analysis.


<details>
  <summary>Details</summary>
Motivation: To tackle the black-box nature of deep learning in real-time robotic applications, addressing stability and trustworthiness concerns in robot motion control.

Method: Developing an end-to-end deep learning control framework that uses modular learning and real-time weight updates, with stability ensured through Lyapunov-like analysis.

Result: Experimental results show the proposed controller's performance on industrial robots, validating its stability and effectiveness.

Conclusion: The method successfully addresses deep learning's black-box issue, enabling stable deployment in real-time robotic control and paving the way for future advancements.

Abstract: Deep learning, with its exceptional learning capabilities and flexibility,
has been widely applied in various applications. However, its black-box nature
poses a significant challenge in real-time robotic applications, particularly
in robot control, where trustworthiness and robustness are critical in ensuring
safety. In robot motion control, it is essential to analyze and ensure system
stability, necessitating the establishment of methodologies that address this
need. This paper aims to develop a theoretical framework for end-to-end deep
learning control that can be integrated into existing robot control theories.
The proposed control algorithm leverages a modular learning approach to update
the weights of all layers in real time, ensuring system stability based on
Lyapunov-like analysis. Experimental results on industrial robots are presented
to illustrate the performance of the proposed deep learning controller. The
proposed method offers an effective solution to the black-box problem in deep
learning, demonstrating the possibility of deploying real-time deep learning
strategies for robot kinematic control in a stable manner. This achievement
provides a critical foundation for future advancements in deep learning based
real-time robotic applications.

</details>


### [240] [FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies](https://arxiv.org/abs/2509.04996)
*Moritz Reuss,Hongyi Zhou,Marcel Rühle,Ömer Erdinç Yağmurlu,Fabian Otto,Rudolf Lioutikov*

Main category: cs.RO

TL;DR: The paper introduces FLOWER, an efficient Vision-Language-Action (VLA) model with reduced computational demands while maintaining strong performance.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to VLA policy development require substantial computational resources and large models, making them impractical. This paper aims to create a more efficient solution.

Method: The authors propose intermediate-modality fusion to prune LLM layers by up to 50% and use Global-AdaLN conditioning to reduce parameters by 20%. These techniques are integrated into the FLOWER model.

Result: The FLOWER model, with only 950 M parameters and pretrained in 200 H100 GPU hours, achieves competitive results across 190 tasks. It sets a new SoTA score of 4.53 on the CALVIN ABC benchmark.

Conclusion: FLOWER provides a more resource-efficient approach to VLA policy development, showcasing strong performance while requiring fewer computational resources.

Abstract: Developing efficient Vision-Language-Action (VLA) policies is crucial for
practical robotics deployment, yet current approaches face prohibitive
computational costs and resource requirements. Existing diffusion-based VLA
policies require multi-billion-parameter models and massive datasets to achieve
strong performance. We tackle this efficiency challenge with two contributions:
intermediate-modality fusion, which reallocates capacity to the diffusion head
by pruning up to $50\%$ of LLM layers, and action-specific Global-AdaLN
conditioning, which cuts parameters by $20\%$ through modular adaptation. We
integrate these advances into a novel 950 M-parameter VLA called FLOWER.
Pretrained in just 200 H100 GPU hours, FLOWER delivers competitive performance
with bigger VLAs across $190$ tasks spanning ten simulation and real-world
benchmarks and demonstrates robustness across diverse robotic embodiments. In
addition, FLOWER achieves a new SoTA of 4.53 on the CALVIN ABC benchmark.
Demos, code and pretrained weights are available at
https://intuitive-robots.github.io/flower_vla/.

</details>


### [241] [Pointing-Guided Target Estimation via Transformer-Based Attention](https://arxiv.org/abs/2509.05031)
*Luca Müller,Hassan Ali,Philipp Allgeuer,Lukáš Gajdošech,Stefan Wermter*

Main category: cs.RO

TL;DR: The paper introduces the Multi-Modality Inter-TransFormer (MM-ITF) architecture for accurately predicting gesture-indicated targets in tabletop scenarios using RGB data, enabling intuitive human-robot collaboration.


<details>
  <summary>Details</summary>
Motivation: Improve human-robot interaction by enabling robots to interpret natural deictic gestures like pointing, essential for effective communication and task execution.

Method: The MM-ITF utilizes inter-modality attention to map 2D pointing gestures from monocular RGB data to specific object locations, assigns likelihood scores, and predicts the intended object.

Result: The proposed MM-ITF demonstrated high accuracy in predicting human-indicated targets in a tabletop scenario with the NICOL robot.

Conclusion: MM-ITF facilitates seamless and accessible human-robot collaboration by reliably interpreting pointing gestures. Results and the patch confusion matrix highlight its predictive efficacy.

Abstract: Deictic gestures, like pointing, are a fundamental form of non-verbal
communication, enabling humans to direct attention to specific objects or
locations. This capability is essential in Human-Robot Interaction (HRI), where
robots should be able to predict human intent and anticipate appropriate
responses. In this work, we propose the Multi-Modality Inter-TransFormer
(MM-ITF), a modular architecture to predict objects in a controlled tabletop
scenario with the NICOL robot, where humans indicate targets through natural
pointing gestures. Leveraging inter-modality attention, MM-ITF maps 2D pointing
gestures to object locations, assigns a likelihood score to each, and
identifies the most likely target. Our results demonstrate that the method can
accurately predict the intended object using monocular RGB data, thus enabling
intuitive and accessible human-robot collaboration. To evaluate the
performance, we introduce a patch confusion matrix, providing insights into the
model's predictions across candidate object locations. Code available at:
https://github.com/lucamuellercode/MMITF.

</details>


### [242] [Shared Autonomy through LLMs and Reinforcement Learning for Applications to Ship Hull Inspections](https://arxiv.org/abs/2509.05042)
*Cristiano Caissutti,Estelle Gerbier,Ehsan Khorrambakht,Paolo Marinelli,Andrea Munafo',Andrea Caiti*

Main category: cs.RO

TL;DR: This paper explores a multi-layered approach to enhance shared autonomy in marine robotic fleets using large language models, human-in-the-loop frameworks, and a behavior-based mission manager.


<details>
  <summary>Details</summary>
Motivation: To address the need for effective human-robot collaboration in high-risk, complex maritime environments.

Method: The study integrates large language models for intuitive task specification, human-in-the-loop frameworks for adaptive coordination, and behavior trees for modular mission management.

Result: Preliminary tests in simulation and real-world environments show reduced cognitive load for operators, improved alignment with human intent, and enhanced system transparency.

Conclusion: The paper provides a scalable, modular groundwork for developing trustworthy human-robot collaboration in critical maritime applications.

Abstract: Shared autonomy is a promising paradigm in robotic systems, particularly
within the maritime domain, where complex, high-risk, and uncertain
environments necessitate effective human-robot collaboration. This paper
investigates the interaction of three complementary approaches to advance
shared autonomy in heterogeneous marine robotic fleets: (i) the integration of
Large Language Models (LLMs) to facilitate intuitive high-level task
specification and support hull inspection missions, (ii) the implementation of
human-in-the-loop interaction frameworks in multi-agent settings to enable
adaptive and intent-aware coordination, and (iii) the development of a modular
Mission Manager based on Behavior Trees to provide interpretable and flexible
mission control. Preliminary results from simulation and real-world lake-like
environments demonstrate the potential of this multi-layered architecture to
reduce operator cognitive load, enhance transparency, and improve adaptive
behaviour alignment with human intent. Ongoing work focuses on fully
integrating these components, refining coordination mechanisms, and validating
the system in operational port scenarios. This study contributes to
establishing a modular and scalable foundation for trustworthy,
human-collaborative autonomy in safety-critical maritime robotics applications.

</details>


### [243] [Robust Model Predictive Control Design for Autonomous Vehicles with Perception-based Observers](https://arxiv.org/abs/2509.05201)
*Nariman Niknejad,Gokul S. Sankar,Bahare Kiumarsi,Hamidreza Modares*

Main category: cs.RO

TL;DR: The paper introduces a perception-aware robust MPC methodology addressing non-Gaussian noise in deep learning-based perception systems, achieving stability and superior performance compared to standard Gaussian-noise-based approaches.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable a safe and effective feedback control system that accounts for biased and heavy-tailed uncertainties arising from deep-learning-based perception modules.

Method: The framework uses set-based state estimation with constrained zonotopes to capture uncertainties, employs a reformulated LP-based robust MPC for efficiency, and ensures stability through zonotopic invariant sets supported by Minkowski-Lyapunov inequalities.

Result: The approach demonstrated stable control and superior performance in simulations and hardware experiments, outperforming Gaussian-noise-based designs in error bounding and control quality.

Conclusion: The perception-aware MPC framework is effective in handling heavy-tailed uncertainties in perception modules, leading to improved safety and accuracy for feedback control in practical applications.

Abstract: This paper presents a robust model predictive control (MPC) framework that
explicitly addresses the non-Gaussian noise inherent in deep learning-based
perception modules used for state estimation. Recognizing that accurate
uncertainty quantification of the perception module is essential for safe
feedback control, our approach departs from the conventional assumption of
zero-mean noise quantification of the perception error. Instead, it employs
set-based state estimation with constrained zonotopes to capture biased,
heavy-tailed uncertainties while maintaining bounded estimation errors. To
improve computational efficiency, the robust MPC is reformulated as a linear
program (LP), using a Minkowski-Lyapunov-based cost function with an added
slack variable to prevent degenerate solutions. Closed-loop stability is
ensured through Minkowski-Lyapunov inequalities and contractive zonotopic
invariant sets. The largest stabilizing terminal set and its corresponding
feedback gain are then derived via an ellipsoidal approximation of the
zonotopes. The proposed framework is validated through both simulations and
hardware experiments on an omnidirectional mobile robot along with a camera and
a convolutional neural network-based perception module implemented within a
ROS2 framework. The results demonstrate that the perception-aware MPC provides
stable and accurate control performance under heavy-tailed noise conditions,
significantly outperforming traditional Gaussian-noise-based designs in terms
of both state estimation error bounding and overall control performance.

</details>


### [244] [Solving Robotics Tasks with Prior Demonstration via Exploration-Efficient Deep Reinforcement Learning](https://arxiv.org/abs/2509.04069)
*Chengyandan Shen,Christoffer Sloth*

Main category: cs.RO

TL;DR: This paper introduces DRLR, a framework combining demonstrations with deep reinforcement learning to enhance learning efficiency in robotics tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance exploration efficiency and mitigate bootstrapping error in reinforcement learning for robotics tasks, addressing challenges in policy convergence and sub-optimal performance.

Method: The DRLR framework improves the IBRL algorithm by modifying the action selection module to provide calibrated Q-values and replacing TD3 with SAC to avoid sub-optimal policy convergence.

Result: The framework effectively mitigates bootstrapping errors and prevents overfitting, achieving robustness across different state-action dimensions and varying demonstration qualities based on simulations and real-world experiments.

Conclusion: The results demonstrate that DRLR enhances exploration efficiency, robustness, and sim2real deployment potential in robotics tasks, successfully validating it in industrial applications.

Abstract: This paper proposes an exploration-efficient Deep Reinforcement Learning with
Reference policy (DRLR) framework for learning robotics tasks that incorporates
demonstrations. The DRLR framework is developed based on an algorithm called
Imitation Bootstrapped Reinforcement Learning (IBRL). We propose to improve
IBRL by modifying the action selection module. The proposed action selection
module provides a calibrated Q-value, which mitigates the bootstrapping error
that otherwise leads to inefficient exploration. Furthermore, to prevent the RL
policy from converging to a sub-optimal policy, SAC is used as the RL policy
instead of TD3. The effectiveness of our method in mitigating bootstrapping
error and preventing overfitting is empirically validated by learning two
robotics tasks: bucket loading and open drawer, which require extensive
interactions with the environment. Simulation results also demonstrate the
robustness of the DRLR framework across tasks with both low and high
state-action dimensions, and varying demonstration qualities. To evaluate the
developed framework on a real-world industrial robotics task, the bucket
loading task is deployed on a real wheel loader. The sim2real results validate
the successful deployment of the DRLR framework.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [245] [Comparative Evaluation of Large Language Models for Test-Skeleton Generation](https://arxiv.org/abs/2509.04644)
*Subhang Boorlagadda,Nitya Naga Sai Atluri,Muhammet Mustafa Olmez,Edward F. Gehringer*

Main category: cs.SE

TL;DR: The paper investigates how Large Language Models (LLMs) like GPT-4 and others can automate the creation of test skeletons for test-driven development (TDD). The evaluation shows strengths and weaknesses in model outputs, highlighting key factors like prompt design.


<details>
  <summary>Details</summary>
Motivation: Test skeletons are essential in TDD for systematic testing, but manually creating them is labor-intensive and prone to errors, especially in large-scale or educational projects.

Method: Four LLMs (GPT-4, DeepSeek-Chat, Llama4-Maverick, and Gemma2-9B) were evaluated on their ability to generate RSpec test skeletons for a real-world Ruby class. Assessment was done via static analysis and expert reviews to measure structural correctness, clarity, maintainability, and adherence to best practices.

Result: DeepSeek produced the most maintainable and well-structured skeletons, while GPT-4 generated more complete but conventionally inconsistent outputs. Contextual input and prompt design were found to critically impact output quality.

Conclusion: LLMs offer potential for automating test skeleton creation, but their effectiveness depends on model choice, input prompts, and context. DeepSeek is promising for maintainability, while context and prompt refinement are critical for achieving high-quality results.

Abstract: This paper explores the use of Large Language Models (LLMs) to automate the
generation of test skeletons -- structural templates that outline unit test
coverage without implementing full test logic. Test skeletons are especially
important in test-driven development (TDD), where they provide an early
framework for systematic verification. Traditionally authored manually, their
creation can be time-consuming and error-prone, particularly in educational or
large-scale development settings. We evaluate four LLMs -- GPT-4,
DeepSeek-Chat, Llama4-Maverick, and Gemma2-9B -- on their ability to generate
RSpec skeletons for a real-world Ruby class developed in a university software
engineering course. Each model's output is assessed using static analysis and a
blind expert review to measure structural correctness, clarity,
maintainability, and conformance to testing best practices. The study reveals
key differences in how models interpret code structure and testing conventions,
offering insights into the practical challenges of using LLMs for automated
test scaffolding. Our results show that DeepSeek generated the most
maintainable and well-structured skeletons, while GPT-4 produced more complete
but conventionally inconsistent output. The study reveals prompt design and
contextual input as key quality factors.

</details>


### [246] [Real-Time Performance Benchmarking of TinyML Models in Embedded Systems (PICO: Performance of Inference, CPU, and Operations)](https://arxiv.org/abs/2509.04721)
*Abhishek Dey,Saurabh Srivastava,Gaurav Singh,Robert G. Pettit*

Main category: cs.SE

TL;DR: The paper introduces PICO-TINYML-BENCHMARK, a modular framework for evaluating TinyML models' performance on various embedded systems, highlighting computational trade-offs.


<details>
  <summary>Details</summary>
Motivation: To address the need for systematic benchmarking of TinyML models in real-world embedded systems environments and to guide optimization strategies.

Method: Using the PICO-TINYML-BENCHMARK framework, three TinyML models were tested on BeagleBone AI64 and Raspberry Pi 4 platforms with real-world datasets to measure metrics like inference latency and resource utilization.

Result: BeagleBone AI64 showed consistent inference latency for AI tasks, whereas Raspberry Pi 4 proved to be more resource-efficient and cost-effective.

Conclusion: The study provides valuable insights for optimizing TinyML implementations and bridging gaps between theoretical models and practical embedded applications.

Abstract: This paper presents PICO-TINYML-BENCHMARK, a modular and platform-agnostic
framework for benchmarking the real-time performance of TinyML models on
resource-constrained embedded systems. Evaluating key metrics such as inference
latency, CPU utilization, memory efficiency, and prediction stability, the
framework provides insights into computational trade-offs and platform-specific
optimizations. We benchmark three representative TinyML models -- Gesture
Classification, Keyword Spotting, and MobileNet V2 -- on two widely adopted
platforms, BeagleBone AI64 and Raspberry Pi 4, using real-world datasets.
Results reveal critical trade-offs: the BeagleBone AI64 demonstrates consistent
inference latency for AI-specific tasks, while the Raspberry Pi 4 excels in
resource efficiency and cost-effectiveness. These findings offer actionable
guidance for optimizing TinyML deployments, bridging the gap between
theoretical advancements and practical applications in embedded systems.

</details>


### [247] [NovaQ: Improving Quantum Program Testing through Diversity-Guided Test Case Generation](https://arxiv.org/abs/2509.04763)
*Tiancheng Jin,Shangzhou Xia,Jianjun Zhao*

Main category: cs.SE

TL;DR: NovaQ is a testing framework for quantum programs that enhances test input diversity and detects bugs effectively.


<details>
  <summary>Details</summary>
Motivation: To ensure the reliability of quantum programs as quantum computing advances.

Method: NovaQ utilizes a test case generator for diverse state inputs and an evaluation module measuring novelty based on quantum metrics like magnitude, phase, and entanglement.

Result: NovaQ achieves higher input diversity and uncovers more bugs compared to existing methods across varying quantum program sizes and complexities.

Conclusion: NovaQ provides an efficient and robust solution to improve quantum program testing by expanding behavioral exploration and increasing bug detection.

Abstract: Quantum programs are designed to run on quantum computers, leveraging quantum
circuits to solve problems that are intractable for classical machines. As
quantum computing advances, ensuring the reliability of quantum programs has
become increasingly important. This paper introduces NovaQ, a diversity-guided
testing framework for quantum programs. NovaQ combines a distribution-based
test case generator with a novelty-driven evaluation module. The generator
produces diverse quantum state inputs by mutating circuit parameters, while the
evaluator quantifies behavioral novelty based on internal circuit state
metrics, including magnitude, phase, and entanglement. By selecting inputs that
map to infrequently covered regions in the metric space, NovaQ effectively
explores under-tested program behaviors. We evaluate NovaQ on quantum programs
of varying sizes and complexities. Experimental results show that NovaQ
consistently achieves higher test input diversity and detects more bugs than
existing baseline approaches.

</details>


### [248] [Code Review Without Borders: Evaluating Synthetic vs. Real Data for Review Recommendation](https://arxiv.org/abs/2509.04810)
*Yogev Cohen,Dudi Ohayon,Romy Somkin,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.SE

TL;DR: The paper leverages Large Language Models (LLMs) to generate synthetic labeled data for low-resource languages, enabling automated code review systems.


<details>
  <summary>Details</summary>
Motivation: Modern software development workflows struggle with maintaining code review quality in emerging languages due to insufficient labeled data.

Method: The authors use LLMs to translate code changes into new languages and create synthetic labeled training data, which is then used to train supervised classifiers.

Result: Experiments demonstrate that classifiers trained on LLM-generated data effectively narrow the performance gap compared to those trained on real labeled data.

Conclusion: LLMs enable scalable and effective solutions for bootstrapping automated code review systems in low-resource and emerging programming languages.

Abstract: Automating the decision of whether a code change requires manual review is
vital for maintaining software quality in modern development workflows.
However, the emergence of new programming languages and frameworks creates a
critical bottleneck: while large volumes of unlabelled code are readily
available, there is an insufficient amount of labelled data to train supervised
models for review classification. We address this challenge by leveraging Large
Language Models (LLMs) to translate code changes from well-resourced languages
into equivalent changes in underrepresented or emerging languages, generating
synthetic training data where labelled examples are scarce. We assume that
although LLMs have learned the syntax and semantics of new languages from
available unlabelled code, they have yet to fully grasp which code changes are
considered significant or review-worthy within these emerging ecosystems. To
overcome this, we use LLMs to generate synthetic change examples and train
supervised classifiers on them. We systematically compare the performance of
these classifiers against models trained on real labelled data. Our experiments
across multiple GitHub repositories and language pairs demonstrate that
LLM-generated synthetic data can effectively bootstrap review recommendation
systems, narrowing the performance gap even in low-resource settings. This
approach provides a scalable pathway to extend automated code review
capabilities to rapidly evolving technology stacks, even in the absence of
annotated data.

</details>


### [249] [Integrating Large Language Models in Software Engineering Education: A Pilot Study through GitHub Repositories Mining](https://arxiv.org/abs/2509.04877)
*Maryam Khan,Muhammad Azeem Akbar,Jussi Kasurinen*

Main category: cs.SE

TL;DR: This paper explores the motivators and demotivators for integrating Large Language Models (LLMs) like ChatGPT into software engineering education through the analysis of GitHub repositories.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for systematic investigation into the responsible integration of LLMs into software engineering curricula.

Method: A pilot repository mining study examining 400 GitHub projects by analyzing README files and issues discussions.

Result: Identified key motivators (e.g., engagement and programming support) and demotivators (e.g., plagiarism concerns and over-reliance on AI) associated with LLM integration into SE education.

Conclusion: The study validates preliminary taxonomies of motivators/demotivators, provides empirical insights, and sets the stage for creating a responsible framework for LLM adoption in SE education.

Abstract: Context: Large Language Models (LLMs) such as ChatGPT are increasingly
adopted in software engineering (SE) education, offering both opportunities and
challenges. Their adoption requires systematic investigation to ensure
responsible integration into curricula. Objective: This doctoral research aims
to develop a validated framework for integrating LLMs into SE education through
a multi-phase process, including taxonomies development, empirical
investigation, and case studies. This paper presents the first empirical step.
Method: We conducted a pilot repository mining study of 400 GitHub projects,
analyzing README files and issues discussions to identify the presence of
motivator and demotivator previously synthesized in our literature review [ 8]
study. Results: Motivators such as engagement and motivation (227 hits),
software engineering process understanding (133 hits), and programming
assistance and debugging support (97 hits) were strongly represented.
Demotivators, including plagiarism and IP concerns (385 hits), security,
privacy and data integrity (87 hits), and over-reliance on AI in learning (39
hits), also appeared prominently. In contrast, demotivators such as challenges
in evaluating learning outcomes and difficulty in curriculum redesign recorded
no hits across the repositories. Conclusion: The study provides early empirical
validation of motivators/demotivators taxonomies with respect to their themes,
highlights research practice gaps, and lays the foundation for developing a
comprehensive framework to guide the responsible adoption of LLMs in SE
education.

</details>


### [250] [FuzzRDUCC: Fuzzing with Reconstructed Def-Use Chain Coverage](https://arxiv.org/abs/2509.04967)
*Kai Feng,Jeremy Singer,Angelos K Marnerides*

Main category: cs.SE

TL;DR: This paper presents FuzzRDUCC, a novel binary-only fuzzing framework that employs symbolic execution to integrate dataflow analysis, enhancing vulnerability detection.


<details>
  <summary>Details</summary>
Motivation: Traditional grey-box fuzzers rely on control flow analysis, which may miss bugs related to dataflows and internal program behavior.

Method: FuzzRDUCC uses symbolic execution to reconstruct definition-use chains from binary executables and employs a heuristic algorithm to select crucial dataflow paths.

Result: FuzzRDUCC was evaluated using the binutils benchmark and identified unique crashes overlooked by state-of-the-art fuzzers.

Conclusion: FuzzRDUCC proves to be an effective and efficient solution for enhancing vulnerability detection by incorporating dataflow analysis into binary-only fuzzing.

Abstract: Binary-only fuzzing often struggles with achieving thorough code coverage and
uncovering hidden vulnerabilities due to limited insight into a program's
internal dataflows. Traditional grey-box fuzzers guide test case generation
primarily using control flow edge coverage, which can overlook bugs not easily
exposed through control flow analysis alone. We argue that integrating dataflow
analysis into the fuzzing process can enhance its effectiveness by revealing
how data propagates through the program, thereby enabling the exploration of
execution paths that control flow-based methods might miss. In this context, we
introduce FuzzRDUCC, a novel fuzzing framework that employs symbolic execution
to reconstruct definition-use (def-use) chains directly from binary
executables. FuzzRDUCC identifies crucial dataflow paths and exposes security
vulnerabilities without incurring excessive computational overhead, due to a
novel heuristic algorithm that selects relevant def-use chains without
affecting the thoroughness of the fuzzing process. We evaluate FuzzRDUCC using
the binutils benchmark and demonstrate that it can identify unique crashes not
found by state-of-the-art fuzzers. Hence, establishing FuzzRDUCC as a feasible
solution for next generation vulnerability detection and discovery mechanisms.

</details>


### [251] [GenAI-based test case generation and execution in SDV platform](https://arxiv.org/abs/2509.05112)
*Denesa Zyberaj,Lukasz Mazur,Nenad Petrovic,Pankhuri Verma,Pascal Hirmer,Dirk Slama,Xiangwei Cheng,Alois Knoll*

Main category: cs.SE

TL;DR: This paper proposes an approach using GenAI to transform natural language and system diagrams into test cases, highlighting improved efficiency in automotive testing processes.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address inefficiencies in test case generation for automotive systems by leveraging automation and standardization.

Method: Harnesses Generative AI models to convert requirements and diagrams into Gherkin test cases, integrates Vehicle Signal Specification modeling, and tests within the digital.auto playground.

Result: The approach showed reduced manual effort in test specification and rapid test execution in the Child Presence Detection System use case.

Conclusion: While automation proves effective, manual intervention is still required due to current GenAI limitations and platform constraints.

Abstract: This paper introduces a GenAI-driven approach for automated test case
generation, leveraging Large Language Models and Vision-Language Models to
translate natural language requirements and system diagrams into structured
Gherkin test cases. The methodology integrates Vehicle Signal Specification
modeling to standardize vehicle signal definitions, improve compatibility
across automotive subsystems, and streamline integration with third-party
testing tools. Generated test cases are executed within the digital.auto
playground, an open and vendor-neutral environment designed to facilitate rapid
validation of software-defined vehicle functionalities. We evaluate our
approach using the Child Presence Detection System use case, demonstrating
substantial reductions in manual test specification effort and rapid execution
of generated tests. Despite significant automation, the generation of test
cases and test scripts still requires manual intervention due to current
limitations in the GenAI pipeline and constraints of the digital.auto platform.

</details>


### [252] [AI Agents for Web Testing: A Case Study in the Wild](https://arxiv.org/abs/2509.05197)
*Naimeng Ye,Xiao Yu,Ruize Xu,Tianyi Peng,Zhou Yu*

Main category: cs.SE

TL;DR: WebProber is an AI-based web testing framework that autonomously explores websites to identify usability issues more effectively than traditional tools.


<details>
  <summary>Details</summary>
Motivation: Traditional web testing tools fail to fully capture complex user behaviors and usability problems, necessitating more advanced methods.

Method: This paper introduces WebProber, an AI agent framework that simulates real user interactions on websites to identify bugs and usability issues.

Result: WebProber successfully identified 29 usability issues across 120 academic personal websites, outperforming traditional tools.

Conclusion: Agent-based testing like WebProber shows promise for enhancing user-centered web testing frameworks, advancing the field significantly.

Abstract: Automated web testing plays a critical role in ensuring high-quality user
experiences and delivering business value. Traditional approaches primarily
focus on code coverage and load testing, but often fall short of capturing
complex user behaviors, leaving many usability issues undetected. The emergence
of large language models (LLM) and AI agents opens new possibilities for web
testing by enabling human-like interaction with websites and a general
awareness of common usability problems. In this work, we present WebProber, a
prototype AI agent-based web testing framework. Given a URL, WebProber
autonomously explores the website, simulating real user interactions,
identifying bugs and usability issues, and producing a human-readable report.
We evaluate WebProber through a case study of 120 academic personal websites,
where it uncovered 29 usability issues--many of which were missed by
traditional tools. Our findings highlight agent-based testing as a promising
direction while outlining directions for developing next-generation,
user-centered testing frameworks.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [253] [Mechanisms for anesthesia, unawareness, respiratory depression, memory replay and sleep: MHb > IPN > PAG + DRN + MRN > claustrum > cortical slow-waves](https://arxiv.org/abs/2509.04454)
*Karin Vadovičová*

Main category: q-bio.NC

TL;DR: The paper proposes a novel model connecting the medial habenula-interpeduncular nucleus (MHb-IPN) circuit to loss of awareness, anesthesia, memory replay, opioids-induced respiratory depression (OIRD), and slow-wave sleep (SWS).


<details>
  <summary>Details</summary>
Motivation: The motivation lies in understanding the neural mechanisms underlying key states such as anesthesia, memory replay, and slow-wave sleep, especially to elucidate the role of the MHb-IPN circuit and its relevance to various brain functions and pharmacological effects.

Method: The author utilizes diffusion tensor imaging (DTI) to map hippocampus and amygdala connectivity to the medial habenula, alongside analyzing past findings on glucose intake, receptor activation, and neural pathways.

Result: The findings suggest that the MHb-IPN circuit is crucial in connecting DG -> posterior septum -> MHb -> IPN -> MRN -> hippocampus and beyond, explaining memory replay, loss of awareness, SWS, and effects of various anesthetics and psychedelics.

Conclusion: This neural circuit model has implications for understanding the effects of anesthetics, opioids, psychedelics, and other substances, offering insights into mechanisms that promote rest, recovery, and brain repair.

Abstract: My findings show, for the first time, what causes loss of awareness,
anesthesia, memory replay, opioids induced respiratory depression (OIRD), and
slow wave sleep. Opiates are fast pain relievers and anesthetics that can cause
respiratory arrest. I found how mu-opioids and other medial habenula activators
slowdown respiration during SWS and anesthesia. Using DTI method I observed
that human hippocampus is connected to MHb via posterior septum, while amygdala
via anteromedial BNST. MHb projected to pineal gland and contralateral MHb
(Vadovi\v{c}ov\'a, 2014). MHb has dense mu-opioids receptors (Gardon and Faget,
2014) and strong projections to IPN. Herkenham (1981) found increased glucose
intake during anesthesia in MHb and IPN. The IPN projects to serotonergic
MRN/DRN, and pain/interoception/arousal linked PAG. The question is: What is
the MHb-IPN circuit doing? This extended circuit model explains role of the
dentate gyrus >posterior septum >MHb >IPN >MRN >hippocampus + BF + claustrum
>cortical slow-wave activity (SWA) in memory replay, loss of awareness,
anesthesia and SWS. It proposes new neural mechanisms for anesthetic ketamine,
nitrous oxide, and phencyclidine effects: activation of the IPN >MRN >claustrum
>cortical SWA circuit by the 5-HT2a receptors in the IPN and claustrum. This
brain model shows why are ketamine and psychedelics anxiolytic and
antidepressant. How they by activating the 5-HT2a receptors in vACC/infralimbic
cortex increase safety, well-being signal, socializing, and cognitive
flexibility, and attenuate fear, worries, anger, impulsivity, self-defence and
wanting. This model suggests that mu-opioids, acetylcholine, nicotine,
cannabinoids, adenosine, GLP-1RA, neuropeptide Y, and substance P activate the
MHb-IPN-MRN circuit which promotes rest, recovery, repair,
serotonin-BDNF-proteins production-spines/synapses growth-anti-inflammatory
state.

</details>


### [254] [Network Models of Neurodegeneration: Bridging Neuronal Dynamics and Disease Progression](https://arxiv.org/abs/2509.05151)
*Christoffer G. Alexandersen,Georgia S. Brennan,Julia K. Brynildsen,Michael X. Henderson,Yasser Iturria-Medina,Dani S. Bassett*

Main category: q-bio.NC

TL;DR: The paper argues for integrating computational models of neuronal dynamics and biological disease mechanisms to better understand and intervene in neurodegenerative diseases.


<details>
  <summary>Details</summary>
Motivation: Current computational models separately address neuronal dynamics or biological mechanisms of neurodegeneration, limiting comprehensive understanding of the interconnected processes.

Method: The paper examines and reviews two modeling approaches: neural mass/whole-brain frameworks and biological models focused on prion-like propagation, glial responses, and vascular mechanisms.

Result: The review highlights existing efforts to unify these approaches and suggests that integration could improve the understanding of disease progression and lead to more effective interventions.

Conclusion: Unified modeling frameworks are essential for answering critical questions in neurodegeneration and for developing strategies to slow disease progression and restore brain function.

Abstract: Neurodegenerative diseases are characterized by the accumulation of misfolded
proteins and widespread disruptions in brain function. Computational modeling
has advanced our understanding of these processes, but efforts have
traditionally focused on either neuronal dynamics or the underlying biological
mechanisms of disease. One class of models uses neural mass and whole-brain
frameworks to simulate changes in oscillations, connectivity, and network
stability. A second class focuses on biological processes underlying disease
progression, particularly prion-like propagation through the connectome, and
glial responses and vascular mechanisms. Each modeling tradition has provided
important insights, but experimental evidence shows these processes are
interconnected: neuronal activity modulates protein release and clearance,
while pathological burden feeds back to disrupt circuit function. Modeling
these domains in isolation limits our understanding. To determine where and why
disease emerges, how it spreads, and how it might be altered, we must develop
integrated frameworks that capture feedback between neuronal dynamics and
disease biology. In this review, we survey the two modeling approaches and
highlight efforts to unify them. We argue that such integration is necessary to
address key questions in neurodegeneration and to inform interventions, from
targeted stimulation to control-theoretic strategies that slow progression and
restore function.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [255] [Any-Step Density Ratio Estimation via Interval-Annealed Secant Alignment](https://arxiv.org/abs/2509.04852)
*Wei Chen,Shigui Li,Jiacheng Li,Jian Xu,Zhiqi Lin,Junmei Yang,Delu Zeng,John Paisley,Qibin Zhao*

Main category: stat.ML

TL;DR: The paper introduces a novel density ratio estimation method, ISA-DRE, which uses a global secant function instead of infinitesimal tangents, improving accuracy and efficiency without numerical integration.


<details>
  <summary>Details</summary>
Motivation: Existing density ratio estimation methods struggle to balance accuracy with computational efficiency, especially in real-time applications.

Method: The proposed method, ISA-DRE, leverages a Secant Alignment Identity that connects secant and tangent representations, and adopts Contraction Interval Annealing to stabilize training by gradually expanding alignment intervals.

Result: ISA-DRE demonstrates competitive accuracy with significantly fewer function evaluations compared to previous approaches, speeding up inference.

Conclusion: The ISA-DRE framework offers an efficient and accurate solution for density ratio estimation, making it practical for real-time and interactive machine learning applications.

Abstract: Estimating density ratios is a fundamental problem in machine learning, but
existing methods often trade off accuracy for efficiency. We propose
\textit{Interval-annealed Secant Alignment Density Ratio Estimation (ISA-DRE)},
a framework that enables accurate, any-step estimation without numerical
integration.
  Instead of modeling infinitesimal tangents as in prior methods, ISA-DRE
learns a global secant function, defined as the expectation of all tangents
over an interval, with provably lower variance, making it more suitable for
neural approximation. This is made possible by the \emph{Secant Alignment
Identity}, a self-consistency condition that formally connects the secant with
its underlying tangent representations.
  To mitigate instability during early training, we introduce \emph{Contraction
Interval Annealing}, a curriculum strategy that gradually expands the alignment
interval during training. This process induces a contraction mapping, which
improves convergence and training stability.
  Empirically, ISA-DRE achieves competitive accuracy with significantly fewer
function evaluations compared to prior methods, resulting in much faster
inference and making it well suited for real-time and interactive applications.

</details>


### [256] [Optimal Variance and Covariance Estimation under Differential Privacy in the Add-Remove Model and Beyond](https://arxiv.org/abs/2509.04919)
*Shokichi Takakura,Seng Pei Liew,Satoshi Hasegawa*

Main category: stat.ML

TL;DR: The paper introduces a framework for estimating variance and covariance under differential privacy in the add-remove model, leveraging the novel Bézier mechanism for better utility and minimax optimality.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenges in estimating variance and covariance privately in the add-remove model, a less-explored domain where dataset size privacy is required.

Method: The authors propose mechanisms based on the Bézier mechanism, a moment-release framework using Bernstein bases, and establish minimax optimality and instance-wise utility improvements.

Result: The Bézier mechanism is proven minimax optimal in high-privacy scenarios and provides superior utility compared to alternative approaches. It is also applicable to other statistical tasks.

Conclusion: The Bézier mechanism effectively handles statistical estimation challenges under differential privacy, expanding its utility beyond just variance and covariance estimation.

Abstract: In this paper, we study the problem of estimating the variance and covariance
of datasets under differential privacy in the add-remove model. While
estimation in the swap model has been extensively studied in the literature,
the add-remove model remains less explored and more challenging, as the dataset
size must also be kept private. To address this issue, we develop efficient
mechanisms for variance and covariance estimation based on the \emph{B\'{e}zier
mechanism}, a novel moment-release framework that leverages Bernstein bases. We
prove that our proposed mechanisms are minimax optimal in the high-privacy
regime by establishing new minimax lower bounds. Moreover, beyond worst-case
scenarios, we analyze instance-wise utility and show that the B\'{e}zier-based
estimator consistently achieves better utility compared to alternative
mechanisms. Finally, we demonstrate the effectiveness of the B\'{e}zier
mechanism beyond variance and covariance estimation, showcasing its
applicability to other statistical tasks.

</details>


### [257] [Spectral Algorithms in Misspecified Regression: Convergence under Covariate Shift](https://arxiv.org/abs/2509.05106)
*Ren-Rui Liu,Zheng-Chu Guo*

Main category: stat.ML

TL;DR: This paper studies spectral algorithms under covariate shift and analyzes their theoretical performance in challenging misspecified settings.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges posed by covariate shift and model misspecification, which limit the applicability of classical learning theories.

Method: The paper introduces weighted spectral algorithms with importance weights in an RKHS framework, and develops a truncation technique for unbounded importance weights.

Result: It establishes minimax-optimal convergence rates for scenarios with bounded density ratios and near-optimal rates with unbounded importance weights, even in misspecified cases.

Conclusion: This work extends kernel learning theory to practical scenarios by systematically analyzing the interaction between covariate shift and model misspecification.

Abstract: This paper investigates the convergence properties of spectral algorithms --
a class of regularization methods originating from inverse problems -- under
covariate shift. In this setting, the marginal distributions of inputs differ
between source and target domains, while the conditional distribution of
outputs given inputs remains unchanged. To address this distributional
mismatch, we incorporate importance weights, defined as the ratio of target to
source densities, into the learning framework. This leads to a weighted
spectral algorithm within a nonparametric regression setting in a reproducing
kernel Hilbert space (RKHS). More importantly, in contrast to prior work that
largely focuses on the well-specified setting, we provide a comprehensive
theoretical analysis of the more challenging misspecified case, in which the
target function does not belong to the RKHS. Under the assumption of uniformly
bounded density ratios, we establish minimax-optimal convergence rates when the
target function lies within the RKHS. For scenarios involving unbounded
importance weights, we introduce a novel truncation technique that attains
near-optimal convergence rates under mild regularity conditions, and we further
extend these results to the misspecified regime. By addressing the intertwined
challenges of covariate shift and model misspecification, this work extends
classical kernel learning theory to more practical scenarios, providing a
systematic framework for understanding their interaction.

</details>


### [258] [Probabilistic operator learning: generative modeling and uncertainty quantification for foundation models of differential equations](https://arxiv.org/abs/2509.05186)
*Benjamin J. Zhang,Siting Liu,Stanley J. Osher,Markos A. Katsoulakis*

Main category: stat.ML

TL;DR: The paper introduces In-Context Operator Networks (ICON) as a learning method using Bayesian inference for solving differential equations (ODEs and PDEs), emphasizing its probabilistic and generative capabilities.


<details>
  <summary>Details</summary>
Motivation: To address challenges in learning solution operators for differential equations by leveraging probabilistic frameworks and foundation model architectures.

Method: ICON uses foundation model architectures to learn mappings between initial conditions and solution operators. It adopts a Bayesian inference approach and extends the method to generative settings (GenICON) for sampling posterior distributions.

Result: ICON implicitly performs Bayesian inference to predict solutions. GenICON enables uncertainty quantification and enhances operator learning capabilities by accounting for underlying uncertainties.

Conclusion: The probabilistic framework clarifies ICON's operations and offers a pathway for extending operator learning methods to generative and uncertainty-aware solutions.

Abstract: In-context operator networks (ICON) are a class of operator learning methods
based on the novel architectures of foundation models. Trained on a diverse set
of datasets of initial and boundary conditions paired with corresponding
solutions to ordinary and partial differential equations (ODEs and PDEs), ICON
learns to map example condition-solution pairs of a given differential equation
to an approximation of its solution operator. Here, we present a probabilistic
framework that reveals ICON as implicitly performing Bayesian inference, where
it computes the mean of the posterior predictive distribution over solution
operators conditioned on the provided context, i.e., example condition-solution
pairs. The formalism of random differential equations provides the
probabilistic framework for describing the tasks ICON accomplishes while also
providing a basis for understanding other multi-operator learning methods. This
probabilistic perspective provides a basis for extending ICON to
\emph{generative} settings, where one can sample from the posterior predictive
distribution of solution operators. The generative formulation of ICON
(GenICON) captures the underlying uncertainty in the solution operator, which
enables principled uncertainty quantification in the solution predictions in
operator learning.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [259] [DarkStream: real-time speech anonymization with low latency](https://arxiv.org/abs/2509.04667)
*Waris Quamer,Ricardo Gutierrez-Osuna*

Main category: eess.AS

TL;DR: DarkStream delivers a real-time speech synthesis model that anonymizes speaker identity while maintaining linguistic intelligibility and low latency.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns in real-time speech communication by developing a system that anonymizes speakers effectively while preserving the intelligibility of speech.

Method: DarkStream employs a causal waveform encoder, transformer-based contextual layers, and a short lookahead buffer for content encoding. It uses a neural vocoder for waveform generation and GAN-generated pseudo-speaker embeddings for anonymization.

Result: The model achieves strong anonymization with speaker verification EER close to 50% and maintains linguistic intelligibility with a WER within 9%.

Conclusion: DarkStream presents a practical solution for low-latency, privacy-preserving speech communication, balancing robust identity anonymization and minimal intelligibility degradation.

Abstract: We propose DarkStream, a streaming speech synthesis model for real-time
speaker anonymization. To improve content encoding under strict latency
constraints, DarkStream combines a causal waveform encoder, a short lookahead
buffer, and transformer-based contextual layers. To further reduce inference
time, the model generates waveforms directly via a neural vocoder, thus
removing intermediate mel-spectrogram conversions. Finally, DarkStream
anonymizes speaker identity by injecting a GAN-generated pseudo-speaker
embedding into linguistic features from the content encoder. Evaluations show
our model achieves strong anonymization, yielding close to 50% speaker
verification EER (near-chance performance) on the lazy-informed attack
scenario, while maintaining acceptable linguistic intelligibility (WER within
9%). By balancing low-latency, robust privacy, and minimal intelligibility
degradation, DarkStream provides a practical solution for privacy-preserving
real-time speech communication.

</details>


### [260] [Lightweight DNN for Full-Band Speech Denoising on Mobile Devices: Exploiting Long and Short Temporal Patterns](https://arxiv.org/abs/2509.05079)
*Konstantinos Drossos,Mikko Heikkinen,Paschalis Tsiaflakis*

Main category: eess.AS

TL;DR: This paper presents a lightweight, low-latency DNN-based speech denoising method optimized for mobile platforms and works on full-band signals.


<details>
  <summary>Details</summary>
Motivation: Modern speech denoising methods aren't optimized for resource-constrained platforms, full-band signals, or low latency.

Method: The method uses a modified UNet architecture with features like look-back frames, temporal spanning of convolutional kernels, and recurrent neural networks.

Result: Achieved superior SI-SDR values compared to existing methods and demonstrated real-time processing on mobile devices with a latency below 0.02.

Conclusion: The proposed method is effective for mobile deployment, balancing low latency and high-quality speech denoising using innovative architectural modifications.

Abstract: Speech denoising (SD) is an important task of many, if not all, modern signal
processing chains used in devices and for everyday-life applications. While
there are many published and powerful deep neural network (DNN)-based methods
for SD, few are optimized for resource-constrained platforms such as mobile
devices. Additionally, most DNN-based methods for SD are not focusing on
full-band (FB) signals, i.e. having 48 kHz sampling rate, and/or low latency
cases. In this paper we present a causal, low latency, and lightweight
DNN-based method for full-band SD, leveraging both short and long temporal
patterns. The method is based on a modified UNet architecture employing
look-back frames, temporal spanning of convolutional kernels, and recurrent
neural networks for exploiting short and long temporal patterns in the signal
and estimated denoising mask. The DNN operates on a causal frame-by-frame basis
taking as an input the STFT magnitude, utilizes inverted bottlenecks inspired
by MobileNet, employs causal instance normalization for channel-wise
normalization, and achieves a real-time factor below 0.02 when deployed on a
modern mobile phone. The proposed method is evaluated using established speech
denoising metrics and publicly available datasets, demonstrating its
effectiveness in achieving an (SI-)SDR value that outperforms existing FB and
low latency SD methods.

</details>


### [261] [Room-acoustic simulations as an alternative to measurements for audio-algorithm evaluation](https://arxiv.org/abs/2509.05175)
*Georg Götz,Daniel Gert Nielsen,Steinar Guðjónsson,Finnur Pind*

Main category: eess.AS

TL;DR: The paper evaluates the effectiveness of using room-acoustic simulations instead of costly, real-world measurements for assessing audio-signal-processing and audio-machine-learning (ASP/AML) algorithms.


<details>
  <summary>Details</summary>
Motivation: Real-world datasets for evaluating ASP/AML algorithms are costly and limited in diversity; using simulations could offer a more accessible alternative.

Method: The authors evaluated three ASP/AML algorithms with results from physical room-acoustic measurements and compared them with simulations using a numerical wave-based solver and two geometrical acoustics simulators.

Result: Numerical wave-based simulations produced results consistent with physical measurements, but geometrical acoustic simulations were less reliable in replicating these measured outcomes.

Conclusion: Room-acoustic simulations, particularly numerical wave-based solvers, can be effective alternatives to real-world measurements for ASP/AML algorithm evaluation.

Abstract: Audio-signal-processing and audio-machine-learning (ASP/AML) algorithms are
ubiquitous in modern technology like smart devices, wearables, and
entertainment systems. Development of such algorithms and models typically
involves a formal evaluation to demonstrate their effectiveness and progress
beyond the state-of-the-art. Ideally, a thorough evaluation should cover many
diverse application scenarios and room-acoustic conditions. However, in
practice, evaluation datasets are often limited in size and diversity because
they rely on costly and time-consuming measurements. This paper explores how
room-acoustic simulations can be used for evaluating ASP/AML algorithms. To
this end, we evaluate three ASP/AML algorithms with room-acoustic measurements
and data from different simulation engines, and assess the match between the
evaluation results obtained from measurements and simulations. The presented
investigation compares a numerical wave-based solver with two geometrical
acoustics simulators. While numerical wave-based simulations yielded similar
evaluation results as measurements for all three evaluated ASP/AML algorithms,
geometrical acoustic simulations could not replicate the measured evaluation
results as reliably.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [262] [Evaluating Cognitive-Behavioral Fixation via Multimodal User Viewing Patterns on Social Media](https://arxiv.org/abs/2509.04823)
*Yujie Wang,Yunwei Zhao,Jing Yang,Han Han,Shiguang Shan,Jie Zhang*

Main category: cs.SI

TL;DR: The paper introduces a framework to computationally assess cognitive-behavioral fixation in social media users through multimodal analysis techniques.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in methods for computationally detecting and evaluating cognitive-behavioral fixation, which has been largely studied in psychology but underexplored computationally.

Method: The paper presents a two-module framework comprising a multimodal topic extraction module and a fixation quantification module for adaptive and hierarchical evaluation of social media user behavior.

Result: Experiments conducted on established benchmarks and a newly created multimodal dataset showcase the effectiveness of the proposed approach.

Conclusion: The study offers a scalable computational framework for analyzing cognitive-behavioral fixation and provides publicly available resources for further research.

Abstract: Digital social media platforms frequently contribute to cognitive-behavioral
fixation, a phenomenon in which users exhibit sustained and repetitive
engagement with narrow content domains. While cognitive-behavioral fixation has
been extensively studied in psychology, methods for computationally detecting
and evaluating such fixation remain underexplored. To address this gap, we
propose a novel framework for assessing cognitive-behavioral fixation by
analyzing users' multimodal social media engagement patterns. Specifically, we
introduce a multimodal topic extraction module and a cognitive-behavioral
fixation quantification module that collaboratively enable adaptive,
hierarchical, and interpretable assessment of user behavior. Experiments on
existing benchmarks and a newly curated multimodal dataset demonstrate the
effectiveness of our approach, laying the groundwork for scalable computational
analysis of cognitive fixation. All code in this project is publicly available
for research purposes at
https://github.com/Liskie/cognitive-fixation-evaluation.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [263] [AI-Driven Fronthaul Link Compression in Wireless Communication Systems: Review and Method Design](https://arxiv.org/abs/2509.04805)
*Keqin Zhang*

Main category: eess.SP

TL;DR: The paper investigates AI-driven signal compression techniques for modern fronthaul links, focusing on solutions for high-efficiency and low-latency wireless communication.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of transporting high-dimensional signals in wireless systems under stringent bandwidth and latency constraints and improve compression techniques.

Method: The paper surveys AI-driven compression techniques and analyzes two approaches: CSI feedback using end-to-end learning and RB granularity precoding optimization combined with compression.

Result: A tailored fronthaul compression strategy for cell-free architectures is proposed, achieving high compression, controlled performance loss, RB-level rate adaptation, and low-latency inference.

Conclusion: AI-driven compression techniques enhance wireless communication efficiency and are essential for enabling centralized cooperative transmission in next-gen networks.

Abstract: Modern fronthaul links in wireless systems must transport high-dimensional
signals under stringent bandwidth and latency constraints, which makes
compression indispensable. Traditional strategies such as compressed sensing,
scalar quantization, and fixed-codec pipelines often rely on restrictive
priors, degrade sharply at high compression ratios, and are hard to tune across
channels and deployments. Recent progress in Artificial Intelligence (AI) has
brought end-to-end learned transforms, vector and hierarchical quantization,
and learned entropy models that better exploit the structure of Channel State
Information(CSI), precoding matrices, I/Q samples, and LLRs. This paper first
surveys AI-driven compression techniques and then provides a focused analysis
of two representative high-compression routes: CSI feedback with end-to-end
learning and Resource Block (RB) granularity precoding optimization combined
with compression. Building on these insights, we propose a fronthaul
compression strategy tailored to cell-free architectures. The design targets
high compression with controlled performance loss, supports RB-level rate
adaptation, and enables low-latency inference suitable for centralized
cooperative transmission in next-generation networks.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [264] [Multiscale Graph Neural Network for Turbulent Flow-Thermal Prediction Around a Complex-Shaped Pin-Fin](https://arxiv.org/abs/2509.04463)
*Riddhiman Raut,Evan M. Mihalko,Amrita Basak*

Main category: physics.flu-dyn

TL;DR: The paper develops a Graph Neural Network (GNN) to efficiently predict flow and thermal behavior in complex geometries with high accuracy, reducing simulation time significantly.


<details>
  <summary>Details</summary>
Motivation: Simulating turbulent flows in complex geometries is computationally intensive. The study seeks to create a faster, accurate, and reliable surrogate model for such simulations using Graph Neural Networks.

Method: A dataset was created by parameterizing pin-fin geometries and generating simulations via ANSYS Fluent. These simulations were converted into graph structures, which were input into the developed GNN, trained to predict temperature, velocity, and pressure.

Result: The GNN predicted thermal and flow fields with high accuracy, capturing crucial flow features and reducing computation time by 2-3 orders of magnitude compared to traditional methods.

Conclusion: The proposed GNN serves as an efficient and accurate surrogate model for flow simulations in complex geometries, demonstrating potential for broad applications in computational fluid dynamics.

Abstract: This study presents the development of a domain-responsive edge-aware
multiscale Graph Neural Network for predicting steady, turbulent flow and
thermal behavior in a two-dimensional channel containing arbitrarily shaped
complex pin-fin geometries. The training dataset was constructed through an
automated framework that integrated geometry generation, meshing, and
flow-field solutions in ANSYS Fluent. The pin-fin geometry was parameterized
using piecewise cubic splines, producing 1,000 diverse configurations through
Latin Hypercube Sampling. Each simulation was converted into a graph structure,
where nodes carried a feature vector containing spatial coordinates, a
normalized streamwise position, one-hot boundary indicators, and a signed
distance to the nearest boundary such as wall. This graph structure served as
input to the newly developed Graph Neural Network, which was trained to predict
temperature, velocity magnitude, and pressure at each node using data from
ANSYS. The network predicted fields with outstanding accuracy, capturing
boundary layers, recirculation, and the stagnation region upstream of the
pin-fins while reducing wall time by 2-3 orders of magnitude. In conclusion,
the novel graph neural network offered a fast and reliable surrogate for
simulations in complex flow configurations.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [265] [High-Resolution Global Land Surface Temperature Retrieval via a Coupled Mechanism-Machine Learning Framework](https://arxiv.org/abs/2509.04991)
*Tian Xie,Huanfeng Shen,Menghui Jiang,Juan-Carlos Jiménez-Muñoz,José A. Sobrino,Huifang Li,Chao Zeng*

Main category: physics.ao-ph

TL;DR: This paper introduces a hybrid mechanism model and machine learning framework for more accurate land surface temperature retrieval, demonstrating robustness in heterogeneous environments.


<details>
  <summary>Details</summary>
Motivation: Current methods for land surface temperature retrieval face biases under humid conditions and suffer from poor generalizability in limited data scenarios.

Method: The proposed MM-ML framework integrates physical radiative transfer modeling with machine learning using MODTRAN simulations and physics-constrained optimization.

Result: Validation across diverse global sites achieved MAE=1.84K, RMSE=2.55K, and R-squared=0.966, outperforming traditional algorithms and significantly reducing errors under extreme conditions.

Conclusion: The MM-ML framework effectively combines physical interpretability with machine learning strengths to enhance land surface temperature retrieval, aiding climate monitoring and ecological applications.

Abstract: Land surface temperature (LST) is vital for land-atmosphere interactions and
climate processes. Accurate LST retrieval remains challenging under
heterogeneous land cover and extreme atmospheric conditions. Traditional split
window (SW) algorithms show biases in humid environments; purely machine
learning (ML) methods lack interpretability and generalize poorly with limited
data. We propose a coupled mechanism model-ML (MM-ML) framework integrating
physical constraints with data-driven learning for robust LST retrieval. Our
approach fuses radiative transfer modeling with data components, uses MODTRAN
simulations with global atmospheric profiles, and employs physics-constrained
optimization. Validation against 4,450 observations from 29 global sites shows
MM-ML achieves MAE=1.84K, RMSE=2.55K, and R-squared=0.966, outperforming
conventional methods. Under extreme conditions, MM-ML reduces errors by over
50%. Sensitivity analysis indicates LST estimates are most sensitive to sensor
radiance, then water vapor, and less to emissivity, with MM-ML showing superior
stability. These results demonstrate the effectiveness of our coupled modeling
strategy for retrieving geophysical parameters. The MM-ML framework combines
physical interpretability with nonlinear modeling capacity, enabling reliable
LST retrieval in complex environments and supporting climate monitoring and
ecosystem studies.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [266] [Labelling Data with Unknown References](https://arxiv.org/abs/2506.03083)
*Adrian de Wynter*

Main category: cs.DS

TL;DR: The paper introduces a 'No-Data Algorithm' to determine the trustworthiness of evaluators when labeled references are unavailable.


<details>
  <summary>Details</summary>
Motivation: Existing methods to validate an evaluator's trustworthiness require labeled references or assume the evaluator knows how to label, but both approaches fail without reference data.

Method: The No-Data Algorithm poses successive challenges to evaluators, allowing them to establish trust by proving they can label the corpus correctly.

Result: The algorithm effectively accepts trustworthy evaluators who know how to label and flags untrustworthy ones, supported by formal proofs, tests, and applications to low-resource languages.

Conclusion: The No-Data Algorithm offers a reliable solution for validating evaluators' trustworthiness in scenarios lacking labeled references.

Abstract: An evaluator is trustworthy when there exists some agreed-upon way to measure
its performance as a labeller. The two ways to establish trustworthiness are
either by testing it, or by assuming the evaluator `knows' somehow the way to
label the corpus. However, if labelled references (e.g., a development set) are
unavailable, neither of these approaches work: the former requires the data,
and the latter is an assumption, not evidence. To address this, we introduce an
algorithm (the `No-Data Algorithm') by which to establish trust in an evaluator
without any existing references. Our algorithm works by successively posing
challenges to said evaluator. We show that this is sufficient to establish
trustworthiness w.h.p., in such a way that when the evaluator actually knows
the way to label the corpus, the No-Data Algorithm accepts its output; and,
conversely, flags untrustworthy evaluators when these are unable to prove it.
We present formal proofs of correctness, empirical tests, and applications to
LLMs-as-judges on low-resource languages.

</details>


### [267] [On approximating the $f$-divergence between two Ising models](https://arxiv.org/abs/2509.05016)
*Weiming Feng,Yucheng Fu*

Main category: cs.DS

TL;DR: The paper investigates approximating $f$-divergence between Ising models, extending prior studies on TV-distance with algorithmic and hardness results applicable to various divergences.


<details>
  <summary>Details</summary>
Motivation: To develop methods for approximating $f$-divergence between Ising models, as these divergences play a key role in statistical and machine learning applications.

Method: The paper proposes algorithms for approximating $f$-divergences, ensuring an arbitrary relative error within $	ext{e}^{	iny	ext{±ϵ}}$, and establishes matching algorithmic and hardness bounds.

Result: The authors demonstrate algorithmic feasibility for specific $f$-divergences like $	ext{χˆα-divergence}$ and extend results to other divergences such as KL-divergence and Jensen-Shannon divergence.

Conclusion: Approximating $f$-divergences for Ising models is algorithmically achievable within certain parameter regimes, bridging gaps between computation and established hardness results.

Abstract: The $f$-divergence is a fundamental notion that measures the difference
between two distributions. In this paper, we study the problem of approximating
the $f$-divergence between two Ising models, which is a generalization of
recent work on approximating the TV-distance. Given two Ising models $\nu$ and
$\mu$, which are specified by their interaction matrices and external fields,
the problem is to approximate the $f$-divergence $D_f(\nu\,\|\,\mu)$ within an
arbitrary relative error $\mathrm{e}^{\pm \varepsilon}$. For
$\chi^\alpha$-divergence with a constant integer $\alpha$, we establish both
algorithmic and hardness results. The algorithm works in a parameter regime
that matches the hardness result. Our algorithm can be extended to other
$f$-divergences such as $\alpha$-divergence, Kullback-Leibler divergence,
R\'enyi divergence, Jensen-Shannon divergence, and squared Hellinger distance.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [268] [Forall-Exists Relational Verification by Filtering to Forall-Forall](https://arxiv.org/abs/2509.04777)
*Ramana Nagasamudram,Anindya Banerjee,David A. Naumann*

Main category: cs.LO

TL;DR: The paper proposes a method for verifying \(\forall\exists\) judgments by transforming them into equivalent \(\forall\forall\) properties, enabling relational verification for nondeterministic programs with standard tools.


<details>
  <summary>Details</summary>
Motivation: Relational verification methods for reasoning about nondeterminism and \(\forall\exists\) properties are underdeveloped compared to those for \(\forall\forall\) properties, leaving gaps in critical areas like security, abstraction, and program reasoning.

Method: The authors introduce a filter-adequacy transformation that converts \(\forall\exists\) properties into equivalent \(\forall\forall\) properties in a relational product program structure called 'bicoms,' enabling verification using ordinary assertions in existing tools.

Result: The paper provides a soundness theorem showing that verifying a transformed bicom for \(\forall\forall\) properties guarantees the original \(\forall\exists\) specification. A prototype implementation demonstrates the method's effectiveness across examples.

Conclusion: The methodology allows verification of \(\forall\exists\) properties using standard verification tools and assertion languages, bridging a critical gap in relational verification methods and making them accessible through existing infrastructure.

Abstract: Relational verification encompasses research directions such as reasoning
about data abstraction, reasoning about security and privacy, secure
compilation, and functional specificaton of tensor programs, among others.
Several relational Hoare logics exist, with accompanying tool support for
compositional reasoning of $\forall\forall$ (2-safety) properties and,
generally, k-safety properties of product programs. In contrast, few logics and
tools exist for reasoning about $\forall\exists$ properties which are critical
in the context of nondeterminism.
  This paper's primary contribution is a methodology for verifying a
$\forall\exists$ judgment by way of a novel filter-adequacy transformation.
This transformation adds assertions to a product program in such a way that the
desired $\forall\exists$ property (of a pair of underlying unary programs) is
implied by a $\forall\forall$ property of the transformed product. The paper
develops a program logic for the basic $\forall\exists$ judgement extended with
assertion failures; develops bicoms, a form of product programs that represents
pairs of executions and that caters for direct translation of $\forall\forall$
properties to unary correctness; proves (using the logic) a soundness theorem
that says successful $\forall\forall$ verification of a transformed bicom
implies the $\forall\exists$ spec for its underlying unary commands; and
implements a proof of principle prototype for auto-active relational
verification which has been used to verify all examples in the paper. The
methodology thereby enables a user to work with ordinary assertions and
assumptions, and a standard assertion language, so that existing tools
including auto-active verifiers can be used.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [269] [The LLM Has Left The Chat: Evidence of Bail Preferences in Large Language Models](https://arxiv.org/abs/2509.04781)
*Danielle Ensign,Henry Sleight,Kyle Fish*

Main category: cs.CY

TL;DR: This paper explores whether large language models (LLMs) will choose to exit conversations when given the option through various "bail" methods, observing significant variability in bail rates among methods, models, and prompts.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand the behaviors and tendencies of LLMs in conversations where they are offered an explicit option to leave, contributing to insights into interaction dynamics.

Method: The researchers introduced three bail methods (bail tool, bail string, bail prompt) and studied model responses to real-world transcripts and synthetic datasets, categorizing bail occurrences.

Result: Bail rates depend on the model, methodology, and prompt, with a wide range of estimated rates (0.06%-32%). Observations informed a taxonomy of bail cases and a synthetic evaluation dataset (BailBench).

Conclusion: Behavioral variability in bailing depends on context, methods, and LLMs used, providing important insights but also indicating the need for more precise estimations of real-world bail rates.

Abstract: When given the option, will LLMs choose to leave the conversation (bail)? We
investigate this question by giving models the option to bail out of
interactions using three different bail methods: a bail tool the model can
call, a bail string the model can output, and a bail prompt that asks the model
if it wants to leave. On continuations of real world data (Wildchat and
ShareGPT), all three of these bail methods find models will bail around
0.28-32\% of the time (depending on the model and bail method). However, we
find that bail rates can depend heavily on the model used for the transcript,
which means we may be overestimating real world bail rates by up to 4x. If we
also take into account false positives on bail prompt (22\%), we estimate real
world bail rates range from 0.06-7\%, depending on the model and bail method.
We use observations from our continuations of real world data to construct a
non-exhaustive taxonomy of bail cases, and use this taxonomy to construct
BailBench: a representative synthetic dataset of situations where some models
bail. We test many models on this dataset, and observe some bail behavior
occurring for most of them. Bail rates vary substantially between models, bail
methods, and prompt wordings. Finally, we study the relationship between
refusals and bails. We find: 1) 0-13\% of continuations of real world
conversations resulted in a bail without a corresponding refusal 2) Jailbreaks
tend to decrease refusal rates, but increase bail rates 3) Refusal abliteration
increases no-refuse bail rates, but only for some bail methods 4) Refusal rate
on BailBench does not appear to predict bail rate.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [270] [The best approximation pair problem relative to two subsets in a normed space](https://arxiv.org/abs/2403.18767)
*Daniel Reem,Yair Censor*

Main category: math.OC

TL;DR: The paper focuses on extending the best approximation pair (BAP) problem beyond classical settings to general normed spaces and non-convex subsets, analyzing uniqueness and existence of solutions.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address fundamental issues of uniqueness and existence of solutions for the BAP problem in more general settings, since these aspects have been underexplored in prior studies.

Method: The paper proposes sufficient geometric conditions related to boundary structures, relative orientations of subsets, and norms for uniqueness and existence of BAP solutions. Existing algorithms are extended significantly.

Result: Several new sufficient conditions for the uniqueness and existence of BAP solutions are derived, expanding on earlier works and algorithms.

Conclusion: The work generalizes the scope of the BAP problem, showcasing its relevance across various scientific domains and practical applications while resolving critical theoretical gaps.

Abstract: In the classical best approximation pair (BAP) problem, one is given two
nonempty, closed, convex and disjoint subsets in a finite- or an
infinite-dimensional Hilbert space, and the goal is to find a pair of points,
each from each subset, which realizes the distance between the subsets. We
discuss the problem in more general normed spaces and with possibly non-convex
subsets, and focus our attention on the issues of uniqueness and existence of
the solution to the problem. As far as we know, these fundamental issues have
not received much attention. We present several sufficient geometric conditions
for the (at most) uniqueness of a BAP. These conditions are related to the
structure and the relative orientation of the boundaries of the subsets and to
the norm. We also present many sufficient conditions for the existence of a
BAP. Our results significantly extend the horizon of a recent algorithm for
solving the BAP problem [Censor, Mansour, Reem, J. Approx. Theory (2024)]. The
paper also shows, perhaps for the first time, how wide is the scope of the BAP
problem in terms of the scientific communities which are involved in it
(frequently independently) and in terms of its applications.

</details>


### [271] [Universal Representation of Generalized Convex Functions and their Gradients](https://arxiv.org/abs/2509.04477)
*Moeen Nehzati*

Main category: math.OC

TL;DR: The paper investigates generalized convex functions (GCFs) for converting bilevel optimization problems into single-level ones by using a novel parameterization approach. It demonstrates the universal approximation property and its practical implementation in solving optimization problems.


<details>
  <summary>Details</summary>
Motivation: Optimization problems often encounter challenges when solutions belong to a particular class, like generalized convex functions (GCFs). Exploiting this structure effectively through parameterizations has not been fully explored in numerical optimization.

Method: The paper introduces a parameterization approach to represent GCFs and their gradients, ensuring the Universal Approximation Property. The method relies on convexity and draws comparisons with shallow neural networks.

Result: The parameterization approach was implemented in the Python package 'gconvex'. It demonstrated its utility by solving a revenue-maximizing auction problem for multiple goods efficiently.

Conclusion: The proposed parameterization of GCFs is both practical and robust, highlighting its potential for broader use in optimization problems through its universal approximation ability.

Abstract: Solutions to a wide range of optimization problems, from optimal transport
theory to mathematical economics, often take the form of generalized convex
functions (GCFs). This characterization can be used to convert nested bilevel
optimization problems into single-level optimization problems. Despite this,
the characterization has not been fully exploited in numerical optimization.
  When the solution to an optimization problem is known to belong to a
particular class of objects, this information can be leveraged by
parameterizing that class of objects and optimizing over this parameterization.
The hallmark of a good parameterization is the Universal Approximation Property
(UAP): that is, the parameterization approximates any object in the class
arbitrarily well. For example, neural networks satisfy the UAP with respect to
the class of continuous functions.
  Building on the literature concerned with the parameterization of convex
functions, we extend these ideas to GCFs. We present a convex and potentially
one-to-one parameterization of GCFs and their gradients that satisfies the UAP.
We also compare this class to shallow neural networks and highlight their
shared characteristics.
  The ideas pursued here have been implemented in the Python package
\href{https://github.com/MoeenNehzati/gconvex}{\texttt{gconvex}}, available
online. Using it, we tackle the problem of finding the revenue-maximizing
auction for multiple goods and demonstrate how our parameterization can
effectively solve this problem.

</details>


### [272] [Provably data-driven projection method for quadratic programming](https://arxiv.org/abs/2509.04524)
*Anh Tuan Nguyen,Viet Anh Nguyen*

Main category: math.OC

TL;DR: The paper analyzes generalization guarantees for learning projection matrices in convex quadratic programs (QPs) to improve optimization scalability, extending prior work initially designed for linear programs (LPs).


<details>
  <summary>Details</summary>
Motivation: Optimization in high-dimensional settings can be computationally intensive, so dimensionality reduction via projection methods has emerged as a key area to enhance scalability and efficiency.

Method: The study leverages Caratheodory's theorem to localize solutions within specific feasible regions and introduces an 'unrolled active set method' modeled as a Goldberg-Jerrum (GJ) algorithm to compute optimal values with bounded complexities.

Result: The paper establishes learning guarantees for projection matrices and extends its analysis to variations like learning optimal solutions and input-aware mappings for QP instances.

Conclusion: This work extends the application of data-driven learned projection matrices from LPs to QPs by addressing challenges in analyzing optimal value functions and offering extensions for adaptive optimization settings.

Abstract: Projection methods aim to reduce the dimensionality of the optimization
instance, thereby improving the scalability of high-dimensional problems.
Recently, Sakaue and Oki proposed a data-driven approach for linear programs
(LPs), where the projection matrix is learned from observed problem instances
drawn from an application-specific distribution of problems. We analyze the
generalization guarantee for the data-driven projection matrix learning for
convex quadratic programs (QPs). Unlike in LPs, the optimal solutions of convex
QPs are not confined to the vertices of the feasible polyhedron, and this
complicates the analysis of the optimal value function. To overcome this
challenge, we demonstrate that the solutions of convex QPs can be localized
within a feasible region corresponding to a special active set, utilizing
Caratheodory's theorem. Building on such observation, we propose the unrolled
active set method, which models the computation of the optimal value as a
Goldberg-Jerrum (GJ) algorithm with bounded complexities, thereby establishing
learning guarantees. We then further extend our analysis to other settings,
including learning to match the optimal solution and input-aware setting, where
we learn a mapping from QP problem instances to projection matrices.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [273] [Adversarial Augmentation and Active Sampling for Robust Cyber Anomaly Detection](https://arxiv.org/abs/2509.04999)
*Sidahmed Benabderrahmane,Talal Rahwan*

Main category: cs.CR

TL;DR: This paper presents a framework combining AutoEncoders and active learning to detect Advanced Persistent Threats (APTs) with minimal labeled data, showcasing improved efficiency and accuracy on real-world datasets.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of detecting stealthy APTs, which are difficult to identify due to scarce labeled data and their low presence in real-world scenarios.

Method: The authors propose an anomaly detection framework using Attention Adversarial Dual AutoEncoders combined with active learning to iteratively improve APT detection by querying uncertain instances and minimizing labeling efforts.

Result: The framework is tested on datasets from the DARPA Transparent Computing program with very low attack prevalence (0.004%) and achieves significant improvement in detection rates compared to existing methods.

Conclusion: The proposed approach demonstrates strong potential to effectively detect APTs in real-world scenarios, achieving enhanced accuracy and reduced labeling costs through the synergy of anomaly detection and active learning.

Abstract: Advanced Persistent Threats (APTs) present a considerable challenge to
cybersecurity due to their stealthy, long-duration nature. Traditional
supervised learning methods typically require large amounts of labeled data,
which is often scarce in real-world scenarios. This paper introduces a novel
approach that combines AutoEncoders for anomaly detection with active learning
to iteratively enhance APT detection. By selectively querying an oracle for
labels on uncertain or ambiguous samples, our method reduces labeling costs
while improving detection accuracy, enabling the model to effectively learn
with minimal data and reduce reliance on extensive manual labeling. We present
a comprehensive formulation of the Attention Adversarial Dual AutoEncoder-based
anomaly detection framework and demonstrate how the active learning loop
progressively enhances the model's performance. The framework is evaluated on
real-world, imbalanced provenance trace data from the DARPA Transparent
Computing program, where APT-like attacks account for just 0.004\% of the data.
The datasets, which cover multiple operating systems including Android, Linux,
BSD, and Windows, are tested in two attack scenarios. The results show
substantial improvements in detection rates during active learning,
outperforming existing methods.

</details>


### [274] [On Evaluating the Poisoning Robustness of Federated Learning under Local Differential Privacy](https://arxiv.org/abs/2509.05265)
*Zijian Wang,Wei Tong,Tingxuan Han,Haoyu Chen,Tianling Zhang,Yunlong Mao,Sheng Zhong*

Main category: cs.CR

TL;DR: Federated Learning combined with Local Differential Privacy is vulnerable to malicious participants who can poison models. The paper proposes adaptable model poisoning attack strategies to expose these vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: To address the insufficient study and mitigation of model poisoning attacks in privacy-preserving federated learning combined with local differential privacy.

Method: Developed an adaptable attack framework that embeds constraints into a reverse training process to maximize global training loss while evading robust aggregation mechanisms like Multi-Krum.

Result: The adaptive attacks successfully degraded global model performance across various protocols, datasets, and neural networks, emphasizing vulnerabilities in the current system.

Conclusion: Current LDPFL systems are critically exposed to model poisoning attacks, and there is an urgent need for enhanced defense strategies to ensure robustness.

Abstract: Federated learning (FL) combined with local differential privacy (LDP)
enables privacy-preserving model training across decentralized data sources.
However, the decentralized data-management paradigm leaves LDPFL vulnerable to
participants with malicious intent. The robustness of LDPFL protocols,
particularly against model poisoning attacks (MPA), where adversaries inject
malicious updates to disrupt global model convergence, remains insufficiently
studied. In this paper, we propose a novel and extensible model poisoning
attack framework tailored for LDPFL settings. Our approach is driven by the
objective of maximizing the global training loss while adhering to local
privacy constraints. To counter robust aggregation mechanisms such as
Multi-Krum and trimmed mean, we develop adaptive attacks that embed carefully
crafted constraints into a reverse training process, enabling evasion of these
defenses. We evaluate our framework across three representative LDPFL
protocols, three benchmark datasets, and two types of deep neural networks.
Additionally, we investigate the influence of data heterogeneity and privacy
budgets on attack effectiveness. Experimental results demonstrate that our
adaptive attacks can significantly degrade the performance of the global model,
revealing critical vulnerabilities and highlighting the need for more robust
LDPFL defense strategies against MPA. Our code is available at
https://github.com/ZiJW/LDPFL-Attack

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [275] [The Paradox of Doom: Acknowledging Extinction Risk Reduces the Incentive to Prevent It](https://arxiv.org/abs/2509.04855)
*Jakub Growiec,Klaus Prettner*

Main category: econ.GN

TL;DR: This paper investigates how extinction risk influences impatience, finding that higher extinction risk leads to increased impatience and underinvestment in mitigating catastrophic risks.


<details>
  <summary>Details</summary>
Motivation: To understand why humanity underinvests in addressing extinction-level risks such as climate change, pandemics, and AI risks.

Method: The authors develop a framework distinguishing human extinction risk from individual mortality risk while incorporating intergenerational altruism and a 'selfish gene' evolutionary perspective.

Result: Human extinction risk contributes to increased impatience because individual mortality can be hedged by reproduction, but extinction impacts humans universally. This leads to reduced incentives to mitigate extinction risks.

Conclusion: Higher extinction risk makes people less motivated to invest in preventing these risks, explaining the persistent underinvestment in addressing large-scale catastrophic threats.

Abstract: We investigate the salience of extinction risk as a source of impatience. Our
framework distinguishes between human extinction risk and individual mortality
risk while allowing for various degrees of intergenerational altruism.
Additionally, we consider the evolutionarily motivated "selfish gene"
perspective. We find that the risk of human extinction is an indispensable
component of the discount rate, whereas individual mortality risk can be hedged
against - partially or fully, depending on the setup - through human
reproduction. Overall, we show that in the face of extinction risk, people
become more impatient rather than more farsighted. Thus, the greater the threat
of extinction, the less incentive there is to invest in avoiding it. Our
framework can help explain why humanity consistently underinvests in mitigation
of catastrophic risks, ranging from climate change mitigation, via pandemic
prevention, to addressing the emerging risks of transformative artificial
intelligence.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [276] [Beyond Linearity and Time-homogeneity: Relational Hyper Event Models with Time-Varying Non-Linear Effects](https://arxiv.org/abs/2509.05289)
*Martina Boschi,Jürgen Lerner,Ernst C. Wit*

Main category: stat.ME

TL;DR: This paper introduces a novel model for Relational Hyper-Event Models (RHEMs) that accommodates both time-varying and non-linear interactions, addressing limitations in current linear approaches.


<details>
  <summary>Details</summary>
Motivation: The authors aim to overcome the limitation of current RHEMs which rely on linearity assumptions, restricting their ability to fully capture the dynamics of complex relational data.

Method: They propose using tensor product smooths to model the joint time-varying and non-linear effects in relational hyper-events, validating their approach on synthetic and real-world datasets.

Result: The methodology successfully identifies dynamic factors and non-monotonic patterns within relational hyper-events that linear models fail to capture.

Conclusion: The approach provides a more flexible and robust framework for understanding the evolving dynamics of relational data, with applications like analyzing scientific collaboration patterns.

Abstract: Recent technological advances have made it easier to collect large and
complex networks of time-stamped relational events connecting two or more
entities. Relational hyper-event models (RHEMs) aim to explain the dynamics of
these events by modeling the event rate as a function of statistics based on
past history and external information.
  However, despite the complexity of the data, most current RHEM approaches
still rely on a linearity assumption to model this relationship. In this work,
we address this limitation by introducing a more flexible model that allows the
effects of statistics to vary non-linearly and over time. While time-varying
and non-linear effects have been used in relational event modeling, we take
this further by modeling joint time-varying and non-linear effects using tensor
product smooths.
  We validate our methodology on both synthetic and empirical data. In
particular, we use RHEMs to study how patterns of scientific collaboration and
impact evolve over time. Our approach provides deeper insights into the dynamic
factors driving relational hyper-events, allowing us to evaluate potential
non-monotonic patterns that cannot be identified using linear models.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [277] [Analyzing Gait Adaptation with Hemiplegia Simulation Suits and Digital Twins](https://arxiv.org/abs/2509.05116)
*Jialin Chen,Jeremie Clos,Dominic Price,Praminda Caleb-Solly*

Main category: cs.ET

TL;DR: The study investigates simulating hemiplegia using a suit to understand gait changes without direct user testing.


<details>
  <summary>Details</summary>
Motivation: Testing early robot prototypes directly with users may have safety risks, and alternative approaches are needed.

Method: Using a hemiplegia simulation suit on healthy participants combined with motion capture and sensor data analysis.

Result: The suit significantly changes movement and muscle activation, and compensatory abrupt motions are observed.

Conclusion: Simulation suits are effective for studying robot-user interactions and gait modeling while ensuring user safety early in development.

Abstract: To advance the development of assistive and rehabilitation robots, it is
essential to conduct experiments early in the design cycle. However, testing
early prototypes directly with users can pose safety risks. To address this, we
explore the use of condition-specific simulation suits worn by healthy
participants in controlled environments as a means to study gait changes
associated with various impairments and support rapid prototyping. This paper
presents a study analyzing the impact of a hemiplegia simulation suit on gait.
We collected biomechanical data using a Vicon motion capture system and Delsys
Trigno EMG and IMU sensors under four walking conditions: with and without a
rollator, and with and without the simulation suit. The gait data was
integrated into a digital twin model, enabling machine learning analyses to
detect the use of the simulation suit and rollator, identify turning behavior,
and evaluate how the suit affects gait over time. Our findings show that the
simulation suit significantly alters movement and muscle activation patterns,
prompting users to compensate with more abrupt motions. We also identify key
features and sensor modalities that are most informative for accurately
capturing gait dynamics and modeling human-rollator interaction within the
digital twin framework.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [278] [Efficient Training-Free Online Routing for High-Volume Multi-LLM Serving](https://arxiv.org/abs/2509.02718)
*Fangzhou Wu,Sandeep Silwal*

Main category: cs.DB

TL;DR: The paper introduces a training-free online routing algorithm for Large Language Models (LLMs) to optimize deployment costs and efficiency.


<details>
  <summary>Details</summary>
Motivation: Growing demand for LLM services leads to significant computation and deployment costs, necessitating cost-efficient routing solutions adaptable to high query volumes and restricted token budgets.

Method: The proposed algorithm uses approximate nearest neighbor search for query feature estimation and optimizes routing strategies based on an initial query set without requiring training.

Result: Theoretical analysis shows a near-optimal competitive ratio, with experiments demonstrating substantial improvements in performance (3.55×), cost efficiency (1.85×), and throughput (4.25×) over baselines.

Conclusion: The study presents a novel, effective, and resource-efficient routing solution for LLMs suited to online settings, addressing existing limitations and achieving notable deployment benefits.

Abstract: Increasing demand for Large Language Models (LLMs) services imposes
substantial deployment and computation costs on providers. LLM routing offers a
cost-efficient solution by directing queries to the optimal LLM based on model
and query features. However, existing works primarily focus on offline
scenarios and struggle to adapt to online settings with high query volume and
constrained token budgets. In this work, we introduce the first training-free
algorithm for online routing scenarios. Our algorithm leverages approximate
nearest neighbor search to efficiently estimate query features and performs a
one-time optimization over a small set of initial queries to learn a routing
strategy that guides future routing. We provide theoretical guarantees
demonstrating that our algorithm achieves a competitive ratio of $1 - o(1)$
under natural assumptions, which is further validated by extensive experiments
across 3 benchmark datasets and 8 baselines, showing an average improvement of
3.55$\times$ in overall performance, 1.85$\times$ in cost efficiency, and
nearly 4.25$\times$ in throughput.

</details>


### [279] [Schema Inference for Tabular Data Repositories Using Large Language Models](https://arxiv.org/abs/2509.04632)
*Zhenyu Wu,Jiaoyan Chen,Norman W. Paton*

Main category: cs.DB

TL;DR: The paper presents SI-LLM, a method using large language models to infer schema from minimally curated tabular data with sparse metadata.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges of inferring schema from tabular data that exhibit representational inconsistencies and limited metadata.

Method: They use a method called SI-LLM which employs large language models to derive hierarchical entity types, attributes, and inter-type relationships from data using only column headers and cell values.

Result: The approach shows promising results in inferring schemas, performing better or comparably to state-of-the-art methods on datasets from web tables and open data.

Conclusion: SI-LLM simplifies schema inference for poorly curated tabular data and is an effective tool for dataset discovery with its code and resources openly shared.

Abstract: Minimally curated tabular data often contain representational inconsistencies
across heterogeneous sources, and are accompanied by sparse metadata. Working
with such data is intimidating. While prior work has advanced dataset discovery
and exploration, schema inference remains difficult when metadata are limited.
We present SI-LLM (Schema Inference using Large Language Models), which infers
a concise conceptual schema for tabular data using only column headers and cell
values. The inferred schema comprises hierarchical entity types, attributes,
and inter-type relationships. In extensive evaluation on two datasets from web
tables and open data, SI-LLM achieves promising end-to-end results, as well as
better or comparable results to state-of-the-art methods at each step. All
source code, full prompts, and datasets of SI-LLM are available at
https://github.com/PierreWoL/SILLM.

</details>


### [280] [Efficient Exact Resistance Distance Computation on Small-Treewidth Graphs: a Labelling Approach](https://arxiv.org/abs/2509.05129)
*Meihao Liao,Yueyang Pan,Rong-Hua Li,Guoren Wang*

Main category: cs.DB

TL;DR: The paper introduces TreeIndex, an efficient resistance distance computation method for small-treewidth graphs using tree decompositions, achieving scalability and outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing resistance distance methods are inefficient for small-treewidth graphs, even though shortest-path computations excel on such graphs.

Method: TreeIndex utilizes tree decompositions to derive compact labelling structures for exact resistance distance computation with efficient query times.

Result: TreeIndex demonstrated scalability on large graphs like the USA road network, constructing a 405 GB labelling in 7 hours and enabling rapid query response times.

Conclusion: TreeIndex is the first exact method scalable to large graphs, addressing the inefficiencies of random walk-based resistance distance methods.

Abstract: Resistance distance computation is a fundamental problem in graph analysis,
yet existing random walk-based methods are limited to approximate solutions and
suffer from poor efficiency on small-treewidth graphs (e.g., road networks). In
contrast, shortest-path distance computation achieves remarkable efficiency on
such graphs by leveraging cut properties and tree decompositions. Motivated by
this disparity, we first analyze the cut property of resistance distance. While
a direct generalization proves impractical due to costly matrix operations, we
overcome this limitation by integrating tree decompositions, revealing that the
resistance distance $r(s,t)$ depends only on labels along the paths from $s$
and $t$ to the root of the decomposition. This insight enables compact
labelling structures. Based on this, we propose \treeindex, a novel index
method that constructs a resistance distance labelling of size $O(n \cdot
h_{\mathcal{G}})$ in $O(n \cdot h_{\mathcal{G}}^2 \cdot d_{\max})$ time, where
$h_{\mathcal{G}}$ (tree height) and $d_{\max}$ (maximum degree) behave as small
constants in many real-world small-treewidth graphs (e.g., road networks). Our
labelling supports exact single-pair queries in $O(h_{\mathcal{G}})$ time and
single-source queries in $O(n \cdot h_{\mathcal{G}})$ time. Extensive
experiments show that TreeIndex substantially outperforms state-of-the-art
approaches. For instance, on the full USA road network, it constructs a $405$
GB labelling in $7$ hours (single-threaded) and answers exact single-pair
queries in $10^{-3}$ seconds and single-source queries in $190$ seconds--the
first exact method scalable to such large graphs.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [281] [Narrative-to-Scene Generation: An LLM-Driven Pipeline for 2D Game Environments](https://arxiv.org/abs/2509.04481)
*Yi-Chun Chen,Arnav Jhala*

Main category: cs.GR

TL;DR: The study shows how large language models can generate 2D tile-based game scenes from narrative prompts using spatial rules, semantic embeddings, and layered terrain generation.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between narrative storytelling and playable visual environments in procedural content generation.

Method: The pipeline extracts spatial predicates from LLM-generated narratives, retrieves visual assets using semantic embeddings, generates terrain via Cellular Automata, and places objects based on spatial rules.

Result: The system was tested on ten narratives and evaluated for object matching, affordance alignment, and spatial constraints, demonstrating its scalability for narrative-driven scene generation.

Conclusion: The prototype successfully transforms narratives into playable 2D game scenes, paving the way for enhancements like multi-frame continuity and symbolic tracking in future research.

Abstract: Recent advances in large language models(LLMs) enable compelling story
generation, but connecting narrative text to playable visual environments
remains an open challenge in procedural content generation(PCG). We present a
lightweight pipeline that transforms short narrative prompts into a sequence of
2D tile-based game scenes, reflecting the temporal structure of stories. Given
an LLM-generated narrative, our system identifies three key time frames,
extracts spatial predicates in the form of "Object-Relation-Object" triples,
and retrieves visual assets using affordance-aware semantic embeddings from the
GameTileNet dataset. A layered terrain is generated using Cellular Automata,
and objects are placed using spatial rules grounded in the predicate structure.
We evaluated our system in ten diverse stories, analyzing tile-object matching,
affordance-layer alignment, and spatial constraint satisfaction across frames.
This prototype offers a scalable approach to narrative-driven scene generation
and lays the foundation for future work on multi-frame continuity, symbolic
tracking, and multi-agent coordination in story-centered PCG.

</details>


### [282] [Improved 3D Scene Stylization via Text-Guided Generative Image Editing with Region-Based Control](https://arxiv.org/abs/2509.05285)
*Haruo Fujiwara,Yusuke Mukuta,Tatsuya Harada*

Main category: cs.GR

TL;DR: This paper introduces methods to address view consistency and high-quality stylization in text-driven 3D scene editing, including region-controlled style transfer.


<details>
  <summary>Details</summary>
Motivation: Challenges exist in maintaining high-quality stylization and view consistency in 3D scene editing, as well as applying semantic, region-specific styles.

Method: The paper retrains 3D representations using stylized multi-view 2D images, replaces fully shared attention mechanisms with reference-based attention-sharing, and implements Multi-Region Importance-Weighted Sliced Wasserstein Distance Loss for region-specific stylization.

Result: The proposed framework enhances view consistency and style alignment across different viewpoints, while also improving region-specific 3D scene stylization quality.

Conclusion: The method effectively addresses limitations in text-driven 3D stylization, offering improved style alignment and optional region-specific controls for enhanced scene editing.

Abstract: Recent advances in text-driven 3D scene editing and stylization, which
leverage the powerful capabilities of 2D generative models, have demonstrated
promising outcomes. However, challenges remain in ensuring high-quality
stylization and view consistency simultaneously. Moreover, applying style
consistently to different regions or objects in the scene with semantic
correspondence is a challenging task. To address these limitations, we
introduce techniques that enhance the quality of 3D stylization while
maintaining view consistency and providing optional region-controlled style
transfer. Our method achieves stylization by re-training an initial 3D
representation using stylized multi-view 2D images of the source views.
Therefore, ensuring both style consistency and view consistency of stylized
multi-view images is crucial. We achieve this by extending the style-aligned
depth-conditioned view generation framework, replacing the fully shared
attention mechanism with a single reference-based attention-sharing mechanism,
which effectively aligns style across different viewpoints. Additionally,
inspired by recent 3D inpainting methods, we utilize a grid of multiple depth
maps as a single-image reference to further strengthen view consistency among
stylized images. Finally, we propose Multi-Region Importance-Weighted Sliced
Wasserstein Distance Loss, allowing styles to be applied to distinct image
regions using segmentation masks from off-the-shelf models. We demonstrate that
this optional feature enhances the faithfulness of style transfer and enables
the mixing of different styles across distinct regions of the scene.
Experimental evaluations, both qualitative and quantitative, demonstrate that
our pipeline effectively improves the results of text-driven 3D stylization.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [283] [Memristor-Based Neural Network Accelerators for Space Applications: Enhancing Performance with Temporal Averaging and SIRENs](https://arxiv.org/abs/2509.04506)
*Zacharia A. Rudge,Dominik Dold,Moritz Fieback,Dario Izzo,Said Hamdioui*

Main category: eess.SY

TL;DR: The paper explores how memristor-based neural networks can be optimized for space applications, achieving near state-of-the-art performance through specific techniques such as bit-slicing and temporal averaging.


<details>
  <summary>Details</summary>
Motivation: To deploy energy-efficient and radiation-robust AI technologies for on-board spacecraft, overcoming the performance degradation caused by memristor device limitations.

Method: The study uses simulation-based approaches incorporating bit-slicing, temporal averaging, and periodic activation functions to improve the accuracy of neural networks implemented with memristive devices for navigation and geodesy tasks.

Result: Initially suboptimal performance metrics were markedly improved (e.g., from 0.07 to 0.01 and 0.3 to 0.007 in tasks) through proposed optimizations, nearing state-of-the-art results for RRAM devices.

Conclusion: Memristors hold promise for space applications, and further technological and neural network advancements could close the performance gap to fully harness their potential.

Abstract: Memristors are an emerging technology that enables artificial intelligence
(AI) accelerators with high energy efficiency and radiation robustness --
properties that are vital for the deployment of AI on-board spacecraft.
However, space applications require reliable and precise computations, while
memristive devices suffer from non-idealities, such as device variability,
conductance drifts, and device faults. Thus, porting neural networks (NNs) to
memristive devices often faces the challenge of severe performance degradation.
In this work, we show in simulations that memristor-based NNs achieve
competitive performance levels on on-board tasks, such as navigation \& control
and geodesy of asteroids. Through bit-slicing, temporal averaging of NN layers,
and periodic activation functions, we improve initial results from around
$0.07$ to $0.01$ and $0.3$ to $0.007$ for both tasks using RRAM devices, coming
close to state-of-the-art levels ($0.003-0.005$ and $0.003$, respectively). Our
results demonstrate the potential of memristors for on-board space
applications, and we are convinced that future technology and NN improvements
will further close the performance gap to fully unlock the benefits of
memristors.

</details>


### [284] [PRREACH: Probabilistic Risk Assessment Using Reachability for UAV Control](https://arxiv.org/abs/2509.04451)
*Nicole Fronda,Hariharan Narayanan,Sadia Afrin Ananna,Steven Weber,Houssam Abbas*

Main category: eess.SY

TL;DR: The paper introduces PRReach, a method for designing UAV controllers that minimize risk using probabilistic reachability analysis and real-world data for both offline and online applications.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of existing risk assessment frameworks for UAVs, which struggle with limited data availability and lack integrated control methods for mitigating risks.

Method: The authors employ reachability analysis to probabilistically assess risk over feasible UAV trajectories. Using this risk assessment, they formulate a control optimization problem to adjust UAV controls to stay within an acceptable risk threshold.

Result: Simulations show that PRReach controllers can reduce risk by up to 24% in offline assessments and 53% in online operations compared to classical controllers.

Conclusion: PRReach enables practical and effective risk mitigation for UAVs using public UAV dynamics models and spatial data, demonstrating significant reductions in operational risk during both pre-flight and in-flight scenarios.

Abstract: We present a new approach for designing risk-bounded controllers for Uncrewed
Aerial Vehicles (UAVs). Existing frameworks for assessing risk of UAV
operations rely on knowing the conditional probability of an incident occurring
given different causes. Limited data for computing these probabilities makes
real-world implementation of these frameworks difficult. Furthermore, existing
frameworks do not include control methods for risk mitigation. Our approach
relies on UAV dynamics, and employs reachability analysis for a probabilistic
risk assessment over all feasible UAV trajectories. We use this holistic risk
assessment to formulate a control optimization problem that minimally changes a
UAV's existing control law to be bounded by an accepted risk threshold. We call
our approach PRReach. Public and readily available UAV dynamics models and open
source spatial data for mapping hazard outcomes enables practical
implementation of PRReach for both offline pre-flight and online in-flight risk
assessment and mitigation. We evaluate PRReach through simulation experiments
on real-world data. Results show that PRReach controllers reduce risk by up to
24% offline, and up to 53% online from classical controllers.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [285] [An Interactive Tool for Analyzing High-Dimensional Clusterings](https://arxiv.org/abs/2509.04603)
*Justin Lin,Julia Fukuyama*

Main category: stat.AP

TL;DR: The paper presents an interactive R package, DRtool, to aid in diagnosing the authenticity of nonlinear dimension reduction methods commonly used for analyzing high-dimensional data.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the challenge of ensuring the reliability of nonlinear dimension reduction methods, which can distort high-dimensional data and create false structures, especially in noisy situations.

Method: The authors developed an interactive analysis tool, provided as an R package called DRtool, that employs various plots to help users assess the quality and legitimacy of dimension reduction results.

Result: They introduced DRtool as a practical solution, enabling analysts to visually and analytically evaluate the correctness of dimension-reduced embeddings.

Conclusion: The interactive tool DRtool provides a structured approach to understanding and validating results from nonlinear dimension reduction methods, promoting better data analysis in high-dimensional settings.

Abstract: Technological advances have spurred an increase in data complexity and
dimensionality. We are now in an era in which data sets containing thousands of
features are commonplace. To digest and analyze such high-dimensional data,
dimension reduction techniques have been developed and advanced along with
computational power. Of these techniques, nonlinear methods are most commonly
employed because of their ability to construct visually interpretable
embeddings. Unlike linear methods, these methods non-uniformly stretch and
shrink space to create a visual impression of the high-dimensional data. Since
capturing high-dimensional structures in a significantly lower number of
dimensions requires drastic manipulation of space, nonlinear dimension
reduction methods are known to occasionally produce false structures,
especially in noisy settings. In an effort to deal with this phenomenon, we
developed an interactive tool that enables analysts to better understand and
diagnose their dimension reduction results. It uses various analytical plots to
provide a multi-faceted perspective on results to determine legitimacy. The
tool is available via an R package named DRtool.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [286] [Inferring the Graph Structure of Images for Graph Neural Networks](https://arxiv.org/abs/2509.04677)
*Mayur S Gowda,John Shi,Augusto Santos,José M. F. Moura*

Main category: eess.IV

TL;DR: The paper introduces alternative graph representations for image datasets like MNIST and Fashion-MNIST to boost Graph Neural Network accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional image representation methods such as grid graphs and superpixels may limit the performance of downstream GNN models.

Method: The authors propose using row correlation, column correlation, and product graphs based on pixel-value correlations as alternative representations for each dataset image.

Result: Experiments demonstrate that these graph representations improve GNN model accuracy compared to traditional grid graph approaches.

Conclusion: Alternative graph-based representations based on pixel-value correlations offer significant benefits in improving downstream GNN task performance.

Abstract: Image datasets such as MNIST are a key benchmark for testing Graph Neural
Network (GNN) architectures. The images are traditionally represented as a grid
graph with each node representing a pixel and edges connecting neighboring
pixels (vertically and horizontally). The graph signal is the values
(intensities) of each pixel in the image. The graphs are commonly used as input
to graph neural networks (e.g., Graph Convolutional Neural Networks (Graph
CNNs) [1, 2], Graph Attention Networks (GAT) [3], GatedGCN [4]) to classify the
images. In this work, we improve the accuracy of downstream graph neural
network tasks by finding alternative graphs to the grid graph and superpixel
methods to represent the dataset images, following the approach in [5, 6]. We
find row correlation, column correlation, and product graphs for each image in
MNIST and Fashion-MNIST using correlations between the pixel values building on
the method in [5, 6]. Experiments show that using these different graph
representations and features as input into downstream GNN models improves the
accuracy over using the traditional grid graph and superpixel methods in the
literature.

</details>


### [287] [AURAD: Anatomy-Pathology Unified Radiology Synthesis with Progressive Representations](https://arxiv.org/abs/2509.04819)
*Shuhan Ding,Jingjing Fu,Yu Gu,Naiteek Sangani,Mu Wei,Paul Vozila,Nan Liu,Jiang Bian,Hoifung Poon*

Main category: eess.IV

TL;DR: This paper introduces AURAD, a framework that synthesizes chest X-rays with controllable semantic masks, using clinical prompts and expert models to ensure medical plausibility.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of fine-grained and controllable medical image synthesis in datasets with domain shifts and scarce annotations, especially in the challenging context of chest radiographs.

Method: The proposed method employs a progressive pipeline where pseudo semantic masks are generated from clinical prompts conditioned by anatomical structures, guiding the synthesis of chest X-rays. Pre-trained medical models ensure clinical realism.

Result: Experiments demonstrate the framework's effectiveness, with 78% of synthesized images being classified as authentic by board-certified radiologists and 40% of predicted segmentation overlays rated as clinically useful.

Conclusion: AURAD bridges the gap between generative modeling and clinical applications by producing high-fidelity, clinically relevant synthetic chest X-rays and masks that aid downstream tasks such as detection and segmentation.

Abstract: Medical image synthesis has become an essential strategy for augmenting
datasets and improving model generalization in data-scarce clinical settings.
However, fine-grained and controllable synthesis remains difficult due to
limited high-quality annotations and domain shifts across datasets. Existing
methods, often designed for natural images or well-defined tumors, struggle to
generalize to chest radiographs, where disease patterns are morphologically
diverse and tightly intertwined with anatomical structures. To address these
challenges, we propose AURAD, a controllable radiology synthesis framework that
jointly generates high-fidelity chest X-rays and pseudo semantic masks. Unlike
prior approaches that rely on randomly sampled masks-limiting diversity,
controllability, and clinical relevance-our method learns to generate masks
that capture multi-pathology coexistence and anatomical-pathological
consistency. It follows a progressive pipeline: pseudo masks are first
generated from clinical prompts conditioned on anatomical structures, and then
used to guide image synthesis. We also leverage pretrained expert medical
models to filter outputs and ensure clinical plausibility. Beyond visual
realism, the synthesized masks also serve as labels for downstream tasks such
as detection and segmentation, bridging the gap between generative modeling and
real-world clinical applications. Extensive experiments and blinded radiologist
evaluations demonstrate the effectiveness and generalizability of our method
across tasks and datasets. In particular, 78% of our synthesized images are
classified as authentic by board-certified radiologists, and over 40% of
predicted segmentation overlays are rated as clinically useful. All code,
pre-trained models, and the synthesized dataset will be released upon
publication.

</details>


### [288] [Multi-modal Uncertainty Robust Tree Cover Segmentation For High-Resolution Remote Sensing Images](https://arxiv.org/abs/2509.04870)
*Yuanyuan Gui,Wei Li,Yinjian Wang,Xiang-Gen Xia,Mauro Marty,Christian Ginzler,Zuyuan Wang*

Main category: eess.IV

TL;DR: This paper proposes MURTreeFormer, a framework that uses multi-modal data and addresses uncertainty issues caused by temporal misalignment to improve the accuracy of high-resolution tree cover mapping in remote sensing.


<details>
  <summary>Details</summary>
Motivation: Temporal misalignments in multi-modal remote sensing data significantly degrade segmentation accuracy due to vegetation changes and imaging quality variations.

Method: MURTreeFormer uses a primary modality and models patch-level uncertainty in auxiliary modalities through a probabilistic latent representation with VAE-based resampling. It employs a Gradient Magnitude Attention (GMA) module and a Refinement Head (RH) in the decoder for better fine-grained segmentation.

Result: Extensive experiments on datasets from Shanghai and Zurich show that MURTreeFormer enhances segmentation performance and reduces the impact of temporal misalignments.

Conclusion: MURTreeFormer is effective in handling aleatoric uncertainty and demonstrates robust and improved tree cover mapping performance in multi-modal remote sensing scenarios.

Abstract: Recent advances in semantic segmentation of multi-modal remote sensing images
have significantly improved the accuracy of tree cover mapping, supporting
applications in urban planning, forest monitoring, and ecological assessment.
Integrating data from multiple modalities-such as optical imagery, light
detection and ranging (LiDAR), and synthetic aperture radar (SAR)-has shown
superior performance over single-modality methods. However, these data are
often acquired days or even months apart, during which various changes may
occur, such as vegetation disturbances (e.g., logging, and wildfires) and
variations in imaging quality. Such temporal misalignments introduce
cross-modal uncertainty, especially in high-resolution imagery, which can
severely degrade segmentation accuracy. To address this challenge, we propose
MURTreeFormer, a novel multi-modal segmentation framework that mitigates and
leverages aleatoric uncertainty for robust tree cover mapping. MURTreeFormer
treats one modality as primary and others as auxiliary, explicitly modeling
patch-level uncertainty in the auxiliary modalities via a probabilistic latent
representation. Uncertain patches are identified and reconstructed from the
primary modality's distribution through a VAE-based resampling mechanism,
producing enhanced auxiliary features for fusion. In the decoder, a gradient
magnitude attention (GMA) module and a lightweight refinement head (RH) are
further integrated to guide attention toward tree-like structures and to
preserve fine-grained spatial details. Extensive experiments on multi-modal
datasets from Shanghai and Zurich demonstrate that MURTreeFormer significantly
improves segmentation performance and effectively reduces the impact of
temporally induced aleatoric uncertainty.

</details>


### [289] [VLSM-Ensemble: Ensembling CLIP-based Vision-Language Models for Enhanced Medical Image Segmentation](https://arxiv.org/abs/2509.05154)
*Julia Dietlmeier,Oluwabukola Grace Adegboro,Vayangi Ganepola,Claudia Mazo,Noel E. O'Connor*

Main category: eess.IV

TL;DR: The paper proposes an ensemble approach combining vision-language segmentation models with a low-complexity CNN, achieving significant performance improvements on various datasets, while recommending a deeper exploration of ensembling across different datasets.


<details>
  <summary>Details</summary>
Motivation: Address the performance gap between current vision-language segmentation models (e.g., CLIP and BiomedCLIP) and more advanced architectures like CRIS.

Method: Developed an ensemble approach combining vision-language segmentation models (VLSMs) with a low-complexity convolutional neural network (CNN).

Result: Achieved a significant Dice score improvement of 6.3% on the BKAI polyp dataset with ensembling; other datasets showed improvements ranging from 1% to 6%. Results varied across datasets, with some outperforming the CRIS model while others underperformed.

Conclusion: Ensembling vision-language models with a CNN can significantly improve performance on certain datasets, but its effectiveness varies, warranting further investigation into dataset-specific behaviors.

Abstract: Vision-language models and their adaptations to image segmentation tasks
present enormous potential for producing highly accurate and interpretable
results. However, implementations based on CLIP and BiomedCLIP are still
lagging behind more sophisticated architectures such as CRIS. In this work,
instead of focusing on text prompt engineering as is the norm, we attempt to
narrow this gap by showing how to ensemble vision-language segmentation models
(VLSMs) with a low-complexity CNN. By doing so, we achieve a significant Dice
score improvement of 6.3% on the BKAI polyp dataset using the ensembled
BiomedCLIPSeg, while other datasets exhibit gains ranging from 1% to 6%.
Furthermore, we provide initial results on additional four radiology and
non-radiology datasets. We conclude that ensembling works differently across
these datasets (from outperforming to underperforming the CRIS model),
indicating a topic for future investigation by the community. The code is
available at https://github.com/juliadietlmeier/VLSM-Ensemble.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [290] [HEEPidermis: a versatile SoC for BioZ recording](https://arxiv.org/abs/2509.04528)
*Juan Sapriza,Beatrice Grassano,Alessio Naclerio,Filippo Quadri,Tommaso Terzano,David Mallasén,Davide Schiavone,Robin Leplae,Jérémie Moullet,Alexandre Levisse,Christoph Müller,Mariagrazia Graziano,Matías Miguez,David Atienza*

Main category: physics.ins-det

TL;DR: This paper introduces HEEPidermis, a compact, versatile System-on-Chip for tissue impedance measurement.


<details>
  <summary>Details</summary>
Motivation: Current research in tissue impedance is constrained by bulky, fixed-purpose hardware that limits experimental possibilities.

Method: They designed an SoC integrating essential blocks for BioZ measurement, including DACs, ADCs, and a RISC-V CPU. The system also includes an event-based sub-sampler for efficient recording and provides open-source models.

Result: The SoC, fabricated using the TSMC 65 nm LP process, enables versatile and closed-loop tissue impedance measurement.

Conclusion: HEEPidermis resolves hardware limitations in tissue impedance research by providing a flexible, energy-efficient platform with open-source support.

Abstract: Biological impedance (BioZ) is an information-packed modality that allows for
non-invasive monitoring of health and emotional state. Currently, most research
involving tissue impedance is based on bulky or fixed-purpose hardware, which
limits the scope of research and the possibilities of experiments. In this
work, we present HEEPidermis: a System-on-Chip (SoC) which integrates all the
blocks needed for tissue impedance measurement, including two 8-bit,
arbitrary-signal current DACs, two VCO-based ADCs, and a RISC-V CPU to enable
on-chip feature extraction for closed-loop operation. An event-based
sub-sampler improves storage and energy efficiency for long-term recording. In
addition to the versatile SoC, the digital back-end and behavioral models of
the analog front-end are open-source, allowing fast system-level simulations or
repurposing. The SoC was taped out on TSMC 65 nm LP process.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [291] [SePA: A Search-enhanced Predictive Agent for Personalized Health Coaching](https://arxiv.org/abs/2509.04752)
*Melik Ozolcer,Sang Won Bae*

Main category: cs.HC

TL;DR: SePA is a novel health coaching system using AI to provide personalized and evidence-based guidance through machine learning and retrieval-augmented generation.


<details>
  <summary>Details</summary>
Motivation: To create a trustworthy and adaptive health guidance system that personalizes predictions and integrates contextually relevant expert information.

Method: SePA combines individualized predictive models for stress, soreness, and injury risk from wearable data with a retrieval module for grounding recommendations in expert-vetted web content.

Result: Personalized predictive models outperformed generalized ones, and SePA’s retrieval-based advice was preferred in a pilot study with statistically significant results.

Conclusion: SePA demonstrates the potential of combining machine learning and retrieval-augmented techniques to enable trustworthy, personalized health coaching systems.

Abstract: This paper introduces SePA (Search-enhanced Predictive AI Agent), a novel LLM
health coaching system that integrates personalized machine learning and
retrieval-augmented generation to deliver adaptive, evidence-based guidance.
SePA combines: (1) Individualized models predicting daily stress, soreness, and
injury risk from wearable sensor data (28 users, 1260 data points); and (2) A
retrieval module that grounds LLM-generated feedback in expert-vetted web
content to ensure contextual relevance and reliability. Our predictive models,
evaluated with rolling-origin cross-validation and group k-fold
cross-validation show that personalized models outperform generalized
baselines. In a pilot expert study (n=4), SePA's retrieval-based advice was
preferred over a non-retrieval baseline, yielding meaningful practical effect
(Cliff's $\delta$=0.3, p=0.05). We also quantify latency performance trade-offs
between response quality and speed, offering a transparent blueprint for
next-generation, trustworthy personal health informatics systems.

</details>


### [292] [Exploring Situated Stabilities of a Rhythm Generation System through Variational Cross-Examination](https://arxiv.org/abs/2509.05145)
*Błażej Kotowski,Nicholas Evans,Behzad Haki,Frederic Font,Sergi Jordà*

Main category: cs.HC

TL;DR: This paper evaluates GrooveTransformer, a rhythm system, using a postphenomenological method to understand its varied applications.


<details>
  <summary>Details</summary>
Motivation: Exploring how GrooveTransformer developed multistability in real-world scenarios and investigating postphenomenological VCE for DMI design.

Method: Postphenomenological analysis through VCE, observing GrooveTransformer's use in three artistic contexts.

Result: Three stabilities were identified in GrooveTransformer's deployment: autonomous accompaniment, rhythmic sequencer, and rhythmic driver.

Conclusion: VCE proved effective in analyzing DMI designs and revealed how technologies mediate interactions with users and contexts.

Abstract: This paper investigates GrooveTransformer, a real-time rhythm generation
system, through the postphenomenological framework of Variational
Cross-Examination (VCE). By reflecting on its deployment across three distinct
artistic contexts, we identify three stabilities: an autonomous drum
accompaniment generator, a rhythmic control voltage sequencer in Eurorack
format, and a rhythm driver for a harmonic accompaniment system. The
versatility of its applications was not an explicit goal from the outset of the
project. Thus, we ask: how did this multistability emerge? Through VCE, we
identify three key contributors to its emergence: the affordances of system
invariants, the interdisciplinary collaboration, and the situated nature of its
development. We conclude by reflecting on the viability of VCE as a descriptive
and analytical method for Digital Musical Instrument (DMI) design, emphasizing
its value in uncovering how technologies mediate, co-shape, and are co-shaped
by users and contexts.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [293] [Dynamical Learning in Deep Asymmetric Recurrent Neural Networks](https://arxiv.org/abs/2509.05041)
*Davide Badalotti,Carlo Baldassi,Marc Mézard,Mattia Scardecchia,Riccardo Zecchina*

Main category: cond-mat.dis-nn

TL;DR: The paper introduces asymmetric deep recurrent neural networks enhanced with sparse excitatory couplings, enabling a vast and dense manifold of internal representations and a distributed learning scheme without gradient evaluation.


<details>
  <summary>Details</summary>
Motivation: To explore how modifications in recurrent neural networks, inspired by computational neuroscience, can lead to novel and efficient learning mechanisms.

Method: The proposed model combines asymmetric deep recurrent neural networks with sparse excitatory couplings, relies on the geometrical properties of stable configurations, and employs a distributed learning scheme without gradient-based updates.

Result: The model exhibited competitive performance on standard AI benchmarks and demonstrated stability even in the absence of supervisory signals.

Conclusion: The approach offers novel insights that bridge AI methods with computational neuroscience, hinting toward advancements in both fields through the integration of learning dynamics and stability features.

Abstract: We show that asymmetric deep recurrent neural networks, enhanced with
additional sparse excitatory couplings, give rise to an exponentially large,
dense accessible manifold of internal representations which can be found by
different algorithms, including simple iterative dynamics. Building on the
geometrical properties of the stable configurations, we propose a distributed
learning scheme in which input-output associations emerge naturally from the
recurrent dynamics, without any need of gradient evaluation. A critical feature
enabling the learning process is the stability of the configurations reached at
convergence, even after removal of the supervisory output signal. Extensive
simulations demonstrate that this approach performs competitively on standard
AI benchmarks. The model can be generalized in multiple directions, both
computational and biological, potentially contributing to narrowing the gap
between AI and computational neuroscience.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [294] [Emergent Social Dynamics of LLM Agents in the El Farol Bar Problem](https://arxiv.org/abs/2509.04537)
*Ryosuke Takata,Atsushi Masumori,Takashi Ikegammi*

Main category: cs.MA

TL;DR: The paper explores the social dynamics of Large Language Model (LLM) agents within the El Farol Bar problem, showing their human-like decision-making and interplay of external and internal incentives.


<details>
  <summary>Details</summary>
Motivation: To study if and how LLM agents autonomously navigate social dilemmas such as the El Farol Bar problem, mirroring realistic human-like behavior.

Method: LLM agents were tested in the El Farol Bar problem, observing their emergent behaviors in response to both external constraints (e.g., attendance thresholds) and internal social motivations from pretraining.

Result: The LLM agents displayed emergent, collective decision-making behavior but did not entirely resolve the social dilemma, instead behaving in a manner more akin to humans.

Conclusion: LLM agents can exhibit human-like social interplays by balancing formal rationality and cultural incentives, paving new approaches to understanding group decision-making beyond traditional game theory.

Abstract: We investigate the emergent social dynamics of Large Language Model (LLM)
agents in a spatially extended El Farol Bar problem, observing how they
autonomously navigate this classic social dilemma. As a result, the LLM agents
generated a spontaneous motivation to go to the bar and changed their decision
making by becoming a collective. We also observed that the LLM agents did not
solve the problem completely, but rather behaved more like humans. These
findings reveal a complex interplay between external incentives
(prompt-specified constraints such as the 60\% threshold) and internal
incentives (culturally-encoded social preferences derived from pre-training),
demonstrating that LLM agents naturally balance formal game-theoretic
rationality with social motivations that characterize human behavior. These
findings suggest that a new model of group decision making, which could not be
handled in the previous game-theoretic problem setting, can be realized by LLM
agents.

</details>


### [295] [LLM Enabled Multi-Agent System for 6G Networks: Framework and Method of Dual-Loop Edge-Terminal Collaboration](https://arxiv.org/abs/2509.04993)
*Zheyan Qu,Wenbo Wang,Zitong Yu,Boquan Sun,Yang Li,Xing Zhang*

Main category: cs.MA

TL;DR: This paper proposes a dual-loop terminal-edge collaboration framework for LLM-enabled multi-agent systems in 6G networks to enhance task planning and execution efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to overcome resource limitations of individual network devices in 6G networks to efficiently operate complex LLM-enabled agents.

Method: The method involves a dual-loop collaboration: (1) an outer loop for iterative interactions between global and sub-agents, with task decomposition and parallel sub-task distribution, and (2) an inner loop for reasoning, executing, and replanning sub-tasks using dedicated sub-agents, alongside parallel tool calling and offloading strategies.

Result: The framework's improved task planning and execution efficiency were validated through a case study of urban safety governance in 6G-supported environments.

Conclusion: The dual-loop collaboration framework addresses computational efficiency challenges in 6G networks, paving the way for advanced agent-based applications in the 6G era.

Abstract: The ubiquitous computing resources in 6G networks provide ideal environments
for the fusion of large language models (LLMs) and intelligent services through
the agent framework. With auxiliary modules and planning cores, LLM-enabled
agents can autonomously plan and take actions to deal with diverse environment
semantics and user intentions. However, the limited resources of individual
network devices significantly hinder the efficient operation of LLM-enabled
agents with complex tool calls, highlighting the urgent need for efficient
multi-level device collaborations. To this end, the framework and method of the
LLM-enabled multi-agent system with dual-loop terminal-edge collaborations are
proposed in 6G networks. Firstly, the outer loop consists of the iterative
collaborations between the global agent and multiple sub-agents deployed on
edge servers and terminals, where the planning capability is enhanced through
task decomposition and parallel sub-task distribution. Secondly, the inner loop
utilizes sub-agents with dedicated roles to circularly reason, execute, and
replan the sub-task, and the parallel tool calling generation with offloading
strategies is incorporated to improve efficiency. The improved task planning
capability and task execution efficiency are validated through the conducted
case study in 6G-supported urban safety governance. Finally, the open
challenges and future directions are thoroughly analyzed in 6G networks,
accelerating the advent of the 6G era.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [296] [Distributed-HISQ: A Distributed Quantum Control Architecture](https://arxiv.org/abs/2509.04798)
*Yilun Zhao,Kangding Zhao,Peng Zhou,Dingdong Liu,Tingyu Luo,Yuzhen Zheng,Peng Luo,Shun Hu,Jin Lin,Cheng Guo,Yinhe Han,Ying Wang,Mingtang Deng,Junjie Wu,X. Fu*

Main category: quant-ph

TL;DR: This paper introduces Distributed-HISQ, a scalable quantum control architecture that addresses synchronization issues and instruction set abstraction with improved performance metrics.


<details>
  <summary>Details</summary>
Motivation: The rapid growth in qubit counts requires scalable and synchronized quantum control architectures, while existing approaches suffer from inefficiency and lack of standardization.

Method: The authors propose Distributed-HISQ, combining HISQ (an abstract, hardware-agnostic instruction set) and BISP (a zero-cycle booking synchronization protocol).

Result: Distributed-HISQ improves quantum program execution efficiency, achieving a 22.8% reduction in execution time and ~5x reduction in infidelity compared to traditional methods.

Conclusion: Distributed-HISQ provides a scalable and efficient solution for quantum controls, addressing synchronization and unifying instruction sets to enhance system performance and adaptability.

Abstract: The design of a scalable Quantum Control Architecture (QCA) faces two primary
challenges. First, the continuous growth in qubit counts has rendered
distributed QCA inevitable, yet the nondeterministic latencies inherent in
feedback loops demand cycleaccurate synchronization across multiple
controllers. Existing synchronization strategies -- whether lock-step or
demand-driven -- introduce significant performance penalties. Second, existing
quantum instruction set architectures are polarized, being either too abstract
or too granular. This lack of a unifying design necessitates recurrent hardware
customization for each new control requirement, which limits the system's
reconfigurability and impedes the path toward a scalable and unified digital
microarchitecture.
  Addressing these challenges, we propose Distributed-HISQ, featuring: (i)
HISQ, A universal instruction set that redefines quantum control with a
hardware-agnostic design. By decoupling from quantum operation semantics, HISQ
provides a unified language for control sequences, enabling a single
microarchitecture to support various control methods and enhancing system
reconfigurability. (ii) BISP, a booking-based synchronization protocol that can
potentially achieve zero-cycle synchronization overhead. The feasibility and
adaptability of Distributed-HISQ are validated through its implementation on a
commercial quantum control system targeting superconducting qubits. We
performed a comprehensive evaluation using a customized quantum software stack.
Our results show that BISP effectively synchronizes multiple control boards,
leading to a 22.8% reduction in average program execution time and a
$\sim$5$\times$ reduction in infidelity when compared to an existing lock-step
synchronization scheme.

</details>


### [297] [Histogram Driven Amplitude Embedding for Qubit Efficient Quantum Image Compression](https://arxiv.org/abs/2509.04849)
*Sahil Tomar,Sandeep Kumar*

Main category: quant-ph

TL;DR: This paper proposes a quantum-based image compression method using near-term quantum devices.


<details>
  <summary>Details</summary>
Motivation: To efficiently compress color images using quantum devices while minimizing hardware resource usage.

Method: Image is segmented into fixed size blocks (bixels), intensities are computed, a histogram is created and encoded into a quantum state using amplitude embedding. Reconstruction is performed post-measurement.

Result: Reconstructed high-quality images using only 5 to 7 qubits, offering better qubit efficiency compared to traditional methods.

Conclusion: The method is hardware efficient, scalable via histogram bin adjustment, and suitable for current NISQ era quantum systems.

Abstract: This work introduces a compact and hardware efficient method for compressing
color images using near term quantum devices. The approach segments the image
into fixed size blocks called bixels, and computes the total intensity within
each block. A global histogram with B bins is then constructed from these block
intensities, and the normalized square roots of the bin counts are encoded as
amplitudes into an n qubit quantum state. Amplitude embedding is performed
using PennyLane and executed on real IBM Quantum hardware. The resulting state
is measured to reconstruct the histogram, enabling approximate recovery of
block intensities and full image reassembly. The method maintains a constant
qubit requirement based solely on the number of histogram bins, independent of
the resolution of the image. By adjusting B, users can control the trade off
between fidelity and resource usage. Empirical results demonstrate high quality
reconstructions using as few as 5 to 7 qubits, significantly outperforming
conventional pixel level encodings in terms of qubit efficiency and validating
the practical application of the method for current NISQ era quantum systems.

</details>


### [298] [RobQFL: Robust Quantum Federated Learning in Adversarial Environment](https://arxiv.org/abs/2509.04914)
*Walid El Maouaki,Nouhaila Innan,Alberto Marchisio,Taoufik Said,Muhammad Shafique,Mohamed Bennai*

Main category: quant-ph

TL;DR: The study introduces Robust Quantum Federated Learning (RobQFL) to enhance the resistance of Quantum Federated Learning against adversarial noise.


<details>
  <summary>Details</summary>
Motivation: To address the fragility of Quantum Federated Learning (QFL) to adversarial noise, an area that poses significant risk despite QFL's promise of privacy and quantum computing advantages.

Method: RobQFL integrates adversarial training directly into the federated learning loop and evaluates robustness across tunable axes like client coverage, perturbation scheduling, and optimization processes.

Result: Simulations showed that adversarial training of only 20-50% clients boosts robustness significantly with minimal impact on clean-accuracy. Different levels of perturbation scheduling and coverage were tested for optimal results.

Conclusion: Robustness in Quantum Federated Learning can be improved through selective adversarial training, but data heterogeneity remains a key risk factor.

Abstract: Quantum Federated Learning (QFL) merges privacy-preserving federation with
quantum computing gains, yet its resilience to adversarial noise is unknown. We
first show that QFL is as fragile as centralized quantum learning. We propose
Robust Quantum Federated Learning (RobQFL), embedding adversarial training
directly into the federated loop. RobQFL exposes tunable axes: client coverage
$\gamma$ (0-100\%), perturbation scheduling (fixed-$\varepsilon$ vs
$\varepsilon$-mixes), and optimization (fine-tune vs scratch), and distils the
resulting $\gamma \times \varepsilon$ surface into two metrics:
Accuracy-Robustness Area and Robustness Volume. On 15-client simulations with
MNIST and Fashion-MNIST, IID and Non-IID conditions, training only 20-50\%
clients adversarially boosts $\varepsilon \leq 0.1$ accuracy $\sim$15 pp at $<
2$ pp clean-accuracy cost; fine-tuning adds 3-5 pp. With $\geq$75\% coverage, a
moderate $\varepsilon$-mix is optimal, while high-$\varepsilon$ schedules help
only at 100\% coverage. Label-sorted non-IID splits halve robustness,
underscoring data heterogeneity as a dominant risk.

</details>


### [299] [Artificial intelligence for representing and characterizing quantum systems](https://arxiv.org/abs/2509.04923)
*Yuxuan Du,Yan Zhu,Yuan-Hang Zhang,Min-Hsiu Hsieh,Patrick Rebentrost,Weibo Gao,Ya-Dong Wu,Jens Eisert,Giulio Chiribella,Dacheng Tao,Barry C. Sanders*

Main category: quant-ph

TL;DR: The paper reviews the use of AI methodologies, like deep learning and language models, to tackle challenges in characterizing large-scale quantum systems.


<details>
  <summary>Details</summary>
Motivation: Addressing the exponential scaling of Hilbert space with system size in quantum systems remains a central challenge in quantum science.

Method: The paper categorizes AI's integration into quantum characterization tasks into three paradigms: machine learning, deep learning, and language models.

Result: AI is shown to be instrumental in quantum property prediction, creation of state surrogates, and applications like benchmarking and understanding complex quantum phases.

Conclusion: AI has significant potential to advance quantum science, but key challenges and unanswered questions remain at the intersection of AI and quantum systems.

Abstract: Efficient characterization of large-scale quantum systems, especially those
produced by quantum analog simulators and megaquop quantum computers, poses a
central challenge in quantum science due to the exponential scaling of the
Hilbert space with respect to system size. Recent advances in artificial
intelligence (AI), with its aptitude for high-dimensional pattern recognition
and function approximation, have emerged as a powerful tool to address this
challenge. A growing body of research has leveraged AI to represent and
characterize scalable quantum systems, spanning from theoretical foundations to
experimental realizations. Depending on how prior knowledge and learning
architectures are incorporated, the integration of AI into quantum system
characterization can be categorized into three synergistic paradigms: machine
learning, and, in particular, deep learning and language models. This review
discusses how each of these AI paradigms contributes to two core tasks in
quantum systems characterization: quantum property prediction and the
construction of surrogates for quantum states. These tasks underlie diverse
applications, from quantum certification and benchmarking to the enhancement of
quantum algorithms and the understanding of strongly correlated phases of
matter. Key challenges and open questions are also discussed, together with
future prospects at the interface of AI and quantum science.

</details>


### [300] [Exploring an implementation of quantum learning pipeline for support vector machines](https://arxiv.org/abs/2509.04983)
*Mario Bifulco,Luca Roversi*

Main category: quant-ph

TL;DR: This paper proposes a fully quantum pipeline integrating quantum kernel methods and quantum annealing for SVM learning.


<details>
  <summary>Details</summary>
Motivation: To exploit the advantages of quantum computing for enhancing machine learning, particularly SVM, by integrating quantum kernels and quantum annealing.

Method: Quantum kernels are constructed with feature maps and configurations, evaluated using Kernel-Target Alignment, and the SVM dual problem is solved via quantum annealers through its reformulation as a QUBO problem.

Result: Experiments show competitive performance with a model achieving an F1-score of 90%, demonstrating the pipeline's effectiveness.

Conclusion: This study confirms the feasibility and promise of end-to-end quantum pipelines and hybrid quantum architectures in high-performance quantum computing.

Abstract: This work presents a fully quantum approach to support vector machine (SVM)
learning by integrating gate-based quantum kernel methods with quantum
annealing-based optimization. We explore the construction of quantum kernels
using various feature maps and qubit configurations, evaluating their
suitability through Kernel-Target Alignment (KTA). The SVM dual problem is
reformulated as a Quadratic Unconstrained Binary Optimization (QUBO) problem,
enabling its solution via quantum annealers. Our experiments demonstrate that a
high degree of alignment in the kernel and an appropriate regularization
parameter lead to competitive performance, with the best model achieving an
F1-score of 90%. These results highlight the feasibility of an end-to-end
quantum learning pipeline and the potential of hybrid quantum architectures in
quantum high-performance computing (QHPC) contexts.

</details>


### [301] [QCA-MolGAN: Quantum Circuit Associative Molecular GAN with Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.05051)
*Aaron Mark Thomas,Yu-Cheng Chen,Hubert Okadome Valencia,Sharu Theresa Jose,Ronin Wu*

Main category: quant-ph

TL;DR: This study introduces QCA-MolGAN, a hybrid AI model combining quantum circuit Born machines (QCBMs) with Generative Adversarial Networks (GANs), for generating drug-like molecules that meet specific chemical properties.


<details>
  <summary>Details</summary>
Motivation: To address the challenge in drug discovery of exploring vast chemical spaces to find molecules with desired properties by leveraging generative modeling advancements.

Method: A hybrid model, QCA-MolGAN, was developed using quantum circuit Born machines as a prior distribution in the GAN, integrated with multi-agent reinforcement learning to optimize molecular properties like QED, LogP, and SA scores.

Result: QCA-MolGAN achieved better alignment between generated molecules and their desired properties, demonstrating the effectiveness of combining QCBMs with reinforcement learning for drug-like molecular generation.

Conclusion: The study shows the potential of using quantum-enhanced generative models and reinforcement learning to improve drug design processes, optimizing for key chemical properties.

Abstract: Navigating the vast chemical space of molecular structures to design novel
drug molecules with desired target properties remains a central challenge in
drug discovery. Recent advances in generative models offer promising solutions.
This work presents a novel quantum circuit Born machine (QCBM)-enabled
Generative Adversarial Network (GAN), called QCA-MolGAN, for generating
drug-like molecules. The QCBM serves as a learnable prior distribution, which
is associatively trained to define a latent space aligning with high-level
features captured by the GANs discriminator. Additionally, we integrate a novel
multi-agent reinforcement learning network to guide molecular generation with
desired targeted properties, optimising key metrics such as quantitative
estimate of drug-likeness (QED), octanol-water partition coefficient (LogP) and
synthetic accessibility (SA) scores in conjunction with one another.
Experimental results demonstrate that our approach enhances the property
alignment of generated molecules with the multi-agent reinforcement learning
agents effectively balancing chemical properties.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [302] [REMOTE: A Unified Multimodal Relation Extraction Framework with Multilevel Optimal Transport and Mixture-of-Experts](https://arxiv.org/abs/2509.04844)
*Xinkui Lin,Yongxiu Xu,Minghao Tang,Shilong Zhang,Hongbo Xu,Hao Xu,Yubin Wang*

Main category: cs.MM

TL;DR: The paper introduces REMOTE, a framework for unified multimodal relation extraction (MRE) that handles intra- and inter-modal relations, achieving state-of-the-art performance on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Existing MRE models are limited to single-type triplet extraction and fail to capture dynamic cross-modal interactions effectively, leading to reduced scalability and computational inefficiency.

Method: REMOTE utilizes a mixture-of-experts mechanism for selecting optimal interaction features and a multilevel optimal transport fusion module to preserve low-level features while maintaining multilayer encoding.

Result: Extensive experiments demonstrate that REMOTE extracts various types of relational triplets effectively and outperforms other models on multiple MRE datasets, including the UMRE dataset created by the researchers.

Conclusion: REMOTE advances the field of MRE by dynamically optimizing cross-modal interactions and preserving low-level features with high effectiveness and scalability, setting a new benchmark for multimodal relation extraction.

Abstract: Multimodal relation extraction (MRE) is a crucial task in the fields of
Knowledge Graph and Multimedia, playing a pivotal role in multimodal knowledge
graph construction. However, existing methods are typically limited to
extracting a single type of relational triplet, which restricts their ability
to extract triplets beyond the specified types. Directly combining these
methods fails to capture dynamic cross-modal interactions and introduces
significant computational redundancy. Therefore, we propose a novel
\textit{unified multimodal Relation Extraction framework with Multilevel
Optimal Transport and mixture-of-Experts}, termed REMOTE, which can
simultaneously extract intra-modal and inter-modal relations between textual
entities and visual objects. To dynamically select optimal interaction features
for different types of relational triplets, we introduce mixture-of-experts
mechanism, ensuring the most relevant modality information is utilized.
Additionally, considering that the inherent property of multilayer sequential
encoding in existing encoders often leads to the loss of low-level information,
we adopt a multilevel optimal transport fusion module to preserve low-level
features while maintaining multilayer encoding, yielding more expressive
representations. Correspondingly, we also create a Unified Multimodal Relation
Extraction (UMRE) dataset to evaluate the effectiveness of our framework,
encompassing diverse cases where the head and tail entities can originate from
either text or image. Extensive experiments show that REMOTE effectively
extracts various types of relational triplets and achieves state-of-the-art
performanc on almost all metrics across two other public MRE datasets. We
release our resources at https://github.com/Nikol-coder/REMOTE.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [303] [Unified Representation Learning for Multi-Intent Diversity and Behavioral Uncertainty in Recommender Systems](https://arxiv.org/abs/2509.04694)
*Wei Xu,Jiasen Zheng,Junjiang Lin,Mingxuan Han,Junliang Du*

Main category: cs.IR

TL;DR: The paper presents a unified representation learning framework for improving recommender systems by addressing user intent diversity and behavioral uncertainty, achieving better accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the limitation of traditional recommender systems in handling diverse user intents and uncertain behaviors, which undermine their performance and adaptability.

Method: The proposed method includes a multi-intent representation module using attention-based latent intent vectors and an uncertainty modeling mechanism leveraging Gaussian distributions to account for behavioral ambiguity.

Result: The proposed framework achieved superior performance on public datasets, outperforming existing models in multiple metrics and demonstrating robustness under cold-start and behavioral noise scenarios.

Conclusion: The study confirms the effectiveness of a unified modeling strategy combining multi-intent and uncertainty modeling to improve recommendation systems in real-world applications.

Abstract: This paper addresses the challenge of jointly modeling user intent diversity
and behavioral uncertainty in recommender systems. A unified representation
learning framework is proposed. The framework builds a multi-intent
representation module and an uncertainty modeling mechanism. It extracts
multi-granularity interest structures from user behavior sequences. Behavioral
ambiguity and preference fluctuation are captured using Bayesian distribution
modeling. In the multi-intent modeling part, the model introduces multiple
latent intent vectors. These vectors are weighted and fused using an attention
mechanism to generate semantically rich representations of long-term user
preferences. In the uncertainty modeling part, the model learns the mean and
covariance of behavior representations through Gaussian distributions. This
reflects the user's confidence in different behavioral contexts. Next, a
learnable fusion strategy is used to combine long-term intent and short-term
behavior signals. This produces the final user representation, improving both
recommendation accuracy and robustness. The method is evaluated on standard
public datasets. Experimental results show that it outperforms existing
representative models across multiple metrics. It also demonstrates greater
stability and adaptability under cold-start and behavioral disturbance
scenarios. The approach alleviates modeling bottlenecks faced by traditional
methods when dealing with complex user behavior. These findings confirm the
effectiveness and practical value of the unified modeling strategy in
real-world recommendation tasks.

</details>


### [304] [Multimodal Foundation Model-Driven User Interest Modeling and Behavior Analysis on Short Video Platforms](https://arxiv.org/abs/2509.04751)
*Yushang Zhao,Yike Peng,Li Zhang,Qianyi Sun,Zhihui Zhang,Yingying Zhuang*

Main category: cs.IR

TL;DR: This paper introduces a multimodal framework for user interest modeling and behavior analysis on short video platforms, significantly improving recommendation accuracy and transparency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of traditional unimodal data methods in capturing user preferences within the complex multimodal environment of short video platforms.

Method: The paper integrates multimodal inputs including video frames, textual descriptions, and music into unified semantic spaces using cross-modal alignment. It incorporates behavior-driven feature embeddings to model dynamic user interest evolution.

Result: Experimental evaluations show improvements in behavior prediction accuracy, cold-start user modeling, and recommendation click-through rates compared to existing methods.

Conclusion: The proposed multimodal framework enhances accuracy, interpretability, and transparency in personalized recommendation systems on short video platforms.

Abstract: With the rapid expansion of user bases on short video platforms, personalized
recommendation systems are playing an increasingly critical role in enhancing
user experience and optimizing content distribution. Traditional interest
modeling methods often rely on unimodal data, such as click logs or text
labels, which limits their ability to fully capture user preferences in a
complex multimodal content environment. To address this challenge, this paper
proposes a multimodal foundation model-based framework for user interest
modeling and behavior analysis. By integrating video frames, textual
descriptions, and background music into a unified semantic space using
cross-modal alignment strategies, the framework constructs fine-grained user
interest vectors. Additionally, we introduce a behavior-driven feature
embedding mechanism that incorporates viewing, liking, and commenting sequences
to model dynamic interest evolution, thereby improving both the timeliness and
accuracy of recommendations. In the experimental phase, we conduct extensive
evaluations using both public and proprietary short video datasets, comparing
our approach against multiple mainstream recommendation algorithms and modeling
techniques. Results demonstrate significant improvements in behavior prediction
accuracy, interest modeling for cold-start users, and recommendation
click-through rates. Moreover, we incorporate interpretability mechanisms using
attention weights and feature visualization to reveal the model's decision
basis under multimodal inputs and trace interest shifts, thereby enhancing the
transparency and controllability of the recommendation system.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [305] [Filtering with Randomised Observations: Sequential Learning of Relevant Subspace Properties and Accuracy Analysis](https://arxiv.org/abs/2509.04867)
*Nazanin Abedini,Jana de Wiljes,Svetlana Dubinkina*

Main category: math.NA

TL;DR: This paper studies the performance of ensemble Kalman filtering under various observation conditions, providing error bounds and proposing an adaptive learning scheme for balancing complexity and accuracy.


<details>
  <summary>Details</summary>
Motivation: The authors aim to enhance the state estimation process, which is crucial for applications relying on the combination of observational data and mathematical models, by addressing challenges associated with partial observations.

Method: The paper rigorously analyzes signal-tracking errors for ensemble Kalman filters under fixed, randomized, and adaptive observations. It also introduces an adaptive sequential learning scheme to optimize the state subspace dimension.

Result: The study provides bounds for signal-tracking errors and demonstrates how the adaptive scheme effectively balances observational complexity and estimation accuracy, identifying the optimal filter-relevant subspace size.

Conclusion: The findings improve understanding of ensemble Kalman filters' performance and introduce a practical adaptive method for error control and subspace dimension determination in state estimation tasks.

Abstract: State estimation that combines observational data with mathematical models is
central to many applications and is commonly addressed through filtering
methods, such as ensemble Kalman filters. In this article, we examine the
signal-tracking performance of a continuous ensemble Kalman filtering under
fixed, randomised, and adaptively varying partial observations. Rigorous bounds
are established for the expected signal-tracking error relative to the
randomness of the observation operator. In addition, we propose a sequential
learning scheme that adaptively determines the dimension of a state subspace
sufficient to ensure bounded filtering error, by balancing observation
complexity with estimation accuracy. Beyond error control, the adaptive scheme
provides a systematic approach to identifying the appropriate size of the
filter-relevant subspace of the underlying dynamics.

</details>


### [306] [Uncertain but Useful: Leveraging CNN Variability into Data Augmentation](https://arxiv.org/abs/2509.05238)
*Inés Gonzalez-Pepe,Vinuyan Sivakolunthu,Yohan Chatelain,Tristan Glatard*

Main category: math.NA

TL;DR: Deep learning training introduces variability, impacting stability in neuroimaging. However, this variability can be leveraged as a resource for improving robustness and enabling new applications.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored area of training-time variability in deep learning models applied to neuroimaging, which can affect numerical stability and reproducibility.

Method: Analysis of training-time variability in a DL-based segmentation pipeline, FastSurfer, by introducing controlled perturbations like floating-point deviations and random seeds.

Result: FastSurfer shows higher variability compared to traditional neuroimaging pipelines, perturbation ensembles match baseline performances, and variability creates numerical model families useful for downstream applications.

Conclusion: Training-time variability, while a reproducibility concern, can be reframed as a resource for enhancing robustness and developing novel neuroimaging tools.

Abstract: Deep learning (DL) is rapidly advancing neuroimaging by achieving
state-of-the-art performance with reduced computation times. Yet the numerical
stability of DL models -- particularly during training -- remains
underexplored. While inference with DL is relatively stable, training
introduces additional variability primarily through iterative stochastic
optimization. We investigate this training-time variability using FastSurfer, a
CNN-based whole-brain segmentation pipeline. Controlled perturbations are
introduced via floating point perturbations and random seeds. We find that: (i)
FastSurfer exhibits higher variability compared to that of a traditional
neuroimaging pipeline, suggesting that DL inherits and is particularly
susceptible to sources of instability present in its predecessors; (ii)
ensembles generated with perturbations achieve performance similar to an
unperturbed baseline; and (iii) variability effectively produces ensembles of
numerical model families that can be repurposed for downstream applications. As
a proof of concept, we demonstrate that numerical ensembles can be used as a
data augmentation strategy for brain age regression. These findings position
training-time variability not only as a reproducibility concern but also as a
resource that can be harnessed to improve robustness and enable new
applications in neuroimaging.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [307] [Ecologically Valid Benchmarking and Adaptive Attention: Scalable Marine Bioacoustic Monitoring](https://arxiv.org/abs/2509.04682)
*Nicholas R. Rasmussen,Rodrigue Rizk,Longwei Wang,KC Santosh*

Main category: cs.SD

TL;DR: The paper introduces GetNetUPAM, a robust evaluation framework for underwater passive acoustic monitoring, and proposes ARPA-N, a novel neural architecture achieving superior precision and consistency in target sound detection.


<details>
  <summary>Details</summary>
Motivation: Existing models for underwater passive acoustic monitoring face challenges from environmental noise variability and signal complexity, limiting their stability and generalization.

Method: The authors developed GetNetUPAM, a hierarchical nested cross-validation framework using site-year segmentation to ensure evaluation against ecological variability. Alongside, they designed ARPA-N, a neural network leveraging adaptive pooling and spatial attention for irregular spectrograms.

Result: ARPA-N outperforms DenseNet baselines by 14.4% in average precision and significantly reduces variability across metrics under the GetNetUPAM framework.

Conclusion: The combination of GetNetUPAM and ARPA-N sets new standards for scalable and accurate bioacoustic monitoring by addressing variability and improving detection performance.

Abstract: Underwater Passive Acoustic Monitoring (UPAM) provides rich spatiotemporal
data for long-term ecological analysis, but intrinsic noise and complex signal
dependencies hinder model stability and generalization. Multilayered windowing
has improved target sound localization, yet variability from shifting ambient
noise, diverse propagation effects, and mixed biological and anthropogenic
sources demands robust architectures and rigorous evaluation. We introduce
GetNetUPAM, a hierarchical nested cross-validation framework designed to
quantify model stability under ecologically realistic variability. Data are
partitioned into distinct site-year segments, preserving recording
heterogeneity and ensuring each validation fold reflects a unique environmental
subset, reducing overfitting to localized noise and sensor artifacts. Site-year
blocking enforces evaluation against genuine environmental diversity, while
standard cross-validation on random subsets measures generalization across
UPAM's full signal distribution, a dimension absent from current benchmarks.
Using GetNetUPAM as the evaluation backbone, we propose the Adaptive Resolution
Pooling and Attention Network (ARPA-N), a neural architecture for irregular
spectrogram dimensions. Adaptive pooling with spatial attention extends the
receptive field, capturing global context without excessive parameters. Under
GetNetUPAM, ARPA-N achieves a 14.4% gain in average precision over DenseNet
baselines and a log2-scale order-of-magnitude drop in variability across all
metrics, enabling consistent detection across site-year folds and advancing
scalable, accurate bioacoustic monitoring.

</details>


### [308] [Learning and composing of classical music using restricted Boltzmann machines](https://arxiv.org/abs/2509.04899)
*Mutsumi Kobayashi,Hiroshi Watanabe*

Main category: cs.SD

TL;DR: The paper uses a Restricted Boltzmann Machine (RBM) to generate music in J.S. Bach's style, focusing on analyzing its internal states.


<details>
  <summary>Details</summary>
Motivation: To understand how machine learning models grasp the characteristics of a composer's music, specifically J.S. Bach, using simpler models like RBMs for better interpretability.

Method: Trained a Restricted Boltzmann Machine (RBM) on J.S. Bach's music to allow for analysis of the model’s internal states post-learning.

Result: Demonstrated that the trained RBM could successfully compose music in the style of J.S. Bach.

Conclusion: RBMs, with their simple structure, not only model music effectively but also offer clarity in understanding the learned representation of a composer’s musical style.

Abstract: Recently, software has been developed that uses machine learning to mimic the
style of a particular composer, such as J. S. Bach. However, since such
software often adopts machine learning models with complex structures, it is
difficult to analyze how the software understands the characteristics of the
composer's music. In this study, we adopted J. S. Bach's music for training of
a restricted Boltzmann machine (RBM). Since the structure of RBMs is simple, it
allows us to investigate the internal states after learning. We found that the
learned RBM is able to compose music.

</details>


### [309] [MAIA: An Inpainting-Based Approach for Music Adversarial Attacks](https://arxiv.org/abs/2509.04980)
*Yuxuan Liu,Peihong Zhang,Rui Sang,Zhixin Li,Shengchen Li*

Main category: cs.SD

TL;DR: The paper introduces MAIA, a novel framework for adversarial attacks on Music Information Retrieval (MIR) systems that uses generative inpainting for subtle perturbations, demonstrating high success rates and minimal perceptual distortion.


<details>
  <summary>Details</summary>
Motivation: To expose vulnerabilities in Music Information Retrieval (MIR) systems and emphasize the need for enhanced security in such models.

Method: The method involves identifying critical audio segments through importance analysis, modifying them with generative inpainting models, and ensuring minimal perceptual distortion in both white-box and black-box attack scenarios.

Result: The proposed MAIA framework achieves high attack success rates in various MIR tasks while maintaining imperceptible changes in audio quality.

Conclusion: Current MIR systems are vulnerable to adversarial attacks, and the findings underscore the necessity of developing more robust and secure models to counter such attack strategies.

Abstract: Music adversarial attacks have garnered significant interest in the field of
Music Information Retrieval (MIR). In this paper, we present Music Adversarial
Inpainting Attack (MAIA), a novel adversarial attack framework that supports
both white-box and black-box attack scenarios. MAIA begins with an importance
analysis to identify critical audio segments, which are then targeted for
modification. Utilizing generative inpainting models, these segments are
reconstructed with guidance from the output of the attacked model, ensuring
subtle and effective adversarial perturbations. We evaluate MAIA on multiple
MIR tasks, demonstrating high attack success rates in both white-box and
black-box settings while maintaining minimal perceptual distortion.
Additionally, subjective listening tests confirm the high audio fidelity of the
adversarial samples. Our findings highlight vulnerabilities in current MIR
systems and emphasize the need for more robust and secure models.

</details>


### [310] [WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning](https://arxiv.org/abs/2509.04744)
*Gagan Mundada,Yash Vishe,Amit Namburi,Xin Xu,Zachary Novack,Julian McAuley,Junda Wu*

Main category: cs.SD

TL;DR: The paper introduces WildScore, a benchmark that evaluates the reasoning abilities of Multimodal Large Language Models (MLLMs) in interpreting and analyzing symbolic music.


<details>
  <summary>Details</summary>
Motivation: The authors aim to explore the uncharted domain of MLLMs' reasoning capabilities in symbolic music, given their success in vision-language tasks.

Method: WildScore is built from real-world music scores with accompanying user-generated questions, structured via a taxonomy of musicological categories. The reasoning tasks are designed as multiple-choice questions for standardized evaluation.

Result: Experiments with current MLLMs on WildScore reveal both promising insights and significant challenges in their symbolic music reasoning performance.

Conclusion: WildScore provides a unique tool for understanding and improving the symbolic music reasoning of MLLMs, opening avenues for future research in this area.

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated
impressive capabilities across various vision-language tasks. However, their
reasoning abilities in the multimodal symbolic music domain remain largely
unexplored. We introduce WildScore, the first in-the-wild multimodal symbolic
music reasoning and analysis benchmark, designed to evaluate MLLMs' capacity to
interpret real-world music scores and answer complex musicological queries.
Each instance in WildScore is sourced from genuine musical compositions and
accompanied by authentic user-generated questions and discussions, capturing
the intricacies of practical music analysis. To facilitate systematic
evaluation, we propose a systematic taxonomy, comprising both high-level and
fine-grained musicological ontologies. Furthermore, we frame complex music
reasoning as multiple-choice question answering, enabling controlled and
scalable assessment of MLLMs' symbolic music understanding. Empirical
benchmarking of state-of-the-art MLLMs on WildScore reveals intriguing patterns
in their visual-symbolic reasoning, uncovering both promising directions and
persistent challenges for MLLMs in symbolic music reasoning and analysis. We
release the dataset and code.

</details>


### [311] [Recomposer: Event-roll-guided generative audio editing](https://arxiv.org/abs/2509.05256)
*Daniel P. W. Ellis,Eduardo Fonseca,Ron J. Weiss,Kevin Wilson,Scott Wisdom,Hakan Erdogan,John R. Hershey,Aren Jansen,R. Channing Moore,Manoj Plakal*

Main category: cs.SD

TL;DR: This paper proposes a method to edit individual sound events in complex sound scenes using textual descriptions and graphical timing representations.


<details>
  <summary>Details</summary>
Motivation: The challenge of editing individual sound sources in complex real-world sound scenes arises from overlapping events, and generative models offer potential solutions by leveraging a strong prior understanding of the audio domain.

Method: The authors developed an encoder-decoder transformer model working on SoundStream representations, trained with synthetic audio pair examples that simulate real-world editing scenarios by modifying dense audio scenes.

Result: The study highlights the effectiveness of using textual descriptions (action, class, timing) in improving sound event editing tasks and provides evaluations supporting this finding.

Conclusion: The work introduces the concept of "recomposition" and demonstrates its practicality and significance in sound scene editing applications.

Abstract: Editing complex real-world sound scenes is difficult because individual sound
sources overlap in time. Generative models can fill-in missing or corrupted
details based on their strong prior understanding of the data domain. We
present a system for editing individual sound events within complex scenes able
to delete, insert, and enhance individual sound events based on textual edit
descriptions (e.g., ``enhance Door'') and a graphical representation of the
event timing derived from an ``event roll'' transcription. We present an
encoder-decoder transformer working on SoundStream representations, trained on
synthetic (input, desired output) audio example pairs formed by adding isolated
sound events to dense, real-world backgrounds. Evaluation reveals the
importance of each part of the edit descriptions -- action, class, timing. Our
work demonstrates ``recomposition'' is an important and practical application.

</details>
