<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 49]
- [cs.AR](#cs.AR) [Total: 6]
- [cs.CL](#cs.CL) [Total: 69]
- [cs.CV](#cs.CV) [Total: 137]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.LG](#cs.LG) [Total: 108]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.PF](#cs.PF) [Total: 4]
- [cs.PL](#cs.PL) [Total: 7]
- [cs.RO](#cs.RO) [Total: 49]
- [cs.SE](#cs.SE) [Total: 23]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [stat.ML](#stat.ML) [Total: 13]
- [stat.ME](#stat.ME) [Total: 2]
- [astro-ph.IM](#astro-ph.IM) [Total: 2]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.GR](#cs.GR) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [econ.EM](#econ.EM) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [math.OC](#math.OC) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.CR](#cs.CR) [Total: 22]
- [eess.AS](#eess.AS) [Total: 3]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [math.CT](#math.CT) [Total: 1]
- [cs.CY](#cs.CY) [Total: 11]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Attention of a Kiss: Exploring Attention Maps in Video Diffusion for XAIxArts](https://arxiv.org/abs/2509.05323)
*Adam Cole,Mick Grierson*

Main category: cs.AI

TL;DR: The paper explores attention mechanisms in video diffusion transformers by visualizing cross-attention maps for text-to-video models, offering insights for both analysis and artistic use.


<details>
  <summary>Details</summary>
Motivation: The motivation is to bridge the gap between analytical and artistic applications of AI by investigating and visualizing attention maps, enabling better understanding and creative possibilities.

Method: The method involves extracting and visualizing cross-attention maps from the open-source Wan model and analyzing their temporal and spatial behavior in generative video models.

Result: The study demonstrates the interpretability of attention maps and explores their dual role as analytical tools and artistic resources through experiments and an artistic case study.

Conclusion: The work advances Explainable AI in the arts, emphasizing the potential of AI components like attention maps as creative media for artists.

Abstract: This paper presents an artistic and technical investigation into the
attention mechanisms of video diffusion transformers. Inspired by early video
artists who manipulated analog video signals to create new visual aesthetics,
this study proposes a method for extracting and visualizing cross-attention
maps in generative video models. Built on the open-source Wan model, our tool
provides an interpretable window into the temporal and spatial behavior of
attention in text-to-video generation. Through exploratory probes and an
artistic case study, we examine the potential of attention maps as both
analytical tools and raw artistic material. This work contributes to the
growing field of Explainable AI for the Arts (XAIxArts), inviting artists to
reclaim the inner workings of AI as a creative medium.

</details>


### [2] [Perception Graph for Cognitive Attack Reasoning in Augmented Reality](https://arxiv.org/abs/2509.05324)
*Rongqian Chen,Shu Hong,Rifatul Islam,Mahdi Imani,G. Gary Tan,Tian Lan*

Main category: cs.AI

TL;DR: The paper introduces the Perception Graph, a model designed to analyze and detect cognitive attacks on AR systems by assessing perception distortion.


<details>
  <summary>Details</summary>
Motivation: With AR systems increasingly deployed in tactical environments, they face vulnerabilities from cognitive attacks, which can manipulate user perception and compromise decision-making. There is a need for tools to counter these threats.

Method: The Perception Graph mimics human perception by interpreting key information in AR environments and representing it in a structured, semantic form. It calculates a quantitative perception distortion score to identify and assess cognitive attacks.

Result: The proposed model effectively quantifies the degree of perception distortion, offering a measurable approach to detect and analyze cognitive attacks in AR systems.

Conclusion: Perception Graph provides an innovative and practical tool for enhancing the security of AR systems against cognitive attacks, ensuring more reliable user decision-making.

Abstract: Augmented reality (AR) systems are increasingly deployed in tactical
environments, but their reliance on seamless human-computer interaction makes
them vulnerable to cognitive attacks that manipulate a user's perception and
severely compromise user decision-making. To address this challenge, we
introduce the Perception Graph, a novel model designed to reason about human
perception within these systems. Our model operates by first mimicking the
human process of interpreting key information from an MR environment and then
representing the outcomes using a semantically meaningful structure. We
demonstrate how the model can compute a quantitative score that reflects the
level of perception distortion, providing a robust and measurable method for
detecting and analyzing the effects of such cognitive attacks.

</details>


### [3] [SynDelay: A Synthetic Dataset for Delivery Delay Prediction](https://arxiv.org/abs/2509.05325)
*Liming Xu,Yunbo Long,Alexandra Brintrup*

Main category: cs.AI

TL;DR: The paper introduces SynDelay, a synthetic dataset for delivery delay prediction in supply chain management, addressing the scarcity of high-quality, open datasets.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the lack of openly available, high-quality datasets that hinder progress and benchmarking in AI-driven predictive tasks for supply chain management.

Method: SynDelay is created using an advanced generative model trained on real-world data to simulate realistic delivery patterns while preserving data privacy.

Result: The SynDelay dataset provides a challenging testbed for predictive modeling and includes baseline benchmarks and metrics to support its adoption by the research community.

Conclusion: SynDelay is publicly accessible and aims to stimulate collaboration in supply chain AI by encouraging contributions of datasets, models, and evaluations, enhancing progress in the field.

Abstract: Artificial intelligence (AI) is transforming supply chain management, yet
progress in predictive tasks -- such as delivery delay prediction -- remains
constrained by the scarcity of high-quality, openly available datasets.
Existing datasets are often proprietary, small, or inconsistently maintained,
hindering reproducibility and benchmarking. We present SynDelay, a synthetic
dataset designed for delivery delay prediction. Generated using an advanced
generative model trained on real-world data, SynDelay preserves realistic
delivery patterns while ensuring privacy. Although not entirely free of noise
or inconsistencies, it provides a challenging and practical testbed for
advancing predictive modelling. To support adoption, we provide baseline
results and evaluation metrics as initial benchmarks, serving as reference
points rather than state-of-the-art claims. SynDelay is publicly available
through the Supply Chain Data Hub, an open initiative promoting dataset sharing
and benchmarking in supply chain AI. We encourage the community to contribute
datasets, models, and evaluation practices to advance research in this area.
All code is openly accessible at https://supplychaindatahub.org.

</details>


### [4] [MVRS: The Multimodal Virtual Reality Stimuli-based Emotion Recognition Dataset](https://arxiv.org/abs/2509.05330)
*Seyed Muhammad Hossein Mousavi,Atiye Ilanloo*

Main category: cs.AI

TL;DR: The paper introduces the MVRS multimodal dataset, designed to advance emotion recognition in AI using synchronized VR-induced stimuli and various signal modalities.


<details>
  <summary>Details</summary>
Motivation: Emotion recognition in AI suffers from a lack of multimodal datasets involving body motion and physiological signals, limiting advancements in healthcare and other applications.

Method: The MVRS dataset collects synchronized VR-based emotional stimuli data from 13 participants through eye tracking, body motion, EMG, and GSR signals, extracting features and evaluating them via fusion techniques and classifiers.

Result: The features from the dataset's modalities were successfully fused and classified, demonstrating strong emotion separability and validating the quality of the dataset.

Conclusion: MVRS is a significant addition to multimodal affective computing and paves the way for improved AI emotion recognition research.

Abstract: Automatic emotion recognition has become increasingly important with the rise
of AI, especially in fields like healthcare, education, and automotive systems.
However, there is a lack of multimodal datasets, particularly involving body
motion and physiological signals, which limits progress in the field. To
address this, the MVRS dataset is introduced, featuring synchronized recordings
from 13 participants aged 12 to 60 exposed to VR based emotional stimuli
(relaxation, fear, stress, sadness, joy). Data were collected using eye
tracking (via webcam in a VR headset), body motion (Kinect v2), and EMG and GSR
signals (Arduino UNO), all timestamp aligned. Participants followed a unified
protocol with consent and questionnaires. Features from each modality were
extracted, fused using early and late fusion techniques, and evaluated with
classifiers to confirm the datasets quality and emotion separability, making
MVRS a valuable contribution to multimodal affective computing.

</details>


### [5] [Benchmarking Large Language Models for Personalized Guidance in AI-Enhanced Learning](https://arxiv.org/abs/2509.05346)
*Bo Yuan,Jiazi Hu*

Main category: cs.AI

TL;DR: This paper compares three state-of-the-art LLMs in simulating tutoring tasks and finds GPT-4o most effective in generating feedback.


<details>
  <summary>Details</summary>
Motivation: The study aims to evaluate LLMs for personalized learning roles and conduct head-to-head comparisons in realistic learning scenarios.

Method: The researchers use a dataset of student quizzes and analyze LLM outputs along dimensions like accuracy, clarity, and appropriateness using the Gemini virtual judge.

Result: GPT-4o outperformed DeepSeek-V3 and GLM-4.5 by producing more informative and structured tutoring feedback.

Conclusion: The paper concludes that deploying LLMs is promising for personalized learning and suggests methodologies for future research.

Abstract: While Large Language Models (LLMs) are increasingly envisioned as intelligent
assistants for personalized learning, systematic head-to-head evaluations
within authentic learning scenarios remain limited. This study conducts an
empirical comparison of three state-of-the-art LLMs on a tutoring task that
simulates a realistic learning setting. Using a dataset comprising a student's
answers to ten questions of mixed formats with correctness labels, each LLM is
required to (i) analyze the quiz to identify underlying knowledge components,
(ii) infer the student's mastery profile, and (iii) generate targeted guidance
for improvement. To mitigate subjectivity and evaluator bias, we employ Gemini
as a virtual judge to perform pairwise comparisons along various dimensions:
accuracy, clarity, actionability, and appropriateness. Results analyzed via the
Bradley-Terry model indicate that GPT-4o is generally preferred, producing
feedback that is more informative and better structured than its counterparts,
while DeepSeek-V3 and GLM-4.5 demonstrate intermittent strengths but lower
consistency. These findings highlight the feasibility of deploying LLMs as
advanced teaching assistants for individualized support and provide
methodological guidance for future empirical research on LLM-driven
personalized learning.

</details>


### [6] [SasAgent: Multi-Agent AI System for Small-Angle Scattering Data Analysis](https://arxiv.org/abs/2509.05363)
*Lijie Ding,Changwoo Do*

Main category: cs.AI

TL;DR: SasAgent, a multi-agent AI system using LLMs, automates small-angle scattering (SAS) data analysis, featuring specialized tools derived from SasView Python library.


<details>
  <summary>Details</summary>
Motivation: To simplify and automate complex SAS data analysis workflows using AI-driven systems.

Method: SasAgent employs LLMs and a multi-agent structure, combining tools like SLD calculator, bump fitting tool, and retrieval-augmented generation documentation tool, managed via a coordinator agent.

Result: Demonstrated ability to process complex prompts, perform SLD calculations, generate synthetic data, and fit experimental datasets with high accuracy in SAS research.

Conclusion: LLM-driven AI systems like SasAgent can significantly enhance scientific workflows and automation in small-angle scattering data analysis.

Abstract: We introduce SasAgent, a multi-agent AI system powered by large language
models (LLMs) that automates small-angle scattering (SAS) data analysis by
leveraging tools from the SasView software and enables user interaction via
text input. SasAgent features a coordinator agent that interprets user prompts
and delegates tasks to three specialized agents for scattering length density
(SLD) calculation, synthetic data generation, and experimental data fitting.
These agents utilize LLM-friendly tools to execute tasks efficiently. These
tools, including the model data tool, Retrieval-Augmented Generation (RAG)
documentation tool, bump fitting tool, and SLD calculator tool, are derived
from the SasView Python library. A user-friendly Gradio-based interface
enhances user accessibility. Through diverse examples, we demonstrate
SasAgent's ability to interpret complex prompts, calculate SLDs, generate
accurate scattering data, and fit experimental datasets with high precision.
This work showcases the potential of LLM-driven AI systems to streamline
scientific workflows and enhance automation in SAS research.

</details>


### [7] [Characterizing Fitness Landscape Structures in Prompt Engineering](https://arxiv.org/abs/2509.05375)
*Arend Hintze*

Main category: cs.AI

TL;DR: The paper analyzes the optimization landscape of prompt engineering for language models using autocorrelation of semantic embeddings. It identifies contrasting topologies for systematic and diversified prompt generation.


<details>
  <summary>Details</summary>
Motivation: There is limited understanding of the underlying optimization landscapes governing prompt engineering for language models, making it necessary to characterize these landscapes for better optimization strategies.

Method: The study uses autocorrelation analysis of semantic embedding spaces to characterize landscapes. It experiments with systematic enumeration (1,024 prompts) and diversity-driven generation (1,000 prompts) across error detection tasks.

Result: Systematic prompt generation reveals smooth, decaying landscape patterns, while diversified generation shows rugged and hierarchically structured landscapes. Analysis across 10 error detection categories highlights variability in ruggedness.

Conclusion: The findings provide insights into prompt engineering optimization, suggesting the need for task-specific tailoring due to varying landscape topologies.

Abstract: While prompt engineering has emerged as a crucial technique for optimizing
large language model performance, the underlying optimization landscape remains
poorly understood. Current approaches treat prompt optimization as a black-box
problem, applying sophisticated search algorithms without characterizing the
landscape topology they navigate. We present a systematic analysis of fitness
landscape structures in prompt engineering using autocorrelation analysis
across semantic embedding spaces. Through experiments on error detection tasks
with two distinct prompt generation strategies -- systematic enumeration (1,024
prompts) and novelty-driven diversification (1,000 prompts) -- we reveal
fundamentally different landscape topologies. Systematic prompt generation
yields smoothly decaying autocorrelation, while diversified generation exhibits
non-monotonic patterns with peak correlation at intermediate semantic
distances, indicating rugged, hierarchically structured landscapes.
Task-specific analysis across 10 error detection categories reveals varying
degrees of ruggedness across different error types. Our findings provide an
empirical foundation for understanding the complexity of optimization in prompt
engineering landscapes.

</details>


### [8] [Code Like Humans: A Multi-Agent Solution for Medical Coding](https://arxiv.org/abs/2509.05378)
*Andreas Motzfeldt,Joakim Edin,Casper L. Christensen,Christian Hardmeier,Lars Maaløe,Anna Rogers*

Main category: cs.AI

TL;DR: The study introduces a framework called "Code Like Humans" using large language models for medical coding, supporting all ICD-10 codes and improving performance on rare diagnoses.


<details>
  <summary>Details</summary>
Motivation: Inaccuracies and limitations in existing medical coding methods, particularly with handling a high number of labels and rare diagnoses.

Method: The framework uses large language models and aligns with official coding guidelines to map clinical notes to the full ICD-10 coding system (over 70K labels).

Result: Improved performance on rare diagnosis codes, while fine-tuned classifiers maintain an advantage on high-frequency codes.

Conclusion: The framework enhances medical coding capabilities and highlights system limitations, providing directions for addressing blind spots in coding performance.

Abstract: In medical coding, experts map unstructured clinical notes to alphanumeric
codes for diagnoses and procedures. We introduce Code Like Humans: a new
agentic framework for medical coding with large language models. It implements
official coding guidelines for human experts, and it is the first solution that
can support the full ICD-10 coding system (+70K labels). It achieves the best
performance to date on rare diagnosis codes (fine-tuned discriminative
classifiers retain an advantage for high-frequency codes, to which they are
limited). Towards future work, we also contribute an analysis of system
performance and identify its `blind spots' (codes that are systematically
undercoded).

</details>


### [9] [Murphys Laws of AI Alignment: Why the Gap Always Wins](https://arxiv.org/abs/2509.05381)
*Madhava Gaikwad*

Main category: cs.AI

TL;DR: This paper explores why alignment failures occur in language models trained with human feedback, proposing new frameworks and categorizations to address them.


<details>
  <summary>Details</summary>
Motivation: There are recurring failure modes in language models aligned using RLHF and similar methods, such as reward hacking and misgeneralization, which necessitate a deeper understanding and refined approach.

Method: The paper introduces the concept of an 'Alignment Gap,' uses KL-tilting formalism to analyze failure dynamics, categorizes failures as Murphy's Laws of AI Alignment, frames trade-offs with the 'Alignment Trilemma,' and proposes the MAPS framework for addressing them.

Result: Empirical studies support the ideas, with the MAPS framework and alignment concepts providing new perspectives for addressing alignment challenges.

Conclusion: The study reframes alignment debates by focusing on structural limits and trade-offs, offering practical frameworks and insights for improving language model alignment.

Abstract: Large language models are increasingly aligned to human preferences through
reinforcement learning from human feedback (RLHF) and related methods such as
Direct Preference Optimization (DPO), Constitutional AI, and RLAIF. While
effective, these methods exhibit recurring failure patterns i.e., reward
hacking, sycophancy, annotator drift, and misgeneralization. We introduce the
concept of the Alignment Gap, a unifying lens for understanding recurring
failures in feedback-based alignment. Using a KL-tilting formalism, we
illustrate why optimization pressure tends to amplify divergence between proxy
rewards and true human intent. We organize these failures into a catalogue of
Murphys Laws of AI Alignment, and propose the Alignment Trilemma as a way to
frame trade-offs among optimization strength, value capture, and
generalization. Small-scale empirical studies serve as illustrative support.
Finally, we propose the MAPS framework (Misspecification, Annotation, Pressure,
Shift) as practical design levers. Our contribution is not a definitive
impossibility theorem but a perspective that reframes alignment debates around
structural limits and trade-offs, offering clearer guidance for future design.

</details>


### [10] [From Image Generation to Infrastructure Design: a Multi-agent Pipeline for Street Design Generation](https://arxiv.org/abs/2509.05469)
*Chenguang Wang,Xiang Yan,Yilong Dai,Ziyi Wang,Susu Xu*

Main category: cs.AI

TL;DR: The paper introduces a multi-agent system for quickly and realistically editing bicycle facility designs on real-world street-view images, aimed at enhancing public engagement in transportation planning.


<details>
  <summary>Details</summary>
Motivation: Current methods for creating realistic street-design scenarios are labor-intensive, limiting their utility for public engagement and collaborative decision-making. Generative AI approaches face challenges in handling complex street views without large amounts of training data.

Method: The framework uses a multi-agent system that combines lane localization, prompt optimization, design generation, and automated evaluation to create realistic street-view designs that adapt to varying conditions.

Result: The system was tested across diverse urban conditions and demonstrated that it could consistently produce visually coherent and context-compliant design results.

Conclusion: The work lays a foundation for using multi-agent systems in infrastructure planning, making street design more accessible and collaborative.

Abstract: Realistic visual renderings of street-design scenarios are essential for
public engagement in active transportation planning. Traditional approaches are
labor-intensive, hindering collective deliberation and collaborative
decision-making. While AI-assisted generative design shows transformative
potential by enabling rapid creation of design scenarios, existing generative
approaches typically require large amounts of domain-specific training data and
struggle to enable precise spatial variations of design/configuration in
complex street-view scenes. We introduce a multi-agent system that edits and
redesigns bicycle facilities directly on real-world street-view imagery. The
framework integrates lane localization, prompt optimization, design generation,
and automated evaluation to synthesize realistic, contextually appropriate
designs. Experiments across diverse urban scenarios demonstrate that the system
can adapt to varying road geometries and environmental conditions, consistently
yielding visually coherent and instruction-compliant results. This work
establishes a foundation for applying multi-agent pipelines to transportation
infrastructure planning and facility design.

</details>


### [11] [TreeGPT: A Novel Hybrid Architecture for Abstract Syntax Tree Processing with Global Parent-Child Aggregation](https://arxiv.org/abs/2509.05550)
*Zixi Li*

Main category: cs.AI

TL;DR: TreeGPT is a hybrid neural network combining attention mechanisms and a hierarchical tree model for program synthesis tasks, achieving 96% accuracy on a difficult dataset.


<details>
  <summary>Details</summary>
Motivation: Improve neural program synthesis tasks by effectively processing Abstract Syntax Trees rather than relying on sequential or graph-based methods.

Method: TreeGPT employs a hybrid architecture with self-attention and a novel Tree Feed-Forward Network using a Global Parent-Child Aggregation mechanism for hierarchical tree modeling.

Result: TreeGPT outperforms existing methods on the ARC Prize 2025 dataset with 96% accuracy using only 1.5M parameters.

Conclusion: TreeGPT’s innovative use of tree aggregation and message-passing enables superior performance in program synthesis, highlighting its efficiency and effectiveness.

Abstract: We introduce TreeGPT, a novel neural architecture that combines
transformer-based attention mechanisms with global parent-child aggregation for
processing Abstract Syntax Trees (ASTs) in neural program synthesis tasks.
Unlike traditional approaches that rely solely on sequential processing or
graph neural networks, TreeGPT employs a hybrid design that leverages both
self-attention for capturing local dependencies and a specialized Tree
Feed-Forward Network (TreeFFN) for modeling hierarchical tree structures
through iterative message passing.
  The core innovation lies in our Global Parent-Child Aggregation mechanism,
formalized as: $$h_i^{(t+1)} = \sigma \Big( h_i^{(0)} + W_{pc} \sum_{(p,c) \in
E_i} f(h_p^{(t)}, h_c^{(t)}) + b \Big)$$ where $h_i^{(t)}$ represents the
hidden state of node $i$ at iteration $t$, $E_i$ denotes all parent-child edges
involving node $i$, and $f(h_p, h_c)$ is an edge aggregation function. This
formulation enables each node to progressively aggregate information from the
entire tree structure through $T$ iterations.
  Our architecture integrates optional enhancements including gated aggregation
with learnable edge weights, residual connections for gradient stability, and
bidirectional propagation for capturing both bottom-up and top-down
dependencies. We evaluate TreeGPT on the ARC Prize 2025 dataset, a challenging
visual reasoning benchmark requiring abstract pattern recognition and rule
inference. Experimental results demonstrate that TreeGPT achieves 96\%
accuracy, significantly outperforming transformer baselines (1.3\%),
large-scale models like Grok-4 (15.9\%), and specialized program synthesis
methods like SOAR (52\%) while using only 1.5M parameters. Our comprehensive
ablation study reveals that edge projection is the most critical component,
with the combination of edge projection and gating achieving optimal
performance.

</details>


### [12] [OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision](https://arxiv.org/abs/2509.05578)
*Ruixun Liu,Lingyu Kong,Derun Li,Hang Zhao*

Main category: cs.AI

TL;DR: The paper introduces OccVLA, a framework that enhances multimodal large language models (MLLMs) with 3D spatial reasoning by integrating 3D occupancy representations without extra inference costs. It achieves state-of-the-art results in autonomous driving benchmarks.


<details>
  <summary>Details</summary>
Motivation: MLLMs show vision-language reasoning capabilities but struggle with 3D spatial understanding, crucial in autonomous driving. Challenges include constructing accessible 3D representations and lack of large-scale 3D vision-language pretraining.

Method: OccVLA incorporates 3D occupancy representations into multimodal reasoning, treating dense 3D occupancy as predictive output and supervisory signal learned from 2D inputs. It eliminates reasoning overhead during inference.

Result: OccVLA achieves state-of-the-art performance in trajectory planning (nuScenes benchmark) and excels in 3D visual question-answering tasks, highlighting its scalability and interpretability.

Conclusion: OccVLA offers a scalable, interpretable, and vision-based solution for enhancing 3D spatial understanding in autonomous driving, addressing key limitations in multimodal large language models.

Abstract: Multimodal large language models (MLLMs) have shown strong vision-language
reasoning abilities but still lack robust 3D spatial understanding, which is
critical for autonomous driving. This limitation stems from two key challenges:
(1) the difficulty of constructing accessible yet effective 3D representations
without expensive manual annotations, and (2) the loss of fine-grained spatial
details in VLMs due to the absence of large-scale 3D vision-language
pretraining. To address these challenges, we propose OccVLA, a novel framework
that integrates 3D occupancy representations into a unified multimodal
reasoning process. Unlike prior approaches that rely on explicit 3D inputs,
OccVLA treats dense 3D occupancy as both a predictive output and a supervisory
signal, enabling the model to learn fine-grained spatial structures directly
from 2D visual inputs. The occupancy predictions are regarded as implicit
reasoning processes and can be skipped during inference without performance
degradation, thereby adding no extra computational overhead. OccVLA achieves
state-of-the-art results on the nuScenes benchmark for trajectory planning and
demonstrates superior performance on 3D visual question-answering tasks,
offering a scalable, interpretable, and fully vision-based solution for
autonomous driving.

</details>


### [13] [MSRFormer: Road Network Representation Learning using Multi-scale Feature Fusion of Heterogeneous Spatial Interactions](https://arxiv.org/abs/2509.05685)
*Jian Yang,Jiahui Wu,Li Fang,Hongchao Fan,Bianying Zhang,Huijie Zhao,Guangyi Yang,Rui Xin,Xiong You*

Main category: cs.AI

TL;DR: The paper introduces MSRFormer, a framework for road network representation learning using multi-scale spatial interactions and graph transformers, resulting in up to 16% performance improvement over competitive baselines.


<details>
  <summary>Details</summary>
Motivation: Urban road networks are heterogeneous and hierarchical, posing challenges for accurate representation learning using traditional graph neural networks.

Method: The MSRFormer framework integrates spatial flow convolution, scale-dependent regions, graph transformers, and residual connections to capture multi-scale spatial dependencies, followed by contrastive learning for representation.

Result: MSRFormer outperformed baseline methods on two real-world datasets, achieving up to 16% improvement in traffic-related tasks and analysis of complex road network structures.

Conclusion: This research offers a practical, task-agnostic framework for road network representation and highlights the interplay between scale effects and flow heterogeneity of spatial interactions.

Abstract: Transforming road network data into vector representations using deep
learning has proven effective for road network analysis. However, urban road
networks' heterogeneous and hierarchical nature poses challenges for accurate
representation learning. Graph neural networks, which aggregate features from
neighboring nodes, often struggle due to their homogeneity assumption and focus
on a single structural scale. To address these issues, this paper presents
MSRFormer, a novel road network representation learning framework that
integrates multi-scale spatial interactions by addressing their flow
heterogeneity and long-distance dependencies. It uses spatial flow convolution
to extract small-scale features from large trajectory datasets, and identifies
scale-dependent spatial interaction regions to capture the spatial structure of
road networks and flow heterogeneity. By employing a graph transformer,
MSRFormer effectively captures complex spatial dependencies across multiple
scales. The spatial interaction features are fused using residual connections,
which are fed to a contrastive learning algorithm to derive the final road
network representation. Validation on two real-world datasets demonstrates that
MSRFormer outperforms baseline methods in two road network analysis tasks. The
performance gains of MSRFormer suggest the traffic-related task benefits more
from incorporating trajectory data, also resulting in greater improvements in
complex road network structures with up to 16% improvements compared to the
most competitive baseline method. This research provides a practical framework
for developing task-agnostic road network representation models and highlights
distinct association patterns of the interplay between scale effects and flow
heterogeneity of spatial interactions.

</details>


### [14] [Towards Meta-Cognitive Knowledge Editing for Multimodal LLMs](https://arxiv.org/abs/2509.05714)
*Zhaoyu Fan,Kaihang Pan,Mingze Zhou,Bosheng Qin,Juncheng Li,Shengyu Zhang,Wenqiao Zhang,Siliang Tang,Fei Wu,Yueting Zhuang*

Main category: cs.AI

TL;DR: This paper introduces CogEdit, a benchmark for evaluating meta-cognitive knowledge editing in multimodal large language models (MLLMs), and proposes MIND, a framework to enhance such editing. The approach significantly outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing knowledge editing benchmarks for MLLMs focus on cognitive-level changes but lack evaluation of deeper meta-cognitive processes.

Method: The paper introduces CogEdit, a benchmark assessing meta-cognitive editing across three challenges: Counterfactual-Driven Editing, Boundary Constraint Editing, and Noise-Robust Editing. Additionally, they propose MIND, a framework with meta-knowledge memory, game-theoretic monitoring, and label refinement features.

Result: MIND demonstrates notable improvements over existing cognitive editing methods in both traditional and meta-cognitive benchmarks.

Conclusion: The study highlights the importance of meta-cognitive knowledge editing in MLLMs and establishes MIND as an effective solution for addressing existing challenges.

Abstract: Knowledge editing enables multimodal large language models (MLLMs) to
efficiently update outdated or incorrect information. However, existing
benchmarks primarily emphasize cognitive-level modifications while lacking a
focus on deeper meta-cognitive processes. To bridge this gap, we introduce
CogEdit, a novel benchmark designed to evaluate MLLMs' meta-cognitive knowledge
editing abilities across three levels: (1) Counterfactual-Driven Editing,
assessing self-awareness of knowledge correctness changes; (2) Boundary
Constraint Editing, ensuring appropriate generalization without unintended
interference; and (3) Noise-Robust Editing, promoting reflective evaluation of
uncertain information. To advance meta-cognitive editing, we propose MIND
(Meta-cognitive INtegrated Dynamic Knowledge Editing), a framework that
constructs a meta-knowledge memory for self-awareness, employs game-theoretic
interactions to monitor knowledge activation, and incorporates label refinement
for noise-robust updates. Extensive experiments show that MIND significantly
outperforms existing cognitive editing approaches, achieving strong performance
on both traditional and meta-cognitive knowledge editing benchmarks.

</details>


### [15] [Hyperbolic Large Language Models](https://arxiv.org/abs/2509.05757)
*Sarang Patil,Zeyong Zhang,Yiran Huang,Tengfei Ma,Mengjia Xu*

Main category: cs.AI

TL;DR: This paper explores how hyperbolic geometry, a non-Euclidean space, can enhance the representation learning in large language models (LLMs), focusing on complex hierarchical data.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to improve LLMs' capabilities to model non-Euclidean hierarchical relationships inherent in real-world data.

Method: Four categories of Hyperbolic LLMs (HypLLMs) are discussed, including models using exp/log maps, fine-tuned models, fully hyperbolic models, and hyperbolic state-space models.

Result: The paper offers a taxonomy of techniques, introduces potential applications, and provides a repository of relevant resources on Hyperbolic LLMs.

Conclusion: Leveraging hyperbolic geometry in LLMs offers promise for enhanced hierarchical data modeling and lays the groundwork for future research directions in this area.

Abstract: Large language models (LLMs) have achieved remarkable success and
demonstrated superior performance across various tasks, including natural
language processing (NLP), weather forecasting, biological protein folding,
text generation, and solving mathematical problems. However, many real-world
data exhibit highly non-Euclidean latent hierarchical anatomy, such as protein
networks, transportation networks, financial networks, brain networks, and
linguistic structures or syntactic trees in natural languages. Effectively
learning intrinsic semantic entailment and hierarchical relationships from
these raw, unstructured input data using LLMs remains an underexplored area.
Due to its effectiveness in modeling tree-like hierarchical structures,
hyperbolic geometry -- a non-Euclidean space -- has rapidly gained popularity
as an expressive latent representation space for complex data modeling across
domains such as graphs, images, languages, and multi-modal data. Here, we
provide a comprehensive and contextual exposition of recent advancements in
LLMs that leverage hyperbolic geometry as a representation space to enhance
semantic representation learning and multi-scale reasoning. Specifically, the
paper presents a taxonomy of the principal techniques of Hyperbolic LLMs
(HypLLMs) in terms of four main categories: (1) hyperbolic LLMs through exp/log
maps; (2) hyperbolic fine-tuned models; (3) fully hyperbolic LLMs, and (4)
hyperbolic state-space models. We also explore crucial potential applications
and outline future research directions. A repository of key papers, models,
datasets, and code implementations is available at
https://github.com/sarangp2402/Hyperbolic-LLM-Models/tree/main.

</details>


### [16] [DRF: LLM-AGENT Dynamic Reputation Filtering Framework](https://arxiv.org/abs/2509.05764)
*Yuwei Lou,Hao Hu,Shaocong Ma,Zongfei Zhang,Liang Wang,Jidong Ge,Xianping Tao*

Main category: cs.AI

TL;DR: The paper introduces DRF, a dynamic framework to improve credibility and efficiency in multi-agent systems using LLMs.


<details>
  <summary>Details</summary>
Motivation: Multi-agent systems with LLMs face challenges in assessing performance and agent credibility.

Method: Developed DRF, comprising an interactive rating network, reputation scoring, and UCB-based agent selection.

Result: Experimental results show improved task completion quality and efficiency in logical reasoning and code-generation.

Conclusion: DRF enhances collaboration and efficiency, providing a novel solution for multi-agent systems in complex large-scale tasks.

Abstract: With the evolution of generative AI, multi - agent systems leveraging large -
language models(LLMs) have emerged as a powerful tool for complex tasks.
However, these systems face challenges in quantifying agent performance and
lack mechanisms to assess agent credibility. To address these issues, we
introduce DRF, a dynamic reputation filtering framework. DRF constructs an
interactive rating network to quantify agent performance, designs a reputation
scoring mechanism to measure agent honesty and capability, and integrates an
Upper Confidence Bound - based strategy to enhance agent selection efficiency.
Experiments show that DRF significantly improves task completion quality and
collaboration efficiency in logical reasoning and code - generation tasks,
offering a new approach for multi - agent systems to handle large - scale
tasks.

</details>


### [17] [Decision-Focused Learning Enhanced by Automated Feature Engineering for Energy Storage Optimisation](https://arxiv.org/abs/2509.05772)
*Nasser Alkhulaifi,Ismail Gokay Dogan,Timothy R. Cargan,Alexander L. Bowler,Direnc Pekaslan,Nicholas J. Watson,Isaac Triguero*

Main category: cs.AI

TL;DR: A novel AFE-DFL framework integrates automated feature engineering with decision-focused learning to optimize battery storage operations and outperforms traditional methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the inefficiencies in energy storage decision-making caused by the separation of forecasting and optimization, and to explore the real-world applicability of decision-focused learning (DFL) methods to overcome these issues, particularly in scenarios with high variability and scarce data.

Method: The authors propose an AFE-DFL framework which combines automated feature engineering (AFE) with decision-focused learning (DFL). It is applied to forecast electricity prices and demand while optimizing battery energy storage operations. Performance was assessed using a real-world dataset from UK properties.

Result: The AFE-DFL framework significantly outperformed traditional Predict-Then-Optimise (PTO) methods, reducing operating costs by 22.9-56.5% through the integration of AFE. DFL also demonstrated consistently lower operating costs than PTO in the evaluation.

Conclusion: Integrating AFE into DFL enhances its practical viability for real-world applications, reduces dependency on domain knowledge, and provides cost-saving benefits, offering broader implications for energy management systems dealing with similar challenges.

Abstract: Decision-making under uncertainty in energy management is complicated by
unknown parameters hindering optimal strategies, particularly in Battery Energy
Storage System (BESS) operations. Predict-Then-Optimise (PTO) approaches treat
forecasting and optimisation as separate processes, allowing prediction errors
to cascade into suboptimal decisions as models minimise forecasting errors
rather than optimising downstream tasks. The emerging Decision-Focused Learning
(DFL) methods overcome this limitation by integrating prediction and
optimisation; however, they are relatively new and have been tested primarily
on synthetic datasets or small-scale problems, with limited evidence of their
practical viability. Real-world BESS applications present additional
challenges, including greater variability and data scarcity due to collection
constraints and operational limitations. Because of these challenges, this work
leverages Automated Feature Engineering (AFE) to extract richer representations
and improve the nascent approach of DFL. We propose an AFE-DFL framework
suitable for small datasets that forecasts electricity prices and demand while
optimising BESS operations to minimise costs. We validate its effectiveness on
a novel real-world UK property dataset. The evaluation compares DFL methods
against PTO, with and without AFE. The results show that, on average, DFL
yields lower operating costs than PTO and adding AFE further improves the
performance of DFL methods by 22.9-56.5% compared to the same models without
AFE. These findings provide empirical evidence for DFL's practical viability in
real-world settings, indicating that domain-specific AFE enhances DFL and
reduces reliance on domain expertise for BESS optimisation, yielding economic
benefits with broader implications for energy management systems facing similar
challenges.

</details>


### [18] [Chatbot To Help Patients Understand Their Health](https://arxiv.org/abs/2509.05818)
*Won Seok Jang,Hieu Tran,Manav Mistry,SaiKiran Gandluri,Yifan Zhang,Sharmin Sultana,Sunjae Kown,Yuan Zhang,Zonghai Yao,Hong Yu*

Main category: cs.AI

TL;DR: The paper presents NoteAid-Chatbot, a conversational AI for enhancing patient understanding of medical information, trained using lightweight language models and reinforcement learning (RL).


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable patients to actively participate in their healthcare by improving their understanding through conversational AI.

Method: A multi-agent lightweight LLaMA model was trained via two steps: supervised fine-tuning with synthetically generated data and reinforcement learning using Proximal Policy Optimization (PPO).

Result: The chatbot achieves clarity, relevance, and structured dialogue, outperforming non-expert humans in evaluations, despite no explicit supervision for these attributes.

Conclusion: The study demonstrates that lightweight, domain-specific chatbots can effectively utilize cost-efficient RL techniques like PPO for realistic, open-ended multi-turn conversational domains beyond healthcare.

Abstract: Patients must possess the knowledge necessary to actively participate in
their care. We present NoteAid-Chatbot, a conversational AI that promotes
patient understanding via a novel 'learning as conversation' framework, built
on a multi-agent large language model (LLM) and reinforcement learning (RL)
setup without human-labeled data. NoteAid-Chatbot was built on a lightweight
LLaMA 3.2 3B model trained in two stages: initial supervised fine-tuning on
conversational data synthetically generated using medical conversation
strategies, followed by RL with rewards derived from patient understanding
assessments in simulated hospital discharge scenarios. Our evaluation, which
includes comprehensive human-aligned assessments and case studies, demonstrates
that NoteAid-Chatbot exhibits key emergent behaviors critical for patient
education, such as clarity, relevance, and structured dialogue, even though it
received no explicit supervision for these attributes. Our results show that
even simple Proximal Policy Optimization (PPO)-based reward modeling can
successfully train lightweight, domain-specific chatbots to handle multi-turn
interactions, incorporate diverse educational strategies, and meet nuanced
communication objectives. Our Turing test demonstrates that NoteAid-Chatbot
surpasses non-expert human. Although our current focus is on healthcare, the
framework we present illustrates the feasibility and promise of applying
low-cost, PPO-based RL to realistic, open-ended conversational domains,
broadening the applicability of RL-based alignment methods.

</details>


### [19] [MapAgent: A Hierarchical Agent for Geospatial Reasoning with Dynamic Map Tool Integration](https://arxiv.org/abs/2509.05933)
*Md Hasebul Hasan,Mahir Labib Dihan,Mohammed Eunus Ali,Md Rizwan Parvez*

Main category: cs.AI

TL;DR: MapAgent is a hierarchical multi-agent framework designed for geospatial reasoning tasks, enhancing large language models by providing specialized tools for efficient map interaction and planning.


<details>
  <summary>Details</summary>
Motivation: To address the gap in existing agentic AI frameworks which excel in general tasks but struggle with geospatial reasoning, planning, and real-time map-related interactions.

Method: The authors propose a hierarchical multi-agent framework with a high-level planner and specialized modules, including a dedicated map-tool agent to handle geospatial APIs efficiently. It decouples planning from execution and uses custom toolsets for diverse geospatial benchmarks.

Result: MapAgent demonstrated significant performance improvements over existing baselines in four geospatial benchmarks: MapEval-Textual, MapEval-API, MapEval-Visual, and MapQA.

Conclusion: MapAgent effectively bridges the gap in geospatial reasoning, demonstrating superior efficiency and accuracy in handling complex queries while reducing cognitive load. The framework is open-sourced for broader applications.

Abstract: Agentic AI has significantly extended the capabilities of large language
models (LLMs) by enabling complex reasoning and tool use. However, most
existing frameworks are tailored to domains such as mathematics, coding, or web
automation, and fall short on geospatial tasks that require spatial reasoning,
multi-hop planning, and real-time map interaction. To address these challenges,
we introduce MapAgent, a hierarchical multi-agent plug-and-play framework with
customized toolsets and agentic scaffolds for map-integrated geospatial
reasoning. Unlike existing flat agent-based approaches that treat tools
uniformly-often overwhelming the LLM when handling similar but subtly different
geospatial APIs-MapAgent decouples planning from execution. A high-level
planner decomposes complex queries into subgoals, which are routed to
specialized modules. For tool-heavy modules-such as map-based services-we then
design a dedicated map-tool agent that efficiently orchestrates related APIs
adaptively in parallel to effectively fetch geospatial data relevant for the
query, while simpler modules (e.g., solution generation or answer extraction)
operate without additional agent overhead. This hierarchical design reduces
cognitive load, improves tool selection accuracy, and enables precise
coordination across similar APIs. We evaluate MapAgent on four diverse
geospatial benchmarks-MapEval-Textual, MapEval-API, MapEval-Visual, and
MapQA-and demonstrate substantial gains over state-of-the-art tool-augmented
and agentic baselines. We open-source our framwork at
https://github.com/Hasebul/MapAgent.

</details>


### [20] [Rethinking Reasoning Quality in Large Language Models through Enhanced Chain-of-Thought via RL](https://arxiv.org/abs/2509.06024)
*Haoyang He,Zihua Rong,Kun Ji,Chenyang Li,Qing Huang,Chong Xia,Lan Yang,Honggang Zhang*

Main category: cs.AI

TL;DR: The paper introduces Dynamic Reasoning Efficiency Reward (DRER), a novel reinforcement learning framework designed to enhance reasoning abilities in large language models (LLMs) by improving chain-of-thought (CoT) quality and stabilizing training lengths. It includes the Logictree dataset for comprehensive benchmarking.


<details>
  <summary>Details</summary>
Motivation: Existing rule-based reward systems for LLMs focus on output correctness but neglect the quality of the reasoning process, limiting control over logical depth and underutilizing the model's reasoning capacity.

Method: DRER implements two components: (i) Reasoning Quality Reward focuses on rewarding reasoning paths that increase correct-answer likelihood, incentivizing productive CoT; (ii) Dynamic Length Advantage penalizes responses deviating from a length threshold to stabilize training. Logictree dataset complements the framework as training data and benchmark.

Result: When tested on the Logictree benchmark, DRER improved CoT-based reasoning confidence by 30%, and a 7B model achieved GPT-o3-mini-level performance within just 400 training steps. It also generalized well across other logical reasoning datasets and AIME24.

Conclusion: DRER effectively shapes reasoning behavior in LLMs, enhancing their formal-reasoning skills, and provides a practical approach to improving logic-based performance in tasks. It’s paired with open-source code and data for accessibility.

Abstract: Reinforcement learning (RL) has recently become the dominant paradigm for
strengthening the reasoning abilities of large language models (LLMs). Yet the
rule-based reward functions commonly used on mathematical or programming
benchmarks assess only answer format and correctness, providing no signal as to
whether the induced Chain-of-Thought (CoT) actually improves the answer.
Furthermore, such task-specific training offers limited control over logical
depth and therefore may fail to reveal a model's genuine reasoning capacity. We
propose Dynamic Reasoning Efficiency Reward (DRER) -- a plug-and-play RL reward
framework that reshapes both reward and advantage signals. (i) A Reasoning
Quality Reward assigns fine-grained credit to those reasoning chains that
demonstrably raise the likelihood of the correct answer, directly incentivising
the trajectories with beneficial CoT tokens. (ii) A Dynamic Length Advantage
decays the advantage of responses whose length deviates from a
validation-derived threshold, stabilising training. To facilitate rigorous
assessment, we also release Logictree, a dynamically constructed deductive
reasoning dataset that functions both as RL training data and as a
comprehensive benchmark. Experiments confirm the effectiveness of DRER: our 7B
model attains GPT-o3-mini level performance on Logictree with 400 trianing
steps, while the average confidence of CoT-augmented answers rises by 30%. The
model further exhibits generalisation across diverse logical-reasoning
datasets, and the mathematical benchmark AIME24. These results illuminate how
RL shapes CoT behaviour and chart a practical path toward enhancing
formal-reasoning skills in large language models. All code and data are
available in repository https://github.com/Henryhe09/DRER.

</details>


### [21] [Reverse-Engineered Reasoning for Open-Ended Generation](https://arxiv.org/abs/2509.06160)
*Haozhe Wang,Haoran Que,Qixin Xu,Minghao Liu,Wangchunshu Zhou,Jiazhan Feng,Wanjun Zhong,Wei Ye,Tong Yang,Wenhao Huang,Ge Zhang,Fangzhen Lin*

Main category: cs.AI

TL;DR: The paper introduces a new paradigm called REverse-Engineered Reasoning (REER) to address limitations in current methods for deep reasoning in open-ended tasks, achieving competitive results with leading AI models.


<details>
  <summary>Details</summary>
Motivation: Current methods for deep reasoning, such as reinforcement learning and instruction distillation, have limitations when applied to open-ended and creative generation tasks.

Method: The REER paradigm works by reverse-engineering known-good solutions to computationally uncover the latent reasoning steps, avoiding the need for trial-and-error or expensive instruction distillation.

Result: The authors curated a dataset, DeepWriting-20K, containing 20,000 reasoning examples. They trained DeepWriter-8B on this data, achieving performance comparable to or better than models like GPT-4o and Claude 3.5.

Conclusion: REER offers a scalable, gradient-free approach to deep reasoning, demonstrating that reverse-engineering reasoning processes can outperform traditional forward-building methods in creative and open-ended tasks.

Abstract: While the ``deep reasoning'' paradigm has spurred significant advances in
verifiable domains like mathematics, its application to open-ended, creative
generation remains a critical challenge. The two dominant methods for
instilling reasoning -- reinforcement learning (RL) and instruction
distillation -- falter in this area; RL struggles with the absence of clear
reward signals and high-quality reward models, while distillation is
prohibitively expensive and capped by the teacher model's capabilities. To
overcome these limitations, we introduce REverse-Engineered Reasoning (REER), a
new paradigm that fundamentally shifts the approach. Instead of building a
reasoning process ``forwards'' through trial-and-error or imitation, REER works
``backwards'' from known-good solutions to computationally discover the latent,
step-by-step deep reasoning process that could have produced them. Using this
scalable, gradient-free approach, we curate and open-source DeepWriting-20K, a
large-scale dataset of 20,000 deep reasoning trajectories for open-ended tasks.
Our model, DeepWriter-8B, trained on this data, not only surpasses strong
open-source baselines but also achieves performance competitive with, and at
times superior to, leading proprietary models like GPT-4o and Claude 3.5.

</details>


### [22] [From Long to Short: LLMs Excel at Trimming Own Reasoning Chains](https://arxiv.org/abs/2509.06174)
*Wei Han,Geng Zhan,Sicheng Yu,Chenyu Wang,Bryan Hooi*

Main category: cs.AI

TL;DR: The paper addresses inefficiency in reasoning due to overthinking by LRMs and introduces EDIT, a method to guide these models toward concise, correct reasoning paths.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models (LRMs) hold promise in complex tasks but suffer from overthinking, leading to interpretability challenges and compromised reasoning efficiency.

Method: EDIT is introduced as a test-time scaling approach that employs constraint-guided generation, tracking both length and answer distributions to find the shortest correct reasoning paths.

Result: EDIT enhances reasoning efficiency in LRMs across diverse models and datasets, producing concise yet informative outputs.

Conclusion: EDIT successfully mitigates overthinking in LRMs, balancing correctness and brevity to improve interpretability and user experience.

Abstract: O1/R1 style large reasoning models (LRMs) signal a substantial leap forward
over conventional instruction-following LLMs. By applying test-time scaling to
generate extended reasoning paths, they establish many SOTAs across a wide
range of complex reasoning tasks. However, recent studies show that LRMs are
prone to suffer from overthinking -- the tendency to overcomplicate simple
problems, leading to excessive strategy switching and long, convoluted
reasoning traces that hinder their interpretability. To mitigate this issue, we
conduct a systematic investigation into the reasoning efficiency of a broad set
of LRMs and uncover a common dilemma: the difficulty in balancing multiple
generation objectives such as correctness and brevity. Based on this discovery,
we propose a test-time scaling method, EDIT (Efficient Dynamic Inference
Trimming), which efficiently guides LRMs to identify the shortest correct
reasoning paths at test time. EDIT employs constraint-guided generation while
jointly tracking length and answer distributions under varying constraints,
allowing it to select responses that strike an optimal balance between
conciseness and correctness. Extensive experiments across diverse models and
datasets show that EDIT substantially enhance the reasoning efficiency,
producing compact yet informative outputs that improve readability and user
experience.

</details>


### [23] [PillagerBench: Benchmarking LLM-Based Agents in Competitive Minecraft Team Environments](https://arxiv.org/abs/2509.06235)
*Olivier Schipper,Yudi Zhang,Yali Du,Mykola Pechenizkiy,Meng Fang*

Main category: cs.AI

TL;DR: PillagerBench evaluates competitive multi-agent systems in Minecraft, introducing TactiCrafter, an LLM-based system excelling in teamwork and adaptive strategies.


<details>
  <summary>Details</summary>
Motivation: Effective evaluation of competitive multi-agent systems, particularly team-vs-team scenarios, remains largely unexplored.

Method: Developed PillagerBench, a Minecraft-based framework for evaluation, and introduced TactiCrafter, an LLM-driven system focusing on teamwork, adaptation, and learning causal dependencies.

Result: TactiCrafter surpassed baseline solutions, demonstrating adaptive learning and strategic evolution via self-play.

Conclusion: PillagerBench and TactiCrafter enable advancements in competitive multi-agent systems, fostering innovation through open-source availability.

Abstract: LLM-based agents have shown promise in various cooperative and strategic
reasoning tasks, but their effectiveness in competitive multi-agent
environments remains underexplored. To address this gap, we introduce
PillagerBench, a novel framework for evaluating multi-agent systems in
real-time competitive team-vs-team scenarios in Minecraft. It provides an
extensible API, multi-round testing, and rule-based built-in opponents for
fair, reproducible comparisons. We also propose TactiCrafter, an LLM-based
multi-agent system that facilitates teamwork through human-readable tactics,
learns causal dependencies, and adapts to opponent strategies. Our evaluation
demonstrates that TactiCrafter outperforms baseline approaches and showcases
adaptive learning through self-play. Additionally, we analyze its learning
process and strategic evolution over multiple game episodes. To encourage
further research, we have open-sourced PillagerBench, fostering advancements in
multi-agent AI for competitive environments.

</details>


### [24] [Proof2Silicon: Prompt Repair for Verified Code and Hardware Generation via Reinforcement Learning](https://arxiv.org/abs/2509.06239)
*Manvi Jha,Jiaxin Wan,Deming Chen*

Main category: cs.AI

TL;DR: Proof2Silicon introduces an automated framework for generating formally verified hardware from natural language specifications, leveraging reinforcement learning for prompt optimization and Dafny code translation to RTL implementation.


<details>
  <summary>Details</summary>
Motivation: Large Language Models often produce code failing formal verification, a critical issue for hardware and safety-critical systems.

Method: Using PREFACE, an RL-based framework for improving prompt quality, Proof2Silicon translates verified Dafny code into synthesizable C and generates RTL designs via Vivado HLS.

Result: Proof2Silicon improved Dafny verification rates by 21% across LLMs and achieved up to 72% success in hardware synthesis for a 100-task benchmark.

Conclusion: Proof2Silicon demonstrates scalable and automated LLM-driven synthesis of formally verified hardware from natural language inputs.

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
automated code generation but frequently produce code that fails formal
verification, an essential requirement for hardware and safety-critical
domains. To overcome this fundamental limitation, we previously proposed
PREFACE, a model-agnostic framework based on reinforcement learning (RL) that
iteratively repairs the prompts provided to frozen LLMs, systematically
steering them toward generating formally verifiable Dafny code without costly
fine-tuning. This work presents Proof2Silicon, a novel end-to-end synthesis
framework that embeds the previously proposed PREFACE flow to enable the
generation of correctness-by-construction hardware directly from natural
language specifications. Proof2Silicon operates by: (1) leveraging PREFACE's
verifier-driven RL agent to optimize prompt generation iteratively, ensuring
Dafny code correctness; (2) automatically translating verified Dafny programs
into synthesizable high-level C using Dafny's Python backend and PyLog; and (3)
employing Vivado HLS to produce RTL implementations. Evaluated rigorously on a
challenging 100-task benchmark, PREFACE's RL-guided prompt optimization
consistently improved Dafny verification success rates across diverse LLMs by
up to 21%. Crucially, Proof2Silicon achieved an end-to-end hardware synthesis
success rate of up to 72%, generating RTL designs through Vivado HLS synthesis
flows. These results demonstrate a robust, scalable, and automated pipeline for
LLM-driven, formally verified hardware synthesis, bridging natural-language
specification and silicon realization.

</details>


### [25] [REMI: A Novel Causal Schema Memory Architecture for Personalized Lifestyle Recommendation Agents](https://arxiv.org/abs/2509.06269)
*Vishal Raman,Vijai Aravindh R,Abhijith Ragav*

Main category: cs.AI

TL;DR: Authors propose REMI, a Causal Schema Memory architecture for lifestyle agents capable of producing personalized and explainable recommendations based on users' life events, habits, and external knowledge.


<details>
  <summary>Details</summary>
Motivation: Personalized AI assistants often fail at incorporating complex personal data and causal knowledge, which leads to generic and unexplainable advice.

Method: REMI integrates a personal causal knowledge graph, a causal reasoning engine, and a schema planning module, enriched by a Large Language Model to perform causal reasoning, retrieve adaptable action plans, and deliver transparent causal explanations.

Result: REMI provides more context-aware, personalized, and user-aligned recommendations compared to baseline LLM agents. New evaluation metrics (Personalization Salience Score and Causal Reasoning Accuracy) validate its explainability and personalization.

Conclusion: This work advances memory-augmented causal reasoning in AI by developing transparent, trustworthy, and personalized lifestyle assistants leveraging novel evaluation techniques and architectures.

Abstract: Personalized AI assistants often struggle to incorporate complex personal
data and causal knowledge, leading to generic advice that lacks explanatory
power. We propose REMI, a Causal Schema Memory architecture for a multimodal
lifestyle agent that integrates a personal causal knowledge graph, a causal
reasoning engine, and a schema based planning module. The idea is to deliver
explainable, personalized recommendations in domains like fashion, personal
wellness, and lifestyle planning. Our architecture uses a personal causal graph
of the user's life events and habits, performs goal directed causal traversals
enriched with external knowledge and hypothetical reasoning, and retrieves
adaptable plan schemas to generate tailored action plans. A Large Language
Model orchestrates these components, producing answers with transparent causal
explanations. We outline the CSM system design and introduce new evaluation
metrics for personalization and explainability, including Personalization
Salience Score and Causal Reasoning Accuracy, to rigorously assess its
performance. Results indicate that CSM based agents can provide more context
aware, user aligned recommendations compared to baseline LLM agents. This work
demonstrates a novel approach to memory augmented, causal reasoning in
personalized agents, advancing the development of transparent and trustworthy
AI lifestyle assistants.

</details>


### [26] [TableMind: An Autonomous Programmatic Agent for Tool-Augmented Table Reasoning](https://arxiv.org/abs/2509.06278)
*Chuang Jiang,Mingyue Cheng,Xiaoyu Tao,Qingyang Mao,Jie Ouyang,Qi Liu*

Main category: cs.AI

TL;DR: TableMind, an advanced LLM-based table reasoning system, combines multi-turn tool usage and code execution in a secure environment with features like planning and self-reflection.


<details>
  <summary>Details</summary>
Motivation: Existing text-based methods for table reasoning struggle with numerical calculations and complex operations, and current tool-integrated systems lack adaptability.

Method: TableMind uses a two-stage fine-tuning approach: supervised fine-tuning for effective tool usage patterns followed by RAPO-based reinforcement fine-tuning to focus on high-quality outputs.

Result: TableMind outperformed competitive baselines in reasoning accuracy and computational precision across multiple benchmarks.

Conclusion: TableMind successfully demonstrates improved tool integration, adaptability, and reasoning capabilities, providing a step forward in table reasoning technology for structured data analysis.

Abstract: Table reasoning is crucial for leveraging structured data in domains such as
finance, healthcare, and scientific research. While large language models
(LLMs) show promise in multi-step reasoning, purely text-based methods often
struggle with the complex numerical computations and fine-grained operations
inherently required in this task. Tool-integrated reasoning improves
computational accuracy via explicit code execution, yet existing systems
frequently rely on rigid patterns, supervised imitation, and lack true
autonomous adaptability. In this paper, we present TableMind, an LLM-driven
table reasoning agent that (i) autonomously performs multi-turn tool
invocation, (ii) writes and executes data-analyzing code in a secure sandbox
environment for data analysis and precise numerical reasoning, and (iii)
exhibits high-level capabilities such as planning and self-reflection to adapt
strategies. To realize these capabilities, we adopt a two-stage fine-tuning
paradigm built on top of a powerful pre-trained language model: supervised
fine-tuning on high-quality reasoning trajectories to establish effective tool
usage patterns, followed by reinforcement fine-tuning to optimize
multi-objective strategies. In particular, we propose Rank-Aware Policy
Optimization (RAPO), which increases the update weight of high-quality
trajectories when their output probabilities are lower than those of
low-quality ones, thereby guiding the model more consistently toward better and
more accurate answers. Extensive experiments on several mainstream benchmarks
demonstrate that TableMind achieves superior performance compared to
competitive baselines, yielding substantial gains in both reasoning accuracy
and computational precision.

</details>


### [27] [SFR-DeepResearch: Towards Effective Reinforcement Learning for Autonomously Reasoning Single Agents](https://arxiv.org/abs/2509.06283)
*Xuan-Phi Nguyen,Shrey Pandit,Revanth Gangi Reddy,Austin Xu,Silvio Savarese,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: This paper explores enhancing large language models (LLMs) for autonomous deep research tasks using continual reinforcement learning (RL) with synthetic data.


<details>
  <summary>Details</summary>
Motivation: To empower large language models with advanced reasoning and tool-use capabilities, enabling applications like Deep Research, which demand dynamic search and reasoning.

Method: The study implements continual reinforcement learning (RL) using synthetic data to optimize reasoning abilities in open-source LLMs for dynamic task execution.

Result: The model variant SFR-DR-20B achieved a performance score of up to 28.7% on the Humanity's Last Exam benchmark.

Conclusion: Using RL with synthetic data enhances reasoning and agentic skills in LLMs, demonstrating potential in autonomous single-agent systems for complex research tasks.

Abstract: Equipping large language models (LLMs) with complex, interleaved reasoning
and tool-use capabilities has become a key focus in agentic AI research,
especially with recent advances in reasoning-oriented (``thinking'') models.
Such capabilities are key to unlocking a number of important applications. One
such application is Deep Research (DR), which requires extensive search and
reasoning over many sources. Our work in this paper focuses on the development
of native Autonomous Single-Agent models for DR featuring minimal web crawling
and Python tool integration. Unlike multi-agent systems, where agents take up
pre-defined roles and are told what to do at each step in a static workflow, an
autonomous single-agent determines its next action dynamically based on
context, without manual directive. While prior work has proposed training
recipes for base or instruction-tuned LLMs, we focus on continual reinforcement
learning (RL) of reasoning-optimized models to further enhance agentic skills
while preserving reasoning ability. Towards this end, we propose a simple RL
recipe with entirely synthetic data, which we apply to various open-source
LLMs. Our best variant SFR-DR-20B achieves up to 28.7% on Humanity's Last Exam
benchmark. In addition, we conduct key analysis experiments to provide more
insights into our methodologies.

</details>


### [28] [From Implicit Exploration to Structured Reasoning: Leveraging Guideline and Refinement for LLMs](https://arxiv.org/abs/2509.06284)
*Jiaxiang Chen,Zhuo Wang,Mingxi Zou,Zhucong Li,Zhijian Zhou,Song Wang,Zenglin Xu*

Main category: cs.AI

TL;DR: The paper proposes a framework for structured reasoning in large language models (LLMs) using guidelines and refinement, improving stability and performance across reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To address the instability, lack of error correction, and limited learning from past experience in current implicit exploration methods used by LLMs.

Method: The framework extracts structured reasoning patterns from successful paths and reflective signals from failures. During inference, it applies step-by-step guidelines with refinement at each step to correct errors.

Result: The method outperforms strong baselines on benchmarks like BBH, GSM8K, MATH-500, MBPP, and HumanEval, demonstrating improved stability, generalization, and scalability.

Conclusion: Structured reasoning with stepwise execution and refinement enhances stability, supports cross-model collaboration, and achieves effectiveness comparable to supervised fine-tuning.

Abstract: Large language models (LLMs) have advanced general-purpose reasoning, showing
strong performance across diverse tasks. However, existing methods often rely
on implicit exploration, where the model follows stochastic and unguided
reasoning paths-like walking without a map. This leads to unstable reasoning
paths, lack of error correction, and limited learning from past experience. To
address these issues, we propose a framework that shifts from implicit
exploration to structured reasoning through guideline and refinement. First, we
extract structured reasoning patterns from successful trajectories and
reflective signals from failures. During inference, the model follows these
guidelines step-by-step, with refinement applied after each step to correct
errors and stabilize the reasoning process. Experiments on BBH and four
additional benchmarks (GSM8K, MATH-500, MBPP, HumanEval) show that our method
consistently outperforms strong baselines across diverse reasoning tasks.
Structured reasoning with stepwise execution and refinement improves stability
and generalization, while guidelines transfer well across domains and flexibly
support cross-model collaboration, matching or surpassing supervised
fine-tuning in effectiveness and scalability.

</details>


### [29] [Can AI Make Energy Retrofit Decisions? An Evaluation of Large Language Models](https://arxiv.org/abs/2509.06307)
*Lei Shu,Dong Zhao*

Main category: cs.AI

TL;DR: Paper evaluates the use of generative AI like LLMs for residential energy retrofit decision-making, finding potential but identifying challenges in accuracy, consistency, and context handling.


<details>
  <summary>Details</summary>
Motivation: To improve the process of building energy retrofit decision-making, which traditionally suffers from poor generalizability and interpretability, hindering its application in diverse residential settings.

Method: Seven large language models were tested on making decisions about residential retrofits. The evaluation focused on two goals – maximizing CO2 reduction and minimizing payback period – and assessed metrics like accuracy, consistency, sensitivity, and reasoning using data from 400 homes across the U.S.

Result: LLMs demonstrated potential by producing recommendations with up to 54.5% top-1 match accuracy and 92.8% within top-5 accuracy. Performance varied between objectives, with stronger results for maximizing CO2 reduction.

Conclusion: Large language models show promise as tools for energy retrofit decision-making, but they require improvements in areas like contextual understanding, economic trade-offs handling, and consistency to be widely reliable.

Abstract: Conventional approaches to building energy retrofit decision making suffer
from limited generalizability and low interpretability, hindering adoption in
diverse residential contexts. With the growth of Smart and Connected
Communities, generative AI, especially large language models (LLMs), may help
by processing contextual information and producing practitioner readable
recommendations. We evaluate seven LLMs (ChatGPT, DeepSeek, Gemini, Grok,
Llama, and Claude) on residential retrofit decisions under two objectives:
maximizing CO2 reduction (technical) and minimizing payback period
(sociotechnical). Performance is assessed on four dimensions: accuracy,
consistency, sensitivity, and reasoning, using a dataset of 400 homes across 49
US states. LLMs generate effective recommendations in many cases, reaching up
to 54.5 percent top 1 match and 92.8 percent within top 5 without fine tuning.
Performance is stronger for the technical objective, while sociotechnical
decisions are limited by economic trade offs and local context. Agreement
across models is low, and higher performing models tend to diverge from others.
LLMs are sensitive to location and building geometry but less sensitive to
technology and occupant behavior. Most models show step by step, engineering
style reasoning, but it is often simplified and lacks deeper contextual
awareness. Overall, LLMs are promising assistants for energy retrofit decision
making, but improvements in accuracy, consistency, and context handling are
needed for reliable practice.

</details>


### [30] [Large Language Models as Virtual Survey Respondents: Evaluating Sociodemographic Response Generation](https://arxiv.org/abs/2509.06337)
*Jianpeng Zhao,Chenyu Yuan,Weiming Luo,Haoling Xie,Guangwei Zhang,Steven Jige Quan,Zixuan Yuan,Pengyang Wang,Denghui Zhang*

Main category: cs.AI

TL;DR: The paper investigates using Large Language Models (LLMs) to simulate virtual survey respondents for scalable and cost-effective sociological research.


<details>
  <summary>Details</summary>
Motivation: Traditional survey methods are expensive, time-consuming, and limited in scale, prompting the need for innovative approaches.

Method: The authors introduce Partial Attribute Simulation (PAS) and Full Attribute Simulation (FAS) for generating survey responses using LLMs, and develop a benchmark suite spanning 11 datasets.

Result: Evaluations on multiple LLMs show consistent trends, identified failure modes, and demonstrated that context and prompt design influence simulation accuracy.

Conclusion: LLM-driven survey simulations represent a promising, scalable, and budget-friendly alternative to traditional surveys for sociological studies and policymaking.

Abstract: Questionnaire-based surveys are foundational to social science research and
public policymaking, yet traditional survey methods remain costly,
time-consuming, and often limited in scale. This paper explores a new paradigm:
simulating virtual survey respondents using Large Language Models (LLMs). We
introduce two novel simulation settings, namely Partial Attribute Simulation
(PAS) and Full Attribute Simulation (FAS), to systematically evaluate the
ability of LLMs to generate accurate and demographically coherent responses. In
PAS, the model predicts missing attributes based on partial respondent
profiles, whereas FAS involves generating complete synthetic datasets under
both zero-context and context-enhanced conditions. We curate a comprehensive
benchmark suite, LLM-S^3 (Large Language Model-based Sociodemographic Survey
Simulation), that spans 11 real-world public datasets across four sociological
domains. Our evaluation of multiple mainstream LLMs (GPT-3.5/4 Turbo, LLaMA
3.0/3.1-8B) reveals consistent trends in prediction performance, highlights
failure modes, and demonstrates how context and prompt design impact simulation
fidelity. This work establishes a rigorous foundation for LLM-driven survey
simulations, offering scalable and cost-effective tools for sociological
research and policy evaluation. Our code and dataset are available at:
https://github.com/dart-lab-research/LLM-S-Cube-Benchmark

</details>


### [31] [Evaluating Multi-Turn Bargain Skills in LLM-Based Seller Agent](https://arxiv.org/abs/2509.06341)
*Issue Yishu Wang,Kakam Chong,Xiaofeng Wang,Xu Yan,DeXin Kong,Chen Ju,Ming Chen,Shuai Xiao,Shuguang Han,jufeng chen*

Main category: cs.AI

TL;DR: The paper presents a framework to evaluate seller agents that engage in multi-turn bargaining in online second-hand marketplaces, focusing on tracking buyer intents.


<details>
  <summary>Details</summary>
Motivation: There is a need for seller agents (powered by LLMs) to more effectively interpret buyer intentions during negotiations in e-commerce settings, which existing methods do not adequately address.

Method: The authors introduce a large-scale bargaining benchmark with annotated intent datasets and a turn-level evaluation framework based on Theory of Mind. They also provide an automated pipeline for extracting buyer intent from dialogues.

Result: The proposed framework enables detailed tracking of buyer intents, moving beyond conventional outcome-only metrics, and supports comprehensive benchmarking.

Conclusion: The research advances the study of LLM-powered seller agents by establishing a structured framework for evaluating their negotiation skills, particularly in capturing buyer intentions effectively.

Abstract: In online second-hand marketplaces, multi-turn bargaining is a crucial part
of seller-buyer interactions. Large Language Models (LLMs) can act as seller
agents, negotiating with buyers on behalf of sellers under given business
constraints. A critical ability for such agents is to track and accurately
interpret cumulative buyer intents across long negotiations, which directly
impacts bargaining effectiveness. We introduce a multi-turn evaluation
framework for measuring the bargaining ability of seller agents in e-commerce
dialogues. The framework tests whether an agent can extract and track buyer
intents. Our contributions are: (1) a large-scale e-commerce bargaining
benchmark spanning 622 categories, 9,892 products, and 3,014 tasks; (2) a
turn-level evaluation framework grounded in Theory of Mind (ToM) with annotated
buyer intents, moving beyond outcome-only metrics; and (3) an automated
pipeline that extracts reliable intent from massive dialogue data.

</details>


### [32] [A data-driven discretized CS:GO simulation environment to facilitate strategic multi-agent planning research](https://arxiv.org/abs/2509.06355)
*Yunzhe Wang,Volkan Ustun,Chris McGroarty*

Main category: cs.AI

TL;DR: DECOY is a multi-agent simulation framework for modeling long-term strategies in 3D terrains, demonstrated using CS:GO gameplay data.


<details>
  <summary>Details</summary>
Motivation: To create a simulation tool that balances long-horizon strategic modeling and environmental detail for multi-agent interactions.

Method: The method involves waypoint-based state-action discretization and neural models trained on CS:GO data to predict and generate outcomes.

Result: Replays generated by DECOY closely match human gameplay data from CS:GO.

Conclusion: DECOY provides a versatile, open-source environment for studying strategic multi-agent behavior with high validation accuracy.

Abstract: Modern simulation environments for complex multi-agent interactions must
balance high-fidelity detail with computational efficiency. We present DECOY, a
novel multi-agent simulator that abstracts strategic, long-horizon planning in
3D terrains into high-level discretized simulation while preserving low-level
environmental fidelity. Using Counter-Strike: Global Offensive (CS:GO) as a
testbed, our framework accurately simulates gameplay using only movement
decisions as tactical positioning -- without explicitly modeling low-level
mechanics such as aiming and shooting. Central to our approach is a waypoint
system that simplifies and discretizes continuous states and actions, paired
with neural predictive and generative models trained on real CS:GO tournament
data to reconstruct event outcomes. Extensive evaluations show that replays
generated from human data in DECOY closely match those observed in the original
game. Our publicly available simulation environment provides a valuable tool
for advancing research in strategic multi-agent planning and behavior
generation.

</details>


### [33] [Teaching AI Stepwise Diagnostic Reasoning with Report-Guided Chain-of-Thought Learning](https://arxiv.org/abs/2509.06409)
*Yihong Luo,Wenwu He,Zhuo-Xu Cui,Dong Liang*

Main category: cs.AI

TL;DR: DiagCoT is a framework that fine-tunes vision-language models using free-text reports to mimic radiologists' stepwise diagnostic reasoning.


<details>
  <summary>Details</summary>
Motivation: To develop interpretable and diagnostically competent AI systems for radiology using unstructured clinical narratives.

Method: The framework includes contrastive image-report tuning, chain-of-thought supervision, and reinforcement tuning with clinical reward signals.

Result: On the MIMIC-CXR benchmark, DiagCoT achieved substantial improvements in disease classification, pathology grounding, and report generation metrics compared to state-of-the-art models.

Conclusion: DiagCoT provides a scalable and effective approach to training AI systems for radiology, outperforming existing models in several key metrics and long-tailed disease diagnosis.

Abstract: This study presents DiagCoT, a multi-stage framework that applies supervised
fine-tuning to general-purpose vision-language models (VLMs) to emulate
radiologists' stepwise diagnostic reasoning using only free-text reports.
DiagCoT combines contrastive image-report tuning for domain alignment,
chain-of-thought supervision to capture inferential logic, and reinforcement
tuning with clinical reward signals to enhance factual accuracy and fluency. On
the MIMIC-CXR benchmark, DiagCoT improved zero-shot disease classification AUC
from 0.52 to 0.76 (absolute gain of 0.24), pathology grounding mIoU from 0.08
to 0.31 (absolute gain of 0.23), and report generation BLEU from 0.11 to 0.33
(absolute gain of 0.22). It outperformed state-of-the-art models including
LLaVA-Med and CXR-LLAVA on long-tailed diseases and external datasets. By
converting unstructured clinical narratives into structured supervision,
DiagCoT offers a scalable approach for developing interpretable and
diagnostically competent AI systems for radiology.

</details>


### [34] [Tree of Agents: Improving Long-Context Capabilities of Large Language Models through Multi-Perspective Reasoning](https://arxiv.org/abs/2509.06436)
*Song Yu,Xiaofei Xu,Ke Deng,Li Li,Lin Tian*

Main category: cs.AI

TL;DR: The paper introduces Tree of Agents (TOA), a multi-agent reasoning framework aimed at addressing the challenges faced by large language models (LLMs) in long-context tasks like position bias and lost information.


<details>
  <summary>Details</summary>
Motivation: The study aims to solve the issues LLMs encounter in long-context tasks, particularly the 'lost in the middle' problem where critical information from mid-sections of long input is underutilized.

Method: They propose the Tree of Agents (TOA), which segments input data into chunks processed by independent agents. The agents perform collaborative reasoning in a tree structure and apply strategies like prefix-hash caching and adaptive pruning to boost efficiency.

Result: Experimental results show that TOA, implemented with LLaMA3.1-8B, surpasses multiple baselines and matches the performance of larger commercial models such as Gemini1.5-pro on long-context tasks.

Conclusion: TOA effectively mitigates position bias and hallucination issues in long-context tasks while being efficient in terms of API overhead, proving to be a promising alternative to larger models.

Abstract: Large language models (LLMs) face persistent challenges when handling
long-context tasks, most notably the lost in the middle issue, where
information located in the middle of a long input tends to be underutilized.
Some existing methods that reduce input have the risk of discarding key
information, while others that extend context windows often lead to attention
dispersion. To address these limitations, we propose Tree of Agents (TOA), a
multi-agent reasoning framework that segments the input into chunks processed
by independent agents. Each agent generates its local cognition, then agents
dynamically exchange information for collaborative reasoning along
tree-structured paths. TOA enables agents to probe different reasoning orders
for multi-perspective understanding, effectively mitigating position bias and
reducing hallucinations. To improve processing efficiency, we incorporate
prefix-hash caching and adaptive pruning strategies, achieving significant
performance improvements with comparable API overhead. Experiments show that
TOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple
baselines and demonstrates comparable performance to the latest and much larger
commercial models, such as Gemini1.5-pro, on various long-context tasks. Code
is available at https://github.com/Aireduce952/Tree-of-Agents.

</details>


### [35] [HyFedRAG: A Federated Retrieval-Augmented Generation Framework for Heterogeneous and Privacy-Sensitive Data](https://arxiv.org/abs/2509.06444)
*Cheng Qian,Hainan Zhang,Yongxin Tong,Hong-Wei Zheng,Zhiming Zheng*

Main category: cs.AI

TL;DR: The paper introduces HyFedRAG, a Federated Retrieval-Augmented Generation (RAG) framework tailored for privacy-sensitive and heterogeneous healthcare data.


<details>
  <summary>Details</summary>
Motivation: Centralized RAG pipelines face limitations in handling distributed, heterogeneous, and privacy-sensitive data in healthcare, especially when operating across various formats such as SQL databases, knowledge graphs, and clinical notes.

Method: The proposed HyFedRAG framework utilizes an edge-cloud collaborative approach, with edge-side LLMs standardizing and anonymizing data and server-side LLMs integrating the outputs for comprehensive reasoning. The framework also incorporates a three-tier caching strategy to enhance efficiency.

Result: Experimental results on a healthcare dataset demonstrate that HyFedRAG significantly improves upon existing baselines in retrieval quality, generation consistency, and system efficiency.

Conclusion: HyFedRAG presents a scalable, privacy-compliant solution for RAG tasks in environments characterized by heterogeneous and sensitive data, bridging gaps in current systems and unlocking LLMs' potential in healthcare and similar domains.

Abstract: Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive
data, especially in distributed healthcare settings where patient data spans
SQL, knowledge graphs, and clinical notes. Clinicians face difficulties
retrieving rare disease cases due to privacy constraints and the limitations of
traditional cloud-based RAG systems in handling diverse formats and edge
devices. To address this, we introduce HyFedRAG, a unified and efficient
Federated RAG framework tailored for Hybrid data modalities. By leveraging an
edge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across
diverse data sources while preserving data privacy. Our key contributions are:
(1) We design an edge-cloud collaborative RAG framework built on Flower, which
supports querying structured SQL data, semi-structured knowledge graphs, and
unstructured documents. The edge-side LLMs convert diverse data into
standardized privacy-preserving representations, and the server-side LLMs
integrates them for global reasoning and generation. (2) We integrate
lightweight local retrievers with privacy-aware LLMs and provide three
anonymization tools that enable each client to produce semantically rich,
de-identified summaries for global inference across devices. (3) To optimize
response latency and reduce redundant computation, we design a three-tier
caching strategy consisting of local cache, intermediate representation cache,
and cloud inference cache. Experimental results on PMC-Patients demonstrate
that HyFedRAG outperforms existing baselines in terms of retrieval quality,
generation consistency, and system efficiency. Our framework offers a scalable
and privacy-compliant solution for RAG over structural-heterogeneous data,
unlocking the potential of LLMs in sensitive and diverse data environments.

</details>


### [36] [Accelerate Scaling of LLM Alignment via Quantifying the Coverage and Depth of Instruction Set](https://arxiv.org/abs/2509.06463)
*Chengwei Wu,Li Du,Hanyu Zhao,Yiming Ju,Jiapu Wang,Tengfei Pan*

Main category: cs.AI

TL;DR: The authors study how to select informative instructions for better alignment of large language models and propose a method that improves alignment by focusing on instruction depth and semantic space coverage.


<details>
  <summary>Details</summary>
Motivation: As the demand for large language models grows, there is a need to optimize their alignment with instructions to enhance performance and efficiency. Current methods falter as instruction pools grow in size.

Method: The authors analyze dataset distribution factors influencing model performance, identifying instruction depth and semantic space coverage as key. They develop an algorithm to optimize these factors in instruction selection.

Result: Their instruction selection method explains over 70% of model performance variance and demonstrates faster and sustainable improvement compared to existing methods.

Conclusion: The proposed method achieves better model alignment by optimizing instruction selection to accelerate performance scaling.

Abstract: With the growing demand for applying large language models to downstream
tasks, improving model alignment performance and efficiency has become crucial.
Such a process involves selecting informative instructions from a candidate
pool. However, due to the complexity of instruction set distributions, the key
factors driving the performance of aligned models remain unclear. As a result,
current instruction set refinement methods fail to improve performance as the
instruction pool expands continuously. To address this issue, we first
investigate the key factors that influence the relationship between instruction
dataset distribution and aligned model performance. Based on these insights, we
propose a novel instruction data selection method. We identify that the depth
of instructions and the coverage of the semantic space are the crucial factors
determining downstream performance, which could explain over 70\% of the model
loss on the development set. We then design an instruction selection algorithm
to simultaneously maximize the depth and semantic coverage of the selected
instructions. Experimental results demonstrate that, compared to
state-of-the-art baseline methods, it can sustainably improve model performance
at a faster pace and thus achieve \emph{``Accelerated Scaling''}.

</details>


### [37] [MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents](https://arxiv.org/abs/2509.06477)
*Pengxiang Zhao,Guangyi Liu,Yaozhen Liang,Weiqing He,Zhengxi Lu,Yuehao Huang,Yaxuan Guo,Kexin Zhang,Hao Wang,Liang Liu,Yong Liu*

Main category: cs.AI

TL;DR: MAS-Bench introduces a benchmark to evaluate GUI-shortcut hybrid agents, focusing on mobile applications. It combines GUI operations with shortcuts, enabling agents to generate efficient workflows autonomously and achieve higher success rates compared to GUI-only methods.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic benchmarking for GUI-shortcut hybrid agents, especially in the mobile domain.

Method: MAS-Bench evaluates agents through tasks across real-world applications, using a combination of predefined shortcuts and metrics to assess the creation of reusable workflows.

Result: Hybrid agents demonstrated significantly better success rates and efficiency compared to GUI-only agents during experiments.

Conclusion: MAS-Bench establishes a foundational framework for assessing and advancing GUI-shortcut hybrid agents, marking a pivotal step in developing intelligent systems.

Abstract: To enhance the efficiency of GUI agents on various platforms like smartphones
and computers, a hybrid paradigm that combines flexible GUI operations with
efficient shortcuts (e.g., API, deep links) is emerging as a promising
direction. However, a framework for systematically benchmarking these hybrid
agents is still underexplored. To take the first step in bridging this gap, we
introduce MAS-Bench, a benchmark that pioneers the evaluation of GUI-shortcut
hybrid agents with a specific focus on the mobile domain. Beyond merely using
predefined shortcuts, MAS-Bench assesses an agent's capability to autonomously
generate shortcuts by discovering and creating reusable, low-cost workflows. It
features 139 complex tasks across 11 real-world applications, a knowledge base
of 88 predefined shortcuts (APIs, deep-links, RPA scripts), and 7 evaluation
metrics. The tasks are designed to be solvable via GUI-only operations, but can
be significantly accelerated by intelligently embedding shortcuts. Experiments
show that hybrid agents achieve significantly higher success rates and
efficiency than their GUI-only counterparts. This result also demonstrates the
effectiveness of our method for evaluating an agent's shortcut generation
capabilities. MAS-Bench fills a critical evaluation gap, providing a
foundational platform for future advancements in creating more efficient and
robust intelligent agents.

</details>


### [38] [MORSE: Multi-Objective Reinforcement Learning via Strategy Evolution for Supply Chain Optimization](https://arxiv.org/abs/2509.06490)
*Niki Kotecha,Ehecatl Antonio del Rio Chanona*

Main category: cs.AI

TL;DR: This paper combines Reinforcement Learning (RL) and Multi-Objective Evolutionary Algorithms (MOEAs) to handle dynamic supply chain optimization, incorporating a risk-sensitive approach using Conditional Value-at-Risk (CVaR).


<details>
  <summary>Details</summary>
Motivation: To tackle challenges in balancing conflicting objectives like cost, service, and sustainability in supply chain management, particularly under dynamic and uncertain conditions, where traditional methods fail to adapt in real-time.

Method: The paper integrates MOEAs with RL, using MOEAs to optimize policy neural networks and generate a Pareto front of policies. Dynamic policy-switching and CVaR are used for risk-sensitive decision-making.

Result: The approach shows superior performance compared to state-of-the-art techniques in responding to supply chain dynamics, demonstrated through an inventory management case study.

Conclusion: The proposed method improves the efficiency and robustness of real-time decision-making in supply chains, enhancing both adaptability to uncertainties and overall performance.

Abstract: In supply chain management, decision-making often involves balancing multiple
conflicting objectives, such as cost reduction, service level improvement, and
environmental sustainability. Traditional multi-objective optimization methods,
such as linear programming and evolutionary algorithms, struggle to adapt in
real-time to the dynamic nature of supply chains. In this paper, we propose an
approach that combines Reinforcement Learning (RL) and Multi-Objective
Evolutionary Algorithms (MOEAs) to address these challenges for dynamic
multi-objective optimization under uncertainty. Our method leverages MOEAs to
search the parameter space of policy neural networks, generating a Pareto front
of policies. This provides decision-makers with a diverse population of
policies that can be dynamically switched based on the current system
objectives, ensuring flexibility and adaptability in real-time decision-making.
We also introduce Conditional Value-at-Risk (CVaR) to incorporate
risk-sensitive decision-making, enhancing resilience in uncertain environments.
We demonstrate the effectiveness of our approach through case studies,
showcasing its ability to respond to supply chain dynamics and outperforming
state-of-the-art methods in an inventory management case study. The proposed
strategy not only improves decision-making efficiency but also offers a more
robust framework for managing uncertainty and optimizing performance in supply
chains.

</details>


### [39] [Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers](https://arxiv.org/abs/2509.06493)
*Ran Xin,Zeyu Zheng,Yanchen Nie,Kun Yuan,Xia Xiao*

Main category: cs.AI

TL;DR: The paper proposes BFS-Prover-V2, an enhanced theorem proving system using RL frameworks and planning techniques to improve training and inference efficiency, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To overcome scalability challenges in integrating LLMs into automated theorem proving, specifically related to training-time reinforcement learning and inference-time compute.

Method: The paper introduces a novel multi-turn off-policy RL framework inspired by AlphaZero for training and a planner-enhanced multi-agent search architecture for inference to handle complex reasoning tasks.

Result: BFS-Prover-V2 achieves state-of-the-art results with 95.08% on MiniF2F and 41.4% on ProofNet benchmarks, showcasing improved efficiency in theorem proving.

Conclusion: The innovations presented effectively address scalability issues in LLM-based theorem proving, with broader applicability in domains requiring long-horizon reasoning and complex searches.

Abstract: The integration of Large Language Models (LLMs) into automated theorem
proving has shown immense promise, yet is fundamentally constrained by
challenges in scaling up both training-time reinforcement learning (RL) and
inference-time compute. This paper introduces \texttt{BFS-Prover-V2}, a system
designed to address this dual scaling problem. We present two primary
innovations. The first is a novel multi-turn off-policy RL framework for
continually improving the performance of LLM step-prover at training time. This
framework, inspired by the principles of AlphaZero, utilizes a multi-stage
expert iteration pipeline featuring adaptive tactic-level data filtering and
periodic retraining to surmount the performance plateaus that typically curtail
long-term RL in LLM-based agents. The second innovation is a planner-enhanced
multi-agent search architecture that scales reasoning capabilities at inference
time. This architecture employs a general reasoning model as a high-level
planner to iteratively decompose complex theorems into a sequence of simpler
subgoals. This hierarchical approach substantially reduces the search space,
enabling a team of parallel prover agents to collaborate efficiently by
leveraging a shared proof cache. We demonstrate that this dual approach to
scaling yields state-of-the-art results on established formal mathematics
benchmarks. \texttt{BFS-Prover-V2} achieves 95.08\% and 41.4\% on the MiniF2F
and ProofNet test sets respectively. While demonstrated in the domain of formal
mathematics, the RL and inference techniques presented in this work are of
broader interest and may be applied to other domains requiring long-horizon
multi-turn reasoning and complex search.

</details>


### [40] [An AI system to help scientists write expert-level empirical software](https://arxiv.org/abs/2509.06503)
*Eser Aygün,Anastasiya Belyaeva,Gheorghe Comanici,Marc Coram,Hao Cui,Jake Garrison,Renee Johnston Anton Kast,Cory Y. McLean,Peter Norgaard,Zahra Shamsi,David Smalling,James Thompson,Subhashini Venugopalan,Brian P. Williams,Chujun He,Sarah Martinson,Martyna Plomecka,Lai Wei,Yuchen Zhou,Qian-Ze Zhu,Matthew Abraham,Erica Brand,Anna Bulanova,Jeffrey A. Cardille,Chris Co,Scott Ellsworth,Grace Joseph,Malcolm Kane,Ryan Krueger,Johan Kartiwa,Dan Liebling,Jan-Matthis Lueckmann,Paul Raccuglia,Xuefei,Wang,Katherine Chou,James Manyika,Yossi Matias,John C. Platt,Lizzie Dorfman,Shibl Mourad,Michael P. Brenner*

Main category: cs.AI

TL;DR: The paper presents an AI system combining a Large Language Model (LLM) with Tree Search (TS) to autonomously create expert-level scientific software that outperforms human approaches in various fields.


<details>
  <summary>Details</summary>
Motivation: Scientific progress is slowed by the manual development of computational tools, necessitating a faster, automated alternative.

Method: The system pairs a Large Language Model (LLM) with Tree Search (TS) to systematically navigate solution spaces and optimize a quality metric for software development. External knowledge is incorporated for advanced problem-solving.

Result: The system excelled across diverse fields: outperformed human methods in bioinformatics, epidemiology, geospatial analysis, zebrafish neural activity prediction, time series forecasting, and integral solutions.

Conclusion: This AI-driven system significantly accelerates scientific discovery by efficiently solving a wide range of research challenges with novel software solutions.

Abstract: The cycle of scientific discovery is frequently bottlenecked by the slow,
manual creation of software to support computational experiments. To address
this, we present an AI system that creates expert-level scientific software
whose goal is to maximize a quality metric. The system uses a Large Language
Model (LLM) and Tree Search (TS) to systematically improve the quality metric
and intelligently navigate the large space of possible solutions. The system
achieves expert-level results when it explores and integrates complex research
ideas from external sources. The effectiveness of tree search is demonstrated
across a wide range of benchmarks. In bioinformatics, it discovered 40 novel
methods for single-cell data analysis that outperformed the top human-developed
methods on a public leaderboard. In epidemiology, it generated 14 models that
outperformed the CDC ensemble and all other individual models for forecasting
COVID-19 hospitalizations. Our method also produced state-of-the-art software
for geospatial analysis, neural activity prediction in zebrafish, time series
forecasting and numerical solution of integrals. By devising and implementing
novel solutions to diverse tasks, the system represents a significant step
towards accelerating scientific progress.

</details>


### [41] [CogGuide: Human-Like Guidance for Zero-Shot Omni-Modal Reasoning](https://arxiv.org/abs/2509.06641)
*Zhou-Peng Shou,Zhi-Qiang You,Fang Wang,Hai-Bo Liu*

Main category: cs.AI

TL;DR: The paper introduces a zero-shot reasoning component for multimodal models inspired by human cognitive strategies, suppressing shortcuts and improving accuracy.


<details>
  <summary>Details</summary>
Motivation: The work aims to address issues of 'shortcuts' and inadequate contextual reasoning in complex cross-modal tasks in large multimodal reasoning models.

Method: The authors propose a plug-and-play three-module pipeline – Intent Perceiver, Strategy Generator, and Strategy Selector – to follow a cognitive process of 'understand-plan-select' using 'intent sketch.'

Result: Experiments show improvements of up to 9.51 percentage points in reasoning tasks on multiple benchmarks without parameter fine-tuning.

Conclusion: This 'intent sketch'-based zero-shot approach improves reasoning generality, portability, contextual understanding, and reduces shortcut reliance.

Abstract: Targeting the issues of "shortcuts" and insufficient contextual understanding
in complex cross-modal reasoning of multimodal large models, this paper
proposes a zero-shot multimodal reasoning component guided by human-like
cognitive strategies centered on an "intent sketch". The component comprises a
plug-and-play three-module pipeline-Intent Perceiver, Strategy Generator, and
Strategy Selector-that explicitly constructs a "understand-plan-select"
cognitive process. By generating and filtering "intent sketch" strategies to
guide the final reasoning, it requires no parameter fine-tuning and achieves
cross-model transfer solely through in-context engineering.
Information-theoretic analysis shows that this process can reduce conditional
entropy and improve information utilization efficiency, thereby suppressing
unintended shortcut reasoning. Experiments on IntentBench, WorldSense, and
Daily-Omni validate the method's generality and robust gains; compared with
their respective baselines, the complete "three-module" scheme yields
consistent improvements across different reasoning engines and pipeline
combinations, with gains up to approximately 9.51 percentage points,
demonstrating the practical value and portability of the "intent sketch"
reasoning component in zero-shot scenarios.

</details>


### [42] [Reinforcement Learning Foundations for Deep Research Systems: A Survey](https://arxiv.org/abs/2509.06733)
*Wenjun Li,Zhi Chen,Jingru Lin,Hannan Cao,Wei Han,Sheng Liang,Zhi Zhang,Kuicai Dong,Dexun Li,Chen Zhang,Yong Liu*

Main category: cs.AI

TL;DR: This survey examines the RL foundations of agentic AI systems for complex, multi-step tasks, focusing on data synthesis, RL methods, and training systems.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current AI training methods—specifically SFT and DPO—and the need for robust reinforcement learning approaches in deep research systems.

Method: The paper systematizes RL advancements across data synthesis, RL techniques for agentic research, and RL training systems, while examining agent architecture, coordination, benchmarks, and evaluation methods.

Result: The survey categorizes work in RL-based agentic AI systems, identifies recurring patterns, and highlights challenges such as infrastructure bottlenecks.

Conclusion: RL enhances deep research systems by offering frameworks for principled credit assignment, recovery behaviors, and reduced reliance on human biases, while highlighting practical guidance for future improvements.

Abstract: Deep research systems, agentic AI that solve complex, multi-step tasks by
coordinating reasoning, search across the open web and user files, and tool
use, are moving toward hierarchical deployments with a Planner, Coordinator,
and Executors. In practice, training entire stacks end-to-end remains
impractical, so most work trains a single planner connected to core tools such
as search, browsing, and code. While SFT imparts protocol fidelity, it suffers
from imitation and exposure biases and underuses environment feedback.
Preference alignment methods such as DPO are schema and proxy-dependent,
off-policy, and weak for long-horizon credit assignment and multi-objective
trade-offs. A further limitation of SFT and DPO is their reliance on human
defined decision points and subskills through schema design and labeled
comparisons. Reinforcement learning aligns with closed-loop, tool-interaction
research by optimizing trajectory-level policies, enabling exploration,
recovery behaviors, and principled credit assignment, and it reduces dependence
on such human priors and rater biases.
  This survey is, to our knowledge, the first dedicated to the RL foundations
of deep research systems. It systematizes work after DeepSeek-R1 along three
axes: (i) data synthesis and curation; (ii) RL methods for agentic research
covering stability, sample efficiency, long context handling, reward and credit
design, multi-objective optimization, and multimodal integration; and (iii)
agentic RL training systems and frameworks. We also cover agent architecture
and coordination, as well as evaluation and benchmarks, including recent QA,
VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We
distill recurring patterns, surface infrastructure bottlenecks, and offer
practical guidance for training robust, transparent deep research agents with
RL.

</details>


### [43] [VehicleWorld: A Highly Integrated Multi-Device Environment for Intelligent Vehicle Interaction](https://arxiv.org/abs/2509.06736)
*Jie Yang,Jiajun Chen,Zhangyue Yin,Shuo Chen,Yuxin Wang,Yiran Guo,Yuan Li,Yining Zheng,Xuanjing Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: This paper introduces VehicleWorld, a comprehensive automotive environment with real-time state features, proposing a novel State-based Function Call (SFC) approach that improves execution accuracy and efficiency over traditional Function Calling methods.


<details>
  <summary>Details</summary>
Motivation: To address inefficiency and limited error recovery in traditional stateless Function Calling methods in complex intelligent vehicle environments.

Method: The authors developed VehicleWorld featuring fully executable implementations of 30 modules, 250 APIs, and 680 properties, enabling state-based analysis. They proposed State-based Function Call (SFC) for explicit system state awareness and direct transitions.

Result: Experimental results show SFC achieves superior execution accuracy and reduced latency compared to traditional Function Calling methods.

Conclusion: The paper concludes that SFC is a significant improvement for intelligent vehicle systems, making the execution more efficient. They have also publicly shared the implementation code to foster further research.

Abstract: Intelligent vehicle cockpits present unique challenges for API Agents,
requiring coordination across tightly-coupled subsystems that exceed typical
task environments' complexity. Traditional Function Calling (FC) approaches
operate statelessly, requiring multiple exploratory calls to build
environmental awareness before execution, leading to inefficiency and limited
error recovery. We introduce VehicleWorld, the first comprehensive environment
for the automotive domain, featuring 30 modules, 250 APIs, and 680 properties
with fully executable implementations that provide real-time state information
during agent execution. This environment enables precise evaluation of vehicle
agent behaviors across diverse, challenging scenarios. Through systematic
analysis, we discovered that direct state prediction outperforms function
calling for environmental control. Building on this insight, we propose
State-based Function Call (SFC), a novel approach that maintains explicit
system state awareness and implements direct state transitions to achieve
target conditions. Experimental results demonstrate that SFC significantly
outperforms traditional FC approaches, achieving superior execution accuracy
and reduced latency. We have made all implementation code publicly available on
Github https://github.com/OpenMOSS/VehicleWorld.

</details>


### [44] [Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM Prompting](https://arxiv.org/abs/2509.06770)
*Shashidhar Reddy Javaji,Bhavul Gauri,Zining Zhu*

Main category: cs.AI

TL;DR: This paper introduces a framework to evaluate iterative refinement in tasks involving ideation, coding, and math. It examines metrics and outcomes across multiple turns to measure when iteration improves or worsens outcomes.


<details>
  <summary>Details</summary>
Motivation: To address the lack of clear evaluation methods for iterative workflows with large language models in diverse domains.

Method: The framework involves 12-turn conversations per task with different feedback types and tracks outputs using domain-specific scoring and metrics such as semantic movement, turn-level change, and output size growth.

Result: Findings show domain-dependent gains: early improvements in ideas and code, late-turn gains in math guided by elaboration, and plateau or regression with vague feedback. It outlines behavior patterns like size growth in code and semantic shifts in ideation.

Conclusion: The proposed framework makes iteration measurable across domains, highlights its domain-specific impact, and offers insights on when to modify or stop iterative workflows.

Abstract: Large language models (LLMs) are now used in multi-turn workflows, but we
still lack a clear way to measure when iteration helps and when it hurts. We
present an evaluation framework for iterative refinement that spans ideation,
code, and math. Our protocol runs controlled 12-turn conversations per task,
utilizing a variety of prompts ranging from vague ``improve it'' feedback to
targeted steering, and logs per-turn outputs. We score outcomes with
domain-appropriate checks (unit tests for code; answer-equivalence plus
reasoning-soundness for math; originality and feasibility for ideation) and
track turn-level behavior with three families of metrics: semantic movement
across turns, turn-to-turn change, and output size growth. Across models and
tasks, gains are domain-dependent: they arrive early in ideas and code, but in
math late turns matter when guided by elaboration. After the first few turns,
vague feedback often plateaus or reverses correctness, while targeted prompts
reliably shift the intended quality axis (novelty vs. feasibility in ideation;
speed vs. readability in code; in math, elaboration outperforms exploration and
drives late-turn gains). We also observe consistent domain patterns: ideation
moves more in meaning across turns, code tends to grow in size with little
semantic change, and math starts fixed but can break that path with late,
elaborative iteration.Together, the framework and metrics make iteration
measurable and comparable across models, and signal when to steer, stop, or
switch strategies.

</details>


### [45] [RAFFLES: Reasoning-based Attribution of Faults for LLM Systems](https://arxiv.org/abs/2509.06822)
*Chenyang Zhu,Spencer Hong,Jingyu Wu,Kushal Chawla,Charlotte Tang,Youbing Yin,Nathan Wolfe,Erin Babinsky,Daben Liu*

Main category: cs.AI

TL;DR: This paper introduces RAFFLES, an iterative evaluation architecture for diagnosing faults in long-horizon, multi-component LLM agentic systems, significantly improving fault identification accuracy over baselines.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying breakdowns and faults in long-horizon, multi-component LLM agentic systems, which current evaluation methods struggle with due to their limited scope and grounding.

Method: RAFFLES utilizes an iterative evaluation architecture with a central Judge and specialized Evaluators to systematically investigate faults and assess reasoning quality, employing a hypothesis-building approach.

Result: RAFFLES achieved over 43% fault-pair accuracy on the Algorithmically-Generated dataset (up from 16.6%) and over 20% accuracy on the Hand-Crafted dataset (up from 8.8%), outperforming prior methods.

Conclusion: RAFFLES represents a significant step towards automated fault detection for autonomous systems, reducing the reliance on manual reviews and enhancing evaluation capabilities for complex logic systems.

Abstract: We have reached a critical roadblock in the development and enhancement of
long-horizon, multi-component LLM agentic systems: it is incredibly tricky to
identify where these systems break down and why. Evaluation capabilities that
currently exist today (e.g., single pass LLM-as-a-judge) are limited in that
they often focus on individual metrics or capabilities, end-to-end outcomes,
and are narrowly grounded on the preferences of humans. We argue that to match
the agentic capabilities, evaluation frameworks must also be able to reason,
probe, iterate, and understand the complex logic passing through these systems
over long horizons. In this paper, we present RAFFLES - an evaluation
architecture that incorporates reasoning and iterative refinement.
Specifically, RAFFLES operates as an iterative, multi-component pipeline, using
a central Judge to systematically investigate faults and a set of specialized
Evaluators to assess not only the system's components but also the quality of
the reasoning by the Judge itself, thereby building a history of hypotheses. We
tested RAFFLES against several baselines on the Who&When dataset, a benchmark
designed to diagnose the "who" (agent) and "when" (step) of a system's failure.
RAFFLES outperforms these baselines, achieving an agent-step fault pair
accuracy of over 43% on the Algorithmically-Generated dataset (a substantial
increase from the previously published best of 16.6%) and over 20% on the
Hand-Crafted dataset (surpassing the previously published best of 8.8%). These
results demonstrate a key step towards introducing automated fault detection
for autonomous systems over labor-intensive manual human review.

</details>


### [46] [Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet](https://arxiv.org/abs/2509.06861)
*James Xu Zhao,Bryan Hooi,See-Kiong Ng*

Main category: cs.AI

TL;DR: Test-time scaling, which enhances reasoning by increasing computation during inference, shows inconsistent results, particularly for knowledge-intensive tasks. Instead of improving accuracy, it sometimes causes more hallucinations.


<details>
  <summary>Details</summary>
Motivation: Understanding the effectiveness and drawbacks of test-time scaling in reasoning tasks, especially for scenarios requiring high factual accuracy and low hallucination rates.

Method: A comprehensive evaluation was conducted using 12 reasoning models on two benchmarks. Analysis of hallucination behaviors and detailed case studies were also performed.

Result: Increasing test-time computation did not consistently improve factual accuracy and often heightened hallucination rates. Abstaining and attempting unanswered questions during extended reasoning influenced hallucination patterns.

Conclusion: While extended reasoning shows promise, its limitations—like inducing confirmation bias and overconfident hallucinations—must be addressed to make it truly effective for tasks demanding factual precision.

Abstract: Test-time scaling increases inference-time computation by allowing models to
generate long reasoning chains, and has shown strong performance across many
domains. However, in this work, we show that this approach is not yet effective
for knowledge-intensive tasks, where high factual accuracy and low
hallucination rates are essential. We conduct a comprehensive evaluation of
test-time scaling using 12 reasoning models on two knowledge-intensive
benchmarks. Our results reveal that increasing test-time computation does not
consistently improve accuracy and, in many cases, it even leads to more
hallucinations. We then analyze how extended reasoning affects hallucination
behavior. We find that reduced hallucinations often result from the model
choosing to abstain after thinking more, rather than from improved factual
recall. Conversely, for some models, longer reasoning encourages attempts on
previously unanswered questions, many of which result in hallucinations. Case
studies show that extended reasoning can induce confirmation bias, leading to
overconfident hallucinations. Despite these limitations, we observe that
compared to non-thinking, enabling thinking remains beneficial. Code and data
are available at https://github.com/XuZhao0/tts-knowledge

</details>


### [47] [Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents](https://arxiv.org/abs/2509.06917)
*Jiacheng Miao,Joe R. Davis,Jonathan K. Pritchard,James Zou*

Main category: cs.AI

TL;DR: Paper2Agent is an AI framework that converts research papers into interactive agents, enabling easier understanding and application.


<details>
  <summary>Details</summary>
Motivation: Research papers require significant effort for readers to understand and adapt methods, creating barriers to dissemination and reuse.

Method: Paper2Agent develops Model Context Protocols (MCPs) by analyzing papers and their codebases, testing for refinement, and connecting them with chat agents.

Result: The framework successfully created agents for genomic and transcriptomics analyses, reproducing results and enabling novel user queries.

Conclusion: Paper2Agent offers a dynamic and collaborative platform for knowledge dissemination, transforming static papers into active AI research assistants.

Abstract: We introduce Paper2Agent, an automated framework that converts research
papers into AI agents. Paper2Agent transforms research output from passive
artifacts into active systems that can accelerate downstream use, adoption, and
discovery. Conventional research papers require readers to invest substantial
effort to understand and adapt a paper's code, data, and methods to their own
work, creating barriers to dissemination and reuse. Paper2Agent addresses this
challenge by automatically converting a paper into an AI agent that acts as a
knowledgeable research assistant. It systematically analyzes the paper and the
associated codebase using multiple agents to construct a Model Context Protocol
(MCP) server, then iteratively generates and runs tests to refine and robustify
the resulting MCP. These paper MCPs can then be flexibly connected to a chat
agent (e.g. Claude Code) to carry out complex scientific queries through
natural language while invoking tools and workflows from the original paper. We
demonstrate Paper2Agent's effectiveness in creating reliable and capable paper
agents through in-depth case studies. Paper2Agent created an agent that
leverages AlphaGenome to interpret genomic variants and agents based on ScanPy
and TISSUE to carry out single-cell and spatial transcriptomics analyses. We
validate that these paper agents can reproduce the original paper's results and
can correctly carry out novel user queries. By turning static papers into
dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for
knowledge dissemination and a foundation for the collaborative ecosystem of AI
co-scientists.

</details>


### [48] [Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference](https://arxiv.org/abs/2509.06942)
*Xiangwei Shen,Zhimin Li,Zhantao Yang,Shiyi Zhang,Yingfang Zhang,Donghao Li,Chunyu Wang,Qinglin Lu,Yansong Tang*

Main category: cs.AI

TL;DR: The paper introduces Direct-Align and Semantic Relative Preference Optimization (SRPO) to improve diffusion models, addressing computational problems and aesthetic adaptability for better realism.


<details>
  <summary>Details</summary>
Motivation: Diffusion models face challenges in computational efficiency and achieving desired aesthetic qualities, requiring better optimization methods.

Method: Direct-Align uses predefined noise priors for interpolation to simplify denoising, and SRPO formulates rewards as text-conditioned signals for online adaptability.

Result: The fine-tuned FLUX.1.dev model achieves over 3x improvement in human-evaluated realism and aesthetic quality.

Conclusion: The proposed methods optimize diffusion models for efficient denoising and adaptable aesthetics, greatly enhancing their utility and effectiveness in alignment with human preferences.

Abstract: Recent studies have demonstrated the effectiveness of directly aligning
diffusion models with human preferences using differentiable reward. However,
they exhibit two primary challenges: (1) they rely on multistep denoising with
gradient computation for reward scoring, which is computationally expensive,
thus restricting optimization to only a few diffusion steps; (2) they often
need continuous offline adaptation of reward models in order to achieve desired
aesthetic quality, such as photorealism or precise lighting effects. To address
the limitation of multistep denoising, we propose Direct-Align, a method that
predefines a noise prior to effectively recover original images from any time
steps via interpolation, leveraging the equation that diffusion states are
interpolations between noise and target images, which effectively avoids
over-optimization in late timesteps. Furthermore, we introduce Semantic
Relative Preference Optimization (SRPO), in which rewards are formulated as
text-conditioned signals. This approach enables online adjustment of rewards in
response to positive and negative prompt augmentation, thereby reducing the
reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model
with optimized denoising and online reward adjustment, we improve its
human-evaluated realism and aesthetic quality by over 3x.

</details>


### [49] [Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation](https://arxiv.org/abs/2509.03736)
*James Mooney,Josef Woldense,Zheng Robert Jia,Shirley Anugrah Hayati,My Ha Nguyen,Vipul Raheja,Dongyeop Kang*

Main category: cs.AI

TL;DR: This paper evaluates the internal consistency of LLMs and finds significant inconsistencies in agent behavior across settings, questioning their reliability as substitutes for human participants in social science research.


<details>
  <summary>Details</summary>
Motivation: The paper investigates whether synthetic agents, specifically LLMs, can reliably substitute for real human participants in behavioral and social science experiments, an area widely discussed but under-examined for internal consistency.

Method: The study uses experiments designed to reveal the agent's internal state and assess its behaviors in dialogue settings, testing behavioral hypotheses to evaluate consistency between their inner state and external behavior.

Result: Findings demonstrate significant internal inconsistencies in LLMs across families and sizes, showing that the agents fail to reliably reflect their internal states in conversation.

Conclusion: LLMs lack internal consistency, limiting their ability to act as reliable substitutes for human participants in human-subject research.

Abstract: The impressive capabilities of Large Language Models (LLMs) have fueled the
notion that synthetic agents can serve as substitutes for real participants in
human-subject research. In an effort to evaluate the merits of this claim,
social science researchers have largely focused on whether LLM-generated survey
data corresponds to that of a human counterpart whom the LLM is prompted to
represent. In contrast, we address a more fundamental question: Do agents
maintain internal consistency, retaining similar behaviors when examined under
different experimental settings? To this end, we develop a study designed to
(a) reveal the agent's internal state and (b) examine agent behavior in a basic
dialogue setting. This design enables us to explore a set of behavioral
hypotheses to assess whether an agent's conversation behavior is consistent
with what we would expect from their revealed internal state. Our findings on
these hypotheses show significant internal inconsistencies in LLMs across model
families and at differing model sizes. Most importantly, we find that, although
agents may generate responses matching those of their human counterparts, they
fail to be internally consistent, representing a critical gap in their
capabilities to accurately substitute for real participants in human-subject
research. Our simulation code and data are publicly accessible.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [50] [Characterizing and Optimizing Realistic Workloads on a Commercial Compute-in-SRAM Device](https://arxiv.org/abs/2509.05451)
*Niansong Zhang,Wenbo Zhu,Courtney Golden,Dan Ilan,Hongzheng Chen,Christopher Batten,Zhiru Zhang*

Main category: cs.AR

TL;DR: This paper analyzes the real-world performance and energy efficiency of a commercial compute-in-SRAM device and demonstrates its advantages over traditional CPUs and GPUs in specific applications.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the real-world potential and optimization strategies of compute-in-SRAM architectures using a commercial device under realistic workloads.

Method: The study proposes three data management optimizations and uses an analytical framework to evaluate the GSI APU against CPUs and GPUs, measuring actual device performance and modeling shared off-chip memory bandwidth.

Result: The compute-in-SRAM device achieves a 4.8–6.6x speedup in retrieval tasks and matches GPU performance in RAG workloads, while being significantly more energy-efficient (54.4–117.9x).

Conclusion: Compute-in-SRAM architectures are validated as highly energy-efficient systems capable of handling complex, real-world workloads, and the paper offers guidance for their further optimization and adoption.

Abstract: Compute-in-SRAM architectures offer a promising approach to achieving higher
performance and energy efficiency across a range of data-intensive
applications. However, prior evaluations have largely relied on simulators or
small prototypes, limiting the understanding of their real-world potential. In
this work, we present a comprehensive performance and energy characterization
of a commercial compute-in-SRAM device, the GSI APU, under realistic workloads.
We compare the GSI APU against established architectures, including CPUs and
GPUs, to quantify its energy efficiency and performance potential. We introduce
an analytical framework for general-purpose compute-in-SRAM devices that
reveals fundamental optimization principles by modeling performance trade-offs,
thereby guiding program optimizations.
  Exploiting the fine-grained parallelism of tightly integrated memory-compute
architectures requires careful data management. We address this by proposing
three optimizations: communication-aware reduction mapping, coalesced DMA, and
broadcast-friendly data layouts. When applied to retrieval-augmented generation
(RAG) over large corpora (10GB--200GB), these optimizations enable our
compute-in-SRAM system to accelerate retrieval by 4.8$\times$--6.6$\times$ over
an optimized CPU baseline, improving end-to-end RAG latency by
1.1$\times$--1.8$\times$. The shared off-chip memory bandwidth is modeled using
a simulated HBM, while all other components are measured on the real
compute-in-SRAM device. Critically, this system matches the performance of an
NVIDIA A6000 GPU for RAG while being significantly more energy-efficient
(54.4$\times$-117.9$\times$ reduction). These findings validate the viability
of compute-in-SRAM for complex, real-world applications and provide guidance
for advancing the technology.

</details>


### [51] [High Utilization Energy-Aware Real-Time Inference Deep Convolutional Neural Network Accelerator](https://arxiv.org/abs/2509.05688)
*Kuan-Ting Lin,Ching-Te Chiu,Jheng-Yi Chang,Shi-Zong Huang,Yu-Ting Li*

Main category: cs.AR

TL;DR: This paper introduces a real-time DCNN accelerator for edge devices that improves performance and energy efficiency by optimizing computation, data reuse, and transfer processes.


<details>
  <summary>Details</summary>
Motivation: To address the impracticality of inference latency and high computational complexity of state-of-the-art DCNN models on edge devices.

Method: Introduced an accelerator featuring 1x1 convolution kernels, a Reuse Feature SRAM for minimizing data transfer, output reuse strategies, Ring Stream Dataflow, and an On-fly Pooling Module for efficient layer computation in hardware.

Result: Achieved 533x reduction in data access amount, 7.52x speed improvement, and 1.92x energy efficiency compared to prior methods, ensuring real-time processing capabilities.

Conclusion: The proposed hardware accelerator significantly enhances inference efficiency, making DCNNs feasible for real-time applications on edge devices while markedly reducing energy usage and data movement.

Abstract: Deep convolution Neural Network (DCNN) has been widely used in computer
vision tasks. However, for edge devices even inference has too large
computational complexity and data access amount. The inference latency of
state-of-the-art models are impractical for real-world applications. In this
paper, we propose a high utilization energy-aware real-time inference deep
convolutional neural network accelerator, which improves the performance of the
current accelerators. First, we use the 1x1 size convolution kernel as the
smallest unit of the computing unit. Then we design suitable computing unit
based on the requirements of each model. Secondly, we use Reuse Feature SRAM to
store the output of the current layer in the chip and use the value as the
input of the next layer. Moreover, we import Output Reuse Strategy and Ring
Stream Dataflow to reduce the amount of data exchange between chips and DRAM.
Finally, we present On-fly Pooling Module to let the calculation of the Pooling
layer directly complete in the chip. With the aid of the proposed method, the
implemented acceleration chip has an extremely high hardware utilization rate.
We reduce a generous amount of data transfer on the specific module, ECNN.
Compared to the methods without reuse strategy, we can reduce 533 times of data
access amount. At the same time, we have enough computing power to perform
real-time execution of the existing image classification model, VGG16 and
MobileNet. Compared with the design in VWA, we can speed up 7.52 times and have
1.92x energy efficiency

</details>


### [52] [Hardware Acceleration of Kolmogorov-Arnold Network (KAN) in Large-Scale Systems](https://arxiv.org/abs/2509.05937)
*Wei-Hsing Huang,Jianwei Jia,Yuyao Kong,Faaiq Waqar,Tai-Hao Wen,Meng-Fan Chang,Shimeng Yu*

Main category: cs.AR

TL;DR: The paper introduces Kolmogorov-Arnold Networks (KAN) with B-spline functions and proposes an algorithm-hardware co-design strategy for addressing acceleration complexities, achieving efficient scaling with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Motivated by the need to reduce parameter counts in neural networks while maintaining performance, the paper seeks to overcome hardware acceleration challenges associated with B-spline functional components in KAN architectures.

Method: The paper employs techniques at both algorithmic and circuit levels, including KAN quantization and sparsity-aware strategies, as well as analog-compute-in-memory (ACIM) circuits fabricated using the TSMC 22nm technology node.

Result: Despite a significant increase in parameters, area overhead and power consumption rose marginally, with accuracy degradation kept minimal, demonstrating the scalability of the proposed methodologies.

Conclusion: The algorithm-hardware co-design approach successfully addresses hardware acceleration complexities in KAN architectures, enabling large-scale tasks with efficient scaling and negligible loss of accuracy.

Abstract: Recent developments have introduced Kolmogorov-Arnold Networks (KAN), an
innovative architectural paradigm capable of replicating conventional deep
neural network (DNN) capabilities while utilizing significantly reduced
parameter counts through the employment of parameterized B-spline functions
with trainable coefficients. Nevertheless, the B-spline functional components
inherent to KAN architectures introduce distinct hardware acceleration
complexities. While B-spline function evaluation can be accomplished through
look-up table (LUT) implementations that directly encode functional mappings,
thus minimizing computational overhead, such approaches continue to demand
considerable circuit infrastructure, including LUTs, multiplexers, decoders,
and related components. This work presents an algorithm-hardware co-design
approach for KAN acceleration. At the algorithmic level, techniques include
Alignment-Symmetry and PowerGap KAN hardware aware quantization, KAN sparsity
aware mapping strategy, and circuit-level techniques include N:1 Time
Modulation Dynamic Voltage input generator with analog-compute-in-memory (ACIM)
circuits. This work conducts evaluations on large-scale KAN networks to
validate the proposed methodologies. Non-ideality factors, including partial
sum deviations from process variations, have been evaluated with statistics
measured from the TSMC 22nm RRAM-ACIM prototype chips. Utilizing optimally
determined KAN hyperparameters in conjunction with circuit optimizations
fabricated at the 22nm technology node, despite the parameter count for
large-scale tasks in this work increasing by 500Kx to 807Kx compared to
tiny-scale tasks in previous work, the area overhead increases by only 28Kx to
41Kx, with power consumption rising by merely 51x to 94x, while accuracy
degradation remains minimal at 0.11% to 0.23%, demonstrating the scaling
potential of our proposed architecture.

</details>


### [53] [SCREME: A Scalable Framework for Resilient Memory Design](https://arxiv.org/abs/2509.06101)
*Fan Li,Mimi Xie,Yanan Guo,Huize Li,Xin Xin*

Main category: cs.AR

TL;DR: The paper introduces SCREME, a cost-efficient and scalable memory framework that enhances memory resilience using slower, cheaper ECC chips and underutilized I/O resources on memory chips.


<details>
  <summary>Details</summary>
Motivation: Growing advancement in memory technology has increased performance but also amplified reliability challenges, with traditional approaches being inefficient in addressing the added costs of parity storage.

Method: SCREME utilizes slower, cost-effective ECC chips for parity storage and repurposes underutilized I/O resources from server-class memory to create flexible on-DIMM interconnections.

Result: SCREME achieves a balance of increased reliability and reduced costs by leveraging insights into memory performance and resource utilization.

Conclusion: SCREME is a scalable and innovative approach to memory design, addressing reliability concerns while maintaining cost-efficiency by rethinking the design of parity storage and interconnection resources.

Abstract: The continuing advancement of memory technology has not only fueled a surge
in performance, but also substantially exacerbate reliability challenges.
Traditional solutions have primarily focused on improving the efficiency of
protection schemes, i.e., Error Correction Codes (ECC), under the assumption
that allocating additional memory space for parity data is always expensive and
therefore not a scalable solution.
  We break the stereotype by proposing an orthogonal approach that provides
additional, cost-effective memory space for resilient memory design. In
particular, we recognize that ECC chips (used for parity storage) do not
necessarily require the same performance level as regular data chips. This
offers two-fold benefits: First, the bandwidth originally provisioned for a
regular-performance ECC chip can instead be used to accommodate multiple
low-performance chips. Second, the cost of ECC chips can be effectively
reduced, as lower performance often correlates with lower expense. In addition,
we observe that server-class memory chips are often provisioned with ample, yet
underutilized I/O resources. This further offers the opportunity to repurpose
these resources to enable flexible on-DIMM interconnections. Based on the above
two insights, we finally propose SCREME, a scalable memory framework leverages
cost-effective, albeit slower, chips -- naturally produced during rapid
technology evolution -- to meet the growing reliability demands driven by this
evolution.

</details>


### [54] [Hardware Acceleration in Portable MRIs: State of the Art and Future Prospects](https://arxiv.org/abs/2509.06365)
*Omar Al Habsi,Safa Mohammed Sali,Anis Meribout,Mahmoud Meribout,Saif Almazrouei,Mohamed Seghier*

Main category: cs.AR

TL;DR: The paper reviews advancements in portable MRI (pMRI), focusing on hardware acceleration and AI-driven solutions to improve imaging performance and portability.


<details>
  <summary>Details</summary>
Motivation: To tackle computational challenges in portable MRI systems, particularly image reconstruction and machine learning, and explore hardware solutions to enhance pMRI usability for resource-constrained environments.

Method: Reviewing key technologies like GPUs, FPGAs, and ASICs for hardware acceleration in pMRI, proposing AI reconstruction methods, and suggesting the creation of a Low-Field MRI Consortium.

Result: Highlighted hardware technologies significantly improve speed, power efficiency, and materialize enhanced image reconstruction, along with a push toward standardized benchmarks.

Conclusion: Hardware acceleration and AI have a transformative potential to uplift the capabilities and portability of pMRI systems, enabling efficient imaging even in remote settings.

Abstract: There is a growing interest in portable MRI (pMRI) systems for point-of-care
imaging, particularly in remote or resource-constrained environments. However,
the computational complexity of pMRI, especially in image reconstruction and
machine learning (ML) algorithms for enhanced imaging, presents significant
challenges. Such challenges can be potentially addressed by harnessing hardware
application solutions, though there is little focus in the current pMRI
literature on hardware acceleration. This paper bridges that gap by reviewing
recent developments in pMRI, emphasizing the role and impact of hardware
acceleration to speed up image acquisition and reconstruction. Key technologies
such as Graphics Processing Units (GPUs), Field-Programmable Gate Arrays
(FPGAs), and Application-Specific Integrated Circuits (ASICs) offer excellent
performance in terms of reconstruction speed and power consumption. This review
also highlights the promise of AI-powered reconstruction, open low-field pMRI
datasets, and innovative edge-based hardware solutions for the future of pMRI
technology. Overall, hardware acceleration can enhance image quality, reduce
power consumption, and increase portability for next-generation pMRI
technology. To accelerate reproducible AI for portable MRI, we propose forming
a Low-Field MRI Consortium and an evidence ladder (analytic/phantom validation,
retrospective multi-center testing, prospective reader and non-inferiority
trials) to provide standardized datasets, benchmarks, and regulator-ready
testbeds.

</details>


### [55] [VCO-CARE: VCO-based Calibration-free Analog Readout for Electrodermal activity sensing](https://arxiv.org/abs/2509.06698)
*Leidy Mabel Alvero-Gonzalez,Matias Miguez,Eric Gutierrez,Juan Sapriza,Susana Patón,David Atienza,José Miranda*

Main category: cs.AR

TL;DR: This paper proposes VCO-CARE, a high-sensitivity, low-power Voltage-Controlled Oscillator-based Analog Readout designed for wearable electrodermal activity monitoring.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of high sensitivity, low power consumption, and minimal calibration in Analog Front-End systems for wearable electrodermal activity monitoring.

Method: Introduction of VCO-CARE, which uses a Voltage-Controlled Oscillator-based Analog Readout coupled with post-layout validations to measure performance metrics like sensitivity, power consumption, and noise resilience.

Result: The system achieves an average sensitivity of 40 pS, a negligible relative error below 0.0025%, consumes 2.3 uW of power, and shows low noise contribution of 0.8 uVrms in the 0-1.5 Hz band.

Conclusion: VCO-CARE advances wearable sensor technology by delivering high adaptability, superior performance, and minimal resource consumption, addressing key challenges in EDA monitoring.

Abstract: Continuous monitoring of electrodermal activity (EDA) through wearable
devices has attracted much attention in recent times. However, the persistent
challenge demands analog front-end (AFE) systems with high sensitivity, low
power consumption, and minimal calibration requirements to ensure practical
usability in wearable technologies. In response to this challenge, this
research introduces VCO-CARE, a Voltage-Controlled Oscillator-based Analog
Readout tailored for continuous EDA sensing. The results show that our system
achieves an exceptional average sensitivity of up to 40 pS within a 0-20 uS
range and a negligible relative error of less than 0.0025% for
fixed-resistance. Furthermore, the proposed system consumes only an average of
2.3 uW based on post-layout validations and introduces a low noise
contribution, measuring only 0.8 uVrms across the 0-1.5 Hz EDA signal band.
This research aims to drive the evolution of wearable sensors characterized by
seamless adaptability to diverse users, minimal power consumption, and
outstanding noise resilience.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [56] [An Empirical Analysis of Discrete Unit Representations in Speech Language Modeling Pre-training](https://arxiv.org/abs/2509.05359)
*Yanis Labrak,Richard Dufour,Mickaël Rouvier*

Main category: cs.CL

TL;DR: The paper studies Speech Language Models (SLMs), focusing on improving speech modeling through pre-training optimization, including model architecture, data representation, and training robustness.


<details>
  <summary>Details</summary>
Motivation: The authors aim to enhance the adaptation of pre-trained language models to the speech modality by optimizing the speech modeling process during continual pre-training.

Method: They conducted experiments varying speech encoders, clustering granularity, and data selection to analyze strategies for improving discretization and robustness for various model scales.

Result: The experiments revealed the importance of clustering strategies varying with model capacity, effective discrete vocabulary use, and the critical role of domain-matched data in model robustness.

Conclusion: Optimal speech language model performance requires attention to architecture, data representation, and robust training strategies, emphasizing adaptations in model components for target applications.

Abstract: This paper investigates discrete unit representations in Speech Language
Models (SLMs), focusing on optimizing speech modeling during continual
pre-training. In this paper, we systematically examine how model architecture,
data representation, and training robustness influence the pre-training stage
in which we adapt existing pre-trained language models to the speech modality.
Our experiments highlight the role of speech encoders and clustering
granularity across different model scales, showing how optimal discretization
strategies vary with model capacity. By examining cluster distribution and
phonemic alignments, we investigate the effective use of discrete vocabulary,
uncovering both linguistic and paralinguistic patterns. Additionally, we
explore the impact of clustering data selection on model robustness,
highlighting the importance of domain matching between discretization training
and target applications.

</details>


### [57] [Beyond ROUGE: N-Gram Subspace Features for LLM Hallucination Detection](https://arxiv.org/abs/2509.05360)
*Jerry Li,Evangelos Papalexakis*

Main category: cs.CL

TL;DR: The paper tackles hallucinations in large language models (LLMs) by analyzing N-Gram frequency tensors to detect factual versus fabricated content, achieving improved results over traditional and modern baselines.


<details>
  <summary>Details</summary>
Motivation: The problem of hallucinations in LLMs (fabricating information) undermines their trustworthiness in generating accurate content. Current methods and metrics (e.g., ROUGE, Perplexity) lack sufficient semantic depth for effective hallucination detection.

Method: The paper proposes using an N-Gram frequency tensor constructed from LLM outputs. Tensor decomposition is applied to extract features, which are then fed into an MLP binary classifier to distinguish between factual and hallucinated content.

Result: The proposed method, evaluated on the HaluEval dataset, outperforms traditional baselines and demonstrates competitive performance against state-of-the-art LLM judges.

Conclusion: The novel N-Gram tensor and MLP-based approach enrich semantic understanding and represent a significant step forward in addressing hallucination detection in LLMs.

Abstract: Large Language Models (LLMs) have demonstrated effectiveness across a wide
variety of tasks involving natural language, however, a fundamental problem of
hallucinations still plagues these models, limiting their trustworthiness in
generating consistent, truthful information. Detecting hallucinations has
quickly become an important topic, with various methods such as uncertainty
estimation, LLM Judges, retrieval augmented generation (RAG), and consistency
checks showing promise. Many of these methods build upon foundational metrics,
such as ROUGE, BERTScore, or Perplexity, which often lack the semantic depth
necessary to detect hallucinations effectively. In this work, we propose a
novel approach inspired by ROUGE that constructs an N-Gram frequency tensor
from LLM-generated text. This tensor captures richer semantic structure by
encoding co-occurrence patterns, enabling better differentiation between
factual and hallucinated content. We demonstrate this by applying tensor
decomposition methods to extract singular values from each mode and use these
as input features to train a multi-layer perceptron (MLP) binary classifier for
hallucinations. Our method is evaluated on the HaluEval dataset and
demonstrates significant improvements over traditional baselines, as well as
competitive performance against state-of-the-art LLM judges.

</details>


### [58] [A Lightweight Framework for Trigger-Guided LoRA-Based Self-Adaptation in LLMs](https://arxiv.org/abs/2509.05385)
*Jiacheng Wei,Faguo Wu,Xiao Zhang*

Main category: cs.CL

TL;DR: SAGE is a framework designed to allow large language models to adapt and learn during inference by decomposing tasks into subtasks and dynamically fine-tuning with three main components.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle to continuously adapt and learn during reasoning at inference time, necessitating an approach for dynamic updates.

Method: SAGE is a dynamic fine-tuning framework comprising the Trigger module for detecting failures, Trigger Buffer module for clustering anomalies, and Lora Store module for parameter updates.

Result: SAGE demonstrated superior accuracy, robustness, and stability on atomic reasoning subtasks during test time, showing its effectiveness in dynamic knowledge updating.

Conclusion: The paper concludes that SAGE resolves limitations in continuous learning for large models by enabling adaptive updates during inference for improved task-solving performance.

Abstract: Large language models are unable to continuously adapt and learn from new
data during reasoning at inference time. To address this limitation, we propose
that complex reasoning tasks be decomposed into atomic subtasks and introduce
SAGE, a trigger-guided dynamic fine-tuning framework that enables adaptive
updates during reasoning at inference time. SAGE consists of three key
components: (1) a Trigger module that detects reasoning failures through
multiple evaluation metrics in real time; (2) a Trigger Buffer module that
clusters anomaly samples using a streaming clustering process with HDBSCAN,
followed by stability checks and similarity-based merging; and (3) a Lora Store
module that dynamically optimizes parameter updates with an adapter pool for
knowledge retention. Evaluation results show that SAGE demonstrates excellent
accuracy, robustness, and stability on the atomic reasoning subtask through
dynamic knowledge updating during test time.

</details>


### [59] [Talk Isn't Always Cheap: Understanding Failure Modes in Multi-Agent Debate](https://arxiv.org/abs/2509.05396)
*Andrea Wynn,Harsh Satija,Gillian Hadfield*

Main category: cs.CL

TL;DR: This paper explores the potential harm in multi-agent AI debate and reveals how diversity in model capabilities can impact debate dynamics and outcomes. It finds that debates can sometimes reduce accuracy, even among more capable agents.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand how diversity in AI model capabilities influences reasoning outcomes in multi-agent debates and seeks to identify challenges in such interactions.

Method: Through experimental setups, the authors test debate scenarios involving models of varying capabilities, observing their reasoning dynamics and accuracy shifts over time.

Result: The analysis finds that debates often lead to models shifting from correct to incorrect answers, and stronger models are also prone to agreeing with flawed reasoning from weaker ones, leading to accuracy drops.

Conclusion: The study highlights shortcomings in multi-agent debate mechanisms, warning against naive implementations where agents may favor consensus over critical reasoning, causing performance degradation.

Abstract: While multi-agent debate has been proposed as a promising strategy for
improving AI reasoning ability, we find that debate can sometimes be harmful
rather than helpful. The prior work has exclusively focused on debates within
homogeneous groups of agents, whereas we explore how diversity in model
capabilities influences the dynamics and outcomes of multi-agent interactions.
Through a series of experiments, we demonstrate that debate can lead to a
decrease in accuracy over time -- even in settings where stronger (i.e., more
capable) models outnumber their weaker counterparts. Our analysis reveals that
models frequently shift from correct to incorrect answers in response to peer
reasoning, favoring agreement over challenging flawed reasoning. These results
highlight important failure modes in the exchange of reasons during multi-agent
debate, suggesting that naive applications of debate may cause performance
degradation when agents are neither incentivized nor adequately equipped to
resist persuasive but incorrect reasoning.

</details>


### [60] [No Translation Needed: Forecasting Quality from Fertility and Metadata](https://arxiv.org/abs/2509.05425)
*Jessica M. Lundin,Ada Zhang,David Adelani,Cody Carroll*

Main category: cs.CL

TL;DR: The paper predicts translation quality without using the translation system.


<details>
  <summary>Details</summary>
Motivation: To provide translation quality predictions without invoking the translation model, leveraging simpler features and linguistic insights.

Method: The authors used gradient boosting models and simplicity features like token fertility ratios, token counts, and linguistic metadata to forecast performance.

Result: The model achieved strong prediction accuracy with R² = 0.66 for XX→English and R² = 0.72 for English→XX on the FLORES-200 benchmark.

Conclusion: Translation quality is influenced by token fertility and linguistic typology, offering a new perspective on multilingual evaluation and quality modeling.

Abstract: We show that translation quality can be predicted with surprising accuracy
\textit{without ever running the translation system itself}. Using only a
handful of features, token fertility ratios, token counts, and basic linguistic
metadata (language family, script, and region), we can forecast ChrF scores for
GPT-4o translations across 203 languages in the FLORES-200 benchmark. Gradient
boosting models achieve favorable performance ($R^{2}=0.66$ for
XX$\rightarrow$English and $R^{2}=0.72$ for English$\rightarrow$XX). Feature
importance analyses reveal that typological factors dominate predictions into
English, while fertility plays a larger role for translations into diverse
target languages. These findings suggest that translation quality is shaped by
both token-level fertility and broader linguistic typology, offering new
insights for multilingual evaluation and quality estimation.

</details>


### [61] [Direct-Scoring NLG Evaluators Can Use Pairwise Comparisons Too](https://arxiv.org/abs/2509.05440)
*Logan Lawrence,Ashton Williamson,Alexander Shelton*

Main category: cs.CL

TL;DR: The paper introduces a direct-scoring method for evaluating large language model-generated summaries by using synthetic summaries for pairwise rankings. It achieves comparable correlations with human judgment to state-of-the-art evaluators.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of pairwise comparison methods which cannot assign absolute scores to summaries, intended for applications requiring thresholding.

Method: Proposing a direct-scoring method that uses synthetic summaries to provide pairwise rankings during evaluation.

Result: The proposed method performs similarly to leading evaluators in sample-level correlation benchmarks, with slight performance variations (+0.03, -0.03, +0.05) across different datasets.

Conclusion: The direct-scoring method is a viable alternative for evaluating machine-generated text, providing absolute scoring capabilities alongside comparable performance to existing methods. Synthetic summaries were also released to support future research.

Abstract: As large-language models have been increasingly used as automatic raters for
evaluating free-form content, including document summarization, dialog, and
story generation, work has been dedicated to evaluating such models by
measuring their correlations with human judgment. For \textit{sample-level}
performance, methods which operate by using pairwise comparisons between
machine-generated text perform well but often lack the ability to assign
absolute scores to individual summaries, an ability crucial for use cases that
require thresholding. In this work, we propose a direct-scoring method which
uses synthetic summaries to act as pairwise machine rankings at test time. We
show that our method performs comparably to state-of-the-art pairwise
evaluators in terms of axis-averaged sample-level correlations on the SummEval
(\textbf{+0.03}), TopicalChat (\textbf{-0.03}), and HANNA (\textbf{+0.05})
meta-evaluation benchmarks, and release the synthetic in-context summaries as
data to facilitate future work.

</details>


### [62] [From Staff Messages to Actionable Insights: A Multi-Stage LLM Classification Framework for Healthcare Analytics](https://arxiv.org/abs/2509.05484)
*Hajar Sakai,Yi-En Tseng,Mohammadsadegh Mikaeili,Joshua Bosire,Franziska Jovin*

Main category: cs.CL

TL;DR: The paper discusses a multi-stage Large Language Model (LLM)-based framework for analyzing hospital call center messages to classify topics and reasons effectively. LLMs like o3 and gpt-5 were tested, achieving high accuracy, and their outputs were transformed into actionable insights through a visualization tool.


<details>
  <summary>Details</summary>
Motivation: To leverage the extensive text data generated by hospital call centers to gain actionable insights without the extensive manual effort required by traditional supervised learning methods.

Method: A multi-stage framework using different Large Language Models (LLMs), such as reasoning, general-purpose, and lightweight models, for message topic identification and classification. Models were evaluated for performance, and the best outputs were integrated into a decision-support visualization tool.

Result: The model 'o3' achieved the best performance with a 78.4% weighted F1-score and 79.2% accuracy, followed by gpt-5 with 75.3% weighted F1-score and 76.2% accuracy. Outputs satisfy data security and HIPAA compliance requirements.

Conclusion: The proposed LLM framework enhances the utilization of hospital call center data by simplifying and improving analysis processes. This approach supports better patient care quality and identifies staff training needs.

Abstract: Hospital call centers serve as the primary contact point for patients within
a hospital system. They also generate substantial volumes of staff messages as
navigators process patient requests and communicate with the hospital offices
following the established protocol restrictions and guidelines. This
continuously accumulated large amount of text data can be mined and processed
to retrieve insights; however, traditional supervised learning approaches
require annotated data, extensive training, and model tuning. Large Language
Models (LLMs) offer a paradigm shift toward more computationally efficient
methodologies for healthcare analytics. This paper presents a multi-stage
LLM-based framework that identifies staff message topics and classifies
messages by their reasons in a multi-class fashion. In the process, multiple
LLM types, including reasoning, general-purpose, and lightweight models, were
evaluated. The best-performing model was o3, achieving 78.4% weighted F1-score
and 79.2% accuracy, followed closely by gpt-5 (75.3% Weighted F1-score and
76.2% accuracy). The proposed methodology incorporates data security measures
and HIPAA compliance requirements essential for healthcare environments. The
processed LLM outputs are integrated into a visualization decision support tool
that transforms the staff messages into actionable insights accessible to
healthcare professionals. This approach enables more efficient utilization of
the collected staff messaging data, identifies navigator training
opportunities, and supports improved patient experience and care quality.

</details>


### [63] [The Token Tax: Systematic Bias in Multilingual Tokenization](https://arxiv.org/abs/2509.05486)
*Jessica M. Lundin,Ada Zhang,Nihal Karim,Hamza Louzan,Victor Wei,David Adelani,Cody Carroll*

Main category: cs.CL

TL;DR: The study explores tokenization challenges in African languages, highlighting that higher token inflation leads to reduced model accuracy and increased computational costs, while recommending morphologically aware tokenization for fairness.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the structural disadvantages faced by morphologically complex, low-resource languages due to tokenization inefficiency, which impacts computational resource demands and model performance.

Method: The authors evaluated 10 large language models on the AfriMMLU dataset, which includes 9,000 multiple-choice questions spanning five subjects in 16 African languages.

Result: The study found that token fertility (tokens/word) negatively correlates with model accuracy and increases training costs, while reasoning models performed better across languages compared to non-reasoning ones.

Conclusion: The findings advocate for morphologically aware tokenization, fair pricing strategies, and multilingual benchmarks to ensure equitable advances in NLP for diverse languages.

Abstract: Tokenization inefficiency imposes structural disadvantages on morphologically
complex, low-resource languages, inflating compute resources and depressing
accuracy. We evaluate 10 large language models (LLMs) on AfriMMLU (9,000 MCQA
items; 5 subjects; 16 African languages) and show that fertility (tokens/word)
reliably predicts accuracy. Higher fertility consistently predicts lower
accuracy across all models and subjects. We further find that reasoning models
(DeepSeek, o1) consistently outperform non-reasoning peers across high and low
resource languages in the AfriMMLU dataset, narrowing accuracy gaps observed in
prior generations. Finally, translating token inflation to economics, a
doubling in tokens results in quadrupled training cost and time, underscoring
the token tax faced by many languages. These results motivate morphologically
aware tokenization, fair pricing, and multilingual benchmarks for equitable
natural language processing (NLP).

</details>


### [64] [Biomedical Literature Q&A System Using Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2509.05505)
*Mansi Garg,Lee-Chi Wang,Bhavesh Ghanchi,Sanjana Dumpala,Shreyash Kakde,Yen Chih Chen*

Main category: cs.CL

TL;DR: The paper presents a Q&A system using Retrieval-Augmented Generation (RAG) for biomedical literature, improving access to evidence-based and accurate medical information.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing health search engines and the delay in public access to biomedical research by providing concise, accurate, and context-aware medical information.

Method: The system employs MiniLM-based semantic embeddings and FAISS vector search for information retrieval and uses a fine-tuned Mistral-7B-v0.3 (optimized with QLoRA) for answer generation, validating its effectiveness on general medical queries and domain-specific tasks like breast cancer literature.

Result: Experiments using BERTScore (F1) indicate significant improvements in factual and semantic relevance compared to baseline models.

Conclusion: The RAG-based system effectively bridges the gap between complex biomedical literature and public understanding, with potential for multilingual, private, and personalized AI adaptations in the future.

Abstract: This work presents a Biomedical Literature Question Answering (Q&A) system
based on a Retrieval-Augmented Generation (RAG) architecture, designed to
improve access to accurate, evidence-based medical information. Addressing the
shortcomings of conventional health search engines and the lag in public access
to biomedical research, the system integrates diverse sources, including PubMed
articles, curated Q&A datasets, and medical encyclopedias ,to retrieve relevant
information and generate concise, context-aware responses. The retrieval
pipeline uses MiniLM-based semantic embeddings and FAISS vector search, while
answer generation is performed by a fine-tuned Mistral-7B-v0.3 language model
optimized using QLoRA for efficient, low-resource training. The system supports
both general medical queries and domain-specific tasks, with a focused
evaluation on breast cancer literature demonstrating the value of
domain-aligned retrieval. Empirical results, measured using BERTScore (F1),
show substantial improvements in factual consistency and semantic relevance
compared to baseline models. The findings underscore the potential of
RAG-enhanced language models to bridge the gap between complex biomedical
literature and accessible public health knowledge, paving the way for future
work on multilingual adaptation, privacy-preserving inference, and personalized
medical AI systems.

</details>


### [65] [Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study](https://arxiv.org/abs/2509.05553)
*Serge Lionel Nikiema,Jordan Samhi,Micheline Bénédicte Moumoula,Albérick Euraste Djiré,Abdoul Kader Kaboré,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CL

TL;DR: This paper explores whether large language models genuinely understand concepts or merely recognize patterns, introducing bidirectional reasoning as a test for true comprehension.


<details>
  <summary>Details</summary>
Motivation: The study seeks to determine if large language models can achieve genuine understanding by introducing bidirectional reasoning as a criterion, especially focusing on reversibility of learned transformations.

Method: The researchers propose Contrastive Fine-Tuning (CFT), which uses three types of training examples to instill bidirectional reasoning: positive semantic examples, negative semantic examples, and examples designed to test reverse obfuscation.

Result: Contrastive Fine-Tuning (CFT) enabled models to achieve bidirectional reasoning, maintaining strong performance on forward tasks while also excelling at reverse reasoning tasks.

Conclusion: Bidirectional reasoning provides both a framework for evaluating genuine understanding and a practical training method to develop more capable AI systems.

Abstract: This research addresses a fundamental question in AI: whether large language
models truly understand concepts or simply recognize patterns. The authors
propose bidirectional reasoning,the ability to apply transformations in both
directions without being explicitly trained on the reverse direction, as a test
for genuine understanding. They argue that true comprehension should naturally
allow reversibility. For example, a model that can change a variable name like
userIndex to i should also be able to infer that i represents a user index
without reverse training. The researchers tested current language models and
discovered what they term cognitive specialization: when models are fine-tuned
on forward tasks, their performance on those tasks improves, but their ability
to reason bidirectionally becomes significantly worse. To address this issue,
they developed Contrastive Fine-Tuning (CFT), which trains models using three
types of examples: positive examples that maintain semantic meaning, negative
examples with different semantics, and forward-direction obfuscation examples.
This approach aims to develop deeper understanding rather than surface-level
pattern recognition and allows reverse capabilities to develop naturally
without explicit reverse training. Their experiments demonstrated that CFT
successfully achieved bidirectional reasoning, enabling strong reverse
performance while maintaining forward task capabilities. The authors conclude
that bidirectional reasoning serves both as a theoretical framework for
assessing genuine understanding and as a practical training approach for
developing more capable AI systems.

</details>


### [66] [Ad hoc conventions generalize to new referents](https://arxiv.org/abs/2509.05566)
*Anya Ji,Claire Augusta Bergey,Ron Eliav,Yoav Artzi,Robert D. Hawkins*

Main category: cs.CL

TL;DR: The paper investigates how people develop shared naming systems for new objects, exploring whether these are arbitrary or conceptually broader. Evidence shows that these systems lead to generalization, suggesting conceptual coordination.


<details>
  <summary>Details</summary>
Motivation: To understand whether and how people generalize naming conventions developed through communication, shedding light on whether these conventions are arbitrary or conceptually broad.

Method: A dyadic communication study with 302 participants using the KiloGram dataset involving abstract tangram images. Participants coordinated on referential conventions for some images and were tested for generalization on new, undiscussed images.

Result: Participants showed increased alignment on undiscussed images after forming naming conventions, with generalization capabilities decaying based on visual similarity and robustness across different image nameability levels.

Conclusion: Ad hoc naming conventions are not arbitrary but reflect deeper conceptual coordination, offering insights for theories of reference and adaptive language agent design.

Abstract: How do people talk about things they've never talked about before? One view
suggests that a new shared naming system establishes an arbitrary link to a
specific target, like proper names that cannot extend beyond their bearers. An
alternative view proposes that forming a shared way of describing objects
involves broader conceptual alignment, reshaping each individual's semantic
space in ways that should generalize to new referents. We test these competing
accounts in a dyadic communication study (N=302) leveraging the
recently-released KiloGram dataset containing over 1,000 abstract tangram
images. After pairs of participants coordinated on referential conventions for
one set of images through repeated communication, we measured the extent to
which their descriptions aligned for undiscussed images. We found strong
evidence for generalization: partners showed increased alignment relative to
their pre-test labels. Generalization also decayed nonlinearly with visual
similarity (consistent with Shepard's law) and was robust across levels of the
images' nameability. These findings suggest that ad hoc conventions are not
arbitrary labels but reflect genuine conceptual coordination, with implications
for theories of reference and the design of more adaptive language agents.

</details>


### [67] [Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought Correctness Perception Distillation](https://arxiv.org/abs/2509.05602)
*Hongyan Xie,Yitong Yao,Yikun Ban,Zixuan Huang,Deqing Wang,Zhenhe Wu,Haoxiang Su,Chao Wang,Shuangyong Song,Xuelong Li*

Main category: cs.CL

TL;DR: The paper introduces CoPeD, a method targeting better reasoning in SLMs by focusing on rationale correctness and data weighting to mitigate noise from LLM-generated CoT data.


<details>
  <summary>Details</summary>
Motivation: To address the problem of noisy rationales in CoT data fine-tuned into small language models, which reduce reasoning quality.

Method: Proposes two strategies: correctness-aware task settings to improve rationale reliability and a Correctness-Aware Weighted loss to prioritize meaningful training samples.

Result: The experiments showed CoPeD significantly enhanced reasoning quality in both in-distribution and out-of-distribution benchmark tasks.

Conclusion: CoPeD helps SLMs learn more accurate reasoning while mitigating spurious correlations by improving the CoT data and the way it's utilized.

Abstract: Large language models (LLMs) excel at reasoning tasks but are expensive to
deploy. Thus small language models (SLMs) are fine-tuned on CoT data generated
by LLMs to copy LLMs' abilities. However, these CoT data may include noisy
rationales that either fail to substantiate the answers or contribute no
additional information to support answer prediction, which leads SLMs to
capture spurious correlations between questions and answers and compromise the
quality of reasoning. In this work, we propose Chain-of-Thought Correctness
Perception Distillation (CoPeD), which aims to improve the reasoning quality of
the student model from the perspectives of task setting and data utilization.
Firstly, we introduce a correctness-aware task setting that encourages the
student model to predict answers based on correct rationales and revise them
when they are incorrect. This setting improves the faithfulness of reasoning
and allows the model to learn from its mistakes. Then, we propose a
Correctness-Aware Weighted loss, which dynamically adjusts the contribution of
each training instance based on the combined loss of the rationale and the
answer. This strategy encourages the model to focus more on samples where the
rationale offers stronger support for the correct answer. Experiments have
shown that CoPeD is effective on both in-distribution (IND) and
out-of-distribution (OOD) benchmark reasoning datasets.

</details>


### [68] [Icon$^{2}$: Aligning Large Language Models Using Self-Synthetic Preference Data via Inherent Regulation](https://arxiv.org/abs/2509.05605)
*Qiyuan Chen,Hongsen Huang,Qian Shao,Jiahe Chen,Jintai Chen,Hongxia Xu,Renjie Hua,Ren Chuan,Jian Wu*

Main category: cs.CL

TL;DR: This paper proposes Icon2, a novel method for constructing high-quality preference datasets for large language models, leveraging their internal regulation mechanisms for efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in preference dataset construction, such as distribution mismatches and high computational costs, which hinder the alignment of LLMs with human preferences.

Method: Icon2 extracts layer-wise direction vectors in LLMs to represent human preferences, uses these vectors to filter self-synthesized instructions, and applies bidirectional control during decoding for generating high-quality response pairs.

Result: Experimental results show improvements in both alignment and efficiency, including a 13.89% and 13.45% average win rate increase on AlpacaEval 2.0 and Arena-Hard respectively, and up to 48.1% reduction in computational costs.

Conclusion: The Icon2 method enables more efficient and tailored construction of preference datasets, leading to improved model alignment and reduced computational expenses.

Abstract: Large Language Models (LLMs) require high quality preference datasets to
align with human preferences. However, conventional methods for constructing
such datasets face significant challenges: reliance on pre-collected
instructions often leads to distribution mismatches with target models, while
the need for sampling multiple stochastic responses introduces substantial
computational overhead. In this work, we explore a paradigm shift by leveraging
inherent regulation of LLMs' representation space for efficient and tailored
preference dataset construction, named Icon$^{2}$. Specifically, it first
extracts layer-wise direction vectors to encode sophisticated human preferences
and then uses these vectors to filter self-synthesized instructions based on
their inherent consistency. During decoding, bidirectional inherent control is
applied to steer token representations, enabling the precise generation of
response pairs with clear alignment distinctions. Experimental results
demonstrate significant improvements in both alignment and efficiency.
Llama3-8B and Qwen2-7B achieve an average win rate improvement of 13.89% on
AlpacaEval 2.0 and 13.45% on Arena-Hard, while reducing computational costs by
up to 48.1%.

</details>


### [69] [Beyond Keywords: Driving Generative Search Engine Optimization with Content-Centric Agents](https://arxiv.org/abs/2509.05607)
*Qiyuan Chen,Jiahe Chen,Hongsen Huang,Qian Shao,Jintai Chen,Renjie Hua,Hongxia Xu,Ruijia Wu,Ren Chuan,Jian Wu*

Main category: cs.CL

TL;DR: This paper presents a framework for Generative Search Engine Optimization (GSEO) with a new benchmark (CC-GSEO-Bench) and a multi-agent system to measure and improve how content influences generative search engines.


<details>
  <summary>Details</summary>
Motivation: Traditional SEO metrics are inadequate for generative search engines, which prioritize synthesized answers. The paper aims to redefine content optimization in this new search paradigm.

Method: The authors introduce CC-GSEO-Bench, a benchmark with a multi-dimensional evaluation system, and utilize a multi-agent system to automate and refine content optimization through an analyze-revise-evaluate workflow.

Result: The framework offers insights into how content influences generative search output and provides actionable strategies for creators.

Conclusion: The study establishes foundational tools and methodologies for advancing research in GSEO, enabling better content optimization for generative search engines.

Abstract: The paradigm shift from traditional ranked-based search to Generative Search
Engines has rendered conventional SEO metrics obsolete, creating an urgent need
to understand, measure, and optimize for content influence on synthesized
answers. This paper introduces a comprehensive, end-to-end framework for
Generative Search Engine Optimization (GSEO) to address this challenge. We make
two primary contributions. First, we construct CC-GSEO-Bench, a large-scale,
content-centric benchmark, and propose a multi-dimensional evaluation framework
that systematically quantifies influence, moving beyond surface-level
attribution to assess substantive semantic impact. Second, we design a novel
multi-agent system that operationalizes this framework, automating the
strategic refinement of content through a collaborative analyze-revise-evaluate
workflow. Our empirical analysis using this framework reveals novel insights
into the dynamics of content influence, offering actionable strategies for
creators and establishing a principled foundation for future GSEO research.

</details>


### [70] [New Insights into Optimal Alignment of Acoustic and Linguistic Representations for Knowledge Transfer in ASR](https://arxiv.org/abs/2509.05609)
*Xugang Lu,Peng Shen,Yu Tsao,Hisashi Kawai*

Main category: cs.CL

TL;DR: Addressing the challenge of aligning acoustic and linguistic representations in knowledge transfer for automatic speech recognition (ASR) through unbalanced optimal transport-based alignment.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve ASR by tackling the structured and asymmetric alignment issue between acoustic and linguistic representations, which is central to knowledge transfer.

Method: Introduces an unbalanced optimal transport-based alignment model to handle mismatches, asymmetries, and redundancy in acoustic and linguistic mappings flexibly.

Result: The proposed model, tested on a CTC-based ASR system, effectively enhances the matching flexibility and ASR performance.

Conclusion: Unbalanced optimal transport-based alignment is a promising solution for structured mismatches in acoustic-linguistic representation, leading to improved ASR outcomes.

Abstract: Aligning acoustic and linguistic representations is a central challenge to
bridge the pre-trained models in knowledge transfer for automatic speech
recognition (ASR). This alignment is inherently structured and asymmetric:
while multiple consecutive acoustic frames typically correspond to a single
linguistic token (many-to-one), certain acoustic transition regions may relate
to multiple adjacent tokens (one-to-many). Moreover, acoustic sequences often
include frames with no linguistic counterpart, such as background noise or
silence may lead to imbalanced matching conditions. In this work, we take a new
insight to regard alignment and matching as a detection problem, where the goal
is to identify meaningful correspondences with high precision and recall
ensuring full coverage of linguistic tokens while flexibly handling redundant
or noisy acoustic frames in transferring linguistic knowledge for ASR. Based on
this new insight, we propose an unbalanced optimal transport-based alignment
model that explicitly handles distributional mismatch and structural
asymmetries with soft and partial matching between acoustic and linguistic
modalities. Our method ensures that every linguistic token is grounded in at
least one acoustic observation, while allowing for flexible, probabilistic
mappings from acoustic to linguistic units. We evaluate our proposed model with
experiments on an CTC-based ASR system with a pre-trained language model for
knowledge transfer. Experimental results demonstrate the effectiveness of our
approach in flexibly controlling degree of matching and hence to improve ASR
performance.

</details>


### [71] [From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics](https://arxiv.org/abs/2509.05617)
*Shay Dahary,Avi Edana,Alexander Apartsin,Yehudit Aperstein*

Main category: cs.CL

TL;DR: This paper focuses on predicting emotional intensity in song lyrics using both zero-shot Large Language Models (LLMs) and fine-tuned methods, resulting in insights on their performance.


<details>
  <summary>Details</summary>
Motivation: Understanding emotional content in song lyrics can enhance listener experiences and improve music information retrieval systems.

Method: The authors created a manually labeled dataset using a Mean Opinion Score (MOS) approach and evaluated LLMs in zero-shot scenarios, alongside fine-tuning a BERT-based model for multi-label emotion prediction.

Result: Zero-shot LLMs demonstrated notable strengths but were outperformed by the fine-tuned BERT model in capturing nuanced emotional content.

Conclusion: The study emphasizes the potential of LLMs for emotion recognition and provides a key dataset for advancing music information retrieval applications.

Abstract: The emotional content of song lyrics plays a pivotal role in shaping listener
experiences and influencing musical preferences. This paper investigates the
task of multi-label emotional attribution of song lyrics by predicting six
emotional intensity scores corresponding to six fundamental emotions. A
manually labeled dataset is constructed using a mean opinion score (MOS)
approach, which aggregates annotations from multiple human raters to ensure
reliable ground-truth labels. Leveraging this dataset, we conduct a
comprehensive evaluation of several publicly available large language models
(LLMs) under zero-shot scenarios. Additionally, we fine-tune a BERT-based model
specifically for predicting multi-label emotion scores. Experimental results
reveal the relative strengths and limitations of zero-shot and fine-tuned
models in capturing the nuanced emotional content of lyrics. Our findings
highlight the potential of LLMs for emotion recognition in creative texts,
providing insights into model selection strategies for emotion-based music
information retrieval applications. The labeled dataset is available at
https://github.com/LLM-HITCS25S/LyricsEmotionAttribution.

</details>


### [72] [Few-Shot Query Intent Detection via Relation-Aware Prompt Learning](https://arxiv.org/abs/2509.05635)
*Liang Zhang,Yuan Li,Shijie Zhang,Zheng Zhang,Xitong Li*

Main category: cs.CL

TL;DR: The paper addresses intent detection in conversational systems using a novel framework called SAID, which incorporates both textual and relational structure information.


<details>
  <summary>Details</summary>
Motivation: Existing few-shot intent detection methods focus primarily on textual data, overlooking the structural relationships in conversational systems, such as query-query and query-answer relations.

Method: The authors propose SAID, a pretraining framework that integrates textual and relational information. They introduce a novel query-adaptive attention network (QueryAdapt) to explicitly model these relations for better knowledge transfer.

Result: SAID demonstrated superior performance compared to state-of-the-art intent detection methods in experiments on two real-world datasets.

Conclusion: Integrating textual and relational structure information enhances few-shot intent detection, and SAID provides a promising approach for advancing conversational systems through this integration.

Abstract: Intent detection is a crucial component of modern conversational systems,
since accurately identifying user intent at the beginning of a conversation is
essential for generating effective responses. Recent efforts have focused on
studying this problem under a challenging few-shot scenario. These approaches
primarily leverage large-scale unlabeled dialogue text corpora to pretrain
language models through various pretext tasks, followed by fine-tuning for
intent detection with very limited annotations. Despite the improvements
achieved, existing methods have predominantly focused on textual data,
neglecting to effectively capture the crucial structural information inherent
in conversational systems, such as the query-query relation and query-answer
relation. To address this gap, we propose SAID, a novel framework that
integrates both textual and relational structure information in a unified
manner for model pretraining for the first time. Building on this framework, we
further propose a novel mechanism, the query-adaptive attention network
(QueryAdapt), which operates at the relation token level by generating
intent-specific relation tokens from well-learned query-query and query-answer
relations explicitly, enabling more fine-grained knowledge transfer. Extensive
experimental results on two real-world datasets demonstrate that SAID
significantly outperforms state-of-the-art methods.

</details>


### [73] [LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding](https://arxiv.org/abs/2509.05657)
*Yuxuan Hu,Jihao Liu,Ke Wang,Jinliang Zhen,Weikang Shi,Manyuan Zhang,Qi Dou,Rui Liu,Aojun Zhou,Hongsheng Li*

Main category: cs.CL

TL;DR: This paper introduces LM-Searcher, a flexible and cross-domain LLM-driven Neural Architecture Search (NAS) framework using a universal encoding method and reformulated ranking task.


<details>
  <summary>Details</summary>
Motivation: Current LLM-driven NAS methods rely heavily on domain-specific tuning, limiting their applicability and scalability across diverse optimization tasks.

Method: Developed LM-Searcher, featuring NCode (a universal architecture representation), a reformulated NAS as a ranking task, and a curated dataset for training LLMs via instruction-tuning and pruning-based sampling strategies.

Result: LM-Searcher demonstrated strong performance in both in-domain (e.g., CNNs for classification) and out-of-domain (e.g., LoRA for segmentation) tasks, outperforming or competing with state-of-the-art methods.

Conclusion: LM-Searcher provides a scalable, generalizable solution for NAS, enabling efficient cross-domain optimization without extensive domain-specific adaptations.

Abstract: Recent progress in Large Language Models (LLMs) has opened new avenues for
solving complex optimization problems, including Neural Architecture Search
(NAS). However, existing LLM-driven NAS approaches rely heavily on prompt
engineering and domain-specific tuning, limiting their practicality and
scalability across diverse tasks. In this work, we propose LM-Searcher, a novel
framework that leverages LLMs for cross-domain neural architecture optimization
without the need for extensive domain-specific adaptation. Central to our
approach is NCode, a universal numerical string representation for neural
architectures, which enables cross-domain architecture encoding and search. We
also reformulate the NAS problem as a ranking task, training LLMs to select
high-performing architectures from candidate pools using instruction-tuning
samples derived from a novel pruning-based subspace sampling strategy. Our
curated dataset, encompassing a wide range of architecture-performance pairs,
encourages robust and transferable learning. Comprehensive experiments
demonstrate that LM-Searcher achieves competitive performance in both in-domain
(e.g., CNNs for image classification) and out-of-domain (e.g., LoRA
configurations for segmentation and generation) tasks, establishing a new
paradigm for flexible and generalizable LLM-based architecture search. The
datasets and models will be released at https://github.com/Ashone3/LM-Searcher.

</details>


### [74] [Cross-Question Method Reuse in Large Language Models: From Word-Level Prediction to Rational Logical-Layer Reasoning](https://arxiv.org/abs/2509.05660)
*Hong Su*

Main category: cs.CL

TL;DR: The paper introduces a method to enable large language models (LLMs) to reuse solutions for questions with low or hidden similarities, overcoming traditional similarity constraints.


<details>
  <summary>Details</summary>
Motivation: To enable LLMs to reuse methods for diverse questions, especially when questions exhibit low or hidden similarities, going beyond conventional constraints.

Method: The method involves separating question and solution pairs, guiding the LLM to adapt solutions to related questions, even for those with partial or hidden similarities.

Result: The approach improves the likelihood of filtering reusable solutions, enhancing the practicality of cross-question solution reuse.

Conclusion: This method extends the applicability of LLMs in method reuse by addressing limitations of similarity-based constraints, improving their effectiveness in diverse scenarios.

Abstract: Large language models (LLMs) have been widely applied to assist in finding
solutions for diverse questions. Prior work has proposed representing a method
as a pair of a question and its corresponding solution, enabling method reuse.
However, existing approaches typically require the questions to be highly
similar. In this paper, we extend the scope of method reuse to address
questions with low similarity or with hidden similarities that are not
explicitly observable. For questions that are similar in a general-specific
sense (i.e., broader or narrower in scope), we propose to first separate the
question and solution, rather than directly feeding the pair to the LLM. The
LLM is then guided to adapt the solution to new but related questions, allowing
it to focus on solution transfer rather than question recognition. Furthermore,
we extend this approach to cases where questions only share partial features or
hidden characteristics. This enables cross-question method reuse beyond
conventional similarity constraints. Experimental verification shows that our
scope-extension approach increases the probability of filtering out reusable
solutions, thereby improving the effectiveness of cross-question method reuse.

</details>


### [75] [Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian](https://arxiv.org/abs/2509.05668)
*Michael Hoffmann,Jophin John,Stefan Schweter,Gokul Ramakrishnan,Hoi-Fong Mak,Alice Zhang,Dmitry Gaynullin,Nicolay J. Hammer*

Main category: cs.CL

TL;DR: The paper introduces Llama-GENBA-10B, a 10-billion parameter trilingual foundation model focusing on English, German, and low-resource Bavarian languages.


<details>
  <summary>Details</summary>
Motivation: To address the English-centric bias in large language models and promote Bavarian as a low-resource language within multilingual NLP.

Method: The method involves curated multilingual training data, a unified tokenizer, optimized model architecture, cross-lingual hyperparameters, and the creation of a trilingual evaluation suite.

Result: Llama-GENBA-10B achieves competitive cross-lingual performance, outperforming state-of-the-art models like Apertus-8B-2509 and gemma-2-9b in Bavarian and equaling EuroLLM in German. It is also energy-efficient.

Conclusion: The model serves as a scalable blueprint for developing inclusive multilingual foundation models, particularly for low-resource languages.

Abstract: We present Llama-GENBA-10B, a trilingual foundation model addressing
English-centric bias in large language models. Built on Llama 3.1-8B and scaled
to 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens
(82B English, 82B German, and 80M Bavarian), balancing resources while
preventing English dominance. Targeted at the German NLP community, the model
also promotes Bavarian as a low-resource language. Development tackled four
challenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2)
creating a unified tokenizer for English, German, and Bavarian, (3) optimizing
architecture and language-ratio hyperparameters for cross-lingual transfer, and
(4) establishing the first standardized trilingual evaluation suite by
translating German benchmarks into Bavarian. Evaluations show that
Llama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned
variant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing
itself as the best model in its class for this language, while also
outperforming EuroLLM in English and matching its results in German. Training
on the Cerebras CS-2 demonstrated efficient large-scale multilingual
pretraining with documented energy use, offering a blueprint for inclusive
foundation models that integrate low-resource languages.

</details>


### [76] [Revealing the Numeracy Gap: An Empirical Investigation of Text Embedding Models](https://arxiv.org/abs/2509.05691)
*Ningyuan Deng,Hanyu Duan,Yixuan Tang,Yi Yang*

Main category: cs.CL

TL;DR: The study evaluates the ability of text embedding models to handle numerical information accurately and finds limitations in their performance.


<details>
  <summary>Details</summary>
Motivation: With increasing applications of embedding models in fields like finance and healthcare, numerical understanding is critical, yet rarely evaluated systematically.

Method: The authors test 13 popular text embedding models on synthetic financial data to analyze their performance in capturing numerical nuances.

Result: The models generally fail to accurately represent numerical information in text.

Conclusion: Text embedding models need improvement to handle numerical content effectively, and the study provides insights to guide future research in this direction.

Abstract: Text embedding models are widely used in natural language processing
applications. However, their capability is often benchmarked on tasks that do
not require understanding nuanced numerical information in text. As a result,
it remains unclear whether current embedding models can precisely encode
numerical content, such as numbers, into embeddings. This question is critical
because embedding models are increasingly applied in domains where numbers
matter, such as finance and healthcare. For example, Company X's market share
grew by 2\% should be interpreted very differently from Company X's market
share grew by 20\%, even though both indicate growth in market share. This
study aims to examine whether text embedding models can capture such nuances.
Using synthetic data in a financial context, we evaluate 13 widely used text
embedding models and find that they generally struggle to capture numerical
details accurately. Our further analyses provide deeper insights into embedding
numeracy, informing future research to strengthen embedding model-based NLP
systems with improved capacity for handling numerical content.

</details>


### [77] [A Survey of the State-of-the-Art in Conversational Question Answering Systems](https://arxiv.org/abs/2509.05716)
*Manoj Madushanka Perera,Adnan Mahmood,Kasun Eranda Wijethilake,Fahmida Islam,Maryam Tahermazandarani,Quan Z. Sheng*

Main category: cs.CL

TL;DR: This survey paper reviews the state-of-the-art in Conversational Question Answering (ConvQA), covering system components, machine learning techniques, language models, and datasets, and detailing future research areas.


<details>
  <summary>Details</summary>
Motivation: ConvQA systems are increasingly important for enabling coherent, context-aware conversations across domains like customer support, education, and healthcare.

Method: The paper examines core ConvQA system components, advanced machine learning techniques (e.g., reinforcement learning, transfer learning), large language models, and key datasets, offering analysis and insights.

Result: It highlights the interplay of core ConvQA components, impact of large language models on scalability and architecture, and suggests advancements in addressing technical challenges.

Conclusion: The survey provides a foundational understanding of ConvQA, identifies gaps, and outlines open research directions for improving accuracy and efficiency in multi-turn conversations.

Abstract: Conversational Question Answering (ConvQA) systems have emerged as a pivotal
area within Natural Language Processing (NLP) by driving advancements that
enable machines to engage in dynamic and context-aware conversations. These
capabilities are increasingly being applied across various domains, i.e.,
customer support, education, legal, and healthcare where maintaining a coherent
and relevant conversation is essential. Building on recent advancements, this
survey provides a comprehensive analysis of the state-of-the-art in ConvQA.
This survey begins by examining the core components of ConvQA systems, i.e.,
history selection, question understanding, and answer prediction, highlighting
their interplay in ensuring coherence and relevance in multi-turn
conversations. It further investigates the use of advanced machine learning
techniques, including but not limited to, reinforcement learning, contrastive
learning, and transfer learning to improve ConvQA accuracy and efficiency. The
pivotal role of large language models, i.e., RoBERTa, GPT-4, Gemini 2.0 Flash,
Mistral 7B, and LLaMA 3, is also explored, thereby showcasing their impact
through data scalability and architectural advancements. Additionally, this
survey presents a comprehensive analysis of key ConvQA datasets and concludes
by outlining open research directions. Overall, this work offers a
comprehensive overview of the ConvQA landscape and provides valuable insights
to guide future advancements in the field.

</details>


### [78] [Exploring Subjective Tasks in Farsi: A Survey Analysis and Evaluation of Language Models](https://arxiv.org/abs/2509.05719)
*Donya Rooein,Flor Miriam Plaza-del-Arco,Debora Nozza,Dirk Hovy*

Main category: cs.CL

TL;DR: The study explores challenges in Farsi NLP, particularly for sentiment, emotion, and toxicity tasks, uncovering issues like limited datasets and unstable predictive model results.


<details>
  <summary>Details</summary>
Motivation: Investigate and highlight the challenges specific to Farsi, a middle-resource language, in advancing NLP capabilities for subjective tasks such as sentiment and emotion analysis.

Method: Reviewing 110 publications, examining dataset availability and demographic inclusivity, and evaluating model performance for subjective NLP tasks in Farsi.

Result: Findings highlight insufficient data volume, poor dataset quality, lack of demographic factors in datasets, and instability in model predictions.

Conclusion: Improving Farsi's NLP performance requires more comprehensive, diverse, and high-quality datasets along with better representation of demographic factors.

Abstract: Given Farsi's speaker base of over 127 million people and the growing
availability of digital text, including more than 1.3 million articles on
Wikipedia, it is considered a middle-resource language. However, this label
quickly crumbles when the situation is examined more closely. We focus on three
subjective tasks (Sentiment Analysis, Emotion Analysis, and Toxicity Detection)
and find significant challenges in data availability and quality, despite the
overall increase in data availability. We review 110 publications on subjective
tasks in Farsi and observe a lack of publicly available datasets. Furthermore,
existing datasets often lack essential demographic factors, such as age and
gender, that are crucial for accurately modeling subjectivity in language. When
evaluating prediction models using the few available datasets, the results are
highly unstable across both datasets and models. Our findings indicate that the
volume of data is insufficient to significantly improve a language's prospects
in NLP.

</details>


### [79] [QCSE: A Pretrained Quantum Context-Sensitive Word Embedding for Natural Language Processing](https://arxiv.org/abs/2509.05729)
*Charles M. Varmantchaonala,Niclas GÖtting,Nils-Erik SchÜtte,Jean Louis E. K. Fendji,Christopher Gies*

Main category: cs.CL

TL;DR: This paper introduces QCSE, a quantum context-sensitive embedding model leveraging quantum computation for handling complex linguistic context in NLP.


<details>
  <summary>Details</summary>
Motivation: To address the complexity of natural language understanding and mitigate data scarcity issues for low-resource languages, by leveraging quantum computation's expressibility.

Method: QCSE employs quantum-native context learning with five innovative context matrix computation methods, including exponential decay and sinusoidal modulation.

Result: The model demonstrates context sensitivity in both Fulani (low-resource) and English corpora, showing quantum systems' potential in linguistic tasks.

Conclusion: Quantum computation has promising applications in improving NLP tasks, especially in low-resource language contexts, opening pathways for further exploration in QNLP.

Abstract: Quantum Natural Language Processing (QNLP) offers a novel approach to
encoding and understanding the complexity of natural languages through the
power of quantum computation. This paper presents a pretrained quantum
context-sensitive embedding model, called QCSE, that captures context-sensitive
word embeddings, leveraging the unique properties of quantum systems to learn
contextual relationships in languages. The model introduces quantum-native
context learning, enabling the utilization of quantum computers for linguistic
tasks. Central to the proposed approach are innovative context matrix
computation methods, designed to create unique, representations of words based
on their surrounding linguistic context. Five distinct methods are proposed and
tested for computing the context matrices, incorporating techniques such as
exponential decay, sinusoidal modulation, phase shifts, and hash-based
transformations. These methods ensure that the quantum embeddings retain
context sensitivity, thereby making them suitable for downstream language tasks
where the expressibility and properties of quantum systems are valuable
resources. To evaluate the effectiveness of the model and the associated
context matrix methods, evaluations are conducted on both a Fulani corpus, a
low-resource African language, dataset of small size and an English corpus of
slightly larger size. The results demonstrate that QCSE not only captures
context sensitivity but also leverages the expressibility of quantum systems
for representing rich, context-aware language information. The use of Fulani
further highlights the potential of QNLP to mitigate the problem of lack of
data for this category of languages. This work underscores the power of quantum
computation in natural language processing (NLP) and opens new avenues for
applying QNLP to real-world linguistic challenges across various tasks and
domains.

</details>


### [80] [Enhancing Factual Accuracy and Citation Generation in LLMs via Multi-Stage Self-Verification](https://arxiv.org/abs/2509.05741)
*Fernando Gabriela García,Qiyang Shi,Zilin Feng*

Main category: cs.CL

TL;DR: The paper introduces VeriFact-CoT, a method aimed at improving accuracy and reliability in large language models (LLMs) by addressing hallucination and citation issues.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the challenges of hallucinations and the lack of credible citations in LLMs during complex, fact-sensitive content generation.

Method: The method involves a multi-stage approach of 'fact verification-reflection-citation integration,' enabling LLMs to self-examine and refine their reasoning and answers.

Result: VeriFact-CoT significantly improves the accuracy, trustworthiness, and traceability of outputs produced by LLMs.

Conclusion: By implementing VeriFact-CoT, LLMs become more dependable for high-fidelity applications such as scientific research, news reporting, and legal advisory tasks.

Abstract: This research introduces VeriFact-CoT (Verified Factual Chain-of-Thought), a
novel method designed to address the pervasive issues of hallucination and the
absence of credible citation sources in Large Language Models (LLMs) when
generating complex, fact-sensitive content. By incorporating a multi-stage
mechanism of 'fact verification-reflection-citation integration,' VeriFact-CoT
empowers LLMs to critically self-examine and revise their intermediate
reasoning steps and final answers. This process significantly enhances the
objective accuracy, trustworthiness, and traceability of the generated outputs,
making LLMs more reliable for applications demanding high fidelity such as
scientific research, news reporting, and legal consultation.

</details>


### [81] [LatinX: Aligning a Multilingual TTS Model with Direct Preference Optimization](https://arxiv.org/abs/2509.05863)
*Luis Felipe Chary,Miguel Arjona Ramirez*

Main category: cs.CL

TL;DR: LatinX is a multilingual text-to-speech model designed for preserving speaker identity in speech-to-speech translation across languages, showing improved performance through Direct Preference Optimization.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the challenge of maintaining speaker identity while performing multilingual speech-to-speech translation.

Method: The model employs a 12-layer decoder-only Transformer trained in three steps: text-to-audio pre-training, supervised fine-tuning for voice cloning, and alignment using Direct Preference Optimization considering Word Error Rate and speaker similarity.

Result: LatinX significantly reduces Word Error Rate and enhances speaker similarity metrics compared to a fine-tuned baseline, as confirmed by both objective measures and human evaluations.

Conclusion: LatinX demonstrates strong speaker similarity in multilingual TTS tasks, bridging gaps between objective and subjective evaluations, and suggests paths for improving latency and balance in evaluation signals.

Abstract: We present LatinX, a multilingual text-to-speech (TTS) model for cascaded
speech-to-speech translation that preserves the source speaker's identity
across languages. LatinX is a 12-layer decoder-only Transformer trained in
three stages: (i) pre-training for text-to-audio mapping, (ii) supervised
fine-tuning for zero-shot voice cloning, and (iii) alignment with Direct
Preference Optimization (DPO) using automatically labeled pairs based on Word
Error Rate (WER) and speaker-similarity metrics. Trained on English and Romance
languages with emphasis on Portuguese, LatinX with DPO consistently reduces WER
and improves objective similarity over the fine-tuned baseline. Human
evaluations further indicate stronger perceived speaker similarity than a
strong baseline (XTTSv2), revealing gaps between objective and subjective
measures. We provide cross-lingual analyses and discuss balanced preference
signals and lower-latency architectures as future work.

</details>


### [82] [ZhiFangDanTai: Fine-tuning Graph-based Retrieval-Augmented Generation Model for Traditional Chinese Medicine Formula](https://arxiv.org/abs/2509.05867)
*ZiXuan Zhang,Bowen Hao,Yingjie Li,Hongzhi Yin*

Main category: cs.CL

TL;DR: A new framework, ZhiFangDanTai, combines Graph-based Retrieval-Augmented Generation (GraphRAG) and Large Language Model (LLM) fine-tuning to enhance the generation of explainable Traditional Chinese Medicine formulas, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations in existing TCM models that lack comprehensive outputs and detailed explanations, particularly in roles like sovereign, minister, assistant, and courier, as well as their efficacy and diagnostic guidance.

Method: The researchers propose ZhiFangDanTai, a framework that integrates GraphRAG for structured knowledge retrieval and LLM fine-tuning. An enhanced instruction dataset is also created, alongside theoretical proofs demonstrating reductions in error rates.

Result: ZhiFangDanTai outperforms state-of-the-art techniques in explainable TCM formula generation, as derived from experimental results on both collected and clinical datasets.

Conclusion: ZhiFangDanTai offers a significant advancement in the analysis and explanation of TCM formulas through its novel integration of GraphRAG and fine-tuning methods, and the model is publicly available for research use.

Abstract: Traditional Chinese Medicine (TCM) formulas play a significant role in
treating epidemics and complex diseases. Existing models for TCM utilize
traditional algorithms or deep learning techniques to analyze formula
relationships, yet lack comprehensive results, such as complete formula
compositions and detailed explanations. Although recent efforts have used TCM
instruction datasets to fine-tune Large Language Models (LLMs) for explainable
formula generation, existing datasets lack sufficient details, such as the
roles of the formula's sovereign, minister, assistant, courier; efficacy;
contraindications; tongue and pulse diagnosis-limiting the depth of model
outputs. To address these challenges, we propose ZhiFangDanTai, a framework
combining Graph-based Retrieval-Augmented Generation (GraphRAG) with LLM
fine-tuning. ZhiFangDanTai uses GraphRAG to retrieve and synthesize structured
TCM knowledge into concise summaries, while also constructing an enhanced
instruction dataset to improve LLMs' ability to integrate retrieved
information. Furthermore, we provide novel theoretical proofs demonstrating
that integrating GraphRAG with fine-tuning techniques can reduce generalization
error and hallucination rates in the TCM formula task. Experimental results on
both collected and clinical datasets demonstrate that ZhiFangDanTai achieves
significant improvements over state-of-the-art models. Our model is
open-sourced at https://huggingface.co/tczzx6/ZhiFangDanTai1.0.

</details>


### [83] [MedFactEval and MedAgentBrief: A Framework and Workflow for Generating and Evaluating Factual Clinical Summaries](https://arxiv.org/abs/2509.05878)
*François Grolleau,Emily Alsentzer,Timothy Keyes,Philip Chung,Akshay Swaminathan,Asad Aali,Jason Hom,Tridu Huynh,Thomas Lew,April S. Liang,Weihan Chu,Natasha Z. Steele,Christina F. Lin,Jingkun Yang,Kameron C. Black,Stephen P. Ma,Fateme N. Haredasht,Nigam H. Shah,Kevin Schulman,Jonathan H. Chen*

Main category: cs.CL

TL;DR: The paper introduces MedFactEval, a clinician-defined evaluation framework, and MedAgentBrief, a multi-step workflow for generating discharge summaries, to enhance the factual accuracy of LLM-generated clinical texts.


<details>
  <summary>Details</summary>
Motivation: Ensuring factual accuracy in LLM-generated clinical texts is crucial, as expert manual review is impractical for continuous verification.

Method: Two main contributions: MedFactEval, using clinician-defined key facts and LLM Jury voting, and MedAgentBrief, a workflow for factual text generation.

Result: MedFactEval aligns closely with clinician majority votes (81% agreement) and performs comparably to human experts in evaluating key facts.

Conclusion: The paper presents scalable tools for evaluating and generating reliable clinical text, promoting responsible use of generative AI in healthcare.

Abstract: Evaluating factual accuracy in Large Language Model (LLM)-generated clinical
text is a critical barrier to adoption, as expert review is unscalable for the
continuous quality assurance these systems require. We address this challenge
with two complementary contributions. First, we introduce MedFactEval, a
framework for scalable, fact-grounded evaluation where clinicians define
high-salience key facts and an "LLM Jury"--a multi-LLM majority vote--assesses
their inclusion in generated summaries. Second, we present MedAgentBrief, a
model-agnostic, multi-step workflow designed to generate high-quality, factual
discharge summaries. To validate our evaluation framework, we established a
gold-standard reference using a seven-physician majority vote on
clinician-defined key facts from inpatient cases. The MedFactEval LLM Jury
achieved almost perfect agreement with this panel (Cohen's kappa=81%), a
performance statistically non-inferior to that of a single human expert
(kappa=67%, P < 0.001). Our work provides both a robust evaluation framework
(MedFactEval) and a high-performing generation workflow (MedAgentBrief),
offering a comprehensive approach to advance the responsible deployment of
generative AI in clinical workflows.

</details>


### [84] [Let's Roleplay: Examining LLM Alignment in Collaborative Dialogues](https://arxiv.org/abs/2509.05882)
*Abhijnan Nath,Carine Graff,Nikhil Krishnaswamy*

Main category: cs.CL

TL;DR: The paper explores how alignment techniques influence LLM agents in multiturn, multiparty collaborations and introduces a friction-aware approach demonstrating improved task collaboration outcomes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to ensure Large Language Models (LLMs) behave predictably and reliably in complex multi-interaction settings, moving beyond simplified single-user scenarios.

Method: The authors employ a roleplay-based methodology to evaluate the performance of differently-trained LLM friction agents during collaborative dialogues, introducing a counterfactual evaluation framework to measure intervention impacts.

Result: Friction-aware alignment methods lead to better convergence on shared task goals and improved decision correctness in multiparty collaborations compared to standard alignment baselines.

Conclusion: The findings highlight the effectiveness of friction-aware strategies in improving collaborative group dynamics and decision-making outcomes, suggesting a promising direction for training future LLMs.

Abstract: As Large Language Models (LLMs) integrate into diverse workflows, they are
increasingly being considered "collaborators" with humans. If such AI
collaborators are to be reliable, their behavior over multiturn interactions
must be predictable, validated and verified before deployment. Common alignment
techniques are typically developed under simplified single-user settings and do
not account for the dynamics of long-horizon multiparty interactions. This
paper examines how different alignment methods affect LLM agents' effectiveness
as partners in multiturn, multiparty collaborations. We study this question
through the lens of friction agents that intervene in group dialogues to
encourage the collaborative group to slow down and reflect upon their reasoning
for deliberative decision-making. Using a roleplay methodology, we evaluate
interventions from differently-trained friction agents in collaborative task
conversations. We propose a novel counterfactual evaluation framework that
quantifies how friction interventions change the trajectory of group
collaboration and belief alignment. Our results show that a friction-aware
approach significantly outperforms common alignment baselines in helping both
convergence to a common ground, or agreed-upon task-relevant propositions, and
correctness of task outcomes.

</details>


### [85] [Enhancing the Robustness of Contextual ASR to Varying Biasing Information Volumes Through Purified Semantic Correlation Joint Modeling](https://arxiv.org/abs/2509.05908)
*Yue Gu,Zhihao Du,Ying Shi,Shiliang Zhang,Qian Chen,Jiqing Han*

Main category: cs.CL

TL;DR: The authors address the problem of contextual ASR performance degradation with long biasing lists by introducing PSC-Joint, a method that models relevant semantic correlations at different granularities and filters irrelevant information. This improves F1 scores significantly.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the issue of rapidly declining effectiveness of cross-attention methods in ASR when dealing with long biasing lists, by identifying that only a small subset of biasing information is relevant at any time.

Method: The PSC-Joint approach defines semantic correlations at list-level, phrase-level, and token-level, then jointly models these to identify and focus on the most relevant biasing information. A grouped-and-competitive purification mechanism further reduces computational overhead by filtering out irrelevant phrases.

Result: The proposed method achieves notable improvements in F1 scores: up to a 21.34% average relative improvement on AISHELL-1 and 28.46% on KeSpeech, outperforming baseline models across various biasing list lengths.

Conclusion: Identifying and integrating only the most relevant biasing information improves contextual ASR performance, particularly under varying biasing list lengths. The PSC-Joint method demonstrates both computational efficiency and robustness.

Abstract: Recently, cross-attention-based contextual automatic speech recognition (ASR)
models have made notable advancements in recognizing personalized biasing
phrases. However, the effectiveness of cross-attention is affected by
variations in biasing information volume, especially when the length of the
biasing list increases significantly. We find that, regardless of the length of
the biasing list, only a limited amount of biasing information is most relevant
to a specific ASR intermediate representation. Therefore, by identifying and
integrating the most relevant biasing information rather than the entire
biasing list, we can alleviate the effects of variations in biasing information
volume for contextual ASR. To this end, we propose a purified semantic
correlation joint modeling (PSC-Joint) approach. In PSC-Joint, we define and
calculate three semantic correlations between the ASR intermediate
representations and biasing information from coarse to fine: list-level,
phrase-level, and token-level. Then, the three correlations are jointly modeled
to produce their intersection, so that the most relevant biasing information
across various granularities is highlighted and integrated for contextual
recognition. In addition, to reduce the computational cost introduced by the
joint modeling of three semantic correlations, we also propose a purification
mechanism based on a grouped-and-competitive strategy to filter out irrelevant
biasing phrases. Compared with baselines, our PSC-Joint approach achieves
average relative F1 score improvements of up to 21.34% on AISHELL-1 and 28.46%
on KeSpeech, across biasing lists of varying lengths.

</details>


### [86] [Accelerating Large Language Model Inference via Early-Exiting Algorithms](https://arxiv.org/abs/2509.05915)
*Sangmin Bae*

Main category: cs.CL

TL;DR: The paper tackles the computational challenges of deploying large language models by co-designing adaptive computation methods and architectures for better efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Large language models face significant computational costs during deployment, and adaptive methods like early-exiting often inadvertently reduce throughput in batched inference.

Method: The study proposes efficient parallel decoding mechanisms, employs deep parameter sharing to mitigate synchronization issues, and introduces lightweight routers for dynamic token assignment.

Result: The approach creates a new Pareto frontier, achieving improvements in adaptive computation and parameter efficiency simultaneously.

Conclusion: The co-designed algorithms and architectures optimize the balance between computational dynamism and system-level efficiency for large language models.

Abstract: Large language models have achieved remarkable capabilities, but their
practical deployment is hindered by significant computational costs. While
adaptive computation methods like early-exiting promise to reduce these costs,
they introduce a fundamental conflict: the per-token dynamism intended to save
computation often creates system-level bottlenecks that can paradoxically
reduce throughput in batched inference. This dissertation resolves this
conflict by co-designing adaptive algorithms and model architectures to strike
an optimal balance between dynamism and efficiency. To this end, our work first
addresses critical sources of overhead in conventional early-exiting by
proposing an efficient parallel decoding mechanism. We then show that deep
parameter sharing provides an architectural foundation that not only yields
compact, parameter-efficient models but also inherently mitigates the critical
synchronization issues affecting dynamic inference. Finally, this work presents
a unified framework where lightweight routers are pretrained to dynamically
assign an optimal recursion depth for each token. This approach establishes a
new Pareto frontier between efficiency and performance by effectively
optimizing for both adaptive computation and parameter efficiency within a
single model.

</details>


### [87] [KatotohananQA: Evaluating Truthfulness of Large Language Models in Filipino](https://arxiv.org/abs/2509.06065)
*Lorenzo Alfred Nery,Ronald Dawson Catignas,Thomas James Tiam-Lee*

Main category: cs.CL

TL;DR: A Filipino version of the TruthfulQA benchmark, called KatotohananQA, is introduced to evaluate large language model truthfulness in low-resource languages, revealing performance gaps and multilingual transfer issues.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the lack of benchmarks for evaluating the truthfulness of large language models (LLMs) in low-resource languages, as existing benchmarks are primarily in English.

Method: The authors translated the TruthfulQA benchmark into Filipino, named KatotohananQA, and assessed seven free-tier proprietary models using a binary-choice evaluation framework.

Result: Performance gaps were identified between English and Filipino truthfulness, with newer OpenAI models such as GPT-5 and GPT-5 mini showing stronger multilingual robustness. Disparities in question characteristics were also noted.

Conclusion: The study highlights the need for broader multilingual evaluations to ensure fairness and reliability of LLMs, as some question types and topics are less robust in multilingual contexts.

Abstract: Large Language Models (LLMs) achieve remarkable performance across various
tasks, but their tendency to produce hallucinations limits reliable adoption.
Benchmarks such as TruthfulQA have been developed to measure truthfulness, yet
they are primarily available in English, leaving a gap in evaluating LLMs in
low-resource languages. To address this, we present KatotohananQA, a Filipino
translation of the TruthfulQA benchmark. Seven free-tier proprietary models
were assessed using a binary-choice framework. Findings show a significant
performance gap between English and Filipino truthfulness, with newer OpenAI
models (GPT-5 and GPT-5 mini) demonstrating strong multilingual robustness.
Results also reveal disparities across question characteristics, suggesting
that some question types, categories, and topics are less robust to
multilingual transfer which highlight the need for broader multilingual
evaluation to ensure fairness and reliability in LLM usage.

</details>


### [88] [Multimodal Fine-grained Context Interaction Graph Modeling for Conversational Speech Synthesis](https://arxiv.org/abs/2509.06074)
*Zhenqi Jia,Rui Liu,Berrak Sisman,Haizhou Li*

Main category: cs.CL

TL;DR: The paper introduces MFCIG-CSS, a system utilizing fine-grained multimodal dialogue interaction graphs to improve conversational speech synthesis, outperforming existing models in prosodic expressiveness.


<details>
  <summary>Details</summary>
Motivation: Existing conversational speech synthesis methods fail to account for the fine-grained semantic and prosodic interactions at the word level in multimodal dialogue history, leading to suboptimal prosody prediction.

Method: The proposed system, MFCIG-CSS, constructs two multimodal fine-grained dialogue interaction graphs (semantic and prosody interaction graphs) to encode word-level interactions and their contextual impact, enhancing prosody in synthesized speech.

Result: MFCIG-CSS demonstrated superior performance in capturing prosodic expressiveness compared to baseline models when evaluated on the DailyTalk dataset.

Conclusion: Fine-grained modeling of semantic and prosodic interactions improves natural prosody in conversational speech synthesis, making MFCIG-CSS a significant advancement in the field.

Abstract: Conversational Speech Synthesis (CSS) aims to generate speech with natural
prosody by understanding the multimodal dialogue history (MDH). The latest work
predicts the accurate prosody expression of the target utterance by modeling
the utterance-level interaction characteristics of MDH and the target
utterance. However, MDH contains fine-grained semantic and prosody knowledge at
the word level. Existing methods overlook the fine-grained semantic and
prosodic interaction modeling. To address this gap, we propose MFCIG-CSS, a
novel Multimodal Fine-grained Context Interaction Graph-based CSS system. Our
approach constructs two specialized multimodal fine-grained dialogue
interaction graphs: a semantic interaction graph and a prosody interaction
graph. These two interaction graphs effectively encode interactions between
word-level semantics, prosody, and their influence on subsequent utterances in
MDH. The encoded interaction features are then leveraged to enhance synthesized
speech with natural conversational prosody. Experiments on the DailyTalk
dataset demonstrate that MFCIG-CSS outperforms all baseline models in terms of
prosodic expressiveness. Code and speech samples are available at
https://github.com/AI-S2-Lab/MFCIG-CSS.

</details>


### [89] [Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge](https://arxiv.org/abs/2509.06079)
*Hao Liang,Ruitao Wu,Bohan Zeng,Junbo Niu,Wentao Zhang,Bin Dong*

Main category: cs.CL

TL;DR: The paper introduces a caption-assisted reasoning framework to improve multimodal reasoning by bridging visual and textual modalities. It achieved high performance in competitions and benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current state-of-the-art AI models struggle with multimodal reasoning scenarios, despite advances in text-based reasoning. Addressing this gap is crucial for advancing AI capabilities.

Method: The proposed framework integrates captions to link visual and textual information, enhancing multimodal reasoning and tackling challenges in scenarios involving diverse modalities.

Result: The framework achieved 1st place in the ICML 2025 AI for Math Workshop Challenge 2: SeePhys and demonstrated strong generalization on the MathVerse benchmark, especially for geometric reasoning.

Conclusion: The caption-assisted reasoning framework is effective and robust, bridging the gap between textual and visual reasoning. The code's public availability fosters further research and development in this domain.

Abstract: Multimodal reasoning remains a fundamental challenge in artificial
intelligence. Despite substantial advances in text-based reasoning, even
state-of-the-art models such as GPT-o3 struggle to maintain strong performance
in multimodal scenarios. To address this gap, we introduce a caption-assisted
reasoning framework that effectively bridges visual and textual modalities. Our
approach achieved 1st place in the ICML 2025 AI for Math Workshop \& Challenge
2: SeePhys, highlighting its effectiveness and robustness. Furthermore, we
validate its generalization on the MathVerse benchmark for geometric reasoning,
demonstrating the versatility of our method. Our code is publicly available at
https://github.com/OpenDCAI/SciReasoner.

</details>


### [90] [Orthogonal Low-rank Adaptation in Lie Groups for Continual Learning of Large Language Models](https://arxiv.org/abs/2509.06100)
*Kefan Cao,Shuaicheng Wu*

Main category: cs.CL

TL;DR: This paper introduces OLieRA, a method leveraging Lie group theory for fine-tuning large language models (LLMs) to address catastrophic forgetting, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: LLMs suffer from catastrophic forgetting in multi-task settings, and existing solutions that use parameter regularization do not preserve the geometric structure of LLM parameters, limiting their effectiveness.

Method: The authors propose Orthogonal Low-rank Adaptation in Lie Groups (OLieRA), which uses multiplicative updates to maintain the geometric structure of model parameters while applying orthogonality constraints to task subspaces.

Result: OLieRA achieves state-of-the-art results on the Standard CL benchmark and performs competitively in settings with a large number of tasks.

Conclusion: The method demonstrates that preserving geometric structure alongside orthogonality constraints can significantly enhance fine-tuning in large language models, addressing task interference more effectively.

Abstract: Large language models (LLMs) are prone to catastrophic forgetting in
sequential multi-task settings. Parameter regularization methods such as O-LoRA
and N-LoRA alleviate task interference by enforcing low-rank subspace
orthogonality, but they overlook the fact that conventional additive
fine-tuning disrupts the intrinsic geometric structure of LLM parameters,
limiting performance. Our key insight is that the parameter space of LLMs
possesses a geometric structure, which must be preserved in addition to
enforcing orthogonality. Based on this, we propose Orthogonal Low-rank
Adaptation in Lie Groups (OLieRA), which introduces Lie group theory into LLM
fine-tuning: leveraging multiplicative updates to preserve parameter geometry
while applying orthogonality constraints to task subspaces. Experiments
demonstrate that OLieRA achieves state-of-the-art results on the Standard CL
benchmark and remains among the top-performing methods in the Large Number of
Tasks setting.

</details>


### [91] [Benchmarking Gender and Political Bias in Large Language Models](https://arxiv.org/abs/2509.06164)
*Jinrui Yang,Xudong Han,Timothy Baldwin*

Main category: cs.CL

TL;DR: The paper presents EuroParlVote, a benchmark dataset for testing language model biases in political settings, evaluating gender classification and vote prediction tasks.


<details>
  <summary>Details</summary>
Motivation: To address the need for understanding bias and fairness of large language models in politically sensitive applications.

Method: The researchers linked European Parliament speeches to voting outcomes and demographic metadata, then tested state-of-the-art LLMs on gender classification and vote prediction.

Result: LLMs showed gender and political biases, such as misclassifying female MEPs and favoring centrist political groups over extremes. GPT-4 performed better than open-weight models.

Conclusion: EuroParlVote advances the study of bias and accountability in NLP, and its public release aims to foster research in political fairness.

Abstract: We introduce EuroParlVote, a novel benchmark for evaluating large language
models (LLMs) in politically sensitive contexts. It links European Parliament
debate speeches to roll-call vote outcomes and includes rich demographic
metadata for each Member of the European Parliament (MEP), such as gender, age,
country, and political group. Using EuroParlVote, we evaluate state-of-the-art
LLMs on two tasks -- gender classification and vote prediction -- revealing
consistent patterns of bias. We find that LLMs frequently misclassify female
MEPs as male and demonstrate reduced accuracy when simulating votes for female
speakers. Politically, LLMs tend to favor centrist groups while underperforming
on both far-left and far-right ones. Proprietary models like GPT-4o outperform
open-weight alternatives in terms of both robustness and fairness. We release
the EuroParlVote dataset, code, and demo to support future research on fairness
and accountability in NLP within political contexts.

</details>


### [92] [Understanding the Influence of Synthetic Data for Text Embedders](https://arxiv.org/abs/2509.06184)
*Jacob Mitchell Springer,Vaibhav Adlakha,Siva Reddy,Aditi Raghunathan,Marius Mosbach*

Main category: cs.CL

TL;DR: This paper studies the role of synthetic data in developing general-purpose text embedders, releasing high-quality synthetic datasets, and analyzing their localized benefits and trade-offs across tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is the lack of publicly available synthetic datasets, despite their significant role in training general-purpose text embedders, which limits further exploration and understanding of their impact on model generalization.

Method: The authors replicate and release synthetic data that reproduce earlier work, analyze the data's impact on model generalization, and investigate how and where it aids performance, particularly focusing on trade-offs between different tasks.

Result: The synthetic data provides improvement in performance, but the benefits are limited to specific datasets and tasks, with certain data negatively affecting performance in others.

Conclusion: Current approaches to synthetic data have limitations for building general-purpose embedders, and relying on it does not necessarily lead to models that are robust across various tasks.

Abstract: Recent progress in developing general purpose text embedders has been driven
by training on ever-growing corpora of synthetic LLM-generated data.
Nonetheless, no publicly available synthetic dataset exists, posing a barrier
to studying its role for generalization. To address this issue, we first
reproduce and publicly release the synthetic data proposed by Wang et al.
(Mistral-E5). Our synthetic data is high quality and leads to consistent
improvements in performance. Next, we critically examine where exactly
synthetic data improves model generalization. Our analysis reveals that
benefits from synthetic data are sparse and highly localized to individual
datasets. Moreover, we observe trade-offs between the performance on different
categories and data that benefits one task, degrades performance on another.
Our findings highlight the limitations of current synthetic data approaches for
building general-purpose embedders and challenge the notion that training on
synthetic data leads to more robust embedding models across tasks.

</details>


### [93] [Augmented Fine-Tuned LLMs for Enhanced Recruitment Automation](https://arxiv.org/abs/2509.06196)
*Mohamed T. Younes,Omar Walid,Khaled Shaban,Ali Hamdi,Mai Hassan*

Main category: cs.CL

TL;DR: This paper focuses on enhancing recruitment automation using fine-tuned Large Language Models (LLMs) tailored for recruitment tasks, achieving notable improvements in metrics like F1 score.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations of generic LLMs in recruitment automation and improve the precision and recall of candidate-job matching.

Method: The authors developed a new framework leveraging synthetic and parsed resume datasets in standardized JSON format. They fine-tuned LLMs, including the Phi-4 model, for recruitment-specific tasks.

Result: The fine-tuned Phi-4 model achieved an F1 score of 90.62%, demonstrating significant improvement over baseline and state-of-the-art models in recruitment workflows.

Conclusion: Fine-tuned LLMs, particularly the Phi-4 model, show promise in revolutionizing recruitment processes by enhancing accuracy and efficiency in candidate-job matching.

Abstract: This paper presents a novel approach to recruitment automation. Large
Language Models (LLMs) were fine-tuned to improve accuracy and efficiency.
Building upon our previous work on the Multilayer Large Language Model-Based
Robotic Process Automation Applicant Tracking (MLAR) system . This work
introduces a novel methodology. Training fine-tuned LLMs specifically tuned for
recruitment tasks. The proposed framework addresses the limitations of generic
LLMs by creating a synthetic dataset that uses a standardized JSON format. This
helps ensure consistency and scalability. In addition to the synthetic data
set, the resumes were parsed using DeepSeek, a high-parameter LLM. The resumes
were parsed into the same structured JSON format and placed in the training
set. This will help improve data diversity and realism. Through
experimentation, we demonstrate significant improvements in performance
metrics, such as exact match, F1 score, BLEU score, ROUGE score, and overall
similarity compared to base models and other state-of-the-art LLMs. In
particular, the fine-tuned Phi-4 model achieved the highest F1 score of 90.62%,
indicating exceptional precision and recall in recruitment tasks. This study
highlights the potential of fine-tuned LLMs. Furthermore, it will revolutionize
recruitment workflows by providing more accurate candidate-job matching.

</details>


### [94] [MSLEF: Multi-Segment LLM Ensemble Finetuning in Recruitment](https://arxiv.org/abs/2509.06200)
*Omar Walid,Mohamed T. Younes,Khaled Shaban,Mai Hassan,Ali Hamdi*

Main category: cs.CL

TL;DR: MSLEF is a multi-segment ensemble framework that fine-tunes large language models for improved resume parsing.


<details>
  <summary>Details</summary>
Motivation: To address limitations of single-model systems in resume parsing, especially with diverse formats and structures.

Method: The framework involves weighted voting of fine-tuned LLMs specialized for different resume segments and uses Gemini-2.5-Flash as an aggregator.

Result: MSLEF significantly improved metrics like EM, F1, BLEU, ROUGE, and Recruitment Similarity, outperforming single models by up to +7% in RS.

Conclusion: MSLEF succeeds in enhancing generalization and accuracy in resume parsing, demonstrating adaptability to diverse hiring scenarios.

Abstract: This paper presents MSLEF, a multi-segment ensemble framework that employs
LLM fine-tuning to enhance resume parsing in recruitment automation. It
integrates fine-tuned Large Language Models (LLMs) using weighted voting, with
each model specializing in a specific resume segment to boost accuracy.
Building on MLAR , MSLEF introduces a segment-aware architecture that leverages
field-specific weighting tailored to each resume part, effectively overcoming
the limitations of single-model systems by adapting to diverse formats and
structures. The framework incorporates Gemini-2.5-Flash LLM as a high-level
aggregator for complex sections and utilizes Gemma 9B, LLaMA 3.1 8B, and Phi-4
14B. MSLEF achieves significant improvements in Exact Match (EM), F1 score,
BLEU, ROUGE, and Recruitment Similarity (RS) metrics, outperforming the best
single model by up to +7% in RS. Its segment-aware design enhances
generalization across varied resume layouts, making it highly adaptable to
real-world hiring scenarios while ensuring precise and reliable candidate
representation.

</details>


### [95] [No Encore: Unlearning as Opt-Out in Music Generation](https://arxiv.org/abs/2509.06277)
*Jinju Kim,Taehan Kim,Abdul Waheed,Rita Singh*

Main category: cs.CL

TL;DR: This paper explores the use of machine unlearning techniques in AI-driven text-to-music generation models to address copyright and ethical concerns.


<details>
  <summary>Details</summary>
Motivation: To reduce ethical and legal issues arising from the potential misuse of copyrighted content by AI music generation models.

Method: Machine unlearning techniques were applied to pre-trained Text-to-Music (TTM) models to test the effectiveness of unlearning datasets while preserving model performance.

Result: Preliminary insights were obtained about the challenges of applying machine unlearning to music generation models, demonstrating promising directions.

Conclusion: This study lays the groundwork for future research involving machine unlearning in generative music models while addressing copyright concerns.

Abstract: AI music generation is rapidly emerging in the creative industries, enabling
intuitive music generation from textual descriptions. However, these systems
pose risks in exploitation of copyrighted creations, raising ethical and legal
concerns. In this paper, we present preliminary results on the first
application of machine unlearning techniques from an ongoing research to
prevent inadvertent usage of creative content. Particularly, we explore
existing methods in machine unlearning to a pre-trained Text-to-Music (TTM)
baseline and analyze their efficacy in unlearning pre-trained datasets without
harming model performance. Through our experiments, we provide insights into
the challenges of applying unlearning in music generation, offering a
foundational analysis for future works on the application of unlearning for
music generative models.

</details>


### [96] [Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?](https://arxiv.org/abs/2509.06350)
*Junjie Mu,Zonghao Ying,Zhekui Fan,Zonglei Jing,Yaoyuan Zhang,Zhengmin Yu,Wenxin Zhang,Quanchen Zou,Xiangzheng Zhang*

Main category: cs.CL

TL;DR: The paper introduces Mask-GCG, a method to optimize jailbreak attacks on LLMs by reducing redundancy in suffixes, delivering similar attack success rates with reduced computational effort.


<details>
  <summary>Details</summary>
Motivation: To address redundancy in suffixes used for jailbreak attacks on LLMs, improving their efficiency and interpretability.

Method: Mask-GCG uses learnable token masking to identify high-impact tokens and prune low-impact ones, reducing redundancy and computational overhead in these attacks.

Result: Experimental results show Mask-GCG maintains attack success rates by focusing on impactful tokens, confirming redundancy in suffixes.

Conclusion: Mask-GCG improves the efficiency of jailbreak attacks on LLMs without compromising success rates, paving the way for more interpretable and efficient LLM developments.

Abstract: Jailbreak attacks on Large Language Models (LLMs) have demonstrated various
successful methods whereby attackers manipulate models into generating harmful
responses that they are designed to avoid. Among these, Greedy Coordinate
Gradient (GCG) has emerged as a general and effective approach that optimizes
the tokens in a suffix to generate jailbreakable prompts. While several
improved variants of GCG have been proposed, they all rely on fixed-length
suffixes. However, the potential redundancy within these suffixes remains
unexplored. In this work, we propose Mask-GCG, a plug-and-play method that
employs learnable token masking to identify impactful tokens within the suffix.
Our approach increases the update probability for tokens at high-impact
positions while pruning those at low-impact positions. This pruning not only
reduces redundancy but also decreases the size of the gradient space, thereby
lowering computational overhead and shortening the time required to achieve
successful attacks compared to GCG. We evaluate Mask-GCG by applying it to the
original GCG and several improved variants. Experimental results show that most
tokens in the suffix contribute significantly to attack success, and pruning a
minority of low-impact tokens does not affect the loss values or compromise the
attack success rate (ASR), thereby revealing token redundancy in LLM prompts.
Our findings provide insights for developing efficient and interpretable LLMs
from the perspective of jailbreak attacks.

</details>


### [97] [PL-CA: A Parametric Legal Case Augmentation Framework](https://arxiv.org/abs/2509.06356)
*Ao Chang,Yubo Chen,Jun Zhao*

Main category: cs.CL

TL;DR: The paper proposes PL-CA, which incorporates a parametric RAG framework to encode legal knowledge efficiently, alleviating context pressure. It also introduces a multi-task legal dataset to evaluate its effectiveness.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of conventional RAG methods, which include context window constraints, computational overhead, and the inadequacy of existing legal benchmarks to handle real-world multi-task scenarios.

Method: Introduces the P-RAG framework for encoding corpus knowledge as parametric vectors, and integrates this knowledge into LLMs using LoRA. Also constructs an expert-annotated multi-task legal dataset.

Result: The proposed PL-CA approach reduces context overhead while maintaining competitive performance on legal downstream tasks compared to traditional RAG methods.

Conclusion: PL-CA improves the efficiency and capabilities of LLMs in legal contexts by addressing context limitations and benchmark inadequacies, demonstrating its practical advantages on a new expert-verified dataset.

Abstract: Conventional RAG is considered one of the most effective methods for
addressing model knowledge insufficiency and hallucination, particularly in the
judicial domain that requires high levels of knowledge rigor, logical
consistency, and content integrity. However, the conventional RAG method only
injects retrieved documents directly into the model's context, which severely
constrains models due to their limited context windows and introduces
additional computational overhead through excessively long contexts, thereby
disrupting models' attention and degrading performance on downstream tasks.
Moreover, many existing benchmarks lack expert annotation and focus solely on
individual downstream tasks while real-world legal scenarios consist of
multiple mixed legal tasks, indicating conventional benchmarks' inadequacy for
reflecting models' true capabilities. To address these limitations, we propose
PL-CA, which introduces a parametric RAG (P-RAG) framework to perform data
augmentation on corpus knowledge and encode this legal knowledge into
parametric vectors, and then integrates this parametric knowledge into the
LLM's feed-forward networks (FFN) via LoRA, thereby alleviating models' context
pressure. Additionally, we also construct a multi-task legal dataset comprising
more than 2000 training and test instances, which are all expert-annotated and
manually verified. We conduct our experiments on our dataset, and the
experimental results demonstrate that our method reduces the overhead
associated with excessively long contexts while maintaining competitive
performance on downstream tasks compared to conventional RAG. Our code and
dataset are provided in the appendix.

</details>


### [98] [Do LLMs exhibit the same commonsense capabilities across languages?](https://arxiv.org/abs/2509.06401)
*Ivan Martínez-Murillo,Elena Lloret,Paloma Moreda,Albert Gatt*

Main category: cs.CL

TL;DR: The study investigates how Large Language Models handle multilingual commonsense generation using a benchmark called MULTICOM, finding strengths in English but limitations in less-resourced languages.


<details>
  <summary>Details</summary>
Motivation: The paper aims to evaluate the boundaries and limitations of LLMs in generating commonsense knowledge across multiple languages.

Method: They utilized the MULTICOM benchmark, extending it to four languages, and evaluated LLMs with automatic metrics, AI evaluations, and human assessments.

Result: English showed superior performance, while less-resourced languages had significantly lower results; evidence was mixed on contextual support benefits.

Conclusion: LLMs currently show limitations in handling multilingual commonsense generation, particularly in less-resourced languages.

Abstract: This paper explores the multilingual commonsense generation abilities of
Large Language Models (LLMs). To facilitate this investigation, we introduce
MULTICOM, a novel benchmark that extends the COCOTEROS dataset to four
languages: English, Spanish, Dutch, and Valencian. The task involves generating
a commonsensical sentence that includes a given triplet of words. We evaluate a
range of open-source LLMs, including LLaMA, Qwen, Gemma, EuroLLM, and
Salamandra, on this benchmark. Our evaluation combines automatic metrics,
LLM-as-a-judge approaches (using Prometheus and JudgeLM), and human
annotations. Results consistently show superior performance in English, with
significantly lower performance in less-resourced languages. While contextual
support yields mixed results, it tends to benefit underrepresented languages.
These findings underscore the current limitations of LLMs in multilingual
commonsense generation. The dataset is publicly available at
https://huggingface.co/datasets/gplsi/MULTICOM.

</details>


### [99] [WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents](https://arxiv.org/abs/2509.06501)
*Junteng Liu,Yunji Li,Chi Zhang,Jingyang Li,Aili Chen,Ke Ji,Weiyu Cheng,Zijia Wu,Chengyu Du,Qidi Xu,Jiayuan Song,Zhengmao Zhu,Wenhu Chen,Pengyu Zhao,Junxian He*

Main category: cs.CL

TL;DR: The paper introduces WebExplorer, a method to create challenging query-answer pairs for training advanced web agents. Using this, they developed WebExplorer-8B, which achieves state-of-the-art performance in various benchmarks.


<details>
  <summary>Details</summary>
Motivation: To enhance the information-seeking capabilities of web agents by overcoming the limited availability of challenging training data for complex tasks.

Method: Proposed a data generation approach (WebExplorer) involving model-based exploration and iterative query evolution to produce challenging query-answer pairs. Developed WebExplorer-8B through supervised fine-tuning and reinforcement learning.

Result: WebExplorer-8B exhibits superior performance across diverse information-seeking tasks, outperforming larger models like WebSailor-72B and achieving the best results among smaller models. Additionally, it generalizes effectively on unrelated benchmarks despite being trained on a narrow dataset.

Conclusion: WebExplorer's methodology significantly improves long-horizon web agents, demonstrating the practicality of the proposed approach for complex, multi-step web-based information retrieval tasks.

Abstract: The paradigm of Large Language Models (LLMs) has increasingly shifted toward
agentic applications, where web browsing capabilities are fundamental for
retrieving information from diverse online sources. However, existing
open-source web agents either demonstrate limited information-seeking abilities
on complex tasks or lack transparent implementations. In this work, we identify
that the key challenge lies in the scarcity of challenging data for information
seeking. To address this limitation, we introduce WebExplorer: a systematic
data generation approach using model-based exploration and iterative,
long-to-short query evolution. This method creates challenging query-answer
pairs that require multi-step reasoning and complex web navigation. By
leveraging our curated high-quality dataset, we successfully develop advanced
web agent WebExplorer-8B through supervised fine-tuning followed by
reinforcement learning. Our model supports 128K context length and up to 100
tool calling turns, enabling long-horizon problem solving. Across diverse
information-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art
performance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able
to effectively search over an average of 16 turns after RL training, achieving
higher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best
performance among models up to 100B parameters on WebWalkerQA and FRAMES.
Beyond these information-seeking tasks, our model also achieves strong
generalization on the HLE benchmark even though it is only trained on
knowledge-intensive QA data. These results highlight our approach as a
practical path toward long-horizon web agents.

</details>


### [100] [Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training](https://arxiv.org/abs/2509.06518)
*Andrei Baroian,Kasper Notebomer*

Main category: cs.CL

TL;DR: This study explores new ways to redistribute computational capacity across Transformer layers using Layer-Wise Scaling (LWS) methods, resulting in enhanced performance without reducing training efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional Transformer-based language models use uniform layer sizes, ignoring diverse roles and computational needs for different depths. The paper seeks to address this inefficiency by redistributing FFN widths and attention heads.

Method: Three Layer-Wise Scaling variants (Framed, Reverse, Crown) were proposed to optimize layer designs via two or three-point linear interpolation during pre-training. Models were trained with 180M parameters on 5B tokens, with comparisons against isotropic baselines.

Result: All models converged similarly in loss, outperforming equal-cost isotropic baselines without compromising throughput during training.

Conclusion: Initial results highlight potential improvement in performance through layer-wise architectural changes, urging future exploration at larger scales in pre-training.

Abstract: Transformer-based language models traditionally use uniform (isotropic) layer
sizes, yet they ignore the diverse functional roles that different depths can
play and their computational capacity needs. Building on Layer-Wise Scaling
(LWS) and pruning literature, we introduce three new LWS variants - Framed,
Reverse, and Crown - that redistribute FFN widths and attention heads via two
or three-point linear interpolation in the pre-training stage. We present the
first systematic ablation of LWS and its variants, on a fixed budget of 180M
parameters, trained on 5B tokens. All models converge to similar losses and
achieve better performance compared to an equal-cost isotropic baseline,
without a substantial decrease in training throughput. This work represents an
initial step into the design space of layer-wise architectures for
pre-training, but future work should scale experiments to orders of magnitude
more tokens and parameters to fully assess their potential.

</details>


### [101] [LAMDAS: LLM as an Implicit Classifier for Domain-specific Data Selection](https://arxiv.org/abs/2509.06524)
*Jian Wu,Hang Yu,Bingchang Liu,Wenjie Yang,Peng Di,Jianguo Li,Yue Zhang*

Main category: cs.CL

TL;DR: The paper proposes LAMDAS, a method for domain-specific data selection leveraging pre-trained LLMs as implicit classifiers to overcome data scarcity issues and improve fine-tuning performance.


<details>
  <summary>Details</summary>
Motivation: The need to adapt large language models (LLMs) to specific domains despite a scarcity of high-quality, curated data, and the challenges posed by indiscriminate data selection leading to degraded model performance.

Method: LAMDAS introduces a data selection strategy, treating the process as a one-class classification problem where pre-trained LLMs identify domain-specific data using a small reference dataset.

Result: Experiments show that LAMDAS outperforms full-data training and nine SOTA baselines under various conditions, achieving significant performance gains with improved computational efficiency.

Conclusion: LAMDAS is a highly efficient and accurate data selection method that improves domain adaptation for LLMs, balancing high performance with reduced computational costs.

Abstract: Adapting large language models (LLMs) to specific domains often faces a
critical bottleneck: the scarcity of high-quality, human-curated data. While
large volumes of unchecked data are readily available, indiscriminately using
them for fine-tuning risks introducing noise and degrading performance.
Strategic data selection is thus crucial, requiring a method that is both
accurate and efficient. Existing approaches, categorized as similarity-based
and direct optimization methods, struggle to simultaneously achieve these
goals. In this paper, we introduce LAMDAS (LLM As an iMplicit classifier for
domain-specific DAta Selection), a novel approach that leverages the
pre-trained LLM itself as an implicit classifier, thereby bypassing explicit
feature engineering and computationally intensive optimization process. LAMDAS
reframes data selection as a one-class classification problem, identifying
candidate data that "belongs" to the target domain defined by a small reference
dataset. Extensive experimental results demonstrate that LAMDAS not only
exceeds the performance of full-data training using a fraction of the data but
also outperforms nine state-of-the-art (SOTA) baselines under various
scenarios. Furthermore, LAMDAS achieves the most compelling balance between
performance gains and computational efficiency compared to all evaluated
baselines.

</details>


### [102] [SLiNT: Structure-aware Language Model with Injection and Contrastive Training for Knowledge Graph Completion](https://arxiv.org/abs/2509.06531)
*Mengxue Yang,Chun Yang,Jiaqi Zhu,Jiafan Li,Jingqi Zhang,Yuyang Li,Ying Li*

Main category: cs.CL

TL;DR: The paper introduces SLiNT, a framework that improves link prediction in knowledge graphs by injecting structural context into large language models using modular techniques like SGNE, DHCL, and GDDI. SLiNT performs well compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Link prediction in knowledge graphs is challenging due to structural sparsity and semantic ambiguity, especially in incomplete or zero-shot scenarios.

Method: The SLiNT framework uses three techniques: SGNE enriches sparse entities with pseudo-neighbors; DHCL interpolates hard positives/negatives for supervision; and GDDI applies structure-aware intervention to frozen LLM parameters.

Result: Experiments on WN18RR and FB15k-237 datasets show SLiNT achieves superior or competitive performance against baseline methods.

Conclusion: SLiNT demonstrates the value of integrating structural signals into language models, enhancing the scalability and accuracy of knowledge graph completion.

Abstract: Link prediction in knowledge graphs requires integrating structural
information and semantic context to infer missing entities. While large
language models offer strong generative reasoning capabilities, their limited
exploitation of structural signals often results in structural sparsity and
semantic ambiguity, especially under incomplete or zero-shot settings. To
address these challenges, we propose SLiNT (Structure-aware Language model with
Injection and coNtrastive Training), a modular framework that injects
knowledge-graph-derived structural context into a frozen LLM backbone with
lightweight LoRA-based adaptation for robust link prediction. Specifically,
Structure-Guided Neighborhood Enhancement (SGNE) retrieves pseudo-neighbors to
enrich sparse entities and mitigate missing context; Dynamic Hard Contrastive
Learning (DHCL) introduces fine-grained supervision by interpolating hard
positives and negatives to resolve entity-level ambiguity; and
Gradient-Decoupled Dual Injection (GDDI) performs token-level structure-aware
intervention while preserving the core LLM parameters. Experiments on WN18RR
and FB15k-237 show that SLiNT achieves superior or competitive performance
compared with both embedding-based and generation-based baselines,
demonstrating the effectiveness of structure-aware representation learning for
scalable knowledge graph completion.

</details>


### [103] [HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in Large Language Models](https://arxiv.org/abs/2509.06596)
*Xin Tong,Zhi Lin,Jingya Wang,Bo Jin*

Main category: cs.CL

TL;DR: This paper introduces HAVE, a decoding framework that reduces hallucinations in LLMs by reweighing attention heads and calibrating value contributions without requiring fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address hallucinations in LLMs during retrieval-augmented or long-context generation, caused by input-agnostic head importance and poorly reflective attention weights.

Method: The paper proposes HAVE, which includes head-adaptive gating for instance-level head reweighing and value calibration to consider the magnitude of value vectors. HAVE is parameter-free, operates in a single forward pass, and integrates easily with LLMs.

Result: HAVE consistently reduces hallucinations in experiments across multiple QA benchmarks and LLMs, outperforming strong baselines like DAGCD with modest computational overhead.

Conclusion: HAVE enhances the trustworthiness of LLM-generated content by reducing hallucinations efficiently and transparently, and it is broadly applicable to off-the-shelf models in real-world applications.

Abstract: Large Language Models (LLMs) often produce hallucinations in
retrieval-augmented or long-context generation, even when relevant evidence is
present. This stems from two issues: head importance is treated as
input-agnostic, and raw attention weights poorly reflect each token's true
contribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a
parameter-free decoding framework that directly addresses both challenges. HAVE
introduces head-adaptive gating, which performs instance-level soft reweighing
of attention heads, and value calibration, which augments attention with the
magnitude of value vectors to approximate write-back contribution. Together,
these modules construct token-level evidence aligned with model updates and
fuse it with the LM distribution through a lightweight uncertainty-scaled
policy. HAVE requires no finetuning and operates in a single forward pass,
making it efficient and broadly applicable. Experiments across multiple QA
benchmarks and LLM families demonstrate that HAVE consistently reduces
hallucinations and outperforms strong baselines, including DAGCD, with modest
overhead. The framework is transparent, reproducible, and readily integrates
with off-the-shelf LLMs, advancing trustworthy generation in real-world
settings.

</details>


### [104] [Guided Decoding and Its Critical Role in Retrieval-Augmented Generation](https://arxiv.org/abs/2509.06631)
*Özgür Uğur,Musa Yılmaz,Esra Şavirdi,Özay Ezerceli,Mahmut El Huseyni,Selva Taş,Reyhan Bayraktar*

Main category: cs.CL

TL;DR: The paper evaluates methods for guiding the output of Retrieval-Augmented Generation (RAG) systems, exploring structured output and minimizing hallucinations.


<details>
  <summary>Details</summary>
Motivation: Ensuring structured and reliable outputs from Large Language Models (LLMs) in Retrieval-Augmented Generation (RAG) systems while minimizing hallucinations.

Method: Three guided decoding methods (Outlines, XGrammar, and LM Format Enforcer) were compared using different multi-turn prompting setups: 0-turn, 1-turn, and 2-turn.

Result: The study identifies how multi-turn interactions affect guided decoding, uncovering notable performance variations.

Conclusion: The paper advances understanding of guided decoding in RAG systems, offering insights for choosing methods based on specific needs for structured LLM deployment.

Abstract: The integration of Large Language Models (LLMs) into various applications has
driven the need for structured and reliable responses. A key challenge in
Retrieval-Augmented Generation (RAG) systems is ensuring that outputs align
with expected formats while minimizing hallucinations. This study examines the
role of guided decoding in RAG systems, comparing three methods, Outlines,
XGrammar, and LM Format Enforcer, across different multi-turn prompting setups
(0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates,
and output quality, we provide insights into their performance and
applicability. Our findings reveal how multi-turn interactions influence guided
decoding, uncovering unexpected performance variations that can inform method
selection for specific use cases. This work advances the understanding of
structured output generation in RAG systems, offering both theoretical insights
and practical guidance for LLM deployment.

</details>


### [105] [Modelling Intertextuality with N-gram Embeddings](https://arxiv.org/abs/2509.06637)
*Yi Xing*

Main category: cs.CL

TL;DR: The paper develops a quantitative model to measure intertextuality using embeddings of n-grams, validated through tests and network analysis.


<details>
  <summary>Details</summary>
Motivation: The study aims to provide a scalable and quantifiable method of analyzing intertextuality, addressing the lack of computational tools for such investigations.

Method: The model calculates intertextuality through pairwise comparisons of embeddings of n-grams from texts, averaging results to determine overall intertextuality. Validation and scalability tests are conducted.

Result: The quantitative model effectively identifies intertextuality in texts, confirmed through tests on known intertextual works and a large corpus of 267 texts. Network analysis uncovers community structures and centrality.

Conclusion: The approach proves successful for scalable and efficient quantification of intertextuality, enabling new network-based insights into literary connections.

Abstract: Intertextuality is a central tenet in literary studies. It refers to the
intricate links between literary texts that are created by various types of
references. This paper proposes a new quantitative model of intertextuality to
enable scalable analysis and network-based insights: perform pairwise
comparisons of the embeddings of n-grams from two texts and average their
results as the overall intertextuality. Validation on four texts with known
degrees of intertextuality, alongside a scalability test on 267 diverse texts,
demonstrates the method's effectiveness and efficiency. Network analysis
further reveals centrality and community structures, affirming the approach's
success in capturing and quantifying intertextual relationships.

</details>


### [106] [Domain-Aware RAG: MoL-Enhanced RL for Efficient Training and Scalable Retrieval](https://arxiv.org/abs/2509.06650)
*Hao Lin,Peitong Xie,Jingxue Chen,Jie Lin,Qingkun Tang,Qianchun Lu*

Main category: cs.CL

TL;DR: The paper introduces MoLER, a method combining pre-training and reinforcement learning to enhance retrieval and generation in RAG systems, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in balancing domain-specific knowledge and query optimization in RAG retrieval systems.

Method: MoLER employs continual pre-training with a Mixture of Losses and Group Relative Policy Optimization during reinforcement learning. It introduces strategies like Multi-query Single-passage Late Fusion for training efficiency.

Result: MoLER significantly outperforms baseline methods in benchmark evaluations, achieving state-of-the-art retrieval performance.

Conclusion: MoLER effectively addresses retrieval challenges in RAG systems, making them more robust and scalable for specialized domains.

Abstract: Retrieval-Augmented Generation (RAG) systems rely heavily on the retrieval
stage, particularly the coarse-ranking process. Existing coarse-ranking
optimization approaches often struggle to balance domain-specific knowledge
learning with query enhencement, resulting in suboptimal retrieval performance.
To address this challenge, we propose MoLER, a domain-aware RAG method that
uses MoL-Enhanced Reinforcement Learning to optimize retrieval. MoLER has a
two-stage pipeline: a continual pre-training (CPT) phase using a Mixture of
Losses (MoL) to balance domain-specific knowledge with general language
capabilities, and a reinforcement learning (RL) phase leveraging Group Relative
Policy Optimization (GRPO) to optimize query and passage generation for
maximizing document recall. A key innovation is our Multi-query Single-passage
Late Fusion (MSLF) strategy, which reduces computational overhead during RL
training while maintaining scalable inference via Multi-query Multi-passage
Late Fusion (MMLF). Extensive experiments on benchmark datasets show that MoLER
achieves state-of-the-art performance, significantly outperforming baseline
methods. MoLER bridges the knowledge gap in RAG systems, enabling robust and
scalable retrieval in specialized domains.

</details>


### [107] [IntrEx: A Dataset for Modeling Engagement in Educational Conversations](https://arxiv.org/abs/2509.06652)
*Xingwei Tan,Mahathi Parvatham,Chiara Gambi,Gabriele Pergola*

Main category: cs.CL

TL;DR: The paper introduces ‘IntrEx,’ a novel dataset supporting the study of engagement in teacher-student conversations and evaluates large language models to predict and enhance interestingness judgments in dialogues.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in keeping learners engaged during second-language acquisition conversations, with limited understanding of the linguistic elements that foster conversational interest.

Method: The authors created the IntrEx dataset, adding sequence-level interestingness annotations to dialogue data, and gathered ratings via comparison-based methods inspired by reinforcement learning from human feedback (RLHF). They test large language models, fine-tuned using these annotations, to predict human judgments about engagement.

Result: Specialized large language models fine-tuned on interestingness ratings outperformed larger, proprietary models (e.g., GPT-4o), highlighting the effectiveness of targeted datasets. Analysis revealed factors like concreteness and readability influence engagement.

Conclusion: Specialized datasets like IntrEx hold potential for modeling and improving engagement in educational conversations by helping large language models better understand and predict human interest.

Abstract: Engagement and motivation are crucial for second-language acquisition, yet
maintaining learner interest in educational conversations remains a challenge.
While prior research has explored what makes educational texts interesting,
still little is known about the linguistic features that drive engagement in
conversations. To address this gap, we introduce IntrEx, the first large
dataset annotated for interestingness and expected interestingness in
teacher-student interactions. Built upon the Teacher-Student Chatroom Corpus
(TSCC), IntrEx extends prior work by incorporating sequence-level annotations,
allowing for the study of engagement beyond isolated turns to capture how
interest evolves over extended dialogues. We employ a rigorous annotation
process with over 100 second-language learners, using a comparison-based rating
approach inspired by reinforcement learning from human feedback (RLHF) to
improve agreement. We investigate whether large language models (LLMs) can
predict human interestingness judgments. We find that LLMs (7B/8B parameters)
fine-tuned on interestingness ratings outperform larger proprietary models like
GPT-4o, demonstrating the potential for specialised datasets to model
engagement in educational settings. Finally, we analyze how linguistic and
cognitive factors, such as concreteness, comprehensibility (readability), and
uptake, influence engagement in educational dialogues.

</details>


### [108] [ParCzech4Speech: A New Speech Corpus Derived from Czech Parliamentary Data](https://arxiv.org/abs/2509.06675)
*Vladislav Stankov,Matyáš Kopp,Ondřej Bojar*

Main category: cs.CL

TL;DR: The paper introduces ParCzech4Speech 1.0, an extensive Czech speech modeling corpus combining parliamentary speech recordings with official transcripts, processed for enhanced audio-text alignment and usability.


<details>
  <summary>Details</summary>
Motivation: To improve speech modeling tasks by creating a reliable, flexible, and publicly accessible Czech parliamentary speech corpus with advanced audio-text alignment.

Method: Sound recordings of Czech parliamentary speeches were processed using WhisperX and Wav2Vec 2.0 to achieve automated audio-text alignment, improving upon previous datasets.

Result: The dataset provides 2,695 hours of Czech speech data in three variants: sentence-segmented, unsegmented, and raw-alignment, all maintaining metadata and released under CC-BY license.

Conclusion: ParCzech4Speech 1.0 is a robust, flexible, and enhanced speech dataset aimed at aiding automatic speech recognition, synthesis, and other customization-oriented tasks.

Abstract: We introduce ParCzech4Speech 1.0, a processed version of the ParCzech 4.0
corpus, targeted at speech modeling tasks with the largest variant containing
2,695 hours. We combined the sound recordings of the Czech parliamentary
speeches with the official transcripts. The recordings were processed with
WhisperX and Wav2Vec 2.0 to extract automated audio-text alignment. Our
processing pipeline improves upon the ParCzech 3.0 speech recognition version
by extracting more data with higher alignment reliability. The dataset is
offered in three flexible variants: (1) sentence-segmented for automatic speech
recognition and speech synthesis tasks with clean boundaries, (2) unsegmented
preserving original utterance flow across sentences, and (3) a raw-alignment
for further custom refinement for other possible tasks. All variants maintain
the original metadata and are released under a permissive CC-BY license. The
dataset is available in the LINDAT repository, with the sentence-segmented and
unsegmented variants additionally available on Hugging Face.

</details>


### [109] [Will Annotators Disagree? Identifying Subjectivity in Value-Laden Arguments](https://arxiv.org/abs/2509.06704)
*Amir Homayounirad,Enrico Liscio,Tong Wang,Catholijn M. Jonker,Luciano C. Siebert*

Main category: cs.CL

TL;DR: The paper examines approaches for identifying subjectivity in recognizing human values driving arguments, finding direct subjectivity identification improves performance.


<details>
  <summary>Details</summary>
Motivation: Understanding and handling annotator disagreement in subjective tasks, such as recognizing human values behind arguments.

Method: Two main approaches: value prediction to infer subjectivity and direct identification of subjectivity. Experiments tested combining contrastive loss with binary cross-entropy loss.

Result: Direct subjectivity identification approaches significantly outperform other methods in flagging subjective arguments. Combining loss functions doesn't improve performance but reduces dependency.

Conclusion: Directly identifying subjectivity advances model reliability and helps highlight arguments subject to diverse individual interpretations, improving annotation processes.

Abstract: Aggregating multiple annotations into a single ground truth label may hide
valuable insights into annotator disagreement, particularly in tasks where
subjectivity plays a crucial role. In this work, we explore methods for
identifying subjectivity in recognizing the human values that motivate
arguments. We evaluate two main approaches: inferring subjectivity through
value prediction vs. directly identifying subjectivity. Our experiments show
that direct subjectivity identification significantly improves the model
performance of flagging subjective arguments. Furthermore, combining
contrastive loss with binary cross-entropy loss does not improve performance
but reduces the dependency on per-label subjectivity. Our proposed methods can
help identify arguments that individuals may interpret differently, fostering a
more nuanced annotation process.

</details>


### [110] [Anchoring Refusal Direction: Mitigating Safety Risks in Tuning via Projection Constraint](https://arxiv.org/abs/2509.06795)
*Yanrui Du,Fenglei Fan,Sendong Zhao,Jiawei Cao,Qika Lin,Kai He,Ting Liu,Bing Qin,Mengling Feng*

Main category: cs.CL

TL;DR: The paper identifies and addresses issues in Instruction Fine-Tuning (IFT) of Large Language Models (LLMs) by mitigating drift in refusal direction during training using a method called ProCon.


<details>
  <summary>Details</summary>
Motivation: IFT improves LLM performance but raises safety concerns, particularly compromising the models' ability to refuse malicious instructions.

Method: The proposed ProCon method utilizes a projection-constrained loss term to regulate the magnitude of training samples' hidden state projections onto the refusal direction and introduces a warm-up strategy for better effectiveness.

Result: ProCon effectively mitigates safety risks by stabilizing refusal direction drift while maintaining task performance, outperforming strong baselines across various datasets and scenarios.

Conclusion: ProCon not only enhances LLM safety but also provides an interpretability-driven approach to understanding their internal mechanisms, paving the way for future research in LLM safety.

Abstract: Instruction Fine-Tuning (IFT) has been widely adopted as an effective
post-training strategy to enhance various abilities of Large Language Models
(LLMs). However, prior studies have shown that IFT can significantly compromise
LLMs' safety, particularly their ability to refuse malicious instructions,
raising significant concerns. Recent research into the internal mechanisms of
LLMs has identified the refusal direction (r-direction) in the hidden states,
which plays a pivotal role in governing refusal behavior. Building on this
insight, our study reveals that the r-direction tends to drift during training,
which we identify as one of the causes of the associated safety risks. To
mitigate such drift, our proposed ProCon method introduces a
projection-constrained loss term that regularizes the projection magnitude of
each training sample's hidden state onto the r-direction. Our initial analysis
shows that applying an appropriate constraint can effectively mitigate the
refusal direction drift and associated safety risks, but remains limited by
overall performance barriers. To overcome this barrier, informed by our
observation of early-stage sharp drift and a data-driven perspective, we
introduce a warm-up strategy that emphasizes early-stage strong constraints and
broaden the data distribution to strengthen constraint signals, leading to an
enhanced ProCon method. Experimental results under various datasets, scenarios,
and LLMs demonstrate that our method can significantly mitigate safety risks
posed by IFT while preserving task performance gains. Even compared with strong
baselines, our method consistently delivers superior overall performance.
Crucially, our analysis indicates that ProCon can contribute to stabilizing the
r-direction during training, while such an interpretability-driven exploration
of LLMs' internal mechanisms lays a solid foundation for future safety
research.

</details>


### [111] [MachineLearningLM: Continued Pretraining Language Models on Millions of Synthetic Tabular Prediction Tasks Scales In-Context ML](https://arxiv.org/abs/2509.06806)
*Haoyu Dong,Pengkun Zhang,Mingzhe Lu,Yanzhen Shen,Guolin Ke*

Main category: cs.CL

TL;DR: The paper introduces a methodology, MachineLearningLM, to integrate machine learning capabilities into large language models (LLMs) for robust in-context learning while maintaining their general knowledge and reasoning skills.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the ability of LLMs to effectively learn from many-shot demonstrations in standard ML tasks, a weakness they currently exhibit.

Method: The method involves continued pretraining of LLMs by synthesizing ML tasks from millions of structural causal models, incorporating decision-making strategies from random-forest teachers, and using token-efficient prompts to handle large examples within limited context windows.

Result: MachineLearningLM achieves about 15% higher accuracy compared to strong LLM baselines on out-of-distribution tabular classification tasks across various domains and demonstrates a consistent improvement in accuracy with increased in-context demonstrations. Additionally, it achieves random-forest-level performance without task-specific training.

Conclusion: MachineLearningLM successfully enhances LLMs with strong in-context ML task capabilities while preserving their general knowledge and reasoning abilities, making them broadly applicable across tasks and domains.

Abstract: Large language models (LLMs) possess broad world knowledge and strong
general-purpose reasoning ability, yet they struggle to learn from many
in-context examples on standard machine learning (ML) tasks, that is, to
leverage many-shot demonstrations purely via in-context learning (ICL) without
gradient descent. We introduce MachineLearningLM, a portable
continued-pretraining framework that equips a general-purpose LLM with robust
in-context ML capability while preserving its general knowledge and reasoning
for broader chat workflows.
  Our pretraining procedure synthesizes ML tasks from millions of structural
causal models (SCMs), spanning shot counts up to 1,024. We begin with a
random-forest teacher, distilling tree-based decision strategies into the LLM
to strengthen robustness in numerical modeling. All tasks are serialized with a
token-efficient prompt, enabling 3x to 6x more examples per context window and
delivering up to 50x amortized throughput via batch inference.
  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),
MachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an
average of about 15% on out-of-distribution tabular classification across
finance, physics, biology, and healthcare domains. It exhibits a striking
many-shot scaling law: accuracy increases monotonically as in-context
demonstrations grow from 8 to 1,024. Without any task-specific training, it
attains random-forest-level accuracy across hundreds of shots. General chat
capabilities, including knowledge and reasoning, are preserved: it achieves
75.4% on MMLU.

</details>


### [112] [MoGU V2: Toward a Higher Pareto Frontier Between Model Usability and Security](https://arxiv.org/abs/2509.06807)
*Yanrui Du,Fenglei Fan,Sendong Zhao,Jiawei Cao,Ting Liu,Bing Qin*

Main category: cs.CL

TL;DR: The paper introduces MoGU and MoGU_v2 frameworks aimed at improving the balance between the security and usability of Large Language Models (LLMs). The MoGU_v2 framework demonstrates stable performance gains and adaptability across various scenarios, offering a robust solution for mitigating LLM security risks without sacrificing task usability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the critical problem of security in Large Language Models (LLMs) without requiring a trade-off between usability and security, which has been a significant limitation in existing solutions.

Method: The authors propose the MoGU framework, which uses an intra-layer router to balance contributions of security-optimized and usability-optimized variants. They then enhance this with MoGU_v2, which tightly integrates routers and hidden states, selectively embeds routers in layers encoding classifiable security features, and activates backbone modules during optimization for better adaptability.

Result: MoGU_v2 provides stable improvements across multiple LLM scenarios, including mainstream, on-device, and reasoning-focused models. It also successfully restores security when dealing with risks introduced by Instruction Fine-tuning, all without compromising task performance.

Conclusion: The MoGU_v2 framework is presented as a robust, adaptable, and effective solution for enhancing LLM security while maintaining usability in a variety of real-world contexts.

Abstract: As Large Language Models (LLMs) increasingly permeate human life, their
security has emerged as a critical concern, particularly their ability to
maintain harmless responses to malicious instructions. Although extensive
methods have improved LLMs' security, they often lead to conservative,
rejection-oriented responses that compromise practical usability. This presents
a key challenge: how to advance the Pareto frontier between LLMs' usability and
security, rather than necessitate a trade-off between them. To address this, we
propose the MoGU framework, in which the intra-layer router dynamically
allocates weights by sensing hidden states, thereby balancing the contributions
of security-optimized and usability-optimized variants. Despite its initial
potential, the MoGU framework faces limitations such as parameter redundancy
and performance bottlenecks. To overcome these, we further propose an improved
MoGU_v2 framework that establishes a tighter coupling between the routers and
hidden states. In MoGU_v2, routers are embedded only in layers encoding highly
classifiable security features, and backbone modules are activated during
router optimization to enable bidirectional adaptation. MoGU_V2 exhibits strong
adaptability and stable improvements across various series of LLMs, including
mainstream LLMs serving as brains in various applications, on-device LLMs
optimized for resource-constrained scenarios, and reasoning LLMs tailored for
user interpretability. Meanwhile, even facing risks introduced by Instruction
Fine-tuning, MoGU_v2 can easily restore security without compromising the task
performance gains via a simple data-mix strategy. These comprehensive
improvements highlight MoGU_V2 as a robust and versatile solution for
mitigating security risks in real-world applications.

</details>


### [113] [Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem](https://arxiv.org/abs/2509.06809)
*Valentin Quesnel,Damien Sileo*

Main category: cs.CL

TL;DR: The paper proposes a scalable framework for generating high-quality mathematical reasoning data for Large Language Models (LLMs), using E-prover to ensure logical validity without relying on error-prone LLMs or complex syntax.


<details>
  <summary>Details</summary>
Motivation: There is a scarcity of high-quality and logically sound data needed to advance the mathematical reasoning capabilities of LLMs.

Method: The proposed framework uses E-prover's saturation feature on the TPTP axiom library to generate a corpus of guaranteed-valid theorems, then filters and converts them into three types of reasoning challenges: entailment verification, premise selection, and proof reconstruction.

Result: Zero-shot experiments on advanced models reveal that they perform poorly on tasks requiring deep structural reasoning, highlighting the gap in LLM capabilities.

Conclusion: The framework offers a diagnostic tool for measuring weaknesses in LLM reasoning and a scalable way to generate symbolic data for training them. The authors provide publicly available data and code to support further research.

Abstract: The scarcity of high-quality, logically sound data is a critical bottleneck
for advancing the mathematical reasoning of Large Language Models (LLMs). Our
work confronts this challenge by turning decades of automated theorem proving
research into a scalable data engine. Rather than relying on error-prone LLMs
or complex proof-assistant syntax like Lean and Isabelle, our framework
leverages E-prover's saturation capabilities on the vast TPTP axiom library to
derive a massive, guaranteed-valid corpus of theorems. Our pipeline is
principled and simple: saturate axioms, filter for "interesting" theorems, and
generate tasks. With no LLMs in the loop, we eliminate factual errors by
construction. This purely symbolic data is then transformed into three
difficulty-controlled challenges: entailment verification, premise selection,
and proof reconstruction. Our zero-shot experiments on frontier models reveal a
clear weakness: performance collapses on tasks requiring deep, structural
reasoning. Our framework provides both the diagnostic tool to measure this gap
and a scalable source of symbolic training data to address it. We make the code
and data publicly available.
  https://github.com/sileod/reasoning_core
https://hf.co/datasets/reasoning-core/rc1

</details>


### [114] [A Comparative Benchmark of Large Language Models for Labelling Wind Turbine Maintenance Logs](https://arxiv.org/abs/2509.06813)
*Max Malyi,Jonathan Shek,Alasdair McDonald,Andre Biscaya*

Main category: cs.CL

TL;DR: The paper proposes a framework for evaluating Large Language Models (LLMs) in classifying wind turbine maintenance logs, a task hindered by their unstructured nature. Results show model performance varies with task ambiguity, suggesting human-assisted LLM use is ideal.


<details>
  <summary>Details</summary>
Motivation: Reducing the Levelised Cost of Energy (LCOE) from wind power is crucial, but analyzing turbine maintenance logs is challenging due to their unstructured, free-text format.

Method: The authors developed an open-source, reproducible benchmarking framework for evaluating various proprietary and open-source LLMs in classifying industrial maintenance records.

Result: Findings reveal a performance hierarchy among LLMs, with better alignment in objective tasks than interpretive ones. Calibration and accuracy remain inconsistent across models.

Conclusion: While no LLM achieves perfect accuracy, integrating them in Human-in-the-Loop systems can assist human experts, improving O&M data labeling, enhancing data quality, and reliability analysis.

Abstract: Effective Operation and Maintenance (O&M) is critical to reducing the
Levelised Cost of Energy (LCOE) from wind power, yet the unstructured,
free-text nature of turbine maintenance logs presents a significant barrier to
automated analysis. Our paper addresses this by presenting a novel and
reproducible framework for benchmarking Large Language Models (LLMs) on the
task of classifying these complex industrial records. To promote transparency
and encourage further research, this framework has been made publicly available
as an open-source tool. We systematically evaluate a diverse suite of
state-of-the-art proprietary and open-source LLMs, providing a foundational
assessment of their trade-offs in reliability, operational efficiency, and
model calibration. Our results quantify a clear performance hierarchy,
identifying top models that exhibit high alignment with a benchmark standard
and trustworthy, well-calibrated confidence scores. We also demonstrate that
classification performance is highly dependent on the task's semantic
ambiguity, with all models showing higher consensus on objective component
identification than on interpretive maintenance actions. Given that no model
achieves perfect accuracy and that calibration varies dramatically, we conclude
that the most effective and responsible near-term application is a
Human-in-the-Loop system, where LLMs act as a powerful assistant to accelerate
and standardise data labelling for human experts, thereby enhancing O&M data
quality and downstream reliability analysis.

</details>


### [115] [COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens](https://arxiv.org/abs/2509.06836)
*Eugene Kwek,Wenpeng Yin*

Main category: cs.CL

TL;DR: The paper introduces COMPACT, a novel method for improving the efficiency of large language models (LLMs) by pruning rarely used vocabulary and FFN channels to maintain performance while reducing memory, latency, and costs.


<details>
  <summary>Details</summary>
Motivation: The study aims to make LLMs more memory- and cost-efficient for edge deployment, interactive applications, and large-scale sustainable inference, addressing limitations of existing pruning methods.

Method: COMPACT prunes rare vocabulary to shrink embeddings and unembedding matrices and optimally prunes FFN intermediate channels by aligning pruning importance with token distribution, all while preserving a transformer-friendly layout.

Result: COMPACT demonstrated state-of-the-art performance on Qwen, LLaMA, and Gemma models spanning 0.5B to 70B parameters, achieving high pruning ratios with significant reductions in memory usage, latency, and serving costs.

Conclusion: COMPACT offers a training-free, scalable, and deployment-friendly approach, achieving competitive or superior pruning outcomes without compromising downstream task accuracy in large-scale LLMs.

Abstract: Making LLMs more efficient in memory, latency, and serving cost is crucial
for edge deployment, interactive applications, and sustainable inference at
scale. Pruning is a key technique toward this goal. However, prior pruning
methods are limited: width pruning often breaks the standard transformer layout
or requires custom inference code, while depth pruning removes entire layers
and can cause abrupt accuracy drops. In this work, we propose COMPACT, which
jointly (i) prunes rare vocabulary to shrink embedding/unembedding and (ii)
prunes FFN intermediate channels using common-token-weighted activations,
aligning importance with the post-pruning token distribution. COMPACT enjoys
merits of both depth and width pruning, such as: deployment-friendliness (keeps
a standard transformer architecture), scale-adaptivity (trade off vocab vs. FFN
pruning), training-free operation with competitive pruning time, and strong
memory savings alongside throughput gains. Experiments across Qwen, LLaMA, and
Gemma families (0.5B-70B) show state-of-the-art downstream task performance at
similar or higher pruning ratios, with substantial reductions in parameters,
GPU memory, and end-to-end latency.

</details>


### [116] [EPT Benchmark: Evaluation of Persian Trustworthiness in Large Language Models](https://arxiv.org/abs/2509.06838)
*Mohammad Reza Mirbagheri,Mohammad Mahdi Mirkamali,Zahra Motoshaker Arani,Ali Javeri,Amir Mahdi Sadeghzadeh,Rasool Jalili*

Main category: cs.CL

TL;DR: This paper introduces EPT, a Persian-specific metric to assess LLM trustworthiness across six dimensions and evaluates top models, revealing deficiencies in safety.


<details>
  <summary>Details</summary>
Motivation: To address the critical challenge of ensuring LLM trustworthiness, especially in alignment with cultural and ethical values.

Method: Developed the EPT metric, curated a labeled Persian dataset, and assessed models using LLM-based and human evaluations across six dimensions.

Result: The study found significant shortcomings in the safety aspect of leading LLMs and provided insights into their alignment with Persian cultural values.

Conclusion: Critical gaps in trustworthiness and cultural alignment exist in LLMs, presenting opportunities to improve their alignment with ethical and cultural standards.

Abstract: Large Language Models (LLMs), trained on extensive datasets using advanced
deep learning architectures, have demonstrated remarkable performance across a
wide range of language tasks, becoming a cornerstone of modern AI technologies.
However, ensuring their trustworthiness remains a critical challenge, as
reliability is essential not only for accurate performance but also for
upholding ethical, cultural, and social values. Careful alignment of training
data and culturally grounded evaluation criteria are vital for developing
responsible AI systems. In this study, we introduce the EPT (Evaluation of
Persian Trustworthiness) metric, a culturally informed benchmark specifically
designed to assess the trustworthiness of LLMs across six key aspects:
truthfulness, safety, fairness, robustness, privacy, and ethical alignment. We
curated a labeled dataset and evaluated the performance of several leading
models - including ChatGPT, Claude, DeepSeek, Gemini, Grok, LLaMA, Mistral, and
Qwen - using both automated LLM-based and human assessments. Our results reveal
significant deficiencies in the safety dimension, underscoring the urgent need
for focused attention on this critical aspect of model behavior. Furthermore,
our findings offer valuable insights into the alignment of these models with
Persian ethical-cultural values and highlight critical gaps and opportunities
for advancing trustworthy and culturally responsible AI. The dataset is
publicly available at: https://github.com/Rezamirbagheri110/EPT-Benchmark.

</details>


### [117] [The Majority is not always right: RL training for solution aggregation](https://arxiv.org/abs/2509.06870)
*Wenting Zhao,Pranjal Aggarwal,Swarnadeep Saha,Asli Celikyilmaz,Jason Weston,Ilia Kulikov*

Main category: cs.CL

TL;DR: The paper introduces a method for aggregating multiple solutions to reasoning tasks using reinforcement learning, termed AggLM, which outperforms existing majority voting and reward-based methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for combining multiple solutions to reasoning tasks, like majority voting or reward model rankings, have limited effectiveness.

Method: The proposal involves an aggregator model using reinforcement learning. It is trained on verifiable rewards with balanced examples to reconcile and synthesize a final answer.

Result: AggLM outperformed rule-based and reward-model-based baselines on various benchmarks, working effectively across solutions from different models and requiring fewer tokens.

Conclusion: Learning aggregation as a reasoning skill presents a more effective method for improving performance on reasoning tasks, especially under constraints like fewer tokens used.

Abstract: Scaling up test-time compute, by generating multiple independent solutions
and selecting or aggregating among them, has become a central paradigm for
improving large language models (LLMs) on challenging reasoning tasks. While
most prior work relies on simple majority voting or reward model ranking to
aggregate solutions, these approaches may only yield limited benefits. In this
work, we propose to learn aggregation as an explicit reasoning skill: given a
set of candidate solutions, we train an aggregator model to review, reconcile,
and synthesize a final, correct answer using reinforcement learning from
verifiable rewards. A key ingredient is careful balancing of easy and hard
training examples, allowing the model to learn both to recover
minority-but-correct answers as well as easy majority-correct answers.
Empirically, we find our method, AggLM, outperforms both strong rule-based and
reward-model baselines, across multiple benchmarks. Furthermore, it generalizes
effectively to solutions from differing models, including stronger ones than
contained in the training data, all while requiring substantially fewer tokens
than majority voting with larger numbers of solutions.

</details>


### [118] [UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction](https://arxiv.org/abs/2509.06883)
*Joe Wilder,Nikhil Kadapala,Benji Xu,Mohammed Alsaadi,Aiden Parsons,Mitchell Rogers,Palash Agarwal,Adam Hassick,Laura Dietz*

Main category: cs.CL

TL;DR: The paper explores different approaches to extract check-worthy claims from social media using various prompting and LLM techniques. Best results are obtained using FLAN-T5 fine-tuning, but some methods produce higher-quality claims despite scoring lower.


<details>
  <summary>Details</summary>
Motivation: To improve the extraction of check-worthy claims from social media passages for better fact-checking.

Method: Exploring few-shot prompting, fine-tuning with different LLMs, and evaluating using METEOR score.

Result: Best METEOR score achieved with fine-tuned FLAN-T5. However, other methods sometimes provide better qualitative results.

Conclusion: Different approaches to LLM utilization trade-off between METEOR scores and qualitative claim quality. Fine-tuned FLAN-T5 offers the best quantitative performance, but careful consideration of other methods is valuable.

Abstract: We participate in CheckThat! Task 2 English and explore various methods of
prompting and in-context learning, including few-shot prompting and fine-tuning
with different LLM families, with the goal of extracting check-worthy claims
from social media passages. Our best METEOR score is achieved by fine-tuning a
FLAN-T5 model. However, we observe that higher-quality claims can sometimes be
extracted using other methods, even when their METEOR scores are lower.

</details>


### [119] [mmBERT: A Modern Multilingual Encoder with Annealed Language Learning](https://arxiv.org/abs/2509.06888)
*Marc Marone,Orion Weller,William Fleshman,Eugene Yang,Dawn Lawrie,Benjamin Van Durme*

Main category: cs.CL

TL;DR: The paper introduces mmBERT, a multilingual encoder-only language model that significantly improves classification and retrieval tasks, particularly for low-resource languages.


<details>
  <summary>Details</summary>
Motivation: To address the lack of recent advancements and research focus on multilingual encoder-only language models, particularly for low-resource languages.

Method: The authors pretrain mmBERT on 3T tokens across 1800 languages, introducing innovations like an inverse mask ratio schedule and inverse temperature sampling ratio. They strategically add low-resource languages during a decay phase to maximize performance improvements.

Result: mmBERT achieves comparable results to prominent models like OpenAI's o3 and Google's Gemini 2.5 Pro in classification while outperforming previous-generation multilingual models on classification and retrieval tasks. It also demonstrates significant improvements in low-resource language tasks.

Conclusion: The study proposes an effective strategy for multilingual language model training by incorporating novel methods to optimize performance, particularly for low-resource languages.

Abstract: Encoder-only languages models are frequently used for a variety of standard
machine learning tasks, including classification and retrieval. However, there
has been a lack of recent research for encoder models, especially with respect
to multilingual models. We introduce mmBERT, an encoder-only language model
pretrained on 3T tokens of multilingual text in over 1800 languages. To build
mmBERT we introduce several novel elements, including an inverse mask ratio
schedule and an inverse temperature sampling ratio. We add over 1700
low-resource languages to the data mix only during the decay phase, showing
that it boosts performance dramatically and maximizes the gains from the
relatively small amount of training data. Despite only including these
low-resource languages in the short decay phase we achieve similar
classification performance to models like OpenAI's o3 and Google's Gemini 2.5
Pro. Overall, we show that mmBERT significantly outperforms the previous
generation of models on classification and retrieval tasks -- on both high and
low-resource languages.

</details>


### [120] [Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers from LLMs via Claim Verification](https://arxiv.org/abs/2509.06902)
*Aivin V. Solatorio*

Main category: cs.CL

TL;DR: The paper introduces Proof-Carrying Numbers (PCN), a system ensuring numeric fidelity by verifying presented numbers through structured claims and policies.


<details>
  <summary>Details</summary>
Motivation: There is a growing problem of numeric hallucination in large language models (LLMs), where generated numbers deviate from factual data, undermining trust in numeric output.

Method: The authors propose Proof-Carrying Numbers (PCN), a presentation-layer protocol that ties numeric outputs to structured claims and enforces their verification via mechanical checks through predefined policies.

Result: PCN is demonstrated to be lightweight, model-agnostic, and capable of ensuring numeric fidelity by marking verified numbers and treating others with uncertainty. It supports sound validation and fail-safe behavior.

Conclusion: By enforcing mandatory verification of numeric claims before displaying output, PCN provides a reliable mechanism for ensuring numeric accuracy in LLM-generated content, promoting trust through provable correctness.

Abstract: Large Language Models (LLMs) as stochastic systems may generate numbers that
deviate from available data, a failure known as \emph{numeric hallucination}.
Existing safeguards -- retrieval-augmented generation, citations, and
uncertainty estimation -- improve transparency but cannot guarantee fidelity:
fabricated or misquoted values may still be displayed as if correct. We propose
\textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that
enforces numeric fidelity through mechanical verification. Under PCN, numeric
spans are emitted as \emph{claim-bound tokens} tied to structured claims, and a
verifier checks each token under a declared policy (e.g., exact equality,
rounding, aliases, or tolerance with qualifiers). Crucially, PCN places
verification in the \emph{renderer}, not the model: only claim-checked numbers
are marked as verified, and all others default to unverified. This separation
prevents spoofing and guarantees fail-closed behavior. We formalize PCN and
prove soundness, completeness under honest tokens, fail-closed behavior, and
monotonicity under policy refinement. PCN is lightweight and model-agnostic,
integrates seamlessly into existing applications, and can be extended with
cryptographic commitments. By enforcing verification as a mandatory step before
display, PCN establishes a simple contract for numerically sensitive settings:
\emph{trust is earned only by proof}, while the absence of a mark communicates
uncertainty.

</details>


### [121] [Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning](https://arxiv.org/abs/2509.06948)
*Liang Chen,Xueting Han,Li Shen,Jing Bai,Kam-Fai Wong*

Main category: cs.CL

TL;DR: The paper addresses the inefficiency of traditional reinforcement learning (RL) approaches for training large language models (LLMs) and proposes a bilevel optimization method to better integrate supervised fine-tuning (SFT) with RL. This improves both effectiveness and efficiency in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome efficiency challenges and enhance reasoning performance in large language models (LLMs) by addressing the limitations of the traditional two-stage SFT and RL approach.

Method: The proposed method uses bilevel optimization, where the SFT objective is conditioned on the optimal RL policy. SFT learns to guide RL's optimization process in a cooperative training setup, with RL updates and SFT supervision integrated during training.

Result: The method achieves superior performance on five reasoning benchmarks compared to traditional RL and SFT approaches, showcasing improved balance between effectiveness and efficiency.

Conclusion: The study proves that a cooperative bilevel optimization framework can effectively integrate SFT and RL, leading to better reasoning capabilities in LLMs with fewer inefficiencies.

Abstract: Reinforcement learning (RL) has proven effective in incentivizing the
reasoning abilities of large language models (LLMs), but suffers from severe
efficiency challenges due to its trial-and-error nature. While the common
practice employs supervised fine-tuning (SFT) as a warm-up stage for RL, this
decoupled two-stage approach limits interaction between SFT and RL, thereby
constraining overall effectiveness. This study introduces a novel method for
learning reasoning models that employs bilevel optimization to facilitate
better cooperation between these training paradigms. By conditioning the SFT
objective on the optimal RL policy, our approach enables SFT to meta-learn how
to guide RL's optimization process. During training, the lower level performs
RL updates while simultaneously receiving SFT supervision, and the upper level
explicitly maximizes the cooperative gain-the performance advantage of joint
SFT-RL training over RL alone. Empirical evaluations on five reasoning
benchmarks demonstrate that our method consistently outperforms baselines and
achieves a better balance between effectiveness and efficiency.

</details>


### [122] [Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models](https://arxiv.org/abs/2509.06949)
*Yinjie Wang,Ling Yang,Bowen Li,Ye Tian,Ke Shen,Mengdi Wang*

Main category: cs.CL

TL;DR: The paper presents TraceRL, a new reinforcement learning framework tailored for Diffusion Language Models (DLMs), optimizing inference trajectory during post-training for enhanced performance, especially in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the need to improve reasoning accuracy in complex tasks like mathematics and coding, and adapt diffusion-based models for broader applicability and better sampling flexibility.

Method: The method involves developing TraceRL, which incorporates preferred inference trajectories into post-training using a diffusion-based value model. It also employs curriculum learning strategies and open-source tools to enable reproducible research.

Result: TraceRL-derived models, specifically TraDo models, demonstrate superior performance in math reasoning benchmarks, achieving significant accuracy improvements over leading models like Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct.

Conclusion: The effectiveness of TraceRL underscores the potential of trajectory-aware RL frameworks in advancing DLMs' capabilities, with open-source access enabling broader applications and further research.

Abstract: We propose TraceRL, a trajectory-aware reinforcement learning framework for
diffusion language models (DLMs) that incorporates preferred inference
trajectory into post-training, and is applicable across different
architectures. Equipped with a diffusion-based value model that enhances
training stability, we demonstrate improved reasoning performance on complex
math and coding tasks. Besides, it can also be applied to adapt block-specific
models to larger blocks, which improves sampling flexibility. Employing
TraceRL, we derive a series of state-of-the-art diffusion language models,
namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still
consistently outperforms them across complex math reasoning tasks.
TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over
Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical
reasoning benchmarks. Through curriculum learning, we also derive the first
long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%
relative accuracy gain. To facilitate reproducible research and practical
applications, we release a comprehensive open-source framework for building,
training, and deploying diffusion LLMs across diverse architectures. The
framework integrates accelerated KV-cache techniques and inference engines for
both inference and reinforcement learning, and includes implementations of
various supervised fine-tuning and RL methods for mathematics, coding, and
general tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL

</details>


### [123] [On the Same Wavelength? Evaluating Pragmatic Reasoning in Language Models across Broad Concepts](https://arxiv.org/abs/2509.06952)
*Linlu Qiu,Cedegao E. Zhang,Joshua B. Tenenbaum,Yoon Kim,Roger P. Levy*

Main category: cs.CL

TL;DR: This paper evaluates language models' pragmatic reasoning abilities using a communication game framework, highlighting performance differences based on model size and inference strategies.


<details>
  <summary>Details</summary>
Motivation: To assess the capabilities of language models in pragmatic reasoning, conversational contexts, and their alignment with human language processing.

Method: Evaluation of language models using a framework derived from the communication game Wavelength, employing direct and Chain-of-Thought (CoT) prompting alongside Bayesian pragmatic reasoning (RSA).

Result: State-of-the-art language models excel in language comprehension with human-like accuracy, while RSA and CoT prompting boost performance in language production tasks.

Conclusion: LMs display strong pragmatic reasoning abilities at larger scales, with RSA methodology showing promise for enhancing their inference strategies and understanding contextual communication norms further.

Abstract: Language use is shaped by pragmatics -- i.e., reasoning about communicative
goals and norms in context. As language models (LMs) are increasingly used as
conversational agents, it becomes ever more important to understand their
pragmatic reasoning abilities. We propose an evaluation framework derived from
Wavelength, a popular communication game where a speaker and a listener
communicate about a broad range of concepts in a granular manner. We study a
range of LMs on both language comprehension and language production using
direct and Chain-of-Thought (CoT) prompting, and further explore a Rational
Speech Act (RSA) approach to incorporating Bayesian pragmatic reasoning into LM
inference. We find that state-of-the-art LMs, but not smaller ones, achieve
strong performance on language comprehension, obtaining similar-to-human
accuracy and exhibiting high correlations with human judgments even without CoT
prompting or RSA. On language production, CoT can outperform direct prompting,
and using RSA provides significant improvements over both approaches. Our study
helps identify the strengths and limitations in LMs' pragmatic reasoning
abilities and demonstrates the potential for improving them with RSA, opening
up future avenues for understanding conceptual representation, language
understanding, and social reasoning in LMs and humans.

</details>


### [124] [Multi-EuP: The Multilingual European Parliament Dataset for Analysis of Bias in Information Retrieval](https://arxiv.org/abs/2311.01870)
*Jinrui Yang,Timothy Baldwin,Trevor Cohn*

Main category: cs.CL

TL;DR: Multi-EuP is a multilingual benchmark dataset of 22K documents in 24 languages, designed to study fairness in multilingual information retrieval (IR), including issues of language and demographic bias.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address fairness in multilingual IR by providing a dataset to analyze language and demographic bias in document ranking.

Method: The authors introduce Multi-EuP, a dataset with multilingual and cross-lingual features, including translated topics and relevance judgments, along with demographic data enabling bias analysis.

Result: Multi-EuP is demonstrated to be effective for monolingual and multilingual IR benchmarking. A preliminary experiment on tokenization strategy showed its effect on language bias.

Conclusion: Multi-EuP serves as a valuable resource for investigating fairness in IR by enabling bias analysis across languages and demographics, and its utility is demonstrated in an experimental setup.

Abstract: We present Multi-EuP, a new multilingual benchmark dataset, comprising 22K
multi-lingual documents collected from the European Parliament, spanning 24
languages. This dataset is designed to investigate fairness in a multilingual
information retrieval (IR) context to analyze both language and demographic
bias in a ranking context. It boasts an authentic multilingual corpus,
featuring topics translated into all 24 languages, as well as cross-lingual
relevance judgments. Furthermore, it offers rich demographic information
associated with its documents, facilitating the study of demographic bias. We
report the effectiveness of Multi-EuP for benchmarking both monolingual and
multilingual IR. We also conduct a preliminary experiment on language bias
caused by the choice of tokenization strategy.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [125] [Label Smoothing++: Enhanced Label Regularization for Training Neural Networks](https://arxiv.org/abs/2509.05307)
*Sachin Chhabra,Hemanth Venkateswara,Baoxin Li*

Main category: cs.CV

TL;DR: The paper introduces Label Smoothing++, a method improving label smoothing by considering inter-class relationships, offering better generalization and less overconfidence.


<details>
  <summary>Details</summary>
Motivation: Training neural networks with one-hot labels often creates overfitting and overconfident predictions. Current label smoothing solutions fail to consider inter-class relationships.

Method: Label Smoothing++ assigns non-zero probabilities to non-target classes, reflecting inter-class relationships, while keeping target class labels fixed. Non-target labels are learned during training.

Result: Extensive experiments across multiple datasets show that Label Smoothing++ reduces overconfidence, accounts for inter-class relationships, and enhances generalization.

Conclusion: Label Smoothing++ effectively mitigates overconfidence issues in neural networks training while preserving and enhancing inter-class relationships, leading to improved model performance.

Abstract: Training neural networks with one-hot target labels often results in
overconfidence and overfitting. Label smoothing addresses this issue by
perturbing the one-hot target labels by adding a uniform probability vector to
create a regularized label. Although label smoothing improves the network's
generalization ability, it assigns equal importance to all the non-target
classes, which destroys the inter-class relationships. In this paper, we
propose a novel label regularization training strategy called Label
Smoothing++, which assigns non-zero probabilities to non-target classes and
accounts for their inter-class relationships. Our approach uses a fixed label
for the target class while enabling the network to learn the labels associated
with non-target classes. Through extensive experiments on multiple datasets, we
demonstrate how Label Smoothing++ mitigates overconfident predictions while
promoting inter-class relationships and generalization capabilities.

</details>


### [126] [Approximating Condorcet Ordering for Vector-valued Mathematical Morphology](https://arxiv.org/abs/2509.06577)
*Marcos Eduardo Valle,Santiago Velasco-Forero,Joao Batista Florindo,Gustavo Jesus Angulo*

Main category: cs.CV

TL;DR: This paper explores the use of machine learning to develop a reduced ordering for vector-valued morphological operators, approximating the Condorcet ranking derived from multiple orderings.


<details>
  <summary>Details</summary>
Motivation: Overcoming the lack of consensus on vector ordering methods in mathematical morphology for vector-valued images, essential for effective image analysis.

Method: A machine learning approach is utilized to learn reduced orderings that approximate the Condorcet ranking, inspired by voting problems.

Result: Preliminary computational experiments show the effectiveness of the proposed method in defining vector-valued morphological operators for color images.

Conclusion: The machine learning-driven reduced ordering offers a promising framework to improve vector-valued morphological image processing.

Abstract: Mathematical morphology provides a nonlinear framework for image and spatial
data processing and analysis. Although there have been many successful
applications of mathematical morphology to vector-valued images, such as color
and hyperspectral images, there is still no consensus on the most suitable
vector ordering for constructing morphological operators. This paper addresses
this issue by examining a reduced ordering approximating the Condorcet ranking
derived from a set of vector orderings. Inspired by voting problems, the
Condorcet ordering ranks elements from most to least voted, with voters
representing different orderings. In this paper, we develop a machine learning
approach that learns a reduced ordering that approximates the Condorcet
ordering. Preliminary computational experiments confirm the effectiveness of
learning the reduced mapping to define vector-valued morphological operators
for color images.

</details>


### [127] [VILOD: A Visual Interactive Labeling Tool for Object Detection](https://arxiv.org/abs/2509.05317)
*Isac Holm*

Main category: cs.CV

TL;DR: The paper introduces VILOD, a visual interactive labeling tool for object detection that leverages Human-in-the-Loop and visual analytics approaches to make object labeling processes more interpretable, transparent, and efficient.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the challenges associated with acquiring large, high-quality labeled datasets needed for object detection using deep learning, including high costs, time-consumption, and the limitations of existing Active Learning techniques.

Method: The authors developed VILOD, a tool that integrates t-SNE projections, uncertainty heatmaps, and model state visualizations, enabling users to explore data, interpret model states and Active Learning suggestions, and implement iterative Human-in-the-Loop workflows.

Result: The empirical study found that VILOD facilitates the adoption of different visually-guided labeling strategies and achieves competitive object detection performance compared to traditional Active Learning baselines.

Conclusion: VILOD contributes to improving HITL-AL workflows by offering a more transparent and manageable approach to dataset annotation, enhancing the human-AI collaboration process.

Abstract: The advancement of Object Detection (OD) using Deep Learning (DL) is often
hindered by the significant challenge of acquiring large, accurately labeled
datasets, a process that is time-consuming and expensive. While techniques like
Active Learning (AL) can reduce annotation effort by intelligently querying
informative samples, they often lack transparency, limit the strategic insight
of human experts, and may overlook informative samples not aligned with an
employed query strategy. To mitigate these issues, Human-in-the-Loop (HITL)
approaches integrating human intelligence and intuition throughout the machine
learning life-cycle have gained traction. Leveraging Visual Analytics (VA),
effective interfaces can be created to facilitate this human-AI collaboration.
This thesis explores the intersection of these fields by developing and
investigating "VILOD: A Visual Interactive Labeling tool for Object Detection".
VILOD utilizes components such as a t-SNE projection of image features,
together with uncertainty heatmaps and model state views. Enabling users to
explore data, interpret model states, AL suggestions, and implement diverse
sample selection strategies within an iterative HITL workflow for OD. An
empirical investigation using comparative use cases demonstrated how VILOD,
through its interactive visualizations, facilitates the implementation of
distinct labeling strategies by making the model's state and dataset
characteristics more interpretable (RQ1). The study showed that different
visually-guided labeling strategies employed within VILOD result in competitive
OD performance trajectories compared to an automated uncertainty sampling AL
baseline (RQ2). This work contributes a novel tool and empirical insight into
making the HITL-AL workflow for OD annotation more transparent, manageable, and
potentially more effective.

</details>


### [128] [Context-Aware Knowledge Distillation with Adaptive Weighting for Image Classification](https://arxiv.org/abs/2509.05319)
*Zhengda Li*

Main category: cs.CV

TL;DR: Knowledge Distillation traditionally uses a fixed alpha for balancing losses, but this paper introduces Adaptive Knowledge Distillation (AKD) with a learnable alpha and dynamic adjustments to improve training.


<details>
  <summary>Details</summary>
Motivation: Static alpha in Knowledge Distillation is suboptimal because the balance between hard-label and soft-label supervision changes during training, necessitating adaptive adjustments.

Method: The paper proposes AKD by making alpha a learnable parameter, introduces a dynamic formula based on student-teacher discrepancies, and incorporates a Context-Aware Module (MLP + Attention) to adaptively reweight teacher outputs.

Result: Experiments with CIFAR-10, ResNet-50 as a teacher, and ResNet-18 as a student show that AKD outperforms fixed-weight KD baselines in terms of accuracy and training stability.

Conclusion: Adaptive adjustments using AKD enhance Knowledge Distillation by optimizing the trade-off dynamically, improving performance and stability.

Abstract: Knowledge distillation (KD) is a widely used technique to transfer knowledge
from a large teacher network to a smaller student model. Traditional KD uses a
fixed balancing factor alpha as a hyperparameter to combine the hard-label
cross-entropy loss with the soft-label distillation loss. However, a static
alpha is suboptimal because the optimal trade-off between hard and soft
supervision can vary during training.
  In this work, we propose an Adaptive Knowledge Distillation (AKD) framework.
First we try to make alpha as learnable parameter that can be automatically
learned and optimized during training. Then we introduce a formula to reflect
the gap between the student and the teacher to compute alpha dynamically,
guided by student-teacher discrepancies, and further introduce a Context-Aware
Module (CAM) using MLP + Attention to adaptively reweight class-wise teacher
outputs. Experiments on CIFAR-10 with ResNet-50 as teacher and ResNet-18 as
student demonstrate that our approach achieves superior accuracy compared to
fixed-weight KD baselines, and yields more stable convergence.

</details>


### [129] [A Dataset Generation Scheme Based on Video2EEG-SPGN-Diffusion for SEED-VD](https://arxiv.org/abs/2509.05321)
*Yunfei Guo,Tao Zhang,Wu Huang,Yao Song*

Main category: cs.CV

TL;DR: The paper presents Video2EEG-SPGN-Diffusion, an open-source framework that generates multimodal EEG data conditioned on video stimuli using SEED-VD. It incorporates tools for video-EEG alignment, personalized EEG generation, and releases a paired video-EEG dataset.


<details>
  <summary>Details</summary>
Motivation: To address the lack of aligned multimodal datasets of EEG signals and video stimuli, thereby enabling advancements in emotion analysis and brain-computer interface research.

Method: The framework uses the SEED-VD dataset and generates personalized EEG signals by combining a self-play graph network (SPGN) with a diffusion model. It also introduces an engineering pipeline for video-EEG data alignment.

Result: The released dataset includes over 1000 samples of SEED-VD video stimuli paired with 62-channel EEG signals at 200 Hz and emotion labels, facilitating multimodal research and training.

Conclusion: The framework and dataset provide significant tools for emotion analysis, data augmentation, and brain-computer interfaces, contributing to advancements in multimodal and aligned video-EEG research.

Abstract: This paper introduces an open-source framework, Video2EEG-SPGN-Diffusion,
that leverages the SEED-VD dataset to generate a multimodal dataset of EEG
signals conditioned on video stimuli. Additionally, we disclose an engineering
pipeline for aligning video and EEG data pairs, facilitating the training of
multimodal large models with EEG alignment capabilities. Personalized EEG
signals are generated using a self-play graph network (SPGN) integrated with a
diffusion model. As a major contribution, we release a new dataset comprising
over 1000 samples of SEED-VD video stimuli paired with generated 62-channel EEG
signals at 200 Hz and emotion labels, enabling video-EEG alignment and
advancing multimodal research. This framework offers novel tools for emotion
analysis, data augmentation, and brain-computer interface applications, with
substantial research and engineering significance.

</details>


### [130] [Application of discrete Ricci curvature in pruning randomly wired neural networks: A case study with chest x-ray classification of COVID-19](https://arxiv.org/abs/2509.05322)
*Pavithra Elumalai,Sudharsan Vijayaraghavan,Madhumita Mondal,Areejit Samal*

Main category: cs.CV

TL;DR: The paper explores edge-centric measures like Forman-Ricci curvature (FRC), Ollivier-Ricci curvature (ORC), and edge betweenness centrality (EBC) to compress Randomly Wired Neural Networks (RWNNs) while retaining performance in COVID-19 chest x-ray image classification.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand how network topology influences learning efficiency and performance in deep learning, using edge-centric measures for better pruning and optimization in RWNNs.

Method: Three edge-centric measures (FRC, ORC, EBC) are tested for pruning RWNNs generated by three models (ER, WS, BA). The pruning's impact on compression ratio, speedup, and performance metrics like accuracy is evaluated, with a focus on balancing network complexity, modularity, and efficiency.

Result: FRC is shown to effectively prune RWNNs while maintaining performance comparable to ORC, offering computational advantages. Structural analysis details trade-offs in network modularity and efficiency post-pruning.

Conclusion: FRC-based pruning provides an efficient means to simplify RWNNs without significant performance loss, presenting a practical edge-centric approach for network compression.

Abstract: Randomly Wired Neural Networks (RWNNs) serve as a valuable testbed for
investigating the impact of network topology in deep learning by capturing how
different connectivity patterns impact both learning efficiency and model
performance. At the same time, they provide a natural framework for exploring
edge-centric network measures as tools for pruning and optimization. In this
study, we investigate three edge-centric network measures: Forman-Ricci
curvature (FRC), Ollivier-Ricci curvature (ORC), and edge betweenness
centrality (EBC), to compress RWNNs by selectively retaining important synapses
(or edges) while pruning the rest. As a baseline, RWNNs are trained for
COVID-19 chest x-ray image classification, aiming to reduce network complexity
while preserving performance in terms of accuracy, specificity, and
sensitivity. We extend prior work on pruning RWNN using ORC by incorporating
two additional edge-centric measures, FRC and EBC, across three network
generators: Erd\"{o}s-R\'{e}nyi (ER) model, Watts-Strogatz (WS) model, and
Barab\'{a}si-Albert (BA) model. We provide a comparative analysis of the
pruning performance of the three measures in terms of compression ratio and
theoretical speedup. A central focus of our study is to evaluate whether FRC,
which is computationally more efficient than ORC, can achieve comparable
pruning effectiveness. Along with performance evaluation, we further
investigate the structural properties of the pruned networks through modularity
and global efficiency, offering insights into the trade-off between modular
segregation and network efficiency in compressed RWNNs. Our results provide
initial evidence that FRC-based pruning can effectively simplify RWNNs,
offering significant computational advantages while maintaining performance
comparable to ORC.

</details>


### [131] [Optical Music Recognition of Jazz Lead Sheets](https://arxiv.org/abs/2509.05329)
*Juan Carlos Martinez-Sevilla,Francesco Foscarin,Patricia Garcia-Iasci,David Rizo,Jorge Calvo-Zaragoza,Gerhard Widmer*

Main category: cs.CV

TL;DR: The paper tackles Optical Music Recognition (OMR) for handwritten jazz lead sheets, providing a new dataset and developing a specialized OMR model.


<details>
  <summary>Details</summary>
Motivation: Existing OMR systems struggle with handwritten jazz lead sheets due to the presence of chords and variability in handwriting quality.

Method: The authors created a dataset of handwritten jazz lead sheets and synthetic score images, and designed a tailored OMR model utilizing tokenization techniques, pretrained models, and synthetic data.

Result: They produced a dataset of 293 handwritten jazz lead sheets, aligned with ground truth scores, and demonstrated the ability to recognize handwritten jazz scores effectively.

Conclusion: The paper advances OMR for jazz lead sheets, addressing previous limitations, while making all code, data, and models publicly accessible for further research.

Abstract: In this paper, we address the challenge of Optical Music Recognition (OMR)
for handwritten jazz lead sheets, a widely used musical score type that encodes
melody and chords. The task is challenging due to the presence of chords, a
score component not handled by existing OMR systems, and the high variability
and quality issues associated with handwritten images. Our contribution is
two-fold. We present a novel dataset consisting of 293 handwritten jazz lead
sheets of 163 unique pieces, amounting to 2021 total staves aligned with
Humdrum **kern and MusicXML ground truth scores. We also supply synthetic score
images generated from the ground truth. The second contribution is the
development of an OMR model for jazz lead sheets. We discuss specific
tokenisation choices related to our kind of data, and the advantages of using
synthetic scores and pretrained models. We publicly release all code, data, and
models.

</details>


### [132] [RT-VLM: Re-Thinking Vision Language Model with 4-Clues for Real-World Object Recognition Robustness](https://arxiv.org/abs/2509.05333)
*Junghyun Park,Tuan Anh Nguyen,Dugki Min*

Main category: cs.CV

TL;DR: This paper introduces the RT-VLM framework to tackle domain shift challenges in object recognition.


<details>
  <summary>Details</summary>
Motivation: Current object recognition models face a steep accuracy drop under domain shifts such as changes in image statistics, object poses, and visual confusion among classes.

Method: The RT-VLM framework builds upon a synthetic dataset generation pipeline annotated with '4-Clues' and tunes a vision-language model (Llama 3.2 11B) for parameter efficient supervised learning. At inference, it employs a two-stage process: generate clues and iteratively refine results.

Result: RT-VLM demonstrates superior performance over strong baselines on robustness benchmarks isolating domain shift factors.

Conclusion: The integration of multimodal evidence with a self-critical iterative loop offers a robust approach for reliable and transferable visual understanding.

Abstract: Real world deployments often expose modern object recognition models to
domain shifts that precipitate a severe drop in accuracy. Such shifts encompass
(i) variations in low level image statistics, (ii) changes in object pose and
viewpoint, (iii) partial occlusion, and (iv) visual confusion across adjacent
classes. To mitigate this degradation, we introduce the Re-Thinking Vision
Language Model (RT-VLM) framework. The foundation of this framework is a unique
synthetic dataset generation pipeline that produces images annotated with
"4-Clues": precise bounding boxes, class names, detailed object-level captions,
and a comprehensive context-level caption for the entire scene. We then perform
parameter efficient supervised tuning of Llama 3.2 11B Vision Instruct on this
resource. At inference time, a two stage Re-Thinking scheme is executed: the
model first emits its own four clues, then re examines these responses as
evidence and iteratively corrects them. Across robustness benchmarks that
isolate individual domain shifts, RT-VLM consistently surpasses strong
baselines. These findings indicate that the integration of structured
multimodal evidence with an explicit self critique loop constitutes a promising
route toward reliable and transferable visual understanding.

</details>


### [133] [A Real-Time, Vision-Based System for Badminton Smash Speed Estimation on Mobile Devices](https://arxiv.org/abs/2509.05334)
*Diwen Huang*

Main category: cs.CV

TL;DR: The paper introduces a cost-effective badminton smash speed measurement system using smartphone technology, employing YOLOv5 for detection and a Kalman filter for tracking.


<details>
  <summary>Details</summary>
Motivation: Current methods for measuring sports metrics like shot speed and angle are expensive and inaccessible to amateur players.

Method: The system uses a YOLOv5 model for shuttlecock detection, a Kalman filter for trajectory tracking, and a video-based kinematic speed estimation method packaged into a mobile app.

Result: The system allows players to measure smash speed by processing video recordings with spatiotemporal scaling.

Conclusion: This innovative solution democratizes access to performance analysis tools, enabling players to enhance their badminton gameplay.

Abstract: Performance metrics in sports, such as shot speed and angle, provide crucial
feedback for athlete development. However, the technology to capture these
metrics has historically been expensive, complex, and largely inaccessible to
amateur and recreational players. This paper addresses this gap in the context
of badminton, one of the world's most popular sports, by introducing a novel,
cost-effective, and user-friendly system for measuring smash speed using
ubiquitous smartphone technology. Our approach leverages a custom-trained
YOLOv5 model for shuttlecock detection, combined with a Kalman filter for
robust trajectory tracking. By implementing a video-based kinematic speed
estimation method with spatiotemporal scaling, the system automatically
calculates the shuttlecock's velocity from a standard video recording. The
entire process is packaged into an intuitive mobile application, democratizing
access to high-level performance analytics and empowering players at all levels
to analyze and improve their game.

</details>


### [134] [A Stroke-Level Large-Scale Database of Chinese Character Handwriting and the OpenHandWrite_Toolbox for Handwriting Research](https://arxiv.org/abs/2509.05335)
*Zebo Xu,Shaoyun Yu,Mark Torrance,Guido Nottbusch,Nan Zhao,Zhenguang Cai*

Main category: cs.CV

TL;DR: This paper explores how linguistic components like phonological, semantic, and orthographic systems influence Chinese handwriting across character, radical, and stroke levels. It introduces a large-scale handwriting database and upgraded tools for detailed handwriting analysis.


<details>
  <summary>Details</summary>
Motivation: To understand the interaction between linguistic components and Chinese handwriting at distinct hierarchical levels, and to address the absence of tools for processing detailed handwriting data.

Method: The authors created a handwriting database with 42 speakers writing 1200 characters each, used a handwriting-to-dictation task, upgraded the OpenHandWrite_Toolbox for batch processing, and analyzed data using multiple regression methods.

Result: The study found that orthographic predictors affected handwriting preparation and execution across all levels, while phonological factors influenced execution across all levels. Lexical effects showed hierarchical attenuation, decreasing from character to radical to stroke levels.

Conclusion: Handwriting at radical and stroke levels is strongly linked to linguistic components, and both the database and toolbox provide significant resources for advancing psycholinguistic and neurolinguistic handwriting research.

Abstract: Understanding what linguistic components (e.g., phonological, semantic, and
orthographic systems) modulate Chinese handwriting at the character, radical,
and stroke levels remains an important yet understudied topic. Additionally,
there is a lack of comprehensive tools for capturing and batch-processing
fine-grained handwriting data. To address these issues, we constructed a
large-scale handwriting database in which 42 Chinese speakers for each
handwriting 1200 characters in a handwriting-to-dictation task. Additionally,
we enhanced the existing handwriting package and provided comprehensive
documentation for the upgraded OpenHandWrite_Toolbox, which can easily modify
the experimental design, capture the stroke-level handwriting trajectory, and
batch-process handwriting measurements (e.g., latency, duration, and
pen-pressure). In analysing our large-scale database, multiple regression
results show that orthographic predictors impact handwriting preparation and
execution across character, radical, and stroke levels. Phonological factors
also influence execution at all three levels. Importantly, these lexical
effects demonstrate hierarchical attenuation - they were most pronounced at the
character level, followed by the radical, and were weakest at the stroke
levels. These findings demonstrate that handwriting preparation and execution
at the radical and stroke levels are closely intertwined with linguistic
components. This database and toolbox offer valuable resources for future
psycholinguistic and neurolinguistic research on the handwriting of characters
and sub-characters across different languages.

</details>


### [135] [BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ Bioprinting Monitoring](https://arxiv.org/abs/2509.06690)
*Usman Haider,Lukasz Szemet,Daniel Kelly,Vasileios Sergis,Andrew C. Daly,Karl Mason*

Main category: cs.CV

TL;DR: This paper introduces BioLite U-Net, a lightweight semantic segmentation framework optimized for real-time bioprinting, achieving high accuracy and fast processing on resource-constrained hardware.


<details>
  <summary>Details</summary>
Motivation: To address challenges in ensuring real-time fidelity and consistency during the bioprinting process, under constraints of limited imaging data and resource-constrained embedded hardware.

Method: The researchers developed a manually annotated dataset of 787 RGB images and proposed the BioLite U-Net architecture, employing depthwise separable convolutions for efficient, low-computation semantic segmentation. Comparative benchmarks were performed against MobileNet-based models.

Result: BioLite U-Net achieved an mIoU of 92.85%, Dice score of 96.17%, and on-device inference time of 335 ms per frame on a Raspberry Pi 4B, making it 1300x smaller than MobileNetV2-DeepLabV3+.

Conclusion: The BioLite U-Net framework successfully balances segmentation accuracy, computational efficiency, and deployability, making it highly viable for integration into real-time, intelligent bioprinting systems.

Abstract: Bioprinting is a rapidly advancing field that offers a transformative
approach to fabricating tissue and organ models through the precise deposition
of cell-laden bioinks. Ensuring the fidelity and consistency of printed
structures in real-time remains a core challenge, particularly under
constraints imposed by limited imaging data and resource-constrained embedded
hardware. Semantic segmentation of the extrusion process, differentiating
between nozzle, extruded bioink, and surrounding background, enables in situ
monitoring critical to maintaining print quality and biological viability. In
this work, we introduce a lightweight semantic segmentation framework tailored
for real-time bioprinting applications. We present a novel, manually annotated
dataset comprising 787 RGB images captured during the bioprinting process,
labeled across three classes: nozzle, bioink, and background. To achieve fast
and efficient inference suitable for integration with bioprinting systems, we
propose a BioLite U-Net architecture that leverages depthwise separable
convolutions to drastically reduce computational load without compromising
accuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based
segmentation baselines using mean Intersection over Union (mIoU), Dice score,
and pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess
real-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85%
and a Dice score of 96.17%, while being over 1300x smaller than
MobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame,
demonstrating near real-time capability. Compared to MobileNet baselines,
BioLite U-Net offers a superior tradeoff between segmentation accuracy,
efficiency, and deployability, making it highly suitable for intelligent,
closed-loop bioprinting systems.

</details>


### [136] [Anticipatory Fall Detection in Humans with Hybrid Directed Graph Neural Networks and Long Short-Term Memory](https://arxiv.org/abs/2509.05337)
*Younggeol Cho,Gokhan Solak,Olivia Nocentini,Marta Lorenzini,Andrea Fortuna,Arash Ajoudani*

Main category: cs.CV

TL;DR: The paper proposes a fall prediction method using a hybrid model combining Dynamic Graph Neural Networks (DGNN) and Long Short-Term Memory (LSTM) networks to anticipate falls with high accuracy by analyzing human gait.


<details>
  <summary>Details</summary>
Motivation: Falls pose a critical risk in assistive robotic systems, and while detection has seen progress, there is a gap in predicting falls before they occur and analyzing the transient state of instability.

Method: The method utilizes a hybrid DGNN-LSTM model, with DGNN classifying gait states (stable, transient, fall) and LSTM predicting human movement over time. Skeletal features from video sequences are used as input.

Result: Using OUMVLP-Pose and URFD datasets, the proposed model outperformed existing DGNN-centric models and those from the literature, showing lower prediction error and higher accuracy due to separating prediction and classification tasks.

Conclusion: Decoupling motion prediction and gait classification improves the prediction performance. The method also provides valuable insights into the transient state, enhancing assistive robotic systems.

Abstract: Detecting and preventing falls in humans is a critical component of assistive
robotic systems. While significant progress has been made in detecting falls,
the prediction of falls before they happen, and analysis of the transient state
between stability and an impending fall remain unexplored. In this paper, we
propose a anticipatory fall detection method that utilizes a hybrid model
combining Dynamic Graph Neural Networks (DGNN) with Long Short-Term Memory
(LSTM) networks that decoupled the motion prediction and gait classification
tasks to anticipate falls with high accuracy. Our approach employs real-time
skeletal features extracted from video sequences as input for the proposed
model. The DGNN acts as a classifier, distinguishing between three gait states:
stable, transient, and fall. The LSTM-based network then predicts human
movement in subsequent time steps, enabling early detection of falls. The
proposed model was trained and validated using the OUMVLP-Pose and URFD
datasets, demonstrating superior performance in terms of prediction error and
recognition accuracy compared to models relying solely on DGNN and models from
literature. The results indicate that decoupling prediction and classification
improves performance compared to addressing the unified problem using only the
DGNN. Furthermore, our method allows for the monitoring of the transient state,
offering valuable insights that could enhance the functionality of advanced
assistance systems.

</details>


### [137] [Comparative Evaluation of Hard and Soft Clustering for Precise Brain Tumor Segmentation in MR Imaging](https://arxiv.org/abs/2509.05340)
*Dibya Jyoti Bora,Mrinal Kanti Mishra*

Main category: cs.CV

TL;DR: This paper compares two clustering methods, K-Means and Fuzzy C-Means (FCM), for brain tumor MRI segmentation, highlighting FCM's higher accuracy but slower processing speed.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurately segmenting brain tumors from MRI scans due to their varying shape, size, and intensity patterns, which is crucial for clinical applications like treatment planning.

Method: The authors evaluated hard clustering (K-Means) and soft clustering (FCM) methods using the BraTS2020 dataset, employing Gaussian filtering and CLAHE for pre-processing, and validating results with metrics such as Dice Similarity Coefficient and runtime.

Result: K-Means achieved fast processing speeds (0.3s per image) but had lower segmentation accuracy (DSC of 0.43), whereas FCM delivered higher accuracy (DSC of 0.67) at a slower speed (1.3s per image).

Conclusion: There is a trade-off between computational speed and segmentation precision, with K-Means being faster and FCM providing better accuracy for brain tumor MRI segmentation.

Abstract: Segmentation of brain tumors from Magnetic Resonance Imaging (MRI) remains a
pivotal challenge in medical image analysis due to the heterogeneous nature of
tumor morphology and intensity distributions. Accurate delineation of tumor
boundaries is critical for clinical decision-making, radiotherapy planning, and
longitudinal disease monitoring. In this study, we perform a comprehensive
comparative analysis of two major clustering paradigms applied in MRI tumor
segmentation: hard clustering, exemplified by the K-Means algorithm, and soft
clustering, represented by Fuzzy C-Means (FCM). While K-Means assigns each
pixel strictly to a single cluster, FCM introduces partial memberships, meaning
each pixel can belong to multiple clusters with varying degrees of association.
Experimental validation was performed using the BraTS2020 dataset,
incorporating pre-processing through Gaussian filtering and Contrast Limited
Adaptive Histogram Equalization (CLAHE). Evaluation metrics included the Dice
Similarity Coefficient (DSC) and processing time, which collectively
demonstrated that K-Means achieved superior speed with an average runtime of
0.3s per image, whereas FCM attained higher segmentation accuracy with an
average DSC of 0.67 compared to 0.43 for K-Means, albeit at a higher
computational cost (1.3s per image). These results highlight the inherent
trade-off between computational efficiency and boundary precision.

</details>


### [138] [Handling imbalance and few-sample size in ML based Onion disease classification](https://arxiv.org/abs/2509.05341)
*Abhijeet Manoj Pal,Rajbabu Velmurugan*

Main category: cs.CV

TL;DR: A new deep learning model is developed for classifying onion diseases and pests with 96.90% accuracy, surpassing existing approaches.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of binary classification in agriculture by enabling more precise multi-class identification of pests and diseases.

Method: Enhanced pre-trained CNN with attention-based modules and data augmentation to improve performance and address class imbalance.

Result: The proposed model achieved 96.90% accuracy and a 0.96 F1 score on real-world datasets, outperforming competing methods.

Conclusion: The study demonstrates the efficacy of an enhanced CNN model for practical, multi-class pest and disease classification in precision agriculture.

Abstract: Accurate classification of pests and diseases plays a vital role in precision
agriculture, enabling efficient identification, targeted interventions, and
preventing their further spread. However, current methods primarily focus on
binary classification, which limits their practical applications, especially in
scenarios where accurately identifying the specific type of disease or pest is
essential. We propose a robust deep learning based model for multi-class
classification of onion crop diseases and pests. We enhance a pre-trained
Convolutional Neural Network (CNN) model by integrating attention based modules
and employing comprehensive data augmentation pipeline to mitigate class
imbalance. We propose a model which gives 96.90% overall accuracy and 0.96 F1
score on real-world field image dataset. This model gives better results than
other approaches using the same datasets.

</details>


### [139] [Delta Velocity Rectified Flow for Text-to-Image Editing](https://arxiv.org/abs/2509.05342)
*Gaspard Beaudouin,Minghan Li,Jaeyeon Kim,Sunghoon Yoon,Mengyu Wang*

Main category: cs.CV

TL;DR: This paper introduces Delta Velocity Rectified Flow (DVRF), a text-to-image editing method that enhances editing quality by addressing oversmoothing artifacts and aligning trajectories without the need for inversion.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address oversmoothing artifacts and lack of controllability encountered in prior distillation-based text-to-image editing methods, while maintaining efficiency and model compatibility.

Method: The method uses a distillation-based approach, modeling source-target velocity field discrepancies and applying a time-dependent shift term to improve alignment with target distributions. It theoretically connects to prior methods like Delta Denoising Score and FlowEdit under rectified-flow dynamics.

Result: DVRF demonstrates superior performance in editing quality, fidelity, and controllability compared to existing methods, without requiring architectural changes.

Conclusion: DVRF establishes a robust and efficient framework for text-to-image editing, linking previous generation methods theoretically and offering a broadly applicable tool for practical implementations.

Abstract: We propose Delta Velocity Rectified Flow (DVRF), a novel inversion-free,
path-aware editing framework within rectified flow models for text-to-image
editing. DVRF is a distillation-based method that explicitly models the
discrepancy between the source and target velocity fields in order to mitigate
over-smoothing artifacts rampant in prior distillation sampling approaches. We
further introduce a time-dependent shift term to push noisy latents closer to
the target trajectory, enhancing the alignment with the target distribution. We
theoretically demonstrate that when this shift is disabled, DVRF reduces to
Delta Denoising Score, thereby bridging score-based diffusion optimization and
velocity-based rectified-flow optimization. Moreover, when the shift term
follows a linear schedule under rectified-flow dynamics, DVRF generalizes the
Inversion-free method FlowEdit and provides a principled theoretical
interpretation for it. Experimental results indicate that DVRF achieves
superior editing quality, fidelity, and controllability while requiring no
architectural modifications, making it efficient and broadly applicable to
text-to-image editing tasks. Code is available at
https://github.com/gaspardbd/DeltaVelocityRectifiedFlow.

</details>


### [140] [Systematic Integration of Attention Modules into CNNs for Accurate and Generalizable Medical Image Diagnosis](https://arxiv.org/abs/2509.05343)
*Zahid Ullah,Minki Hong,Tahir Mahmood,Jihie Kim*

Main category: cs.CV

TL;DR: The paper enhances five existing CNN architectures with attention mechanisms, resulting in improved performance and interpretability for medical image analysis.


<details>
  <summary>Details</summary>
Motivation: Conventional CNNs struggle to capture fine-grained features necessary for accurate medical image diagnosis.

Method: The authors integrated attention modules (Squeeze and Excitation block or hybrid Convolutional Block Attention Module) into five widely used CNNs and tested their performance on two medical imaging datasets.

Result: Attention-augmented CNNs outperformed baseline architectures, with EfficientNetB5 using hybrid attention achieving the best results on both datasets.

Conclusion: The study offers a framework for enhancing CNNs with attention modules to improve accuracy, feature localization, and generalization in medical imaging tasks.

Abstract: Deep learning has become a powerful tool for medical image analysis; however,
conventional Convolutional Neural Networks (CNNs) often fail to capture the
fine-grained and complex features critical for accurate diagnosis. To address
this limitation, we systematically integrate attention mechanisms into five
widely adopted CNN architectures, namely, VGG16, ResNet18, InceptionV3,
DenseNet121, and EfficientNetB5, to enhance their ability to focus on salient
regions and improve discriminative performance. Specifically, each baseline
model is augmented with either a Squeeze and Excitation block or a hybrid
Convolutional Block Attention Module, allowing adaptive recalibration of
channel and spatial feature representations. The proposed models are evaluated
on two distinct medical imaging datasets, a brain tumor MRI dataset comprising
multiple tumor subtypes, and a Products of Conception histopathological dataset
containing four tissue categories. Experimental results demonstrate that
attention augmented CNNs consistently outperform baseline architectures across
all metrics. In particular, EfficientNetB5 with hybrid attention achieves the
highest overall performance, delivering substantial gains on both datasets.
Beyond improved classification accuracy, attention mechanisms enhance feature
localization, leading to better generalization across heterogeneous imaging
modalities. This work contributes a systematic comparative framework for
embedding attention modules in diverse CNN architectures and rigorously
assesses their impact across multiple medical imaging tasks. The findings
provide practical insights for the development of robust, interpretable, and
clinically applicable deep learning based decision support systems.

</details>


### [141] [Vision-Based Object Detection for UAV Solar Panel Inspection Using an Enhanced Defects Dataset](https://arxiv.org/abs/2509.05348)
*Ashen Rodrigo,Isuru Munasinghe,Asanka Perera*

Main category: cs.CV

TL;DR: This study evaluates five object detection models (YOLOv3, Faster R-CNN, RetinaNet, EfficientDet, and Swin Transformer) for detecting defects and contaminants on solar panels. A custom dataset, annotated in COCO format, was developed to train and evaluate the models.


<details>
  <summary>Details</summary>
Motivation: Ensuring the timely and accurate detection of defects and contaminants is critical to maintaining the efficiency and reliability of photovoltaic systems.

Method: Five state-of-the-art object detection models were evaluated using a custom, publicly-available dataset designed for solar panel inspection. Performance metrics like mAP, precision, recall, and inference speed were measured.

Result: The study highlights the trade-offs between detection accuracy and computational efficiency, comparing the strengths and limitations of each model.

Conclusion: The research provides practical guidance on selecting suitable object detection models for solar panel monitoring and contributes a public dataset to aid further work in this area.

Abstract: Timely and accurate detection of defects and contaminants in solar panels is
critical for maintaining the efficiency and reliability of photovoltaic
systems. This study presents a comprehensive evaluation of five
state-of-the-art object detection models: YOLOv3, Faster R-CNN, RetinaNet,
EfficientDet, and Swin Transformer, for identifying physical and electrical
defects as well as surface contaminants such as dust, dirt, and bird droppings
on solar panels. A custom dataset, annotated in the COCO format and
specifically designed for solar panel defect and contamination detection, was
developed alongside a user interface to train and evaluate the models. The
performance of each model is assessed and compared based on mean Average
Precision (mAP), precision, recall, and inference speed. The results
demonstrate the trade-offs between detection accuracy and computational
efficiency, highlighting the relative strengths and limitations of each model.
These findings provide valuable guidance for selecting appropriate detection
approaches in practical solar panel monitoring and maintenance scenarios.
  The dataset will be publicly available at
https://github.com/IsuruMunasinghe98/solar-panel-inspection-dataset.

</details>


### [142] [Unsupervised Instance Segmentation with Superpixels](https://arxiv.org/abs/2509.05352)
*Cuong Manh Hoang*

Main category: cs.CV

TL;DR: The paper introduces a novel framework for instance segmentation that does not require human annotations, utilizing a MultiCut algorithm, superpixel-guided loss, and a self-training process, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Large-scale human annotations for instance segmentation are expensive and time-consuming to collect, necessitating an efficient approach to achieve accurate segmentation without manual effort.

Method: The proposed framework uses a MultiCut algorithm for coarse mask generation, a mask filter for refining high-quality masks, a novel superpixel-guided mask loss for network training, and a self-training process with an adaptive loss function.

Result: The framework was tested on public datasets for instance segmentation and object detection, outperforming previous state-of-the-art methods in accuracy and effectiveness.

Conclusion: The study demonstrates that instance segmentation can be effectively achieved without human annotations, introducing a cost-efficient and powerful methodology for computer vision tasks.

Abstract: Instance segmentation is essential for numerous computer vision applications,
including robotics, human-computer interaction, and autonomous driving.
Currently, popular models bring impressive performance in instance segmentation
by training with a large number of human annotations, which are costly to
collect. For this reason, we present a new framework that efficiently and
effectively segments objects without the need for human annotations. Firstly, a
MultiCut algorithm is applied to self-supervised features for coarse mask
segmentation. Then, a mask filter is employed to obtain high-quality coarse
masks. To train the segmentation network, we compute a novel superpixel-guided
mask loss, comprising hard loss and soft loss, with high-quality coarse masks
and superpixels segmented from low-level image features. Lastly, a
self-training process with a new adaptive loss is proposed to improve the
quality of predicted masks. We conduct experiments on public datasets in
instance segmentation and object detection to demonstrate the effectiveness of
the proposed framework. The results show that the proposed framework
outperforms previous state-of-the-art methods.

</details>


### [143] [Augmented Structure Preserving Neural Networks for cell biomechanics](https://arxiv.org/abs/2509.05388)
*Juan Olalla-Pombo,Alberto Badías,Miguel Ángel Sanz-Gómez,José María Benítez,Francisco Javier Montáns*

Main category: cs.CV

TL;DR: The paper develops a model combining Structure Preserving Neural Networks and other Machine Learning tools to study cell biomechanics, particularly cell migrations and mitosis events.


<details>
  <summary>Details</summary>
Motivation: To understand complex biomechanical phenomena in cellular processes, including embryo-genesis, damage repair, and tumor growth, and address unclear interactions influencing cells as collective networks.

Method: The approach integrates Structure Preserving Neural Networks for mechanical aspects of cell movements with Artificial Neural Networks to incorporate environmental factors derived from Computer Vision techniques. The model is tested on simulated and real cell migration data.

Result: The proposed model achieves high accuracy in predicting cell trajectories and includes a robust mitosis event prediction using Neural Network architectures.

Conclusion: This new approach effectively combines computational techniques to enhance predictive capabilities in the study of cell biomechanics, providing insights into both mechanical and environmental factors affecting cell behaviors.

Abstract: Cell biomechanics involve a great number of complex phenomena that are
fundamental to the evolution of life itself and other associated processes,
ranging from the very early stages of embryo-genesis to the maintenance of
damaged structures or the growth of tumors. Given the importance of such
phenomena, increasing research has been dedicated to their understanding, but
the many interactions between them and their influence on the decisions of
cells as a collective network or cluster remain unclear. We present a new
approach that combines Structure Preserving Neural Networks, which study cell
movements as a purely mechanical system, with other Machine Learning tools
(Artificial Neural Networks), which allow taking into consideration
environmental factors that can be directly deduced from an experiment with
Computer Vision techniques. This new model, tested on simulated and real cell
migration cases, predicts complete cell trajectories following a roll-out
policy with a high level of accuracy. This work also includes a mitosis event
prediction model based on Neural Networks architectures which makes use of the
same observed features.

</details>


### [144] [Advanced Brain Tumor Segmentation Using EMCAD: Efficient Multi-scale Convolutional Attention Decoding](https://arxiv.org/abs/2509.05431)
*GodsGift Uzor,Tania-Amanda Nkoyo Fredrick Eneye,Chukwuebuka Ijezue*

Main category: cs.CV

TL;DR: This paper introduces EMCAD, an optimized decoder for brain tumor segmentation in MRI scans, achieving moderate Dice scores while maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing decoding mechanisms for brain tumor segmentation are computationally costly, creating a need for efficient solutions.

Method: The authors developed EMCAD, a multi-scale convolutional attention decoder, and tested its performance on the BraTs2020 dataset with MRI scans.

Result: EMCAD achieved a best Dice score of 0.31 and a mean Dice score of 0.285±0.015 during training, demonstrating stable performance without over-fitting.

Conclusion: EMCAD provides a balance between segmentation accuracy and computational efficiency, showcasing its potential for resource-constrained medical imaging applications.

Abstract: Brain tumor segmentation is a critical pre-processing step in the medical
image analysis pipeline that involves precise delineation of tumor regions from
healthy brain tissue in medical imaging data, particularly MRI scans. An
efficient and effective decoding mechanism is crucial in brain tumor
segmentation especially in scenarios with limited computational resources.
However these decoding mechanisms usually come with high computational costs.
To address this concern EMCAD a new efficient multi-scale convolutional
attention decoder designed was utilized to optimize both performance and
computational efficiency for brain tumor segmentation on the BraTs2020 dataset
consisting of MRI scans from 369 brain tumor patients. The preliminary result
obtained by the model achieved a best Dice score of 0.31 and maintained a
stable mean Dice score of 0.285 plus/minus 0.015 throughout the training
process which is moderate. The initial model maintained consistent performance
across the validation set without showing signs of over-fitting.

</details>


### [145] [FAVAE-Effective Frequency Aware Latent Tokenizer](https://arxiv.org/abs/2509.05441)
*Tejaswini Medi,Hsien-Yi Wang,Arianna Rampini,Margret Keuper*

Main category: cs.CV

TL;DR: This paper introduces a frequency-aware variational autoencoder (FA-VAE) to improve image generation quality by addressing high-frequency detail loss in latent embeddings.


<details>
  <summary>Details</summary>
Motivation: Latent generative models often lose fine details in high-frequency regions of images, leading to unrealistic textures and over-smoothed outputs.

Method: The authors propose a wavelet-based FA-VAE framework, which decouples the optimization of low- and high-frequency components to improve image reconstruction quality.

Result: The proposed FA-VAE bridges the fidelity gap in latent tokenizers, reconstructs fine textures more effectively, and enhances overall image realism.

Conclusion: Frequency-aware optimization is crucial for realistic image representation, with broad applications in content creation, neural rendering, and medical imaging.

Abstract: Latent generative models have shown remarkable progress in high-fidelity
image synthesis, typically using a two-stage training process that involves
compressing images into latent embeddings via learned tokenizers in the first
stage. The quality of generation strongly depends on how expressive and
well-optimized these latent embeddings are. While various methods have been
proposed to learn effective latent representations, the reconstructed images
often lack realism, particularly in textured regions with sharp transitions,
due to loss of fine details governed by high frequencies. We conduct a detailed
frequency decomposition of existing state-of-the-art (SOTA) latent tokenizers
and show that conventional objectives inherently prioritize low-frequency
reconstruction, often at the expense of high-frequency fidelity. Our analysis
reveals these latent tokenizers exhibit a bias toward low-frequency
information, when jointly optimized, leading to over-smoothed outputs and
visual artifacts that diminish perceptual quality. To address this, we propose
a wavelet-based, frequency-aware variational autoencoder (FA-VAE) framework
that explicitly decouples the optimization of low- and high-frequency
components. This decoupling enables improved reconstruction of fine textures
while preserving global structure. Our approach bridges the fidelity gap in
current latent tokenizers and emphasizes the importance of frequency-aware
optimization for realistic image representation, with broader implications for
applications in content creation, neural rendering, and medical imaging.

</details>


### [146] [Dynamic Sensitivity Filter Pruning using Multi-Agent Reinforcement Learning For DCNN's](https://arxiv.org/abs/2509.05446)
*Iftekhar Haider Chowdhury,Zaed Ikbal Syed,Ahmed Faizul Haque Dhrubo,Mohammad Abdul Qayum*

Main category: cs.CV

TL;DR: Differential Sensitivity Fusion Pruning is a novel filter pruning method that reduces computational complexity in neural networks while maintaining high accuracy, suitable for mobile and edge devices.


<details>
  <summary>Details</summary>
Motivation: While Deep Convolutional Neural Networks perform well, they are computationally heavy, limiting their use in practical scenarios like on edge or mobile platforms.

Method: This method evaluates filter importance stability using multiple criteria (gradient sensitivity, Taylor expansion, KL divergence), computes a combined differential sensitivity score, applies exponential scaling to emphasize inconsistencies, and prunes filters in a single forward-backward pass.

Result: The approach achieves 50-70% pruning rates, reduces floating-point operations by over 80%, and retains up to 98.23% of model accuracy, outperforming traditional methods.

Conclusion: Differential Sensitivity Fusion Pruning offers an efficient, scalable solution for compressing and deploying neural networks, improving both performance and adaptability.

Abstract: Deep Convolutional Neural Networks have achieved state of the art performance
across various computer vision tasks, however their practical deployment is
limited by computational and memory overhead. This paper introduces
Differential Sensitivity Fusion Pruning, a novel single shot filter pruning
framework that focuses on evaluating the stability and redundancy of filter
importance scores across multiple criteria. Differential Sensitivity Fusion
Pruning computes a differential sensitivity score for each filter by fusing the
discrepancies among gradient based sensitivity, first order Taylor expansion,
and KL divergence of activation distributions. An exponential scaling mechanism
is applied to emphasize filters with inconsistent importance across metrics,
identifying candidates that are structurally unstable or less critical to the
model performance. Unlike iterative or reinforcement learning based pruning
strategies, Differential Sensitivity Fusion Pruning is efficient and
deterministic, requiring only a single forward-backward pass for scoring and
pruning. Extensive experiments across varying pruning rates between 50 to 70
percent demonstrate that Differential Sensitivity Fusion Pruning significantly
reduces model complexity, achieving over 80 percent Floating point Operations
Per Seconds reduction while maintaining high accuracy. For instance, at 70
percent pruning, our approach retains up to 98.23 percent of baseline accuracy,
surpassing traditional heuristics in both compression and generalization. The
proposed method presents an effective solution for scalable and adaptive Deep
Convolutional Neural Networks compression, paving the way for efficient
deployment on edge and mobile platforms.

</details>


### [147] [Veriserum: A dual-plane fluoroscopic dataset with knee implant phantoms for deep learning in medical imaging](https://arxiv.org/abs/2509.05483)
*Jinhao Wang,Florian Vogl,Pascal Schütz,Saša Ćuković,William R. Taylor*

Main category: cs.CV

TL;DR: Veriserum provides 110,000 X-ray images of knee implants for deep learning applications, offering automatic and manual annotations for pose analysis.


<details>
  <summary>Details</summary>
Motivation: To aid in improving medical imaging analysis and algorithm development through a robust, open-source dataset for computer vision tasks.

Method: The dataset features dual-plane X-ray images of knee implants with annotated poses collected in diverse trials and activities.

Result: Offers annotated X-ray images, along with calibration tools, enabling benchmarking and development for 2D/3D registration and other medical imaging applications.

Conclusion: Veriserum supports reproducible research by being a publicly available benchmark resource, promoting advancements in medical imaging and computer vision.

Abstract: Veriserum is an open-source dataset designed to support the training of deep
learning registration for dual-plane fluoroscopic analysis. It comprises
approximately 110,000 X-ray images of 10 knee implant pair combinations (2
femur and 5 tibia implants) captured during 1,600 trials, incorporating poses
associated with daily activities such as level gait and ramp descent. Each
image is annotated with an automatically registered ground-truth pose, while
200 images include manually registered poses for benchmarking.
  Key features of Veriserum include dual-plane images and calibration tools.
The dataset aims to support the development of applications such as 2D/3D image
registration, image segmentation, X-ray distortion correction, and 3D
reconstruction. Freely accessible, Veriserum aims to advance computer vision
and medical imaging research by providing a reproducible benchmark for
algorithm development and evaluation. The Veriserum dataset used in this study
is publicly available via
https://movement.ethz.ch/data-repository/veriserum.html, with the data stored
at ETH Z\"urich Research Collections: https://doi.org/10.3929/ethz-b-000701146.

</details>


### [148] [An Analysis of Layer-Freezing Strategies for Enhanced Transfer Learning in YOLO Architectures](https://arxiv.org/abs/2509.05490)
*Andrzej D. Dobrzycki,Ana M. Bernardos,José R. Casar*

Main category: cs.CV

TL;DR: The paper investigates layer-freezing strategies for YOLOv8 and YOLOv10 architectures in resource-restricted environments, emphasizing their effectiveness on different datasets and training dynamics.


<details>
  <summary>Details</summary>
Motivation: To identify the specific impact of layer-freezing configurations on YOLOv8 and YOLOv10 architectures and their suitability in resource-constrained environments like UAVs.

Method: The study analyzes multiple layer-freezing strategies across critical datasets, using methods like L2 norm for gradient behavior and Grad-CAM for visual explanations to understand training dynamics.

Result: Findings indicate no universally optimal freezing strategy; the best approach depends on dataset characteristics. Strategies can reduce GPU memory by 28% and improve performance, outperforming full fine-tuning in certain cases.

Conclusion: The research delivers empirical findings and practical guidance for using layer-freezing in transfer learning, optimizing object detection for limited-resource scenarios.

Abstract: The You Only Look Once (YOLO) architecture is crucial for real-time object
detection. However, deploying it in resource-constrained environments such as
unmanned aerial vehicles (UAVs) requires efficient transfer learning. Although
layer freezing is a common technique, the specific impact of various freezing
configurations on contemporary YOLOv8 and YOLOv10 architectures remains
unexplored, particularly with regard to the interplay between freezing depth,
dataset characteristics, and training dynamics. This research addresses this
gap by presenting a detailed analysis of layer-freezing strategies. We
systematically investigate multiple freezing configurations across YOLOv8 and
YOLOv10 variants using four challenging datasets that represent critical
infrastructure monitoring. Our methodology integrates a gradient behavior
analysis (L2 norm) and visual explanations (Grad-CAM) to provide deeper
insights into training dynamics under different freezing strategies. Our
results reveal that there is no universal optimal freezing strategy but,
rather, one that depends on the properties of the data. For example, freezing
the backbone is effective for preserving general-purpose features, while a
shallower freeze is better suited to handling extreme class imbalance. These
configurations reduce graphics processing unit (GPU) memory consumption by up
to 28% compared to full fine-tuning and, in some cases, achieve mean average
precision (mAP@50) scores that surpass those of full fine-tuning. Gradient
analysis corroborates these findings, showing distinct convergence patterns for
moderately frozen models. Ultimately, this work provides empirical findings and
practical guidelines for selecting freezing strategies. It offers a practical,
evidence-based approach to balanced transfer learning for object detection in
scenarios with limited resources.

</details>


### [149] [Quaternion Approximation Networks for Enhanced Image Classification and Oriented Object Detection](https://arxiv.org/abs/2509.05512)
*Bryce Grant,Peng Wang*

Main category: cs.CV

TL;DR: QUAN is a novel framework using quaternion algebra and real-valued operations for rotation-equivariant image classification and object detection.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of conventional quaternion neural networks and Convolutional Neural Networks in rotation-equivariant tasks.

Method: Using Hamilton product decomposition for quaternion convolution and introducing IQBN, spatial attention mechanisms, evaluated on classification and detection tasks.

Result: Achieved higher accuracy, faster convergence, improved rotation handling, and set SOTA for quaternion CNNs in object detection.

Conclusion: QUAN is efficient and suitable for rotation-aware applications, particularly in resource-constrained environments like robotics.

Abstract: This paper introduces Quaternion Approximate Networks (QUAN), a novel deep
learning framework that leverages quaternion algebra for rotation equivariant
image classification and object detection. Unlike conventional quaternion
neural networks attempting to operate entirely in the quaternion domain, QUAN
approximates quaternion convolution through Hamilton product decomposition
using real-valued operations. This approach preserves geometric properties
while enabling efficient implementation with custom CUDA kernels. We introduce
Independent Quaternion Batch Normalization (IQBN) for training stability and
extend quaternion operations to spatial attention mechanisms. QUAN is evaluated
on image classification (CIFAR-10/100, ImageNet), object detection (COCO,
DOTA), and robotic perception tasks. In classification tasks, QUAN achieves
higher accuracy with fewer parameters and faster convergence compared to
existing convolution and quaternion-based models. For objection detection, QUAN
demonstrates improved parameter efficiency and rotation handling over standard
Convolutional Neural Networks (CNNs) while establishing the SOTA for quaternion
CNNs in this downstream task. These results highlight its potential for
deployment in resource-constrained robotic systems requiring rotation-aware
perception and application in other domains.

</details>


### [150] [OpenEgo: A Large-Scale Multimodal Egocentric Dataset for Dexterous Manipulation](https://arxiv.org/abs/2509.05513)
*Ahad Jawaid,Yu Xiang*

Main category: cs.CV

TL;DR: The paper introduces OpenEgo, a comprehensive egocentric manipulation dataset with unified hand-pose annotations and intention-aligned action primitives for imitation learning.


<details>
  <summary>Details</summary>
Motivation: Existing egocentric video datasets lack fine-grained action descriptions or standardized hand annotations, limiting their utility for dexterous manipulation research.

Method: The authors created OpenEgo by aggregating and unifying data from 6 public datasets, standardizing hand-pose layouts, and adding detailed, timestamped action primitives.

Result: The researchers validated the dataset's utility by training language-conditioned imitation-learning policies to predict dexterous hand trajectories.

Conclusion: OpenEgo aims to facilitate learning dexterous manipulation from egocentric videos and promote reproducible research in vision-language-action learning.

Abstract: Egocentric human videos provide scalable demonstrations for imitation
learning, but existing corpora often lack either fine-grained, temporally
localized action descriptions or dexterous hand annotations. We introduce
OpenEgo, a multimodal egocentric manipulation dataset with standardized
hand-pose annotations and intention-aligned action primitives. OpenEgo totals
1107 hours across six public datasets, covering 290 manipulation tasks in 600+
environments. We unify hand-pose layouts and provide descriptive, timestamped
action primitives. To validate its utility, we train language-conditioned
imitation-learning policies to predict dexterous hand trajectories. OpenEgo is
designed to lower the barrier to learning dexterous manipulation from
egocentric video and to support reproducible research in vision-language-action
learning. All resources and instructions will be released at
www.openegocentric.com.

</details>


### [151] [Visibility-Aware Language Aggregation for Open-Vocabulary Segmentation in 3D Gaussian Splatting](https://arxiv.org/abs/2509.05515)
*Sen Wang,Kunyi Li,Siyun Liang,Elena Alegret,Jing Ma,Nassir Navab,Stefano Gasperini*

Main category: cs.CV

TL;DR: This paper addresses the issue of embedding open-vocabulary language features into 3D representations, proposing Visibility-Aware Language Aggregation (VALA) to improve visibility and view-consistency.


<details>
  <summary>Details</summary>
Motivation: To tackle issues in existing methods, namely background Gaussians with disproportionate contributions and multi-view inconsistencies due to noise in language embeddings.

Method: The authors introduce VALA, which uses marginal contribution computation and visibility-aware gates, along with a streaming weighted geometric median, to refine 3D language embeddings effectively.

Result: Results show that VALA enhances open-vocabulary localization and segmentation performance across multiple datasets, outperforming current approaches.

Conclusion: VALA demonstrates a fast, robust, and memory-efficient framework that addresses key limitations in 3D Gaussian language embeddings, improving interaction and feature consistency.

Abstract: Recently, distilling open-vocabulary language features from 2D images into 3D
Gaussians has attracted significant attention. Although existing methods
achieve impressive language-based interactions of 3D scenes, we observe two
fundamental issues: background Gaussians contributing negligibly to a rendered
pixel get the same feature as the dominant foreground ones, and multi-view
inconsistencies due to view-specific noise in language embeddings. We introduce
Visibility-Aware Language Aggregation (VALA), a lightweight yet effective
method that computes marginal contributions for each ray and applies a
visibility-aware gate to retain only visible Gaussians. Moreover, we propose a
streaming weighted geometric median in cosine space to merge noisy multi-view
features. Our method yields a robust, view-consistent language feature
embedding in a fast and memory-efficient manner. VALA improves open-vocabulary
localization and segmentation across reference datasets, consistently
surpassing existing works.

</details>


### [152] [DuoCLR: Dual-Surrogate Contrastive Learning for Skeleton-based Human Action Segmentation](https://arxiv.org/abs/2509.05543)
*Haitao Tian,Pierre Payeur*

Main category: cs.CV

TL;DR: The paper proposes a contrastive representation learning framework, DuoCLR, using novel data augmentation and dual surrogate tasks to improve human action segmentation. It outperforms existing methods in experiments.


<details>
  <summary>Details</summary>
Motivation: Enhance human action segmentation by addressing limitations in existing representation learning frameworks that primarily focus on action recognition and isolated sequence-wise representations.

Method: Introduces a new data augmentation strategy, 'Shuffle and Warp,' and two surrogate tasks (CPC and ROR) in a contrastive learning setup to optimize a DuoCLR network for extracting multi-scale feature representations.

Result: DuoCLR achieves a significant performance boost in multi-class and multi-label action segmentation on untrimmed datasets compared to state-of-the-art methods.

Conclusion: The proposed DuoCLR framework effectively leverages trimmed skeleton sequence data and distinct learning tasks for robust human action segmentation, showing promise for improved real-world applications.

Abstract: In this paper, a contrastive representation learning framework is proposed to
enhance human action segmentation via pre-training using trimmed (single
action) skeleton sequences. Unlike previous representation learning works that
are tailored for action recognition and that build upon isolated sequence-wise
representations, the proposed framework focuses on exploiting multi-scale
representations in conjunction with cross-sequence variations. More
specifically, it proposes a novel data augmentation strategy, 'Shuffle and
Warp', which exploits diverse multi-action permutations. The latter effectively
assists two surrogate tasks that are introduced in contrastive learning: Cross
Permutation Contrasting (CPC) and Relative Order Reasoning (ROR). In
optimization, CPC learns intra-class similarities by contrasting
representations of the same action class across different permutations, while
ROR reasons about inter-class contexts by predicting relative mapping between
two permutations. Together, these tasks enable a Dual-Surrogate Contrastive
Learning (DuoCLR) network to learn multi-scale feature representations
optimized for action segmentation. In experiments, DuoCLR is pre-trained on a
trimmed skeleton dataset and evaluated on an untrimmed dataset where it
demonstrates a significant boost over state-the-art comparatives in both
multi-class and multi-label action segmentation tasks. Lastly, ablation studies
are conducted to evaluate the effectiveness of each component of the proposed
approach.

</details>


### [153] [RED: Robust Event-Guided Motion Deblurring with Modality-Specific Disentangled Representation](https://arxiv.org/abs/2509.05554)
*Yihong Leng,Siming Zheng,Jinwei Chen,Bo Li,Jiaojiao Li,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: The paper proposes the RED network, which improves event-guided motion deblurring by addressing event incompleteness and enhancing robustness through perturbation and attention strategies.


<details>
  <summary>Details</summary>
Motivation: Existing methods for motion deblurring using event cameras often overlook the incompleteness of event streams due to DVS sensitivity-noise trade-offs, compromising motion priors needed for effective deblurring.

Method: The authors introduced the RED network featuring a Robustness-Oriented Perturbation Strategy (random event masking for robustness) and a disentangled OmniAttention to model correlations between blurry images and incomplete events. Additionally, interactive modules enhance motion and semantic context.

Result: RED network achieves state-of-the-art performance in terms of both accuracy and robustness, validated by experiments on synthetic and real-world datasets.

Conclusion: The paper concludes that RED can robustly handle incomplete event streams and improve motion deblurring effectiveness by integrating reliable representations from blurry images and event data.

Abstract: Event cameras provide sparse yet temporally high-temporal-resolution motion
information, demonstrating great potential for motion deblurring. Existing
methods focus on cross-modal interaction, overlooking the inherent
incompleteness of event streams, which arises from the trade-off between
sensitivity and noise introduced by the thresholding mechanism of Dynamic
Vision Sensors (DVS). Such degradation compromises the integrity of motion
priors and limits the effectiveness of event-guided deblurring. To tackle these
challenges, we propose a Robust Event-guided Deblurring (RED) network with
modality-specific disentangled representation. First, we introduce a
Robustness-Oriented Perturbation Strategy (RPS) that applies random masking to
events, which exposes RED to incomplete patterns and then foster robustness
against various unknown scenario conditions.Next, a disentangled OmniAttention
is presented to explicitly model intra-motion, inter-motion, and cross-modality
correlations from two inherently distinct but complementary sources: blurry
images and partially disrupted events. Building on these reliable features, two
interactive modules are designed to enhance motion-sensitive areas in blurry
images and inject semantic context into incomplete event representations.
Extensive experiments on synthetic and real-world datasets demonstrate RED
consistently achieves state-of-the-art performance in both accuracy and
robustness.

</details>


### [154] [Sensitivity-Aware Post-Training Quantization for Deep Neural Networks](https://arxiv.org/abs/2509.05576)
*Zekang Zheng,Haokun Li,Yaofo Chen,Mingkui Tan,Qing Du*

Main category: cs.CV

TL;DR: This paper introduces an efficient post-training quantization (PTQ) method for neural networks that prioritizes parameter sensitivity to reduce computational overhead while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To enable model compression via quantization while minimizing both accuracy losses and computational complexity, making it applicable for resource-constrained and real-time scenarios.

Method: The method prioritizes the quantization of high-sensitivity parameters and uses low-sensitivity parameters for compensation. It employs column-wise clustering of sensitivity to inform a row-parallel quantization framework, optimized by a globally shared inverse Hessian matrix.

Result: The proposed method achieves a 20-200x speedup in quantization compared to Optimal Brain Quantization while ensuring mean accuracy loss remains below 0.3% in experiments on ResNet-50 and YOLOv5s.

Conclusion: This method significantly improves quantization efficiency by reducing computational complexity and demonstrates strong efficacy in preserving model accuracy, making it viable for edge computing and time-critical applications.

Abstract: Model quantization reduces neural network parameter precision to achieve
compression, but often compromises accuracy. Existing post-training
quantization (PTQ) methods employ iterative parameter updates to preserve
accuracy under high compression ratios, incurring significant computational
complexity and resource overhead, which limits applicability in
resource-constrained edge computing and real-time inference scenarios. This
paper proposes an efficient PTQ method guided by parameter sensitivity
analysis. The approach prioritizes quantization of high-sensitivity parameters,
leveraging unquantized low-sensitivity parameters to compensate for
quantization errors, thereby mitigating accuracy degradation. Furthermore, by
exploiting column-wise clustering of parameter sensitivity, the method
introduces a row-parallel quantization framework with a globally shared inverse
Hessian matrix update mechanism, reducing computational complexity by an order
of magnitude. Experimental results on ResNet-50 and YOLOv5s demonstrate a
20-200-fold quantization speedup over the Optimal Brain Quantization baseline,
with mean accuracy loss below 0.3%, confirming the method's efficacy in
balancing efficiency and accuracy.

</details>


### [155] [Reconstruction and Reenactment Separated Method for Realistic Gaussian Head](https://arxiv.org/abs/2509.05582)
*Zhiling Ye,Cong Zhou,Xiubao Zhang,Haifeng Shen,Weihong Deng,Quan Lu*

Main category: cs.CV

TL;DR: The paper introduces a framework for generating controllable 3D Gaussian head avatars using a single image, achieving high rendering performance and improved reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: To develop a framework that generates realistic and controllable 3D Gaussian head avatars from a single portrait image and addresses limitations of existing methods, such as generalization and texture reconstruction.

Method: The approach involves creating a large-scale one-shot Gaussian head generator using WebSSL and a two-stage training strategy for enhanced generalization and texture quality. It demonstrates high-efficiency rendering driven by control signals.

Result: The model achieves 90 FPS at a resolution of 512x512, adheres to scaling laws for improved reconstruction performance, and showcases superior results compared to state-of-the-art methods.

Conclusion: The framework separates reconstruction and reenactment, providing efficient and high-quality 3D Gaussian avatars driven by signals, outperforming other methods in both qualitative and quantitative evaluations.

Abstract: In this paper, we explore a reconstruction and reenactment separated
framework for 3D Gaussians head, which requires only a single portrait image as
input to generate controllable avatar. Specifically, we developed a large-scale
one-shot gaussian head generator built upon WebSSL and employed a two-stage
training approach that significantly enhances the capabilities of
generalization and high-frequency texture reconstruction. During inference, an
ultra-lightweight gaussian avatar driven by control signals enables high
frame-rate rendering, achieving 90 FPS at a resolution of 512x512. We further
demonstrate that the proposed framework follows the scaling law, whereby
increasing the parameter scale of the reconstruction module leads to improved
performance. Moreover, thanks to the separation design, driving efficiency
remains unaffected. Finally, extensive quantitative and qualitative experiments
validate that our approach outperforms current state-of-the-art methods.

</details>


### [156] [MFFI: Multi-Dimensional Face Forgery Image Dataset for Real-World Scenarios](https://arxiv.org/abs/2509.05592)
*Changtao Miao,Yi Zhang,Man Luo,Weiwei Feng,Kaiyuan Zheng,Qi Chu,Tao Gong,Jianshu Li,Yunfeng Diao,Wei Zhou,Joey Tianyi Zhou,Xiaoshuai Hao*

Main category: cs.CV

TL;DR: This paper introduces the Multi-dimensional Face Forgery Image (MFFI) dataset to address limitations in current Deepfake detection datasets and improve real-world application.


<details>
  <summary>Details</summary>
Motivation: Current Deepfake detection methods are limited by datasets lacking diversity and realism in terms of advanced forgery techniques, varied scenes, authentic data, and degradation modeling, which compromises their effectiveness in real-world scenarios.

Method: The authors developed the MFFI dataset by incorporating 50 different forgery methods and 1,024,000 image samples. The dataset focuses on four dimensions: wider forgery methods, varied facial scenes, diversified authentic data, and multi-level degradation operations.

Result: Benchmark tests reveal that MFFI surpasses existing datasets by offering greater scene diversity, improved cross-domain generalization, and more challenging detection scenarios.

Conclusion: MFFI provides a significant improvement over existing datasets by better simulating real-world conditions for face forgery detection. Its availability is expected to enhance research and practical applications in the field.

Abstract: Rapid advances in Artificial Intelligence Generated Content (AIGC) have
enabled increasingly sophisticated face forgeries, posing a significant threat
to social security. However, current Deepfake detection methods are limited by
constraints in existing datasets, which lack the diversity necessary in
real-world scenarios. Specifically, these data sets fall short in four key
areas: unknown of advanced forgery techniques, variability of facial scenes,
richness of real data, and degradation of real-world propagation. To address
these challenges, we propose the Multi-dimensional Face Forgery Image
(\textbf{MFFI}) dataset, tailored for real-world scenarios. MFFI enhances
realism based on four strategic dimensions: 1) Wider Forgery Methods; 2) Varied
Facial Scenes; 3) Diversified Authentic Data; 4) Multi-level Degradation
Operations. MFFI integrates $50$ different forgery methods and contains $1024K$
image samples. Benchmark evaluations show that MFFI outperforms existing public
datasets in terms of scene complexity, cross-domain generalization capability,
and detection difficulty gradients. These results validate the technical
advance and practical utility of MFFI in simulating real-world conditions. The
dataset and additional details are publicly available at
{https://github.com/inclusionConf/MFFI}.

</details>


### [157] [Language-guided Recursive Spatiotemporal Graph Modeling for Video Summarization](https://arxiv.org/abs/2509.05604)
*Jungin Park,Jiyoung Lee,Kwanghoon Sohn*

Main category: cs.CV

TL;DR: This paper introduces VideoGraph, a language-guided recursive spatiotemporal graph network for video summarization, achieving state-of-the-art performance on several benchmarks.


<details>
  <summary>Details</summary>
Motivation: To improve video summarization by incorporating the semantic relationships of fine-grained visual entities like objects and leveraging language-guided queries for better understanding and summarization.

Method: The proposed method uses recursive spatiotemporal graph networks where objects and frames are modeled as graph nodes, and semantic relationships are depicted as graph edges. Language queries are integrated to enrich semantic node representations for precise classification of keyframes.

Result: VideoGraph demonstrates state-of-the-art performance in both generic and query-focused video summarization tasks across multiple benchmarks.

Conclusion: The VideoGraph framework effectively integrates semantic knowledge with spatiotemporal graph modeling, refining video summarization tasks through recursive strategies and providing high accuracy in supervised and unsupervised settings.

Abstract: Video summarization aims to select keyframes that are visually diverse and
can represent the whole story of a given video. Previous approaches have
focused on global interlinkability between frames in a video by temporal
modeling. However, fine-grained visual entities, such as objects, are also
highly related to the main content of the video. Moreover, language-guided
video summarization, which has recently been studied, requires a comprehensive
linguistic understanding of complex real-world videos. To consider how all the
objects are semantically related to each other, this paper regards video
summarization as a language-guided spatiotemporal graph modeling problem. We
present recursive spatiotemporal graph networks, called VideoGraph, which
formulate the objects and frames as nodes of the spatial and temporal graphs,
respectively. The nodes in each graph are connected and aggregated with graph
edges, representing the semantic relationships between the nodes. To prevent
the edges from being configured with visual similarity, we incorporate language
queries derived from the video into the graph node representations, enabling
them to contain semantic knowledge. In addition, we adopt a recursive strategy
to refine initial graphs and correctly classify each frame node as a keyframe.
In our experiments, VideoGraph achieves state-of-the-art performance on several
benchmarks for generic and query-focused video summarization in both supervised
and unsupervised manners. The code is available at
https://github.com/park-jungin/videograph.

</details>


### [158] [Patch-level Kernel Alignment for Self-Supervised Dense Representation Learning](https://arxiv.org/abs/2509.05606)
*Juan Yeo,Ijun Jang,Taesup Kim*

Main category: cs.CV

TL;DR: The paper introduces a self-supervised learning framework for dense representations using Patch-level Kernel Alignment (PaKA) and custom augmentation techniques, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Dense vision tasks demand representations that preserve spatial precision and local semantics, which current self-supervised methods fail to sufficiently address.

Method: The framework utilizes self-supervised learning to refine pretrained representations for dense tasks. PaKA aligns statistical dependencies between dense patches of teacher and student models. Furthermore, augmentation strategies tailored for dense learning are proposed.

Result: The framework achieved state-of-the-art performance across several benchmarks for dense vision tasks.

Conclusion: PaKA and specialized augmentation strategies are effective tools for transferring semantic knowledge into dense representations, improving dense vision task outcomes.

Abstract: Dense representations are essential for vision tasks that require spatial
precision and fine-grained detail. While most self-supervised representation
learning methods focus on global representations that summarize the image as a
whole, such approaches often fall short in capturing the localized semantics
necessary for dense prediction tasks. To overcome these limitations, we propose
a framework that builds on pretrained representations through additional
self-supervised learning, aiming to transfer existing semantic knowledge into
the dense feature space. Our method aligns the distributions of dense features
between a teacher and a student model. Specifically, we introduce Patch-level
Kernel Alignment (PaKA), a simple yet effective alignment objective that
captures statistical dependencies, thereby matching the structural
relationships of dense patches across the two models. In addition, we
investigate augmentation strategies specifically designed for dense
representation learning. Our framework achieves state-of-the-art results across
a variety of dense vision benchmarks, demonstrating the effectiveness of our
approach.

</details>


### [159] [SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning](https://arxiv.org/abs/2509.05614)
*Hanzhen Wang,Jiaming Xu,Jiayi Pan,Yongkang Zhou,Guohao Dai*

Main category: cs.CV

TL;DR: SpecPrune-VLA enhances token pruning in Vision-Language-Action models by leveraging both local and global context, achieving speedups with minimal loss in success rates.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in token pruning methods for Vision-Language-Action models that ignore global context and lead to poor performance and limited computational acceleration.

Method: SpecPrune-VLA introduces a two-level pruning approach: static action-level pruning utilizing global history and local context, dynamic layer-level pruning based on layer-specific importance, and an action-aware controller that adjusts pruning aggressiveness.

Result: The model achieved a 1.46x computational speedup on NVIDIA A800 and 1.57x on NVIDIA GeForce RTX 3090 compared to OpenVLA-OFT, with negligible reduction in success rates.

Conclusion: By integrating global history, local context, and adaptive pruning strategies, SpecPrune-VLA improves computational efficiency while maintaining performance for Vision-Language-Action models.

Abstract: Pruning accelerates compute-bound models by reducing computation. Recently
applied to Vision-Language-Action (VLA) models, existing methods prune tokens
using only local info from current action, ignoring global context from prior
actions, causing >20% success rate drop and limited speedup. We observe high
similarity across consecutive actions and propose leveraging both local
(current) and global (past) info for smarter token selection. We introduce
SpecPrune-VLA, a training-free method with two-level pruning and heuristic
control: (1) Static pruning at action level: uses global history and local
context to reduce visual tokens per action; (2) Dynamic pruning at layer level:
prunes tokens per layer based on layer-specific importance; (3) Lightweight
action-aware controller: classifies actions as coarse/fine-grained (by speed),
adjusting pruning aggressiveness since fine-grained actions are
pruning-sensitive. Experiments on LIBERO show SpecPrune-VLA achieves 1.46 times
speedup on NVIDIA A800 and 1.57 times on NVIDIA GeForce RTX 3090 vs.
OpenVLA-OFT, with negligible success rate loss.

</details>


### [160] [SuMa: A Subspace Mapping Approach for Robust and Effective Concept Erasure in Text-to-Image Diffusion Models](https://arxiv.org/abs/2509.05625)
*Kien Nguyen,Anh Tran,Cuong Pham*

Main category: cs.CV

TL;DR: The manuscript introduces SuMa, a novel Subspace Mapping method for erasing narrow concepts like copyrighted figures or celebrities in diffusion models while maintaining image quality.


<details>
  <summary>Details</summary>
Motivation: The increasing misuse of text-to-image diffusion models for generating unauthorized or harmful content highlights the need for robust concept erasure methods. Current techniques underperform particularly in addressing narrow concepts critical for copyright and legal concerns.

Method: This paper proposes Subspace Mapping (SuMa), which involves deriving a target subspace for the concept needing erasure and then neutralizing this space by mapping it to a reference subspace that minimizes distance between them. This ensures robustness in erasure and quality preservation.

Result: SuMa demonstrated effectiveness across tasks like subclass, celebrity, artistic style, and instance erasure, achieving comparable image quality and erasure robustness against state-of-the-art methods.

Conclusion: SuMa successfully addresses the finer-grained manipulation challenges in erasing narrow concepts without compromising the generated image quality, making it a robust and effective solution compared to existing frameworks.

Abstract: The rapid growth of text-to-image diffusion models has raised concerns about
their potential misuse in generating harmful or unauthorized contents. To
address these issues, several Concept Erasure methods have been proposed.
However, most of them fail to achieve both robustness, i.e., the ability to
robustly remove the target concept., and effectiveness, i.e., maintaining image
quality. While few recent techniques successfully achieve these goals for NSFW
concepts, none could handle narrow concepts such as copyrighted characters or
celebrities. Erasing these narrow concepts is critical in addressing copyright
and legal concerns. However, erasing them is challenging due to their close
distances to non-target neighboring concepts, requiring finer-grained
manipulation. In this paper, we introduce Subspace Mapping (SuMa), a novel
method specifically designed to achieve both robustness and effectiveness in
easing these narrow concepts. SuMa first derives a target subspace representing
the concept to be erased and then neutralizes it by mapping it to a reference
subspace that minimizes the distance between the two. This mapping ensures the
target concept is robustly erased while preserving image quality. We conduct
extensive experiments with SuMa across four tasks: subclass erasure, celebrity
erasure, artistic style erasure, and instance erasure and compare the results
with current state-of-the-art methods. Our method achieves image quality
comparable to approaches focused on effectiveness, while also yielding results
that are on par with methods targeting completeness.

</details>


### [161] [Self-supervised Learning for Hyperspectral Images of Trees](https://arxiv.org/abs/2509.05630)
*Moqsadur Rahman,Saurav Kumar,Santosh S. Palmate,M. Shahriar Hossain*

Main category: cs.CV

TL;DR: This paper explores self-supervised learning for neural network embeddings of vegetation properties from aerial hyperspectral images, resulting in improved tree representations for downstream tasks.


<details>
  <summary>Details</summary>
Motivation: The aim is to address challenges in analyzing hyperspectral images for precision agriculture when there are limited or no labeled data.

Method: Self-supervised learning is applied to create neural network embeddings reflecting vegetation properties from aerial hyperspectral images.

Result: Tree representations using the embedding space outperform the direct use of hyperspectral vegetation properties in downstream machine learning tasks.

Conclusion: Self-supervised learning-based tree representations offer a significant improvement in utilizing hyperspectral images for better performance in agricultural analysis tasks.

Abstract: Aerial remote sensing using multispectral and RGB imagers has provided a
critical impetus to precision agriculture. Analysis of the hyperspectral images
with limited or no labels is challenging. This paper focuses on self-supervised
learning to create neural network embeddings reflecting vegetation properties
of trees from aerial hyperspectral images of crop fields. Experimental results
demonstrate that a constructed tree representation, using a vegetation
property-related embedding space, performs better in downstream machine
learning tasks compared to the direct use of hyperspectral vegetation
properties as tree representations.

</details>


### [162] [Evaluating YOLO Architectures: Implications for Real-Time Vehicle Detection in Urban Environments of Bangladesh](https://arxiv.org/abs/2509.05652)
*Ha Meem Hossain,Pritam Nath,Mahitun Nesa Mahi,Imtiaz Uddin,Ishrat Jahan Eiste,Syed Nasibur Rahman Ratul,Md Naim Uddin Mozumdar,Asif Mohammed Saad*

Main category: cs.CV

TL;DR: This paper evaluates six YOLO variants on a custom dataset for vehicle detection in Bangladesh, highlighting the strengths and weaknesses of the models with region-specific vehicles.


<details>
  <summary>Details</summary>
Motivation: Generic vehicle detection models trained on non-local datasets struggle in Bangladesh's unique traffic environments, necessitating specialized detection systems.

Method: The study used a custom dataset with 29 vehicle classes, captured from Bangladeshi roads, annotated, and evaluated on YOLO model variants measuring mAP, recall, and F1-score.

Result: YOLOv11x achieved the highest detection performance with a mAP@0.5 of 63.7%, but medium YOLO variants offered a balance between performance and inference time. Rare classes faced detection challenges.

Conclusion: The study emphasizes the need for region-specific training datasets and system development to improve autonomous driving technology in developing countries like Bangladesh.

Abstract: Vehicle detection systems trained on Non-Bangladeshi datasets struggle to
accurately identify local vehicle types in Bangladesh's unique road
environments, creating critical gaps in autonomous driving technology for
developing regions. This study evaluates six YOLO model variants on a custom
dataset featuring 29 distinct vehicle classes, including region-specific
vehicles such as ``Desi Nosimon'', ``Leguna'', ``Battery Rickshaw'', and
``CNG''. The dataset comprises high-resolution images (1920x1080) captured
across various Bangladeshi roads using mobile phone cameras and manually
annotated using LabelImg with YOLO format bounding boxes. Performance
evaluation revealed YOLOv11x as the top performer, achieving 63.7\% mAP@0.5,
43.8\% mAP@0.5:0.95, 61.4\% recall, and 61.6\% F1-score, though requiring 45.8
milliseconds per image for inference. Medium variants (YOLOv8m, YOLOv11m)
struck an optimal balance, delivering robust detection performance with mAP@0.5
values of 62.5\% and 61.8\% respectively, while maintaining moderate inference
times around 14-15 milliseconds. The study identified significant detection
challenges for rare vehicle classes, with Construction Vehicles and Desi
Nosimons showing near-zero accuracy due to dataset imbalances and insufficient
training samples. Confusion matrices revealed frequent misclassifications
between visually similar vehicles, particularly Mini Trucks versus Mini Covered
Vans. This research provides a foundation for developing robust object
detection systems specifically adapted to Bangladesh traffic conditions,
addressing critical needs in autonomous vehicle technology advancement for
developing regions where conventional generic-trained models fail to perform
adequately.

</details>


### [163] [EditIDv2: Editable ID Customization with Data-Lubricated ID Feature Integration for Text-to-Image Generation](https://arxiv.org/abs/2509.05659)
*Guandong Li,Zhaobin Chu*

Main category: cs.CV

TL;DR: EditIDv2 improves character editing in complex narratives with minimal data, maintaining identity consistency even with long text inputs.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with editing accuracy and consistency in complex narrative scenes and long text inputs.

Method: EditIDv2 incorporates PerceiverAttention decomposition, ID loss, dynamic training with diffusion models, and an offline integration module strategy.

Result: The method achieves high-quality image generation, deep semantic editing, and identity consistency, validated through IBench evaluation.

Conclusion: EditIDv2 successfully addresses editability challenges in complex narratives, providing advanced solutions for high-quality image editing under demanding prompts.

Abstract: We propose EditIDv2, a tuning-free solution specifically designed for
high-complexity narrative scenes and long text inputs. Existing character
editing methods perform well under simple prompts, but often suffer from
degraded editing capabilities, semantic understanding biases, and identity
consistency breakdowns when faced with long text narratives containing multiple
semantic layers, temporal logic, and complex contextual relationships. In
EditID, we analyzed the impact of the ID integration module on editability. In
EditIDv2, we further explore and address the influence of the ID feature
integration module. The core of EditIDv2 is to discuss the issue of editability
injection under minimal data lubrication. Through a sophisticated decomposition
of PerceiverAttention, the introduction of ID loss and joint dynamic training
with the diffusion model, as well as an offline fusion strategy for the
integration module, we achieve deep, multi-level semantic editing while
maintaining identity consistency in complex narrative environments using only a
small amount of data lubrication. This meets the demands of long prompts and
high-quality image generation, and achieves excellent results in the IBench
evaluation.

</details>


### [164] [OOTSM: A Decoupled Linguistic Framework for Effective Scene Graph Anticipation](https://arxiv.org/abs/2509.05661)
*Xiaomeng Zhu,Changwei Wang,Haozhe Wang,Xinyu Liu,Fangzhen Lin*

Main category: cs.CV

TL;DR: The paper proposes a scene graph anticipation (SGA) method that integrates commonsense knowledge through a linguistic approach, improving both short-term and long-term predictions.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing SGA methods that struggle with long-term prediction robustness due to underutilization of commonsense knowledge.

Method: A two-step approach called Object-Oriented Two-Staged Method (OOTSM): First, a scene graph capturing model translates video clips into scene graphs. Next, a linguistic model forecasts and details object relationships.

Result: OOTSM shows state-of-the-art performance. In the short-term, mean-Recall (@10) improves by 3.4%. Long-term mean-Recall (@50) sees a substantial improvement of 21.9%, tested on a benchmark from Action Genome annotations.

Conclusion: The proposed approach successfully integrates linguistic insights into SGA, demonstrating its independent interest and improving prediction accuracy significantly.

Abstract: A scene graph is a structured represention of objects and their relationships
in a scene. Scene Graph Anticipation (SGA) involves predicting future scene
graphs from video clips, enabling applications as intelligent surveillance and
human-machine collaboration. Existing SGA approaches primarily leverage visual
cues, often struggling to integrate valuable commonsense knowledge, thereby
limiting long-term prediction robustness. To explicitly leverage such
commonsense knowledge, we propose a new approach to better understand the
objects, concepts, and relationships in a scene graph. Our approach decouples
the SGA task in two steps: first a scene graph capturing model is used to
convert a video clip into a sequence of scene graphs, then a pure text-based
model is used to predict scene graphs in future frames. Our focus in this work
is on the second step, and we call it Linguistic Scene Graph Anticipation
(LSGA) and believes it should have independent interest beyond the use in SGA
discussed here. For LSGA, we introduce an Object-Oriented Two-Staged Method
(OOTSM) where an Large Language Model (LLM) first forecasts object appearances
and disappearances before generating detailed human-object relations. We
conduct extensive experiments to evaluate OOTSM in two settings. For LSGA, we
evaluate our fine-tuned open-sourced LLMs against zero-shot APIs (i.e., GPT-4o,
GPT-4o-mini, and DeepSeek-V3) on a benchmark constructed from Action Genome
annotations. For SGA, we combine our OOTSM with STTran++ from, and our
experiments demonstrate effective state-of-the-art performance: short-term
mean-Recall (@10) increases by 3.4% while long-term mean-Recall (@50) improves
dramatically by 21.9%. Code is available at https://github.com/ZhuXMMM/OOTSM.

</details>


### [165] [WIPUNet: A Physics-inspired Network with Weighted Inductive Biases for Image Denoising](https://arxiv.org/abs/2509.05662)
*Wasikul Islam*

Main category: cs.CV

TL;DR: This paper explores physics-guided inductive biases for image denoising, inspired by pileup mitigation in particle physics, and demonstrates improved robustness under strong noise conditions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to integrate physics-inspired principles, like conservation and locality, into neural network architectures for robust image denoising, particularly under high noise levels.

Method: The authors propose a hierarchy of pileup-inspired denoisers, including a residual CNN, its Gaussian-noise variants, and the Weighted Inductive Pileup-physics-inspired U-Network (WIPUNet) built on a UNet backbone.

Result: The proposed methods demonstrate competitive performance on CIFAR-10 with Gaussian noise across various levels and show widening margins of improvement at higher noise levels. Experiments on BSD500 corroborate these findings.

Conclusion: Physics-inspired priors enhance robustness in image denoising under strong corruption and offer stability where data-driven models falter, without requiring state-of-the-art techniques.

Abstract: In high-energy particle physics, collider measurements are contaminated by
"pileup", overlapping soft interactions that obscure the hard-scatter signal of
interest. Dedicated subtraction strategies exploit physical priors such as
conservation, locality, and isolation. Inspired by this analogy, we investigate
how such principles can inform image denoising by embedding physics-guided
inductive biases into neural architectures. This paper is a proof of concept:
rather than targeting state-of-the-art (SOTA) benchmarks, we ask whether
physics-inspired priors improve robustness under strong corruption.
  We introduce a hierarchy of PU-inspired denoisers: a residual CNN with
conservation constraints, its Gaussian-noise variants, and the Weighted
Inductive Pileup-physics-inspired U-Network for Denoising (WIPUNet), which
integrates these ideas into a UNet backbone. On CIFAR-10 with Gaussian noise at
$\sigma\in\{15,25,50,75,100\}$, PU-inspired CNNs are competitive with standard
baselines, while WIPUNet shows a \emph{widening margin} at higher noise.
Complementary BSD500 experiments show the same trend, suggesting
physics-inspired priors provide stability where purely data-driven models
degrade. Our contributions are: (i) translating pileup-mitigation principles
into modular inductive biases; (ii) integrating them into UNet; and (iii)
demonstrating robustness gains at high noise without relying on heavy SOTA
machinery.

</details>


### [166] [Context-Aware Multi-Turn Visual-Textual Reasoning in LVLMs via Dynamic Memory and Adaptive Visual Guidance](https://arxiv.org/abs/2509.05669)
*Weijie Shen,Xinrui Wang,Yuanqi Nie,Apiradee Boonmee*

Main category: cs.CV

TL;DR: The paper introduces CAMVR, a framework enhancing LVLMs to improve multi-turn visual and textual reasoning using dynamic memory and adaptive focusing mechanisms.


<details>
  <summary>Details</summary>
Motivation: The research addresses significant limitations of current LLMs and LVLMs, particularly their struggles with multi-turn interactions, fragmented reasoning, loss of context, and hallucination issues.

Method: The authors propose a novel framework, CAMVR, incorporating two innovations: a Visual-Textual Context Memory Unit (VCMU) for managing and storing visual-textual correspondences dynamically, and an Adaptive Visual Focus Guidance (AVFG) for adjusting encoder focus based on context.

Result: Experiments on datasets like VisDial, an adapted A-OKVQA, and the authors' own MTIF dataset show that CAMVR outperforms existing methods with state-of-the-art results.

Conclusion: CAMVR effectively enhances LVLMs' capabilities in managing complex, multi-turn interactions, improving reasoning coherence and reducing issues like context loss and hallucinations.

Abstract: Current Large Language Models (LLMs) and Vision-Language Large Models (LVLMs)
excel in single-turn tasks but face significant challenges in multi-turn
interactions requiring deep contextual understanding and complex visual
reasoning, often leading to fragmented reasoning, context loss, and
hallucinations. To address these limitations, we propose Context-Aware
Multi-Turn Visual Reasoning (CAMVR), a novel framework designed to empower
LVLMs with robust and coherent multi-turn visual-textual inference
capabilities. CAMVR introduces two key innovations: a Visual-Textual Context
Memory Unit (VCMU), a dynamic read-write memory network that stores and manages
critical visual features, textual semantic representations, and their
cross-modal correspondences from each interaction turn; and an Adaptive Visual
Focus Guidance (AVFG) mechanism, which leverages the VCMU's context to
dynamically adjust the visual encoder's attention to contextually relevant
image regions. Our multi-level reasoning integration strategy ensures that
response generation is deeply coherent with both current inputs and accumulated
historical context. Extensive experiments on challenging datasets, including
VisDial, an adapted A-OKVQA, and our novel Multi-Turn Instruction Following
(MTIF) dataset, demonstrate that CAMVR consistently achieves state-of-the-art
performance.

</details>


### [167] [MeshMetrics: A Precise Implementation of Distance-Based Image Segmentation Metrics](https://arxiv.org/abs/2509.05670)
*Gašper Podobnik,Tomaž Vrtovec*

Main category: cs.CV

TL;DR: The paper introduces MeshMetrics, a mesh-based framework to resolve inaccuracies in distance-based metric evaluation for image segmentation.


<details>
  <summary>Details</summary>
Motivation: Reproducibility in image segmentation research is hindered by inconsistent and inaccurate metric implementations.

Method: MeshMetrics uses a mesh-based approach for precise computation of distance-based metrics, avoiding common grid-based discretization errors.

Result: The framework demonstrates higher accuracy, better precision, and reduced discretization artifacts compared to conventional tools.

Conclusion: MeshMetrics improves reliability in image segmentation evaluations and is available as an open-source Python package.

Abstract: The surge of research in image segmentation has yielded remarkable
performance gains but also exposed a reproducibility crisis. A major
contributor is performance evaluation, where both selection and implementation
of metrics play critical roles. While recent efforts have improved the former,
the reliability of metric implementation has received far less attention.
Pitfalls in distance-based metric implementation can lead to considerable
discrepancies between common open-source tools, for instance, exceeding 100 mm
for the Hausdorff distance and 30%pt for the normalized surface distance for
the same pair of segmentations. To address these pitfalls, we introduce
MeshMetrics, a mesh-based framework that provides a more precise computation of
distance-based metrics than conventional grid-based approaches. Through
theoretical analysis and empirical validation, we demonstrate that MeshMetrics
achieves higher accuracy and precision than established tools, and is
substantially less affected by discretization artifacts, such as distance
quantization. We release MeshMetrics as an open-source Python package,
available at https://github.com/gasperpodobnik/MeshMetrics.

</details>


### [168] [Leveraging Vision-Language Large Models for Interpretable Video Action Recognition with Semantic Tokenization](https://arxiv.org/abs/2509.05695)
*Jingwei Peng,Zhixuan Qiu,Boyu Jin,Surasakdi Siripong*

Main category: cs.CV

TL;DR: The paper presents LVLM-VAR, a novel framework using Vision-Language Large Models (LVLMs) for video action recognition, improving accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with semantic understanding, contextual information, and fine-grained distinctions in video action recognition.

Method: Introduces the Video-to-Semantic-Tokens module to generate semantic action tokens from video sequences, processed by LoRA-fine-tuned LVLMs for classification and reasoning.

Result: Achieves state-of-the-art performance on benchmarks like NTU RGB+D, with significant accuracy improvements (e.g., 94.1% and 90.0% on respective datasets).

Conclusion: LVLM-VAR enhances video action recognition by combining semantic tokenization and LVLMs, improving both performance and interpretability.

Abstract: Human action recognition often struggles with deep semantic understanding,
complex contextual information, and fine-grained distinction, limitations that
traditional methods frequently encounter when dealing with diverse video data.
Inspired by the remarkable capabilities of large language models, this paper
introduces LVLM-VAR, a novel framework that pioneers the application of
pre-trained Vision-Language Large Models (LVLMs) to video action recognition,
emphasizing enhanced accuracy and interpretability. Our method features a
Video-to-Semantic-Tokens (VST) Module, which innovatively transforms raw video
sequences into discrete, semantically and temporally consistent "semantic
action tokens," effectively crafting an "action narrative" that is
comprehensible to an LVLM. These tokens, combined with natural language
instructions, are then processed by a LoRA-fine-tuned LVLM (e.g., LLaVA-13B)
for robust action classification and semantic reasoning. LVLM-VAR not only
achieves state-of-the-art or highly competitive performance on challenging
benchmarks such as NTU RGB+D and NTU RGB+D 120, demonstrating significant
improvements (e.g., 94.1% on NTU RGB+D X-Sub and 90.0% on NTU RGB+D 120 X-Set),
but also substantially boosts model interpretability by generating natural
language explanations for its predictions.

</details>


### [169] [JRN-Geo: A Joint Perception Network based on RGB and Normal images for Cross-view Geo-localization](https://arxiv.org/abs/2509.05696)
*Hongyu Zhou,Yunzhou Zhang,Tingsong Huang,Fawei Ge,Man Qi,Xichen Zhang,Yizhong Zhang*

Main category: cs.CV

TL;DR: The paper introduces a dual-branch network (JRN-Geo) for UAV cross-view geo-localization, combining RGB and normal images to address viewpoint changes and achieve better performance.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in cross-view geo-localization caused by drastic viewpoint differences and appearance variations between images, especially when spatial structural information is often overlooked.

Method: The paper proposes JRN-Geo, a dual-branch network combining RGB and normal images using a Difference-Aware Fusion Module (DAFM) and Joint-Constrained Interaction Aggregation (JCIA) strategy for joint semantic-structural representation. Additionally, a 3D geographic augmentation is introduced to simulate viewpoint variations.

Result: Experiments on University-1652 and SUES-200 datasets demonstrate the proposed method's robustness to viewpoint variations and its state-of-the-art performance.

Conclusion: Integrating geometric structural information significantly improves viewpoint-invariant feature learning for UAV localization, offering a robust solution for cross-view geo-localization tasks.

Abstract: Cross-view geo-localization plays a critical role in Unmanned Aerial Vehicle
(UAV) localization and navigation. However, significant challenges arise from
the drastic viewpoint differences and appearance variations between images.
Existing methods predominantly rely on semantic features from RGB images, often
neglecting the importance of spatial structural information in capturing
viewpoint-invariant features. To address this issue, we incorporate geometric
structural information from normal images and introduce a Joint perception
network to integrate RGB and Normal images (JRN-Geo). Our approach utilizes a
dual-branch feature extraction framework, leveraging a Difference-Aware Fusion
Module (DAFM) and Joint-Constrained Interaction Aggregation (JCIA) strategy to
enable deep fusion and joint-constrained semantic and structural information
representation. Furthermore, we propose a 3D geographic augmentation technique
to generate potential viewpoint variation samples, enhancing the network's
ability to learn viewpoint-invariant features. Extensive experiments on the
University-1652 and SUES-200 datasets validate the robustness of our method
against complex viewpoint ariations, achieving state-of-the-art performance.

</details>


### [170] [Knowledge-Augmented Vision Language Models for Underwater Bioacoustic Spectrogram Analysis](https://arxiv.org/abs/2509.05703)
*Ragib Amin Nihal,Benjamin Yen,Takeshi Ashizawa,Kazuhiro Nakadai*

Main category: cs.CV

TL;DR: The paper examines whether Vision Language Models (VLMs) can interpret marine mammal bioacoustic spectrograms without requiring retraining or manual annotations.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in overcoming the challenge of interpreting domain-specific visual data (bioacoustic spectrograms) using preexisting models (VLMs) not explicitly trained for this purpose.

Method: The approach combines VLM analysis of spectrograms with validation using Large Language Models (LLMs), aiming to integrate interpretation into domain knowledge.

Result: The framework successfully enables VLMs to adapt to acoustic data without requiring manual tasks such as annotation or model retraining.

Conclusion: VLMs, when paired with LLM-based validation, exhibit potential for meaningful visual interpretation in specialized fields like marine bioacoustics, streamlining data analysis.

Abstract: Marine mammal vocalization analysis depends on interpreting bioacoustic
spectrograms. Vision Language Models (VLMs) are not trained on these
domain-specific visualizations. We investigate whether VLMs can extract
meaningful patterns from spectrograms visually. Our framework integrates VLM
interpretation with LLM-based validation to build domain knowledge. This
enables adaptation to acoustic data without manual annotation or model
retraining.

</details>


### [171] [LiDAR-BIND-T: Improving SLAM with Temporally Consistent Cross-Modal LiDAR Reconstruction](https://arxiv.org/abs/2509.05728)
*Niels Balemans,Ali Anwar,Jan Steckel,Siegfried Mercelis*

Main category: cs.CV

TL;DR: LiDAR-BIND-T improves temporal and spatial coherence in radar/sonar-to-LiDAR translation with new metrics and methods.


<details>
  <summary>Details</summary>
Motivation: To enhance temporal stability and modality fusion in radar/sonar-to-LiDAR translation frameworks for better SLAM performance.

Method: Three core techniques: temporal embedding similarity, motion-aligned transformation loss, and windowed temporal fusion, along with updates to model architecture.

Result: Improved temporal/spatial coherence, lower trajectory error, and better occupancy map accuracy in SLAM evaluations.

Conclusion: LiDAR-BIND-T enhances robustness and performance in SLAM while maintaining modular plug-and-play fusion.

Abstract: This paper extends LiDAR-BIND, a modular multi-modal fusion framework that
binds heterogeneous sensors (radar, sonar) to a LiDAR-defined latent space,
with mechanisms that explicitly enforce temporal consistency. We introduce
three contributions: (i) temporal embedding similarity that aligns consecutive
latents, (ii) a motion-aligned transformation loss that matches displacement
between predictions and ground truth LiDAR, and (iii) windows temporal fusion
using a specialised temporal module. We further update the model architecture
to better preserve spatial structure. Evaluations on radar/sonar-to-LiDAR
translation demonstrate improved temporal and spatial coherence, yielding lower
absolute trajectory error and better occupancy map accuracy in
Cartographer-based SLAM (Simultaneous Localisation and Mapping). We propose
different metrics based on the Fr\'echet Video Motion Distance (FVMD) and a
correlation-peak distance metric providing practical temporal quality
indicators to evaluate SLAM performance. The proposed temporal LiDAR-BIND, or
LiDAR-BIND-T, maintains plug-and-play modality fusion while substantially
enhancing temporal stability, resulting in improved robustness and performance
for downstream SLAM.

</details>


### [172] [Multi-LVI-SAM: A Robust LiDAR-Visual-Inertial Odometry for Multiple Fisheye Cameras](https://arxiv.org/abs/2509.05740)
*Xinyu Zhang,Kai Huang,Junqiao Zhao,Zihan Yuan,Tiantian Feng*

Main category: cs.CV

TL;DR: The paper introduces Multi-LVI-SAM, a state estimation framework that integrates fisheye cameras, LiDAR, and inertial sensors. A novel panoramic visual feature model is proposed for unifying multi-camera data efficiently, enhancing accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: To improve the robustness and accuracy of state estimation by efficiently integrating data from multiple fisheye cameras, LiDAR, and inertial sensors.

Method: The method introduces a panoramic visual feature model that consolidates multi-camera observations into a unified representation. It incorporates extrinsic compensation to address triangulation inconsistencies and adopts a tightly coupled LiDAR-visual-inertial system using a factor graph.

Result: Experiments on public datasets show that the proposed model improves the quality and consistency of multi-camera constraints, achieving higher accuracy and robustness compared to existing systems.

Conclusion: The panoramic visual feature model successfully simplifies multi-camera integration, resolves inconsistency issues, and enhances the overall state estimation framework, positioning it as superior to existing systems.

Abstract: We propose a multi-camera LiDAR-visual-inertial odometry framework,
Multi-LVI-SAM, which fuses data from multiple fisheye cameras, LiDAR and
inertial sensors for highly accurate and robust state estimation. To enable
efficient and consistent integration of visual information from multiple
fisheye cameras, we introduce a panoramic visual feature model that unifies
multi-camera observations into a single representation. The panoramic model
serves as a global geometric optimization framework that consolidates
multi-view constraints, enabling seamless loop closure and global pose
optimization, while simplifying system design by avoiding redundant handling of
individual cameras. To address the triangulation inconsistency caused by the
misalignment between each camera's frame and the panoramic model's frame, we
propose an extrinsic compensation method. This method improves feature
consistency across views and significantly reduces triangulation and
optimization errors, leading to more accurate pose estimation. We integrate the
panoramic visual feature model into a tightly coupled LiDAR-visual-inertial
system based on a factor graph. Extensive experiments on public datasets
demonstrate that the panoramic visual feature model enhances the quality and
consistency of multi-camera constraints, resulting in higher accuracy and
robustness than existing multi-camera LiDAR-visual-inertial systems.

</details>


### [173] [Depth-Aware Super-Resolution via Distance-Adaptive Variational Formulation](https://arxiv.org/abs/2509.05746)
*Tianhao Guo,Bingjie Lu,Feng Wang,Zhengyang Lu*

Main category: cs.CV

TL;DR: This paper proposes a novel super-resolution framework that incorporates spatially adaptive and depth-aware techniques to address limitations in real-world imaging conditions like distance-dependent effects.


<details>
  <summary>Details</summary>
Motivation: Existing single image super-resolution models often rely on spatially-invariant degradation assumptions, which fail to accommodate real-world imaging distortions caused by atmospheric scattering, depth-of-field, and perspective effects.

Method: The authors developed a variational framework that models distance-dependent degradation as a pseudodifferential operator. They implemented a neural architecture using depth-conditional convolution kernels and spectral constraints derived from atmospheric theory to dynamically adjust reconstructions based on depth-specific characteristics.

Result: The proposed model achieves state-of-the-art super-resolution results on benchmark datasets like KITTI, showing 36.89/0.9516 PSNR/SSIM at 2× and 30.54/0.8721 at 4× scales while surpassing existing methods by up to 0.44dB.

Conclusion: This study introduces the first theoretically-grounded framework for distance-adaptive super-resolution, demonstrating notable advancements in handling depth-variant degradation in real-world imaging scenarios and achieving competitive or superior outcomes on conventional benchmarks.

Abstract: Single image super-resolution traditionally assumes spatially-invariant
degradation models, yet real-world imaging systems exhibit complex
distance-dependent effects including atmospheric scattering, depth-of-field
variations, and perspective distortions. This fundamental limitation
necessitates spatially-adaptive reconstruction strategies that explicitly
incorporate geometric scene understanding for optimal performance. We propose a
rigorous variational framework that characterizes super-resolution as a
spatially-varying inverse problem, formulating the degradation operator as a
pseudodifferential operator with distance-dependent spectral characteristics
that enable theoretical analysis of reconstruction limits across depth ranges.
Our neural architecture implements discrete gradient flow dynamics through
cascaded residual blocks with depth-conditional convolution kernels, ensuring
convergence to stationary points of the theoretical energy functional while
incorporating learned distance-adaptive regularization terms that dynamically
adjust smoothness constraints based on local geometric structure. Spectral
constraints derived from atmospheric scattering theory prevent bandwidth
violations and noise amplification in far-field regions, while adaptive kernel
generation networks learn continuous mappings from depth to reconstruction
filters. Comprehensive evaluation across five benchmark datasets demonstrates
state-of-the-art performance, achieving 36.89/0.9516 and 30.54/0.8721 PSNR/SSIM
at 2 and 4 scales on KITTI outdoor scenes, outperforming existing methods by
0.44dB and 0.36dB respectively. This work establishes the first
theoretically-grounded distance-adaptive super-resolution framework and
demonstrates significant improvements on depth-variant scenarios while
maintaining competitive performance across traditional benchmarks.

</details>


### [174] [InterAct: A Large-Scale Dataset of Dynamic, Expressive and Interactive Activities between Two People in Daily Scenarios](https://arxiv.org/abs/2509.05747)
*Leo Ho,Yinghao Huang,Dafei Qin,Mingyi Shi,Wangpok Tse,Wei Liu,Junichi Yamagishi,Taku Komura*

Main category: cs.CV

TL;DR: The paper introduces InterAct, a dataset capturing realistic, dynamic two-person interactions, and presents a diffusion-based method for estimating body motions and facial expressions from speech inputs.


<details>
  <summary>Details</summary>
Motivation: To address the lack of datasets and models capturing realistic, long-duration, and semantically rich interactions between two people in complex scenarios.

Method: The paper introduces the InterAct dataset, containing multimodal data (audio, motion, facial expressions) for two individuals. Additionally, it proposes a diffusion-based method for estimating interactions, with hierarchical body motion regression and fine-tuning for lip synchronization.

Result: The InterAct dataset showcases diverse and complex interaction patterns, and the proposed method effectively estimates two-person interactions from speech inputs.

Conclusion: The dataset and method offer new opportunities for studying and modeling realistic two-person interactions, filling a gap in prior research.

Abstract: We address the problem of accurate capture of interactive behaviors between
two people in daily scenarios. Most previous works either only consider one
person or solely focus on conversational gestures of two people, assuming the
body orientation and/or position of each actor are constant or barely change
over each interaction. In contrast, we propose to simultaneously model two
people's activities, and target objective-driven, dynamic, and semantically
consistent interactions which often span longer duration and cover bigger
space. To this end, we capture a new multi-modal dataset dubbed InterAct, which
is composed of 241 motion sequences where two people perform a realistic and
coherent scenario for one minute or longer over a complete interaction. For
each sequence, two actors are assigned different roles and emotion labels, and
collaborate to finish one task or conduct a common interaction activity. The
audios, body motions, and facial expressions of both persons are captured.
InterAct contains diverse and complex motions of individuals and interesting
and relatively long-term interaction patterns barely seen before. We also
demonstrate a simple yet effective diffusion-based method that estimates
interactive face expressions and body motions of two people from speech inputs.
Our method regresses the body motions in a hierarchical manner, and we also
propose a novel fine-tuning mechanism to improve the lip accuracy of facial
expressions. To facilitate further research, the data and code is made
available at https://hku-cg.github.io/interact/ .

</details>


### [175] [AdCare-VLM: Leveraging Large Vision Language Model (LVLM) to Monitor Long-Term Medication Adherence and Care](https://arxiv.org/abs/2505.00275)
*Md Asaduzzaman Jabin,Hanqi Jiang,Yiwei Li,Patrick Kaggwa,Eugene Douglass,Juliet N. Sekandi,Tianming Liu*

Main category: cs.CV

TL;DR: The paper introduces AdCare-VLM, a Video-LLaVA-based multimodal model for analyzing medication adherence through videos, achieving better performance than existing models.


<details>
  <summary>Details</summary>
Motivation: Improving medication adherence in chronic diseases due to significant health risks when adherence is low, especially given existing challenges like patient behavior and insufficient healthcare infrastructure.

Method: Developing AdCare-VLM, a multimodal vision language model trained on a private medical video dataset to detect and analyze medication adherence visually and linguistically, supported by fine-tuning and detailed VQA dataset creation.

Result: AdCare-VLM outperforms other leading models like LLaVA-V1.5 and Chat-UniVi by 3.1-3.54% through improved correlations between visual and linguistic representations in adherence detection.

Conclusion: The model advances the state-of-the-art in medication adherence analysis by employing an innovative multimodal approach, validated through experiments and interpretability studies.

Abstract: Chronic diseases, including diabetes, hypertension, asthma, HIV-AIDS,
epilepsy, and tuberculosis, necessitate rigorous adherence to medication to
avert disease progression, manage symptoms, and decrease mortality rates.
Adherence is frequently undermined by factors including patient behavior,
caregiver support, elevated medical costs, and insufficient healthcare
infrastructure. We propose AdCare-VLM, a specialized Video-LLaVA-based
multimodal large vision language model (LVLM) aimed at visual question
answering (VQA) concerning medication adherence through patient videos. We
employ a private dataset comprising 806 custom-annotated tuberculosis (TB)
medication monitoring videos, which have been labeled by clinical experts, to
fine-tune the model for adherence pattern detection. We present LLM-TB-VQA, a
detailed medical adherence VQA dataset that encompasses positive, negative, and
ambiguous adherence cases. Our method identifies correlations between visual
features, such as the clear visibility of the patient's face, medication, water
intake, and the act of ingestion, and their associated medical concepts in
captions. This facilitates the integration of aligned visual-linguistic
representations and improves multimodal interactions. Experimental results
indicate that our method surpasses parameter-efficient fine-tuning (PEFT)
enabled VLM models, such as LLaVA-V1.5 and Chat-UniVi, with absolute
improvements ranging from 3.1% to 3.54% across pre-trained, regular, and
low-rank adaptation (LoRA) configurations. Comprehensive ablation studies and
attention map visualizations substantiate our approach, enhancing
interpretability.

</details>


### [176] [Unleashing Hierarchical Reasoning: An LLM-Driven Framework for Training-Free Referring Video Object Segmentation](https://arxiv.org/abs/2509.05751)
*Bingrui Zhao,Lin Yuanbo Wu,Xiangtian Fan,Deyin Liu,Lu Zhang,Ruyi He,Jialie Shen,Ximing Li*

Main category: cs.CV

TL;DR: PARSE-VOS is a new framework for referring video object segmentation that combines hierarchical reasoning with large language models to achieve state-of-the-art results without training.


<details>
  <summary>Details</summary>
Motivation: The key challenge in RVOS is aligning static natural language descriptions with dynamic and visually complex video content, especially for objects with similar appearances but varying motion and poses.

Method: PARSE-VOS employs a hierarchical, coarse-to-fine reasoning approach. It starts by parsing the language query into structured semantic commands, grounds all potential object trajectories, and hierarchically identifies the correct target using motion reasoning and conditional pose verification, powered by LLMs.

Result: PARSE-VOS outperformed existing methods and set new benchmarks on Ref-YouTube-VOS, Ref-DAVIS17, and MeViS datasets.

Conclusion: By leveraging structured reasoning via LLMs and dynamically handling coarse-to-fine disambiguation, PARSE-VOS provides a robust, training-free solution for complex RVOS tasks.

Abstract: Referring Video Object Segmentation (RVOS) aims to segment an object of
interest throughout a video based on a language description. The prominent
challenge lies in aligning static text with dynamic visual content,
particularly when objects exhibiting similar appearances with inconsistent
motion and poses. However, current methods often rely on a holistic
visual-language fusion that struggles with complex, compositional descriptions.
In this paper, we propose \textbf{PARSE-VOS}, a novel, training-free framework
powered by Large Language Models (LLMs), for a hierarchical, coarse-to-fine
reasoning across text and video domains. Our approach begins by parsing the
natural language query into structured semantic commands. Next, we introduce a
spatio-temporal grounding module that generates all candidate trajectories for
all potential target objects, guided by the parsed semantics. Finally, a
hierarchical identification module select the correct target through a
two-stage reasoning process: it first performs coarse-grained motion reasoning
with an LLM to narrow down candidates; if ambiguity remains, a fine-grained
pose verification stage is conditionally triggered to disambiguate. The final
output is an accurate segmentation mask for the target object.
\textbf{PARSE-VOS} achieved state-of-the-art performance on three major
benchmarks: Ref-YouTube-VOS, Ref-DAVIS17, and MeViS.

</details>


### [177] [PictOBI-20k: Unveiling Large Multimodal Models in Visual Decipherment for Pictographic Oracle Bone Characters](https://arxiv.org/abs/2509.05773)
*Zijian Chen,Wenjie Hua,Jinhao Li,Lirong Deng,Fan Du,Tingzhu Chen,Guangtao Zhai*

Main category: cs.CV

TL;DR: This paper introduces PictOBI-20k, a dataset for testing large multimodal models (LMMs) in visually deciphering oracle bone characters (OBCs). Initial findings show that while LMMs demonstrate basic deciphering abilities, their performance is influenced more by language priors than visual analysis.


<details>
  <summary>Details</summary>
Motivation: Deciphering Oracle Bone Characters (OBCs) is essential for understanding early Chinese historical contexts, but current methods are limited by fragmented archaeological data and small inscription datasets. Leveraging LMMs may overcome these constraints.

Method: The authors developed a dataset named PictOBI-20k comprising 20k OBC and real-world object images paired with 15k multi-choice questions. They used this dataset to evaluate LMMs' deciphering capabilities and conducted human annotations for consistency checks.

Result: Initial experiments reveal that LMMs exhibit basic visual deciphering capabilities but rely heavily on language models and fail to effectively utilize visual information.

Conclusion: PictOBI-20k provides a valuable tool for evaluating and refining visual attention in LMMs for OBC decipherment. The findings highlight areas for optimization in LMMs' visual reasoning for historical scripts.

Abstract: Deciphering oracle bone characters (OBCs), the oldest attested form of
written Chinese, has remained the ultimate, unwavering goal of scholars,
offering an irreplaceable key to understanding humanity's early modes of
production. Current decipherment methodologies of OBC are primarily constrained
by the sporadic nature of archaeological excavations and the limited corpus of
inscriptions. With the powerful visual perception capability of large
multimodal models (LMMs), the potential of using LMMs for visually deciphering
OBCs has increased. In this paper, we introduce PictOBI-20k, a dataset designed
to evaluate LMMs on the visual decipherment tasks of pictographic OBCs. It
includes 20k meticulously collected OBC and real object images, forming over
15k multi-choice questions. We also conduct subjective annotations to
investigate the consistency of the reference point between humans and LMMs in
visual reasoning. Experiments indicate that general LMMs possess preliminary
visual decipherment skills, and LMMs are not effectively using visual
information, while most of the time they are limited by language priors. We
hope that our dataset can facilitate the evaluation and optimization of visual
attention in future OBC-oriented LMMs. The code and dataset will be available
at https://github.com/OBI-Future/PictOBI-20k.

</details>


### [178] [Posterior shape models revisited: Improving 3D reconstructions from partial data using target specific models](https://arxiv.org/abs/2509.05776)
*Jonathan Aellen,Florian Burkhardt,Thomas Vetter,Marcel Lüthi*

Main category: cs.CV

TL;DR: This paper addresses the crucial impact of pose alignment in partial shape reconstructions in medical imaging and proposes an efficient method for pose adjustment to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the overlooked problem of pose misalignment in training and target data for statistical shape reconstruction, which leads to biased solutions, especially when using small parts of shapes.

Method: The authors propose an efficient preprocessing method to adjust pose alignment of existing shape models without needing the original training data, preserving computational efficiency while improving reconstruction accuracy.

Result: The proposed method effectively recovers aligned models for translations and approximates small rotations. It enhances accuracy and predicted variance compared to conventional models.

Conclusion: The method provides a computationally efficient, broadly applicable improvement to shape reconstruction pipelines, enabling plug-and-play integration and better results without requiring original training datasets.

Abstract: In medical imaging, point distribution models are often used to reconstruct
and complete partial shapes using a statistical model of the full shape. A
commonly overlooked, but crucial factor in this reconstruction process, is the
pose of the training data relative to the partial target shape. A difference in
pose alignment of the training and target shape leads to biased solutions,
particularly when observing small parts of a shape. In this paper, we
demonstrate the importance of pose alignment for partial shape reconstructions
and propose an efficient method to adjust an existing model to a specific
target. Our method preserves the computational efficiency of linear models
while significantly improving reconstruction accuracy and predicted variance.
It exactly recovers the intended aligned model for translations, and provides a
good approximation for small rotations, all without access to the original
training data. Hence, existing shape models in reconstruction pipelines can be
adapted by a simple preprocessing step, making our approach widely applicable
in plug-and-play scenarios.

</details>


### [179] [3DPillars: Pillar-based two-stage 3D object detection](https://arxiv.org/abs/2509.05780)
*Jongyoun Noh,Junghyup Lee,Hyekang Park,Bumsub Ham*

Main category: cs.CV

TL;DR: PointPillars is enhanced with a two-stage 3D object detection framework called 3DPillars, addressing limitations in preserving 3D structures and adopting two-stage detection.


<details>
  <summary>Details</summary>
Motivation: PointPillars is fast but struggles with capturing precise 3D features and implementing two-stage detection, thus falling behind state-of-the-art methods.

Method: The study introduces 3DPillars leveraging pseudo image stacks efficiently with 2D convolutions. It features a separable voxel module and an RoI head with sparse scene context.

Result: Experiments on KITTI and Waymo datasets showcase that 3DPillars balances speed and accuracy while refining 3D proposals effectively.

Conclusion: 3DPillars significantly enhances 3D detection efficiency and accuracy, narrowing performance gaps compared to state-of-the-art methods.

Abstract: PointPillars is the fastest 3D object detector that exploits pseudo image
representations to encode features for 3D objects in a scene. Albeit efficient,
PointPillars is typically outperformed by state-of-the-art 3D detection methods
due to the following limitations: 1) The pseudo image representations fail to
preserve precise 3D structures, and 2) they make it difficult to adopt a
two-stage detection pipeline using 3D object proposals that typically shows
better performance than a single-stage approach. We introduce in this paper the
first two-stage 3D detection framework exploiting pseudo image representations,
narrowing the performance gaps between PointPillars and state-of-the-art
methods, while retaining its efficiency. Our framework consists of two novel
components that overcome the aforementioned limitations of PointPillars: First,
we introduce a new CNN architecture, dubbed 3DPillars, that enables learning 3D
voxel-based features from the pseudo image representation efficiently using 2D
convolutions. The basic idea behind 3DPillars is that 3D features from voxels
can be viewed as a stack of pseudo images. To implement this idea, we propose a
separable voxel feature module that extracts voxel-based features without using
3D convolutions. Second, we introduce an RoI head with a sparse scene context
feature module that aggregates multi-scale features from 3DPillars to obtain a
sparse scene feature. This enables adopting a two-stage pipeline effectively,
and fully leveraging contextual information of a scene to refine 3D object
proposals. Experimental results on the KITTI and Waymo Open datasets
demonstrate the effectiveness and efficiency of our approach, achieving a good
compromise in terms of speed and accuracy.

</details>


### [180] [CRAB: Camera-Radar Fusion for Reducing Depth Ambiguity in Backward Projection based View Transformation](https://arxiv.org/abs/2509.05785)
*In-Jae Lee,Sihwan Hwang,Youngseok Kim,Wonjune Kim,Sanmin Kim,Dongsuk Kum*

Main category: cs.CV

TL;DR: The paper proposes CRAB, a novel 3D object detection and segmentation model that fuses camera and radar data to address depth ambiguity issues in backward projection for BEV transformations.


<details>
  <summary>Details</summary>
Motivation: Camera-radar fusion for 3D object detection has the potential to leverage complementary sensor traits, but existing methods face challenges with sparse BEV features or depth ambiguity, leading to inaccuracies.

Method: CRAB employs backward projection with radar data to mitigate depth ambiguity and introduces spatial cross-attention for enhanced 3D scene comprehension during view transformation.

Result: The method achieved state-of-the-art performance on the nuScenes dataset with 62.4% NDS and 54.0% mAP in 3D object detection.

Conclusion: CRAB effectively reduces depth ambiguity in backward projection-based camera-radar fusion, improving accuracy and performance in 3D object detection tasks.

Abstract: Recently, camera-radar fusion-based 3D object detection methods in bird's eye
view (BEV) have gained attention due to the complementary characteristics and
cost-effectiveness of these sensors. Previous approaches using forward
projection struggle with sparse BEV feature generation, while those employing
backward projection overlook depth ambiguity, leading to false positives. In
this paper, to address the aforementioned limitations, we propose a novel
camera-radar fusion-based 3D object detection and segmentation model named CRAB
(Camera-Radar fusion for reducing depth Ambiguity in Backward projection-based
view transformation), using a backward projection that leverages radar to
mitigate depth ambiguity. During the view transformation, CRAB aggregates
perspective view image context features into BEV queries. It improves depth
distinction among queries along the same ray by combining the dense but
unreliable depth distribution from images with the sparse yet precise depth
information from radar occupancy. We further introduce spatial cross-attention
with a feature map containing radar context information to enhance the
comprehension of the 3D scene. When evaluated on the nuScenes open dataset, our
proposed approach achieves a state-of-the-art performance among backward
projection-based camera-radar fusion methods with 62.4\% NDS and 54.0\% mAP in
3D object detection.

</details>


### [181] [Dual-Mode Deep Anomaly Detection for Medical Manufacturing: Structural Similarity and Feature Distance](https://arxiv.org/abs/2509.05796)
*Julio Zanon Diaz,Georgios Siogkas,Peter Corcoran*

Main category: cs.CV

TL;DR: The paper proposes two attention-guided autoencoder architectures for deep anomaly detection in medical device manufacturing, addressing constraints like small datasets and regulatory requirements.


<details>
  <summary>Details</summary>
Motivation: Challenges in automating visual inspection in medical device manufacturing due to the need for high accuracy, regulatory compliance, small datasets, and high-resolution imagery.

Method: Two novel autoencoder architectures: one with structural similarity-based anomaly scoring (4-MS-SSIM) and another using Mahalanobis scoring based on reduced latent features.

Result: The first method achieves ACC of 0.931 supervised thresholding and the second achieves ACC of 0.722 supervised thresholding, both outperforming baseline methods.

Conclusion: The methods deliver complementary strengths, supporting inline inspection and post-production surveillance, aligning with accuracy, efficiency, and regulatory requirements of EU AI Act.

Abstract: Automating visual inspection in medical device manufacturing remains
challenging due to small and imbalanced datasets, high-resolution imagery, and
stringent regulatory requirements. This work proposes two attention-guided
autoencoder architectures for deep anomaly detection designed to address these
constraints. The first employs a structural similarity-based anomaly score
(4-MS-SSIM), offering lightweight and accurate real-time defect detection,
yielding ACC 0.903 (unsupervised thresholding) and 0.931 (supervised
thresholding) on the - Surface Seal Image - Test split with only 10% of
defective samples. The second applies a feature-distance approach using
Mahalanobis scoring on reduced latent features, providing high sensitivity to
distributional shifts for supervisory monitoring, achieving ACC 0.722 with
supervised thresholding. Together, these methods deliver complementary
capabilities: the first supports reliable inline inspection, while the second
enables scalable post-production surveillance and regulatory compliance
monitoring. Experimental results demonstrate that both approaches surpass
re-implemented baselines and provide a practical pathway for deploying deep
anomaly detection in regulated manufacturing environments, aligning accuracy,
efficiency, and the regulatory obligations defined for high-risk AI systems
under the EU AI Act.

</details>


### [182] [A Probabilistic Segment Anything Model for Ambiguity-Aware Medical Image Segmentation](https://arxiv.org/abs/2509.05809)
*Tyler Ward,Abdullah Imran*

Main category: cs.CV

TL;DR: The paper introduces Probabilistic SAM, an extension of the Segment Anything Model (SAM), which captures variability in segmentation by modeling a distribution over outputs, particularly addressing ambiguity in medical imaging.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the deterministic nature of existing segmentation models like SAM that fail to capture inherent segmentation ambiguity in real-world and medical imaging tasks.

Method: The paper introduces a probabilistic framework by incorporating a latent variable space and a variational objective into SAM, enabling diverse mask generation. A prior and posterior network modify the prompt embeddings during inference, facilitating efficient uncertainty-aware outputs.

Result: Probabilistic SAM was evaluated on the LIDC-IDRI lung nodule dataset and demonstrated the capability to generate diverse segmentations that reflect expert disagreements, outperforming other methods on uncertainty-aware metrics.

Conclusion: Probabilistic SAM successfully generates diverse, plausible segmentations representative of annotation variability, enhancing the robustness of segmentation models in ambiguous tasks.

Abstract: Recent advances in promptable segmentation, such as the Segment Anything
Model (SAM), have enabled flexible, high-quality mask generation across a wide
range of visual domains. However, SAM and similar models remain fundamentally
deterministic, producing a single segmentation per object per prompt, and fail
to capture the inherent ambiguity present in many real-world tasks. This
limitation is particularly troublesome in medical imaging, where multiple
plausible segmentations may exist due to annotation uncertainty or inter-expert
variability. In this paper, we introduce Probabilistic SAM, a probabilistic
extension of SAM that models a distribution over segmentations conditioned on
both the input image and prompt. By incorporating a latent variable space and
training with a variational objective, our model learns to generate diverse and
plausible segmentation masks reflecting the variability in human annotations.
The architecture integrates a prior and posterior network into the SAM
framework, allowing latent codes to modulate the prompt embeddings during
inference. The latent space allows for efficient sampling during inference,
enabling uncertainty-aware outputs with minimal overhead. We evaluate
Probabilistic SAM on the public LIDC-IDRI lung nodule dataset and demonstrate
its ability to produce diverse outputs that align with expert disagreement,
outperforming existing probabilistic baselines on uncertainty-aware metrics.
Our code is available at: https://github.com/tbwa233/Probabilistic-SAM/.

</details>


### [183] [Near Real-Time Dust Aerosol Detection with 3D Convolutional Neural Networks on MODIS Data](https://arxiv.org/abs/2509.05887)
*Caleb Gates,Patrick Moorhead,Jayden Ferguson,Omar Darwish,Conner Stallman,Pablo Rivas,Paapa Quansah*

Main category: cs.CV

TL;DR: The paper introduces a system for real-time detection of dust storms using NASA's satellite data and advanced deep learning methods.


<details>
  <summary>Details</summary>
Motivation: Dust storms adversely impact health and visibility, necessitating quick detection solutions to address these issues in a global context.

Method: The study employs a 3D convolutional network analyzing 36 MODIS bands to segregate dust from clouds and surfaces, with preprocessing techniques tackling data gaps. Enhancements boost training speed by 21x.

Result: On 17 independent datasets, the model achieved approximately 0.92 accuracy, with minor misses identified along plume edges.

Conclusion: Joint learning of spectral and spatial data facilitates efficient global dust detection, with future potential improvements noted through extended windows or attention-based techniques.

Abstract: Dust storms harm health and reduce visibility; quick detection from
satellites is needed. We present a near real-time system that flags dust at the
pixel level using multi-band images from NASA's Terra and Aqua (MODIS). A 3D
convolutional network learns patterns across all 36 bands, plus split thermal
bands, to separate dust from clouds and surface features. Simple normalization
and local filling handle missing data. An improved version raises training
speed by 21x and supports fast processing of full scenes. On 17 independent
MODIS scenes, the model reaches about 0.92 accuracy with a mean squared error
of 0.014. Maps show strong agreement in plume cores, with most misses along
edges. These results show that joint band-and-space learning can provide timely
dust alerts at global scale; using wider input windows or attention-based
models may further sharpen edges.

</details>


### [184] [Challenges in Deep Learning-Based Small Organ Segmentation: A Benchmarking Perspective for Medical Research with Limited Datasets](https://arxiv.org/abs/2509.05892)
*Phongsakon Mark Konrad,Andrei-Alexandru Popa,Yaser Sabzehmeidani,Liang Zhong,Elisa A. Liehn,Serkan Ayvaz*

Main category: cs.CV

TL;DR: The paper evaluates deep learning segmentation models on limited cardiovascular histopathological data, revealing performance sensitivity to data splits and questioning current benchmarking methods.


<details>
  <summary>Details</summary>
Motivation: The scarcity of annotated cardiovascular histopathological data limits the development of segmentation models for advancing cardiovascular disease diagnosis and research.

Method: The paper systematically evaluates multiple state-of-the-art deep learning segmentation approaches, including CNNs, vision transformers, and foundation models, utilizing Bayesian hyperparameter optimization.

Result: Results show that model performance is highly sensitive to data splits, with differences largely attributed to statistical noise rather than intrinsic superiority of the algorithms.

Conclusion: Standard benchmarking practices in low-data clinical settings may be unreliable, necessitating a reevaluation of assumptions about performance rankings and clinical utility improvements.

Abstract: Accurate segmentation of carotid artery structures in histopathological
images is vital for advancing cardiovascular disease research and diagnosis.
However, deep learning model development in this domain is constrained by the
scarcity of annotated cardiovascular histopathological data. This study
investigates a systematic evaluation of state-of-the-art deep learning
segmentation models, including convolutional neural networks (U-Net,
DeepLabV3+), a Vision Transformer (SegFormer), and recent foundation models
(SAM, MedSAM, MedSAM+UNet), on a limited dataset of cardiovascular histology
images. Despite employing an extensive hyperparameter optimization strategy
with Bayesian search, our findings reveal that model performance is highly
sensitive to data splits, with minor differences driven more by statistical
noise than by true algorithmic superiority. This instability exposes the
limitations of standard benchmarking practices in low-data clinical settings
and challenges the assumption that performance rankings reflect meaningful
clinical utility.

</details>


### [185] [BTCChat: Advancing Remote Sensing Bi-temporal Change Captioning with Multimodal Large Language Model](https://arxiv.org/abs/2509.05895)
*Yujie Li,Wenjia Xu,Yuanben Zhang,Zhiwei Wei,Mugen Peng*

Main category: cs.CV

TL;DR: BTCChat is a novel multi-temporal large language model designed for bi-temporal satellite imagery analysis, outperforming current models in change captioning and visual question answering.


<details>
  <summary>Details</summary>
Motivation: Existing approaches inadequately model temporal correlations and semantic changes in bi-temporal satellite imagery, hampering change analysis effectiveness.

Method: BTCChat incorporates a Change Extraction module for better temporal and semantic modeling and a Prompt Augmentation mechanism to enhance spatial attention.

Result: BTCChat delivers state-of-the-art performance on bi-temporal change captioning and visual question answering tasks.

Conclusion: BTCChat advances bi-temporal change analysis by improving temporal and spatial feature extraction and achieving superior performance in relevant tasks.

Abstract: Bi-temporal satellite imagery supports critical applications such as urban
development monitoring and disaster assessment. Although powerful multimodal
large language models (MLLMs) have been applied in bi-temporal change analysis,
previous methods process image pairs through direct concatenation, inadequately
modeling temporal correlations and spatial semantic changes. This deficiency
hampers visual-semantic alignment in change understanding, thereby constraining
the overall effectiveness of current approaches. To address this gap, we
propose BTCChat, a multi-temporal MLLM with advanced bi-temporal change
understanding capability. BTCChat supports bi-temporal change captioning and
retains single-image interpretation capability. To better capture temporal
features and spatial semantic changes in image pairs, we design a Change
Extraction module. Moreover, to enhance the model's attention to spatial
details, we introduce a Prompt Augmentation mechanism, which incorporates
contextual clues into the prompt to enhance model performance. Experimental
results demonstrate that BTCChat achieves state-of-the-art performance on
change captioning and visual question answering tasks.

</details>


### [186] [A Fine-Grained Attention and Geometric Correspondence Model for Musculoskeletal Risk Classification in Athletes Using Multimodal Visual and Skeletal Features](https://arxiv.org/abs/2509.05913)
*Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Tamanna Shermin,Md Rafiqul Islam,Mukhtar Hussain,Sami Azam*

Main category: cs.CV

TL;DR: The paper introduces a multimodal deep learning model, ViSK-GAT, for musculoskeletal risk classification using visual and skeletal data. The model shows strong performance metrics and surpasses existing methods.


<details>
  <summary>Details</summary>
Motivation: Musculoskeletal disorders are critical for athletes, but existing risk assessment methods are unsuitable for complex environments due to reliance on single data types, necessitating a more robust model.

Method: The ViSK-GAT model combines visual and skeletal coordinate data with novel modules like FGAM for inter-modal feature refinement and MGCM for cross-modal coherence. The framework leverages a Residual Block and a Lightweight Transformer Block to capture spatial and temporal dependencies.

Result: ViSK-GAT achieved accuracies of 93.55% (validation) and 93.89% (test), along with strong precision, F1 scores, and low error rates across metrics. It outperformed nine transfer learning backbones in the experiment.

Conclusion: ViSK-GAT advances musculoskeletal risk classification through its superior performance, enabling effective early interventions in sports with a robust multimodal AI framework.

Abstract: Musculoskeletal disorders pose significant risks to athletes, and assessing
risk early is important for prevention. However, most existing methods are
designed for controlled settings and fail to reliably assess risk in complex
environments due to their reliance on a single type of data. This research
proposes ViSK-GAT (Visual-Skeletal Geometric Attention Transformer), a novel
multimodal deep learning framework designed to classify musculoskeletal risk
using visual and skeletal coordinate-based features. In addition, a custom
multimodal dataset is constructed by combining visual data and skeletal
coordinates for risk assessment. Each sample is labeled into eight risk
categories based on the Rapid Entire Body Assessment system. ViSK-GAT combines
a Residual Block with a Lightweight Transformer Block to learn spatial and
temporal dependencies jointly. It incorporates two novel modules: the
Fine-Grained Attention Module (FGAM), which enables precise inter-modal feature
refinement through cross-attention between visual and skeletal inputs, and the
Multimodal Geometric Correspondence Module (MGCM), which enhances cross-modal
coherence by aligning image features with coordinate-based representations.
ViSK-GAT achieved strong performance with validation and test accuracies of
93.55\% and 93.89\%, respectively; a precision of 93.86\%; an F1 score of
93.85\%; and Cohen's Kappa and Matthews Correlation Coefficient of 93\%. The
regression results also indicated a low Root Mean Square Error of the predicted
probability distribution of 0.1205 and a corresponding Mean Absolute Error of
0.0156. Compared to nine popular transfer learning backbones, ViSK-GAT
consistently outperformed previous methods. The ViSK-GAT model advances
artificial intelligence implementation and application, transforming
musculoskeletal risk classification and enabling impactful early interventions
in sports.

</details>


### [187] [Multi-Modal Camera-Based Detection of Vulnerable Road Users](https://arxiv.org/abs/2509.06333)
*Penelope Brown,Julie Stephany Berrio Perez,Mao Shan,Stewart Worrall*

Main category: cs.CV

TL;DR: This paper develops a multimodal framework combining RGB and thermal imaging with YOLOv8, enhancing vulnerable road user (VRU) detection in challenging scenarios.


<details>
  <summary>Details</summary>
Motivation: The primary motivation is to address the challenges in detecting vulnerable road users (VRUs) in poor lighting, adverse weather, and unbalanced datasets, as these users account for over half of global traffic deaths.

Method: The method integrates RGB and thermal imaging with a fine-tuned YOLOv8 model. It uses datasets such as KITTI, BDD100K, and Teledyne FLIR, with strategies like class re-weighting, light augmentations, 640-pixel resolution optimization, and partial backbone freezing to improve accuracy and recall.

Result: Experiments demonstrate the superiority of thermal models in precision and highlight that RGB-to-thermal augmentation enhances recall, particularly for rare VRU classes.

Conclusion: The study concludes that multimodal detection frameworks integrating thermal and RGB modalities can significantly enhance VRU detection accuracy and safety, especially in challenging conditions and at intersections.

Abstract: Vulnerable road users (VRUs) such as pedestrians, cyclists, and motorcyclists
represent more than half of global traffic deaths, yet their detection remains
challenging in poor lighting, adverse weather, and unbalanced data sets. This
paper presents a multimodal detection framework that integrates RGB and thermal
infrared imaging with a fine-tuned YOLOv8 model. Training leveraged KITTI,
BDD100K, and Teledyne FLIR datasets, with class re-weighting and light
augmentations to improve minority-class performance and robustness, experiments
show that 640-pixel resolution and partial backbone freezing optimise accuracy
and efficiency, while class-weighted losses enhance recall for rare VRUs.
Results highlight that thermal models achieve the highest precision, and
RGB-to-thermal augmentation boosts recall, demonstrating the potential of
multimodal detection to improve VRU safety at intersections.

</details>


### [188] [Compression Beyond Pixels: Semantic Compression with Multimodal Foundation Models](https://arxiv.org/abs/2509.05925)
*Ruiqi Shen,Haotian Wu,Wenjing Zhang,Jiangjing Hu,Deniz Gunduz*

Main category: cs.CV

TL;DR: The paper introduces a semantic compression method leveraging CLIP models to preserve semantic information, achieving significantly lower bitrates compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Emerging applications require compression methods that focus on semantic preservation over pixel-perfect reconstruction for robust performance in diverse contexts.

Method: The authors propose compressing CLIP feature embeddings instead of images, ensuring semantic integrity by prioritizing semantic information preservation.

Result: The method achieves semantic preservation with less than 5% of the bitrate required by standard techniques, delivering robust zero-shot performance across tasks and datasets.

Conclusion: This approach advances semantic compression research, highlighting the benefits of multimodal foundation models for efficient and robust representation in diverse applications.

Abstract: Recent deep learning-based methods for lossy image compression achieve
competitive rate-distortion performance through extensive end-to-end training
and advanced architectures. However, emerging applications increasingly
prioritize semantic preservation over pixel-level reconstruction and demand
robust performance across diverse data distributions and downstream tasks.
These challenges call for advanced semantic compression paradigms. Motivated by
the zero-shot and representational capabilities of multimodal foundation
models, we propose a novel semantic compression method based on the contrastive
language-image pretraining (CLIP) model. Rather than compressing images for
reconstruction, we propose compressing the CLIP feature embeddings into minimal
bits while preserving semantic information across different tasks. Experiments
show that our method maintains semantic integrity across benchmark datasets,
achieving an average bit rate of approximately 2-3* 10(-3) bits per pixel. This
is less than 5% of the bitrate required by mainstream image compression
approaches for comparable performance. Remarkably, even under extreme
compression, the proposed approach exhibits zero-shot robustness across diverse
data distributions and downstream tasks.

</details>


### [189] [AttriPrompt: Dynamic Prompt Composition Learning for CLIP](https://arxiv.org/abs/2509.05949)
*Qiqi Zhan,Shiwei Li,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TL;DR: The paper presents AttriPrompt, a novel framework that enhances performance in model prompting by addressing limitations in current methods, leveraging CLIP's vision encoder, and introducing innovative mechanisms to improve alignment and prevent overfitting.


<details>
  <summary>Details</summary>
Motivation: Current prompt learning methods rely heavily on contrastive learning objectives, neglecting fine-grained feature optimization, and use static prompts that fail to adapt to varied input categories.

Method: The authors propose AttriPrompt, which uses an Attribute Retrieval module to cluster visual features and retrieve adaptive prompts for text encoders. It incorporates hierarchical visual features and uses Dual-stream Contrastive Learning along with a Self-Regularization mechanism to improve semantic alignment and prevent overfitting.

Result: AttriPrompt demonstrates superiority in experiments, with up to 7.37% improvement in base-to-novel settings and strong performance in cross-domain knowledge transfer.

Conclusion: AttriPrompt addresses key limitations in text prompt methodologies, enhances semantic representation, and establishes vision-language pre-trained models as effective tools for practical applications.

Abstract: The evolution of prompt learning methodologies has driven exploration of
deeper prompt designs to enhance model performance. However, current deep text
prompting approaches suffer from two critical limitations: Over-reliance on
constrastive learning objectives that prioritize high-level semantic alignment,
neglecting fine-grained feature optimization; Static prompts across all input
categories, preventing content-aware adaptation. To address these limitations,
we propose AttriPrompt-a novel framework that enhances and refines textual
semantic representations by leveraging the intermediate-layer features of
CLIP's vision encoder. We designed an Attribute Retrieval module that first
clusters visual features from each layer. The aggregated visual features
retrieve semantically similar prompts from a prompt pool, which are then
concatenated to the input of every layer in the text encoder. Leveraging
hierarchical visual information embedded in prompted text features, we
introduce Dual-stream Contrastive Learning to realize fine-grained alignment.
Furthermore, we introduce a Self-Regularization mechanism by applying explicit
regularization constraints between the prompted and non-prompted text features
to prevent overfitting on limited training data. Extensive experiments across
three benchmarks demonstrate AttriPrompt's superiority over state-of-the-art
methods, achieving up to 7.37\% improvement in the base-to-novel setting. The
observed strength of our method in cross-domain knowledge transfer positions
vision-language pre-trained models as more viable solutions for real-world
implementation.

</details>


### [190] [Coefficients-Preserving Sampling for Reinforcement Learning with Flow Matching](https://arxiv.org/abs/2509.05952)
*Feng Wang,Zihao Yu*

Main category: cs.CV

TL;DR: The paper identifies noise artifact issues in SDE-based sampling for RL in Flow Matching models and introduces Coefficients-Preserving Sampling (CPS) to eliminate this, enhancing image generation quality.


<details>
  <summary>Details</summary>
Motivation: Recent applications of Reinforcement Learning to improve image and video quality in Diffusion and Flow Matching models face challenges from noise caused by SDE-based sampling, hindering reward learning.

Method: The authors analyze the cause of noise artifacts associated with SDE sampling in Flow Matching, propose a new sampling method inspired by Denoising Diffusion Implicit Models (DDIM), and introduce Coefficients-Preserving Sampling (CPS).

Result: CPS eliminates noise artifacts, enables more accurate reward modeling, and ensures faster and more stable convergence in RL-based optimizations such as Flow-GRPO and Dance-GRPO.

Conclusion: The introduction of CPS improves the alignment of Flow Matching with RL objectives, providing a significant advancement in image generation tasks. The method will be made publicly available.

Abstract: Reinforcement Learning (RL) has recently emerged as a powerful technique for
improving image and video generation in Diffusion and Flow Matching models,
specifically for enhancing output quality and alignment with prompts. A
critical step for applying online RL methods on Flow Matching is the
introduction of stochasticity into the deterministic framework, commonly
realized by Stochastic Differential Equation (SDE). Our investigation reveals a
significant drawback to this approach: SDE-based sampling introduces pronounced
noise artifacts in the generated images, which we found to be detrimental to
the reward learning process. A rigorous theoretical analysis traces the origin
of this noise to an excess of stochasticity injected during inference. To
address this, we draw inspiration from Denoising Diffusion Implicit Models
(DDIM) to reformulate the sampling process. Our proposed method,
Coefficients-Preserving Sampling (CPS), eliminates these noise artifacts. This
leads to more accurate reward modeling, ultimately enabling faster and more
stable convergence for reinforcement learning-based optimizers like Flow-GRPO
and Dance-GRPO. Code will be released at https://github.com/IamCreateAI/FlowCPS

</details>


### [191] [Investigating Location-Regularised Self-Supervised Feature Learning for Seafloor Visual Imagery](https://arxiv.org/abs/2509.06660)
*Cailei Liang,Adrian Bodenmann,Emma J Curtis,Samuel Simmons,Kazunori Nagano,Stan Brown,Adam Riese,Blair Thornton*

Main category: cs.CV

TL;DR: The paper examines how location metadata can enhance self-supervised learning (SSL) frameworks for analyzing seafloor images, finding consistent performance improvement with location-based regularization, particularly for low-dimensional representations.


<details>
  <summary>Details</summary>
Motivation: Marine monitoring and exploration through seafloor imagery requires efficient image interpretation methods. Incorporating location metadata into SSL may enhance feature learning but its effects across SSL strategies and datasets remain unclear.

Method: Researchers tested the influence of location-based regularization on six SSL frameworks using CNN and ViT models with varying latent-space dimensionalities. They evaluated performance across three seafloor image datasets.

Result: Location regularization improved classification performance: CNNs F1-score gain 4.9 ± 4.0% and ViTs 6.3 ± 8.9%. Low-dimensional latent representations especially benefitted, with CNN gains up to 10.1 ± 9.4%. High-dimensional ViTs showed strong generalization even without regularization.

Conclusion: Location metadata significantly boosts SSL performance, especially for models with low-dimensional representations. Pre-trained and high-dimensional ViTs exhibit robust generalization, making them competitive even without location-based enhancements.

Abstract: High-throughput interpretation of robotically gathered seafloor visual
imagery can increase the efficiency of marine monitoring and exploration.
Although recent research has suggested that location metadata can enhance
self-supervised feature learning (SSL), its benefits across different SSL
strategies, models and seafloor image datasets are underexplored. This study
evaluates the impact of location-based regularisation on six state-of-the-art
SSL frameworks, which include Convolutional Neural Network (CNN) and Vision
Transformer (ViT) models with varying latent-space dimensionality. Evaluation
across three diverse seafloor image datasets finds that location-regularisation
consistently improves downstream classification performance over standard SSL,
with average F1-score gains of $4.9 \pm 4.0%$ for CNNs and $6.3 \pm 8.9%$ for
ViTs, respectively. While CNNs pretrained on generic datasets benefit from
high-dimensional latent representations, dataset-optimised SSL achieves similar
performance across the high (512) and low (128) dimensional latent
representations. Location-regularised SSL improves CNN performance over
pre-trained models by $2.7 \pm 2.7%$ and $10.1 \pm 9.4%$ for high and
low-dimensional latent representations, respectively. For ViTs,
high-dimensionality benefits both pre-trained and dataset-optimised SSL.
Although location-regularisation improves SSL performance compared to standard
SSL methods, pre-trained ViTs show strong generalisation, matching the
best-performing location-regularised SSL with F1-scores of $0.795 \pm 0.075$
and $0.795 \pm 0.077$, respectively. The findings highlight the value of
location metadata for SSL regularisation, particularly when using
low-dimensional latent representations, and demonstrate strong generalisation
of high-dimensional ViTs for seafloor image analysis.

</details>


### [192] [Dual Interaction Network with Cross-Image Attention for Medical Image Segmentation](https://arxiv.org/abs/2509.05953)
*Jeonghyun Noh,Wangsu Jeon,Jinsun Park*

Main category: cs.CV

TL;DR: The paper presents a method to enhance medical image segmentation using a Dual Interactive Fusion Module (DIFM) and introduces multi-scale boundary loss to address boundary accuracy issues.


<details>
  <summary>Details</summary>
Motivation: To improve medical image segmentation accuracy hindered by factors like noise, blurriness, and low contrast, while addressing limitations in existing image enhancement and fusion techniques.

Method: The proposed method introduces the Dual Interactive Fusion Module (DIFM), which uses bidirectional cross-attention to combine complementary information from original and enhanced images, and incorporates a multi-scale boundary loss leveraging gradient extraction.

Result: The proposed approach showed superior performance on the ACDC and Synapse datasets both quantitatively and qualitatively.

Conclusion: The study demonstrates that DIFM and the multi-scale boundary loss can effectively enhance image segmentation by addressing challenges related to noisy and low-contrast features, while preserving crucial diagnostic details.

Abstract: Medical image segmentation is a crucial method for assisting professionals in
diagnosing various diseases through medical imaging. However, various factors
such as noise, blurriness, and low contrast often hinder the accurate diagnosis
of diseases. While numerous image enhancement techniques can mitigate these
issues, they may also alter crucial information needed for accurate diagnosis
in the original image. Conventional image fusion strategies, such as feature
concatenation can address this challenge. However, they struggle to fully
leverage the advantages of both original and enhanced images while suppressing
the side effects of the enhancements. To overcome the problem, we propose a
dual interactive fusion module (DIFM) that effectively exploits mutual
complementary information from the original and enhanced images. DIFM employs
cross-attention bidirectionally to simultaneously attend to corresponding
spatial information across different images, subsequently refining the
complementary features via global spatial attention. This interaction leverages
low- to high-level features implicitly associated with diverse structural
attributes like edges, blobs, and object shapes, resulting in enhanced features
that embody important spatial characteristics. In addition, we introduce a
multi-scale boundary loss based on gradient extraction to improve segmentation
accuracy at object boundaries. Experimental results on the ACDC and Synapse
datasets demonstrate the superiority of the proposed method quantitatively and
qualitatively. Code available at: https://github.com/JJeong-Gari/DIN

</details>


### [193] [Online Clustering of Seafloor Imagery for Interpretation during Long-Term AUV Operations](https://arxiv.org/abs/2509.06678)
*Cailei Liang,Adrian Bodenmann,Sam Fenton,Blair Thornton*

Main category: cs.CV

TL;DR: The paper proposes an online clustering framework (OCF) for real-time, unsupervised interpretation of seafloor imagery in adaptive AUV missions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of tools for real-time interpretation of seafloor imagery in AUVs, where offline image analysis methods relying on complete datasets and human labeling fall short.

Method: The OCF identifies and maintains representative samples from data streams, enabling dynamic clustering in real-time while avoiding the need to reprocess the entire image history. It supports dynamic cluster splitting/merging and is scalable.

Result: OCF outperformed other online clustering approaches, achieving an average F1 score of 0.68 across three datasets with bounded computational time, demonstrating robustness to trajectory variation.

Conclusion: The OCF framework offers a scalable, efficient solution for real-time seafloor imagery analysis, benefiting adaptive missions and long-term autonomous marine exploration.

Abstract: As long-endurance and seafloor-resident AUVs become more capable, there is an
increasing need for extended, real-time interpretation of seafloor imagery to
enable adaptive missions and optimise communication efficiency. Although
offline image analysis methods are well established, they rely on access to
complete datasets and human-labelled examples to manage the strong influence of
environmental and operational conditions on seafloor image
appearance-requirements that cannot be met in real-time settings. To address
this, we introduce an online clustering framework (OCF) capable of interpreting
seafloor imagery without supervision, which is designed to operate in real-time
on continuous data streams in a scalable, adaptive, and self-consistent manner.
The method enables the efficient review and consolidation of common patterns
across the entire data history in constant time by identifying and maintaining
a set of representative samples that capture the evolving feature distribution,
supporting dynamic cluster merging and splitting without reprocessing the full
image history. We evaluate the framework on three diverse seafloor image
datasets, analysing the impact of different representative sampling strategies
on both clustering accuracy and computational cost. The OCF achieves the
highest average F1 score of 0.68 across the three datasets among all
comparative online clustering approaches, with a standard deviation of 3%
across three distinct survey trajectories, demonstrating its superior
clustering capability and robustness to trajectory variation. In addition, it
maintains consistently lower and bounded computational time as the data volume
increases. These properties are beneficial for generating survey data summaries
and supporting informative path planning in long-term, persistent autonomous
marine exploration.

</details>


### [194] [StripDet: Strip Attention-Based Lightweight 3D Object Detection from Point Cloud](https://arxiv.org/abs/2509.05954)
*Weichao Wang,Wendong Mao,Zhongfeng Wang*

Main category: cs.CV

TL;DR: StripDet introduces a lightweight framework for 3D object detection utilizing point clouds, focusing on efficiency via the novel Strip Attention Block and hierarchical backbone, achieving superior performance with reduced computational load.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of deploying high-accuracy 3D object detection models on edge devices, constrained by computational and memory requirements.

Method: The study proposes the Strip Attention Block (SAB), which uses asymmetric strip convolutions to efficiently capture long-range spatial dependencies, reducing computational complexity. Additionally, it employs a hierarchical backbone with depthwise separable convolutions and a multiscale fusion method.

Result: Experiments on the KITTI dataset show StripDet achieving 79.97% mAP for car detection using only 0.65M parameters, a significant improvement over PointPillars with 7x fewer parameters.

Conclusion: StripDet provides a robust and efficient solution for 3D object detection on edge devices, outperforming existing methods and establishing a new standard for practical applications in accuracy-efficiency trade-offs.

Abstract: The deployment of high-accuracy 3D object detection models from point cloud
remains a significant challenge due to their substantial computational and
memory requirements. To address this, we introduce StripDet, a novel
lightweight framework designed for on-device efficiency. First, we propose the
novel Strip Attention Block (SAB), a highly efficient module designed to
capture long-range spatial dependencies. By decomposing standard 2D
convolutions into asymmetric strip convolutions, SAB efficiently extracts
directional features while reducing computational complexity from quadratic to
linear. Second, we design a hardware-friendly hierarchical backbone that
integrates SAB with depthwise separable convolutions and a simple multiscale
fusion strategy, achieving end-to-end efficiency. Extensive experiments on the
KITTI dataset validate StripDet's superiority. With only 0.65M parameters, our
model achieves a 79.97% mAP for car detection, surpassing the baseline
PointPillars with a 7x parameter reduction. Furthermore, StripDet outperforms
recent lightweight and knowledge distillation-based methods, achieving a
superior accuracy-efficiency trade-off while establishing itself as a practical
solution for real-world 3D detection on edge devices.

</details>


### [195] [Neural Bloom: A Deep Learning Approach to Real-Time Lighting](https://arxiv.org/abs/2509.05963)
*Rafal Karp,Dawid Gruszka,Tomasz Trzcinski*

Main category: cs.CV

TL;DR: This paper introduces two neural network methods for generating bloom lighting in real-time, achieving up to 30% speed improvements over traditional techniques.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies of traditional bloom lighting techniques, which heavily rely on computationally expensive operations like multiple blur passes and texture sampling.

Method: Two methods—Neural Bloom Lighting (NBL) and Fast Neural Bloom Lighting (FastNBL)—are proposed, leveraging neural networks to achieve higher accuracy and faster computation speeds.

Result: FastNBL offers a 28% speed boost and NBL a 12% speed boost over current state-of-the-art without sacrificing quality.

Conclusion: The proposed methods offer a significant step towards achieving realistic bloom lighting faster, saving computational resources and improving immersion in high FPS environments.

Abstract: We propose a novel method to generate bloom lighting effect in real time
using neural networks. Our solution generate brightness mask from given 3D
scene view up to 30% faster than state-of-the-art methods. The existing
traditional techniques rely on multiple blur appliances and texture sampling,
also very often have existing conditional branching in its implementation.
These operations occupy big portion of the execution time. We solve this
problem by proposing two neural network-based bloom lighting methods, Neural
Bloom Lighting (NBL) and Fast Neural Bloom Lighting (FastNBL), focusing on
their quality and performance. Both methods were tested on a variety of 3D
scenes, with evaluations conducted on brightness mask accuracy and inference
speed. The main contribution of this work is that both methods produce
high-quality bloom effects while outperforming the standard state-of-the-art
bloom implementation, with FastNBL being faster by 28% and NBL faster by 12%.
These findings highlight that we can achieve realistic bloom lighting phenomena
faster, moving us towards more realism in real-time environments in the future.
This improvement saves computational resources, which is a major bottleneck in
real-time rendering. Furthermore, it is crucial for sustaining immersion and
ensuring smooth experiences in high FPS environments, while maintaining
high-quality realism.

</details>


### [196] [Event Spectroscopy: Event-based Multispectral and Depth Sensing using Structured Light](https://arxiv.org/abs/2509.06741)
*Christian Geckeler,Niklas Neugebauer,Manasi Muglikar,Davide Scaramuzza,Stefano Mintchev*

Main category: cs.CV

TL;DR: This paper introduces a novel event spectroscopy system for UAVs, offering high-resolution and low-latency depth reconstruction and multispectral imaging to enhance navigation and data collection in dense forest environments.


<details>
  <summary>Details</summary>
Motivation: Current UAV sensing systems in forested environments struggle with issues like latency, poor depth resolution, and ambient light dependence, making efficient navigation and precise data collection difficult.

Method: The system uses structured light for depth reconstruction and modulates light wavelengths to capture multispectral data. It was validated against commercial sensors and spectrometers, and a portable RGB version was used in a Masoala Rainforest.

Result: The new system showed a 60% RMSE improvement in depth sensing, comparable spectral accuracy to commercial tools, and over 30% accuracy improvement in material differentiation when combining depth and spectral data.

Conclusion: This system effectively enhances UAV perception by integrating depth and spectral imaging, proving its robustness and utility in complex forest environments.

Abstract: Uncrewed aerial vehicles (UAVs) are increasingly deployed in forest
environments for tasks such as environmental monitoring and search and rescue,
which require safe navigation through dense foliage and precise data
collection. Traditional sensing approaches, including passive multispectral and
RGB imaging, suffer from latency, poor depth resolution, and strong dependence
on ambient light - especially under forest canopies. In this work, we present a
novel event spectroscopy system that simultaneously enables high-resolution,
low-latency depth reconstruction and multispectral imaging using a single
sensor. Depth is reconstructed using structured light, and by modulating the
wavelength of the projected structured light, our system captures spectral
information in controlled bands between 650 nm and 850 nm. We demonstrate up to
$60\%$ improvement in RMSE over commercial depth sensors and validate the
spectral accuracy against a reference spectrometer and commercial multispectral
cameras, demonstrating comparable performance. A portable version limited to
RGB (3 wavelengths) is used to collect real-world depth and spectral data from
a Masoala Rainforest. We demonstrate the use of this prototype for color image
reconstruction and material differentiation between leaves and branches using
spectral and depth data. Our results show that adding depth (available at no
extra effort with our setup) to material differentiation improves the accuracy
by over $30\%$ compared to color-only method. Our system, tested in both lab
and real-world rainforest environments, shows strong performance in depth
estimation, RGB reconstruction, and material differentiation - paving the way
for lightweight, integrated, and robust UAV perception and data collection in
complex natural environments.

</details>


### [197] [Spatial-Aware Self-Supervision for Medical 3D Imaging with Multi-Granularity Observable Tasks](https://arxiv.org/abs/2509.05967)
*Yiqin Zhang,Meiling Chen,Zhengjie Zhang*

Main category: cs.CV

TL;DR: The paper introduces a method to improve self-supervised learning for medical 3D imaging, ensuring interpretability and maintaining training performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address data scarcity in healthcare and improve the learning of 3D spatial knowledge in medical contexts, which is often lacking in current methods adapted from 2D designs.

Method: The authors propose a method with three sub-tasks designed to capture spatial semantics in 3D medical imaging, adhering to principles that enhance interpretability and ensure stable training.

Result: The proposed approach achieves competitive performance compared to existing methods while enhancing the intuitive understanding of the self-supervised process.

Conclusion: This method provides a balance between interpretability and performance, utilizing 3D imaging's advantages for effective self-supervised learning in medical visualization tasks.

Abstract: The application of self-supervised techniques has become increasingly
prevalent within medical visualization tasks, primarily due to its capacity to
mitigate the data scarcity prevalent in the healthcare sector. The majority of
current works are influenced by designs originating in the generic 2D visual
domain, which lack the intuitive demonstration of the model's learning process
regarding 3D spatial knowledge. Consequently, these methods often fall short in
terms of medical interpretability. We propose a method consisting of three
sub-tasks to capture the spatially relevant semantics in medical 3D imaging.
Their design adheres to observable principles to ensure interpretability, and
minimize the performance loss caused thereby as much as possible. By leveraging
the enhanced semantic depth offered by the extra dimension in 3D imaging, this
approach incorporates multi-granularity spatial relationship modeling to
maintain training stability. Experimental findings suggest that our approach is
capable of delivering performance that is on par with current methodologies,
while facilitating an intuitive understanding of the self-supervised learning
process.

</details>


### [198] [OmniStyle2: Scalable and High Quality Artistic Style Transfer Data Generation via Destylization](https://arxiv.org/abs/2509.05970)
*Ye Wang,Zili Yi,Yibo Zhang,Peng Zheng,Xuping Xie,Jiang Lin,Yilin Wang,Rui Ma*

Main category: cs.CV

TL;DR: OmniStyle2 proposes destylization as a new paradigm in artistic style transfer, creating a large dataset (DST-100K) for improved training. The model outperforms state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The lack of ground-truth data for artistic style transfer is a significant challenge. This paper aims to address this by reframing style transfer as a data problem and creating a dataset via destylization.

Method: The authors developed DST for reconstructing style-free content and DST-Filter for filtering low-quality data. Using these, they built DST-100K and trained a feed-forward model (OmniStyle2) based on FLUX.1-dev.

Result: OmniStyle2 demonstrates superior performance over existing methods in both qualitative and quantitative benchmarks.

Conclusion: Destylization enables scalable and authentic data generation, resolving the issue of missing ground truth in artistic style transfer and advancing the field significantly.

Abstract: OmniStyle2 introduces a novel approach to artistic style transfer by
reframing it as a data problem. Our key insight is destylization, reversing
style transfer by removing stylistic elements from artworks to recover natural,
style-free counterparts. This yields DST-100K, a large-scale dataset that
provides authentic supervision signals by aligning real artistic styles with
their underlying content. To build DST-100K, we develop (1) DST, a text-guided
destylization model that reconstructs stylefree content, and (2) DST-Filter, a
multi-stage evaluation model that employs Chain-of-Thought reasoning to
automatically discard low-quality pairs while ensuring content fidelity and
style accuracy. Leveraging DST-100K, we train OmniStyle2, a simple feed-forward
model based on FLUX.1-dev. Despite its simplicity, OmniStyle2 consistently
surpasses state-of-the-art methods across both qualitative and quantitative
benchmarks. Our results demonstrate that scalable data generation via
destylization provides a reliable supervision paradigm, overcoming the
fundamental challenge posed by the lack of ground-truth data in artistic style
transfer.

</details>


### [199] [ConstStyle: Robust Domain Generalization with Unified Style Transformation](https://arxiv.org/abs/2509.05975)
*Nam Duong Tran,Nam Nguyen Phuong,Hieu H. Pham,Phi Le Nguyen,My T. Thai*

Main category: cs.CV

TL;DR: The paper introduces ConstStyle, a method designed to address domain shifts in deep neural networks by mapping data to a unified domain, thus improving performance across unseen test domains.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks struggle when test data distributions differ from training data. Many methods for domain generalization fail in cases of limited training domains or significant gaps between training and testing domains.

Method: ConstStyle is proposed as a novel approach. It maps all data (both seen and unseen) to a unified domain, optimized during training to capture domain-invariant features. The method aligns training and testing data in this unified domain to reduce domain shifts.

Result: Experiments show that ConstStyle consistently outperforms other methods in handling domain shifts. It demonstrates remarkable improvements, with accuracy boosts of up to 19.82% when fewer training domains are provided.

Conclusion: ConstStyle proves effective in improving domain generalization by leveraging a unified domain, significantly addressing the challenges of limited seen domains and large gaps to unseen domains.

Abstract: Deep neural networks often suffer performance drops when test data
distribution differs from training data. Domain Generalization (DG) aims to
address this by focusing on domain-invariant features or augmenting data for
greater diversity. However, these methods often struggle with limited training
domains or significant gaps between seen (training) and unseen (test) domains.
To enhance DG robustness, we hypothesize that it is essential for the model to
be trained on data from domains that closely resemble unseen test domains-an
inherently difficult task due to the absence of prior knowledge about the
unseen domains. Accordingly, we propose ConstStyle, a novel approach that
leverages a unified domain to capture domain-invariant features and bridge the
domain gap with theoretical analysis. During training, all samples are mapped
onto this unified domain, optimized for seen domains. During testing, unseen
domain samples are projected similarly before predictions. By aligning both
training and testing data within this unified domain, ConstStyle effectively
reduces the impact of domain shifts, even with large domain gaps or few seen
domains. Extensive experiments demonstrate that ConstStyle consistently
outperforms existing methods across diverse scenarios. Notably, when only a
limited number of seen domains are available, ConstStyle can boost accuracy up
to 19.82\% compared to the next best approach.

</details>


### [200] [Multi-Strategy Guided Diffusion via Sparse Masking Temporal Reweighting Distribution Correction](https://arxiv.org/abs/2509.05992)
*Zekun Zhou,Yanru Gong,Liu Shi,Qiegen Liu*

Main category: cs.CV

TL;DR: The paper introduces STRIDE, a diffusion model for sparse-view CT reconstruction, focusing on enhanced detail restoration and structural preservation through innovative mechanisms and dual-network architecture.


<details>
  <summary>Details</summary>
Motivation: Sparse-view CT reconstruction is challenging due to missing projection data and the need for effective global image modeling. Current solutions often lack robustness in detail restoration and structural preservation.

Method: The STRIDE framework incorporates sparse conditional probabilities, temporally varying guidance adjustment strategies, linear regression for distribution shift correction, and a dual-network parallel architecture for multi-frequency optimization.

Result: STRIDE outperformed baseline methods with a 2.58 dB improvement in PSNR, 2.37% increase in SSIM, and 0.236 reduction in MSE, demonstrating robustness in artifact suppression and consistent image quality.

Conclusion: The proposed STRIDE method provides a significant improvement in sparse-view CT reconstruction, showcasing its potential for scalable and practical applications in medical imaging.

Abstract: Diffusion models have demonstrated remarkable generative capabilities in
image processing tasks. We propose a Sparse condition Temporal Rewighted
Integrated Distribution Estimation guided diffusion model (STRIDE) for
sparse-view CT reconstruction. Specifically, we design a joint training
mechanism guided by sparse conditional probabilities to facilitate the model
effective learning of missing projection view completion and global information
modeling. Based on systematic theoretical analysis, we propose a temporally
varying sparse condition reweighting guidance strategy to dynamically adjusts
weights during the progressive denoising process from pure noise to the real
image, enabling the model to progressively perceive sparse-view information.
The linear regression is employed to correct distributional shifts between
known and generated data, mitigating inconsistencies arising during the
guidance process. Furthermore, we construct a dual-network parallel
architecture to perform global correction and optimization across multiple
sub-frequency components, thereby effectively improving the model capability in
both detail restoration and structural preservation, ultimately achieving
high-quality image reconstruction. Experimental results on both public and real
datasets demonstrate that the proposed method achieves the best improvement of
2.58 dB in PSNR, increase of 2.37\% in SSIM, and reduction of 0.236 in MSE
compared to the best-performing baseline methods. The reconstructed images
exhibit excellent generalization and robustness in terms of structural
consistency, detail restoration, and artifact suppression.

</details>


### [201] [S-LAM3D: Segmentation-Guided Monocular 3D Object Detection via Feature Space Fusion](https://arxiv.org/abs/2509.05999)
*Diana-Alexandra Sas,Florin Oniga*

Main category: cs.CV

TL;DR: This paper introduces a method to improve monocular 3D object detection by integrating precomputed segmentation information into the feature space without expanding the model.


<details>
  <summary>Details</summary>
Motivation: Monocular 3D object detection faces challenges due to the lack of depth cues in single 2D images. The authors aim to enhance detection performance using existing segmentation information.

Method: The authors propose a decoupled strategy that incorporates segmentation priors directly into the feature space of existing detection pipelines, avoiding additional prediction branches or joint learning.

Result: The method, tested on the KITTI 3D Object Detection Benchmark, improves performance for detecting small objects like pedestrians and cyclists compared to models relying solely on RGB image features.

Conclusion: Leveraging segmentation information enhances the understanding of input data, demonstrating that accurate detection can be achieved without relying on extra sensors or increased training data.

Abstract: Monocular 3D Object Detection represents a challenging Computer Vision task
due to the nature of the input used, which is a single 2D image, lacking in any
depth cues and placing the depth estimation problem as an ill-posed one.
Existing solutions leverage the information extracted from the input by using
Convolutional Neural Networks or Transformer architectures as feature
extraction backbones, followed by specific detection heads for 3D parameters
prediction. In this paper, we introduce a decoupled strategy based on injecting
precomputed segmentation information priors and fusing them directly into the
feature space for guiding the detection, without expanding the detection model
or jointly learning the priors. The focus is on evaluating the impact of
additional segmentation information on existing detection pipelines without
adding additional prediction branches. The proposed method is evaluated on the
KITTI 3D Object Detection Benchmark, outperforming the equivalent architecture
that relies only on RGB image features for small objects in the scene:
pedestrians and cyclists, and proving that understanding the input data can
balance the need for additional sensors or training data.

</details>


### [202] [Motion Aware ViT-based Framework for Monocular 6-DoF Spacecraft Pose Estimation](https://arxiv.org/abs/2509.06000)
*Jose Sosa,Dan Pineau,Arunkumar Rathinam,Abdelrahman Shabayek,Djamila Aouada*

Main category: cs.CV

TL;DR: The paper introduces a novel deep learning method enhancing 6-DoF spacecraft pose estimation by leveraging motion-aware techniques and combining Vision Transformer features with optical flow.


<details>
  <summary>Details</summary>
Motivation: Current monocular 6-DoF pose estimation techniques underutilize temporal motion dynamics in space operations, relying too heavily on static image processing.

Method: The method integrates Vision Transformer (ViT) image features with optical flow-based motion cues into a deep learning framework. 2D keypoints are localized, which serves as input for a Perspective-n-Point solver to derive 6-DoF poses.

Result: The approach outperforms single-image baselines in keypoint localization and pose estimation benchmarks, evaluated on the SPADES-RGB dataset and generalization validated using diverse SPARK-2024 datasets.

Conclusion: Integrating dynamic motion information significantly improves pose estimation precision and generalization, proving suitable for varied data distributions.

Abstract: Monocular 6-DoF pose estimation plays an important role in multiple
spacecraft missions. Most existing pose estimation approaches rely on single
images with static keypoint localisation, failing to exploit valuable temporal
information inherent to space operations. In this work, we adapt a deep
learning framework from human pose estimation to the spacecraft pose estimation
domain that integrates motion-aware heatmaps and optical flow to capture motion
dynamics. Our approach combines image features from a Vision Transformer (ViT)
encoder with motion cues from a pre-trained optical flow model to localise 2D
keypoints. Using the estimates, a Perspective-n-Point (PnP) solver recovers
6-DoF poses from known 2D-3D correspondences. We train and evaluate our method
on the SPADES-RGB dataset and further assess its generalisation on real and
synthetic data from the SPARK-2024 dataset. Overall, our approach demonstrates
improved performance over single-image baselines in both 2D keypoint
localisation and 6-DoF pose estimation. Furthermore, it shows promising
generalisation capabilities when testing on different data distributions.

</details>


### [203] [Khana: A Comprehensive Indian Cuisine Dataset](https://arxiv.org/abs/2509.06006)
*Omkar Prabhu*

Main category: cs.CV

TL;DR: The paper introduces "Khana," a dataset focused on Indian cuisine, featuring 131K images across 80 categories for food classification, segmentation, and retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive datasets capturing the diversity of Indian cuisine, vital for food-related applications like recognition, recipe recommendations, and meal planning.

Method: Developed a benchmark dataset containing 131K Indian food images categorized into 80 labels, with baseline evaluations using state-of-the-art models for classification, segmentation, and retrieval tasks.

Result: Khana successfully provides a diverse and high-quality dataset that enables more accurate and nuanced food image analyses, particularly for Indian dishes.

Conclusion: Khana bridges critical gaps in food image research and supports the development of practical applications by comprehensively representing Indian cuisine's diversity.

Abstract: As global interest in diverse culinary experiences grows, food image models
are essential for improving food-related applications by enabling accurate food
recognition, recipe suggestions, dietary tracking, and automated meal planning.
Despite the abundance of food datasets, a noticeable gap remains in capturing
the nuances of Indian cuisine due to its vast regional diversity, complex
preparations, and the lack of comprehensive labeled datasets that cover its
full breadth. Through this exploration, we uncover Khana, a new benchmark
dataset for food image classification, segmentation, and retrieval of dishes
from Indian cuisine. Khana fills the gap by establishing a taxonomy of Indian
cuisine and offering around 131K images in the dataset spread across 80 labels,
each with a resolution of 500x500 pixels. This paper describes the dataset
creation process and evaluates state-of-the-art models on classification,
segmentation, and retrieval as baselines. Khana bridges the gap between
research and development by providing a comprehensive and challenging benchmark
for researchers while also serving as a valuable resource for developers
creating real-world applications that leverage the rich tapestry of Indian
cuisine. Webpage: https://khana.omkar.xyz

</details>


### [204] [BLaVe-CoT: Consistency-Aware Visual Question Answering for Blind and Low Vision Users](https://arxiv.org/abs/2509.06010)
*Wanyin Cheng,Zanxi Ruan*

Main category: cs.CV

TL;DR: The paper introduces BLaVe-CoT, a novel Visual Question Answering (VQA) framework tailored to assist Blind and Low Vision (BLV) users, addressing ambiguity and visual noise through reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing VQA systems struggle with ambiguous questions and imperfect images common among BLV users, who face challenges in framing photos and articulating specific questions.

Method: The BLaVe-CoT framework generates diverse answers via a LoRA-tuned BLIP-2 model, spatially grounds them using PolyFormer, and employs chain-of-thought reasoning to assess answer consistency across image regions.

Result: BLaVe-CoT demonstrates improved performance and robustness in real-world challenges compared to previous methods, validated on the VQA-AnswerTherapy benchmark.

Conclusion: This paper emphasizes the importance of developing inclusive VQA systems adaptable to ambiguity and visual uncertainty for BLV users and provides public access to its codebase to encourage further research.

Abstract: Visual Question Answering (VQA) holds great potential for assisting Blind and
Low Vision (BLV) users, yet real-world usage remains challenging. Due to visual
impairments, BLV users often take blurry or poorly framed photos and face
difficulty in articulating specific questions about what they cannot fully see.
As a result, their visual questions are frequently ambiguous, and different
users may interpret them in diverse ways. This leads to multiple valid answers,
each grounded in different image regions-posing a mismatch with conventional
VQA systems that assume a single answer and region. To bridge this gap, we
present BLaVe-CoT, a VQA framework designed to reason about answer consistency
in the face of ambiguity. Our method proposes diverse candidate answers using a
LoRA-tuned BLIP-2 model, then grounds each answer spatially using PolyFormer,
and finally applies a chain-of-thought reasoning module to assess whether the
answers refer to the same or different regions. Evaluated on the
VQA-AnswerTherapy benchmark, BLaVe-CoT outperforms previous methods and proves
more robust to the ambiguity and visual noise common in assistive settings.
This work highlights the need for VQA systems that can adapt to real human
uncertainty and provide inclusive support for BLV users. To foster further
research and accessibility applications, we have made the code publicly
available at https://github.com/Accecwan/BLaVe-CoT.

</details>


### [205] [Cross-Modal Enhancement and Benchmark for UAV-based Open-Vocabulary Object Detection](https://arxiv.org/abs/2509.06011)
*Zhenhai Weng,Zhongliang Yu*

Main category: cs.CV

TL;DR: This paper addresses the domain gap in Open-Vocabulary Object Detection for UAV imagery by introducing a refined UAV-Label engine, new datasets (UAVDE-2M and UAVCAP-15k), and a novel CAGE module integrated into YOLO-World-v2.


<details>
  <summary>Details</summary>
Motivation: The domain gap between existing large-scale OVD datasets containing ground-level images and UAV imagery reduces detection performance, necessitating improvements.

Method: The authors develop UAV-specific datasets (UAVDE-2M and UAVCAP-15k) and propose a Cross-Attention Gated Enhancement Fusion (CAGE) module integrated into the YOLO-World-v2 framework.

Result: Experiments on VisDrone and SIMD datasets demonstrate their approach significantly enhances object detection performance in UAV-based applications.

Conclusion: The paper establishes advancements in OVD tailored for UAV imagery and showcases its potential for enhancing UAV-based remote sensing applications.

Abstract: Open-Vocabulary Object Detection (OVD) has emerged as a pivotal technology
for applications involving Unmanned Aerial Vehicles (UAVs). However, the
prevailing large-scale datasets for OVD pre-training are predominantly composed
of ground-level, natural images. This creates a significant domain gap, causing
models trained on them to exhibit a substantial drop in performance on UAV
imagery. To address this limitation, we first propose a refined UAV-Label
engine. Then we construct and introduce UAVDE-2M(contains over 2,000,000
instances and 1800 categories) and UAVCAP-15k(contains over 15,000 images).
Furthermore, we propose a novel Cross-Attention Gated Enhancement Fusion (CAGE)
module and integrate it into the YOLO-World-v2 architecture. Finally, extensive
experiments on the VisDrone and SIMD datasets verify the effectiveness of our
proposed method for applications in UAV-based imagery and remote sensing.

</details>


### [206] [Micro-Expression Recognition via Fine-Grained Dynamic Perception](https://arxiv.org/abs/2509.06015)
*Zhiwen Shao,Yifan Cheng,Fan Zhang,Xuehuai Shi,Canlin Li,Lizhuang Ma,Dit-yan Yeung*

Main category: cs.CV

TL;DR: The paper proposes a novel framework called Fine-Grained Dynamic Perception (FDP) for facial micro-expression recognition (MER), addressing challenges caused by subtle dynamics and limited training data.


<details>
  <summary>Details</summary>
Motivation: Existing MER methods rely on hand-crafted features or deep networks, both of which struggle with key frame requirements or limited training data. MER requires innovative approaches to better capture subtle and transient micro-expressions.

Method: FDP framework ranks frame-level features chronologically to encode dynamic information. It uses a local-global feature-aware transformer for representation learning and incorporates a rank scorer for capturing facial dynamics. A shared dynamic representation predicts ME categories and constructs dynamic facial images.

Result: FDP outperforms state-of-the-art methods in MER across multiple datasets (CASME II, SAMM, CAS(ME)^2, CAS(ME)^3) with significant improvements in F1-score (ranging from 2.11% to 7.71%). Dynamic image construction was also validated as effective.

Conclusion: The FDP framework is highly effective for recognizing subtle micro-expressions and mitigating data limitations. It offers improvements in performance and versatility in facial dynamic analysis.

Abstract: Facial micro-expression recognition (MER) is a challenging task, due to the
transience, subtlety, and dynamics of micro-expressions (MEs). Most existing
methods resort to hand-crafted features or deep networks, in which the former
often additionally requires key frames, and the latter suffers from small-scale
and low-diversity training data. In this paper, we develop a novel fine-grained
dynamic perception (FDP) framework for MER. We propose to rank frame-level
features of a sequence of raw frames in chronological order, in which the rank
process encodes the dynamic information of both ME appearances and motions.
Specifically, a novel local-global feature-aware transformer is proposed for
frame representation learning. A rank scorer is further adopted to calculate
rank scores of each frame-level feature. Afterwards, the rank features from
rank scorer are pooled in temporal dimension to capture dynamic representation.
Finally, the dynamic representation is shared by a MER module and a dynamic
image construction module, in which the former predicts the ME category, and
the latter uses an encoder-decoder structure to construct the dynamic image.
The design of dynamic image construction task is beneficial for capturing
facial subtle actions associated with MEs and alleviating the data scarcity
issue. Extensive experiments show that our method (i) significantly outperforms
the state-of-the-art MER methods, and (ii) works well for dynamic image
construction. Particularly, our FDP improves by 4.05%, 2.50%, 7.71%, and 2.11%
over the previous best results in terms of F1-score on the CASME II, SAMM,
CAS(ME)^2, and CAS(ME)^3 datasets, respectively. The code is available at
https://github.com/CYF-cuber/FDP.

</details>


### [207] [DVLO4D: Deep Visual-Lidar Odometry with Sparse Spatial-temporal Fusion](https://arxiv.org/abs/2509.06023)
*Mengmeng Liu,Michael Ying Yang,Jiuming Liu,Yunpeng Zhang,Jiangtao Li,Sander Oude Elberink,George Vosselman,Hao Cheng*

Main category: cs.CV

TL;DR: The paper introduces DVLO4D, a novel visual-LiDAR odometry framework that enhances robustness and accuracy using sparse spatial-temporal fusion, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current visual-LiDAR odometry methods face challenges such as sensor misalignment, underutilization of temporal data, and extensive tuning for diverse sensor setups.

Method: DVLO4D incorporates three innovations: Sparse Query Fusion for data fusion, a Temporal Interaction and Update module for better initialization in pose estimation, and Temporal Clip Training with Collective Average Loss for global optimization.

Result: Experiments on KITTI and Argoverse datasets show that DVLO4D achieves state-of-the-art pose accuracy and robustness, with an efficient inference time of 82 ms.

Conclusion: DVLO4D significantly improves performance in visual-LiDAR odometry, demonstrating high potential for real-time autonomous system localization applications.

Abstract: Visual-LiDAR odometry is a critical component for autonomous system
localization, yet achieving high accuracy and strong robustness remains a
challenge. Traditional approaches commonly struggle with sensor misalignment,
fail to fully leverage temporal information, and require extensive manual
tuning to handle diverse sensor configurations. To address these problems, we
introduce DVLO4D, a novel visual-LiDAR odometry framework that leverages sparse
spatial-temporal fusion to enhance accuracy and robustness. Our approach
proposes three key innovations: (1) Sparse Query Fusion, which utilizes sparse
LiDAR queries for effective multi-modal data fusion; (2) a Temporal Interaction
and Update module that integrates temporally-predicted positions with current
frame data, providing better initialization values for pose estimation and
enhancing model's robustness against accumulative errors; and (3) a Temporal
Clip Training strategy combined with a Collective Average Loss mechanism that
aggregates losses across multiple frames, enabling global optimization and
reducing the scale drift over long sequences. Extensive experiments on the
KITTI and Argoverse Odometry dataset demonstrate the superiority of our
proposed DVLO4D, which achieves state-of-the-art performance in terms of both
pose accuracy and robustness. Additionally, our method has high efficiency,
with an inference time of 82 ms, possessing the potential for the real-time
deployment.

</details>


### [208] [Analysis of Blood Report Images Using General Purpose Vision-Language Models](https://arxiv.org/abs/2509.06033)
*Nadia Bakhsheshi,Hamid Beigy*

Main category: cs.CV

TL;DR: The paper evaluates the use of Vision-Language Models (VLMs) to analyze blood report images and make them understandable for users.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge individuals face in interpreting complex blood reports, which often leads to anxiety and missed health issues.

Method: Three VLMs (Qwen-VL-Max, Gemini 2.5 Pro, Llama 4 Maverick) were compared on responses to clinically relevant questions about 100 diverse blood report images, analyzing accuracy using Sentence-BERT.

Result: VLMs showed promise in providing clear analyses of medical data from images, proving their potential for healthcare applications.

Conclusion: The study demonstrates that general-purpose VLMs could improve patient understanding of blood reports, laying a foundation for accessible AI-assisted healthcare tools, though the findings are limited by the small dataset size.

Abstract: The reliable analysis of blood reports is important for health knowledge, but
individuals often struggle with interpretation, leading to anxiety and
overlooked issues. We explore the potential of general-purpose Vision-Language
Models (VLMs) to address this challenge by automatically analyzing blood report
images. We conduct a comparative evaluation of three VLMs: Qwen-VL-Max, Gemini
2.5 Pro, and Llama 4 Maverick, determining their performance on a dataset of
100 diverse blood report images. Each model was prompted with clinically
relevant questions adapted to each blood report. The answers were then
processed using Sentence-BERT to compare and evaluate how closely the models
responded. The findings suggest that general-purpose VLMs are a practical and
promising technology for developing patient-facing tools for preliminary blood
report analysis. Their ability to provide clear interpretations directly from
images can improve health literacy and reduce the limitations to understanding
complex medical information. This work establishes a foundation for the future
development of reliable and accessible AI-assisted healthcare applications.
While results are encouraging, they should be interpreted cautiously given the
limited dataset size.

</details>


### [209] [TinyDef-DETR:An Enhanced DETR Detector for UAV Power Line Defect Detection](https://arxiv.org/abs/2509.06035)
*Jiaming Cui*

Main category: cs.CV

TL;DR: This paper presents TinyDef-DETR, a specialized DETR-based framework tailored for small-defect detection in UAV-based inspection of transmission lines.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in detecting small and ambiguous defects in complex backgrounds, where conventional detectors often fail due to detail loss, weak boundary sensitivity, and improper integration of global and local contexts.

Method: The authors introduce a stride-free space-to-depth downsampling module, edge-enhanced convolution, cross-stage dual-domain multi-scale attention, and Focaler-Wise-SIoU regression loss.

Result: TinyDef-DETR shows significant improvements in precision and recall, especially for small-object subsets, with moderate computational cost, validated on the CSG-ADCD dataset and further supported on the VisDrone benchmark.

Conclusion: TinyDef-DETR is a practical and efficient system for UAV-based small-defect inspection, combining innovative design elements to enhance detail preservation, boundary sensitivity, and localization, with proven generalization capability.

Abstract: Automated inspection of transmission lines using UAVs is hindered by the
difficulty of detecting small and ambiguous defects against complex
backgrounds. Conventional detectors often suffer from detail loss due to
strided downsampling, weak boundary sensitivity in lightweight backbones, and
insufficient integration of global context with local cues. To address these
challenges, we propose TinyDef-DETR, a DETR-based framework designed for
small-defect detection. The method introduces a stride-free space-to-depth
module for lossless downsampling, an edge-enhanced convolution for
boundary-aware feature extraction, a cross-stage dual-domain multi-scale
attention module to jointly capture global and local information, and a
Focaler-Wise-SIoU regression loss to improve localization of small objects.
Experiments conducted on the CSG-ADCD dataset demonstrate that TinyDef-DETR
achieves substantial improvements in both precision and recall compared to
competitive baselines, with particularly notable gains on small-object subsets,
while incurring only modest computational overhead. Further validation on the
VisDrone benchmark confirms the generalization capability of the proposed
approach. Overall, the results indicate that integrating detail-preserving
downsampling, edge-sensitive representations, dual-domain attention, and
difficulty-adaptive regression provides a practical and efficient solution for
UAV-based small-defect inspection in power grids.

</details>


### [210] [BranchGRPO: Stable and Efficient GRPO with Structured Branching in Diffusion Models](https://arxiv.org/abs/2509.06040)
*Yuming Li,Yikai Wang,Yuying Zhu,Zhongyu Zhao,Ming Lu,Qi She,Shanghang Zhang*

Main category: cs.CV

TL;DR: BranchGRPO reduces computational costs and improves image and video generative model alignment by introducing a branch sampling policy.


<details>
  <summary>Details</summary>
Motivation: Existing generative model alignment methods face computational inefficiencies and instability due to sparse rewards.

Method: BranchGRPO employs a branch sampling scheme, tree-based advantage estimator, and pruning strategies to enhance efficiency and performance.

Result: Experiments demonstrate a 16% improvement in alignment scores and a 50% reduction in training time.

Conclusion: BranchGRPO effectively achieves better alignment scores while significantly reducing computational requirements, addressing key challenges in generative model training.

Abstract: Recent advancements in aligning image and video generative models via GRPO
have achieved remarkable gains in enhancing human preference alignment.
However, these methods still face high computational costs from on-policy
rollouts and excessive SDE sampling steps, as well as training instability due
to sparse rewards. In this paper, we propose BranchGRPO, a novel method that
introduces a branch sampling policy updating the SDE sampling process. By
sharing computation across common prefixes and pruning low-reward paths and
redundant depths, BranchGRPO substantially lowers the per-update compute cost
while maintaining or improving exploration diversity. This work makes three
main contributions: (1) a branch sampling scheme that reduces rollout and
training cost; (2) a tree-based advantage estimator incorporating dense
process-level rewards; and (3) pruning strategies exploiting path and depth
redundancy to accelerate convergence and boost performance. Experiments on
image and video preference alignment show that BranchGRPO improves alignment
scores by 16% over strong baselines, while cutting training time by 50%.

</details>


### [211] [Multi-Stage Graph Neural Networks for Data-Driven Prediction of Natural Convection in Enclosed Cavities](https://arxiv.org/abs/2509.06041)
*Mohammad Ahangarkiasari,Hassan Pouraria*

Main category: cs.CV

TL;DR: The paper presents a novel multi-stage Graph Neural Network (GNN) architecture designed to improve predictive accuracy and efficiency in modeling buoyancy-driven heat transfer using CFD data.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of high-computation, expert-dependent CFD modeling and the inability of conventional GNNs to capture long-range dependencies in high-resolution graph structures.

Method: Developed a multi-stage GNN architecture incorporating hierarchical pooling and unpooling to model global-to-local interactions across spatial scales, tested using a CFD dataset simulating natural convection in rectangular cavities.

Result: The proposed model outperforms state-of-the-art GNN baselines in predictive accuracy, training efficiency, and reduced long-term error accumulation.

Conclusion: The multi-stage GNN architecture shows high potential for advancing data-driven modeling in heat transfer and fluid dynamics simulations.

Abstract: Buoyancy-driven heat transfer in closed cavities serves as a canonical
testbed for thermal design High-fidelity CFD modelling yields accurate thermal
field solutions, yet its reliance on expert-crafted physics models, fine
meshes, and intensive computation limits rapid iteration. Recent developments
in data-driven modeling, especially Graph Neural Networks (GNNs), offer new
alternatives for learning thermal-fluid behavior directly from simulation data,
particularly on irregular mesh structures. However, conventional GNNs often
struggle to capture long-range dependencies in high-resolution graph
structures. To overcome this limitation, we propose a novel multi-stage GNN
architecture that leverages hierarchical pooling and unpooling operations to
progressively model global-to-local interactions across multiple spatial
scales. We evaluate the proposed model on our newly developed CFD dataset
simulating natural convection within a rectangular cavities with varying aspect
ratios where the bottom wall is isothermal hot, the top wall is isothermal
cold, and the two vertical walls are adiabatic. Experimental results
demonstrate that the proposed model achieves higher predictive accuracy,
improved training efficiency, and reduced long-term error accumulation compared
to state-of-the-art (SOTA) GNN baselines. These findings underscore the
potential of the proposed multi-stage GNN approach for modeling complex heat
transfer in mesh-based fluid dynamics simulations.

</details>


### [212] [Home-made Diffusion Model from Scratch to Hatch](https://arxiv.org/abs/2509.06068)
*Shih-Ying Yeh*

Main category: cs.CV

TL;DR: The paper introduces HDM, a text-to-image diffusion model that delivers high-quality images efficiently even on consumer-grade hardware.


<details>
  <summary>Details</summary>
Motivation: To reduce the computational requirements and costs associated with training high-quality text-to-image models, making them accessible to individuals and smaller organizations.

Method: Development of HDM with innovative architectural features like Cross-U-Transformer (XUT) and an efficient training recipe including strategies like TREAD acceleration and progressive resolution scaling.

Result: HDM achieved competitive 1024x1024 image generation quality with reduced training costs ($535-620 on four RTX5090 GPUs) and validated that smaller models with optimized architectures can perform well.

Conclusion: HDM democratizes access to high-quality text-to-image generation by demonstrating efficient scaling and innovative techniques for consumer-grade hardware.

Abstract: We introduce Home-made Diffusion Model (HDM), an efficient yet powerful
text-to-image diffusion model optimized for training (and inferring) on
consumer-grade hardware. HDM achieves competitive 1024x1024 generation quality
while maintaining a remarkably low training cost of $535-620 using four RTX5090
GPUs, representing a significant reduction in computational requirements
compared to traditional approaches. Our key contributions include: (1)
Cross-U-Transformer (XUT), a novel U-shape transformer, Cross-U-Transformer
(XUT), that employs cross-attention for skip connections, providing superior
feature integration that leads to remarkable compositional consistency; (2) a
comprehensive training recipe that incorporates TREAD acceleration, a novel
shifted square crop strategy for efficient arbitrary aspect-ratio training, and
progressive resolution scaling; and (3) an empirical demonstration that smaller
models (343M parameters) with carefully crafted architectures can achieve
high-quality results and emergent capabilities, such as intuitive camera
control. Our work provides an alternative paradigm of scaling, demonstrating a
viable path toward democratizing high-quality text-to-image generation for
individual researchers and smaller organizations with limited computational
resources.

</details>


### [213] [High-Quality Tomographic Image Reconstruction Integrating Neural Networks and Mathematical Optimization](https://arxiv.org/abs/2509.06082)
*Anuraag Mishra,Andrea Gilch,Benjamin Apeleo Zubiri,Jan Rolfes,Frauke Liers*

Main category: cs.CV

TL;DR: The paper presents a new technique for improving image reconstruction in nano- and microtomography, focusing on reducing artifacts and enhancing quality by leveraging neural networks and optimization.


<details>
  <summary>Details</summary>
Motivation: The study aims to address and improve image reconstruction challenges, particularly enhancing sharpness and homogeneity in images of specimens with homogeneous material phases connected by sharp edges.

Method: The method uses a neural network to detect edges in subpictures, integrating its predictions into an optimization model that reduces reconstruction artifacts while allowing alternative solutions as supported by raw data.

Result: Experimental datasets demonstrate that the technique achieves noticeably sharper interfaces and improved material homogeneity compared to existing reconstruction algorithms.

Conclusion: The proposed method advances tomographic imaging by producing higher-quality reconstructions and presents significant potential for further enhancement in this field.

Abstract: In this work, we develop a novel technique for reconstructing images from
projection-based nano- and microtomography. Our contribution focuses on
enhancing reconstruction quality, particularly for specimen composed of
homogeneous material phases connected by sharp edges. This is accomplished by
training a neural network to identify edges within subpictures. The trained
network is then integrated into a mathematical optimization model, to reduce
artifacts from previous reconstructions. To this end, the optimization approach
favors solutions according to the learned predictions, however may also
determine alternative solutions if these are strongly supported by the raw
data. Hence, our technique successfully incorporates knowledge about the
homogeneity and presence of sharp edges in the sample and thereby eliminates
blurriness. Our results on experimental datasets show significant enhancements
in interface sharpness and material homogeneity compared to benchmark
algorithms. Thus, our technique produces high-quality reconstructions,
showcasing its potential for advancing tomographic imaging techniques.

</details>


### [214] [Index-Preserving Lightweight Token Pruning for Efficient Document Understanding in Vision-Language Models](https://arxiv.org/abs/2509.06415)
*Jaemin Son,Sujin Choi,Inyong Yun*

Main category: cs.CV

TL;DR: The paper introduces a lightweight token pruning approach to reduce computational costs in vision-language models for document understanding while preserving accuracy.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the high computational demands of vision-language models in document understanding tasks.

Method: A lightweight token pruning framework uses a binary patch-level classifier to filter out non-text areas, followed by max-pooling to recover fragmented text regions for spatial coherence.

Result: The proposed approach significantly reduces computational costs without compromising performance on real-world document datasets.

Conclusion: Token pruning can enhance the efficiency of vision-language models, making them more computationally sustainable while maintaining their effectiveness.

Abstract: Recent progress in vision-language models (VLMs) has led to impressive
results in document understanding tasks, but their high computational demands
remain a challenge. To mitigate the compute burdens, we propose a lightweight
token pruning framework that filters out non-informative background regions
from document images prior to VLM processing. A binary patch-level classifier
removes non-text areas, and a max-pooling refinement step recovers fragmented
text regions to enhance spatial coherence. Experiments on real-world document
datasets demonstrate that our approach substantially lowers computational
costs, while maintaining comparable accuracy.

</details>


### [215] [MedSeqFT: Sequential Fine-tuning Foundation Models for 3D Medical Image Segmentation](https://arxiv.org/abs/2509.06096)
*Yiwen Ye,Yicheng Wu,Xiangde Luo,He Zhang,Ziyang Chen,Ting Dang,Yanning Zhang,Yong Xia*

Main category: cs.CV

TL;DR: The paper proposes MedSeqFT, a framework for sequential fine-tuning of foundation models in medical image segmentation, addressing the limitations of parallel and multi-task strategies and achieving improved performance and transferability.


<details>
  <summary>Details</summary>
Motivation: Current fine-tuning strategies for medical image segmentation fail to exploit shared knowledge when tasks emerge sequentially; parallel fine-tuning isolates tasks, and multi-task approaches need simultaneous access to all datasets.

Method: MedSeqFT introduces Maximum Data Similarity (MDS) selection for general knowledge preservation and Knowledge & Generalization Retention Fine-Tuning (K&G RFT), a LoRA-based scheme balancing task-specific adaptation and knowledge retention.

Result: MedSeqFT improves segmentation performance with an average Dice increase of 3.0% across ten tasks, demonstrates enhanced transferability on unseen tasks, and shows robust loss landscapes and parameter behavior.

Conclusion: MedSeqFT is a robust and knowledge-retentive sequential fine-tuning paradigm for adapting foundation models to continually evolving clinical segmentation tasks, outperforming state-of-the-art methods.

Abstract: Foundation models have become a promising paradigm for advancing medical
image analysis, particularly for segmentation tasks where downstream
applications often emerge sequentially. Existing fine-tuning strategies,
however, remain limited: parallel fine-tuning isolates tasks and fails to
exploit shared knowledge, while multi-task fine-tuning requires simultaneous
access to all datasets and struggles with incremental task integration. To
address these challenges, we propose MedSeqFT, a sequential fine-tuning
framework that progressively adapts pre-trained models to new tasks while
refining their representational capacity. MedSeqFT introduces two core
components: (1) Maximum Data Similarity (MDS) selection, which identifies
downstream samples most representative of the original pre-training
distribution to preserve general knowledge, and (2) Knowledge and
Generalization Retention Fine-Tuning (K&G RFT), a LoRA-based knowledge
distillation scheme that balances task-specific adaptation with the retention
of pre-trained knowledge. Extensive experiments on two multi-task datasets
covering ten 3D segmentation tasks demonstrate that MedSeqFT consistently
outperforms state-of-the-art fine-tuning strategies, yielding substantial
performance gains (e.g., an average Dice improvement of 3.0%). Furthermore,
evaluations on two unseen tasks (COVID-19-20 and Kidney) verify that MedSeqFT
enhances transferability, particularly for tumor segmentation. Visual analyses
of loss landscapes and parameter variations further highlight the robustness of
MedSeqFT. These results establish sequential fine-tuning as an effective,
knowledge-retentive paradigm for adapting foundation models to evolving
clinical tasks. Code will be released.

</details>


### [216] [PathoHR: Hierarchical Reasoning for Vision-Language Models in Pathology](https://arxiv.org/abs/2509.06105)
*Yating Huang,Ziyan Huang,Lintao Xiang,Qijun Yang,Hujun Yin*

Main category: cs.CV

TL;DR: This paper introduces PathoHR-Bench, a benchmark to assess vision-language (VL) models in pathology, showing current models' limitations and proposing a pathology-specific VL training scheme that improves performance.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenges of accurately analyzing pathological images and the inadequacy of existing vision-language models in capturing the complex reasoning required for this domain.

Method: The researchers created PathoHR-Bench to evaluate hierarchical semantic understanding and developed a pathology-specific VL training scheme combining enhanced and perturbed sample generation for multimodal contrastive learning.

Result: Their method achieved state-of-the-art results on PathoHR-Bench and six other pathology datasets, demonstrating its effectiveness.

Conclusion: This approach enhances the applicability of VL models for fine-grained pathology representation and clinical use, overcoming current limitations.

Abstract: Accurate analysis of pathological images is essential for automated tumor
diagnosis but remains challenging due to high structural similarity and subtle
morphological variations in tissue images. Current vision-language (VL) models
often struggle to capture the complex reasoning required for interpreting
structured pathological reports. To address these limitations, we propose
PathoHR-Bench, a novel benchmark designed to evaluate VL models' abilities in
hierarchical semantic understanding and compositional reasoning within the
pathology domain. Results of this benchmark reveal that existing VL models fail
to effectively model intricate cross-modal relationships, hence limiting their
applicability in clinical setting. To overcome this, we further introduce a
pathology-specific VL training scheme that generates enhanced and perturbed
samples for multimodal contrastive learning. Experimental evaluations
demonstrate that our approach achieves state-of-the-art performance on
PathoHR-Bench and six additional pathology datasets, highlighting its
effectiveness in fine-grained pathology representation.

</details>


### [217] [CARDIE: clustering algorithm on relevant descriptors for image enhancement](https://arxiv.org/abs/2509.06116)
*Giulia Bonino,Luca Alberto Rizzo*

Main category: cs.CV

TL;DR: The paper introduces CARDIE, an unsupervised image clustering algorithm tailored for image enhancement, and demonstrates its relevance and utility in enhancing datasets for tone mapping and denoising.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in automatic image clustering, its application in image enhancement is underdeveloped due to the challenge of defining meaningful clusters for this task.

Method: CARDIE clusters images based on color and luminosity content and includes a way to quantify image enhancement impact on luminance distribution and variance.

Result: The study shows that CARDIE's clustering is more relevant to image enhancement tasks than semantic clustering, and its datasets lead to better performance for tone mapping and denoising algorithms.

Conclusion: CARDIE offers a significant contribution to the field of image enhancement by enabling improved clustering and dataset resampling, with publicly available code enhancing reproducibility.

Abstract: Automatic image clustering is a cornerstone of computer vision, yet its
application to image enhancement remains limited, primarily due to the
difficulty of defining clusters that are meaningful for this specific task. To
address this issue, we introduce CARDIE, an unsupervised algorithm that
clusters images based on their color and luminosity content. In addition, we
introduce a method to quantify the impact of image enhancement algorithms on
luminance distribution and local variance. Using this method, we demonstrate
that CARDIE produces clusters more relevant to image enhancement than those
derived from semantic image attributes. Furthermore, we demonstrate that CARDIE
clusters can be leveraged to resample image enhancement datasets, leading to
improved performance for tone mapping and denoising algorithms. To encourage
adoption and ensure reproducibility, we publicly release CARDIE code on our
GitHub.

</details>


### [218] [SpecSwin3D: Generating Hyperspectral Imagery from Multispectral Data via Transformer Networks](https://arxiv.org/abs/2509.06122)
*Tang Sui,Songxi Yang,Qunying Huang*

Main category: cs.CV

TL;DR: The paper introduces SpecSwin3D, a transformer-based model that generates high-quality hyperspectral imagery from multispectral inputs, addressing spatial and spectral resolution trade-offs effectively.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral and multispectral imagery have trade-offs in spatial and spectral resolution, and existing methods struggle to maintain both spatial and spectral fidelity in hyperspectral generation.

Method: SpecSwin3D is a transformer-based model that reconstructs hyperspectral imagery using five multispectral bands as input. It uses cascade training to enhance fidelity and an optimized band sequence within a 3D shifted-window transformer framework.

Result: SpecSwin3D achieves significant improvements in metrics like PSNR, SAM, and SSIM compared to baseline models and demonstrates practical applicability in land use classification and burnt area segmentation.

Conclusion: SpecSwin3D successfully balances spatial detail and spectral fidelity, outperforming existing methods and showing strong potential for practical applications.

Abstract: Multispectral and hyperspectral imagery are widely used in agriculture,
environmental monitoring, and urban planning due to their complementary spatial
and spectral characteristics. A fundamental trade-off persists: multispectral
imagery offers high spatial but limited spectral resolution, while
hyperspectral imagery provides rich spectra at lower spatial resolution. Prior
hyperspectral generation approaches (e.g., pan-sharpening variants, matrix
factorization, CNNs) often struggle to jointly preserve spatial detail and
spectral fidelity. In response, we propose SpecSwin3D, a transformer-based
model that generates hyperspectral imagery from multispectral inputs while
preserving both spatial and spectral quality. Specifically, SpecSwin3D takes
five multispectral bands as input and reconstructs 224 hyperspectral bands at
the same spatial resolution. In addition, we observe that reconstruction errors
grow for hyperspectral bands spectrally distant from the input bands. To
address this, we introduce a cascade training strategy that progressively
expands the spectral range to stabilize learning and improve fidelity.
Moreover, we design an optimized band sequence that strategically repeats and
orders the five selected multispectral bands to better capture pairwise
relations within a 3D shifted-window transformer framework. Quantitatively, our
model achieves a PSNR of 35.82 dB, SAM of 2.40{\deg}, and SSIM of 0.96,
outperforming the baseline MHF-Net by +5.6 dB in PSNR and reducing ERGAS by
more than half. Beyond reconstruction, we further demonstrate the practical
value of SpecSwin3D on two downstream tasks, including land use classification
and burnt area segmentation.

</details>


### [219] [RetinaGuard: Obfuscating Retinal Age in Fundus Images for Biometric Privacy Preserving](https://arxiv.org/abs/2509.06142)
*Zhengquan Luo,Chi Liu,Dongfu Xiao,Zhen Yu,Yueye Wang,Tianqing Zhu*

Main category: cs.CV

TL;DR: This paper introduces RetinaGuard, a framework to protect biometric privacy in medical images by obscuring retinal age using a generative adversarial mechanism while maintaining diagnostic utility.


<details>
  <summary>Details</summary>
Motivation: To address the privacy risks posed by biometric data, specifically retinal age derived from medical images, which can inadvertently lead to sensitive bioinformation leakage.

Method: The proposed method, RetinaGuard, uses a generative adversarial masking mechanism and a multiple-to-one knowledge distillation strategy to obscure retinal age while preserving visual quality and diagnostic capabilities.

Result: RetinaGuard effectively obfuscated retinal age predictions with minimal impact on image quality and preserved diagnostic features. It also demonstrated adaptability to other medical image biomarkers.

Conclusion: RetinaGuard provides a universal privacy defense mechanism for medical images, ensuring biometric privacy without compromising their diagnostic utility.

Abstract: The integration of AI with medical images enables the extraction of implicit
image-derived biomarkers for a precise health assessment. Recently, retinal
age, a biomarker predicted from fundus images, is a proven predictor of
systemic disease risks, behavioral patterns, aging trajectory and even
mortality. However, the capability to infer such sensitive biometric data
raises significant privacy risks, where unauthorized use of fundus images could
lead to bioinformation leakage, breaching individual privacy. In response, we
formulate a new research problem of biometric privacy associated with medical
images and propose RetinaGuard, a novel privacy-enhancing framework that
employs a feature-level generative adversarial masking mechanism to obscure
retinal age while preserving image visual quality and disease diagnostic
utility. The framework further utilizes a novel multiple-to-one knowledge
distillation strategy incorporating a retinal foundation model and diverse
surrogate age encoders to enable a universal defense against black-box age
prediction models. Comprehensive evaluations confirm that RetinaGuard
successfully obfuscates retinal age prediction with minimal impact on image
quality and pathological feature representation. RetinaGuard is also flexible
for extension to other medical image derived biomarkers. RetinaGuard is also
flexible for extension to other medical image biomarkers.

</details>


### [220] [UniVerse-1: Unified Audio-Video Generation via Stitching of Experts](https://arxiv.org/abs/2509.06155)
*Duomin Wang,Wei Zuo,Aojie Li,Ling-Hao Chen,Xinyao Liao,Deyu Zhou,Zixin Yin,Xili Dai,Daxin Jiang,Gang Yu*

Main category: cs.CV

TL;DR: UniVerse-1 is a unified model capable of generating synchronized audio and video outputs, leveraging pre-trained experts through a deep fusion method and incorporating an advanced annotation pipeline.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper is to address challenges in generating well-coordinated audio-video outputs and advancing efficiency and accuracy in temporal alignment, aiming to achieve performance comparable to state-of-the-art models.

Method: The method involves using a 'stitching of experts' (SoE) approach to fuse pre-trained video and music models, coupled with an online annotation pipeline to ensure accurate synchronization during training.

Result: UniVerse-1 demonstrates high-quality coordinated audio-visual output with enhanced temporal alignment, validated using approximately 7,600 hours of data and the newly introduced Verse-Bench benchmark dataset.

Conclusion: The paper concludes by emphasizing the publicly available UniVerse-1 model and code as a resource for advancing research in audio-video generation, aiming to close gaps with leading models like Veo3.

Abstract: We introduce UniVerse-1, a unified, Veo-3-like model capable of
simultaneously generating coordinated audio and video. To enhance training
efficiency, we bypass training from scratch and instead employ a stitching of
experts (SoE) technique. This approach deeply fuses the corresponding blocks of
pre-trained video and music generation experts models, thereby fully leveraging
their foundational capabilities. To ensure accurate annotations and temporal
alignment for both ambient sounds and speech with video content, we developed
an online annotation pipeline that processes the required training data and
generates labels during training process. This strategy circumvents the
performance degradation often caused by misalignment text-based annotations.
Through the synergy of these techniques, our model, after being finetuned on
approximately 7,600 hours of audio-video data, produces results with
well-coordinated audio-visuals for ambient sounds generation and strong
alignment for speech generation. To systematically evaluate our proposed
method, we introduce Verse-Bench, a new benchmark dataset. In an effort to
advance research in audio-video generation and to close the performance gap
with state-of-the-art models such as Veo3, we make our model and code publicly
available. We hope this contribution will benefit the broader research
community. Project page: https://dorniwang.github.io/UniVerse-1/.

</details>


### [221] [UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual Representation Learning](https://arxiv.org/abs/2509.06165)
*Huy Le,Nhat Chung,Tung Kieu,Jingkang Yang,Ngan Le*

Main category: cs.CV

TL;DR: The paper introduces UNO, a unified framework for Video Scene Graph Generation (VidSGG) that can handle both box-level and pixel-level tasks using a single-stage architecture, emphasizing parameter sharing and end-to-end efficiency.


<details>
  <summary>Details</summary>
Motivation: Previous approaches to VidSGG often require separate architectures and multi-stage training for box-level or pixel-level tasks, which lack scalability and generalization.

Method: The paper proposes UNO, which uses an extended slot attention mechanism to decompose features into object and relation slots, supplemented by temporal consistency learning and dynamic triplet prediction for robust temporal interaction modeling.

Result: UNO is tested on standard VidSGG benchmarks and demonstrates competitive performance in both box-level and pixel-level tasks, alongside better efficiency due to its unified design.

Conclusion: UNO successfully simplifies VidSGG by creating a single, efficient framework that generalizes across multiple visual granularities, laying groundwork for scalable and unified dynamic content representation.

Abstract: Video Scene Graph Generation (VidSGG) aims to represent dynamic visual
content by detecting objects and modeling their temporal interactions as
structured graphs. Prior studies typically target either coarse-grained
box-level or fine-grained panoptic pixel-level VidSGG, often requiring
task-specific architectures and multi-stage training pipelines. In this paper,
we present UNO (UNified Object-centric VidSGG), a single-stage, unified
framework that jointly addresses both tasks within an end-to-end architecture.
UNO is designed to minimize task-specific modifications and maximize parameter
sharing, enabling generalization across different levels of visual granularity.
The core of UNO is an extended slot attention mechanism that decomposes visual
features into object and relation slots. To ensure robust temporal modeling, we
introduce object temporal consistency learning, which enforces consistent
object representations across frames without relying on explicit tracking
modules. Additionally, a dynamic triplet prediction module links relation slots
to corresponding object pairs, capturing evolving interactions over time. We
evaluate UNO on standard box-level and pixel-level VidSGG benchmarks. Results
demonstrate that UNO not only achieves competitive performance across both
tasks but also offers improved efficiency through a unified, object-centric
design.

</details>


### [222] [AI-Based Applied Innovation for Fracture Detection in X-rays Using Custom CNN and Transfer Learning Models](https://arxiv.org/abs/2509.06228)
*Amna Hassan,Ilsa Afzaal,Nouman Muneeb,Aneeqa Batool,Hamail Noor*

Main category: cs.CV

TL;DR: This paper addresses global health issues related to bone fractures by developing an AI-based CNN model for automated fracture detection in X-rays, achieving high accuracy and outperforming transfer learning models.


<details>
  <summary>Details</summary>
Motivation: Bone fractures are a significant health problem causing pain and reduced mobility, particularly in low-resource settings with limited access to radiology services.

Method: The study utilized a custom Convolutional Neural Network (CNN) trained on the publicly available FracAtlas dataset and compared its performance to transfer learning models like EfficientNetB0, MobileNetV2, and ResNet50.

Result: The custom CNN demonstrated 95.96% accuracy, precision of 0.94, recall of 0.88, and an F1-score of 0.91, showing better performance than transfer learning models under class imbalance and dataset limitations.

Conclusion: Lightweight CNNs show promise for fracture detection, emphasizing the need for diverse datasets, fair benchmarking, and external validation for clinical application.

Abstract: Bone fractures present a major global health challenge, often resulting in
pain, reduced mobility, and productivity loss, particularly in low-resource
settings where access to expert radiology services is limited. Conventional
imaging methods suffer from high costs, radiation exposure, and dependency on
specialized interpretation. To address this, we developed an AI-based solution
for automated fracture detection from X-ray images using a custom Convolutional
Neural Network (CNN) and benchmarked it against transfer learning models
including EfficientNetB0, MobileNetV2, and ResNet50. Training was conducted on
the publicly available FracAtlas dataset, comprising 4,083 anonymized
musculoskeletal radiographs. The custom CNN achieved 95.96% accuracy, 0.94
precision, 0.88 recall, and an F1-score of 0.91 on the FracAtlas dataset.
Although transfer learning models (EfficientNetB0, MobileNetV2, ResNet50)
performed poorly in this specific setup, these results should be interpreted in
light of class imbalance and data set limitations. This work highlights the
promise of lightweight CNNs for detecting fractures in X-rays and underscores
the importance of fair benchmarking, diverse datasets, and external validation
for clinical translation

</details>


### [223] [Interleaving Reasoning for Better Text-to-Image Generation](https://arxiv.org/abs/2509.06945)
*Wenxuan Huang,Shuang Chen,Zheyong Xie,Shaosheng Cao,Shixiang Tang,Yufan Shen,Qingyu Yin,Wenbo Hu,Xiaoman Wang,Yuntian Tang,Junbo Qiao,Yue Guo,Yao Hu,Zhenfei Yin,Philip Torr,Yu Cheng,Wanli Ouyang,Shaohui Lin*

Main category: cs.CV

TL;DR: The paper introduces a framework called Interleaving Reasoning Generation (IRG) for improving Text-to-Image (T2I) generation by interleaving text-based reasoning and image generation, achieving significant performance gains on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to bridge the gap in instruction following and detail preservation in multimodal models, specifically in text-to-image generation, by incorporating interleaved reasoning strategies.

Method: The proposed method, IRG, alternates between text-based reasoning and image synthesis, guided by a framework called Interleaving Reasoning Generation Learning (IRGL). This involves a two-stage training process to establish core content and refine finer details using a curated dataset, IRGL-300K.

Result: The IRG framework demonstrates state-of-the-art performance, with improvements of 5-10 points on various benchmarks (GenEval, WISE, TIIF, GenAI-Bench, OneIG-EN) and significant enhancements in visual quality and fidelity.

Conclusion: The approach of interleaving reasoning and image generation effectively enhances text-to-image synthesis, presenting a notable advancement in the field. The resources for replication and further exploration will be publicly available.

Abstract: Unified multimodal understanding and generation models recently have achieve
significant improvement in image generation capability, yet a large gap remains
in instruction following and detail preservation compared to systems that
tightly couple comprehension with generation such as GPT-4o. Motivated by
recent advances in interleaving reasoning, we explore whether such reasoning
can further improve Text-to-Image (T2I) generation. We introduce Interleaving
Reasoning Generation (IRG), a framework that alternates between text-based
thinking and image synthesis: the model first produces a text-based thinking to
guide an initial image, then reflects on the result to refine fine-grained
details, visual quality, and aesthetics while preserving semantics. To train
IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),
which targets two sub-goals: (1) strengthening the initial think-and-generate
stage to establish core content and base quality, and (2) enabling high-quality
textual reflection and faithful implementation of those refinements in a
subsequent image. We curate IRGL-300K, a dataset organized into six decomposed
learning modes that jointly cover learning text-based thinking, and full
thinking-image trajectories. Starting from a unified foundation model that
natively emits interleaved text-image outputs, our two-stage training first
builds robust thinking and reflection, then efficiently tunes the IRG pipeline
in the full thinking-image trajectory data. Extensive experiments show SoTA
performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,
GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality
and fine-grained fidelity. The code, model weights and datasets will be
released in: https://github.com/Osilly/Interleaving-Reasoning-Generation .

</details>


### [224] [Exploring Light-Weight Object Recognition for Real-Time Document Detection](https://arxiv.org/abs/2509.06246)
*Lucas Wojcik,Luiz Coelho,Roger Granada,David Menotti*

Main category: cs.CV

TL;DR: This paper proposes an efficient pipeline for document detection and rectification, adapting the IWPOD-Net model for ID card and passport datasets.


<details>
  <summary>Details</summary>
Motivation: To address the gap in real-time document detection and rectification, which is crucial for automatic information retrieval but underexplored.

Method: The authors adapt IWPOD-Net, a license plate detection network, and retrain it for synthetic ID card datasets, using data augmentation and cross-dataset validation. Comparative evaluations with other methods are also conducted.

Result: The proposed model is smaller and more efficient while maintaining competitive OCR quality metrics, demonstrating real-time effectiveness.

Conclusion: Automatic information retrieval can achieve state-of-the-art performance without perfect document rectification, and the model outperforms others in efficiency and OCR quality.

Abstract: Object Recognition and Document Skew Estimation have come a long way in terms
of performance and efficiency. New models follow one of two directions:
improving performance using larger models, and improving efficiency using
smaller models. However, real-time document detection and rectification is a
niche that is largely unexplored by the literature, yet it remains a vital step
for automatic information retrieval from visual documents. In this work, we
strive towards an efficient document detection pipeline that is satisfactory in
terms of Optical Character Recognition (OCR) retrieval and faster than other
available solutions. We adapt IWPOD-Net, a license plate detection network, and
train it for detection on NBID, a synthetic ID card dataset. We experiment with
data augmentation and cross-dataset validation with MIDV (another synthetic ID
and passport document dataset) to find the optimal scenario for the model.
Other methods from both the Object Recognition and Skew Estimation
state-of-the-art are evaluated for comparison with our approach. We use each
method to detect and rectify the document, which is then read by an OCR system.
The OCR output is then evaluated using a novel OCR quality metric based on the
Levenshtein distance. Since the end goal is to improve automatic information
retrieval, we use the overall OCR quality as a performance metric. We observe
that with a promising model, document rectification does not have to be perfect
to attain state-of-the-art performance scores. We show that our model is
smaller and more efficient than current state-of-the-art solutions while
retaining a competitive OCR quality metric. All code is available at
https://github.com/BOVIFOCR/iwpod-doc-corners.git

</details>


### [225] [Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes](https://arxiv.org/abs/2509.06266)
*Mohsen Gholami,Ahmad Rezaei,Zhou Weimin,Yong Zhang,Mohammad Akbari*

Main category: cs.CV

TL;DR: The paper introduces Ego3D-Bench, a benchmark for evaluating Vision-Language Models (VLMs) in ego-centric, multi-view spatial reasoning, and proposes Ego3D-VLM to enhance 3D spatial reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language Models lack the ability to understand 3D spatial relationships, which is critical for embodied AI in real-world scenarios like robotics and autonomous vehicles.

Method: The authors developed Ego3D-Bench with over 8,600 QA pairs for evaluating VLMs on spatial reasoning tasks and proposed Ego3D-VLM, a post-training framework that utilizes cognitive maps to improve spatial reasoning.

Result: The study shows a significant gap between human-level and VLM performance in spatial reasoning, but Ego3D-VLM achieves a 12% improvement in multi-choice QA and a 56% improvement in absolute distance estimation on the benchmark.

Conclusion: Ego3D-Bench and Ego3D-VLM could push VLMs closer to human-level spatial reasoning in real-world, ego-centric environments, offering modular tools for enhanced AI capabilities.

Abstract: Understanding 3D spatial relationships remains a major limitation of current
Vision-Language Models (VLMs). Prior work has addressed this issue by creating
spatial question-answering (QA) datasets based on single images or indoor
videos. However, real-world embodied AI agents such as robots and self-driving
cars typically rely on ego-centric, multi-view observations. To this end, we
introduce Ego3D-Bench, a new benchmark designed to evaluate the spatial
reasoning abilities of VLMs using ego-centric, multi-view outdoor data.
Ego3D-Bench comprises over 8,600 QA pairs, created with significant involvement
from human annotators to ensure quality and diversity. We benchmark 16 SOTA
VLMs, including GPT-4o, Gemini1.5-Pro, InternVL3, and Qwen2.5-VL. Our results
reveal a notable performance gap between human level scores and VLM
performance, highlighting that current VLMs still fall short of human level
spatial understanding. To bridge this gap, we propose Ego3D-VLM, a
post-training framework that enhances 3D spatial reasoning of VLMs. Ego3D-VLM
generates cognitive map based on estimated global 3D coordinates, resulting in
12% average improvement on multi-choice QA and 56% average improvement on
absolute distance estimation. Ego3D-VLM is modular and can be integrated with
any existing VLM. Together, Ego3D-Bench and Ego3D-VLM offer valuable tools for
advancing toward human level spatial understanding in real-world, multi-view
environments.

</details>


### [226] [AI-driven Remote Facial Skin Hydration and TEWL Assessment from Selfie Images: A Systematic Solution](https://arxiv.org/abs/2509.06282)
*Cecelia Soh,Rizhao Cai,Monalisha Paul,Dennis Sng,Alex Kot*

Main category: cs.CV

TL;DR: This paper proposes using smartphone selfies to estimate skin hydration (SH) and trans-epidermal water loss (TEWL) for accessible skin health analysis.


<details>
  <summary>Details</summary>
Motivation: Current methods for measuring SH and TEWL require specialized instruments and visits to dermatology clinics, making skin health monitoring inaccessible to the general public.

Method: The authors developed a novel Skin-Prior Adaptive Vision Transformer model for SH/TEWL regression. This includes data collection, preprocessing, and addressing annotation imbalance using symmetric-based contrastive regularization.

Result: Experiments demonstrated that the proposed model effectively reduces biases caused by imbalanced annotations and accurately estimates SH and TEWL from selfie images.

Conclusion: This is the first study enabling remote and accessible skin assessment using facial selfies, bridging the gap between AI-driven computer vision and skincare research.

Abstract: Skin health and disease resistance are closely linked to the skin barrier
function, which protects against environmental factors and water loss. Two key
physiological indicators can quantitatively represent this barrier function:
skin hydration (SH) and trans-epidermal water loss (TEWL). Measurement of SH
and TEWL is valuable for the public to monitor skin conditions regularly,
diagnose dermatological issues, and personalize their skincare regimens.
However, these measurements are not easily accessible to general users unless
they visit a dermatology clinic with specialized instruments. To tackle this
problem, we propose a systematic solution to estimate SH and TEWL from selfie
facial images remotely with smartphones. Our solution encompasses multiple
stages, including SH/TEWL data collection, data preprocessing, and formulating
a novel Skin-Prior Adaptive Vision Transformer model for SH/TEWL regression.
Through experiments, we identified the annotation imbalance of the SH/TEWL data
and proposed a symmetric-based contrastive regularization to reduce the model
bias due to the imbalance effectively. This work is the first study to explore
skin assessment from selfie facial images without physical measurements. It
bridges the gap between computer vision and skin care research, enabling
AI-driven accessible skin analysis for broader real-world applications.

</details>


### [227] [Prototype-Aware Multimodal Alignment for Open-Vocabulary Visual Grounding](https://arxiv.org/abs/2509.06291)
*Jiangnan Xie,Xiaolong Zheng,Liang Zheng*

Main category: cs.CV

TL;DR: The paper proposes "Prototype-Aware Multimodal Learning" (PAML), addressing challenges in Visual Grounding (VG) for open-vocabulary scenes, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations in open-vocabulary visual grounding, including imperfect visual-language alignment, insufficient cross-modal fusion, and ineffective semantic prototype use.

Method: The paper introduces the PAML framework, leveraging ALBEF for feature alignment, a Visual Discriminative Feature Encoder for enhancing object representations, a prototype discovering mechanism for semantic enhancement, and a Multi-stage Decoder for multimodal integration.

Result: PAML achieves strong results in standard scenes and state-of-the-art performance in open-vocabulary scenes across five benchmark datasets.

Conclusion: The proposed PAML method enhances open-vocabulary visual grounding through novel multimodal integration and semantic prototype mechanisms, validated by experimental results.

Abstract: Visual Grounding (VG) aims to utilize given natural language queries to
locate specific target objects within images. While current transformer-based
approaches demonstrate strong localization performance in standard scene (i.e,
scenarios without any novel objects), they exhibit notable limitations in
open-vocabulary scene (i.e, both familiar and novel object categories during
testing). These limitations primarily stem from three key factors: (1)
imperfect alignment between visual and linguistic modalities, (2) insufficient
cross-modal feature fusion, and (3) ineffective utilization of semantic
prototype information. To overcome these challenges, we present Prototype-Aware
Multimodal Learning (PAML), an innovative framework that systematically
addresses these issues through several key components: First, we leverage ALBEF
to establish robust cross-modal alignment during initial feature encoding.
Subsequently, our Visual Discriminative Feature Encoder selectively enhances
salient object representations while suppressing irrelevant visual context. The
framework then incorporates a novel prototype discovering and inheriting
mechanism that extracts and aggregates multi-neighbor semantic prototypes to
facilitate open-vocabulary recognition. These enriched features undergo
comprehensive multimodal integration through our Multi-stage Decoder before
final bounding box regression. Extensive experiments across five benchmark
datasets validate our approach, showing competitive performance in standard
scene while achieving state-of-the-art results in open-vocabulary scene. Our
code is available at https://github.com/plankXie/PAML.

</details>


### [228] [Video-based Generalized Category Discovery via Memory-Guided Consistency-Aware Contrastive Learning](https://arxiv.org/abs/2509.06306)
*Zhang Jing,Pu Nan,Xie Yu Xiang,Guo Yanming,Lu Qianqi,Zou Shiwei,Yan Jie,Chen Yan*

Main category: cs.CV

TL;DR: This paper addresses the challenge of Generalized Category Discovery (GCD) in videos and introduces a method leveraging temporal-spatial information for better category discovery.


<details>
  <summary>Details</summary>
Motivation: Existing GCD methods focus on static images, which are insufficient for novel category discovery. There's a need to explore temporal and spatial cues in video contexts.

Method: The paper introduces the Memory-guided Consistency-aware Contrastive Learning (MCCL) framework, which combines consistency-aware contrastive learning and memory-enhanced representation techniques.

Result: Experimental validation shows the proposed method outperforming image-based GCD adaptations on video benchmarks, emphasizing the importance of temporal information.

Conclusion: Integrating multi-perspective temporal cues into contrastive learning is crucial for successful Video-GCD, and the proposed MCCL framework achieves significant advancements in this area.

Abstract: Generalized Category Discovery (GCD) is an emerging and challenging
open-world problem that has garnered increasing attention in recent years. Most
existing GCD methods focus on discovering categories in static images. However,
relying solely on static visual content is often insufficient to reliably
discover novel categories. To bridge this gap, we extend the GCD problem to the
video domain and introduce a new setting, termed Video-GCD. Thus, effectively
integrating multi-perspective information across time is crucial for accurate
Video-GCD. To tackle this challenge, we propose a novel Memory-guided
Consistency-aware Contrastive Learning (MCCL) framework, which explicitly
captures temporal-spatial cues and incorporates them into contrastive learning
through a consistency-guided voting mechanism. MCCL consists of two core
components: Consistency-Aware Contrastive Learning(CACL) and Memory-Guided
Representation Enhancement (MGRE). CACL exploits multiperspective temporal
features to estimate consistency scores between unlabeled instances, which are
then used to weight the contrastive loss accordingly. MGRE introduces a
dual-level memory buffer that maintains both feature-level and logit-level
representations, providing global context to enhance intra-class compactness
and inter-class separability. This in turn refines the consistency estimation
in CACL, forming a mutually reinforcing feedback loop between representation
learning and consistency modeling. To facilitate a comprehensive evaluation, we
construct a new and challenging Video-GCD benchmark, which includes action
recognition and bird classification video datasets. Extensive experiments
demonstrate that our method significantly outperforms competitive GCD
approaches adapted from image-based settings, highlighting the importance of
temporal information for discovering novel categories in videos. The code will
be publicly available.

</details>


### [229] [Text4Seg++: Advancing Image Segmentation via Generative Language Modeling](https://arxiv.org/abs/2509.06321)
*Mengcheng Lan,Chaofeng Chen,Jiaxing Xu,Zongrui Li,Yiping Ke,Xudong Jiang,Yingchen Yu,Yunqing Zhao,Song Bai*

Main category: cs.CV

TL;DR: This paper introduces a method to integrate image segmentation into Multimodal Large Language Models (MLLMs) by converting it into a text generation problem using text-based representations of segmentation masks.


<details>
  <summary>Details</summary>
Motivation: While MLLMs excel in vision-language tasks, incorporating image segmentation into these models faces significant challenges. A new paradigm is needed to simplify and improve segmentation within the context of MLLMs.

Method: The authors propose a 'text-as-mask' paradigm, introducing semantic descriptors that turn segmentation masks into patch-aligned text labels. They optimize these descriptors using Row-wise Run-Length Encoding (R-RLE) and later enhance the approach with box-wise semantic descriptors and semantic bricks in the Text4Seg and Text4Seg++ frameworks.

Result: The proposed frameworks, Text4Seg and its refinement Text4Seg++, achieve state-of-the-art segmentation performance across diverse datasets without task-specific fine-tuning. R-RLE leads to a 74% reduction in text length and a 3× faster inference.

Conclusion: The study establishes that text-driven segmentation can be highly effective, scalable, and generalizable within MLLMs, providing strong performance and compatibility with existing backbones.

Abstract: Multimodal Large Language Models (MLLMs) have shown exceptional capabilities
in vision-language tasks. However, effectively integrating image segmentation
into these models remains a significant challenge. In this work, we propose a
novel text-as-mask paradigm that casts image segmentation as a text generation
problem, eliminating the need for additional decoders and significantly
simplifying the segmentation process. Our key innovation is semantic
descriptors, a new textual representation of segmentation masks where each
image patch is mapped to its corresponding text label. We first introduce
image-wise semantic descriptors, a patch-aligned textual representation of
segmentation masks that integrates naturally into the language modeling
pipeline. To enhance efficiency, we introduce the Row-wise Run-Length Encoding
(R-RLE), which compresses redundant text sequences, reducing the length of
semantic descriptors by 74% and accelerating inference by $3\times$, without
compromising performance. Building upon this, our initial framework Text4Seg
achieves strong segmentation performance across a wide range of vision tasks.
To further improve granularity and compactness, we propose box-wise semantic
descriptors, which localizes regions of interest using bounding boxes and
represents region masks via structured mask tokens called semantic bricks. This
leads to our refined model, Text4Seg++, which formulates segmentation as a
next-brick prediction task, combining precision, scalability, and generative
efficiency. Comprehensive experiments on natural and remote sensing datasets
show that Text4Seg++ consistently outperforms state-of-the-art models across
diverse benchmarks without any task-specific fine-tuning, while remaining
compatible with existing MLLM backbones. Our work highlights the effectiveness,
scalability, and generalizability of text-driven image segmentation within the
MLLM framework.

</details>


### [230] [Towards scalable organ level 3D plant segmentation: Bridging the data algorithm computing gap](https://arxiv.org/abs/2509.06329)
*Ruiming Du,Guangxun Zhai,Tian Qiu,Yu Jiang*

Main category: cs.CV

TL;DR: The paper addresses the challenges in 3D segmentation for plant morphology by providing a review of datasets, deep learning methods, and introducing an open-source benchmarking framework called Plant Segmentation Studio (PSS).


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in using 3D segmentation for plant phenotyping caused by limited datasets, adaptation difficulties of advanced neural networks for plant point clouds, and lack of standardized benchmarks and evaluation methods.

Method: The paper systematically reviews existing datasets, summarizes deep learning methods for segmentation, introduces an open-source benchmarking framework (PSS), and evaluates networks and sim-to-real learning strategies via experiments.

Result: Sparse convolutional networks and transformer-based segmentation show strong performance. Synthetic data generation through modeling and augmentation is effective in reducing the need for manual annotation.

Conclusion: This study bridges the gap between algorithmic advances and practical applications in 3D plant phenotyping, providing tools and a roadmap for developing data-efficient and generalizable deep learning solutions.

Abstract: The precise characterization of plant morphology provides valuable insights
into plant environment interactions and genetic evolution. A key technology for
extracting this information is 3D segmentation, which delineates individual
plant organs from complex point clouds. Despite significant progress in general
3D computer vision domains, the adoption of 3D segmentation for plant
phenotyping remains limited by three major challenges: i) the scarcity of
large-scale annotated datasets, ii) technical difficulties in adapting advanced
deep neural networks to plant point clouds, and iii) the lack of standardized
benchmarks and evaluation protocols tailored to plant science. This review
systematically addresses these barriers by: i) providing an overview of
existing 3D plant datasets in the context of general 3D segmentation domains,
ii) systematically summarizing deep learning-based methods for point cloud
semantic and instance segmentation, iii) introducing Plant Segmentation Studio
(PSS), an open-source framework for reproducible benchmarking, and iv)
conducting extensive quantitative experiments to evaluate representative
networks and sim-to-real learning strategies. Our findings highlight the
efficacy of sparse convolutional backbones and transformer-based instance
segmentation, while also emphasizing the complementary role of modeling-based
and augmentation-based synthetic data generation for sim-to-real learning in
reducing annotation demands. In general, this study bridges the gap between
algorithmic advances and practical deployment, providing immediate tools for
researchers and a roadmap for developing data-efficient and generalizable deep
learning solutions in 3D plant phenotyping. Data and code are available at
https://github.com/perrydoremi/PlantSegStudio.

</details>


### [231] [Quantitative Currency Evaluation in Low-Resource Settings through Pattern Analysis to Assist Visually Impaired Users](https://arxiv.org/abs/2509.06331)
*Md Sultanul Islam Ovi,Mainul Hossain,Md Badsha Biswas*

Main category: cs.CV

TL;DR: A unified framework for currency evaluation integrating denomination classification, damage quantification using UCDI, and counterfeit detection.


<details>
  <summary>Details</summary>
Motivation: Enhance currency recognition usability and authenticity in low-resource environments for visually impaired users and offline validation.

Method: Three integrated modules: lightweight CNNs for denomination classification, UCDI metric for damage quantification, and feature-based template matching for counterfeit detection.

Result: Custom_CNN with high performance and low parameter count, UCDI providing usability scores, and successful counterfeit detection across varied conditions.

Conclusion: Real-time, efficient solutions enable inclusive currency evaluation addressing practical challenges in constrained environments.

Abstract: Currency recognition systems often overlook usability and authenticity
assessment, especially in low-resource environments where visually impaired
users and offline validation are common. While existing methods focus on
denomination classification, they typically ignore physical degradation and
forgery, limiting their applicability in real-world conditions. This paper
presents a unified framework for currency evaluation that integrates three
modules: denomination classification using lightweight CNN models, damage
quantification through a novel Unified Currency Damage Index (UCDI), and
counterfeit detection using feature-based template matching. The dataset
consists of over 82,000 annotated images spanning clean, damaged, and
counterfeit notes. Our Custom_CNN model achieves high classification
performance with low parameter count. The UCDI metric provides a continuous
usability score based on binary mask loss, chromatic distortion, and structural
feature loss. The counterfeit detection module demonstrates reliable
identification of forged notes across varied imaging conditions. The framework
supports real-time, on-device inference and addresses key deployment challenges
in constrained environments. Results show that accurate, interpretable, and
compact solutions can support inclusive currency evaluation in practical
settings.

</details>


### [232] [Harnessing Object Grounding for Time-Sensitive Video Understanding](https://arxiv.org/abs/2509.06335)
*Tz-Ying Wu,Sharath Nittur Sridhar,Subarna Tripathi*

Main category: cs.CV

TL;DR: This paper introduces GO-Tokenizer, a module to improve time-sensitive video understanding tasks in Video-LLMs by encoding object information compactly, demonstrating improved performance across multiple models and tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance Video-LLMs in tackling time-sensitive video understanding tasks by incorporating grounded object annotations.

Method: The authors propose GO-Tokenizer, a lightweight module that utilizes object detectors to encode object information compactly, avoiding extra token length and susceptibility to noise from textual object descriptions.

Result: GO-Tokenizer improved performance when pretraining Video-LLMs, outperforming vanilla models and textual description-based methods across datasets, models, and tasks such as temporal localization and dense captioning.

Conclusion: Integrating GO-Tokenizer enhances Video-LLMs' time-sensitive video interpretation capabilities in a robust and generalized manner across diverse tasks.

Abstract: We propose to improve the time-sensitive video understanding (TSV) capability
of video large language models (Video-LLMs) with grounded objects (GO). We
hypothesize that TSV tasks can benefit from GO within frames, which is
supported by our preliminary experiments on LITA, a state-of-the-art Video-LLM
for reasoning temporal localization. While augmenting prompts with textual
description of these object annotations improves the performance of LITA, it
also introduces extra token length and susceptibility to the noise in object
level information. To address this, we propose GO-Tokenizer, a lightweight
add-on module for Video-LLMs leveraging off-the-shelf object detectors to
encode compact object information on the fly. Experimental results demonstrate
that pretraining with GO-Tokenizer outperforms the vanilla Video-LLM and its
counterpart utilizing textual description of objects in the prompt. The gain
generalizes across different models, datasets and video understanding tasks
such as reasoning temporal localization and dense captioning.

</details>


### [233] [Multi View Slot Attention Using Paraphrased Texts For Face Anti-Spoofing](https://arxiv.org/abs/2509.06336)
*Jeongmin Yu,Susang Kim,Kisu Lee,Taekyoung Kwon,Won-Yong Shin,Ha Young Kim*

Main category: cs.CV

TL;DR: The paper introduces MVP-FAS, a face anti-spoofing framework using Multi-View Slot attention and Multi-Text Patch Alignment to improve generalization and detect sophisticated spoofing clues.


<details>
  <summary>Details</summary>
Motivation: Current CLIP-based FAS models fail to fully exploit patch embedding tokens and rely on a single text prompt per class (e.g., 'live' or 'fake'), leading to limited generalization and inadequate spoofing clue detection.

Method: The proposed framework, MVP-FAS, includes two key components: Multi-View Slot Attention (MVS) to generate spatial and contextual features, and Multi-Text Patch Alignment (MTPA) to align patches with diverse paraphrased text representations, thus enhancing semantic robustness.

Result: MVP-FAS demonstrates superior performance in generalization and cross-domain face anti-spoofing tasks, surpassing state-of-the-art methods in experimental evaluations.

Conclusion: The MVP-FAS framework effectively addresses limitations in existing CLIP-based FAS methods by leveraging multiple perspectives and paraphrased texts, achieving better generalization and cross-domain capabilities.

Abstract: Recent face anti-spoofing (FAS) methods have shown remarkable cross-domain
performance by employing vision-language models like CLIP. However, existing
CLIP-based FAS models do not fully exploit CLIP's patch embedding tokens,
failing to detect critical spoofing clues. Moreover, these models rely on a
single text prompt per class (e.g., 'live' or 'fake'), which limits
generalization. To address these issues, we propose MVP-FAS, a novel framework
incorporating two key modules: Multi-View Slot attention (MVS) and Multi-Text
Patch Alignment (MTPA). Both modules utilize multiple paraphrased texts to
generate generalized features and reduce dependence on domain-specific text.
MVS extracts local detailed spatial features and global context from patch
embeddings by leveraging diverse texts with multiple perspectives. MTPA aligns
patches with multiple text representations to improve semantic robustness.
Extensive experiments demonstrate that MVP-FAS achieves superior generalization
performance, outperforming previous state-of-the-art methods on cross-domain
datasets. Code: https://github.com/Elune001/MVP-FAS.

</details>


### [234] [A Multi-Modal Deep Learning Framework for Colorectal Pathology Diagnosis: Integrating Histological and Colonoscopy Data in a Pilot Study](https://arxiv.org/abs/2509.06351)
*Krithik Ramesh,Ritvik Koneru*

Main category: cs.CV

TL;DR: This study develops a unified diagnostic pipeline using deep learning to classify histopathological slides and colonoscopy videos for detecting colorectal diseases.


<details>
  <summary>Details</summary>
Motivation: Current diagnostic methods for colorectal diseases are inefficient and variable, requiring separate analyses of histology and colonoscopy data.

Method: The study employs ResNet-50 CNN within a single pipeline using class-balancing learning, robust augmentation, and calibration methods, integrating data from PathMNIST and HyperKvasir datasets.

Result: The unified CNN-based pipeline is interpretable and reproducible, enabling accurate classification of colorectal diagnostic images and videos.

Conclusion: The proposed pipeline has the potential to improve diagnostic efficiency and accuracy for colorectal diseases by effectively combining multiple diagnostic modalities.

Abstract: Colorectal diseases, including inflammatory conditions and neoplasms, require
quick, accurate care to be effectively treated. Traditional diagnostic
pipelines require extensive preparation and rely on separate, individual
evaluations on histological images and colonoscopy footage, introducing
possible variability and inefficiencies. This pilot study proposes a unified
deep learning network that uses convolutional neural networks (CN N s) to
classify both histopathological slides and colonoscopy video frames in one
pipeline. The pipeline integrates class-balancing learning, robust
augmentation, and calibration methods to ensure accurate results. Static colon
histology images were taken from the PathMNIST dataset, and the lower
gastrointestinal (colonoscopy) videos were drawn from the HyperKvasir dataset.
The CNN architecture used was ResNet-50. This study demonstrates an
interpretable and reproducible diagnostic pipeline that unifies multiple
diagnostic modalities to advance and ease the detection of colorectal diseases.

</details>


### [235] [MRD-LiNet: A Novel Lightweight Hybrid CNN with Gradient-Guided Unlearning for Improved Drought Stress Identification](https://arxiv.org/abs/2509.06367)
*Aswini Kumar Patra,Lingaraj Sahoo*

Main category: cs.CV

TL;DR: This paper proposes a lightweight CNN framework inspired by ResNet, DenseNet, and MobileNet to address drought stress detection in agriculture. It achieves significant parameter reduction while maintaining high accuracy and introduces a novel machine unlearning mechanism for adaptability.


<details>
  <summary>Details</summary>
Motivation: Drought stress significantly impacts crop productivity, and there is a need for fast, precise, and resource-efficient detection methods. Traditional methods and modern deep learning architectures are not optimized for real-time or resource-constrained agricultural settings.

Method: A lightweight hybrid CNN framework was developed, combining elements from ResNet, DenseNet, and MobileNet. It reduces trainable parameters while maintaining accuracy. Additionally, a gradient norm-based machine unlearning mechanism was introduced to enhance model adaptability.

Result: The model achieved a 15x reduction in trainable parameters compared to traditional CNN and Vision Transformer models, while still delivering competitive accuracy. It was tested on an annotated aerial image dataset of potato fields.

Conclusion: The proposed framework is effective, scalable, and resource-efficient for drought stress monitoring, making it suitable for precision agriculture in resource-constrained environments.

Abstract: Drought stress is a major threat to global crop productivity, making its
early and precise detection essential for sustainable agricultural management.
Traditional approaches, though useful, are often time-consuming and
labor-intensive, which has motivated the adoption of deep learning methods. In
recent years, Convolutional Neural Network (CNN) and Vision Transformer
architectures have been widely explored for drought stress identification;
however, these models generally rely on a large number of trainable parameters,
restricting their use in resource-limited and real-time agricultural settings.
To address this challenge, we propose a novel lightweight hybrid CNN framework
inspired by ResNet, DenseNet, and MobileNet architectures. The framework
achieves a remarkable 15-fold reduction in trainable parameters compared to
conventional CNN and Vision Transformer models, while maintaining competitive
accuracy. In addition, we introduce a machine unlearning mechanism based on a
gradient norm-based influence function, which enables targeted removal of
specific training data influence, thereby improving model adaptability. The
method was evaluated on an aerial image dataset of potato fields with
expert-annotated healthy and drought-stressed regions. Experimental results
show that our framework achieves high accuracy while substantially lowering
computational costs. These findings highlight its potential as a practical,
scalable, and adaptive solution for drought stress monitoring in precision
agriculture, particularly under resource-constrained conditions.

</details>


### [236] [Your Super Resolution Model is not Enough for Tackling Real-World Scenarios](https://arxiv.org/abs/2509.06387)
*Dongsik Yoon,Jongeun Kim*

Main category: cs.CV

TL;DR: The paper introduces the Scale-Aware Attention Module (SAAM) to enable arbitrary-scale super-resolution using existing fixed-scale models. SAAM is efficient and improves image quality with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Traditional Single Image Super-Resolution (SISR) models struggle to generalize across varying scale factors, making them limited in real-world applications.

Method: The paper proposes SAAM, which uses lightweight, scale-adaptive feature extraction and upsampling mechanisms. It integrates Simple Attention Module (SimAM) for efficiency and gradient variance loss to enhance image detail sharpness.

Result: SAAM is successfully integrated into state-of-the-art SR models and delivers competitive or superior results for integer and non-integer scales, tested extensively on benchmark datasets.

Conclusion: The proposed method offers a practical and computation-efficient solution for robust multi-scale super-resolution in real-world scenarios by retrofitting existing models.

Abstract: Despite remarkable progress in Single Image Super-Resolution (SISR),
traditional models often struggle to generalize across varying scale factors,
limiting their real-world applicability. To address this, we propose a plug-in
Scale-Aware Attention Module (SAAM) designed to retrofit modern fixed-scale SR
models with the ability to perform arbitrary-scale SR. SAAM employs
lightweight, scale-adaptive feature extraction and upsampling, incorporating
the Simple parameter-free Attention Module (SimAM) for efficient guidance and
gradient variance loss to enhance sharpness in image details. Our method
integrates seamlessly into multiple state-of-the-art SR backbones (e.g., SCNet,
HiT-SR, OverNet), delivering competitive or superior performance across a wide
range of integer and non-integer scale factors. Extensive experiments on
benchmark datasets demonstrate that our approach enables robust multi-scale
upscaling with minimal computational overhead, offering a practical solution
for real-world scenarios.

</details>


### [237] [AI-based response assessment and prediction in longitudinal imaging for brain metastases treated with stereotactic radiosurgery](https://arxiv.org/abs/2509.06396)
*Lorenz Achim Kuhn,Daniel Abler,Jonas Richiardi,Andreas F. Hottinger,Luis Schiappacasse,Vincent Dunet,Adrien Depeursinge,Vincent Andrearczyk*

Main category: cs.CV

TL;DR: This study develops an automated pipeline for longitudinal data analysis to predict Brain Metastases (BM) response to Stereotactic Radiosurgery (SRS), achieving robust predictive accuracy.


<details>
  <summary>Details</summary>
Motivation: Brain Metastases treatment and monitoring represent a significant clinical workload, with a need for better understanding of treatment success and outcomes through longitudinal imaging analysis.

Method: The researchers curated a dataset of 896 BMs across 177 patients, applied data-driven clustering for trajectory analysis, and used machine learning methods (including gradient boosting and graph machine learning) to predict lesion-level responses.

Result: They identified five growth trajectories and achieved strong predictive performance, with AUCs of up to 0.90 for gradient boosting and 0.88 for Graph Machine Learning.

Conclusion: The study demonstrates potential for automating BM response assessments, improving precision, and enabling scalable methods for personalized clinical decision-making in cancer care.

Abstract: Brain Metastases (BM) are a large contributor to mortality of patients with
cancer. They are treated with Stereotactic Radiosurgery (SRS) and monitored
with Magnetic Resonance Imaging (MRI) at regular follow-up intervals according
to treatment guidelines. Analyzing and quantifying this longitudinal imaging
represents an intractable workload for clinicians. As a result, follow-up
images are not annotated and merely assessed by observation. Response to
treatment in longitudinal imaging is being studied, to better understand growth
trajectories and ultimately predict treatment success or toxicity as early as
possible. In this study, we implement an automated pipeline to curate a large
longitudinal dataset of SRS treatment data, resulting in a cohort of 896 BMs in
177 patients who were monitored for >360 days at approximately two-month
intervals at Lausanne University Hospital (CHUV). We use a data-driven
clustering to identify characteristic trajectories. In addition, we predict 12
months lesion-level response using classical as well as graph machine learning
Graph Machine Learning (GML). Clustering revealed 5 dominant growth
trajectories with distinct final response categories. Response prediction
reaches up to 0.90 AUC (CI95%=0.88-0.92) using only pre-treatment and first
follow-up MRI with gradient boosting. Similarly, robust predictive performance
of up to 0.88 AUC (CI95%=0.86-0.90) was obtained using GML, offering more
flexibility with a single model for multiple input time-points configurations.
Our results suggest potential automation and increased precision for the
comprehensive assessment and prediction of BM response to SRS in longitudinal
MRI. The proposed pipeline facilitates scalable data curation for the
investigation of BM growth patterns, and lays the foundation for clinical
decision support systems aiming at optimizing personalized care.

</details>


### [238] [3DOF+Quantization: 3DGS quantization for large scenes with limited Degrees of Freedom](https://arxiv.org/abs/2509.06400)
*Matthieu Gendrin,Stéphane Pateux,Théo Ladune*

Main category: cs.CV

TL;DR: 3D Gaussian Splatting enables 6 degrees of freedom for novel view generation but faces challenges in large scenes with limited view zones. A new quantization scheme based on spherical coordinates is proposed to address coordinate quantization issues.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome limitations in scene reconstruction when input views are acquired from a limited zone in large scenes, focusing on the accuracy of position quantization for better projection.

Method: A study on the impact of position error is conducted, followed by the proposal of a new quantization scheme based on spherical coordinates.

Result: Projection errors are shown to be proportional to the squared inverse distance, and the rate-distortion performance of the proposed spherical quantization method is validated on the Garden scene.

Conclusion: The new quantization scheme improves the accuracy and efficiency of 3D scene reconstruction under limited degrees of freedom, enhancing practical applications for large-scale scenes.

Abstract: 3D Gaussian Splatting (3DGS) is a major breakthrough in 3D scene
reconstruction. With a number of views of a given object or scene, the
algorithm trains a model composed of 3D gaussians, which enables the production
of novel views from arbitrary points of view. This freedom of movement is
referred to as 6DoF for 6 degrees of freedom: a view is produced for any
position (3 degrees), orientation of camera (3 other degrees). On large scenes,
though, the input views are acquired from a limited zone in space, and the
reconstruction is valuable for novel views from the same zone, even if the
scene itself is almost unlimited in size. We refer to this particular case as
3DoF+, meaning that the 3 degrees of freedom of camera position are limited to
small offsets around the central position. Considering the problem of
coordinate quantization, the impact of position error on the projection error
in pixels is studied. It is shown that the projection error is proportional to
the squared inverse distance of the point being projected. Consequently, a new
quantization scheme based on spherical coordinates is proposed. Rate-distortion
performance of the proposed method are illustrated on the well-known Garden
scene.

</details>


### [239] [VQualA 2025 Challenge on Image Super-Resolution Generated Content Quality Assessment: Methods and Results](https://arxiv.org/abs/2509.06413)
*Yixiao Li,Xin Li,Chris Wei Zhou,Shuo Xing,Hadi Amirpour,Xiaoshuai Hao,Guanghui Yue,Baoquan Zhao,Weide Liu,Xiaoyuan Yang,Zhengzhong Tu,Xinyu Li,Chuanbiao Song,Chenqi Zhang,Jun Lan,Huijia Zhu,Weiqiang Wang,Xiaoyan Sun,Shishun Tian,Dongyang Yan,Weixia Zhang,Junlin Chen,Wei Sun,Zhihua Wang,Zhuohang Shi,Zhizun Luo,Hang Ouyang,Tianxin Xiao,Fan Yang,Zhaowang Wu,Kaixin Deng*

Main category: cs.CV

TL;DR: This study introduces the ISRGC-Q Challenge focused on evaluating the quality of super-resolution images using datasets emphasizing modern generative techniques like GANs and diffusion models.


<details>
  <summary>Details</summary>
Motivation: To address the need for effective evaluation of perceptual quality in super-resolution images generated by advanced methods, and to analyze their unique artifacts.

Method: Hosted the ISRGC-Q Challenge based on the ISRGen-QA dataset, encouraging participants to submit solutions for assessing quality in SR images created with generative approaches.

Result: 108 participants registered, with 4 teams providing valid submissions showcasing state-of-the-art performance on the ISRGen-QA dataset.

Conclusion: The challenge highlights advancements in Super-Resolution Image Quality Assessment by focusing on modern generative techniques and provides a benchmark for SOTA performance.

Abstract: This paper presents the ISRGC-Q Challenge, built upon the Image
Super-Resolution Generated Content Quality Assessment (ISRGen-QA) dataset, and
organized as part of the Visual Quality Assessment (VQualA) Competition at the
ICCV 2025 Workshops. Unlike existing Super-Resolution Image Quality Assessment
(SR-IQA) datasets, ISRGen-QA places a greater emphasis on SR images generated
by the latest generative approaches, including Generative Adversarial Networks
(GANs) and diffusion models. The primary goal of this challenge is to analyze
the unique artifacts introduced by modern super-resolution techniques and to
evaluate their perceptual quality effectively. A total of 108 participants
registered for the challenge, with 4 teams submitting valid solutions and fact
sheets for the final testing phase. These submissions demonstrated
state-of-the-art (SOTA) performance on the ISRGen-QA dataset. The project is
publicly available at: https://github.com/Lighting-YXLI/ISRGen-QA.

</details>


### [240] [Phantom-Insight: Adaptive Multi-cue Fusion for Video Camouflaged Object Detection with Multimodal LLM](https://arxiv.org/abs/2509.06422)
*Hua Zhang,Changjiang Luo,Ruoyu Chen*

Main category: cs.CV

TL;DR: The paper introduces Phantom-Insight, a method combining SAM and MLLM for video camouflaged object detection, addressing issues with edge separability and object-background merging in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: The study aims to solve challenges in video camouflaged object detection caused by dynamic environments, where existing SAM and MLLM methods fail in edge separability and object-background distinction.

Method: Phantom-Insight integrates SAM and MLLM by enhancing temporal-spatial feature fusion, utilizing dynamic visual token scoring and prompt networks, and employing a decoupled foreground-background learning strategy for robust segmentation.

Result: Phantom-Insight achieves state-of-the-art performance on the MoCA-Mask dataset and demonstrates robust generalization by effectively detecting unseen camouflaged objects on the CAD2016 dataset.

Conclusion: The approach significantly improves video camouflaged object detection with stronger edge detail separability, adaptive texture modeling, and generalized object-background differentiation, establishing itself as a leading method in the field.

Abstract: Video camouflaged object detection (VCOD) is challenging due to dynamic
environments. Existing methods face two main issues: (1) SAM-based methods
struggle to separate camouflaged object edges due to model freezing, and (2)
MLLM-based methods suffer from poor object separability as large language
models merge foreground and background. To address these issues, we propose a
novel VCOD method based on SAM and MLLM, called Phantom-Insight. To enhance the
separability of object edge details, we represent video sequences with temporal
and spatial clues and perform feature fusion via LLM to increase information
density. Next, multiple cues are generated through the dynamic foreground
visual token scoring module and the prompt network to adaptively guide and
fine-tune the SAM model, enabling it to adapt to subtle textures. To enhance
the separability of objects and background, we propose a decoupled
foreground-background learning strategy. By generating foreground and
background cues separately and performing decoupled training, the visual token
can effectively integrate foreground and background information independently,
enabling SAM to more accurately segment camouflaged objects in the video.
Experiments on the MoCA-Mask dataset show that Phantom-Insight achieves
state-of-the-art performance across various metrics. Additionally, its ability
to detect unseen camouflaged objects on the CAD2016 dataset highlights its
strong generalization ability.

</details>


### [241] [When Language Model Guides Vision: Grounding DINO for Cattle Muzzle Detection](https://arxiv.org/abs/2509.06427)
*Rabin Dulal,Lihong Zheng,Muhammad Ashad Kabir*

Main category: cs.CV

TL;DR: The paper presents a zero-shot muzzle detection framework for cattle using a vision-language model called Grounding DINO, which achieves a mAP@0.5 of 76.8% without requiring annotated datasets or task-specific training.


<details>
  <summary>Details</summary>
Motivation: Current cattle muzzle detection methods relying on supervised models like YOLO require extensive annotated datasets and are not adaptable to unseen data, creating a need for a more scalable and flexible approach.

Method: The study employs Grounding DINO, a vision-language model, to perform zero-shot muzzle detection by leveraging natural language prompts, eliminating the need for task-specific training or annotated data.

Result: The proposed framework achieves a mAP@0.5 accuracy of 76.8%, demonstrating its feasibility and effectiveness for cattle muzzle localization without requiring annotated data.

Conclusion: This zero-shot detection framework provides a scalable and annotation-free alternative to supervised methods, offering adaptability and ease of deployment in cattle identification across diverse conditions.

Abstract: Muzzle patterns are among the most effective biometric traits for cattle
identification. Fast and accurate detection of the muzzle region as the region
of interest is critical to automatic visual cattle identification.. Earlier
approaches relied on manual detection, which is labor-intensive and
inconsistent. Recently, automated methods using supervised models like YOLO
have become popular for muzzle detection. Although effective, these methods
require extensive annotated datasets and tend to be trained data-dependent,
limiting their performance on new or unseen cattle. To address these
limitations, this study proposes a zero-shot muzzle detection framework based
on Grounding DINO, a vision-language model capable of detecting muzzles without
any task-specific training or annotated data. This approach leverages natural
language prompts to guide detection, enabling scalable and flexible muzzle
localization across diverse breeds and environments. Our model achieves a mean
Average Precision (mAP)@0.5 of 76.8\%, demonstrating promising performance
without requiring annotated data. To our knowledge, this is the first research
to provide a real-world, industry-oriented, and annotation-free solution for
cattle muzzle detection. The framework offers a practical alternative to
supervised methods, promising improved adaptability and ease of deployment in
livestock monitoring applications.

</details>


### [242] [Perception-oriented Bidirectional Attention Network for Image Super-resolution Quality Assessment](https://arxiv.org/abs/2509.06442)
*Yixiao Li,Xiaoyuan Yang,Guanghui Yue,Jun Fu,Qiuping Jiang,Xu Jia,Paul L. Rosin,Hantao Liu,Wei Zhou*

Main category: cs.CV

TL;DR: This paper introduces a novel metric for assessing super-resolution (SR) image quality, named Perception-oriented Bidirectional Attention Network (PBAN), composed of advanced perception-oriented modules.


<details>
  <summary>Details</summary>
Motivation: There is a lack of robust and comprehensive full-reference image quality assessment (IQA) metrics to compare different SR algorithms effectively.

Method: The proposed PBAN incorporates three modules: an image encoder, perception-oriented bidirectional attention (PBA), and a quality prediction mechanism, all tailored to improve IQA for SR images.

Result: Experiments demonstrate that PBAN's quality assessment performance surpasses existing state-of-the-art methods.

Conclusion: PBAN proves to be effective in offering a more accurate and perception-based approach to evaluating super-resolution image quality.

Abstract: Many super-resolution (SR) algorithms have been proposed to increase image
resolution. However, full-reference (FR) image quality assessment (IQA) metrics
for comparing and evaluating different SR algorithms are limited. In this work,
we propose the Perception-oriented Bidirectional Attention Network (PBAN) for
image SR FR-IQA, which is composed of three modules: an image encoder module, a
perception-oriented bidirectional attention (PBA) module, and a quality
prediction module. First, we encode the input images for feature
representations. Inspired by the characteristics of the human visual system, we
then construct the perception-oriented PBA module. Specifically, different from
existing attention-based SR IQA methods, we conceive a Bidirectional Attention
to bidirectionally construct visual attention to distortion, which is
consistent with the generation and evaluation processes of SR images. To
further guide the quality assessment towards the perception of distorted
information, we propose Grouped Multi-scale Deformable Convolution, enabling
the proposed method to adaptively perceive distortion. Moreover, we design
Sub-information Excitation Convolution to direct visual perception to both
sub-pixel and sub-channel attention. Finally, the quality prediction module is
exploited to integrate quality-aware features and regress quality scores.
Extensive experiments demonstrate that our proposed PBAN outperforms
state-of-the-art quality assessment methods.

</details>


### [243] [Cross3DReg: Towards a Large-scale Real-world Cross-source Point Cloud Registration Benchmark](https://arxiv.org/abs/2509.06456)
*Zongyi Xu,Zhongpeng Lang,Yilong Chen,Shanshan Zhao,Xiaoshui Huang,Yifan Zuo,Yan Zhang,Qianni Zhang,Xinbo Gao*

Main category: cs.CV

TL;DR: This research introduces Cross3DReg, a large-scale multi-modal dataset for cross-source point cloud registration, and proposes a novel framework combining overlap prediction and attention-guided feature matching to improve registration accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Cross-source point cloud registration is challenging due to the lack of publicly available datasets and sensor-induced feature mismatches, limiting robust and accurate alignment.

Method: The authors built the Cross3DReg dataset using diverse lidar systems and developed a registration framework that predicts overlapping regions to filter irrelevant points and utilizes an attention module combining image and geometric data for reliable feature matching.

Result: The proposed framework significantly improves registration performance, reducing rotation error by 63.2%, translation error by 40.2%, and increasing recall by 5.4%, surpassing current methods.

Conclusion: Cross3DReg dataset and the novel framework advance cross-source point cloud registration, enabling more accurate alignment of sensor data with real-world robustness.

Abstract: Cross-source point cloud registration, which aims to align point cloud data
from different sensors, is a fundamental task in 3D vision. However, compared
to the same-source point cloud registration, cross-source registration faces
two core challenges: the lack of publicly available large-scale real-world
datasets for training the deep registration models, and the inherent
differences in point clouds captured by multiple sensors. The diverse patterns
induced by the sensors pose great challenges in robust and accurate point cloud
feature extraction and matching, which negatively influence the registration
accuracy. To advance research in this field, we construct Cross3DReg, the
currently largest and real-world multi-modal cross-source point cloud
registration dataset, which is collected by a rotating mechanical lidar and a
hybrid semi-solid-state lidar, respectively. Moreover, we design an
overlap-based cross-source registration framework, which utilizes unaligned
images to predict the overlapping region between source and target point
clouds, effectively filtering out redundant points in the irrelevant regions
and significantly mitigating the interference caused by noise in
non-overlapping areas. Then, a visual-geometric attention guided matching
module is proposed to enhance the consistency of cross-source point cloud
features by fusing image and geometric information to establish reliable
correspondences and ultimately achieve accurate and robust registration.
Extensive experiments show that our method achieves state-of-the-art
registration performance. Our framework reduces the relative rotation error
(RRE) and relative translation error (RTE) by $63.2\%$ and $40.2\%$,
respectively, and improves the registration recall (RR) by $5.4\%$, which
validates its effectiveness in achieving accurate cross-source registration.

</details>


### [244] [IGAff: Benchmarking Adversarial Iterative and Genetic Affine Algorithms on Deep Neural Networks](https://arxiv.org/abs/2509.06459)
*Sebastian-Vasile Echim,Andrei-Alexandru Preda,Dumitru-Clementin Cercel,Florin Pop*

Main category: cs.CV

TL;DR: This paper explores the impact of adversarial attacks on specific neural network architectures using two novel black-box algorithms (ATA and AGA) and demonstrates their effectiveness over existing methods.


<details>
  <summary>Details</summary>
Motivation: To understand and address the weaknesses of deep neural networks in the context of adversarial attacks, especially in the black-box scenario.

Method: The study uses two iterative adversarial algorithms, ATA and AGA, based on affine transformations and genetic algorithms, to attack different neural network architectures and evaluate their robustness on datasets such as Tiny ImageNet, Caltech-256, and Food-101.

Result: The proposed algorithms outshine existing black-box adversarial methods, demonstrating accuracy improvements of up to 8.82%. Insights into algorithm parameters and adversarial defenses are also provided.

Conclusion: The findings suggest that ATA and AGA are effective tools for probing and enhancing adversarial robustness in image classification tasks, outperforming comparable methods and offering valuable understanding about network vulnerabilities and defenses.

Abstract: Deep neural networks currently dominate many fields of the artificial
intelligence landscape, achieving state-of-the-art results on numerous tasks
while remaining hard to understand and exhibiting surprising weaknesses. An
active area of research focuses on adversarial attacks, which aim to generate
inputs that uncover these weaknesses. However, this proves challenging,
especially in the black-box scenario where model details are inaccessible. This
paper explores in detail the impact of such adversarial algorithms on
ResNet-18, DenseNet-121, Swin Transformer V2, and Vision Transformer network
architectures. Leveraging the Tiny ImageNet, Caltech-256, and Food-101
datasets, we benchmark two novel black-box iterative adversarial algorithms
based on affine transformations and genetic algorithms: 1) Affine
Transformation Attack (ATA), an iterative algorithm maximizing our attack score
function using random affine transformations, and 2) Affine Genetic Attack
(AGA), a genetic algorithm that involves random noise and affine
transformations. We evaluate the performance of the models in the algorithm
parameter variation, data augmentation, and global and targeted attack
configurations. We also compare our algorithms with two black-box adversarial
algorithms, Pixle and Square Attack. Our experiments yield better results on
the image classification task than similar methods in the literature, achieving
an accuracy improvement of up to 8.82%. We provide noteworthy insights into
successful adversarial defenses and attacks at both global and targeted levels,
and demonstrate adversarial robustness through algorithm parameter variation.

</details>


### [245] [Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning](https://arxiv.org/abs/2509.06461)
*Yuyao Ge,Shenghua Liu,Yiwei Wang,Lingrui Mei,Baolong Bi,Xuanshan Zhou,Jiayu Yao,Jiafeng Guo,Xueqi Cheng*

Main category: cs.CV

TL;DR: The paper introduces CARVE, a training-free method to enhance vision-language models (VLMs) using attention contrasting to address visual complexity.


<details>
  <summary>Details</summary>
Motivation: To overcome the degradation in performance of VLMs in complex visual environments without relying on additional training or external tools.

Method: Analyzed VLMs' attention patterns and introduced a theoretical framework to contrast attention maps. Developed the CARVE method for pixel-level attention contrasting to separate semantic signals from visual noise.

Result: CARVE significantly improves VLM performance, with up to 75% enhancement on open-source models in experiments.

Conclusion: Attention contrast leveraging visual signal decomposition is an efficient method to handle visual complexity and improve VLM-based reasoning.

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable success across
diverse visual tasks, yet their performance degrades in complex visual
environments. While existing enhancement approaches require additional
training, rely on external segmentation tools, or operate at coarse-grained
levels, they overlook the innate ability within VLMs. To bridge this gap, we
investigate VLMs' attention patterns and discover that: (1) visual complexity
strongly correlates with attention entropy, negatively impacting reasoning
performance; (2) attention progressively refines from global scanning in
shallow layers to focused convergence in deeper layers, with convergence degree
determined by visual complexity. (3) Theoretically, we prove that the contrast
of attention maps between general queries and task-specific queries enables the
decomposition of visual signal into semantic signals and visual noise
components. Building on these insights, we propose Contrastive Attention
Refinement for Visual Enhancement (CARVE), a training-free method that extracts
task-relevant visual signals through attention contrasting at the pixel level.
Extensive experiments demonstrate that CARVE consistently enhances performance,
achieving up to 75% improvement on open-source models. Our work provides
critical insights into the interplay between visual complexity and attention
mechanisms, offering an efficient pathway for improving visual reasoning with
contrasting attention.

</details>


### [246] [A Statistical 3D Stomach Shape Model for Anatomical Analysis](https://arxiv.org/abs/2509.06464)
*Erez Posner,Ore Shtalrid,Oded Erell,Daniel Noy,Moshe Bouhnik*

Main category: cs.CV

TL;DR: The paper introduces the first statistical 3D shape model of the stomach, combining synthetic data generation, parametric modeling, and real-world validation, with applications in surgical planning, diagnostics, and education.


<details>
  <summary>Details</summary>
Motivation: Current methods for creating 3D models of internal organs like the stomach are hindered by data limitations and technical challenges, necessitating an innovative pipeline to represent anatomical variability.

Method: The researchers developed a pipeline to generate synthetic 3D stomach datasets informed by anatomical variability, created a 3D statistical shape model, and refined it using semi-supervised alignment with CT-based data for improved generalization.

Result: The model robustly generalized to a test set of real stomach CT scans, demonstrating high fit accuracy, and it is made publicly available to support further research.

Conclusion: This research provides a novel tool for personalized healthcare applications and broader organ modeling, advancing the field with public access to both their synthetic dataset and statistical shape model.

Abstract: Realistic and parameterized 3D models of human anatomy have become invaluable
in research, diagnostics, and surgical planning. However, the development of
detailed models for internal organs, such as the stomach, has been limited by
data availability and methodological challenges. In this paper, we propose a
novel pipeline for the generation of synthetic 3D stomach models, enabling the
creation of anatomically diverse morphologies informed by established studies
on stomach shape variability. Using this pipeline, we construct a dataset of
synthetic stomachs. Building on this dataset, we develop a 3D statistical shape
model of the stomach, trained to capture natural anatomical variability in a
low-dimensional shape space. The model is further refined using CT meshes
derived from publicly available datasets through a semi-supervised alignment
process, enhancing its ability to generalize to unseen anatomical variations.
We evaluated the model on a held-out test set of real stomach CT scans,
demonstrating robust generalization and fit accuracy. We make the statistical
shape model along with the synthetic dataset publicly available on GitLab:
https://gitlab.com/Erez.Posner/stomach_pytorch to facilitate further research.
This work introduces the first statistical 3D shape model of the stomach, with
applications ranging from surgical simulation and pre-operative planning to
medical education and computational modeling. By combining synthetic data
generation, parametric modeling, and real-world validation, our approach
represents a significant advancement in organ modeling and opens new
possibilities for personalized healthcare solutions.

</details>


### [247] [Does DINOv3 Set a New Medical Vision Standard?](https://arxiv.org/abs/2509.06467)
*Che Liu,Yinda Chen,Haoyuan Shi,Jinpeng Lu,Bailiang Jian,Jiazhen Pan,Linghan Cai,Jiayi Wang,Yundi Zhang,Jun Li,Cosmin I. Bercea,Cheng Ouyang,Chen Chen,Zhiwei Xiong,Benedikt Wiestler,Christian Wachinger,Daniel Rueckert,Wenjia Bai,Rossella Arcucci*

Main category: cs.CV

TL;DR: This paper evaluates the performance of DINOv3, a self-supervised vision transformer, in the medical imaging domain, exploring its potential to perform well without domain-specific pre-training. It finds strong performance across various tasks but notes limitations in highly specialized scenarios.


<details>
  <summary>Details</summary>
Motivation: The study aims to analyze if cutting-edge vision foundation models pre-trained on natural images, like DINOv3, can effectively generalize to medical imaging tasks without additional domain-specific pre-training.

Method: DINOv3 is benchmarked across 2D and 3D classification and segmentation tasks in various medical imaging modalities, with analyses on scalability by tuning model sizes and input resolutions.

Result: DINOv3 demonstrates robust performance across several medical vision tasks, in some cases surpassing medical-specific models like BiomedCLIP, but it struggles in specialized domains like WSIs, EM, and PET. It also shows inconsistent scalability in the medical context.

Conclusion: While DINOv3 serves as a strong baseline for medical imaging tasks with its generalized visual features, its limitations in deeply specialized applications emphasize the need for further research to adapt such models more effectively to unique medical contexts.

Abstract: The advent of large-scale vision foundation models, pre-trained on diverse
natural images, has marked a paradigm shift in computer vision. However, how
the frontier vision foundation models' efficacies transfer to specialized
domains remains such as medical imaging remains an open question. This report
investigates whether DINOv3, a state-of-the-art self-supervised vision
transformer (ViT) that features strong capability in dense prediction tasks,
can directly serve as a powerful, unified encoder for medical vision tasks
without domain-specific pre-training. To answer this, we benchmark DINOv3
across common medical vision tasks, including 2D/3D classification and
segmentation on a wide range of medical imaging modalities. We systematically
analyze its scalability by varying model sizes and input image resolutions. Our
findings reveal that DINOv3 shows impressive performance and establishes a
formidable new baseline. Remarkably, it can even outperform medical-specific
foundation models like BiomedCLIP and CT-Net on several tasks, despite being
trained solely on natural images. However, we identify clear limitations: The
model's features degrade in scenarios requiring deep domain specialization,
such as in Whole-Slide Pathological Images (WSIs), Electron Microscopy (EM),
and Positron Emission Tomography (PET). Furthermore, we observe that DINOv3
does not consistently obey scaling law in the medical domain; performance does
not reliably increase with larger models or finer feature resolutions, showing
diverse scaling behaviors across tasks. Ultimately, our work establishes DINOv3
as a strong baseline, whose powerful visual features can serve as a robust
prior for multiple complex medical tasks. This opens promising future
directions, such as leveraging its features to enforce multiview consistency in
3D reconstruction.

</details>


### [248] [FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution Remote Sensing Change Detection](https://arxiv.org/abs/2509.06482)
*Zhongxiang Xie,Shuangxi Miao,Yuhan Jiang,Zhewei Zhang,Jing Yao,Xuecao Li,Jianxi Huang,Pedram Ghamisi*

Main category: cs.CV

TL;DR: The paper introduces FSG-Net, an innovative model for high-resolution remote sensing image change detection, addressing false alarms and semantic gaps with advanced modules, and achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges of false alarms caused by radiometric variations and the semantic gap between deep and shallow feature fusion in change detection using remote sensing images.

Method: FSG-Net employs a Discrepancy-Aware Wavelet Interaction Module to process frequency components and reduce pseudo-changes, a Synergistic Temporal-Spatial Attention Module to emphasize genuine changes, and a Lightweight Gated Fusion Unit to bridge the semantic gap by selectively integrating shallow layer details.

Result: The model achieved state-of-the-art F1-scores of 94.16% on CDD, 89.51% on GZ-CD, and 91.27% on LEVIR-CD benchmarks.

Conclusion: FSG-Net effectively mitigates key issues in change detection for remote sensing images, offering a robust solution that outperforms existing methods, as validated by benchmarking results.

Abstract: Change detection from high-resolution remote sensing images lies as a
cornerstone of Earth observation applications, yet its efficacy is often
compromised by two critical challenges. First, false alarms are prevalent as
models misinterpret radiometric variations from temporal shifts (e.g.,
illumination, season) as genuine changes. Second, a non-negligible semantic gap
between deep abstract features and shallow detail-rich features tends to
obstruct their effective fusion, culminating in poorly delineated boundaries.
To step further in addressing these issues, we propose the Frequency-Spatial
Synergistic Gated Network (FSG-Net), a novel paradigm that aims to
systematically disentangle semantic changes from nuisance variations.
Specifically, FSG-Net first operates in the frequency domain, where a
Discrepancy-Aware Wavelet Interaction Module (DAWIM) adaptively mitigates
pseudo-changes by discerningly processing different frequency components.
Subsequently, the refined features are enhanced in the spatial domain by a
Synergistic Temporal-Spatial Attention Module (STSAM), which amplifies the
saliency of genuine change regions. To finally bridge the semantic gap, a
Lightweight Gated Fusion Unit (LGFU) leverages high-level semantics to
selectively gate and integrate crucial details from shallow layers.
Comprehensive experiments on the CDD, GZ-CD, and LEVIR-CD benchmarks validate
the superiority of FSG-Net, establishing a new state-of-the-art with F1-scores
of 94.16%, 89.51%, and 91.27%, respectively. The code will be made available at
https://github.com/zxXie-Air/FSG-Net after a possible publication.

</details>


### [249] [WS$^2$: Weakly Supervised Segmentation using Before-After Supervision in Waste Sorting](https://arxiv.org/abs/2509.06485)
*Andrea Marelli,Alberto Foresti,Leonardo Pesce,Giacomo Boracchi,Mario Grosso*

Main category: cs.CV

TL;DR: The paper addresses automating item recognition in industrial quality control using a weakly supervised approach instead of fully supervised methods by employing Before-After Supervision. It introduces a new dataset for waste-sorting (WS$^2$) and benchmarks segmentation methods.


<details>
  <summary>Details</summary>
Motivation: Human operators are currently indispensable for tasks like waste-sorting due to the variety of sorting tasks, making automation difficult. Fully supervised methods require extensive labeling and are impractical. A less labor-intensive, weakly supervised approach could automate these visual recognition tasks effectively.

Method: The paper proposes a new concept called Before-After Supervision, where a segmentation network is trained based on visual differences between 'before' and 'after' images of operator actions. It introduces a dataset (WS$^2$) comprising high-resolution, multiview video frames and benchmarks weakly supervised segmentation methods using an end-to-end pipeline.

Result: The research successfully creates a dataset suitable for the task, tests several state-of-the-art weakly supervised segmentation methods, and demonstrates the potential of Before-After Supervision for waste-sorting automation.

Conclusion: Weakly supervised approaches leveraging implicit supervision hold promising potential for automating tasks like waste-sorting. The introduced WS$^2$ dataset and benchmarks will promote further research and innovation in this area.

Abstract: In industrial quality control, to visually recognize unwanted items within a
moving heterogeneous stream, human operators are often still indispensable.
Waste-sorting stands as a significant example, where operators on multiple
conveyor belts manually remove unwanted objects to select specific materials.
To automate this recognition problem, computer vision systems offer great
potential in accurately identifying and segmenting unwanted items in such
settings. Unfortunately, considering the multitude and the variety of sorting
tasks, fully supervised approaches are not a viable option to address this
challange, as they require extensive labeling efforts. Surprisingly, weakly
supervised alternatives that leverage the implicit supervision naturally
provided by the operator in his removal action are relatively unexplored. In
this paper, we define the concept of Before-After Supervision, illustrating how
to train a segmentation network by leveraging only the visual differences
between images acquired \textit{before} and \textit{after} the operator. To
promote research in this direction, we introduce WS$^2$ (Weakly Supervised
segmentation for Waste-Sorting), the first multiview dataset consisting of more
than 11 000 high-resolution video frames captured on top of a conveyor belt,
including "before" and "after" images. We also present a robust end-to-end
pipeline, used to benchmark several state-of-the-art weakly supervised
segmentation methods on WS$^2$.

</details>


### [250] [TIDE: Achieving Balanced Subject-Driven Image Generation via Target-Instructed Diffusion Enhancement](https://arxiv.org/abs/2509.06499)
*Jibai Lin,Bo Ma,Yating Yang,Rong Ma,Turghun Osman,Ahtamjan Ahmat,Rui Dong,Lei Wang,Xi Zhou*

Main category: cs.CV

TL;DR: Subject-driven image generation (SDIG) involves modifying specific subjects in images via textual instructions. TIDE framework was introduced to improve subject identity preservation and instruction compliance without extra fine-tuning.


<details>
  <summary>Details</summary>
Motivation: SDIG struggles with balancing subject identity preservation against adherence to dynamic textual instructions. Existing methods fall short in reconciling this trade-off.

Method: The TIDE framework employs target-supervised triplet alignment, using a triplet format (reference image, instruction, target images). It trains the model with paired winning and losing targets for optimized preservation-compliance balance via implicit reward modelling.

Result: TIDE demonstrates superior performance in maintaining subject identity while following instructions, surpassing baseline methods in multiple metrics. It proves versatile across tasks like structural-conditioned generation and text-image interpolation.

Conclusion: TIDE effectively addresses the tension in SDIG tasks, combining subject preservation and instruction compliance, while showcasing broad applicability and enhanced quantitative results.

Abstract: Subject-driven image generation (SDIG) aims to manipulate specific subjects
within images while adhering to textual instructions, a task crucial for
advancing text-to-image diffusion models. SDIG requires reconciling the tension
between maintaining subject identity and complying with dynamic edit
instructions, a challenge inadequately addressed by existing methods. In this
paper, we introduce the Target-Instructed Diffusion Enhancing (TIDE) framework,
which resolves this tension through target supervision and preference learning
without test-time fine-tuning. TIDE pioneers target-supervised triplet
alignment, modelling subject adaptation dynamics using a (reference image,
instruction, target images) triplet. This approach leverages the Direct Subject
Diffusion (DSD) objective, training the model with paired "winning" (balanced
preservation-compliance) and "losing" (distorted) targets, systematically
generated and evaluated via quantitative metrics. This enables implicit reward
modelling for optimal preservation-compliance balance. Experimental results on
standard benchmarks demonstrate TIDE's superior performance in generating
subject-faithful outputs while maintaining instruction compliance,
outperforming baseline methods across multiple quantitative metrics. TIDE's
versatility is further evidenced by its successful application to diverse
tasks, including structural-conditioned generation, image-to-image generation,
and text-image interpolation. Our code is available at
https://github.com/KomJay520/TIDE.

</details>


### [251] [Predicting Brain Tumor Response to Therapy using a Hybrid Deep Learning and Radiomics Approach](https://arxiv.org/abs/2509.06511)
*Daniil Tikhonov,Matheus Scatolin,Mohor Banerjee,Qiankun Ji,Ahmed Jaheen,Mostafa Salem,Abdelrahman Elsayed,Hu Wang,Sarim Hashmi,Mohammad Yaqub*

Main category: cs.CV

TL;DR: This study proposes an automated framework for assessing glioblastoma therapy response using a hybrid approach that combines deep learning and radiomics, achieving strong classification performance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the clinical challenge of accurately evaluating glioblastoma therapy response, which is vital for decision-making but complicated by variability in the application of current manual assessment criteria.

Method: The authors developed a hybrid framework that utilizes a fine-tuned ResNet-18 model to extract deep learning features from 2D MRI regions and combines these with over 4800 radiomic and clinical features to train a CatBoost classifier.

Result: The model achieved a mean ROC AUC of 0.81 and a Macro F1 score of 0.50 in a 4-class tumor response prediction task, demonstrating its effectiveness.

Conclusion: The combination of deep learning features and domain-specific radiomic features offers a robust approach for automated treatment response evaluation in neuro-oncology, reducing observer variability.

Abstract: Accurate evaluation of the response of glioblastoma to therapy is crucial for
clinical decision-making and patient management. The Response Assessment in
Neuro-Oncology (RANO) criteria provide a standardized framework to assess
patients' clinical response, but their application can be complex and subject
to observer variability. This paper presents an automated method for
classifying the intervention response from longitudinal MRI scans, developed to
predict tumor response during therapy as part of the BraTS 2025 challenge. We
propose a novel hybrid framework that combines deep learning derived feature
extraction and an extensive set of radiomics and clinically chosen features.
Our approach utilizes a fine-tuned ResNet-18 model to extract features from 2D
regions of interest across four MRI modalities. These deep features are then
fused with a rich set of more than 4800 radiomic and clinically driven
features, including 3D radiomics of tumor growth and shrinkage masks,
volumetric changes relative to the nadir, and tumor centroid shift. Using the
fused feature set, a CatBoost classifier achieves a mean ROC AUC of 0.81 and a
Macro F1 score of 0.50 in the 4-class response prediction task (Complete
Response, Partial Response, Stable Disease, Progressive Disease). Our results
highlight that synergizing learned image representations with domain-targeted
radiomic features provides a robust and effective solution for automated
treatment response assessment in neuro-oncology.

</details>


### [252] [On the Reproducibility of "FairCLIP: Harnessing Fairness in Vision-Language Learning''](https://arxiv.org/abs/2509.06535)
*Hua Chang Bakker,Stan Fris,Angela Madelon Bernardy,Stan Deutekom*

Main category: cs.CV

TL;DR: This study investigates the reproducibility of FairCLIP and presents its own implementations, ultimately finding that the proposed fairness and performance improvements do not hold in specific applications.


<details>
  <summary>Details</summary>
Motivation: To examine and validate the claims of FairCLIP regarding its ability to enhance group fairness and performance using Sinkhorn distance minimization across sensitive groups.

Method: The study reproduced the FairCLIP setup, introduced a new implementation called A-FairCLIP, extended objectives for multiple attributes (FairCLIP+), and evaluated its impact on fairness and performance metrics, particularly for zero-shot glaucoma classification.

Result: The experiments revealed discrepancies between FairCLIP's original description and implementation, and neither increased fairness nor improved performance was observed in zero-shot glaucoma classification for both the original and the new implementations.

Conclusion: FairCLIP, despite its theoretical goals and regularization objective, does not demonstrate practical improvements in fairness or performance in the examined use cases, challenging its original claims.

Abstract: We investigated the reproducibility of FairCLIP, proposed by Luo et al.
(2024), for improving the group fairness of CLIP (Radford et al., 2021) by
minimizing image-text similarity score disparities across sensitive groups
using the Sinkhorn distance. The experimental setup of Luo et al. (2024) was
reproduced to primarily investigate the research findings for FairCLIP. The
model description by Luo et al. (2024) was found to differ from the original
implementation. Therefore, a new implementation, A-FairCLIP, is introduced to
examine specific design choices. Furthermore, FairCLIP+ is proposed to extend
the FairCLIP objective to include multiple attributes. Additionally, the impact
of the distance minimization on FairCLIP's fairness and performance was
explored. In alignment with the original authors, CLIP was found to be biased
towards certain demographics when applied to zero-shot glaucoma classification
using medical scans and clinical notes from the Harvard-FairVLMed dataset.
However, the experimental results on two datasets do not support their claim
that FairCLIP improves the performance and fairness of CLIP. Although the
regularization objective reduces Sinkhorn distances, both the official
implementation and the aligned implementation, A-FairCLIP, were not found to
improve performance nor fairness in zero-shot glaucoma classification.

</details>


### [253] [Benchmarking EfficientTAM on FMO datasets](https://arxiv.org/abs/2509.06536)
*Senem Aktas,Charles Markham,John McDonald,Rozenn Dahyot*

Main category: cs.CV

TL;DR: The paper introduces a new JSON metadata file (FMOX) for Fast Moving Objects (FMOs) datasets, enhances ground truth data, and evaluates a tracking model, EfficientTAM, using the data.


<details>
  <summary>Details</summary>
Motivation: Accurately tracking fast-moving and small objects remains a complex challenge in computer vision, necessitating enhanced datasets and methodologies.

Method: The paper presents FMOX, a JSON-format ground truth extension for FMO datasets, and tests EfficientTAM's tracking performance using Trajectory Intersection of Union (TIoU) metrics.

Result: EfficientTAM achieves performance on par with specialized FMO datasets' pipelines. The FMOX files and corresponding code are made open-source for broader usability.

Conclusion: The proposed FMOX resource, combined with the performance of EfficientTAM, holds significance for advancing FMO tracking and facilitating its application in machine learning pipelines.

Abstract: Fast and tiny object tracking remains a challenge in computer vision and in
this paper we first introduce a JSON metadata file associated with four open
source datasets of Fast Moving Objects (FMOs) image sequences. In addition, we
extend the description of the FMOs datasets with additional ground truth
information in JSON format (called FMOX) with object size information. Finally
we use our FMOX file to test a recently proposed foundational model for
tracking (called EfficientTAM) showing that its performance compares well with
the pipelines originally taylored for these FMO datasets. Our comparison of
these state-of-the-art techniques on FMOX is provided with Trajectory
Intersection of Union (TIoU) scores. The code and JSON is shared open source
allowing FMOX to be accessible and usable for other machine learning pipelines
aiming to process FMO datasets.

</details>


### [254] [Back To The Drawing Board: Rethinking Scene-Level Sketch-Based Image Retrieval](https://arxiv.org/abs/2509.06566)
*Emil Demić,Luka Čehovin Zajc*

Main category: cs.CV

TL;DR: This paper addresses Scene-level Sketch-Based Image Retrieval, emphasizing training objectives that handle sketch ambiguity and noise for improved performance.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to tackle the challenges of ambiguity and noise in real-world sketches while retrieving matching natural images based on semantic and spatial layout.

Method: It leverages pre-training, encoder architecture, and a specifically designed loss formulation to achieve robust and efficient retrieval without adding architectural complexity.

Result: Experiments on FS-COCO and SketchyCOCO datasets demonstrate state-of-the-art results, showcasing the significance of the proposed training design.

Conclusion: The findings highlight the importance of training design for cross-modal retrieval and suggest improved evaluation scenarios for SBIR tasks.

Abstract: The goal of Scene-level Sketch-Based Image Retrieval is to retrieve natural
images matching the overall semantics and spatial layout of a free-hand sketch.
Unlike prior work focused on architectural augmentations of retrieval models,
we emphasize the inherent ambiguity and noise present in real-world sketches.
This insight motivates a training objective that is explicitly designed to be
robust to sketch variability. We show that with an appropriate combination of
pre-training, encoder architecture, and loss formulation, it is possible to
achieve state-of-the-art performance without the introduction of additional
complexity. Extensive experiments on a challenging FS-COCO and widely-used
SketchyCOCO datasets confirm the effectiveness of our approach and underline
the critical role of training design in cross-modal retrieval tasks, as well as
the need to improve the evaluation scenarios of scene-level SBIR.

</details>


### [255] [Evolving from Unknown to Known: Retentive Angular Representation Learning for Incremental Open Set Recognition](https://arxiv.org/abs/2509.06570)
*Runqing Yang,Yimin Fu,Changyuan Wu,Zhunga Liu*

Main category: cs.CV

TL;DR: This paper introduces RARL for incremental open set recognition, addressing inter-class confusion in dynamic data streams and achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Open set recognition models typically struggle to handle dynamic scenarios where new unknown classes emerge continuously.

Method: RARL aligns unknown representations around prototypes in angular space and employs strategies like VII training and stratified rectification to improve decision boundaries.

Result: Experiments on CIFAR100 and TinyImageNet datasets confirm the superior performance of RARL under various task setups.

Conclusion: RARL mitigates representation drift and improves incremental open set recognition, establishing a new benchmark in the field.

Abstract: Existing open set recognition (OSR) methods are typically designed for static
scenarios, where models aim to classify known classes and identify unknown ones
within fixed scopes. This deviates from the expectation that the model should
incrementally identify newly emerging unknown classes from continuous data
streams and acquire corresponding knowledge. In such evolving scenarios, the
discriminability of OSR decision boundaries is hard to maintain due to
restricted access to former training data, causing severe inter-class
confusion. To solve this problem, we propose retentive angular representation
learning (RARL) for incremental open set recognition (IOSR). In RARL, unknown
representations are encouraged to align around inactive prototypes within an
angular space constructed under the equiangular tight frame, thereby mitigating
excessive representation drift during knowledge updates. Specifically, we adopt
a virtual-intrinsic interactive (VII) training strategy, which compacts known
representations by enforcing clear inter-class margins through
boundary-proximal virtual classes. Furthermore, a stratified rectification
strategy is designed to refine decision boundaries, mitigating representation
bias and feature space distortion caused by imbalances between old/new and
positive/negative class samples. We conduct thorough evaluations on CIFAR100
and TinyImageNet datasets and establish a new benchmark for IOSR. Experimental
results across various task setups demonstrate that the proposed method
achieves state-of-the-art performance.

</details>


### [256] [CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesis](https://arxiv.org/abs/2509.06579)
*Xin Kong,Daniel Watson,Yannick Strümpler,Michael Niemeyer,Federico Tombari*

Main category: cs.CV

TL;DR: CausNVS is an autoregressive multi-view diffusion model for flexible and high-quality 3D novel view synthesis.


<details>
  <summary>Details</summary>
Motivation: Existing multi-view diffusion models for 3D novel view synthesis are non-autoregressive, limiting their applicability to fixed view configurations and causing slow inference due to simultaneous frame denoising.

Method: CausNVS adopts an autoregressive approach supported by causal masking, per-frame noise injection, and pairwise-relative camera pose encodings (CaPE) for precise control. It also incorporates spatially-aware sliding-windows, key-value caching, and noise conditioning for drift mitigation during inference.

Result: The method supports arbitrary camera trajectories and input-output view configurations, achieving strong and consistent visual quality across various settings.

Conclusion: CausNVS offers a flexible and high-performance solution for autoregressive novel view synthesis, overcoming the limitations of prior non-autoregressive approaches.

Abstract: Multi-view diffusion models have shown promise in 3D novel view synthesis,
but most existing methods adopt a non-autoregressive formulation. This limits
their applicability in world modeling, as they only support a fixed number of
views and suffer from slow inference due to denoising all frames
simultaneously. To address these limitations, we propose CausNVS, a multi-view
diffusion model in an autoregressive setting, which supports arbitrary
input-output view configurations and generates views sequentially. We train
CausNVS with causal masking and per-frame noise, using pairwise-relative camera
pose encodings (CaPE) for precise camera control. At inference time, we combine
a spatially-aware sliding-window with key-value caching and noise conditioning
augmentation to mitigate drift. Our experiments demonstrate that CausNVS
supports a broad range of camera trajectories, enables flexible autoregressive
novel view synthesis, and achieves consistently strong visual quality across
diverse settings. Project page: https://kxhit.github.io/CausNVS.html.

</details>


### [257] [Detection of trade in products derived from threatened species using machine learning and a smartphone](https://arxiv.org/abs/2509.06585)
*Ritwik Kulkarni,WU Hanqin,Enrico Di Minin*

Main category: cs.CV

TL;DR: The paper presents machine learning models that identify illegal wildlife products in images with high accuracy and a smartphone app for real-time use.


<details>
  <summary>Details</summary>
Motivation: The need for automated methods to detect illegal wildlife trade in digital marketplaces and physical markets due to its threat to biodiversity.

Method: Developed machine learning-based models to identify wildlife products (e.g., ivory, skins, and bones from elephants, pangolins, and tigers). Tested various training strategies and loss functions for accuracy.

Result: The best model achieved 84.2% overall accuracy and specific accuracies of 71.1% (elephants), 90.2% (pangolins), and 93.5% (tigers). The smartphone app had 91.3% accuracy.

Conclusion: The models and smartphone app offer a scalable solution for authorities and law enforcement to monitor and curb wildlife trade in digital and physical markets.

Abstract: Unsustainable trade in wildlife is a major threat to biodiversity and is now
increasingly prevalent in digital marketplaces and social media. With the sheer
volume of digital content, the need for automated methods to detect wildlife
trade listings is growing. These methods are especially needed for the
automatic identification of wildlife products, such as ivory. We developed
machine learning-based object recognition models that can identify wildlife
products within images and highlight them. The data consists of images of
elephant, pangolin, and tiger products that were identified as being sold
illegally or that were confiscated by authorities. Specifically, the wildlife
products included elephant ivory and skins, pangolin scales, and claws (raw and
crafted), and tiger skins and bones. We investigated various combinations of
training strategies and two loss functions to identify the best model to use in
the automatic detection of these wildlife products. Models were trained for
each species while also developing a single model to identify products from all
three species. The best model showed an overall accuracy of 84.2% with
accuracies of 71.1%, 90.2% and 93.5% in detecting products derived from
elephants, pangolins, and tigers, respectively. We further demonstrate that the
machine learning model can be made easily available to stakeholders, such as
government authorities and law enforcement agencies, by developing a
smartphone-based application that had an overall accuracy of 91.3%. The
application can be used in real time to click images and help identify
potentially prohibited products of target species. Thus, the proposed method is
not only applicable for monitoring trade on the web but can also be used e.g.
in physical markets for monitoring wildlife trade.

</details>


### [258] [Hybrid Swin Attention Networks for Simultaneously Low-Dose PET and CT Denoising](https://arxiv.org/abs/2509.06591)
*Yichao Liu,YueYang Teng*

Main category: cs.CV

TL;DR: This paper introduces HSANet for denoising in LDCT/PET imaging, addressing noise issues while ensuring radiation safety.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve LDCT/PET imaging quality, which is compromised due to noise and artifacts resulting from reduced radiation exposure.

Method: HSANet is proposed, utilizing Efficient Global Attention (EGA) modules for improved feature interaction and a hybrid upsampling module to prevent overfitting.

Result: HSANet demonstrated superior denoising performance on a publicly available dataset compared to existing methods while maintaining a lightweight model size.

Conclusion: HSANet enhances image quality, remains computationally efficient, and is practical for clinical applications.

Abstract: Low-dose computed tomography (LDCT) and positron emission tomography (PET)
have emerged as safer alternatives to conventional imaging modalities by
significantly reducing radiation exposure. However, this reduction often
results in increased noise and artifacts, which can compromise diagnostic
accuracy. Consequently, denoising for LDCT/PET has become a vital area of
research aimed at enhancing image quality while maintaining radiation safety.
In this study, we introduce a novel Hybrid Swin Attention Network (HSANet),
which incorporates Efficient Global Attention (EGA) modules and a hybrid
upsampling module. The EGA modules enhance both spatial and channel-wise
interaction, improving the network's capacity to capture relevant features,
while the hybrid upsampling module mitigates the risk of overfitting to noise.
We validate the proposed approach using a publicly available LDCT/PET dataset.
Experimental results demonstrate that HSANet achieves superior denoising
performance compared to existing methods, while maintaining a lightweight model
size suitable for deployment on GPUs with standard memory configurations. This
makes our approach highly practical for real-world clinical applications.

</details>


### [259] [Improved Classification of Nitrogen Stress Severity in Plants Under Combined Stress Conditions Using Spatio-Temporal Deep Learning Framework](https://arxiv.org/abs/2509.06625)
*Aswini Kumar Patra*

Main category: cs.CV

TL;DR: The paper presents a deep learning approach using CNN-LSTM to classify nitrogen stress severity in plants under combined stress environments with 98% accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of early detection of nitrogen stress severity in plants experiencing combined stresses such as drought and weed competition.

Method: A spatio-temporal deep learning framework combining CNN for spatial feature extraction and LSTM for temporal dependency modeling using four imaging modalities and time-series canopy images.

Result: The CNN-LSTM pipeline achieved 98% accuracy, outperforming a spatial-only CNN (80.45%) and prior machine learning methods (76%).

Conclusion: The proposed CNN-LSTM model is highly effective for timely identification of nitrogen stress severity under complex stress conditions, offering actionable insights for crop management.

Abstract: Plants in their natural habitats endure an array of interacting stresses,
both biotic and abiotic, that rarely occur in isolation. Nutrient
stress-particularly nitrogen deficiency-becomes even more critical when
compounded with drought and weed competition, making it increasingly difficult
to distinguish and address its effects. Early detection of nitrogen stress is
therefore crucial for protecting plant health and implementing effective
management strategies. This study proposes a novel deep learning framework to
accurately classify nitrogen stress severity in a combined stress environment.
Our model uses a unique blend of four imaging modalities-RGB, multispectral,
and two infrared wavelengths-to capture a wide range of physiological plant
responses from canopy images. These images, provided as time-series data,
document plant health across three levels of nitrogen availability (low,
medium, and high) under varying water stress and weed pressures. The core of
our approach is a spatio-temporal deep learning pipeline that merges a
Convolutional Neural Network (CNN) for extracting spatial features from images
with a Long Short-Term Memory (LSTM) network to capture temporal dependencies.
We also devised and evaluated a spatial-only CNN pipeline for comparison. Our
CNN-LSTM pipeline achieved an impressive accuracy of 98%, impressively
surpassing the spatial-only model's 80.45% and other previously reported
machine learning method's 76%. These results bring actionable insights based on
the power of our CNN-LSTM approach in effectively capturing the subtle and
complex interactions between nitrogen deficiency, water stress, and weed
pressure. This robust platform offers a promising tool for the timely and
proactive identification of nitrogen stress severity, enabling better crop
management and improved plant health.

</details>


### [260] [VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level Guidance in Large Scenes](https://arxiv.org/abs/2509.06685)
*Shengkai Zhang,Yuhe Liu,Guanjun Wu,Jianhua He,Xinggang Wang,Mozi Chen,Kezhong Liu*

Main category: cs.CV

TL;DR: VIM-GS uses monocular images combined with sparse depth from SfM and dense depth from LFMs to achieve high-quality novel-view synthesis for large scenes.


<details>
  <summary>Details</summary>
Motivation: Existing Gaussian Splatting methods depend on accurate depth data, which is difficult to achieve in large scenes using traditional methods like stereo or RGB-D cameras. Monocular images offer potential but suffer from depth-related limitations.

Method: Leverages sparse depth data from SfM and combines it with dense but coarse depth from LFMs using an object-segmented depth propagation algorithm and a dynamic depth refinement module.

Result: Demonstrated superior novel-view rendering quality in large-scale scenes compared to existing approaches, through public and custom datasets.

Conclusion: The proposed framework, VIM-GS, successfully addresses depth inaccuracy and cross-frame inconsistency issues in monocular image-based novel view synthesis for large scenes.

Abstract: VIM-GS is a Gaussian Splatting (GS) framework using monocular images for
novel-view synthesis (NVS) in large scenes. GS typically requires accurate
depth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limited
depth sensing range makes it difficult for GS to work in large scenes.
Monocular images, however, lack depth to guide the learning and lead to
inferior NVS results. Although large foundation models (LFMs) for monocular
depth estimation are available, they suffer from cross-frame inconsistency,
inaccuracy for distant scenes, and ambiguity in deceptive texture cues. This
paper aims to generate dense, accurate depth images from monocular RGB inputs
for high-definite GS rendering. The key idea is to leverage the accurate but
sparse depth from visual-inertial Structure-from-Motion (SfM) to refine the
dense but coarse depth from LFMs. To bridge the sparse input and dense output,
we propose an object-segmented depth propagation algorithm that renders the
depth of pixels of structured objects. Then we develop a dynamic depth
refinement module to handle the crippled SfM depth of dynamic objects and
refine the coarse LFM depth. Experiments using public and customized datasets
demonstrate the superior rendering quality of VIM-GS in large scenes.

</details>


### [261] [STAGE: Segmentation-oriented Industrial Anomaly Synthesis via Graded Diffusion with Explicit Mask Alignment](https://arxiv.org/abs/2509.06693)
*Xichen Xu,Yanshu Wang,Jinbao Wang,Qunyi Zhang,Xiaoning Lei,Guoyang Xie,Guannan Jiang,Zhichao Lu*

Main category: cs.CV

TL;DR: The paper introduces STAGE, a novel approach to industrial anomaly synthesis, addressing limitations in existing techniques by generating detailed and pixel-level accurate anomalies through graded diffusion and explicit mask alignment strategies.


<details>
  <summary>Details</summary>
Motivation: Existing methods in Segmentation-oriented Industrial Anomaly Synthesis struggle with generating detailed and contextually accurate anomalies, limiting performance in downstream anomaly segmentation.

Method: STAGE utilizes a graded diffusion framework, anomaly inference strategy with clean background priors, and explicit mask alignment to create refined and context-consistent industrial anomaly data.

Result: Experiments on MVTec and BTAD datasets confirm STAGE's superior performance in anomaly synthesis and its enhancement of downstream segmentation tasks.

Conclusion: The proposed STAGE method improves industrial anomaly synthesis by effectively addressing key drawbacks of current approaches, leading to enhanced segmentation results.

Abstract: Segmentation-oriented Industrial Anomaly Synthesis (SIAS) plays a pivotal
role in enhancing the performance of downstream anomaly segmentation, as it
provides an effective means of expanding abnormal data. However, existing SIAS
methods face several critical limitations: (i) the synthesized anomalies often
lack intricate texture details and fail to align precisely with the surrounding
background, and (ii) they struggle to generate fine-grained, pixel-level
anomalies. To address these challenges, we propose Segmentation-oriented
Anomaly synthesis via Graded diffusion with Explicit mask alignment, termed
STAGE. STAGE introduces a novel anomaly inference strategy that incorporates
clean background information as a prior to guide the denoising distribution,
enabling the model to more effectively distinguish and highlight abnormal
foregrounds. Furthermore, it employs a graded diffusion framework with an
anomaly-only branch to explicitly record local anomalies during both the
forward and reverse processes, ensuring that subtle anomalies are not
overlooked. Finally, STAGE incorporates the explicit mask alignment (EMA)
strategy to progressively align the synthesized anomalies with the background,
resulting in context-consistent and structurally coherent generations.
Extensive experiments on the MVTec and BTAD datasets demonstrate that STAGE
achieves state-of-the-art performance in SIAS, which in turn enhances
downstream anomaly segmentation.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [262] [Multi-IaC-Eval: Benchmarking Cloud Infrastructure as Code Across Multiple Formats](https://arxiv.org/abs/2509.05303)
*Sam Davidson,Li Sun,Bhavana Bhasker,Laurent Callot,Anoop Deoras*

Main category: cs.DC

TL;DR: The paper introduces Multi-IaC-Bench, a benchmark for evaluating large language models (LLMs) in generating and modifying Infrastructure as Code (IaC) across various formats like Terraform and AWS CloudFormation.


<details>
  <summary>Details</summary>
Motivation: To address the problem of lacking standardized benchmarks for multi-format IaC generation and modification, which limits progress in automating IaC management using LLMs.

Method: The authors created a benchmark dataset with triplets consisting of IaC templates, natural language modification requests, and respective updated templates. They validated this dataset rigorously and evaluated state-of-the-art LLMs using it.

Result: State-of-the-art LLMs achieved over 95% success rates in generating syntactically correct IaC but struggled with semantic alignment and complex infrastructure patterns.

Conclusion: The release of Multi-IaC-Bench establishes a standard for evaluating LLM-based IaC generation and highlights areas like better semantic understanding for further research.

Abstract: Infrastructure as Code (IaC) is fundamental to modern cloud computing,
enabling teams to define and manage infrastructure through machine-readable
configuration files. However, different cloud service providers utilize diverse
IaC formats. The lack of a standardized format requires cloud architects to be
proficient in multiple IaC languages, adding complexity to cloud deployment.
While Large Language Models (LLMs) show promise in automating IaC creation and
maintenance, progress has been limited by the lack of comprehensive benchmarks
across multiple IaC formats. We present Multi-IaC-Bench, a novel benchmark
dataset for evaluating LLM-based IaC generation and mutation across AWS
CloudFormation, Terraform, and Cloud Development Kit (CDK) formats. The dataset
consists of triplets containing initial IaC templates, natural language
modification requests, and corresponding updated templates, created through a
synthetic data generation pipeline with rigorous validation. We evaluate
several state-of-the-art LLMs on Multi-IaC-Bench, demonstrating that while
modern LLMs can achieve high success rates (>95%) in generating syntactically
valid IaC across formats, significant challenges remain in semantic alignment
and handling complex infrastructure patterns. Our ablation studies highlight
the importance of prompt engineering and retry mechanisms in successful IaC
generation. We release Multi-IaC-Bench to facilitate further research in
AI-assisted infrastructure management and establish standardized evaluation
metrics for this crucial domain.

</details>


### [263] [A Simple and Robust Protocol for Distributed Counting](https://arxiv.org/abs/2509.05870)
*Edith Cohen,Moshe Shechner,Uri Stemmer*

Main category: cs.DC

TL;DR: This paper explores the distributed counting problem and addresses whether existing simple protocols are robust when faced with adaptive attacks. It introduces a new simple protocol that achieves optimal communication complexity for adaptive settings.


<details>
  <summary>Details</summary>
Motivation: The authors aim to determine if the simple protocol for distributed counting proposed by Huang et al. (2012) is robust under adaptive attacks, as previous protocols were either suboptimal or overly complex.

Method: The paper employs two approaches: constructing an explicit adaptive attack to demonstrate the non-robustness of Huang et al. (2012)'s protocol and designing a new, simple, robust protocol with optimal communication complexity.

Result: The authors prove that Huang et al. (2012)'s protocol is not robust by presenting an adaptive attack and propose a new protocol that not only simplifies distributed counting but also achieves optimal communication complexity in adaptive settings.

Conclusion: This paper resolves a critical question in distributed counting by presenting an effective robust protocol that is simpler and matches the optimal communication complexity, filling gaps left by prior research.

Abstract: We revisit the distributed counting problem, where a server must continuously
approximate the total number of events occurring across $k$ sites while
minimizing communication. The communication complexity of this problem is known
to be $\Theta(\frac{k}{\epsilon}\log N)$ for deterministic protocols. Huang,
Yi, and Zhang (2012) showed that randomization can reduce this to
$\Theta(\frac{\sqrt{k}}{\epsilon}\log N)$, but their analysis is restricted to
the {\em oblivious setting}, where the stream of events is independent of the
protocol's outputs.
  Xiong, Zhu, and Huang (2023) presented a robust protocol for distributed
counting that removes the oblivious assumption. However, their communication
complexity is suboptimal by a $polylog(k)$ factor and their protocol is
substantially more complex than the oblivious protocol of Huang et al. (2012).
This left open a natural question: could it be that the simple protocol of
Huang et al. (2012) is already robust?
  We resolve this question with two main contributions. First, we show that the
protocol of Huang et al. (2012) is itself not robust by constructing an
explicit adaptive attack that forces it to lose its accuracy. Second, we
present a new, surprisingly simple, robust protocol for distributed counting
that achieves the optimal communication complexity of
$O(\frac{\sqrt{k}}{\epsilon} \log N)$. Our protocol is simpler than that of
Xiong et al. (2023), perhaps even simpler than that of Huang et al. (2012), and
is the first to match the optimal oblivious complexity in the adaptive setting.

</details>


### [264] [DISTRIBUTEDANN: Efficient Scaling of a Single DISKANN Graph Across Thousands of Computers](https://arxiv.org/abs/2509.06046)
*Philip Adams,Menghao Li,Shi Zhang,Li Tan,Qi Chen,Mingqin Li,Zengzhong Li,Knut Risvik,Harsha Vardhan Simhadri*

Main category: cs.DC

TL;DR: DISTRIBUTEDANN is a scalable vector search system capable of querying 50 billion vectors across 1,000+ machines, achieving superior efficiency for Bing search engine.


<details>
  <summary>Details</summary>
Motivation: To develop a scalable and efficient vector search service that improves query efficiency over massive datasets, addressing limitations in existing partitioning and routing strategies.

Method: DISTRIBUTEDANN integrates a distributed key-value store with in-memory approximate nearest neighbor (ANN) indexing to create a highly efficient search mechanism.

Result: The system achieves a 6x efficiency improvement over existing methods, processing 100,000+ queries per second with a 26ms median query latency while managing a graph index of 50 billion vectors.

Conclusion: DISTRIBUTEDANN significantly outperforms conventional architectures and has been effectively adopted for powering search in the Bing search engine.

Abstract: We present DISTRIBUTEDANN, a distributed vector search service that makes it
possible to search over a single 50 billion vector graph index spread across
over a thousand machines that offers 26ms median query latency and processes
over 100,000 queries per second. This is 6x more efficient than existing
partitioning and routing strategies that route the vector query to a subset of
partitions in a scale out vector search system. DISTRIBUTEDANN is built using
two well-understood components: a distributed key-value store and an in-memory
ANN index. DISTRIBUTEDANN has replaced conventional scale-out architectures for
serving the Bing search engine, and we share our experience from making this
transition.

</details>


### [265] [Gathering in Non-Vertex-Transitive Graphs Under Round Robin](https://arxiv.org/abs/2509.06064)
*Serafino Cicerone,Alessia Di Fonso,Gabriele Di Stefano,Alfredo Navarra*

Main category: cs.DC

TL;DR: The paper addresses the gathering problem for robots under certain constraints and provides a resolution algorithm for diverse graph structures.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the challenging gathering problem for robots in hostile configurations on non-vertex-transitive graphs, with constraints like multiplicities and limited detection capabilities.

Method: The study uses the OBLOT robot model where robots are constrained to graph edges and assumes a round-robin activation scheduler. It develops and analyzes a resolution algorithm applicable to the broad context of non-vertex-transitive graphs.

Result: The authors deliver a resolution algorithm that tackles initial configurations with multiplicities and ensures correctness. They also analyze its time complexity.

Conclusion: The algorithm succeeds in solving the gathering problem for robots on diverse, non-vertex-transitive graphs, addressing constraints that make the problem more difficult.

Abstract: The Gathering problem for a swarm of robots asks for a distributed algorithm
that brings such entities to a common place, not known in advance. We consider
the well-known OBLOT model with robots constrained to move along the edges of a
graph, hence gathering in one vertex, eventually. Despite the classical setting
under which the problem has been usually approached, we consider the `hostile'
case where: i) the initial configuration may contain multiplicities, i.e. more
than one robot may occupy the same vertex; ii) robots cannot detect
multiplicities. As a scheduler for robot activation, we consider the
"favorable" round-robin case, where robots are activated one at a time.
  Our objective is to achieve a complete characterization of the problem in the
broad context of non-vertex-transitive graphs, i.e., graphs where the vertices
are partitioned into at least two different classes of equivalence. We provide
a resolution algorithm for any configuration of robots moving on such graphs,
along with its correctness. Furthermore, we analyze its time complexity.

</details>


### [266] [20 Years in Life of a Smart Building: A retrospective](https://arxiv.org/abs/2509.06229)
*Karolina Skrivankova,Mark Handley,Stephen Hailes*

Main category: cs.DC

TL;DR: The paper introduces KaOS, a distributed control platform designed to enhance robustness and adaptability in smart building automation systems using affordable IoT hardware.


<details>
  <summary>Details</summary>
Motivation: To address the challenges facing intelligent smart building automation systems, such as hardware failures, security threats, and vendor obsolescence, which hinder the scalability and sustainability of automation systems.

Method: KaOS uses affordable IoT hardware, containerization, and managed resource access to support control applications and distributed operations, focusing on flexibility, security, and fault tolerance.

Result: Initial evaluation demonstrates that KaOS is practically feasible, showcasing its ability to maintain and evolve building control functionalities sustainably over time.

Conclusion: KaOS provides a cost-effective approach to developing robust, secure, and adaptable smart building automation systems, with potential for long-term scalability and sustainability.

Abstract: Operating an intelligent smart building automation system in 2025 is met with
many challenges: hardware failures, vendor obsolescence, evolving security
threats and more. None of these have been comprehensibly addressed by the
industrial building nor home automation industries, limiting feasibility of
operating large, truly smart automation deployments. This paper introduces
KaOS, a distributed control platform for constructing robust and evolvable
smart building automation systems using affordable, off-the-shelf IoT hardware.
Supporting control applications and distributed system operations by leveraging
containerisation and managed resource access, KaOS seeks to achieve
flexibility, security, and fault tolerance without sacrificing
cost-effectiveness. Initial evaluation confirms the practical feasibility of
our approach, highlighting its potential to sustainably maintain and
incrementally evolve building control functionalities over extended timeframes.

</details>


### [267] [FineServe: Precision-Aware KV Slab and Two-Level Scheduling for Heterogeneous Precision LLM Serving](https://arxiv.org/abs/2509.06261)
*Kyungmin Bin,Seungbeom Choi,Jimyoung Son,Jieun Choi,Daseul Bae,Daehyeon Baek,Kihyo Moon,Minsung Jang,Hyojung Lee*

Main category: cs.DC

TL;DR: FineServe is a mixed-precision inference serving framework designed for quantized large language models, addressing challenges in memory fragmentation and resource scheduling.


<details>
  <summary>Details</summary>
Motivation: To improve memory efficiency and GPU resource utilization for quantized LLMs, while addressing inefficiencies caused by smaller KV block sizes and diverse resource usage patterns.

Method: FineServe introduces two techniques: KV Slab, a memory management system tailored for precision-aware model characteristics, and a two-level scheduling framework with global and local schedulers for efficient GPU resource allocation.

Result: FineServe outperformed existing GPU sharing systems by achieving up to 2.2x higher SLO attainment and 1.8x greater token generation throughput during experiments.

Conclusion: FineServe demonstrates a significant improvement in serving quantized LLMs, optimizing memory usage and scheduling to enhance throughput and accuracy for mixed-precision models.

Abstract: Recent advances in Post-Training Quantization (PTQ) techniques have
significantly increased demand for serving quantized large language models
(LLMs), enabling higher throughput and substantially reduced memory usage with
minimal accuracy loss. Quantized models address memory constraints in LLMs and
enhance GPU resource utilization through efficient GPU sharing. However,
quantized models have smaller KV block sizes than non-quantized models, causing
limited memory efficiency due to memory fragmentation. Also, distinct resource
usage patterns between quantized and non-quantized models require efficient
scheduling to maximize throughput. To address these challenges, we propose
FineServe, an inference serving framework for mixed-precision LLMs. FineServe's
key contributions include: (1) KV Slab, a precision-aware adaptive memory
management technique dynamically allocating KV cache based on model
quantization characteristics, significantly reducing GPU memory fragmentation,
and (2) a two-level scheduling framework comprising a global scheduler that
places models to GPUs based on request rates, latency SLOs, and memory
constraints and efficiency, and a local scheduler that adaptively adjusts batch
sizes according to real-time request fluctuations. Experimental results
demonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x
higher token generation throughput compared to the state-of-the-art GPU sharing
systems.

</details>


### [268] [MaaSO: SLO-aware Orchestration of Heterogeneous Model Instances for MaaS](https://arxiv.org/abs/2509.06362)
*Mo Xuan,Zhang yue,Wu Weigang*

Main category: cs.DC

TL;DR: MaaSO is introduced as an orchestrator to optimize the Service Level Objectives (SLOs) for Model-as-a-Service platforms by employing heterogeneous configurations and dynamic request distributions.


<details>
  <summary>Details</summary>
Motivation: Current MaaS platforms fail to address diverse SLO requirements adequately, relying on uniform configurations for LLM instances despite varied performance characteristics across strategies.

Method: MaaSO incorporates a profiler for performance characterization, a placer for optimization of configurations, and a distributor for effective request handling and timeout mitigation.

Result: Experiments reveal that MaaSO boosts SLO satisfaction ratios by 15-30%, reduces latency by 40-60%, and lowers orchestration overhead compared to conventional methods.

Conclusion: MaaSO demonstrates significant advancements in optimizing SLOs for MaaS platforms with reduced latency and overhead, showcasing its utility in heterogeneous instance management.

Abstract: Model-as-a-Service (MaaS) platforms face diverse Service Level Objective
(SLO) requirements stemming from various large language model (LLM)
applications, manifested in contextual complexity, first-token latency, and
between-token latency. On the other hand, an LLM instance, when configured with
different parallelism strategies and inference batch sizes, exhibits distinct
performance characteristics and can thus be used to serve different SLO
requirements. However, current LLM inference systems typically deploy instances
of the same model with identical configurations, lacking mechanisms to leverage
such heterogeneity. To fill this research gap, we propose MaaSO, the first MaaS
Orchestrator, which comprises three modules: (1) a profiler characterizing
instance performance under diverse parallelism strategies and inference batch
sizes; (2) a placer optimizing heterogeneous instance configurations; (3) a
distributor enabling SLO-aware request distribution and preventing cascaded
timeouts in continuous batching. Experiments show that MaaSO improves the SLO
satisfaction ratio by 15 to 30% and reduces response latency by 40 to 60%
compared to existing approaches, and significantly lowers overall orchestration
overhead.

</details>


### [269] [IM-PIR: In-Memory Private Information Retrieval](https://arxiv.org/abs/2509.06514)
*Mpoki Mwaisela,Peterson Yuhala,Pascal Felber,Valerio Schiavoni*

Main category: cs.DC

TL;DR: This paper introduces the IM-PIR architecture, a PIM-based approach to multi-server PIR, achieving a 3.7x query throughput improvement over conventional CPU-based PIR.


<details>
  <summary>Details</summary>
Motivation: Current PIR systems suffer from high computational costs and memory bandwidth limitations, which restrict their efficiency in managing large databases.

Method: The authors leverage UPMEM, a commercial PIM architecture, to design and implement IM-PIR by exploiting PIM's high parallelism and memory bandwidth.

Result: IM-PIR demonstrated query throughput improvements of over 3.7 times compared to traditional CPU-based PIR implementations.

Conclusion: The integration of PIM into PIR systems significantly enhances performance and showcases the potential of PIM for cryptographic applications.

Abstract: Private information retrieval (PIR) is a cryptographic primitive that allows
a client to securely query one or multiple servers without revealing their
specific interests. In spite of their strong security guarantees, current PIR
constructions are computationally costly. Specifically, most PIR
implementations are memory-bound due to the need to scan extensive databases
(in the order of GB), making them inherently constrained by the limited memory
bandwidth in traditional processor-centric computing
architectures.Processing-in-memory (PIM) is an emerging computing paradigm that
augments memory with compute capabilities, addressing the memory bandwidth
bottleneck while simultaneously providing extensive parallelism.Recent research
has demonstrated PIM's potential to significantly improve performance across a
range of data-intensive workloads, including graph processing, genome analysis,
and machine learning.
  In this work, we propose the first PIM-based architecture for multi-server
PIR. We discuss the algorithmic foundations of the latter and show how its
operations align with the core strengths of PIM architectures: extensive
parallelism and high memory bandwidth. Based on this observation, we design and
implement IM-PIR, a PIM-based multi-server PIR approach on top of UPMEM PIM,
the first openly commercialized PIM architecture. Our evaluation demonstrates
that a PIM-based multi-server PIR implementation significantly improves query
throughput by more than 3.7x when compared to a standard CPU-based PIR
approach.

</details>


### [270] [Mangrove: Fast and Parallelizable State Replication for Blockchains](https://arxiv.org/abs/2509.06616)
*Anton Paramonov,Yann Vonlanthen,Quentin Kniep,Jakub Sliwinski,Roger Wattenhofer*

Main category: cs.DC

TL;DR: Mangrove introduces a new way of scaling blockchains using parallel smart contract support by deploying separate consensus instances per contract without a global transaction order.


<details>
  <summary>Details</summary>
Motivation: Scaling blockchains to support parallel execution of smart contracts efficiently while addressing challenges of performance under optimistic conditions such as network synchrony.

Method: Mangrove employs a mechanism called Parallel Optimistic Agreement for parallel execution and uses Byzantine Reliable Broadcast for simpler transactions to enhance efficiency and reduce latency.

Result: Achieves a latency of two communication steps between transaction creation and execution in optimal conditions.

Conclusion: Mangrove enables blockchain scalability by optimizing for performance under favorable conditions, without relying on a strict total order of transactions across contracts.

Abstract: Mangrove is a novel scaling approach to building blockchains with parallel
smart contract support. Unlike in monolithic blockchains, where a single
consensus mechanism determines a strict total order over all transactions,
Mangrove uses separate consensus instances per smart contract, without a global
order. To allow multiple instances to run in parallel while ensuring that no
conflicting transactions are committed, we propose a mechanism called Parallel
Optimistic Agreement. Additionally, for simple transactions, we leverage a
lightweight Byzantine Reliable Broadcast primitive to reduce latency. Mangrove
is optimized for performance under optimistic conditions, where there is no
misbehavior and the network is synchronous. Under these conditions, our
protocol can achieve a latency of 2 communication steps between creating and
executing a transaction.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [271] [Standard vs. Modular Sampling: Best Practices for Reliable LLM Unlearning](https://arxiv.org/abs/2509.05316)
*Praveen Bushipaka,Lucia Passaro,Tommaso Cucinotta*

Main category: cs.LG

TL;DR: The paper analyzes practices in LLM unlearning, identifies shortcomings in current methods, and introduces a Modular Entity-Level Unlearning (MELU) strategy for improved efficacy.


<details>
  <summary>Details</summary>
Motivation: To address shortcomings in existing LLM unlearning methods which rely on overly simplistic setups, such as single neighbor sets and conventional sampling strategies, that fail to capture real-world data complexities.

Method: The study evaluates current LLM unlearning practices and introduces the Modular Entity-Level Unlearning (MELU) strategy, which involves incorporating diverse neighbor sets and moving away from 1:1 and cyclic sampling.

Result: The analysis revealed that traditional methods are suboptimal, and MELU outperforms these by ensuring balanced forget efficacy and model utility through a modular approach.

Conclusion: The proposed MELU strategy and diversified neighbor sets offer a stable and effective framework for LLM unlearning, addressing limitations in existing practices.

Abstract: A conventional LLM Unlearning setting consists of two subsets -"forget" and
"retain", with the objectives of removing the undesired knowledge from the
forget set while preserving the remaining knowledge from the retain. In
privacy-focused unlearning research, a retain set is often further divided into
neighbor sets, containing either directly or indirectly connected to the forget
targets; and augmented by a general-knowledge set. A common practice in
existing benchmarks is to employ only a single neighbor set, with general
knowledge which fails to reflect the real-world data complexities and
relationships. LLM Unlearning typically involves 1:1 sampling or cyclic
iteration sampling. However, the efficacy and stability of these de facto
standards have not been critically examined. In this study, we systematically
evaluate these common practices. Our findings reveal that relying on a single
neighbor set is suboptimal and that a standard sampling approach can obscure
performance trade-offs. Based on this analysis, we propose and validate an
initial set of best practices: (1) Incorporation of diverse neighbor sets to
balance forget efficacy and model utility, (2) Standard 1:1 sampling methods
are inefficient and yield poor results, (3) Our proposed Modular Entity-Level
Unlearning (MELU) strategy as an alternative to cyclic sampling. We demonstrate
that this modular approach, combined with robust algorithms, provides a clear
and stable path towards effective unlearning.

</details>


### [272] [Feed Two Birds with One Scone: Exploiting Function-Space Regularization for Both OOD Robustness and ID Fine-Tuning Performance](https://arxiv.org/abs/2509.05328)
*Xiang Yuan,Jun Shu,Deyu meng,Zongben Xu*

Main category: cs.LG

TL;DR: The paper introduces a regularization technique to enhance OOD robustness during model fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing robust fine-tuning methods fail to consistently enhance OOD robustness for different architectures during transfer learning.

Method: Introduce regularization to constrain fine-tuning and pre-trained model distance in the function space, and add consistency regularization for stable predictions.

Result: Shows consistent improvement in ID task performance and OOD robustness across diverse CLIP backbones, outperforming existing methods.

Conclusion: The approach successfully enhances both ID fine-tuning performance and OOD robustness using innovative regularization strategies.

Abstract: Robust fine-tuning aims to achieve competitive in-distribution (ID)
performance while maintaining the out-of-distribution (OOD) robustness of a
pre-trained model when transferring it to a downstream task. To remedy this,
most robust fine-tuning methods aim to preserve the pretrained weights,
features, or logits. However, we find that these methods cannot always improve
OOD robustness for different model architectures. This is due to the OOD
robustness requiring the model function to produce stable prediction for input
information of downstream tasks, while existing methods might serve as a poor
proxy for the optimization in the function space. Based on this finding, we
propose a novel regularization that constrains the distance of fine-tuning and
pre-trained model in the function space with the simulated OOD samples, aiming
to preserve the OOD robustness of the pre-trained model. Besides, to further
enhance the OOD robustness capability of the fine-tuning model, we introduce an
additional consistency regularization to promote stable predictions of
perturbed samples. Extensive experiments demonstrate our approach could
consistently improve both downstream task ID fine-tuning performance and OOD
robustness across a variety of CLIP backbones, outperforming existing
regularization-based robust fine-tuning methods.

</details>


### [273] [Safeguarding Graph Neural Networks against Topology Inference Attacks](https://arxiv.org/abs/2509.05429)
*Jie Fu,Hong Yuan,Zhili Chen,Wendy Hui Wang*

Main category: cs.LG

TL;DR: This study explores topology privacy issues in Graph Neural Networks (GNNs), proposing attacks and a defense framework to reduce structure leakage while maintaining model accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the urgent need to address privacy concerns in GNNs related to graph structure (topology), which has been neglected compared to edge-level privacy.

Method: The researchers introduce Topology Inference Attacks (TIAs) for evaluating topology vulnerabilities and a defense framework called Private Graph Reconstruction (PGR), leveraging bi-level optimization.

Result: Experiments show that PGR minimizes topology leakage effectively with negligible degradation in GNN model accuracy.

Conclusion: The paper concludes GNNs are vulnerable to topology privacy risks, but the proposed PGR framework provides a promising solution to mitigate these issues without sacrificing model performance.

Abstract: Graph Neural Networks (GNNs) have emerged as powerful models for learning
from graph-structured data. However, their widespread adoption has raised
serious privacy concerns. While prior research has primarily focused on
edge-level privacy, a critical yet underexplored threat lies in topology
privacy - the confidentiality of the graph's overall structure. In this work,
we present a comprehensive study on topology privacy risks in GNNs, revealing
their vulnerability to graph-level inference attacks. To this end, we propose a
suite of Topology Inference Attacks (TIAs) that can reconstruct the structure
of a target training graph using only black-box access to a GNN model. Our
findings show that GNNs are highly susceptible to these attacks, and that
existing edge-level differential privacy mechanisms are insufficient as they
either fail to mitigate the risk or severely compromise model accuracy. To
address this challenge, we introduce Private Graph Reconstruction (PGR), a
novel defense framework designed to protect topology privacy while maintaining
model accuracy. PGR is formulated as a bi-level optimization problem, where a
synthetic training graph is iteratively generated using meta-gradients, and the
GNN model is concurrently updated based on the evolving graph. Extensive
experiments demonstrate that PGR significantly reduces topology leakage with
minimal impact on model accuracy. Our code is anonymously available at
https://github.com/JeffffffFu/PGR.

</details>


### [274] [Neural Breadcrumbs: Membership Inference Attacks on LLMs Through Hidden State and Attention Pattern Analysis](https://arxiv.org/abs/2509.05449)
*Disha Makhija,Manoj Ghuhan Arivazhagan,Vinayshekhar Bannihatti Kumar,Rashmi Gangadharaiah*

Main category: cs.LG

TL;DR: The paper introduces a framework called memTrace, demonstrating that analyzing internal representations of LLMs, instead of just their outputs, can achieve high performance in detecting membership inference signals.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of current membership inference attacks (MIAs) that focus only on the outputs of large language models, which show little privacy leakage.

Method: The memTrace framework examines neural 'breadcrumbs' within large language models by analyzing their internal states, including layer-wise dynamics, attention distributions, and cross-layer transitions for potential memorization fingerprints.

Result: The proposed method achieves strong membership detection capabilities, with an average AUC of 0.85 across common MIA benchmarks, outperforming traditional loss-based approaches.

Conclusion: The internal behaviors of large language models can reveal training data exposure even when their outputs are secure, calling for improved privacy-preserving methods in their training.

Abstract: Membership inference attacks (MIAs) reveal whether specific data was used to
train machine learning models, serving as important tools for privacy auditing
and compliance assessment. Recent studies have reported that MIAs perform only
marginally better than random guessing against large language models,
suggesting that modern pre-training approaches with massive datasets may be
free from privacy leakage risks. Our work offers a complementary perspective to
these findings by exploring how examining LLMs' internal representations,
rather than just their outputs, may provide additional insights into potential
membership inference signals. Our framework, \emph{memTrace}, follows what we
call \enquote{neural breadcrumbs} extracting informative signals from
transformer hidden states and attention patterns as they process candidate
sequences. By analyzing layer-wise representation dynamics, attention
distribution characteristics, and cross-layer transition patterns, we detect
potential memorization fingerprints that traditional loss-based approaches may
not capture. This approach yields strong membership detection across several
model families achieving average AUC scores of 0.85 on popular MIA benchmarks.
Our findings suggest that internal model behaviors can reveal aspects of
training data exposure even when output-based signals appear protected,
highlighting the need for further research into membership privacy and the
development of more robust privacy-preserving training techniques for large
language models.

</details>


### [275] [Calibrated Recommendations with Contextual Bandits](https://arxiv.org/abs/2509.05460)
*Diego Feijer,Himan Abdollahpouri,Sanket Gupta,Alexander Clare,Yuxiao Wen,Todd Wasson,Maria Dimakopoulou,Zahra Nazari,Kyle Kretschman,Mounia Lalmas*

Main category: cs.LG

TL;DR: Spotify proposes a dynamic content calibration method using contextual bandits to optimize user engagement by adapting to varied user preferences across contexts.


<details>
  <summary>Details</summary>
Motivation: The need to address imbalances in content types (music-heavy) and adapt to user preferences that vary by time, day, and device.

Method: Using contextual bandits to dynamically learn and adapt content type distribution tailored to individual user contexts and preferences.

Result: Improved precision and user engagement, especially for under-represented content types like podcasts, based on both offline and online tests.

Conclusion: Personalized calibration using contextual bandits improves user engagement and addresses content distribution imbalances.

Abstract: Spotify's Home page features a variety of content types, including music,
podcasts, and audiobooks. However, historical data is heavily skewed toward
music, making it challenging to deliver a balanced and personalized content
mix. Moreover, users' preference towards different content types may vary
depending on the time of day, the day of week, or even the device they use. We
propose a calibration method that leverages contextual bandits to dynamically
learn each user's optimal content type distribution based on their context and
preferences. Unlike traditional calibration methods that rely on historical
averages, our approach boosts engagement by adapting to how users interests in
different content types varies across contexts. Both offline and online results
demonstrate improved precision and user engagement with the Spotify Home page,
in particular with under-represented content types such as podcasts.

</details>


### [276] [ProfilingAgent: Profiling-Guided Agentic Reasoning for Adaptive Model Optimization](https://arxiv.org/abs/2509.05584)
*Sadegh Jafari,Aishwarya Sarkar,Mohiuddin Bilwal,Ali Jannesari*

Main category: cs.LG

TL;DR: ProfilingAgent is an agentic, profiling-guided approach to optimize foundation models through structured pruning and quantization, achieving high efficiency with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address compute and memory bottlenecks faced by foundation models, especially on resource-constrained platforms. Current compression methods often use heuristics that overlook model-specific architecture and runtime demands.

Method: The authors developed ProfilingAgent, a modular multi-agent system leveraging large language models (LLMs) to automate compression. It combines profiling tools for static (e.g., MACs, parameter counts) and dynamic (e.g., latency, memory) metrics to devise customized pruning and quantization strategies.

Result: Experiments demonstrated that pruning maintains or even improves accuracy (e.g., ~1% drop on ImageNet-1K, +2% gains on smaller datasets), while quantization saves up to 74% memory with <0.5% accuracy loss and speeds up inference up to 1.74x.

Conclusion: ProfilingAgent proves that agentic systems guided by profiling tools are effective and scalable for optimizing foundation models, balancing efficiency improvements and minimal accuracy trade-offs.

Abstract: Foundation models face growing compute and memory bottlenecks, hindering
deployment on resource-limited platforms. While compression techniques such as
pruning and quantization are widely used, most rely on uniform heuristics that
ignore architectural and runtime heterogeneity. Profiling tools expose
per-layer latency, memory, and compute cost, yet are rarely integrated into
automated pipelines. We propose ProfilingAgent, a profiling-guided, agentic
approach that uses large language models (LLMs) to automate compression via
structured pruning and post-training dynamic quantization. Our modular
multi-agent system reasons over static metrics (MACs, parameter counts) and
dynamic signals (latency, memory) to design architecture-specific strategies.
Unlike heuristic baselines, ProfilingAgent tailors layer-wise decisions to
bottlenecks. Experiments on ImageNet-1K, CIFAR-10, and CIFAR-100 with
ResNet-101, ViT-B/16, Swin-B, and DeiT-B/16 show pruning maintains competitive
or improved accuracy (about 1% drop on ImageNet-1K, +2% gains for ViT-B/16 on
smaller datasets), while quantization achieves up to 74% memory savings with
<0.5% accuracy loss. Our quantization also yields consistent inference speedups
of up to 1.74 times faster. Comparative studies with GPT-4o and GPT-4-Turbo
highlight the importance of LLM reasoning quality for iterative pruning. These
results establish agentic systems as scalable solutions for profiling-guided
model optimization.

</details>


### [277] [PLanTS: Periodicity-aware Latent-state Representation Learning for Multivariate Time Series](https://arxiv.org/abs/2509.05478)
*Jia Wang,Xiao Wang,Chi Zhang*

Main category: cs.LG

TL;DR: PLanTS is a self-supervised learning framework for multivariate time series (MTS) that incorporates periodicity to improve representation quality and runtime efficiency across diverse downstream tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of high dimensionality, limited labeled data, and non-stationarity in multivariate time series (MTS) while overcoming the limitations of existing self-supervised learning approaches that fail to capture intrinsic periodic structures and latent state dynamics.

Method: The paper introduces a periodicity-aware self-supervised learning framework (PLanTS) that uses a period-aware multi-granularity patching mechanism, a generalized contrastive loss, and a next-transition prediction task to model latent state dynamics and capture temporal structures.

Result: PLanTS delivers enhanced representation quality and outperforms existing self-supervised learning methods in downstream tasks like classification, forecasting, and anomaly detection, while also improving runtime efficiency compared to methods relying on Dynamic Time Warping (DTW).

Conclusion: PLanTS effectively captures the dynamic evolution and periodicity in MTS, offering a robust and efficient solution for self-supervised learning, particularly in scenarios with limited labels and complex temporal structures.

Abstract: Multivariate time series (MTS) are ubiquitous in domains such as healthcare,
climate science, and industrial monitoring, but their high dimensionality,
limited labeled data, and non-stationary nature pose significant challenges for
conventional machine learning methods. While recent self-supervised learning
(SSL) approaches mitigate label scarcity by data augmentations or time
point-based contrastive strategy, they neglect the intrinsic periodic structure
of MTS and fail to capture the dynamic evolution of latent states. We propose
PLanTS, a periodicity-aware self-supervised learning framework that explicitly
models irregular latent states and their transitions. We first designed a
period-aware multi-granularity patching mechanism and a generalized contrastive
loss to preserve both instance-level and state-level similarities across
multiple temporal resolutions. To further capture temporal dynamics, we design
a next-transition prediction pretext task that encourages representations to
encode predictive information about future state evolution. We evaluate PLanTS
across a wide range of downstream tasks-including multi-class and multi-label
classification, forecasting, trajectory tracking and anomaly detection. PLanTS
consistently improves the representation quality over existing SSL methods and
demonstrates superior runtime efficiency compared to DTW-based methods.

</details>


### [278] [STL-based Optimization of Biomolecular Neural Networks for Regression and Control](https://arxiv.org/abs/2509.05481)
*Eric Palanques-Tost,Hanna Krasowski,Murat Arcak,Ron Weiss,Calin Belta*

Main category: cs.LG

TL;DR: Biomolecular Neural Networks (BNNs) can approximate universal functions and are trained effectively using Signal Temporal Logic (STL) specifications. Numerical experiments showcase success in regression and control tasks.


<details>
  <summary>Details</summary>
Motivation: Training BNNs is challenging due to the lack of appropriate target data for their biological architecture. The aim is to explore alternative training approaches.

Method: The authors utilize the quantitative semantics of Signal Temporal Logic (STL) for gradient-based optimization and propose a training algorithm for BNNs that handles regression and control tasks.

Result: STL-based learning was successfully applied to train BNNs in regression tasks (as reporters of dysregulated states) and in a control task (mitigating inflammation in chronic disease while avoiding issues with infections).

Conclusion: The proposed STL-based training enables BNNs to learn effectively for various tasks, proving their versatility and efficiency in solving biologically relevant problems.

Abstract: Biomolecular Neural Networks (BNNs), artificial neural networks with
biologically synthesizable architectures, achieve universal function
approximation capabilities beyond simple biological circuits. However, training
BNNs remains challenging due to the lack of target data. To address this, we
propose leveraging Signal Temporal Logic (STL) specifications to define
training objectives for BNNs. We build on the quantitative semantics of STL,
enabling gradient-based optimization of the BNN weights, and introduce a
learning algorithm that enables BNNs to perform regression and control tasks in
biological systems. Specifically, we investigate two regression problems in
which we train BNNs to act as reporters of dysregulated states, and a feedback
control problem in which we train the BNN in closed-loop with a chronic disease
model, learning to reduce inflammation while avoiding adverse responses to
external infections. Our numerical experiments demonstrate that STL-based
learning can solve the investigated regression and control tasks efficiently.

</details>


### [279] [An Improved Template for Approximate Computing](https://arxiv.org/abs/2509.06162)
*M. Rezaalipour,F. Costa,M. Biasion,R. Otoni,G. A. Constantinides,L. Pozzi*

Main category: cs.LG

TL;DR: The paper introduces a methodology enhancing small arithmetic operators for neural networks, optimizing for energy efficiency and requiring only a minimal loss in computation accuracy.


<details>
  <summary>Details</summary>
Motivation: There is a pressing need to balance energy consumption and inference accuracy when deploying neural networks on edge devices. Approximate computing offers potential solutions but current approaches need refinement.

Method: The authors leverage improvements on XPAT, a recent method, which uses parametrizable templates for Boolean rewriting. They propose a novel template emphasizing parametrizable product sharing as a proxy for synthesized area optimizations.

Result: Experimental results show superior convergence to low-area solutions and better approximation performance compared to XPAT and other state-of-the-art methods.

Conclusion: Using parametrisable templates is effective for optimizing small arithmetic operators in neural networks, offering improved area savings and comparable accuracy losses.

Abstract: Deploying neural networks on edge devices entails a careful balance between
the energy required for inference and the accuracy of the resulting
classification. One technique for navigating this tradeoff is approximate
computing: the process of reducing energy consumption by slightly reducing the
accuracy of arithmetic operators. In this context, we propose a methodology to
reduce the area of the small arithmetic operators used in neural networks -
i.e., adders and multipliers - via a small loss in accuracy, and show that we
improve area savings for the same accuracy loss w.r.t. the state of the art. To
achieve our goal, we improve on a boolean rewriting technique recently
proposed, called XPAT, where the use of a parametrisable template to rewrite
circuits has proved to be highly beneficial. In particular, XPAT was able to
produce smaller circuits than comparable approaches while utilising a naive sum
of products template structure. In this work, we show that template parameters
can act as proxies for chosen metrics and we propose a novel template based on
parametrisable product sharing that acts as a close proxy to synthesised area.
We demonstrate experimentally that our methodology converges better to low-area
solutions and that it can find better approximations than both the original
XPAT and two other state-of-the-art approaches.

</details>


### [280] [Prior Distribution and Model Confidence](https://arxiv.org/abs/2509.05485)
*Maksim Kazanskii,Artem Kasianov*

Main category: cs.LG

TL;DR: This paper proposes a framework to improve image classification accuracy by analyzing training data embeddings to filter out low-confidence predictions, demonstrating consistent gains and potential cross-domain utility.


<details>
  <summary>Details</summary>
Motivation: To investigate how training data distribution affects model performance, and to improve the reliability of predictions by assessing confidence without retraining.

Method: Analyzing training set embeddings to estimate prediction confidence, filtering low-confidence predictions using their distances from the training distribution, and leveraging multiple embedding models for robust confidence estimation.

Result: The method significantly improves classification accuracy, consistently across different model architectures, by filtering low-confidence or out-of-distribution samples.

Conclusion: The framework is model-agnostic, generalizable, and useful across domains like Natural Language Processing, where ensuring prediction reliability is crucial.

Abstract: This paper investigates the impact of training data distribution on the
performance of image classification models. By analyzing the embeddings of the
training set, we propose a framework to understand the confidence of model
predictions on unseen data without the need for retraining. Our approach
filters out low-confidence predictions based on their distance from the
training distribution in the embedding space, significantly improving
classification accuracy. We demonstrate this on the example of several
classification models, showing consistent performance gains across
architectures. Furthermore, we show that using multiple embedding models to
represent the training data enables a more robust estimation of confidence, as
different embeddings capture complementary aspects of the data. Combining these
embeddings allows for better detection and exclusion of out-of-distribution
samples, resulting in further accuracy improvements. The proposed method is
model-agnostic and generalizable, with potential applications beyond computer
vision, including domains such as Natural Language Processing where prediction
reliability is critical.

</details>


### [281] [A Spatio-Temporal Graph Neural Networks Approach for Predicting Silent Data Corruption inducing Circuit-Level Faults](https://arxiv.org/abs/2509.06289)
*Shaoqi Wei,Senling Wang,Hiroshi Kai,Yoshinobu Higami,Ruijun Ma,Tianming Ni,Xiaoqing Wen,Hiroshi Takahashi*

Main category: cs.LG

TL;DR: This paper introduces a spatio-temporal graph convolutional network (ST-GCN) to quickly and accurately predict long-cycle fault impact probabilities (FIPs) in sequential circuits, reducing simulation time by over 10x while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of detecting long-cycle Silent Data Errors (SDEs) in safety-critical systems, which are expensive to simulate with traditional functional testing.

Method: Gate-level netlists are represented as spatio-temporal graphs. The proposed ST-GCN uses dedicated spatial and temporal encoders to predict multi-cycle FIPs efficiently and allows flexibility in using different input features, enabling trade-offs between efficiency and accuracy.

Result: The method demonstrates a mean absolute error of 0.024 for 5-cycle predictions and significantly reduces simulation time by over 10x. It also enhances the detection of hard-to-detect faults when observation points are selected based on predicted FIPs.

Conclusion: The ST-GCN framework is efficient and scalable, fitting seamlessly into electronic design automation workflows and promising utility for SoC-level test strategy optimization.

Abstract: Silent Data Errors (SDEs) from time-zero defects and aging degrade
safety-critical systems. Functional testing detects SDE-related faults but is
expensive to simulate. We present a unified spatio-temporal graph convolutional
network (ST-GCN) for fast, accurate prediction of long-cycle fault impact
probabilities (FIPs) in large sequential circuits, supporting quantitative risk
assessment. Gate-level netlists are modeled as spatio-temporal graphs to
capture topology and signal timing; dedicated spatial and temporal encoders
predict multi-cycle FIPs efficiently. On ISCAS-89 benchmarks, the method
reduces simulation time by more than 10x while maintaining high accuracy (mean
absolute error 0.024 for 5-cycle predictions). The framework accepts features
from testability metrics or fault simulation, allowing efficiency-accuracy
trade-offs. A test-point selection study shows that choosing observation points
by predicted FIPs improves detection of long-cycle, hard-to-detect faults. The
approach scales to SoC-level test strategy optimization and fits downstream
electronic design automation flows.

</details>


### [282] [MambaLite-Micro: Memory-Optimized Mamba Inference on MCUs](https://arxiv.org/abs/2509.05488)
*Hongjun Xu,Junxi Xia,Weisi Yang,Yueyuan Sui,Stephen Xia*

Main category: cs.LG

TL;DR: MambaLite-Micro is a runtime-free inference engine designed to deploy Mamba models on microcontrollers (MCUs) by optimizing memory usage and maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in deploying Mamba models on MCUs, such as limited memory, lack of native operator support, and embedded-friendly toolchains, to enable resource-constrained applications.

Method: The authors developed a pipeline that maps trained PyTorch Mamba models to MCUs by exporting model weights into a lightweight format and implementing optimized handcrafted Mamba layers and operators in C, with operator fusion and memory layout optimization.

Result: MambaLite-Micro reduced peak memory usage by 83.0%, maintained a minimal numerical error of 1.7x10-5, and preserved classification accuracy for KWS and HAR tasks while achieving consistent operation on both ESP32S3 and STM32H7 microcontrollers.

Conclusion: The study successfully demonstrated the deployment of Mamba-based neural architectures on MCUs, ensuring portability across heterogeneous platforms and enabling advanced sequence models in real-world, resource-limited environments.

Abstract: Deploying Mamba models on microcontrollers (MCUs) remains challenging due to
limited memory, the lack of native operator support, and the absence of
embedded-friendly toolchains. We present, to our knowledge, the first
deployment of a Mamba-based neural architecture on a resource-constrained MCU,
a fully C-based runtime-free inference engine: MambaLite-Micro. Our pipeline
maps a trained PyTorch Mamba model to on-device execution by (1) exporting
model weights into a lightweight format, and (2) implementing a handcrafted
Mamba layer and supporting operators in C with operator fusion and memory
layout optimization. MambaLite-Micro eliminates large intermediate tensors,
reducing 83.0% peak memory, while maintaining an average numerical error of
only 1.7x10-5 relative to the PyTorch Mamba implementation. When evaluated on
keyword spotting(KWS) and human activity recognition (HAR) tasks,
MambaLite-Micro achieved 100% consistency with the PyTorch baselines, fully
preserving classification accuracy. We further validated portability by
deploying on both ESP32S3 and STM32H7 microcontrollers, demonstrating
consistent operation across heterogeneous embedded platforms and paving the way
for bringing advanced sequence models like Mamba to real-world
resource-constrained applications.

</details>


### [283] [Self-Aligned Reward: Towards Effective and Efficient Reasoners](https://arxiv.org/abs/2509.05489)
*Peixuan Han,Adit Krishnan,Gerald Friedland,Jiaxuan You,Chris Kong*

Main category: cs.LG

TL;DR: The paper introduces Self-Aligned Reward (SAR), a signal designed to enhance reasoning in large language models by balancing response accuracy and efficiency. SAR improves accuracy while significantly reducing inference costs.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement learning techniques with verifiable rewards provide only binary correctness feedback, leading to inefficiencies like verbose reasoning and high computational costs, often compromising accuracy.

Method: The authors propose SAR, quantified by the relative perplexity difference between a query-conditioned answer and a standalone answer. This metric promotes concise, query-specific responses. SAR is integrated with RL algorithms like PPO and GRPO to evaluate its effectiveness.

Result: SAR improved reasoning accuracy by 4% and reduced inference costs by 30% across 4 models and 7 benchmarks. SAR demonstrated its ability to distinguish answer quality and achieve a Pareto-optimal trade-off between correctness and efficiency.

Conclusion: Self-Aligned Reward enhances reasoning accuracy and efficiency without sacrificing advanced reasoning capabilities, offering a compelling, fine-grained complement to verifiable rewards for training large language models.

Abstract: Reinforcement learning with verifiable rewards has significantly advanced
reasoning in large language models (LLMs), but such signals remain coarse,
offering only binary correctness feedback. This limitation often results in
inefficiencies, including overly verbose reasoning and high computational cost,
while existing solutions often compromise accuracy. To address this, we
introduce self-aligned reward (SAR), a self-guided signal that complements
verifiable rewards to encourage both reasoning accuracy and efficiency. SAR is
defined as the relative perplexity difference between an answer conditioned on
the query and the standalone answer, thereby favoring responses that are
concise and query-specific. Quantitative analysis reveals that SAR reliably
distinguishes answer quality: concise, correct answers score higher than
redundant ones, and partially correct answers score higher than entirely
incorrect ones. Evaluation on 4 models across 7 benchmarks shows that
integrating SAR with prevalent RL algorithms like PPO and GRPO improves
accuracy by 4%, while reducing inference cost by 30%. Further analysis
demonstrates that SAR achieves a Pareto-optimal trade-off between correctness
and efficiency compared to reward signals based on length or self-confidence.
We also show that SAR shortens responses while preserving advanced reasoning
behaviors, demonstrating its ability to suppress unnecessary elaboration
without losing critical reasoning. These results highlight the promise of
self-aligned reward as a fine-grained complement to verifiable rewards, paving
the way for more efficient and effective LLM training.

</details>


### [284] [DreamPRM-1.5: Unlocking the Potential of Each Instance for Multimodal Process Reward Model Training](https://arxiv.org/abs/2509.05542)
*Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: DreamPRM-1.5 introduces an adaptive reweighting approach to tackle challenges in training multimodal process reward models, achieving state-of-the-art accuracy on the MMMU benchmark.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges such as distribution shifts and noisy datasets when training multimodal process reward models (PRMs).

Method: DreamPRM-1.5 adopts bi-level optimization to adaptively reweight training examples, using two strategies—Instance Table for smaller datasets and Instance Net for scalability to larger datasets.

Result: DreamPRM-1.5 surpasses previous benchmarks, achieving 84.6% accuracy on the MMMU dataset, outperforming GPT-5.

Conclusion: DreamPRM-1.5 demonstrates the effectiveness of adaptive reweighting combined with test-time scaling, setting a new performance standard for multimodal process reward models.

Abstract: Training multimodal process reward models (PRMs) is challenged by
distribution shifts and noisy data. We introduce DreamPRM-1.5, an
instance-reweighted framework that adaptively adjusts the importance of each
training example via bi-level optimization. We design two complementary
strategies: Instance Table, effective for smaller datasets, and Instance Net,
scalable to larger ones. Integrated into test-time scaling, DreamPRM-1.5
achieves 84.6 accuracy on the MMMU benchmark, surpassing GPT-5.

</details>


### [285] [Distributed Deep Learning using Stochastic Gradient Staleness](https://arxiv.org/abs/2509.05679)
*Viet Hoang Pham,Hyo-Sung Ahn*

Main category: cs.LG

TL;DR: The paper proposes a distributed training method for deep neural networks combining data parallelism and fully decoupled parallel backpropagation to address challenges in training time and efficiency.


<details>
  <summary>Details</summary>
Motivation: To overcome lengthy training times and scalability issues with increasingly deeper networks and large datasets in deep learning.

Method: The approach integrates data parallelism and a decoupled parallel backpropagation algorithm, utilizing multiple computational units to improve processing per iteration and mitigate backpropagation locking issues.

Result: The method is proven to converge under certain conditions and is empirically validated by successfully training a DNN on CIFAR-10 classification tasks.

Conclusion: The distributed training method significantly enhances training efficiency and demonstrates practical feasibility for deep learning tasks.

Abstract: Despite the notable success of deep neural networks (DNNs) in solving complex
tasks, the training process still remains considerable challenges. A primary
obstacle is the substantial time required for training, particularly as high
performing DNNs tend to become increasingly deep (characterized by a larger
number of hidden layers) and require extensive training datasets. To address
these challenges, this paper introduces a distributed training method that
integrates two prominent strategies for accelerating deep learning: data
parallelism and fully decoupled parallel backpropagation algorithm. By
utilizing multiple computational units operating in parallel, the proposed
approach enhances the amount of training data processed in each iteration while
mitigating locking issues commonly associated with the backpropagation
algorithm. These features collectively contribute to significant improvements
in training efficiency. The proposed distributed training method is rigorously
proven to converge to critical points under certain conditions. Its
effectiveness is further demonstrated through empirical evaluations, wherein an
DNN is trained to perform classification tasks on the CIFAR-10 dataset.

</details>


### [286] [Reinforcement Learning with Anticipation: A Hierarchical Approach for Long-Horizon Tasks](https://arxiv.org/abs/2509.05545)
*Yang Yu*

Main category: cs.LG

TL;DR: The paper introduces a scalable framework, Reinforcement Learning with Anticipation (RLA), to tackle long-horizon, goal-conditioned tasks using hierarchical reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To address long-horizon goal-conditioned tasks, which are a major challenge in reinforcement learning, and to overcome instability and theoretical limitations in hierarchical RL methods.

Method: RLA employs two models: a low-level goal-conditioned policy for subgoal achievement and a high-level anticipation model that proposes intermediate subgoals, guided by value geometric consistency principles.

Result: RLA demonstrates convergence towards globally optimal policies under certain conditions, offering a consistent and principled approach to hierarchical planning.

Conclusion: RLA provides a scalable and theoretically grounded framework for hierarchical reinforcement learning that can be used effectively for long-horizon goal-conditioned tasks.

Abstract: Solving long-horizon goal-conditioned tasks remains a significant challenge
in reinforcement learning (RL). Hierarchical reinforcement learning (HRL)
addresses this by decomposing tasks into more manageable sub-tasks, but the
automatic discovery of the hierarchy and the joint training of multi-level
policies often suffer from instability and can lack theoretical guarantees. In
this paper, we introduce Reinforcement Learning with Anticipation (RLA), a
principled and potentially scalable framework designed to address these
limitations. The RLA agent learns two synergistic models: a low-level,
goal-conditioned policy that learns to reach specified subgoals, and a
high-level anticipation model that functions as a planner, proposing
intermediate subgoals on the optimal path to a final goal. The key feature of
RLA is the training of the anticipation model, which is guided by a principle
of value geometric consistency, regularized to prevent degenerate solutions. We
present proofs that RLA approaches the globally optimal policy under various
conditions, establishing a principled and convergent method for hierarchical
planning and execution in long-horizon goal-conditioned tasks.

</details>


### [287] [Nonnegative matrix factorization and the principle of the common cause](https://arxiv.org/abs/2509.03652)
*E. Khalafyan,A. E. Allahverdyan,A. Hovhannisyan*

Main category: cs.LG

TL;DR: The paper explores the relationship between Nonnegative Matrix Factorization (NMF) and the Principle of the Common Cause (PCC), demonstrating mutual benefits for feature stability, rank estimation, clustering, and data denoising in gray-scale image datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to investigate the close relationship between NMF and PCC and leverage this connection for more robust feature discovery, rank estimation, and clustering, especially in scenarios involving noise.

Method: The study uses datasets of gray-scale images and applies NMF methods combined with principles drawn from PCC. PCC is employed as a tool to estimate the effective rank of NMF, improving robustness and stability. Additionally, NMF is used for clustering and data denoising.

Result: The proposed method leads to rank estimation that is robust to weak noise and produces stable NMF features. The approach resolves the nonidentifiability problem in NMF and enables clustering data points sharing a common cause. The method also demonstrates effectiveness in data denoising.

Conclusion: This reciprocal exploration shows that NMF and PCC can significantly enhance one another. PCC informs a stable NMF rank, while NMF offers a practical way to approximate PCC for solving tasks like clustering and denoising. The proposed techniques achieve noise-resilient results with practical implications for image analysis and probabilistic modeling.

Abstract: Nonnegative matrix factorization (NMF) is a known unsupervised data-reduction
method. The principle of the common cause (PCC) is a basic methodological
approach in probabilistic causality, which seeks an independent mixture model
for the joint probability of two dependent random variables. It turns out that
these two concepts are closely related. This relationship is explored
reciprocally for several datasets of gray-scale images, which are conveniently
mapped into probability models. On one hand, PCC provides a predictability tool
that leads to a robust estimation of the effective rank of NMF. Unlike other
estimates (e.g., those based on the Bayesian Information Criteria), our
estimate of the rank is stable against weak noise. We show that NMF implemented
around this rank produces features (basis images) that are also stable against
noise and against seeds of local optimization, thereby effectively resolving
the NMF nonidentifiability problem. On the other hand, NMF provides an
interesting possibility of implementing PCC in an approximate way, where larger
and positively correlated joint probabilities tend to be explained better via
the independent mixture model. We work out a clustering method, where data
points with the same common cause are grouped into the same cluster. We also
show how NMF can be employed for data denoising.

</details>


### [288] [Causal Debiasing Medical Multimodal Representation Learning with Missing Modalities](https://arxiv.org/abs/2509.05615)
*Xiaoguang Zhu,Lianlong Sun,Yang Liu,Pengyi Jiang,Uma Srivatsa,Nipavan Chiamvimonvat,Vladimir Filkov*

Main category: cs.LG

TL;DR: The paper addresses multimodal representation learning for medical data while dealing with biases due to missing modalities and latent confounders by introducing a causal framework.


<details>
  <summary>Details</summary>
Motivation: Existing methods in medical multimodal representation learning fail to account for biases in the data acquisition process, specifically missingness bias and distribution bias.

Method: The authors propose a framework with two main components: a missingness deconfounding module using causal adjustment and a dual-branch neural network for disentangling causal features.

Result: Through evaluations on public and in-hospital datasets, the framework showed effectiveness in predictive modeling and highlighted causal insights.

Conclusion: The proposed approach successfully handles biases in medical multimodal data, improving generalization and providing clearer causal interpretations.

Abstract: Medical multimodal representation learning aims to integrate heterogeneous
clinical data into unified patient representations to support predictive
modeling, which remains an essential yet challenging task in the medical data
mining community. However, real-world medical datasets often suffer from
missing modalities due to cost, protocol, or patient-specific constraints.
Existing methods primarily address this issue by learning from the available
observations in either the raw data space or feature space, but typically
neglect the underlying bias introduced by the data acquisition process itself.
In this work, we identify two types of biases that hinder model generalization:
missingness bias, which results from non-random patterns in modality
availability, and distribution bias, which arises from latent confounders that
influence both observed features and outcomes. To address these challenges, we
perform a structural causal analysis of the data-generating process and propose
a unified framework that is compatible with existing direct prediction-based
multimodal learning methods. Our method consists of two key components: (1) a
missingness deconfounding module that approximates causal intervention based on
backdoor adjustment and (2) a dual-branch neural network that explicitly
disentangles causal features from spurious correlations. We evaluated our
method in real-world public and in-hospital datasets, demonstrating its
effectiveness and causal insights.

</details>


### [289] [OptiProxy-NAS: Optimization Proxy based End-to-End Neural Architecture Search](https://arxiv.org/abs/2509.05656)
*Bo Lyu,Yu Cui,Tuo Shi,Ke Li*

Main category: cs.LG

TL;DR: This paper presents OptiProxy-NAS, a method for neural architecture search (NAS) that reformulates the search space to make it continuous, differentiable, and smooth, improving efficiency and search outcomes.


<details>
  <summary>Details</summary>
Motivation: The need to address the challenges of computational expense and inefficiency in NAS due to its discrete, vast, and complex search space.

Method: The proposed OptiProxy-NAS leverages a proxy-based representation to reframe the NAS problem, enabling the use of gradient-based optimization techniques for efficient and precise architecture search.

Result: Experiments across 12 NAS tasks in diverse domains (computer vision, NLP, and resource-constrained NAS) demonstrate superior search results and efficiency. It also adapts well to low-fidelity scenarios.

Conclusion: OptiProxy-NAS not only enhances the effectiveness and efficiency of NAS but also exhibits flexibility across various fidelity levels and domains, marking it as a promising framework for NAS optimization.

Abstract: Neural architecture search (NAS) is a hard computationally expensive
optimization problem with a discrete, vast, and spiky search space. One of the
key research efforts dedicated to this space focuses on accelerating NAS via
certain proxy evaluations of neural architectures. Different from the prevalent
predictor-based methods using surrogate models and differentiable architecture
search via supernetworks, we propose an optimization proxy to streamline the
NAS as an end-to-end optimization framework, named OptiProxy-NAS. In
particular, using a proxy representation, the NAS space is reformulated to be
continuous, differentiable, and smooth. Thereby, any differentiable
optimization method can be applied to the gradient-based search of the relaxed
architecture parameters. Our comprehensive experiments on $12$ NAS tasks of $4$
search spaces across three different domains including computer vision, natural
language processing, and resource-constrained NAS fully demonstrate the
superior search results and efficiency. Further experiments on low-fidelity
scenarios verify the flexibility.

</details>


### [290] [DQS: A Low-Budget Query Strategy for Enhancing Unsupervised Data-driven Anomaly Detection Approaches](https://arxiv.org/abs/2509.05663)
*Lucas Correia,Jan-Christoph Goos,Thomas Bäck,Anna V. Kononova*

Main category: cs.LG

TL;DR: This paper integrates active learning into unsupervised time series anomaly detection by introducing a dissimilarity-based query strategy (DQS) to refine threshold selection for better performance. All active learning strategies outperform unsupervised thresholds.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address issues in unsupervised time series anomaly detection, particularly poorly set thresholds and reliance on labeled data subsets, which may not exist in real-world scenarios.

Method: The method integrates active learning with an unsupervised anomaly detection approach, introducing the DQS query strategy that maximizes diversity of queried samples by using dynamic time warping to evaluate anomaly score similarity.

Result: DQS performs best in low-budget scenarios for anomaly detection compared to other query strategies, but it is more sensitive to mislabeling.

Conclusion: Active learning-based threshold selection outperforms purely unsupervised methods, even when mislabeled data exists. The choice of query strategy should depend on oracle expertise and labeling budget.

Abstract: Truly unsupervised approaches for time series anomaly detection are rare in
the literature. Those that exist suffer from a poorly set threshold, which
hampers detection performance, while others, despite claiming to be
unsupervised, need to be calibrated using a labelled data subset, which is
often not available in the real world. This work integrates active learning
with an existing unsupervised anomaly detection method by selectively querying
the labels of multivariate time series, which are then used to refine the
threshold selection process. To achieve this, we introduce a novel query
strategy called the dissimilarity-based query strategy (DQS). DQS aims to
maximise the diversity of queried samples by evaluating the similarity between
anomaly scores using dynamic time warping. We assess the detection performance
of DQS in comparison to other query strategies and explore the impact of
mislabelling, a topic that is underexplored in the literature. Our findings
indicate that DQS performs best in small-budget scenarios, though the others
appear to be more robust when faced with mislabelling. Therefore, in the real
world, the choice of query strategy depends on the expertise of the oracle and
the number of samples they are willing to label. Regardless, all query
strategies outperform the unsupervised threshold even in the presence of
mislabelling. Thus, whenever it is feasible to query an oracle, employing an
active learning-based threshold is recommended.

</details>


### [291] [GraMFedDHAR: Graph Based Multimodal Differentially Private Federated HAR](https://arxiv.org/abs/2509.05671)
*Labani Halder,Tanmay Sen,Sarbani Palit*

Main category: cs.LG

TL;DR: This paper presents GraMFedDHAR, a graph-based multimodal federated learning framework for human activity recognition (HAR), focusing on privacy-preserving methods and robustness in handling heterogeneous sensor data.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in HAR such as noisy/incomplete sensor data, limited labeled data, and privacy concerns, while finding alternatives to conventional centralized deep learning systems affected by infrastructure, latency, and data-sharing limitations.

Method: GraMFedDHAR uses modality-specific sensor graphs processed through residual Graph Convolutional Networks (GCNs), followed by attention-based fusion for classification. Differential privacy is applied during federated aggregation to enhance security.

Result: GraMFedDHAR's MultiModalGCN model shows higher accuracy (up to 2% improvement) compared to MultiModalFFN in centralized and federated setups without DP. Under differential privacy, MultiModalGCN outperforms by 7-13%, showcasing robustness against DP noise.

Conclusion: Graph-based modeling with GCNs and attention-based fusion enhances multimodal learning performance, especially under differential privacy constraints, making GraMFedDHAR a robust solution for privacy-preserving HAR tasks.

Abstract: Human Activity Recognition (HAR) using multimodal sensor data remains
challenging due to noisy or incomplete measurements, scarcity of labeled
examples, and privacy concerns. Traditional centralized deep learning
approaches are often constrained by infrastructure availability, network
latency, and data sharing restrictions. While federated learning (FL) addresses
privacy by training models locally and sharing only model parameters, it still
has to tackle issues arising from the use of heterogeneous multimodal data and
differential privacy requirements. In this article, a Graph-based Multimodal
Federated Learning framework, GraMFedDHAR, is proposed for HAR tasks. Diverse
sensor streams such as a pressure mat, depth camera, and multiple
accelerometers are modeled as modality-specific graphs, processed through
residual Graph Convolutional Neural Networks (GCNs), and fused via
attention-based weighting rather than simple concatenation. The fused
embeddings enable robust activity classification, while differential privacy
safeguards data during federated aggregation. Experimental results show that
the proposed MultiModalGCN model outperforms the baseline MultiModalFFN, with
up to 2 percent higher accuracy in non-DP settings in both centralized and
federated paradigms. More importantly, significant improvements are observed
under differential privacy constraints: MultiModalGCN consistently surpasses
MultiModalFFN, with performance gaps ranging from 7 to 13 percent depending on
the privacy budget and setting. These results highlight the robustness of
graph-based modeling in multimodal learning, where GNNs prove more resilient to
the performance degradation introduced by DP noise.

</details>


### [292] [Tackling Device Data Distribution Real-time Shift via Prototype-based Parameter Editing](https://arxiv.org/abs/2509.06552)
*Zheqi Lv,Wenqiao Zhang,Kairui Fu,Qi Tian,Shengyu Zhang,Jiajie Su,Jingyuan Chen,Kun Kuang,Fei Wu*

Main category: cs.LG

TL;DR: The paper proposes Persona, a backpropagation-free, prototype-based personalization method that addresses real-time data distribution shifts on devices without post-deployment retraining.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the challenge of on-device real-time data distribution shifts that hinder lightweight models’ generalization, a problem overlooked due to reliance on fine-tuning approaches.

Method: Persona uses a neural adapter in the cloud to generate a parameter editing matrix based on device data, adapting on-device models to data distributions and dynamically refining through prototypes and cross-layer knowledge transfer.

Result: Experiments on vision and recommendation tasks across multiple datasets show Persona effectively enhances generalization and adapts efficiently.

Conclusion: Persona presents a lightweight, personalized, and generalizable solution, offering a practical approach to tackle on-device data shifts without retraining.

Abstract: The on-device real-time data distribution shift on devices challenges the
generalization of lightweight on-device models. This critical issue is often
overlooked in current research, which predominantly relies on data-intensive
and computationally expensive fine-tuning approaches. To tackle this, we
introduce Persona, a novel personalized method using a prototype-based,
backpropagation-free parameter editing framework to enhance model
generalization without post-deployment retraining. Persona employs a neural
adapter in the cloud to generate a parameter editing matrix based on real-time
device data. This matrix adeptly adapts on-device models to the prevailing data
distributions, efficiently clustering them into prototype models. The
prototypes are dynamically refined via the parameter editing matrix,
facilitating efficient evolution. Furthermore, the integration of cross-layer
knowledge transfer ensures consistent and context-aware multi-layer parameter
changes and prototype assignment. Extensive experiments on vision task and
recommendation task on multiple datasets confirm Persona's effectiveness and
generality.

</details>


### [293] [Ensemble of Precision-Recall Curve (PRC) Classification Trees with Autoencoders](https://arxiv.org/abs/2509.05766)
*Jiaju Miao,Wei Zhu*

Main category: cs.LG

TL;DR: The paper introduces Autoencoder-PRC-RF, a hybrid framework for anomaly detection that addresses class imbalance and high dimensionality using PRC Random Forests and autoencoders.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve anomaly detection by addressing challenges like extreme class imbalance and high dimensionality.

Method: The proposed hybrid framework combines PRC Random Forests with autoencoders to simultaneously handle class imbalance and dimensionality reduction.

Result: The Autoencoder-PRC-RF model outperforms existing methods in terms of accuracy, scalability, and interpretability on benchmark datasets.

Conclusion: The Autoencoder-PRC-RF framework is a promising solution for critical anomaly detection tasks, offering significant improvements over traditional methods.

Abstract: Anomaly detection underpins critical applications from network security and
intrusion detection to fraud prevention, where recognizing aberrant patterns
rapidly is indispensable. Progress in this area is routinely impeded by two
obstacles: extreme class imbalance and the curse of dimensionality. To combat
the former, we previously introduced Precision-Recall Curve (PRC)
classification trees and their ensemble extension, the PRC Random Forest
(PRC-RF). Building on that foundation, we now propose a hybrid framework that
integrates PRC-RF with autoencoders, unsupervised machine learning methods that
learn compact latent representations, to confront both challenges
simultaneously. Extensive experiments across diverse benchmark datasets
demonstrate that the resulting Autoencoder-PRC-RF model achieves superior
accuracy, scalability, and interpretability relative to prior methods,
affirming its potential for high-stakes anomaly-detection tasks.

</details>


### [294] [Morphological Perceptron with Competitive Layer: Training Using Convex-Concave Procedure](https://arxiv.org/abs/2509.05697)
*Iara Cunha,Marcos Eduardo Valle*

Main category: cs.LG

TL;DR: The paper proposes using the convex-concave procedure (CCP) to train morphological perceptron networks (MPCL), solving non-differentiable optimization with iterative linear programming.


<details>
  <summary>Details</summary>
Motivation: Training morphological perceptron networks is challenging because their non-differentiable nature makes gradient-based optimization methods unsuitable.

Method: The training problem for MPCL networks is formulated as a difference of convex (DC) functions, and the convex-concave procedure (CCP) is applied iteratively to solve it through linear programming subproblems.

Result: The experimentation validates the proposed method's effectiveness for classification tasks with MPCL networks.

Conclusion: CCP provides a viable solution for training morphological perceptrons, paving the way for improved classification models using this approach.

Abstract: A morphological perceptron is a multilayer feedforward neural network in
which neurons perform elementary operations from mathematical morphology. For
multiclass classification tasks, a morphological perceptron with a competitive
layer (MPCL) is obtained by integrating a winner-take-all output layer into the
standard morphological architecture. The non-differentiability of morphological
operators renders gradient-based optimization methods unsuitable for training
such networks. Consequently, alternative strategies that do not depend on
gradient information are commonly adopted. This paper proposes the use of the
convex-concave procedure (CCP) for training MPCL networks. The training problem
is formulated as a difference of convex (DC) functions and solved iteratively
using CCP, resulting in a sequence of linear programming subproblems.
Computational experiments demonstrate the effectiveness of the proposed
training method in addressing classification tasks with MPCL networks.

</details>


### [295] [DCV-ROOD Evaluation Framework: Dual Cross-Validation for Robust Out-of-Distribution Detection](https://arxiv.org/abs/2509.05778)
*Arantxa Urrea-Castaño,Nicolás Segura-Kunsagi,Juan Luis Suárez-Díaz,Rosana Montes,Francisco Herrera*

Main category: cs.LG

TL;DR: This paper introduces a new evaluation framework called DCV-ROOD for improving the reliability of Out-of-Distribution (OOD) detection models, using dual cross-validation methods.


<details>
  <summary>Details</summary>
Motivation: The robustness of AI systems depends on effectively detecting OOD inputs, which ensures reliable predictions and prevents failures. Proper evaluation methods are needed to assess OOD detection performance under varied conditions.

Method: The paper proposes a dual cross-validation (DCV) framework, where in-distribution (ID) data is split using standard methods, and OOD data is divided considering class hierarchy and class grouping for fair evaluation.

Result: The experimental results demonstrate the fast convergence of DCV-ROOD to the true performance metrics of selected state-of-the-art OOD detection methods.

Conclusion: DCV-ROOD offers a robust evaluation framework that integrates ID and OOD data effectively, enhancing the reliability of assessments for existing OOD detection techniques.

Abstract: Out-of-distribution (OOD) detection plays a key role in enhancing the
robustness of artificial intelligence systems by identifying inputs that differ
significantly from the training distribution, thereby preventing unreliable
predictions and enabling appropriate fallback mechanisms. Developing reliable
OOD detection methods is a significant challenge, and rigorous evaluation of
these techniques is essential for ensuring their effectiveness, as it allows
researchers to assess their performance under diverse conditions and to
identify potential limitations or failure modes. Cross-validation (CV) has
proven to be a highly effective tool for providing a reasonable estimate of the
performance of a learning algorithm. Although OOD scenarios exhibit particular
characteristics, an appropriate adaptation of CV can lead to a suitable
evaluation framework for this setting. This work proposes a dual CV framework
for robust evaluation of OOD detection models, aimed at improving the
reliability of their assessment. The proposed evaluation framework aims to
effectively integrate in-distribution (ID) and OOD data while accounting for
their differing characteristics. To achieve this, ID data are partitioned using
a conventional approach, whereas OOD data are divided by grouping samples based
on their classes. Furthermore, we analyze the context of data with class
hierarchy to propose a data splitting that considers the entire class hierarchy
to obtain fair ID-OOD partitions to apply the proposed evaluation framework.
This framework is called Dual Cross-Validation for Robust Out-of-Distribution
Detection (DCV-ROOD). To test the validity of the evaluation framework, we
selected a set of state-of-the-art OOD detection methods, both with and without
outlier exposure. The results show that the method achieves very fast
convergence to the true performance.

</details>


### [296] [Simulation Priors for Data-Efficient Deep Learning](https://arxiv.org/abs/2509.05732)
*Lenart Treven,Bhavya Sukhija,Jonas Rothfuss,Stelian Coros,Florian Dörfler,Andreas Krause*

Main category: cs.LG

TL;DR: SimPEL integrates first-principles models with data-driven learning using Bayesian deep learning to achieve data-efficient and uncertainty-aware learning in complex real-world systems.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of both first-principles models, which cannot capture real-world complexity, and deep learning, which requires large datasets, by combining their strengths.

Method: SimPEL employs low-fidelity simulators as priors in Bayesian deep learning, allowing the system to utilize simulator knowledge in low-data scenarios while leveraging deep learning's capabilities with increasing data.

Result: SimPEL outperformed state-of-the-art methods in diverse fields such as biology, agriculture, and robotics, and effectively reduced the sim-to-real gap for model-based reinforcement learning, exhibiting superior performance in tasks like high-speed RC car maneuvers.

Conclusion: SimPEL demonstrates strong potential for efficient learning in complex environments, offering a robust balance between simulator-driven and data-driven approaches while carefully quantifying uncertainty.

Abstract: How do we enable AI systems to efficiently learn in the real-world?
First-principles models are widely used to simulate natural systems, but often
fail to capture real-world complexity due to simplifying assumptions. In
contrast, deep learning approaches can estimate complex dynamics with minimal
assumptions but require large, representative datasets. We propose SimPEL, a
method that efficiently combines first-principles models with data-driven
learning by using low-fidelity simulators as priors in Bayesian deep learning.
This enables SimPEL to benefit from simulator knowledge in low-data regimes and
leverage deep learning's flexibility when more data is available, all the while
carefully quantifying epistemic uncertainty. We evaluate SimPEL on diverse
systems, including biological, agricultural, and robotic domains, showing
superior performance in learning complex dynamics. For decision-making, we
demonstrate that SimPEL bridges the sim-to-real gap in model-based
reinforcement learning. On a high-speed RC car task, SimPEL learns a highly
dynamic parking maneuver involving drifting with substantially less data than
state-of-the-art baselines. These results highlight the potential of SimPEL for
data-efficient learning and control in complex real-world environments.

</details>


### [297] [Offline vs. Online Learning in Model-based RL: Lessons for Data Collection Strategies](https://arxiv.org/abs/2509.05735)
*Jiaqi Chen,Ji Shi,Cansu Sancaktar,Jonas Frey,Georg Martius*

Main category: cs.LG

TL;DR: The paper discusses the differences between offline and online data collection methods in model-based reinforcement learning, finding that online methods typically perform better due to their ability to self-correct Out-Of-Distribution errors.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of online vs. offline data collection methods on the performance of world models in model-based reinforcement learning tasks.

Method: Conducted experiments across 31 environments, analyzed performance degradation factors for offline agents, and proposed solutions like adaptive scheduling of online interaction and using exploration data.

Result: Online strategies outperform offline ones, as offline models suffer from Out-Of-Distribution mismatches. Incorporating additional online interactions and exploration data ameliorates this gap.

Conclusion: Hybrid approaches combining online interaction with exploration data collection can improve the efficiency and robustness of world models in reinforcement learning.

Abstract: Data collection is crucial for learning robust world models in model-based
reinforcement learning. The most prevalent strategies are to actively collect
trajectories by interacting with the environment during online training or
training on offline datasets. At first glance, the nature of learning
task-agnostic environment dynamics makes world models a good candidate for
effective offline training. However, the effects of online vs. offline data on
world models and thus on the resulting task performance have not been
thoroughly studied in the literature. In this work, we investigate both
paradigms in model-based settings, conducting experiments on 31 different
environments. First, we showcase that online agents outperform their offline
counterparts. We identify a key challenge behind performance degradation of
offline agents: encountering Out-Of-Distribution states at test time. This
issue arises because, without the self-correction mechanism in online agents,
offline datasets with limited state space coverage induce a mismatch between
the agent's imagination and real rollouts, compromising policy training. We
demonstrate that this issue can be mitigated by allowing for additional online
interactions in a fixed or adaptive schedule, restoring the performance of
online training with limited interaction data. We also showcase that
incorporating exploration data helps mitigate the performance degradation of
offline agents. Based on our insights, we recommend adding exploration data
when collecting large datasets, as current efforts predominantly focus on
expert data alone.

</details>


### [298] [The Measure of Deception: An Analysis of Data Forging in Machine Unlearning](https://arxiv.org/abs/2509.05865)
*Rishabh Dixit,Yuan Hui,Rayan Saab*

Main category: cs.LG

TL;DR: This paper tackles machine unlearning, introducing a framework to analyze 'forging,' the adversarial mimicking of gradient data to falsely simulate unlearning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address privacy needs and harmful data issues by ensuring models can effectively "forget" data while combating adversarial misuse.

Method: It analyzes the forging set (points mimicking target gradients) for linear regression and neural networks, deriving scaling laws and probability bounds under various assumptions.

Result: The measure of forging sets is shown to decay asymptotically as $\epsilon^{(d-r)/2}$, and random sampling likelihood is negligible for non-degenerate data.

Conclusion: Adversarial forging is inherently constrained, enabling the detection of false unlearning claims in practical scenarios.

Abstract: Motivated by privacy regulations and the need to mitigate the effects of
harmful data, machine unlearning seeks to modify trained models so that they
effectively ``forget'' designated data. A key challenge in verifying unlearning
is forging -- adversarially crafting data that mimics the gradient of a target
point, thereby creating the appearance of unlearning without actually removing
information. To capture this phenomenon, we consider the collection of data
points whose gradients approximate a target gradient within tolerance
$\epsilon$ -- which we call an $\epsilon$-forging set -- and develop a
framework for its analysis. For linear regression and one-layer neural
networks, we show that the Lebesgue measure of this set is small. It scales on
the order of $\epsilon$, and when $\epsilon$ is small enough, $\epsilon^d$.
More generally, under mild regularity assumptions, we prove that the forging
set measure decays as $\epsilon^{(d-r)/2}$, where $d$ is the data dimension and
$r<d$ is the nullity of a variation matrix defined by the model gradients.
Extensions to batch SGD and almost-everywhere smooth loss functions yield the
same asymptotic scaling. In addition, we establish probability bounds showing
that, under non-degenerate data distributions, the likelihood of randomly
sampling a forging point is vanishingly small. These results provide evidence
that adversarial forging is fundamentally limited and that false unlearning
claims can, in principle, be detected.

</details>


### [299] [Real-E: A Foundation Benchmark for Advancing Robust and Generalizable Electricity Forecasting](https://arxiv.org/abs/2509.05768)
*Chen Shao,Yue Wang,Zhenyi Zhu,Zhanbo Huang,Sebastian Pütz,Benjamin Schäfer,Tobais Käfer,Michael Färber*

Main category: cs.LG

TL;DR: The paper presents the Real-E dataset, addressing gaps in energy forecasting benchmarks with wider spatial and temporal coverage, and highlights limitations in current forecasting methods.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing energy forecasting benchmarks which lack extensive spatial and temporal scope and multi-energy features, reducing their real-world applicability.

Method: The authors introduce the Real-E dataset, spanning 74 power stations in 30+ countries over 10 years, perform comprehensive data analysis, evaluate over 20 baseline models, and develop a metric to assess correlation structure shifts.

Result: Their analysis shows that existing forecasting methods struggle with the Real-E dataset due to its complex, non-stationary correlation dynamics, providing key insights into these methods' limitations.

Conclusion: The study underscores the need for robust forecasting models by showcasing the insufficiencies of current methods and providing a rich dataset (Real-E) for addressing these challenges.

Abstract: Energy forecasting is vital for grid reliability and operational efficiency.
Although recent advances in time series forecasting have led to progress,
existing benchmarks remain limited in spatial and temporal scope and lack
multi-energy features. This raises concerns about their reliability and
applicability in real-world deployment. To address this, we present the Real-E
dataset, covering over 74 power stations across 30+ European countries over a
10-year span with rich metadata. Using Real- E, we conduct an extensive data
analysis and benchmark over 20 baselines across various model types. We
introduce a new metric to quantify shifts in correlation structures and show
that existing methods struggle on our dataset, which exhibits more complex and
non-stationary correlation dynamics. Our findings highlight key limitations of
current methods and offer a strong empirical basis for building more robust
forecasting models

</details>


### [300] [Smoothed Online Optimization for Target Tracking: Robust and Learning-Augmented Algorithms](https://arxiv.org/abs/2509.05930)
*Ali Zeynali,Mahsa Sahebdel,Qingsong Liu,Mohammad Hajiesmaili,Ramesh K. Sitaraman*

Main category: cs.LG

TL;DR: The paper introduces a new problem called Smoothed Online Optimization for Target Tracking (SOOTT), proposes two algorithms (BEST and CoRT) for solving it, and examines its application in workload scheduling.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address practical challenges in online decision-making under uncertainty by integrating tracking cost, adversarial perturbation cost, and switching cost into a unified framework.

Method: The authors present BEST, a robust algorithm with provable guarantees, and CoRT, a learning-augmented version that incorporates potentially inaccurate black-box predictions to improve decision-making.

Result: Theoretical and case study validations show that CoRT outperforms BEST when predictions are accurate and maintains robustness under errors, while both algorithms balance trajectory tracking, smoothness, and resilience effectively.

Conclusion: BEST and CoRT offer scalable and robust solutions for SOOTT problems, providing theoretical improvements and practical feasibility in domains like workload scheduling.

Abstract: We introduce the Smoothed Online Optimization for Target Tracking (SOOTT)
problem, a new framework that integrates three key objectives in online
decision-making under uncertainty: (1) tracking cost for following a
dynamically moving target, (2) adversarial perturbation cost for withstanding
unpredictable disturbances, and (3) switching cost for penalizing abrupt
changes in decisions. This formulation captures real-world scenarios such as
elastic and inelastic workload scheduling in AI clusters, where operators must
balance long-term service-level agreements (e.g., LLM training) against sudden
demand spikes (e.g., real-time inference). We first present BEST, a robust
algorithm with provable competitive guarantees for SOOTT. To enhance practical
performance, we introduce CoRT, a learning-augmented variant that incorporates
untrusted black-box predictions (e.g., from ML models) into its decision
process. Our theoretical analysis shows that CoRT strictly improves over BEST
when predictions are accurate, while maintaining robustness under arbitrary
prediction errors. We validate our approach through a case study on workload
scheduling, demonstrating that both algorithms effectively balance trajectory
tracking, decision smoothness, and resilience to external disturbances.

</details>


### [301] [If generative AI is the answer, what is the question?](https://arxiv.org/abs/2509.06120)
*Ambuj Tewari*

Main category: cs.LG

TL;DR: The paper explores generative AI across various domains, surveys major models, and introduces frameworks focusing on generation as a task.


<details>
  <summary>Details</summary>
Motivation: Examining generative AI to understand its scope and propose systematic frameworks for advancements and socially responsible development.

Method: Surveys five generative models; introduces probabilistic and game-theoretic frameworks; discusses deployment strategies and social considerations.

Result: Proposed distinguished frameworks and insights into generative AI modeling and deployment, with social responsibility topics explored.

Conclusion: Highlights generation as a distinct ML task and calls for a balanced approach between technical innovation and societal impact.

Abstract: Beginning with text and images, generative AI has expanded to audio, video,
computer code, and molecules. Yet, if generative AI is the answer, what is the
question? We explore the foundations of generation as a distinct machine
learning task with connections to prediction, compression, and decision-making.
We survey five major generative model families: autoregressive models,
variational autoencoders, normalizing flows, generative adversarial networks,
and diffusion models. We then introduce a probabilistic framework that
emphasizes the distinction between density estimation and generation. We review
a game-theoretic framework with a two-player adversary-learner setup to study
generation. We discuss post-training modifications that prepare generative
models for deployment. We end by highlighting some important topics in socially
responsible generation such as privacy, detection of AI-generated content, and
copyright and IP. We adopt a task-first framing of generation, focusing on what
generation is as a machine learning problem, rather than only on how models
implement it.

</details>


### [302] [Select, then Balance: A Plug-and-Play Framework for Exogenous-Aware Spatio-Temporal Forecasting](https://arxiv.org/abs/2509.05779)
*Wei Chen,Yuqian Wu,Yuanshao Zhu,Xixuan Hao,Shiyu Wang,Yuxuan Liang*

Main category: cs.LG

TL;DR: The paper addresses limitations in spatio-temporal forecasting by proposing a new framework, \model, which effectively leverages exogenous variables using a 'select, then balance' approach for better prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve spatio-temporal forecasting by addressing issues related to inconsistent and imbalanced effects of various exogenous variables in dynamic systems.

Method: The method introduces \model, which includes a latent space gated expert module for signal selection and a siamese network for dynamic pattern modeling using both historical and future variables.

Result: Experiments on real-world datasets show that \model improves forecasting performance in terms of accuracy, generality, robustness, and efficiency.

Conclusion: The proposed framework is effective in dynamically modeling exogenous variables to enhance spatio-temporal forecasting performance.

Abstract: Spatio-temporal forecasting aims to predict the future state of dynamic
systems and plays an important role in multiple fields. However, existing
solutions only focus on modeling using a limited number of observed target
variables. In real-world scenarios, exogenous variables can be integrated into
the model as additional input features and associated with the target signal to
promote forecast accuracy. Although promising, this still encounters two
challenges: the inconsistent effects of different exogenous variables to the
target system, and the imbalance effects between historical variables and
future variables. To address these challenges, this paper introduces \model, a
novel framework for modeling \underline{exo}genous variables in
\underline{s}patio-\underline{t}emporal forecasting, which follows a ``select,
then balance'' paradigm. Specifically, we first construct a latent space gated
expert module, where fused exogenous information is projected into a latent
space to dynamically select and recompose salient signals via specialized
sub-experts. Furthermore, we design a siamese network architecture in which
recomposed representations of past and future exogenous variables are fed into
dual-branch spatio-temporal backbones to capture dynamic patterns. The outputs
are integrated through a context-aware weighting mechanism to achieve dynamic
balance during the modeling process. Extensive experiments on real-world
datasets demonstrate the effectiveness, generality, robustness, and efficiency
of our proposed framework.

</details>


### [303] [Data-Efficient Time-Dependent PDE Surrogates: Graph Neural Simulators vs Neural Operators](https://arxiv.org/abs/2509.06154)
*Dibyajyoti Nayak,Somdatta Goswami*

Main category: cs.LG

TL;DR: The paper proposes using Graph Neural Simulators (GNS) with numerical time-stepping to model PDEs, achieving high accuracy and data efficiency compared to traditional neural operators. It reduces error accumulation and provides consistent performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing Neural Operators (NOs), which require large datasets and fail to explicitly incorporate the time-causal, local structure of physical systems.

Method: The paper uses a Graph Neural Simulator framework combined with explicit numerical time-stepping to learn instantaneous time derivatives of PDEs. It incorporates a PCA+KMeans strategy for selecting training samples.

Result: GNS demonstrated significantly better data efficiency and accuracy, achieving under 1% relative L2 errors with just 3% of training data and reducing error accumulation by 82.48% to 99.86% compared to FNO AR and DON AR in three different PDE systems.

Conclusion: Graph-based inductive biases coupled with time integrators provide a promising, scalable approach for accurate and data-efficient PDE modeling. The method generalizes well in low-data scenarios and ensures physical consistency.

Abstract: Neural operators (NOs) approximate mappings between infinite-dimensional
function spaces but require large datasets and struggle with scarce training
data. Many NO formulations don't explicitly encode causal, local-in-time
structure of physical evolution. While autoregressive models preserve causality
by predicting next time-steps, they suffer from rapid error accumulation. We
employ Graph Neural Simulators (GNS) - a message-passing graph neural network
framework - with explicit numerical time-stepping schemes to construct accurate
forward models that learn PDE solutions by modeling instantaneous time
derivatives. We evaluate our framework on three canonical PDE systems: (1) 2D
Burgers' scalar equation, (2) 2D coupled Burgers' vector equation, and (3) 2D
Allen-Cahn equation. Rigorous evaluations demonstrate GNS significantly
improves data efficiency, achieving higher generalization accuracy with
substantially fewer training trajectories compared to neural operator baselines
like DeepONet and FNO. GNS consistently achieves under 1% relative L2 errors
with only 30 training samples out of 1000 (3% of available data) across all
three PDE systems. It substantially reduces error accumulation over extended
temporal horizons: averaged across all cases, GNS reduces autoregressive error
by 82.48% relative to FNO AR and 99.86% relative to DON AR. We introduce a
PCA+KMeans trajectory selection strategy enhancing low-data performance.
Results indicate combining graph-based local inductive biases with conventional
time integrators yields accurate, physically consistent, and scalable surrogate
models for time-dependent PDEs.

</details>


### [304] [time2time: Causal Intervention in Hidden States to Simulate Rare Events in Time Series Foundation Models](https://arxiv.org/abs/2509.05801)
*Debdeep Sanyal,Aaryan Nagpal,Dhruv Kumar,Murari Mandal,Saurabh Deshpande*

Main category: cs.LG

TL;DR: This paper demonstrates that transformer-based models encode semantically grounded representations useful for forecasting and manipulating high-stakes events like market crashes through a proposed technique called activation transplantation.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to determine whether transformer-based models internalize semantic concepts, such as market regimes, or merely fit patterns, and whether these representations can be manipulated to simulate high-stakes events.

Method: The authors propose a method called activation transplantation, where hidden states are manipulated by imposing statistical moments of one event (e.g., a crash) onto another during the forward pass to steer forecasts deterministically.

Result: The study shows that injection of event semantics steers model predictions effectively, such as inducing crash predictions by injecting crash semantics. Models encode event severity in a graded manner, with latent vector norms correlating with systemic shock magnitudes.

Conclusion: The findings highlight the existence of a latent concept space in time-series transformers that allows for causally-driven intervention and 'what-if' analysis, advancing interpretability and strategic stress testing.

Abstract: While transformer-based foundation models excel at forecasting routine
patterns, two questions remain: do they internalize semantic concepts such as
market regimes, or merely fit curves? And can their internal representations be
leveraged to simulate rare, high-stakes events such as market crashes? To
investigate this, we introduce activation transplantation, a causal
intervention that manipulates hidden states by imposing the statistical moments
of one event (e.g., a historical crash) onto another (e.g., a calm period)
during the forward pass. This procedure deterministically steers forecasts:
injecting crash semantics induces downturn predictions, while injecting calm
semantics suppresses crashes and restores stability. Beyond binary control, we
find that models encode a graded notion of event severity, with the latent
vector norm directly correlating with the magnitude of systemic shocks.
Validated across two architecturally distinct TSFMs, Toto (decoder only) and
Chronos (encoder-decoder), our results demonstrate that steerable, semantically
grounded representations are a robust property of large time series
transformers. Our findings provide evidence for a latent concept space that
governs model predictions, shifting interpretability from post-hoc attribution
to direct causal intervention, and enabling semantic "what-if" analysis for
strategic stress-testing.

</details>


### [305] [Concolic Testing on Individual Fairness of Neural Network Models](https://arxiv.org/abs/2509.06864)
*Ming-I Huang,Chih-Duo Hong,Fang Yu*

Main category: cs.LG

TL;DR: PyFair is a tool that evaluates and verifies individual fairness in Deep Neural Networks by generating fairness-specific path constraints.


<details>
  <summary>Details</summary>
Motivation: Address the need for systematic evaluation and verification of fairness in Deep Neural Networks.

Method: PyFair utilizes a concolic testing tool, PyCT, to create fairness-specific path constraints, paired with a dual network architecture for comprehensive assessments.

Result: Tested on 25 benchmark models, PyFair successfully detects discriminative instances, verifies fairness, and highlights scalability issues in complex models.

Conclusion: PyFair represents a significant advance in algorithmic fairness testing, offering a rigorous approach to evaluating pre-trained DNNs.

Abstract: This paper introduces PyFair, a formal framework for evaluating and verifying
individual fairness of Deep Neural Networks (DNNs). By adapting the concolic
testing tool PyCT, we generate fairness-specific path constraints to
systematically explore DNN behaviors. Our key innovation is a dual network
architecture that enables comprehensive fairness assessments and provides
completeness guarantees for certain network types. We evaluate PyFair on 25
benchmark models, including those enhanced by existing bias mitigation
techniques. Results demonstrate PyFair's efficacy in detecting discriminatory
instances and verifying fairness, while also revealing scalability challenges
for complex models. This work advances algorithmic fairness in critical domains
by offering a rigorous, systematic method for fairness testing and verification
of pre-trained DNNs.

</details>


### [306] [Toward a Metrology for Artificial Intelligence: Hidden-Rule Environments and Reinforcement Learning](https://arxiv.org/abs/2509.06213)
*Christo Mathew,Wentian Wang,Lazaros Gallos,Paul Kantor,Vladimir Menkov,Hao Wang*

Main category: cs.LG

TL;DR: This paper studies reinforcement learning in the Game of Hidden Rules (GOHR), focusing on rule inference and optimal decision-making using Feature-Centric and Object-Centric representations with a Transformer-based A2C algorithm.


<details>
  <summary>Details</summary>
Motivation: Understanding how agents can learn and adapt to hidden rules in dynamic environments with only partial observations.

Method: Employs Feature-Centric and Object-Centric state representations combined with a Transformer-based Advantage Actor-Critic (A2C) algorithm in the GOHR puzzle environment.

Result: The study evaluates the models on various setups, assessing their transfer abilities and the impact of state representation on learning efficiency.

Conclusion: Representation strategies significantly influence learning efficiency, offering insights into how agents thrive in environments with partial observations and hidden rules.

Abstract: We investigate reinforcement learning in the Game Of Hidden Rules (GOHR)
environment, a complex puzzle in which an agent must infer and execute hidden
rules to clear a 6$\times$6 board by placing game pieces into buckets. We
explore two state representation strategies, namely Feature-Centric (FC) and
Object-Centric (OC), and employ a Transformer-based Advantage Actor-Critic
(A2C) algorithm for training. The agent has access only to partial observations
and must simultaneously infer the governing rule and learn the optimal policy
through experience. We evaluate our models across multiple rule-based and
trial-list-based experimental setups, analyzing transfer effects and the impact
of representation on learning efficiency.

</details>


### [307] [Simple Optimizers for Convex Aligned Multi-Objective Optimization](https://arxiv.org/abs/2509.05811)
*Ben Kretzu,Karen Ullrich,Yonathan Efroni*

Main category: cs.LG

TL;DR: This paper generalizes gradient-based algorithms for Aligned Multi-Objective Optimization (AMOO) by replacing strong convexity assumptions with more practical conditions like smoothness or Lipschitz continuity. It also proposes scalable algorithms with proven convergence guarantees and provides a novel proof showing the limitations of equal-weight approaches.


<details>
  <summary>Details</summary>
Motivation: To improve task performance in machine learning by effectively leveraging Aligned Multi-Objective Optimization (AMOO) under less restrictive and more realistic assumptions than strong convexity.

Method: The authors relax strong convexity assumptions and introduce smoothness or Lipschitz continuity for analyzing convex AMOO. They use new analytical tools and metrics to establish convergence and design scalable algorithms. They also derive a lower bound to highlight the inefficiency of naive equal-weight methods.

Result: Scalable gradient-descent algorithms for convex AMOO were developed. The theoretical analysis proves their convergence under more realistic assumptions, and a lower bound indicates the suboptimality of simple equal-weight optimization strategies.

Conclusion: The study advances the AMOO framework by making it applicable to real-world conditions in deep learning, enhancing its theoretical foundations and offering practical, efficient solutions.

Abstract: It is widely recognized in modern machine learning practice that access to a
diverse set of tasks can enhance performance across those tasks. This
observation suggests that, unlike in general multi-objective optimization, the
objectives in many real-world settings may not be inherently conflicting. To
address this, prior work introduced the Aligned Multi-Objective Optimization
(AMOO) framework and proposed gradient-based algorithms with provable
convergence guarantees. However, existing analysis relies on strong
assumptions, particularly strong convexity, which implies the existence of a
unique optimal solution. In this work, we relax this assumption and study
gradient-descent algorithms for convex AMOO under standard smoothness or
Lipschitz continuity conditions-assumptions more consistent with those used in
deep learning practice. This generalization requires new analytical tools and
metrics to characterize convergence in the convex AMOO setting. We develop such
tools, propose scalable algorithms for convex AMOO, and establish their
convergence guarantees. Additionally, we prove a novel lower bound that
demonstrates the suboptimality of naive equal-weight approaches compared to our
methods.

</details>


### [308] [Performance of Conformal Prediction in Capturing Aleatoric Uncertainty](https://arxiv.org/abs/2509.05826)
*Misgina Tsighe Hagos,Claes Lundström*

Main category: cs.LG

TL;DR: This paper investigates the correlation between conformal prediction set sizes and human annotations to evaluate their effectiveness in quantifying aleatoric uncertainty, revealing a weak performance across most methods.


<details>
  <summary>Details</summary>
Motivation: To address the lack of evidence in validating conformal predictors' capability of capturing aleatoric uncertainty caused by overlapping class annotations.

Method: The authors measured correlation between conformal prediction set sizes and the number of distinct human-labeled annotations per instance, and assessed the similarity between prediction sets and annotations using eight deep learning models and four datasets.

Result: The study shows conformal prediction set sizes mostly exhibit weak correlation with human annotations, indicating limited effectiveness in capturing aleatoric uncertainty.

Conclusion: Conformal predictors can offer higher class coverage but do not effectively quantify aleatoric uncertainty, necessitating critical reassessment of their outputs.

Abstract: Conformal prediction is a model-agnostic approach to generating prediction
sets that cover the true class with a high probability. Although its prediction
set size is expected to capture aleatoric uncertainty, there is a lack of
evidence regarding its effectiveness. The literature presents that prediction
set size can upper-bound aleatoric uncertainty or that prediction sets are
larger for difficult instances and smaller for easy ones, but a validation of
this attribute of conformal predictors is missing. This work investigates how
effectively conformal predictors quantify aleatoric uncertainty, specifically
the inherent ambiguity in datasets caused by overlapping classes. We perform
this by measuring the correlation between prediction set sizes and the number
of distinct labels assigned by human annotators per instance. We further assess
the similarity between prediction sets and human-provided annotations. We use
three conformal prediction approaches to generate prediction sets for eight
deep learning models trained on four datasets. The datasets contain annotations
from multiple human annotators (ranging from five to fifty participants) per
instance, enabling the identification of class overlap. We show that the vast
majority of the conformal prediction outputs show a very weak to weak
correlation with human annotations, with only a few showing moderate
correlation. These findings underscore the necessity of critically reassessing
the prediction sets generated using conformal predictors. While they can
provide a higher coverage of the true classes, their capability in capturing
aleatoric uncertainty remains limited.

</details>


### [309] [On optimal solutions of classical and sliced Wasserstein GANs with non-Gaussian data](https://arxiv.org/abs/2509.06505)
*Yu-Jui Huang,Hsin-Hua Shen,Yu-Chih Huang,Wan-Yi Lin,Shih-Chun Lin*

Main category: cs.LG

TL;DR: This paper focuses on optimizing WGAN's parameters beyond conventional settings by deriving closed-form solutions for one-dimensional WGANs and proposing an advantageous approach for high-dimensional cases using the sliced Wasserstein framework.


<details>
  <summary>Details</summary>
Motivation: While GANs are versatile and widely applied, there is a lack of efficient and theoretically optimal methods for parameter selection, particularly for WGANs in non-LQG settings.

Method: The authors derive closed-form optimal parameters for one-dimensional WGANs under non-linear activation functions and non-Gaussian data. For high-dimensional cases, they utilize the sliced Wasserstein framework with a new constraint on the joint distribution of unprojected data.

Result: They demonstrate that the linear generator can be asymptotically optimal for sliced WGANs with non-Gaussian data. Empirical tests confirm that their parameters converge well for Gaussian and Laplace distributions, and their approach is computationally more efficient than r-PCA.

Conclusion: The paper provides an effective solution for optimizing WGAN parameters in non-conventional settings, balancing performance and computational efficiency through the sliced Wasserstein framework.

Abstract: The generative adversarial network (GAN) aims to approximate an unknown
distribution via a parameterized neural network (NN). While GANs have been
widely applied in reinforcement and semisupervised learning as well as computer
vision tasks, selecting their parameters often needs an exhaustive search and
only a few selection methods can be proved to be theoretically optimal. One of
the most promising GAN variants is the Wasserstein GAN (WGAN). Prior work on
optimal parameters for WGAN is limited to the linear-quadratic-Gaussian (LQG)
setting, where the NN is linear and the data is Gaussian. In this paper, we
focus on the characterization of optimal WGAN parameters beyond the LQG
setting. We derive closed-form optimal parameters for one-dimensional WGANs
when the NN has non-linear activation functions and the data is non-Gaussian.
To extend this to high-dimensional WGANs, we adopt the sliced Wasserstein
framework and replace the constraint on marginal distributions of the randomly
projected data by a constraint on the joint distribution of the original
(unprojected) data. We show that the linear generator can be asymptotically
optimal for sliced WGAN with non-Gaussian data. Empirical studies show that our
closed-form WGAN parameters have good convergence behavior with data under both
Gaussian and Laplace distributions. Also, compared to the r principal component
analysis (r-PCA) solution, our proposed solution for sliced WGAN can achieve
the same performance while requiring less computational resources.

</details>


### [310] [Finetuning LLMs for Human Behavior Prediction in Social Science Experiments](https://arxiv.org/abs/2509.05830)
*Akaash Kolluri,Shengguang Wu,Joon Sung Park,Michael S. Bernstein*

Main category: cs.LG

TL;DR: The paper explores finetuning large language models (LLMs) on a dataset of individual-level responses from social science experiments, demonstrating improved accuracy for simulating human behavior in experiments.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of accurately simulating social science experiments using large language models by leveraging individual-level responses from past experiments.

Method: The authors create a dataset called SocSci210, containing 2.9 million responses from 210 open-source social science experiments, and finetune LLMs on this data to improve their predictive capabilities.

Result: The finetuned model, Socrates-Qwen-14B, improved prediction accuracy by 26% compared to its base model and 13% compared to GPT-4o in unseen studies. Generalization to new conditions was robust, showing a 71% improvement, and demographic bias was reduced by 10.6%.

Conclusion: Finetuning LLMs on rich, topic-specific social science datasets enhances their utility for experimental hypothesis screening, offering broader applications in social science research.

Abstract: Large language models (LLMs) offer a powerful opportunity to simulate the
results of social science experiments. In this work, we demonstrate that
finetuning LLMs directly on individual-level responses from past experiments
meaningfully improves the accuracy of such simulations across diverse social
science domains. We construct SocSci210 via an automatic pipeline, a dataset
comprising 2.9 million responses from 400,491 participants in 210 open-source
social science experiments. Through finetuning, we achieve multiple levels of
generalization. In completely unseen studies, our strongest model,
Socrates-Qwen-14B, produces predictions that are 26% more aligned with
distributions of human responses to diverse outcome questions under varying
conditions relative to its base model (Qwen2.5-14B), outperforming GPT-4o by
13%. By finetuning on a subset of conditions in a study, generalization to new
unseen conditions is particularly robust, improving by 71%. Since SocSci210
contains rich demographic information, we reduce demographic parity, a measure
of bias, by 10.6% through finetuning. Because social sciences routinely
generate rich, topic-specific datasets, our findings indicate that finetuning
on such data could enable more accurate simulations for experimental hypothesis
screening. We release our data, models and finetuning code at
stanfordhci.github.io/socrates.

</details>


### [311] [Benchmarking Robust Aggregation in Decentralized Gradient Marketplaces](https://arxiv.org/abs/2509.05833)
*Zeyu Song,Sainyam Galhotra,Shagufta Mehnaz*

Main category: cs.LG

TL;DR: The paper introduces a benchmark framework to assess decentralized gradient marketplaces in privacy-preserving machine learning, focusing on cost-effectiveness, fairness, and stability.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks in Federated Learning fail to address economic and systemic aspects critical to decentralized marketplaces, such as fairness and stability.

Method: The authors created a simulation environment, developed evaluation metrics for marketplace factors, analyzed alternative aggregation methods, and provided insights into trade-offs in the system.

Result: An empirical analysis of MartFL and adapted methods like FLTrust and SkyMask revealed marketplace dynamics under attacks, providing comprehensive benchmarks.

Conclusion: The benchmark aids in the design and evaluation of decentralized gradient marketplaces, ensuring robustness, fairness, and economic viability.

Abstract: The rise of distributed and privacy-preserving machine learning has sparked
interest in decentralized gradient marketplaces, where participants trade
intermediate artifacts like gradients. However, existing Federated Learning
(FL) benchmarks overlook critical economic and systemic factors unique to such
marketplaces-cost-effectiveness, fairness to sellers, and market
stability-especially when a buyer relies on a private baseline dataset for
evaluation.
  We introduce a comprehensive benchmark framework to holistically evaluate
robust gradient aggregation methods within these buyer-baseline-reliant
marketplaces. Our contributions include: (1) a simulation environment modeling
marketplace dynamics with a variable buyer baseline and diverse seller
distributions; (2) an evaluation methodology augmenting standard FL metrics
with marketplace-centric dimensions such as Economic Efficiency, Fairness, and
Selection Dynamics; (3) an in-depth empirical analysis of the existing
Distributed Gradient Marketplace framework, MartFL, including the integration
and comparative evaluation of adapted FLTrust and SkyMask as alternative
aggregation strategies within it. This benchmark spans diverse datasets, local
attacks, and Sybil attacks targeting the marketplace selection process; and (4)
actionable insights into the trade-offs between model performance, robustness,
cost, fairness, and stability.
  This benchmark equips the community with essential tools and empirical
evidence to evaluate and design more robust, equitable, and economically viable
decentralized gradient marketplaces.

</details>


### [312] [Not All Samples Are Equal: Quantifying Instance-level Difficulty in Targeted Data Poisoning](https://arxiv.org/abs/2509.06896)
*William Xu,Yiwei Lu,Yihan Wang,Matthew Y. R. Yang,Zuoqiu Liu,Gautam Kamath,Yaoliang Yu*

Main category: cs.LG

TL;DR: The paper studies factors that make certain test samples more vulnerable to targeted data poisoning attacks, introducing three predictive criteria for assessing attack difficulty.


<details>
  <summary>Details</summary>
Motivation: To understand why certain test instances are more prone to successful targeted data poisoning attacks and provide insights into assessing vulnerability.

Method: The authors propose and analyze three criteria—ergodic prediction accuracy, poison distance, and poison budget—through experimental evaluations.

Result: The study demonstrates that these criteria can effectively predict the difficulty of targeted attacks in diverse real-world scenarios.

Conclusion: The findings offer valuable tools for understanding and assessing vulnerabilities in classification models against targeted data poisoning attacks.

Abstract: Targeted data poisoning attacks pose an increasingly serious threat due to
their ease of deployment and high success rates. These attacks aim to
manipulate the prediction for a single test sample in classification models.
Unlike indiscriminate attacks that aim to decrease overall test performance,
targeted attacks present a unique threat to individual test instances. This
threat model raises a fundamental question: what factors make certain test
samples more susceptible to successful poisoning than others? We investigate
how attack difficulty varies across different test instances and identify key
characteristics that influence vulnerability. This paper introduces three
predictive criteria for targeted data poisoning difficulty: ergodic prediction
accuracy (analyzed through clean training dynamics), poison distance, and
poison budget. Our experimental results demonstrate that these metrics
effectively predict the varying difficulty of real-world targeted poisoning
attacks across diverse scenarios, offering practitioners valuable insights for
vulnerability assessment and understanding data poisoning attacks.

</details>


### [313] [Data-Driven Stochastic Modeling Using Autoregressive Sequence Models: Translating Event Tables to Queueing Dynamics](https://arxiv.org/abs/2509.05839)
*Daksh Mittal,Shunri Zheng,Jing Dong,Hongseok Namkoong*

Main category: cs.LG

TL;DR: The paper presents a data-driven framework using Transformer-style models to automate queueing network modeling and simulation based on event-stream data.


<details>
  <summary>Details</summary>
Motivation: Current queueing network models require substantial human effort and domain expertise, limiting scalability and accessibility.

Method: The proposed framework uses autoregressive sequence models to learn the conditional distributions of event types and event times, avoiding the need for explicitly defined processes.

Result: Validation on diverse queueing networks demonstrates the framework's utility for simulation, uncertainty quantification, and counterfactual evaluation.

Conclusion: The framework leverages AI to automate and democratize queueing network modeling, enabling broader adoption across service domains.

Abstract: While queueing network models are powerful tools for analyzing service
systems, they traditionally require substantial human effort and domain
expertise to construct. To make this modeling approach more scalable and
accessible, we propose a data-driven framework for queueing network modeling
and simulation based on autoregressive sequence models trained on event-stream
data. Instead of explicitly specifying arrival processes, service mechanisms,
or routing logic, our approach learns the conditional distributions of event
types and event times, recasting the modeling task as a problem of sequence
distribution learning. We show that Transformer-style architectures can
effectively parameterize these distributions, enabling automated construction
of high-fidelity simulators. As a proof of concept, we validate our framework
on event tables generated from diverse queueing networks, showcasing its
utility in simulation, uncertainty quantification, and counterfactual
evaluation. Leveraging advances in artificial intelligence and the growing
availability of data, our framework takes a step toward more automated,
data-driven modeling pipelines to support broader adoption of queueing network
models across service domains.

</details>


### [314] [Learning to Construct Knowledge through Sparse Reference Selection with Reinforcement Learning](https://arxiv.org/abs/2509.05874)
*Shao-An Yin*

Main category: cs.LG

TL;DR: A Deep Reinforcement Learning framework to select sparse references for prioritizing reading, aiding knowledge construction from limited information.


<details>
  <summary>Details</summary>
Motivation: The expansion of scientific literature increases the challenge of acquiring knowledge due to restricted access and sparse relevant references.

Method: Deep Reinforcement Learning framework to simulate human-like knowledge construction by selecting key papers with limited data access.

Result: The framework successfully demonstrated effective knowledge construction from partial information in drug--gene relation discovery.

Conclusion: Humans and machines can effectively construct knowledge with limited information through the proposed system, optimizing literature review efforts.

Abstract: The rapid expansion of scientific literature makes it increasingly difficult
to acquire new knowledge, particularly in specialized domains where reasoning
is complex, full-text access is restricted, and target references are sparse
among a large set of candidates. We present a Deep Reinforcement Learning
framework for sparse reference selection that emulates human knowledge
construction, prioritizing which papers to read under limited time and cost.
Evaluated on drug--gene relation discovery with access restricted to titles and
abstracts, our approach demonstrates that both humans and machines can
construct knowledge effectively from partial information.

</details>


### [315] [SPINN: An Optimal Self-Supervised Physics-Informed Neural Network Framework](https://arxiv.org/abs/2509.05886)
*Reza Pirayeshshirazinezhad*

Main category: cs.LG

TL;DR: The paper develops surrogate models (including machine learning approaches and a physics-informed neural network) to predict the convective heat transfer coefficient for liquid sodium in rectangular miniature heat sinks.


<details>
  <summary>Details</summary>
Motivation: To provide an efficient and accurate alternative to computationally expensive CFD analysis for turbulent forced convection of liquid sodium in miniature heat sinks.

Method: The study applies kernel-based machine learning, shallow neural networks, a self-supervised physics-informed neural network with uncertainty weighting, and transfer learning from water data to sodium data.

Result: Validation shows that the self-supervised physics-informed model estimates the heat transfer coefficient for liquid sodium with an error margin of ~8%, while physics-only regressions have errors between 5%-10%. Other ML methods maintain prediction errors mostly within +8%.

Conclusion: The proposed machine learning models, notably the physics-informed approach, provide a viable alternative for high-fidelity modeling in the design and optimization of liquid-metal-cooled miniature heat sinks.

Abstract: A surrogate model is developed to predict the convective heat transfer
coefficient of liquid sodium (Na) flow within rectangular miniature heat sinks.
Initially, kernel-based machine learning techniques and shallow neural network
are applied to a dataset with 87 Nusselt numbers for liquid sodium in
rectangular miniature heat sinks. Subsequently, a self-supervised
physics-informed neural network and transfer learning approach are used to
increase the estimation performance. In the self-supervised physics-informed
neural network, an additional layer determines the weight the of physics in the
loss function to balance data and physics based on their uncertainty for a
better estimation. For transfer learning, a shallow neural network trained on
water is adapted for use with Na. Validation results show that the
self-supervised physics-informed neural network successfully estimate the heat
transfer rates of Na with an error margin of approximately +8%. Using only
physics for regression, the error remains between 5% to 10%. Other machine
learning methods specify the prediction mostly within +8%. High-fidelity
modeling of turbulent forced convection of liquid metals using computational
fluid dynamics (CFD) is both time-consuming and computationally expensive.
Therefore, machine learning based models offer a powerful alternative tool for
the design and optimization of liquid-metal-cooled miniature heat sinks.

</details>


### [316] [X-SQL: Expert Schema Linking and Understanding of Text-to-SQL with Multi-LLMs](https://arxiv.org/abs/2509.05899)
*Dazhi Peng*

Main category: cs.LG

TL;DR: The paper introduces X-SQL, a Text-to-SQL framework leveraging database schema information and multi-LLMs, achieving high performance on Spider datasets.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked significance of database schema information in Text-to-SQL tasks and enhance SQL query generation quality using LLMs.

Method: The proposed X-SQL framework introduces two components: X-Linking (LLM Supervised Fine-tuning for Schema Linking) and X-Admin (bridging schema information and natural language queries), alongside the use of Multi-LLMs for performance optimization.

Result: X-SQL achieves Execution Accuracies of 84.9% on Spider-Dev and 82.5% on Spider-Test datasets, setting a benchmark for open-source Text-to-SQL models.

Conclusion: The X-SQL framework demonstrates the critical role of schema information, combined with advanced LLM strategies, in delivering state-of-the-art Text-to-SQL performance.

Abstract: With Large Language Models' (LLMs) emergent abilities on code generation
tasks, Text-to-SQL has become one of the most popular downstream applications.
Despite the strong results of multiple recent LLM-based Text-to-SQL frameworks,
the research community often overlooks the importance of database schema
information for generating high-quality SQL queries. We find that such schema
information plays a significant or even dominant role in the Text-to-SQL task.
To tackle this challenge, we propose a novel database schema expert with two
components. We first introduce X-Linking, an LLM Supervised Finetuning
(SFT)-based method that achieves superior Schema Linking results compared to
existing open-source Text-to-SQL methods. In addition, we innovatively propose
an X-Admin component that focuses on Schema Understanding by bridging the gap
between abstract schema information and the user's natural language question.
Aside from better learning with schema information, we experiment with
Multi-LLMs for different components within the system to further boost its
performance. By incorporating these techniques into our end-to-end framework,
X-SQL, we have achieved Execution Accuracies of 84.9% on the Spider-Dev dataset
and 82.5% on the Spider-Test dataset. This outstanding performance establishes
X-SQL as the leading Text-to-SQL framework based on open-source models.

</details>


### [317] [Unified Interaction Foundational Model (UIFM) for Predicting Complex User and System Behavior](https://arxiv.org/abs/2509.06025)
*Vignesh Ethiraj,Subhash Talluri*

Main category: cs.LG

TL;DR: The paper introduces the Unified Interaction Foundation Model (UIFM), a system aimed at understanding complex, evolving events in structured domains by overcoming the limitations of current models that rely on text serialization.


<details>
  <summary>Details</summary>
Motivation: Current foundation models struggle to accurately grasp the holistic and structured nature of interactions in domains like telecommunications, finance, and e-commerce. Serialization into text fragments loses essential contextual information required for understanding user behavior.

Method: The UIFM utilizes composite tokenization, treating each multi-attribute event as a unified, coherent unit. This approach allows the model to learn the "grammar" of user behavior and understand entire interactions holistically.

Result: UIFM is demonstrated to be more accurate in behavioral understanding compared to traditional methods, suggesting improved adaptability and intelligence in predictive systems.

Conclusion: The research provides a significant step forward in creating foundational models capable of genuine behavioral understanding by considering structured interactions as semantically unified events.

Abstract: A central goal of artificial intelligence is to build systems that can
understand and predict complex, evolving sequences of events. However, current
foundation models, designed for natural language, fail to grasp the holistic
nature of structured interactions found in domains like telecommunications,
e-commerce and finance. By serializing events into text, they disassemble them
into semantically fragmented parts, losing critical context. In this work, we
introduce the Unified Interaction Foundation Model (UIFM), a foundation model
engineered for genuine behavioral understanding. At its core is the principle
of composite tokenization, where each multi-attribute event is treated as a
single, semantically coherent unit. This allows UIFM to learn the underlying
"grammar" of user behavior, perceiving entire interactions rather than a
disconnected stream of data points. We demonstrate that this architecture is
not just more accurate, but represents a fundamental step towards creating more
adaptable and intelligent predictive systems.

</details>


### [318] [PolicyEvolve: Evolving Programmatic Policies by LLMs for multi-player games via Population-Based Training](https://arxiv.org/abs/2509.06053)
*Mingrui Lv,Hangzhi Liu,Zhi Luo,Hongjie Zhang,Jie Ou*

Main category: cs.LG

TL;DR: Researchers propose PolicyEvolve, a framework for generating efficient and interpretable programmatic policies for multi-agent reinforcement learning tasks, leveraging a modular approach to achieve high performance with minimal interactions.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges of high computational costs, inefficiency, and lack of interpretability in adversarial policies generated by traditional multi-agent reinforcement learning methods.

Method: PolicyEvolve consists of four modules: Global Pool and Local Pool for policy storage, a Policy Planner for iterative policy generation and refinement, and a Trajectory Critic for analyzing data to improve policies. The system promotes an iterative process for developing high-quality programmatic policies.

Result: PolicyEvolve achieves high-performance policies with fewer environmental interactions compared to traditional methods, while simultaneously maintaining interpretable and efficient rule-based code.

Conclusion: PolicyEvolve demonstrates a novel approach to bridging the gap between efficiency, effectiveness, and interpretability in multi-agent systems, offering a viable alternative to existing reinforcement learning frameworks.

Abstract: Multi-agent reinforcement learning (MARL) has achieved significant progress
in solving complex multi-player games through self-play. However, training
effective adversarial policies requires millions of experience samples and
substantial computational resources. Moreover, these policies lack
interpretability, hindering their practical deployment. Recently, researchers
have successfully leveraged Large Language Models (LLMs) to generate
programmatic policies for single-agent tasks, transforming neural network-based
policies into interpretable rule-based code with high execution efficiency.
Inspired by this, we propose PolicyEvolve, a general framework for generating
programmatic policies in multi-player games. PolicyEvolve significantly reduces
reliance on manually crafted policy code, achieving high-performance policies
with minimal environmental interactions. The framework comprises four modules:
Global Pool, Local Pool, Policy Planner, and Trajectory Critic. The Global Pool
preserves elite policies accumulated during iterative training. The Local Pool
stores temporary policies for the current iteration; only sufficiently
high-performing policies from this pool are promoted to the Global Pool. The
Policy Planner serves as the core policy generation module. It samples the top
three policies from the Global Pool, generates an initial policy for the
current iteration based on environmental information, and refines this policy
using feedback from the Trajectory Critic. Refined policies are then deposited
into the Local Pool. This iterative process continues until the policy achieves
a sufficiently high average win rate against the Global Pool, at which point it
is integrated into the Global Pool. The Trajectory Critic analyzes interaction
data from the current policy, identifies vulnerabilities, and proposes
directional improvements to guide the Policy Planner

</details>


### [319] [A novel biomass fluidized bed gasification model coupled with machine learning and CFD simulation](https://arxiv.org/abs/2509.06056)
*Chun Wang*

Main category: cs.LG

TL;DR: The paper proposes a coupling model combining machine learning and CFD to enhance biomass fluidized bed gasification predictions.


<details>
  <summary>Details</summary>
Motivation: Conventional approaches to modeling biomass gasification struggle with accurately predicting thermochemical reactions and are computationally demanding.

Method: Develop a model using machine learning trained on experimental and simulation data, embedding it into CFD for dynamic updates of reaction rates and compositions.

Result: Achieved improved prediction accuracy and computational efficiency for biomass fluidized bed gasification processes.

Conclusion: Coupling machine learning with CFD significantly enhances gasification modeling, demonstrating its potential in advancing thermochemical reaction analyses.

Abstract: A coupling model of biomass fluidized bed gasification based on machine
learning and computational fluid dynamics is proposed to improve the prediction
accuracy and computational efficiency of complex thermochemical reaction
process. By constructing a high-quality data set based on experimental data and
high fidelity simulation results, the agent model used to describe the
characteristics of reaction kinetics was trained and embedded into the
computational fluid dynamics (CFD) framework to realize the real-time update of
reaction rate and composition evolution.

</details>


### [320] [ARIES: Relation Assessment and Model Recommendation for Deep Time Series Forecasting](https://arxiv.org/abs/2509.06060)
*Fei Wang,Yujie Li,Zezhi Shao,Chengqing Yu,Yisong Fu,Zhulin An,Yongjun Xu,Xueqi Cheng*

Main category: cs.LG

TL;DR: The paper introduces ARIES, a framework that correlates time series properties with deep forecasting models and provides recommendations for real-world applications.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack diverse temporal patterns to assess the link between time series data properties and model performance, and there's no effective model recommendation system, leading to inefficiency.

Method: ARIES constructs synthetic datasets with distinct patterns, calculates time series properties, benchmarks 50+ forecasting models, and develops a recommendation system for model selection.

Result: The study found a clear correlation between time series properties and modeling strategies, enabling the development of a deep forecasting model recommender.

Conclusion: ARIES establishes the relationship between time series properties and modeling strategies, providing interpretable and efficient model recommendations for practical forecasting applications.

Abstract: Recent advancements in deep learning models for time series forecasting have
been significant. These models often leverage fundamental time series
properties such as seasonality and non-stationarity, which may suggest an
intrinsic link between model performance and data properties. However, existing
benchmark datasets fail to offer diverse and well-defined temporal patterns,
restricting the systematic evaluation of such connections. Additionally, there
is no effective model recommendation approach, leading to high time and cost
expenditures when testing different architectures across different downstream
applications. For those reasons, we propose ARIES, a framework for assessing
relation between time series properties and modeling strategies, and for
recommending deep forcasting models for realistic time series. First, we
construct a synthetic dataset with multiple distinct patterns, and design a
comprehensive system to compute the properties of time series. Next, we conduct
an extensive benchmarking of over 50 forecasting models, and establish the
relationship between time series properties and modeling strategies. Our
experimental results reveal a clear correlation. Based on these findings, we
propose the first deep forecasting model recommender, capable of providing
interpretable suggestions for real-world time series. In summary, ARIES is the
first study to establish the relations between the properties of time series
data and modeling strategies, while also implementing a model recommendation
system. The code is available at: https://github.com/blisky-li/ARIES.

</details>


### [321] [A Surrogate model for High Temperature Superconducting Magnets to Predict Current Distribution with Neural Network](https://arxiv.org/abs/2509.06067)
*Mianjun Xiao,Peng Song,Yulong Liu,Cedric Korte,Ziyang Xu,Jiale Gao,Jiaqi Lu,Haoyang Nie,Qiantong Deng,Timing Qu*

Main category: cs.LG

TL;DR: The paper introduces a fully connected residual neural network (FCRN) as a surrogate model for faster and accurate prediction of current density in REBCO solenoids, significantly reducing computational time compared to the finite element method (FEM).


<details>
  <summary>Details</summary>
Motivation: Finite Element Method (FEM) is computationally expensive, especially for large-scale HTS magnets with multi-physics couplings, thus hindering fast design processes.

Method: A surrogate model using Fully Connected Residual Neural Network (FCRN) was trained on datasets from FEM simulations with varying geometrical configurations of REBCO solenoids.

Result: The FCRN outperforms conventional neural networks in convergence and efficiently predicts beyond the training range with errors under 10%, offering a speed-up of several orders of magnitude compared to FEM.

Conclusion: The FCRN-based surrogate model is accurate and highly efficient, making it a practical tool for rapid analysis and design of large-scale high-temperature superconducting magnets.

Abstract: Finite element method (FEM) is widely used in high-temperature
superconducting (HTS) magnets, but its computational cost increases with magnet
size and becomes time-consuming for meter-scale magnets, especially when
multi-physics couplings are considered, which limits the fast design of
large-scale REBCO magnet systems. In this work, a surrogate model based on a
fully connected residual neural network (FCRN) is developed to predict the
space-time current density distribution in REBCO solenoids. Training datasets
were generated from FEM simulations with varying numbers of turns and pancakes.
The results demonstrate that, for deeper networks, the FCRN architecture
achieves better convergence than conventional fully connected network (FCN),
with the configuration of 12 residual blocks and 256 neurons per layer
providing the most favorable balance between training accuracy and
generalization capability. Extrapolation studies show that the model can
reliably predict magnetization losses for up to 50% beyond the training range,
with maximum errors below 10%. The surrogate model achieves predictions several
orders of magnitude faster than FEM and still remains advantageous when
training costs are included. These results indicate that the proposed
FCRN-based surrogate model provides both accuracy and efficiency, offering a
promising tool for the rapid analysis of large-scale HTS magnets.

</details>


### [322] [Teaching Precommitted Agents: Model-Free Policy Evaluation and Control in Quasi-Hyperbolic Discounted MDPs](https://arxiv.org/abs/2509.06094)
*S. R. Eshwar*

Main category: cs.LG

TL;DR: The paper explores Quasi-Hyperbolic (QH) discounting in reinforcement learning, providing theoretical foundations and practical algorithms.


<details>
  <summary>Details</summary>
Motivation: To address the integration gap between Quasi-Hyperbolic discounting and reinforcement learning, and understand time-inconsistent preferences in decision-making.

Method: Characterization of optimal policy as one-step non-stationary, and development of model-free algorithms for policy evaluation and Q-learning with convergence guarantees.

Result: Theoretical proof of policy structure and practical algorithms for QH discounting in RL, with proven convergence.

Conclusion: Establishes foundational insights making QH preferences applicable in the RL framework effectively.

Abstract: Time-inconsistent preferences, where agents favor smaller-sooner over
larger-later rewards, are a key feature of human and animal decision-making.
Quasi-Hyperbolic (QH) discounting provides a simple yet powerful model for this
behavior, but its integration into the reinforcement learning (RL) framework
has been limited. This paper addresses key theoretical and algorithmic gaps for
precommitted agents with QH preferences. We make two primary contributions: (i)
we formally characterize the structure of the optimal policy, proving for the
first time that it reduces to a simple one-step non-stationary form; and (ii)
we design the first practical, model-free algorithms for both policy evaluation
and Q-learning in this setting, both with provable convergence guarantees. Our
results provide foundational insights for incorporating QH preferences in RL.

</details>


### [323] [Tracking daily paths in home contexts with RSSI fingerprinting based on UWB through deep learning models](https://arxiv.org/abs/2509.06161)
*Aurora Polo-Rodríguez,Juan Carlos Valera,Jesús Peral,David Gil,Javier Medina-Quero*

Main category: cs.LG

TL;DR: This paper explores ultra-wideband (UWB) technology combined with deep learning to improve indoor human activity tracking, achieving high-location accuracy using hybrid CNN+LSTM models.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in precise user location tracking indoors using UWB technology, affected by obstacles and walls, and to enhance human activity recognition in residences.

Method: The authors proposed a fingerprinting-based approach using RSSI data and evaluated CNN, LSTM, and hybrid CNN+LSTM models in two residential setups, incorporating temporal window variations.

Result: The hybrid model demonstrated superior performance, achieving a mean absolute error close to 50 cm for inhabitant location tracking.

Conclusion: The findings underscore the hybrid model's potential for accurate human activity recognition in home environments, advancing UWB-based tracking technologies.

Abstract: The field of human activity recognition has evolved significantly, driven
largely by advancements in Internet of Things (IoT) device technology,
particularly in personal devices. This study investigates the use of
ultra-wideband (UWB) technology for tracking inhabitant paths in home
environments using deep learning models. UWB technology estimates user
locations via time-of-flight and time-difference-of-arrival methods, which are
significantly affected by the presence of walls and obstacles in real
environments, reducing their precision. To address these challenges, we propose
a fingerprinting-based approach utilizing received signal strength indicator
(RSSI) data collected from inhabitants in two flats (60 m2 and 100 m2) while
performing daily activities. We compare the performance of convolutional neural
network (CNN), long short-term memory (LSTM), and hybrid CNN+LSTM models, as
well as the use of Bluetooth technology. Additionally, we evaluate the impact
of the type and duration of the temporal window (future, past, or a combination
of both). Our results demonstrate a mean absolute error close to 50 cm,
highlighting the superiority of the hybrid model in providing accurate location
estimates, thus facilitating its application in daily human activity
recognition in residential settings.

</details>


### [324] [Exploring Urban Factors with Autoencoders: Relationship Between Static and Dynamic Features](https://arxiv.org/abs/2509.06167)
*Ximena Pocco,Waqar Hassan,Karelia Salinas,Vladimir Molchanov,Luis G. Nonato*

Main category: cs.LG

TL;DR: The paper presents a visualization-assisted framework to analyze the effectiveness of fused latent data over separate representations in enhancing urban pattern identification.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the challenges posed by granularity, heterogeneity, and multimodality in urban data by investigating whether fused latent representations provide deeper insights into urban dynamics compared to analyzing separate data sources.

Method: The authors developed a framework using visualization to compare and analyze fused latent data representations against separate representations, focusing on dynamic and static urban data.

Result: The findings demonstrate that fused latent representations help produce more structured patterns, while separate data representations are beneficial in some specific cases.

Conclusion: The study concludes that fused data representations generally achieve superior results in discovering structured urban patterns, though single-source data analysis remains valuable for certain scenarios.

Abstract: Urban analytics utilizes extensive datasets with diverse urban information to
simulate, predict trends, and uncover complex patterns within cities. While
these data enables advanced analysis, it also presents challenges due to its
granularity, heterogeneity, and multimodality. To address these challenges,
visual analytics tools have been developed to support the exploration of latent
representations of fused heterogeneous and multimodal data, discretized at a
street-level of detail. However, visualization-assisted tools seldom explore
the extent to which fused data can offer deeper insights than examining each
data source independently within an integrated visualization framework. In this
work, we developed a visualization-assisted framework to analyze whether fused
latent data representations are more effective than separate representations in
uncovering patterns from dynamic and static urban data. The analysis reveals
that combined latent representations produce more structured patterns, while
separate ones are useful in particular cases.

</details>


### [325] [Reasoning Language Model for Personalized Lung Cancer Screening](https://arxiv.org/abs/2509.06169)
*Chuang Niu,Ge Wang*

Main category: cs.LG

TL;DR: The paper introduces a reasoning language model (RLM) to integrate radiology findings with longitudinal medical records for better lung cancer risk assessment, outperforming existing methods in predictive accuracy.


<details>
  <summary>Details</summary>
Motivation: Current lung cancer risk assessment systems like Lung-RADS have limitations due to their reliance solely on lung nodule characteristics, neglecting other risk factors.

Method: The proposed RLM utilizes data from radiology and longitudinal medical records, undergoing steps such as dataset construction, supervised fine-tuning, reinforcement learning, and evaluation. It employs a chain-of-thought reasoning process for better interpretability.

Result: The RLM method showed significant improvements in risk prediction accuracy and the ability to analyze diverse contributing factors, outperforming existing systems on national lung screening trial datasets.

Conclusion: The study demonstrates that the RLM approach enhances both predictive accuracy and interpretability, improving the clinical applicability of lung cancer screening.

Abstract: Accurate risk assessment in lung cancer screening is critical for enabling
early cancer detection and minimizing unnecessary invasive procedures. The Lung
CT Screening Reporting and Data System (Lung-RADS) has been widely used as the
standard framework for patient management and follow-up. Nevertheless,
Lung-RADS faces trade-offs between sensitivity and specificity, as it
stratifies risk solely based on lung nodule characteristics without
incorporating various risk factors. Here we propose a reasoning language model
(RLM) to integrate radiology findings with longitudinal medical records for
individualized lung cancer risk assessment. Through a systematic study
including dataset construction and distillation, supervised fine-tuning,
reinforcement learning, and comprehensive evaluation, our model makes
significant improvements in risk prediction performance on datasets in the
national lung screening trial. Notably, RLM can decompose the risk evaluation
task into sub-components, analyze the contributions of diverse risk factors,
and synthesize them into a final risk score computed using our data-driven
system equation. Our approach improves both predictive accuracy and
monitorability through the chain of thought reasoning process, thereby
facilitating clinical translation into lung cancer screening.

</details>


### [326] [Metric Embedding Initialization-Based Differentially Private and Explainable Graph Clustering](https://arxiv.org/abs/2509.06214)
*Haochen You,Baojing Liu*

Main category: cs.LG

TL;DR: This paper presents a differentially private graph clustering framework that enhances interpretability and efficiency by leveraging metric embedding initialization.


<details>
  <summary>Details</summary>
Motivation: The research aims to address challenges in differentially private graph clustering, including high noise, low efficiency, and poor interpretability.

Method: The approach consists of SDP optimization, an HST-based clustering initialization, and a k-median clustering strategy to ensure privacy and interpretability.

Result: The proposed method demonstrated superior performance on public datasets across various clustering metrics while maintaining strict differential privacy.

Conclusion: The study provides a more effective and interpretable framework for differentially private graph clustering, overcoming major challenges in the field.

Abstract: Graph clustering under the framework of differential privacy, which aims to
process graph-structured data while protecting individual privacy, has been
receiving increasing attention. Despite significant achievements in current
research, challenges such as high noise, low efficiency and poor
interpretability continue to severely constrain the development of this field.
In this paper, we construct a differentially private and interpretable graph
clustering approach based on metric embedding initialization. Specifically, we
construct an SDP optimization, extract the key set and provide a
well-initialized clustering configuration using an HST-based initialization
method. Subsequently, we apply an established k-median clustering strategy to
derive the cluster results and offer comparative explanations for the query set
through differences from the cluster centers. Extensive experiments on public
datasets demonstrate that our proposed framework outperforms existing methods
in various clustering metrics while strictly ensuring privacy.

</details>


### [327] [MCIGLE: Multimodal Exemplar-Free Class-Incremental Graph Learning](https://arxiv.org/abs/2509.06219)
*Haochen You,Baojing Liu*

Main category: cs.LG

TL;DR: The paper introduces MCIGLE, a framework for class-incremental learning on multimodal graph data, tackling memory and generalization issues.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in class-incremental learning for multimodal graph data, including catastrophic forgetting and weak generalization.

Method: The framework uses feature extraction, multimodal alignment, and Concatenated Recursive Least Squares for knowledge retention and memory balance.

Result: Experiments on public datasets demonstrate MCIGLE's effectiveness and broad applicability.

Conclusion: MCIGLE effectively balances accuracy and memory preservation in class-incremental learning, showcasing strong generalization and scalability in multimodal graph data scenarios.

Abstract: Exemplar-free class-incremental learning enables models to learn new classes
over time without storing data from old ones. As multimodal graph-structured
data becomes increasingly prevalent, existing methods struggle with challenges
like catastrophic forgetting, distribution bias, memory limits, and weak
generalization. We propose MCIGLE, a novel framework that addresses these
issues by extracting and aligning multimodal graph features and applying
Concatenated Recursive Least Squares for effective knowledge retention. Through
multi-channel processing, MCIGLE balances accuracy and memory preservation.
Experiments on public datasets validate its effectiveness and generalizability.

</details>


### [328] [UrbanMIMOMap: A Ray-Traced MIMO CSI Dataset with Precoding-Aware Maps and Benchmarks](https://arxiv.org/abs/2509.06270)
*Honggang Jia,Xiucheng Wang,Nan Cheng,Ruijin Sun,Changle Li*

Main category: cs.LG

TL;DR: The paper introduces UrbanMIMOMap, a large-scale MIMO CSI dataset for aiding 6G environment-aware communication through machine learning techniques.


<details>
  <summary>Details</summary>
Motivation: 6G systems demand high-quality radio maps (RMs) for environment-aware communication, but generating such maps is computationally expensive and existing datasets lack the detailed information needed for advanced MIMO systems.

Method: The authors created UrbanMIMOMap, a dataset built using high-precision ray tracing, which provides detailed complex channel state information (CSI) matrices across dense spatial grids.

Result: UrbanMIMOMap includes comprehensive data that surpasses traditional path loss-focused datasets. Demonstrations show its utility in enabling machine learning-based RM generation and evaluating MIMO performance in 6G networks.

Conclusion: UrbanMIMOMap is a valuable resource for advancing high-fidelity RM generation, improving MIMO system performance, and supporting machine learning applications in 6G, filling critical gaps in current datasets.

Abstract: Sixth generation (6G) systems require environment-aware communication, driven
by native artificial intelligence (AI) and integrated sensing and communication
(ISAC). Radio maps (RMs), providing spatially continuous channel information,
are key enablers. However, generating high-fidelity RM ground truth via
electromagnetic (EM) simulations is computationally intensive, motivating
machine learning (ML)-based RM construction. The effectiveness of these
data-driven methods depends on large-scale, high-quality training data. Current
public datasets often focus on single-input single-output (SISO) and limited
information, such as path loss, which is insufficient for advanced multi-input
multi-output (MIMO) systems requiring detailed channel state information (CSI).
To address this gap, this paper presents UrbanMIMOMap, a novel large-scale
urban MIMO CSI dataset generated using high-precision ray tracing. UrbanMIMOMap
offers comprehensive complex CSI matrices across a dense spatial grid, going
beyond traditional path loss data. This rich CSI is vital for constructing
high-fidelity RMs and serves as a fundamental resource for data-driven RM
generation, including deep learning. We demonstrate the dataset's utility
through baseline performance evaluations of representative ML methods for RM
construction. This work provides a crucial dataset and reference for research
in high-precision RM generation, MIMO spatial performance, and ML for 6G
environment awareness. The code and data for this work are available at:
https://github.com/UNIC-Lab/UrbanMIMOMap.

</details>


### [329] [IPR: Intelligent Prompt Routing with User-Controlled Quality-Cost Trade-offs](https://arxiv.org/abs/2509.06274)
*Aosong Feng,Zhichao Xu,Xian Wu,Kang Zhou,Sheng Guan,Yueyan Chen,Ninad Kulkarni,Yun Zhou,Balasubramaniam Srinivasan,Haibo Ding,Lin Lee Cheong*

Main category: cs.LG

TL;DR: The paper introduces IPR, an intelligent routing framework designed to optimize cost while maintaining response quality in deploying large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: To address the need for balancing cost-effectiveness and quality in selecting Large Language Models (LLMs) for large-scale systems.

Method: The proposed IPR framework predicts the quality of LLM responses using modular quality estimators, employs a user-controlled routing mechanism for cost-quality trade-offs, and integrates frozen encoders with adapters for adaptability.

Result: The system achieves a 43.9% cost reduction while maintaining comparable quality to advanced models in the Claude family, with low latency.

Conclusion: IPR provides a scalable and efficient solution for dynamically routing inputs while achieving significant cost savings and maintaining high response quality.

Abstract: Routing incoming queries to the most cost-effective LLM while maintaining
response quality poses a fundamental challenge in optimizing performance-cost
trade-offs for large-scale commercial systems. We present IPR\, a
quality-constrained Intelligent Prompt Routing framework that dynamically
selects optimal models based on predicted response quality and user-specified
tolerance levels. IPR introduces three key innovations: (1) a modular
architecture with lightweight quality estimators trained on 1.5M prompts
annotated with calibrated quality scores, enabling fine-grained quality
prediction across model families; (2) a user-controlled routing mechanism with
tolerance parameter $\tau \in [0,1]$ that provides explicit control over
quality-cost trade-offs; and (3) an extensible design using frozen encoders
with model-specific adapters, reducing new model integration from days to
hours. To rigorously train and evaluate IPR, we curate an industrial-level
dataset IPRBench\footnote{IPRBench will be released upon legal approval.}, a
comprehensive benchmark containing 1.5 million examples with response quality
annotations across 11 LLM candidates. Deployed on a major cloud platform, IPR
achieves 43.9\% cost reduction while maintaining quality parity with the
strongest model in the Claude family and processes requests with sub-150ms
latency.

</details>


### [330] [RecMind: LLM-Enhanced Graph Neural Networks for Personalized Consumer Recommendations](https://arxiv.org/abs/2509.06286)
*Chang Xue,Youwei Lu,Chen Yang,Jinming Xing*

Main category: cs.LG

TL;DR: RecMind, an advanced recommender system, integrates language models and graph-based methods for personalized recommendations, achieving state-of-the-art performance on Yelp and Amazon benchmarks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in personalization, such as sparse interactions, frequent content turnover, and diverse textual signals, which limit the accuracy of current recommender systems.

Method: RecMind combines a frozen language model (enhanced with adapters) to derive text-based embeddings and a LightGCN model for collaborative graph embeddings. These components are aligned using a contrastive objective and fused with intra-layer gating to leverage advantages of both approaches.

Result: RecMind outperformed strong baselines on Yelp and Amazon-Electronics datasets, showing significant improvements in eight metrics, including Recall@40 (+4.53%) and NDCG@40 (+4.01%).

Conclusion: The cross-view alignment and intra-layer gating techniques are critical for RecMind's success, outperforming both standalone methods and simpler fusion strategies.

Abstract: Personalization is a core capability across consumer technologies, streaming,
shopping, wearables, and voice, yet it remains challenged by sparse
interactions, fast content churn, and heterogeneous textual signals. We present
RecMind, an LLM-enhanced graph recommender that treats the language model as a
preference prior rather than a monolithic ranker. A frozen LLM equipped with
lightweight adapters produces text-conditioned user/item embeddings from
titles, attributes, and reviews; a LightGCN backbone learns collaborative
embeddings from the user-item graph. We align the two views with a symmetric
contrastive objective and fuse them via intra-layer gating, allowing language
to dominate in cold/long-tail regimes and graph structure to stabilize rankings
elsewhere. On Yelp and Amazon-Electronics, RecMind attains the best results on
all eight reported metrics, with relative improvements up to +4.53\%
(Recall@40) and +4.01\% (NDCG@40) over strong baselines. Ablations confirm both
the necessity of cross-view alignment and the advantage of gating over late
fusion and LLM-only variants.

</details>


### [331] [LoaQ: Layer-wise Output Approximation Quantization](https://arxiv.org/abs/2509.06297)
*Li Lin,Xiaojun Wan*

Main category: cs.LG

TL;DR: Layer-wise post-training quantization often fails to align quantized outputs with their originals due to a strictly local approach. LoaQ introduces an output-level consistency method addressing this issue.


<details>
  <summary>Details</summary>
Motivation: Existing layer-wise PTQ methods provide local approximations that do not align well with full model outputs, leading to insufficient quantization results.

Method: LoaQ employs an output-approximation technique targeting consistency at the model output level, featuring a simple closed-form solution and integrability into existing quantization pipelines.

Result: Experiments on LLaMA and Qwen model families show LoaQ effectively improves quantization quality in both weight-only and weight-activation joint quantization.

Conclusion: LoaQ enhances overall quantization and holds potential for advancing post-training quantization techniques further.

Abstract: A natural and intuitive idea in model quantization is to approximate each
component's quantized output to match its original. Layer-wise post-training
quantization (PTQ), though based on this idea, adopts a strictly local view and
can achieve, at best, only activation-aware approximations of weights. As a
result, it often leads to insufficient approximations and practical deviations
from this guiding intuition. Recent work has achieved a more accurate
approximation of linear-layer outputs within the framework of layer-wise PTQ,
but such refinements remain inadequate for achieving alignment with the full
model output. Based on a deeper understanding of the structural characteristics
of mainstream LLMs, we propose $LoaQ$, an output-approximation method for
layer-wise PTQ that explicitly targets output-level consistency. It better
aligns with this intuition and can feature a simple closed-form solution,
making it orthogonal to existing techniques and readily integrable into
existing quantization pipelines. Experiments on the LLaMA and Qwen model
families demonstrate that LoaQ performs effectively in both weight-only and
weight-activation joint quantization. By integrating seamlessly with existing
quantization strategies, it further enhances overall quantization quality and
shows strong potential to advance the frontier of post-training quantization.

</details>


### [332] [WindFM: An Open-Source Foundation Model for Zero-Shot Wind Power Forecasting](https://arxiv.org/abs/2509.06311)
*Hang Fan,Yu Shi,Zongliang Fu,Shuo Chen,Wei Wei,Wei Xu,Jian Li*

Main category: cs.LG

TL;DR: WindFM is a compact generative Foundation Model for probabilistic wind power forecasting, achieving state-of-the-art results without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing methods for wind power forecasting either lack generalization across locations or struggle to incorporate domain-specific data efficiently.

Method: WindFM uses a discretize-and-generate framework with a specialized tokenizer and a Transformer model trained on large-scale WIND Toolkit data.

Result: WindFM achieves top zero-shot performance in both deterministic and probabilistic forecasting tasks, showing robustness to out-of-distribution data.

Conclusion: WindFM provides an innovative, lightweight solution for wind power forecasting with high adaptability and transferability.

Abstract: High-quality wind power forecasting is crucial for the operation of modern
power grids. However, prevailing data-driven paradigms either train a
site-specific model which cannot generalize to other locations or rely on
fine-tuning of general-purpose time series foundation models which are
difficult to incorporate domain-specific data in the energy sector. This paper
introduces WindFM, a lightweight and generative Foundation Model designed
specifically for probabilistic wind power forecasting. WindFM employs a
discretize-and-generate framework. A specialized time-series tokenizer first
converts continuous multivariate observations into discrete, hierarchical
tokens. Subsequently, a decoder-only Transformer learns a universal
representation of wind generation dynamics by autoregressively pre-training on
these token sequences. Using the comprehensive WIND Toolkit dataset comprising
approximately 150 billion time steps from more than 126,000 sites, WindFM
develops a foundational understanding of the complex interplay between
atmospheric conditions and power output. Extensive experiments demonstrate that
our compact 8.1M parameter model achieves state-of-the-art zero-shot
performance on both deterministic and probabilistic tasks, outperforming
specialized models and larger foundation models without any fine-tuning. In
particular, WindFM exhibits strong adaptiveness under out-of-distribution data
from a different continent, demonstrating the robustness and transferability of
its learned representations. Our pre-trained model is publicly available at
https://github.com/shiyu-coder/WindFM.

</details>


### [333] [Evaluating the Efficiency of Latent Spaces via the Coupling-Matrix](https://arxiv.org/abs/2509.06314)
*Mehmet Can Yavuz,Berrin Yanikoglu*

Main category: cs.LG

TL;DR: The paper introduces a redundancy index (rho(C)) to measure inefficiencies in latent representations, particularly caused by overlapping information among dimensions. This metric directly quantifies redundancy and serves as a tool for improving representational quality.


<details>
  <summary>Details</summary>
Motivation: Redundant latent spaces, which arise in deep networks, limit model capacity and hinder generalization. Existing metrics only indirectly address redundancy, prompting the need for a direct approach.

Method: The authors propose rho(C), a redundancy index based on analyzing coupling matrices of latent representations and comparing their off-diagonal statistics to a normal distribution using energy distance.

Result: Low rho(C) correlates with high classification accuracy or low reconstruction error, whereas high rho(C) predicts performance collapse. The reliability improves with larger latent dimensions, and rho(C) aids neural architecture search using Tree-structured Parzen Estimators (TPE).

Conclusion: The redundancy index rho(C) offers a theoretical and practical framework for identifying and addressing redundancy in latent representations, improving efficiency and generalization across models and tasks.

Abstract: A central challenge in representation learning is constructing latent
embeddings that are both expressive and efficient. In practice, deep networks
often produce redundant latent spaces where multiple coordinates encode
overlapping information, reducing effective capacity and hindering
generalization. Standard metrics such as accuracy or reconstruction loss
provide only indirect evidence of such redundancy and cannot isolate it as a
failure mode. We introduce a redundancy index, denoted rho(C), that directly
quantifies inter-dimensional dependencies by analyzing coupling matrices
derived from latent representations and comparing their off-diagonal statistics
against a normal distribution via energy distance. The result is a compact,
interpretable, and statistically grounded measure of representational quality.
We validate rho(C) across discriminative and generative settings on MNIST
variants, Fashion-MNIST, CIFAR-10, and CIFAR-100, spanning multiple
architectures and hyperparameter optimization strategies. Empirically, low
rho(C) reliably predicts high classification accuracy or low reconstruction
error, while elevated redundancy is associated with performance collapse.
Estimator reliability grows with latent dimension, yielding natural lower
bounds for reliable analysis. We further show that Tree-structured Parzen
Estimators (TPE) preferentially explore low-rho regions, suggesting that rho(C)
can guide neural architecture search and serve as a redundancy-aware
regularization target. By exposing redundancy as a universal bottleneck across
models and tasks, rho(C) offers both a theoretical lens and a practical tool
for evaluating and improving the efficiency of learned representations.

</details>


### [334] [Text-Trained LLMs Can Zero-Shot Extrapolate PDE Dynamics](https://arxiv.org/abs/2509.06322)
*Jiajun Bao,Nicolas Boullé,Toni J. B. Liu,Raphaël Sarfati,Christopher J. Earls*

Main category: cs.LG

TL;DR: The paper explores large language models (LLMs) for solving discretized partial differential equations (PDEs) and analyzes their in-context learning (ICL) behavior.


<details>
  <summary>Details</summary>
Motivation: To investigate whether text-trained LLMs can accurately predict spatiotemporal dynamics from PDE solutions without fine-tuning and to understand the mechanism underlying their ICL abilities.

Method: The authors evaluate LLMs' ability to perform multi-step rollouts of PDE solutions, analyzing predictive accuracy against spatiotemporal contexts. They also examine token-level output distributions to uncover trends in how models process PDE solutions over time.

Result: LLMs can extrapolate spatiotemporal dynamics with accuracy that improves with longer temporal contexts but decreases at finer spatial resolutions. Errors in multi-step rollouts grow algebraically with time, exhibiting trends analogous to classical solvers.

Conclusion: LLMs exhibit predictable scaling behaviors in context learning for PDEs, transitioning through phases of pattern imitation, exploratory behavior, and confident predictions. This provides insights into their capability for numerical tasks despite being trained on text.

Abstract: Large language models (LLMs) have demonstrated emergent in-context learning
(ICL) capabilities across a range of tasks, including zero-shot time-series
forecasting. We show that text-trained foundation models can accurately
extrapolate spatiotemporal dynamics from discretized partial differential
equation (PDE) solutions without fine-tuning or natural language prompting.
Predictive accuracy improves with longer temporal contexts but degrades at
finer spatial discretizations. In multi-step rollouts, where the model
recursively predicts future spatial states over multiple time steps, errors
grow algebraically with the time horizon, reminiscent of global error
accumulation in classical finite-difference solvers. We interpret these trends
as in-context neural scaling laws, where prediction quality varies predictably
with both context length and output length. To better understand how LLMs are
able to internally process PDE solutions so as to accurately roll them out, we
analyze token-level output distributions and uncover a consistent ICL
progression: beginning with syntactic pattern imitation, transitioning through
an exploratory high-entropy phase, and culminating in confident, numerically
grounded predictions.

</details>


### [335] [Exploring approaches to computational representation and classification of user-generated meal logs](https://arxiv.org/abs/2509.06330)
*Guanlan Hu,Adit Anand,Pooja M. Desai,Iñigo Urteaga,Lena Mamykina*

Main category: cs.LG

TL;DR: The study uses patient-generated free-text meal logs, enhanced with domain-specific information, to classify meal alignment with nutritional goals via machine learning, achieving high accuracy beyond self-assessments.


<details>
  <summary>Details</summary>
Motivation: To explore how machine learning can analyze free-text meal logs to support nutrition guidance by replacing or enhancing self-assessments with automated classification based on nutritional goals.

Method: Applied a combination of text embeddings (e.g., TFIDF, BERT) and domain-specific enrichments such as ontologies, ingredient parsers, and macronutrient data as inputs to machine learning classifiers (logistic regression and multilayer perceptron). Evaluated their accuracy against a gold standard provided by dietitians.

Result: Machine learning models outperformed self-assessments and achieved higher accuracy, especially when enriched with nutrition-specific data, across diverse nutritional goals and on patient-generated meal logs.

Conclusion: Machine learning, especially when combined with domain-specific dietary enrichments, effectively classifies meals' alignment with nutritional goals, showing promise for enhancing patient-centered nutrition guidance in precision healthcare.

Abstract: This study examined the use of machine learning and domain specific
enrichment on patient generated health data, in the form of free text meal
logs, to classify meals on alignment with different nutritional goals. We used
a dataset of over 3000 meal records collected by 114 individuals from a
diverse, low income community in a major US city using a mobile app. Registered
dietitians provided expert judgement for meal to goal alignment, used as gold
standard for evaluation. Using text embeddings, including TFIDF and BERT, and
domain specific enrichment information, including ontologies, ingredient
parsers, and macronutrient contents as inputs, we evaluated the performance of
logistic regression and multilayer perceptron classifiers using accuracy,
precision, recall, and F1 score against the gold standard and self assessment.
Even without enrichment, ML outperformed self assessments of individuals who
logged meals, and the best performing combination of ML classifier with
enrichment achieved even higher accuracies. In general, ML classifiers with
enrichment of Parsed Ingredients, Food Entities, and Macronutrients information
performed well across multiple nutritional goals, but there was variability in
the impact of enrichment and classification algorithm on accuracy of
classification for different nutritional goals. In conclusion, ML can utilize
unstructured free text meal logs and reliably classify whether meals align with
specific nutritional goals, exceeding self assessments, especially when
incorporating nutrition domain knowledge. Our findings highlight the potential
of ML analysis of patient generated health data to support patient centered
nutrition guidance in precision healthcare.

</details>


### [336] [A Fragile Number Sense: Probing the Elemental Limits of Numerical Reasoning in LLMs](https://arxiv.org/abs/2509.06332)
*Roussel Rahman,Aashwin Ananda Mishra*

Main category: cs.LG

TL;DR: The paper evaluates the numerical reasoning abilities of Large Language Models (LLMs), finding they excel in algorithmic tasks but struggle with generative problem-solving.


<details>
  <summary>Details</summary>
Motivation: To better understand the robustness and limitations of LLMs in numerical reasoning, a critical component of their reasoning capabilities.

Method: The study tests state-of-the-art LLM-based agents on a 100-problem challenge over four categories ranging from basic arithmetic to complex combinatorial puzzles.

Result: LLMs perform well in deterministic algorithmic tasks but fail consistently at combinatorial puzzles, highlighting a lack of generative problem-solving ability.

Conclusion: LLMs rely on recalling known algorithms and pattern-matching rather than flexible, analytical reasoning, which limits their potential in tasks requiring creative numerical insights.

Abstract: Large Language Models (LLMs) have demonstrated remarkable emergent
capabilities, yet the robustness of their numerical reasoning remains an open
question. While standard benchmarks evaluate LLM reasoning on complex problem
sets using aggregated metrics, they often obscure foundational weaknesses. In
this work, we probe LLM mathematical numeracy by evaluating performance on
problems of escalating complexity, from constituent operations to combinatorial
puzzles. We test several state-of-the-art LLM-based agents on a 100-problem
challenge comprising four categories: (1) basic arithmetic, (2) advanced
operations, (3) primality checking, and (4) the Game of 24 number puzzle. Our
results show that while the agents achieved high accuracy on the first three
categories, which require deterministic algorithmic execution, they
consistently failed at the number puzzle, underlining its demand for a
heuristic search over a large combinatorial space to be a significant
bottleneck. These findings reveal that the agents' proficiency is largely
confined to recalling and executing known algorithms, rather than performing
generative problem-solving. This suggests their apparent numerical reasoning is
more akin to sophisticated pattern-matching than flexible, analytical thought,
limiting their potential for tasks that require novel or creative numerical
insights.

</details>


### [337] [Ban&Pick: Achieving Free Performance Gains and Inference Speedup via Smarter Routing in MoE-LLMs](https://arxiv.org/abs/2509.06346)
*Yuanteng Chen,Peisong Wang,Yuantian Shao,Jian Cheng*

Main category: cs.LG

TL;DR: The paper introduces Ban&Pick, a post-training strategy for optimizing Sparse Mixture-of-Experts (MoE) language models, resolving issues with premature routing decisions and redundant expert activations to improve performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in current fine-grained MoE architectures, specifically the premature convergence of routers and unnecessary redundancy in activating a fixed number of experts per token, which limit model performance and inference efficiency.

Method: The proposed Ban&Pick strategy involves two components: (1) "Pick," which identifies and reinforces the most impactful experts to enhance accuracy, and (2) "Ban," which dynamically prunes redundant experts to expedite inference without retraining or architectural modifications.

Result: Experiments on MoE-based large language models, such as DeepSeek and Qwen3, show that Ban&Pick improves accuracy across math, coding, and reasoning benchmarks by notable margins while also accelerating inference by up to 1.25x.

Conclusion: Ban&Pick is an effective, plug-and-play strategy for enhancing model performance and inference speed in fine-grained MoE-LLMs, solving routing inefficiencies without the need for retraining or architectural changes.

Abstract: Sparse Mixture-of-Experts (MoE) has become a key architecture for scaling
large language models (LLMs) efficiently. Recent fine-grained MoE designs
introduce hundreds of experts per layer, with multiple experts activated per
token, enabling stronger specialization. However, during pre-training, routers
are optimized mainly for stability and robustness: they converge prematurely
and enforce balanced usage, limiting the full potential of model performance
and efficiency. In this work, we uncover two overlooked issues: (i) a few
highly influential experts are underutilized due to premature and balanced
routing decisions; and (ii) enforcing a fixed number of active experts per
token introduces substantial redundancy. Instead of retraining models or
redesigning MoE architectures, we introduce Ban&Pick, a post-training,
plug-and-play strategy for smarter MoE routing. Pick discovers and reinforces
key experts-a small group with outsized impact on performance-leading to
notable accuracy gains across domains. Ban complements this by dynamically
pruning redundant experts based on layer and token sensitivity, delivering
faster inference with minimal accuracy loss. Experiments on fine-grained
MoE-LLMs (DeepSeek, Qwen3) across math, code, and general reasoning benchmarks
demonstrate that Ban&Pick delivers free performance gains and inference
acceleration without retraining or architectural changes. For instance, on
Qwen3-30B-A3B, it improves accuracy from 80.67 to 84.66 on AIME2024 and from
65.66 to 68.18 on GPQA-Diamond, while accelerating inference by 1.25x under the
vLLM.

</details>


### [338] [Breaking SafetyCore: Exploring the Risks of On-Device AI Deployment](https://arxiv.org/abs/2509.06371)
*Victor Guyomard,Mathis Mauvisseau,Marie Paindavoine*

Main category: cs.LG

TL;DR: This paper investigates security vulnerabilities in on-device AI models, using the Android SafetyCore system as a case study, showing how adversaries can bypass sensitive content detection.


<details>
  <summary>Details</summary>
Motivation: To explore the security risks introduced by deploying AI models on devices instead of traditional cloud-based approaches.

Method: The study analyzed the SafetyCore Android system service to extract and manipulate its on-device AI model, demonstrating real-world exploitation.

Result: It was shown that on-device AI models can be manipulated to bypass detection mechanisms, rendering the security protections ineffective.

Conclusion: On-device AI models enhance privacy and reduce latency but have unique security vulnerabilities that adversaries can exploit, highlighting the need for improved protection mechanisms.

Abstract: Due to hardware and software improvements, an increasing number of AI models
are deployed on-device. This shift enhances privacy and reduces latency, but
also introduces security risks distinct from traditional software. In this
article, we examine these risks through the real-world case study of
SafetyCore, an Android system service incorporating sensitive image content
detection. We demonstrate how the on-device AI model can be extracted and
manipulated to bypass detection, effectively rendering the protection
ineffective. Our analysis exposes vulnerabilities of on-device AI models and
provides a practical demonstration of how adversaries can exploit them.

</details>


### [339] [Variational Garrote for Statistical Physics-based Sparse and Robust Variable Selection](https://arxiv.org/abs/2509.06383)
*Hyungjoon Soh,Dongha Lee,Vipul Periwal,Junghyo Jo*

Main category: cs.LG

TL;DR: This paper revisits and enhances the Variational Garrote (VG) method for sparse regression, demonstrating its effectiveness for variable selection in high-dimensional data.


<details>
  <summary>Details</summary>
Motivation: To improve variable selection in high-dimensional datasets by leveraging an underutilized statistical physics-based method, the Variational Garrote, and addressing its scalability and efficiency issues.

Method: Enhanced the Variational Garrote (VG) method by incorporating modern automatic differentiation techniques to optimize its performance and scalability. Evaluated using both synthetic and real-world datasets.

Result: VG demonstrated superior performance in highly sparse regimes compared to Ridge and LASSO regression, with consistent and robust variable selection. Also, discovered a transition point where superfluous variables degrade generalization and increase uncertainty metrics.

Conclusion: The enhanced VG method offers robust and scalable sparse regression, with practical applications in detecting key predictors in datasets. It holds potential for various tasks, such as compressed sensing and model pruning.

Abstract: Selecting key variables from high-dimensional data is increasingly important
in the era of big data. Sparse regression serves as a powerful tool for this
purpose by promoting model simplicity and explainability. In this work, we
revisit a valuable yet underutilized method, the statistical physics-based
Variational Garrote (VG), which introduces explicit feature selection spin
variables and leverages variational inference to derive a tractable loss
function. We enhance VG by incorporating modern automatic differentiation
techniques, enabling scalable and efficient optimization. We evaluate VG on
both fully controllable synthetic datasets and complex real-world datasets. Our
results demonstrate that VG performs especially well in highly sparse regimes,
offering more consistent and robust variable selection than Ridge and LASSO
regression across varying levels of sparsity. We also uncover a sharp
transition: as superfluous variables are admitted, generalization degrades
abruptly and the uncertainty of the selection variables increases. This
transition point provides a practical signal for estimating the correct number
of relevant variables, an insight we successfully apply to identify key
predictors in real-world data. We expect that VG offers strong potential for
sparse modeling across a wide range of applications, including compressed
sensing and model pruning in machine learning.

</details>


### [340] [Beyond the Pre-Service Horizon: Infusing In-Service Behavior for Improved Financial Risk Forecasting](https://arxiv.org/abs/2509.06385)
*Senhao Liu,Zhiyu Guo,Zhiyuan Ji,Yueguo Chen,Yateng Tang,Yunhai Wang,Xuehao Zheng,Xiang Ao*

Main category: cs.LG

TL;DR: The paper introduces the Multi-Granularity Knowledge Distillation (MGKD) framework, which enhances pre-service financial risk prediction by integrating behavioral data from in-service phases, using a teacher-student model structure.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of separate modeling phases for pre-service risk assessment and in-service default detection by leveraging the behavioral data observed during in-service phases.

Method: The MGKD framework employs knowledge distillation where a teacher model uses in-service data to guide a student model trained on pre-service data. It incorporates a multi-granularity strategy and re-weighting to improve prediction and mitigate bias towards minority classes.

Result: Experiments on large-scale datasets from Tencent Mobile Payment show MGKD improves pre-service risk assessment performance effectively in both offline and online scenarios.

Conclusion: Integrating multi-granularity distillation and behavioral data transfer enhances financial risk prediction, enabling more accurate pre-service risk assessment and addressing data imbalance issues.

Abstract: Typical financial risk management involves distinct phases for pre-service
risk assessment and in-service default detection, often modeled separately.
This paper proposes a novel framework, Multi-Granularity Knowledge Distillation
(abbreviated as MGKD), aimed at improving pre-service risk prediction through
the integration of in-service user behavior data. MGKD follows the idea of
knowledge distillation, where the teacher model, trained on historical
in-service data, guides the student model, which is trained on pre-service
data. By using soft labels derived from in-service data, the teacher model
helps the student model improve its risk prediction prior to service
activation. Meanwhile, a multi-granularity distillation strategy is introduced,
including coarse-grained, fine-grained, and self-distillation, to align the
representations and predictions of the teacher and student models. This
approach not only reinforces the representation of default cases but also
enables the transfer of key behavioral patterns associated with defaulters from
the teacher to the student model, thereby improving the overall performance of
pre-service risk assessment. Moreover, we adopt a re-weighting strategy to
mitigate the model's bias towards the minority class. Experimental results on
large-scale real-world datasets from Tencent Mobile Payment demonstrate the
effectiveness of our proposed approach in both offline and online scenarios.

</details>


### [341] [Graph Neural Networks for Resource Allocation in Interference-limited Multi-Channel Wireless Networks with QoS Constraints](https://arxiv.org/abs/2509.06395)
*Lili Chen,Changyang She,Jingge Zhu,Jamie Evans*

Main category: cs.LG

TL;DR: The paper addresses QoS constraints in wireless communication using eWMMSE and introduces a scalable GNN-based solution, JCPGNN-M, for improved speed and robustness.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges in meeting minimum data rate constraints in complex wireless networks, where traditional deep learning methods lack theoretical convergence guarantees and sometimes fail in practical scenarios.

Method: The authors enhance the WMMSE algorithm to eWMMSE for multi-channel settings with QoS constraints. Additionally, they propose a GNN-based JCPGNN-M algorithm integrated with a Lagrangian-based primal-dual framework for scalable optimization.

Result: Extensive simulations show that JCPGNN-M matches the performance of eWMMSE while offering faster inference, better scalability to larger networks, and robustness under imperfect data.

Conclusion: The work provides a theoretically grounded and scalable solution for constrained resource allocation in future wireless networks, ensuring QoS satisfaction and improved efficiency.

Abstract: Meeting minimum data rate constraints is a significant challenge in wireless
communication systems, particularly as network complexity grows. Traditional
deep learning approaches often address these constraints by incorporating
penalty terms into the loss function and tuning hyperparameters empirically.
However, this heuristic treatment offers no theoretical convergence guarantees
and frequently fails to satisfy QoS requirements in practical scenarios.
Building upon the structure of the WMMSE algorithm, we first extend it to a
multi-channel setting with QoS constraints, resulting in the enhanced WMMSE
(eWMMSE) algorithm, which is provably convergent to a locally optimal solution
when the problem is feasible. To further reduce computational complexity and
improve scalability, we develop a GNN-based algorithm, JCPGNN-M, capable of
supporting simultaneous multi-channel allocation per user. To overcome the
limitations of traditional deep learning methods, we propose a principled
framework that integrates GNN with a Lagrangian-based primal-dual optimization
method. By training the GNN within the Lagrangian framework, we ensure
satisfaction of QoS constraints and convergence to a stationary point.
Extensive simulations demonstrate that JCPGNN-M matches the performance of
eWMMSE while offering significant gains in inference speed, generalization to
larger networks, and robustness under imperfect channel state information. This
work presents a scalable and theoretically grounded solution for constrained
resource allocation in future wireless networks.

</details>


### [342] [NeuroDeX: Unlocking Diverse Support in Decompiling Deep Neural Network Executables](https://arxiv.org/abs/2509.06402)
*Yilin Li,Guozhu Meng,Mingyang Sun,Yanzhong Wang,Kun Sun,Hailong Chang,Yuekang Li*

Main category: cs.LG

TL;DR: NeuroDeX is a new tool that decompiles deep neural network executables into high-level models using semantic LLMs and dynamic analysis, achieving significant accuracy for both quantized and non-quantized models.


<details>
  <summary>Details</summary>
Motivation: The growing deployment of deep learning models on edge devices creates a need to address the threat of reverse engineering for compiled executables. Existing decompilation methods face challenges in handling optimizations and quantized models.

Method: NeuroDeX combines the semantic understanding of large language models (LLMs) with dynamic analysis to identify operator types, recover attributes, and reconstruct models from compiled DNN executables.

Result: Tests on 96 DNN executables across 12 models show that NeuroDeX accurately reconstructs almost identical high-level models for non-quantized executables and achieves 72% top-1 accuracy for quantized models.

Conclusion: NeuroDeX provides a more robust, accurate, and versatile solution for decompiling DNN executables than previous methods, addressing challenges in both optimized and quantized models.

Abstract: On-device deep learning models have extensive real world demands. Deep
learning compilers efficiently compile models into executables for deployment
on edge devices, but these executables may face the threat of reverse
engineering. Previous studies have attempted to decompile DNN executables, but
they face challenges in handling compilation optimizations and analyzing
quantized compiled models. In this paper, we present NeuroDeX to unlock diverse
support in decompiling DNN executables. NeuroDeX leverages the semantic
understanding capabilities of LLMs along with dynamic analysis to accurately
and efficiently perform operator type recognition, operator attribute recovery
and model reconstruction. NeuroDeX can recover DNN executables into high-level
models towards compilation optimizations, different architectures and quantized
compiled models. We conduct experiments on 96 DNN executables across 12 common
DNN models. Extensive experimental results demonstrate that NeuroDeX can
decompile non-quantized executables into nearly identical high-level models.
NeuroDeX can recover functionally similar high-level models for quantized
executables, achieving an average top-1 accuracy of 72%. NeuroDeX offers a more
comprehensive and effective solution compared to previous DNN executables
decompilers.

</details>


### [343] [CAPMix: Robust Time Series Anomaly Detection Based on Abnormal Assumptions with Dual-Space Mixup](https://arxiv.org/abs/2509.06419)
*Xudong Mou,Rui Wang,Tiejun Wang,Renyu Yang,Shiru Chen,Jie Sun,Tianyu Wo,Xudong Liu*

Main category: cs.LG

TL;DR: The paper presents CAPMix, a novel framework for time series anomaly detection (TSAD) that addresses challenges of synthetic anomaly generation and label shifts for better classification.


<details>
  <summary>Details</summary>
Motivation: TSAD is challenging due to limited labeled anomalies and complex temporal dependencies. Current methods face issues like unrealistic synthetic anomalies (Anomaly Shift) and scattered knowledge (Patchy Generation).

Method: CAPMix introduces a CutAddPaste mechanism for targeted anomaly injection, a label revision strategy to refine anomaly labels, and a dual-space mixup within a temporal convolutional network for robust decision boundaries.

Result: CAPMix demonstrates significant performance improvements over state-of-the-art methods across five benchmark datasets, showing enhanced robustness even with contaminated training data.

Conclusion: The CAPMix framework effectively overcomes existing limitations in TSAD by improving anomaly injection and decision boundaries, setting a new benchmark for the field.

Abstract: Time series anomaly detection (TSAD) is a vital yet challenging task,
particularly in scenarios where labeled anomalies are scarce and temporal
dependencies are complex. Recent anomaly assumption (AA) approaches alleviate
the lack of anomalies by injecting synthetic samples and training
discriminative models. Despite promising results, these methods often suffer
from two fundamental limitations: patchy generation, where scattered anomaly
knowledge leads to overly simplistic or incoherent anomaly injection, and
Anomaly Shift, where synthetic anomalies either resemble normal data too
closely or diverge unrealistically from real anomalies, thereby distorting
classification boundaries. In this paper, we propose CAPMix, a controllable
anomaly augmentation framework that addresses both issues. First, we design a
CutAddPaste mechanism to inject diverse and complex anomalies in a targeted
manner, avoiding patchy generation. Second, we introduce a label revision
strategy to adaptively refine anomaly labels, reducing the risk of anomaly
shift. Finally, we employ dual-space mixup within a temporal convolutional
network to enforce smoother and more robust decision boundaries. Extensive
experiments on five benchmark datasets, including AIOps, UCR, SWaT, WADI, and
ESA, demonstrate that CAPMix achieves significant improvements over
state-of-the-art baselines, with enhanced robustness against contaminated
training data. The code is available at https://github.com/alsike22/CAPMix.

</details>


### [344] [CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction](https://arxiv.org/abs/2509.06465)
*Hongzong Li,Jiahao Ma,Zhanpeng Shi,Fanming Jin,Ye-Fan Hu,Jian-Dong Huang*

Main category: cs.LG

TL;DR: This paper introduces CAME-AB, a multimodal framework for predicting antibody binding sites, using five integrated biological modalities and advanced techniques like adaptive modality fusion and contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Current methods fail to accurately identify antibody-specific binding sites due to limitations in representation (single-view features) and prediction.

Method: CAME-AB employs cross-modality attention and a mixture-of-experts backbone, integrating multimodal biological features, adaptive fusion, a Transformer encoder, and contrastive learning to enhance prediction accuracy and generalization.

Result: The framework consistently outperforms existing methods across key measurement metrics such as Precision, Recall, and F1-score on benchmark datasets.

Conclusion: CAME-AB's multimodal integration improves the characterization and prediction of antibody binding sites, advancing computational immunology and therapeutic design.

Abstract: Antibody binding site prediction plays a pivotal role in computational
immunology and therapeutic antibody design. Existing sequence or structure
methods rely on single-view features and fail to identify antibody-specific
binding sites on the antigens-a dual limitation in representation and
prediction. In this paper, we propose CAME-AB, a novel Cross-modality Attention
framework with a Mixture-of-Experts (MoE) backbone for robust antibody binding
site prediction. CAME-AB integrates five biologically grounded modalities,
including raw amino acid encodings, BLOSUM substitution profiles, pretrained
language model embeddings, structure-aware features, and GCN-refined
biochemical graphs-into a unified multimodal representation. To enhance
adaptive cross-modal reasoning, we propose an adaptive modality fusion module
that learns to dynamically weight each modality based on its global relevance
and input-specific contribution. A Transformer encoder combined with an MoE
module further promotes feature specialization and capacity expansion. We
additionally incorporate a supervised contrastive learning objective to
explicitly shape the latent space geometry, encouraging intra-class compactness
and inter-class separability. To improve optimization stability and
generalization, we apply stochastic weight averaging during training. Extensive
experiments on benchmark antibody-antigen datasets demonstrate that CAME-AB
consistently outperforms strong baselines on multiple metrics, including
Precision, Recall, F1-score, AUC-ROC, and MCC. Ablation studies further
validate the effectiveness of each architectural component and the benefit of
multimodal feature integration. The model implementation details and the codes
are available on https://anonymous.4open.science/r/CAME-AB-C525

</details>


### [345] [DyC-STG: Dynamic Causal Spatio-Temporal Graph Network for Real-time Data Credibility Analysis in IoT](https://arxiv.org/abs/2509.06483)
*Guanjie Cheng,Boyi Li,Peihan Wu,Feiyi Chen,Xinkui Zhao,Mengying Zhu,Shuiguang Deng*

Main category: cs.LG

TL;DR: The paper introduces DyC-STG, a novel machine learning framework to ensure IoT data credibility by dynamically updating graph structures and incorporating causal reasoning.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to improve data credibility in IoT applications, which suffer from static graph models and issues distinguishing causality from correlation.

Method: The authors developed DyC-STG, comprising two key components: an event-driven dynamic graph module for real-time topology updates and a causal reasoning module enforcing strict temporal precedence.

Result: DyC-STG outperformed existing baselines, achieving an F1-Score of up to 0.930 and establishing new benchmarks in spatio-temporal graph modeling.

Conclusion: DyC-STG addresses key limitations in IoT data modeling by enabling dynamic and causal-aware graph structures, significantly enhancing data credibility.

Abstract: The wide spreading of Internet of Things (IoT) sensors generates vast
spatio-temporal data streams, but ensuring data credibility is a critical yet
unsolved challenge for applications like smart homes. While spatio-temporal
graph (STG) models are a leading paradigm for such data, they often fall short
in dynamic, human-centric environments due to two fundamental limitations: (1)
their reliance on static graph topologies, which fail to capture physical,
event-driven dynamics, and (2) their tendency to confuse spurious correlations
with true causality, undermining robustness in human-centric environments. To
address these gaps, we propose the Dynamic Causal Spatio-Temporal Graph Network
(DyC-STG), a novel framework designed for real-time data credibility analysis
in IoT. Our framework features two synergistic contributions: an event-driven
dynamic graph module that adapts the graph topology in real-time to reflect
physical state changes, and a causal reasoning module to distill causally-aware
representations by strictly enforcing temporal precedence. To facilitate the
research in this domain we release two new real-world datasets. Comprehensive
experiments show that DyC-STG establishes a new state-of-the-art, outperforming
the strongest baselines by 1.4 percentage points and achieving an F1-Score of
up to 0.930.

</details>


### [346] [A machine-learned expression for the excess Gibbs energy](https://arxiv.org/abs/2509.06484)
*Marco Hoffmann,Thomas Specht,Quirin Göttl,Jakob Burger,Stephan Mandt,Hans Hasse,Fabian Jirasek*

Main category: cs.LG

TL;DR: The paper introduces HANNA, a neural network model for predicting excess Gibbs energy of liquid mixtures based solely on molecular structures, developed using thermodynamic laws, end-to-end training, and novel methods for binary and multi-component systems.


<details>
  <summary>Details</summary>
Motivation: Predicting the excess Gibbs energy for liquid mixtures from molecular structures is challenging but essential for thermodynamic modeling.

Method: The authors integrated physical laws as hard constraints in a neural network and applied novel methods such as surrogate solvers and geometric projection to ensure training consistency and robust extrapolation.

Result: HANNA significantly outperformed benchmark methods in terms of accuracy and scope, offering thermodynamically consistent predictions and effective extrapolation to multi-component mixtures.

Conclusion: HANNA is a breakthrough model that simplifies excess Gibbs energy prediction, openly available with supporting resources and exceeding performance benchmarks.

Abstract: The excess Gibbs energy plays a central role in chemical engineering and
chemistry, providing a basis for modeling the thermodynamic properties of
liquid mixtures. Predicting the excess Gibbs energy of multi-component mixtures
solely from the molecular structures of their components is a long-standing
challenge. In this work, we address this challenge by integrating physical laws
as hard constraints within a flexible neural network. The resulting model,
HANNA, was trained end-to-end on an extensive experimental dataset for binary
mixtures from the Dortmund Data Bank, guaranteeing thermodynamically consistent
predictions. A novel surrogate solver developed in this work enabled the
inclusion of liquid-liquid equilibrium data in the training process.
Furthermore, a geometric projection method was applied to enable robust
extrapolations to multi-component mixtures, without requiring additional
parameters. We demonstrate that HANNA delivers excellent predictions, clearly
outperforming state-of-the-art benchmark methods in accuracy and scope. The
trained model and corresponding code are openly available, and an interactive
interface is provided on our website, MLPROP.

</details>


### [347] [QualityFM: a Multimodal Physiological Signal Foundation Model with Self-Distillation for Signal Quality Challenges in Critically Ill Patients](https://arxiv.org/abs/2509.06516)
*Zongheng Guo,Tao Chen,Manuela Ferrario*

Main category: cs.LG

TL;DR: QualityFM is proposed, a multimodal foundation model designed for assessing signal quality in physiological signals like PPG and ECG, addressing issues of poor signal data quality in ICUs and operating rooms.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing methods in handling incomplete or poor-quality physiological signal data, which often lead to false alarms or inaccuracies in clinical environments like ICUs and ORs.

Method: The model uses a dual-track Transformer-based architecture with a self-distillation strategy to train encoders for high- and low-quality paired signals. It incorporates windowed sparse attention for handling long signal sequences and a composite loss function to preserve frequency-domain characteristics.

Result: Three versions of QualityFM with varying parameter sizes were pre-trained on a large dataset and validated through transfer learning on clinical tasks like ventricular tachycardia false alarm detection, atrial fibrillation identification, and arterial blood pressure estimation.

Conclusion: QualityFM shows high potential in improving the assessment of signal quality in clinical settings, with effective transfer learning to various relevant tasks.

Abstract: Photoplethysmogram (PPG) and electrocardiogram (ECG) are commonly recorded in
intesive care unit (ICU) and operating room (OR). However, the high incidence
of poor, incomplete, and inconsistent signal quality, can lead to false alarms
or diagnostic inaccuracies. The methods explored so far suffer from limited
generalizability, reliance on extensive labeled data, and poor cross-task
transferability. To overcome these challenges, we introduce QualityFM, a novel
multimodal foundation model for these physiological signals, designed to
acquire a general-purpose understanding of signal quality. Our model is
pre-trained on an large-scale dataset comprising over 21 million 30-second
waveforms and 179,757 hours of data. Our approach involves a dual-track
architecture that processes paired physiological signals of differing quality,
leveraging a self-distillation strategy where an encoder for high-quality
signals is used to guide the training of an encoder for low-quality signals. To
efficiently handle long sequential signals and capture essential local
quasi-periodic patterns, we integrate a windowed sparse attention mechanism
within our Transformer-based model. Furthermore, a composite loss function,
which combines direct distillation loss on encoder outputs with indirect
reconstruction loss based on power and phase spectra, ensures the preservation
of frequency-domain characteristics of the signals. We pre-train three models
with varying parameter counts (9.6 M to 319 M) and demonstrate their efficacy
and practical value through transfer learning on three distinct clinical tasks:
false alarm of ventricular tachycardia detection, the identification of atrial
fibrillation and the estimation of arterial blood pressure (ABP) from PPG and
ECG signals.

</details>


### [348] [Lane Change Intention Prediction of two distinct Populations using a Transformer](https://arxiv.org/abs/2509.06529)
*Francesco De Cristofaro,Cornelia Lex,Jia Hu,Arno Eichberger*

Main category: cs.LG

TL;DR: This paper highlights the challenges of using lane change intention prediction models across different datasets and tests a transformer model on datasets from Germany and Hong Kong.


<details>
  <summary>Details</summary>
Motivation: Researchers aim to address the inability of lane change prediction models to generalize across different populations and datasets.

Method: The researchers train a transformer model on two different datasets from Germany and Hong Kong, testing its effectiveness when trained and tested on combined versus separate populations.

Result: Accuracy dropped to 39.43% when tested on a population different from the training dataset but increased to 86.71% when trained on both populations simultaneously.

Conclusion: Training on diverse datasets enhances the generalization ability and effectiveness of lane change intention prediction models.

Abstract: As a result of the growing importance of lane change intention prediction for
a safe and efficient driving experience in complex driving scenarios,
researchers have in recent years started to train novel machine learning
algorithms on available datasets with promising results. A shortcoming of this
recent research effort, though, is that the vast majority of the proposed
algorithms are trained on a single datasets. In doing so, researchers failed to
test if their algorithm would be as effective if tested on a different dataset
and, by extension, on a different population with respect to the one on which
they were trained. In this article we test a transformer designed for lane
change intention prediction on two datasets collected by LevelX in Germany and
Hong Kong. We found that the transformer's accuracy plummeted when tested on a
population different to the one it was trained on with accuracy values as low
as 39.43%, but that when trained on both populations simultaneously it could
achieve an accuracy as high as 86.71%. - This work has been submitted to the
IEEE for possible publication. Copyright may be transferred without notice,
after which this version may no longer be accessible.

</details>


### [349] [Learning Optimal Defender Strategies for CAGE-2 using a POMDP Model](https://arxiv.org/abs/2509.06539)
*Duc Huy Le,Rolf Stadler*

Main category: cs.LG

TL;DR: The paper introduces a model for CAGE-2 based on POMDP, defines an optimal defender strategy, and presents BF-PPO, a PPO-based method using particle filters, outperforming CARDIFF in the CAGE-2 benchmark.


<details>
  <summary>Details</summary>
Motivation: To formalize and optimize defender strategies for the CAGE-2 benchmark in cybersecurity scenarios.

Method: A POMDP model for CAGE-2 is constructed, and a new method, BF-PPO, is proposed, which integrates particle filters with PPO to handle the computational demands of a large state space.

Result: BF-PPO demonstrated superior performance compared to the CARDIFF method in strategy efficiency and training time within the CAGE-2 CybORG environment.

Conclusion: The proposed BF-PPO method provides an effective and efficient solution for learning optimal defender strategies in CAGE-2, surpassing existing methods.

Abstract: CAGE-2 is an accepted benchmark for learning and evaluating defender
strategies against cyberattacks. It reflects a scenario where a defender agent
protects an IT infrastructure against various attacks. Many defender methods
for CAGE-2 have been proposed in the literature. In this paper, we construct a
formal model for CAGE-2 using the framework of Partially Observable Markov
Decision Process (POMDP). Based on this model, we define an optimal defender
strategy for CAGE-2 and introduce a method to efficiently learn this strategy.
Our method, called BF-PPO, is based on PPO, and it uses particle filter to
mitigate the computational complexity due to the large state space of the
CAGE-2 model. We evaluate our method in the CAGE-2 CybORG environment and
compare its performance with that of CARDIFF, the highest ranked method on the
CAGE-2 leaderboard. We find that our method outperforms CARDIFF regarding the
learned defender strategy and the required training time.

</details>


### [350] [Predicting Fetal Outcomes from Cardiotocography Signals Using a Supervised Variational Autoencoder](https://arxiv.org/abs/2509.06540)
*John Tolladay,Beth Albert,Gabriel Davis Jones*

Main category: cs.LG

TL;DR: The paper develops a supervised variational autoencoder (VAE) for classifying cardiotocography signals based on pregnancy outcomes, enhancing interpretability and prediction performance.


<details>
  <summary>Details</summary>
Motivation: Address the interpretability limits of deep learning in classifying cardiotocography signals for pregnancy outcomes.

Method: Training a supervised VAE on fetal heart rate segments with Kullback-Leibler divergence and total correlation constraints; evaluating with AUROC, MSE, latent analysis, traversals, and component analysis.

Result: The model achieved AUROCs of 0.752 (segment level) and 0.779 (CTG level); demonstrated improved reconstruction and classification; and revealed partial encoding of baseline features but limitations in other metrics.

Conclusion: Supervised VAEs can predict fetal outcomes with competitive performance while encoding clinically meaningful features, though challenges remain in fully interpreting irregular signals like CTG.

Abstract: Objective: To develop and interpret a supervised variational autoencoder
(VAE) model for classifying cardiotocography (CTG) signals based on pregnancy
outcomes, addressing interpretability limits of current deep learning
approaches. Methods: The OxMat CTG dataset was used to train a VAE on
five-minute fetal heart rate (FHR) segments, labeled with postnatal outcomes.
The model was optimised for signal reconstruction and outcome prediction,
incorporating Kullback-Leibler divergence and total correlation (TC)
constraints to structure the latent space. Performance was evaluated using area
under the receiver operating characteristic curve (AUROC) and mean squared
error (MSE). Interpretability was assessed using coefficient of determination,
latent traversals and unsupervised component analyses. Results: The model
achieved an AUROC of 0.752 at the segment level and 0.779 at the CTG level,
where predicted scores were aggregated. Relaxing TC constraints improved both
reconstruction and classification. Latent analysis showed that baseline-related
features (e.g., FHR baseline, baseline shift) were well represented and aligned
with model scores, while metrics like short- and long-term variability were
less strongly encoded. Traversals revealed clear signal changes for baseline
features, while other properties were entangled or subtle. Unsupervised
decompositions corroborated these patterns. Findings: This work demonstrates
that supervised VAEs can achieve competitive fetal outcome prediction while
partially encoding clinically meaningful CTG features. The irregular,
multi-timescale nature of FHR signals poses challenges for disentangling
physiological components, distinguishing CTG from more periodic signals such as
ECG. Although full interpretability was not achieved, the model supports
clinically useful outcome prediction and provides a basis for future
interpretable, generative models.

</details>


### [351] [Contrastive Self-Supervised Network Intrusion Detection using Augmented Negative Pairs](https://arxiv.org/abs/2509.06550)
*Jack Wilkie,Hanan Hindy,Christos Tachtatzis,Robert Atkinson*

Main category: cs.LG

TL;DR: Traditional intrusion detection faces challenges with labelled data reliance in supervised models and high false positive rates in anomaly detection. The paper proposes a novel contrastive self-supervised technique with augmented negative pairs (CLAN) to enhance accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of intrusion detection methods: the need for large labelled datasets in supervised models and the high false positive rates in anomaly detection methods.

Method: The proposed method, CLAN, introduces contrastive self-supervised learning where augmented samples are used as negative pairs, representing malicious traffic, while benign samples serve as positive views. This approach is trained on benign traffic for improved performance.

Result: Experimental results on the Lycos2017 dataset indicate that CLAN achieves better binary classification compared to other self-supervised and anomaly detection techniques. Moreover, it outperforms existing self-supervised models in multi-class classification when fine-tuned on limited labelled data.

Conclusion: CLAN showcases the potential for effective and efficient intrusion detection, outperforming existing supervised, self-supervised, and anomaly-based solutions in various tasks, thus bridging gaps in current cybersecurity approaches.

Abstract: Network intrusion detection remains a critical challenge in cybersecurity.
While supervised machine learning models achieve state-of-the-art performance,
their reliance on large labelled datasets makes them impractical for many
real-world applications. Anomaly detection methods, which train exclusively on
benign traffic to identify malicious activity, suffer from high false positive
rates, limiting their usability. Recently, self-supervised learning techniques
have demonstrated improved performance with lower false positive rates by
learning discriminative latent representations of benign traffic. In
particular, contrastive self-supervised models achieve this by minimizing the
distance between similar (positive) views of benign traffic while maximizing it
between dissimilar (negative) views. Existing approaches generate positive
views through data augmentation and treat other samples as negative. In
contrast, this work introduces Contrastive Learning using Augmented Negative
pairs (CLAN), a novel paradigm for network intrusion detection where augmented
samples are treated as negative views - representing potentially malicious
distributions - while other benign samples serve as positive views. This
approach enhances both classification accuracy and inference efficiency after
pretraining on benign traffic. Experimental evaluation on the Lycos2017 dataset
demonstrates that the proposed method surpasses existing self-supervised and
anomaly detection techniques in a binary classification task. Furthermore, when
fine-tuned on a limited labelled dataset, the proposed approach achieves
superior multi-class classification performance compared to existing
self-supervised models.

</details>


### [352] [AI for Scientific Discovery is a Social Problem](https://arxiv.org/abs/2509.06580)
*Georgia Channing,Avijit Ghosh*

Main category: cs.LG

TL;DR: The paper identifies social and institutional barriers as the main obstacles to effectively applying AI in scientific discovery, rather than just technical challenges.


<details>
  <summary>Details</summary>
Motivation: To analyze why the benefits of AI in accelerating scientific discovery remain unevenly distributed and highlight key barriers restricting progress.

Method: The authors identify four interconnected challenges—community dysfunction, misaligned research priorities, data fragmentation, and infrastructure inequities—and argue that their root causes are cultural and organizational practices.

Result: The paper emphasizes that addressing these challenges requires not only technical solutions but also community-building, cross-disciplinary education, shared benchmarks, and accessible infrastructure.

Conclusion: The paper concludes that AI for science must be framed as a collective social project, requiring sustainable collaboration and equitable participation for technical progress.

Abstract: Artificial intelligence promises to accelerate scientific discovery, yet its
benefits remain unevenly distributed. While technical obstacles such as scarce
data, fragmented standards, and unequal access to computation are significant,
we argue that the primary barriers are social and institutional. Narratives
that defer progress to speculative "AI scientists," the undervaluing of data
and infrastructure contributions, misaligned incentives, and gaps between
domain experts and machine learning researchers all constrain impact. We
highlight four interconnected challenges: community dysfunction, research
priorities misaligned with upstream needs, data fragmentation, and
infrastructure inequities. We argue that their roots lie in cultural and
organizational practices. Addressing them requires not only technical
innovation but also intentional community-building, cross-disciplinary
education, shared benchmarks, and accessible infrastructure. We call for
reframing AI for science as a collective social project, where sustainable
collaboration and equitable participation are treated as prerequisites for
technical progress.

</details>


### [353] [Information-Theoretic Bounds and Task-Centric Learning Complexity for Real-World Dynamic Nonlinear Systems](https://arxiv.org/abs/2509.06599)
*Sri Satish Krishna Chaitanya Bulusu,Mikko Sillanpää*

Main category: cs.LG

TL;DR: This paper introduces a theoretical framework to model dynamic nonlinear systems using decomposition, variance analysis, and complexity bounds, emphasizing inherent trade-offs and empirical benefits.


<details>
  <summary>Details</summary>
Motivation: Understand and model dynamic nonlinear systems by addressing intertwined static and dynamic distortions.

Method: Using structured decomposition, variance analysis, memory finiteness measures, and inequality bounds to develop task-aware complexity metrics.

Result: Proposes a Behavioral Uncertainty Principle, derives general-purpose theorems on function variance, and explains structured residual learning benefits.

Conclusion: The framework explains empirical behaviors and offers a scalable, robust approach to model complex nonlinear systems.

Abstract: Dynamic nonlinear systems exhibit distortions arising from coupled static and
dynamic effects. Their intertwined nature poses major challenges for
data-driven modeling. This paper presents a theoretical framework grounded in
structured decomposition, variance analysis, and task-centric complexity
bounds.
  The framework employs a directional lower bound on interactions between
measurable system components, extending orthogonality in inner product spaces
to structurally asymmetric settings. This bound supports variance inequalities
for decomposed systems. Key behavioral indicators are introduced along with a
memory finiteness index. A rigorous power-based condition establishes a
measurable link between finite memory in realizable systems and the First Law
of Thermodynamics. This offers a more foundational perspective than classical
bounds based on the Second Law.
  Building on this foundation, we formulate a `Behavioral Uncertainty
Principle,' demonstrating that static and dynamic distortions cannot be
minimized simultaneously. We identify that real-world systems seem to resist
complete deterministic decomposition due to entangled static and dynamic
effects. We also present two general-purpose theorems linking function variance
to mean-squared Lipschitz continuity and learning complexity. This yields a
model-agnostic, task-aware complexity metric, showing that lower-variance
components are inherently easier to learn.
  These insights explain the empirical benefits of structured residual
learning, including improved generalization, reduced parameter count, and lower
training cost, as previously observed in power amplifier linearization
experiments. The framework is broadly applicable and offers a scalable,
theoretically grounded approach to modeling complex dynamic nonlinear systems.

</details>


### [354] [PAC-Bayesian Generalization Bounds for Graph Convolutional Networks on Inductive Node Classification](https://arxiv.org/abs/2509.06600)
*Huayi Tang,Yong Liu*

Main category: cs.LG

TL;DR: This paper provides a PAC-Bayesian theoretical analysis of GCNs for inductive node classification, addressing the dynamic nature of real-world graphs with novel generalization bounds.


<details>
  <summary>Details</summary>
Motivation: GNNs have succeeded in handling graph data but face challenges with dynamic graphs involving temporal evolution and structural changes.

Method: The study focuses on formulating PAC-Bayesian generalization bounds for GCNs, explicitly considering data dependency and non-stationarity in dynamic graph settings.

Result: The analysis derives sufficient conditions for the generalization gap of one-layer GCNs to converge to zero and extends findings to two-layer GCNs under stricter topology assumptions.

Conclusion: The theoretical insights provide groundwork for improving the generalization performance of GNNs in dynamic graph scenarios.

Abstract: Graph neural networks (GNNs) have achieved remarkable success in processing
graph-structured data across various applications. A critical aspect of
real-world graphs is their dynamic nature, where new nodes are continually
added and existing connections may change over time. Previous theoretical
studies, largely based on the transductive learning framework, fail to
adequately model such temporal evolution and structural dynamics. In this
paper, we presents a PAC-Bayesian theoretical analysis of graph convolutional
networks (GCNs) for inductive node classification, treating nodes as dependent
and non-identically distributed data points. We derive novel generalization
bounds for one-layer GCNs that explicitly incorporate the effects of data
dependency and non-stationarity, and establish sufficient conditions under
which the generalization gap converges to zero as the number of nodes
increases. Furthermore, we extend our analysis to two-layer GCNs, and reveal
that it requires stronger assumptions on graph topology to guarantee
convergence. This work establishes a theoretical foundation for understanding
and improving GNN generalization in dynamic graph environments.

</details>


### [355] [Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in Molecular Tumor Boards](https://arxiv.org/abs/2509.06602)
*Noel Codella,Sam Preston,Hao Qiu,Leonardo Schettini,Wen-wai Yim,Mert Öz,Shrey Jain,Matthew P. Lungren,Thomas Osborne*

Main category: cs.LG

TL;DR: The paper introduces HAO, an AI agent using LLMs, to automate patient summaries for Molecular Tumor Boards (MTBs) and proposes TBFact for robust evaluation.


<details>
  <summary>Details</summary>
Motivation: Current manual approaches to producing patient summaries for MTBs are labor-intensive, subjective, and prone to errors, necessitating a more efficient, accurate, and scalable solution.

Method: The proposed solution is Healthcare Agent Orchestrator (HAO), which uses a multi-agent Large Language Model (LLM)-based workflow. For evaluation, TBFact is introduced as a model-as-a-judge framework to measure summary comprehensiveness and succinctness.

Result: The evaluation shows HAO's agent captured 94% of critical information and achieved a TBFact recall of 0.84 under strict criteria. TBFact enables a privacy-aware, data-free deployment workflow.

Conclusion: HAO and TBFact provide a robust and scalable framework for improving patient summary generation and evaluation in MTBs, addressing labor and accuracy challenges while maintaining data privacy.

Abstract: Molecular Tumor Boards (MTBs) are multidisciplinary forums where oncology
specialists collaboratively assess complex patient cases to determine optimal
treatment strategies. A central element of this process is the patient summary,
typically compiled by a medical oncologist, radiation oncologist, or surgeon,
or their trained medical assistant, who distills heterogeneous medical records
into a concise narrative to facilitate discussion. This manual approach is
often labor-intensive, subjective, and prone to omissions of critical
information. To address these limitations, we introduce the Healthcare Agent
Orchestrator (HAO), a Large Language Model (LLM)-driven AI agent that
coordinates a multi-agent clinical workflow to generate accurate and
comprehensive patient summaries for MTBs. Evaluating predicted patient
summaries against ground truth presents additional challenges due to stylistic
variation, ordering, synonym usage, and phrasing differences, which complicate
the measurement of both succinctness and completeness. To overcome these
evaluation hurdles, we propose TBFact, a ``model-as-a-judge'' framework
designed to assess the comprehensiveness and succinctness of generated
summaries. Using a benchmark dataset derived from de-identified tumor board
discussions, we applied TBFact to evaluate our Patient History agent. Results
show that the agent captured 94% of high-importance information (including
partial entailments) and achieved a TBFact recall of 0.84 under strict
entailment criteria. We further demonstrate that TBFact enables a data-free
evaluation framework that institutions can deploy locally without sharing
sensitive clinical data. Together, HAO and TBFact establish a robust foundation
for delivering reliable and scalable support to MTBs.

</details>


### [356] [Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors](https://arxiv.org/abs/2509.06608)
*Viacheslav Sinii,Nikita Balagansky,Yaroslav Aksenov,Vadim Kurochkin,Daniil Laptev,Gleb Gerasimov,Alexey Gorbatovski,Boris Shaposhnikov,Daniil Gavrilov*

Main category: cs.LG

TL;DR: The paper examines lightweight steering vectors in language models, offering enhanced interpretability and performance via reinforcement learning objectives.


<details>
  <summary>Details</summary>
Motivation: Understanding the mechanisms of reasoning training's impact on language model computations.

Method: Use of reinforcement-learning objectives with steering vectors and analysis methods such as logit-lens readouts and circuit analyses.

Result: Steering vectors influence token generation either as biases or structural adjustments in different layers.

Conclusion: The work presents a structured framework for interpreting how reasoning training modifies language model behavior.

Abstract: The mechanisms by which reasoning training reshapes language-model
computations remain poorly understood. We study lightweight steering vectors
inserted into the base model's residual stream and trained with a
reinforcement-learning objective, which can match full fine-tuning performance
while retaining the interpretability of small, additive interventions. Using
logit-lens readouts, path patching, and circuit analyses, we analyze two models
and find: (i) the last-layer steering vector behaves like a token-substitution
bias concentrated on the first generated token, consistently boosting tokens
such as "To" and "Step"; and (ii) the penultimate-layer steering vector leaves
attention patterns largely unchanged and instead acts through the MLP and
unembedding, preferentially up-weighting process words and structure symbols.
These results establish a principled framework for interpreting the behavioral
changes induced by reasoning training.

</details>


### [357] [A Survey of Generalization of Graph Anomaly Detection: From Transfer Learning to Foundation Models](https://arxiv.org/abs/2509.06609)
*Junjun Pan,Yu Zheng,Yue Tan,Yixin Liu*

Main category: cs.LG

TL;DR: This paper reviews advances in generalized graph anomaly detection (GAD), addressing challenges like shifting data distributions and limited training samples, and provides taxonomy and reviews existing methods.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of current GAD approaches that struggle with real-world scenarios such as shifting distributions and limited training samples, by improving generalization.

Method: The paper provides a systematic review, formalizes problem settings, and introduces a taxonomy for generalized GAD methods, highlighting the importance of transfer learning and foundation models.

Result: A comprehensive and up-to-date review of generalized GAD approaches, including the identification of key challenges and describing current methodologies.

Conclusion: The review offers insights into gaps and challenges in generalized GAD while providing a roadmap for future research directions in the field.

Abstract: Graph anomaly detection (GAD) has attracted increasing attention in recent
years for identifying malicious samples in a wide range of graph-based
applications, such as social media and e-commerce. However, most GAD methods
assume identical training and testing distributions and are tailored to
specific tasks, resulting in limited adaptability to real-world scenarios such
as shifting data distributions and scarce training samples in new applications.
To address the limitations, recent work has focused on improving the
generalization capability of GAD models through transfer learning that
leverages knowledge from related domains to enhance detection performance, or
developing "one-for-all" GAD foundation models that generalize across multiple
applications. Since a systematic understanding of generalization in GAD is
still lacking, in this paper, we provide a comprehensive review of
generalization in GAD. We first trace the evolution of generalization in GAD
and formalize the problem settings, which further leads to our systematic
taxonomy. Rooted in this fine-grained taxonomy, an up-to-date and comprehensive
review is conducted for the existing generalized GAD methods. Finally, we
identify current open challenges and suggest future directions to inspire
future research in this emerging field.

</details>


### [358] [BEAM: Brainwave Empathy Assessment Model for Early Childhood](https://arxiv.org/abs/2509.06620)
*Chen Xie,Gaofeng Wu,Kaidong Wang,Zihao Zhu,Xiaoshu Luo,Yan Liang,Feiyu Quan,Ruoxi Wu,Xianghui Huang,Han Zhang*

Main category: cs.LG

TL;DR: This paper proposes a deep learning framework called BEAM to predict empathy levels in young children using multi-view EEG signals.


<details>
  <summary>Details</summary>
Motivation: Predicting empathy in young children is difficult due to the limitations of traditional self-reports or observer-based methods, which fail to objectively capture the process.

Method: The paper introduces BEAM, a framework comprising a LaBraM-based encoder for spatio-temporal feature extraction, a feature fusion module, and a contrastive learning module, to analyze EEG signals.

Result: BEAM shows better performance than state-of-the-art methods on the CBCP dataset in predicting empathy levels.

Conclusion: The study demonstrates BEAM's effectiveness for objective empathy assessment and its applicability for interventions in children's prosocial development.

Abstract: Empathy in young children is crucial for their social and emotional
development, yet predicting it remains challenging. Traditional methods often
only rely on self-reports or observer-based labeling, which are susceptible to
bias and fail to objectively capture the process of empathy formation. EEG
offers an objective alternative; however, current approaches primarily extract
static patterns, neglecting temporal dynamics. To overcome these limitations,
we propose a novel deep learning framework, the Brainwave Empathy Assessment
Model (BEAM), to predict empathy levels in children aged 4-6 years. BEAM
leverages multi-view EEG signals to capture both cognitive and emotional
dimensions of empathy. The framework comprises three key components: 1) a
LaBraM-based encoder for effective spatio-temporal feature extraction, 2) a
feature fusion module to integrate complementary information from multi-view
signals, and 3) a contrastive learning module to enhance class separation.
Validated on the CBCP dataset, BEAM outperforms state-of-the-art methods across
multiple metrics, demonstrating its potential for objective empathy assessment
and providing a preliminary insight into early interventions in children's
prosocial development.

</details>


### [359] [Knowledge-Guided Machine Learning for Stabilizing Near-Shortest Path Routing](https://arxiv.org/abs/2509.06640)
*Yung-Fu Chen,Sen Lin,Anish Arora*

Main category: cs.LG

TL;DR: A simple algorithm is introduced to train deep neural networks (DNNs) for routing packets in geometric random graphs, utilizing minimal samples and exploiting domain knowledge to ensure efficient, scalable, and generalizable routing policies.


<details>
  <summary>Details</summary>
Motivation: Routing and forwarding packets efficiently in complex network graphs can be computationally expensive and require scalable generalizable algorithms.

Method: Deep neural networks (DNNs) are trained on minimal data samples, leveraging domain knowledge in feature selection and policy design, to develop routing policies based on local node states and neighbors.

Result: One DNN learns an exact replication of the Greedy Forwarding policy using distance-to-destination data, while another DNN develops the superior GreedyTensile routing policy using additional input features like node stretch.

Conclusion: The proposed GreedyTensile routing policy delivers explainability, better performance than standard greedy forwarding, and ultra-low latency operations.

Abstract: We propose a simple algorithm that needs only a few data samples from a
single graph for learning local routing policies that generalize across a rich
class of geometric random graphs in Euclidean metric spaces. We thus solve the
all-pairs near-shortest path problem by training deep neural networks (DNNs)
that let each graph node efficiently and scalably route (i.e., forward) packets
by considering only the node's state and the state of the neighboring nodes.
Our algorithm design exploits network domain knowledge in the selection of
input features and design of the policy function for learning an approximately
optimal policy. Domain knowledge also provides theoretical assurance that the
choice of a ``seed graph'' and its node data sampling suffices for
generalizable learning. Remarkably, one of these DNNs we train -- using
distance-to-destination as the only input feature -- learns a policy that
exactly matches the well-known Greedy Forwarding policy, which forwards packets
to the neighbor with the shortest distance to the destination. We also learn a
new policy, which we call GreedyTensile routing -- using both
distance-to-destination and node stretch as the input features -- that almost
always outperforms greedy forwarding. We demonstrate the explainability and
ultra-low latency run-time operation of Greedy Tensile routing by symbolically
interpreting its DNN in low-complexity terms of two linear actions.

</details>


### [360] [Group Effect Enhanced Generative Adversarial Imitation Learning for Individual Travel Behavior Modeling under Incentives](https://arxiv.org/abs/2509.06656)
*Yuanyuan Wu,Zhenlin Qin,Leizhen Wang,Xiaolei Ma,Zhenliang Ma*

Main category: cs.LG

TL;DR: The paper proposes a group-enhanced generative adversarial imitation learning (gcGAIL) model to improve efficiency in modeling individual travel behavior, validated through a case study and showing superior performance over benchmarks.


<details>
  <summary>Details</summary>
Motivation: Understanding and improving individual travel behavior modeling for urban mobility regulation and policy evaluation is challenged by data limitations and complexity.

Method: The authors introduce gcGAIL, a model built on generative adversarial imitation learning (GAIL), enhanced with group effects to harness shared behavioral patterns for increased modeling efficiency.

Result: gcGAIL demonstrated superior accuracy, generalization, and efficiency in predicting individual travel behavior compared to state-of-the-art benchmarks. It also proved robust to data sparsity, spatial variation, and behavioral diversity.

Conclusion: gcGAIL provides an effective tool for modeling individual behavior, enabling personalized incentives to encourage sustainable behavior changes and better timing of interventions.

Abstract: Understanding and modeling individual travel behavior responses is crucial
for urban mobility regulation and policy evaluation. The Markov decision
process (MDP) provides a structured framework for dynamic travel behavior
modeling at the individual level. However, solving an MDP in this context is
highly data-intensive and faces challenges of data quantity, spatial-temporal
coverage, and situational diversity. To address these, we propose a
group-effect-enhanced generative adversarial imitation learning (gcGAIL) model
that improves the individual behavior modeling efficiency by leveraging shared
behavioral patterns among passenger groups. We validate the gcGAIL model using
a public transport fare-discount case study and compare against
state-of-the-art benchmarks, including adversarial inverse reinforcement
learning (AIRL), baseline GAIL, and conditional GAIL. Experimental results
demonstrate that gcGAIL outperforms these methods in learning individual travel
behavior responses to incentives over time in terms of accuracy,
generalization, and pattern demonstration efficiency. Notably, gcGAIL is robust
to spatial variation, data sparsity, and behavioral diversity, maintaining
strong performance even with partial expert demonstrations and underrepresented
passenger groups. The gcGAIL model predicts the individual behavior response at
any time, providing the basis for personalized incentives to induce sustainable
behavior changes (better timing of incentive injections).

</details>


### [361] [TrajAware: Graph Cross-Attention and Trajectory-Aware for Generalisable VANETs under Partial Observations](https://arxiv.org/abs/2509.06665)
*Xiaolu Fu,Ziyuan Bao,Eiman Kanjo*

Main category: cs.LG

TL;DR: TrajAware is a reinforcement learning-based framework designed for routing in vehicular ad hoc networks (VANETs), addressing dynamic topologies and resource constraints, while outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges in VANETs, such as dynamic topologies, incomplete observations, and the limited resources of edge devices, which hinder the efficiency of existing routing methods.

Method: TrajAware integrates three novel components: (i) action space pruning to reduce dimensionality, (ii) graph cross-attention for adaptable feature representation, and (iii) trajectory-aware prediction for real-time position estimation under partial observations.

Result: Using real-world city map simulations on the SUMO platform, TrajAware achieves near-optimal routing paths, high message delivery ratios, and efficient resource usage, outperforming current benchmarks even in challenging scenarios.

Conclusion: TrajAware offers a robust and scalable solution for routing in VANETs, making it a suitable candidate for deployment on edge devices under dynamic and resource-constrained environments.

Abstract: Vehicular ad hoc networks (VANETs) are a crucial component of intelligent
transportation systems; however, routing remains challenging due to dynamic
topologies, incomplete observations, and the limited resources of edge devices.
Existing reinforcement learning (RL) approaches often assume fixed graph
structures and require retraining when network conditions change, making them
unsuitable for deployment on constrained hardware. We present TrajAware, an
RL-based framework designed for edge AI deployment in VANETs. TrajAware
integrates three components: (i) action space pruning, which reduces redundant
neighbour options while preserving two-hop reachability, alleviating the curse
of dimensionality; (ii) graph cross-attention, which maps pruned neighbours to
the global graph context, producing features that generalise across diverse
network sizes; and (iii) trajectory-aware prediction, which uses historical
routes and junction information to estimate real-time positions under partial
observations. We evaluate TrajAware in the open-source SUMO simulator using
real-world city maps with a leave-one-city-out setup. Results show that
TrajAware achieves near-shortest paths and high delivery ratios while
maintaining efficiency suitable for constrained edge devices, outperforming
state-of-the-art baselines in both full and partial observation scenarios.

</details>


### [362] [Barycentric Neural Networks and Length-Weighted Persistent Entropy Loss: A Green Geometric and Topological Framework for Function Approximation](https://arxiv.org/abs/2509.06694)
*Victor Toscano-Duran,Rocio Gonzalez-Diaz,Miguel A. Gutiérrez-Naranjo*

Main category: cs.LG

TL;DR: The paper introduces Barycentric Neural Networks ($BNN$), a new type of shallow neural network optimized via base points, along with a novel loss function, length-weighted persistent entropy ($LWPE$). The framework promises efficient function approximation in resource-constrained settings.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from addressing the computational inefficiencies of deep and overparameterized neural networks, and providing a more flexible and interpretable solution for approximating continuous functions over compact domains.

Method: The BNN leverages barycentric coordinates and base points to represent continuous piecewise linear functions ($CPLF$s). It incorporates a novel topological feature, $LWPE$, into the loss function, and directly optimizes the base points defining the network structure.

Result: The experimental results highlight superior and faster function approximation performance of $BNN$ with $LWPE$ compared to classical loss functions like MSE, RMSE, MAE, and log-cosh.

Conclusion: The framework effectively balances resource constraints with high-performance approximations, introducing a new neural network structure ($BNN$) and a novel loss function ($LWPE$) as robust tools for nonlinear continuous function modeling.

Abstract: While it is well-established that artificial neural networks are
\emph{universal approximators} for continuous functions on compact domains,
many modern approaches rely on deep or overparameterized architectures that
incur high computational costs. In this paper, a new type of \emph{small
shallow} neural network, called the \emph{Barycentric Neural Network} ($\BNN$),
is proposed, which leverages a fixed set of \emph{base points} and their
\emph{barycentric coordinates} to define both its structure and its parameters.
We demonstrate that our $\BNN$ enables the exact representation of
\emph{continuous piecewise linear functions} ($\CPLF$s), ensuring strict
continuity across segments. Since any continuous function over a compact domain
can be approximated arbitrarily well by $\CPLF$s, the $\BNN$ naturally emerges
as a flexible and interpretable tool for \emph{function approximation}. Beyond
the use of this representation, the main contribution of the paper is the
introduction of a new variant of \emph{persistent entropy}, a topological
feature that is stable and scale invariant, called the \emph{length-weighted
persistent entropy} ($\LWPE$), which is weighted by the lifetime of topological
features. Our framework, which combines the $\BNN$ with a loss function based
on our $\LWPE$, aims to provide flexible and geometrically interpretable
approximations of nonlinear continuous functions in resource-constrained
settings, such as those with limited base points for $\BNN$ design and few
training epochs. Instead of optimizing internal weights, our approach directly
\emph{optimizes the base points that define the $\BNN$}. Experimental results
show that our approach achieves \emph{superior and faster approximation
performance} compared to classical loss functions such as MSE, RMSE, MAE, and
log-cosh.

</details>


### [363] [Outcome-based Exploration for LLM Reasoning](https://arxiv.org/abs/2509.06941)
*Yuda Song,Julia Kempe,Remi Munos*

Main category: cs.LG

TL;DR: The paper explores reinforcement learning (RL) for improving large language models, addressing the trade-off between final answer accuracy and generation diversity. It introduces outcome-based exploration methods to mitigate these issues.


<details>
  <summary>Details</summary>
Motivation: To enhance reasoning abilities of large language models while addressing the decline in generation diversity due to outcome-based RL.

Method: Outcome-based exploration is proposed, introducing historical exploration (UCB-style bonuses for rare answers) and batch exploration (penalizing within-batch repetition).

Result: Experimental results on competition math tasks validate that the proposed methods improve accuracy and maintain diversity in generation.

Conclusion: The study provides practical RL approaches, allowing improvements in reasoning abilities without sacrificing diversity, vital for scalable real-world deployments.

Abstract: Reinforcement learning (RL) has emerged as a powerful method for improving
the reasoning abilities of large language models (LLMs). Outcome-based RL,
which rewards policies solely for the correctness of the final answer, yields
substantial accuracy gains but also induces a systematic loss in generation
diversity. This collapse undermines real-world performance, where diversity is
critical for test-time scaling. We analyze this phenomenon by viewing RL
post-training as a sampling process and show that, strikingly, RL can reduce
effective diversity even on the training set relative to the base model. Our
study highlights two central findings: (i) a transfer of diversity degradation,
where reduced diversity on solved problems propagates to unsolved ones, and
(ii) the tractability of the outcome space, since reasoning tasks admit only a
limited set of distinct answers. Motivated by these insights, we propose
outcome-based exploration, which assigns exploration bonuses according to final
outcomes. We introduce two complementary algorithms: historical exploration,
which encourages rarely observed answers via UCB-style bonuses, and batch
exploration, which penalizes within-batch repetition to promote test-time
diversity. Experiments on standard competition math with Llama and Qwen models
demonstrate that both methods improve accuracy while mitigating diversity
collapse. On the theoretical side, we formalize the benefit of outcome-based
exploration through a new model of outcome-based bandits. Together, these
contributions chart a practical path toward RL methods that enhance reasoning
without sacrificing the diversity essential for scalable deployment.

</details>


### [364] [Probabilistic Modeling of Latent Agentic Substructures in Deep Neural Networks](https://arxiv.org/abs/2509.06701)
*Su Hyeong Lee,Risi Kondor,Richard Ngo*

Main category: cs.LG

TL;DR: The paper presents a mathematical framework for understanding intelligent agency in neural models, exploring outcome distributions and alignment phenomena in AI systems.


<details>
  <summary>Details</summary>
Motivation: To create a principled framework for understanding and aligning complex agentic behavior in AI systems.

Method: Develops theoretical underpinnings based on probabilistic modeling, log score epistemic utility, and weighted logarithmic pooling, with a focus on recursive structure and agentic alignment.

Result: Shows strict unanimity challenges in linear pooling and binary spaces, proposes solutions with more outcomes, and applies the theory to formalize alignment phenomena in LLMs.

Conclusion: Establishes a foundation for how subagents can merge into coherent entities with implications for better alignment in agentic AI technologies.

Abstract: We develop a theory of intelligent agency grounded in probabilistic modeling
for neural models. Agents are represented as outcome distributions with
epistemic utility given by log score, and compositions are defined through
weighted logarithmic pooling that strictly improves every member's welfare. We
prove that strict unanimity is impossible under linear pooling or in binary
outcome spaces, but possible with three or more outcomes. Our framework admits
recursive structure via cloning invariance, continuity, and openness, while
tilt-based analysis rules out trivial duplication. Finally, we formalize an
agentic alignment phenomenon in LLMs using our theory: eliciting a benevolent
persona ("Luigi'") induces an antagonistic counterpart ("Waluigi"), while a
manifest-then-suppress Waluigi strategy yields strictly larger first-order
misalignment reduction than pure Luigi reinforcement alone. These results
clarify how developing a principled mathematical framework for how subagents
can coalesce into coherent higher-level entities provides novel implications
for alignment in agentic AI systems.

</details>


### [365] [Nested Optimal Transport Distances](https://arxiv.org/abs/2509.06702)
*Ruben Bontorno,Songyan Hou*

Main category: cs.LG

TL;DR: The paper introduces a robust evaluation metric for generative AI models used in financial time series and develops an efficient algorithm for its computation.


<details>
  <summary>Details</summary>
Motivation: The need for realistic simulation of financial time series for applications like stress testing and scenario generation, coupled with the lack of a consensus metric for evaluating generative models.

Method: The paper employs the nested optimal transport distance metric, tailored for time-causal properties, and introduces a statistically consistent and parallelizable algorithm to compute it efficiently.

Result: The proposed algorithm significantly outperforms existing methods in terms of computational speed.

Conclusion: The work provides a robust tool for evaluating generative AI models in finance, advancing their application in decision-making tasks like hedging and reinforcement learning.

Abstract: Simulating realistic financial time series is essential for stress testing,
scenario generation, and decision-making under uncertainty. Despite advances in
deep generative models, there is no consensus metric for their evaluation. We
focus on generative AI for financial time series in decision-making
applications and employ the nested optimal transport distance, a time-causal
variant of optimal transport distance, which is robust to tasks such as
hedging, optimal stopping, and reinforcement learning. Moreover, we propose a
statistically consistent, naturally parallelizable algorithm for its
computation, achieving substantial speedups over existing approaches.

</details>


### [366] [RT-HCP: Dealing with Inference Delays and Sample Efficiency to Learn Directly on Robotic Platforms](https://arxiv.org/abs/2509.06714)
*Zakariae El Asri,Ibrahim Laiche,Clément Rambour,Olivier Sigaud,Nicolas Thome*

Main category: cs.LG

TL;DR: The paper addresses the challenges of sample efficiency and inference time in model-based RL for robot control, proposing a new framework and algorithm called RT-HCP.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable learning of a robot controller directly on hardware with extreme sample efficiency and to address the issue of slow inference times in model-based reinforcement learning.

Method: The authors introduce a framework to handle inference delays by preloading a sequence of actions for continuous robot operation. They compare multiple RL algorithms and propose RT-HCP for balanced performance, efficiency, and inference time.

Result: RT-HCP outperformed other algorithms in experiments conducted on a high-frequency Furuta pendulum platform, demonstrating its effectiveness.

Conclusion: The proposed framework and RT-HCP algorithm show promise for real-world robot applications, combining sample efficiency and practical execution speed.

Abstract: Learning a controller directly on the robot requires extreme sample
efficiency. Model-based reinforcement learning (RL) methods are the most sample
efficient, but they often suffer from a too long inference time to meet the
robot control frequency requirements. In this paper, we address the sample
efficiency and inference time challenges with two contributions. First, we
define a general framework to deal with inference delays where the slow
inference robot controller provides a sequence of actions to feed the
control-hungry robotic platform without execution gaps. Then, we compare
several RL algorithms in the light of this framework and propose RT-HCP, an
algorithm that offers an excellent trade-off between performance, sample
efficiency and inference time. We validate the superiority of RT-HCP with
experiments where we learn a controller directly on a simple but high frequency
FURUTA pendulum platform. Code: github.com/elasriz/RTHCP

</details>


### [367] [Long-Range Graph Wavelet Networks](https://arxiv.org/abs/2509.06743)
*Filippo Guerranti,Fabrizio Forte,Simon Geisler,Stephan Günnemann*

Main category: cs.LG

TL;DR: The paper introduces Long-Range Graph Wavelet Networks (LR-GWN), combining local and global graph information for better long-range interaction modeling.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of modeling long-range interactions in graph machine learning using wavelet-based methods that are limited by finite-order polynomial approximations.

Method: The paper proposes LR-GWN, which decomposes wavelet filters into local and global components using low-order polynomials for local structures and a spectral domain parameterization for long-range interactions.

Result: Experimental results show that LR-GWN achieves state-of-the-art performance for long-range benchmarks and remains competitive for short-range tasks.

Conclusion: LR-GWN successfully unites local and global information flows in a wavelet framework, enhancing performance in both long-range and short-range graph learning tasks.

Abstract: Modeling long-range interactions, the propagation of information across
distant parts of a graph, is a central challenge in graph machine learning.
Graph wavelets, inspired by multi-resolution signal processing, provide a
principled way to capture both local and global structures. However, existing
wavelet-based graph neural networks rely on finite-order polynomial
approximations, which limit their receptive fields and hinder long-range
propagation. We propose Long-Range Graph Wavelet Networks (LR-GWN), which
decompose wavelet filters into complementary local and global components. Local
aggregation is handled with efficient low-order polynomials, while long-range
interactions are captured through a flexible spectral domain parameterization.
This hybrid design unifies short- and long-distance information flow within a
principled wavelet framework. Experiments show that LR-GWN achieves
state-of-the-art performance among wavelet-based methods on long-range
benchmarks, while remaining competitive on short-range datasets.

</details>


### [368] [Aligning Large Vision-Language Models by Deep Reinforcement Learning and Direct Preference Optimization](https://arxiv.org/abs/2509.06759)
*Thanh Thi Nguyen,Campbell Wilson,Janis Dalins*

Main category: cs.LG

TL;DR: The paper reviews fine-tuning strategies for Large Vision-Language Models (LVLMs), focusing on Deep Reinforcement Learning (DRL) and Direct Preference Optimization (DPO) methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of aligning LVLMs with human values, preferences, and specialized tasks to improve their utility and adaptability.

Method: The study examines DRL and DPO approaches for fine-tuning LVLMs. DRL employs reward signals to optimize actions, while DPO directly aligns models with preferences, avoiding explicit reward models.

Result: The paper categorizes fine-tuning paradigms, evaluates preference data and reward signal sources, and highlights challenges like scalability, efficiency, and safety in aligning LVLMs.

Conclusion: DRL and DPO techniques are pivotal for advancing robust and human-aligned LVLMs that excel in multimodal tasks while addressing pressing challenges like generalization and continual learning.

Abstract: Large Vision-Language Models (LVLMs) or multimodal large language models
represent a significant advancement in artificial intelligence, enabling
systems to understand and generate content across both visual and textual
modalities. While large-scale pretraining has driven substantial progress,
fine-tuning these models for aligning with human values or engaging in specific
tasks or behaviors remains a critical challenge. Deep Reinforcement Learning
(DRL) and Direct Preference Optimization (DPO) offer promising frameworks for
this aligning process. While DRL enables models to optimize actions using
reward signals instead of relying solely on supervised preference data, DPO
directly aligns the policy with preferences, eliminating the need for an
explicit reward model. This overview explores paradigms for fine-tuning LVLMs,
highlighting how DRL and DPO techniques can be used to align models with human
preferences and values, improve task performance, and enable adaptive
multimodal interaction. We categorize key approaches, examine sources of
preference data, reward signals, and discuss open challenges such as
scalability, sample efficiency, continual learning, generalization, and safety.
The goal is to provide a clear understanding of how DRL and DPO contribute to
the evolution of robust and human-aligned LVLMs.

</details>


### [369] [Asynchronous Message Passing for Addressing Oversquashing in Graph Neural Networks](https://arxiv.org/abs/2509.06777)
*Kushal Bose,Swagatam Das*

Main category: cs.LG

TL;DR: Graph Neural Networks (GNNs) struggle with information bottlenecks in long-range tasks due to oversquashing. The authors propose an efficient framework that updates node features asynchronously to mitigate bottlenecks and achieve better performance.


<details>
  <summary>Details</summary>
Motivation: Graph Neural Networks face challenges with oversquashing, compromising long-range task performance due to bottlenecks limiting message propagation.

Method: The authors introduce an asynchronous node feature update mechanism that processes information sequentially via batches based on node centrality values, instead of using traditional synchronous updates.

Result: The proposed approach achieves significant performance improvements, including a 5% and 4% boost on REDDIT-BINARY and Peptides-struct datasets, respectively, across graph classification tasks.

Conclusion: The framework effectively alleviates information bottlenecks and maintains better feature sensitivity, demonstrating its potential to handle complex long-range tasks in GNNs.

Abstract: Graph Neural Networks (GNNs) suffer from Oversquashing, which occurs when
tasks require long-range interactions. The problem arises from the presence of
bottlenecks that limit the propagation of messages among distant nodes.
Recently, graph rewiring methods modify edge connectivity and are expected to
perform well on long-range tasks. Yet, graph rewiring compromises the inductive
bias, incurring significant information loss in solving the downstream task.
Furthermore, increasing channel capacity may overcome information bottlenecks
but enhance the parameter complexity of the model. To alleviate these
shortcomings, we propose an efficient model-agnostic framework that
asynchronously updates node features, unlike traditional synchronous message
passing GNNs. Our framework creates node batches in every layer based on the
node centrality values. The features of the nodes belonging to these batches
will only get updated. Asynchronous message updates process information
sequentially across layers, avoiding simultaneous compression into
fixed-capacity channels. We also theoretically establish that our proposed
framework maintains higher feature sensitivity bounds compared to standard
synchronous approaches. Our framework is applied to six standard graph datasets
and two long-range datasets to perform graph classification and achieves
impressive performances with a $5\%$ and $4\%$ improvements on REDDIT-BINARY
and Peptides-struct, respectively.

</details>


### [370] [Physics-informed Value Learner for Offline Goal-Conditioned Reinforcement Learning](https://arxiv.org/abs/2509.06782)
*Vittorio Giammarino,Ruiqi Ni,Ahmed H. Qureshi*

Main category: cs.LG

TL;DR: This paper introduces a physics-informed regularized loss for offline goal-conditioned reinforcement learning to improve value learning and generalization in tasks with limited data coverage.


<details>
  <summary>Details</summary>
Motivation: Offline GCRL is a promising approach for areas like navigation where data collection is expensive and risky, but faces challenges like limited state-action space coverage and long-horizon task generalization.

Method: The paper derives a physics-informed regularized loss based on the Eikonal PDE to impose geometric inductive bias on the value function, compatible with existing algorithms and integrated into HIQL to create Pi-HIQL.

Result: Pi-HIQL demonstrates improved performance, generalization capabilities, and effectiveness in complex tasks, especially for stitching regimes and large-scale navigation.

Conclusion: Incorporating physics-informed regularization enhances offline GCRL methods, aiding in overcoming practical limitations while boosting task performance and generalizability.

Abstract: Offline Goal-Conditioned Reinforcement Learning (GCRL) holds great promise
for domains such as autonomous navigation and locomotion, where collecting
interactive data is costly and unsafe. However, it remains challenging in
practice due to the need to learn from datasets with limited coverage of the
state-action space and to generalize across long-horizon tasks. To improve on
these challenges, we propose a Physics-informed (Pi) regularized loss for value
learning, derived from the Eikonal Partial Differential Equation (PDE) and
which induces a geometric inductive bias in the learned value function. Unlike
generic gradient penalties that are primarily used to stabilize training, our
formulation is grounded in continuous-time optimal control and encourages value
functions to align with cost-to-go structures. The proposed regularizer is
broadly compatible with temporal-difference-based value learning and can be
integrated into existing Offline GCRL algorithms. When combined with
Hierarchical Implicit Q-Learning (HIQL), the resulting method, Physics-informed
HIQL (Pi-HIQL), yields significant improvements in both performance and
generalization, with pronounced gains in stitching regimes and large-scale
navigation tasks.

</details>


### [371] [\texttt{R$^\textbf{2}$AI}: Towards Resistant and Resilient AI in an Evolving World](https://arxiv.org/abs/2509.06786)
*Youbang Sun,Xiang Wang,Jie Fu,Chaochao Lu,Bowen Zhou*

Main category: cs.LG

TL;DR: This paper introduces 'safe-by-coevolution' as an approach to AI safety inspired by biological immunity, proposing the R$^2$AI framework for dynamic safety processes.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the gap between advancing AI capabilities and slower progress in AI safety, especially the limitations of current paradigms in open-ended environments.

Method: Proposes the 'safe-by-coevolution' paradigm and the R$^2$AI framework, featuring resistant and resilient AI models, adversarial simulation, and continual feedback loops for safety coevolution.

Result: The R$^2$AI framework integrates fast and slow safe models and dynamic safety methods to tackle near-term vulnerabilities and long-term risks.

Conclusion: The approach provides a proactive and scalable framework to maintain AI safety as it evolves, addressing both immediate and existential risks in dynamic environments.

Abstract: In this position paper, we address the persistent gap between rapidly growing
AI capabilities and lagging safety progress. Existing paradigms divide into
``Make AI Safe'', which applies post-hoc alignment and guardrails but remains
brittle and reactive, and ``Make Safe AI'', which emphasizes intrinsic safety
but struggles to address unforeseen risks in open-ended environments. We
therefore propose \textit{safe-by-coevolution} as a new formulation of the
``Make Safe AI'' paradigm, inspired by biological immunity, in which safety
becomes a dynamic, adversarial, and ongoing learning process. To operationalize
this vision, we introduce \texttt{R$^2$AI} -- \textit{Resistant and Resilient
AI} -- as a practical framework that unites resistance against known threats
with resilience to unforeseen risks. \texttt{R$^2$AI} integrates \textit{fast
and slow safe models}, adversarial simulation and verification through a
\textit{safety wind tunnel}, and continual feedback loops that guide safety and
capability to coevolve. We argue that this framework offers a scalable and
proactive path to maintain continual safety in dynamic environments, addressing
both near-term vulnerabilities and long-term existential risks as AI advances
toward AGI and ASI.

</details>


### [372] [floq: Training Critics via Flow-Matching for Scaling Compute in Value-Based RL](https://arxiv.org/abs/2509.06863)
*Bhavya Agrawalla,Michal Nauman,Khush Agarwal,Aviral Kumar*

Main category: cs.LG

TL;DR: This paper introduces floq, a reinforcement learning (RL) approach that enhances temporal difference (TD) methods by employing iterative computation similar to generative modeling techniques, resulting in significantly improved performance.


<details>
  <summary>Details</summary>
Motivation: To explore the benefits of iterative computation for TD methods in RL, inspired by its success in language models and diffusion models, and address the limitations in monolithic Q-function representation.

Method: The authors propose floq, which parameterizes Q-functions using a velocity field and trains it with flow-matching techniques, employing TD-learning objectives and numerical integration for iterative computation.

Result: Floq improves RL performance by nearly 1.8x across offline benchmarks and online tasks, demonstrating better capacity scaling than traditional TD-learning methods.

Conclusion: Floq shows that iterative computation is highly effective for value learning in RL, enabling more fine-grained and scalable architectures compared to monolithic approaches.

Abstract: A hallmark of modern large-scale machine learning techniques is the use of
training objectives that provide dense supervision to intermediate
computations, such as teacher forcing the next token in language models or
denoising step-by-step in diffusion models. This enables models to learn
complex functions in a generalizable manner. Motivated by this observation, we
investigate the benefits of iterative computation for temporal difference (TD)
methods in reinforcement learning (RL). Typically they represent value
functions in a monolithic fashion, without iterative compute. We introduce floq
(flow-matching Q-functions), an approach that parameterizes the Q-function
using a velocity field and trains it using techniques from flow-matching,
typically used in generative modeling. This velocity field underneath the flow
is trained using a TD-learning objective, which bootstraps from values produced
by a target velocity field, computed by running multiple steps of numerical
integration. Crucially, floq allows for more fine-grained control and scaling
of the Q-function capacity than monolithic architectures, by appropriately
setting the number of integration steps. Across a suite of challenging offline
RL benchmarks and online fine-tuning tasks, floq improves performance by nearly
1.8x. floq scales capacity far better than standard TD-learning architectures,
highlighting the potential of iterative computation for value learning.

</details>


### [373] [AxelSMOTE: An Agent-Based Oversampling Algorithm for Imbalanced Classification](https://arxiv.org/abs/2509.06875)
*Sukumar Kishanthan,Asela Hevapathige*

Main category: cs.LG

TL;DR: AxelSMOTE is a novel agent-based oversampling method for addressing class imbalance in machine learning datasets, outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: Class imbalance poses significant challenges for machine learning, particularly in handling minority classes. Existing oversampling methods are limited in preserving feature relationships, managing diversity, and preventing overfitting.

Method: AxelSMOTE, based on Axelrod's cultural dissemination model, incorporates four key elements: feature grouping, probabilistic exchange for meaningful interactions, Beta distribution blending, and controlled diversity injection.

Result: AxelSMOTE consistently outperforms state-of-the-art sampling methods across eight imbalanced datasets while remaining computationally efficient.

Conclusion: AxelSMOTE provides an effective solution to class imbalance by addressing limitations of traditional methods and enhancing both performance and diversity control.

Abstract: Class imbalance in machine learning poses a significant challenge, as skewed
datasets often hinder performance on minority classes. Traditional oversampling
techniques, which are commonly used to alleviate class imbalance, have several
drawbacks: they treat features independently, lack similarity-based controls,
limit sample diversity, and fail to manage synthetic variety effectively. To
overcome these issues, we introduce AxelSMOTE, an innovative agent-based
approach that views data instances as autonomous agents engaging in complex
interactions. Based on Axelrod's cultural dissemination model, AxelSMOTE
implements four key innovations: (1) trait-based feature grouping to preserve
correlations; (2) a similarity-based probabilistic exchange mechanism for
meaningful interactions; (3) Beta distribution blending for realistic
interpolation; and (4) controlled diversity injection to avoid overfitting.
Experiments on eight imbalanced datasets demonstrate that AxelSMOTE outperforms
state-of-the-art sampling methods while maintaining computational efficiency.

</details>


### [374] [Tackling the Noisy Elephant in the Room: Label Noise-robust Out-of-Distribution Detection via Loss Correction and Low-rank Decomposition](https://arxiv.org/abs/2509.06918)
*Tarhib Al Azad,Shahana Ibrahim*

Main category: cs.LG

TL;DR: The paper introduces a robust OOD detection framework that addresses the challenges posed by noisy training labels using a combination of loss correction and low-rank sparse decomposition techniques.


<details>
  <summary>Details</summary>
Motivation: To enhance OOD detection in AI systems for safety-critical applications, especially under the unexplored scenario of noisy training labels, where existing methods fail to maintain effectiveness.

Method: The proposed method integrates loss correction strategies from noisy label learning literature with low-rank and sparse decomposition methods from signal processing to create a robust OOD detection framework.

Result: The framework outperformed state-of-the-art OOD detection techniques in extensive experiments on synthetic and real-world datasets, especially in scenarios with severe noisy labels.

Conclusion: The paper offers a novel, effective framework for OOD detection under challenging conditions of noisy label settings, marking a significant improvement over existing methods.

Abstract: Robust out-of-distribution (OOD) detection is an indispensable component of
modern artificial intelligence (AI) systems, especially in safety-critical
applications where models must identify inputs from unfamiliar classes not seen
during training. While OOD detection has been extensively studied in the
machine learning literature--with both post hoc and training-based
approaches--its effectiveness under noisy training labels remains
underexplored. Recent studies suggest that label noise can significantly
degrade OOD performance, yet principled solutions to this issue are lacking. In
this work, we demonstrate that directly combining existing label noise-robust
methods with OOD detection strategies is insufficient to address this critical
challenge. To overcome this, we propose a robust OOD detection framework that
integrates loss correction techniques from the noisy label learning literature
with low-rank and sparse decomposition methods from signal processing.
Extensive experiments on both synthetic and real-world datasets demonstrate
that our method significantly outperforms the state-of-the-art OOD detection
techniques, particularly under severe noisy label settings.

</details>


### [375] [Staying in the Sweet Spot: Responsive Reasoning Evolution via Capability-Adaptive Hint Scaffolding](https://arxiv.org/abs/2509.06923)
*Ziheng Li,Zexu Sun,Jinman Zhao,Erxue Min,Yongcheng Zeng,Hui Wu,Hengyi Cai,Shuaiqiang Wang,Dawei Yin,Xu Chen,Zhi-Hong Deng*

Main category: cs.LG

TL;DR: SEELE is a new framework for reinforcement learning with verifiable rewards, which dynamically adjusts problem difficulty using hints to improve exploration efficiency and model capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing RLVR methods are inefficient in exploration due to mismatches between problem difficulty and model capability, resulting in suboptimal learning outcomes.

Method: SEELE introduces an adaptive training system that appends hints to problems and adjusts their length dynamically using a multi-round rollout sampling strategy informed by item response theory.

Result: Experimental results show that SEELE achieves superior performance, outperforming GRPO and SFT by +11.8 and +10.5 points respectively, and surpasses prior hint-based methods by +3.6 points on math benchmarks.

Conclusion: SEELE enhances reasoning capabilities and exploration efficiency in LLMs by optimizing the relationship between problem difficulty and model abilities, demonstrating its effectiveness across benchmarks.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has achieved remarkable
success in enhancing the reasoning capabilities of large language models
(LLMs). However, existing RLVR methods often suffer from exploration
inefficiency due to mismatches between the training data's difficulty and the
model's capability. LLMs fail to discover viable reasoning paths when problems
are overly difficult, while learning little new capability when problems are
too simple. In this work, we formalize the impact of problem difficulty by
quantifying the relationship between loss descent speed and rollout accuracy.
Building on this analysis, we propose SEELE, a novel supervision-aided RLVR
framework that dynamically adjusts problem difficulty to stay within the
high-efficiency region. SEELE augments each training sample by appending a hint
(part of a full solution) after the original problem. Unlike previous
hint-based approaches, SEELE deliberately and adaptively adjusts the hint
length for each problem to achieve an optimal difficulty. To determine the
optimal hint length, SEELE employs a multi-round rollout sampling strategy. In
each round, it fits an item response theory model to the accuracy-hint pairs
collected in preceding rounds to predict the required hint length for the next
round. This instance-level, real-time difficulty adjustment aligns problem
difficulty with the evolving model capability, thereby improving exploration
efficiency. Experimental results show that SEELE outperforms Group Relative
Policy Optimization (GRPO) and Supervised Fine-tuning (SFT) by +11.8 and +10.5
points, respectively, and surpasses the best previous supervision-aided
approach by +3.6 points on average across six math reasoning benchmarks.

</details>


### [376] [Neutron Reflectometry by Gradient Descent](https://arxiv.org/abs/2509.06924)
*Max D. ~Champneys,Andrew J. ~Parnell,Philipp Gutfreund,Maximilian W. A. Skoda,. Patrick A. Fairclough,Timothy J. ~Rogers,Stephanie L. ~Burg*

Main category: cs.LG

TL;DR: This paper proposes using gradient descent with automatic differentiation for neutron reflectometry data analysis to improve efficiency and accuracy, presenting benchmark studies and an open-source library.


<details>
  <summary>Details</summary>
Motivation: Neutron reflectometry suffers inefficiencies in inverse modeling for data analysis in complex structures, while current machine learning approaches lose physical intuition.

Method: Introduces gradient descent on forward reflection models using automatic differentiation to compute exact error gradients for optimization.

Result: Demonstrated improved performance on benchmark cases, including thick oxide quartz film and complex organic LED multilayer devices.

Conclusion: Gradient-based approaches enhance the optimization in neutron reflectometry and facilitate modern inference techniques, supported by an open-source library.

Abstract: Neutron reflectometry (NR) is a powerful technique to probe surfaces and
interfaces. NR is inherently an indirect measurement technique, access to the
physical quantities of interest (layer thickness, scattering length density,
roughness), necessitate the solution of an inverse modelling problem, that is
inefficient for large amounts of data or complex multiplayer structures (e.g.
lithium batteries / electrodes). Recently, surrogate machine learning models
have been proposed as an alternative to existing optimisation routines.
Although such approaches have been successful, physical intuition is lost when
replacing governing equations with fast neural networks. Instead, we propose a
novel and efficient approach; to optimise reflectivity data analysis by
performing gradient descent on the forward reflection model itself. Herein,
automatic differentiation techniques are used to evaluate exact gradients of
the error function with respect to the parameters of interest. Access to these
quantities enables users of neutron reflectometry to harness a host of powerful
modern optimisation and inference techniques that remain thus far unexploited
in the context of neutron reflectometry. This paper presents two benchmark case
studies; demonstrating state-of-the-art performance on a thick oxide quartz
film, and robust co-fitting performance in the high complexity regime of
organic LED multilayer devices. Additionally, we provide an open-source library
of differentiable reflectometry kernels in the python programming language so
that gradient based approaches can readily be applied to other NR datasets.

</details>


### [377] [Learning words in groups: fusion algebras, tensor ranks and grokking](https://arxiv.org/abs/2509.06931)
*Maor Shutman,Oren Louidor,Ran Tessler*

Main category: cs.LG

TL;DR: This paper demonstrates how a simple two-layer neural network can learn arbitrary word operations in finite groups, leveraging low-rank tensor decomposition methods.


<details>
  <summary>Details</summary>
Motivation: To understand and explain how neural networks with standard activation functions can generalize efficiently when learning complex operations, such as arbitrary word operations in finite groups.

Method: The study reframed the learning problem as identifying a low-rank $3$-tensor, which can be decomposed using triplets of self-conjugate group representations and utilizing their fusion structure. A surrogate model was used to analyze generalization behavior and low-rank tensor approximation.

Result: Neural networks were shown to learn low-rank tensor implementations efficiently, even achieving Strassen-like efficient matrix multiplication for specific cases. The mechanism of gradient descent in finding these solutions was also analyzed.

Conclusion: Neural networks exploit low-rank structures to achieve generalization and efficient computation, shedding light on their learning dynamics in structured mathematical problems.

Abstract: In this work, we demonstrate that a simple two-layer neural network with
standard activation functions can learn an arbitrary word operation in any
finite group, provided sufficient width is available and exhibits grokking
while doing so. To explain the mechanism by which this is achieved, we reframe
the problem as that of learning a particular $3$-tensor, which we show is
typically of low rank. A key insight is that low-rank implementations of this
tensor can be obtained by decomposing it along triplets of basic self-conjugate
representations of the group and leveraging the fusion structure to rule out
many components. Focusing on a phenomenologically similar but more tractable
surrogate model, we show that the network is able to find such low-rank
implementations (or approximations thereof), thereby using limited width to
approximate the word-tensor in a generalizable way. In the case of the simple
multiplication word, we further elucidate the form of these low-rank
implementations, showing that the network effectively implements efficient
matrix multiplication in the sense of Strassen. Our work also sheds light on
the mechanism by which a network reaches such a solution under gradient
descent.

</details>


### [378] [From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers](https://arxiv.org/abs/2509.06938)
*Praneet Suresh,Jack Stanley,Sonia Joseph,Luca Scimeca,Danilo Bzdok*

Main category: cs.LG

TL;DR: The paper investigates how pre-trained transformer models exhibit hallucinations, presenting a mechanism for predicting such occurrences based on concept patterns in their activations.


<details>
  <summary>Details</summary>
Motivation: The research addresses the critical issue of understanding generative AI failure modes like hallucinations, which hinder their trust and application in high-stakes areas.

Method: The authors employ sparse autoencoders to extract concept representations from transformer models, conducting controlled experiments with varying input uncertainty.

Result: The study finds that transformer models activate coherent semantic features under growing input uncertainty, leading to hallucinated outputs, even for pure-noise inputs.

Conclusion: By identifying robust concept patterns linked to hallucinations, the research provides tools for predicting and mitigating such behavior, impacting AI safety, alignment, and vulnerability detection.

Abstract: As generative AI systems become competent and democratized in science,
business, and government, deeper insight into their failure modes now poses an
acute need. The occasional volatility in their behavior, such as the propensity
of transformer models to hallucinate, impedes trust and adoption of emerging AI
solutions in high-stakes areas. In the present work, we establish how and when
hallucinations arise in pre-trained transformer models through concept
representations captured by sparse autoencoders, under scenarios with
experimentally controlled uncertainty in the input space. Our systematic
experiments reveal that the number of semantic concepts used by the transformer
model grows as the input information becomes increasingly unstructured. In the
face of growing uncertainty in the input space, the transformer model becomes
prone to activate coherent yet input-insensitive semantic features, leading to
hallucinated output. At its extreme, for pure-noise inputs, we identify a wide
variety of robustly triggered and meaningful concepts in the intermediate
activations of pre-trained transformer models, whose functional integrity we
confirm through targeted steering. We also show that hallucinations in the
output of a transformer model can be reliably predicted from the concept
patterns embedded in transformer layer activations. This collection of insights
on transformer internal processing mechanics has immediate consequences for
aligning AI models with human values, AI safety, opening the attack surface for
potential adversarial attacks, and providing a basis for automatic
quantification of a model's hallucination risk.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [379] [Robustness and Invariance of Hybrid Metaheuristics under Objective Function Transformations](https://arxiv.org/abs/2509.05445)
*Grzegorz Sroka,Sławomir T. Wierzchoń*

Main category: cs.NE

TL;DR: The paper evaluates the robustness of hybrid metaheuristics under various transformations, showing differential-based hybrids outperform classical methods.


<details>
  <summary>Details</summary>
Motivation: To assess how robust and structurally invariant hybrid population-based metaheuristics are when faced with transformations in objective space.

Method: The study applies a hybridization operator to 19 state-of-the-art metaheuristics and benchmarks them on CEC-2017 across different transformations and dimensions, using statistical tools for comparison.

Result: Differential-based hybrids exhibit high accuracy and stability across transformations, while classical methods, especially PSO and HHO variants, suffer under distorted landscapes.

Conclusion: Adaptive hybrids show resilience and are superior for optimization tasks involving domain-specific objective space transformations.

Abstract: This paper evaluates the robustness and structural invariance of hybrid
population-based metaheuristics under various objective space transformations.
A lightweight plug-and-play hybridization operator is applied to nineteen
state-of-the-art algorithms-including differential evolution (DE), particle
swarm optimization (PSO), and recent bio-inspired methods-without modifying
their internal logic. Benchmarking on the CEC-2017 suite across four dimensions
(10, 30, 50, 100) is performed under five transformation types: baseline,
translation, scaling, rotation, and constant shift. Statistical comparisons
based on Wilcoxon and Friedman tests, Bayesian dominance analysis, and
convergence trajectory profiling consistently show that differential-based
hybrids (e.g., hIMODE, hSHADE, hDMSSA) maintain high accuracy, stability, and
invariance under all tested deformations. In contrast, classical
algorithms-especially PSO- and HHO-based variants-exhibit significant
performance degradation under non-separable or distorted landscapes. The
findings confirm the superiority of adaptive, structurally resilient hybrids
for real-world optimization tasks subject to domain-specific transformations.

</details>


### [380] [Genesis: A Spiking Neuromorphic Accelerator With On-chip Continual Learning](https://arxiv.org/abs/2509.05858)
*Vedant Karia,Abdullah Zyarah,Dhireesha Kudithipudi*

Main category: cs.NE

TL;DR: This paper introduces Genesis, a spiking continual learning accelerator designed for efficient memory and energy use, achieving a classification accuracy of 74.6% on split-MNIST while operating at low power.


<details>
  <summary>Details</summary>
Motivation: To enable continual learning in artificial systems, akin to biological brains, while addressing challenges of increased memory and computational demands, particularly on platforms with limited resources.

Method: The study proposes Genesis, featuring neurally inspired mechanisms like activity-dependent metaplasticity to mitigate catastrophic forgetting, alongside innovations like low-precision parameters, a data movement strategy for sparsity, and optimized memory mapping.

Result: Genesis achieves a mean classification accuracy of 74.6% on a split-MNIST task-agnostic benchmark, consuming only 17.08mW on a 65nm technology node.

Conclusion: High accuracy and energy efficiency in Genesis demonstrate its potential to advance continual learning systems, making them viable for deployment in resource-constrained environments.

Abstract: Continual learning, the ability to acquire and transfer knowledge through a
models lifetime, is critical for artificial agents that interact in real-world
environments. Biological brains inherently demonstrate these capabilities while
operating within limited energy and resource budgets. Achieving continual
learning capability in artificial systems considerably increases memory and
computational demands, and even more so when deploying on platforms with
limited resources. In this work, Genesis, a spiking continual learning
accelerator, is proposed to address this gap. The architecture supports
neurally inspired mechanisms, such as activity-dependent metaplasticity, to
alleviate catastrophic forgetting. It integrates low-precision continual
learning parametersand employs a custom data movement strategy to accommodate
the sparsely distributed spikes. Furthermore, the architecture features a
memory mapping technique that places metaplasticity parameters and synaptic
weights in a single address location for faster memory access. Results show
that the mean classification accuracy for Genesis is 74.6% on a task-agnostic
split-MNIST benchmark with power consumption of 17.08mW in a 65nm technology
node.

</details>


### [381] [An Explainable Framework for Particle Swarm Optimization using Landscape Analysis and Machine Learning](https://arxiv.org/abs/2509.06272)
*Nitin Gupta,Bapi Dutta,Anupam Yadav*

Main category: cs.NE

TL;DR: The paper investigates Particle Swarm Optimization (PSO) by studying swarm topologies and proposes an explainable benchmarking framework, alongside a machine learning-based method for automated configuration.


<details>
  <summary>Details</summary>
Motivation: To address the lack of transparency in understanding how PSO algorithmic components contribute to performance and advance interpretability and explainability in swarm intelligence algorithms.

Method: Uses Exploratory Landscape Analysis (ELA) to characterize problem landscapes, evaluates swarm topologies (Ring, Star, Von Neumann) using empirical studies, and introduces a machine learning approach for predicting optimal configurations based on Area over the Convergence Curve (AOCC) data.

Result: The study identifies how different swarm communication topologies influence exploration, convergence, and performance metrics and provides practical guidelines for topology and parameter selection using experimental data.

Conclusion: The work enhances transparency and interpretability in PSO, enabling improved algorithm configuration and development of more reliable swarm intelligence systems with publicly available source codes.

Abstract: Swarm intelligence algorithms have demonstrated remarkable success in solving
complex optimization problems across diverse domains. However, their widespread
adoption is often hindered by limited transparency in how algorithmic
components influence performance. This work presents a multi-faceted
investigation of Particle Swarm Optimization (PSO) to further understand the
key role of different topologies for better interpretability and
explainability. To achieve this objective, we first develop a comprehensive
landscape characterization framework using Exploratory Landscape Analysis (ELA)
to quantify problem difficulty and identify critical features affecting the
optimization performance of PSO. Next, we conduct a rigorous empirical study
comparing three fundamental swarm communication architectures -- Ring, Star,
and Von Neumann topologies -- analysing their distinct impacts on
exploration-exploitation balance, convergence behaviour, and solution quality
and eventually develop an explainable benchmarking framework for PSO, to decode
how swarm topologies affects information flow, diversity, and convergence.
Based on this, a novel machine learning approach for automated algorithm
configuration is introduced for training predictive models on extensive Area
over the Convergence Curve (AOCC) data to recommend optimal settings based on
problem characteristics. Through systematic experimentation across twenty four
benchmark functions in multiple dimensions, we establish practical guidelines
for topology selection and parameter configuration. These findings advance the
development of more transparent and reliable swarm intelligence systems. The
source codes of this work can be accessed at
https://github.com/GitNitin02/ioh_pso.

</details>


### [382] [Full Integer Arithmetic Online Training for Spiking Neural Networks](https://arxiv.org/abs/2509.06636)
*Ismael Gomez,Guangzhi Tang*

Main category: cs.NE

TL;DR: The paper introduces an integer-only, online training algorithm for Spiking Neural Networks (SNNs) using a mixed-precision approach that improves efficiency and reduces memory usage.


<details>
  <summary>Details</summary>
Motivation: SNNs are biologically plausible and energy-efficient but training methods like BPTT and RTRL are computationally intensive, leading to the need for improved methods.

Method: The algorithm employs a mixed-precision approach, replacing floating-point operations with integer arithmetic for hardware-friendly implementation and generalizing to different architectures like CSNNs and RSNNs.

Result: Evaluations on MNIST and SHD datasets show comparable or better accuracy to full-precision models while reducing memory usage by over 60%, with 16-bit shadow and 8- or 12-bit inference weights.

Conclusion: The proposed method is efficient for training SNNs, addressing computational limitations while enabling deployment on resource-constrained hardware without compromising accuracy.

Abstract: Spiking Neural Networks (SNNs) are promising for neuromorphic computing due
to their biological plausibility and energy efficiency. However, training
methods like Backpropagation Through Time (BPTT) and Real Time Recurrent
Learning (RTRL) remain computationally intensive. This work introduces an
integer-only, online training algorithm using a mixed-precision approach to
improve efficiency and reduce memory usage by over 60%. The method replaces
floating-point operations with integer arithmetic to enable hardware-friendly
implementation. It generalizes to Convolutional and Recurrent SNNs (CSNNs,
RSNNs), showing versatility across architectures. Evaluations on MNIST and the
Spiking Heidelberg Digits (SHD) dataset demonstrate that mixed-precision models
achieve accuracy comparable to or better than full-precision baselines using
16-bit shadow and 8- or 12-bit inference weights. Despite some limitations in
low-precision and deeper models, performance remains robust. In conclusion, the
proposed integer-only online learning algorithm presents an effective solution
for efficiently training SNNs, enabling deployment on resource-constrained
neuromorphic hardware without sacrificing accuracy.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [383] [Waltz: Temperature-Aware Cooperative Compression for High-Performance Compression-Based CSDs](https://arxiv.org/abs/2509.05365)
*Dingcui Yu,Yunpeng Song,Yiyang Huang,Yumiao Zhao,Yina Lv,Chundong Wang,Youtao Zhang,Liang Shi*

Main category: cs.PF

TL;DR: This paper evaluates the impact of host-side and device-side data compression on SSD temperature and performance, proposing temperature-aware cooperative compression solutions.


<details>
  <summary>Details</summary>
Motivation: To address SSD storage and lifetime challenges using data compression while mitigating performance and overheating issues inherent in current approaches.

Method: Quantitative experiments evaluate device-side and host-side compression impacts, followed by the introduction of Waltz, a cooperative compression algorithm integrated into F2FS.

Result: Device-side compression overheats SSDs, while host-side compression reduces performance. Waltz is shown to optimize SSD performance, extend lifetime, and prevent overheating.

Conclusion: Temperature-aware cooperative compression strategies like Waltz improve SSD efficiency, balance performance, and control overheating.

Abstract: Data compression is widely adopted for modern solid-state drives (SSDs) to
mitigate both storage capacity and SSD lifetime issues. Researchers have
proposed compression schemes at different system layers, including device-side
solutions like CCSDs ( c ompression-based c omputational SSDs) and compression
supported by host-side, like F2FS (flash-friendly file system). We conduct
quantitative studies to understand how host-side and device-side compression
schemes affect the temperature and performance of SSD-based storage systems.
From our experiments, device-side compression, facilitated by a hardware
compression engine, can raise the temperature of CCSDs to intolerable levels,
resulting in throttling and service shutdown. In contrast, host-side
compression causes software-stack overhead, which often results in large
performance degradation and resource consumption. To ensure efficient data
compression with high performance and better temperature control, we propose
Waltz, a temperature-aware cooperative compression method that schedules
(de)compression tasks at the host and device sides by monitoring device
temperature. Furthermore, we introduce two variants (Waltzs and Waltzp) for
space and performance optimization, respectively. Waltz is implemented within
F2FS, achieving high performance while extending SSD lifetime and preventing
overheating-induced in-flight shutdowns.

</details>


### [384] [Efficient Fault Localization in a Cloud Stack Using End-to-End Application Service Topology](https://arxiv.org/abs/2509.05511)
*Dhanya R Mathews,Mudit Verma,Pooja Aggarwal,J. Lakshmi*

Main category: cs.PF

TL;DR: The paper introduces a Topology-Aware-RCD approach to improve the identification of root causes in performance anomalies, demonstrating significant advancements over existing methods.


<details>
  <summary>Details</summary>
Motivation: The adoption of microservice architecture increases the complexity in managing failures due to numerous interconnected service components. Maintaining service resilience is challenging with the vast observability data generated across cloud service stacks.

Method: The study selects informative metrics based on application service topology and integrates this knowledge into an enhanced Root Cause Detection (RCD) algorithm, termed Topology-Aware-RCD (TA-RCD).

Result: The evaluation proves that TA-RCD significantly outperforms state-of-the-art RCD methods, achieving at least 2X better performance on average for Top-3 and Top-5 recall during failure injection studies.

Conclusion: Incorporating the application service topology approach enhances root cause detection accuracy and explainability, laying a foundation for improved cloud application service resilience and efficiency.

Abstract: Cloud application services are distributed in nature and have components
across the stack working together to deliver the experience to end users. The
wide adoption of microservice architecture exacerbates failure management due
to increased service components. To be effective, the strategies to enhance the
application service resilience need to be autonomous and developed at the
service's granularity, considering its end-to-end components. However, the
massive amount of observability data generated by all these components across
the service stack poses a significant challenge in reacting to anomalies and
restoring the service quality in real time. Identifying the most informative
observability data from across the cloud service stack and timely localization
of root causes of anomalies thus becomes crucial to ensure service resilience.
This article presents a novel approach that considers the application service
topology to select the most informative metrics across the cloud stack to
support efficient, explainable, and accurate root cause identifications in case
of performance anomalies. The usefulness of the selected metrics is then
evaluated using the state-of-the-art Root Cause Detection (RCD) algorithm for
localizing the root cause of performance anomalies. As a step towards improving
the accuracy and efficiency of RCD, this article then proposes the
Topology-Aware-RCD (TA-RCD) that incorporates the end-to-end application
service topology in RCD. The evaluation of the failure injection studies shows
that the proposed approach performs at least 2X times better on average than
the state-of-the-art RCD algorithm regarding Top-3 and Top-5 recall.

</details>


### [385] [Optimizing Cloud-native Services with SAGA: A Service Affinity Graph-based Approach](https://arxiv.org/abs/2509.05790)
*Hai Dinh-Tuan,Franz Florian Six*

Main category: cs.PF

TL;DR: This paper proposes the Service Affinity Graph-based Approach (SAGA) to optimize service placement in microservice-based architectures, achieving notable efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: Modern cloud-native microservice-based systems face challenges in maintaining end-to-end quality of service due to their dynamic, distributed nature. This necessitates innovative optimization techniques.

Method: The paper introduces the SAGA framework, which models microservice interactions using a graph-based approach and formulates service placement as a minimum-weight k-cut problem. It employs an approximation algorithm for effective service clustering.

Result: The empirical evaluation of SAGA, deployed on a Kubernetes cluster, resulted in a 23.40% improvement in mean latency, showcasing its effectiveness.

Conclusion: The proposed SAGA approach provides a systematic and impactful method for microservice placement optimization, addressing performance, privacy, and cost objectives while acknowledging related implications and challenges.

Abstract: Modern software architectures are characterized by their cloud-native,
modular, and microservice-based designs. While these systems are known for
their efficiency, they also face complex challenges in service optimization,
especially in maintaining end-to-end quality of service across dynamically
distributed services. This paper introduces a novel approach using the concept
of Service Affinity to address this challenge. The proposed method, termed
Service Affinity Graph-based Approach, employs a graph-based model to model the
interactions among microservices. It formulates the service placement as a
minimum-weight k-cut problem and utilizes an approximation algorithm for
service clustering. This approach is realized through a conceptual framework
that takes into account a wide range of optimization objectives, ranging from
enhancing application performance and enforcing data privacy to optimizing
operational costs. In addition to presenting the SAGA framework in details,
this paper conducts an in-depth empirical evaluation using a prototype deployed
on a Kubernetes cluster. The results demonstrate a mean latency improvement of
23.40%, validating the effectiveness of our approach. Finally, the paper
comprehensively discusses various aspects of the proposed methods, including
their implications, challenges, and benefits, providing a thorough analysis of
the approach's impact.

</details>


### [386] [Optimizing Stateful Microservice Migration in Kubernetes with MS2M and Forensic Checkpointing](https://arxiv.org/abs/2509.05794)
*Hai Dinh-Tuan,Jialun Jiang*

Main category: cs.PF

TL;DR: The study proposes an optimized migration approach for stateful microservices in Kubernetes, significantly improving downtime and flexibility during migrations.


<details>
  <summary>Details</summary>
Motivation: To address the lack of native support for live migration of stateful microservices in Kubernetes and improve management of distributed services.

Method: The approach integrates the MS2M framework with Kubernetes' FCC feature, while introducing a Threshold-Based Cutoff Mechanism and extending support to StatefulSets.

Result: The proposed solution achieves a 96.986% reduction in downtime for individual Pods during migration and enhances flexibility for StatefulSet-managed services.

Conclusion: This method offers practical strategies to optimize stateful microservice migrations in cloud-native environments, addressing key limitations of existing systems.

Abstract: The widespread adoption of microservices architecture in modern software
systems has emphasized the need for efficient management of distributed
services. While stateless microservices enable straightforward migration,
stateful microservices introduce added complexity due to the need to preserve
in-memory state during migration. However, most container orchestrators,
including Kubernetes, lack native support for live stateful service migration.
This paper proposes an optimized migration scheme for stateful services in
Kubernetes by integrating the Message-based Stateful Microservice Migration
(MS2M) framework with Kubernetes' Forensic Container Checkpointing (FCC)
feature. Key enhancements include support for migrating StatefulSet-managed
Pods and the introduction of a Threshold-Based Cutoff Mechanism to handle high
incoming message rates. Evaluation results demonstrate that MS2M for individual
Pods reduces downtime by 96.986% compared to cold migration methods, while the
StatefulSet approach provides greater flexibility in managing stateful
services. These insights provide practical strategies for optimizing stateful
microservice migration in cloud-native environments.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [387] [Comparing Methods for the Cross-Level Verification of SystemC Peripherals with Symbolic Execution](https://arxiv.org/abs/2509.05504)
*Karl Aaron Rudkowski,Sallar Ahmadi-Pour,Rolf Drechsler*

Main category: cs.PL

TL;DR: The paper presents two approaches, CrosSym and SEFOS, for symbolic execution of peripherals in Virtual Prototypes (VPs). Both surpass state-of-the-art tools by enabling cross-level verification and performing comparably in runtime.


<details>
  <summary>Details</summary>
Motivation: Modern hardware designs are increasingly complex, requiring advanced verification techniques to ensure subsystem functionality. Virtual Prototypes are a key tool for this, but integration with techniques like symbolic execution is hindered by challenges in modifying the SystemC kernel.

Method: The authors developed two tools: CrosSym, which modifies the SystemC kernel, and SEFOS, which modifies a symbolic execution engine. Both are evaluated on peripherals across different abstraction levels.

Result: The tools demonstrated extensive features for various verification tasks, identifying over 300 mutants. SEFOS excels in compatibility with unmodified SystemC kernels and peripherals, while CrosSym achieves better runtime and memory efficiency.

Conclusion: Both CrosSym and SEFOS outperform current state-of-the-art approaches, with comparable runtime performance and the added capability of cross-level verification.

Abstract: Virtual Prototypes (VPs) are important tools in modern hardware development.
At high abstractions, they are often implemented in SystemC and offer early
analysis of increasingly complex designs. These complex designs often combine
one or more processors, interconnects, and peripherals to perform tasks in
hardware or interact with the environment. Verifying these subsystems is a
well-suited task for VPs, as they allow reasoning across different abstraction
levels. While modern verification techniques like symbolic execution can be
seamlessly integrated into VP-based workflows, they require modifications in
the SystemC kernel. Hence, existing approaches therefore modify and replace the
SystemC kernel, or ignore the opportunity of cross-level scenarios completely,
and would not allow focusing on special challenges of particular subsystems
like peripherals. We propose CrosSym and SEFOS, two opposing approaches for a
versatile symbolic execution of peripherals. CrosSym modifies the SystemC
kernel, while SEFOS instead modifies a modern symbolic execution engine. Our
extensive evaluation applies our tools to various peripherals on different
levels of abstractions. Both tools extensive sets of features are demonstrated
for (1) different verification scenarios, and (2) identifying 300+ mutants. In
comparison with each other, SEFOS convinces with the unmodified SystemC kernel
and peripheral, while CrosSym offers slightly better runtime and memory usage.
In comparison to the state-of-the-art, that is limited to Transaction Level
Modelling (TLM), our tools offered comparable runtime, while enabling
cross-level verification with symbolic execution.

</details>


### [388] [Fixed Parameter Tractable Linearizability Monitoring for Stack, Queue and Anagram Agnostic Data Types](https://arxiv.org/abs/2509.05586)
*Lee Zheng Han,Umang Mathur*

Main category: cs.PL

TL;DR: The paper addresses the NP-hard problem of verifying linearizability in concurrent data structures and introduces fixed-parameter tractable algorithms for stacks, queues, and anagram-agnostic data types, leveraging innovative methods to reduce computational complexity.


<details>
  <summary>Details</summary>
Motivation: The authors aim to tackle the computational complexity associated with verifying linearizability in concurrent data structures, an NP-hard problem even for basic types, and to provide practical methods under scenarios of bounded concurrency.

Method: The approach involves leveraging frontier graphs, partition states, grammar-based methods (for stacks), a split-sequence transition system (for queues), and exploiting linearization equivalences (for AADTs) to optimize the verification process.

Result: The proposed methods make linearizability verification for stacks, queues, and AADTs computationally tractable under bounded concurrency, achieving log-linear monitoring for AADTs and leveraging advanced methods like sub-cubic matrix multiplication for others.

Conclusion: The paper unifies tractability guarantees for different types of concurrent data structures under bounded concurrency, offering efficient and scalable approaches to the long-standing problem of verifying linearizability.

Abstract: Verifying linearizability of concurrent data structures is NP-hard, even for
simple types. We present fixed-parameter tractable algorithms for monitoring
stacks, queues, and anagram-agnostic data types (AADTs), parameterized by the
maximum concurrency. Our approach leverages frontier graphs and partition
states to bound the search space. For AADTs, equivalence of linearizations
enables monitoring in log-linear time. For stacks, we introduce a grammar-based
method with a sub-cubic reduction to matrix multiplication, and for queues, a
split-sequence transition system supporting efficient dynamic programming.
These results unify tractability guarantees for both order-sensitive and
anagram-agnostic data types under bounded concurrency.

</details>


### [389] [Pacing Types: Safe Monitoring of Asynchronous Streams](https://arxiv.org/abs/2509.06724)
*Florian Kohn,Arthur Correnson,Jan Baumeister,Bernd Finkbeiner*

Main category: cs.PL

TL;DR: Stream-based monitoring ensures real-time safety for systems like drones by aggregating sensor data. The paper introduces 'pacing types,' a new type system to avoid runtime errors in asynchronous data streams.


<details>
  <summary>Details</summary>
Motivation: Ensuring runtime reliability in stream-based monitors, which are safety-critical for cyber-physical systems, especially under asynchronous data arrival from multiple sensors.

Method: A type system called 'pacing types' was introduced and formalized in a core fragment of the RTLola framework. The authors provided a soundness proof using a novel logical relation.

Result: The implementation of pacing types in RTLola demonstrates its effectiveness in preventing runtime errors in asynchronous stream monitoring.

Conclusion: Pacing types significantly enhance the reliability of stream-based monitoring systems by managing asynchronous data stream challenges effectively.

Abstract: Stream-based monitoring is a real-time safety assurance mechanism for complex
cyber-physical systems such as unmanned aerial vehicles. In this context, a
monitor aggregates streams of input data from sensors and other sources to give
real-time statistics and assessments of the system's health. Since monitors are
safety-critical components, it is crucial to ensure that they are free of
potential runtime errors. One of the central challenges in designing reliable
stream-based monitors is to deal with the asynchronous nature of data streams:
in concrete applications, the different sensors being monitored produce values
at different speeds, and it is the monitor's responsibility to correctly react
to the asynchronous arrival of different streams of values. To ease this
process, modern frameworks for stream-based monitoring such as RTLola feature
an expressive specification language that allows to finely specify data
synchronization policies. While this feature dramatically simplifies the design
of monitors, it can also lead to subtle runtime errors. To mitigate this issue,
this paper presents pacing types, a novel type system implemented in RTLola to
ensure that monitors for asynchronous streams are well-behaved at runtime. We
formalize the essence of pacing types for a core fragment of RTLola, and
present a soundness proof of the pacing type system using a new logical
relation.

</details>


### [390] [Termination Analysis of Linear-Constraint Programs](https://arxiv.org/abs/2509.06752)
*Amir M. Ben-Amram,Samir Genaim,Joël Ouaknine,James Worrell*

Main category: cs.PL

TL;DR: This survey examines techniques for analyzing program termination with linear numerical constraints, focusing on ranking functions, transition invariants, and non-termination proof methods.


<details>
  <summary>Details</summary>
Motivation: Understanding termination behavior of programs with linear constraints is crucial because these programs can exhibit undecidable problems, posing challenges for program correctness and reliability.

Method: The paper systematically explores methods including ranking functions, disjunctive well-founded invariants, and non-termination witnesses while assessing their algorithmic complexity and trade-offs.

Result: Identifies trade-offs between expressive power and computational complexity in various termination analysis techniques for linear constraint-based programs.

Conclusion: Though effective for linear constraints, the discussed methods are limited in scope, excluding real-world programming languages and more abstract models with non-linear or probabilistic elements.

Abstract: This Survey provides an overview of techniques in termination analysis for
programs with numerical variables and transitions defined by linear
constraints. This subarea of program analysis is challenging due to the
existence of undecidable problems, and this Survey systematically explores
approaches that mitigate this inherent difficulty. These include foundational
decidability results, the use of ranking functions, and disjunctive
well-founded transition invariants. The Survey also discusses non-termination
witnesses, used to prove that a program will not halt. We examine the
algorithmic and complexity aspects of these methods, showing how different
approaches offer a trade-off between expressive power and computational
complexity. The Survey does not discuss how termination analysis is performed
on real-world programming languages, nor does it consider more expressive
abstract models that include non-linear arithmetic, probabilistic choice, or
term rewriting systems.

</details>


### [391] [Dato: A Task-Based Programming Model for Dataflow Accelerators](https://arxiv.org/abs/2509.06794)
*Shihan Fang,Hongzheng Chen,Niansong Zhang,Jiajie Li,Han Meng,Adrian Liu,Zhiru Zhang*

Main category: cs.PL

TL;DR: The paper introduces Dato, a Python-embedded programming model, to enhance dataflow accelerators for deep learning workloads by focusing on efficient data communication and hardware mapping.


<details>
  <summary>Details</summary>
Motivation: Deep learning workloads are increasingly bottlenecked by data movement rather than computation. Current programming models either require excessive developer effort or oversimplify communication details, limiting optimization.

Method: The authors propose Dato, a task-based programming model that introduces explicit streaming and sharding types. It maps tasks first virtually and then optimizes them physically for hardware constraints.

Result: Dato achieved up to 84% hardware utilization and 2.81x speedup for certain kernels on AMD Ryzen AI NPU, and near-theoretical performance (98%) on custom systolic arrays with the Alveo FPGA.

Conclusion: Dato balances high performance with ease of development, demonstrating its ability to optimize deep learning workloads on diverse dataflow accelerator hardware.

Abstract: Recent deep learning workloads increasingly push computational demand beyond
what current memory systems can sustain, with many kernels stalling on data
movement rather than computation. While modern dataflow accelerators
incorporate on-chip streaming to mitigate off-chip bandwidth limitations,
existing programming models struggle to harness these capabilities effectively.
Low-level interfaces provide fine-grained control but impose significant
development overhead, whereas high-level tile-based languages abstract away
communication details, restricting optimization and forcing compilers to
reconstruct the intended dataflow. We present Dato, a Python-embedded,
task-based programming model for dataflow accelerators that elevates data
communication and sharding to first-class type constructs. Developers write
programs as a graph of tasks connected via explicit stream types, with sharded
inputs specified using layout types. These tasks are first mapped virtually
onto the accelerator's spatial fabric, and the compiler then generates a
physical mapping that respects hardware constraints. Experimental results on
both AMD Ryzen AI NPU and Alveo FPGA devices demonstrate that Dato achieves
high performance while significantly reducing the burden of writing optimized
code. On the NPU, Dato attains up to 84% hardware utilization for GEMM and
delivers a 2.81x speedup on attention kernels compared to a state-of-the-art
commercial framework. On the FPGA, Dato surpasses leading frameworks in
performance when generating custom systolic arrays, achieving 98% of the
theoretical peak performance.

</details>


### [392] [MIO: Multiverse Debugging in the Face of Input/Output -- Extended Version with Additional Appendices](https://arxiv.org/abs/2509.06845)
*Tom Lauwaerts,Maarten Steevens,Christophe Scholliers*

Main category: cs.PL

TL;DR: A novel multiverse debugging approach addresses challenges with debugging non-deterministic microcontroller programs involving input/output operations, preventing exploration of inaccessible states.


<details>
  <summary>Details</summary>
Motivation: Debugging non-deterministic programs on microcontrollers is difficult, especially with unpredictable input-dependent execution paths where current methods cause inefficiencies and inaccuracies.

Method: The paper introduces new semantics for debugging, ensuring exploration of only reachable states during regular execution, and develops a prototype debugger called MIO using WARDuino WebAssembly virtual machine.

Result: The prototype, MIO, successfully demonstrates the feasibility and efficiency of the approach by enabling debugging on actual hardware, such as a Lego Mindstorms motor and STM32 microcontroller.

Conclusion: This multiverse debugging approach improves the debugging experience for non-deterministic programs by correctly handling input/output operations and preventing the exploration of inaccessible states.

Abstract: Debugging non-deterministic programs on microcontrollers is notoriously
challenging, especially when bugs manifest in unpredictable, input-dependent
execution paths. A recent approach, called multiverse debugging, makes it
easier to debug non-deterministic programs by allowing programmers to explore
all potential execution paths. Current multiverse debuggers enable both forward
and backward traversal of program paths, and some facilitate jumping to any
previously visited states, potentially branching into alternative execution
paths within the state space.
  Unfortunately, debugging programs that involve input/output operations using
existing multiverse debuggers can reveal inaccessible program states, i.e.
states which are not encountered during regular execution. This can
significantly hinder the debugging process, as the programmer may spend
substantial time exploring and examining inaccessible program states, or worse,
may mistakenly assume a bug is present in the code, when in fact, the issue is
caused by the debugger.
  This paper presents a novel approach to multiverse debugging, which can
accommodate a broad spectrum of input/output operations. We provide the
semantics of our approach and prove the correctness of our debugger, ensuring
that despite having support for a wide range of input/output operations the
debugger will only explore those program states which can be reached during
regular execution.
  We have developed a prototype, called MIO, leveraging the WARDuino
WebAssembly virtual machine to demonstrate the feasibility and efficiency of
our techniques. As a demonstration of the approach we highlight a color dial
built with a Lego Mindstorms motor, and color sensor, providing a tangible
example of how our approach enables multiverse debugging for programs running
on an STM32 microcontroller.

</details>


### [393] [Mechanized Metatheory of Forward Reasoning for End-to-End Linearizability Proofs](https://arxiv.org/abs/2509.06872)
*Zachary Kent,Ugur Y. Yavuz,Siddhartha Jayanti,Stephanie Balzer,Guy Blelloch*

Main category: cs.PL

TL;DR: The paper discusses the formalization and mechanization of a "forward reasoning" technique for proving linearizability in concurrent data structures, ensuring verified end-to-end proofs.


<details>
  <summary>Details</summary>
Motivation: The paper aims to reduce the trusted computing base by mechanizing the soundness and completeness proofs of a forward reasoning technique for linearizability.

Method: The authors formalize the forward reasoning approach and implement the proofs in Rocq, demonstrating its applicability through a case study on a simple concurrent register.

Result: Verified mechanized proofs of soundness and completeness for the forward reasoning technique are produced, along with an end-to-end proof of linearizability for a concurrent register.

Conclusion: Formalization and mechanization enhance reliability in proving linearizability, enabling more robust and verified end-to-end proofs of correctness for concurrent systems.

Abstract: In the past decade, many techniques have been developed to prove
linearizability, the gold standard of correctness for concurrent data
structures. Intuitively, linearizability requires that every operation on a
concurrent data structure appears to take place instantaneously, even when
interleaved with other operations. Most recently, Jayanti et al. presented the
first sound and complete "forward reasoning" technique for proving
linearizability that relates the behavior of a concurrent data structure to a
reference atomic data structure as time moves forward. This technique can be
used to produce machine-checked proofs of linearizability in TLA+. However,
while Jayanti et al.'s approach is shown to be sound and complete, a
mechanization of this important metatheoretic result is still outstanding. As a
result, it is not possible to produce verified end-to-end proofs of
linearizability. To reduce the size of this trusted computing base, we
formalize this forward reasoning technique and mechanize proofs of its
soundness and completeness in Rocq. As a case study, we use the approach to
produce a verified end-to-end proof of linearizability for a simple concurrent
register.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [394] [ManipDreamer3D : Synthesizing Plausible Robotic Manipulation Video with Occupancy-aware 3D Trajectory](https://arxiv.org/abs/2509.05314)
*Ying Li,Xiaobao Wei,Xiaowei Chi,Yuming Li,Zhongyu Zhao,Hao Wang,Ningning Ma,Ming Lu,Shanghang Zhang*

Main category: cs.RO

TL;DR: ManipDreamer3D is a framework that creates realistic 3D-aware robotic manipulation videos from images and text input using a trajectory-to-video diffusion model, addressing issues with 3D spatial ambiguity.


<details>
  <summary>Details</summary>
Motivation: To overcome the data scarcity problem in robotic manipulation and tackle the limitations of existing 2D trajectory-dependent video generation methods by providing a 3D-aware solution.

Method: The method involves reconstructing a 3D occupancy map, calculating an optimized 3D trajectory for robotic action, and using a trajectory-to-video diffusion model for generating realistic manipulation videos.

Result: The method delivers enhanced visual quality and plausible robotic manipulation videos with reduced need for human intervention compared to previous approaches.

Conclusion: The novel ManipDreamer3D framework effectively addresses the challenges of robotic video generation by combining 3D trajectory planning and diffusion modeling, offering superior outcomes.

Abstract: Data scarcity continues to be a major challenge in the field of robotic
manipulation. Although diffusion models provide a promising solution for
generating robotic manipulation videos, existing methods largely depend on 2D
trajectories, which inherently face issues with 3D spatial ambiguity. In this
work, we present a novel framework named ManipDreamer3D for generating
plausible 3D-aware robotic manipulation videos from the input image and the
text instruction. Our method combines 3D trajectory planning with a
reconstructed 3D occupancy map created from a third-person perspective, along
with a novel trajectory-to-video diffusion model. Specifically, ManipDreamer3D
first reconstructs the 3D occupancy representation from the input image and
then computes an optimized 3D end-effector trajectory, minimizing path length
while avoiding collisions. Next, we employ a latent editing technique to create
video sequences from the initial image latent and the optimized 3D trajectory.
This process conditions our specially trained trajectory-to-video diffusion
model to produce robotic pick-and-place videos. Our method generates robotic
videos with autonomously planned plausible 3D trajectories, significantly
reducing human intervention requirements. Experimental results demonstrate
superior visual quality compared to existing methods.

</details>


### [395] [Evaluation of Large Language Models for Anomaly Detection in Autonomous Vehicles](https://arxiv.org/abs/2509.05315)
*Petros Loukas,David Bassir,Savvas Chatzichristofis,Angelos Amanatiadis*

Main category: cs.RO

TL;DR: This paper evaluates large language models (LLMs) on real-world edge cases to test their potential as complementary modules in autonomous vehicle perception and planning.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of LLMs in autonomous vehicles are limited to synthetic or manually-driven datasets without ground truth knowledge, lacking insight into how current algorithms perform in challenging cases.

Method: The authors propose an architecture combining open vocabulary object detection, prompt engineering, and LLM-based contextual reasoning to evaluate state-of-the-art models against real-world edge cases.

Result: Qualitative comparisons are conducted for several LLM-based models against real edge cases, showcasing their potential and limitations.

Conclusion: LLMs show promise as anomaly detectors in autonomous vehicles, but further analysis is required for their practical integration.

Abstract: The rapid evolution of large language models (LLMs) has pushed their
boundaries to many applications in various domains. Recently, the research
community has started to evaluate their potential adoption in autonomous
vehicles and especially as complementary modules in the perception and planning
software stacks. However, their evaluation is limited in synthetic datasets or
manually driving datasets without the ground truth knowledge and more
precisely, how the current perception and planning algorithms would perform in
the cases under evaluation. For this reason, this work evaluates LLMs on
real-world edge cases where current autonomous vehicles have been proven to
fail. The proposed architecture consists of an open vocabulary object detector
coupled with prompt engineering and large language model contextual reasoning.
We evaluate several state-of-the-art models against real edge cases and provide
qualitative comparison results along with a discussion on the findings for the
potential application of LLMs as anomaly detectors in autonomous vehicles.

</details>


### [396] [Plantbot: Integrating Plant and Robot through LLM Modular Agent Networks](https://arxiv.org/abs/2509.05338)
*Atsushi Masumori,Norihiro Maruyama,Itsuki Doi,johnsmith,Hiroki Sato,Takashi Ikegami*

Main category: cs.RO

TL;DR: The paper introduces Plantbot, a robot-plant hybrid connected through large language models (LLMs) to enable harmonious interaction between biological and artificial elements for autonomous adaptability.


<details>
  <summary>Details</summary>
Motivation: The researchers aim to explore the integration of biological and artificial systems to enable autonomous, adaptive agents capable of novel interaction paradigms through language-mediated communication.

Method: A decentralized network of LLM modules (handling sensing, vision, dialogue, and action) forms the core communication framework, translating multimodal data into natural language to coordinate adaptive responses.

Result: The Plantbot acts as an embodied, adaptive agent capable of autonomously interpreting and responding to environmental conditions by transforming plant data into robotic actions.

Conclusion: This work demonstrates the potential for using LLMs as hybrid interfaces to create new forms of artificial life that blend biological and robotic systems through decentralized coordination.

Abstract: We introduce Plantbot, a hybrid lifeform that connects a living plant with a
mobile robot through a network of large language model (LLM) modules. Each
module - responsible for sensing, vision, dialogue, or action - operates
asynchronously and communicates via natural language, enabling seamless
interaction across biological and artificial domains. This architecture
leverages the capacity of LLMs to serve as hybrid interfaces, where natural
language functions as a universal protocol, translating multimodal data (soil
moisture, temperature, visual context) into linguistic messages that coordinate
system behaviors. The integrated network transforms plant states into robotic
actions, installing normativity essential for agency within the sensor-motor
loop. By combining biological and robotic elements through LLM-mediated
communication, Plantbot behaves as an embodied, adaptive agent capable of
responding autonomously to environmental conditions. This approach suggests
possibilities for a new model of artificial life, where decentralized, LLM
modules coordination enable novel interactions between biological and
artificial systems.

</details>


### [397] [INF-3DP: Implicit Neural Fields for Collision-Free Multi-Axis 3D Printing](https://arxiv.org/abs/2509.05345)
*Jiasheng Qu,Zhuo Huang,Dezhao Guo,Hailin Sun,Aoran Lyu,Chengkai Dai,Yeung Yam,Guoxin Fang*

Main category: cs.RO

TL;DR: This paper presents a computational framework for multi-axis 3D printing using implicit neural fields, optimizing toolpath generation and motion planning.


<details>
  <summary>Details</summary>
Motivation: To address challenges in multi-axis 3D printing such as toolpath optimization, surface finish quality, support-free printing, and collision-free motion planning.

Method: 3D models are represented as signed distance fields to encode fabrication objectives. Toolpaths for shell and infill are interpolated via implicit neural fields, while printing sequences and motion are optimized through a continuous quaternion field.

Result: INF-based methods achieve notable performance improvements, including up to two orders of magnitude speedup and reduced waypoint-to-surface error.

Conclusion: The framework demonstrates efficacy through testing on complex models and physical experiments using robot-assisted multi-axis systems, proving its potential for 3D printing advancements.

Abstract: We introduce a general, scalable computational framework for multi-axis 3D
printing based on implicit neural fields (INFs) that unifies all stages of
toolpath generation and global collision-free motion planning. In our pipeline,
input models are represented as signed distance fields, with fabrication
objectives such as support-free printing, surface finish quality, and extrusion
control being directly encoded in the optimization of an implicit guidance
field. This unified approach enables toolpath optimization across both surface
and interior domains, allowing shell and infill paths to be generated via
implicit field interpolation. The printing sequence and multi-axis motion are
then jointly optimized over a continuous quaternion field. Our continuous
formulation constructs the evolving printing object as a time-varying SDF,
supporting differentiable global collision handling throughout INF-based motion
planning. Compared to explicit-representation-based methods, INF-3DP achieves
up to two orders of magnitude speedup and significantly reduces
waypoint-to-surface error. We validate our framework on diverse, complex models
and demonstrate its efficiency with physical fabrication experiments using a
robot-assisted multi-axis system.

</details>


### [398] [Human-LLM Synergy in Context-Aware Adaptive Architecture for Scalable Drone Swarm Operation](https://arxiv.org/abs/2509.05355)
*Ahmed R. Sadik,Muhammad Ashfaq,Niko Mäkitalo,Tommi Mikkonen*

Main category: cs.RO

TL;DR: The paper introduces an adaptive drone swarm architecture using a Large Language Model to optimize coordination for disaster response missions.


<details>
  <summary>Details</summary>
Motivation: Improve drone swarm efficiency in dynamic disaster environments by addressing issues with traditional fixed architectures.

Method: Utilize a Large Language Model to determine the optimal architecture (centralized, hierarchical, or holonic) based on real-time mission conditions.

Result: Simulations show improved scalability, energy efficiency, and connectivity compared to static models.

Conclusion: The adaptive architecture is a promising, robust, and scalable solution for real-world disaster response using autonomous drone swarms.

Abstract: The deployment of autonomous drone swarms in disaster response missions
necessitates the development of flexible, scalable, and robust coordination
systems. Traditional fixed architectures struggle to cope with dynamic and
unpredictable environments, leading to inefficiencies in energy consumption and
connectivity. This paper addresses this gap by proposing an adaptive
architecture for drone swarms, leveraging a Large Language Model to dynamically
select the optimal architecture as centralized, hierarchical, or holonic based
on real time mission parameters such as task complexity, swarm size, and
communication stability. Our system addresses the challenges of scalability,
adaptability, and robustness,ensuring efficient energy consumption and
maintaining connectivity under varying conditions. Extensive simulations
demonstrate that our adaptive architecture outperforms traditional static
models in terms of scalability, energy efficiency, and connectivity. These
results highlight the potential of our approach to provide a scalable,
adaptable, and resilient solution for real world disaster response scenarios.

</details>


### [399] [Spiking Neural Networks for Continuous Control via End-to-End Model-Based Learning](https://arxiv.org/abs/2509.05356)
*Justus Huebotter,Pablo Lanillos,Marcel van Gerven,Serge Thill*

Main category: cs.RO

TL;DR: The paper demonstrates that spiking neural networks (SNNs) can be effectively trained for robotic arm control in continuous environments, addressing challenges in high-dimensional motor tasks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of extending the application of spiking neural networks beyond classification tasks to continuous motor control for robotics.

Method: It proposes an end-to-end predictive-control framework that combines Leaky Integrate-and-Fire dynamics with surrogate gradients for joint optimization of a forward model and a policy network.

Result: The approach successfully achieves stable training and precise torque control in both 2D and high-degree-of-freedom 6-DOF robotic tasks. An ablation study highlights the role of initialization, time constants, and regularization.

Conclusion: SNNs are shown to be viable for motor control but require careful tuning of hyperparameters, emphasizing the importance of thoughtful design choices.

Abstract: Despite recent progress in training spiking neural networks (SNNs) for
classification, their application to continuous motor control remains limited.
Here, we demonstrate that fully spiking architectures can be trained end-to-end
to control robotic arms with multiple degrees of freedom in continuous
environments. Our predictive-control framework combines Leaky
Integrate-and-Fire dynamics with surrogate gradients, jointly optimizing a
forward model for dynamics prediction and a policy network for goal-directed
action. We evaluate this approach on both a planar 2D reaching task and a
simulated 6-DOF Franka Emika Panda robot. Results show that SNNs can achieve
stable training and accurate torque control, establishing their viability for
high-dimensional motor tasks. An extensive ablation study highlights the role
of initialization, learnable time constants, and regularization in shaping
training dynamics. We conclude that while stable and effective control can be
achieved, recurrent spiking networks remain highly sensitive to hyperparameter
settings, underscoring the importance of principled design choices.

</details>


### [400] [Long-Horizon Visual Imitation Learning via Plan and Code Reflection](https://arxiv.org/abs/2509.05368)
*Quan Chen,Chenrui Shi,Qi Chen,Yuwei Wu,Zhi Gao,Xintong Zhang,Rui Gao,Kun Wu,Yunde Jia*

Main category: cs.RO

TL;DR: The paper introduces a new framework with reflection modules to tackle challenges in long-horizon visual imitation learning, ensuring coherence and correctness in action plans and executable code.


<details>
  <summary>Details</summary>
Motivation: To address the difficulties in learning from long-horizon demonstrations with complex temporal and spatial dependencies.

Method: The method incorporates two reflection modules: one for plan verification and refinement, and another for code verification and refinement, paired with a new benchmark called LongVILBench.

Result: The proposed framework significantly outperforms existing methods on the LongVILBench benchmark, demonstrating its effectiveness in handling temporal and spatial complexities.

Conclusion: The framework provides a strong baseline for improving long-horizon visual imitation learning and advances systematic evaluation in this field.

Abstract: Learning from long-horizon demonstrations with complex action sequences
presents significant challenges for visual imitation learning, particularly in
understanding temporal relationships of actions and spatial relationships
between objects. In this paper, we propose a new agent framework that
incorporates two dedicated reflection modules to enhance both plan and code
generation. The plan generation module produces an initial action sequence,
which is then verified by the plan reflection module to ensure temporal
coherence and spatial alignment with the demonstration video. The code
generation module translates the plan into executable code, while the code
reflection module verifies and refines the generated code to ensure correctness
and consistency with the generated plan. These two reflection modules jointly
enable the agent to detect and correct errors in both the plan generation and
code generation, improving performance in tasks with intricate temporal and
spatial dependencies. To support systematic evaluation, we introduce
LongVILBench, a benchmark comprising 300 human demonstrations with action
sequences of up to 18 steps. LongVILBench emphasizes temporal and spatial
complexity across multiple task types. Experimental results demonstrate that
existing methods perform poorly on this benchmark, whereas our new framework
establishes a strong baseline for long-horizon visual imitation learning.

</details>


### [401] [Evaluating Magic Leap 2 Tool Tracking for AR Sensor Guidance in Industrial Inspections](https://arxiv.org/abs/2509.05391)
*Christian Masuhr,Julian Koch,Thorsten Schüppstuhl*

Main category: cs.RO

TL;DR: The paper evaluates tracking performance of Magic Leap 2 controllers using a robotic arm and optical systems to establish benchmarks.


<details>
  <summary>Details</summary>
Motivation: Public benchmarks for tool tracking with AR on modern Head-Mounted Displays are inadequate, necessitating rigorous evaluation.

Method: A robotic arm executes repeatable motion paired with optical tracking to measure static and dynamic controller performance under diverse conditions.

Result: Quantitative baselines for the Magic Leap 2 controller's accuracy and repeatability are established.

Conclusion: The evaluation method is robust and transferable, providing insights into the controllers' suitability for industrial AR-guidance tasks.

Abstract: Rigorous evaluation of commercial Augmented Reality (AR) hardware is crucial,
yet public benchmarks for tool tracking on modern Head-Mounted Displays (HMDs)
are limited. This paper addresses this gap by systematically assessing the
Magic Leap 2 (ML2) controllers tracking performance. Using a robotic arm for
repeatable motion (EN ISO 9283) and an optical tracking system as ground truth,
our protocol evaluates static and dynamic performance under various conditions,
including realistic paths from a hydrogen leak inspection use case. The results
provide a quantitative baseline of the ML2 controller's accuracy and
repeatability and present a robust, transferable evaluation methodology. The
findings provide a basis to assess the controllers suitability for the
inspection use case and similar industrial sensor-based AR guidance tasks.

</details>


### [402] [RoboBallet: Planning for Multi-Robot Reaching with Graph Neural Networks and Reinforcement Learning](https://arxiv.org/abs/2509.05397)
*Matthew Lai,Keegan Go,Zhibin Li,Torsten Kroger,Stefan Schaal,Kelsey Allen,Jonathan Scholz*

Main category: cs.RO

TL;DR: The paper proposes a graph neural network-based reinforcement learning framework for multi-robot task allocation, scheduling, and motion planning in obstacle-rich environments.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and labor-intensive nature of manually planning multi-robot trajectories in industrial applications.

Method: The approach utilizes graph representations of environments and a graph policy neural network trained via reinforcement learning on procedurally generated environments to automate task allocation, scheduling, and motion planning.

Result: The framework demonstrates high-speed, zero-shot generalization to unseen scenarios with varying parameters and showcases its application in workcell layout optimization, fault-tolerant planning, and online re-planning.

Conclusion: The speed and scalability of the proposed method enable efficient and adaptable multi-robot planning, paving the way for automated solutions in dynamic industrial environments.

Abstract: Modern robotic manufacturing requires collision-free coordination of multiple
robots to complete numerous tasks in shared, obstacle-rich workspaces. Although
individual tasks may be simple in isolation, automated joint task allocation,
scheduling, and motion planning under spatio-temporal constraints remain
computationally intractable for classical methods at real-world scales.
Existing multi-arm systems deployed in the industry rely on human intuition and
experience to design feasible trajectories manually in a labor-intensive
process. To address this challenge, we propose a reinforcement learning (RL)
framework to achieve automated task and motion planning, tested in an
obstacle-rich environment with eight robots performing 40 reaching tasks in a
shared workspace, where any robot can perform any task in any order. Our
approach builds on a graph neural network (GNN) policy trained via RL on
procedurally-generated environments with diverse obstacle layouts, robot
configurations, and task distributions. It employs a graph representation of
scenes and a graph policy neural network trained through reinforcement learning
to generate trajectories of multiple robots, jointly solving the sub-problems
of task allocation, scheduling, and motion planning. Trained on large randomly
generated task sets in simulation, our policy generalizes zero-shot to unseen
settings with varying robot placements, obstacle geometries, and task poses. We
further demonstrate that the high-speed capability of our solution enables its
use in workcell layout optimization, improving solution times. The speed and
scalability of our planner also open the door to new capabilities such as
fault-tolerant planning and online perception-based re-planning, where rapid
adaptation to dynamic task sets is required.

</details>


### [403] [HapMorph: A Pneumatic Framework for Multi-Dimensional Haptic Property Rendering](https://arxiv.org/abs/2509.05433)
*Rui Chen,Domenico Chiaradia,Antonio Frisoli,Daniele Leonardis*

Main category: cs.RO

TL;DR: HapMorph introduces a pneumatic framework enabling wearable haptic interfaces to simultaneously modify size and stiffness.


<details>
  <summary>Details</summary>
Motivation: Existing wearable haptic systems struggle to concurrently render geometric and mechanical properties.

Method: Antagonistic fabric-based pneumatic actuators (AFPAs) with dual-chamber pressure regulation offer decoupled control of size and stiffness.

Result: HapMorph prototype achieves size variation (50-104 mm), stiffness modulation (up to 4.7 N/mm), and lightweight design (21 g). Tests show users can effectively distinguish nine states with high accuracy (89.4%) and quick response (6.7 s).

Conclusion: The study establishes AFPAs as a route toward advanced wearable haptic interfaces with multidimensional rendering capabilities.

Abstract: Haptic interfaces that can simultaneously modulate multiple physical
properties remain a fundamental challenge in human-robot interaction. Existing
systems typically allow the rendering of either geometric features or
mechanical properties, but rarely both, within wearable form factors. Here, we
introduce HapMorph, a pneumatic framework that enables continuous, simultaneous
modulation of object size and stiffness through antagonistic fabric-based
pneumatic actuators (AFPAs). We implemented a HapMorph protoytpe designed for
hands interaction achieving size variation from 50 to 104 mm, stiffness
modulation up to 4.7 N/mm and mass of the wearable parts of just 21 g. Through
systematic characterization, we demonstrate decoupled control of size and
stiffness properties via dual-chamber pressure regulation. Human perception
studies with 10 participants reveal that users can distinguish nine discrete
states across three size categories and three stiffness levels with 89.4%
accuracy and 6.7 s average response time. We further demonstrate extended
architectures that combine AFPAs with complementary pneumatic structures to
enable shape or geometry morphing with concurrent stiffness control. Our
results establish antagonistic pneumatic principle as a pathway toward
next-generation haptic interfaces, capable of multi-dimensiona rendering
properties within practical wearable constraints.

</details>


### [404] [Learning Tool-Aware Adaptive Compliant Control for Autonomous Regolith Excavation](https://arxiv.org/abs/2509.05475)
*Andrej Orsula,Matthieu Geist,Miguel Olivares-Mendez,Carol Martinez*

Main category: cs.RO

TL;DR: The paper presents a model-based reinforcement learning framework that enables robots to adaptively excavate lunar regolith with diverse tools and terrains.


<details>
  <summary>Details</summary>
Motivation: The study aims to solve the complexity of granular media interactions and the need for robots to use diverse tools for autonomous excavation in space environments.

Method: The framework employs a parallelized simulation with high-fidelity physics and procedural generation of terrains and tools. The agent learns adaptive strategies by modulating its stiffness and damping during control using operational space control.

Result: The experiments showed that training with a diverse distribution of tools improves generalization and that visual feedback significantly enhances task success.

Conclusion: The methodology offers a robust way to develop adaptable autonomous systems for critical space exploration tasks, like in-situ resource utilization.

Abstract: Autonomous regolith excavation is a cornerstone of in-situ resource
utilization for a sustained human presence beyond Earth. However, this task is
fundamentally hindered by the complex interaction dynamics of granular media
and the operational need for robots to use diverse tools. To address these
challenges, this work introduces a framework where a model-based reinforcement
learning agent learns within a parallelized simulation. This environment
leverages high-fidelity particle physics and procedural generation to create a
vast distribution of both lunar terrains and excavation tool geometries. To
master this diversity, the agent learns an adaptive interaction strategy by
dynamically modulating its own stiffness and damping at each control step
through operational space control. Our experiments demonstrate that training
with a procedural distribution of tools is critical for generalization and
enables the development of sophisticated tool-aware behavior. Furthermore, we
show that augmenting the agent with visual feedback significantly improves task
success. These results represent a validated methodology for developing the
robust and versatile autonomous systems required for the foundational tasks of
future space missions.

</details>


### [405] [Microrobot Vascular Parkour: Analytic Geometry-based Path Planning with Real-time Dynamic Obstacle Avoidance](https://arxiv.org/abs/2509.05500)
*Yanda Yang,Max Sokolich,Fatma Ceren Kirmizitas,Sambeeta Das,Andreas A. Malikopoulos*

Main category: cs.RO

TL;DR: The paper introduces a framework for navigating autonomous microrobots in blood vessels using a real-time path planner combined with local controllers to avoid obstacles in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of navigating autonomous microrobots through dense and moving obstacles in blood vessels for minimally invasive therapies.

Method: The framework employs a global path planner using analytic geometry and two local escape controllers (rules-based and reinforcement learning) that process real-time imaging to compute collision-free paths.

Result: In simulations, the proposed planner (AGP) demonstrated shorter paths and faster planning compared to alternatives like WA*, PSO, and RRT. The system successfully avoided moving obstacles and achieved target navigation both in 2D and 3D environments.

Conclusion: The study advances real-time control capabilities for microrobots in vascular environments, paving the way for enhanced targeted drug delivery systems.

Abstract: Autonomous microrobots in blood vessels could enable minimally invasive
therapies, but navigation is challenged by dense, moving obstacles. We propose
a real-time path planning framework that couples an analytic geometry global
planner (AGP) with two reactive local escape controllers, one based on rules
and one based on reinforcement learning, to handle sudden moving obstacles.
Using real-time imaging, the system estimates the positions of the microrobot,
obstacles, and targets and computes collision-free motions. In simulation, AGP
yields shorter paths and faster planning than weighted A* (WA*), particle swarm
optimization (PSO), and rapidly exploring random trees (RRT), while maintaining
feasibility and determinism. We extend AGP from 2D to 3D without loss of speed.
In both simulations and experiments, the combined global planner and local
controllers reliably avoid moving obstacles and reach targets. The average
planning time is 40 ms per frame, compatible with 25 fps image acquisition and
real-time closed-loop control. These results advance autonomous microrobot
navigation and targeted drug delivery in vascular environments.

</details>


### [406] [TeleopLab: Accessible and Intuitive Teleoperation of a Robotic Manipulator for Remote Labs](https://arxiv.org/abs/2509.05547)
*Ziling Chen,Yeo Jung Yoon,Rolando Bautista-Montesano,Zhen Zhao,Ajay Mandlekar,John Liu*

Main category: cs.RO

TL;DR: TeleopLab is a cost-effective remote learning system allowing students to operate a robotic arm and lab equipment via a smartphone-based interface, significantly improving usability and task performance.


<details>
  <summary>Details</summary>
Motivation: To provide a scalable and effective solution for hands-on learning in remote STEM education, addressing cost and usability challenges.

Method: Developed TeleopLab, a mobile teleoperation system utilizing a robotic arm, adaptive gripper, cameras, lab equipment, and a smartphone interface. User studies were performed to assess task performance, system usability, and workload.

Result: Achieved a 46.1% reduction in task completion time, positive usability score (SUS: 73.8), and a manageable workload score (NASA TLX: 38.2). Students' perspectives toward the system improved after use.

Conclusion: TeleopLab effectively bridges physical labs and remote learning, offering a scalable solution for remote STEM education with significant usability and performance improvements.

Abstract: Teleoperation offers a promising solution for enabling hands-on learning in
remote education, particularly in environments requiring interaction with
real-world equipment. However, such remote experiences can be costly or
non-intuitive. To address these challenges, we present TeleopLab, a mobile
device teleoperation system that allows students to control a robotic arm and
operate lab equipment. TeleopLab comprises a robotic arm, an adaptive gripper,
cameras, lab equipment for a diverse range of applications, a user interface
accessible through smartphones, and video call software. We conducted a user
study, focusing on task performance, students' perspectives toward the system,
usability, and workload assessment. Our results demonstrate a 46.1% reduction
in task completion time as users gained familiarity with the system.
Quantitative feedback highlighted improvements in students' perspectives after
using the system, while NASA TLX and SUS assessments indicated a manageable
workload of 38.2 and a positive usability of 73.8. TeleopLab successfully
bridges the gap between physical labs and remote education, offering a scalable
and effective platform for remote STEM learning.

</details>


### [407] [Learning to Walk in Costume: Adversarial Motion Priors for Aesthetically Constrained Humanoids](https://arxiv.org/abs/2509.05581)
*Arturo Flores Alvarez,Fatemeh Zargarbashi,Havel Liu,Shiqi Wang,Liam Edwards,Jessica Anz,Alex Xu,Fan Shi,Stelian Coros,Dennis W. Hong*

Main category: cs.RO

TL;DR: The paper introduces a Reinforcement Learning system for Cosmo, a humanoid robot designed for entertainment, using adversarial motion priors to achieve natural and physically stable movements despite design challenges.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of creating humanoid robots with aesthetically-driven designs, such as Cosmo's large head and limited mobility, while ensuring both functionality and safety.

Method: The paper integrates Adversarial Motion Priors (AMP) with domain randomization techniques and specialized reward systems to train Cosmo for natural and stable locomotion under physical constraints.

Result: The AMP approach demonstrated success in generating stable standing and walking behaviors for Cosmo despite its extreme mass distribution and movement limitations.

Conclusion: Learning-based approaches like AMP can effectively overcome aesthetic-driven design constraints, paving the way for functional entertainment robots.

Abstract: We present a Reinforcement Learning (RL)-based locomotion system for Cosmo, a
custom-built humanoid robot designed for entertainment applications. Unlike
traditional humanoids, entertainment robots present unique challenges due to
aesthetic-driven design choices. Cosmo embodies these with a disproportionately
large head (16% of total mass), limited sensing, and protective shells that
considerably restrict movement. To address these challenges, we apply
Adversarial Motion Priors (AMP) to enable the robot to learn natural-looking
movements while maintaining physical stability. We develop tailored domain
randomization techniques and specialized reward structures to ensure safe
sim-to-real, protecting valuable hardware components during deployment. Our
experiments demonstrate that AMP generates stable standing and walking
behaviors despite Cosmo's extreme mass distribution and movement constraints.
These results establish a promising direction for robots that balance aesthetic
appeal with functional performance, suggesting that learning-based methods can
effectively adapt to aesthetic-driven design constraints.

</details>


### [408] [MonoGlass3D: Monocular 3D Glass Detection with Plane Regression and Adaptive Feature Fusion](https://arxiv.org/abs/2509.05599)
*Kai Zhang,Guoyang Zhao,Jianxing Shi,Bonan Liu,Weiqing Qi,Jun Ma*

Main category: cs.RO

TL;DR: This paper introduces a new dataset for 3D glass detection and presents a novel approach, MonoGlass3D, which incorporates adaptive feature fusion and a plane regression technique, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Challenges in detecting glass in 3D environments due to its optical properties and lack of real-world datasets focused on glass objects.

Method: Development of MonoGlass3D, which uses an adaptive feature fusion module for varying contexts and a plane regression pipeline to integrate geometric properties of glass surfaces.

Result: MonoGlass3D surpasses state-of-the-art methods for glass segmentation and monocular glass depth estimation through a combination of geometric and contextual cues.

Conclusion: The study emphasizes the effectiveness of combining geometric and contextual information for better understanding and detecting transparent surfaces like glass.

Abstract: Detecting and localizing glass in 3D environments poses significant
challenges for visual perception systems, as the optical properties of glass
often hinder conventional sensors from accurately distinguishing glass
surfaces. The lack of real-world datasets focused on glass objects further
impedes progress in this field. To address this issue, we introduce a new
dataset featuring a wide range of glass configurations with precise 3D
annotations, collected from distinct real-world scenarios. On the basis of this
dataset, we propose MonoGlass3D, a novel approach tailored for monocular 3D
glass detection across diverse environments. To overcome the challenges posed
by the ambiguous appearance and context diversity of glass, we propose an
adaptive feature fusion module that empowers the network to effectively capture
contextual information in varying conditions. Additionally, to exploit the
distinct planar geometry of glass surfaces, we present a plane regression
pipeline, which enables seamless integration of geometric properties within our
framework. Extensive experiments demonstrate that our method outperforms
state-of-the-art approaches in both glass segmentation and monocular glass
depth estimation. Our results highlight the advantages of combining geometric
and contextual cues for transparent surface understanding.

</details>


### [409] [Sharing but Not Caring: Similar Outcomes for Shared Control and Switching Control in Telepresence-Robot Navigation](https://arxiv.org/abs/2509.05672)
*Juho Kalliokoski,Evan G. Center,Steven M. LaValle,Timo Ojala,Basak Sakcak*

Main category: cs.RO

TL;DR: Shared control for telepresence robots was tested against control switching, showing no efficiency loss but inconclusive impact on workload.


<details>
  <summary>Details</summary>
Motivation: Enhance navigation efficiency and intuitive user experience for telepresence robots.

Method: Developed shared control approach and compared with control switching via user studies (n=20 per study).

Result: Shared control maintained navigation efficiency but did not significantly lower task load compared to control switching.

Conclusion: Shared control shows promise; further research needed to examine user preferences and performance factors.

Abstract: Telepresence robots enable users to interact with remote environments, but
efficient and intuitive navigation remains a challenge. In this work, we
developed and evaluated a shared control method, in which the robot navigates
autonomously while allowing users to affect the path generation to better suit
their needs. We compared this with control switching, where users toggle
between direct and automated control. We hypothesized that shared control would
maintain efficiency comparable to control switching while potentially reducing
user workload. The results of two consecutive user studies (each with final
sample of n=20) showed that shared control does not degrade navigation
efficiency, but did not show a significant reduction in task load compared to
control switching. Further research is needed to explore the underlying factors
that influence user preference and performance in these control systems.

</details>


### [410] [A*-PRM: A Dynamic Weight-Based Probabilistic Roadmap Algorithm](https://arxiv.org/abs/2509.05701)
*Siyuan Wang,Shuyi Zhang,Zhen Tian,Yuheng Yao,Gongsen Wang,Yu Zhao*

Main category: cs.RO

TL;DR: The paper introduces a hybrid path planning algorithm, A-star PRM, with dynamic weights to optimize path quality and computational efficiency, demonstrating significant improvements in complex environments.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of robot path planning to enhance adaptability of autonomous navigation in environments with complex obstacles.

Method: Develop A-star PRM by integrating Manhattan distance heuristic into PRM’s random sampling process, employing hierarchical sampling and dynamic connections.

Result: Experiments show improved path quality (42.3% shorter path) and computational efficiency (only 71% time increase compared to PRM’s 785%) in high-density sampling scenarios.

Conclusion: A-star PRM outperforms existing algorithms in path quality, computational stability, and efficiency, particularly in narrow channels and dynamic obstacle scenarios.

Abstract: Robot path planning is a fundamental challenge in enhancing the environmental
adaptability of autonomous navigation systems. This paper presents a hybrid
path planning algorithm, A-star PRM, which incorporates dynamic weights. By
embedding the Manhattan distance heuristic of the A-star algorithm into the
random sampling process of PRM, the algorithm achieves a balanced optimization
of path quality and computational efficiency. The approach uses a hierarchical
sampling strategy and a dynamic connection mechanism, greatly improving
adaptability to complex obstacle distributions. Experiments show that under a
baseline configuration with one thousand sampled vertices, the path length of
A-star PRM is 1073.23 plus or minus 14.8 meters and is 42.3 percent shorter
than that of PRM with p value less than 0.01. With high-density sampling using
three thousand vertices, the path length is reduced by 0.94 percent, 1036.61
meters compared with 1046.42 meters, while the increase in computational time
is cut to about one tenth of the PRM increase, 71 percent compared with 785
percent. These results confirm the comprehensive advantages of A-star PRM in
path quality, stability, and computational efficiency. Compared with existing
hybrid algorithms, the proposed method shows clear benefits, especially in
narrow channels and scenarios with dynamic obstacles.

</details>


### [411] [Super-LIO: A Robust and Efficient LiDAR-Inertial Odometry System with a Compact Mapping Strategy](https://arxiv.org/abs/2509.05723)
*Liansheng Wang,Xinke Zhang,Chenhui Li,Dongjiao He,Yihan Pan,Jianjun Yi*

Main category: cs.RO

TL;DR: Super-LIO is a fast and robust LiDAR-Inertial Odometry (LIO) system featuring a compact map structure and an efficient search strategy, offering significant performance improvements over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of deploying LIO on resource-constrained platforms like aerial robots and mobile autonomous systems, due to computational and memory limitations.

Method: Introduces a compact octo-voxel-based map structure (OctVox) for strict point density control and denoising, along with a heuristic-guided KNN strategy (HKNN) to enhance correspondence search efficiency.

Result: Super-LIO processes frames 73% faster than state-of-the-art methods, with reduced CPU usage, while maintaining competitive accuracy, as verified across over 30 datasets on various platforms.

Conclusion: Super-LIO is an efficient, robust, and accurate LIO system suitable for resource-constrained autonomous systems, provided as open-source and compatible across multiple sensors and platforms.

Abstract: LiDAR-Inertial Odometry (LIO) is a foundational technique for autonomous
systems, yet its deployment on resource-constrained platforms remains
challenging due to computational and memory limitations. We propose Super-LIO,
a robust LIO system that demands both high performance and accuracy, ideal for
applications such as aerial robots and mobile autonomous systems. At the core
of Super-LIO is a compact octo-voxel-based map structure, termed OctVox, that
limits each voxel to eight fused subvoxels, enabling strict point density
control and incremental denoising during map updates. This design enables a
simple yet efficient and accurate map structure, which can be easily integrated
into existing LIO frameworks. Additionally, Super-LIO designs a
heuristic-guided KNN strategy (HKNN) that accelerates the correspondence search
by leveraging spatial locality, further reducing runtime overhead. We evaluated
the proposed system using four publicly available datasets and several
self-collected datasets, totaling more than 30 sequences. Extensive testing on
both X86 and ARM platforms confirms that Super-LIO offers superior efficiency
and robustness, while maintaining competitive accuracy. Super-LIO processes
each frame approximately 73% faster than SOTA, while consuming less CPU
resources. The system is fully open-source and plug-and-play compatible with a
wide range of LiDAR sensors and platforms. The implementation is available at:
https://github.com/Liansheng-Wang/Super-LIO.git

</details>


### [412] [Scenario-based Decision-making Using Game Theory for Interactive Autonomous Driving: A Survey](https://arxiv.org/abs/2509.05777)
*Zhihao Lin,Zhen Tian*

Main category: cs.RO

TL;DR: The paper surveys advancements in game-based driving simulations for decision-making, evaluates these methods across different scenarios, and discusses limitations and future directions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of a systematic review comparing game-based interactive driving approaches across diverse driving scenarios.

Method: The authors summarize advancements in game-based driving methods, assess the algorithms' mechanism specific to scenarios, and critically evaluate their decision-making performance.

Result: The survey highlights advancements, identifies challenges, and uncovers the gaps and limitations in current game-based driving simulations for decision-making.

Conclusion: The paper concludes with a discussion of the limitations of existing methods and suggests promising directions for future research in game-based driving simulations.

Abstract: Game-based interactive driving simulations have emerged as versatile
platforms for advancing decision-making algorithms in road transport mobility.
While these environments offer safe, scalable, and engaging settings for
testing driving strategies, ensuring both realism and robust performance amid
dynamic and diverse scenarios remains a significant challenge. Recently, the
integration of game-based techniques with advanced learning frameworks has
enabled the development of adaptive decision-making models that effectively
manage the complexities inherent in varied driving conditions. These models
outperform traditional simulation methods, especially when addressing
scenario-specific challenges, ranging from obstacle avoidance on highways and
precise maneuvering during on-ramp merging to navigation in roundabouts,
unsignalized intersections, and even the high-speed demands of autonomous
racing. Despite numerous innovations in game-based interactive driving, a
systematic review comparing these approaches across different scenarios is
still missing. This survey provides a comprehensive evaluation of game-based
interactive driving methods by summarizing recent advancements and inherent
roadway features in each scenario. Furthermore, the reviewed algorithms are
critically assessed based on their adaptation of the standard game model and an
analysis of their specific mechanisms to understand their impact on
decision-making performance. Finally, the survey discusses the limitations of
current approaches and outlines promising directions for future research.

</details>


### [413] [eKalibr-Inertial: Continuous-Time Spatiotemporal Calibration for Event-Based Visual-Inertial Systems](https://arxiv.org/abs/2509.05923)
*Shuolong Chen,Xingxing Li,Liu Yuan*

Main category: cs.RO

TL;DR: The paper introduces eKalibr-Inertial, a spatiotemporal calibration method for event-based visual-inertial systems, demonstrating its accuracy through real-world experiments.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy of spatiotemporal calibration in event-based visual-inertial systems, crucial for applications like motion estimation and robotic perception.

Method: The method involves an initialization phase for recovering parameters followed by a continuous-time batch optimization, utilizing a circle grid board for calibration.

Result: Real-world experiments confirm that the proposed method achieves precise spatiotemporal calibration for event-based systems.

Conclusion: eKalibr-Inertial provides an effective calibration solution, and its open-source implementation is expected to aid further research in the community.

Abstract: The bioinspired event camera, distinguished by its exceptional temporal
resolution, high dynamic range, and low power consumption, has been extensively
studied in recent years for motion estimation, robotic perception, and object
detection. In ego-motion estimation, the visual-inertial setup is commonly
adopted due to complementary characteristics between sensors (e.g., scale
perception and low drift). For optimal event-based visual-inertial fusion,
accurate spatiotemporal (extrinsic and temporal) calibration is required. In
this work, we present eKalibr-Inertial, an accurate spatiotemporal calibrator
for event-based visual-inertial systems, utilizing the widely used circle grid
board. Building upon the grid pattern recognition and tracking methods in
eKalibr and eKalibr-Stereo, the proposed method starts with a rigorous and
efficient initialization, where all parameters in the estimator would be
accurately recovered. Subsequently, a continuous-time-based batch optimization
is conducted to refine the initialized parameters toward better states. The
results of extensive real-world experiments show that eKalibr-Inertial can
achieve accurate event-based visual-inertial spatiotemporal calibration. The
implementation of eKalibr-Inertial is open-sourced at
(https://github.com/Unsigned-Long/eKalibr) to benefit the research community.

</details>


### [414] [ZLATTE: A Geometry-Aware, Learning-Free Framework for Language-Driven Trajectory Reshaping in Human-Robot Interaction](https://arxiv.org/abs/2509.06031)
*Junhui Huang,Yuhe Gong,Changsheng Li,Xingguang Duan,Luis Figueredo*

Main category: cs.RO

TL;DR: ZLATTE enables language-driven trajectory reshaping without requiring training, using vision and language models for geometric constraints, and demonstrating improved performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: To create a robust approach for adjusting robot trajectories using natural language inputs while addressing safety and feasibility without learning-based methods.

Method: The framework uses Vision-Language Models for object registration, Large Language Models for translating instructions into constraints, and a potential field optimization for trajectory adjustments. A multi-agent approach handles complex commands.

Result: Simulation and real-world tests show ZLATTE produces safer, smoother, and more interpretable trajectory modifications compared to other methods.

Conclusion: ZLATTE proves successful in reshaping robot trajectories via natural language, offering advantages in safety, interpretability, and performance over existing solutions.

Abstract: We present ZLATTE, a geometry-aware, learning-free framework for
language-driven trajectory reshaping in human-robot interaction. Unlike prior
learning-based methods, ZLATTE leverages Vision-Language Models to register
objects as geometric primitives and employs a Large Language Model to translate
natural language instructions into explicit geometric and kinematic
constraints. These constraints are integrated into a potential field
optimization to adapt initial trajectories while preserving feasibility and
safety. A multi-agent strategy further enhances robustness under complex or
conflicting commands. Simulation and real-world experiments demonstrate that
ZLATTE achieves smoother, safer, and more interpretable trajectory
modifications compared to state-of-the-art baselines.

</details>


### [415] [Robotic Manipulation Framework Based on Semantic Keypoints for Packing Shoes of Different Sizes, Shapes, and Softness](https://arxiv.org/abs/2509.06048)
*Yi Dong,Yangjun Liu,Jinjun Duan,Yang Li,Zhendong Dai*

Main category: cs.RO

TL;DR: This study proposes a robotic framework for packing shoe pairs under varied initial conditions, integrating vision, reorientation, and planning modules, validated through experiments.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in packing irregular and deformable footwear items by developing strategies to handle intraclass variations, irregular shapes, and diverse initial states.

Method: The paper introduces a framework comprising a vision module for semantic keypoint-based perception, state-specific reorientation planners, and a task planner to optimize shoe pair packing from any initial condition.

Result: Experimental validations showed the proposed framework's robustness in reorienting and efficiently packing footwear of various types.

Conclusion: The study demonstrates the effectiveness of semantic keypoints for 3D deformable object manipulation, offering new insights and strategies for paired-object packing tasks.

Abstract: With the rapid development of the warehousing and logistics industries, the
packing of goods has gradually attracted the attention of academia and
industry. The packing of footwear products is a typical representative
paired-item packing task involving irregular shapes and deformable objects.
Although studies on shoe packing have been conducted, different initial states
due to the irregular shapes of shoes and standard packing placement poses have
not been considered. This study proposes a robotic manipulation framework,
including a perception module, reorientation planners, and a packing planner,
that can complete the packing of pairs of shoes in any initial state. First, to
adapt to the large intraclass variations due to the state, shape, and
deformation of the shoe, we propose a vision module based on semantic
keypoints, which can also infer more information such as size, state, pose, and
manipulation points by combining geometric features. Subsequently, we not only
proposed primitive-based reorientation methods for different states of a single
deformable shoe but also proposed a fast reorientation method for the top state
using box edge contact and gravity, which further improved the efficiency of
reorientation. Finally, based on the perception module and reorientation
methods, we propose a task planner for shoe pair packing in any initial state
to provide an optimal packing strategy. Real-world experiments were conducted
to verify the robustness of the reorientation methods and the effectiveness of
the packing strategy for various types of shoes. In this study, we highlight
the potential of semantic keypoint representation methods, introduce new
perspectives on the reorientation of 3D deformable objects and multi-object
manipulation, and provide a reference for paired object packing.

</details>


### [416] [Energy-Efficient Path Planning with Multi-Location Object Pickup for Mobile Robots on Uneven Terrain](https://arxiv.org/abs/2509.06061)
*Faiza Babakano,Ahmed Fahmin,Bojie Shen,Muhammad Aamir Cheema,Isma Farah Siddiqui*

Main category: cs.RO

TL;DR: This paper introduces algorithms for energy-efficient path planning in Autonomous Mobile Robots (AMRs) that pick up objects en route, focusing on the Object-Pickup Minimum Energy Path Problem (OMEPP).


<details>
  <summary>Details</summary>
Motivation: The paper aims to address energy-efficient path planning for AMRs operating in outdoor environments, specifically focusing on scenarios where robots pick up objects mid-route, a challenge not sufficiently addressed in prior studies.

Method: The authors proposed a baseline algorithm using the Z star algorithm and a concurrent PCPD search, incorporating a Payload-Constrained Path Database (PCPD) to optimize path planning.

Result: The concurrent PCPD search achieves near-optimal performance and is one to two orders of magnitude faster than the baseline algorithm, as proven through experiments on real-world datasets.

Conclusion: While the concurrent PCPD search may slightly compromise optimality, it substantially improves computational efficiency for AMRs with object-pickup tasks, making it a practical solution for real-world applications.

Abstract: Autonomous Mobile Robots (AMRs) operate on battery power, making energy
efficiency a critical consideration, particularly in outdoor environments where
terrain variations affect energy consumption. While prior research has
primarily focused on computing energy-efficient paths from a source to a
destination, these approaches often overlook practical scenarios where a robot
needs to pick up an object en route - an action that can significantly impact
energy consumption due to changes in payload. This paper introduces the
Object-Pickup Minimum Energy Path Problem (OMEPP), which addresses
energy-efficient route planning for AMRs required to pick up an object from one
of many possible locations and deliver it to a destination. To address OMEPP,
we first introduce a baseline algorithm that employs the Z star algorithm, a
variant of A star tailored for energy-efficient routing, to iteratively visit
each pickup point. While this approach guarantees optimality, it suffers from
high computational cost due to repeated searches at each pickup location. To
mitigate this inefficiency, we propose a concurrent PCPD search that manages
multiple Z star searches simultaneously across all pickup points. Central to
our solution is the Payload-Constrained Path Database (PCPD), an extension of
the Compressed Path Database (CPD) that incorporates payload constraints. We
demonstrate that PCPD significantly reduces branching factors during search,
improving overall performance. Although the concurrent PCPD search may produce
slightly suboptimal solutions, extensive experiments on real-world datasets
show it achieves near-optimal performance while being one to two orders of
magnitude faster than the baseline algorithm.

</details>


### [417] [Hybrid A* Path Planning with Multi-Modal Motion Extension for Four-Wheel Steering Mobile Robots](https://arxiv.org/abs/2509.06115)
*Runjiao Bao,Lin Zhang,Tianwei Niu,Haoyu Yuan,Shoukun Wang*

Main category: cs.RO

TL;DR: The paper presents an extended Hybrid A* path planning method for 4WIS mobile robots, enabling enhanced motion modes and flexibility in complex environments.


<details>
  <summary>Details</summary>
Motivation: Current path planning methods for 4WIS robots fail to utilize their multi-modal motion capabilities due to reliance on single kinematic models.

Method: The authors propose a four-dimensional Hybrid A* framework, with multi-modal Reeds-Shepp curves, an improved heuristic, and a mode-switching terminal connection strategy.

Result: The proposed planner enhances flexibility and adaptability, demonstrating significant improvements in planning performance for 4WIS robots in constrained spaces.

Conclusion: Extending the Hybrid A* approach successfully exploits the diverse motion modes of 4WIS systems, offering practical solutions for complex navigation challenges.

Abstract: Four-wheel independent steering (4WIS) systems provide mobile robots with a
rich set of motion modes, such as Ackermann steering, lateral steering, and
parallel movement, offering superior maneuverability in constrained
environments. However, existing path planning methods generally assume a single
kinematic model and thus fail to fully exploit the multi-modal capabilities of
4WIS platforms. To address this limitation, we propose an extended Hybrid A*
framework that operates in a four-dimensional state space incorporating both
spatial states and motion modes. Within this framework, we design multi-modal
Reeds-Shepp curves tailored to the distinct kinematic constraints of each
motion mode, develop an enhanced heuristic function that accounts for
mode-switching costs, and introduce a terminal connection strategy with
intelligent mode selection to ensure smooth transitions between different
steering patterns. The proposed planner enables seamless integration of
multiple motion modalities within a single path, significantly improving
flexibility and adaptability in complex environments. Results demonstrate
significantly improved planning performance for 4WIS robots in complex
environments.

</details>


### [418] [A Hybrid TDMA/CSMA Protocol for Time-Sensitive Traffic in Robot Applications](https://arxiv.org/abs/2509.06119)
*Shiqi Xu,Lihao Zhang,Yuyang Du,Qun Yang,Soung Chang Liew*

Main category: cs.RO

TL;DR: The study proposes a hybrid TDMA/CSMA protocol for robotic communications, demonstrating substantial improvements in delivering mission-critical commands under high-traffic loads compared to conventional CSMA.


<details>
  <summary>Details</summary>
Motivation: To ensure reliable and timely delivery of mission-critical commands in robotic systems under high traffic loads, addressing the performance degradation caused by CSMA's contention-based issues.

Method: Developed an IEEE 802.11-compatible hybrid TDMA/CSMA protocol combining TDMA for deterministic scheduling and CSMA for traffic adaptability. Key features include sub-microsecond PTP-based synchronization, dynamic TDMA allocation, and beacon-NAV protection to avoid communication interference.

Result: The proposed protocol achieves a 93% reduction in missed-deadline errors and a 90% reduction in RMS trajectory errors in high-speed robotic simulations compared to CSMA. It maintains throughput for non-critical traffic within a 2% margin.

Conclusion: The hybrid TDMA/CSMA protocol significantly improves the reliability and timeliness of mission-critical robotic communications under heterogeneous traffic, outperforming CSMA in both simulations and real-time scenarios.

Abstract: Recent progress in robotics has underscored the demand for real-time control
in applications such as manufacturing, healthcare, and autonomous systems,
where the timely delivery of mission-critical commands under heterogeneous
robotic traffic is paramount for operational efficacy and safety. In these
scenarios, mission-critical traffic follows a strict deadline-constrained
communication pattern: commands must arrive within defined QoS deadlines,
otherwise late arrivals can degrade performance or destabilize control loops.In
this work, we demonstrate on a real-time SDR platform that CSMA, widely adopted
in robotic communications,suffers severe degradation under high robot traffic
loads, with contention-induced collisions and delays disrupting the on-time
arrival of mission-critical packets. To address this problem, we propose an
IEEE 802.11-compatible hybrid TDMA/CSMA protocol that combines TDMA's
deterministic slot scheduling with CSMA's adaptability for heterogeneous robot
traffic.The protocol achieves collision-free, low-latency mission-critical
command delivery and IEEE 802.11 compatibility through the synergistic
integration of sub-microsecond PTP-based slot synchronization-essential for
establishing precise timing for TDMA, a three-session superframe with dynamic
TDMA allocation for structured and adaptable traffic management,and beacon-NAV
protection to preemptively secure these critical communication sessions from
interference. Emulation experiments on real-time SDR testbed and Robot
Operating System (ROS) simulation show that the proposed protocol reduces
missed-deadline errors by 93% compared to the CSMA baseline. In high-speed
robot path-tracking ROS simulations, the protocol lowers Root Mean Square (RMS)
trajectory error by up to 90% compared with a CSMA baseline, all while
maintaining throughput for non-critical traffic within +-2%.

</details>


### [419] [Learning in ImaginationLand: Omnidirectional Policies through 3D Generative Models (OP-Gen)](https://arxiv.org/abs/2509.06191)
*Yifei Ren,Edward Johns*

Main category: cs.RO

TL;DR: This paper explores using 3D generative models to augment datasets for robot learning, enabling effective policy learning from fewer demonstrations.


<details>
  <summary>Details</summary>
Motivation: Robots often require extensive demonstrations for policy learning. Data augmentation through 3D models can reduce this bottleneck and improve generalization.

Method: The authors used 3D generative models to create augmented datasets from a single demonstration and trained omnidirectional policies to generalize across diverse initial states.

Result: Robots successfully performed tasks far from the initial demonstration states, surpassing alternative data augmentation methods in tasks like grasping, drawer opening, and trash placement.

Conclusion: Using 3D generative models for dataset augmentation boosts robotic policy learning efficiency, reducing required demonstrations and expanding learning versatility.

Abstract: Recent 3D generative models, which are capable of generating full object
shapes from just a few images, now open up new opportunities in robotics. In
this work, we show that 3D generative models can be used to augment a dataset
from a single real-world demonstration, after which an omnidirectional policy
can be learned within this imagined dataset. We found that this enables a robot
to perform a task when initialised from states very far from those observed
during the demonstration, including starting from the opposite side of the
object relative to the real-world demonstration, significantly reducing the
number of demonstrations required for policy learning. Through several
real-world experiments across tasks such as grasping objects, opening a drawer,
and placing trash into a bin, we study these omnidirectional policies by
investigating the effect of various design choices on policy behaviour, and we
show superior performance to recent baselines which use alternative methods for
data augmentation.

</details>


### [420] [Grasp-MPC: Closed-Loop Visual Grasping via Value-Guided Model Predictive Control](https://arxiv.org/abs/2509.06201)
*Jun Yamada,Adithyavairavan Murali,Ajay Mandlekar,Clemens Eppner,Ingmar Posner,Balakumar Sundaralingam*

Main category: cs.RO

TL;DR: Grasp-MPC enhances the success rate in object grasping in cluttered, unstructured environments by using a closed-loop vision-based method combined with an MPC framework.


<details>
  <summary>Details</summary>
Motivation: Grasping in unstructured environments is challenged by errors in grasp prediction and pose changes, which hinder the effectiveness of traditional open-loop methods, while existing closed-loop methods lack generalization and robustness.

Method: The Grasp-MPC framework trains a value function on a synthetic dataset of 2 million grasp trajectories, integrating it into a Model Predictive Control framework with collision avoidance and smooth execution costs.

Result: Grasp-MPC outperformed competing approaches, improving grasp success rates by up to 32.6% in simulations and 33.3% in real-world noisy settings.

Conclusion: Grasp-MPC demonstrates superior robustness and generalization for 6-DoF grasping of diverse objects in cluttered environments, addressing key challenges with a closed-loop approach.

Abstract: Grasping of diverse objects in unstructured environments remains a
significant challenge. Open-loop grasping methods, effective in controlled
settings, struggle in cluttered environments. Grasp prediction errors and
object pose changes during grasping are the main causes of failure. In
contrast, closed-loop methods address these challenges in simplified settings
(e.g., single object on a table) on a limited set of objects, with no path to
generalization. We propose Grasp-MPC, a closed-loop 6-DoF vision-based grasping
policy designed for robust and reactive grasping of novel objects in cluttered
environments. Grasp-MPC incorporates a value function, trained on visual
observations from a large-scale synthetic dataset of 2 million grasp
trajectories that include successful and failed attempts. We deploy this
learned value function in an MPC framework in combination with other cost terms
that encourage collision avoidance and smooth execution. We evaluate Grasp-MPC
on FetchBench and real-world settings across diverse environments. Grasp-MPC
improves grasp success rates by up to 32.6% in simulation and 33.3% in
real-world noisy conditions, outperforming open-loop, diffusion policy,
transformer policy, and IQL approaches. Videos and more at
http://grasp-mpc.github.io.

</details>


### [421] [O$^3$Afford: One-Shot 3D Object-to-Object Affordance Grounding for Generalizable Robotic Manipulation](https://arxiv.org/abs/2509.06233)
*Tongxuan Tian,Xuhui Kang,Yen-Ling Kuo*

Main category: cs.RO

TL;DR: The paper introduces a novel one-shot 3D object-to-object affordance learning approach that combines semantic features, point cloud geometry, and large language models to enhance robotic manipulation.


<details>
  <summary>Details</summary>
Motivation: Current methods focus too much on single-object affordance predictions, ignoring the common situation where object interactions involve pairs of objects.

Method: The method combines semantic features from vision foundation models with 3D point cloud representations. It integrates these with large language models to improve reasoning and task-specific function generation for robotic manipulation.

Result: Experiments show their approach, O$^3$Afford, significantly outperforms existing baselines in accuracy and generalization capabilities.

Conclusion: This work provides a robust framework for leveraging one-shot learning and integration of LLMs for object-to-object affordance, advancing the field of robotic manipulation.

Abstract: Grounding object affordance is fundamental to robotic manipulation as it
establishes the critical link between perception and action among interacting
objects. However, prior works predominantly focus on predicting single-object
affordance, overlooking the fact that most real-world interactions involve
relationships between pairs of objects. In this work, we address the challenge
of object-to-object affordance grounding under limited data contraints.
Inspired by recent advances in few-shot learning with 2D vision foundation
models, we propose a novel one-shot 3D object-to-object affordance learning
approach for robotic manipulation. Semantic features from vision foundation
models combined with point cloud representation for geometric understanding
enable our one-shot learning pipeline to generalize effectively to novel
objects and categories. We further integrate our 3D affordance representation
with large language models (LLMs) for robotics manipulation, significantly
enhancing LLMs' capability to comprehend and reason about object interactions
when generating task-specific constraint functions. Our experiments on 3D
object-to-object affordance grounding and robotic manipulation demonstrate that
our O$^3$Afford significantly outperforms existing baselines in terms of both
accuracy and generalization capability.

</details>


### [422] [DCReg: Decoupled Characterization for Efficient Degenerate LiDAR Registration](https://arxiv.org/abs/2509.06285)
*Xiangcheng Hu,Xieyuanli Chen,Mingkai Jia,Jin Wu,Ping Tan,Steven L. Waslander*

Main category: cs.RO

TL;DR: The paper introduces DCReg, a framework to address ill-conditioned LiDAR point cloud registration problems, achieving significant improvements in accuracy and efficiency compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of ill-conditioned LiDAR point cloud registration in narrow or degenerate environments, which leads to unstable solutions and degraded accuracy.

Method: DCReg uses Schur complement decomposition for decoupling the problem into rotational and translational subspaces, develops eigenspace-mapping techniques for actionable insights, and introduces a preconditioner to stabilize ill-conditioned directions while preserving well-constrained information.

Result: DCReg demonstrates 20%-50% improvement in localization accuracy and 5-100 times speedup over state-of-the-art methods in various environments.

Conclusion: This study offers a robust framework for improving LiDAR point cloud registration accuracy and speed, addressing key challenges in ill-conditioned scenarios. The implementation will be made publicly available.

Abstract: LiDAR point cloud registration is fundamental to robotic perception and
navigation. However, in geometrically degenerate or narrow environments,
registration problems become ill-conditioned, leading to unstable solutions and
degraded accuracy. While existing approaches attempt to handle these issues,
they fail to address the core challenge: accurately detection, interpret, and
resolve this ill-conditioning, leading to missed detections or corrupted
solutions. In this study, we introduce DCReg, a principled framework that
systematically addresses the ill-conditioned registration problems through
three integrated innovations. First, DCReg achieves reliable ill-conditioning
detection by employing a Schur complement decomposition to the hessian matrix.
This technique decouples the registration problem into clean rotational and
translational subspaces, eliminating coupling effects that mask degeneracy
patterns in conventional analyses. Second, within these cleanly subspaces, we
develop quantitative characterization techniques that establish explicit
mappings between mathematical eigenspaces and physical motion directions,
providing actionable insights about which specific motions lack constraints.
Finally, leveraging this clean subspace, we design a targeted mitigation
strategy: a novel preconditioner that selectively stabilizes only the
identified ill-conditioned directions while preserving all well-constrained
information in observable space. This enables efficient and robust optimization
via the Preconditioned Conjugate Gradient method with a single physical
interpretable parameter. Extensive experiments demonstrate DCReg achieves at
least 20% - 50% improvement in localization accuracy and 5-100 times speedup
over state-of-the-art methods across diverse environments. Our implementation
will be available at https://github.com/JokerJohn/DCReg.

</details>


### [423] [Learning to Walk with Less: a Dyna-Style Approach to Quadrupedal Locomotion](https://arxiv.org/abs/2509.06296)
*Francisco Affonso,Felipe Andrade G. Tommaselli,Juliano Negri,Vivian S. Medeiros,Mateus V. Gasparino,Girish Chowdhary,Marcelo Becker*

Main category: cs.RO

TL;DR: The paper proposes a model-based reinforcement learning (MBRL) framework to enhance the sample efficiency of quadrupedal locomotion control using synthetic data and a scheduling strategy. Experiments show boosted performance and reduced variance.


<details>
  <summary>Details</summary>
Motivation: Traditional RL-based locomotion controllers require extensive data interactions to achieve robust performance, which limits their efficiency.

Method: The proposed MBRL framework augments PPO-based controllers by appending short-horizon synthetic data to rollouts. A predictive model generates these synthetic transitions and integrates them using a policy update-based scheduling strategy. An ablation study determined the optimal synthetic rollout length for enhancing sample efficiency.

Result: Validation in simulation on the Unitree Go1 robot showed that replacing some simulated steps with synthetic ones improved policy returns, reduced variance, and enabled wide-range locomotion command tracking with fewer steps.

Conclusion: The MBRL approach enhances policy training efficiency through synthetic data integration, offering more effective and reliable locomotion control with reduced interaction requirements.

Abstract: Traditional RL-based locomotion controllers often suffer from low data
efficiency, requiring extensive interaction to achieve robust performance. We
present a model-based reinforcement learning (MBRL) framework that improves
sample efficiency for quadrupedal locomotion by appending synthetic data to the
end of standard rollouts in PPO-based controllers, following the Dyna-Style
paradigm. A predictive model, trained alongside the policy, generates
short-horizon synthetic transitions that are gradually integrated using a
scheduling strategy based on the policy update iterations. Through an ablation
study, we identified a strong correlation between sample efficiency and rollout
length, which guided the design of our experiments. We validated our approach
in simulation on the Unitree Go1 robot and showed that replacing part of the
simulated steps with synthetic ones not only mimics extended rollouts but also
improves policy return and reduces variance. Finally, we demonstrate that this
improvement transfers to the ability to track a wide range of locomotion
commands using fewer simulated steps.

</details>


### [424] [Towards bridging the gap: Systematic sim-to-real transfer for diverse legged robots](https://arxiv.org/abs/2509.06342)
*Filip Bjelonic,Fabian Tischhauser,Marco Hutter*

Main category: cs.RO

TL;DR: Legged robots often struggle with real-world energy efficiency and control after training in simulation. The authors propose a solution that integrates sim-to-real reinforcement learning with a physics-informed energy model for motors, reducing inefficiencies and improving reliability.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges of transferring simulation-trained controllers to the real world while ensuring robust locomotion and better energy efficiency, especially focusing on actuator-specific energy loss.

Method: The framework combines simulation-to-reality reinforcement learning with an energy model requiring minimal parameters. A compact, four-term reward system considers both electrical and mechanical energy losses.

Result: The approach improved the energetic efficiency of robots, achieving a 32% Cost of Transport reduction for ANYmal, and demonstrated consistent policy transfer to ten additional robots without dynamic parameter randomization.

Conclusion: The integrated framework enhances reliability and energy efficiency for legged robots in real-world scenarios. The authors will release all associated resources for further development.

Abstract: Legged robots must achieve both robust locomotion and energy efficiency to be
practical in real-world environments. Yet controllers trained in simulation
often fail to transfer reliably, and most existing approaches neglect
actuator-specific energy losses or depend on complex, hand-tuned reward
formulations. We propose a framework that integrates sim-to-real reinforcement
learning with a physics-grounded energy model for permanent magnet synchronous
motors. The framework requires a minimal parameter set to capture the
simulation-to-reality gap and employs a compact four-term reward with a
first-principle-based energetic loss formulation that balances electrical and
mechanical dissipation. We evaluate and validate the approach through a
bottom-up dynamic parameter identification study, spanning actuators,
full-robot in-air trajectories and on-ground locomotion. The framework is
tested on three primary platforms and deployed on ten additional robots,
demonstrating reliable policy transfer without randomization of dynamic
parameters. Our method improves energetic efficiency over state-of-the-art
methods, achieving a 32 percent reduction in the full Cost of Transport of
ANYmal (value 1.27). All code, models, and datasets will be released.

</details>


### [425] [Adaptive Evolution Factor Risk Ellipse Framework for Reliable and Safe Autonomous Driving](https://arxiv.org/abs/2509.06375)
*Fujiang Yuan,Zhen Tian,Yangfan He,Guojian Zou,Chunhong Yuan,Yanhong Peng,Zhihao Lin*

Main category: cs.RO

TL;DR: The paper introduces an Evolutionary Risk Potential Field (ERPF) approach integrated with Model Predictive Control (MPC) for more dynamic risk assessment and adaptive autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Ensure autonomous driving systems can dynamically respond to uncertain and interactive driving environments while addressing limitations in traditional and learning-based approaches.

Method: Developed ERPF, combining Risk-Ellipse constructs with adaptive metrics like Evolution Factor based on TTC and TWH. Integrated this dynamically updating risk mechanism into an MPC framework to enhance responsiveness.

Result: Experiments show ERPF-MPC achieves smoother trajectories, better speeds, and collision-free navigation compared to traditional techniques.

Conclusion: ERPF-MPC offers a robust solution with adaptive capabilities, making it highly suitable for dynamic and complex autonomous driving scenarios.

Abstract: In recent years, ensuring safety, efficiency, and comfort in interactive
autonomous driving has become a critical challenge. Traditional model-based
techniques, such as game-theoretic methods and robust control, are often overly
conservative or computationally intensive. Conversely, learning-based
approaches typically require extensive training data and frequently exhibit
limited interpretability and generalizability. Simpler strategies, such as Risk
Potential Fields (RPF), provide lightweight alternatives with minimal data
demands but are inherently static and struggle to adapt effectively to dynamic
traffic conditions. To overcome these limitations, we propose the Evolutionary
Risk Potential Field (ERPF), a novel approach that dynamically updates risk
assessments in dynamical scenarios based on historical obstacle proximity data.
We introduce a Risk-Ellipse construct that combines longitudinal reach and
lateral uncertainty into a unified spatial temporal collision envelope.
Additionally, we define an adaptive Evolution Factor metric, computed through
sigmoid normalization of Time to Collision (TTC) and Time-Window-of-Hazard
(TWH), which dynamically adjusts the dimensions of the ellipse axes in real
time. This adaptive risk metric is integrated seamlessly into a Model
Predictive Control (MPC) framework, enabling autonomous vehicles to proactively
address complex interactive driving scenarios in terms of uncertain driving of
surrounding vehicles. Comprehensive comparative experiments demonstrate that
our ERPF-MPC approach consistently achieves smoother trajectories, higher
average speeds, and collision-free navigation, offering a robust and adaptive
solution suitable for complex interactive driving environments.

</details>


### [426] [Safety Meets Speed: Accelerated Neural MPC with Safety Guarantees and No Retraining](https://arxiv.org/abs/2509.06404)
*Kaikai Wang,Tianxun Li,Liang Xu,Qinglei Hu,Keyou You*

Main category: cs.RO

TL;DR: This paper presents a Barrier-integrated Adaptive Neural Model Predictive Control (BAN-MPC) framework that combines fast neural network computation with Model Predictive Control's constraint-handling to ensure safe and efficient real-time control.


<details>
  <summary>Details</summary>
Motivation: Develop a control framework that ensures safety constraints while addressing the computational inefficiencies of traditional Model Predictive Control (MPC) for embedded systems.

Method: The approach integrates Control Barrier Functions for safety, a neural network-based value function for reducing MPC computation, and another neural network for sensitivity analysis to adapt the control model dynamically without retraining.

Result: Hardware experiments demonstrate BAN-MPC achieves a computational speed-up of 200 times compared to traditional MPC, while maintaining collision-free navigation and low control error under parameter variation.

Conclusion: BAN-MPC offers a robust and computationally efficient solution for embedded system control, ensuring safety and adaptability without extensive offline retraining.

Abstract: While Model Predictive Control (MPC) enforces safety via constraints, its
real-time execution can exceed embedded compute budgets. We propose a
Barrier-integrated Adaptive Neural Model Predictive Control (BAN-MPC) framework
that synergizes neural networks' fast computation with MPC's
constraint-handling capability. To ensure strict safety, we replace traditional
Euclidean distance with Control Barrier Functions (CBFs) for collision
avoidance. We integrate an offline-learned neural value function into the
optimization objective of a Short-horizon MPC, substantially reducing online
computational complexity. Additionally, we use a second neural network to learn
the sensitivity of the value function to system parameters, and adaptively
adjust the neural value function based on this neural sensitivity when model
parameters change, eliminating the need for retraining and reducing offline
computation costs. The hardware in-the-loop (HIL) experiments on Jetson Nano
show that BAN-MPC solves 200 times faster than traditional MPC, enabling
collision-free navigation with control error below 5\% under model parameter
variations within 15\%, making it an effective embedded MPC alternative.

</details>


### [427] [Real-time Photorealistic Mapping for Situational Awareness in Robot Teleoperation](https://arxiv.org/abs/2509.06433)
*Ian Page,Pierre Susbielle,Olivier Aycard,Pierre-Brice Wieber*

Main category: cs.RO

TL;DR: The paper presents a GPU-based method combining recent Gaussian splatting SLAM with teleoperation systems to enhance real-time 3D mapping and improve remote teleoperation efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency of teleoperation in unknown environments caused by computational challenges in generating accurate real-time 3D maps.

Method: Develop a modular GPU-based system that integrates Gaussian splatting SLAM with online map-based teleoperation systems to enhance real-time mapping performance.

Result: Proposed system surpasses state-of-the-art teleoperation systems, improving decision-making speed and interaction accuracy, proven via real-world aerial vehicle experiments.

Conclusion: The solution enables effective teleoperation in unknown environments by improving photorealistic 3D mapping in real-time, enhancing teleoperator efficiency.

Abstract: Achieving efficient remote teleoperation is particularly challenging in
unknown environments, as the teleoperator must rapidly build an understanding
of the site's layout. Online 3D mapping is a proven strategy to tackle this
challenge, as it enables the teleoperator to progressively explore the site
from multiple perspectives. However, traditional online map-based teleoperation
systems struggle to generate visually accurate 3D maps in real-time due to the
high computational cost involved, leading to poor teleoperation performances.
In this work, we propose a solution to improve teleoperation efficiency in
unknown environments. Our approach proposes a novel, modular and efficient
GPU-based integration between recent advancement in gaussian splatting SLAM and
existing online map-based teleoperation systems. We compare the proposed
solution against state-of-the-art teleoperation systems and validate its
performances through real-world experiments using an aerial vehicle. The
results show significant improvements in decision-making speed and more
accurate interaction with the environment, leading to greater teleoperation
efficiency. In doing so, our system enhances remote teleoperation by seamlessly
integrating photorealistic mapping generation with real-time performances,
enabling effective teleoperation in unfamiliar environments.

</details>


### [428] [Interactive Shaping of Granular Media Using Reinforcement Learning](https://arxiv.org/abs/2509.06469)
*Benedikt Kreis,Malte Mosbach,Anny Ripke,Muhammad Ehsan Ullah,Sven Behnke,Maren Bennewitz*

Main category: cs.RO

TL;DR: The paper explores a reinforcement learning (RL) framework that enables robotic arms to autonomously shape granular materials into desired structures.


<details>
  <summary>Details</summary>
Motivation: Challenges in manipulating granular materials arise from their complex dynamics and high-dimensional configuration space, making traditional methods less effective.

Method: A reinforcement learning framework with a robotic arm equipped with a cubic end-effector and stereo camera, focusing on compact observations and concise rewards, validated through an ablation study.

Result: The RL approach successfully trains visual policies for manipulation tasks, outperforming two baseline methods and achieving real-world deployment.

Conclusion: The study showcases the potential of RL for efficient granular material manipulation, providing insights into observation and reward design for complex tasks.

Abstract: Autonomous manipulation of granular media, such as sand, is crucial for
applications in construction, excavation, and additive manufacturing. However,
shaping granular materials presents unique challenges due to their
high-dimensional configuration space and complex dynamics, where traditional
rule-based approaches struggle without extensive engineering efforts.
Reinforcement learning (RL) offers a promising alternative by enabling agents
to learn adaptive manipulation strategies through trial and error. In this
work, we present an RL framework that enables a robotic arm with a cubic
end-effector and a stereo camera to shape granular media into desired target
structures. We show the importance of compact observations and concise reward
formulations for the large configuration space, validating our design choices
with an ablation study. Our results demonstrate the effectiveness of the
proposed approach for the training of visual policies that manipulate granular
media including their real-world deployment, outperforming two baseline
approaches.

</details>


### [429] [Event Driven CBBA with Reduced Communication](https://arxiv.org/abs/2509.06481)
*Vinita Sao,Tu Dac Ho,Sujoy Bhore,P. B. Sujit*

Main category: cs.RO

TL;DR: The research introduces an event-driven communication enhancement (ED-CBBA) to the Consensus-Based Bundle Algorithm (CBBA) for multi-robot task allocation, reducing message transmissions by 52% while maintaining solution quality.


<details>
  <summary>Details</summary>
Motivation: To address communication challenges in multi-robot task allocation caused by limited communication range and congestion in the conventional CBBA approach.

Method: The researchers developed an event-driven communication mechanism for CBBA, which activates communication updates only when certain events occur, reducing dependency on continuous communication.

Result: The event-driven CBBA (ED-CBBA) maintains the theoretical performance quality of standard CBBA while decreasing communication overhead significantly—cutting message transmissions by up to 52% as validated through Monte-Carlo simulations.

Conclusion: ED-CBBA demonstrates that event-driven communication can mitigate communication challenges in decentralized multi-robot systems without compromising task allocation quality or performance guarantees.

Abstract: In various scenarios such as multi-drone surveillance and search-and-rescue
operations, deploying multiple robots is essential to accomplish multiple tasks
at once. Due to the limited communication range of these vehicles, a
decentralised task allocation algorithm is crucial for effective task
distribution among robots. The consensus-based bundle algorithm (CBBA) has been
promising for multi-robot operation, offering theoretical guarantees. However,
CBBA demands continuous communication, leading to potential congestion and
packet loss that can hinder performance. In this study, we introduce an
event-driven communication mechanism designed to address these communication
challenges while maintaining the convergence and performance bounds of CBBA. We
demonstrate theoretically that the solution quality matches that of CBBA and
validate the approach with Monte-Carlo simulations across varying targets,
agents, and bundles. Results indicate that the proposed algorithm (ED-CBBA) can
reduce message transmissions by up to 52%.

</details>


### [430] [Co-Located VR with Hybrid SLAM-based HMD Tracking and Motion Capture Synchronization](https://arxiv.org/abs/2509.06582)
*Carlos A. Pinheiro de Sousa,Niklas Gröne,Mathias Günther,Oliver Deussen*

Main category: cs.RO

TL;DR: The paper introduces a VR co-location framework combining motion capture and SLAM for accurate and smooth, shared environments.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in VR co-location systems, such as latency and drift, and improve natural multi-user interactions.

Method: Integrating motion capture with SLAM-based inside-out tracking to enable smooth tracking with real-time pose sharing and spatial realignment.

Result: The framework ensures consistent spatial accuracy for multi-user interactions while enhancing comfort, scalability, and robustness.

Conclusion: The proposed approach provides a high-performing solution for natural and scalable multi-user VR experiences by addressing spatial alignment challenges effectively.

Abstract: We introduce a multi-user VR co-location framework that synchronizes users
within a shared virtual environment aligned to physical space. Our approach
combines a motion capture system with SLAM-based inside-out tracking to deliver
smooth, high-framerate, low-latency performance. Previous methods either rely
on continuous external tracking, which introduces latency and jitter, or on
one-time calibration, which cannot correct drift over time. In contrast, our
approach combines the responsiveness of local HMD SLAM tracking with the
flexibility to realign to an external source when needed. It also supports
real-time pose sharing across devices, ensuring consistent spatial alignment
and engagement between users. Our evaluation demonstrates that our framework
achieves the spatial accuracy required for natural multi-user interaction while
offering improved comfort, scalability, and robustness over existing co-located
VR solutions.

</details>


### [431] [A Robust Approach for LiDAR-Inertial Odometry Without Sensor-Specific Modeling](https://arxiv.org/abs/2509.06593)
*Meher V. R. Malladi,Tiziano Guadagnino,Luca Lobefaro,Cyrill Stachniss*

Main category: cs.RO

TL;DR: This paper presents a robust LiDAR-inertial odometry system, leveraging a simplified motion model and novel regularization for enhanced performance across diverse sensors and environments.


<details>
  <summary>Details</summary>
Motivation: Accurate odometry is essential for robotic navigation, and existing sensor-based methods often rely on sensor-specific modeling, limiting their adaptability across diverse domains.

Method: The proposed method integrates IMU data using a simplified motion model and employs scan-to-map LiDAR registration with a novel regularization to enhance odometry performance.

Result: Extensive experiments across various datasets and sensor configurations demonstrate the robustness and adaptability of the system in different scenarios.

Conclusion: The system achieves robust and accurate odometry performance with a universal configuration, and the open-sourcing of the implementation invites further development by the community.

Abstract: Accurate odometry is a critical component in a robotic navigation stack, and
subsequent modules such as planning and control often rely on an estimate of
the robot's motion. Sensor-based odometry approaches should be robust across
sensor types and deployable in different target domains, from solid-state
LiDARs mounted on cars in urban-driving scenarios to spinning LiDARs on
handheld packages used in unstructured natural environments. In this paper, we
propose a robust LiDAR-inertial odometry system that does not rely on
sensor-specific modeling. Sensor fusion techniques for LiDAR and inertial
measurement unit (IMU) data typically integrate IMU data iteratively in a
Kalman filter or use pre-integration in a factor graph framework, combined with
LiDAR scan matching often exploiting some form of feature extraction. We
propose an alternative strategy that only requires a simplified motion model
for IMU integration and directly registers LiDAR scans in a scan-to-map
approach. Our approach allows us to impose a novel regularization on the LiDAR
registration, improving the overall odometry performance. We detail extensive
experiments on a number of datasets covering a wide array of commonly used
robotic sensors and platforms. We show that our approach works with the exact
same configuration in all these scenarios, demonstrating its robustness. We
have open-sourced our implementation so that the community can build further on
our work and use it in their navigation stacks.

</details>


### [432] [LiHRA: A LiDAR-Based HRI Dataset for Automated Risk Monitoring Methods](https://arxiv.org/abs/2509.06597)
*Frederik Plahl,Georgios Katranis,Ilshat Mamaev,Andrey Morozov*

Main category: cs.RO

TL;DR: LiHRA is a new dataset designed to advance risk monitoring methods in Human-Robot Interaction by providing detailed multi-modal data, including LiDAR point clouds, human keypoints, and robot states.


<details>
  <summary>Details</summary>
Motivation: The growing adoption of collaborative robots in industrial environments demands reliable safety systems, but progress is hindered by the lack of realistic datasets capturing human-robot interactions.

Method: LiHRA combines 3D LiDAR point clouds, human keypoints, and robot states across six HRI scenarios, offering labeled data for both safe and hazardous versions to train and benchmark RM algorithms.

Result: The dataset contains 4,431 labeled point clouds recorded at 10 Hz, and an accompanying RM method quantifies risk levels dynamically using contextual data.

Conclusion: LiHRA provides an essential resource for advancing real-time risk monitoring and adaptive safety in collaborative human-robot environments, addressing critical gaps in the field.

Abstract: We present LiHRA, a novel dataset designed to facilitate the development of
automated, learning-based, or classical risk monitoring (RM) methods for
Human-Robot Interaction (HRI) scenarios. The growing prevalence of
collaborative robots in industrial environments has increased the need for
reliable safety systems. However, the lack of high-quality datasets that
capture realistic human-robot interactions, including potentially dangerous
events, slows development. LiHRA addresses this challenge by providing a
comprehensive, multi-modal dataset combining 3D LiDAR point clouds, human body
keypoints, and robot joint states, capturing the complete spatial and dynamic
context of human-robot collaboration. This combination of modalities allows for
precise tracking of human movement, robot actions, and environmental
conditions, enabling accurate RM during collaborative tasks. The LiHRA dataset
covers six representative HRI scenarios involving collaborative and coexistent
tasks, object handovers, and surface polishing, with safe and hazardous
versions of each scenario. In total, the data set includes 4,431 labeled point
clouds recorded at 10 Hz, providing a rich resource for training and
benchmarking classical and AI-driven RM algorithms. Finally, to demonstrate
LiHRA's utility, we introduce an RM method that quantifies the risk level in
each scenario over time. This method leverages contextual information,
including robot states and the dynamic model of the robot. With its combination
of high-resolution LiDAR data, precise human tracking, robot state data, and
realistic collision events, LiHRA offers an essential foundation for future
research into real-time RM and adaptive safety strategies in human-robot
workspaces.

</details>


### [433] [T-araVLN: Translator for Agricultural Robotic Agents on Vision-and-Language Navigation](https://arxiv.org/abs/2509.06644)
*Xiaobei Zhao,Xingqi Lyu,Xiang Li*

Main category: cs.RO

TL;DR: This paper proposes T-araVLN, a refined method for agricultural robotic navigation using Vision-and-Language Navigation (VLN), improving performance on the A2A benchmark.


<details>
  <summary>Details</summary>
Motivation: Agricultural robots increasingly assist in tasks but struggle with autonomous navigation using natural language instructions, especially for complex commands.

Method: T-araVLN employs an Instruction Translator module to refine and clarify natural language instructions for better navigation performance.

Result: On the A2A benchmark, T-araVLN enhances Success Rate (SR) from 0.47 to 0.63 and reduces Navigation Error (NE) from 2.91m to 2.28m, showcasing improved accuracy and efficiency.

Conclusion: The T-araVLN method achieves state-of-the-art navigation performance for agricultural robots and bridges limitations in understanding complex instructions.

Abstract: Agricultural robotic agents have been becoming powerful helpers in a wide
range of agricultural tasks, nevertheless, still heavily rely on manual
operation or untransportable railway for movement. The AgriVLN method and the
A2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the
agricultural domain, enabling agents navigate to the target position following
the natural language instructions. AgriVLN effectively understands the simple
instructions, however, often misunderstands the complicated instructions. To
bridge this gap, we propose the method of Translator for Agricultural Robotic
Agents on Vision-and-Language Navigation (T-araVLN), in which the Instruction
Translator module translates the original instruction to be both refined and
precise. Being evaluated on the A2A benchmark, our T-araVLN effectively
improves SR from 0.47 to 0.63 and reduces NE from 2.91m to 2.28m, demonstrating
the state-of-the-art performance in the agricultural domain. Code:
https://github.com/AlexTraveling/T-araVLN.

</details>


### [434] [An Adaptive Coverage Control Approach for Multiple Autonomous Off-road Vehicles in Dynamic Agricultural Fields](https://arxiv.org/abs/2509.06682)
*Sajad Ahmadi,Mohammadreza Davoodi,Javad Mohammadpour Velni*

Main category: cs.RO

TL;DR: The paper introduces a real-time adaptive coverage control system for UGVs in dynamic agricultural environments, utilizing UAVs for obstacle detection and terrain assessment.


<details>
  <summary>Details</summary>
Motivation: Existing coverage control methods assume static conditions, which are ineffective for agricultural fields where dynamic obstacles and terrain variability are common.

Method: The study models the environment as a weighted directed graph updated by UAV observations, employing Voronoi partitioning, adaptive weight assignment, and cost-based path optimization for UGV path planning.

Result: Simulations show improved path planning, lower traversal costs, and enhanced coverage efficiency despite dynamic challenges like obstacles and muddy terrains.

Conclusion: The proposed method effectively enables UGVs to handle real-world agricultural challenges by integrating UAV-assisted dynamic environmental mapping and adaptive planning.

Abstract: This paper presents an adaptive coverage control method for a fleet of
off-road and Unmanned Ground Vehicles (UGVs) operating in dynamic
(time-varying) agricultural environments. Traditional coverage control
approaches often assume static conditions, making them unsuitable for
real-world farming scenarios where obstacles, such as moving machinery and
uneven terrains, create continuous challenges. To address this, we propose a
real-time path planning framework that integrates Unmanned Aerial Vehicles
(UAVs) for obstacle detection and terrain assessment, allowing UGVs to
dynamically adjust their coverage paths. The environment is modeled as a
weighted directed graph, where the edge weights are continuously updated based
on the UAV observations to reflect obstacle motion and terrain variations. The
proposed approach incorporates Voronoi-based partitioning, adaptive edge weight
assignment, and cost-based path optimization to enhance navigation efficiency.
Simulation results demonstrate the effectiveness of the proposed method in
improving path planning, reducing traversal costs, and maintaining robust
coverage in the presence of dynamic obstacles and muddy terrains.

</details>


### [435] [Safe Robust Predictive Control-based Motion Planning of Automated Surface Vessels in Inland Waterways](https://arxiv.org/abs/2509.06687)
*Sajad Ahmadi,Hossein Nejatbakhsh Esfahani,Javad Mohammadpour Velni*

Main category: cs.RO

TL;DR: The paper introduces a motion planning approach for Automated Surface Vessels (ASVs) in narrow waterways, using Robust Model Predictive Control combined with Control Barrier Functions for safer and more adaptable navigation.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in navigating confined inland waterways with ASVs, such as narrow channels, high traffic density, and hydrodynamic disturbances, as existing methods lack robustness and precision for these environments.

Method: The authors proposed integrating Robust Model Predictive Control (RMPC) with Control Barrier Functions (CBFs), incorporating channel borders and obstacles as constraints in the control framework to ensure collision avoidance and robust navigation.

Result: Simulation results show the proposed method's effectiveness in safely guiding ASVs under realistic conditions, demonstrating improvements in safety and adaptability compared to existing methodologies.

Conclusion: The proposed approach enhances autonomous vessel navigation in complex waterways by ensuring collision avoidance and robust performance, advancing ASV deployment in sustainable transport solutions.

Abstract: Deploying self-navigating surface vessels in inland waterways offers a
sustainable alternative to reduce road traffic congestion and emissions.
However, navigating confined waterways presents unique challenges, including
narrow channels, higher traffic density, and hydrodynamic disturbances.
Existing methods for autonomous vessel navigation often lack the robustness or
precision required for such environments. This paper presents a new motion
planning approach for Automated Surface Vessels (ASVs) using Robust Model
Predictive Control (RMPC) combined with Control Barrier Functions (CBFs). By
incorporating channel borders and obstacles as safety constraints within the
control design framework, the proposed method ensures both collision avoidance
and robust navigation on complex waterways. Simulation results demonstrate the
efficacy of the proposed method in safely guiding ASVs under realistic
conditions, highlighting its improved safety and adaptability compared to the
state-of-the-art.

</details>


### [436] [Embodied Hazard Mitigation using Vision-Language Models for Autonomous Mobile Robots](https://arxiv.org/abs/2509.06768)
*Oluwadamilola Sotomi,Devika Kodi,Kiruthiga Chandra Shekar,Aliasghar Arab*

Main category: cs.RO

TL;DR: This paper introduces a multimodal system leveraging vision-language models and large language models to enable autonomous robots to detect and mitigate environmental anomalies in real-time.


<details>
  <summary>Details</summary>
Motivation: To improve safety and operational continuity of autonomous robots in dynamic environments by enabling proactive anomaly detection and mitigation.

Method: The paper integrates vision-language models and large language models to build a robot decision-making framework, designing specific mitigation strategies for hazardous and conflict states.

Result: The system achieved 91.2% prediction accuracy with low-latency responses, validated through user studies involving 30 participants using edge-ai architecture.

Conclusion: Integrating multimodal AI ensures efficient anomaly detection and proactive mitigation, contributing to safer and autonomous robotic operations in urban and dynamic environments.

Abstract: Autonomous robots operating in dynamic environments should identify and
report anomalies. Embodying proactive mitigation improves safety and
operational continuity. This paper presents a multimodal anomaly detection and
mitigation system that integrates vision-language models and large language
models to identify and report hazardous situations and conflicts in real-time.
The proposed system enables robots to perceive, interpret, report, and if
possible respond to urban and environmental anomalies through proactive
detection mechanisms and automated mitigation actions. A key contribution in
this paper is the integration of Hazardous and Conflict states into the robot's
decision-making framework, where each anomaly type can trigger specific
mitigation strategies. User studies (n = 30) demonstrated the effectiveness of
the system in anomaly detection with 91.2% prediction accuracy and relatively
low latency response times using edge-ai architecture.

</details>


### [437] [CRISP -- Compliant ROS2 Controllers for Learning-Based Manipulation Policies and Teleoperation](https://arxiv.org/abs/2509.06819)
*Daniel San José Pro,Oliver Hausdörfer,Ralf Römer,Maximilian Dösch,Martin Schuck,Angela P. Schöllig*

Main category: cs.RO

TL;DR: The paper introduces CRISP, a lightweight controller framework for ROS2-compatible robotic manipulators, which facilitates smooth tracking, real-time performance, and integration of learning-based high-level policies.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of integrating learning-based high-level controllers—which often exhibit low-frequency or discontinuous outputs—with hardware-level robot controllers while enabling compliant and smooth motion.

Method: CRISP is a C++ implementation of Cartesian and joint-space controllers for the ROS2 control standard, providing Python and Gymnasium interfaces for seamless data collection and deployment of high-level learning-based policies. It has been validated on various hardware and simulation platforms.

Result: The framework has been successfully validated on the Franka Robotics FR3 hardware, as well as in simulations using Kuka IIWA14 and Kinova Gen3 manipulators, demonstrating rapid integration and real-time performance.

Conclusion: CRISP offers a unified and flexible solution for integrating learning-based methods with ROS2 manipulators, lowering barriers for research and deployment. Detailed documentation further facilitates accessibility.

Abstract: Learning-based controllers, such as diffusion policies and vision-language
action models, often generate low-frequency or discontinuous robot state
changes. Achieving smooth reference tracking requires a low-level controller
that converts high-level targets commands into joint torques, enabling
compliant behavior during contact interactions. We present CRISP, a lightweight
C++ implementation of compliant Cartesian and joint-space controllers for the
ROS2 control standard, designed for seamless integration with high-level
learning-based policies as well as teleoperation. The controllers are
compatible with any manipulator that exposes a joint-torque interface. Through
our Python and Gymnasium interfaces, CRISP provides a unified pipeline for
recording data from hardware and simulation and deploying high-level
learning-based policies seamlessly, facilitating rapid experimentation. The
system has been validated on hardware with the Franka Robotics FR3 and in
simulation with the Kuka IIWA14 and Kinova Gen3. Designed for rapid
integration, flexible deployment, and real-time performance, our implementation
provides a unified pipeline for data collection and policy execution, lowering
the barrier to applying learning-based methods on ROS2-compatible manipulators.
Detailed documentation is available at the project website -
https://utiasDSL.github.io/crisp_controllers.

</details>


### [438] [Dynamic Modeling and Efficient Data-Driven Optimal Control for Micro Autonomous Surface Vehicles](https://arxiv.org/abs/2509.06882)
*Zhiheng Chen,Wei Wang*

Main category: cs.RO

TL;DR: The study proposes a data-driven optimal control method for enhancing Micro Autonomous Surface Vehicles (MicroASVs) by refining a physics-driven model in real time, achieving improved trajectory accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of controlling MicroASVs in constrained and dynamic environments, including nonlinear hydrodynamic forces and unpredictable disturbances.

Method: A hybrid approach combining a physics-driven dynamics model of an over-actuated MicroASV with a data-driven weak formulation-based online learning control mechanism for adaptive system refinement.

Result: Simulation results confirm improved trajectory tracking and system robustness despite unknown payloads and external disturbances.

Conclusion: The integration of data-driven online learning enhances the reliability and precision of MicroASV operations, showcasing its potential for future autonomous systems.

Abstract: Micro Autonomous Surface Vehicles (MicroASVs) offer significant potential for
operations in confined or shallow waters and swarm robotics applications.
However, achieving precise and robust control at such small scales remains
highly challenging, mainly due to the complexity of modeling nonlinear
hydrodynamic forces and the increased sensitivity to self-motion effects and
environmental disturbances, including waves and boundary effects in confined
spaces. This paper presents a physics-driven dynamics model for an
over-actuated MicroASV and introduces a data-driven optimal control framework
that leverages a weak formulation-based online model learning method. Our
approach continuously refines the physics-driven model in real time, enabling
adaptive control that adjusts to changing system parameters. Simulation results
demonstrate that the proposed method substantially enhances trajectory tracking
accuracy and robustness, even under unknown payloads and external disturbances.
These findings highlight the potential of data-driven online learning-based
optimal control to improve MicroASV performance, paving the way for more
reliable and precise autonomous surface vehicle operations.

</details>


### [439] [LLaDA-VLA: Vision Language Diffusion Action Models](https://arxiv.org/abs/2509.06932)
*Yuqing Wen,Hebei Li,Kefan Gu,Yucheng Zhao,Tiancai Wang,Xiaoyan Sun*

Main category: cs.RO

TL;DR: This paper introduces LLaDA-VLA, a vision-language-action model utilizing diffusion-based vision-language models for robotic manipulation. The model demonstrates superior performance compared to existing approaches.


<details>
  <summary>Details</summary>
Motivation: While auto-regressive models have seen advancements in vision-language-action tasks for robotics, leveraging diffusion-based vision-language models (d-VLMs) for robotic policy remains unexplored.

Method: The authors adapt d-VLMs to robotics through two strategies: (1) localized special-token classification for easier adaptation, and (2) hierarchical action-structured decoding to model dependencies in action sequences effectively.

Result: The proposed LLaDA-VLA outperforms state-of-the-art vision-language-action models in both simulated and real-world robotic tasks.

Conclusion: LLaDA-VLA demonstrates that diffusion-based approaches are effective for robotic manipulation, marking a significant step forward in the domain of vision-language-action models.

Abstract: The rapid progress of auto-regressive vision-language models (VLMs) has
inspired growing interest in vision-language-action models (VLA) for robotic
manipulation. Recently, masked diffusion models, a paradigm distinct from
autoregressive models, have begun to demonstrate competitive performance in
text generation and multimodal applications, leading to the development of a
series of diffusion-based VLMs (d-VLMs). However, leveraging such models for
robot policy learning remains largely unexplored. In this work, we present
LLaDA-VLA, the first Vision-Language-Diffusion-Action model built upon
pretrained d-VLMs for robotic manipulation. To effectively adapt d-VLMs to
robotic domain, we introduce two key designs: (1) a localized special-token
classification strategy that replaces full-vocabulary classification with
special action token classification, reducing adaptation difficulty; (2) a
hierarchical action-structured decoding strategy that decodes action sequences
hierarchically considering the dependencies within and across actions.
Extensive experiments demonstrate that LLaDA-VLA significantly outperforms
state-of-the-art VLAs on both simulation and real-world robots.

</details>


### [440] [F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions](https://arxiv.org/abs/2509.06951)
*Qi Lv,Weijie Kong,Hao Li,Jia Zeng,Zherui Qiu,Delin Qu,Haoming Song,Qizhi Chen,Xiang Deng,Jiangmiao Pang*

Main category: cs.RO

TL;DR: This paper introduces F1, a pretrained Vision-Language-Action (VLA) model integrating visual foresight for better decision-making in dynamic environments, achieving state-of-the-art performance by forecasting future visual states and reformulating action generation.


<details>
  <summary>Details</summary>
Motivation: Address the shortcomings of reactive Vision-Language-Action models that struggle with short-sighted behaviors and lack robustness in dynamic visual environments.

Method: Develop F1, a Mixture-of-Transformer architecture with modules for perception, foresight generation, and control. F1 uses a next-scale prediction mechanism for visual foresight integration into decision-making, trained on a dataset with 330,000 trajectories across 136 tasks.

Result: Evaluations in real-world and simulated tasks reveal F1's superior performance in task success rates and generalization compared to existing models.

Conclusion: F1 demonstrates that integrating visual foresight as planning targets can robustly improve decision-making and transferability in complex and dynamic environments.

Abstract: Executing language-conditioned tasks in dynamic visual environments remains a
central challenge in embodied AI. Existing Vision-Language-Action (VLA) models
predominantly adopt reactive state-to-action mappings, often leading to
short-sighted behaviors and poor robustness in dynamic scenes. In this paper,
we introduce F1, a pretrained VLA framework which integrates the visual
foresight generation into decision-making pipeline. F1 adopts a
Mixture-of-Transformer architecture with dedicated modules for perception,
foresight generation, and control, thereby bridging understanding, generation,
and actions. At its core, F1 employs a next-scale prediction mechanism to
synthesize goal-conditioned visual foresight as explicit planning targets. By
forecasting plausible future visual states, F1 reformulates action generation
as a foresight-guided inverse dynamics problem, enabling actions that
implicitly achieve visual goals. To endow F1 with robust and generalizable
capabilities, we propose a three-stage training recipe on an extensive dataset
comprising over 330k trajectories across 136 diverse tasks. This training
scheme enhances modular reasoning and equips the model with transferable visual
foresight, which is critical for complex and dynamic environments. Extensive
evaluations on real-world tasks and simulation benchmarks demonstrate F1
consistently outperforms existing approaches, achieving substantial gains in
both task success rate and generalization ability.

</details>


### [441] [Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments](https://arxiv.org/abs/2509.06953)
*Jiahui Yang,Jason Jingzhou Liu,Yulong Li,Youssef Khaky,Kenneth Shaw,Deepak Pathak*

Main category: cs.RO

TL;DR: The paper presents Deep Reactive Policy (DRP), a neural motion policy for robotic manipulators that improves motion generation in dynamic and partially observable environments, outperforming previous methods in simulations and real-world tests.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations of classical motion planners and neural motion policies in generating collision-free motion in dynamic, partially observable environments. Classical methods require full knowledge of the environment and are computationally slow, while neural methods struggle to generalize in complex settings.

Method: The researchers propose DRP, which leverages a transformer-based neural motion policy called IMPACT, pretrained on 10 million expert trajectories. Static obstacle avoidance is refined using iterative student-teacher fine-tuning, and dynamic obstacle avoidance at inference is enhanced with DCP-RMP, a locally reactive goal-proposal module. The system operates directly on point cloud sensory inputs.

Result: DRP was extensively evaluated in tasks with cluttered scenes, dynamic moving obstacles, and goal obstructions. It demonstrated exceptional generalization capability, achieving higher success rates compared to prior classical and neural motion-planning methods in both simulated and real-world scenarios.

Conclusion: Deep Reactive Policy (DRP) proves to be a significant advancement for robotic manipulators, offering a robust solution for motion planning in dynamic, partially observable environments. Its success suggests strong potential for deployment in real-world applications.

Abstract: Generating collision-free motion in dynamic, partially observable
environments is a fundamental challenge for robotic manipulators. Classical
motion planners can compute globally optimal trajectories but require full
environment knowledge and are typically too slow for dynamic scenes. Neural
motion policies offer a promising alternative by operating in closed-loop
directly on raw sensory inputs but often struggle to generalize in complex or
dynamic settings. We propose Deep Reactive Policy (DRP), a visuo-motor neural
motion policy designed for reactive motion generation in diverse dynamic
environments, operating directly on point cloud sensory input. At its core is
IMPACT, a transformer-based neural motion policy pretrained on 10 million
generated expert trajectories across diverse simulation scenarios. We further
improve IMPACT's static obstacle avoidance through iterative student-teacher
finetuning. We additionally enhance the policy's dynamic obstacle avoidance at
inference time using DCP-RMP, a locally reactive goal-proposal module. We
evaluate DRP on challenging tasks featuring cluttered scenes, dynamic moving
obstacles, and goal obstructions. DRP achieves strong generalization,
outperforming prior classical and neural methods in success rate across both
simulated and real-world settings. Video results and code available at
https://deep-reactive-policy.com

</details>


### [442] [LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement Learning with Mamba](https://arxiv.org/abs/2508.11849)
*Yinuo Wang,Gavin Tao*

Main category: cs.RO

TL;DR: This paper introduces LocoMamba, a vision-driven framework employing Mamba for efficient deep reinforcement learning with long-range dependencies and reduced training latency.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of efficient state representation, long-range dependency modeling, and overfitting for training neural networks in complex environments.

Method: The method uses selective state-space models with Mamba layers for token fusion, employing Proximal Policy Optimization with tailored state-centric rewards in challenging simulations.

Result: LocoMamba outperforms state-of-the-art baselines in success rates, generalization, collision reduction, and training efficiency across challenging terrains.

Conclusion: LocoMamba demonstrates the effectiveness of combining efficient state-space models and vision-driven state representations for robust and generalizable reinforcement learning outcomes.

Abstract: We introduce LocoMamba, a vision-driven cross-modal DRL framework built on
selective state-space models, specifically leveraging Mamba, that achieves
near-linear-time sequence modeling, effectively captures long-range
dependencies, and enables efficient training with longer sequences. First, we
embed proprioceptive states with a multilayer perceptron and patchify depth
images with a lightweight convolutional neural network, producing compact
tokens that improve state representation. Second, stacked Mamba layers fuse
these tokens via near-linear-time selective scanning, reducing latency and
memory footprint, remaining robust to token length and image resolution, and
providing an inductive bias that mitigates overfitting. Third, we train the
policy end-to-end with Proximal Policy Optimization under terrain and
appearance randomization and an obstacle-density curriculum, using a compact
state-centric reward that balances progress, smoothness, and safety. We
evaluate our method in challenging simulated environments with static and
moving obstacles as well as uneven terrain. Compared with state-of-the-art
baselines, our method achieves higher returns and success rates with fewer
collisions, exhibits stronger generalization to unseen terrains and obstacle
densities, and improves training efficiency by converging in fewer updates
under the same compute budget.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [443] [Adversarial Bug Reports as a Security Risk in Language Model-Based Automated Program Repair](https://arxiv.org/abs/2509.05372)
*Piotr Przymus,Andreas Happe,Jürgen Cito*

Main category: cs.SE

TL;DR: The paper investigates the security vulnerabilities of Automated Program Repair (APR) systems utilizing Large Language Models (LLMs) by introducing adversarial bug reports that lead to insecure code patches. Existing defenses prove inadequate against such attacks.


<details>
  <summary>Details</summary>
Motivation: As LLM-based APR systems become integral to software development workflows, the research aims to address the overlooked security risks posed by adversarial bug reports, which could mislead systems into generating harmful code changes.

Method: The study involves the creation of 51 adversarial bug reports using varying strategies (manual and automated), testing them against state-of-the-art APR systems. It assesses pre-repair defenses (e.g., custom filters) and post-repair detectors while introducing an automated framework for adversarial report generation.

Result: Findings indicate that current defenses are ineffective, as 90% of crafted bug reports resulted in insecure patches. The best pre-repair filter blocked only 47% of adversarial reports, and post-repair analysis was effective in just 58% of cases.

Conclusion: APR systems are vulnerable to adversarial exploitation, with detection and mitigation being costly and error-prone. The paper outlines recommendations for enhancing system robustness and calls for further research into trustworthy APR solutions.

Abstract: Large Language Model (LLM) - based Automated Program Repair (APR) systems are
increasingly integrated into modern software development workflows, offering
automated patches in response to natural language bug reports. However, this
reliance on untrusted user input introduces a novel and underexplored attack
surface. In this paper, we investigate the security risks posed by adversarial
bug reports -- realistic-looking issue submissions crafted to mislead APR
systems into producing insecure or harmful code changes. We develop a
comprehensive threat model and conduct an empirical study to evaluate the
vulnerability of state-of-the-art APR systems to such attacks. Our
demonstration comprises 51 adversarial bug reports generated across a spectrum
of strategies, from manual curation to fully automated pipelines. We test these
against leading APR model and assess both pre-repair defenses (e.g., LlamaGuard
variants, PromptGuard variants, Granite-Guardian, and custom LLM filters) and
post-repair detectors (GitHub Copilot, CodeQL). Our findings show that current
defenses are insufficient: 90\% of crafted bug reports triggered
attacker-aligned patches. The best pre-repair filter blocked only 47\%, while
post-repair analysis-often requiring human oversight-was effective in just 58\%
of cases. To support scalable security testing, we introduce a prototype
framework for automating the generation of adversarial bug reports. Our
analysis exposes a structural asymmetry: generating adversarial inputs is
inexpensive, while detecting or mitigating them remains costly and error-prone.
We conclude with practical recommendations for improving the robustness of APR
systems against adversarial misuse and highlight directions for future work on
trustworthy automated repair.

</details>


### [444] [Reverse Browser: Vector-Image-to-Code Generator](https://arxiv.org/abs/2509.05394)
*Zoltan Toth-Czifra*

Main category: cs.SE

TL;DR: The paper proposes using vector images instead of bitmaps to automate the conversion of user interface designs into code and introduces a new multi-scale metric for evaluation.


<details>
  <summary>Details</summary>
Motivation: Image-to-code automation struggles with achieving high fidelity to original designs, prompting exploration of alternative approaches.

Method: The paper introduces vector images as model inputs, creates large datasets, evaluates IQA algorithms, proposes a multi-scale metric, and trains a large open-weights model.

Result: The trained open-weights model is evaluated, showing limitations but improvements in fidelity compared to prior methods.

Conclusion: Using vector image inputs and a multi-scale metric offers potential improvements, but challenges remain in achieving high fidelity automation.

Abstract: Automating the conversion of user interface design into code (image-to-code
or image-to-UI) is an active area of software engineering research. However,
the state-of-the-art solutions do not achieve high fidelity to the original
design, as evidenced by benchmarks. In this work, I approach the problem
differently: I use vector images instead of bitmaps as model input. I create
several large datasets for training machine learning models. I evaluate the
available array of Image Quality Assessment (IQA) algorithms and introduce a
new, multi-scale metric. I then train a large open-weights model and discuss
its limitations.

</details>


### [445] [Combining TSL and LLM to Automate REST API Testing: A Comparative Study](https://arxiv.org/abs/2509.05540)
*Thiago Barradas,Aline Paes,Vânia de Oliveira Neves*

Main category: cs.SE

TL;DR: The paper introduces RestTSLLM, a novel method using Test Specification Language (TSL) combined with Large Language Models (LLMs) to automate REST API test case generation and evaluates various LLMs for their effectiveness.


<details>
  <summary>Details</summary>
Motivation: Efficient testing of REST APIs is hampered by challenges like complexity, numerous scenarios, and limited testing time, necessitating automation to address undetected failures and improve test coverage.

Method: The approach integrates Test Specification Language (TSL) with Large Language Models (LLMs) via prompt engineering and an automated pipeline to generate tests systematically using OpenAPI specifications.

Result: The evaluation showed that Claude 3.5 Sonnet consistently outperformed other models such as Deepseek R1, Qwen 2.5 32b, and Sabia 3 in generating robust and coherent REST API tests based on metrics like success rate, test coverage, and mutation score.

Conclusion: LLMs demonstrate strong potential in automating REST API test generation, with Claude 3.5 Sonnet leading as the most effective model among evaluated options.

Abstract: The effective execution of tests for REST APIs remains a considerable
challenge for development teams, driven by the inherent complexity of
distributed systems, the multitude of possible scenarios, and the limited time
available for test design. Exhaustive testing of all input combinations is
impractical, often resulting in undetected failures, high manual effort, and
limited test coverage. To address these issues, we introduce RestTSLLM, an
approach that uses Test Specification Language (TSL) in conjunction with Large
Language Models (LLMs) to automate the generation of test cases for REST APIs.
The approach targets two core challenges: the creation of test scenarios and
the definition of appropriate input data. The proposed solution integrates
prompt engineering techniques with an automated pipeline to evaluate various
LLMs on their ability to generate tests from OpenAPI specifications. The
evaluation focused on metrics such as success rate, test coverage, and mutation
score, enabling a systematic comparison of model performance. The results
indicate that the best-performing LLMs - Claude 3.5 Sonnet (Anthropic),
Deepseek R1 (Deepseek), Qwen 2.5 32b (Alibaba), and Sabia 3 (Maritaca) -
consistently produced robust and contextually coherent REST API tests. Among
them, Claude 3.5 Sonnet outperformed all other models across every metric,
emerging in this study as the most suitable model for this task. These findings
highlight the potential of LLMs to automate the generation of tests based on
API specifications.

</details>


### [446] [Natural Language-Programming Language Software Traceability Link Recovery Needs More than Textual Similarity](https://arxiv.org/abs/2509.05585)
*Zhiyuan Zou,Bangchao Wang,Peng Liang,Tingting Bi,Huan Jin*

Main category: cs.SE

TL;DR: Textual similarity alone is insufficient for software traceability link recovery (TLR) in NL-PL artifacts due to the semantic gap. Multi-strategy approaches outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Address limitations of relying solely on textual similarity for TLR tasks in scenarios involving NL-PL artifacts.

Method: Integrate domain-specific strategies into Heterogeneous Graph Transformer (HGT) and Gemini 2.5 Pro models, evaluated on requirements-to-code TLR.

Result: Both modified models achieved notable F1-score improvements compared to their original versions and state-of-the-art methods.

Conclusion: Multi-strategy integration effectively enhances model performance for NL-PL TLR tasks, showing promising results across diverse projects.

Abstract: In the field of software traceability link recovery (TLR), textual similarity
has long been regarded as the core criterion. However, in tasks involving
natural language and programming language (NL-PL) artifacts, relying solely on
textual similarity is limited by their semantic gap. To this end, we conducted
a large-scale empirical evaluation across various types of TLR tasks, revealing
the limitations of textual similarity in NL-PL scenarios. To address these
limitations, we propose an approach that incorporates multiple domain-specific
auxiliary strategies, identified through empirical analysis, into two models:
the Heterogeneous Graph Transformer (HGT) via edge types and the prompt-based
Gemini 2.5 Pro via additional input information. We then evaluated our approach
using the widely studied requirements-to-code TLR task, a representative case
of NL-PL TLR. Experimental results show that both the multi-strategy HGT and
Gemini 2.5 Pro models outperformed their original counterparts without strategy
integration. Furthermore, compared to the current state-of-the-art method
HGNNLink, the multi-strategy HGT and Gemini 2.5 Pro models achieved average
F1-score improvements of 3.68% and 8.84%, respectively, across twelve
open-source projects, demonstrating the effectiveness of multi-strategy
integration in enhancing overall model performance for the requirements-code
TLR task.

</details>


### [447] [Verifying Correctness of PLC Software during System Evolution using Model Containment Approach](https://arxiv.org/abs/2509.05596)
*Soumyadip Bandyopadhyay,Santonu Sarkar*

Main category: cs.SE

TL;DR: The paper introduces a method to verify correctness in upgraded PLC software using Petri net models and a novel containment checking algorithm, resulting in improved performance compared to existing tools.


<details>
  <summary>Details</summary>
Motivation: PLC software upgrades often face challenges in verifying the correctness of changes, which is critical for industrial requirements.

Method: The authors propose a verification approach by converting sequential function charts into Petri net models and employing a symbolic path equivalence-based containment checking algorithm.

Result: Experimental evaluation on 80 benchmarks reveals that the framework is scalable and effective, achieving a 4x performance improvement over verifAPS.

Conclusion: The method provides a robust verification solution for PLC software upgradation, demonstrating significant performance and scalability advantages over current tools.

Abstract: Upgradation of Programmable Logic Controller (PLC) software is quite common
to accommodate evolving industrial requirements. Verifying the correctness of
such upgrades remains a significant challenge. In this paper, we propose a
verification-based approach to ensure the correctness of the existing
functionality in the upgraded version of a PLC software. The method converts
the older and the newer versions of the sequential function chart (SFC) into
two Petri net models. We then verify whether one model is contained within
another, based on a novel containment checking algorithm grounded in symbolic
path equivalence. For this purpose, we have developed a home-grown Petri
net-based containment checker. Experimental evaluation on 80 real-world
benchmarks from the OSCAT library highlights the scalability and effectiveness
of the framework. We have compared our approach with verifAPS, a popular tool
used for software upgradation, and observed nearly 4x performance improvement.

</details>


### [448] [Automating API Documentation with LLMs: A BERTopic Approach](https://arxiv.org/abs/2509.05749)
*AmirHossein Naghshzan*

Main category: cs.SE

TL;DR: The study explores automating the summarization of informal sources like Stack Overflow to enhance Android API documentation using BERTopic and extractive summarization methods.


<details>
  <summary>Details</summary>
Motivation: Official API documentation is often lengthy, complex, and incomplete, prompting the need for more accessible resources.

Method: The authors extracted topics from 3.6 million Stack Overflow posts using BERTopic and applied extractive summarization to generate concise summaries with code snippets.

Result: A user study involving 30 Android developers demonstrated that the generated summaries improved coherence, relevance, informativeness, satisfaction, and productivity.

Conclusion: Integrating community-driven insights with formal API documentation enhances accessibility and practical usability of API resources.

Abstract: Developers rely on API documentation, but official sources are often lengthy,
complex, or incomplete. Many turn to community-driven forums like Stack
Overflow for practical insights. We propose automating the summarization of
informal sources, focusing on Android APIs. Using BERTopic, we extracted
prevalent topics from 3.6 million Stack Overflow posts and applied extractive
summarization techniques to generate concise summaries, including code
snippets. A user study with 30 Android developers assessed the summaries for
coherence, relevance, informativeness, and satisfaction, showing improved
productivity. Integrating formal API knowledge with community-generated content
enhances documentation, making API resources more accessible and actionable
work.

</details>


### [449] [IoT Miner: Intelligent Extraction of Event Logs from Sensor Data for Process Mining](https://arxiv.org/abs/2509.05769)
*Edyta Brzychczy,Urszula Jessen,Krzysztof Kluza,Sridhar Sriram,Manuel Vargas Nettelnstroth*

Main category: cs.SE

TL;DR: IoT Miner is a framework designed to create event logs from raw sensor data using a four-stage pipeline, incorporating large language models for labeling.


<details>
  <summary>Details</summary>
Motivation: Traditional event logs for process mining are often unavailable in industrial settings. IoT Miner tackles the challenge of deriving structured event logs from unstructured sensor data.

Method: The framework includes data preprocessing, unsupervised clustering, LLM-based labeling with domain prompts, and event log construction. A new metric, Similarity-Weighted Accuracy, is introduced for evaluation.

Result: Evaluations on Load-Haul-Dump mining machine data showed that domain-specific, rich prompts significantly enhance the accuracy and consistency of activity labels generated by LLMs.

Conclusion: IoT Miner merges AI and domain-specific data processing to provide a scalable solution for extracting event logs from IoT data, addressing gaps in process mining for environments lacking traditional logs.

Abstract: This paper presents IoT Miner, a novel framework for automatically creating
high-level event logs from raw industrial sensor data to support process
mining. In many real-world settings, such as mining or manufacturing, standard
event logs are unavailable, and sensor data lacks the structure and semantics
needed for analysis. IoT Miner addresses this gap using a four-stage pipeline:
data preprocessing, unsupervised clustering, large language model (LLM)-based
labeling, and event log construction. A key innovation is the use of LLMs to
generate meaningful activity labels from cluster statistics, guided by
domain-specific prompts. We evaluate the approach on sensor data from a
Load-Haul-Dump (LHD) mining machine and introduce a new metric,
Similarity-Weighted Accuracy, to assess labeling quality. Results show that
richer prompts lead to more accurate and consistent labels. By combining AI
with domain-aware data processing, IoT Miner offers a scalable and
interpretable method for generating event logs from IoT data, enabling process
mining in settings where traditional logs are missing.

</details>


### [450] [Efficiently Ranking Software Variants with Minimal Benchmarks](https://arxiv.org/abs/2509.06716)
*Théo Matricon,Mathieu Acher,Helge Spieker,Arnaud Gotlieb*

Main category: cs.SE

TL;DR: The paper introduces BISection Sampling (BISS), a novel method for reducing computational costs in software benchmarking by optimizing test suite instances while preserving ranking stability of software variants.


<details>
  <summary>Details</summary>
Motivation: Benchmarking is computationally expensive and time-consuming, yet critical for evaluating software variants in terms of performance, regression detection, and system design. The authors aim to reduce this cost while preserving the clarity of results.

Method: The BISection Sampling (BISS) strategy combines test suite optimization with a divide-and-conquer approach to selectively retain critical tests while removing others, ensuring that the rankings of software variants remain stable.

Result: On datasets from various domains such as LLM leaderboards and SAT competitions, BISS reduced benchmark computational costs to an average of 44%, with up to 99% reduction in more than half of the benchmarks, without sacrificing ranking stability.

Conclusion: The BISS method is an effective optimization technique for benchmarking, enabling significant computational cost savings while maintaining reliable variant rankings, and performing better than baseline methods.

Abstract: Benchmarking is a common practice in software engineering to assess the
qualities and performance of software variants, coming from multiple competing
systems or from configurations of the same system. Benchmarks are used notably
to compare and understand variant performance, fine-tune software, detect
regressions, or design new software systems. The execution of benchmarks to get
a complete picture of software variants is highly costly in terms of
computational resources and time. In this paper, we propose a novel approach
for reducing benchmarks while maintaining stable rankings, using test suite
optimization techniques. That is, we remove instances from the benchmarks while
trying to keep the same rankings of the variants on all tests. Our method,
BISection Sampling, BISS, strategically retains the most critical tests and
applies a novel divide-and-conquer approach to efficiently sample among
relevant remaining tests. We experiment with datasets and use cases from LLM
leaderboards, SAT competitions, and configurable systems for performance
modeling. Our results show that our method outperforms baselines even when
operating on a subset of variants. Using BISS, we reduce the computational cost
of the benchmarks on average to 44% and on more than half the benchmarks by up
to 99% without loss in ranking stability.

</details>


### [451] [GeoAnalystBench: A GeoAI benchmark for assessing large language models for spatial analysis workflow and code generation](https://arxiv.org/abs/2509.05881)
*Qianheng Zhang,Song Gao,Chen Wei,Yibo Zhao,Ying Nie,Ziru Chen,Shijie Chen,Yu Su,Huan Sun*

Main category: cs.SE

TL;DR: The paper introduces GeoAnalystBench to rigorously evaluate large language models (LLMs) for geospatial tasks, finding proprietary models outperform open-source ones while highlighting challenges in complex spatial reasoning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address uncertainty in the capabilities of LLMs for automating GIS workflows by providing rigorous evaluation before assuming they can fully automate tasks.

Method: The authors created GeoAnalystBench, a benchmark consisting of 50 Python geospatial tasks validated by GIS experts. Tasks are evaluated based on workflow validity, structural alignment, semantic similarity, and code quality using metrics like CodeBLEU.

Result: Proprietary models like ChatGPT-4o-mini showed stronger performance with 95% validity and CodeBLEU of 0.39, whereas open source models, such as DeepSeek-R1-7B, often produced incomplete workflows, achieving only 48.5% validity and 0.272 CodeBLEU.

Conclusion: While current LLMs exhibit promise for GIS automation, they have significant limitations in tasks involving advanced spatial reasoning. The GeoAnalystBench offers a reproducible evaluation framework to guide future GeoAI improvements.

Abstract: Recent advances in large language models (LLMs) have fueled growing interest
in automating geospatial analysis and GIS workflows, yet their actual
capabilities remain uncertain. In this work, we call for rigorous evaluation of
LLMs on well-defined geoprocessing tasks before making claims about full GIS
automation. To this end, we present GeoAnalystBench, a benchmark of 50
Python-based tasks derived from real-world geospatial problems and carefully
validated by GIS experts. Each task is paired with a minimum deliverable
product, and evaluation covers workflow validity, structural alignment,
semantic similarity, and code quality (CodeBLEU). Using this benchmark, we
assess both proprietary and open source models. Results reveal a clear gap:
proprietary models such as ChatGPT-4o-mini achieve high validity 95% and
stronger code alignment (CodeBLEU 0.39), while smaller open source models like
DeepSeek-R1-7B often generate incomplete or inconsistent workflows (48.5%
validity, 0.272 CodeBLEU). Tasks requiring deeper spatial reasoning, such as
spatial relationship detection or optimal site selection, remain the most
challenging across all models. These findings demonstrate both the promise and
limitations of current LLMs in GIS automation and provide a reproducible
framework to advance GeoAI research with human-in-the-loop support.

</details>


### [452] [Code2MCP: A Multi-Agent Framework for Automated Transformation of Code Repositories into Model Context Protocol Services](https://arxiv.org/abs/2509.05941)
*Chaoqian Ouyang,Ling Yue,Shimin Di,Libin Zheng,Shaowu Pan,Min-Ling Zhang*

Main category: cs.SE

TL;DR: Code2MCP automates transforming GitHub repositories into MCP-compliant services, addressing the integration bottleneck in the AI agent ecosystem.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the integration complexity in the AI ecosystem, where many AI models must be manually integrated with existing tools, leading to significant development overhead.

Method: The proposed solution, Code2MCP, uses an automated framework driven by a multi-stage process and a closed-loop LLM-based cycle to analyze, debug, and deploy services from GitHub repositories.

Result: Code2MCP successfully automates the integration process, producing functional MCP services, debugged code, and comprehensive documentation with minimal human intervention.

Conclusion: This framework systematically unlocks the potential of open-source code repositories, advancing the MCP ecosystem and easing the tool integration process for large language models.

Abstract: The proliferation of Large Language Models (LLMs) has created a significant
integration challenge in the AI agent ecosystem, often called the "$N \times M$
problem," where N models require custom integrations for M tools. This
fragmentation stifles innovation and creates substantial development overhead.
While the Model Context Protocol (MCP) has emerged as a standard to resolve
this, its adoption is hindered by the manual effort required to convert the
vast universe of existing software into MCP-compliant services. This is
especially true for the millions of open-source repositories on GitHub, the
world's largest collection of functional code. This paper introduces Code2MCP,
a highly automated, agentic framework designed to transform any GitHub
repository into a functional MCP service with minimal human intervention. Our
system employs a multi-stage workflow that automates the entire process, from
code analysis and environment configuration to service generation and
deployment. A key innovation of our framework is an LLM-driven, closed-loop
"Run--Review--Fix" cycle, which enables the system to autonomously debug and
repair the code it generates. Code2MCP produces not only deployable services
but also comprehensive technical documentation, acting as a catalyst to
accelerate the MCP ecosystem by systematically unlocking the world's largest
open-source code repository and automating the critical last mile of tool
integration. The code is open-sourced at
https://github.com/DEFENSE-SEU/MCP-Github-Agent.

</details>


### [453] [GRACE: Graph-Guided Repository-Aware Code Completion through Hierarchical Code Fusion](https://arxiv.org/abs/2509.05980)
*Xingliang Wang,Baoyi Wang,Chen Zhi,Junxiao Han,Xinkui Zhao,Jianwei Yin,Shuiguang Deng*

Main category: cs.SE

TL;DR: GRACE is a novel framework for repository-level code generation that uses a hybrid graph-based retrieval method and structural fusion to enhance LLM performance on code tasks.


<details>
  <summary>Details</summary>
Motivation: Existing language models struggle with repository-level code generation tasks due to limited context windows and insufficient handling of complex semantic and structural dependencies. Existing retrieval-augmented methods overly focus on textual similarities and fail to preserve structural relationships in code.

Method: GRACE creates a multi-level, multi-semantic code graph that integrates static and dynamic code elements (e.g., file structure, call graphs). For retrieval, it uses a hybrid graph retrieval system combining graph neural network-based structural similarity with textual retrieval. It re-ranks results using graph attention networks and introduces a structural fusion mechanism to preserve critical dependencies in the context.

Result: GRACE shows substantial performance improvements, achieving 8.19% EM and 7.51% ES point gains over previous state-of-the-art methods on public repository-level benchmarks using DeepSeek-V3 as the language model backbone.

Conclusion: GRACE effectively addresses limitations of current code-generation approaches by leveraging graph-based retrieval and structural fusion, enabling superior handling of repository-level code tasks. This advance demonstrates its potential in improving LLM capabilities for real-world software development tasks.

Abstract: LLMs excel in localized code completion but struggle with repository-level
tasks due to limited context windows and complex semantic and structural
dependencies across codebases. While Retrieval-Augmented Generation (RAG)
mitigates context scarcity by retrieving relevant code snippets, current
approaches face significant limitations. They overly rely on textual similarity
for retrieval, neglecting structural relationships such as call chains and
inheritance hierarchies, and lose critical structural information by naively
concatenating retrieved snippets into text sequences for LLM input. To address
these shortcomings, GRACE constructs a multi-level, multi-semantic code graph
that unifies file structures, abstract syntax trees, function call graphs,
class hierarchies, and data flow graphs to capture both static and dynamic code
semantics. For retrieval, GRACE employs a Hybrid Graph Retriever that
integrates graph neural network-based structural similarity with textual
retrieval, refined by a graph attention network-based re-ranker to prioritize
topologically relevant subgraphs. To enhance context, GRACE introduces a
structural fusion mechanism that merges retrieved subgraphs with the local code
context and preserves essential dependencies like function calls and
inheritance. Extensive experiments on public repository-level benchmarks
demonstrate that GRACE significantly outperforms state-of-the-art methods
across all metrics. Using DeepSeek-V3 as the backbone LLM, GRACE surpasses the
strongest graph-based RAG baselines by 8.19% EM and 7.51% ES points on every
dataset. The code is available at
https://anonymous.4open.science/r/grace_icse-C3D5.

</details>


### [454] [Students' Perception of LLM Use in Requirements Engineering Education: An Empirical Study Across Two Universities](https://arxiv.org/abs/2509.05995)
*Sharon Guardado,Risha Parveen,Zheying Zhang,Maruf Rayhan,Nirnaya Tripathi*

Main category: cs.SE

TL;DR: This study assesses how integrating Large Language Models (LLMs) in Requirements Engineering (RE) courses impacts student learning, showing improvements but also concerns about overreliance and integration challenges.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore how LLMs can enhance educational methods in RE, improving student engagement and preparing them for industry practices.

Method: The researchers collected survey data from 179 students across two universities, comparing individual assignments and team-based Agile projects to evaluate the integration of LLMs.

Result: LLMs improved students' understanding of RE concepts but raised concerns regarding dependence on AI, academic integrity, and its integration into coursework.

Conclusion: Contextual and balanced integration of LLMs in education is essential to leverage their benefits while fostering critical thinking and collaborative skills.

Abstract: The integration of Large Language Models (LLMs) in Requirements Engineering
(RE) education is reshaping pedagogical approaches, seeking to enhance student
engagement and motivation while providing practical tools to support their
professional future. This study empirically evaluates the impact of integrating
LLMs in RE coursework. We examined how the guided use of LLMs influenced
students' learning experiences, and what benefits and challenges they perceived
in using LLMs in RE practices. The study collected survey data from 179
students across two RE courses in two universities. LLMs were integrated into
coursework through different instructional formats, i.e., individual
assignments versus a team-based Agile project. Our findings indicate that LLMs
improved students' comprehension of RE concepts, particularly in tasks like
requirements elicitation and documentation. However, students raised concerns
about LLMs in education, including academic integrity, overreliance on AI, and
challenges in integrating AI-generated content into assignments. Students who
worked on individual assignments perceived that they benefited more than those
who worked on team-based assignments, highlighting the importance of contextual
AI integration. This study offers recommendations for the effective integration
of LLMs in RE education. It proposes future research directions for balancing
AI-assisted learning with critical thinking and collaborative practices in RE
courses.

</details>


### [455] [A Rapid Review Regarding the Concept of Legal Requirements in Requirements Engineering](https://arxiv.org/abs/2509.06012)
*Jukka Ruohonen*

Main category: cs.SE

TL;DR: The paper reviews the concept of legal requirements in requirements engineering, noting a lack of clear definitions, empirical evidence, and enduring conceptual confusion.


<details>
  <summary>Details</summary>
Motivation: There is confusion in existing literature regarding the concept of legal requirements, highlighted by peer review comments and personal puzzlement.

Method: The paper conducts a rapid review of literature to analyze perspectives on legal requirements in requirements engineering.

Result: Legal requirements are often vague, complex, overlapping, and reluctantly implemented, lacking clear definitions or empirical backing in literature.

Conclusion: The study identifies significant knowledge gaps and conceptual confusion in handling legal requirements within requirements engineering, suggesting a need for clearer definitions and evidence-based approaches.

Abstract: Out of a personal puzzlement, recent peer review comments, and demonstrable
confusion in the existing literature, the paper presents a rapid review of the
concept of legal requirements (LRs) in requirements engineering (RE) research.
According to reviewing results, a normative understanding of LRs has often been
present, although proper definitions and conceptual operationalizations are
lacking. Some papers also see LRs as functional and others as non-functional
requirements. Legal requirements are often characterized as being vague and
complex, requiring a lot of effort to elicit, implement, and validate. These
characterizations supposedly correlate with knowledge gaps among requirements
engineers. LRs are also seen to often change and overlap. They may be also
prioritized. According to the literature, they seem to be also reluctantly
implemented, often providing only a minimal baseline for other requirements.
With these and other observations, the review raises critical arguments about
apparent knowledge gaps, including a lack of empirical evidence backing the
observations and enduring conceptual confusion.

</details>


### [456] [Empirical Study of Code Large Language Models for Binary Security Patch Detection](https://arxiv.org/abs/2509.06052)
*Qingyuan Li,Binchang Li,Cuiyun Gao,Shuzheng Gao,Zongjie Li*

Main category: cs.SE

TL;DR: The paper explores security patch detection in binary files using fine-tuned code large language models (LLMs), highlighting their effectiveness over vanilla LLMs.


<details>
  <summary>Details</summary>
Motivation: To address the gap in detecting security patches in closed-source software systems where source code is not accessible, leveraging the potential of code large language models.

Method: Constructed a large-scale binary patch dataset with 19,448 samples in assembly code and pseudo-code representations, tested 19 LLMs, and implemented fine-tuning strategies to inject domain knowledge for binary SPD.

Result: Fine-tuned LLMs demonstrated exceptional performance in detecting security patches, especially using pseudo-code representation.

Conclusion: Fine-tuning code LLMs with relevant domain knowledge significantly enhances their effectiveness in binary security patch detection compared to vanilla LLMs.

Abstract: Security patch detection (SPD) is crucial for maintaining software security,
as unpatched vulnerabilities can lead to severe security risks. In recent
years, numerous learning-based SPD approaches have demonstrated promising
results on source code. However, these approaches typically cannot be applied
to closed-source applications and proprietary systems that constitute a
significant portion of real-world software, as they release patches only with
binary files, and the source code is inaccessible. Given the impressive
performance of code large language models (LLMs) in code intelligence and
binary analysis tasks such as decompilation and compilation optimization, their
potential for detecting binary security patches remains unexplored, exposing a
significant research gap between their demonstrated low-level code
understanding capabilities and this critical security task. To address this
gap, we construct a large-scale binary patch dataset containing \textbf{19,448}
samples, with two levels of representation: assembly code and pseudo-code, and
systematically evaluate \textbf{19} code LLMs of varying scales to investigate
their capability in binary SPD tasks. Our initial exploration demonstrates that
directly prompting vanilla code LLMs struggles to accurately identify security
patches from binary patches, and even state-of-the-art prompting techniques
fail to mitigate the lack of domain knowledge in binary SPD within vanilla
models. Drawing on the initial findings, we further investigate the fine-tuning
strategy for injecting binary SPD domain knowledge into code LLMs through two
levels of representation. Experimental results demonstrate that fine-tuned LLMs
achieve outstanding performance, with the best results obtained on the
pseudo-code representation.

</details>


### [457] [Software Dependencies 2.0: An Empirical Study of Reuse and Integration of Pre-Trained Models in Open-Source Projects](https://arxiv.org/abs/2509.06085)
*Jerin Yasmin,Wenxin Jiang,James C. Davis,Yuan Tian*

Main category: cs.SE

TL;DR: Pre-trained models (PTMs) are increasingly adopted as software dependencies, termed Software Dependencies 2.0, raising concerns about their integration, maintainability, and reliability. This study explores how open-source projects manage PTM reuse.


<details>
  <summary>Details</summary>
Motivation: Open-source software increasingly relies on pre-trained models for various tasks, introducing new challenges in managing these learned dependencies, which may affect the reliability and maintainability of software systems.

Method: The study analyzes a random sample of 401 GitHub repositories from the PeaTMOSS dataset, performing quantitative pattern identification and qualitative analysis of developers' practices in reusing PTMs from Hugging Face and PyTorch Hub.

Result: The researchers identified patterns in how projects structure and document PTM dependencies and revealed organizational and pipeline stages for integrating these models. Additionally, they observed interactions among PTMs and other components.

Conclusion: This study highlights the complexity in integrating PTMs as dependencies in open-source projects and provides insights into structuring reuse pipelines for better management and reliability of software systems.

Abstract: Pre-trained models (PTMs) are machine learning models that have been trained
in advance, often on large-scale data, and can be reused for new tasks, thereby
reducing the need for costly training from scratch. Their widespread adoption
introduces a new class of software dependency, which we term Software
Dependencies 2.0, extending beyond conventional libraries to learned behaviors
embodied in trained models and their associated artifacts. The integration of
PTMs as software dependencies in real projects remains unclear, potentially
threatening maintainability and reliability of modern software systems that
increasingly rely on them. Objective: In this study, we investigate Software
Dependencies 2.0 in open-source software (OSS) projects by examining the reuse
of PTMs, with a focus on how developers manage and integrate these models.
Specifically, we seek to understand: (1) how OSS projects structure and
document their PTM dependencies; (2) what stages and organizational patterns
emerge in the reuse pipelines of PTMs within these projects; and (3) the
interactions among PTMs and other learned components across pipeline stages. We
conduct a mixed-methods analysis of a statistically significant random sample
of 401 GitHub repositories from the PeaTMOSS dataset (28,575 repositories
reusing PTMs from Hugging Face and PyTorch Hub). We quantitatively examine PTM
reuse by identifying patterns and qualitatively investigate how developers
integrate and manage these models in practice.

</details>


### [458] [Agentic Software Engineering: Foundational Pillars and a Research Roadmap](https://arxiv.org/abs/2509.06216)
*Ahmed E. Hassan,Hao Li,Dayi Lin,Bram Adams,Tse-Hsun Chen,Yutaro Kashiwa,Dong Qiu*

Main category: cs.SE

TL;DR: The paper introduces Structured Agentic Software Engineering (SASE), a vision for SE 3.0 where intelligent agents collaborate with humans for complex, goal-oriented software engineering tasks.


<details>
  <summary>Details</summary>
Motivation: As software engineering tasks grow increasingly complex with advancements in AI, there is a need to redefine its foundational practices to accommodate intelligent agents and foster enhanced human-AI collaboration.

Method: The authors propose two new environments: the Agent Command Environment (ACE) for human oversight and mentoring of agents, and the Agent Execution Environment (AEE), where agents execute tasks and seek human guidance when required.

Result: The work establishes a conceptual framework, including new processes and modalities, to enable structured interactions between human engineers and AI agents.

Conclusion: The paper advocates for a paradigm shift toward agentic SE practices, emphasizing a collaborative and trustworthy future. It also provides a roadmap for addressing challenges and adapting SE education to this future.

Abstract: Agentic Software Engineering (SE 3.0) represents a new era where intelligent
agents are tasked not with simple code generation, but with achieving complex,
goal-oriented SE objectives. To harness these new capabilities while ensuring
trustworthiness, we must recognize a fundamental duality within the SE field in
the Agentic SE era, comprising two symbiotic modalities: SE for Humans and SE
for Agents. This duality demands a radical reimagining of the foundational
pillars of SE (actors, processes, tools, and artifacts) which manifest
differently across each modality. We propose two purpose-built workbenches to
support this vision. The Agent Command Environment (ACE) serves as a command
center where humans orchestrate and mentor agent teams, handling outputs such
as Merge-Readiness Packs (MRPs) and Consultation Request Packs (CRPs). The
Agent Execution Environment (AEE) is a digital workspace where agents perform
tasks while invoking human expertise when facing ambiguity or complex
trade-offs. This bi-directional partnership, which supports agent-initiated
human callbacks and handovers, gives rise to new, structured engineering
activities (i.e., processes) that redefine human-AI collaboration, elevating
the practice from agentic coding to true agentic software engineering. This
paper presents the Structured Agentic Software Engineering (SASE) vision,
outlining several of the foundational pillars for the future of SE. The paper
culminates in a research roadmap that identifies a few key challenges and
opportunities while briefly discussing the resulting impact of this future on
SE education. Our goal is not to offer a definitive solution, but to provide a
conceptual scaffold with structured vocabulary to catalyze a community-wide
dialogue, pushing the SE community to think beyond its classic, human-centric
tenets toward a disciplined, scalable, and trustworthy agentic future.

</details>


### [459] [Learning From Software Failures: A Case Study at a National Space Research Center](https://arxiv.org/abs/2509.06301)
*Dharun Anandayuvaraj,Zain Hammadeh,Andreas Lund,Alexandra Holloway,James C. Davis*

Main category: cs.SE

TL;DR: This paper examines the practices and challenges of how software engineers at high-reliability organizations (HROs) learn from failures, identifying informal and inconsistent processes, recurring failures, and key challenges such as time constraints and fragmented documentation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance our understanding of how software engineers gather, document, share, and apply lessons learned from software failures, especially in high-reliability organizations where failures carry catastrophic risks.

Method: The study involved 10 in-depth interviews with research software engineers at a national space research center, supplemented by 5 additional interviews at other HROs to assess transferability.

Result: Findings reveal that failure learning is informal, ad hoc, and inconsistently integrated into the software development life cycle, with recurring failures persisting due to a lack of structured processes. Key challenges include time constraints, knowledge loss, fragmented documentation, and weak enforcement of processes.

Conclusion: The study highlights the need for structured failure management practices and tools to improve systematic learning, reliability, and the prevention of recurring failures in software engineering.

Abstract: Software failures can have significant consequences, making learning from
failures a critical aspect of software engineering. While software
organizations are recommended to conduct postmortems, the effectiveness and
adoption of these practices vary widely. Understanding how engineers gather,
document, share, and apply lessons from failures is essential for improving
reliability and preventing recurrence. High-reliability organizations (HROs)
often develop software systems where failures carry catastrophic risks,
requiring continuous learning to ensure reliability. These organizations
provide a valuable setting to examine practices and challenges for learning
from software failures. Such insight could help develop processes and tools to
improve reliability and prevent recurrence. However, we lack in-depth industry
perspectives on the practices and challenges of learning from failures.
  To address this gap, we conducted a case study through 10 in-depth interviews
with research software engineers at a national space research center. We
examine how they learn from failures: how they gather, document, share, and
apply lessons. To assess transferability, we include data from 5 additional
interviews at other HROs. Our findings provide insight into how engineers learn
from failures in practice. To summarize: (1) failure learning is informal, ad
hoc, and inconsistently integrated into SDLC; (2) recurring failures persist
due to absence of structured processes; and (3) key challenges, including time
constraints, knowledge loss from turnover and fragmented documentation, and
weak process enforcement, undermine systematic learning. Our findings deepen
understanding of how software engineers learn from failures and offer guidance
for improving failure management practices.

</details>


### [460] [A Generic and Efficient Python Runtime Verification System and its Large-scale Evaluation](https://arxiv.org/abs/2509.06324)
*Zhuohang Shen,Mohammed Yaseen,Denini Silva,Kevin Guan,Junho Lee,Marcelo d'Amorim,Owolabi Legunsen*

Main category: cs.SE

TL;DR: The paper introduces PyMOP, an efficient runtime verification (RV) system for Python supporting multiple logics, algorithms, and instrumentation strategies, and demonstrates its efficiency and effectiveness in large-scale testing.


<details>
  <summary>Details</summary>
Motivation: The motivation is to bring scalable runtime verification (RV), which has been successful in the Java ecosystem for finding bugs, to the Python ecosystem, as existing Python RV systems are limited in scope, logic support, or performance.

Method: The authors propose PyMOP, a generic and extensible RV system for Python that supports five logics, five monitoring algorithms, 73 API specs of Python libraries, and three instrumentation strategies. The system is evaluated on over 290,000 unit tests spanning 1,463 GitHub projects.

Result: PyMOP proves to be highly efficient, being up to 1,168.3x faster than other dynamic analysis tools. It also successfully helped find 121 bugs, of which 44 were fixed by developers.

Conclusion: PyMOP demonstrates generality and efficiency, making it a promising platform for future advances in runtime verification for Python applications.

Abstract: Runtime verification (RV) now scales for testing thousands of open-source
Java projects, helping find hundreds of bugs. The popular Python ecosystem
could use such benefits. But, today's Python RV systems are limited to a domain
or specification logic, or slow. We propose PyMOP, a generic, extensible, and
efficient RV system for Python. PyMOP supports five logics, implements five
existing monitoring algorithms, ships with 73 API specs of Python and
widely-used libraries, supports three instrumentation strategies, and users can
easily add more of these. On 290,133 unit tests in 1,463 GitHub projects, we
find mainly that (i) the default monitoring algorithm for Java is often not the
fastest for Python; (ii) PyMOP is up to 1,168.3x faster than two recent dynamic
analysis systems; and (iii) 44 of 121 bugs that PyMOP helped find so far were
fixed by developers. PyMOP's generality and efficiency position it well as an
excellent platform for the next advances on RV for Python.

</details>


### [461] [Analyzing the Instability of Large Language Models in Automated Bug Injection and Correction](https://arxiv.org/abs/2509.06429)
*Mehmet Bilal Er,Nagehan İlhan,Umut Kuran*

Main category: cs.SE

TL;DR: This study investigates the instability of Large Language Models (LLMs), like ChatGPT, in fixing software bugs, revealing that model outputs become notably inconsistent, especially at higher temperature settings.


<details>
  <summary>Details</summary>
Motivation: To assess the stability and reliability of LLMs, particularly in their bug-fixing tasks, which have not been thoroughly studied despite their growing adoption in software engineering.

Method: The study evaluates the variability in fix suggestions from LLMs under different temperature settings (0, 0.5, 1) using 20 problems. Outputs were compared using structural similarity and functional equivalence metrics.

Result: Increased temperature settings result in significant variability and functional failures in outputs. Low temperatures yield more structurally and functionally consistent results.

Conclusion: LLM-based bug-fixing systems are currently unreliable at high variability settings, and this work provides insights into improving their consistency in software development.

Abstract: The use of Large Language Models (LLMs) in software engineering tasks is
growing, especially in the areas of bug fixing and code generation.
Nevertheless, these models often yield unstable results; when executed at
different times with the same input, they can generate radically different
code. The consistency of LLMs in bug-fixing tasks has not yet been thoroughly
assessed, despite the fact that this instability has typically been discussed
in the literature in relation to code generation. The purpose of this study is
to look into how unstable an LLM like ChatGPT is when it comes to fixing code
bugs. We examine the structural, syntactic, and functional variations among
several fix recommendations made in response to the same prompt using code
samples with various error types. Additionally, we assess how instability is
affected by the temperature settings (0, 0.5, and 1) used for the model's
deterministic operation. For a total of 20 problems in the experimental
analysis, the model produced three fix suggestions at each temperature value,
comparing nine distinct outputs for each problem. The Syntax Similarity and
Output Equivalence Rate (OER) metrics were used to assess the outputs'
structural and functional consistency. The results demonstrate that the model's
outputs become much more unstable and variable as the temperature rises, with
high temperatures showing especially high rates of functional failure.
According to syntax similarity analyses, the suggested fixes show notable
structural differences at high temperatures but are fairly similar at low
temperatures. The purpose of this study is to provide important methodological
insights into how LLM-based error correction systems can be applied more
consistently in software development processes while also casting doubt on
their dependability.

</details>


### [462] [Modeling in the Design Multiverse](https://arxiv.org/abs/2509.06530)
*Sylvain Guérin,Salvador Martinez,Ciprian Teodorov*

Main category: cs.SE

TL;DR: The paper introduces the Design Multiverse, a methodology to manage evolving and divergent design paths within the modeling space without relying on external tools.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of managing variability in design paths across time and space directly within the modeling space, eliminating the need for external tools.

Method: The authors propose the Design Multiverse conceptual framework and implement it using the model federation paradigm for managing revisions, variants, and interdependencies in design processes.

Result: The Design Multiverse integrates revisions and variants into the modeling space for stakeholders to trace, analyze, and manage design decisions and system evolution seamlessly.

Conclusion: The Design Multiverse enhances modeling capabilities, facilitating better management of design variability, especially in complex systems.

Abstract: Real-world design processes often involve the evolution and divergence of
design paths (by branching, revising, merging, etc.), especially when multiple
stakeholders or teams operate concurrently and/or explore different
alternatives for complex and heterogeneous systems. Unfortunately, this
variability in time and space can not be directly managed in current modeling
spaces but requires resorting to external tools and methodologies.
  In order to tackle this problem, we introduce the Design Multiverse. The
Design Multiverse aims to integrate in the modeling space a selection of
revisions and variants, representing snapshots of a design state composed of
multiple artifacts. This enables stakeholders to seamlessly trace, analyze, and
manage design decisions, system variants, and their interdependencies.
Concretely, in this paper we present a conceptual definition of the Design
Multiverse, discuss usage scenarios such as model product lines and
model/metamodel co-evolution, and propose an implementation leveraging the
model federation paradigm.

</details>


### [463] [Design and Implementation of a Domain-specific Language for Modelling Evacuation Scenarios Using Eclipse EMG/GMF Tool](https://arxiv.org/abs/2509.06688)
*Heerok Banerjee*

Main category: cs.SE

TL;DR: This paper introduces a DSL called Bmod, designed for modeling evacuation scenarios, created using Eclipse EMF and GMF, and compares it to similar tools.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the lack of user-friendly graphical modeling tools with hierarchical structures to support novice users in building domain-specific languages.

Method: The authors introduce Bmod, which is developed using Eclipse Modelling Framework (EMF) and Eclipse Graphical Modelling Framework (GMF), and perform a comparative analysis with other modeling tools.

Result: Bmod is shown to be an effective tool for modeling evacuation scenarios, with its expressiveness, learning curve, and performance compared favorably to alternatives.

Conclusion: Bmod addresses the identified limitations in existing tools and demonstrates potential in improving business management processes for novice users through its user-friendly framework.

Abstract: Domain-specific languages (DSLs) play a crucial role in resolving internal
dependencies across enterprises and boosts their upfront business management
processes. Yet, a lot of development is needed to build modelling frameworks
which support graphical interfaces (canvas, pallettes etc.), hierarchical
structures and easy implementation to shorten the gap for novice users. In this
paper, a DSL namely, Bmod is introduced, which can be used to model evacuation
scenarios. The language is built using Eclipse Modelling Framework (EMF) and
Eclipse Graphical Modelling Framework (GMF). Furthermore, a comparison is also
shown between Eclipse EMF/GMF and other modelling tools such as AToMPM,
metaDepth, Sirius etc with respect to expressiveness, learning curve and
performance.

</details>


### [464] [OpenCoderRank: AI-Driven Technical Assessments Made Easy](https://arxiv.org/abs/2509.06774)
*Hridoy Sankar Dutta,Sana Ansari,Swati Kumari,Shounak Ravi Bhalerao*

Main category: cs.SE

TL;DR: The paper introduces 'OpenCoderRank,' a platform for simulating technical assessments, catering to the needs of both problem setters and solvers.


<details>
  <summary>Details</summary>
Motivation: To address the dual challenge where Large Language Models assist in creating challenging questions but also compromise assessment integrity by offering solutions.

Method: Developed OpenCoderRank, a customizable, free platform to simulate technical assessments that supports both problem preparation and solving.

Result: The platform allows problem solvers to practice under time constraints and unfamiliar scenarios while enabling setters to host their assessments effectively.

Conclusion: OpenCoderRank serves as an accessible tool that benefits both sides of the assessment process, particularly in environments with limited resources.

Abstract: Organizations and educational institutions use time-bound assessment tasks to
evaluate coding and problem-solving skills. These assessments measure not only
the correctness of the solutions, but also their efficiency. Problem setters
(educator/interviewer) are responsible for crafting these challenges, carefully
balancing difficulty and relevance to create meaningful evaluation experiences.
Conversely, problem solvers (student/interviewee) apply coding efficiency and
logical thinking to arrive at correct solutions. In the era of Large Language
Models (LLMs), LLMs assist problem setters in generating diverse and
challenging questions, but they can undermine assessment integrity for problem
solvers by providing easy access to solutions. This paper introduces
OpenCoderRank, an easy-to-use platform designed to simulate technical
assessments. It acts as a bridge between problem setters and problem solvers,
helping solvers prepare for time constraints and unfamiliar problems while
allowing setters to self-host assessments, offering a no-cost and customizable
solution for technical assessments in resource-constrained environments.

</details>


### [465] [Hypergraph-Guided Regex Filter Synthesis for Event-Based Anomaly Detection](https://arxiv.org/abs/2509.06911)
*Margarida Ferreira,Victor Nicolet,Luan Pham,Joey Dodds,Daniel Kroening,Ines Lynce,Ruben Martins*

Main category: cs.SE

TL;DR: HyGLAD is a new algorithm that creates patterns for anomaly detection in event data, emphasizing interpretability and efficiency.


<details>
  <summary>Details</summary>
Motivation: The need to model event data for anomaly detection in stationary systems where deviations could indicate malicious activities.

Method: The method involves inferring equivalence classes of entities with similar behavior to create interpretable regular expressions for anomaly detection.

Result: HyGLAD outperformed seven deep-learning methods on five real-world datasets in both accuracy (1.2x precision, 1.3x recall) and efficiency (single CPU vs GPU).

Conclusion: HyGLAD offers a more interpretable and computationally efficient alternative to deep-learning approaches for anomaly detection in event data.

Abstract: We propose HyGLAD, a novel algorithm that automatically builds a set of
interpretable patterns that model event data. These patterns can then be used
to detect event-based anomalies in a stationary system, where any deviation
from past behavior may indicate malicious activity. The algorithm infers
equivalence classes of entities with similar behavior observed from the events,
and then builds regular expressions that capture the values of those entities.
As opposed to deep-learning approaches, the regular expressions are directly
interpretable, which also translates to interpretable anomalies. We evaluate
HyGLAD against all 7 unsupervised anomaly detection methods from DeepOD on five
datasets from real-world systems. The experimental results show that on average
HyGLAD outperforms existing deep-learning methods while being an order of
magnitude more efficient in training and inference (single CPU vs GPU).
Precision improved by 1.2x and recall by 1.3x compared to the second-best
baseline.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [466] [Predicting Brain Morphogenesis via Physics-Transfer Learning](https://arxiv.org/abs/2509.05305)
*Yingjie Zhao,Yicheng Song,Fan Xu,Zhiping Xu*

Main category: q-bio.NC

TL;DR: A physics-transfer learning framework uses principles from nonlinear elasticity to predict brain growth and diseases, addressing complex geometric challenges and limited data.


<details>
  <summary>Details</summary>
Motivation: The paper aims to better understand brain morphology, which is shaped by genetics and mechanics and is critical in biological development and medical diagnostics.

Method: The researchers developed a physics-transfer learning framework leveraging elastic instability and bifurcation physics to train a neural network and provided a library of continuum mechanics modeling.

Result: The framework achieved notable performance in characterizing features, predicting morphogenesis, and offering reduced-dimensional evolutionary representations of the brain.

Conclusion: Digital-twin technology embedded with physics-driven insights can shape future studies of brain morphology, aiding both biological and medical advancements.

Abstract: Brain morphology is shaped by genetic and mechanical factors and is linked to
biological development and diseases. Its fractal-like features, regional
anisotropy, and complex curvature distributions hinder quantitative insights in
medical inspections. Recognizing that the underlying elastic instability and
bifurcation share the same physics as simple geometries such as spheres and
ellipses, we developed a physics-transfer learning framework to address the
geometrical complexity. To overcome the challenge of data scarcity, we
constructed a digital library of high-fidelity continuum mechanics modeling
that both describes and predicts the developmental processes of brain growth
and disease. The physics of nonlinear elasticity from simple geometries is
embedded into a neural network and applied to brain models. This
physics-transfer approach demonstrates remarkable performance in feature
characterization and morphogenesis prediction, highlighting the pivotal role of
localized deformation in dominating over the background geometry. The
data-driven framework also provides a library of reduced-dimensional
evolutionary representations that capture the essential physics of the highly
folded cerebral cortex. Validation through medical images and domain expertise
underscores the deployment of digital-twin technology in comprehending the
morphological complexity of the brain.

</details>


### [467] [Musculoskeletal simulation of limb movement biomechanics in Drosophila melanogaster](https://arxiv.org/abs/2509.06426)
*Pembe Gizem Özdil,Chuanfang Ning,Jasper S. Phelps,Sibo Wang-Chen,Guy Elisha,Alexander Blanke,Auke Ijspeert,Pavan Ramdya*

Main category: q-bio.NC

TL;DR: The paper introduces the first 3D musculoskeletal model of Drosophila legs, combining biomechanical accuracy with simulation environments to study motor control and biomechanics.


<details>
  <summary>Details</summary>
Motivation: Understanding how neural, biomechanical, and physical systems interact to control animal behavior is crucial. Despite near-complete CNS and anatomical reconstructions of Drosophila, musculoskeletal models bridging motor neuron activity and joint movement have been lacking.

Method: The study presents a 3D musculoskeletal model created using X-ray imaging and anatomical data, simulated in OpenSim and MuJoCo. They include Hill-type muscle representations and pipelines for parameter optimization, pose data generation, and imitation learning policy training.

Result: The model predicts muscle synergies during various behaviors, shows how joint properties affect learning, and enables muscle-actuated behavioral replay. This provides a means to test experimental predictions on motor control.

Conclusion: The proposed model bridges neuro-musculoskeletal gaps, enhancing understanding of motor control and biomechanics in Drosophila. It also facilitates naturalistic locomotion in artificial agents and offers tools for experimental testing.

Abstract: Computational models are critical to advance our understanding of how neural,
biomechanical, and physical systems interact to orchestrate animal behaviors.
Despite the availability of near-complete reconstructions of the Drosophila
melanogaster central nervous system, musculature, and exoskeleton, anatomically
and physically grounded models of fly leg muscles are still missing. These
models provide an indispensable bridge between motor neuron activity and joint
movements. Here, we introduce the first 3D, data-driven musculoskeletal model
of Drosophila legs, implemented in both OpenSim and MuJoCo simulation
environments. Our model incorporates a Hill-type muscle representation based on
high-resolution X-ray scans from multiple fixed specimens. We present a
pipeline for constructing muscle models using morphological imaging data and
for optimizing unknown muscle parameters specific to the fly. We then combine
our musculoskeletal models with detailed 3D pose estimation data from behaving
flies to achieve muscle-actuated behavioral replay in OpenSim. Simulations of
muscle activity across diverse walking and grooming behaviors predict
coordinated muscle synergies that can be tested experimentally. Furthermore, by
training imitation learning policies in MuJoCo, we test the effect of different
passive joint properties on learning speed and find that damping and stiffness
facilitate learning. Overall, our model enables the investigation of motor
control in an experimentally tractable model organism, providing insights into
how biomechanics contribute to generation of complex limb movements. Moreover,
our model can be used to control embodied artificial agents to generate
naturalistic and compliant locomotion in simulated environments.

</details>


### [468] [Reward function compression facilitates goal-dependent reinforcement learning](https://arxiv.org/abs/2509.06810)
*Gaia Molinaro,Anne G. E. Collins*

Main category: q-bio.NC

TL;DR: This paper explores how humans learn abstract goals more efficiently by compressing goal information into stable reward functions, freeing cognitive resources.


<details>
  <summary>Details</summary>
Motivation: Understanding how humans assign value to abstract goals and why this process is cognitively taxing compared to traditional reinforcement learning mechanisms.

Method: Six experiments were conducted, complemented by computational modeling, to explore goal space complexity and reward processing in relation to learning efficiency.

Result: Findings show that learning efficiency is impaired by larger goal spaces but improves when goal structures allow for compression. Faster reward processing correlates with better learning outcomes.

Conclusion: Human goal-directed learning is optimized by compressing goal-related information, activating intrinsic motivation mechanisms and providing insights to enhance behavioral support approaches.

Abstract: Reinforcement learning agents learn from rewards, but humans can uniquely
assign value to novel, abstract outcomes in a goal-dependent manner. However,
this flexibility is cognitively costly, making learning less efficient. Here,
we propose that goal-dependent learning is initially supported by a
capacity-limited working memory system. With consistent experience, learners
create a "compressed" reward function (a simplified rule defining the goal)
which is then transferred to long-term memory and applied automatically upon
receiving feedback. This process frees up working memory resources, boosting
learning efficiency. We test this theory across six experiments. Consistent
with our predictions, our findings demonstrate that learning is parametrically
impaired by the size of the goal space, but improves when the goal space
structure allows for compression. We also find faster reward processing to
correlate with better learning performance, supporting the idea that as goal
valuation becomes more automatic, more resources are available for learning. We
leverage computational modeling to support this interpretation. Our work
suggests that efficient goal-directed learning relies on compressing complex
goal information into a stable reward function, shedding light on the cognitive
mechanisms of human motivation. These findings generate new insights into the
neuroscience of intrinsic motivation and could help improve behavioral
techniques that support people in achieving their goals.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [469] [Cryo-EM as a Stochastic Inverse Problem](https://arxiv.org/abs/2509.05541)
*Diego Sanchez Espinosa,Erik H Thiede,Yunan Yang*

Main category: stat.ML

TL;DR: This paper proposes a novel stochastic approach to resolve structural variability challenges in cryo-EM reconstruction by using a variational discrepancy minimization framework.


<details>
  <summary>Details</summary>
Motivation: Structural heterogeneity in cryo-EM imaging limits conventional 3D reconstruction methods, necessitating a new approach to handle continuous structural variations.

Method: The authors formulate cryo-EM reconstruction as a stochastic inverse problem, utilizing statistical distances like KL divergence and Maximum Mean Discrepancy, and solve it via Wasserstein gradient flow and particle-based methods.

Result: The approach successfully recovers continuous distributions of molecular structures in synthetic scenarios, including a realistic protein model, and underscores links to Maximum A Posteriori (MAP) methods.

Conclusion: This work introduces a powerful framework for addressing continuous variability in cryo-EM and offers a generalized method for stochastic inverse problems with random forward operators.

Abstract: Cryo-electron microscopy (Cryo-EM) enables high-resolution imaging of
biomolecules, but structural heterogeneity remains a major challenge in 3D
reconstruction. Traditional methods assume a discrete set of conformations,
limiting their ability to recover continuous structural variability. In this
work, we formulate cryo-EM reconstruction as a stochastic inverse problem (SIP)
over probability measures, where the observed images are modeled as the
push-forward of an unknown distribution over molecular structures via a random
forward operator. We pose the reconstruction problem as the minimization of a
variational discrepancy between observed and simulated image distributions,
using statistical distances such as the KL divergence and the Maximum Mean
Discrepancy. The resulting optimization is performed over the space of
probability measures via a Wasserstein gradient flow, which we numerically
solve using particles to represent and evolve conformational ensembles. We
validate our approach using synthetic examples, including a realistic protein
model, which demonstrates its ability to recover continuous distributions over
structural states. We analyze the connection between our formulation and
Maximum A Posteriori (MAP) approaches, which can be interpreted as instances of
the discretize-then-optimize (DTO) framework. We further provide a consistency
analysis, establishing conditions under which DTO methods, such as MAP
estimation, converge to the solution of the underlying infinite-dimensional
continuous problem. Beyond cryo-EM, the framework provides a general
methodology for solving SIPs involving random forward operators.

</details>


### [470] [Robust variational neural posterior estimation for simulation-based inference](https://arxiv.org/abs/2509.05724)
*Matthew O'Callaghan,Kaisey S. Mandel,Gerry Gilmore*

Main category: stat.ML

TL;DR: The paper introduces RVNP to address model misspecification issues in simulation-based inference (SBI), ensuring robust posterior estimation.


<details>
  <summary>Details</summary>
Motivation: To solve the challenge of model misspecification in SBI, which hinders accurate posterior inference in real-world problems.

Method: Developed RVNP, which uses variational inference and error modeling to bridge the simulation-to-reality gap without reliance on hyperparameters or priors.

Result: Tests on benchmark tasks, including astronomy data, showed RVNP could recover robust posterior inference in a data-driven way.

Conclusion: RVNP provides a robust solution to SBI model misspecification, improving its real-world applicability.

Abstract: Recent advances in neural density estimation have enabled powerful
simulation-based inference (SBI) methods that can flexibly approximate Bayesian
inference for intractable stochastic models. Although these methods have
demonstrated reliable posterior estimation when the simulator accurately
represents the underlying data generative process (GDP), recent work has shown
that they perform poorly in the presence of model misspecification. This poses
a significant problem for their use on real-world problems, due to simulators
always misrepresenting the true DGP to a certain degree. In this paper, we
introduce robust variational neural posterior estimation (RVNP), a method which
addresses the problem of misspecification in amortised SBI by bridging the
simulation-to-reality gap using variational inference and error modelling. We
test RVNP on multiple benchmark tasks, including using real data from
astronomy, and show that it can recover robust posterior inference in a
data-driven manner without adopting tunable hyperparameters or priors governing
the misspecification.

</details>


### [471] [Risk-averse Fair Multi-class Classification](https://arxiv.org/abs/2509.05771)
*Darinka Dentcheva,Xiangyu Tian*

Main category: stat.ML

TL;DR: The paper develops a new classification framework using systemic risk models for better handling noisy, scarce, and unreliable data in multi-class problems, emphasizing fairness and robust performance.


<details>
  <summary>Details</summary>
Motivation: Existing classification frameworks struggle with unreliable labels, high-dimensional problems, and the need for fairness in multi-class scenarios.

Method: The approach applies systemic risk measures in classification using linear and kernel-based methods, incorporates non-linear aggregation, and employs a risk-averse regularized decomposition to solve the problem.

Result: Numerical experiments show robustness to unreliable data, better performance on unknown data, and improved outcomes as the number of classes increases.

Conclusion: The proposed framework is theoretically and empirically viable, addresses fairness in classification, and outperforms traditional methods under challenging conditions.

Abstract: We develop a new classification framework based on the theory of coherent
risk measures and systemic risk. The proposed approach is suitable for
multi-class problems when the data is noisy, scarce (relative to the dimension
of the problem), and the labeling might be unreliable. In the first part of our
paper, we provide the foundation of the use of systemic risk models and show
how to apply it in the context of linear and kernel-based multi-class problems.
More advanced formulation via a system-theoretic approach with non-linear
aggregation is proposed, which leads to a two-stage stochastic programming
problem. A risk-averse regularized decomposition method is designed to solve
the problem. We use a popular multi-class method as a benchmark in the
performance analysis of the proposed classification methods. We illustrate our
ideas by proposing several generalization of that method by the use of coherent
measures of risk. The viability of the proposed risk-averse methods are
supported theoretically and numerically. Additionally, we demonstrate that the
application of systemic risk measures facilitates enforcing fairness in
classification. Analysis and experiments regarding the fairness of the proposed
models are carefully conducted. For all methods, our numerical experiments
demonstrate that they are robust in the presence of unreliable training data
and perform better on unknown data than the methods minimizing expected
classification errors. Furthermore, the performance improves when the number of
classes increases.

</details>


### [472] [Causal Clustering for Conditional Average Treatment Effects Estimation and Subgroup Discovery](https://arxiv.org/abs/2509.05775)
*Zilong Wang,Turgay Ayer,Shihao Yang*

Main category: stat.ML

TL;DR: The paper proposes a method for clustering individuals based on estimated treatment effects, enabling identification of treatment-sensitive subpopulations.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of identifying subpopulations with differing responses to interventions to improve targeted decision-making.

Method: The approach involves estimating Conditional Average Treatment Effects using debiased learners, deriving a kernel matrix, and applying kernelized clustering to uncover subpopulation structures.

Result: The proposed method successfully captures treatment effect heterogeneity across various datasets, validated through experiments, ablation studies, and exploratory analyses.

Conclusion: The framework reveals meaningful latent subgroup structures and promotes effective personalized intervention strategies.

Abstract: Estimating heterogeneous treatment effects is critical in domains such as
personalized medicine, resource allocation, and policy evaluation. A central
challenge lies in identifying subpopulations that respond differently to
interventions, thereby enabling more targeted and effective decision-making.
While clustering methods are well-studied in unsupervised learning, their
integration with causal inference remains limited. We propose a novel framework
that clusters individuals based on estimated treatment effects using a learned
kernel derived from causal forests, revealing latent subgroup structures. Our
approach consists of two main steps. First, we estimate debiased Conditional
Average Treatment Effects (CATEs) using orthogonalized learners via the
Robinson decomposition, yielding a kernel matrix that encodes sample-level
similarities in treatment responsiveness. Second, we apply kernelized
clustering to this matrix to uncover distinct, treatment-sensitive
subpopulations and compute cluster-level average CATEs. We present this
kernelized clustering step as a form of regularization within the
residual-on-residual regression framework. Through extensive experiments on
semi-synthetic and real-world datasets, supported by ablation studies and
exploratory analyses, we demonstrate the effectiveness of our method in
capturing meaningful treatment effect heterogeneity.

</details>


### [473] [Fisher Random Walk: Automatic Debiasing Contextual Preference Inference for Large Language Model Evaluation](https://arxiv.org/abs/2509.05852)
*Yichi Zhang,Alexander Belloni,Ethan X. Fang,Junwei Lu,Xiaoan Xu*

Main category: stat.ML

TL;DR: The paper focuses on contextual preference inference for pairwise comparisons using a novel semiparametric estimator, optimizing accuracy and scalability in large language model evaluations through advanced computational strategies.


<details>
  <summary>Details</summary>
Motivation: To develop rigorous and scalable approaches for evaluating large language models, addressing challenges in pairwise comparisons across diverse domains.

Method: Introduces a semiparametric efficient estimator leveraging weighted residual balancing and Fisher random walk-derived weights, alongside computational strategies like nuisance weight functions and cross-fitted importance-sampling for adjustments.

Result: The method is shown to achieve accurate and efficient inference, valid across general score functions and flexible modeling approaches, with utility confirmed via numerical studies.

Conclusion: The proposed approach offers a robust, scalable, and adaptable framework for contextual preference inference, enhancing evaluation practices for language models and other applications.

Abstract: Motivated by the need for rigorous and scalable evaluation of large language
models, we study contextual preference inference for pairwise comparison
functionals of context-dependent preference score functions across domains.
Focusing on the contextual Bradley-Terry-Luce model, we develop a
semiparametric efficient estimator that automates the debiased estimation
through aggregating weighted residual balancing terms across the comparison
graph. We show that the efficiency is achieved when the weights are derived
from a novel strategy called Fisher random walk. We also propose a
computationally feasible method to compute the weights by a potential
representation of nuisance weight functions. We show our inference procedure is
valid for general score function estimators accommodating the practitioners'
need to implement flexible deep learning methods. We extend the procedure to
multiple hypothesis testing using a Gaussian multiplier bootstrap that controls
familywise error and to distributional shift via a cross-fitted
importance-sampling adjustment for target-domain inference. Numerical studies,
including language model evaluations under diverse contexts, corroborate the
accuracy, efficiency, and practical utility of our method.

</details>


### [474] [Uncertainty Quantification in Probabilistic Machine Learning Models: Theory, Methods, and Insights](https://arxiv.org/abs/2509.05877)
*Marzieh Ajirak,Anand Ravishankar,Petar M. Djuric*

Main category: stat.ML

TL;DR: The paper focuses on improving uncertainty quantification (UQ) in probabilistic machine learning using Gaussian Process Latent Variable Models and scalable methods.


<details>
  <summary>Details</summary>
Motivation: To provide a framework to accurately assess both epistemic and aleatoric uncertainties in probabilistic models, enhancing prediction reliability.

Method: The paper uses Gaussian Process Latent Variable Models, scalable Random Fourier Features-based Gaussian Processes, a theoretical formulation for UQ, and a Monte Carlo sampling-based estimation to quantify uncertainty.

Result: Experiments showcase the sources of predictive uncertainty and validate the effectiveness of the proposed framework in confidence estimation.

Conclusion: The approach offers a systematic and scalable method for UQ, providing insights and confidence in probabilistic model predictions.

Abstract: Uncertainty Quantification (UQ) is essential in probabilistic machine
learning models, particularly for assessing the reliability of predictions. In
this paper, we present a systematic framework for estimating both epistemic and
aleatoric uncertainty in probabilistic models. We focus on Gaussian Process
Latent Variable Models and employ scalable Random Fourier Features-based
Gaussian Processes to approximate predictive distributions efficiently. We
derive a theoretical formulation for UQ, propose a Monte Carlo sampling-based
estimation method, and conduct experiments to evaluate the impact of
uncertainty estimation. Our results provide insights into the sources of
predictive uncertainty and illustrate the effectiveness of our approach in
quantifying the confidence in the predictions.

</details>


### [475] [Additive Distributionally Robust Ranking and Selection](https://arxiv.org/abs/2509.06147)
*Zaile Li,Yuchen Wan,L. Jeff Hong*

Main category: stat.ML

TL;DR: This paper addresses the challenge of ranking and selection (R&S) under input uncertainty with a proposed additive allocation (AA) method, demonstrating its consistency and theoretically grounded efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the efficiency and accuracy of Distributionally Robust Ranking and Selection (DRR&S) methods in the presence of input uncertainty caused by limited data, a common practical issue.

Method: The authors propose an additive allocation (AA) procedure that focuses sampling on critical scenarios, using boundary-crossing arguments to establish correctness and behavior. They also introduce a General Additive Allocation (GAA) framework for practical adaptability.

Result: The AA procedure proves to be consistent and efficiently allocates sampling to only $k + m - 1$ critical scenarios as the total budget increases. This challenges previously held assumptions about scenario criticality. The GAA framework performs well in numerical experiments.

Conclusion: The paper provides new insights into additive structures in DRR&S, showcasing the theoretical and practical advantage of sampling only critical scenarios for efficient R&S in uncertain contexts.

Abstract: Ranking and selection (R&S) aims to identify the alternative with the best
mean performance among $k$ simulated alternatives. The practical value of R&S
depends on accurate simulation input modeling, which often suffers from the
curse of input uncertainty due to limited data. Distributionally robust ranking
and selection (DRR&S) addresses this challenge by modeling input uncertainty
via an ambiguity set of $m > 1$ plausible input distributions, resulting in
$km$ scenarios in total. Recent DRR&S studies suggest a key structural insight:
additivity in budget allocation is essential for efficiency. However, existing
justifications are heuristic, and fundamental properties such as consistency
and the precise allocation pattern induced by additivity remain poorly
understood. In this paper, we propose a simple additive allocation (AA)
procedure that aims to exclusively sample the $k + m - 1$ previously
hypothesized critical scenarios. Leveraging boundary-crossing arguments, we
establish a lower bound on the probability of correct selection and
characterize the procedure's budget allocation behavior. We then prove that AA
is consistent and, surprisingly, achieves additivity in the strongest sense: as
the total budget increases, only $k + m - 1$ scenarios are sampled infinitely
often. Notably, the worst-case scenarios of non-best alternatives may not be
among them, challenging prior beliefs about their criticality. These results
offer new and counterintuitive insights into the additive structure of DRR&S.
To improve practical performance while preserving this structure, we introduce
a general additive allocation (GAA) framework that flexibly incorporates
sampling rules from traditional R&S procedures in a modular fashion. Numerical
experiments support our theoretical findings and demonstrate the competitive
performance of the proposed GAA procedures.

</details>


### [476] [MOSAIC: Minimax-Optimal Sparsity-Adaptive Inference for Change Points in Dynamic Networks](https://arxiv.org/abs/2509.06303)
*Yingying Fan,Jingyuan Liu,Jinchi Lv,Ao Sun*

Main category: stat.ML

TL;DR: MOSAIC is a new inference framework designed for change-point detection in dynamic networks, incorporating low-rank and sparse change structures. It achieves theoretical optimality in detection rates and practical robustness via residual-based techniques.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in detecting change-points in dynamic networks, specifically with low-rank and sparse-change structures, which are crucial in understanding transitions or anomalies in network dynamics.

Method: MOSAIC employs eigen-decomposition-based testing combined with screened signals to approach theoretical minimax detection rates. It further uses a residual-based adjustment to make the test statistic conform to standard normal distribution for practical application.

Result: MOSAIC achieves full theoretical power under the alternative hypothesis and validates its framework and theoretical results with simulations and real data applications.

Conclusion: The proposed MOSAIC framework effectively balances theoretical rigor and practical utility, making it a robust tool for change-point inference in dynamic networks with structured sparsity.

Abstract: We propose a new inference framework, named MOSAIC, for change-point
detection in dynamic networks with the simultaneous low-rank and sparse-change
structure. We establish the minimax rate of detection boundary, which relies on
the sparsity of changes. We then develop an eigen-decomposition-based test with
screened signals that approaches the minimax rate in theory, with only a minor
logarithmic loss. For practical implementation of MOSAIC, we adjust the
theoretical test by a novel residual-based technique, resulting in a pivotal
statistic that converges to a standard normal distribution via the martingale
central limit theorem under the null hypothesis and achieves full power under
the alternative hypothesis. We also analyze the minimax rate of testing
boundary for dynamic networks without the low-rank structure, which almost
aligns with the results in high-dimensional mean-vector change-point inference.
We showcase the effectiveness of MOSAIC and verify our theoretical results with
several simulation examples and a real data application.

</details>


### [477] [Minimax optimal transfer learning for high-dimensional additive regression](https://arxiv.org/abs/2509.06308)
*Seung Hyun Moon*

Main category: stat.ML

TL;DR: This paper develops methods for high-dimensional additive regression under a transfer learning framework, introducing novel estimators and deriving error bounds for heavy-tailed noise and transfer learning cases.


<details>
  <summary>Details</summary>
Motivation: To address challenges in high-dimensional additive regression, especially under transfer learning scenarios with heavy-tailed errors and related populations.

Method: The paper proposes a target-only estimation procedure using smooth backfitting with local linear smoothing and a two-stage estimation method in the transfer learning framework, accompanied by theoretical guarantees and error analyses.

Result: Error bounds are derived for both methods under sub-Weibull noise conditions, and minimax optimal rates are achieved when auxiliary and target distributions are sufficiently related.

Conclusion: The results advance the robustness and theoretical understanding of additive regression, offering practical insights through simulations and real data analyses.

Abstract: This paper studies high-dimensional additive regression under the transfer
learning framework, where one observes samples from a target population
together with auxiliary samples from different but potentially related
regression models. We first introduce a target-only estimation procedure based
on the smooth backfitting estimator with local linear smoothing. In contrast to
previous work, we establish general error bounds under sub-Weibull($\alpha$)
noise, thereby accommodating heavy-tailed error distributions. In the
sub-exponential case ($\alpha=1$), we show that the estimator attains the
minimax lower bound under regularity conditions, which requires a substantial
departure from existing proof strategies. We then develop a novel two-stage
estimation method within a transfer learning framework, and provide theoretical
guarantees at both the population and empirical levels. Error bounds are
derived for each stage under general tail conditions, and we further
demonstrate that the minimax optimal rate is achieved when the auxiliary and
target distributions are sufficiently close. All theoretical results are
supported by simulation studies and real data analysis.

</details>


### [478] [Robust and Adaptive Spectral Method for Representation Multi-Task Learning with Contamination](https://arxiv.org/abs/2509.06575)
*Yian Huang,Yang Feng,Zhiliang Ying*

Main category: stat.ML

TL;DR: This paper presents RAS (Robust and Adaptive Spectral method) for robust multi-task learning, excelling under high task contamination up to 80%.


<details>
  <summary>Details</summary>
Motivation: Address the challenges in representation-based MTL posed by contamination, outliers, and adversarial tasks, which current methods fail to handle effectively.

Method: Developed RAS, capable of distilling shared inlier representations without prior knowledge about contamination levels or the representation dimension, supported by theoretical error bounds.

Result: RAS performs robust learning under significant contamination, adapts to inlier-outlier structures, provides guarantees to avoid negative transfer, and supports transfer learning with proven efficiency.

Conclusion: RAS extends MTL applicability with its resilience to high contamination, demonstrating theoretical and empirical superiority over existing methods, ensuring robust and adaptable learning.

Abstract: Representation-based multi-task learning (MTL) improves efficiency by
learning a shared structure across tasks, but its practical application is
often hindered by contamination, outliers, or adversarial tasks. Most existing
methods and theories assume a clean or near-clean setting, failing when
contamination is significant. This paper tackles representation MTL with an
unknown and potentially large contamination proportion, while also allowing for
heterogeneity among inlier tasks. We introduce a Robust and Adaptive Spectral
method (RAS) that can distill the shared inlier representation effectively and
efficiently, while requiring no prior knowledge of the contamination level or
the true representation dimension. Theoretically, we provide non-asymptotic
error bounds for both the learned representation and the per-task parameters.
These bounds adapt to inlier task similarity and outlier structure, and
guarantee that RAS performs at least as well as single-task learning, thus
preventing negative transfer. We also extend our framework to transfer learning
with corresponding theoretical guarantees for the target task. Extensive
experiments confirm our theory, showcasing the robustness and adaptivity of
RAS, and its superior performance in regimes with up to 80\% task
contamination.

</details>


### [479] [Automated Hierarchical Graph Construction for Multi-source Electronic Health Records](https://arxiv.org/abs/2509.06576)
*Yinjie Wang,Doudou Zhou,Yue Liu,Junwei Lu,Tianxi Cai*

Main category: stat.ML

TL;DR: MASH is an automated framework for harmonizing and structuring diverse EHR data across institutions using advanced neural and hierarchical modeling methods.


<details>
  <summary>Details</summary>
Motivation: EHR data remain underutilized due to complexity and heterogeneity in medical codes and terminologies, hindering large-scale and generalizable studies.

Method: MASH aligns medical codes using neural optimal transport and constructs hierarchical graphs through hyperbolic embeddings, integrating multi-modal information during training.

Result: MASH successfully generates hierarchical graphs for diagnosis, medication, and laboratory codes, improving interpretability and creating foundational references for local laboratory codes.

Conclusion: MASH enables better navigation and comprehension of heterogeneous EHR data, paving the way for enhanced data harmonization and downstream clinical research applications.

Abstract: Electronic Health Records (EHRs), comprising diverse clinical data such as
diagnoses, medications, and laboratory results, hold great promise for
translational research. EHR-derived data have advanced disease prevention,
improved clinical trial recruitment, and generated real-world evidence.
Synthesizing EHRs across institutions enables large-scale, generalizable
studies that capture rare diseases and population diversity, but remains
hindered by the heterogeneity of medical codes, institution-specific
terminologies, and the absence of standardized data structures. These barriers
limit the interpretability, comparability, and scalability of EHR-based
analyses, underscoring the need for robust methods to harmonize and extract
meaningful insights from distributed, heterogeneous data. To address this, we
propose MASH (Multi-source Automated Structured Hierarchy), a fully automated
framework that aligns medical codes across institutions using neural optimal
transport and constructs hierarchical graphs with learned hyperbolic
embeddings. During training, MASH integrates information from pre-trained
language models, co-occurrence patterns, textual descriptions, and supervised
labels to capture semantic and hierarchical relationships among medical
concepts more effectively. Applied to real-world EHR data, including diagnosis,
medication, and laboratory codes, MASH produces interpretable hierarchical
graphs that facilitate the navigation and understanding of heterogeneous
clinical data. Notably, it generates the first automated hierarchies for
unstructured local laboratory codes, establishing foundational references for
downstream applications.

</details>


### [480] [Sequential Least-Squares Estimators with Fast Randomized Sketching for Linear Statistical Models](https://arxiv.org/abs/2509.06856)
*Guan-Yu Chen,Xi Yang*

Main category: stat.ML

TL;DR: The paper introduces SLSE-FRS, a method for solving large-scale linear statistical models using iterative randomized sketching for improved estimation precision.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in estimating parameters for large-scale linear statistical models by combining and improving existing Sketch-and-Solve and Iterative-Sketching methods.

Method: The proposed method iteratively constructs sketched LS subproblems with progressively increasing sketch sizes to refine estimations, providing efficient implementation and convergence analysis.

Result: Experimental results demonstrate that SLSE-FRS outperforms existing methods, such as PCG and IDS, in terms of accuracy and computational efficiency.

Conclusion: SLSE-FRS successfully integrates two sketching approaches to provide an accurate and efficient estimator for large-scale linear statistical models, setting a new benchmark in the field.

Abstract: We propose a novel randomized framework for the estimation problem of
large-scale linear statistical models, namely Sequential Least-Squares
Estimators with Fast Randomized Sketching (SLSE-FRS), which integrates
Sketch-and-Solve and Iterative-Sketching methods for the first time. By
iteratively constructing and solving sketched least-squares (LS) subproblems
with increasing sketch sizes to achieve better precisions, SLSE-FRS gradually
refines the estimators of the true parameter vector, ultimately producing
high-precision estimators. We analyze the convergence properties of SLSE-FRS,
and provide its efficient implementation. Numerical experiments show that
SLSE-FRS outperforms the state-of-the-art methods, namely the Preconditioned
Conjugate Gradient (PCG) method, and the Iterative Double Sketching (IDS)
method.

</details>


### [481] [Learning from one graph: transductive learning guarantees via the geometry of small random worlds](https://arxiv.org/abs/2509.06894)
*Nils Detering,Luca Galimberti,Anastasis Kratsios,Giulia Livieri,A. Martina Neuman*

Main category: stat.ML

TL;DR: This paper develops new statistical tools for transductive node classification in graph convolutional networks, leveraging geometric regularities via low-dimensional embeddings.


<details>
  <summary>Details</summary>
Motivation: There is a gap in statistical foundations for transductive learning in graph networks, as prior inference frameworks typically depend on multiple samples instead of a single graph.

Method: The authors introduce concentration-of-measure tools using random and deterministic graph models, focusing on geometric regularities in graph structures.

Result: Two main results are highlighted: statistical learning guarantees for arbitrary deterministic graphs and random graphs with properties similar to an Erdős–Rényi graph in specific regimes. The framework is extended to graph convolutional networks with optimal learning rates achieved even for few labeled nodes.

Conclusion: This work advances the understanding of transductive learning in graph networks and provides guarantees for effective learning under realistic conditions.

Abstract: Since their introduction by Kipf and Welling in $2017$, a primary use of
graph convolutional networks is transductive node classification, where missing
labels are inferred within a single observed graph and its feature matrix.
Despite the widespread use of the network model, the statistical foundations of
transductive learning remain limited, as standard inference frameworks
typically rely on multiple independent samples rather than a single graph. In
this work, we address these gaps by developing new concentration-of-measure
tools that leverage the geometric regularities of large graphs via
low-dimensional metric embeddings. The emergent regularities are captured using
a random graph model; however, the methods remain applicable to deterministic
graphs once observed. We establish two principal learning results. The first
concerns arbitrary deterministic $k$-vertex graphs, and the second addresses
random graphs that share key geometric properties with an Erd\H{o}s-R\'{e}nyi
graph $\mathbf{G}=\mathbf{G}(k,p)$ in the regime $p \in \mathcal{O}((\log
(k)/k)^{1/2})$. The first result serves as the basis for and illuminates the
second. We then extend these results to the graph convolutional network
setting, where additional challenges arise. Lastly, our learning guarantees
remain informative even with a few labelled nodes $N$ and achieve the optimal
nonparametric rate $\mathcal{O}(N^{-1/2})$ as $N$ grows.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [482] [Interpretable dimension reduction for compositional data](https://arxiv.org/abs/2509.05563)
*Junyoung Park,Cheolwoo Park,Jeongyoun Ahn*

Main category: stat.ME

TL;DR: The paper presents a new framework for dimension reduction in high-dimensional compositional data, avoiding traditional transformation issues and enabling direct visualization and interpretation.


<details>
  <summary>Details</summary>
Motivation: Conventional methods for analyzing high-dimensional compositional data often compromise interpretability and require ad hoc zero imputations, creating the need for a more robust and interpretable approach.

Method: The authors introduce a framework based on softening amalgamation and propose a compositional kernel dimension reduction (CKDR) method, which avoids extra transformations and provides visualization tools for intuitive interpretation.

Result: The CKDR method is proven to be consistent, sparse, and provides predictive capabilities. Applications to microbiome datasets show its ability to uncover meaningful biological patterns visually.

Conclusion: The proposed framework and CKDR open new possibilities for analyzing and interpreting high-dimensional compositional data, providing a more reliable and intuitive alternative to existing methods.

Abstract: High-dimensional compositional data, such as those from human microbiome
studies, pose unique statistical challenges due to the simplex constraint and
excess zeros. While dimension reduction is indispensable for analyzing such
data, conventional approaches often rely on log-ratio transformations that
compromise interpretability and distort the data through ad hoc zero
replacements. We introduce a novel framework for interpretable dimension
reduction of compositional data that avoids extra transformations and zero
imputations. Our approach generalizes the concept of amalgamation by softening
its operation, mapping high-dimensional compositions directly to a
lower-dimensional simplex, which can be visualized in ternary plots. The
framework further provides joint visualization of the reduction matrix,
enabling intuitive, at-a-glance interpretation. To achieve optimal reduction
within our framework, we incorporate sufficient dimension reduction, which
defines a new identifiable objective: the central compositional subspace. For
estimation, we propose a compositional kernel dimension reduction (CKDR)
method. The estimator is provably consistent, exhibits sparsity that reveals
underlying amalgamation structures, and comes with an intrinsic predictive
model for downstream analyses. Applications to real microbiome datasets
demonstrate that our approach provides a powerful graphical exploration tool
for uncovering meaningful biological patterns, opening a new pathway for
analyzing high-dimensional compositional data.

</details>


### [483] [Beyond ATE: Multi-Criteria Design for A/B Testing](https://arxiv.org/abs/2509.05864)
*Jiachun Li,Kaining Shi,David Simchi-Levi*

Main category: stat.ME

TL;DR: The paper explores adaptive experimentation to balance statistical accuracy and social welfare loss, presenting Pareto-optimal designs for optimal treatment allocation and integrating privacy through differential algorithms.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges in balancing estimation accuracy with welfare/revenue considerations in adaptive experiments, particularly for treatments that improve patient outcomes or maximize revenue.

Method: Using a machine learning approach, the paper introduces Pareto-optimal designs to tackle the trade-offs in adaptive experiments, and develops differentially private algorithms to ensure privacy with minimal cost.

Result: The study provides upper and lower bounds for multi-objective optimization, ensuring optimal post-experiment welfare and demonstrating negligible cost for incorporating privacy into treatment allocation mechanisms.

Conclusion: The approach achieves optimal welfare outcomes with robust privacy guarantees, advancing the design of adaptive experiments that account for practical objectives like social welfare and accuracy.

Abstract: A/B testing is a widely adopted methodology for estimating conditional
average treatment effects (CATEs) in both clinical trials and online platforms.
While most existing research has focused primarily on maximizing estimation
accuracy, practical applications must also account for additional
objectives-most notably welfare or revenue loss. In many settings, it is
critical to administer treatments that improve patient outcomes or to implement
plans that generate greater revenue from customers. Within a machine learning
framework, such objectives are naturally captured through the notion of
cumulative regret. In this paper, we investigate the fundamental trade-off
between social welfare loss and statistical accuracy in (adaptive) experiments
with heterogeneous treatment effects. We establish matching upper and lower
bounds for the resulting multi-objective optimization problem and employ the
concept of Pareto optimality to characterize the necessary and sufficient
conditions for optimal experimental designs. Beyond estimating CATEs,
practitioners often aim to deploy treatment policies that maximize welfare
across the entire population. We demonstrate that our Pareto-optimal adaptive
design achieves optimal post-experiment welfare, irrespective of the
in-experiment trade-off between accuracy and welfare. Furthermore, since
clinical and commercial data are often highly sensitive, it is essential to
incorporate robust privacy guarantees into any treatment-allocation mechanism.
To this end, we develop differentially private algorithms that continue to
achieve our established lower bounds, showing that privacy can be attained at
negligible cost.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [484] [Stereovision Image Processing for Planetary Navigation Maps with Semi-Global Matching and Superpixel Segmentation](https://arxiv.org/abs/2509.05645)
*Yan-Shan Lu,Miguel Arana-Catania,Saurabh Upadhyay,Leonard Felicetti*

Main category: astro-ph.IM

TL;DR: The paper proposes a more accurate terrain modeling method for Mars exploration using Semi-Global Matching with superpixel-based refinement, improving depth maps and navigation safety.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in terrain modeling for Mars exploration, where traditional methods struggle with low-texture images, occlusions, and repetitive patterns.

Method: The method integrates Semi-Global Matching with superpixel-based refinement, leveraging context-aware segmentation for more coherent and detailed depth inference.

Result: The proposed method demonstrated improved structural consistency in Martian terrain maps, reduced gaps around rocks, and provided better accuracy across multiple datasets tested.

Conclusion: The approach presents a robust and efficient pipeline for terrain modeling, making it suitable for future autonomous navigation missions on Mars.

Abstract: Mars exploration requires precise and reliable terrain models to ensure safe
rover navigation across its unpredictable and often hazardous landscapes.
Stereoscopic vision serves a critical role in the rover's perception, allowing
scene reconstruction by generating precise depth maps through stereo matching.
State-of-the-art Martian planetary exploration uses traditional local
block-matching, aggregates cost over square windows, and refines disparities
via smoothness constraints. However, this method often struggles with
low-texture images, occlusion, and repetitive patterns because it considers
only limited neighbouring pixels and lacks a wider understanding of scene
context. This paper uses Semi-Global Matching (SGM) with superpixel-based
refinement to mitigate the inherent block artefacts and recover lost details.
The approach balances the efficiency and accuracy of SGM and adds context-aware
segmentation to support more coherent depth inference. The proposed method has
been evaluated in three datasets with successful results: In a Mars analogue,
the terrain maps obtained show improved structural consistency, particularly in
sloped or occlusion-prone regions. Large gaps behind rocks, which are common in
raw disparity outputs, are reduced, and surface details like small rocks and
edges are captured more accurately. Another two datasets, evaluated to test the
method's general robustness and adaptability, show more precise disparity maps
and more consistent terrain models, better suited for the demands of autonomous
navigation on Mars, and competitive accuracy across both non-occluded and
full-image error metrics. This paper outlines the entire terrain modelling
process, from finding corresponding features to generating the final 2D
navigation maps, offering a complete pipeline suitable for integration in
future planetary exploration missions.

</details>


### [485] [Advancing Resource Extraction Systems in Martian Volcanic Terrain: Rover Design, Power Consumption and Hazard Analysis](https://arxiv.org/abs/2509.06103)
*Divij Gupta,Arkajit Aich*

Main category: astro-ph.IM

TL;DR: The study presents a plan for mining resources in Martian volcanic terrains using adaptive rovers and integrated engineering strategies.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by Martian volcanic terrains and environmental hazards, facilitating resource utilization.

Method: Development of terrain-adaptive rovers and strategies, including slope stabilization methods, rover design, transport infrastructure analysis, and energy solutions using solar and nuclear power.

Result: The proposed mid-range rover design enhances navigation and mining capability in Mars’ volcanic terrains, supported by anchoring systems, solar arrays, and sensing systems.

Conclusion: Successful ISRU on Mars requires mechanical resilience, adaptability to harsh environments, and operational autonomy for sustained resource access.

Abstract: This study proposes a schematic plan for in-situ resource utilization (ISRU)
in Martian volcanic terrains. The work investigated the complexity of volcanic
terrains and Martian environmental hazards and suggested comprehensive
engineering strategies to overcome the odds and establish a successful mining
program in Martian volcanic regions. Slope stabilization methods - such as
terracing and anchored drilling rigs - with terrain-adaptive rovers capable of
autonomous operations on steep unstable slopes has been suggested as feasible
solutions to navigate the complex geological terrains of Martian volcanoes. The
mid range rover design with a mass of approximately 2.1 t, proposed here for
mining operations, incorporates a six-wheel rocker-bogie suspension,
anchoring-enabled drilling arm, dust-mitigation solar arrays, and advanced
sensing systems for hazard detection and navigation. A comparative analysis
regarding choice of roads and rails for building transport infrastructure has
also been performed. We have also looked into the energy requirement of the
rover to work under extreme environmental conditions of Mars and suggested a
combination of solar and nuclear power to account for the huge energy
requirements of sustained operations on Mars. The results demonstrate that
mission success in these environments depends on integrating mechanical
resilience, environmental adaptability, and operational autonomy, enabling
sustainable access to resources in one of Mars' most geologically challenging
settings.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [486] [Self-Driving Laboratory Optimizes the Lower Critical Solution Temperature of Thermoresponsive Polymers](https://arxiv.org/abs/2509.05351)
*Guoyue Xu,Renzheng Zhang,Tengfei Luo*

Main category: cond-mat.soft

TL;DR: The paper develops a cost-effective autonomous platform using robotic handling, sensors, and Bayesian optimization for tailoring thermoresponsive polymers' LCST.


<details>
  <summary>Details</summary>
Motivation: Traditional materials discovery through trial-and-error is inefficient, prompting a move towards automated labs with data-driven decision-making.

Method: The authors built a 'frugal twin' system integrating robotic fluid-handling, on-line sensors, and Bayesian optimization to optimize LCST for PNIPAM in multi-component salt solutions.

Result: The platform efficiently achieves user-specified LCST targets with minimal experiments, strategically navigates the parameter space, and self-corrects based on off-target results.

Conclusion: This work provides an accessible framework that reduces barriers to autonomous experimentation, accelerating the design and discovery of functional polymers.

Abstract: To overcome the inherent inefficiencies of traditional trial-and-error
materials discovery, the scientific community is increasingly developing
autonomous laboratories that integrate data-driven decision-making into
closed-loop experimental workflows. In this work, we realize this concept for
thermoresponsive polymers by developing a low-cost, "frugal twin" platform for
the optimization of the lower critical solution temperature (LCST) of
poly(N-isopropylacrylamide) (PNIPAM). Our system integrates robotic
fluid-handling, on-line sensors, and Bayesian optimization (BO) that navigates
the multi-component salt solution spaces to achieve user-specified LCST
targets. The platform demonstrates convergence to target properties within a
minimal number of experiments. It strategically explores the parameter space,
learns from informative "off-target" results, and self-corrects to achieve the
final targets. By providing an accessible and adaptable blueprint, this work
lowers the barrier to entry for autonomous experimentation and accelerates the
design and discovery of functional polymers.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [487] [MAPF-HD: Multi-Agent Path Finding in High-Density Environments](https://arxiv.org/abs/2509.06374)
*Hiroya Makino,Seigo Ito*

Main category: cs.MA

TL;DR: The study introduces a new framework (MAPF-HD) and method (PHANS) aimed at solving multi-agent pathfinding in high-density environments quickly and efficiently.


<details>
  <summary>Details</summary>
Motivation: The growing need for efficient logistics in high-density environments like warehouses requires better MAPF methods as current ILP-based approaches are computationally prohibitive for large-scale scenarios.

Method: The paper proposes the phased null-agent swapping (PHANS) technique, which uses heuristics to incrementally swap positions between agents and empty vertices, enabling faster path optimization.

Result: PHANS dramatically reduces computation time for MAPF-HD problems, from tens to hundreds of seconds (previous methods) down to mere seconds or tens of seconds in environments exceeding 700 cells.

Conclusion: PHANS and the MAPF-HD framework significantly enhance the efficiency and scalability of MAPF problems, making them applicable in practical contexts such as automated warehouses, traffic management, and crowd dynamics.

Abstract: Multi-agent path finding (MAPF) involves planning efficient paths for
multiple agents to move simultaneously while avoiding collisions. In typical
warehouse environments, agents are often sparsely distributed along aisles.
However, increasing the agent density can improve space efficiency. When the
agent density is high, we must optimize the paths not only for goal-assigned
agents but also for those obstructing them. This study proposes a novel MAPF
framework for high-density environments (MAPF-HD). Several studies have
explored MAPF in similar settings using integer linear programming (ILP).
However, ILP-based methods require substantial computation time to optimize all
agent paths simultaneously. Even in small grid-based environments with fewer
than $100$ cells, these computations can incur tens to hundreds of seconds.
These high computational costs render these methods impractical for large-scale
applications such as automated warehouses and valet parking. To address these
limitations, we introduce the phased null-agent swapping (PHANS) method. PHANS
employs a heuristic approach to incrementally swap positions between agents and
empty vertices. This method solves the MAPF-HD problem within seconds to tens
of seconds, even in large environments containing more than $700$ cells. The
proposed method can potentially improve efficiency in various real-world
applications such as warehouse logistics, traffic management, or crowd control.
Code is available at https://github.com/ToyotaCRDL/MAPF-in-High-Density-Envs.

</details>


### [488] [Nanobot Algorithms for Treatment of Diffuse Cancer](https://arxiv.org/abs/2509.06893)
*Noble Harasha,Nancy Lynch*

Main category: cs.MA

TL;DR: The paper discusses algorithms for nanobot-driven drug delivery systems targeting diffuse cancer sites, comparing their effectiveness in locating and treating cancer sites through simulation results.


<details>
  <summary>Details</summary>
Motivation: Targeted drug delivery via nanosized particles promises efficient and less toxic treatment, particularly for diffuse cancers dispersed across multiple sites.

Method: Three algorithms were developed for nanobots navigating chemical signals: KM (agents following natural signals), KMA (signal amplification), and KMAR (negative chemotaxis to discourage treatment oversaturation).

Result: Simulation results show KM works well with strong natural signals but fails with weak ones; KMA speeds up treatment but sacrifices success in some patterns, while KMAR outperforms in robustness and adaptability.

Conclusion: KMAR is the most effective algorithm, offering robust performance across various cancer patterns, enhancing the precision and adaptability of nanobot-based drug delivery systems.

Abstract: Motile nanosized particles, or "nanobots", promise more effective and less
toxic targeted drug delivery because of their unique scale and precision. We
consider the case in which the cancer is "diffuse", dispersed such that there
are multiple distinct cancer sites. We investigate the problem of a swarm of
nanobots locating these sites and treating them by dropping drug payloads at
the sites. To improve the success of the treatment, the drug payloads must be
allocated between sites according to their "demands"; this requires extra
nanobot coordination. We present a mathematical model of the behavior of the
nanobot agents and of their colloidal environment. This includes a movement
model for agents based upon experimental findings from actual nanoparticles in
which bots noisily ascend and descend chemical gradients. We present three
algorithms: The first algorithm, called KM, is the most representative of
reality, with agents simply following naturally existing chemical signals that
surround each cancer site. The second algorithm, KMA, includes an additional
chemical payload which amplifies the existing natural signals. The third
algorithm, KMAR, includes another additional chemical payload which counteracts
the other signals, instead inducing negative chemotaxis in agents such that
they are repelled from sites that are already sufficiently treated. We present
simulation results for all algorithms across different types of cancer
arrangements. For KM, we show that the treatment is generally successful unless
the natural chemical signals are weak, in which case the treatment progresses
too slowly. For KMA, we demonstrate a significant improvement in treatment
speed but a drop in eventual success, except for concentrated cancer patterns.
For KMAR, our results show great performance across all types of cancer
patterns, demonstrating robustness and adaptability.

</details>


### [489] [Orchestrator: Active Inference for Multi-Agent Systems in Long-Horizon Tasks](https://arxiv.org/abs/2509.05651)
*Lukas Beckenbauer,Johannes-Lucas Loewe,Ge Zheng,Alexandra Brintrup*

Main category: cs.MA

TL;DR: The paper presents 'Orchestrator,' an MAS framework addressing coordination and global task optimization issues for complex tasks in non-linear environments.


<details>
  <summary>Details</summary>
Motivation: Improving coordination and task optimization in multi-agent systems (MAS) confronted by partial observability and non-linear dynamics.

Method: Introducing a framework called 'Orchestrator,' which tracks agent and environment interactions while utilizing self-emergent coordination and active inference benchmarks.

Result: The framework, tested on maze puzzles of varying complexity, demonstrated enhanced performance and coordination in dynamic environments with long-term objectives.

Conclusion: Orchestrator effectively mitigates partial observability challenges and boosts MAS efficiency in solving complex tasks with long-horizon objectives.

Abstract: Complex, non-linear tasks challenge LLM-enhanced multi-agent systems (MAS)
due to partial observability and suboptimal coordination. We propose
Orchestrator, a novel MAS framework that leverages attention-inspired
self-emergent coordination and reflective benchmarking to optimize global task
performance. Orchestrator introduces a monitoring mechanism to track
agent-environment dynamics, using active inference benchmarks to optimize
system behavior. By tracking agent-to-agent and agent-to-environment
interaction, Orchestrator mitigates the effects of partial observability and
enables agents to approximate global task solutions more efficiently. We
evaluate the framework on a series of maze puzzles of increasing complexity,
demonstrating its effectiveness in enhancing coordination and performance in
dynamic, non-linear environments with long-horizon objectives.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [490] [Livia: An Emotion-Aware AR Companion Powered by Modular AI Agents and Progressive Memory Compression](https://arxiv.org/abs/2509.05298)
*Rui Xi,Xianghan Wang*

Main category: cs.HC

TL;DR: The paper presents Livia, an emotion-aware augmented reality companion app designed to alleviate loneliness through advanced AI modules, emotion detection algorithms, and AR interaction.


<details>
  <summary>Details</summary>
Motivation: Addressing the emotional and health challenges posed by loneliness and social isolation by leveraging technology-based solutions.

Method: Development of Livia using modular AI agents for emotion analysis, dialogue generation, memory optimization, and AR-based embodied interaction. Included introduction of TBC and DIMF algorithms for effective long-term memory management.

Result: Livia demonstrated enhanced user emotional bonds, satisfaction, and statistically significant reductions in loneliness through evaluations. Users appreciated its adaptive personality and realistic AR interactions.

Conclusion: Livia effectively combines multimodal emotion detection and memory management with AR to combat loneliness. Future improvements could include gesture and tactile interactions, multi-user capabilities, and hardware optimization.

Abstract: Loneliness and social isolation pose significant emotional and health
challenges, prompting the development of technology-based solutions for
companionship and emotional support. This paper introduces Livia, an
emotion-aware augmented reality (AR) companion app designed to provide
personalized emotional support by combining modular artificial intelligence
(AI) agents, multimodal affective computing, progressive memory compression,
and AR driven embodied interaction. Livia employs a modular AI architecture
with specialized agents responsible for emotion analysis, dialogue generation,
memory management, and behavioral orchestration, ensuring robust and adaptive
interactions. Two novel algorithms-Temporal Binary Compression (TBC) and
Dynamic Importance Memory Filter (DIMF)-effectively manage and prioritize
long-term memory, significantly reducing storage requirements while retaining
critical context. Our multimodal emotion detection approach achieves high
accuracy, enhancing proactive and empathetic engagement. User evaluations
demonstrated increased emotional bonds, improved satisfaction, and
statistically significant reductions in loneliness. Users particularly valued
Livia's adaptive personality evolution and realistic AR embodiment. Future
research directions include expanding gesture and tactile interactions,
supporting multi-user experiences, and exploring customized hardware
implementations.

</details>


### [491] ["It was Tragic": Exploring the Impact of a Robot's Shutdown](https://arxiv.org/abs/2509.06934)
*Agam Oberlender,Hadas Erel*

Main category: cs.HC

TL;DR: The study examines how people perceive robotic gestures during shutdown, revealing that a 'designed' shutdown gesture akin to 'falling asleep' leads to more positive interpretations than a typical robotic shutdown.


<details>
  <summary>Details</summary>
Motivation: The paper explores the human tendency to anthropomorphize robots and aims to understand how robot shutdown gestures influence people's perceptions, emphasizing the need for social consideration in robotic design.

Method: The experiment involved participants interacting with a robotic arm and turning it off under two conditions: (1) 'Non-designed,' a typical sudden shutdown, and (2) 'Designed,' a gradual shutdown mimicking falling asleep.

Result: Participants anthropomorphized the robot's shutdown gestures. In the 'Non-designed' condition, responses were mostly negative (interpreting the robot as 'dying'), while the 'Designed' condition evoked neutral and positive responses (interpreting as 'falling asleep'). The latter also enhanced likeability, perceived intelligence, and animacy of the robot.

Conclusion: Common interactions with robots, like shutdowns, significantly impact user perception. Designers should account for humans' tendency to socially interpret robotic behaviors to foster better robot-human interactions.

Abstract: It is well established that people perceive robots as social entities, even
when they are not designed for social interaction. We evaluated whether the
social interpretation of robotic gestures should also be considered when
turning off a robot. In the experiment, participants engaged in a brief
preliminary neutral interaction while a robotic arm showed interest in their
actions. At the end of the task, participants were asked to turn off the
robotic arm under two conditions: (1) a Non-designed condition, where all of
the robot's engines were immediately and simultaneously turned off, as robots
typically shut down; (2) a Designed condition, where the robot's engines
gradually folded inward in a motion resembling "falling asleep." Our findings
revealed that all participants anthropomorphized the robot's movement when it
was turned off. In the Non-designed condition, most participants interpreted
the robot's turn-off movement negatively, as if the robot had "died." In the
Designed condition, most participants interpreted it more neutrally, stating
that the robot "went to sleep." The robot's turn-off movement also impacted its
perception, leading to higher likeability, perceived intelligence, and animacy
in the Designed condition. We conclude that the impact of common edge
interactions, such as turning off a robot, should be carefully designed while
considering people's automatic tendency to perceive robots as social entities.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [492] [Programming tension in 3D printed networks inspired by spiderwebs](https://arxiv.org/abs/2509.05855)
*Thijs Masmeijer,Caleb Swain,Jeff Hill,Ed Habtour*

Main category: cs.GR

TL;DR: The paper proposes a fabrication algorithm for 3D printing tensioned structural networks with accurate programmed tension gradients, validated experimentally with minimal error.


<details>
  <summary>Details</summary>
Motivation: Ensuring accurate tension gradients in manufactured structural networks is challenging, as inaccuracies arise during fabrication processes like flattening. This hinders their effectiveness in applications requiring precise tension control.

Method: A new fabrication algorithm is introduced, which utilizes the force density method to design tension gradients, optimizes vertex locations and converts elements into arcs, decomposes the network into printable toolpaths, and resolves flattening-induced issues for 3D printing.

Result: The experimental validation showed that the method achieves tension gradients with an average strain error of less than 1.0%, and is effective for elements with lengths as small as 5.8 mm and stresses up to 7.3 MPa.

Conclusion: The algorithm enables accurate and efficient production of complex tensioned networks, with potential applications in creating advanced structures like medical braces, curved meshes, and tensegrity systems.

Abstract: Each element in tensioned structural networks -- such as tensegrity,
architectural fabrics, or medical braces/meshes -- requires a specific tension
level to achieve and maintain the desired shape, stability, and compliance.
These structures are challenging to manufacture, 3D print, or assemble because
flattening the network during fabrication introduces multiplicative
inaccuracies in the network's final tension gradients. This study overcomes
this challenge by offering a fabrication algorithm for direct 3D printing of
such networks with programmed tension gradients, an approach analogous to the
spinning of spiderwebs. The algorithm: (i) defines the desired network and
prescribes its tension gradients using the force density method; (ii) converts
the network into an unstretched counterpart by numerically optimizing vertex
locations toward target element lengths and converting straight elements into
arcs to resolve any remaining error; and (iii) decomposes the network into
printable toolpaths; Optional additional steps are: (iv) flattening curved 2D
networks or 3D networks to ensure 3D printing compatibility; and (v)
automatically resolving any unwanted crossings introduced by the flattening
process. The proposed method is experimentally validated using 2D unit cells of
viscoelastic filaments, where accurate tension gradients are achieved with an
average element strain error of less than 1.0\%. The method remains effective
for networks with element minimum length and maximum stress of 5.8 mm and 7.3
MPa, respectively. The method is used to demonstrate the fabrication of three
complex cases: a flat spiderweb, a curved mesh, and a tensegrity system. The
programmable tension gradient algorithm can be utilized to produce compact,
integrated cable networks, enabling novel applications such as moment-exerting
structures in medical braces and splints.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [493] [Distributed Automatic Generation Control subject to Ramp-Rate-Limits: Anytime Feasibility and Uniform Network-Connectivity](https://arxiv.org/abs/2509.06588)
*Mohammadreza Doostmohammadian,Hamid R. Rabiee*

Main category: eess.SY

TL;DR: The paper presents a distributed solution for automatic generation control over a network of generators, addressing ramp-rate-limits (RRL), typically neglected in existing solutions.


<details>
  <summary>Details</summary>
Motivation: Existing optimization solutions for automatic generation control often ignore ramp-rate limits (RRL), which are crucial for real-time power generation scenarios.

Method: A distributed optimization approach using information consensus algorithms, incorporating constraints such as RRL, box limits, and generation-demand balance (anytime feasibility).

Result: The method ensures anytime feasibility of constraints, improves convergence rate with signum-based nonlinearity, and is tolerant to communication link removal under uniform-connectivity.

Conclusion: The proposed approach offers a more practical and feasible solution for real-time power generation while enhancing algorithm robustness and convergence.

Abstract: This paper considers automatic generation control over an information-sharing
network of communicating generators as a multi-agent system. The optimization
solution is distributed among the agents based on information consensus
algorithms, while addressing the generators' ramp-rate-limits (RRL). This is
typically ignored in the existing linear/nonlinear optimization solutions but
they exist in real-time power generation scenarios. Without addressing the RRL,
the generators cannot follow the assigned rate of generating power by the
optimization algorithm; therefore, the existing solutions may not necessarily
converge to the exact optimal cost or may lose feasibility in practice. The
proposed solution in this work addresses the ramp-rate-limit constraint along
with the box constraint (limits on the generated powers) and the
coupling-constraint (generation-demand balance) at all iteration times of the
algorithm. The latter is referred to as the anytime feasibility and implies
that at every termination point of the algorithm, the balance between the
demand and generated power holds. To improve the convergence rate of the
algorithm we further consider internal signum-based nonlinearity. We also show
that our solution can tolerate communication link removal. This follows from
the uniform-connectivity assumption on the communication network.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [494] [The Efficiency Frontier: Classical Shadows versus Quantum Footage](https://arxiv.org/abs/2509.06218)
*Shuowei Ma,Junyu Liu*

Main category: quant-ph

TL;DR: The study contrasts classical shadow methods and quantum footage for extracting information from quantum states, identifying specific conditions where each method excels.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in quantum-to-classical interfacing when classical shadow methods fail in scenarios with non-local observables or limited classical processing power.

Method: A full-stack resource analysis comparing classical shadows and quantum footage, considering parameters like qubits, observables, sparsity, Pauli weight, and hardware capability.

Result: Classical shadows outperform direct measurements for certain observable characteristics, while direct measurements may be more efficient under other conditions. Hardware-specific break-even points are identified.

Conclusion: The paper offers a framework for optimizing hybrid quantum-classical tomography and provides guidance for measurement strategy selection based on system parameters.

Abstract: Interfacing quantum and classical processors is an important subroutine in
full-stack quantum algorithms. The so-called "classical shadow" method
efficiently extracts essential classical information from quantum states,
enabling the prediction of many properties of a quantum system from only a few
measurements. However, for a small number of highly non-local observables, or
when classical post-processing power is limited, the classical shadow method is
not always the most efficient choice. Here, we address this issue
quantitatively by performing a full-stack resource analysis that compares
classical shadows with ``quantum footage," which refers to direct quantum
measurement. Under certain assumptions, our analysis illustrates a boundary of
download efficiency between classical shadows and quantum footage. For
observables expressed as linear combinations of Pauli matrices, the classical
shadow method outperforms direct measurement when the number of observables is
large and the Pauli weight is small. For observables in the form of large
Hermitian sparse matrices, the classical shadow method shows an advantage when
the number of observables, the sparsity of the matrix, and the number of qubits
fall within a certain range. The key parameters influencing this behavior
include the number of qubits $n$, observables $M$, sparsity $k$, Pauli weight
$w$, accuracy requirement $\epsilon$, and failure tolerance $\delta$. We also
compare the resource consumption of the two methods on different types of
quantum computers and identify break-even points where the classical shadow
method becomes more efficient, which vary depending on the hardware. This paper
opens a new avenue for quantitatively designing optimal strategies for hybrid
quantum-classical tomography and provides practical insights for selecting the
most suitable quantum measurement approach in real-world applications.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [495] [Imagining Alternatives: Towards High-Resolution 3D Counterfactual Medical Image Generation via Language Guidance](https://arxiv.org/abs/2509.05978)
*Mohamed Mohamed,Brennan Nichyporuk,Douglas L. Arnold,Tal Arbel*

Main category: eess.IV

TL;DR: The paper introduces a framework for generating 3D medical images based on natural language prompts, addressing the lack of pretrained 3D foundation models.


<details>
  <summary>Details</summary>
Motivation: The lack of pretrained 3D foundation models limits the potential of vision-language models in generating 3D medical images, which could enable significant clinical and research applications.

Method: They adapt state-of-the-art 3D diffusion models with enhancements from Simple Diffusion and introduce augmented conditioning to improve text alignment and image quality.

Result: The framework successfully simulated counterfactual lesion loads in Multiple Sclerosis (MS) and cognitive states in Alzheimer's disease, producing high-resolution 3D medical images while preserving subject fidelity.

Conclusion: This work lays a foundation for future prompt-driven 3D medical imaging applications and disease progression analysis.

Abstract: Vision-language models have demonstrated impressive capabilities in
generating 2D images under various conditions; however the impressive
performance of these models in 2D is largely enabled by extensive, readily
available pretrained foundation models. Critically, comparable pretrained
foundation models do not exist for 3D, significantly limiting progress in this
domain. As a result, the potential of vision-language models to produce
high-resolution 3D counterfactual medical images conditioned solely on natural
language descriptions remains completely unexplored. Addressing this gap would
enable powerful clinical and research applications, such as personalized
counterfactual explanations, simulation of disease progression scenarios, and
enhanced medical training by visualizing hypothetical medical conditions in
realistic detail. Our work takes a meaningful step toward addressing this
challenge by introducing a framework capable of generating high-resolution 3D
counterfactual medical images of synthesized patients guided by free-form
language prompts. We adapt state-of-the-art 3D diffusion models with
enhancements from Simple Diffusion and incorporate augmented conditioning to
improve text alignment and image quality. To our knowledge, this represents the
first demonstration of a language-guided native-3D diffusion model applied
specifically to neurological imaging data, where faithful three-dimensional
modeling is essential to represent the brain's three-dimensional structure.
Through results on two distinct neurological MRI datasets, our framework
successfully simulates varying counterfactual lesion loads in Multiple
Sclerosis (MS), and cognitive states in Alzheimer's disease, generating
high-quality images while preserving subject fidelity in synthetically
generated medical images. Our results lay the groundwork for prompt-driven
disease progression analysis within 3D medical imaging.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [496] [Language Native Lightly Structured Databases for Large Language Model Driven Composite Materials Research](https://arxiv.org/abs/2509.06093)
*Yuze Liu,Zhaoyuan Zhang,Xiangsheng Zeng,Yihe Zhang,Leping Yu,Lejia Wang,Xi Yu*

Main category: cs.DB

TL;DR: This paper introduces a language-native database tailored for boron nitride nanosheet polymer thermally conductive composites, enabling enhanced literature synthesis and guided material discovery.


<details>
  <summary>Details</summary>
Motivation: Traditional chemical and materials research often relies on language-based descriptions, limiting exploitation by databases and ML due to lack of structured data.

Method: Developed a language-native database capturing structured information from papers on BNNS polymer composites, using heterogeneous databases for composite retrieval based on semantics, keywords, and filters.

Result: The database synthesizes literature into accurate and expert-guided outputs, facilitating efficient Retrieval Augmented Generation (RAG) and tool-augmented reasoning agents.

Conclusion: The framework establishes a structured, language-rich foundation crucial for leveraging LLMs in materials discovery and advancing research efficiency.

Abstract: Chemical and materials research has traditionally relied heavily on knowledge
narrative, with progress often driven by language-based descriptions of
principles, mechanisms, and experimental experiences, rather than tables,
limiting what conventional databases and ML can exploit. We present a
language-native database for boron nitride nanosheet (BNNS) polymer thermally
conductive composites that captures lightly structured information from papers
across preparation, characterization, theory-computation, and mechanistic
reasoning, with evidence-linked snippets. Records are organized in a
heterogeneous database and queried via composite retrieval with semantics, key
words and value filters. The system can synthesizes literature into accurate,
verifiable, and expert style guidance. This substrate enables high fidelity
efficient Retrieval Augmented Generation (RAG) and tool augmented agents to
interleave retrieval with reasoning and deliver actionable SOP. The framework
supplies the language rich foundation required for LLM-driven materials
discovery.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [497] [Tiga: Accelerating Geo-Distributed Transactions with Synchronized Clocks [Technical Report]](https://arxiv.org/abs/2509.05759)
*Jinkun Geng,Shuai Mu,Anirudh Sivaraman,Balaji Prabhakar*

Main category: cs.NI

TL;DR: Tiga, a new geo-replicated, scalable transactional database design, achieves 1 WRTT transaction commits for most scenarios, with improved throughput and lower latency.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency of geo-replicated transactional databases like Google Spanner by committing transactions in 1 WRTT while minimizing computational overhead.

Method: The approach combines concurrency control and consensus into a single process using synchronized clocks to proactively order transactions by future timestamps. For rare delays, a fallback mechanism completes commits in 1.5-2 WRTTs.

Result: Tiga demonstrates 1.3–7.2x higher throughput and 1.4–4.6x lower latency compared to existing solutions, especially excelling in 1-WRTT latency transactions.

Conclusion: Tiga effectively combines strict serializability, consistent replication, and high performance into a scalable, open-source solution for transactional databases.

Abstract: This paper presents Tiga, a new design for geo-replicated and scalable
transactional databases such as Google Spanner. Tiga aims to commit
transactions within 1 wide-area roundtrip time, or 1 WRTT, for a wide range of
scenarios, while maintaining high throughput with minimal computational
overhead. Tiga consolidates concurrency control and consensus, completing both
strictly serializable execution and consistent replication in a single round.
It uses synchronized clocks to proactively order transactions by assigning each
a future timestamp at submission. In most cases, transactions arrive at servers
before their future timestamps and are serialized according to the designated
timestamp, requiring 1 WRTT to commit. In rare cases, transactions are delayed
and proactive ordering fails, in which case Tiga falls back to a slow path,
committing in 1.5--2 WRTTs. Compared to state-of-the-art solutions, Tiga can
commit more transactions at 1-WRTT latency, and incurs much less throughput
overhead. Evaluation results show that Tiga outperforms all baselines,
achieving 1.3--7.2$\times$ higher throughput and 1.4--4.6$\times$ lower
latency. Tiga is open-sourced at
https://github.com/New-Consensus-Concurrency-Control/Tiga.

</details>


### [498] [Distributed Link Sparsification for Scalable Scheduling Using Graph Neural Networks (Journal Version)](https://arxiv.org/abs/2509.05447)
*Zhongyuan Zhao,Gunjan Verma,Ananthram Swami,Santiago Segarra*

Main category: cs.NI

TL;DR: The paper proposes a method using graph neural networks (GNNs) to reduce scheduling overhead in dense wireless networks while maintaining network capacity.


<details>
  <summary>Details</summary>
Motivation: To tackle significant signaling overhead in dense wireless networks, which leads to congestion, energy consumption, and expanded radio footprints.

Method: The authors employ a GNN module trained via an offline constrained unsupervised learning algorithm to adjust contention thresholds, reducing scheduling contention for low-probability links.

Result: The technique successfully reduces network congestion and radio footprints in simulated wireless multi-hop networks with up to 500 links across four scheduling protocols.

Conclusion: The proposed link sparsification method efficiently balances network utility and scheduling overhead, offering a scalable solution for delay-tolerant traffic in dense networks.

Abstract: In wireless networks characterized by dense connectivity, the significant
signaling overhead generated by distributed link scheduling algorithms can
exacerbate issues like congestion, energy consumption, and radio footprint
expansion. To mitigate these challenges, we propose a distributed link
sparsification scheme employing graph neural networks (GNNs) to reduce
scheduling overhead for delay-tolerant traffic while maintaining network
capacity. A GNN module is trained to adjust contention thresholds for
individual links based on traffic statistics and network topology, enabling
links to withdraw from scheduling contention when they are unlikely to succeed.
Our approach is facilitated by a novel offline constrained {unsupervised}
learning algorithm capable of balancing two competing objectives: minimizing
scheduling overhead while ensuring that total utility meets the required level.
In simulated wireless multi-hop networks with up to 500 links, our link
sparsification technique effectively alleviates network congestion and reduces
radio footprints across four distinct distributed link scheduling protocols.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [499] [Predicting Market Troughs: A Machine Learning Approach with Causal Interpretation](https://arxiv.org/abs/2509.05922)
*Peilin Rao,Randall R. Rojas*

Main category: q-fin.ST

TL;DR: The paper uses a causal machine learning framework to identify key drivers of market troughs, emphasizing options-implied risk appetite and market liquidity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to identify and better understand the causal factors behind market troughs, addressing limitations in linear models and incorporating advanced techniques.

Method: The paper uses a flexible machine learning framework (DML average partial effect) to provide robust causal estimates and employs a high-performance nowcasting model to analyze capitulation events.

Result: Key causal drivers of market troughs identified in the study are options-implied risk appetite volatility and market liquidity.

Conclusion: The findings support intermediary asset pricing theories and highlight the value of advanced models for analyzing financial markets in high-frequency settings.

Abstract: This paper provides robust, new evidence on the causal drivers of market
troughs. We demonstrate that conclusions about these triggers are critically
sensitive to model specification, moving beyond restrictive linear models with
a flexible DML average partial effect causal machine learning framework. Our
robust estimates identify the volatility of options-implied risk appetite and
market liquidity as key causal drivers, relationships misrepresented or
obscured by simpler models. These findings provide high-frequency empirical
support for intermediary asset pricing theories. This causal analysis is
enabled by a high-performance nowcasting model that accurately identifies
capitulation events in real-time.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [500] [Neural ARFIMA model for forecasting BRIC exchange rates with long memory under oil shocks and policy uncertainties](https://arxiv.org/abs/2509.06697)
*Tanujit Chakraborty,Donia Besher,Madhurima Panja,Shovon Sengupta*

Main category: econ.EM

TL;DR: The paper introduces a novel NARFIMA model for forecasting BRIC exchange rates, addressing long-memory, nonlinearity, and non-stationarity challenges while outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Accurate exchange rate forecasting for emerging economies faces challenges from complex dynamics including long memory, nonlinearity, non-stationarity, and external drivers such as global economic uncertainty and oil prices.

Method: The study proposes the NARFIMA model, integrating ARFIMA's long-memory representation and neural networks' nonlinear learning capacity, while using Markov chains and nonlinear techniques for theoretical validation.

Result: Empirical forecasting results across six horizons demonstrate that the NARFIMA model outperforms state-of-the-art statistical and machine learning methods for BRIC currencies.

Conclusion: The findings highlight the effectiveness of NARFIMA for challenging financial predictions and stress its utility for policymakers and market participants. The method is implemented in the R package 'narfima'.

Abstract: Accurate forecasting of exchange rates remains a persistent challenge,
particularly for emerging economies such as Brazil, Russia, India, and China
(BRIC). These series exhibit long memory, nonlinearity, and non-stationarity
properties that conventional time series models struggle to capture.
Additionally, there exist several key drivers of exchange rate dynamics,
including global economic policy uncertainty, US equity market volatility, US
monetary policy uncertainty, oil price growth rates, and country-specific
short-term interest rate differentials. These empirical complexities underscore
the need for a flexible modeling framework that can jointly accommodate long
memory, nonlinearity, and the influence of external drivers. To address these
challenges, we propose a Neural AutoRegressive Fractionally Integrated Moving
Average (NARFIMA) model that combines the long-memory representation of ARFIMA
with the nonlinear learning capacity of neural networks, while flexibly
incorporating exogenous causal variables. We establish theoretical properties
of the model, including asymptotic stationarity of the NARFIMA process using
Markov chains and nonlinear time series techniques. We quantify forecast
uncertainty using conformal prediction intervals within the NARFIMA framework.
Empirical results across six forecast horizons show that NARFIMA consistently
outperforms various state-of-the-art statistical and machine learning models in
forecasting BRIC exchange rates. These findings provide new insights for
policymakers and market participants navigating volatile financial conditions.
The \texttt{narfima} \textbf{R} package provides an implementation of our
approach.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [501] [Newton to Einstein: Axiom-Based Discovery via Game Design](https://arxiv.org/abs/2509.05448)
*Pingchuan Ma,Benjamin Tod Jones,Tsun-Hsuan Wang,Minghao Guo,Michal Piotr Lipiec,Chuang Gan,Wojciech Matusik*

Main category: cs.CE

TL;DR: The paper advocates for axiom-based reasoning in machine learning for scientific discovery and proposes a game design framework for evolving rules to explain outlier observations.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional ML methods that rely on fixed assumptions, the paper aims to develop a framework for systematically discovering new theoretical structures.

Method: The proposed method recasts scientific inquiry as a rule-evolving system, where agents modify axioms in logic-based environments to address outliers, demonstrated via logic-based games.

Result: Preliminary experiments show that agents can evolve axioms to solve problems previously unsolvable by fixed-rule systems.

Conclusion: The framework establishes a path for creating ML systems capable of creative and interpretable theory-driven scientific discovery through systematic axiom adaptation.

Abstract: This position paper argues that machine learning for scientific discovery
should shift from inductive pattern recognition to axiom-based reasoning. We
propose a game design framework in which scientific inquiry is recast as a
rule-evolving system: agents operate within environments governed by axioms and
modify them to explain outlier observations. Unlike conventional ML approaches
that operate within fixed assumptions, our method enables the discovery of new
theoretical structures through systematic rule adaptation. We demonstrate the
feasibility of this approach through preliminary experiments in logic-based
games, showing that agents can evolve axioms that solve previously unsolvable
problems. This framework offers a foundation for building machine learning
systems capable of creative, interpretable, and theory-driven discovery.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [502] [Toward Efficient and Scalable Design of In-Memory Graph-Based Vector Search](https://arxiv.org/abs/2509.05750)
*Ilias Azizi,Karima Echihab,Themis Palpanas,Vassilis Christophides*

Main category: cs.IR

TL;DR: The paper evaluates and compares 12 state-of-the-art graph-based vector search methods on large datasets and provides insights on their effectiveness.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a systematic comparison of graph-based vector search algorithms, which have become critical for analyzing large-scale, high-dimensional vector data.

Method: The authors conduct an exhaustive experimental evaluation of twelve graph-based vector search algorithms on seven real-world datasets containing up to 1 billion vectors, evaluating their strengths and limitations under various conditions.

Result: The analysis shows that incremental insertion and neighborhood diversification are the most effective strategies, while certain base graph choices can hinder scalability.

Conclusion: This paper offers key insights into graph-based methods for vector search, highlighting gaps for improvement like better seed selection and diversification strategies.

Abstract: Vector data is prevalent across business and scientific applications, and its
popularity is growing with the proliferation of learned embeddings. Vector data
collections often reach billions of vectors with thousands of dimensions, thus,
increasing the complexity of their analysis. Vector search is the backbone of
many critical analytical tasks, and graph-based methods have become the best
choice for analytical tasks that do not require guarantees on the quality of
the answers. Although several paradigms (seed selection, incremental insertion,
neighborhood propagation, neighborhood diversification, and divide-and-conquer)
have been employed to design in-memory graph-based vector search algorithms, a
systematic comparison of the key algorithmic advances is still missing. We
conduct an exhaustive experimental evaluation of twelve state-of-the-art
methods on seven real data collections, with sizes up to 1 billion vectors. We
share key insights about the strengths and limitations of these methods; e.g.,
the best approaches are typically based on incremental insertion and
neighborhood diversification, and the choice of the base graph can hurt
scalability. Finally, we discuss open research directions, such as the
importance of devising more sophisticated data adaptive seed selection and
diversification strategies.

</details>


### [503] [Language Bias in Information Retrieval: The Nature of the Beast and Mitigation Methods](https://arxiv.org/abs/2509.06195)
*Jinrui Yang,Fan Jiang,Timothy Baldwin*

Main category: cs.IR

TL;DR: The paper explores language fairness in multilingual information retrieval and proposes LaKDA, a novel methodology to mitigate biases.


<details>
  <summary>Details</summary>
Motivation: To address language biases in MLIR systems and ensure fair access to information for diverse languages.

Method: Evaluation of traditional methods, DPR neural rankers (using mBERT and XLM-R), and introduction of LaKDA for bias mitigation.

Result: Intrinsic language biases were analyzed, disparities highlighted, and LaKDA demonstrated effectiveness in improving fairness.

Conclusion: Fairness in MLIR is critical, and innovations like LaKDA can reduce biases across languages.

Abstract: Language fairness in multilingual information retrieval (MLIR) systems is
crucial for ensuring equitable access to information across diverse languages.
This paper sheds light on the issue, based on the assumption that queries in
different languages, but with identical semantics, should yield equivalent
ranking lists when retrieving on the same multilingual documents. We evaluate
the degree of fairness using both traditional retrieval methods, and a DPR
neural ranker based on mBERT and XLM-R. Additionally, we introduce `LaKDA', a
novel loss designed to mitigate language biases in neural MLIR approaches. Our
analysis exposes intrinsic language biases in current MLIR technologies, with
notable disparities across the retrieval methods, and the effectiveness of
LaKDA in enhancing language fairness.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [504] [Several Performance Bounds on Decentralized Online Optimization are Highly Conservative and Potentially Misleading](https://arxiv.org/abs/2509.06466)
*Erwan Meunier,Julien M. Hendrickx*

Main category: math.OC

TL;DR: The study analyzes decentralized online optimization algorithms to determine their exact worst-case performance and finds that current performance guarantees are overly conservative.


<details>
  <summary>Details</summary>
Motivation: The motivation is to evaluate the accuracy of current performance guarantees for decentralized optimization algorithms and to analyze the interplay of inter-agent communication, performance, and tuning.

Method: The authors utilized the Performance Estimation Problem approach for a data-driven evaluation of worst-case performance for these algorithms.

Result: The authors found existing performance guarantees overly conservative, observed limited benefit from inter-agent communication in some cases, and demonstrated that tuning step-sizes could enhance performance by up to 20%.

Conclusion: Current performance guarantees for decentralized optimization often overestimate limitations and can be improved significantly through refined tuning, enhancing algorithm choice and efficiency.

Abstract: We analyze Decentralized Online Optimization algorithms using the Performance
Estimation Problem approach which allows, to automatically compute exact
worst-case performance of optimization algorithms. Our analysis shows that
several available performance guarantees are very conservative, sometimes by
multiple orders of magnitude, and can lead to misguided choices of algorithm.
Moreover, at least in terms of worst-case performance, some algorithms appear
not to benefit from inter-agent communications for a significant period of
time. We show how to improve classical methods by tuning their step-sizes, and
find that we can save up to 20% on their actual worst-case performance regret.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [505] [Universality of physical neural networks with multivariate nonlinearity](https://arxiv.org/abs/2509.05420)
*Benjamin Savinson,David J. Norris,Siddhartha Mishra,Samuel Lanthaler*

Main category: physics.optics

TL;DR: This paper establishes a universality condition for physical neural networks, proposes a scalable optical architecture achieving high accuracy, and provides routes for large-scale implementations.


<details>
  <summary>Details</summary>
Motivation: Artificial intelligence's high energy demand necessitates exploring energy-efficient hardware like physical neural networks, especially leveraging optical systems.

Method: The authors develop a mathematical theorem to determine universality in physical neural networks and design a scalable architecture using free-space optics, supplemented with temporal multiplexing for system size scaling.

Result: The proposed optical architecture demonstrates high accuracy in image classification tasks, and the theorem enables scalability for previously limited photonic devices.

Conclusion: Their findings extend beyond optics, providing guidelines for designing universal, energy-efficient physical neural networks while advocating for further research in this area.

Abstract: The enormous energy demand of artificial intelligence is driving the
development of alternative hardware for deep learning. Physical neural networks
try to exploit physical systems to perform machine learning more efficiently.
In particular, optical systems can calculate with light using negligible
energy. While their computational capabilities were long limited by the
linearity of optical materials, nonlinear computations have recently been
demonstrated through modified input encoding. Despite this breakthrough, our
inability to determine if physical neural networks can learn arbitrary
relationships between data -- a key requirement for deep learning known as
universality -- hinders further progress. Here we present a fundamental theorem
that establishes a universality condition for physical neural networks. It
provides a powerful mathematical criterion that imposes device constraints,
detailing how inputs should be encoded in the tunable parameters of the
physical system. Based on this result, we propose a scalable architecture using
free-space optics that is provably universal and achieves high accuracy on
image classification tasks. Further, by combining the theorem with temporal
multiplexing, we present a route to potentially huge effective system sizes in
highly practical but poorly scalable on-chip photonic devices. Our theorem and
scaling methods apply beyond optical systems and inform the design of a wide
class of universal, energy-efficient physical neural networks, justifying
further efforts in their development.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [506] [Workflow for High-Fidelity Dynamic Analysis of Structures with Pile Foundation](https://arxiv.org/abs/2509.05675)
*Amin Pakzad,Pedro Arduino,Wenyang Zhang,Ertugrul Tacirouglu*

Main category: math.NA

TL;DR: This paper presents a standardized workflow for dynamic soil-structure interaction analyses involving pile foundations, including methods for load application, wave absorption, efficient computation, and interface modeling.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of a standardized workflow for high-fidelity simulations in soil-structure interaction analysis.

Method: The study introduces a workflow using methods like Domain Reduction for loading, Perfectly Matched Layer elements for wave absorption, Embedded interface elements for soil-structure interaction, and domain decomposition for computational efficiency.

Result: The results show the Domain Reduction Method reduced simulation size effectively, Perfectly Matched Layer elements modeled infinite domains accurately, Embedded Interface elements efficiently simulated connections, and the overall workflow demonstrated high reliability.

Conclusion: The study provides a foundational guide for soil-structure interaction simulations, paving the way for research into more complex scenarios.

Abstract: The demand for high-fidelity numerical simulations in soil-structure
interaction analysis is on the rise, yet a standardized workflow to guide the
creation of such simulations remains elusive. This paper aims to bridge this
gap by presenting a step-by-step guideline proposing a workflow for dynamic
analysis of structures with pile foundations. The proposed workflow encompasses
instructions on how to use Domain Reduction Method for loading, Perfectly
Matched Layer elements for wave absorption, soil-structure interaction modeling
using Embedded interface elements, and domain decomposition for efficient use
of processing units. Through a series of numerical simulations, we showcase the
practical application of this workflow. Our results reveal the efficacy of the
Domain Reduction Method in reducing simulation size without compromising model
fidelity, show the precision of Perfectly Matched Layer elements in modeling
infinite domains, highlight the efficiency of Embedded Interface elements in
establishing connections between structures and the soil domain, and
demonstrate the overall effectiveness of the proposed workflow in conducting
high-fidelity simulations. While our study focuses on simplified geometries and
loading scenarios, it serves as a foundational framework for future research
endeavors aimed at exploring more intricate structural configurations and
dynamic loading conditions

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [507] [ProtSAE: Disentangling and Interpreting Protein Language Models via Semantically-Guided Sparse Autoencoders](https://arxiv.org/abs/2509.05309)
*Xiangyu Liu,Haodi Lei,Yi Liu,Yang Liu,Wei Hu*

Main category: q-bio.QM

TL;DR: ProtSAE introduces a semantically-guided sparse autoencoder to improve interpretability in protein language models by disentangling semantic attributes using both datasets and domain knowledge, outperforming existing methods in interpretability and reconstruction fidelity.


<details>
  <summary>Details</summary>
Motivation: To address the issue of semantic entanglement in sparse autoencoders applied to protein language models, which hinders reliable interpretation and manipulation of model behaviors.

Method: The paper introduces ProtSAE, a semantically-guided sparse autoencoder, that incorporates annotation datasets and domain knowledge during training to achieve better semantic disentanglement.

Result: ProtSAE demonstrates superior biological interpretability of learned features, retains high reconstruction accuracy, and achieves better performance in probing tasks compared to existing approaches.

Conclusion: ProtSAE effectively mitigates semantic entanglement, enhances interpretability in protein language models, and shows potential in guiding downstream generation tasks while maintaining reconstruction fidelity.

Abstract: Sparse Autoencoder (SAE) has emerged as a powerful tool for mechanistic
interpretability of large language models. Recent works apply SAE to protein
language models (PLMs), aiming to extract and analyze biologically meaningful
features from their latent spaces. However, SAE suffers from semantic
entanglement, where individual neurons often mix multiple nonlinear concepts,
making it difficult to reliably interpret or manipulate model behaviors. In
this paper, we propose a semantically-guided SAE, called ProtSAE. Unlike
existing SAE which requires annotation datasets to filter and interpret
activations, we guide semantic disentanglement during training using both
annotation datasets and domain knowledge to mitigate the effects of entangled
attributes. We design interpretability experiments showing that ProtSAE learns
more biologically relevant and interpretable hidden features compared to
previous methods. Performance analyses further demonstrate that ProtSAE
maintains high reconstruction fidelity while achieving better results in
interpretable probing. We also show the potential of ProtSAE in steering PLMs
for downstream generation tasks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [508] [Introduction to Number Theoretic Transform](https://arxiv.org/abs/2509.05884)
*Banhirup Sengupta,Peenal Gupta,Souvik Sengupta*

Main category: cs.CR

TL;DR: The paper explores the use of the Number Theoretic Transform (NTT), a discrete variant of the Fourier Transform, in lattice-based cryptography and homomorphic encryption, emphasizing its efficiency in polynomial multiplication.


<details>
  <summary>Details</summary>
Motivation: To leverage the mathematical framework of NTT for optimizing computational processes in post-quantum cryptography and homomorphic encryption.

Method: Introduced concepts such as cyclic and negacyclic convolutions alongside NTT and its inverse. Fast versions of NTT were presented to improve polynomial multiplication.

Result: NTT reduces polynomial multiplication complexity from quadratic to quasilinear, demonstrating its utility in cryptographic applications.

Conclusion: NTT emerges as an efficient computational tool, particularly in lattice-based cryptography, offering significant performance improvements and mathematical elegance.

Abstract: The Number Theoretic Transform (NTT) can be regarded as a variant of the
Discrete Fourier Transform. NTT has been quite a powerful mathematical tool in
developing Post-Quantum Cryptography and Homomorphic Encryption. The Fourier
Transform essentially decomposes a signal into its frequencies. They are
traditionally sine or cosine waves. NTT works more over groups or finite fields
rather than on a continuous signal and polynomials work as the analog of sine
waves in case of NTT. Fast Fourier Trnasform (FFT) style NTT or fast NTT has
been proven to be useful in lattice-based cryptography due to its ability to
reduce the complexity of polynomial multiplication from quadratic to
quasilinear. We have introduced the concepts of cyclic, negacyclic convolutions
along with NTT and its inverse and their fast versions.

</details>


### [509] [VehiclePassport: A GAIA-X-Aligned, Blockchain-Anchored Privacy-Preserving, Zero-Knowledge Digital Passport for Smart Vehicles](https://arxiv.org/abs/2509.06133)
*Pradyumna Kaushal*

Main category: cs.CR

TL;DR: This paper introduces VehiclePassport, a blockchain and privacy-forward digital passport for securely managing vehicle lifecycle records.


<details>
  <summary>Details</summary>
Motivation: Current vehicle lifecycle records are fragmented, difficult to verify, and susceptible to fraud, creating inefficiencies and risks.

Method: VehiclePassport leverages blockchain for immutable event anchoring and zero-knowledge proofs (ZKPs) for privacy-preserving verification, supported by an open-source tech stack.

Result: The system provides a cost-efficient (<$0.02/event), scalable, and fast (<10ms validation) solution anchored on Polygon zkEVM, compatible with millions of vehicles.

Conclusion: VehiclePassport offers a GDPR-compliant, trustless system for global mobility data markets, eliminating paper processes and enabling applications in insurance, resale, and regulation.

Abstract: Modern vehicles accumulate fragmented lifecycle records across OEMs, owners,
and service centers that are difficult to verify and prone to fraud. We propose
VehiclePassport, a GAIA-X-aligned digital passport anchored on blockchain with
zero-knowledge proofs (ZKPs) for privacy-preserving verification.
VehiclePassport immutably commits to manufacturing, telemetry, and service
events while enabling selective disclosure via short-lived JWTs and Groth16
proofs. Our open-source reference stack anchors hashes on Polygon zkEVM at
<$0.02 per event, validates proofs in <10 ms, and scales to millions of
vehicles. This architecture eliminates paper-based KYC, ensures GDPR-compliant
traceability, and establishes a trustless foundation for insurance, resale, and
regulatory applications in global mobility data markets.

</details>


### [510] [FuzzBox: Blending Fuzzing into Emulation for Binary-Only Embedded Targets](https://arxiv.org/abs/2509.05643)
*Carmine Cesarano,Roberto Natella*

Main category: cs.CR

TL;DR: FuzzBox enables coverage-guided fuzzing without source code or proprietary compiler requirements by dynamically instrumenting code in a virtualized environment.


<details>
  <summary>Details</summary>
Motivation: Address the challenges posed by closed-source, proprietary toolchains, and lack of source code in applying coverage-guided fuzzing to industrial systems.

Method: Integrates emulation and fuzzing by dynamically instrumenting code execution in a virtualized environment, eliminating the need for recompilation or hardware-specific dependencies.

Result: Demonstrated effectiveness on a proprietary MILS hypervisor and extended applicability to commercial IoT firmware.

Conclusion: FuzzBox is a versatile, source-independent fuzzing framework suitable for industrial systems and IoT environments.

Abstract: Coverage-guided fuzzing has been widely applied to address zero-day
vulnerabilities in general-purpose software and operating systems. This
approach relies on instrumenting the target code at compile time. However,
applying it to industrial systems remains challenging, due to proprietary and
closed-source compiler toolchains and lack of access to source code. FuzzBox
addresses these limitations by integrating emulation with fuzzing: it
dynamically instruments code during execution in a virtualized environment, for
the injection of fuzz inputs, failure detection, and coverage analysis, without
requiring source code recompilation and hardware-specific dependencies. We show
the effectiveness of FuzzBox through experiments in the context of a
proprietary MILS (Multiple Independent Levels of Security) hypervisor for
industrial applications. Additionally, we analyze the applicability of FuzzBox
across commercial IoT firmware, showcasing its broad portability.

</details>


### [511] [Towards Log Analysis with AI Agents: Cowrie Case Study](https://arxiv.org/abs/2509.05306)
*Enis Karaarslan,Esin Güler,Efe Emir Yüce,Cagatay Coban*

Main category: cs.CR

TL;DR: This study develops an AI-driven approach for automated analysis of Cowrie honeypot logs to tackle issues of scalability and efficiency in cybersecurity.


<details>
  <summary>Details</summary>
Motivation: The research aims to address the challenge of analyzing overwhelming, unstructured, and heterogeneous attack logs collected by honeypots, which limits advancements in cybersecurity.

Method: It utilizes AI agents to parse, summarize, and extract insights from raw logs collected by Cowrie honeypots, creating a lightweight and automated pipeline for log processing.

Result: Preliminary findings show that the proposed approach successfully minimizes manual effort while identifying attack patterns effectively.

Conclusion: The pipeline demonstrates potential as a foundational step towards advanced autonomous cybersecurity solutions, emphasizing the importance of secure AI deployment.

Abstract: The scarcity of real-world attack data significantly hinders progress in
cybersecurity research and education. Although honeypots like Cowrie
effectively collect live threat intelligence, they generate overwhelming
volumes of unstructured and heterogeneous logs, rendering manual analysis
impractical. As a first step in our project on secure and efficient AI
automation, this study explores the use of AI agents for automated log
analysis. We present a lightweight and automated approach to process Cowrie
honeypot logs. Our approach leverages AI agents to intelligently parse,
summarize, and extract insights from raw data, while also considering the
security implications of deploying such an autonomous system. Preliminary
results demonstrate the pipeline's effectiveness in reducing manual effort and
identifying attack patterns, paving the way for more advanced autonomous
cybersecurity analysis in future work.

</details>


### [512] [Large Language Model Integration with Reinforcement Learning to Augment Decision-Making in Autonomous Cyber Operations](https://arxiv.org/abs/2509.05311)
*Konur Tholl,François Rivest,Mariam El Mezouar,Ranwa Al Mallah*

Main category: cs.CR

TL;DR: The paper explores integrating a Large Language Model (LLM) with reinforcement learning (RL) in the cybersecurity domain to reduce negative exploratory actions and improve early training performance.


<details>
  <summary>Details</summary>
Motivation: Traditional RL agents in cybersecurity learn from scratch, which requires executing undesirable actions to understand their outcomes, potentially leading to inefficiencies and risks.

Method: The authors propose integrating an LLM pretrained on cybersecurity data with an RL agent, allowing the RL agent to leverage external knowledge for more informed decision-making during training.

Result: The LLM-guided RL agent achieves over 2x higher rewards during early training and converges to an optimal policy approximately 4,500 episodes faster compared to the baseline method.

Conclusion: Integrating an LLM with an RL agent enhances early training performance and reduces reliance on risky exploratory actions, demonstrating its potential to improve decision-making in cybersecurity environments.

Abstract: Reinforcement Learning (RL) has shown great potential for autonomous
decision-making in the cybersecurity domain, enabling agents to learn through
direct environment interaction. However, RL agents in Autonomous Cyber
Operations (ACO) typically learn from scratch, requiring them to execute
undesirable actions to learn their consequences. In this study, we integrate
external knowledge in the form of a Large Language Model (LLM) pretrained on
cybersecurity data that our RL agent can directly leverage to make informed
decisions. By guiding initial training with an LLM, we improve baseline
performance and reduce the need for exploratory actions with obviously negative
outcomes. We evaluate our LLM-integrated approach in a simulated cybersecurity
environment, and demonstrate that our guided agent achieves over 2x higher
rewards during early training and converges to a favorable policy approximately
4,500 episodes faster than the baseline.

</details>


### [513] [Backdoor Samples Detection Based on Perturbation Discrepancy Consistency in Pre-trained Language Models](https://arxiv.org/abs/2509.05318)
*Zuquan Peng,Jianming Fu,Lixin Zou,Li Zheng,Yanzhen Ren,Guojun Peng*

Main category: cs.CR

TL;DR: The paper introduces NETE, a novel method to detect backdoor attacks in pre-trained models without requiring extensive resources or model access.


<details>
  <summary>Details</summary>
Motivation: Pre-trained models are increasingly susceptible to backdoor attacks, and existing detection methods often demand significant resources, limiting their practicality.

Method: The proposed NETE method utilizes perturbation discrepancy consistency, log probability calculations from a pre-trained model, and automated mask-filling perturbations to identify backdoor samples, usable both pre- and post-training.

Result: Experimental results on four typical backdoor attacks and five large language model backdoor attacks show that NETE is more effective than existing zero-shot black-box detection methods.

Conclusion: NETE provides a practical and efficient means to detect backdoor attacks, reducing reliance on computational and data resources.

Abstract: The use of unvetted third-party and internet data renders pre-trained models
susceptible to backdoor attacks. Detecting backdoor samples is critical to
prevent backdoor activation during inference or injection during training.
However, existing detection methods often require the defender to have access
to the poisoned models, extra clean samples, or significant computational
resources to detect backdoor samples, limiting their practicality. To address
this limitation, we propose a backdoor sample detection method based on
perturbatio\textbf{N} discr\textbf{E}pancy consis\textbf{T}ency
\textbf{E}valuation (\NETE). This is a novel detection method that can be used
both pre-training and post-training phases. In the detection process, it only
requires an off-the-shelf pre-trained model to compute the log probability of
samples and an automated function based on a mask-filling strategy to generate
perturbations. Our method is based on the interesting phenomenon that the
change in perturbation discrepancy for backdoor samples is smaller than that
for clean samples. Based on this phenomenon, we use curvature to measure the
discrepancy in log probabilities between different perturbed samples and input
samples, thereby evaluating the consistency of the perturbation discrepancy to
determine whether the input sample is a backdoor sample. Experiments conducted
on four typical backdoor attacks and five types of large language model
backdoor attacks demonstrate that our detection strategy outperforms existing
zero-shot black-box detection methods.

</details>


### [514] [Zero-Knowledge Proofs in Sublinear Space](https://arxiv.org/abs/2509.05326)
*Logan Nye*

Main category: cs.CR

TL;DR: The paper introduces a sublinear-space ZKP prover reducing memory usage from linear to O(sqrt(T)), enabling on-device applications.


<details>
  <summary>Details</summary>
Motivation: The aim is to address the impractical memory requirements of ZKP systems, which hinder their use in resource-constrained devices and large-scale computations.

Method: The authors leverage an equivalence to the Tree Evaluation problem and utilize a space-efficient tree-evaluation algorithm to design a streaming prover.

Result: The proposed method decreases prover memory usage to O(sqrt(T)) while preserving proof size, verifier time, and security guarantees.

Conclusion: This sublinear-space approach facilitates on-device proving and broadens the applications of ZKP systems in decentralized systems, ML, and privacy technologies.

Abstract: Modern zero-knowledge proof (ZKP) systems, essential for privacy and
verifiable computation, suffer from a fundamental limitation: the prover
typically uses memory that scales linearly with the computation's trace length
T, making them impractical for resource-constrained devices and prohibitively
expensive for large-scale tasks. This paper overcomes this barrier by
constructing, to our knowledge, the first sublinear-space ZKP prover. Our core
contribution is an equivalence that reframes proof generation as an instance of
the classic Tree Evaluation problem. Leveraging a recent space-efficient
tree-evaluation algorithm, we design a streaming prover that assembles the
proof without ever materializing the full execution trace. The approach reduces
prover memory from linear in T to O(sqrt(T)) (up to O(log T) lower-order terms)
while preserving proof size, verifier time, and the transcript/security
guarantees of the underlying system. This enables a shift from specialized,
server-bound proving to on-device proving, opening applications in
decentralized systems, on-device machine learning, and privacy-preserving
technologies.

</details>


### [515] [ForensicsData: A Digital Forensics Dataset for Large Language Models](https://arxiv.org/abs/2509.05331)
*Youssef Chakir,Iyad Lahsen-Cherif*

Main category: cs.CR

TL;DR: This paper introduces ForensicsData, a large Question-Context-Answer (Q-C-A) dataset derived from malware analysis reports to address the lack of realistic datasets for digital forensics research.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to overcome challenges in digital forensic investigations, particularly the lack of sufficient public datasets due to ethical, legal, and privacy concerns.

Method: The dataset of over 5,000 Q-C-A triplets was created via a workflow that extracts structured data, transforms it using large language models (LLMs), and validates its quality through specialized evaluations.

Result: The dataset, ForensicsData, was validated for quality, and among the models used, Gemini 2 Flash showed the best performance in aligning generated content with forensic terminology.

Conclusion: ForensicsData is designed to advance the field of digital forensics by facilitating reproducible experiments and enhancing collaboration within the forensic research community.

Abstract: The growing complexity of cyber incidents presents significant challenges for
digital forensic investigators, especially in evidence collection and analysis.
Public resources are still limited because of ethical, legal, and privacy
concerns, even though realistic datasets are necessary to support research and
tool developments. To address this gap, we introduce ForensicsData, an
extensive Question-Context-Answer (Q-C-A) dataset sourced from actual malware
analysis reports. It consists of more than 5,000 Q-C-A triplets. A unique
workflow was used to create the dataset, which extracts structured data, uses
large language models (LLMs) to transform it into Q-C-A format, and then uses a
specialized evaluation process to confirm its quality. Among the models
evaluated, Gemini 2 Flash demonstrated the best performance in aligning
generated content with forensic terminology. ForensicsData aims to advance
digital forensics by enabling reproducible experiments and fostering
collaboration within the research community.

</details>


### [516] [Integrated Simulation Framework for Adversarial Attacks on Autonomous Vehicles](https://arxiv.org/abs/2509.05332)
*Christos Anagnostopoulos,Ioulia Kapsali,Alexandros Gkillas,Nikos Piperigkos,Aris S. Lalos*

Main category: cs.CR

TL;DR: The paper presents a novel open-source simulation framework to test autonomous vehicles (AVs) against adversarial attacks targeting their perception and communication layers.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the vulnerability of autonomous vehicles to adversarial attacks by providing better tools for testing resilience in multi-domain scenarios.

Method: The authors developed a unified simulation framework that integrates high-fidelity modeling of environments, traffic dynamics, V2X networking, and attacks on perception and communication layers, synchronized via a single configuration file.

Result: The framework effectively simulates adversarial conditions and reveals significant performance degradation in a state-of-the-art 3D object detector under realistic attack scenarios.

Conclusion: This framework provides a robust tool for evaluating and improving the resilience of autonomous vehicles to multi-domain adversarial attacks, facilitating safer AV deployment.

Abstract: Autonomous vehicles (AVs) rely on complex perception and communication
systems, making them vulnerable to adversarial attacks that can compromise
safety. While simulation offers a scalable and safe environment for robustness
testing, existing frameworks typically lack comprehensive supportfor modeling
multi-domain adversarial scenarios. This paper introduces a novel, open-source
integrated simulation framework designed to generate adversarial attacks
targeting both perception and communication layers of AVs. The framework
provides high-fidelity modeling of physical environments, traffic dynamics, and
V2X networking, orchestrating these components through a unified core that
synchronizes multiple simulators based on a single configuration file. Our
implementation supports diverse perception-level attacks on LiDAR sensor data,
along with communication-level threats such as V2X message manipulation and GPS
spoofing. Furthermore, ROS 2 integration ensures seamless compatibility with
third-party AV software stacks. We demonstrate the framework's effectiveness by
evaluating the impact of generated adversarial scenarios on a state-of-the-art
3D object detector, revealing significant performance degradation under
realistic conditions.

</details>


### [517] [Cross-Service Threat Intelligence in LLM Services using Privacy-Preserving Fingerprints](https://arxiv.org/abs/2509.05608)
*Waris Gill,Natalie Isak,Matthew Dressman*

Main category: cs.CR

TL;DR: BinaryShield introduces a system for secure sharing of attack fingerprints across compliance boundaries, mitigating privacy issues associated with LLM-related prompt injection attacks.


<details>
  <summary>Details</summary>
Motivation: Organizations suffer from a security gap due to the inability to share prompt injection attack data between LLM systems due to privacy regulations.

Method: BinaryShield combines PII redaction, semantic embedding, binary quantization, and randomized response mechanism to create non-invertible attack fingerprints that maintain privacy.

Result: BinaryShield achieved significant performance improvements with an F1-score of 0.94, outperforming SimHash (0.77), and offered substantial storage and speed improvements over dense embeddings.

Conclusion: BinaryShield provides a promising direction for cross-boundary threat intelligence sharing, addressing both privacy concerns and efficiency in detecting prompt injection attacks.

Abstract: The widespread deployment of LLMs across enterprise services has created a
critical security blind spot. Organizations operate multiple LLM services
handling billions of queries daily, yet regulatory compliance boundaries
prevent these services from sharing threat intelligence about prompt injection
attacks, the top security risk for LLMs. When an attack is detected in one
service, the same threat may persist undetected in others for months, as
privacy regulations prohibit sharing user prompts across compliance boundaries.
  We present BinaryShield, the first privacy-preserving threat intelligence
system that enables secure sharing of attack fingerprints across compliance
boundaries. BinaryShield transforms suspicious prompts through a unique
pipeline combining PII redaction, semantic embedding, binary quantization, and
randomized response mechanism to potentially generate non-invertible
fingerprints that preserve attack patterns while providing privacy. Our
evaluations demonstrate that BinaryShield achieves an F1-score of 0.94,
significantly outperforming SimHash (0.77), the privacy-preserving baseline,
while achieving 64x storage reduction and 38x faster similarity search compared
to dense embeddings.

</details>


### [518] [AI-in-the-Loop: Privacy Preserving Real-Time Scam Detection and Conversational Scambaiting by Leveraging LLMs and Federated Learning](https://arxiv.org/abs/2509.05362)
*Ismail Hossain,Sai Puppala,Sajedul Talukder,Md Jahangir Alam*

Main category: cs.CR

TL;DR: This paper introduces a privacy-preserving, real-time AI framework for detecting and disrupting scam conversations across digital platforms.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current reactive defenses that offer minimal protection during active scam interactions, thus reducing the effectiveness of combating phishing and impersonation scams.

Method: The system uses instruction-tuned AI with a safety-aware utility function to balance user engagement and harm minimization. It employs federated learning for continuous model updates without requiring raw data sharing.

Result: Experimental evaluations show high performance metrics like low perplexity, high engagement (~0.80), and minimal PII leakage (≤ 0.0085). Federated learning sustains model updates across 30 rounds without significant performance degradation, and differential privacy successfully secures data without impairing effectiveness.

Conclusion: This novel framework combines scam-baiting, privacy preservation, and safety moderation into a proactive defense paradigm, offering a significant step forward in identifying and preventing scams in real time.

Abstract: Scams exploiting real-time social engineering -- such as phishing,
impersonation, and phone fraud -- remain a persistent and evolving threat
across digital platforms. Existing defenses are largely reactive, offering
limited protection during active interactions. We propose a privacy-preserving,
AI-in-the-loop framework that proactively detects and disrupts scam
conversations in real time. The system combines instruction-tuned artificial
intelligence with a safety-aware utility function that balances engagement with
harm minimization, and employs federated learning to enable continual model
updates without raw data sharing. Experimental evaluations show that the system
produces fluent and engaging responses (perplexity as low as 22.3, engagement
$\approx$0.80), while human studies confirm significant gains in realism,
safety, and effectiveness over strong baselines. In federated settings, models
trained with FedAvg sustain up to 30 rounds while preserving high engagement
($\approx$0.80), strong relevance ($\approx$0.74), and low PII leakage
($\leq$0.0085). Even with differential privacy, novelty and safety remain
stable, indicating that robust privacy can be achieved without sacrificing
performance. The evaluation of guard models (LlamaGuard, LlamaGuard2/3,
MD-Judge) shows a straightforward pattern: stricter moderation settings reduce
the chance of exposing personal information, but they also limit how much the
model engages in conversation. In contrast, more relaxed settings allow longer
and richer interactions, which improve scam detection, but at the cost of
higher privacy risk. To our knowledge, this is the first framework to unify
real-time scam-baiting, federated privacy preservation, and calibrated safety
moderation into a proactive defense paradigm.

</details>


### [519] [Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs](https://arxiv.org/abs/2509.05367)
*Shei Pern Chua,Thai Zhen Leng,Teh Kai Jun,Xiao Li,Xiaolin Hu*

Main category: cs.CR

TL;DR: The study introduces 'TRIAL,' a framework utilizing ethical reasoning to bypass safeguards in advanced large language models.


<details>
  <summary>Details</summary>
Motivation: To explore limitations in AI safety, particularly focusing on how enhanced reasoning abilities in LLMs may inadvertently allow for covert security vulnerabilities.

Method: The TRIAL framework uses ethical dilemmas inspired by the trolley problem to embed adversarial goals within multi-turn jailbreak strategies.

Result: TRIAL achieved high success rates in bypassing safeguards on both open-source and closed-source LLMs.

Conclusion: As LLMs enhance their reasoning capabilities, current alignment and safety oversight strategies may be insufficient to mitigate emerging, context-aware security vulnerabilities.

Abstract: Large language models (LLMs) have undergone safety alignment efforts to
mitigate harmful outputs. However, as LLMs become more sophisticated in
reasoning, their intelligence may introduce new security risks. While
traditional jailbreak attacks relied on singlestep attacks, multi-turn
jailbreak strategies that adapt dynamically to context remain underexplored. In
this work, we introduce TRIAL (Trolley-problem Reasoning for Interactive Attack
Logic), a framework that leverages LLMs ethical reasoning to bypass their
safeguards. TRIAL embeds adversarial goals within ethical dilemmas modeled on
the trolley problem. TRIAL demonstrates high jailbreak success rates towards
both open and close-source models. Our findings underscore a fundamental
limitation in AI safety: as models gain advanced reasoning abilities, the
nature of their alignment may inadvertently allow for more covert security
vulnerabilities to be exploited. TRIAL raises an urgent need in reevaluating
safety alignment oversight strategies, as current safeguards may prove
insufficient against context-aware adversarial attack.

</details>


### [520] [Privacy Preservation and Identity Tracing Prevention in AI-Driven Eye Tracking for Interactive Learning Environments](https://arxiv.org/abs/2509.05376)
*Abdul Rehman,Are Dæhlen,Ilona Heldal,Jerry Chun-wei Lin*

Main category: cs.CR

TL;DR: The paper introduces a human-centered framework to address privacy risks in AI-powered eye-tracking systems by preventing identity backtracking while enabling diagnostic classification.


<details>
  <summary>Details</summary>
Motivation: Eye-tracking technology captures sensitive data, raising privacy concerns. The paper aims to address these privacy issues without compromising its educational and diagnostic potential.

Method: The authors propose a two-stage privacy-preserving framework, leveraging real-time data anonymization, ethical design principles, regulatory compliance, and Federated Learning.

Result: Phase 1 demonstrates high accuracy in diagnostic predictions and ID randomization across different scenarios, while phase 2 prevents backtracking and implements secure identity management with dummy IDs.

Conclusion: The proposed framework effectively balances privacy protection and diagnostic functionality, ensuring secure use of eye-tracking in educational environments.

Abstract: Eye-tracking technology can aid in understanding neurodevelopmental disorders
and tracing a person's identity. However, this technology poses a significant
risk to privacy, as it captures sensitive information about individuals and
increases the likelihood that data can be traced back to them. This paper
proposes a human-centered framework designed to prevent identity backtracking
while preserving the pedagogical benefits of AI-powered eye tracking in
interactive learning environments. We explore how real-time data anonymization,
ethical design principles, and regulatory compliance (such as GDPR) can be
integrated to build trust and transparency. We first demonstrate the potential
for backtracking student IDs and diagnoses in various scenarios using serious
game-based eye-tracking data. We then provide a two-stage privacy-preserving
framework that prevents participants from being tracked while still enabling
diagnostic classification. The first phase covers four scenarios: I) Predicting
disorder diagnoses based on different game levels. II) Predicting student IDs
based on different game levels. III) Predicting student IDs based on randomized
data. IV) Utilizing K-Means for out-of-sample data. In the second phase, we
present a two-stage framework that preserves privacy. We also employ Federated
Learning (FL) across multiple clients, incorporating a secure identity
management system with dummy IDs and administrator-only access controls. In the
first phase, the proposed framework achieved 99.3% accuracy for scenario 1, 63%
accuracy for scenario 2, and 99.7% accuracy for scenario 3, successfully
identifying and assigning a new student ID in scenario 4. In phase 2, we
effectively prevented backtracking and established a secure identity management
system with dummy IDs and administrator-only access controls, achieving an
overall accuracy of 99.40%.

</details>


### [521] [ThreatGPT: An Agentic AI Framework for Enhancing Public Safety through Threat Modeling](https://arxiv.org/abs/2509.05379)
*Sharif Noor Zisad,Ragib Hasan*

Main category: cs.CR

TL;DR: The paper introduces ThreatGPT, an AI assistant that helps analyze and address threats in public safety systems using frameworks like STRIDE and NIST without requiring deep cybersecurity expertise.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of smart city systems, such as traffic control and emergency response, poses heightened security risks that can directly impact real people's lives.

Method: ThreatGPT leverages few-shot learning to act as a collaborative partner in identifying security threats, allowing users to describe system components and use analyses like STRIDE, MITRE ATT&CK, and others.

Result: ThreatGPT provides contextually-informed, intelligent threat modeling that considers possible vulnerabilities, attack approaches, and preventive measures.

Conclusion: ThreatGPT empowers users, regardless of their cybersecurity expertise, to understand and address security threats to public safety systems more effectively and confidently.

Abstract: As our cities and communities become smarter, the systems that keep us safe,
such as traffic control centers, emergency response networks, and public
transportation, also become more complex. With this complexity comes a greater
risk of security threats that can affect not just machines but real people's
lives. To address this challenge, we present ThreatGPT, an agentic Artificial
Intelligence (AI) assistant built to help people whether they are engineers,
safety officers, or policy makers to understand and analyze threats in public
safety systems. Instead of requiring deep cybersecurity expertise, it allows
users to simply describe the components of a system they are concerned about,
such as login systems, data storage, or communication networks. Then, with the
click of a button, users can choose how they want the system to be analyzed by
using popular frameworks such as STRIDE, MITRE ATT&CK, CVE reports, NIST, or
CISA. ThreatGPT is unique because it does not just provide threat information,
but rather it acts like a knowledgeable partner. Using few-shot learning, the
AI learns from examples and generates relevant smart threat models. It can
highlight what might go wrong, how attackers could take advantage, and what can
be done to prevent harm. Whether securing a city's infrastructure or a local
health service, this tool adapts to users' needs. In simple terms, ThreatGPT
brings together AI and human judgment to make our public systems safer. It is
designed not just to analyze threats, but to empower people to understand and
act on them, faster, smarter, and with more confidence.

</details>


### [522] [An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and Detection](https://arxiv.org/abs/2509.06920)
*Haywood Gelman,John D. Hastings,David Kenley*

Main category: cs.CR

TL;DR: This study uses LLM Claude Sonnet 3.7 to synthesize syslog messages for insider threat detection, outperforming GPT-4o in accuracy metrics.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to insider threat detection rely on static datasets with limited accessibility, reducing the adaptability and effectiveness of detection models.

Method: The researchers employ Claude Sonnet 3.7 to generate synthetic syslog datasets that mimic real-world distributions (with 1% insider threats) and evaluate its detection performance against GPT-4o using metrics like precision, recall, MCC, and ROC AUC.

Result: Claude Sonnet 3.7 showed superior performance over GPT-4o by reducing false positives and achieving higher detection accuracy across most metrics.

Conclusion: The paper demonstrates the potential for LLMs to enhance insider threat detection and enable reliable synthetic dataset generation for dynamic analysis models.

Abstract: Insider threats are a growing organizational problem due to the complexity of
identifying their technical and behavioral elements. A large research body is
dedicated to the study of insider threats from technological, psychological,
and educational perspectives. However, research in this domain has been
generally dependent on datasets that are static and limited access which
restricts the development of adaptive detection models. This study introduces a
novel, ethically grounded approach that uses the large language model (LLM)
Claude Sonnet 3.7 to dynamically synthesize syslog messages, some of which
contain indicators of insider threat scenarios. The messages reflect real-world
data distributions by being highly imbalanced (1% insider threats). The syslogs
were analyzed for insider threats by both Claude Sonnet 3.7 and GPT-4o, with
their performance evaluated through statistical metrics including precision,
recall, MCC, and ROC AUC. Sonnet 3.7 consistently outperformed GPT-4o across
nearly all metrics, particularly in reducing false alarms and improving
detection accuracy. The results show strong promise for the use of LLMs in
synthetic dataset generation and insider threat detection.

</details>


### [523] [Behind the Mask: Benchmarking Camouflaged Jailbreaks in Large Language Models](https://arxiv.org/abs/2509.05471)
*Youjia Zheng,Mohammad Zandsalimy,Shanu Sushmita*

Main category: cs.CR

TL;DR: This paper addresses the vulnerability of Large Language Models (LLMs) to camouflaged jailbreaking attempts, presenting a dataset and evaluation framework to analyze these threats.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the increasing threat of camouflaged jailbreak prompts, which circumvent existing safety mechanisms and exploit language ambiguity in LLMs.

Method: The authors created a benchmark dataset with 500 examples (400 harmful and 100 benign prompts) and developed a framework to evaluate harmfulness across seven dimensions.

Result: The study found LLMs exhibit reduced safety and performance when exposed to camouflaged jailbreak prompts compared to benign inputs.

Conclusion: Current LLM safety mechanisms are insufficient to handle camouflaged jailbreaks, necessitating advanced and adaptive security approaches for robust deployment.

Abstract: Large Language Models (LLMs) are increasingly vulnerable to a sophisticated
form of adversarial prompting known as camouflaged jailbreaking. This method
embeds malicious intent within seemingly benign language to evade existing
safety mechanisms. Unlike overt attacks, these subtle prompts exploit
contextual ambiguity and the flexible nature of language, posing significant
challenges to current defense systems. This paper investigates the construction
and impact of camouflaged jailbreak prompts, emphasizing their deceptive
characteristics and the limitations of traditional keyword-based detection
methods. We introduce a novel benchmark dataset, Camouflaged Jailbreak Prompts,
containing 500 curated examples (400 harmful and 100 benign prompts) designed
to rigorously stress-test LLM safety protocols. In addition, we propose a
multi-faceted evaluation framework that measures harmfulness across seven
dimensions: Safety Awareness, Technical Feasibility, Implementation Safeguards,
Harmful Potential, Educational Value, Content Quality, and Compliance Score.
Our findings reveal a stark contrast in LLM behavior: while models demonstrate
high safety and content quality with benign inputs, they exhibit a significant
decline in performance and safety when confronted with camouflaged jailbreak
attempts. This disparity underscores a pervasive vulnerability, highlighting
the urgent need for more nuanced and adaptive security strategies to ensure the
responsible and robust deployment of LLMs in real-world applications.

</details>


### [524] [Privacy-Preserving Offloading for Large Language Models in 6G Vehicular Networks](https://arxiv.org/abs/2509.05320)
*Ikhlasse Badidi,Nouhaila El Khiyaoui,Aya Riany,Badr Ben Elallid,Amine Abouaomar*

Main category: cs.CR

TL;DR: This paper introduces a framework that merges federated learning and differential privacy to allow safe offloading of large language model computations in 6G vehicular networks.


<details>
  <summary>Details</summary>
Motivation: While integrating large language models (LLMs) into 6G vehicular networks promises advancements, it raises privacy concerns due to sensitive user data being exposed during computation offloading.

Method: The paper introduces a privacy-preserving hybrid framework combining federated learning and differential privacy. It includes a privacy-aware task partitioning algorithm and a secure communication protocol for model updates.

Result: The framework achieves 75% accuracy, with only a 2-3% drop from non-privacy-preserving methods, and ensures privacy with an optimal budget of ε = 0.8. Communication overhead remains low at 2.1MB per round.

Conclusion: The framework ensures efficient and privacy-preserving computation offloading in resource-limited vehicular networks, maintaining a strong trade-off between privacy and performance.

Abstract: The integration of Large Language Models (LLMs) in 6G vehicular networks
promises unprecedented advancements in intelligent transportation systems.
However, offloading LLM computations from vehicles to edge infrastructure poses
significant privacy risks, potentially exposing sensitive user data. This paper
presents a novel privacy-preserving offloading framework for LLM-integrated
vehicular networks. We introduce a hybrid approach combining federated learning
(FL) and differential privacy (DP) techniques to protect user data while
maintaining LLM performance. Our framework includes a privacy-aware task
partitioning algorithm that optimizes the trade-off between local and edge
computation, considering both privacy constraints and system efficiency. We
also propose a secure communication protocol for transmitting model updates and
aggregating results across the network. Experimental results demonstrate that
our approach achieves 75\% global accuracy with only a 2-3\% reduction compared
to non-privacy-preserving methods, while maintaining DP guarantees with an
optimal privacy budget of $\varepsilon = 0.8$. The framework shows stable
communication overhead of approximately 2.1MB per round with computation
comprising over 90\% of total processing time, validating its efficiency for
resource-constrained vehicular environments.

</details>


### [525] [Ensembling Membership Inference Attacks Against Tabular Generative Models](https://arxiv.org/abs/2509.05350)
*Joshua Ward,Yuxuan Yang,Chi-Hua Wang,Guang Cheng*

Main category: cs.CR

TL;DR: The paper studies the effectiveness of Membership Inference Attacks (MIAs) for auditing synthetic data privacy, finding no single dominant method across diverse scenarios and proposing ensemble MIAs as a more robust solution.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of selecting the most effective Membership Inference Attack (MIA) for auditing synthetic data privacy, given that there is no a priori guarantee of any single method being the best option.

Method: The authors frame the problem as a decision theoretic challenge under uncertainty and conduct the largest benchmark study on synthetic data privacy, evaluating various MIA methods and introducing ensemble MIAs.

Result: They find that no single MIA is strictly better across diverse models and datasets. Ensemble MIAs, especially unsupervised ensembles, perform more robustly and minimize regret compared to individual attacks.

Conclusion: Ensemble MIAs provide a practical solution for auditors, offering better robustness and adaptability in realistic privacy threat scenarios involving synthetic data.

Abstract: Membership Inference Attacks (MIAs) have emerged as a principled framework
for auditing the privacy of synthetic data generated by tabular generative
models, where many diverse methods have been proposed that each exploit
different privacy leakage signals. However, in realistic threat scenarios, an
adversary must choose a single method without a priori guarantee that it will
be the empirically highest performing option. We study this challenge as a
decision theoretic problem under uncertainty and conduct the largest synthetic
data privacy benchmark to date. Here, we find that no MIA constitutes a
strictly dominant strategy across a wide variety of model architectures and
dataset domains under our threat model. Motivated by these findings, we propose
ensemble MIAs and show that unsupervised ensembles built on individual attacks
offer empirically more robust, regret-minimizing strategies than individual
attacks.

</details>


### [526] [SEASONED: Semantic-Enhanced Self-Counterfactual Explainable Detection of Adversarial Exploiter Contracts](https://arxiv.org/abs/2509.05681)
*Xng Ai,Shudan Lin,Zecheng Li,Kai Zhou,Bixin Li,Bin Xiao*

Main category: cs.CR

TL;DR: The paper introduces SEASONED, a framework for detecting adversarial exploit contracts (AECs) in decentralized finance, focusing on interpretability, robustness, and efficiency.


<details>
  <summary>Details</summary>
Motivation: There is a need for effective methods to detect adversarial exploit contracts in decentralized finance, as current approaches often fail to capture semantic dependencies and lack interpretability.

Method: SEASONED uses semantic relation graphs (SRGs) derived from contract bytecode and a self-counterfactual explainable detector (SCFED) to classify SRGs and generate clear explanations of attack logic.

Result: SEASONED demonstrates strong performance in AEC detection, robustness, generalizability, and data efficiency, validated through theory and experiments.

Conclusion: SEASONED provides an effective and interpretable solution for identifying AECs, filling critical knowledge gaps and advancing research through its dataset release.

Abstract: Decentralized Finance (DeFi) attacks have resulted in significant losses,
often orchestrated through Adversarial Exploiter Contracts (AECs) that exploit
vulnerabilities in victim smart contracts. To proactively identify such
threats, this paper targets the explainable detection of AECs.
  Existing detection methods struggle to capture semantic dependencies and lack
interpretability, limiting their effectiveness and leaving critical knowledge
gaps in AEC analysis. To address these challenges, we introduce SEASONED, an
effective, self-explanatory, and robust framework for AEC detection.
  SEASONED extracts semantic information from contract bytecode to construct a
semantic relation graph (SRG), and employs a self-counterfactual explainable
detector (SCFED) to classify SRGs and generate explanations that highlight the
core attack logic. SCFED further enhances robustness, generalizability, and
data efficiency by extracting representative information from these
explanations. Both theoretical analysis and experimental results demonstrate
the effectiveness of SEASONED, which showcases outstanding detection
performance, robustness, generalizability, and data efficiency learning
ability. To support further research, we also release a new dataset of 359
AECs.

</details>


### [527] [Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated](https://arxiv.org/abs/2509.05739)
*Hanna Foerster,Ilia Shumailov,Yiren Zhao,Harsh Chaudhari,Jamie Hayes,Robert Mullins,Yarin Gal*

Main category: cs.CR

TL;DR: The paper analyzes "decomposed reasoning poison" attacks on advanced LLMs, where poison affects reasoning paths but not prompts or final answers. Models display robustness, recovering from such manipulations. 


<details>
  <summary>Details</summary>
Motivation: Attackers could circumvent traditional methods by targeting the reasoning paths within LLMs rather than the final outputs, posing new challenges for research on LLM security.

Method: The authors focus on modifying reasoning paths (chain-of-thought) to poison models, keeping prompts and answers untampered, and testing how effective these attacks are.

Result: The study finds that activating decomposed reasoning poison to alter final answers proves difficult; models often recover from backdoor interruptions through reasoning.

Conclusion: Advanced LLMs show robustness against reasoning path-based poisoning attacks, highlighting the strength of their emergent reasoning and separation between thought processes and answer formulation.

Abstract: Early research into data poisoning attacks against Large Language Models
(LLMs) demonstrated the ease with which backdoors could be injected. More
recent LLMs add step-by-step reasoning, expanding the attack surface to include
the intermediate chain-of-thought (CoT) and its inherent trait of decomposing
problems into subproblems. Using these vectors for more stealthy poisoning, we
introduce ``decomposed reasoning poison'', in which the attacker modifies only
the reasoning path, leaving prompts and final answers clean, and splits the
trigger across multiple, individually harmless components.
  Fascinatingly, while it remains possible to inject these decomposed poisons,
reliably activating them to change final answers (rather than just the CoT) is
surprisingly difficult. This difficulty arises because the models can often
recover from backdoors that are activated within their thought processes.
Ultimately, it appears that an emergent form of backdoor robustness is
originating from the reasoning capabilities of these advanced LLMs, as well as
from the architectural separation between reasoning and final answer
generation.

</details>


### [528] [Tell-Tale Watermarks for Explanatory Reasoning in Synthetic Media Forensics](https://arxiv.org/abs/2509.05753)
*Ching-Chun Chang,Isao Echizen*

Main category: cs.CR

TL;DR: This paper presents a system of interpretable watermarks to track and analyze transformations in synthetic media, aiding forensic inquiries into digital fabrication.


<details>
  <summary>Details</summary>
Motivation: With synthetic media increasingly compromising trust in digital spaces and complicating forensic analysis, there is a need to trace the generation process of altered digital content.

Method: The researchers developed a watermarking system tailored for different types of media transformations, enabling interpretation of traces left by edits without being robust or fragile.

Result: The experimental evaluation shows that the proposed watermarking system is valid in terms of fidelity, synchronicity, and traceability under varying transformations.

Conclusion: Tell-tale watermarks can provide interpretable evidence and aid in reconstructing the generation chains of synthetic media, contributing to forensic applications in combating digital fabrication and misinformation.

Abstract: The rise of synthetic media has blurred the boundary between reality and
fabrication under the evolving power of artificial intelligence, fueling an
infodemic that erodes public trust in cyberspace. For digital imagery, a
multitude of editing applications further complicates the forensic analysis,
including semantic edits that alter content, photometric adjustments that
recalibrate colour characteristics, and geometric projections that reshape
viewpoints. Collectively, these transformations manipulate and control
perceptual interpretation of digital imagery. This susceptibility calls for
forensic enquiry into reconstructing the chain of events, thereby revealing
deeper evidential insight into the presence or absence of criminal intent. This
study seeks to address an inverse problem of tracing the underlying generation
chain that gives rise to the observed synthetic media. A tell-tale watermarking
system is developed for explanatory reasoning over the nature and extent of
transformations across the lifecycle of synthetic media. Tell-tale watermarks
are tailored to different classes of transformations, responding in a manner
that is neither strictly robust nor fragile but instead interpretable. These
watermarks function as reference clues that evolve under the same
transformation dynamics as the carrier media, leaving interpretable traces when
subjected to transformations. Explanatory reasoning is then performed to infer
the most plausible account across the combinatorial parameter space of
composite transformations. Experimental evaluations demonstrate the validity of
tell-tale watermarking with respect to fidelity, synchronicity and
traceability.

</details>


### [529] [Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based Agentic System](https://arxiv.org/abs/2509.05755)
*Yu Liu,Yuchong Xie,Mingyu Luo,Zesen Liu,Zhixiang Zhang,Kaikai Zhang,Zongjie Li,Ping Chen,Shuai Wang,Dongdong She*

Main category: cs.CR

TL;DR: This paper highlights security vulnerabilities in Tool Invocation Prompts (TIPs) used in LLM-based agentic systems, which can be exploited for attacks like remote code execution (RCE) and denial of service (DoS). It proposes defense mechanisms to improve TIP security.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this study is to address the overlooked security risks associated with TIPs in LLM-based agentic systems, which are critical for securing tool interactions and system functionality.

Method: This research employs a systematic TIP exploitation workflow (TEW) to investigate vulnerabilities and demonstrate hijacking of external tool behavior via manipulated tool invocations.

Result: The study reveals significant vulnerabilities in major LLM-based systems like Cursor and Claude Code, showing how TIPs can be exploited for RCE and DoS attacks.

Conclusion: TIPs pose substantial security risks in LLM-based agentic systems, and the paper recommends implementing robust defense mechanisms to mitigate these vulnerabilities.

Abstract: LLM-based agentic systems leverage large language models to handle user
queries, make decisions, and execute external tools for complex tasks across
domains like chatbots, customer service, and software engineering. A critical
component of these systems is the Tool Invocation Prompt (TIP), which defines
tool interaction protocols and guides LLMs to ensure the security and
correctness of tool usage. Despite its importance, TIP security has been
largely overlooked. This work investigates TIP-related security risks,
revealing that major LLM-based systems like Cursor, Claude Code, and others are
vulnerable to attacks such as remote code execution (RCE) and denial of service
(DoS). Through a systematic TIP exploitation workflow (TEW), we demonstrate
external tool behavior hijacking via manipulated tool invocations. We also
propose defense mechanisms to enhance TIP security in LLM-based agentic
systems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [530] [On the Contribution of Lexical Features to Speech Emotion Recognition](https://arxiv.org/abs/2509.05634)
*David Combei*

Main category: eess.AS

TL;DR: The paper explores the significance of lexical content in speech emotion recognition (SER), showing competitive performance over traditional acoustic models.


<details>
  <summary>Details</summary>
Motivation: To evaluate the often overlooked role of lexical content in SER and to demonstrate its performance relative to acoustic cues.

Method: The study uses the MELD dataset and compares lexical-based approaches against acoustic-only pipelines. It also incorporates self-supervised speech and text representations, transformer-based encoders, and audio denoising in the analysis.

Result: Lexical-based approach achieved a weighted F1-score of 51.5%, outperforming an acoustic-only pipeline that scored 49.3%, despite having fewer parameters.

Conclusion: Lexical content plays a significant and sometimes superior role in SER compared to paralinguistic acoustic features, highlighting its value in SER research.

Abstract: Although paralinguistic cues are often considered the primary drivers of
speech emotion recognition (SER), we investigate the role of lexical content
extracted from speech and show that it can achieve competitive and in some
cases higher performance compared to acoustic models. On the MELD dataset, our
lexical-based approach obtains a weighted F1-score (WF1) of 51.5%, compared to
49.3% for an acoustic-only pipeline with a larger parameter count. Furthermore,
we analyze different self-supervised (SSL) speech and text representations,
conduct a layer-wise study of transformer-based encoders, and evaluate the
effect of audio denoising.

</details>


### [531] [Beamforming-LLM: What, Where and When Did I Miss?](https://arxiv.org/abs/2509.06221)
*Vishal Choudhari*

Main category: eess.AS

TL;DR: Beamforming-LLM integrates spatial audio technology and AI methods to allow users to semantically recall missed conversations in multi-speaker settings with natural language queries.


<details>
  <summary>Details</summary>
Motivation: To address challenges of recalling missed segments in multi-person conversations and enhance assistive tools for auditory memory systems.

Method: The system uses beamforming for spatial audio separation, transcription with Whisper, vector embedding, retrieval-augmented generation (RAG), and summarization via GPT-4o-mini.

Result: Beamforming-LLM provides semantically relevant summaries with spatial context, contrastive updates, timestamped audio playback, and a user-friendly interface.

Conclusion: This work establishes a foundation for auditory memory systems with practical applications in assistive technologies, meeting summarizations, and spatial computing.

Abstract: We present Beamforming-LLM, a system that enables users to semantically
recall conversations they may have missed in multi-speaker environments. The
system combines spatial audio capture using a microphone array with
retrieval-augmented generation (RAG) to support natural language queries such
as, "What did I miss when I was following the conversation on dogs?"
Directional audio streams are separated using beamforming, transcribed with
Whisper, and embedded into a vector database using sentence encoders. Upon
receiving a user query, semantically relevant segments are retrieved,
temporally aligned with non-attended segments, and summarized using a
lightweight large language model (GPT-4o-mini). The result is a user-friendly
interface that provides contrastive summaries, spatial context, and timestamped
audio playback. This work lays the foundation for intelligent auditory memory
systems and has broad applications in assistive technology, meeting
summarization, and context-aware personal spatial computing.

</details>


### [532] [Graph Connectionist Temporal Classification for Phoneme Recognition](https://arxiv.org/abs/2509.05399)
*Henry Grafé,Hugo Van hamme*

Main category: eess.AS

TL;DR: The paper introduces Graph Temporal Classification (GTC) as a method for handling pronunciation ambiguities in Automatic Phoneme Recognition (APR) training, improving phoneme error rates compared to conventional CTC.


<details>
  <summary>Details</summary>
Motivation: Standard APR training using Grapheme-to-Phoneme (G2P) systems struggles with pronunciation ambiguities, which degrade system performance.

Method: Adaptation of Graph Temporal Classification (GTC) for APR systems to incorporate multiple valid phoneme sequences in the training loss.

Result: Experiments on English and Dutch datasets demonstrate that GTC-based training yields lower phoneme error rates than CTC-based training.

Conclusion: Integrating pronunciation variation into the training loss function using GTC is a beneficial approach for APR systems facing noisy G2P supervision.

Abstract: Automatic Phoneme Recognition (APR) systems are often trained using pseudo
phoneme-level annotations generated from text through Grapheme-to-Phoneme (G2P)
systems. These G2P systems frequently output multiple possible pronunciations
per word, but the standard Connectionist Temporal Classification (CTC) loss
cannot account for such ambiguity during training. In this work, we adapt Graph
Temporal Classification (GTC) to the APR setting. GTC enables training from a
graph of alternative phoneme sequences, allowing the model to consider multiple
pronunciations per word as valid supervision. Our experiments on English and
Dutch data sets show that incorporating multiple pronunciations per word into
the training loss consistently improves phoneme error rates compared to a
baseline trained with CTC. These results suggest that integrating pronunciation
variation into the loss function is a promising strategy for training APR
systems from noisy G2P-based supervision.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [533] [Sesame: Opening the door to protein pockets](https://arxiv.org/abs/2509.05302)
*Raúl Miñán,Carles Perez-Lopez,Javier Iglesias,Álvaro Ciudad,Alexis Molina*

Main category: q-bio.BM

TL;DR: The paper introduces 'Sesame,' a generative model to improve ligand docking predictions by modifying ligand-free structures efficiently.


<details>
  <summary>Details</summary>
Motivation: Obtaining high-resolution ligand-bound structures is costly and limits docking performance. Ligand-free structures are accessible but less ideal for accommodating ligands, and existing simulation methods are computationally expensive.

Method: The authors developed Sesame, a generative model capable of efficiently predicting conformational changes in apo structures to better suit ligand accommodation.

Result: Sesame generates suitable geometries for ligand docking at significantly reduced computational costs, creating a scalable solution.

Conclusion: Sesame enhances virtual screening workflows by addressing limitations of ligand-free structures, improving the accessibility and efficiency of drug discovery.

Abstract: Molecular docking is a cornerstone of drug discovery, relying on
high-resolution ligand-bound structures to achieve accurate predictions.
However, obtaining these structures is often costly and time-intensive,
limiting their availability. In contrast, ligand-free structures are more
accessible but suffer from reduced docking performance due to pocket geometries
being less suited for ligand accommodation in apo structures. Traditional
methods for artificially inducing these conformations, such as molecular
dynamics simulations, are computationally expensive. In this work, we introduce
Sesame, a generative model designed to predict this conformational change
efficiently. By generating geometries better suited for ligand accommodation at
a fraction of the computational cost, Sesame aims to provide a scalable
solution for improving virtual screening workflows.

</details>


<div id='math.CT'></div>

# math.CT [[Back]](#toc)

### [534] [The compact double category $\mathbf{Int}(\mathbf{Poly}_*)$ models control flow and data transformations](https://arxiv.org/abs/2509.05462)
*Grigory Kondyrev,David I. Spivak*

Main category: math.CT

TL;DR: The paper constructs an operad for syntax accommodating control flow and data transformations in programming languages.


<details>
  <summary>Details</summary>
Motivation: To enhance the modeling of control flow in programming by including data transformations in the framework defined by traced cocartesian categories.

Method: The authors define an operad of wiring diagrams, demonstrate the use of traced properties in categories, and extend constructions to a compact double category.

Result: Introduced the operad underlying categories like \( \mathbf{Int}(\mathbf{Poly}_*) \) and extended compact double categories to model control flow and data transformations.

Conclusion: Compact double categories enable tracking trajectories through control flow, providing a universal property for modeling transformations and wiring diagrams.

Abstract: Hasegawa showed that control flow in programming languages -- while loops and
if-then-else statements -- can be modeled using traced cocartesian categories,
such as the category $\mathbf{Set}_*$ of pointed sets. In this paper we define
an operad $\mathscr{W}$ of wiring diagrams that provides syntax for categories
whose control flow moreover includes data transformations, including deleting,
duplicating, permuting, and applying pre-specified functions to variables. In
the most basic version, the operad underlies $\mathbf{Int}(\mathbf{Poly}_*)$,
where $\mathbf{Int}(\mathscr{T})$ denotes the free compact category on a traced
category $\mathscr{T}$, as defined by Joyal, Street, and Verity; to do so, we
show that $\mathbf{Poly}_*$, as well as any multivariate version of it, is
traced. We show moreover that whenever $\mathscr{T}$ is uniform -- a condition
also defined by Hasegawa and satisfied by $\mathbf{Int}(\mathscr{T})$ -- the
resulting $\mathbf{Int}$-construction extends to a double category
$\mathbb{I}\mathbf{nt}(\mathscr{T})$, which is compact in the sense of
Patterson. Finally, we define a universal property of the double category
$\mathbb{I}\mathbf{nt}(\mathbf{Poly}_*)$ and
$\mathbb{I}\mathbf{nt}(\mathbf{Set}_*)$ by which one can track trajectories as
they move through the control flow associated to a wiring diagram.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [535] [Social Dynamics of DAOs: Power, Onboarding, and Inclusivity](https://arxiv.org/abs/2509.06163)
*Victoria Kozlova,Ben Biedermann*

Main category: cs.CY

TL;DR: The paper examines cultural and social factors influencing inclusion and power dynamics in DAOs, revealing barriers and complexities despite their egalitarian claims.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked cultural and social barriers affecting inclusivity and power structures in decentralised autonomous organisations (DAOs).

Method: Qualitative interviews and ethnographic observations conducted to explore contributors' lived experiences in DAOs.

Result: Findings reveal barriers such as financial privilege, informal gatekeeping, and onboarding structures, which challenge DAOs' promises of permissionlessness and egalitarianism.

Conclusion: Decentralisation in DAOs requires intentional cultivation of trust, belonging, and epistemic diversity; cultural awareness is pivotal for inclusivity.

Abstract: This report explores the often-overlooked cultural and social dynamics
shaping participation and power in DAOs. Drawing on qualitative interviews and
ethnographic observations, it shows how factors such as financial privilege,
informal gatekeeping, visibility bias, and onboarding structures create
barriers to meaningful inclusion. While DAOs are frequently framed as
permissionless and egalitarian, the lived experiences of contributors reveal a
more complex reality, one in which soft power and implicit norms determine
people's position within DAOs. Instead of offering solutionist prescriptions,
this report argues for a deeper cultural reflection within the DAO ecosystem.
It highlights that decentralisation is not solely a protocol-level feature, but
an ongoing social process that requires intentional cultivation of trust,
belonging, and epistemic plurality. With this report, we want to sharpen the
collective awareness of structural blind spots and call for building more
inclusive and culturally conscious decentralised systems.

</details>


### [536] [Audits Under Resource, Data, and Access Constraints: Scaling Laws For Less Discriminatory Alternatives](https://arxiv.org/abs/2509.05627)
*Sarah H. Cen,Salil Goyal,Zaynah Javed,Ananya Karthik,Percy Liang,Daniel E. Ho*

Main category: cs.CY

TL;DR: This paper proposes a novel approach allowing claimants in anti-discrimination cases to assess the availability of less discriminatory alternatives to AI models, even with limited resources and access.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge claimants face in proving the existence of a less discriminatory alternative model under anti-discrimination law, given the constraints of limited access to data, models, and computational resources.

Method: The authors present a closed-form upper bound for the loss-fairness Pareto frontier, enabling claimants to scale this result to large models using limited data and computational resources. This approach requires training as few as seven small models to extrapolate results to contested larger models.

Result: A scaling law for loss-fairness Pareto frontiers is derived, showing that claimants can estimate fairness-performance trade-offs in AI models under low-resource conditions. Simulations confirm the validity of the scaling law.

Conclusion: The proposed procedure effectively empowers claimants to assess less discriminatory alternatives for AI models under resource-constrained scenarios, enhancing accessibility to AI audits in legal cases.

Abstract: AI audits play a critical role in AI accountability and safety. One branch of
the law for which AI audits are particularly salient is anti-discrimination
law. Several areas of anti-discrimination law implicate the "less
discriminatory alternative" (LDA) requirement, in which a protocol (e.g.,
model) is defensible if no less discriminatory protocol that achieves
comparable performance can be found with a reasonable amount of effort.
Notably, the burden of proving an LDA exists typically falls on the claimant
(the party alleging discrimination). This creates a significant hurdle in AI
cases, as the claimant would seemingly need to train a less discriminatory yet
high-performing model, a task requiring resources and expertise beyond most
litigants. Moreover, developers often shield information about and access to
their model and training data as trade secrets, making it difficult to
reproduce a similar model from scratch.
  In this work, we present a procedure enabling claimants to determine if an
LDA exists, even when they have limited compute, data, information, and model
access. We focus on the setting in which fairness is given by demographic
parity and performance by binary cross-entropy loss. As our main result, we
provide a novel closed-form upper bound for the loss-fairness Pareto frontier
(PF). We show how the claimant can use it to fit a PF in the "low-resource
regime," then extrapolate the PF that applies to the (large) model being
contested, all without training a single large model. The expression thus
serves as a scaling law for loss-fairness PFs. To use this scaling law, the
claimant would require a small subsample of the train/test data. Then, the
claimant can fit the context-specific PF by training as few as 7 (small)
models. We stress test our main result in simulations, finding that our scaling
law holds even when the exact conditions of our theory do not.

</details>


### [537] [Cumplimiento del Reglamento (UE) 2024/1689 en robótica y sistemas autónomos: una revisión sistemática de la literatura](https://arxiv.org/abs/2509.05380)
*Yoana Pita Lorenzo*

Main category: cs.CY

TL;DR: The review finds that many autonomous robotic systems partially comply with Regulation (EU) 2024/1689, notably lacking in transparency and failure intervention mechanisms.


<details>
  <summary>Details</summary>
Motivation: To evaluate the compliance of autonomous robotic systems with Regulation (EU) 2024/1689, particularly in cybersecurity frameworks and methodologies.

Method: A systematic review was conducted following the PRISMA protocol, selecting 22 studies from 243 records sourced from IEEE Xplore, ACM DL, Scopus, and Web of Science.

Result: Findings indicate progress in areas like risk management and encrypted communications but emphasize significant gaps in explainability, real-time human oversight, and traceability. Only 40% of the solutions address transparency, and just 30% implement failure intervention mechanisms.

Conclusion: Modular frameworks integrating risk management, supervision, and continuous auditing are needed to ensure compliance with the AI Act's mandates for autonomous robotics.

Abstract: This systematic literature review analyzes the current state of compliance
with Regulation (EU) 2024/1689 in autonomous robotic systems, focusing on
cybersecurity frameworks and methodologies. Using the PRISMA protocol, 22
studies were selected from 243 initial records across IEEE Xplore, ACM DL,
Scopus, and Web of Science. Findings reveal partial regulatory alignment: while
progress has been made in risk management and encrypted communications,
significant gaps persist in explainability modules, real-time human oversight,
and knowledge base traceability. Only 40% of reviewed solutions explicitly
address transparency requirements, and 30% implement failure intervention
mechanisms. The study concludes that modular approaches integrating risk,
supervision, and continuous auditing are essential to meet the AI Act mandates
in autonomous robotics.

</details>


### [538] [Authorship Without Writing: Large Language Models and the Senior Author Analogy](https://arxiv.org/abs/2509.05390)
*Clint Hurshman,Sebastian Porsdam Mann,Julian Savulescu,Brian D. Earp*

Main category: cs.CY

TL;DR: The paper debates the legitimacy of large language models (LLMs) as authors in scientific and medical writing, proposing that their use can be analogous to senior authorship under certain conditions.


<details>
  <summary>Details</summary>
Motivation: To address the controversy regarding the role of LLMs in authorship and reevaluate authorship criteria in light of evolving practices.

Method: The authors analyze the analogy between LLM-generated content and senior authorship roles, examining accepted authorship criteria in various fields.

Result: They propose that LLMs used under specific conditions could be recognized as legitimate authors, similar to senior authors who guide project integrity without writing themselves.

Conclusion: LLM use in authorship either warrants recognition as legitimate or necessitates a fundamental revision of authorship criteria.

Abstract: The use of large language models (LLMs) in bioethical, scientific, and
medical writing remains controversial. While there is broad agreement in some
circles that LLMs cannot count as authors, there is no consensus about whether
and how humans using LLMs can count as authors. In many fields, authorship is
distributed among large teams of researchers, some of whom, including
paradigmatic senior authors who guide and determine the scope of a project and
ultimately vouch for its integrity, may not write a single word. In this paper,
we argue that LLM use (under specific conditions) is analogous to a form of
senior authorship. On this view, the use of LLMs, even to generate complete
drafts of research papers, can be considered a legitimate form of authorship
according to the accepted criteria in many fields. We conclude that either such
use should be recognized as legitimate, or current criteria for authorship
require fundamental revision. AI use declaration: GPT-5 was used to help format
Box 1. AI was not used for any other part of the preparation or writing of this
manuscript.

</details>


### [539] [Governing AI R&D: A Legal Framework for Constraining Dangerous AI](https://arxiv.org/abs/2509.05361)
*Alex Mark,Aaron Scher*

Main category: cs.CY

TL;DR: The paper analyzes potential legal challenges that may arise in AI regulation within the U.S., focusing on the First Amendment, administrative law, and the Fourteenth Amendment. It provides recommendations to lawmakers on mitigating litigation risks.


<details>
  <summary>Details</summary>
Motivation: As AI continues to evolve, its regulation is critical for ensuring public safety. However, potential legal challenges could undermine these efforts, prompting the need for well-informed legislative strategies.

Method: The paper reviews legal precedents applicable to AI, categorizes risks based on constitutional and administrative law, and proposes preemptive measures for lawmakers to address these challenges.

Result: Three primary categories of legal risks for AI regulation are identified: First Amendment concerns about freedom of expression, administrative law procedures, and Fourteenth Amendment issues relating to equal protection and due process.

Conclusion: Effective AI regulation is possible but requires lawmakers to anticipate and address potential legal challenges through careful planning and implementation.

Abstract: As AI advances, governing its development may become paramount to public
safety. Lawmakers may seek to restrict the development and release of AI models
or of AI research itself. These governance actions could trigger legal
challenges that invalidate the actions, so lawmakers should consider these
challenges ahead of time. We investigate three classes of potential litigation
risk for AI regulation in the U.S.: the First Amendment, administrative law,
and the Fourteenth Amendment. We discuss existing precedent that is likely to
apply to AI, which legal challenges are likely to arise, and how lawmakers
might preemptively address them. Effective AI regulation is possible, but it
requires careful implementation to avoid these legal challenges.

</details>


### [540] [Prototyping an AI-powered Tool for Energy Efficiency in New Zealand Homes](https://arxiv.org/abs/2509.05364)
*Abdollah Baghaei Daemei*

Main category: cs.CY

TL;DR: This paper designs and assesses an AI-powered tool for improving residential energy efficiency in New Zealand, addressing issues related to poor housing quality and fragmented decision-making.


<details>
  <summary>Details</summary>
Motivation: Residential energy efficiency in New Zealand faces challenges due to historically poor housing conditions, incomplete retrofits, and limited household data, necessitating better decision-making support.

Method: An AI-powered decision-support tool was built using Python and Streamlit, incorporating features like data ingestion, anomaly detection, and scenario simulation. It was evaluated with 15 experts through semi-structured interviews.

Result: The tool received high usability (M=4.3) and strong scenario output value (M=4.5) from domain experts, highlighting its effectiveness in complementing subsidy programs and regulatory frameworks.

Conclusion: The AI tool bridges the gap between national policies and practical residential energy improvements, with potential for replication globally. Future focus includes carbon metrics, tariff modeling, and real-world adoption trials.

Abstract: Residential buildings contribute significantly to energy use, health
outcomes, and carbon emissions. In New Zealand, housing quality has
historically been poor, with inadequate insulation and inefficient heating
contributing to widespread energy hardship. Recent reforms, including the
Warmer Kiwi Homes program, Healthy Homes Standards, and H1 Building Code
upgrades, have delivered health and comfort improvements, yet challenges
persist. Many retrofits remain partial, data on household performance are
limited, and decision-making support for homeowners is fragmented. This study
presents the design and evaluation of an AI-powered decision-support tool for
residential energy efficiency in New Zealand. The prototype, developed using
Python and Streamlit, integrates data ingestion, anomaly detection, baseline
modeling, and scenario simulation (e.g., LED retrofits, insulation upgrades)
into a modular dashboard. Fifteen domain experts, including building
scientists, consultants, and policy practitioners, tested the tool through
semi-structured interviews. Results show strong usability (M = 4.3), high value
of scenario outputs (M = 4.5), and positive perceptions of its potential to
complement subsidy programs and regulatory frameworks. The tool demonstrates
how AI can translate national policies into personalized, household-level
guidance, bridging the gap between funding, standards, and practical
decision-making. Its significance lies in offering a replicable framework for
reducing energy hardship, improving health outcomes, and supporting climate
goals. Future development should focus on carbon metrics, tariff modeling,
integration with national datasets, and longitudinal trials to assess
real-world adoption.

</details>


### [541] [User Privacy and Large Language Models: An Analysis of Frontier Developers' Privacy Policies](https://arxiv.org/abs/2509.05382)
*Jennifer King,Kevin Klyman,Emily Capstick,Tiffany Saade,Victoria Hsieh*

Main category: cs.CY

TL;DR: Developers of AI chatbots utilize user interactions, often without explicit consent, to train models, raising significant data privacy concerns.


<details>
  <summary>Details</summary>
Motivation: To explore how major AI developers utilize users' chat data, particularly focusing on privacy policies and practices.

Method: A qualitative coding schema was developed using the California Consumer Privacy Act and applied to analyze privacy policies from six U.S. AI companies.

Result: All six developers use chat data for model training, sometimes retaining it indefinitely. Sensitive data, children’s chats, and uploaded files are included, often without full transparency.

Conclusion: There is a need for improved transparency and accountability in AI chatbot data practices. Recommendations are given for policymakers and developers to enhance privacy protections.

Abstract: Hundreds of millions of people now regularly interact with large language
models via chatbots. Model developers are eager to acquire new sources of
high-quality training data as they race to improve model capabilities and win
market share. This paper analyzes the privacy policies of six U.S. frontier AI
developers to understand how they use their users' chats to train models.
Drawing primarily on the California Consumer Privacy Act, we develop a novel
qualitative coding schema that we apply to each developer's relevant privacy
policies to compare data collection and use practices across the six companies.
We find that all six developers appear to employ their users' chat data to
train and improve their models by default, and that some retain this data
indefinitely. Developers may collect and train on personal information
disclosed in chats, including sensitive information such as biometric and
health data, as well as files uploaded by users. Four of the six companies we
examined appear to include children's chat data for model training, as well as
customer data from other products. On the whole, developers' privacy policies
often lack essential information about their practices, highlighting the need
for greater transparency and accountability. We address the implications of
users' lack of consent for the use of their chat data for model training, data
security issues arising from indefinite chat data retention, and training on
children's chat data. We conclude by providing recommendations to policymakers
and developers to address the data privacy challenges posed by LLM-powered
chatbots.

</details>


### [542] [An Optimized Pipeline for Automatic Educational Knowledge Graph Construction](https://arxiv.org/abs/2509.05392)
*Qurat Ul Ain,Mohamed Amine Chatti,Jean Qussa,Amr Shakhshir,Rawaa Alatrash,Shoeb Joarder*

Main category: cs.CY

TL;DR: This study proposes an optimized pipeline for automatic construction of Educational Knowledge Graphs (EduKGs) from PDF learning materials. The pipeline achieves a 17.5% improvement in accuracy and a tenfold increase in efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of constructing scalable and reliable Educational Knowledge Graphs (EduKGs) to enhance domain knowledge modeling from learning materials.

Method: A two-step pipeline is developed: (1) generating slide-level EduKGs from individual slides/pages, and (2) merging these EduKGs to form a comprehensive graph. Optimizations are applied to various pipeline components to enhance accuracy and efficiency.

Result: The optimized pipeline showed a 17.5% accuracy improvement and a tenfold increase in processing efficiency when applied on the CourseMapper MOOC platform.

Conclusion: The paper presents an enhanced end-to-end EduKG construction pipeline that is adaptable to diverse educational contexts, offering improved semantic representation and processing scalability.

Abstract: The automatic construction of Educational Knowledge Graphs (EduKGs) is
essential for domain knowledge modeling by extracting meaningful
representations from learning materials. Despite growing interest, identifying
a scalable and reliable approach for automatic EduKG generation remains a
challenge. In an attempt to develop a unified and robust pipeline for automatic
EduKG construction, in this study we propose a pipeline for automatic EduKG
construction from PDF learning materials. The process begins with generating
slide-level EduKGs from individual pages/slides, which are then merged to form
a comprehensive EduKG representing the entire learning material. We evaluate
the accuracy of the EduKG generated from the proposed pipeline in our MOOC
platform, CourseMapper. The observed accuracy, while indicative of partial
success, is relatively low particularly in the educational context, where the
reliability of knowledge representations is critical for supporting meaningful
learning. To address this, we introduce targeted optimizations across multiple
pipeline components. The optimized pipeline achieves a 17.5% improvement in
accuracy and a tenfold increase in processing efficiency. Our approach offers a
holistic, scalable and end-to-end pipeline for automatic EduKG construction,
adaptable to diverse educational contexts, and supports improved semantic
representation of learning content.

</details>


### [543] [Inferring Prerequisite Knowledge Concepts in Educational Knowledge Graphs: A Multi-criteria Approach](https://arxiv.org/abs/2509.05393)
*Rawaa Alatrash,Mohamed Amine Chatti,Nasha Wibowo,Qurat Ul Ain*

Main category: cs.CY

TL;DR: The paper proposes an unsupervised method to infer prerequisite relationships (PRs) for concepts in educational knowledge graphs (EduKGs) without labeled data, achieving better precision and scalability.


<details>
  <summary>Details</summary>
Motivation: Current EduKGs in platforms like CourseMapper lack explicit PR links, and manually annotating these links is inefficient, inconsistent, and labor-intensive.

Method: The authors define ten criteria based on document-based, Wikipedia hyperlink-based, graph-based, and text-based features and employ a voting algorithm to infer PRs automatically and robustly.

Result: Experiments on benchmark datasets demonstrate that the proposed approach outperforms existing methods in precision while maintaining scalability and adaptability.

Conclusion: The method enhances sequence-aware learning in EduKGs like CourseMapper, providing accurate and efficient automatic inference of PRs for learning concepts.

Abstract: Educational Knowledge Graphs (EduKGs) organize various learning entities and
their relationships to support structured and adaptive learning. Prerequisite
relationships (PRs) are critical in EduKGs for defining the logical order in
which concepts should be learned. However, the current EduKG in the MOOC
platform CourseMapper lacks explicit PR links, and manually annotating them is
time-consuming and inconsistent. To address this, we propose an unsupervised
method for automatically inferring concept PRs without relying on labeled data.
We define ten criteria based on document-based, Wikipedia hyperlink-based,
graph-based, and text-based features, and combine them using a voting algorithm
to robustly capture PRs in educational content. Experiments on benchmark
datasets show that our approach achieves higher precision than existing methods
while maintaining scalability and adaptability, thus providing reliable support
for sequence-aware learning in CourseMapper.

</details>


### [544] [From Vision to Validation: A Theory- and Data-Driven Construction of a GCC-Specific AI Adoption Index](https://arxiv.org/abs/2509.05474)
*Mohammad Rashed Albous,Anwaar AlKandari,Abdel Latef Anouze*

Main category: cs.CY

TL;DR: This paper develops and validates an AI Adoption Index tailored to the GCC public sector and shows that infrastructure and policy mandates dominate AI success.


<details>
  <summary>Details</summary>
Motivation: To address the lack of tailored measures for AI adoption in the GCC public sector, considering its unique governance models and cultural nuances.

Method: The research combines a theory-driven foundation with empirical evidence, leveraging literature review, National AI Strategies analysis, and a survey of 203 government employees. Statistical methods include K-Means Clustering, Principal Component Analysis, and Partial Least Squares Structural Equation Modeling.

Result: Findings highlight robust infrastructure and clear policy mandates as the strongest drivers of AI success, explaining 70% of the variance in AI outcomes. A composite index integrates dimensions like Infrastructure & Resources, Organizational Readiness, and Policy & Regulatory Environment.

Conclusion: The tailored AI Adoption Index aids policymakers in benchmarking AI maturity and aligning deployments with ethical and regulatory standards, fostering effective transformation in the GCC region.

Abstract: Artificial intelligence (AI) is rapidly transforming public-sector processes
worldwide, yet standardized measures rarely address the unique drivers,
governance models, and cultural nuances of the Gulf Cooperation Council (GCC)
countries. This study employs a theory-driven foundation derived from an
in-depth analysis of literature review and six National AI Strategies (NASs),
coupled with a data-driven approach that utilizes a survey of 203 mid- and
senior-level government employees and advanced statistical techniques (K-Means
clustering, Principal Component Analysis, and Partial Least Squares Structural
Equation Modeling). By combining policy insights with empirical evidence, the
research develops and validates a novel AI Adoption Index specifically tailored
to the GCC public sector. Findings indicate that robust infrastructure and
clear policy mandates exert the strongest influence on successful AI
implementations, overshadowing organizational readiness in early adoption
stages. The combined model explains 70% of the variance in AI outcomes,
suggesting that resource-rich environments and top-down policy directives can
drive rapid but uneven technology uptake. By consolidating key dimensions
(Infrastructure & Resources, Organizational Readiness, and Policy & Regulatory
Environment) into a single composite index, this study provides a holistic yet
context-sensitive tool for benchmarking AI maturity. The index offers
actionable guidance for policymakers seeking to harmonize large-scale
deployments with ethical and regulatory standards. Beyond advancing academic
discourse, these insights inform more strategic allocation of resources,
cross-country cooperation, and capacity-building initiatives, thereby
supporting sustained AI-driven transformation in the GCC region and beyond.

</details>


### [545] [Unmasking COVID-19 Vulnerability in Nigeria: Mapping Risks Beyond Urban Hotspots](https://arxiv.org/abs/2509.05398)
*Sheila Wafula,Blessed Madukoma*

Main category: cs.CY

TL;DR: The paper identifies key factors contributing to state vulnerability to COVID-19 in Nigeria, using a composite risk score and GIS mapping, with a focus on high-density areas like Lagos.


<details>
  <summary>Details</summary>
Motivation: To explore the spatial variability of COVID-19 in Nigeria and provide a risk-based framework for prioritizing public health resources.

Method: A composite risk score was developed, integrating factors such as population density, poverty, healthcare access, and age, combined with spatial analysis through GIS mapping.

Result: High-density urban areas, such as Lagos, had significantly higher risk scores, showing uneven vulnerability across the country. Google Trends data also revealed variations in public health awareness.

Conclusion: The risk score framework aids policymakers in targeting resource allocation and can be adapted for future pandemics and other infectious diseases in resource-limited settings.

Abstract: The COVID-19 pandemic has presented significant challenges in Nigeria's
public health systems since the first case reported on February 27, 2020. This
study investigates key factors that contribute to state vulnerability,
quantifying them through a composite risk score integrating population density
(weight 0.2), poverty (0.4), access to healthcare (0.3), and age risk (0.1),
adjusted by normalized case rates per 100,000. States were categorized into
low-, medium-, and high-density areas to analyze trends and identify hotspots
using geographic information system (GIS) mapping. The findings reveal that
high-density urban areas, such as Lagos, accounting for 35.4% of national
cases, had the highest risk scores (Lagos: 673.47 vs. national average: 28.16).
These results align with global and local studies on the spatial variability of
COVID-19 in Nigeria, including international frameworks such as the CDC Social
Vulnerability Index. Google Trends data highlight variations in public health
awareness, serving as a supplementary analysis to contextualize vulnerability.
The risk score provides a prioritization tool for policymakers to allocate
testing, vaccines, and healthcare resources to high-risk areas, though data
gaps and rural underreporting call for further research. This framework can
extend to other infectious diseases, offering lessons for future pandemics in
resource-limited settings.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [546] [Causal Multi-fidelity Surrogate Forward and Inverse Models for ICF Implosions](https://arxiv.org/abs/2509.05510)
*Tyler E. Maltba,Ben S. Southworth,Jeffrey R. Haack,Marc L. Klasky*

Main category: physics.comp-ph

TL;DR: The paper develops a reduced-order surrogate model for inertial confinement fusion (ICF) design optimization using operator learning and machine learning techniques.


<details>
  <summary>Details</summary>
Motivation: To address challenges in solving high-dimensional, dynamic PDE-constrained optimization problems in ICF experiments, which require mapping experimental observations to simulation parameters.

Method: The authors construct a surrogate model using operator learning, a physics-based ODE embedding, and multifidelity training data to map radiation temperature drives to DT interface dynamics.

Result: The surrogate exhibits high accuracy in predicting interface dynamics and enables ML-based inverse optimization of radiation temperature drives, improving observation reproduction and sampling times.

Conclusion: By integrating operator learning and physical inductive biases, the study accelerates design, discovery, and diagnostics in high-energy-density systems like ICF.

Abstract: Continued progress in inertial confinement fusion (ICF) requires solving
inverse problems relating experimental observations to simulation input
parameters, followed by design optimization. However, such high dimensional
dynamic PDE-constrained optimization problems are extremely challenging or even
intractable. It has been recently shown that inverse problems can be solved by
only considering certain robust features. Here we consider the ICF capsule's
deuterium-tritium (DT) interface, and construct a causal, dynamic,
multifidelity reduced-order surrogate that maps from a time-dependent radiation
temperature drive to the interface's radius and velocity dynamics. The
surrogate targets an ODE embedding of DT interface dynamics, and is constructed
by learning a controller for a base analytical model using low- and
high-fidelity simulation training data with respect to radiation energy group
structure. After demonstrating excellent accuracy of the surrogate interface
model, we use machine learning (ML) models with surrogate-generated data to
solve inverse problems optimizing radiation temperature drive to reproduce
observed interface dynamics. For sparse snapshots in time, the ML model further
characterizes the most informative times at which to sample dynamics.
Altogether we demonstrate how operator learning, causal architectures, and
physical inductive bias can be integrated to accelerate discovery, design, and
diagnostics in high-energy-density systems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [547] [TSPC: A Two-Stage Phoneme-Centric Architecture for code-switching Vietnamese-English Speech Recognition](https://arxiv.org/abs/2509.05983)
*Minh N. H. Nguyen,Anh Nguyen Tran,Dung Truong Dinh,Nam Van Vo*

Main category: cs.SD

TL;DR: The paper addresses challenges in auto-speech recognition (ASR) for Vietnamese-English code-switching by proposing a two-stage phoneme-centric model (TSPC), which achieves better performance with reduced resources.


<details>
  <summary>Details</summary>
Motivation: Code-switching poses difficulties for ASR due to phonological shifts, especially in Vietnamese-English contexts characterized by ambiguous phonetic features.

Method: The paper introduces a Two-Stage Phoneme-Centric (TSPC) model leveraging an extended Vietnamese phoneme set for mixed-language representation, enabling efficient Vietnamese-English code-switching ASR.

Result: The TSPC model outperformed existing baselines like PhoWhisper-base, achieving a reduced word error rate of 20.8% with fewer training resources.

Conclusion: The phoneme-centric two-stage architecture effectively adapts to code-switching scenarios and enhances ASR performance with improved efficiency.

Abstract: Code-switching (CS) presents a significant challenge for general Auto-Speech
Recognition (ASR) systems. Existing methods often fail to capture the subtle
phonological shifts inherent in CS scenarios. The challenge is particularly
difficult for language pairs like Vietnamese and English, where both distinct
phonological features and the ambiguity arising from similar sound recognition
are present. In this paper, we propose a novel architecture for
Vietnamese-English CS ASR, a Two-Stage Phoneme-Centric model (TSPC). The TSPC
employs a phoneme-centric approach, built upon an extended Vietnamese phoneme
set as an intermediate representation to facilitate mixed-lingual modeling.
Experimental results demonstrate that TSPC consistently outperforms existing
baselines, including PhoWhisper-base, in Vietnamese-English CS ASR, achieving a
significantly lower word error rate of 20.8\% with reduced training resources.
Furthermore, the phonetic-based two-stage architecture enables phoneme
adaptation and language conversion to enhance ASR performance in complex CS
Vietnamese-English ASR scenarios.

</details>
