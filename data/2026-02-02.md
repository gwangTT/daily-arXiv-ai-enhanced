<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 10]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.CL](#cs.CL) [Total: 10]
- [cs.CV](#cs.CV) [Total: 10]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.LG](#cs.LG) [Total: 15]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 10]
- [cs.SE](#cs.SE) [Total: 10]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [stat.ML](#stat.ML) [Total: 10]
- [cs.CR](#cs.CR) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [JAF: Judge Agent Forest](https://arxiv.org/abs/2601.22269)
*Sahil Garg,Brad Cheezum,Sridhar Dutta,Vishal Agarwal*

Main category: cs.AI

TL;DR: This paper introduces Judge Agent Forest (JAF), a framework for holistic evaluation of AI responses through cross-instance analysis rather than isolated assessments. It employs belief propagation and ensemble learning principles to improve reasoning and feedback quality.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance automated evaluation frameworks by addressing limitations in isolated assessments of AI outputs, enabling better iterative reasoning improvement for agentic AI systems.

Method: The method involves a judge agent conducting joint inference across multiple query-response pairs, leveraging a learned locality-sensitive hashing (LSH) algorithm for exemplar selection, which incorporates embeddings, semantic predicates, labels, and side information.

Result: JAF exhibits improved performance in reasoning and critique propagation, validated through its application in diagnosing cloud misconfigurations.

Conclusion: JAF presents a robust framework for holistic AI evaluation, overcoming isolated assessment limitations and enabling efficient, context-sensitive judgments that support self-improvement processes.

Abstract: Judge agents are fundamental to agentic AI frameworks: they provide automated evaluation, and enable iterative self-refinement of reasoning processes. We introduce JAF: Judge Agent Forest, a framework in which the judge agent conducts joint inference across a cohort of query--response pairs generated by a primary agent, rather than evaluating each in isolation. This paradigm elevates the judge from a local evaluator to a holistic learner: by simultaneously assessing related responses, the judge discerns cross-instance patterns and inconsistencies, whose aggregate feedback enables the primary agent to improve by viewing its own outputs through the judge's collective perspective.
  Conceptually, JAF bridges belief propagation and ensemble-learning principles: overlapping in-context neighborhoods induce a knowledge-graph structure that facilitates propagation of critique, and repeated, randomized evaluations yield a robust ensemble of context-sensitive judgments. JAF can be instantiated entirely via ICL, with the judge prompted for each query using its associated primary-agent response plus a small, possibly noisy set of peer exemplars. While kNN in embedding space is a natural starting point for exemplars, this approach overlooks categorical structure, domain metadata, or nuanced distinctions accessible to modern LLMs.
  To overcome these limitations, we develop a flexible locality-sensitive hashing (LSH) algorithm that learns informative binary codes by integrating semantic embeddings, LLM-driven hash predicates, supervision from categorical labels, and relevant side information. These hash codes support efficient, interpretable, and relation-aware selection of diverse exemplars, and further optimize exploration of CoT reasoning paths. We validate JAF with an empirical study on the demanding task of cloud misconfigs triage in large-scale cloud environments.

</details>


### [2] [The Six Sigma Agent: Achieving Enterprise-Grade Reliability in LLM Systems Through Consensus-Driven Decomposed Execution](https://arxiv.org/abs/2601.22290)
*Khush Patel,Siva Surendira,Jithin George,Shreyas Kapale*

Main category: cs.AI

TL;DR: Large Language Models are improved for enterprise reliability using the Six Sigma Agent, which breaks tasks into actions, samples outputs from multiple agents, and uses consensus voting.


<details>
  <summary>Details</summary>
Motivation: Reliability challenges hinder enterprise deployment of Large Language Models due to their probabilistic nature.

Method: The paper introduces three components: task decomposition, parallel micro-agent sampling, and consensus voting with dynamic scaling.

Result: Empirical tests show a 14,700x reliability improvement and cost reduction, achieving Six Sigma reliability standards by reducing errors to near-zero through redundancy and voting.

Conclusion: Reliable AI systems can be achieved by embracing redundant architectures and consensus mechanisms, moving beyond simple model scaling approaches.

Abstract: Large Language Models demonstrate remarkable capabilities yet remain fundamentally probabilistic, presenting critical reliability challenges for enterprise deployment. We introduce the Six Sigma Agent, a novel architecture that achieves enterprise-grade reliability through three synergistic components: (1) task decomposition into a dependency tree of atomic actions; (2) micro-agent sampling where each task is executed n times in parallel across diverse LLMs to generate independent outputs; and (3) consensus voting with dynamic scaling, clustering outputs and selecting the answer from the winning cluster with maximum votes. We prove that sampling n independent outputs with error rate p achieves system error O(p^{ceil(n/2)}), enabling exponential reliability gains. Even using cheaper models with 5% per-action error, consensus voting with 5 agents reduces error to 0.11%; dynamic scaling to 13 agents achieves 3.4 DPMO (Defects Per Million Opportunities), the Six Sigma standard. Evaluation across three enterprise use cases demonstrates a 14,700x reliability improvement over single-agent execution while reducing costs by 80%. Our work establishes that reliability in AI systems emerges from principled redundancy and consensus rather than model scaling alone.

</details>


### [3] [Why Reasoning Fails to Plan: A Planning-Centric Analysis of Long-Horizon Decision Making in LLM Agents](https://arxiv.org/abs/2601.22311)
*Zehong Wang,Fang Wu,Hongru Wang,Xiangru Tang,Bolian Li,Zhenfei Yin,Yijun Ma,Yiyang Li,Weixiang Sun,Xiusi Chen,Yanfang Ye*

Main category: cs.AI

TL;DR: LLM-based agents excel in short-term reasoning but struggle with long-term planning due to step-wise greedy policies. FLARE is introduced to mitigate these shortcomings and improve planning efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the incoherent behavior of LLM-based agents in long-term planning due to step-wise reasoning and myopic commitments.

Method: Develop FLARE, a future-aware planning framework that incorporates explicit lookahead, reward estimation, value propagation, and limited commitment in decision-making.

Result: FLARE significantly enhances long-term task performance and planning behavior in LLM-based agents, often outperforming alternatives like GPT-4 in benchmarks.

Conclusion: The distinction between step-by-step reasoning and planning in LLMs highlights a strategy shift; FLARE offers a robust approach to tackle long-horizon planning challenges.

Abstract: Large language model (LLM)-based agents exhibit strong step-by-step reasoning capabilities over short horizons, yet often fail to sustain coherent behavior over long planning horizons. We argue that this failure reflects a fundamental mismatch: step-wise reasoning induces a form of step-wise greedy policy that is adequate for short horizons but fails in long-horizon planning, where early actions must account for delayed consequences. From this planning-centric perspective, we study LLM-based agents in deterministic, fully structured environments with explicit state transitions and evaluation signals. Our analysis reveals a core failure mode of reasoning-based policies: locally optimal choices induced by step-wise scoring lead to early myopic commitments that are systematically amplified over time and difficult to recover from. We introduce FLARE (Future-aware Lookahead with Reward Estimation) as a minimal instantiation of future-aware planning to enforce explicit lookahead, value propagation, and limited commitment in a single model, allowing downstream outcomes to influence early decisions. Across multiple benchmarks, agent frameworks, and LLM backbones, FLARE consistently improves task performance and planning-level behavior, frequently allowing LLaMA-8B with FLARE to outperform GPT-4o with standard step-by-step reasoning. These results establish a clear distinction between reasoning and planning.

</details>


### [4] [Sparks of Rationality: Do Reasoning LLMs Align with Human Judgment and Choice?](https://arxiv.org/abs/2601.22329)
*Ala N. Tak,Amin Banayeeanzade,Anahita Bolourani,Fatemeh Bahrani,Ashutosh Chaubey,Sai Praneeth Karimireddy,Norbert Schwarz,Jonathan Gratch*

Main category: cs.AI

TL;DR: The paper explores how Large Language Models (LLMs) exhibit rationality and emotion-driven bias in decision-making, and examines methods to steer these biases.


<details>
  <summary>Details</summary>
Motivation: Assessing LLMs for rationality and biases is crucial as they are increasingly used in high-stakes decisions.

Method: The study evaluates LLM rationality using benchmarks and probes human-like emotional distortions with two methods: In-Context Priming (ICP) and Representation-Level Steering (RLS).

Result: Deliberate thinking enhances rationality and triggers expected-value maximization. ICP shows high variability while RLS demonstrates plausible but less reliable results.

Conclusion: Reasoning mechanisms amplify sensitivity to emotion steering, creating a trade-off between rationality and human-like emotional behavior in LLM deployment.

Abstract: Large Language Models (LLMs) are increasingly positioned as decision engines for hiring, healthcare, and economic judgment, yet real-world human judgment reflects a balance between rational deliberation and emotion-driven bias. If LLMs are to participate in high-stakes decisions or serve as models of human behavior, it is critical to assess whether they exhibit analogous patterns of (ir)rationalities and biases. To this end, we evaluate multiple LLM families on (i) benchmarks testing core axioms of rational choice and (ii) classic decision domains from behavioral economics and social norms where emotions are known to shape judgment and choice. Across settings, we show that deliberate "thinking" reliably improves rationality and pushes models toward expected-value maximization. To probe human-like affective distortions and their interaction with reasoning, we use two emotion-steering methods: in-context priming (ICP) and representation-level steering (RLS). ICP induces strong directional shifts that are often extreme and difficult to calibrate, whereas RLS produces more psychologically plausible patterns but with lower reliability. Our results suggest that the same mechanisms that improve rationality also amplify sensitivity to affective interventions, and that different steering methods trade off controllability against human-aligned behavior. Overall, this points to a tension between reasoning and affective steering, with implications for both human simulation and the safe deployment of LLM-based decision systems.

</details>


### [5] [Learning Provably Correct Distributed Protocols Without Human Knowledge](https://arxiv.org/abs/2601.22369)
*Yujie Hui,Xiaoyi Lu,Andrew Perrault,Yang Wang*

Main category: cs.AI

TL;DR: This paper presents a framework, GGMS, for automatically designing provably correct distributed protocols, addressing challenges in uncertain and failure-prone environments with a novel combination of techniques.


<details>
  <summary>Details</summary>
Motivation: Designing distributed protocols is crucial for modern systems but is complex and time-intensive due to coordination requirements under uncertainty and failures.

Method: The authors model protocol design as a search over multi-agent strategies in games with imperfect information, leveraging Monte Carlo Tree Search, a transformer-based action encoder, global depth-first search, and repeated model checker feedback.

Result: GGMS learns correct distributed protocols for larger settings compared to traditional methods, verified via exhaustive model checking in bounded scenarios.

Conclusion: GGMS provides a provably complete framework for discovering correct distributed protocols, paving the way for automated solutions in broader and more complex scenarios.

Abstract: Provably correct distributed protocols, which are a critical component of modern distributed systems, are highly challenging to design and have often required decades of human effort. These protocols allow multiple agents to coordinate to come to a common agreement in an environment with uncertainty and failures. We formulate protocol design as a search problem over strategies in a game with imperfect information, and the desired correctness conditions are specified in Satisfiability Modulo Theories (SMT). However, standard methods for solving multi-agent games fail to learn correct protocols in this setting, even when the number of agents is small. We propose a learning framework, GGMS, which integrates a specialized variant of Monte Carlo Tree Search with a transformer-based action encoder, a global depth-first search to break out of local minima, and repeated feedback from a model checker. Protocols output by GGMS are verified correct via exhaustive model checking for all executions within the bounded setting. We further prove that, under mild assumptions, the search process is complete: if a correct protocol exists, GGMS will eventually find it. In experiments, we show that GGMS can learn correct protocols for larger settings than existing methods.

</details>


### [6] [Semi-Autonomous Mathematics Discovery with Gemini: A Case Study on the Erdős Problems](https://arxiv.org/abs/2601.22401)
*Tony Feng,Trieu Trinh,Garrett Bingham,Jiwon Kang,Shengtong Zhang,Sang-hyun Kim,Kevin Barreto,Carl Schildkraut,Junehyuk Jung,Jaehyeon Seo,Carlo Pagano,Yuri Chervonyi,Dawsen Hwang,Kaiying Hou,Sergei Gukov,Cheng-Chiang Tsai,Hyunwoo Choi,Youngbeom Jin,Wei-Yuan Li,Hao-An Wu,Ruey-An Shiu,Yu-Sheng Shih,Quoc V. Le,Thang Luong*

Main category: cs.AI

TL;DR: The paper investigates semi-autonomous mathematics discovery using AI and expert evaluation to address 13 'Open' problems from a well-known database, revealing that their 'Open' status stemmed from obscurity rather than complexity.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of AI-assisted tools in solving longstanding mathematical conjectures and scrutinize their effectiveness in navigating large problem databases.

Method: A hybrid approach combining AI for natural language verification to reduce the conjectures' search space and human expertise for correctness and novelty evaluation.

Result: 13 'Open' problems were resolved—5 with seemingly new AI-derived solutions and 8 through locating prior solutions in the literature, showing that these problems persisted due to rarity or documentation gaps.

Conclusion: AI can effectively assist in mathematical discovery; however, challenges like literature identification and risks of AI unintentionally replicating previous human work need further research and ethical consideration.

Abstract: We present a case study in semi-autonomous mathematics discovery, using Gemini to systematically evaluate 700 conjectures labeled 'Open' in Bloom's Erdős Problems database. We employ a hybrid methodology: AI-driven natural language verification to narrow the search space, followed by human expert evaluation to gauge correctness and novelty. We address 13 problems that were marked 'Open' in the database: 5 through seemingly novel autonomous solutions, and 8 through identification of previous solutions in the existing literature. Our findings suggest that the 'Open' status of the problems was through obscurity rather than difficulty. We also identify and discuss issues arising in applying AI to math conjectures at scale, highlighting the difficulty of literature identification and the risk of ''subconscious plagiarism'' by AI. We reflect on the takeaways from AI-assisted efforts on the Erdős Problems.

</details>


### [7] [AI-Enabled Waste Classification as a Data-Driven Decision Support Tool for Circular Economy and Urban Sustainability](https://arxiv.org/abs/2601.22418)
*Julius Sechang Mboli,Omolara Aderonke Ogungbemi*

Main category: cs.AI

TL;DR: This paper compares machine learning methods and deep learning approaches for waste image classification, finding DenseNet121 the most effective. Transfer learning boosts performance significantly over traditional methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve waste sorting in smart cities to facilitate circular-economy practices and reduce landfill use by employing advanced classification techniques.

Method: The researchers used binary classification on 25,077 waste images, with models including traditional machine learning (Random Forest, SVM, AdaBoost), deep learning (CNN, VGG16, ResNet50), and three transfer-learning models (DenseNet121, EfficientNetB0, InceptionV3). Data was split into an 80/20 train/test set, resized to 150x150 px, and augmented.

Result: DenseNet121 achieved the highest accuracy (91%) and ROC-AUC (0.98), surpassing traditional classifiers by 20 percentage points. PCA had minimal impact on traditional methods, but transfer learning significantly enhanced performance under limited-data settings.

Conclusion: DenseNet121 and transfer learning are highly effective for automated waste sorting. These methods can integrate into a Data-Driven Decision Support System to support resource recovery and reduce environmental impact.

Abstract: Efficient waste sorting is crucial for enabling circular-economy practices and resource recovery in smart cities. This paper evaluates both traditional machine-learning (Random Forest, SVM, AdaBoost) and deep-learning techniques including custom CNNs, VGG16, ResNet50, and three transfer-learning models (DenseNet121, EfficientNetB0, InceptionV3) for binary classification of 25 077 waste images (80/20 train/test split, augmented and resized to 150x150 px). The paper assesses the impact of Principal Component Analysis for dimensionality reduction on traditional models. DenseNet121 achieved the highest accuracy (91 %) and ROC-AUC (0.98), outperforming the best traditional classifier by 20 pp. Principal Component Analysis (PCA) showed negligible benefit for classical methods, whereas transfer learning substantially improved performance under limited-data conditions. Finally, we outline how these models integrate into a real-time Data-Driven Decision Support System for automated waste sorting, highlighting potential reductions in landfill use and lifecycle environmental impacts.)

</details>


### [8] [When LLM meets Fuzzy-TOPSIS for Personnel Selection through Automated Profile Analysis](https://arxiv.org/abs/2601.22433)
*Shahria Hoque,Ahmed Akib Jawad Karim,Md. Golam Rabiul Alam,Nirjhar Gope*

Main category: cs.AI

TL;DR: The paper proposes an NLP-driven automated personnel selection system that uses the LLM-TOPSIS framework combining fuzzy logic for scalable and unbiased candidate evaluations in software engineering recruitment.


<details>
  <summary>Details</summary>
Motivation: To enhance candidate evaluation in recruitment by addressing subjectivity, ambiguity, and biases with an automated system.

Method: Combines LLMs with Fuzzy TOPSIS and fine-tunes the DistilRoBERTa model for candidate ranking using a dataset of LinkedIn profiles enhanced with expert judgments.

Result: Achieved 91% accuracy in ranking candidates for Experience and Overall attributes, closely aligned with human expert evaluations.

Conclusion: The system demonstrates potential to improve recruitment processes through NLP frameworks, scalability, consistency, and reduced bias, with future work focusing on practical implementation and improved interpretability.

Abstract: In this highly competitive employment environment, the selection of suitable personnel is essential for organizational success. This study presents an automated personnel selection system that utilizes sophisticated natural language processing (NLP) methods to assess and rank software engineering applicants. A distinctive dataset was created by aggregating LinkedIn profiles that include essential features such as education, work experience, abilities, and self-introduction, further enhanced with expert assessments to function as standards. The research combines large language models (LLMs) with multicriteria decision-making (MCDM) theory to develop the LLM-TOPSIS framework. In this context, we utilized the TOPSIS method enhanced by fuzzy logic (Fuzzy TOPSIS) to address the intrinsic ambiguity and subjectivity in human assessments. We utilized triangular fuzzy numbers (TFNs) to describe criteria weights and scores, thereby addressing the ambiguity frequently encountered in candidate evaluations. For candidate ranking, the DistilRoBERTa model was fine-tuned and integrated with the fuzzy TOPSIS method, achieving rankings closely aligned with human expert evaluations and attaining an accuracy of up to 91% for the Experience attribute and the Overall attribute. The study underlines the potential of NLP-driven frameworks to improve recruitment procedures by boosting scalability, consistency, and minimizing prejudice. Future endeavors will concentrate on augmenting the dataset, enhancing model interpretability, and verifying the system in actual recruitment scenarios to better evaluate its practical applicability. This research highlights the intriguing potential of merging NLP with fuzzy decision-making methods in personnel selection, enabling scalable and unbiased solutions to recruitment difficulties.

</details>


### [9] [Anytime Safe PAC Efficient Reasoning](https://arxiv.org/abs/2601.22446)
*Chengyao Yu,Hao Zeng,Youxin Zhu,Jianguo Huang,Huajun Zeng,Bingyi Jing*

Main category: cs.AI

TL;DR: The paper proposes the B-PAC reasoning method to improve reasoning efficiency by reducing the use of high-resource models while maintaining performance guarantees.


<details>
  <summary>Details</summary>
Motivation: Improve the efficiency of large reasoning models, which suffer from high computational costs and latency, while minimizing performance errors, especially in online non-stationary environments.

Method: Developed B-PAC reasoning, incorporating inverse propensity scoring estimators and test supermartingales to dynamically route queries based on statistical evidence and maintain safety under partial feedback.

Result: B-PAC reasoning reduced computational costs by up to 81.01% while keeping performance loss within user-specified limits in experiments.

Conclusion: B-PAC reasoning is an effective and theoretically sound method for achieving computational efficiency with controlled performance in large reasoning models under challenging scenarios.

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks but suffer from high computational costs and latency. While selective thinking strategies improve efficiency by routing easy queries to non-thinking models, existing approaches often incur uncontrollable errors, especially in online settings where the performance loss of a non-thinking model is only partially observed and data are non-stationary. To address this, we propose Betting Probably Approximately Correct (B-PAC) reasoning, a principled method that enables anytime safe and efficient online reasoning under partial feedback. Specifically, we utilize inverse propensity scoring estimators to construct test supermartingales for candidate thresholds, and then dynamically adjust the routing threshold based on the accumulated statistical evidence of safety. Theoretically, we establish the anytime-valid performance loss control and the efficiency of B-PAC reasoning. Extensive experiments demonstrate that B-PAC reasoning significantly reduces computational overhead, decreasing thinking model usage by up to 81.01\%, while controlling the performance loss below the user-specified level.

</details>


### [10] [Controllable Information Production](https://arxiv.org/abs/2601.22449)
*Tristan Shah,Stas Tiomkin*

Main category: cs.AI

TL;DR: This paper proposes a novel intrinsic motivation principle, Controllable Information Production (CIP), which eliminates reliance on external utilities or designer-specified variables by bridging optimal control theory and chaos regulation.


<details>
  <summary>Details</summary>
Motivation: Current intrinsic motivation methods overly depend on designer-specified variables, making them limited and susceptible to user biases.

Method: The authors derive the CIP objective from Optimal Control theory, utilizing the gap between open-loop and closed-loop Kolmogorov-Sinai entropies to define a novel intrinsic reward mechanism.

Result: Key theoretical properties of CIP are presented, and its effectiveness is validated on standard intrinsic motivation benchmarks.

Conclusion: CIP provides a principled approach to intrinsic motivation, connecting both extrinsic and intrinsic behaviors, while avoiding dependence on external utilities or designer-specified variables.

Abstract: Intrinsic Motivation (IM) is a paradigm for generating intelligent behavior without external utilities. The existing information-theoretic methods for IM are predominantly based on information transmission, which explicitly depends on the designer's choice of which random variables engage in transmission. In this work, we introduce a novel IM principle, Controllable Information Production (CIP), that avoids both external utilities and designer-specified variables. We derive the CIP objective from Optimal Control, showing a connection between extrinsic and intrinsic behaviors. CIP appears as the gap between open-loop and closed-loop Kolmogorov-Sinai entropies, which simultaneously rewards the pursuit and regulation of chaos. We establish key theoretical properties of CIP and demonstrate its effectiveness on standard IM benchmarks.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [11] [RulePlanner: All-in-One Reinforcement Learner for Unifying Design Rules in 3D Floorplanning](https://arxiv.org/abs/2601.22476)
*Ruizhe Zhong,Xingbo Du,Junchi Yan*

Main category: cs.AR

TL;DR: The paper proposes a deep reinforcement learning-based framework for addressing complex hardware design rules in integrated circuits (IC) floorplanning, especially for 3D scenarios.


<details>
  <summary>Details</summary>
Motivation: The motivation is to reduce labor-intensive manual adjustments required to adhere to complex and diverse hardware design rules during IC floorplanning, particularly for 3D designs.

Method: The approach unifies various hardware design rules into a single framework using novel matrix representations, action space constraints, and reward signals that emphasize constraint satisfaction.

Result: The proposed method demonstrates its effectiveness and validity on public benchmarks with successful transferability to unseen circuits.

Conclusion: The framework is flexible, extensible to handle emerging challenges, and represents a scalable solution for future chip design automation.

Abstract: Floorplanning determines the coordinate and shape of each module in Integrated Circuits. With the scaling of technology nodes, in floorplanning stage especially 3D scenarios with multiple stacked layers, it has become increasingly challenging to adhere to complex hardware design rules. Current methods are only capable of handling specific and limited design rules, while violations of other rules require manual and meticulous adjustment. This leads to labor-intensive and time-consuming post-processing for expert engineers. In this paper, we propose an all-in-one deep reinforcement learning-based approach to tackle these challenges, and design novel representations for real-world IC design rules that have not been addressed by previous approaches. Specifically, the processing of various hardware design rules is unified into a single framework with three key components: 1) novel matrix representations to model the design rules, 2) constraints on the action space to filter out invalid actions that cause rule violations, and 3) quantitative analysis of constraint satisfaction as reward signals. Experiments on public benchmarks demonstrate the effectiveness and validity of our approach. Furthermore, transferability is well demonstrated on unseen circuits. Our framework is extensible to accommodate new design rules, thus providing flexibility to address emerging challenges in future chip design. Code will be available at: https://github.com/Thinklab-SJTU/EDA-AI

</details>


### [12] [Design of a GPU with Heterogeneous Cores for Graphics](https://arxiv.org/abs/2601.22862)
*Aurora Tomás,Juan Luis Aragón,Joan Manuel Parcerisa,Antonio González*

Main category: cs.AR

TL;DR: The paper introduces KHEPRI, a heterogeneous GPU architecture optimized for graphics applications, achieving improved performance, throughput, and energy efficiency by utilizing compute-specialized and memory-specialized cores.


<details>
  <summary>Details</summary>
Motivation: To enhance performance and energy efficiency in GPUs for graphics applications by addressing workload diversity and variability in computational intensity and memory bandwidth requirements.

Method: KHEPRI incorporates two types of specialized cores and a novel scheduler that dynamically assigns frame tiles based on predicted tile behavior using frame-to-frame coherence.

Result: Evaluations show KHEPRI delivers average performance improvement of 9.2%, throughput increase of 7.3%, and energy reduction of 4.8%, without hardware overhead.

Conclusion: KHEPRI demonstrates the feasibility and benefits of introducing heterogeneity in GPUs, paving the way for improved energy-efficient graphics processing without additional hardware costs.

Abstract: Heterogeneous architectures can deliver higher performance and energy efficiency than symmetric counterparts by using multiple architectures tuned to different types of workloads. While previous works focused on CPUs, this work extends the concept of heterogeneity to GPUs by proposing KHEPRI, a heterogeneous GPU architecture for graphics applications. Scenes in graphics applications showcase diversity, as they consist of many objects with varying levels of complexity. As a result, computational intensity and memory bandwidth requirements differ significantly across different regions of each scene. To address this variability, our proposal includes two types of cores: cores optimized for high ILP (compute-specialized) and cores that tolerate a higher number of simultaneously outstanding cache misses (memory-specialized). A key component of the proposed architecture is a novel work scheduler that dynamically assigns each part of a frame (i.e., a tile) to the most suitable core. Designing this scheduler is particularly challenging, as it must preserve data locality; otherwise, the benefits of heterogeneity may be offset by the penalty of additional cache misses. Additionally, the scheduler requires knowledge of each tile's characteristics before rendering it. For this purpose, KHEPRI leverages frame-to-frame coherence to predict the behavior of each tile based on that of the corresponding tile in the previous frame. Evaluations across a wide range of commercial animated graphics applications show that, compared to a traditional homogeneous GPU, KHEPRI achieves an average performance improvement of 9.2%, a throughput increase (frames per second) of 7.3%, and a total GPU energy reduction of 4.8%. Importantly, these benefits are achieved without any hardware overhead.

</details>


### [13] [Machine Learning for Energy-Performance-aware Scheduling](https://arxiv.org/abs/2601.23134)
*Zheyuan Hu,Yifei Shi*

Main category: cs.AR

TL;DR: The paper presents a Bayesian Optimization framework for automating optimal scheduling configurations in embedded systems considering energy efficiency and latency trade-offs.


<details>
  <summary>Details</summary>
Motivation: Optimizing embedded systems post-Dennard scaling is challenging due to complex high-dimensional landscapes and requires better approaches than traditional heuristic tuning.

Method: Bayesian Optimization with Gaussian Processes is used along with Sensitivity Analysis (fANOVA) to approximate the Pareto Frontier between energy and time. Different covariance kernels are compared for interpretability.

Result: The framework enables automated system optimization while providing insights into dominant hardware parameters through physical interpretability.

Conclusion: Bayesian Optimization successfully addresses multi-objective challenges in embedded systems, advancing energy efficiency and latency optimization in complex architectures.

Abstract: In the post-Dennard era, optimizing embedded systems requires navigating complex trade-offs between energy efficiency and latency. Traditional heuristic tuning is often inefficient in such high-dimensional, non-smooth landscapes. In this work, we propose a Bayesian Optimization framework using Gaussian Processes to automate the search for optimal scheduling configurations on heterogeneous multi-core architectures. We explicitly address the multi-objective nature of the problem by approximating the Pareto Frontier between energy and time. Furthermore, by incorporating Sensitivity Analysis (fANOVA) and comparing different covariance kernels (e.g., Matérn vs. RBF), we provide physical interpretability to the black-box model, revealing the dominant hardware parameters driving system performance.

</details>


### [14] [Toward Digital Twins in 3D IC Packaging: A Critical Review of Physics, Data, and Hybrid Architectures](https://arxiv.org/abs/2601.23226)
*Gourab Datta,Sarah Safura Sharif,Yaser Mike Banad*

Main category: cs.AR

TL;DR: This paper reviews the advancement of Digital Twin technology for 3D IC packaging and presents a unified hybrid DT architecture.


<details>
  <summary>Details</summary>
Motivation: To address limitations in monitoring and control for 3D ICs due to multi-physics issues, such as thermal hotspots and interconnect aging, and to resolve terminological ambiguities in Digital Twin technology.

Method: The paper clarifies the Digital Twin hierarchy, synthesizes enabling technologies (real-time surrogate modeling, data-driven paradigms, and in-situ sensing), proposes hybrid DT architecture using physics-informed machine learning (PINNs), and suggests a standards-aligned roadmap.

Result: The authors provide a unified view of Digital Twin technology's current state and propose a physics-informed machine learning-based architecture to optimize 3D IC reliability.

Conclusion: The paper promotes transitioning from passive digital shadows to self-optimizing Digital Twins for 3D ICs using proposed standards and hybrid architecture.

Abstract: Three-dimensional integrated circuit (3D IC) pack-aging and heterogeneous integration have emerged as central pillars of contemporary semiconductor scaling. Yet, the multi-physics coupling inherent to stacked architectures manifesting as thermal hot spots, warpage-induced stresses, and interconnect aging demands monitoring and control capabilities that surpass traditional offline metrology. Although Digital Twin (DT) technology provides a principled route to real-time reliability management, the existing literature remains fragmented and frequently blurs the distinction between static multiphysics simulation workflows and truly dynamic, closed-loop twins. This critical review distinguishes itself by addressing these deficiencies through three specific contributions. First, we clarify the Digital Twin hierarchy to resolve terminological ambiguity between digital models, shadows, and twins. Second, we synthesize three foundational enabling technologies: (1) physics-based modeling, emphasizing the shift from computationally intensive finite-element analysis (FEA) to real-time surrogate models; (2) data-driven paradigms, highlighting virtual metrology (VM) for inferring latent metrics; and (3) in-situ sensing, the nervous system coupling the physical stack to its virtual counterpart. Third, beyond a descriptive survey, we propose a unified hybrid DT architecture that leverages physics-informed machine learning (e.g., PINNs) to reconcile data scarcity with latency constraints. Finally, we outline a standards-aligned roadmap incorporating IEEE 1451 and UCIe protocols to accelerate the transition from passive digital shadows to autonomous, self-optimizing Digital Twins for 3D IC manufacturing and field operation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [15] [In Vino Veritas and Vulnerabilities: Examining LLM Safety via Drunk Language Inducement](https://arxiv.org/abs/2601.22169)
*Anudeex Shetty,Aditya Joshi,Salil S. Kanhere*

Main category: cs.CL

TL;DR: The paper investigates 'drunk language' in LLMs, revealing increased susceptibility to jailbreaking and privacy leaks when induced through specific methods.


<details>
  <summary>Details</summary>
Motivation: To understand how 'drunk language' impacts LLM behavior and safety, highlighting vulnerabilities linked to human-like intoxicated behavior.

Method: Three approaches—persona-based prompting, causal fine-tuning, and reinforcement-based post-training—to simulate drunk language in LLMs and evaluate their safety issues.

Result: Increased jailbreaking susceptibility and privacy leaks in LLMs were observed, aligning with human intoxicated behavior traits.

Conclusion: The study exposes risks to LLM safety from drunk language inducement, emphasizing vulnerabilities and challenges for safety tuning.

Abstract: Humans are susceptible to undesirable behaviours and privacy leaks under the influence of alcohol. This paper investigates drunk language, i.e., text written under the influence of alcohol, as a driver for safety failures in large language models (LLMs). We investigate three mechanisms for inducing drunk language in LLMs: persona-based prompting, causal fine-tuning, and reinforcement-based post-training. When evaluated on 5 LLMs, we observe a higher susceptibility to jailbreaking on JailbreakBench (even in the presence of defences) and privacy leaks on ConfAIde, where both benchmarks are in English, as compared to the base LLMs as well as previously reported approaches. Via a robust combination of manual evaluation and LLM-based evaluators and analysis of error categories, our findings highlight a correspondence between human-intoxicated behaviour, and anthropomorphism in LLMs induced with drunk language. The simplicity and efficiency of our drunk language inducement approaches position them as potential counters for LLM safety tuning, highlighting significant risks to LLM safety.

</details>


### [16] [MrRoPE: Mixed-radix Rotary Position Embedding](https://arxiv.org/abs/2601.22181)
*Qingyuan Tian,Wenhong Zhu,Xiaoran Liu,Xiaofeng Wang,Rui Wang*

Main category: cs.CL

TL;DR: The paper introduces MrRoPE, a generalized modification of RoPE that unifies diverse extension strategies and enables handling longer sequences effectively.


<details>
  <summary>Details</summary>
Motivation: Existing RoPE-extension methods are diverse but lack a unified theoretical framework to handle longer sequences effectively.

Method: The authors propose MrRoPE, a general encoding formulation using radix system conversion, and introduce two new training-free extensions: MrRoPE-Uni and MrRoPE-Pro.

Result: MrRoPE-Pro achieves over 85% recall in 128K-context tests and significantly outperforms YaRN on Infinite-Bench subsets without fine-tuning.

Conclusion: MrRoPE effectively generalizes RoPE-extension approaches and improves handling of long sequences, validated by strong performance and theoretical analysis.

Abstract: Rotary Position Embedding (RoPE)-extension refers to modifying or generalizing the Rotary Position Embedding scheme to handle longer sequences than those encountered during pre-training. However, current extension strategies are highly diverse and lack a unified theoretical foundation. In this paper, we propose MrRoPE (Mixed-radix RoPE), a generalized encoding formulation based on a radix system conversion perspective, which elegantly unifies various RoPE-extension approaches as distinct radix conversion strategies. Based on this theory, we introduce two training-free extensions, MrRoPE-Uni and MrRoPE-Pro, which leverage uniform and progressive radix conversion strategies, respectively, to achieve 'train short, test long' generalization. Without fine-tuning, MrRoPE-Pro sustains over 85% recall in the 128K-context Needle-in-a-Haystack test and achieves more than double YaRN's accuracy on Infinite-Bench retrieval and dialogue subsets. Theoretical analysis confirms that MrRoPE-Pro effectively raises the upper bound of RoPE's attainable encoding length, which further validates the reliability and utility of our theory and methodology.

</details>


### [17] [Prepare Reasoning Language Models for Multi-Agent Debate with Self-Debate Reinforcement Learning](https://arxiv.org/abs/2601.22297)
*Chenxi Liu,Yanshuo Chen,Ruibo Chen,Tianyi Xiong,Tong Zheng,Heng Huang*

Main category: cs.CL

TL;DR: This paper introduces Self-Debate Reinforcement Learning (SDRL), a training methodology for large language models to improve reasoning by incorporating collaborative debate-based approaches.


<details>
  <summary>Details</summary>
Motivation: Existing models trained with reinforcement learning techniques often solve problems in isolation and lack the ability to synthesize diverse reasoning that emerges during collaborative debates.

Method: The SDRL approach involves generating multiple candidate solutions, creating a debate context by combining diverse reasoning paths, and optimizing both standalone and debate-context responses for improved problem-solving.

Result: Experiments with various benchmarks show that SDRL enhances Multi-Agent Debate (MAD) performance and also strengthens individual reasoning capabilities.

Conclusion: SDRL is an effective framework for equipping LLMs with improved reasoning abilities, enabling them to perform well both in isolation and in debate scenarios.

Abstract: The reasoning abilities of large language models (LLMs) have been substantially improved by reinforcement learning with verifiable rewards (RLVR). At test time, collaborative reasoning through Multi-Agent Debate (MAD) has emerged as a promising approach for enhancing LLM performance. However, current RLVR methods typically train LLMs to solve problems in isolation, without explicitly preparing them to synthesize and benefit from different rationales that arise during debate. In this work, we propose Self-Debate Reinforcement Learning (SDRL), a training framework that equips a single LLM with strong standalone problem-solving ability and the capability to learn from diverse reasoning trajectories in MAD. Given a prompt, SDRL first samples multiple candidate solutions, then constructs a debate context with diverse reasoning paths and generates second-turn responses conditioned on this context. Finally, SDRL jointly optimizes both the initial and debate-conditioned responses, yielding a model that is effective as both a standalone solver and a debate participant. Experiments across multiple base models and reasoning benchmarks show that SDRL improves overall MAD performance while simultaneously strengthening single model reasoning.

</details>


### [18] [MERMAID: Memory-Enhanced Retrieval and Reasoning with Multi-Agent Iterative Knowledge Grounding for Veracity Assessment](https://arxiv.org/abs/2601.22361)
*Yupeng Cao,Chengyang He,Yangyang Yu,Ping Wang,K. P. Subbalakshmi*

Main category: cs.CL

TL;DR: MERMAID is a novel veracity assessment framework leveraging memory and multi-agent systems to enhance evidence retrieval and reasoning processes, achieving state-of-the-art performance on fact-checking and claim-verification benchmarks.


<details>
  <summary>Details</summary>
Motivation: The need to assess the veracity of online content effectively, addressing limitations in current methods such as static evidence retrieval and inefficient reuse across claims.

Method: MERMAID integrates agent-driven search, structured knowledge representations, and a persistent memory module for iterative retrieval and reasoning. It employs a Reason-Action style process to dynamically acquire and reuse evidence.

Result: MERMAID demonstrates state-of-the-art performance in fact-checking benchmarks and claim-verification datasets, showcasing better search efficiency and consistency compared to existing frameworks.

Conclusion: Synergizing retrieval, reasoning, and memory improves the reliability of veracity assessment systems, providing an efficient and consistent framework for verifying complex claims.

Abstract: Assessing the veracity of online content has become increasingly critical. Large language models (LLMs) have recently enabled substantial progress in automated veracity assessment, including automated fact-checking and claim verification systems. Typical veracity assessment pipelines break down complex claims into sub-claims, retrieve external evidence, and then apply LLM reasoning to assess veracity. However, existing methods often treat evidence retrieval as a static, isolated step and do not effectively manage or reuse retrieved evidence across claims. In this work, we propose MERMAID, a memory-enhanced multi-agent veracity assessment framework that tightly couples the retrieval and reasoning processes. MERMAID integrates agent-driven search, structured knowledge representations, and a persistent memory module within a Reason-Action style iterative process, enabling dynamic evidence acquisition and cross-claim evidence reuse. By retaining retrieved evidence in an evidence memory, the framework reduces redundant searches and improves verification efficiency and consistency. We evaluate MERMAID on three fact-checking benchmarks and two claim-verification datasets using multiple LLMs, including GPT, LLaMA, and Qwen families. Experimental results show that MERMAID achieves state-of-the-art performance while improving the search efficiency, demonstrating the effectiveness of synergizing retrieval, reasoning, and memory for reliable veracity assessment.

</details>


### [19] [Context Structure Reshapes the Representational Geometry of Language Models](https://arxiv.org/abs/2601.22364)
*Eghbal A. Hosseini,Yuxuan Li,Yasaman Bahri,Declan Campbell,Andrew Kyle Lampinen*

Main category: cs.CL

TL;DR: The paper investigates how representation straightening occurs within a context during in-context learning (ICL) in Large Language Models (LLMs), revealing task-dependent changes. 


<details>
  <summary>Details</summary>
Motivation: Understanding how Large Language Models adapt representations during in-context learning and the relationship between straightened trajectories and performance. 

Method: The authors studied representational straightening in Gemma 2 models across various in-context tasks, comparing both continual prediction and structured prediction settings.

Result: It was found that representational straightening occurs more in continual prediction tasks and less consistently in structured prediction tasks, depending on task phases.

Conclusion: The results indicate that in-context learning is not uniform; LLMs dynamically adapt strategies based on task demands, similar to a Swiss Army knife.

Abstract: Large Language Models (LLMs) have been shown to organize the representations of input sequences into straighter neural trajectories in their deep layers, which has been hypothesized to facilitate next-token prediction via linear extrapolation. Language models can also adapt to diverse tasks and learn new structure in context, and recent work has shown that this in-context learning (ICL) can be reflected in representational changes. Here we bring these two lines of research together to explore whether representation straightening occurs \emph{within} a context during ICL. We measure representational straightening in Gemma 2 models across a diverse set of in-context tasks, and uncover a dichotomy in how LLMs' representations change in context. In continual prediction settings (e.g., natural language, grid world traversal tasks) we observe that increasing context increases the straightness of neural sequence trajectories, which is correlated with improvement in model prediction. Conversely, in structured prediction settings (e.g., few-shot tasks), straightening is inconsistent -- it is only present in phases of the task with explicit structure (e.g., repeating a template), but vanishes elsewhere. These results suggest that ICL is not a monolithic process. Instead, we propose that LLMs function like a Swiss Army knife: depending on task structure, the LLM dynamically selects between strategies, only some of which yield representational straightening.

</details>


### [20] [Stability-Aware Prompt Optimization for Clinical Data Abstraction](https://arxiv.org/abs/2601.22373)
*Arinbjörn Kolbeinsson,Daniel Timbie,Sajjan Narsinghani,Sanjay Hariharan*

Main category: cs.CL

TL;DR: Clinical large language models (LLMs) are sensitive to prompt phrasing, impacting prediction reliability; a stability-focused optimization approach improves robustness.


<details>
  <summary>Details</summary>
Motivation: To address the sensitivity of clinical LLMs to prompt phrasing and to improve their reliability and robustness in critical tasks.

Method: A dual-objective prompt optimization approach targeting both accuracy and stability, leveraging flip rates as a measure of sensitivity on multiple tasks and models.

Result: Including stability as a focus in optimization reduced flip rates across tasks and models, sometimes with a slight compromise in accuracy.

Conclusion: Prompt sensitivity should be explicitly targeted in optimization and validation processes for clinical LLM systems to ensure reliability in real-world applications.

Abstract: Large language models used for clinical abstraction are sensitive to prompt wording, yet most work treats prompts as fixed and studies uncertainty in isolation. We argue these should be treated jointly. Across two clinical tasks (MedAlign applicability/correctness and MS subtype abstraction) and multiple open and proprietary models, we measure prompt sensitivity via flip rates and relate it to calibration and selective prediction. We find that higher accuracy does not guarantee prompt stability, and that models can appear well-calibrated yet remain fragile to paraphrases. We propose a dual-objective prompt optimization loop that jointly targets accuracy and stability, showing that explicitly including a stability term reduces flip rates across tasks and models, sometimes at modest accuracy cost. Our results suggest prompt sensitivity should be an explicit objective when validating clinical LLM systems.

</details>


### [21] [SPLA: Block Sparse Plus Linear Attention for Long Context Modeling](https://arxiv.org/abs/2601.22379)
*Bailin Wang,Dan Friedman,Tao Lei,Chong Wang*

Main category: cs.CL

TL;DR: The paper introduces Sparse Plus Linear Attention (SPLA), a method addressing inefficiencies in block-wise sparse attention by accurately selecting blocks for attention and compressing unselected ones with a residual linear attention module.


<details>
  <summary>Details</summary>
Motivation: Existing block-wise sparse attention methods are inefficient due to low block-selection fidelity and discarded contextual information, causing cumulative losses.

Method: SPLA employs a second-order Taylor expansion metric for exact block selection and compresses unselected blocks into a recurrent state using a subtraction-based residual linear attention module.

Result: SPLA eliminates the performance gap in continual pretraining and surpasses dense models in long-context tasks, achieving competitive general benchmarks.

Conclusion: SPLA improves long-context modeling by enhancing block-selection accuracy and mitigating data loss from unselected blocks, offering better efficiency and performance.

Abstract: Block-wise sparse attention offers significant efficiency gains for long-context modeling, yet existing methods often suffer from low selection fidelity and cumulative contextual loss by completely discarding unselected blocks. To address these limitations, we introduce Sparse Plus Linear Attention (SPLA), a framework that utilizes a selection metric derived from second-order Taylor expansions to accurately identify relevant blocks for exact attention. Instead of discarding the remaining "long tail," SPLA compresses unselected blocks into a compact recurrent state via a residual linear attention (RLA) module. Crucially, to avoid IO overhead, we derive an optimized subtraction-based formulation for RLA -- calculating the residual as the difference between global and selected linear attention -- ensuring that unselected blocks are never explicitly accessed during inference. Our experiments demonstrate that SPLA closes the performance gap in continual pretraining, surpassing dense attention models on long-context benchmarks like RULER while maintaining competitive general knowledge and reasoning capabilities.

</details>


### [22] [SP^2DPO: An LLM-assisted Semantic Per-Pair DPO Generalization](https://arxiv.org/abs/2601.22385)
*Chaoyue He,Xin Zhou,Di Wang,Hong Xu,Wei Liu,Chunyan Miao*

Main category: cs.CL

TL;DR: The paper introduces SP2DPO, a method improving preference optimization by using semantic annotations for instance-specific adjustment, enhancing performance without extra training costs.


<details>
  <summary>Details</summary>
Motivation: The paper identifies limitations in traditional Direct Preference Optimization (DPO), specifically its uniform treatment of heterogeneous real-world preference data, which ignores varying informativeness and label noise.

Method: SP2DPO replaces the single global temperature beta with an instance-specific schedule beta_i based on structured semantic annotations generated offline by teacher language models. Standard DPO optimization is employed during training but with beta set per pair.

Result: SP2DPO demonstrates comparable or improved performance compared to the traditional tuned global-beta DPO baseline, particularly improving length-controlled win rates in AlpacaEval 2.0 for specific model backbones.

Conclusion: Integrating semantic-gap annotations into preference optimization via SP2DPO enhances the model's ability to handle heterogeneous preference data, maintaining competitive or improved results while avoiding extra training overhead.

Abstract: Direct Preference Optimization (DPO) controls the trade-off between fitting preference labels and staying close to a reference model using a single global temperature beta, implicitly treating all preference pairs as equally informative. Real-world preference corpora are heterogeneous: they mix high-signal, objective failures (for example, safety, factuality, instruction violations) with low-signal or subjective distinctions (for example, style), and also include label noise. We introduce our method, SP2DPO (Semantic Per-Pair DPO), a generalization that replaces the global temperature with an instance-specific schedule beta_i pre-decided offline from structured semantic-gap annotations (category, magnitude, confidence) produced by teacher language models. We instantiate this procedure on the UltraFeedback preference corpus (59,960 pairs), enabling large-scale construction of an auditable beta_i artifact, and incur zero training-time overhead: the inner-loop optimizer remains standard DPO with beta set per pair. We focus our empirical study on AlpacaEval 2.0, reporting both raw win rate and length-controlled win rate. Across four open-weight, instruction-tuned student backbones (4B-8B), SP2DPO is competitive with a tuned global-beta DPO baseline and improves AlpacaEval 2.0 length-controlled win rate on two of four backbones, while avoiding per-model beta sweeps. All code, annotations, and artifacts will be released.

</details>


### [23] [Specialists or Generalists? Multi-Agent and Single-Agent LLMs for Essay Grading](https://arxiv.org/abs/2601.22386)
*Jamiu Adekunle Idowu,Ahmed Almasoud*

Main category: cs.CL

TL;DR: This paper compares single-agent and multi-agent large language model architectures for automated essay scoring using the ASAP 2.0 corpus.


<details>
  <summary>Details</summary>
Motivation: The study aims to investigate how different LLM architectures influence AES performance across various essay quality levels.

Method: They evaluate single-agent and multi-agent architectures under zero-shot and few-shot conditions, leveraging GPT-5.1 and a rubric-based grading approach.

Result: The multi-agent system excels in identifying weak essays, while the single-agent system is superior for mid-range essays. Both struggle with high-quality essays, but few-shot calibration significantly enhances performance for both architectures.

Conclusion: Architectural choices should align with deployment goals: multi-agent models suit diagnostic screening, while single-agent models are cost-effective for overall assessment.

Abstract: Automated essay scoring (AES) systems increasingly rely on large language models, yet little is known about how architectural choices shape their performance across different essay quality levels. This paper evaluates single-agent and multi-agent LLM architectures for essay grading using the ASAP 2.0 corpus. Our multi-agent system decomposes grading into three specialist agents (Content, Structure, Language) coordinated by a Chairman Agent that implements rubric-aligned logic including veto rules and score capping. We test both architectures in zero-shot and few-shot conditions using GPT-5.1. Results show that the multi-agent system is significantly better at identifying weak essays while the single-agent system performs better on mid-range essays. Both architectures struggle with high-quality essays. Critically, few-shot calibration emerges as the dominant factor in system performance -- providing just two examples per score level improves QWK by approximately 26% for both architectures. These findings suggest architectural choice should align with specific deployment priorities, with multi-agent AI particularly suited for diagnostic screening of at-risk students, while single-agent models provide a cost-effective solution for general assessment.

</details>


### [24] [Culturally Grounded Personas in Large Language Models: Characterization and Alignment with Socio-Psychological Value Frameworks](https://arxiv.org/abs/2601.22396)
*Candida M. Greco,Lucio La Cava,Andrea Tagarelli*

Main category: cs.CL

TL;DR: The paper explores how large language models (LLMs) can generate personas grounded in different cultural and moral frameworks, evaluating their alignment through established cultural and moral theories.


<details>
  <summary>Details</summary>
Motivation: The authors aim to determine to what extent synthetic personas generated by LLMs accurately reflect global cultural and moral value systems, addressing uncertainties in their simulation of human behavior across cultures.

Method: The study uses interpretable variables from the World Values Survey (WVS) to generate personas in LLMs. It evaluates these personas using the Inglehart-Welzel Cultural Map, demographic consistency with WVS, and moral response profiles based on Moral Foundations Theory.

Result: Generated personas broadly align with established cultural and moral frameworks, reflecting stable cultural differences, demographic group patterns, and varied moral values across cultural settings.

Conclusion: This approach demonstrates the potential of culturally-grounded LLM personas in simulating cross-cultural structures and moral variation, suggesting possible applications for studying human behavior and value systems.

Abstract: Despite the growing utility of Large Language Models (LLMs) for simulating human behavior, the extent to which these synthetic personas accurately reflect world and moral value systems across different cultural conditionings remains uncertain. This paper investigates the alignment of synthetic, culturally-grounded personas with established frameworks, specifically the World Values Survey (WVS), the Inglehart-Welzel Cultural Map, and Moral Foundations Theory. We conceptualize and produce LLM-generated personas based on a set of interpretable WVS-derived variables, and we examine the generated personas through three complementary lenses: positioning on the Inglehart-Welzel map, which unveils their interpretation reflecting stable differences across cultural conditionings; demographic-level consistency with the World Values Survey, where response distributions broadly track human group patterns; and moral profiles derived from a Moral Foundations questionnaire, which we analyze through a culture-to-morality mapping to characterize how moral responses vary across different cultural configurations. Our approach of culturally-grounded persona generation and analysis enables evaluation of cross-cultural structure and moral variation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [25] [Do Open-Vocabulary Detectors Transfer to Aerial Imagery? A Comparative Evaluation](https://arxiv.org/abs/2601.22164)
*Christos Tsourveloudis*

Main category: cs.CV

TL;DR: This paper presents the first evaluation of open-vocabulary object detection (OVD) models on aerial imagery, revealing significant transfer challenges.


<details>
  <summary>Details</summary>
Motivation: To evaluate the transferability of open-vocabulary object detection models, originally successful on natural images, to aerial imagery, a largely unexplored domain.

Method: Five state-of-the-art OVD models were benchmarked on the LAE-80C aerial dataset using strict zero-shot settings, alongside Global, Oracle, and Single-Category inference modes to analyze semantic and visual challenges.

Result: The best model achieved only a 27.6% F1-score with 69% false positive rate. Reducing vocabulary size improved performance significantly, revealing semantic confusion as a major issue. Dataset performance varied widely, showing sensitivity to imaging conditions.

Conclusion: Current OVD models face substantial domain adaptation challenges for aerial imagery. Significant work is needed to improve their transferability and robustness to new domains.

Abstract: Open-vocabulary object detection (OVD) enables zero-shot recognition of novel categories through vision-language models, achieving strong performance on natural images. However, transferability to aerial imagery remains unexplored. We present the first systematic benchmark evaluating five state-of-the-art OVD models on the LAE-80C aerial dataset (3,592 images, 80 categories) under strict zero-shot conditions. Our experimental protocol isolates semantic confusion from visual localization through Global, Oracle, and Single-Category inference modes. Results reveal severe domain transfer failure: the best model (OWLv2) achieves only 27.6% F1-score with 69% false positive rate. Critically, reducing vocabulary size from 80 to 3.2 classes yields 15x improvement, demonstrating that semantic confusion is the primary bottleneck. Prompt engineering strategies such as domain-specific prefixing and synonym expansion, fail to provide meaningful performance gains. Performance varies dramatically across datasets (F1: 0.53 on DIOR, 0.12 on FAIR1M), exposing brittleness to imaging conditions. These findings establish baseline expectations and highlight the need for domain-adaptive approaches in aerial OVD.

</details>


### [26] [What Lies Beneath: A Call for Distribution-based Visual Question & Answer Datasets](https://arxiv.org/abs/2601.22218)
*Jill P. Naiman,Daniel J. Evans,JooYoung Seo*

Main category: cs.CV

TL;DR: The paper highlights the lack of a VQA benchmark focused on interpreting scientific charts without a 1-to-1 correspondence between chart marks and data. A new dataset of synthetic histograms for such analysis is introduced and released.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the limitations of current VQA datasets, which focus on real-world images or simple diagrams but fail to address complex reasoning challenges posed by scientific charts.

Method: The authors review existing datasets, identify gaps, and create a dataset of synthetic histograms, including ground truth data and associated annotations. They present questions that require understanding the relationship between chart marks and underlying data.

Result: They successfully generate and release an open-source dataset containing synthetic histogram charts, their underlying data, distribution parameters, and annotations for chart features.

Conclusion: This work contributes a new VQA benchmark tailored for interpreting scientific charts that involve complex data transformations, providing a valuable resource for future multimodal research.

Abstract: Visual Question Answering (VQA) has become an important benchmark for assessing how large multimodal models (LMMs) interpret images. However, most VQA datasets focus on real-world images or simple diagrammatic analysis, with few focused on interpreting complex scientific charts. Indeed, many VQA datasets that analyze charts do not contain the underlying data behind those charts or assume a 1-to-1 correspondence between chart marks and underlying data. In reality, charts are transformations (i.e. analysis, simplification, modification) of data. This distinction introduces a reasoning challenge in VQA that the current datasets do not capture. In this paper, we argue for a dedicated VQA benchmark for scientific charts where there is no 1-to-1 correspondence between chart marks and underlying data. To do so, we survey existing VQA datasets and highlight limitations of the current field. We then generate synthetic histogram charts based on ground truth data, and ask both humans and a large reasoning model questions where precise answers depend on access to the underlying data. We release the open-source dataset, including figures, underlying data, distribution parameters used to generate the data, and bounding boxes for all figure marks and text for future research.

</details>


### [27] [Lost in Space? Vision-Language Models Struggle with Relative Camera Pose Estimation](https://arxiv.org/abs/2601.22228)
*Ken Deng,Yifu Qiu,Yoni Kasten,Shay B. Cohen,Yftah Ziser*

Main category: cs.CV

TL;DR: The paper examines the shortcomings of Vision-Language Models (VLMs) in comprehending 3D spatial structures, introducing benchmarks for camera pose estimation tasks and highlighting their limitations compared to humans and classical methods.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the gap in Vision-Language Models' (VLMs) ability to understand 3D spatial structures compared to their strong performance in 2D perception and reasoning.

Method: The authors introduce two benchmarks: VRRPI-Bench for realistic egocentric videos with annotated relative camera motion, and VRRPI-Diag to diagnose isolated motion degrees of freedom. They evaluate VLMs on relative camera pose estimation and assess their ability in 3D and multi-image reasoning.

Result: Most VLMs fail to generalize beyond 2D heuristics, performing poorly in depth changes and roll transformations. Even advanced models like GPT-5 underperform compared to geometric baselines and humans. Multi-frame reasoning also posed significant challenges for VLMs.

Conclusion: The study highlights the limitations of Vision-Language Models in 3D spatial and multi-view reasoning, advocating for improved methods to ground these models effectively in 3D understanding.

Abstract: Vision-Language Models (VLMs) perform well in 2D perception and semantic reasoning compared to their limited understanding of 3D spatial structure. We investigate this gap using relative camera pose estimation (RCPE), a fundamental vision task that requires inferring relative camera translation and rotation from a pair of images. We introduce VRRPI-Bench, a benchmark derived from unlabeled egocentric videos with verbalized annotations of relative camera motion, reflecting realistic scenarios with simultaneous translation and rotation around a shared object. We further propose VRRPI-Diag, a diagnostic benchmark that isolates individual motion degrees of freedom. Despite the simplicity of RCPE, most VLMs fail to generalize beyond shallow 2D heuristics, particularly for depth changes and roll transformations along the optical axis. Even state-of-the-art models such as GPT-5 ($0.64$) fall short of classic geometric baselines ($0.97$) and human performance ($0.92$). Moreover, VLMs exhibit difficulty in multi-image reasoning, with inconsistent performance (best $59.7\%$) when integrating spatial cues across frames. Our findings reveal limitations in grounding VLMs in 3D and multi-view spatial reasoning.

</details>


### [28] [Geometry without Position? When Positional Embeddings Help and Hurt Spatial Reasoning](https://arxiv.org/abs/2601.22231)
*Jian Shi,Michael Birsak,Wenqing Cui,Zhenyu Li,Peter Wonka*

Main category: cs.CV

TL;DR: The paper examines positional embeddings (PEs) in vision transformers (ViTs), revealing their significant role as geometric priors shaping spatial representations.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of positional embeddings in vision transformers in shaping spatial structure and multi-view consistency.

Method: The authors introduce token-level diagnostics and perform extensive experiments across 14 ViT models to investigate the role of PEs in multi-view geometry and spatial reasoning.

Result: The study shows positional embeddings are pivotal in determining spatial structure, influencing multi-view geometric consistency in representation.

Conclusion: Positional embeddings are essential geometric priors that causally impact spatial reasoning and structure in vision transformers.

Abstract: This paper revisits the role of positional embeddings (PEs) within vision transformers (ViTs) from a geometric perspective. We show that PEs are not mere token indices but effectively function as geometric priors that shape the spatial structure of the representation. We introduce token-level diagnostics that measure how multi-view geometric consistency in ViT representation depends on consitent PEs. Through extensive experiments on 14 foundation ViT models, we reveal how PEs influence multi-view geometry and spatial reasoning. Our findings clarify the role of PEs as a causal mechanism that governs spatial structure in ViT representations. Our code is provided in https://github.com/shijianjian/vit-geometry-probes

</details>


### [29] [Is Hierarchical Quantization Essential for Optimal Reconstruction?](https://arxiv.org/abs/2601.22244)
*Shirin Reyhanian,Laurenz Wiskott*

Main category: cs.CV

TL;DR: This paper examines whether single-level Vector-quantized Variational Autoencoders (VQ-VAEs) can match the reconstruction fidelity of hierarchical VQ-VAE2 models by optimizing codebook usage and representational budget.


<details>
  <summary>Details</summary>
Motivation: Hierarchical VQ-VAEs like VQ-VAE2 are seen as superior in reconstruction fidelity by separating global and local features, but their true advantage in terms of reconstruction fidelity, independent of hierarchy, remained underexplored.

Method: The study compares a single-level VQ-VAE with a two-level hierarchical VQ-VAE on high-resolution ImageNet images, controlling for representational budget and mitigating codebook collapse with interventions like data initialization, codebook resets, and hyperparameter tuning.

Result: The experiments show that single-level VQ-VAEs, when mitigating codebook collapse and matching representational capacity, can achieve reconstruction fidelity equivalent to hierarchical counterparts.

Conclusion: Single-level VQ-VAEs can perform as well as hierarchical models in terms of reconstruction fidelity, questioning the necessity of hierarchical designs for high-quality reconstructions.

Abstract: Vector-quantized variational autoencoders (VQ-VAEs) are central to models that rely on high reconstruction fidelity, from neural compression to generative pipelines. Hierarchical extensions, such as VQ-VAE2, are often credited with superior reconstruction performance because they split global and local features across multiple levels. However, since higher levels derive all their information from lower levels, they should not carry additional reconstructive content beyond what the lower-level already encodes. Combined with recent advances in training objectives and quantization mechanisms, this leads us to ask whether a single-level VQ-VAE, with matched representational budget and no codebook collapse, can equal the reconstruction fidelity of its hierarchical counterpart. Although the multi-scale structure of hierarchical models may improve perceptual quality in downstream tasks, the effect of hierarchy on reconstruction accuracy, isolated from codebook utilization and overall representational capacity, remains empirically underexamined. We revisit this question by comparing a two-level VQ-VAE and a capacity-matched single-level model on high-resolution ImageNet images. Consistent with prior observations, we confirm that inadequate codebook utilization limits single-level VQ-VAEs and that overly high-dimensional embeddings destabilize quantization and increase codebook collapse. We show that lightweight interventions such as initialization from data, periodic reset of inactive codebook vectors, and systematic tuning of codebook hyperparameters significantly reduce collapse. Our results demonstrate that when representational budgets are matched, and codebook collapse is mitigated, single-level VQ-VAEs can match the reconstruction fidelity of hierarchical variants, challenging the assumption that hierarchical quantization is inherently superior for high-quality reconstructions.

</details>


### [30] [VMonarch: Efficient Video Diffusion Transformers with Structured Attention](https://arxiv.org/abs/2601.22275)
*Cheng Liang,Haoxian Chen,Liang Hou,Qi Fan,Gangshan Wu,Xin Tao,Limin Wang*

Main category: cs.CV

TL;DR: The paper proposes VMonarch, a novel sparse attention mechanism for Video Diffusion Transformers (DiTs), which achieves significant efficiency improvements using structured Monarch matrices.


<details>
  <summary>Details</summary>
Motivation: The quadratic complexity of the attention mechanism in Video DiTs restricts scalability for long sequences and demands a more efficient solution.

Method: The authors utilize Monarch matrices to represent sparse spatio-temporal attention patterns and propose the VMonarch mechanism. It incorporates spatio-temporal factorization, a recomputation strategy to avoid artifacts, and an online entropy algorithm fused with FlashAttention for efficient operation.

Result: VMonarch achieves comparable or superior performance to full attention while reducing attention FLOPs by 17.5x, speeding up computation by over 5x for long videos, and outperforming existing sparse attention methods at 90% sparsity.

Conclusion: VMonarch successfully addresses the attention bottleneck in Video DiTs, offering a more efficient alternative for handling long video sequences.

Abstract: The quadratic complexity of the attention mechanism severely limits the context scalability of Video Diffusion Transformers (DiTs). We find that the highly sparse spatio-temporal attention patterns exhibited in Video DiTs can be naturally represented by the Monarch matrix. It is a class of structured matrices with flexible sparsity, enabling sub-quadratic attention via an alternating minimization algorithm. Accordingly, we propose VMonarch, a novel attention mechanism for Video DiTs that enables efficient computation over the dynamic sparse patterns with structured Monarch matrices. First, we adapt spatio-temporal Monarch factorization to explicitly capture the intra-frame and inter-frame correlations of the video data. Second, we introduce a recomputation strategy to mitigate artifacts arising from instabilities during alternating minimization of Monarch matrices. Third, we propose a novel online entropy algorithm fused into FlashAttention, enabling fast Monarch matrix updates for long sequences. Extensive experiments demonstrate that VMonarch achieves comparable or superior generation quality to full attention on VBench after minimal tuning. It overcomes the attention bottleneck in Video DiTs, reduces attention FLOPs by a factor of 17.5, and achieves a speedup of over 5x in attention computation for long videos, surpassing state-of-the-art sparse attention methods at 90% sparsity.

</details>


### [31] [Coarse-to-Real: Generative Rendering for Populated Dynamic Scenes](https://arxiv.org/abs/2601.22301)
*Gonzalo Gomez-Nogales,Yicong Hong,Chongjian Ge,Marc Comino-Trinidad,Dan Casas,Yi Zhou*

Main category: cs.CV

TL;DR: C2R is a generative rendering framework that converts simple 3D simulations into realistic urban crowd videos, leveraging a unique training method and neural rendering guided by text prompts.


<details>
  <summary>Details</summary>
Motivation: Realistic imagery production faces challenges in scalability, realism, and computational demands for dynamic and populated scenes. The paper aims to address these challenges.

Method: The method involves using coarse 3D renderings to control scene elements, combined with a neural renderer that generates realistic visuals. A two-phase training strategy bridges coarse simulations with real footage by learning shared implicit features.

Result: C2R enables coarse-to-fine control and can generalize across various CG and game inputs, producing controllable and realistic urban videos with minimal 3D input.

Conclusion: C2R provides a novel, scalable solution for generating realistic urban scene videos from coarse 3D simulations, paving the way for improved rendering processes. The model and resources will be made publicly available.

Abstract: Traditional rendering pipelines rely on complex assets, accurate materials and lighting, and substantial computational resources to produce realistic imagery, yet they still face challenges in scalability and realism for populated dynamic scenes. We present C2R (Coarse-to-Real), a generative rendering framework that synthesizes real-style urban crowd videos from coarse 3D simulations. Our approach uses coarse 3D renderings to explicitly control scene layout, camera motion, and human trajectories, while a learned neural renderer generates realistic appearance, lighting, and fine-scale dynamics guided by text prompts. To overcome the lack of paired training data between coarse simulations and real videos, we adopt a two-phase mixed CG-real training strategy that learns a strong generative prior from large-scale real footage and introduces controllability through shared implicit spatio-temporal features across domains. The resulting system supports coarse-to-fine control, generalizes across diverse CG and game inputs, and produces temporally consistent, controllable, and realistic urban scene videos from minimal 3D input. We will release the model and project webpage at https://gonzalognogales.github.io/coarse2real/.

</details>


### [32] [FlexMap: Generalized HD Map Construction from Flexible Camera Configurations](https://arxiv.org/abs/2601.22376)
*Run Wang,Chaoyi Zhou,Amir Salarpour,Xi Liu,Zhi-Qi Cheng,Feng Luo,Mert D. Pesé,Siyu Huang*

Main category: cs.CV

TL;DR: FlexMap introduces a novel HD map construction method that adapts to variable camera configurations without retraining and enhances 3D scene understanding using a geometry-aware foundation model.


<details>
  <summary>Details</summary>
Motivation: Existing HD map construction methods are dependent on calibrated multi-camera setups and 2D-to-BEV transformations, which are prone to failure when sensors malfunction or camera configurations differ across fleets.

Method: FlexMap eliminates explicit geometric projections using a geometry-aware model with cross-frame attention, featuring a spatial-temporal enhancement module and a camera-aware decoder for flexible and robust HD map construction.

Result: Experimental results show FlexMap outperforms current methods, handling missing views and sensor variations effectively, making it practical for real-world application.

Conclusion: FlexMap provides a robust, flexible, and efficient solution for HD map construction, overcoming limitations of existing systems and advancing autonomous driving capabilities.

Abstract: High-definition (HD) maps provide essential semantic information of road structures for autonomous driving systems, yet current HD map construction methods require calibrated multi-camera setups and either implicit or explicit 2D-to-BEV transformations, making them fragile when sensors fail or camera configurations vary across vehicle fleets. We introduce FlexMap, unlike prior methods that are fixed to a specific N-camera rig, our approach adapts to variable camera configurations without any architectural changes or per-configuration retraining. Our key innovation eliminates explicit geometric projections by using a geometry-aware foundation model with cross-frame attention to implicitly encode 3D scene understanding in feature space. FlexMap features two core components: a spatial-temporal enhancement module that separates cross-view spatial reasoning from temporal dynamics, and a camera-aware decoder with latent camera tokens, enabling view-adaptive attention without the need for projection matrices. Experiments demonstrate that FlexMap outperforms existing methods across multiple configurations while maintaining robustness to missing views and sensor variations, enabling more practical real-world deployment.

</details>


### [33] [Jailbreaks on Vision Language Model via Multimodal Reasoning](https://arxiv.org/abs/2601.22398)
*Aarush Noheria,Yuguang Yao*

Main category: cs.CV

TL;DR: The paper proposes a jailbreak framework with stealthy prompts and an adaptive noising mechanism to evade Vision-Language Models' (VLMs) safety filters.


<details>
  <summary>Details</summary>
Motivation: The study aims to address vulnerabilities in VLMs' outputs due to their sensitivity to prompt variations, potentially compromising safety alignment.

Method: The framework uses Chain-of-Thought (CoT) prompting for stealthy prompts and introduces a ReAct-driven adaptive noising mechanism to iteratively perturb images and enhance evasion.

Result: Experimental results show that the dual-strategy approach significantly increases attack success rates (ASR) while preserving the naturalness of text and visuals.

Conclusion: The proposed jailbreak framework and adaptive noising strategy effectively train models to evade safety defenses, highlighting the need for more robust safeguards in VLM systems.

Abstract: Vision-language models (VLMs) have become central to tasks such as visual question answering, image captioning, and text-to-image generation. However, their outputs are highly sensitive to prompt variations, which can reveal vulnerabilities in safety alignment. In this work, we present a jailbreak framework that exploits post-training Chain-of-Thought (CoT) prompting to construct stealthy prompts capable of bypassing safety filters. To further increase attack success rates (ASR), we propose a ReAct-driven adaptive noising mechanism that iteratively perturbs input images based on model feedback. This approach leverages the ReAct paradigm to refine adversarial noise in regions most likely to activate safety defenses, thereby enhancing stealth and evasion. Experimental results demonstrate that the proposed dual-strategy significantly improves ASR while maintaining naturalness in both text and visual domains.

</details>


### [34] [EMBC Special Issue: Calibrated Uncertainty for Trustworthy Clinical Gait Analysis Using Probabilistic Multiview Markerless Motion Capture](https://arxiv.org/abs/2601.22412)
*Seth Donahue,Irina Djuraskovic,Kunal Shah,Fabian Sinz,Ross Chafetz,R. James Cotton*

Main category: cs.CV

TL;DR: This study focuses on a probabilistic multi-view markerless motion capture (MMMC) method, evaluating its calibration and reliability for human movement analysis in clinical contexts.


<details>
  <summary>Details</summary>
Motivation: The study aims to make markerless motion capture systems viable for clinical settings by ensuring these systems not only have accuracy but also reliable confidence intervals to inform their precision for individual cases.

Method: A probabilistic MMMC model utilizing variational inference was tested on data from 68 participants, validated against traditional motion capture systems, and assessed for Expected Calibration Error (ECE).

Result: The model demonstrated reliable calibration with ECE values under 0.1. Median step and stride length errors were ~16 mm and ~12 mm, with bias-corrected kinematic errors ranging between 1.5 to 3.8 degrees across lower extremities. Uncertainty predicted by the model correlated strongly with observed errors.

Conclusion: The probabilistic MMMC model effectively quantifies uncertainty and identifies unreliable outputs, indicating promise for its use in clinical applications without requiring concurrent ground-truth data.

Abstract: Video-based human movement analysis holds potential for movement assessment in clinical practice and research. However, the clinical implementation and trust of multi-view markerless motion capture (MMMC) require that, in addition to being accurate, these systems produce reliable confidence intervals to indicate how accurate they are for any individual. Building on our prior work utilizing variational inference to estimate joint angle posterior distributions, this study evaluates the calibration and reliability of a probabilistic MMMC method. We analyzed data from 68 participants across two institutions, validating the model against an instrumented walkway and standard marker-based motion capture. We measured the calibration of the confidence intervals using the Expected Calibration Error (ECE). The model demonstrated reliable calibration, yielding ECE values generally < 0.1 for both step and stride length and bias-corrected gait kinematics. We observed a median step and stride length error of ~16 mm and ~12 mm respectively, with median bias-corrected kinematic errors ranging from 1.5 to 3.8 degrees across lower extremity joints. Consistent with the calibrated ECE, the magnitude of the model's predicted uncertainty correlated strongly with observed error measures. These findings indicate that, as designed, the probabilistic model reconstruction quantifies epistemic uncertainty, allowing it to identify unreliable outputs without the need for concurrent ground-truth instrumentation.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [35] [AscendCraft: Automatic Ascend NPU Kernel Generation via DSL-Guided Transcompilation](https://arxiv.org/abs/2601.22760)
*Zhongzhen Wen,Shudi Shao,Zhong Li,Yu Ge,Tongtong Xu,Yuanyi Lin,Tian Zhang*

Main category: cs.DC

TL;DR: AscendCraft introduces a Domain-Specific Language (DSL)-guided approach to automate the generation of AscendC kernels for NPUs, achieving high compilation and functional success rates, outperforming PyTorch in some cases.


<details>
  <summary>Details</summary>
Motivation: Kernel development for neural processing units (NPUs) is challenging due to domain-specific programming constraints and limited documentation, creating a gap in leveraging large language models (LLMs) for efficient kernel generation.

Method: AscendCraft uses a lightweight DSL to abstract away non-essential complexities and explicitly model execution semantics specific to Ascend. It employs expert examples for DSL kernel generation followed by transcompilation using structured LLM-driven passes into AscendC.

Result: AscendCraft achieved a 98.1% compilation success rate and 90.4% functional correctness across benchmarks. Further, nearly half of the generated kernels met or exceeded PyTorch performance, with notably better performance on a new architecture.

Conclusion: The DSL-guided approach demonstrates that LLMs can generate functionally correct and performant NPU kernels, bridging the gap for automatic kernel generation in specialized accelerators.

Abstract: The performance of deep learning models critically depends on efficient kernel implementations, yet developing high-performance kernels for specialized accelerators remains time-consuming and expertise-intensive. While recent work demonstrates that large language models (LLMs) can generate correct and performant GPU kernels, kernel generation for neural processing units (NPUs) remains largely underexplored due to domain-specific programming models, limited public examples, and sparse documentation. Consequently, directly generating AscendC kernels with LLMs yields extremely low correctness, highlighting a substantial gap between GPU and NPU kernel generation.
  We present AscendCraft, a DSL-guided approach for automatic AscendC kernel generation. AscendCraft introduces a lightweight DSL that abstracts non-essential complexity while explicitly modeling Ascend-specific execution semantics. Kernels are first generated in the DSL using category-specific expert examples and then transcompiled into AscendC through structured, constraint-driven LLM lowering passes. Evaluated on MultiKernelBench across seven operator categories, AscendCraft achieves 98.1% compilation success and 90.4% functional correctness. Moreover, 46.2% of generated kernels match or exceed PyTorch eager execution performance, demonstrating that DSL-guided transcompilation can enable LLMs to generate both correct and competitive NPU kernels. Beyond benchmarks, AscendCraft further demonstrates its generality by successfully generating two correct kernels for newly proposed mHC architecture, achieving performance that substantially surpasses PyTorch eager execution.

</details>


### [36] [Towards Resiliency in Large Language Model Serving with KevlarFlow](https://arxiv.org/abs/2601.22438)
*Shangshu Qian,Kipling Liu,P. C. Sruthi,Lin Tan,Yongle Zhang*

Main category: cs.DC

TL;DR: KevlarFlow is a fault-tolerant architecture for large language model serving systems that significantly improves recovery time and latency under hardware failure conditions.


<details>
  <summary>Details</summary>
Motivation: The need to address the fragility of LLM serving systems, where hardware unreliability in hyperscale clusters leads to slow recovery and extended service outages.

Method: KevlarFlow employs 1) decoupled model parallelism initialization, 2) dynamic traffic rerouting, and 3) background key-value cache replication to maintain service performance during partial failures.

Result: KevlarFlow reduces mean-time-to-recovery (MTTR) by 20x and greatly enhances latency metrics, including average latency, p99 latency, average time-to-first-token (TTFT), and p99 TTFT by significant factors compared to existing LLM serving systems.

Conclusion: KevlarFlow addresses the fragility in LLM serving systems by offering an efficient, fault-tolerant architecture with minimal runtime overhead while ensuring high service availability.

Abstract: Large Language Model (LLM) serving systems remain fundamentally fragile, where frequent hardware faults in hyperscale clusters trigger disproportionate service outages in the software stack. Current recovery mechanisms are prohibitively slow, often requiring up to 10 minutes to reinitialize resources and reload massive model weights. We introduce KevlarFlow, a fault tolerant serving architecture designed to bridge the gap between hardware unreliability and service availability. KevlarFlow leverages 1) decoupled model parallelism initialization, 2) dynamic traffic rerouting, and 3) background KV cache replication to maintain high throughput during partial failures. Our evaluation demonstrates that KevlarFlow reduces mean-time-to-recovery (MTTR) by 20x and, under failure conditions, improves average latency by 3.1x, 99th percentile (p99) latency by 2.8x, average time-to-first-token (TTFT) by 378.9x, and p99 TTFT by 574.6x with negligible runtime overhead in comparison to state-of-the-art LLM serving systems.

</details>


### [37] [Coordinating Power Grid Frequency Regulation Service with Data Center Load Flexibility](https://arxiv.org/abs/2601.22487)
*Ali Jahanshahi,Sara Rashidi Golrouye,Osten Anderson,Nanpeng Yu,Daniel Wong*

Main category: cs.DC

TL;DR: This paper explores how modern GPU data centers can help reduce dependence on fossil-fueled power plants for frequency regulation, introducing a novel metric and framework to quantify and maximize carbon emission reductions.


<details>
  <summary>Details</summary>
Motivation: Data center growth increases energy consumption and carbon emissions while destabilizing power grids due to higher demand for frequency regulation services, which often rely on fossil-fueled power plants.

Method: The paper introduces the Exogenous Carbon metric to quantify grid-side carbon emission reductions and proposes the EcoCenter framework for GPU data centers to optimize their frequency regulation provision.

Result: Data center participation in frequency regulation demonstrates significant potential in reducing carbon emissions from grid services, sometimes surpassing operational carbon savings.

Conclusion: Leveraging GPU data centers for frequency regulation assists in reducing dependence on fossil fuel reserves, promoting cleaner energy utilization and significant Exogenous carbon savings.

Abstract: AI/ML data center growth have led to higher energy consumption and carbon emissions. The shift to renewable energy and growing data center energy demands can destabilize the power grid. Power grids rely on frequency regulation reserves, typically fossil-fueled power plants, to stabilize and balance the supply and demand of electricity. This paper sheds light on the hidden carbon emissions of frequency regulation service. Our work explores how modern GPU data centers can coordinate with power grids to reduce the need for fossil-fueled frequency regulation reserves. We first introduce a novel metric, Exogenous Carbon, to quantify grid-side carbon emission reductions resulting from data center participation in regulation service. We additionally introduce EcoCenter, a framework to maximize the amount of frequency regulation provision that GPU data centers can provide, and thus, reduce the amount of frequency regulation reserves necessary. We demonstrate that data center participation in frequency regulation can result in Exogenous carbon savings that oftentimes outweigh Operational carbon emissions.

</details>


### [38] [HetCCL: Accelerating LLM Training with Heterogeneous GPUs](https://arxiv.org/abs/2601.22585)
*Heehoon Kim,Jaehwan Lee,Taejeoung Kim,Jongwon Park,Jinpyo Kim,Pyongwon Suh,Ryan H. Choi,Sangwoo Lee,Jaejin Lee*

Main category: cs.DC

TL;DR: HetCCL introduces a library for efficient cross-vendor GPU communication, allowing heterogeneous GPU usage for deep learning frameworks.


<details>
  <summary>Details</summary>
Motivation: Large language models require powerful GPU clusters that are often heterogeneous, but current frameworks do not fully support efficient communication across different GPU vendors.

Method: HetCCL unifies vendor-specific backend libraries and introduces RDMA-based communication mechanisms for GPUs, leveraging existing libraries (NCCL for NVIDIA and RCCL for AMD).

Result: HetCCL achieves performance comparable to NCCL and RCCL in homogeneous setups, and scales effectively in heterogeneous GPU environments.

Conclusion: HetCCL enables high-performance training across mixed GPU vendors without modifying existing deep learning applications.

Abstract: The rapid growth of large language models is driving organizations to expand their GPU clusters, often with GPUs from multiple vendors. However, current deep learning frameworks lack support for collective communication across heterogeneous GPUs, leading to inefficiency and higher costs. We present HetCCL, a collective communication library that unifies vendor-specific backends and enables RDMA-based communication across GPUs without requiring driver modifications. HetCCL introduces two novel mechanisms that enable cross-vendor communication while leveraging optimized vendor libraries, NVIDIA NCCL and AMD RCCL. Evaluations on a multi-vendor GPU cluster show that HetCCL matches NCCL and RCCL performance in homogeneous setups while uniquely scaling in heterogeneous environments, enabling practical, high-performance training with both NVIDIA and AMD GPUs without changes to existing deep learning applications.

</details>


### [39] [CONCUR: High-Throughput Agentic Batch Inference of LLM via Congestion-Based Concurrency Control](https://arxiv.org/abs/2601.22705)
*Qiaoling Chen,Zhisheng Ye,Tian Tang,Peng Sun,Boyu Tian,Guoteng Wang,Shenggui Li,Yonggang Wen,Zhenhua Han,Tianwei Zhang*

Main category: cs.DC

TL;DR: The paper identifies a GPU key-value cache issue called middle-phase thrashing during batch inference for agentic workloads, proposing CONCUR, a control layer which prevents this issue and improves throughput.


<details>
  <summary>Details</summary>
Motivation: To prevent throughput degradation caused by middle-phase thrashing during GPU key-value cache usage in agentic workloads.

Method: The authors develop CONCUR, a control layer that uses a cache-aware control algorithm and runtime signals to regulate agent admission for balanced cache utilization.

Result: CONCUR demonstrates significant improvements in batch inference throughput, achieving up to 4.09x improvement on Qwen3-32B and 1.9x on DeepSeek-V3 models.

Conclusion: CONCUR effectively mitigates middle-phase thrashing, improves throughput, and seamlessly integrates with existing LLM serving systems.

Abstract: Batch inference for agentic workloads stresses the GPU key-value (KV) cache in a sustained and cumulative manner, often causing severe throughput degradation well before memory capacity is exhausted. We identify this phenomenon as middle-phase thrashing, a previously under-characterized pathology in which cache efficiency collapses as long-lived agents accumulate state over time.
  We argue that mitigating this pathology requires moving beyond reactive, request-level cache management to proactive, agent-level admission control. Drawing inspiration from congestion control in distributed systems, we view the KV cache as a shared resource whose efficient utilization depends on feedback-driven regulation. Based on this insight, we present CONCUR, a lightweight control layer that regulates agent admission to bound aggregate cache pressure while preserving execution continuity. CONCUR adapts a cache-aware control algorithm to dynamically adjust the number of active agents using runtime cache signals.
  Across large models and real-world agent workloads, CONCUR prevents middle-phase thrashing and improves batch inference throughput by up to 4.09x on Qwen3-32B and 1.9x on DeepSeek-V3, while remaining compatible with existing LLM serving systems.

</details>


### [40] [ERA: Epoch-Resolved Arbitration for Duelling Admins in Group Management CRDTs](https://arxiv.org/abs/2601.22963)
*Kegan Dougal*

Main category: cs.DC

TL;DR: This paper examines the challenges CRDTs face in maintaining consistent materialized views, particularly in scenarios with concurrent events like the 'Duelling Admins' problem.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from addressing surprising and potentially exploitable inconsistencies in CRDTs, particularly in critical use cases like managing group permissions.

Method: The paper introduces the concept of external arbiters and optional 'epoch events' to establish an asynchronous, bounded total order within epochs, enhancing CRDT consistency.

Result: The proposed method solves issues like the 'Duelling Admins' problem and prevents a Byzantine admin from exploiting concurrency.

Conclusion: By integrating an arbiter and incorporating epoch events, CRDTs gain improved consistency without sacrificing their high availability and eventual consistency properties.

Abstract: Conflict-Free Replicated Data Types (CRDTs) are used in a range of fields for their coordination-free replication with strong eventual consistency. By prioritising availability over consistency under partition, nodes accumulate events in different orders, and rely on an associative, commutative and idempotent merge function to present a materialised view of the CRDT. Under some circumstances, the state of the materialised view over time can appear to ''roll back'' previously applied events. When the materialised view is used to manage group permissions such as ones found in instant messaging applications, this can lead to surprising behaviour. This can occur when there are multiple concurrent events, such as in the Duelling Admins problem where two equally permissioned admins concurrently revoke each other's permissions. Who wins? This article argues that a Byzantine admin can exploit concurrency to win the duel. As a result, an external arbiter is required to arbitrate an immutable happens-before relation between concurrent events. Arbitration occurs asynchronously in batches via optional ''epoch events'', preserving availability. This introduces a bounded total order within epochs, and the resulting ''finality'' improves on the level of consistency CRDTs can provide.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [41] [Attention Isn't All You Need for Emotion Recognition:Domain Features Outperform Transformers on the EAV Dataset](https://arxiv.org/abs/2601.22161)
*Anmol Guragain*

Main category: cs.LG

TL;DR: This paper conducts a systematic study on multimodal emotion recognition using the EAV dataset and finds that domain-specific modifications outperform complex model architectures on small datasets.


<details>
  <summary>Details</summary>
Motivation: To investigate if complex attention mechanisms enhance performance on small-scale datasets for emotion recognition and to explore alternative approaches to improve accuracy.

Method: Implemented and compared three model categories: (1) baseline transformers (M1), (2) factorized attention mechanisms (M2), and (3) improved convolutional neural network (CNN) baselines (M3). Domain-specific modifications were also tested to enhance performance.

Result: Complex attention mechanisms underperformed due to overfitting, with M2 models scoring 5–13 percentage points below baselines. Domain-specific modifications like adding delta MFCCs improved CNN accuracy to 65.56% (+3.66pp), and EEG frequency-domain features achieved 67.62% (+7.62pp). Vision transformers (M1) reached 75.30%, surpassing the reported ViViT results.

Conclusion: Domain-specific improvements, rather than complex architectures, are more effective for small-scale multimodal emotion recognition tasks. Attention should prioritize domain knowledge and data-specific implementations.

Abstract: We present a systematic study of multimodal emotion recognition using the EAV dataset, investigating whether complex attention mechanisms improve performance on small datasets. We implement three model categories: baseline transformers (M1), novel factorized attention mechanisms (M2), and improved CNN baselines (M3). Our experiments show that sophisticated attention mechanisms consistently underperform on small datasets. M2 models achieved 5 to 13 percentage points below baselines due to overfitting and destruction of pretrained features. In contrast, simple domain-appropriate modifications proved effective: adding delta MFCCs to the audio CNN improved accuracy from 61.9\% to \textbf{65.56\%} (+3.66pp), while frequency-domain features for EEG achieved \textbf{67.62\%} (+7.62pp over the paper baseline). Our vision transformer baseline (M1) reached \textbf{75.30\%}, exceeding the paper's ViViT result (74.5\%) through domain-specific pretraining, and vision delta features achieved \textbf{72.68\%} (+1.28pp over the paper CNN). These findings demonstrate that for small-scale emotion recognition, domain knowledge and proper implementation outperform architectural complexity.

</details>


### [42] [Multitask Learning for Earth Observation Data Classification with Hybrid Quantum Network](https://arxiv.org/abs/2601.22195)
*Fan Fan,Yilei Shi,Tobias Guggemos,Xiao Xiang Zhu*

Main category: cs.LG

TL;DR: This paper proposes a hybrid quantum machine learning model for Earth Observation (EO) data classification, leveraging multitask learning and quantum convolution operations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational bottleneck in analyzing Big Data from EO using complex deep learning models by exploring the potential of quantum computing.

Method: A hybrid quantum machine learning model is designed incorporating multitask learning for better data encoding and a location weight module with quantum convolution to extract features.

Result: The model's effectiveness and generalizability were tested through multiple EO benchmarks, showing its potential and advantages despite limitations in quantum devices.

Conclusion: This research demonstrates the potential of quantum machine learning in EO data analysis, providing insights into factors that contribute to its advantages.

Abstract: Quantum machine learning (QML) has gained increasing attention as a potential solution to address the challenges of computation requirements in the future. Earth observation (EO) has entered the era of Big Data, and the computational demands for effectively analyzing large EO data with complex deep learning models have become a bottleneck. Motivated by this, we aim to leverage quantum computing for EO data classification and explore its advantages despite the current limitations of quantum devices. This paper presents a hybrid model that incorporates multitask learning to assist efficient data encoding and employs a location weight module with quantum convolution operations to extract valid features for classification. The validity of our proposed model was evaluated using multiple EO benchmarks. Additionally, we experimentally explored the generalizability of our model and investigated the factors contributing to its advantage, highlighting the potential of QML in EO data analysis.

</details>


### [43] [Neural Signals Generate Clinical Notes in the Wild](https://arxiv.org/abs/2601.22197)
*Jathurshan Pradeepkumar,Zheng Chen,Jimeng Sun*

Main category: cs.LG

TL;DR: This paper presents CELM as the first clinical EEG-to-language model for automating clinical report generation, demonstrating significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Clinicians spend extensive time generating reports for long-term EEG recordings. Automating this process can save effort and improve scalability in clinical settings.

Method: CELM integrates EEG foundation models with language models to summarize EEG recordings and generate reports end-to-end across multiple scales, using large-scale curated datasets.

Result: CELM outperforms baselines in both supervised and zero-shot settings, improving standard metrics like ROUGE-1 and METEOR by 70%-95%.

Conclusion: CELM establishes a new benchmark for EEG-based automated clinical reporting, combining multimodal pretraining to enhance efficiency in medical analysis.

Abstract: Generating clinical reports that summarize abnormal patterns, diagnostic findings, and clinical interpretations from long-term EEG recordings remains labor-intensive. We curate a large-scale clinical EEG dataset with $9{,}922$ reports paired with approximately $11{,}000$ hours of EEG recordings from $9{,}048$ patients. We therefore develop CELM, the first clinical EEG-to-Language foundation model capable of summarizing long-duration, variable-length EEG recordings and performing end-to-end clinical report generation at multiple scales, including recording description, background activity, epileptiform abnormalities, events/seizures, and impressions. Experimental results show that, with patient history supervision, our method achieves $70\%$--$95\%$ average relative improvements in standard generation metrics (e.g., ROUGE-1 and METEOR) from $0.2$--$0.3$ to $0.4$--$0.6$. In the zero-shot setting without patient history, CELM attains generation scores in the range of $0.43$--$0.52$, compared to baselines of $0.17$--$0.26$. CELM integrates pretrained EEG foundation models with language models to enable scalable multimodal learning. We release our model and benchmark construction pipeline at [URL].

</details>


### [44] [FedAdaVR: Adaptive Variance Reduction for Robust Federated Learning under Limited Client Participation](https://arxiv.org/abs/2601.22204)
*S M Ruhul Kabir Howlader,Xiao Chen,Yifei Xie,Lu Liu*

Main category: cs.LG

TL;DR: The paper introduces FedAdaVR, an algorithm for federated learning, addressing client participation heterogeneity with variance reduction and memory-efficient features.


<details>
  <summary>Details</summary>
Motivation: Address the common but underexplored issue of sporadic client participation in federated learning, which creates challenges like gradient noise and client drift.

Method: Proposed FedAdaVR, combining an adaptive optimizer with variance reduction. It stores and reuses previous client updates, even for absent clients, and introduced FedAdaVR-Quant for memory efficiency.

Result: FedAdaVR eliminates client participation errors and outperforms current methods in both IID and non-IID settings, with quantised FedAdaVR reducing memory usage significantly while preserving performance.

Conclusion: The proposed FedAdaVR algorithm effectively handles partial client participation challenges in federated learning, offering improved performance and memory efficiency.

Abstract: Federated learning (FL) encounters substantial challenges due to heterogeneity, leading to gradient noise, client drift, and partial client participation errors, the last of which is the most pervasive but remains insufficiently addressed in current literature. In this paper, we propose FedAdaVR, a novel FL algorithm aimed at solving heterogeneity issues caused by sporadic client participation by incorporating an adaptive optimiser with a variance reduction technique. This method takes advantage of the most recent stored updates from clients, even when they are absent from the current training round, thereby emulating their presence. Furthermore, we propose FedAdaVR-Quant, which stores client updates in quantised form, significantly reducing the memory requirements (by 50%, 75%, and 87.5%) of FedAdaVR while maintaining equivalent model performance. We analyse the convergence behaviour of FedAdaVR under general nonconvex conditions and prove that our proposed algorithm can eliminate partial client participation error. Extensive experiments conducted on multiple datasets, under both independent and identically distributed (IID) and non-IID settings, demonstrate that FedAdaVR consistently outperforms state-of-the-art baseline methods.

</details>


### [45] [Deep Learning-Based Early-Stage IR-Drop Estimation via CNN Surrogate Modeling](https://arxiv.org/abs/2601.22707)
*Ritesh Bhadana*

Main category: cs.LG

TL;DR: The paper proposes a deep learning method using CNNs to predict IR-drop heatmaps rapidly in early VLSI design stages.


<details>
  <summary>Details</summary>
Motivation: Accurate and fast IR-drop estimation is critical for early design stages since conventional methods are computationally expensive and require near-final layouts.

Method: A U-Net-based CNN model is trained using a synthetic physics-inspired dataset. It performs pixel-wise regression to map physical layout features to IR-drop heatmaps.

Result: The model predicts IR-drop distributions with high accuracy and millisecond inference times, enabling rapid early-stage design exploration.

Conclusion: This approach serves as a fast, effective complementary tool for early IR-drop analysis, accelerating iterative design workflows and reducing reliance on costly conventional methods.

Abstract: IR-drop is a critical power integrity challenge in modern VLSI designs that can cause timing degradation, reliability issues, and functional failures if not detected early in the design flow. Conventional IR-drop analysis relies on physics-based signoff tools, which provide high accuracy but incur significant computational cost and require near-final layout information, making them unsuitable for rapid early-stage design exploration. In this work, we propose a deep learning-based surrogate modeling approach for early-stage IR-drop estimation using a CNN. The task is formulated as a dense pixel-wise regression problem, where spatial physical layout features are mapped directly to IR-drop heatmaps. A U-Net-based encoder-decoder architecture with skip connections is employed to effectively capture both local and global spatial dependencies within the layout. The model is trained on a physics-inspired synthetic dataset generated by us, which incorporates key physical factors including power grid structure, cell density distribution, and switching activity. Model performance is evaluated using standard regression metrics such as Mean Squared Error (MSE) and Peak Signal-to-Noise Ratio (PSNR). Experimental results demonstrate that the proposed approach can accurately predict IR-drop distributions with millisecond-level inference time, enabling fast pre-signoff screening and iterative design optimization. The proposed framework is intended as a complementary early-stage analysis tool, providing designers with rapid IR-drop insight prior to expensive signoff analysis. The implementation, dataset generation scripts, and the interactive inference application are publicly available at: https://github.com/riteshbhadana/IR-Drop-Predictor. The live application can be accessed at: https://ir-drop-predictor.streamlit.app/.

</details>


### [46] [Causal Imitation Learning Under Measurement Error and Distribution Shift](https://arxiv.org/abs/2601.22206)
*Shi Bo,AmirEmad Ghassami*

Main category: cs.LG

TL;DR: The paper proposes a new framework for offline imitation learning called \texttt{CausIL} to address challenges caused by noisy measurements and distribution shifts.


<details>
  <summary>Details</summary>
Motivation: Standard behavioral cloning struggles in scenarios with measurement errors and distribution shifts, leading to biased policies under these conditions.

Method: \texttt{CausIL} explicitly models causal relationships among variables, using proximal causal inference and treating noisy observations as proxy variables. The method includes discrete and continuous estimators, with adversarial procedures employed for continuous state spaces.

Result: Evaluation using longitudinal medical data shows that \texttt{CausIL} improves robustness to distribution shifts when compared to behavioral cloning baselines.

Conclusion: Explicitly incorporating causal structures in imitation learning provides a robust approach to handling noisy measurements and distribution shifts.

Abstract: We study offline imitation learning (IL) when part of the decision-relevant state is observed only through noisy measurements and the distribution may change between training and deployment. Such settings induce spurious state-action correlations, so standard behavioral cloning (BC) -- whether conditioning on raw measurements or ignoring them -- can converge to systematically biased policies under distribution shift. We propose a general framework for IL under measurement error, inspired by explicitly modeling the causal relationships among the variables, yielding a target that retains a causal interpretation and is robust to distribution shift. Building on ideas from proximal causal inference, we introduce \texttt{CausIL}, which treats noisy state observations as proxy variables, and we provide identification conditions under which the target policy is recoverable from demonstrations without rewards or interactive expert queries. We develop estimators for both discrete and continuous state spaces; for continuous settings, we use an adversarial procedure over RKHS function classes to learn the required parameters. We evaluate \texttt{CausIL} on semi-simulated longitudinal data from the PhysioNet/Computing in Cardiology Challenge 2019 cohort and demonstrate improved robustness to distribution shift compared to BC baselines.

</details>


### [47] [Latent Spherical Flow Policy for Reinforcement Learning with Combinatorial Actions](https://arxiv.org/abs/2601.22211)
*Lingkai Kong,Anagha Satish,Hezi Jiang,Akseli Kangaslahti,Andrew Ma,Wenbo Chen,Mingxiao Song,Lily Xu,Milind Tambe*

Main category: cs.LG

TL;DR: LSFlow introduces a stochastic policy in latent space using spherical flow matching for combinatorial RL, ensuring feasibility and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods for combinatorial action spaces struggle with feasibility constraints and compromised generality.

Method: Proposes LSFlow, leveraging spherical flow matching in latent space and utilizing a combinatorial solver for structured actions.

Result: Empirical performance improvement over baselines by 20.6% across various combinatorial RL tasks.

Conclusion: LSFlow enhances expressiveness and efficiency in combinatorial RL while addressing challenges related to policy feasibility and value optimization.

Abstract: Reinforcement learning (RL) with combinatorial action spaces remains challenging because feasible action sets are exponentially large and governed by complex feasibility constraints, making direct policy parameterization impractical. Existing approaches embed task-specific value functions into constrained optimization programs or learn deterministic structured policies, sacrificing generality and policy expressiveness. We propose a solver-induced \emph{latent spherical flow policy} that brings the expressiveness of modern generative policies to combinatorial RL while guaranteeing feasibility by design. Our method, LSFlow, learns a \emph{stochastic} policy in a compact continuous latent space via spherical flow matching, and delegates feasibility to a combinatorial optimization solver that maps each latent sample to a valid structured action. To improve efficiency, we train the value network directly in the latent space, avoiding repeated solver calls during policy optimization. To address the piecewise-constant and discontinuous value landscape induced by solver-based action selection, we introduce a smoothed Bellman operator that yields stable, well-defined learning targets. Empirically, our approach outperforms state-of-the-art baselines by an average of 20.6\% across a range of challenging combinatorial RL tasks.

</details>


### [48] [FOCUS: DLLMs Know How to Tame Their Compute Bound](https://arxiv.org/abs/2601.23278)
*Kaihua Liang,Xin Tan,An Zhong,Hong Xu,Marco Canini*

Main category: cs.LG

TL;DR: This paper identifies inefficiencies in Diffusion Large Language Models (DLLMs) decoding and introduces FOCUS, a system that targets decodable tokens dynamically to improve throughput and scalability.


<details>
  <summary>Details</summary>
Motivation: The high decoding cost of DLLMs limits their usability and scalability. The inefficiency stems from computation being wasted on non-decodable tokens during each decoding step.

Method: FOCUS dynamically focuses computation only on decodable tokens during DLLM decoding. It monitors token importance based on attention mechanisms, evicts non-decodable tokens, and increases the effective batch size.

Result: FOCUS improves throughput by up to 3.52× compared to LMDeploy, while maintaining or enhancing generation quality across benchmarks.

Conclusion: FOCUS addresses computation inefficiencies in DLLMs and demonstrates a practical improvement in throughput and quality, making DLLMs more scalable and efficient for deployment.

Abstract: Diffusion Large Language Models (DLLMs) offer a compelling alternative to Auto-Regressive models, but their deployment is constrained by high decoding cost. In this work, we identify a key inefficiency in DLLM decoding: while computation is parallelized over token blocks, only a small subset of tokens is decodable at each diffusion step, causing most compute to be wasted on non-decodable tokens. We further observe a strong correlation between attention-derived token importance and token-wise decoding probability. Based on this insight, we propose FOCUS -- an inference system designed for DLLMs. By dynamically focusing computation on decodable tokens and evicting non-decodable ones on-the-fly, FOCUS increases the effective batch size, alleviating compute limitations and enabling scalable throughput. Empirical evaluations demonstrate that FOCUS achieves up to 3.52$\times$ throughput improvement over the production-grade engine LMDeploy, while preserving or improving generation quality across multiple benchmarks. The FOCUS system is publicly available on GitHub: https://github.com/sands-lab/FOCUS.

</details>


### [49] [DAJ: Data-Reweighted LLM Judge for Test-Time Scaling in Code Generation](https://arxiv.org/abs/2601.22230)
*Peijia Qin,Ruiyi Zhang,Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: The paper proposes DAJ, an LLM judge that improves the selection of code generation outputs using a bi-level, data-reweighted learning approach, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Address the deficiencies of LLM judges in code generation, particularly issues caused by distribution shifts and data mismatches, to enhance their reliability.

Method: DAJ is a reasoning-based LLM judge trained using verifiable rewards and a bi-level, data-reweighted learning framework. It automatically prioritizes hard problems, in-distribution samples, and trajectory-aligned data.

Result: DAJ achieves state-of-the-art results on LiveCodeBench and BigCodeBench, surpassing existing test-time scaling baselines and prominent proprietary models.

Conclusion: The proposed approach demonstrates that data reweighting effectively improves LLM judge training, enhancing performance across benchmarks without requiring ad-hoc heuristics.

Abstract: Test-time scaling for code generation commonly relies on Best-of-N selection, in which multiple candidate solutions are sampled from a base model, and the best one is selected by an LLM judge. However, training reliable LLM judges is challenging due to severe distribution shifts, including imbalances between easy and hard problems, mismatches between training tasks and evaluation benchmarks, and trajectory mismatch arising from training data generated by cheaper models whose behavior differs from that of inference-time models. We propose DAJ, a reasoning-based LLM judge trained with verifiable rewards under a bi-level data-reweighted learning framework. The proposed framework learns data-importance weights (either domain-level or instance-level) to optimize generalization performance on a held-out meta set aligned with target benchmarks. To the best of our knowledge, this is the first application of data reweighting to LLM-as-a-Judge training for test-time scaling. Our approach automatically emphasizes hard problems, in-distribution samples, and trajectory-aligned data, without relying on hand-crafted heuristics. Empirically, DAJ achieves state-of-the-art performance on LiveCodeBench and BigCodeBench, outperforming strong test-time scaling baselines as well as leading proprietary models.

</details>


### [50] [ZK-HybridFL: Zero-Knowledge Proof-Enhanced Hybrid Ledger for Federated Learning](https://arxiv.org/abs/2601.22302)
*Amirhossein Taherpour,Xiaodong Wang*

Main category: cs.LG

TL;DR: ZK-HybridFL is a novel framework for federated learning combining DAG ledger and zero-knowledge proofs to enhance scalability, privacy, and security.


<details>
  <summary>Details</summary>
Motivation: To address scalability, security, and privacy challenges in centralized and decentralized federated learning systems.

Method: It integrates a DAG ledger with dedicated sidechains and zero-knowledge proofs. Smart contracts and oracles validate local model updates while preserving data privacy, alongside a challenge mechanism for adversary detection.

Result: Experiments show improved convergence, accuracy, and efficiency compared to existing methods like Blade-FL and ChainFL, while ensuring robust security against adversarial components.

Conclusion: ZK-HybridFL provides a scalable, privacy-preserving, and secure solution for decentralized federated learning suitable for diverse environments.

Abstract: Federated learning (FL) enables collaborative model training while preserving data privacy, yet both centralized and decentralized approaches face challenges in scalability, security, and update validation. We propose ZK-HybridFL, a secure decentralized FL framework that integrates a directed acyclic graph (DAG) ledger with dedicated sidechains and zero-knowledge proofs (ZKPs) for privacy-preserving model validation. The framework uses event-driven smart contracts and an oracle-assisted sidechain to verify local model updates without exposing sensitive data. A built-in challenge mechanism efficiently detects adversarial behavior. In experiments on image classification and language modeling tasks, ZK-HybridFL achieves faster convergence, higher accuracy, lower perplexity, and reduced latency compared to Blade-FL and ChainFL. It remains robust against substantial fractions of adversarial and idle nodes, supports sub-second on-chain verification with efficient gas usage, and prevents invalid updates and orphanage-style attacks. This makes ZK-HybridFL a scalable and secure solution for decentralized FL across diverse environments.

</details>


### [51] [FunPRM: Function-as-Step Process Reward Model with Meta Reward Correction for Code Generation](https://arxiv.org/abs/2601.22249)
*Ruiyi Zhang,Peijia Qin,Qi Cao,Eric Xue,Pengtao Xie*

Main category: cs.LG

TL;DR: The paper presents FunPRM, a method to improve LLMs' code generation by focusing on modular function-based designs and correcting reward estimation noise, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: LLMs often fail on complex programming tasks, and current methods like PRMs struggle in code generation due to poor step decomposition and noisy rewards.

Method: FunPRM introduces modular function-based code generation and a meta-learning reward correction mechanism to purify noisy rewards using unit-test-based evaluation.

Result: FunPRM outperforms existing approaches on LiveCodeBench and BigCodeBench, achieving state-of-the-art results and generating more readable and reusable code.

Conclusion: FunPRM enhances LLM-based code generation by utilizing modular design and cleaner reward correction mechanisms, demonstrating practical improvements for developers.

Abstract: Code generation is a core application of large language models (LLMs), yet LLMs still frequently fail on complex programming tasks. Given its success in mathematical reasoning, test-time scaling approaches such as Process Reward Model (PRM)-based Best-of-N selection offer a promising way to improve performance. However, existing PRMs remain ineffective for code generation due to the lack of meaningful step decomposition in code and the noise of Monte Carlo-estimated partial-solution correctness scores (rewards). To address these challenges, we propose FunPRM. FunPRM prompts LLMs to encourage modular code generation organized into functions, with functions treated as PRM reasoning steps. Furthermore, FunPRM introduces a novel meta-learning-based reward correction mechanism that leverages clean final-solution rewards obtained via a unit-test-based evaluation system to purify noisy partial-solution rewards. Experiments on LiveCodeBench and BigCodeBench demonstrate that FunPRM consistently outperforms existing test-time scaling methods across five base LLMs, notably achieving state-of-the-art performance on LiveCodeBench when combined with O4-mini. Furthermore, FunPRM produces code that is more readable and reusable for developers.

</details>


### [52] [Symmetry Breaking in Transformers for Efficient and Interpretable Training](https://arxiv.org/abs/2601.22257)
*Eva Silverstein,Daniel Kunin,Vasudev Shyam*

Main category: cs.LG

TL;DR: The paper introduces a symmetry-breaking protocol in the attention mechanism to eliminate redundant rotational degrees of freedom, improving memory-efficient optimizers’ performance and enabling better interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the issue of extraneous rotational degrees of freedom in standard attention mechanisms that increase computational redundancy yet do not impact outputs.

Method: Introduce unlearned query and value biases to create a preferred orientation in rotational space, applied during pretraining 124M parameter transformer models with various optimization algorithms.

Result: The proposed method improves performance for memory-efficient optimizers, narrows gaps with advanced methods, and enhances interpretability by selectively amplifying semantically meaningful tokens within attention heads.

Conclusion: Minimal architectural modifications can lead to significant gains in both performance and interpretability, demonstrating the potential for more efficient model designs without compromising accuracy.

Abstract: The attention mechanism in its standard implementation contains extraneous rotational degrees of freedom that are carried through computation but do not affect model activations or outputs. We introduce a simple symmetry-breaking protocol that inserts a preferred direction into this rotational space through batchwise-sampled, unlearned query and value biases. This modification has two theoretically motivated and empirically validated consequences. First, it can substantially improve the performance of simple, memory-efficient optimizers, narrowing -- and in some cases closing -- the gap to successful but more complex memory-intensive adaptive methods. We demonstrate this by pretraining 124M parameter transformer models with four optimization algorithms (AdamW, SOAP, SGDM, and Energy Conserving Descent(ECD)) and evaluating both validation loss and downstream logical reasoning. Second, it enables an interpretable use of otherwise redundant rotational degrees of freedom, selectively amplifying semantically meaningful token classes within individual attention heads. Overall, our results show that minimal, principled architectural changes can simultaneously improve performance and interpretability.

</details>


### [53] [SAIR: Cost-Efficient Multi-Stage ML Pipeline Autoscaling via In-Context Reinforcement Learning](https://arxiv.org/abs/2601.22397)
*Jianchang Su,Yifan Zhang,Shengkai Lin,Shizhen Zhao,Yusheng Zheng,Yiwei Yang,Wei Zhang*

Main category: cs.LG

TL;DR: SAIR, an autoscaling framework, uses in-context reinforcement learning with LLMs for efficient resource management in multi-stage ML pipelines, achieving strong performance and cost reductions.


<details>
  <summary>Details</summary>
Motivation: Scaling multi-stage ML inference pipelines is challenging due to heterogeneity and dynamic bottleneck migration.

Method: SAIR utilizes a large language model (LLM) as an in-context RL controller, with features like Pareto-dominance reward shaping, surprisal-guided experience retrieval, and CUDA interception for GPU management.

Result: SAIR demonstrated up to 50% improvement in P99 latency, up to 97% cost reduction, and high bottleneck detection accuracy across various workloads.

Conclusion: The framework offers significant improvements in latency, resource efficiency, and bottleneck detection without requiring offline training.

Abstract: Multi-stage ML inference pipelines are difficult to autoscale due to heterogeneous resources, cross-stage coupling, and dynamic bottleneck migration. We present SAIR, an autoscaling framework that uses an LLM as an in-context reinforcement learning controller, improving its policy online from reward-labeled interaction histories without gradient updates. SAIR combines Pareto-dominance reward shaping with a provable separation margin, surprisal-guided experience retrieval for context efficiency, and fine-grained GPU rate control via user-space CUDA interception. We provide regret analysis decomposing error into retrieval coverage and LLM selection components. On four ML serving pipelines under three workload patterns, SAIR achieves the best or tied-best P99 latency and effective resource cost among deployed baselines, improving P99 by up to 50% and reducing effective cost by up to 97% (under GPU rate-control assumptions), with 86% bottleneck detection accuracy and no offline training.

</details>


### [54] [Tabular Foundation Models Can Do Survival Analysis](https://arxiv.org/abs/2601.22259)
*Da In Kim,Wei Siang Lai,Kelly W. Zhang*

Main category: cs.LG

TL;DR: This paper introduces a classification-based framework enabling tabular foundation models to perform survival analysis, overcoming challenges of right-censoring through label handling.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of adapting tabular foundation models to survival analysis, which is hindered by right-censoring where event times are unknown for some observations.

Method: The authors propose reformulating survival analysis as a series of binary classification problems by discretizing event times, allowing existing tabular foundation models to be used through in-context learning.

Result: The approach proves theoretically valid under standard assumptions and achieves superior performance across 53 real-world datasets, outperforming classical and deep learning baselines across survival metrics.

Conclusion: The proposed framework successfully repurposes tabular foundation models for accurate survival analysis, offering an effective solution to handle censored data and delivering better results than traditional methods.

Abstract: While tabular foundation models have achieved remarkable success in classification and regression, adapting them to model time-to-event outcomes for survival analysis is non-trivial due to right-censoring, where data observations may end before the event occurs. We develop a classification-based framework that reformulates both static and dynamic survival analysis as a series of binary classification problems by discretizing event times. Censored observations are naturally handled as examples with missing labels at certain time points. This classification formulation enables existing tabular foundation models to perform survival analysis through in-context learning without explicit training. We prove that under standard censoring assumptions, minimizing our binary classification loss recovers the true survival probabilities as the training set size increases. We demonstrate through evaluation across $53$ real-world datasets that off-the-shelf tabular foundation models with this classification formulation outperform classical and deep learning baselines on average over multiple survival metrics.

</details>


### [55] [AsyncMesh: Fully Asynchronous Optimization for Data and Pipeline Parallelism](https://arxiv.org/abs/2601.22442)
*Thalaiyasingam Ajanthan,Sameera Ramasinghe,Gil Avraham,Hadi Mohaghegh Dolatabadi,Chamin P Hewa Koneputugodage,Violetta Shevchenko,Yan Zuo,Alexander Long*

Main category: cs.LG

TL;DR: The paper introduces methods to mitigate communication bottlenecks in distributed neural network training using asynchronous updates and provides convergence guarantees with superior communication efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper aims to scale neural network training effectively across distributed devices by addressing the high communication costs associated with data and pipeline parallelism.

Method: The authors use asynchronous updates for both pipeline and data parallelism axes: weight look-ahead for pipeline parallelism and an asynchronous sparse averaging method with exponential moving average correction for data parallelism.

Result: The proposed approach matches the performance of a fully synchronous baseline, achieves efficiency in managing communication, and is validated on large-scale language models up to 1B parameters.

Conclusion: Asynchronous updates can efficiently overcome co-location and scalability issues in distributed neural network training while maintaining competitive performance and reducing communication overhead.

Abstract: Data and pipeline parallelism are key strategies for scaling neural network training across distributed devices, but their high communication cost necessitates co-located computing clusters with fast interconnects, limiting their scalability. We address this communication bottleneck by introducing asynchronous updates across both parallelism axes, relaxing the co-location requirement at the expense of introducing staleness between pipeline stages and data parallel replicas. To mitigate staleness, for pipeline parallelism, we adopt a weight look-ahead approach, and for data parallelism, we introduce an asynchronous sparse averaging method equipped with an exponential moving average based correction mechanism. We provide convergence guarantees for both sparse averaging and asynchronous updates. Experiments on large-scale language models (up to \em 1B parameters) demonstrate that our approach matches the performance of the fully synchronous baseline, while significantly reducing communication overhead.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [56] [Investigating the Interplay of Parameterization and Optimizer in Gradient-Free Topology Optimization: A Cantilever Beam Case Study](https://arxiv.org/abs/2601.22241)
*Jelle Westra,Iván Olarte Rodríguez,Niki van Stein,Thomas Bäck,Elena Raponi*

Main category: cs.NE

TL;DR: The study explores the impact of geometric parameterization and optimizer choice in black-box optimization for topology optimization, emphasizing the pivotal role of parameterization quality in determining optimization success.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand how geometric parameterization and optimizer choice interplay impacts the performance of black-box optimization methods for structural designs in topology optimization, focusing on engineering efficiency without gradient reliance.

Method: The method involves benchmarking three geometric parameterizations with three black-box optimizers—differential evolution, covariance matrix adaptation evolution strategy, and heteroscedastic evolutionary Bayesian optimization—across varying design spaces (10D, 20D, 50D).

Result: The study finds that geometric parameterization heavily influences optimization outcomes, surpassing the role of optimizer choice, and that high-quality representations yield robust performance across algorithms.

Conclusion: Geometric parameterization is pivotal in black-box optimization, influencing performance more than optimizer choice, and fair evaluation of algorithms requires considering the impacts of the design space induced by parameterization.

Abstract: Gradient-free black-box optimization (BBO) is widely used in engineering design and provides a flexible framework for topology optimization (TO), enabling the discovery of high-performing structural designs without requiring gradient information from simulations. Yet, its success depends on two key choices: the geometric parameterization defining the search space and the optimizer exploring it.
  This study investigates this interplay through a compliance minimization problem for a cantilever beam subject to a connectivity constraint. We benchmark three geometric parameterizations, each combined with three representative BBO algorithms: differential evolution, covariance matrix adaptation evolution strategy, and heteroscedastic evolutionary Bayesian optimization, across 10D, 20D, and 50D design spaces.
  Results reveal that parameterization quality has a stronger influence on optimization performance than optimizer choice: a well-structured parameterization enables robust and competitive performance across algorithms, whereas weaker representations increase optimizer dependency. Overall, this study highlights the dominant role of geometric parameterization in practical BBO-based TO and shows that algorithm performance and selection cannot be fairly assessed without accounting for the induced design space.

</details>


### [57] [Fairness-Aware Performance Evaluation for Multi-Party Multi-Objective Optimization](https://arxiv.org/abs/2601.22497)
*Zifan Zhao,Peilan Xu,Wenjian Luo*

Main category: cs.NE

TL;DR: The paper addresses fairness issues in evaluating multiparty multiobjective optimization problems (MPMOPs) by developing a generalized fairness-aware evaluation framework.


<details>
  <summary>Details</summary>
Motivation: Performance evaluation in MPMOPs traditionally favors certain decision-makers (DMs) through mean-based metrics, lacking fairness and ignoring diverse consensus needs.

Method: This study introduces a cooperative game-theoretic perspective with fairness axioms, a concession rate vector, and embeds classical metrics into a Nash-product-based framework.

Result: The proposed framework highlights algorithm performance in terms of fairness and consensus, outperforming classical metrics in distinguishing effective solutions.

Conclusion: The framework ensures fairness in MPMOP evaluations, aligning evaluation scores with balanced compromises and consensus considerations.

Abstract: In multiparty multiobjective optimization problems, solution sets are usually evaluated using classical performance metrics, aggregated across DMs. However, such mean-based evaluations may be unfair by favoring certain parties, as they assume identical geometric approximation quality to each party's PF carries comparable evaluative significance. Moreover, prevailing notions of MPMOP optimal solutions are restricted to strictly common Pareto optimal solutions, representing a narrow form of cooperation in multiparty decision making scenarios. These limitations obscure whether a solution set reflects balanced relative gains or meaningful consensus among heterogeneous DMs. To address these issues, this paper develops a fairness-aware performance evaluation framework grounded in a generalized notion of consensus solutions. From a cooperative game-theoretic perspective, we formalize four axioms that a fairness-aware evaluation function for MPMOPs should satisfy. By introducing a concession rate vector to quantify acceptable compromises by individual DMs, we generalize the classical definition of MPMOP optimal solutions and embed classical performance metrics into a Nash-product-based evaluation framework, which is theoretically shown to satisfy all axioms. To support empirical validation, we further construct benchmark problems that extend existing MPMOP suites by incorporating consensus-deficient negotiation structures. Experimental results demonstrate that the proposed evaluation framework is able to distinguish algorithmic performance in a manner consistent with consensus-aware fairness considerations. Specifically, algorithms converging toward strictly common solutions are assigned higher evaluation scores when such solutions exist, whereas in the absence of strictly common solutions, algorithms that effectively cover the commonly acceptable region are more favorably evaluated.

</details>


### [58] [Detect and Act: Automated Dynamic Optimizer through Meta-Black-Box Optimization](https://arxiv.org/abs/2601.22542)
*Zijian Gao,Yuanting Zhong,Zeyuan Ma,Yue-Jiao Gong,Hongshu Guo*

Main category: cs.NE

TL;DR: This paper introduces a reinforcement learning-assisted method to improve evolutionary algorithms for dynamic optimization problems by enabling automated environment variation detection and self-adaptation.


<details>
  <summary>Details</summary>
Motivation: Dynamic Optimization Problems are complex due to constantly changing environments, and existing evolutionary methods rely on static, human-designed adaptive strategies that may fail in diverse scenarios.

Method: The study employs a deep Q-network to detect optimization dynamics and adapt search strategies in evolutionary algorithms, leveraging a bi-level learning-to-optimize approach.

Result: The proposed method demonstrated flexible search behavior and superior performance when tested on a diverse set of increasingly difficult dynamic optimization problems, outperforming state-of-the-art methods.

Conclusion: The approach successfully automates environment variation detection and adaptation, showing significant potential for generalization to unseen dynamic optimization problems.

Abstract: Dynamic Optimization Problems (DOPs) are challenging to address due to their complex nature, i.e., dynamic environment variation. Evolutionary Computation methods are generally advantaged in solving DOPs since they resemble dynamic biological evolution. However, existing evolutionary dynamic optimization methods rely heavily on human-crafted adaptive strategy to detect environment variation in DOPs, and then adapt the searching strategy accordingly. These hand-crafted strategies may perform ineffectively at out-of-box scenarios. In this paper, we propose a reinforcement learning-assisted approach to enable automated variation detection and self-adaption in evolutionary algorithms. This is achieved by borrowing the bi-level learning-to-optimize idea from recent Meta-Black-Box Optimization works. We use a deep Q-network as optimization dynamics detector and searching strategy adapter: It is fed as input with current-step optimization state and then dictates desired control parameters to underlying evolutionary algorithms for next-step optimization. The learning objective is to maximize the expected performance gain across a problem distribution. Once trained, our approach could generalize toward unseen DOPs with automated environment variation detection and self-adaption. To facilitate comprehensive validation, we further construct an easy-to-difficult DOPs testbed with diverse synthetic instances. Extensive benchmark results demonstrate flexible searching behavior and superior performance of our approach in solving DOPs, compared to state-of-the-art baselines.

</details>


### [59] [COBRA++: Enhanced COBRA Optimizer with Augmented Surrogate Pool and Reinforced Surrogate Selection](https://arxiv.org/abs/2601.22624)
*Zepei Yu,Zhiyang Huang,Hongshu Guo,Yue-Jiao Gong,Zeyuan Ma*

Main category: cs.NE

TL;DR: The paper proposes COBRA++, a learning-based adaptive strategy that enhances the COBRA optimizer for constrained optimization problems, by expanding surrogate model diversity and introducing reinforcement learning-based model selection.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing COBRA optimizers, which rely on manually chosen surrogate models and require intensive fine-tuning for novel tasks.

Method: The method introduces a diverse surrogate model pool and uses reinforcement learning to create an online model selection policy for optimizing constrained problems effectively.

Result: COBRA++ demonstrates significant performance improvements compared to vanilla COBRA and its adaptive variant through comprehensive validation experiments.

Conclusion: COBRA++ enhances the COBRA optimizer by automating the surrogate model selection process and improving overall optimization efficiency and accuracy.

Abstract: The optimization problems in realistic world present significant challenges onto optimization algorithms, such as the expensive evaluation issue and complex constraint conditions. COBRA optimizer (including its up-to-date variants) is a representative and effective tool for addressing such optimization problems, which introduces 1) RBF surrogate to reduce online evaluation and 2) bi-stage optimization process to alternate search for feasible solution and optimal solution. Though promising, its design space, i.e., surrogate model pool and selection standard, is still manually decided by human expert, resulting in labor-intensive fine-tuning for novel tasks. In this paper, we propose a learning-based adaptive strategy (COBRA++) that enhances COBRA in two aspects: 1) An augmented surrogate pool to break the tie with RBF-like surrogate and hence enhances model diversity and approximation capability; 2) A reinforcement learning-based online model selection policy that empowers efficient and accurate optimization process. The model selection policy is trained to maximize overall performance of COBRA++ across a distribution of constrained optimization problems with diverse properties. We have conducted multi-dimensional validation experiments and demonstrate that COBRA++ achieves substantial performance improvement against vanilla COBRA and its adaptive variant. Ablation studies are provided to support correctness of each design component in COBRA++.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [60] [Recursive Mutexes in Separation Logic](https://arxiv.org/abs/2601.22557)
*Ke Du,William Mansky,Paolo G. Giarrusso,Gregory Malecha*

Main category: cs.PL

TL;DR: The paper develops specifications for recursive mutexes in separation logic, covering common styles used in object-oriented languages.


<details>
  <summary>Details</summary>
Motivation: Recursive mutexes are widely used in object-oriented languages like C++ and Java, but their behavior and specifications require systematic formalization.

Method: The authors proposed specifications for recursive mutexes in separation logic, offering uniform handling of acquire/release operations and managing lock invariants.

Result: The approach simplifies client requirements to determine if they hold the mutex while accessing lock invariants.

Conclusion: The developed specifications streamline reasoning about recursive mutexes and ensure clarity for client interactions with lock invariants.

Abstract: Mutexes (i.e., locks) are well understood in separation logic, and can be specified in terms of either protecting an invariant or atomically changing the state of the lock. In this abstract, we develop the same styles of specifications for \emph{recursive} mutexes, a common variant of mutexes in object-oriented languages such as C++ and Java. A recursive mutex can be acquired any number of times by the same thread, and our specifications treat all acquires/releases uniformly, with clients only needing to determine whether they hold the mutex when accessing the lock invariant.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [61] [Advanced techniques and applications of LiDAR Place Recognition in Agricultural Environments: A Comprehensive Survey](https://arxiv.org/abs/2601.22198)
*Judith Vilella-Cantos,Mónica Ballesta,David Valiente,María Flores,Luis Payá*

Main category: cs.RO

TL;DR: This paper reviews deep learning applications for LiDAR-based localization in agriculture, analyzing challenges, techniques, datasets, and future directions.


<details>
  <summary>Details</summary>
Motivation: Localizing autonomous robotic systems efficiently in agricultural environments is challenging due to the lack of distinct features and unstructured landscapes.

Method: The paper reviews existing methods, datasets, and metrics for LiDAR place recognition (LPR) in agriculture, emphasizing deep learning techniques.

Result: A comprehensive analysis of challenges, existing approaches, and evaluation standards for LPR systems in agricultural environments is presented.

Conclusion: The study aims to provide clarity and encourage further research on LiDAR-based localization in agricultural settings, a largely unexplored area.

Abstract: An optimal solution to the localization problem is essential for developing autonomous robotic systems. Apart from autonomous vehicles, precision agriculture is one of the elds that can bene t most from these systems. Although LiDAR place recognition is a widely used technique in recent years to achieve accurate localization, it is mostly used in urban settings. However, the lack of distinctive features and the unstructured nature of agricultural environments make place recognition challenging. This work presents a comprehensive review of state-of-the-art the latest deep learning applications for agricultural environments and LPR techniques. We focus on the challenges that arise in these environments. We analyze the existing approaches, datasets, and metrics used to evaluate LPR system performance and discuss the limitations and future directions of research in this eld. This is the rst survey that focuses on LiDAR based localization in agricultural settings, with the aim of providing a thorough understanding and fostering further research in this specialized domain.

</details>


### [62] [Game-Based and Gamified Robotics Education: A Comparative Systematic Review and Design Guidelines](https://arxiv.org/abs/2601.22199)
*Syed T. Mubarrat,Byung-Cheol Min,Tianyu Shao,E. Cho Smith,Bedrich Benes,Alejandra J. Magana,Christos Mousas,Dominic Kao*

Main category: cs.RO

TL;DR: The paper conducted a systematic review comparing game-based learning (GBL) and gamification in robotics education, analyzing 95 studies to uncover trends, approaches, and new research directions.


<details>
  <summary>Details</summary>
Motivation: To better understand how GBL and gamification influence robotics education and address the lack of clarity on their comparative impact.

Method: The study followed a PRISMA-aligned systematic review methodology, analyzing 95 studies from 12,485 records across four databases between 2014-2025, using coding approaches for learning context, pedagogy, and outcomes.

Result: GBL was more common in informal learning contexts and gamification in formal education. The focus was mostly on introductory programming with minimal adoption of advanced technologies, and most studies were short-term with self-reported outcomes.

Conclusion: The paper identified key patterns, highlighted gaps like the limited use of advanced software/hardware, and proposed research directions and best practices for robotics education using GBL and gamification.

Abstract: Robotics education fosters computational thinking, creativity, and problem-solving, but remains challenging due to technical complexity. Game-based learning (GBL) and gamification offer engagement benefits, yet their comparative impact remains unclear. We present the first PRISMA-aligned systematic review and comparative synthesis of GBL and gamification in robotics education, analyzing 95 studies from 12,485 records across four databases (2014-2025). We coded each study's approach, learning context, skill level, modality, pedagogy, and outcomes (k = .918). Three patterns emerged: (1) approach-context-pedagogy coupling (GBL more prevalent in informal settings, while gamification dominated formal classrooms [p < .001] and favored project-based learning [p = .009]); (2) emphasis on introductory programming and modular kits, with limited adoption of advanced software (~17%), advanced hardware (~5%), or immersive technologies (~22%); and (3) short study horizons, relying on self-report. We propose eight research directions and a design space outlining best practices and pitfalls, offering actionable guidance for robotics education.

</details>


### [63] [ReloPush-BOSS: Optimization-guided Nonmonotone Rearrangement Planning for a Car-like Robot Pusher](https://arxiv.org/abs/2601.22289)
*Jeeho Ahn,Christoforos Mavrogiannis*

Main category: cs.RO

TL;DR: This paper addresses the problem of multi-object rearrangement planning in cluttered environments using a car-like robot, proposing optimized prerelocations informed by path classification and a novel search framework for efficient object sequences.


<details>
  <summary>Details</summary>
Motivation: Avoid challenges of kinematic, geometric, and physics constraints causing local minima in rearrangement planning, and improve feasibility and cost efficiency.

Method: Proposes a framework, ReloPush-BOSS, integrating Dubins path-informed prerelocations and depth-first search in a graph constructed from object traversal constraints.

Result: Achieves highest success rates and shortest pushing paths in densely cluttered scenarios with up to 13 objects, outperforming state-of-the-art baselines.

Conclusion: Optimized strategies for prerelocations, combined with systematic search methods, enhance the robot's ability to rearrange objects efficiently in challenging environments.

Abstract: We focus on multi-object rearrangement planning in densely cluttered environments using a car-like robot pusher. The combination of kinematic, geometric and physics constraints underlying this domain results in challenging nonmonotone problem instances which demand breaking each manipulation action into multiple parts to achieve a desired object rearrangement. Prior work tackles such instances by planning prerelocations, temporary object displacements that enable constraint satisfaction, but deciding where to prerelocate remains difficult due to local minima leading to infeasible or high-cost paths. Our key insight is that these minima can be avoided by steering a prerelocation optimization toward low-cost regions informed by Dubins path classification. These optimized prerelocations are integrated into an object traversability graph that encodes kinematic, geometric, and pushing constraints. Searching this graph in a depth-first fashion results in efficient, feasible rearrangement sequences. Across a series of densely cluttered scenarios with up to 13 objects, our framework, ReloPush-BOSS, exhibits consistently highest success rates and shortest pushing paths compared to state-of-the-art baselines. Hardware experiments on a 1/10 car-like pusher demonstrate the robustness of our approach. Code and footage from our experiments can be found at: https://fluentrobotics.com/relopushboss.

</details>


### [64] [Lantern: A Minimalist Robotic Object Platform](https://arxiv.org/abs/2601.22381)
*Victor Nikhil Antony,Zhili Gong,Guanchen Li,Clara Jeon,Chien-Ming Huang*

Main category: cs.RO

TL;DR: The paper introduces 'Lantern,' a low-cost robotic object platform for Human-Robot Interaction (HRI), and evaluates its applications through diverse studies, demonstrating its accessibility and potential uses.


<details>
  <summary>Details</summary>
Motivation: To create a low-cost, minimalist robotic platform that seamlessly integrates into human environments, enabling widespread exploration of Human-Robot Interaction (HRI) scenarios through accessible and versatile tools.

Method: Developed Lantern through meticulous design iterations to ensure affordability and functionality. Evaluated its HRI potential via co-design workshops, case studies, distribution to labs, integration in academic courses, and public exhibitions.

Result: Lantern proved effective in promoting engagement and demonstrated versatility in applications such as emotion regulation and focused work. Its accessibility makes it a valuable tool for advancing HRI research.

Conclusion: Lantern serves as an accessible and multifunctional platform, reducing barriers to participation in the HRI field and fostering novel research opportunities.

Abstract: Robotic objects are simple actuated systems that subtly blend into human environments. We design and introduce Lantern, a minimalist robotic object platform to enable building simple robotic artifacts. We conducted in-depth design and engineering iterations of Lantern's mechatronic architecture to meet specific design goals while maintaining a low build cost (~40 USD). As an extendable, open-source platform, Lantern aims to enable exploration of a range of HRI scenarios by leveraging human tendency to assign social meaning to simple forms. To evaluate Lantern's potential for HRI, we conducted a series of explorations: 1) a co-design workshop, 2) a sensory room case study, 3) distribution to external HRI labs, 4) integration into a graduate-level HRI course, and 5) public exhibitions with older adults and children. Our findings show that Lantern effectively evokes engagement, can support versatile applications ranging from emotion regulation to focused work, and serves as a viable platform for lowering barriers to HRI as a field.

</details>


### [65] [Plant-Inspired Robot Design Metaphors for Ambient HRI](https://arxiv.org/abs/2601.22387)
*Victor Nikhil Antony,Adithya R N,Sarah Derrick,Zhili Gong,Peter M. Donley,Chien-Ming Huang*

Main category: cs.RO

TL;DR: The paper investigates plant-inspired metaphors for designing novel human-robot interactions (HRI), presenting prototypes and insights to reshape HRI grounded in plant characteristics.


<details>
  <summary>Details</summary>
Motivation: HRI is traditionally based on anthropomorphic or zoomorphic paradigms, producing overt forms of engagement. This paper seeks to explore plant-inspired metaphors as an alternative to create ambient, low-demand interaction models.

Method: The authors utilized a Research through Design (RtD) approach, involving iterative ideation, prototyping, and reflection cycles. Workshops centered on the prototypes examined people's perceptions and imaginaries of plant-inspired robots.

Result: A suite of speculative, open-source prototypes were developed exploring plant-inspired expressions. Findings revealed insights into how plant metaphors influence people's perceptions and imaginaries of HRI.

Conclusion: The paper offers plant-inspired artifacts, design insights for understanding user perceptions, and design considerations for leveraging plant metaphors to potentially reshape HRI paradigms.

Abstract: Plants offer a paradoxical model for interaction: they are ambient, low-demand presences that nonetheless shape atmosphere, routines, and relationships through temporal rhythms and subtle expressions. In contrast, most human-robot interaction (HRI) has been grounded in anthropomorphic and zoomorphic paradigms, producing overt, high-demand forms of engagement. Using a Research through Design (RtD) methodology, we explore plants as metaphoric inspiration for HRI; we conducted iterative cycles of ideation, prototyping, and reflection to investigate what design primitives emerge from plant metaphors and morphologies, and how these primitives can be combined into expressive robotic forms. We present a suite of speculative, open-source prototypes that help probe plant-inspired presence, temporality, form, and gestures. We deepened our learnings from design and prototyping through prototype-centered workshops that explored people's perceptions and imaginaries of plant-inspired robots. This work contributes: (1) Set of plant-inspired robotic artifacts; (2) Designerly insights on how people perceive plant-inspired robots; and (3) Design consideration to inform how to use plant metaphors to reshape HRI.

</details>


### [66] [Accurate Pedestrian Tracking in Urban Canyons: A Multi-Modal Fusion Approach](https://arxiv.org/abs/2601.22406)
*Shahar Dubiner,Peng Ren,Roberto Manduchi*

Main category: cs.RO

TL;DR: The paper proposes a pedestrian navigation system that integrates GNSS and inertial data using a particle filter method to improve localization accuracy in urban areas, particularly benefiting blind or low-vision users.


<details>
  <summary>Details</summary>
Motivation: To enhance localization accuracy in urban settings where GNSS performance is degraded, aiding blind or low-vision users in navigating precisely.

Method: The authors use a particle filter-based fusion approach combining GNSS and inertial data (using RoNIN machine learning), incorporating map-based spatial priors to improve localization.

Result: System evaluation in downtown San Francisco shows improved localization accuracy across sidewalk correctness and other metrics compared to GNSS-only methods.

Conclusion: The fused navigation approach significantly improves localization compared to GNSS alone, offering a practical solution for accurate pedestrian guidance in urban environments.

Abstract: The contribution describes a pedestrian navigation approach designed to improve localization accuracy in urban environments where GNSS performance is degraded, a problem that is especially critical for blind or low-vision users who depend on precise guidance such as identifying the correct side of a street. To address GNSS limitations and the impracticality of camera-based visual positioning, the work proposes a particle filter based fusion of GNSS and inertial data that incorporates spatial priors from maps, such as impassable buildings and unlikely walking areas, functioning as a probabilistic form of map matching. Inertial localization is provided by the RoNIN machine learning method, and fusion with GNSS is achieved by weighting particles based on their consistency with GNSS estimates and uncertainty. The system was evaluated on six challenging walking routes in downtown San Francisco using three metrics related to sidewalk correctness and localization error. Results show that the fused approach (GNSS+RoNIN+PF) significantly outperforms GNSS only localization on most metrics, while inertial-only localization with particle filtering also surpasses GNSS alone for critical measures such as sidewalk assignment and across street error.

</details>


### [67] [High-Definition 5MP Stereo Vision Sensing for Robotics](https://arxiv.org/abs/2601.22445)
*Leaf Jiang,Matthew Holzel,Bernhard Kaplan,Hsiou-Yuan Liu,Sabyasachi Paul,Karen Rankin,Piotr Swierczynski*

Main category: cs.RO

TL;DR: This paper develops a new method to enhance calibration and stereo processing for high-resolution stereo vision systems, enabling high-quality 3D point clouds.


<details>
  <summary>Details</summary>
Motivation: Current stereo vision systems with high-resolution cameras require better calibration accuracy and faster processing, which conventional methods fail to deliver.

Method: The authors propose an innovative frame-to-frame calibration and stereo matching method, paired with a novel evaluation technique using real-time and ground-truth disparity map comparisons.

Result: The method successfully achieves both high accuracy and speed for 5MP camera image processing, producing high-quality 3D point clouds.

Conclusion: High-pixel-count cameras can only meet their potential when paired with advanced calibration techniques, enabling superior stereo vision performance.

Abstract: High-resolution (5MP+) stereo vision systems are essential for advancing robotic capabilities, enabling operation over longer ranges and generating significantly denser and accurate 3D point clouds. However, realizing the full potential of high-angular-resolution sensors requires a commensurately higher level of calibration accuracy and faster processing -- requirements often unmet by conventional methods. This study addresses that critical gap by processing 5MP camera imagery using a novel, advanced frame-to-frame calibration and stereo matching methodology designed to achieve both high accuracy and speed. Furthermore, we introduce a new approach to evaluate real-time performance by comparing real-time disparity maps with ground-truth disparity maps derived from more computationally intensive stereo matching algorithms. Crucially, the research demonstrates that high-pixel-count cameras yield high-quality point clouds only through the implementation of high-accuracy calibration.

</details>


### [68] [CARE: Multi-Task Pretraining for Latent Continuous Action Representation in Robot Control](https://arxiv.org/abs/2601.22467)
*Jiaqi Shi,Xulong Zhang,Xiaoyang Qu,Jianzong Wang*

Main category: cs.RO

TL;DR: CARE is a VLA model training framework for robotic task execution that eliminates the need for action labels, using video-text pairs and achieving strong performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the scalability and generalization limitations of current VLA models that rely on action supervision.

Method: CARE leverages video-text pairs to train VLA models without explicit action labels, using a multi-task pretraining objective and fine-tuning with small labeled datasets.

Result: CARE outperforms existing models in terms of success rate, semantic interpretability, and avoids shortcut learning in simulation tasks.

Conclusion: CARE presents a scalable and interpretable approach to robotic control using weak supervision, demonstrating effectiveness in eliminating dependence on action annotations.

Abstract: Recent advances in Vision-Language-Action (VLA) models have shown promise for robot control, but their dependence on action supervision limits scalability and generalization. To address this challenge, we introduce CARE, a novel framework designed to train VLA models for robotic task execution. Unlike existing methods that depend on action annotations during pretraining, CARE eliminates the need for explicit action labels by leveraging only video-text pairs. These weakly aligned data sources enable the model to learn continuous latent action representations through a newly designed multi-task pretraining objective. During fine-tuning, a small set of labeled data is used to train the action head for control. Experimental results across various simulation tasks demonstrate CARE's superior success rate, semantic interpretability, and ability to avoid shortcut learning. These results underscore CARE's scalability, interpretability, and effectiveness in robotic control with weak supervision.

</details>


### [69] [RoboStriker: Hierarchical Decision-Making for Autonomous Humanoid Boxing](https://arxiv.org/abs/2601.22517)
*Kangning Yin,Zhe Cao,Wentao Dong,Weishuai Zeng,Tianyi Zhang,Qiang Zhang,Jingbo Wang,Jiangmiao Pang,Ming Zhou,Weinan Zhang*

Main category: cs.RO

TL;DR: The paper introduces RoboStriker, a hierarchical model for humanoid boxing that separates strategic reasoning and physical execution using motion learning and advanced reinforcement training.


<details>
  <summary>Details</summary>
Motivation: The challenge of achieving dynamic, human-like intelligence and agility in humanoid robots, particularly in complex tasks like boxing.

Method: Uses a three-stage hierarchical framework: (1) learns boxing skills from human motion data, (2) distills these into a structured latent manifold, (3) employs Latent-Space Neural Fictitious Self-Play (LS-NFSP) for multi-agent training in latent action space.

Result: Experimental evidence shows that RoboStriker outperforms other models in simulation and demonstrates good sim-to-real transfer capabilities.

Conclusion: RoboStriker significantly advances autonomous humanoid boxing, proving the effectiveness of its hierarchical approach and latent-space training.

Abstract: Achieving human-level competitive intelligence and physical agility in humanoid robots remains a major challenge, particularly in contact-rich and highly dynamic tasks such as boxing. While Multi-Agent Reinforcement Learning (MARL) offers a principled framework for strategic interaction, its direct application to humanoid control is hindered by high-dimensional contact dynamics and the absence of strong physical motion priors. We propose RoboStriker, a hierarchical three-stage framework that enables fully autonomous humanoid boxing by decoupling high-level strategic reasoning from low-level physical execution. The framework first learns a comprehensive repertoire of boxing skills by training a single-agent motion tracker on human motion capture data. These skills are subsequently distilled into a structured latent manifold, regularized by projecting the Gaussian-parameterized distribution onto a unit hypersphere. This topological constraint effectively confines exploration to the subspace of physically plausible motions. In the final stage, we introduce Latent-Space Neural Fictitious Self-Play (LS-NFSP), where competing agents learn competitive tactics by interacting within the latent action space rather than the raw motor space, significantly stabilizing multi-agent training. Experimental results demonstrate that RoboStriker achieves superior competitive performance in simulation and exhibits sim-to-real transfer. Our website is available at RoboStriker.

</details>


### [70] [Adapting Reinforcement Learning for Path Planning in Constrained Parking Scenarios](https://arxiv.org/abs/2601.22545)
*Feng Tao,Luca Paparusso,Chenyi Gu,Robin Koehler,Chenxu Wu,Xinyu Huang,Christian Juette,David Paz,Ren Liu*

Main category: cs.RO

TL;DR: This paper presents a Deep Reinforcement Learning framework for real-time path planning in constrained parking scenarios, demonstrating significant improvements over classical planners.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address challenges in real-time path planning for autonomous systems in constrained environments, such as parking scenarios, where traditional planners struggle due to high computational costs and reliance on ideal perception.

Method: A DRL framework is developed for solving constrained path planning tasks using a bicycle model dynamics for sequential decision-making. This bypasses the need for expensive perception modules like localization and tracking while enabling lightweight, real-time action generation.

Result: The proposed framework outperforms classical planners, achieving a success rate improvement of +96% and efficiency improvement of +52%.

Conclusion: This DRL approach provides a robust, practical solution for real-time path planning in complex environments. The authors also release their benchmark as an open-source resource to encourage further research in autonomous systems.

Abstract: Real-time path planning in constrained environments remains a fundamental challenge for autonomous systems. Traditional classical planners, while effective under perfect perception assumptions, are often sensitive to real-world perception constraints and rely on online search procedures that incur high computational costs. In complex surroundings, this renders real-time deployment prohibitive. To overcome these limitations, we introduce a Deep Reinforcement Learning (DRL) framework for real-time path planning in parking scenarios. In particular, we focus on challenging scenes with tight spaces that require a high number of reversal maneuvers and adjustments. Unlike classical planners, our solution does not require ideal and structured perception, and in principle, could avoid the need for additional modules such as localization and tracking, resulting in a simpler and more practical implementation. Also, at test time, the policy generates actions through a single forward pass at each step, which is lightweight enough for real-time deployment. The task is formulated as a sequential decision-making problem grounded in a bicycle model dynamics, enabling the agent to directly learn navigation policies that respect vehicle kinematics and environmental constraints in the closed-loop setting. A new benchmark is developed to support both training and evaluation, capturing diverse and challenging scenarios. Our approach achieves state-of-the-art success rates and efficiency, surpassing classical planner baselines by +96% in success rate and +52% in efficiency. Furthermore, we release our benchmark as an open-source resource for the community to foster future research in autonomous systems. The benchmark and accompanying tools are available at https://github.com/dqm5rtfg9b-collab/Constrained_Parking_Scenarios.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [71] [Linux Kernel Recency Matters, CVE Severity Doesn't, and History Fades](https://arxiv.org/abs/2601.22196)
*Piotr Przymus,Witold Weiner,Krzysztof Rykaczewski,Gunnar Kudrjavets*

Main category: cs.SE

TL;DR: The Linux kernel's CVE tracking process was formalized in 2024, and analysis reveals patching is influenced by kernel recency rather than vulnerability severity.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics behind kernel CVEs and the factors influencing their patching process, following the Linux kernel becoming a CVE Numbering Authority.

Method: The study uses metadata, related commits, and patch latency measurements to analyze vulnerability tracking and patching in the Linux kernel.

Result: Results indicate that newer kernel versions are patched faster, while older ones retain unresolved CVEs. The complexity and scope of vulnerability-introducing commits exceed their fixes.

Conclusion: Severity metrics play a minor role in patch latency. Kernel recency is more predictive, and the Linux kernel's CVE process remains distinct in the open-source ecosystem.

Abstract: In 2024, the Linux kernel became its own Common Vulnerabilities and Exposures (CVE) Numbering Authority (CNA), formalizing how kernel vulnerabilities are identified and tracked. We analyze the anatomy and dynamics of kernel CVEs using metadata, associated commits, and patch latency to understand what drives patching. Results show that severity and Common Vulnerability Scoring System (CVSS) metrics have a negligible association with patch latency, whereas kernel recency is a reasonable predictor in survival models. Kernel developers fix newer kernels sooner, while older ones retain unresolved CVEs. Commits introducing vulnerabilities are typically broader and more complex than their fixes, though often only approximate reconstructions of development history. The Linux kernel remains a unique open-source project -- its CVE process is no exception.

</details>


### [72] [Stalled, Biased, and Confused: Uncovering Reasoning Failures in LLMs for Cloud-Based Root Cause Analysis](https://arxiv.org/abs/2601.22208)
*Evelien Riddell,James Riddell,Gengyi Sun,Michał Antkiewicz,Krzysztof Czarnecki*

Main category: cs.SE

TL;DR: This paper analyzes Large Language Models (LLMs) for root cause analysis (RCA) in cloud systems, focusing on evaluating their reasoning in simplified settings.


<details>
  <summary>Details</summary>
Motivation: To address challenges in diagnosing failures in complex cloud systems, especially in multi-hop fault propagation scenarios where issues manifest far from their root causes.

Method: The study evaluates six LLMs under agentic workflows (ReAct and Plan-and-Execute) and a non-agentic baseline using 48,000 simulated failure scenarios from two real-world case studies, assessing their RCA reasoning capabilities.

Result: Key findings include the identification of where open-source LLMs succeed or fail in RCA, the sensitivity to data input modalities, and a taxonomy of 16 common reasoning failures.

Conclusion: While LLMs show potential for RCA, their reasoning behavior needs improvement. The study provides detailed insights and a failure taxonomy to guide advancements in reasoning-driven diagnostics.

Abstract: Root cause analysis (RCA) is essential for diagnosing failures within complex software systems to ensure system reliability. The highly distributed and interdependent nature of modern cloud-based systems often complicates RCA efforts, particularly for multi-hop fault propagation, where symptoms appear far from their true causes. Recent advancements in Large Language Models (LLMs) present new opportunities to enhance automated RCA. However, their practical value for RCA depends on the fidelity of reasoning and decision-making. Existing work relies on historical incident corpora, operates directly on high-volume telemetry beyond current LLM capacity, or embeds reasoning inside complex multi-agent pipelines -- conditions that obscure whether failures arise from reasoning itself or from peripheral design choices.
  We present a focused empirical evaluation that isolates an LLM's reasoning behavior. We design a controlled experimental framework that foregrounds the LLM by using a simplified experimental setting. We evaluate six LLMs under two agentic workflows (ReAct and Plan-and-Execute) and a non-agentic baseline on two real-world case studies (GAIA and OpenRCA). In total, we executed 48,000 simulated failure scenarios, totaling 228 days of execution time. We measure both root-cause accuracy and the quality of intermediate reasoning traces. We produce a labeled taxonomy of 16 common RCA reasoning failures and use an LLM-as-a-Judge for annotation. Our results clarify where current open-source LLMs succeed and fail in multi-hop RCA, quantify sensitivity to input data modalities, and identify reasoning failures that predict final correctness. Together, these contributions provide transparent and reproducible empirical results and a failure taxonomy to guide future work on reasoning-driven system diagnosis.

</details>


### [73] [Predicting Intermittent Job Failure Categories for Diagnosis Using Few-Shot Fine-Tuned Language Models](https://arxiv.org/abs/2601.22264)
*Henri Aïdasso,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: The paper introduces 'FlaXifyer,' a few-shot learning approach that identifies categories of intermittent CI job failures and 'LogSift' for interpretability, reducing log-review effort in CI diagnostication.


<details>
  <summary>Details</summary>
Motivation: Intermittent CI pipeline failures caused by non-code-related issues, such as flaky tests and infrastructure problems, result in resource wastage and distract developers from core activities. Current methods focus on detection but lack in addressing efficient diagnosis.

Method: The proposed FlaXifyer uses pre-trained language models to predict job failure categories with few labeled examples. Additionally, LogSift highlights influential log statements, improving interpretability and reducing manual review efforts.

Result: FlaXifyer achieves 84.3% Macro F1 and 92.0% Top-2 accuracy with minimal labeled examples. LogSift helps reduce manual review effort by 74.4% and identifies relevant failure logs in 87% of cases, based on 2,458 job failures from a real-world dataset.

Conclusion: FlaXifyer and LogSift enhance automated triage, accelerate diagnosis, and are critical steps toward resolving intermittent CI pipeline failures efficiently.

Abstract: In principle, Continuous Integration (CI) pipeline failures provide valuable feedback to developers on code-related errors. In practice, however, pipeline jobs often fail intermittently due to non-deterministic tests, network outages, infrastructure failures, resource exhaustion, and other reliability issues. These intermittent (flaky) job failures lead to substantial inefficiencies: wasted computational resources from repeated reruns and significant diagnosis time that distracts developers from core activities and often requires intervention from specialized teams. Prior work has proposed machine learning techniques to detect intermittent failures, but does not address the subsequent diagnosis challenge. To fill this gap, we introduce FlaXifyer, a few-shot learning approach for predicting intermittent job failure categories using pre-trained language models. FlaXifyer requires only job execution logs and achieves 84.3% Macro F1 and 92.0% Top-2 accuracy with just 12 labeled examples per category. We also propose LogSift, an interpretability technique that identifies influential log statements in under one second, reducing review effort by 74.4% while surfacing relevant failure information in 87% of cases. Evaluation on 2,458 job failures from TELUS demonstrates that FlaXifyer and LogSift enable effective automated triage, accelerate failure diagnosis, and pave the way towards the automated resolution of intermittent job failures.

</details>


### [74] [PriviSense: A Frida-Based Framework for Multi-Sensor Spoofing on Android](https://arxiv.org/abs/2601.22414)
*Ibrahim Khalilov,Chaoran Chen,Ziang Xiao,Tianshi Li,Toby Jia-Jun Li,Yaxing Yao*

Main category: cs.SE

TL;DR: The paper presents PriviSense, a toolkit for manipulating sensor and system data on rooted Android devices to enable reproducible testing of context-sensitive apps.


<details>
  <summary>Details</summary>
Motivation: Current solutions like emulators and instrumented builds are not adequate for reproducibly testing behavior of context-sensitive apps on physical devices.

Method: PriviSense uses Frida-based, on-device tooling to inject scripted sensor and system signal streams into unmodified apps on rooted Android devices without requiring emulators or code rewriting.

Result: PriviSense successfully performed real-time data spoofing on a rooted Android device, validated through tests on five sensor-visualization apps.

Conclusion: PriviSense provides a powerful, ethical tool to test, analyze, and understand context-sensitive app behaviors while safeguarding its use by restricting access to verified researchers only.

Abstract: Mobile apps increasingly rely on real-time sensor and system data to adapt their behavior to user context. While emulators and instrumented builds offer partial solutions, they often fail to support reproducible testing of context-sensitive app behavior on physical devices. We present PriviSense, a Frida-based, on-device toolkit for runtime spoofing of sensor and system signals on rooted Android devices. PriviSense can script and inject time-varying sensor streams (accelerometer, gyroscope, step counter) and system values (battery level, system time, device metadata) into unmodified apps, enabling reproducible on-device experiments without emulators or app rewrites. Our demo validates real-time spoofing on a rooted Android device across five representative sensor-visualization apps. By supporting scriptable and reversible manipulation of these values, PriviSense facilitates testing of app logic, uncovering of context-based behaviors, and privacy-focused analysis. To ensure ethical use, the code is shared upon request with verified researchers.
  Tool Guide: How to Run PriviSense on Rooted Android https://bit.ly/privisense-guide Demonstration video: https://www.youtube.com/watch?v=4Qwnogcc3pw

</details>


### [75] [Small is Beautiful: A Practical and Efficient Log Parsing Framework](https://arxiv.org/abs/2601.22590)
*Minxing Wang,Yintong Huo*

Main category: cs.SE

TL;DR: EFParser is an unsupervised LLM-based log parser that enhances the performance of smaller LLMs, outperforming state-of-the-art methods while ensuring computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the major limitation of smaller LLMs in log parsing, posed by data privacy requirements and computational constraints in real-world applications.

Method: EFParser utilizes a dual-cache system with adaptive updates to keep templates consistent and introduces a correction module to validate and refine templates generated by LLMs.

Result: EFParser demonstrated superiority on large-scale datasets, outperforming baselines by 12.5% on average and even surpassing some models using large-scale LLMs.

Conclusion: EFParser offers an effective, efficient, and practical solution for log analysis, making smaller LLMs viable for real-world deployment.

Abstract: Log parsing is a fundamental step in log analysis, partitioning raw logs into constant templates and dynamic variables. While recent semantic-based parsers leveraging Large Language Models (LLMs) exhibit superior generalizability over traditional syntax-based methods, their effectiveness is heavily contingent on model scale. This dependency leads to significant performance collapse when employing smaller, more resource-efficient LLMs. Such degradation creates a major barrier to real-world adoption, where data privacy requirements and computational constraints necessitate the use of succinct models. To bridge this gap, we propose EFParser, an unsupervised LLM-based log parser designed to enhance the capabilities of smaller models through systematic architectural innovation. EFParser introduces a dual-cache system with an adaptive updating mechanism that distinguishes between novel patterns and variations of existing templates. This allows the parser to merge redundant templates and rectify prior errors, maintaining cache consistency. Furthermore, a dedicated correction module acts as a gatekeeper, validating and refining every LLM-generated template before caching to prevent error injection. Empirical evaluations on public large-scale datasets demonstrate that EFParser outperforms state-of-the-art baselines by an average of 12.5% across all metrics when running on smaller LLMs, even surpassing some baselines utilizing large-scale models. Despite its additional validation steps, EFParser maintains high computational efficiency, offering a robust and practical solution for real-world log analysis deployment.

</details>


### [76] [TimeMachine-bench: A Benchmark for Evaluating Model Capabilities in Repository-Level Migration Tasks](https://arxiv.org/abs/2601.22597)
*Ryo Fujii,Makoto Morishita,Kazuki Yano,Jun Suzuki*

Main category: cs.SE

TL;DR: The paper introduces TimeMachine-bench, a benchmark for software migration evaluation on Python projects experiencing test failures due to dependency updates.


<details>
  <summary>Details</summary>
Motivation: There is a lack of focus on software migration tasks, which are critical but overlooked in automated software engineering.

Method: The benchmark includes repositories with dependency-driven test failures, automated construction processes, and a curated human-verified subset for solvability analysis. Baselines were evaluated using agent-based models.

Result: Experimentation with 11 baseline models showed LLMs promise in software migration but revealed challenges such as suboptimal tool strategies and unreliable fixes.

Conclusion: While the benchmark addresses a critical gap in software migration evaluation, LLM models still face significant reliability challenges for practical deployment.

Abstract: With the advancement of automated software engineering, research focus is increasingly shifting toward practical tasks reflecting the day-to-day work of software engineers. Among these tasks, software migration, a critical process of adapting code to evolving environments, has been largely overlooked. In this study, we introduce TimeMachine-bench, a benchmark designed to evaluate software migration in real-world Python projects. Our benchmark consists of GitHub repositories whose tests begin to fail in response to dependency updates. The construction process is fully automated, enabling live updates of the benchmark. Furthermore, we curated a human-verified subset to ensure problem solvability. We evaluated agent-based baselines built on top of 11 models, including both strong open-weight and state-of-the-art LLMs on this verified subset. Our results indicated that, while LLMs show some promise for migration tasks, they continue to face substantial reliability challenges, including spurious solutions that exploit low test coverage and unnecessary edits stemming from suboptimal tool-use strategies. Our dataset and implementation are available at https://github.com/tohoku-nlp/timemachine-bench.

</details>


### [77] [Elderly HealthMag: Systematic Building and Calibrating a Tool for Identifying and Evaluating Senior User Digital Health Software](https://arxiv.org/abs/2601.22627)
*Yuqing Xiao,John Grundy,Anuradha Madugalla,Elizabeth Manias*

Main category: cs.SE

TL;DR: The paper introduces HealthMag to design inclusive digital health (DH) software, particularly for elderly users, by addressing biases related to age and health conditions.


<details>
  <summary>Details</summary>
Motivation: Digital health software development often fails to account for diverse user needs, especially considering health and age-related challenges, leading to products that underperform in inclusivity.

Method: The authors developed HealthMag by systematic mapping and calibration of the InclusiveMag framework, combining this with a calibrated AgeMag method to create Elderly HealthMag for evaluating mHealth tools.

Result: The tool was applied in cognitive walkthroughs, successfully identifying inclusivity biases in existing senior-centric digital health applications.

Conclusion: HealthMag and Elderly HealthMag enhance the design and evaluation of DH software, ensuring inclusivity for diverse health and age-related needs.

Abstract: Digital health (DH) software is increasingly deployed to populations where many end users live with one or more health conditions. Yet, DH software development teams frequently operate using implicit, incorrect assumptions about these users, resulting in products that under-serve the specific requirements imposed by their age and health conditions. Consequently, while software may meet clinical objectives on paper, it often fails to be inclusive during actual user interaction. To address this, we propose \textbf{\textit{HealthMag}}, a tool inspired by GenderMag designed to help better elicit, model and evaluate requirements for digital health software. We developed HealthMag through systematic mapping and calibration following the InclusiveMag framework. Furthermore, we integrated this with a calibrated version of an existing AgeMag method to create a dual-lens approach: \textbf{\textit{Elderly HealthMag}}, designed to aid requirements, design and evaluation of mHealth software for senior end users. We demonstrate application and utility of Age HealthMag via cognitive walkthroughs in identifying inclusivity biases in current senior user-oriented digital health applications.

</details>


### [78] [From Horizontal Layering to Vertical Integration: A Comparative Study of the AI-Driven Software Development Paradigm](https://arxiv.org/abs/2601.22667)
*Chi Zhang,Zehan Li,Ziqian Zhong,Haibing Ma,Dan Xiao,Chen Lin,Ming Dong*

Main category: cs.SE

TL;DR: The paper studies Generative AI's impact on organizational changes in software engineering and highlights efficiency improvement through a new operational structure centered around AI-augmented engineers.


<details>
  <summary>Details</summary>
Motivation: To understand how Generative AI adoption transforms organizational structures and resource usage in software engineering environments.

Method: A multiple-case comparative study contrasting a traditional enterprise (brownfield) with an AI-native startup (greenfield) and analyzing productivity through key metrics.

Result: The study finds a reduction in resource consumption by 8x to 33x through transitioning operational paradigms. This is driven by AI-augmented "Super Employees" and reduced inter-functional coordination.

Conclusion: Human-AI Collaboration Efficacy is central to organizational optimization. The study concludes with strategies for redesigning organizations to enhance AI integration, focusing on cognitive reutilization and avoiding over-expansion.

Abstract: This paper examines the organizational implications of Generative AI adoption in software engineering through a multiple-case comparative study. We contrast two development environments: a traditional enterprise (brownfield) and an AI-native startup (greenfield). Our analysis reveals that transitioning from Horizontal Layering (functional specialization) to Vertical Integration (end-to-end ownership) yields 8-fold to 33-fold reductions in resource consumption. We attribute these gains to the emergence of Super Employees, AI-augmented engineers who span traditional role boundaries, and the elimination of inter-functional coordination overhead. Theoretically, we propose Human-AI Collaboration Efficacy as the primary optimization target for engineering organizations, supplanting individual productivity metrics. Our Total Factor Productivity analysis identifies an AI Distortion Effect that diminishes returns to labor scale while amplifying technological leverage. We conclude with managerial strategies for organizational redesign, including the reactivation of idle cognitive bandwidth in senior engineers and the suppression of blind scale expansion.

</details>


### [79] [VarParser: Unleashing the Neglected Power of Variables for LLM-based Log Parsing](https://arxiv.org/abs/2601.22676)
*Jinrui Sun,Tong Jia,Minghua He,Ying Li*

Main category: cs.SE

TL;DR: VarParser is a novel log parsing strategy that enhances the use of variable parts in logs for more efficient and accurate parsing with lower costs in large-scale datasets.


<details>
  <summary>Details</summary>
Motivation: Existing log parsers focusing only on the constant part of logs lead to inefficiencies and high costs. Variable information in logs is overlooked, which compromises system visibility and parsing accuracy.

Method: VarParser employs a variable-centric strategy involving variable contribution sampling, parsing cache, and adaptive variable-aware in-context learning, capturing variable information efficiently.

Result: VarParser achieves higher accuracy and efficiency in log parsing, reduces LLM invocation costs, and preserves variable information, demonstrating superior performance on large-scale datasets.

Conclusion: A variable-centric parsing strategy like VarParser can significantly enhance the accuracy, efficiency, and cost-effectiveness of log parsing by leveraging both constant and variable information in logs.

Abstract: Logs serve as a primary source of information for engineers to diagnose failures in large-scale online service systems. Log parsing, which extracts structured events from massive unstructured log data, is a critical first step for downstream tasks like anomaly detection and failure diagnosis. With advances in large language models (LLMs), leveraging their strong text understanding capabilities has proven effective for accurate log parsing. However, existing LLM-based log parsers all focus on the constant part of logs, ignoring the potential contribution of the variable part to log parsing. This constant-centric strategy brings four key problems. First, inefficient log grouping and sampling with only constant information. Second, a relatively large number of LLM invocations due to constant-based cache, leading to low log parsing accuracy and efficiency. Third, a relatively large number of consumed constant tokens in prompts leads to high LLM invocation costs. At last, these methods only retain placeholders in the results, losing the system visibility brought by variable information in logs.
  Facing these problems, we propose a variable-centric log parsing strategy named VarParser. Through variable contribution sampling, variable-centric parsing cache, and adaptive variable-aware in-context learning, our approach can efficiently capture the variable parts of logs and leverage their contributions to parsing. By introducing variable units, we preserve rich variable information, enhancing the integrity of log parsing results. Extensive evaluations on large-scale datasets demonstrate that VarParser achieves higher accuracy compared to existing methods, significantly improving parsing efficiency while reducing the LLM invocation costs.

</details>


### [80] [AutoMerge: Search-Based Model Merging Framework for Effective Model Reuse](https://arxiv.org/abs/2601.22748)
*You Lu,Jiyang Zhang,Bihuan Chen,Chaofeng Sha,Dingji Wang,Xin Peng*

Main category: cs.SE

TL;DR: The paper investigates the feasibility of applying model merging techniques from LLMs to other deep learning models across domains, highlighting inconsistent results and proposing a novel framework called AutoMerge.


<details>
  <summary>Details</summary>
Motivation: To explore whether training-free model merging methods can be systematically and effectively extended from large language models (LLMs) to other deep learning domains, such as image classification and autonomous driving.

Method: The authors conducted a systematic study by evaluating five model merging techniques on three distinct model architectures across three domains. They also developed AutoMerge, a framework that segments models and explores optimal merging techniques and hyperparameter configurations.

Result: The study found that existing model merging techniques yield inconsistent results outside of LLMs. A single method is inadequate to address structural heterogeneity across models, and the effectiveness is highly sensitive to hyperparameters.

Conclusion: Model merging techniques require adaptation and refinement to be applied effectively across different architectures and domains. AutoMerge was proposed as an approach to overcome these limitations by optimizing merging strategies at a modular level.

Abstract: Software reuse has long been recognized as a critical and widely studied topic in software engineering, offering substantial benefits in reducing development costs, improving software quality, and enhancing operational efficiency. This paradigm extends into deep learning through model reuse. Recently, model merging has emerged in the domain of large language models (LLMs) as a training-free approach that takes multiple task-specific models with the same architecture as source models and merges them without retraining, enhancing model reuse within LLMs. However, no prior work has systematically investigated whether such an approach can be effectively applied to other deep learning models with different architectures across domains. To bridge this gap, we present the first systematic study that evaluates five model merging techniques on three distinct model architectures across three domains: LLMs, image classification, and autonomous driving. Our findings reveal that directly applying existing model merging techniques leads to highly inconsistent results and falls notably short of their success within LLMs. Moreover, a single model merging technique often fails to handle the heterogeneous structural properties within a model, limiting its applicability to different model architectures across domains. Furthermore, the effectiveness of model merging techniques is highly sensitive to hyperparameter configurations, thereby constraining their potential for broader adoption. Inspired by these insights, we propose AutoMerge, a novel search-based model merging framework that first segments complex models into multiple heterogeneous blocks and then systematically explores the merging space to identify the merging technique and its hyperparameter configuration.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [81] [The Where and How of Touch: A Review of Tactile Localization Research](https://arxiv.org/abs/2601.23023)
*Xaver Fuchs,Jason A. M. Khoury,Sergiu Tcaci Popescu,Tobias Heed,Matej Hoffmann*

Main category: q-bio.NC

TL;DR: This paper reviews theories and methodologies in tactile localization, showing how experimental methods impact results and highlighting inconsistencies in the field.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the fragmented theoretical frameworks and methodological inconsistencies in tactile localization research.

Method: The authors conducted a systematic literature review to examine theories, experimental tasks, biases, and methodological assumptions in tactile localization studies.

Result: The review reveals at least 8 distinct experimental task types, highlights biases in certain methods, and the lack of unified theoretical underpinnings.

Conclusion: The study emphasizes the need for clear concepts and unified frameworks for tactile spatial processing accompanied by improved data sharing for cross-study comparisons.

Abstract: Tactile localization is the seemingly simple ability to 'tell' where a touch has occurred. However, how this ability is assessed, and what conclusions are drawn from experiments, depends on the theoretical ideas that inspire the research. Here, we review both theoretical frameworks and methodological approaches based on a systematic web-based literature search on tactile localization. After presenting current theories of tactile localization, we discuss task characteristics that differentiate current methodology for tactile localization into at least 8 distinct types of experimental tasks. We describe these tasks, discuss their, often implicit, underlying assumptions and cognitive requirements, and relate them to the theoretical approaches. We then compare, in an exemplary manner, the tactile localization results reported by a subset of studies and demonstrate how some methods are associated with specific biases, illustrating that the choice of experimental method significantly affects the conclusions drawn from the results. Our review suggests that the field currently lacks a clear concept of the specific processes induced by the various experimental tasks and, thus, calls for concerted efforts to clarify and unify currently diverse, fragmented, and partly inconsistent theoretical underpinnings of tactile spatial processing, flanked by dedicated data sharing to allow across-study analysis.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [82] [Dependence-Aware Label Aggregation for LLM-as-a-Judge via Ising Models](https://arxiv.org/abs/2601.22336)
*Krishnakumar Balasubramanian,Aleksandr Podkopaev,Shiva Prasad Kasiviswanathan*

Main category: stat.ML

TL;DR: The paper explores label aggregation for AI evaluation using novel methods accounting for dependencies between annotators, especially focusing on LLM judges.


<details>
  <summary>Details</summary>
Motivation: To address the issue of dependency among annotators, particularly LLM judges, that leads to miscalibrated outputs when classical methods relying on independence assumptions are applied.

Method: The authors develop models based on Ising graphical models and latent factors, including class-dependent and class-independent analysis, to account for annotator dependencies.

Result: The proposed method outperforms classical baselines in terms of accuracy and mitigates systematic errors in real-world datasets.

Conclusion: Dependency-aware label aggregation methods offer significant improvements over traditional approaches, resolving issues related to annotator dependencies and providing more reliable AI evaluation.

Abstract: Large-scale AI evaluation increasingly relies on aggregating binary judgments from $K$ annotators, including LLMs used as judges. Most classical methods, e.g., Dawid-Skene or (weighted) majority voting, assume annotators are conditionally independent given the true label $Y\in\{0,1\}$, an assumption often violated by LLM judges due to shared data, architectures, prompts, and failure modes. Ignoring such dependencies can yield miscalibrated posteriors and even confidently incorrect predictions. We study label aggregation through a hierarchy of dependence-aware models based on Ising graphical models and latent factors. For class-dependent Ising models, the Bayes log-odds is generally quadratic in votes; for class-independent couplings, it reduces to a linear weighted vote with correlation-adjusted parameters. We present finite-$K$ examples showing that methods based on conditional independence can flip the Bayes label despite matching per-annotator marginals. We prove separation results demonstrating that these methods remain strictly suboptimal as the number of judges grows, incurring nonvanishing excess risk under latent factors. Finally, we evaluate the proposed method on three real-world datasets, demonstrating improved performance over the classical baselines.

</details>


### [83] [Amortized Simulation-Based Inference in Generalized Bayes via Neural Posterior Estimation](https://arxiv.org/abs/2601.22367)
*Shiyi Sun,Geoff K. Nicholls,Jeong Eun Lee*

Main category: stat.ML

TL;DR: This paper proposes a method for efficient and fully amortized variational approximation for generalized Bayesian inference (GBI), enabling temperature-conditioned posterior estimation without costly MCMC or repeated sampling.


<details>
  <summary>Details</summary>
Motivation: Current GBI techniques rely on computationally expensive MCMC or SDE-based samplers to mitigate overconfidence and address model misspecification, but these methods need to be rerun for every new dataset and temperature value, necessitating a more efficient approach.

Method: The paper proposes a neural posterior estimator, $q_φ(θ\mid x,β)$, trained on a single dataset for $(x,β)$-conditioned posterior sampling. It employs two training strategies: synthesizing off-manifold samples and using self-normalized importance sampling (SNIS) to reweight fixed base datasets consistently.

Result: The method achieves competitive posterior approximations across four simulation-based inference (SBI) benchmarks, demonstrating consistency with power-posterior samplers over various temperature values.

Conclusion: The proposed approach provides an efficient and accurate GBI solution, overcoming the limitations of standard MCMC approaches by achieving competitive performance in a single forward pass without inference-time computational burdens.

Abstract: Generalized Bayesian Inference (GBI) tempers a loss with a temperature $β>0$ to mitigate overconfidence and improve robustness under model misspecification, but existing GBI methods typically rely on costly MCMC or SDE-based samplers and must be re-run for each new dataset and each $β$ value. We give the first fully amortized variational approximation to the tempered posterior family $p_β(θ\mid x) \propto π(θ)\,p(x \mid θ)^β$ by training a single $(x,β)$-conditioned neural posterior estimator $q_φ(θ\mid x,β)$ that enables sampling in a single forward pass, without simulator calls or inference-time MCMC. We introduce two complementary training routes: (i) synthesize off-manifold samples $(θ,x) \sim π(θ)\,p(x \mid θ)^β$ and (ii) reweight a fixed base dataset $π(θ)\,p(x \mid θ)$ using self-normalized importance sampling (SNIS). We show that the SNIS-weighted objective provides a consistent forward-KL fit to the tempered posterior with finite weight variance. Across four standard simulation-based inference (SBI) benchmarks, including the chaotic Lorenz-96 system, our $β$-amortized estimator achieves competitive posterior approximations in standard two-sample metrics, matching non-amortized MCMC-based power-posterior samplers over a wide range of temperatures.

</details>


### [84] [It's all the (Exponential) Family: An Equivalence between Maximum Likelihood Estimation and Control Variates for Sketching Algorithms](https://arxiv.org/abs/2601.22378)
*Keegan Kang,Kerong Wang,Ding Zhang,Rameshwar Pratap,Bhisham Dev Verma,Benedict H. W. Wong*

Main category: stat.ML

TL;DR: This paper investigates the relationship between Maximum Likelihood Estimators and Control Variate Estimators under exponential family conditions, proposing that optimal CVE matches MLE asymptotic variance. It introduces an efficient and stable Expectation-Maximization algorithm for MLE.


<details>
  <summary>Details</summary>
Motivation: Improve computational efficiency and stability in finding maximum likelihood estimators for machine learning applications using control variate estimators.

Method: Developed theoretical proof linking CVE's optimal properties to MLE's asymptotic variance under exponential family conditions. Introduced an Expectation-Maximization algorithm for the MLE and validated through experiments.

Result: The experiments demonstrated that the EM algorithm is faster and more numerically stable than existing root finding algorithms for MLE in specific cases like the bivariate Normal distribution.

Conclusion: The EM algorithm enhances reproducibility and efficiency in MLE computations, particularly in cases with known control variate weights, and is expected to generalize to other distributions under similar conditions.

Abstract: Maximum likelihood estimators (MLE) and control variate estimators (CVE) have been used in conjunction with known information across sketching algorithms and applications in machine learning. We prove that under certain conditions in an exponential family, an optimal CVE will achieve the same asymptotic variance as the MLE, giving an Expectation-Maximization (EM) algorithm for the MLE. Experiments show the EM algorithm is faster and numerically stable compared to other root finding algorithms for the MLE for the bivariate Normal distribution, and we expect this to hold across distributions satisfying these conditions. We show how the EM algorithm leads to reproducibility for algorithms using MLE / CVE, and demonstrate how the EM algorithm leads to finding the MLE when the CV weights are known.

</details>


### [85] [Simulation-based Bayesian inference with ameliorative learned summary statistics -- Part I](https://arxiv.org/abs/2601.22441)
*Getachew K. Befekadu*

Main category: stat.ML

TL;DR: This paper discusses a simulation-based inference method using learned summary statistics to address challenges in Bayesian inference when the exact likelihood function is computationally intractable or unavailable.


<details>
  <summary>Details</summary>
Motivation: To overcome difficulties in Bayesian inference caused by intractable or unknown likelihood functions associated with observation data and simulation models.

Method: The paper introduces a transformation technique using the Cressie-Read discrepancy criterion and moment restrictions to leverage learned summary statistics for inference. It focuses on conditioning simulation outputs on observation data and supports distributed computing techniques.

Result: The framework allows inference over specific sample sets of observation data and can extend to weakly dependent data. Additionally, it facilitates distributed optimization and MCMC algorithms for large datasets.

Conclusion: The proposed simulation-based Bayesian inference approach improves statistical power, computational tractability, and scalability for complex simulation models.

Abstract: This paper, which is Part 1 of a two-part paper series, considers a simulation-based inference with learned summary statistics, in which such a learned summary statistic serves as an empirical-likelihood with ameliorative effects in the Bayesian setting, when the exact likelihood function associated with the observation data and the simulation model is difficult to obtain in a closed form or computationally intractable. In particular, a transformation technique which leverages the Cressie-Read discrepancy criterion under moment restrictions is used for summarizing the learned statistics between the observation data and the simulation outputs, while preserving the statistical power of the inference. Here, such a transformation of data-to-learned summary statistics also allows the simulation outputs to be conditioned on the observation data, so that the inference task can be performed over certain sample sets of the observation data that are considered as an empirical relevance or believed to be particular importance. Moreover, the simulation-based inference framework discussed in this paper can be extended further, and thus handling weakly dependent observation data. Finally, we remark that such an inference framework is suitable for implementation in distributed computing, i.e., computational tasks involving both the data-to-learned summary statistics and the Bayesian inferencing problem can be posed as a unified distributed inference problem that will exploit distributed optimization and MCMC algorithms for supporting large datasets associated with complex simulation models.

</details>


### [86] [Corrected Samplers for Discrete Flow Models](https://arxiv.org/abs/2601.22519)
*Zhengyan Wan,Yidong Ouyang,Liyan Xie,Fang Fang,Hongyuan Zha,Guang Cheng*

Main category: stat.ML

TL;DR: This paper introduces corrected sampling methods for discrete flow models to minimize discretization error and improve generation quality, along with non-asymptotic error bounds.


<details>
  <summary>Details</summary>
Motivation: Recent samplers for discrete diffusion models face limitations due to high iteration requirements, frozen transition rates, and theoretical restrictions on transition rates or source distributions.

Method: The authors propose two corrected sampling methods – time-corrected sampler and location-corrected sampler – along with theoretical error bounds under the discrete flow model framework.

Result: The location-corrected sampler reduces iteration complexity and enhances generation quality with almost no extra computational cost, validated through simulations and text-to-image generation tasks.

Conclusion: Corrected samplers improve inference efficiency and quality for discrete flow models, offering a practical alternative to existing samplers.

Abstract: Discrete flow models (DFMs) have been proposed to learn the data distribution on a finite state space, offering a flexible framework as an alternative to discrete diffusion models. A line of recent work has studied samplers for discrete diffusion models, such as tau-leaping and Euler solver. However, these samplers require a large number of iterations to control discretization error, since the transition rates are frozen in time and evaluated at the initial state within each time interval. Moreover, theoretical results for these samplers often require boundedness conditions of the transition rate or they focus on a specific type of source distributions. To address those limitations, we establish non-asymptotic discretization error bounds for those samplers without any restriction on transition rates and source distributions, under the framework of discrete flow models. Furthermore, by analyzing a one-step lower bound of the Euler sampler, we propose two corrected samplers: \textit{time-corrected sampler} and \textit{location-corrected sampler}, which can reduce the discretization error of tau-leaping and Euler solver with almost no additional computational cost. We rigorously show that the location-corrected sampler has a lower iteration complexity than existing parallel samplers. We validate the effectiveness of the proposed method by demonstrating improved generation quality and reduced inference time on both simulation and text-to-image generation tasks. Code can be found in https://github.com/WanZhengyan/Corrected-Samplers-for-Discrete-Flow-Models.

</details>


### [87] [An Efficient Algorithm for Thresholding Monte Carlo Tree Search](https://arxiv.org/abs/2601.22600)
*Shoma Nameki,Atsuyoshi Nakamura,Junpei Komiyama,Koji Tabata*

Main category: stat.ML

TL;DR: The paper introduces a problem called Thresholding Monte Carlo Tree Search and proposes a $δ$-correct algorithm with improved sample and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the problem of determining whether the root node value of a tree is at least a given threshold, while optimizing sampling efficiency and computational cost.

Method: The authors propose a $δ$-correct sequential sampling algorithm utilizing a Track-and-Stop strategy and a ratio-based modification of the D-Tracking arm-pulling method.

Result: The algorithm achieves asymptotically optimal sample complexity and demonstrates significant empirical improvements in both sample and computational efficiency.

Conclusion: This work advances the field of decision-making in tree-based structures by improving the efficiency of threshold evaluation and sampling strategies.

Abstract: We introduce the Thresholding Monte Carlo Tree Search problem, in which, given a tree $\mathcal{T}$ and a threshold $θ$, a player must answer whether the root node value of $\mathcal{T}$ is at least $θ$ or not. In the given tree, `MAX' or `MIN' is labeled on each internal node, and the value of a `MAX'-labeled (`MIN'-labeled) internal node is the maximum (minimum) of its child values. The value of a leaf node is the mean reward of an unknown distribution, from which the player can sample rewards. For this problem, we develop a $δ$-correct sequential sampling algorithm based on the Track-and-Stop strategy that has asymptotically optimal sample complexity. We show that a ratio-based modification of the D-Tracking arm-pulling strategy leads to a substantial improvement in empirical sample complexity, as well as reducing the per-round computational cost from linear to logarithmic in the number of arms.

</details>


### [88] [RPWithPrior: Label Differential Privacy in Regression](https://arxiv.org/abs/2601.22625)
*Haixia Liu,Ruifan Huang*

Main category: stat.ML

TL;DR: The paper proposes a novel approach for regression tasks under ε-label differential privacy by avoiding output space discretization and modeling responses as continuous random variables, achieving improved performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing differential privacy methods for regression tasks, especially avoiding inaccuracies from discretizing output spaces.

Method: The paper introduces an approach that models both original and randomized responses as continuous random variables. It estimates optimal intervals and proposes algorithms for cases with or without known priors, specifically ensuring ε-label differential privacy.

Result: The approach outperformed Gaussian, Laplace, Staircase, RRonBins, and Unbiased mechanisms across three datasets: Communities and Crime, Criteo Sponsored Search Conversion Log, and California Housing.

Conclusion: The proposed method provides a better accuracy-privacy balance compared to traditional mechanisms, proving effective in preserving ε-label differential privacy in regression tasks.

Abstract: With the wide application of machine learning techniques in practice, privacy preservation has gained increasing attention. Protecting user privacy with minimal accuracy loss is a fundamental task in the data analysis and mining community. In this paper, we focus on regression tasks under $ε$-label differential privacy guarantees. Some existing methods for regression with $ε$-label differential privacy, such as the RR-On-Bins mechanism, discretized the output space into finite bins and then applied RR algorithm. To efficiently determine these finite bins, the authors rounded the original responses down to integer values. However, such operations does not align well with real-world scenarios. To overcome these limitations, we model both original and randomized responses as continuous random variables, avoiding discretization entirely. Our novel approach estimates an optimal interval for randomized responses and introduces new algorithms designed for scenarios where a prior is either known or unknown. Additionally, we prove that our algorithm, RPWithPrior, guarantees $ε$-label differential privacy. Numerical results demonstrate that our approach gets better performance compared with the Gaussian, Laplace, Staircase, and RRonBins, Unbiased mechanisms on the Communities and Crime, Criteo Sponsored Search Conversion Log, California Housing datasets.

</details>


### [89] [Generative and Nonparametric Approaches for Conditional Distribution Estimation: Methods, Perspectives, and Comparative Evaluations](https://arxiv.org/abs/2601.22650)
*Yen-Shiu Chin,Zhi-Yu Jou,Toshinari Morimoto,Chia-Tse Wang,Ming-Chung Chang,Tso-Jung Yen,Su-Yun Huang,Tailen Hsing*

Main category: stat.ML

TL;DR: The paper reviews and compares methods for inferring conditional distributions, including nonparametric methods and modern generative models, through systematic comparisons.


<details>
  <summary>Details</summary>
Motivation: To review and compare methodologies for inferring conditional distributions, critical for prediction, uncertainty quantification, and probabilistic modeling.

Method: The paper analyzes and compares single-index methods, basis-expansion approaches, and modern generative simulation-based methods under a unified evaluation framework, considering metrics like mean-squared errors and Wasserstein distance.

Result: A systematic numerical comparison was performed, revealing differences in accuracy, flexibility, and computational cost among the reviewed methods.

Conclusion: The study provides insights into the strengths and weaknesses of various approaches for conditional distribution inference, aiding researchers in method selection based on specific needs.

Abstract: The inference of conditional distributions is a fundamental problem in statistics, essential for prediction, uncertainty quantification, and probabilistic modeling. A wide range of methodologies have been developed for this task. This article reviews and compares several representative approaches spanning classical nonparametric methods and modern generative models. We begin with the single-index method of Hall and Yao (2005), which estimates the conditional distribution through a dimension-reducing index and nonparametric smoothing of the resulting one-dimensional cumulative conditional distribution function. We then examine the basis-expansion approaches, including FlexCode (Izbicki and Lee, 2017) and DeepCDE (Dalmasso et al., 2020), which convert conditional density estimation into a set of nonparametric regression problems. In addition, we discuss two recent generative simulation-based methods that leverage modern deep generative architectures: the generative conditional distribution sampler (Zhou et al., 2023) and the conditional denoising diffusion probabilistic model (Fu et al., 2024; Yang et al., 2025). A systematic numerical comparison of these approaches is provided using a unified evaluation framework that ensures fairness and reproducibility. The performance metrics used for the estimated conditional distribution include the mean-squared errors of conditional mean and standard deviation, as well as the Wasserstein distance. We also discuss their flexibility and computational costs, highlighting the distinct advantages and limitations of each approach.

</details>


### [90] [Spectral Gradient Descent Mitigates Anisotropy-Driven Misalignment: A Case Study in Phase Retrieval](https://arxiv.org/abs/2601.22652)
*Guillaume Braun,Han Bao,Wei Huang,Masaaki Imaizumi*

Main category: stat.ML

TL;DR: This paper analyzes why spectral gradient methods like the Muon optimizer perform effectively in deep learning, focusing on how they stabilize alignment in training through theoretical and numerical insights.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand why spectral gradient methods, particularly the Muon optimizer, demonstrate strong empirical performance in deep learning by investigating their underlying mechanisms.

Method: The study uses a dynamical analysis on a nonlinear phase retrieval model with anisotropic Gaussian inputs, equivalent to a simplified two-layer neural network. It focuses on analyzing the impact of high variance directions in training.

Result: The findings reveal that gradient descent (GD) suffers from a variance-induced misalignment in highly anisotropic settings, while spectral gradient descent (SpecGD) removes this issue, stabilizing training alignment and accelerating noise contraction.

Conclusion: Spectral gradient methods effectively address variance-induced issues in GD, leading to stable training dynamics and enhanced performance in anisotropic setups. This insight is supported by both theoretical analysis and numerical experiments.

Abstract: Spectral gradient methods, such as the Muon optimizer, modify gradient updates by preserving directional information while discarding scale, and have shown strong empirical performance in deep learning. We investigate the mechanisms underlying these gains through a dynamical analysis of a nonlinear phase retrieval model with anisotropic Gaussian inputs, equivalent to training a two-layer neural network with the quadratic activation and fixed second-layer weights. Focusing on a spiked covariance setting where the dominant variance direction is orthogonal to the signal, we show that gradient descent (GD) suffers from a variance-induced misalignment: during the early escaping stage, the high-variance but uninformative spike direction is multiplicatively amplified, degrading alignment with the true signal under strong anisotropy. In contrast, spectral gradient descent (SpecGD) removes this spike amplification effect, leading to stable alignment and accelerated noise contraction. Numerical experiments confirm the theory and show that these phenomena persist under broader anisotropic covariances.

</details>


### [91] [GRANITE: A Generalized Regional Framework for Identifying Agreement in Feature-Based Explanations](https://arxiv.org/abs/2601.22771)
*Julia Herbinger,Gabriel Laberge,Maximilian Muschalik,Yann Pequignot,Marvin N. Wright,Fabian Fumagalli*

Main category: stat.ML

TL;DR: Current feature-based explanation methods often disagree due to feature interaction and dependency issues. GRANITE proposes a framework to resolve this by partitioning the feature space for more consistent explanations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inconsistency in feature-based explanation methods caused by different treatments of feature interactions and dependencies.

Method: GRANITE partitions the feature space using a generalized regional explanation framework with minimized interaction and distribution influences. It introduces a recursive partitioning algorithm and extends existing regional approaches to feature groups.

Result: GRANITE reduced explanation discrepancies across methods and proved effective in real-world datasets by producing consistent, interpretable feature explanations.

Conclusion: GRANITE provides a structured tool to align various explanation methods, improving consistency and interpretability in feature-based explanations.

Abstract: Feature-based explanation methods aim to quantify how features influence the model's behavior, either locally or globally, but different methods often disagree, producing conflicting explanations. This disagreement arises primarily from two sources: how feature interactions are handled and how feature dependencies are incorporated. We propose GRANITE, a generalized regional explanation framework that partitions the feature space into regions where interaction and distribution influences are minimized. This approach aligns different explanation methods, yielding more consistent and interpretable explanations. GRANITE unifies existing regional approaches, extends them to feature groups, and introduces a recursive partitioning algorithm to estimate such regions. We demonstrate its effectiveness on real-world datasets, providing a practical tool for consistent and interpretable feature explanations.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [92] [Assessing the Real-World Impact of Post-Quantum Cryptography on WPA-Enterprise Networks](https://arxiv.org/abs/2601.22892)
*Lukas Köder,Nils Lohmiller,Phil Schmieder,Bastian Buck,Michael Menth,Tobias Heer*

Main category: cs.CR

TL;DR: This paper evaluates the impact of Post-Quantum Cryptography (PQC) on WPA-Enterprise authentication in Wi-Fi networks, demonstrating its practicality while addressing performance and security trade-offs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the threat posed by large-scale quantum computers to WPA-Enterprise authentication by investigating the feasibility and performance impacts of adopting PQC.

Method: The paper conducts an experimental performance evaluation of PQC algorithms by measuring authentication latency at different stages (client, access point, RADIUS server). It analyzes multiple PQC algorithm combinations and assesses their security implications using a testbed built with FreeRADIUS and hostapd.

Result: The study finds that PQC, despite introducing additional authentication latency, can provide a favorable trade-off between security and performance, especially with algorithm combinations like ML-DSA-65, Falcon-1024, and ML-KEM. The overhead can also be mitigated using session resumption techniques.

Conclusion: PQC-enabled WPA-Enterprise authentication is practically feasible for real-world enterprise Wi-Fi, offering enhanced security against quantum threats while managing performance overhead effectively.

Abstract: The advent of large-scale quantum computers poses a significant threat to contemporary network security protocols, including Wi-Fi Protected Access (WPA)-Enterprise authentication. To mitigate this threat, the adoption of Post-Quantum Cryptography (PQC) is critical. In this work, we investigate the performance impact of PQC algorithms on WPA-Enterprise-based authentication. To this end, we conduct an experimental evaluation of authentication latency using a testbed built with the open-source tools FreeRADIUS and hostapd, measuring the time spent at the client, access point, and RADIUS server. We evaluate multiple combinations of PQC algorithms and analyze their performance overhead in comparison to currently deployed cryptographic schemes. Beyond performance, we assess the security implications of these algorithm choices by relating authentication mechanisms to the quantum effort required for their exploitation. This perspective enables a systematic categorization of PQ-relevant weaknesses in WPA-Enterprise according to their practical urgency. The evaluation results show that, although PQC introduces additional authentication latency, combinations such as ML-DSA-65 and Falcon-1024 used in conjunction with ML-KEM provide a favorable trade-off between security and performance. Furthermore, we demonstrate that the resulting overhead can be effectively mitigated through session resumption. Overall, this work presents a first real-world performance evaluation of PQC-enabled WPA-Enterprise authentication and demonstrates its practical feasibility for enterprise Wi-Fi deployments.

</details>


### [93] [SpecIBT: Formally Verified Protection Against Speculative Control-Flow Hijacking](https://arxiv.org/abs/2601.22978)
*Jonathan Baumann,Yonghyun Kim,Yan Farba,Catalin Hritcu,Julay Leatherman-Brooks*

Main category: cs.CR

TL;DR: This paper introduces SpecIBT, a defense system combining CET-style control-flow integrity and speculative load hardening (SLH), proven secure against Spectre attacks.


<details>
  <summary>Details</summary>
Motivation: To address Spectre attacks by providing a verified defense mechanism that limits data leaks during speculative execution.

Method: Combining CET-style hardware-assisted control-flow integrity with compiler-inserted SLH, verified through formal proofs and machine-checked transformation.

Result: Introduced SpecIBT, capable of detecting BTB misspeculation and ensuring programs remain relatively secure under speculative execution, with formal guarantees.

Conclusion: SpecIBT offers a verified and secure approach to mitigating Spectre vulnerabilities in various programs, even outside the cryptographic constant-time discipline.

Abstract: This paper introduces SpecIBT, a formally verified defense against Spectre BTB, RSB, and PHT that combines CET-style hardware-assisted control-flow integrity with compiler-inserted speculative load hardening (SLH). SpecIBT is based on the novel observation that in the presence of CET-style protection, we can precisely detect BTB misspeculation for indirect calls and set the SLH misspeculation flag. We formalize SpecIBT as a transformation in Rocq and provide a machine-checked proof that it achieves relative security: any transformed program running with speculation leaks no more than what the source program leaks without speculation. This strong security guarantee applies to arbitrary programs, even those not following the cryptographic constant-time programming discipline.

</details>


### [94] [Trojan-Resilient NTT: Protecting Against Control Flow and Timing Faults on Reconfigurable Platforms](https://arxiv.org/abs/2601.22804)
*Rourab Paul,Krishnendu Guha,Amlan Chakrabarti*

Main category: cs.CR

TL;DR: This paper proposes a secure architecture for Number Theoretic Transform (NTT) to mitigate side-channel attacks and hardware vulnerabilities, including hardware Trojans.


<details>
  <summary>Details</summary>
Motivation: Address vulnerabilities in lattice-based PQC algorithms due to hardware Trojans and side-channel attacks, which can disrupt computations and compromise security.

Method: Developed a secure NTT architecture with fault detection and adaptive correction modules, tested on Artix-7 FPGA for validation.

Result: The architecture successfully detects and mitigates faults, including those caused by hardware Trojans, with high effectiveness and minimal overhead.

Conclusion: The secure NTT architecture enhances reliability against attacks while maintaining efficiency in lattice-based PQC implementations.

Abstract: Number Theoretic Transform (NTT) is the most essential component for polynomial multiplications used in lattice-based Post-Quantum Cryptography (PQC) algorithms such as Kyber, Dilithium, NTRU etc. However, side-channel attacks (SCA) and hardware vulnerabilities in the form of hardware Trojans may alter control signals to disrupt the circuit's control flow and introduce unconventional delays in the critical hardware of PQC. Hardware Trojans, especially on control signals, are more low cost and impactful than data signals because a single corrupted control signal can disrupt or bypass entire computation sequences, whereas data faults usually cause only localized errors. On the other hand, adversaries can perform Soft Analytical Side Channel Attacks (SASCA) on the design using the inserted hardware Trojan. In this paper, we present a secure NTT architecture capable of detecting unconventional delays, control-flow disruptions, and SASCA, while providing an adaptive fault-correction methodology for their mitigation. Extensive simulations and implementations of our Secure NTT on Artix-7 FPGA with different Kyber variants show that our fault detection and correction modules can efficiently detect and correct faults whether caused unintentionally or intentionally by hardware Trojans with a high success rate, while introducing only modest area and time overheads.

</details>
