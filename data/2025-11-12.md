<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 5]
- [cs.AR](#cs.AR) [Total: 5]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.CV](#cs.CV) [Total: 5]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 5]
- [cs.SE](#cs.SE) [Total: 6]
- [q-bio.NC](#q-bio.NC) [Total: 4]
- [stat.ML](#stat.ML) [Total: 5]
- [cs.CR](#cs.CR) [Total: 2]
- [math.NA](#math.NA) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Analysing Environmental Efficiency in AI for X-Ray Diagnosis](https://arxiv.org/abs/2511.07436)
*Liam Kearns*

Main category: cs.AI

TL;DR: This paper integrates large language models (LLMs) and smaller discriminative models into a Mendix application for detecting Covid-19 in chest X-rays, with a focus on accuracy and environmental impact.


<details>
  <summary>Details</summary>
Motivation: To explore and benchmark the integration of LLMs and small discriminative models in medical applications, especially for improving Covid-19 detection, while considering accuracy and environmental impacts.

Method: The study compares 14 model configurations, using LLMs and small discriminative models for chest X-ray analysis within a Mendix application. Specific models, such as GPT-4.1-Nano and Covid-Net, are assessed for accuracy and carbon footprint.

Result: Smaller models were less environmentally impactful but had reduced accuracy and bias in output. The Covid-Net model achieved the highest accuracy (95.5%) with the smallest environmental impact, reducing carbon footprint by 99.9% compared to larger LLMs like GPT-4.5.

Conclusion: While LLMs have potential in medical diagnostics, small discriminative models like Covid-Net are more efficient and environmentally sustainable for classification tasks. Generative AI should be used carefully due to accuracy and environmental trade-offs.

Abstract: The integration of AI tools into medical applications has aimed to improve the efficiency of diagnosis. The emergence of large language models (LLMs), such as ChatGPT and Claude, has expanded this integration even further. Because of LLM versatility and ease of use through APIs, these larger models are often utilised even though smaller, custom models can be used instead. In this paper, LLMs and small discriminative models are integrated into a Mendix application to detect Covid-19 in chest X-rays. These discriminative models are also used to provide knowledge bases for LLMs to improve accuracy. This provides a benchmark study of 14 different model configurations for comparison of accuracy and environmental impact. The findings indicated that while smaller models reduced the carbon footprint of the application, the output was biased towards a positive diagnosis and the output probabilities were lacking confidence. Meanwhile, restricting LLMs to only give probabilistic output caused poor performance in both accuracy and carbon footprint, demonstrating the risk of using LLMs as a universal AI solution. While using the smaller LLM GPT-4.1-Nano reduced the carbon footprint by 94.2% compared to the larger models, this was still disproportionate to the discriminative models; the most efficient solution was the Covid-Net model. Although it had a larger carbon footprint than other small models, its carbon footprint was 99.9% less than when using GPT-4.5-Preview, whilst achieving an accuracy of 95.5%, the highest of all models examined. This paper contributes to knowledge by comparing generative and discriminative models in Covid-19 detection as well as highlighting the environmental risk of using generative tools for classification tasks.

</details>


### [2] [Agentic Educational Content Generation for African Languages on Edge Devices](https://arxiv.org/abs/2511.07437)
*Ravi Gupta,Guneet Bhatia*

Main category: cs.AI

TL;DR: This paper introduces a framework for decentralized, culturally adaptive educational content on edge devices, achieving notable performance metrics and aligning with UN sustainable development goals.


<details>
  <summary>Details</summary>
Motivation: To address educational inequity in Sub-Saharan Africa, especially in resource-constrained environments, by providing accessible and culturally relevant AI-driven educational solutions.

Method: The framework utilizes a multi-agent system with four specialized agents for generating contextually adaptive content. It is evaluated on edge devices like Raspberry Pi 4B and NVIDIA Jetson Nano, focusing on performance, power consumption, and multilingual quality.

Result: Significant performance results include TTFT of 129 ms and throughput of 45.2 tokens/second on Jetson Nano; high BLEU score of 0.688; cultural relevance (4.4/5) and fluency (4.2/5) in African languages.

Conclusion: The framework offers a sustainable, localized solution for rural education, aligning with SDGs 4 (Quality Education), 9 (Industry, Innovation, and Infrastructure), and 10 (Reduced Inequalities), with practical impact potential through community partnerships.

Abstract: Addressing educational inequity in Sub-Saharan Africa, this research presents an autonomous agent-orchestrated framework for decentralized, culturally adaptive educational content generation on edge devices. The system leverages four specialized agents that work together to generate contextually appropriate educational content. Experimental validation on platforms including Raspberry Pi 4B and NVIDIA Jetson Nano demonstrates significant performance achievements. InkubaLM on Jetson Nano achieved a Time-To-First-Token (TTFT) of 129 ms, an average inter-token latency of 33 ms, and a throughput of 45.2 tokens per second while consuming 8.4 W. On Raspberry Pi 4B, InkubaLM also led with 326 ms TTFT and 15.9 tokens per second at 5.8 W power consumption. The framework consistently delivered high multilingual quality, averaging a BLEU score of 0.688, cultural relevance of 4.4/5, and fluency of 4.2/5 across tested African languages. Through potential partnerships with active community organizations including African Youth & Community Organization (AYCO) and Florida Africa Foundation, this research aims to establish a practical foundation for accessible, localized, and sustainable AI-driven education in resource-constrained environments. Keeping focus on long-term viability and cultural appropriateness, it contributes to United Nations SDGs 4, 9, and 10. Index Terms - Multi-Agent Systems, Edge AI Computing, Educational Technology, African Languages, Rural Education, Sustainable Development, UN SDG.

</details>


### [3] [Beyond Correctness: Confidence-Aware Reward Modeling for Enhancing Large Language Model Reasoning](https://arxiv.org/abs/2511.07483)
*Qianxi He,Qingyu Ren,Shanzhe Lei,Xuhong Wang,Yingchun Wang*

Main category: cs.AI

TL;DR: This paper addresses the challenges of using rule-based reinforcement learning for reasoning in LLMs, particularly for smaller models. It introduces a confidence-based reward model to improve reasoning quality, validated through experiments and benchmarks.


<details>
  <summary>Details</summary>
Motivation: To enhance the reasoning abilities of LLMs, especially smaller models, as rule-based RL often fails to ensure consistency and quality in reasoning processes and answers.

Method: The paper proposes a confidence-based reward model that penalizes both incorrect answers and low-confidence correct ones. It undergoes validations like PPO-based RL, Best-of-N inference, and static evaluations.

Result: The proposed approach outperforms leading open-source reward models on various STEM reasoning benchmarks.

Conclusion: The confidence-based reward model enhances reasoning capability in LLMs, providing a more effective and consistent framework, especially for resource-limited organizations. Open-source codes and models are released for wider use.

Abstract: Recent advancements in large language models (LLMs) have shifted the post-training paradigm from traditional instruction tuning and human preference alignment toward reinforcement learning (RL) focused on reasoning capabilities. However, numerous technical reports indicate that purely rule-based reward RL frequently results in poor-quality reasoning chains or inconsistencies between reasoning processes and final answers, particularly when the base model is of smaller scale. During the RL exploration process, models might employ low-quality reasoning chains due to the lack of knowledge, occasionally producing correct answers randomly and receiving rewards based on established rule-based judges. This constrains the potential for resource-limited organizations to conduct direct reinforcement learning training on smaller-scale models. We propose a novel confidence-based reward model tailored for enhancing STEM reasoning capabilities. Unlike conventional approaches, our model penalizes not only incorrect answers but also low-confidence correct responses, thereby promoting more robust and logically consistent reasoning. We validate the effectiveness of our approach through static evaluations, Best-of-N inference tests, and PPO-based RL training. Our method outperforms several state-of-the-art open-source reward models across diverse STEM benchmarks. We release our codes and model in https://github.com/qianxiHe147/C2RM.

</details>


### [4] [Procedural Knowledge Improves Agentic LLM Workflows](https://arxiv.org/abs/2511.07568)
*Vincent Hsiao,Mark Roberts,Leslie Smith*

Main category: cs.AI

TL;DR: The paper explores how hierarchical task networks (HTNs), as procedural knowledge, enhance large language model (LLM) performance on agentic tasks requiring planning, showing significant improvements over baseline models.


<details>
  <summary>Details</summary>
Motivation: LLMs often struggle with agentic tasks without extensive support. The research investigates whether leveraging domain-dependent procedural knowledge through HTNs can improve LLM efficiency in performing tasks requiring implicit planning.

Method: The researchers formalized, implemented, and evaluated an LLM workflow by integrating hierarchical task networks (HTNs). They examined both hand-coded HTNs and HTNs created by LLMs in various agentic task scenarios.

Result: Results reveal that hand-coded HTNs significantly boost performance, enabling smaller LLMs (20b or 70b parameters) to outperform a baseline larger 120b parameter LLM. LLM-created HTNs also improve performance but to a lesser extent.

Conclusion: Leveraging procedural expertise, either human-crafted or LLM-generated, through HTNs can substantially enhance LLM workflows, emphasizing the importance of curating domain knowledge for improving efficiency and effectiveness in agentic tasks.

Abstract: Large language models (LLMs) often struggle when performing agentic tasks without substantial tool support, prom-pt engineering, or fine tuning. Despite research showing that domain-dependent, procedural knowledge can dramatically increase planning efficiency, little work evaluates its potential for improving LLM performance on agentic tasks that may require implicit planning. We formalize, implement, and evaluate an agentic LLM workflow that leverages procedural knowledge in the form of a hierarchical task network (HTN). Empirical results of our implementation show that hand-coded HTNs can dramatically improve LLM performance on agentic tasks, and using HTNs can boost a 20b or 70b parameter LLM to outperform a much larger 120b parameter LLM baseline. Furthermore, LLM-created HTNs improve overall performance, though less so. The results suggest that leveraging expertise--from humans, documents, or LLMs--to curate procedural knowledge will become another important tool for improving LLM workflows.

</details>


### [5] [Think Before You Retrieve: Learning Test-Time Adaptive Search with Small Language Models](https://arxiv.org/abs/2511.07581)
*Supriti Vijay,Aman Priyanshu,Anu Vellore,Baturay Saglam,Amin Karbasi*

Main category: cs.AI

TL;DR: The paper presents Orion, a training framework for compact models to perform iterative retrieval, surpassing larger models in effectiveness on several benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of neural retrievers, LLMs, and static transformations in handling complex iterative search requirements efficiently.

Method: Orion uses synthetic trajectory generation, supervised fine-tuning, reinforcement learning for query refinement and backtracking, and inference-time beam search algorithms.

Result: The Orion model (1.2B parameters) achieves superior or competitive retrieval success compared to methods using larger models across multiple benchmarks, such as SciFact, BRIGHT, and NFCorpus.

Conclusion: Retrieval effectiveness can arise from learned search strategies rather than sheer model scale, demonstrating that compact models can excel with the right training design.

Abstract: Effective information retrieval requires reasoning over partial evidence and refining strategies as information emerges. Yet current approaches fall short: neural retrievers lack reasoning capabilities, large language models (LLMs) provide semantic depth but at prohibitive cost, and query rewriting or decomposition limits improvement to static transformations. As a result, existing methods fail to capture the iterative dynamics of exploration, feedback, and revision that complex user queries demand. We introduce Orion, a training framework that enables compact models (350M-1.2B parameters) to perform iterative retrieval through learned search strategies. Orion combines: (1) synthetic trajectory generation and supervised fine-tuning to encourage diverse exploration patterns in models, (2) reinforcement learning (RL) that rewards effective query refinement and backtracking behaviors, and (3) inference-time beam search algorithms that exploit the self-reflection capabilities learned during RL. Despite using only 3% of the training data available, our 1.2B model achieves 77.6% success on SciFact (vs. 72.6% for prior retrievers), 25.2% on BRIGHT (vs. 22.1%), 63.2% on NFCorpus (vs. 57.8%), and remains competitive on FEVER, HotpotQA, and MSMarco. It outperforms retrievers up to 200-400x larger on five of six benchmarks. These findings suggest that retrieval performance can emerge from learned strategies, not just model scale, when models are trained to search, reflect, and revise.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [6] [FractalCloud: A Fractal-Inspired Architecture for Efficient Large-Scale Point Cloud Processing](https://arxiv.org/abs/2511.07665)
*Yuzhe Fu,Changchun Zhou,Hancheng Ye,Bowen Duan,Qiyu Huang,Chiyue Wei,Cong Guo,Hai "Helen'' Li,Yiran Chen*

Main category: cs.AR

TL;DR: FractalCloud introduces a novel fractal-inspired hardware architecture for efficient 3D point cloud processing, achieving significant speedup and energy reduction.


<details>
  <summary>Details</summary>
Motivation: Existing accelerators for point cloud processing struggle with efficient large-scale workloads due to $O(n^2)$ complexity and memory overhead.

Method: FractalCloud employs a co-designed Fractal method for partitioning and block-parallel point operations, integrated into a hardware design for efficient processing.

Result: Operating on 28 nm technology, FractalCloud shows a 21.7x speedup and 27x energy savings compared to current accelerators while preserving accuracy.

Conclusion: FractalCloud successfully addresses scalability and efficiency challenges in large-scale PNN inference, setting a precedent for future hardware architectures.

Abstract: Three-dimensional (3D) point clouds are increasingly used in applications such as autonomous driving, robotics, and virtual reality (VR). Point-based neural networks (PNNs) have demonstrated strong performance in point cloud analysis, originally targeting small-scale inputs. However, as PNNs evolve to process large-scale point clouds with hundreds of thousands of points, all-to-all computation and global memory access in point cloud processing introduce substantial overhead, causing $O(n^2)$ computational complexity and memory traffic where n is the number of points}. Existing accelerators, primarily optimized for small-scale workloads, overlook this challenge and scale poorly due to inefficient partitioning and non-parallel architectures. To address these issues, we propose FractalCloud, a fractal-inspired hardware architecture for efficient large-scale 3D point cloud processing. FractalCloud introduces two key optimizations: (1) a co-designed Fractal method for shape-aware and hardware-friendly partitioning, and (2) block-parallel point operations that decompose and parallelize all point operations. A dedicated hardware design with on-chip fractal and flexible parallelism further enables fully parallel processing within limited memory resources. Implemented in 28 nm technology as a chip layout with a core area of 1.5 $mm^2$, FractalCloud achieves 21.7x speedup and 27x energy reduction over state-of-the-art accelerators while maintaining network accuracy, demonstrating its scalability and efficiency for PNN inference.

</details>


### [7] [PIMfused: Near-Bank DRAM-PIM with Fused-layer Dataflow for CNN Data Transfer Optimization](https://arxiv.org/abs/2511.07985)
*Simei Yang,Xinyu Shi,Lu Zhao,Yunyu Ling,Quanjun Wang,Francky Catthoor*

Main category: cs.AR

TL;DR: The paper introduces PIMfused, a co-design hardware-software approach, optimizing CNN execution in near-bank DRAM-PIM by reducing cross-bank data dependencies and improving overall PPA (performance, power, area) gains.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of high costs due to off-chip memory accesses in PIM architectures and performance constraints caused by cross-bank data transfers during CNN execution.

Method: PIMfused introduces fused-layer dataflow, a co-design approach that improves data reuse and eliminates inter-bank data dependencies while maintaining bank-level parallelism.

Result: PIMfused demonstrates significant performance, power, and area (PPA) gains. Using a ResNet18 model, it achieves reductions in memory cycles to 30.6%, energy to 83.4%, and area to 76.5% compared to the baseline.

Conclusion: PIMfused is an effective solution to the challenges of executing CNNs on DRAM-PIM architectures, enhancing efficiency by optimizing cross-bank data transfers through a fused-layer dataflow approach.

Abstract: Near-bank Processing-in-Memory (PIM) architectures integrate processing cores (PIMcores) close to DRAM banks to mitigate the high cost of off-chip memory accesses. When accelerating convolutional neural network (CNN) on DRAM-PIM, performance is often constrained by cross-bank (or cross-PIMcore) data transfers, which are induced by the conventional layer-by-layer dataflow that enforces inter-bank (or inter-PIMcore) dependencies across successive CNN layers. To address this challenge, we propose PIMfused, a hardware-software co-design that enables fused-layer dataflow for end-to-end CNN execution in near-bank DRAM-PIM. By adopting fused-layer dataflow, PIMfused improves data reuse and, more importantly, breaks inter-bank data dependencies, thereby optimizing cross-bank data transfers without sacrificing bank-level parallelism. We study the impact of buffer sizes and PIMcore parallelism (1-bank vs. 4-bank) on PIMfused using end-to-end ResNet18. We present three key takeaways and show that with 4-bank PIMcores, PIMfused achieves overall PPA gains over a GDDR6-AiM-like baseline, cutting memory cycles to 30.6%, energy to 83.4%, and area to 76.5%.

</details>


### [8] [Re$^{\text{2}}$MaP: Macro Placement by Recursively Prototyping and Packing Tree-based Relocating](https://arxiv.org/abs/2511.08054)
*Yunqi Shi,Xi Lin,Zhiang Wang,Siyuan Xu,Shixiong Kai,Yao Lai,Chengrui Gao,Ke Xue,Mingxuan Yuan,Chao Qian,Zhi-Hua Zhou*

Main category: cs.AR

TL;DR: The Re$^{\text{2}}$MaP method improves macro placement by recursively prototyping and relocating tree structures, showing significant performance improvements over current methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing macro placement methods in optimized chip design efficiency and performance.

Method: The method involves multi-level macro grouping, analytical angle-based placement (ABPlace), and packing-tree relocations, optimized through evolutionary search, iteratively refining the placements.

Result: Re$^{\text{2}}$MaP outperforms state-of-the-art academic placer Hier-RTLMP, with improvements of up to 22.22% in WNS and 97.91% in TNS, along with gains in power and runtime efficiency.

Conclusion: The proposed method effectively enhances macro placement quality, showcasing robust performance improvements across various metrics and circuits.

Abstract: This work introduces the Re$^{\text{2}}$MaP method, which generates expert-quality macro placements through recursively prototyping and packing tree-based relocating. We first perform multi-level macro grouping and PPA-aware cell clustering to produce a unified connection matrix that captures both wirelength and dataflow among macros and clusters. Next, we use DREAMPlace to build a mixed-size placement prototype and obtain reference positions for each macro and cluster. Based on this prototype, we introduce ABPlace, an angle-based analytical method that optimizes macro positions on an ellipse to distribute macros uniformly near chip periphery, while optimizing wirelength and dataflow. A packing tree-based relocating procedure is then designed to jointly adjust the locations of macro groups and the macros within each group, by optimizing an expertise-inspired cost function that captures various design constraints through evolutionary search. Re$^{\text{2}}$MaP repeats the above process: Only a subset of macro groups are positioned in each iteration, and the remaining macros are deferred to the next iteration to improve the prototype's accuracy. Using a well-established backend flow with sufficient timing optimizations, Re$^{\text{2}}$MaP achieves up to 22.22% (average 10.26%) improvement in worst negative slack (WNS) and up to 97.91% (average 33.97%) improvement in total negative slack (TNS) compared to the state-of-the-art academic placer Hier-RTLMP. It also ranks higher on WNS, TNS, power, design rule check (DRC) violations, and runtime than the conference version ReMaP, across seven tested cases. Our code is available at https://github.com/lamda-bbo/Re2MaP.

</details>


### [9] [BDD2Seq: Enabling Scalable Reversible-Circuit Synthesis via Graph-to-Sequence Learning](https://arxiv.org/abs/2511.08315)
*Mingkai Miao,Jianheng Tang,Guangyu Hu,Hongce Zhang*

Main category: cs.AR

TL;DR: BDD2Seq leverages a graph-to-sequence framework to optimize variable ordering in BDD-based reversible circuit synthesis for quantum computing.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the NP-complete problem of finding optimal variable orderings in BDDs, as existing heuristics degrade with increased circuit complexity.

Method: BDD2Seq uses a Graph Neural Network encoder, Pointer-Network decoder, and Diverse Beam Search to predict optimal variable orderings based on circuit netlist structures.

Result: Experiments on three benchmarks demonstrate BDD2Seq achieves ~1.4 times lower Quantum Cost and ~3.7 times faster synthesis compared to modern heuristic algorithms.

Conclusion: BDD2Seq is the first approach utilizing graph-based generative models for variable-ordering in BDD-based reversible circuit synthesis, showing significant improvements in resource optimization.

Abstract: Binary Decision Diagrams (BDDs) are instrumental in many electronic design automation (EDA) tasks thanks to their compact representation of Boolean functions. In BDD-based reversible-circuit synthesis, which is critical for quantum computing, the chosen variable ordering governs the number of BDD nodes and thus the key metrics of resource consumption, such as Quantum Cost. Because finding an optimal variable ordering for BDDs is an NP-complete problem, existing heuristics often degrade as circuit complexity grows. We introduce BDD2Seq, a graph-to-sequence framework that couples a Graph Neural Network encoder with a Pointer-Network decoder and Diverse Beam Search to predict high-quality orderings. By treating the circuit netlist as a graph, BDD2Seq learns structural dependencies that conventional heuristics overlooked, yielding smaller BDDs and faster synthesis. Extensive experiments on three public benchmarks show that BDD2Seq achieves around 1.4 times lower Quantum Cost and 3.7 times faster synthesis than modern heuristic algorithms. To the best of our knowledge, this is the first work to tackle the variable-ordering problem in BDD-based reversible-circuit synthesis with a graph-based generative model and diversity-promoting decoding.

</details>


### [10] [DRACO: Co-design for DSP-Efficient Rigid Body Dynamics Accelerator](https://arxiv.org/abs/2511.08395)
*Xingyu Liu,Jiawei Liang,Yipu Zhang,Linfeng Du,Chaofang Ma,Hui Yu,Jiang Xu,Wei Zhang*

Main category: cs.AR

TL;DR: The paper introduces a hardware-efficient RBD accelerator using FPGA with innovations in quantization, algorithm optimization, and DSP reuse, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Address the need for more efficient and scalable hardware accelerators for robotic dynamics (RBD) computation, particularly in high-DOF robotic systems.

Method: Developed three innovative techniques: precision-aware quantization framework, division deferring optimization for mass matrix inversion, and inter-module DSP reuse methodology.

Result: The RBD accelerator achieved up to 8x improvement in throughput and 7.4x reduction in latency compared to existing solutions across diverse robotic systems.

Conclusion: The proposed accelerator effectively enhances performance, scalability, and DSP utilization in robotic systems, offering a hardware-efficient solution for high-DOF robotics.

Abstract: We propose a hardware-efficient RBD accelerator based on FPGA, introducing three key innovations. First, we propose a precision-aware quantization framework that reduces DSP demand while preserving motion accuracy. This is also the first study to systematically evaluate quantization impact on robot control and motion for hardware acceleration. Second, we leverage a division deferring optimization in mass matrix inversion algorithm, which decouples reciprocal operations from the longest latency path to improve the performance. Finally, we present an inter-module DSP reuse methodology to improve DSP utilization and save DSP usage. Experiment results show that our work achieves up to 8x throughput improvement and 7.4x latency reduction over state-of-the-art RBD accelerators across various robot types, demonstrating its effectiveness and scalability for high-DOF robotic systems.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [11] [Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models](https://arxiv.org/abs/2511.08577)
*Tianyu Fu,Yichen You,Zekai Chen,Guohao Dai,Huazhong Yang,Yu Wang*

Main category: cs.CL

TL;DR: The paper introduces Think-at-Hard (TaH), a method to enhance reasoning in LLMs by dynamically iterating deeper only on difficult tokens, improving performance while maintaining parameter efficiency.


<details>
  <summary>Details</summary>
Motivation: Improving reasoning capabilities of LLMs is necessary for real-world use, especially under constraints of computational efficiency and fixed parameters.

Method: TaH employs a dynamic latent thinking mechanism, powered by a neural decider and Low-Rank Adaptation (LoRA) modules, to selectively refine hard tokens. It also uses duo-causal attention for better cross-iteration information flow.

Result: Experiments showed TaH improves reasoning accuracy by 8.1-11.3% over 5 benchmarks compared to static iteration methods, exempting 94% of tokens from additional passes, and gains against single-iteration models with minimal parameter increase.

Conclusion: TaH provides an efficient approach to enhance LLM reasoning by enabling dynamic iterative refinement, yielding significant accuracy improvement while maintaining computational resources.

Abstract: Improving reasoning capabilities of Large Language Models (LLMs), especially under parameter constraints, is crucial for real-world applications. Prior work proposes recurrent transformers, which allocate a fixed number of extra iterations per token to improve generation quality. After the first, standard forward pass, instead of verbalization, last-layer hidden states are fed back as inputs for additional iterations to refine token predictions. Yet we identify a latent overthinking phenomenon: easy token predictions that are already correct after the first pass are sometimes revised into errors in additional iterations. To address this, we propose Think-at-Hard (TaH), a dynamic latent thinking method that iterates deeper only at hard tokens. It employs a lightweight neural decider to trigger latent iterations only at tokens that are likely incorrect after the standard forward pass. During latent iterations, Low-Rank Adaptation (LoRA) modules shift the LLM objective from general next-token prediction to focused hard-token refinement. We further introduce a duo-causal attention mechanism that extends attention from the token sequence dimension to an additional iteration depth dimension. This enables cross-iteration information flow while maintaining full sequential parallelism. Experiments show that TaH boosts LLM reasoning performance across five challenging benchmarks while maintaining the same parameter count. Compared with baselines that iterate twice for all output tokens, TaH delivers 8.1-11.3% accuracy gains while exempting 94% of tokens from the second iteration. Against strong single-iteration Qwen3 models finetuned with the same data, it also delivers 4.0-5.0% accuracy gains. When allowing less than 3% additional parameters from LoRA and the iteration decider, the gains increase to 8.5-12.6% and 5.3-5.4%, respectively. Our code is available at https://github.com/thu-nics/TaH.

</details>


### [12] [A Preliminary Study of RAG for Taiwanese Historical Archives](https://arxiv.org/abs/2511.07445)
*Claire Lin,Bo-Han Feng,Xuanjun Chen,Te-Lun Yang,Hung-yi Lee,Jyh-Shing Roger Jang*

Main category: cs.CL

TL;DR: This study investigates the application of Retrieval-Augmented Generation (RAG) to Taiwanese historical archives, analyzing effects of metadata integration and query types.


<details>
  <summary>Details</summary>
Motivation: To explore the use of RAG methods in processing historical archives, particularly from Taiwan, with a focus on enhancing retrieval and answer generation.

Method: Tested RAG mechanisms on two Traditional Chinese datasets, Fort Zeelandia and the Taiwan Provincial Council Gazette, investigating metadata integration strategies and query impacts.

Result: Findings show early metadata integration benefits for retrieval and accuracy, while issues like hallucinations and handling complex historical queries persist.

Conclusion: RAG systems benefit from metadata integration but require further improvement to address hallucinations and complex historical query challenges.

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising approach for knowledge-intensive tasks. However, few studies have examined RAG for Taiwanese Historical Archives. In this paper, we present an initial study of a RAG pipeline applied to two historical Traditional Chinese datasets, Fort Zeelandia and the Taiwan Provincial Council Gazette, along with their corresponding open-ended query sets. We systematically investigate the effects of query characteristics and metadata integration strategies on retrieval quality, answer generation, and the performance of the overall system. The results show that early-stage metadata integration enhances both retrieval and answer accuracy while also revealing persistent challenges for RAG systems, including hallucinations during generation and difficulties in handling temporal or multi-hop historical queries.

</details>


### [13] [Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey](https://arxiv.org/abs/2511.07448)
*Fatemeh Shahhosseini,Arash Marioriyad,Ali Momen,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban,Shaghayegh Haghjooy Javanmard*

Main category: cs.CL

TL;DR: The paper explores the role of large language models (LLMs) in scientific idea generation, categorizing methods to enhance creativity and soundness while using established creativity frameworks for analysis.


<details>
  <summary>Details</summary>
Motivation: To address the inconsistent and underexplored creative capacity of LLMs in generating scientifically novel and empirically sound ideas for advancing scientific discovery.

Method: The paper categorizes methods into five families (External knowledge augmentation, Prompt-based distributional steering, Inference-time scaling, Multi-agent collaboration, Parameter-level adaptation) and evaluates them using Boden's and Rhodes' creativity frameworks.

Result: A structured synthesis of methods is provided, laying out how LLM-driven idea generation balances creativity and empirical soundness while aligning methodologies with creativity theories.

Conclusion: This survey clarifies the current state of LLMs' use in scientific discovery and suggests directions for improving their reliability and transformative capability in scientific ideation.

Abstract: Scientific idea generation lies at the heart of scientific discovery and has driven human progress-whether by solving unsolved problems or proposing novel hypotheses to explain unknown phenomena. Unlike standard scientific reasoning or general creative generation, idea generation in science is a multi-objective and open-ended task, where the novelty of a contribution is as essential as its empirical soundness. Large language models (LLMs) have recently emerged as promising generators of scientific ideas, capable of producing coherent and factual outputs with surprising intuition and acceptable reasoning, yet their creative capacity remains inconsistent and poorly understood. This survey provides a structured synthesis of methods for LLM-driven scientific ideation, examining how different approaches balance creativity with scientific soundness. We categorize existing methods into five complementary families: External knowledge augmentation, Prompt-based distributional steering, Inference-time scaling, Multi-agent collaboration, and Parameter-level adaptation. To interpret their contributions, we employ two complementary frameworks: Boden's taxonomy of Combinatorial, Exploratory and Transformational creativity to characterize the level of ideas each family expected to generate, and Rhodes' 4Ps framework-Person, Process, Press, and Product-to locate the aspect or source of creativity that each method emphasizes. By aligning methodological advances with creativity frameworks, this survey clarifies the state of the field and outlines key directions toward reliable, systematic, and transformative applications of LLMs in scientific discovery.

</details>


### [14] [GRIP: In-Parameter Graph Reasoning through Fine-Tuning Large Language Models](https://arxiv.org/abs/2511.07457)
*Jiarui Feng,Donghong Cai,Yixin Chen,Muhan Zhang*

Main category: cs.CL

TL;DR: The paper introduces GRIP, a framework for adapting Large Language Models (LLMs) to perform graph-related tasks using lightweight LoRA parameters without requiring graphs at inference.


<details>
  <summary>Details</summary>
Motivation: The need to adapt LLMs for structural data efficiently without heavy token overhead or complex post-training.

Method: GRIP fine-tunes LLMs using specific tasks to internalize relational graph information, leveraging lightweight LoRA parameters.

Result: Experiments confirm GRIPâ€™s effectiveness and efficiency across multiple benchmarks.

Conclusion: GRIP achieves practical and efficient adaptation of LLMs to graph-related tasks, solving issues in previous methods.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in modeling sequential textual data and generalizing across diverse tasks. However, adapting LLMs to effectively handle structural data, such as knowledge graphs or web data, remains a challenging problem. Some approaches adopt complex strategies to convert graphs into text sequences, resulting in significant token overhead and rendering them impractical for large-scale graphs. Others introduce additional modules to encode graphs into fixed-size token representations for LLMs. However, these methods typically require large-scale post-training on graph-text corpus and complex alignment procedures, yet often yield sub-optimal results due to poor modality alignment. Inspired by in-parameter knowledge injection for test-time adaptation of LLMs, we propose GRIP, a novel framework that equips LLMs with the ability to internalize complex relational information from graphs through carefully designed fine-tuning tasks. This knowledge is efficiently stored within lightweight LoRA parameters, enabling the fine-tuned LLM to perform a wide range of graph-related tasks without requiring access to the original graph at inference time. Extensive experiments across multiple benchmarks validate the effectiveness and efficiency of our approach.

</details>


### [15] [REFLEX: Reference-Free Evaluation of Log Summarization via Large Language Model Judgment](https://arxiv.org/abs/2511.07458)
*Priyanka Mudgal*

Main category: cs.CL

TL;DR: REFLEX introduces a new evaluation metric for log summarization based on LLM judgment, addressing limitations of traditional metrics and dependency on reference summaries.


<details>
  <summary>Details</summary>
Motivation: Existing metrics for log summarization rely on lexical overlap and lack high-quality references, making evaluation challenging.

Method: REFLEX employs large language models as zero-shot evaluators to assess summaries on dimensions like relevance, informativeness, and coherence without gold-standard references.

Result: REFLEX demonstrates stable, interpretable, and fine-grained evaluations, outperforming traditional metrics in distinguishing model outputs across datasets.

Conclusion: REFLEX offers a scalable and effective solution for evaluating log summaries in scenarios where references are scarce.

Abstract: Evaluating log summarization systems is challenging due to the lack of high-quality reference summaries and the limitations of existing metrics like ROUGE and BLEU, which depend on surface-level lexical overlap. We introduce REFLEX, a reference-free evaluation metric for log summarization based on large language model (LLM) judgment. REFLEX uses LLMs as zero-shot evaluators to assess summary quality along dimensions such as relevance, informativeness, and coherence, without requiring gold-standard references or human annotations. We show that REFLEX produces stable, interpretable, and fine-grained evaluations across multiple log summarization dataset, and more effectively distinguishes model outputs than traditional metrics. REFLEX provides a scalable alternative for evaluating log summaries in real-world settings where reference data is scarce or unavailable.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [16] [Knowledge-Guided Textual Reasoning for Explainable Video Anomaly Detection via LLMs](https://arxiv.org/abs/2511.07429)
*Hari Lee*

Main category: cs.CV

TL;DR: The paper introduces a framework for video anomaly detection using language-based reasoning, evaluated on public surveillance benchmarks.


<details>
  <summary>Details</summary>
Motivation: To offer interpretable and knowledge-grounded reasoning in video anomaly detection, moving away from reliance on explicit visual features.

Method: TbVAD operates in three stages: transforming video content into captions, organizing captions into semantic knowledge slots, and providing slot-wise textual explanations.

Result: The framework is effective and interpretable, demonstrating reliable detection on UCF-Crime and XD-Violence datasets.

Conclusion: TbVAD provides a novel approach to video anomaly detection through textual reasoning, making it suitable for complex surveillance applications.

Abstract: We introduce Text-based Explainable Video Anomaly Detection (TbVAD), a language-driven framework for weakly supervised video anomaly detection that performs anomaly detection and explanation entirely within the textual domain. Unlike conventional WSVAD models that rely on explicit visual features, TbVAD represents video semantics through language, enabling interpretable and knowledge-grounded reasoning. The framework operates in three stages: (1) transforming video content into fine-grained captions using a vision-language model, (2) constructing structured knowledge by organizing the captions into four semantic slots (action, object, context, environment), and (3) generating slot-wise explanations that reveal which semantic factors contribute most to the anomaly decision. We evaluate TbVAD on two public benchmarks, UCF-Crime and XD-Violence, demonstrating that textual knowledge reasoning provides interpretable and reliable anomaly detection for real-world surveillance scenarios.

</details>


### [17] [Two Datasets Are Better Than One: Method of Double Moments for 3-D Reconstruction in Cryo-EM](https://arxiv.org/abs/2511.07438)
*Joe Kileel,Oscar Mickelin,Amit Singer,Sheng Xu*

Main category: cs.CV

TL;DR: The paper introduces a method (MoDM) for reconstructing molecular structures using second-order moments from different orientation distributions, demonstrating enhanced reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: To address challenges in cryo-EM, specifically reconstructing molecular structures under diverse experimental conditions to improve quality.

Method: The framework called MoDM uses second-order moments of projection images, one uniform and one non-uniform, with a convex-relaxation algorithm for structural recovery.

Result: MoDM proved the unique determination of structures (with some symmetry limitations) and performed accurate recovery based on diverse datasets.

Conclusion: Combining diverse datasets and using second-order statistics in cryo-EM imaging improves molecular structure reconstruction significantly.

Abstract: Cryo-electron microscopy (cryo-EM) is a powerful imaging technique for reconstructing three-dimensional molecular structures from noisy tomographic projection images of randomly oriented particles. We introduce a new data fusion framework, termed the method of double moments (MoDM), which reconstructs molecular structures from two instances of the second-order moment of projection images obtained under distinct orientation distributions--one uniform, the other non-uniform and unknown. We prove that these moments generically uniquely determine the underlying structure, up to a global rotation and reflection, and we develop a convex-relaxation-based algorithm that achieves accurate recovery using only second-order statistics. Our results demonstrate the advantage of collecting and modeling multiple datasets under different experimental conditions, illustrating that leveraging dataset diversity can substantially enhance reconstruction quality in computational imaging tasks.

</details>


### [18] [Modulo Video Recovery via Selective Spatiotemporal Vision Transformer](https://arxiv.org/abs/2511.07479)
*Tianyu Geng,Feng Ji,Wee Peng Tay*

Main category: cs.CV

TL;DR: Conventional sensors struggle with high dynamic range scenes. This paper introduces SSViT, a deep learning framework using Transformers for improved modulo video recovery.


<details>
  <summary>Details</summary>
Motivation: Current methods for high-dynamic-range scenes are limited in recovering modulo video data, and deep learning techniques have been underexplored for this task.

Method: The authors propose SSViT, a selective spatiotemporal Vision Transformer that employs token selection for more efficient and accurate modulo video reconstruction.

Result: SSViT achieves state-of-the-art performance in reconstructing folded 8-bit videos, showing significant improvement over existing methods.

Conclusion: SSViT's novel techniques demonstrate the potential for deep learning in capturing complex dependencies for modulo image recovery, advancing the field significantly.

Abstract: Conventional image sensors have limited dynamic range, causing saturation in high-dynamic-range (HDR) scenes. Modulo cameras address this by folding incident irradiance into a bounded range, yet require specialized unwrapping algorithms to reconstruct the underlying signal. Unlike HDR recovery, which extends dynamic range from conventional sampling, modulo recovery restores actual values from folded samples. Despite being introduced over a decade ago, progress in modulo image recovery has been slow, especially in the use of modern deep learning techniques. In this work, we demonstrate that standard HDR methods are unsuitable for modulo recovery. Transformers, however, can capture global dependencies and spatial-temporal relationships crucial for resolving folded video frames. Still, adapting existing Transformer architectures for modulo recovery demands novel techniques. To this end, we present Selective Spatiotemporal Vision Transformer (SSViT), the first deep learning framework for modulo video reconstruction. SSViT employs a token selection strategy to improve efficiency and concentrate on the most critical regions. Experiments confirm that SSViT produces high-quality reconstructions from 8-bit folded videos and achieves state-of-the-art performance in modulo video recovery.

</details>


### [19] [Laplacian Score Sharpening for Mitigating Hallucination in Diffusion Models](https://arxiv.org/abs/2511.07496)
*Barath Chandran. C,Srinivas Anumasa,Dianbo Liu*

Main category: cs.CV

TL;DR: This paper proposes a post-hoc score function adjustment for diffusion models to reduce hallucinated outputs using the Laplacian of the score.


<details>
  <summary>Details</summary>
Motivation: Diffusion models are prone to generating incoherent or unrealistic samples due to mode interpolation hallucination and score smoothening, and there is a need for a method to mitigate these issues.

Method: Introduce a post-hoc adjustment to the score function that applies a Laplacian-based (sharpness) correction. An efficient approximation is derived for higher dimensions using a finite-difference-based Hutchinson trace estimator.

Result: The proposed adjustment significantly reduces hallucinated samples in 1D, 2D, and high-dimensional image data, validated via experiments.

Conclusion: Laplacian-based corrections to the score function improve the reliability of diffusion models and provide insights into the connection between uncertainty and score behaviors.

Abstract: Diffusion models, though successful, are known to suffer from hallucinations that create incoherent or unrealistic samples. Recent works have attributed this to the phenomenon of mode interpolation and score smoothening, but they lack a method to prevent their generation during sampling. In this paper, we propose a post-hoc adjustment to the score function during inference that leverages the Laplacian (or sharpness) of the score to reduce mode interpolation hallucination in unconditional diffusion models across 1D, 2D, and high-dimensional image data. We derive an efficient Laplacian approximation for higher dimensions using a finite-difference variant of the Hutchinson trace estimator. We show that this correction significantly reduces the rate of hallucinated samples across toy 1D/2D distributions and a high- dimensional image dataset. Furthermore, our analysis explores the relationship between the Laplacian and uncertainty in the score.

</details>


### [20] [Toward the Frontiers of Reliable Diffusion Sampling via Adversarial Sinkhorn Attention Guidance](https://arxiv.org/abs/2511.07499)
*Kwanyoung Kim*

Main category: cs.CV

TL;DR: Proposes Adversarial Sinkhorn Attention Guidance (ASAG) to enhance generative quality in diffusion models by modifying attention mechanisms via optimal transport techniques.


<details>
  <summary>Details</summary>
Motivation: Improve the generative performance and reliability of diffusion models by addressing limitations of existing guidance methods that rely on heuristic distortions.

Method: Introduce ASAG, an approach that deliberately distorts self-attention mechanisms using adversarial costs injected via the Sinkhorn algorithm in diffusion models.

Result: Improved sample quality, controllability, and fidelity in text-to-image diffusion models and downstream tasks like IP-Adapter and ControlNet.

Conclusion: ASAG enhances model reliability and output quality in a lightweight, plug-and-play manner without requiring retraining, offering principled improvements for diffusion-based generative models.

Abstract: Diffusion models have demonstrated strong generative performance when using guidance methods such as classifier-free guidance (CFG), which enhance output quality by modifying the sampling trajectory. These methods typically improve a target output by intentionally degrading another, often the unconditional output, using heuristic perturbation functions such as identity mixing or blurred conditions. However, these approaches lack a principled foundation and rely on manually designed distortions. In this work, we propose Adversarial Sinkhorn Attention Guidance (ASAG), a novel method that reinterprets attention scores in diffusion models through the lens of optimal transport and intentionally disrupt the transport cost via Sinkhorn algorithm. Instead of naively corrupting the attention mechanism, ASAG injects an adversarial cost within self-attention layers to reduce pixel-wise similarity between queries and keys. This deliberate degradation weakens misleading attention alignments and leads to improved conditional and unconditional sample quality. ASAG shows consistent improvements in text-to-image diffusion, and enhances controllability and fidelity in downstream applications such as IP-Adapter and ControlNet. The method is lightweight, plug-and-play, and improves reliability without requiring any model retraining.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [21] [Towards Affordable, Adaptive and Automatic GNN Training on CPU-GPU Heterogeneous Platforms](https://arxiv.org/abs/2511.07421)
*Tong Qiao,Ao Zhou,Yingjie Qi,Yiou Wang,Han Wan,Jianlei Yang,Chunming Hu*

Main category: cs.DC

TL;DR: The paper introduces A3GNN, a framework enabling efficient Graph Neural Network training on heterogeneous CPU-GPU systems. It achieves better performance using locality-aware sampling, fine-grained parallelism, and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: High costs and hardware requirements for GNN training limit their accessibility in resource-constrained environments.

Method: A3GNN uses locality-aware sampling, fine-grained parallelism scheduling, and reinforcement learning to optimize resource allocation on CPU-GPU platforms for GNN training.

Result: A3GNN enables remarkable efficiency improvements, allowing systems with lower-tier GPUs to outperform higher-tier counterparts (e.g., 2080Ti surpassing A100 GPUs) in throughput with minimal accuracy trade-offs.

Conclusion: A3GNN demonstrates that affordable and adaptive optimization can significantly enhance accessibility and reduce resource dependency in GNN training.

Abstract: Graph Neural Networks (GNNs) have been widely adopted due to their strong performance. However, GNN training often relies on expensive, high-performance computing platforms, limiting accessibility for many tasks. Profiling of representative GNN workloads indicates that substantial efficiency gains are possible on resource-constrained devices by fully exploiting available resources. This paper introduces A3GNN, a framework for affordable, adaptive, and automatic GNN training on heterogeneous CPU-GPU platforms. It improves resource usage through locality-aware sampling and fine-grained parallelism scheduling. Moreover, it leverages reinforcement learning to explore the design space and achieve pareto-optimal trade-offs among throughput, memory footprint, and accuracy. Experiments show that A3GNN can bridge the performance gap, allowing seven Nvidia 2080Ti GPUs to outperform two A100 GPUs by up to 1.8X in throughput with minimal accuracy loss.

</details>


### [22] [From Attention to Disaggregation: Tracing the Evolution of LLM Inference](https://arxiv.org/abs/2511.07422)
*Madabattula Rajesh Kumar,Srinivasa Rao Aravilli,Mustafa Saify,Shashank Srivastava*

Main category: cs.DC

TL;DR: Large Language Models (LLMs) inference faces bottlenecks like memory bandwidth, computational throughput, and latency. This paper introduces disaggregated inference, separating computation phases to optimize performance.


<details>
  <summary>Details</summary>
Motivation: LLM deployment brings challenges in inference efficiency due to constraints in latency, throughput, and cost. Addressing these is crucial for real-time usage.

Method: The paper proposes disaggregated inference, splitting the compute-heavy prefill phase from the memory-intensive decode phase to optimize system performance.

Result: Disaggregated inference reduces resource contention and improves metrics such as Time to First Token and Inter Token Latency.

Conclusion: Separating components during LLM inference enables effective scaling and optimization, overcoming limitations of conventional GPU-based deployments.

Abstract: The evolution of Large Language Models from the Transformer architecture to models with trillions of parameters has shifted the primary bottleneck from model training to real time inference. Deploying these massive models is a complex distributed systems challenge constrained by memory bandwidth, computational throughput, and latency requirements. LLM inference fundamentally requires solving a multi objective optimization problem to minimize latency, maximize throughput, and reduce cost. This paper explores the necessary architectural shift towards disaggregated inference, which applies distributed systems principles such as service decomposition, resource disaggregation, and workload partitioning to overcome the limitations of traditional monolithic GPU clusters. By decoupling the compute intensive prefill phase from the memory intensive decode phase into independently scalable components, this paradigm mitigates resource contention and enables independent optimization of key metrics like Time to First Token and Inter Token Latency.

</details>


### [23] [Synera: Synergistic LLM Serving across Device and Cloud at Scale](https://arxiv.org/abs/2511.07423)
*Genglin Wang,Liekang Zeng,Bufang Yang,Kaiwei Liu,Guoliang Xing,Chumin Sun,Li Zhou,Jie Sun,Zhenyu Yan*

Main category: cs.DC

TL;DR: The paper introduces Synera, a device-cloud synergistic LLM serving system that improves performance and reduces costs while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: The increasing use of Large Language Models (LLMs) in mobile systems faces performance issues, such as generation quality degradation and latency. Existing solutions like cloud offloading and on-device Small Language Models (SLMs) are either limited by communication challenges or compromise quality.

Method: The authors propose Synera, a device-cloud synergistic system. It employs communication-efficient selective offloading, stall-free parallel inference, and scalable cloud batching to optimize LLM inference. These advancements are based on empirical studies of LLM computing characteristics.

Result: Extensive evaluations show that Synera provides 1.20-5.47x better generation quality compared to competitive baselines with similar latency. Additionally, it lowers cloud serving costs by 8.2-16.5% across various benchmarks.

Conclusion: Synera addresses the limitations of existing solutions by providing an efficient synergistic device-cloud mechanism for LLM inference, achieving superior performance and cost savings.

Abstract: Large Language Models (LLMs) are becoming key components in various mobile operating systems, driving smart applications like interactive chatbots and personal assistants. While bringing enhanced intelligence to mobile ends, their deployment suffers from a set of performance challenges, especially the generation quality degradation and prolonged latency. Prior works have mainly relied on solutions of cloud offloading or on-device Small Language Models (SLMs). However, the former is usually limited by the communication bottleneck, and the latter sacrifices generation quality due to resource constraints. To mitigate these limitations, this paper proposes Synera, a device-cloud synergistic LLM serving system that applies an efficient SLM-LLM synergistic mechanism. Through empirical studies on LLM's unique computing characteristics, Synera identifies a set of underexplored optimization opportunities in device-cloud synergistic LLM inference, including offloading decisions, pipeline stalls, and batching bottlenecks. To translate them into enhanced performance, Synera introduces tailored designs of communication-efficient selective offloading, stall-free parallel inference, and scalable cloud batching. Extensive evaluations with real-world testbeds show that Synera enables 1.20-5.47x better generation quality against competitive baselines with on-par latency performance. Compared with existing cloud serving, Synera achieves 8.2-16.5% lower cloud serving cost on various benchmarks.

</details>


### [24] [Enhancing reliability in AI inference services: An empirical study on real production incidents](https://arxiv.org/abs/2511.07424)
*Bhala Ranganathan,Mickey Zhang,Kai Wu*

Main category: cs.DC

TL;DR: The paper presents an analysis of large language model (LLM) inference failures in cloud systems, using a taxonomy and methodology to identify dominant failures and mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: Understanding and mitigating risks in hyperscale LLM inference operations within cloud systems to ensure reliability and minimize significant user and business impacts.

Method: Developed a taxonomy based on a year of operational data and validated it using 156 high-severity incidents, combined with a focused quantitative study for recency and relevance. Evaluated mitigation strategies and automation possibilities.

Result: Revealed dominant failure modes (~60% inference engine failures, ~40% timeouts within this), assessed successful mitigation methods (~74% auto-detected incidents; remaining required hotfixes or policies like traffic routing), and identified automation opportunities.

Conclusion: Systematic analysis of inference operations guided effective mitigation strategies (e.g., GPU routing, traffic rebalancing), reduced incident impacts, improved system reliability, and outlined a replicable methodology for others to adopt.

Abstract: Hyperscale large language model (LLM) inference places extraordinary demands on cloud systems, where even brief failures can translate into significant user and business impact. To better understand and mitigate these risks, we present one of the first provider-internal, practice-based analysis of LLM inference incidents. We developed a taxonomy and methodology grounded in a year of operational experience, validating it on 156 high-severity incidents, and conducted a focused quantitative study of Apr-Jun 2025 to ensure recency and relevance. Our approach achieves high labeling consistency (Cohen's K ~0.89), identifies dominant failure modes (in our dataset ~60% inference engine failures, within that category ~40% timeouts), and surfaces mitigation levers (~74% auto-detected; ~28% required hotfix). Beyond hotfixes, many incidents were mitigated via traffic routing, node rebalancing, or capacity increase policies, indicating further automation opportunities. We also show how the taxonomy guided targeted strategies such as connection liveness, GPU capacity-aware routing, and per-endpoint isolation and reduced incident impact and accelerated recovery. Finally, we contribute a practitioner-oriented adoption checklist that enables others to replicate our taxonomy, analysis, and automation opportunities in their own systems. This study demonstrates how systematic, empirically grounded analysis of inference operations can drive more reliable and cost-efficient LLM serving at scale.

</details>


### [25] [An Evaluation of LLMs Inference on Popular Single-board Computers](https://arxiv.org/abs/2511.07425)
*Tung,Nguyen,Tuyen Nguyen*

Main category: cs.DC

TL;DR: The study evaluates the performance of 25 quantized open-source large language models (LLMs) on single-board computers (SBCs) like Raspberry Pi and Orange Pi, focusing on throughput, memory, and power efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the feasibility of deploying cost-effective, privacy-preserving AI solutions on edge hardware, such as SBCs, for localized LLM inference.

Method: The authors benchmarked 25 quantized LLMs on three SBCs using two runtimes (Ollama and Llamafile), measuring performance metrics like throughput, memory usage, and power consumption under various configurations.

Result: The study shows SBCs can support models up to 1.5B parameters, with Llamafile showing significantly better throughput (up to 4x) and lower power consumption (30-40%) than Ollama.

Conclusion: SBCs are capable of running lightweight LLMs effectively, making localized AI solutions feasible. The research identifies performance bottlenecks and offers deployment guidance for edge computing.

Abstract: The growing demand for on-device large language model (LLM) inference is driving interest in deploying lightweight, cost-effective AI solutions on edge hardware. Single-board computers (SBCs) such as the Raspberry Pi and Orange Pi offer a promising platform for localized, privacy-preserving inference-but remain underexplored in the context of LLM workloads. In this work, we benchmark the performance of 25 quantized open-source LLMs across three SBCs-Raspberry Pi 4, Raspberry Pi 5, and Orange Pi 5 Pro-using two inference runtimes: Ollama and Llamafile. We evaluate generation throughput, memory usage, and power consumption under varying CPU configurations, using multiple prompt types to simulate realistic workloads. Our results show that SBCs can reliably support models up to 1.5B parameters, with Llamafile achieving up to 4x higher throughput and 30-40% lower power usage than Ollama. We identify architecture-specific bottlenecks, highlight runtime-level trade-offs, and provide practical deployment recommendations. This study offers the first broad evaluation of LLM inference on SBCs, bridging the gap between high-performance language models and affordable edge computing.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [26] [Optimizing Classification of Infrequent Labels by Reducing Variability in Label Distribution](https://arxiv.org/abs/2511.07459)
*Ashutosh Agarwal*

Main category: cs.LG

TL;DR: LEVER, a novel Siamese-style architecture, addresses underperforming infrequent categories in Extreme Classification tasks by mitigating label inconsistency, leading to considerable performance improvements and introducing new datasets.


<details>
  <summary>Details</summary>
Motivation: Address the poor performance and high label inconsistency associated with infrequent categories in Extreme Classification tasks.

Method: LEVER employs a robust Siamese-style architecture and knowledge transfer to reduce label inconsistency and improve the performance of One-vs-All classifiers.

Result: LEVER delivers significant performance enhancements for infrequent categories, validated through extensive testing on various XC datasets.

Conclusion: LEVER sets a new benchmark for XC tasks involving infrequent categories, while also contributing two new multi-intent datasets for further research.

Abstract: This paper presents a novel solution, LEVER, designed to address the challenges posed by underperforming infrequent categories in Extreme Classification (XC) tasks. Infrequent categories, often characterized by sparse samples, suffer from high label inconsistency, which undermines classification performance. LEVER mitigates this problem by adopting a robust Siamese-style architecture, leveraging knowledge transfer to reduce label inconsistency and enhance the performance of One-vs-All classifiers. Comprehensive testing across multiple XC datasets reveals substantial improvements in the handling of infrequent categories, setting a new benchmark for the field. Additionally, the paper introduces two newly created multi-intent datasets, offering essential resources for future XC research.

</details>


### [27] [Slimmable NAM: Neural Amp Models with adjustable runtime computational cost](https://arxiv.org/abs/2511.07470)
*Steven Atkinson*

Main category: cs.LG

TL;DR: The paper introduces slimmable neural amp models allowing musicians to select trade-offs between accuracy and computational cost without extra training.


<details>
  <summary>Details</summary>
Motivation: Develop a method allowing musicians to customize accuracy and compute levels in amp models on-demand.

Method: Slimmable neural amp models are designed to adjust size and cost dynamically with negligible overhead.

Result: The proposed method outperforms commonly-used baselines and is demonstrated through a real-time audio effect plug-in.

Conclusion: Slimmable neural amp models are practical, flexible, and show superiority in audio processing applications.

Abstract: This work demonstrates "slimmable Neural Amp Models", whose size and computational cost can be changed without additional training and with negligible computational overhead, enabling musicians to easily trade off between the accuracy and compute of the models they are using. The method's performance is quantified against commonly-used baselines, and a real-time demonstration of the model in an audio effect plug-in is developed.

</details>


### [28] [Towards Personalized Quantum Federated Learning for Anomaly Detection](https://arxiv.org/abs/2511.07471)
*Ratun Rahman,Sina Shaham,Dinh C. Nguyen*

Main category: cs.LG

TL;DR: This paper introduces Personalized Quantum Federated Learning (PQFL) to improve anomaly detection by addressing heterogeneity challenges in quantum networks.


<details>
  <summary>Details</summary>
Motivation: Current quantum federated learning faces challenges due to client hardware variability and non-IID data distributions, which reduce the effectiveness of global models for anomaly detection.

Method: The authors propose PQFL, which incorporates parameterized quantum circuits and classical optimizers, adapting each client's model to its specific hardware and data characteristics through a quantum-centric personalization strategy.

Result: PQFL significantly enhances anomaly detection accuracy, reducing false errors by up to 23% and achieving 24.2% AUROC and 20.5% AUPR gains compared to state-of-the-art methods.

Conclusion: PQFL demonstrates improved performance and scalability for anomaly detection in heterogeneous and realistic quantum federated learning settings.

Abstract: Anomaly detection has a significant impact on applications such as video surveillance, medical diagnostics, and industrial monitoring, where anomalies frequently depend on context and anomaly-labeled data are limited. Quantum federated learning (QFL) overcomes these concerns by distributing model training among several quantum clients, consequently eliminating the requirement for centralized quantum storage and processing. However, in real-life quantum networks, clients frequently differ in terms of hardware capabilities, circuit designs, noise levels, and how classical data is encoded or preprocessed into quantum states. These differences create inherent heterogeneity across clients - not just in their data distributions, but also in their quantum processing behaviors. As a result, training a single global model becomes ineffective, especially when clients handle imbalanced or non-identically distributed (non-IID) data. To address this, we propose a new framework called personalized quantum federated learning (PQFL) for anomaly detection. PQFL enhances local model training at quantum clients using parameterized quantum circuits and classical optimizers, while introducing a quantum-centric personalization strategy that adapts each client's model to its own hardware characteristics and data representation. Extensive experiments show that PQFL significantly improves anomaly detection accuracy under diverse and realistic conditions. Compared to state-of-the-art methods, PQFL reduces false errors by up to 23%, and achieves gains of 24.2% in AUROC and 20.5% in AUPR, highlighting its effectiveness and scalability in practical quantum federated settings.

</details>


### [29] [Multivariate Variational Autoencoder](https://arxiv.org/abs/2511.07472)
*Mehmet Can Yavuz*

Main category: cs.LG

TL;DR: The authors propose MVAE, a new VAE variant, improving latent representations and providing better reconstruction and calibration while offering robust unsupervised structure.


<details>
  <summary>Details</summary>
Motivation: Diagonal posterior restrictions in VAEs limit representation capabilities. The motivation is to address this restriction while maintaining Gaussian tractability.

Method: MVAE introduces a Gaussian-preserving VAE with global coupling matrices and per-sample diagonal scales to improve latent factorization.

Result: Experiments across datasets validate that MVAE enhances reconstruction, calibration, and unsupervised learning metrics compared to traditional VAEs.

Conclusion: MVAE improves on classical VAEs in calibration, structure learning, and visual latent factors, facilitating detailed latent representations.

Abstract: We present the Multivariate Variational Autoencoder (MVAE), a VAE variant that preserves Gaussian tractability while lifting the diagonal posterior restriction. MVAE factorizes each posterior covariance, where a \emph{global} coupling matrix $\mathbf{C}$ induces dataset-wide latent correlations and \emph{per-sample} diagonal scales modulate local uncertainty. This yields a full-covariance family with analytic KL and an efficient reparameterization via $\mathbf{L}=\mathbf{C}\mathrm{diag}(\boldsymbolÏƒ)$. Across Larochelle-style MNIST variants, Fashion-MNIST, CIFAR-10, and CIFAR-100, MVAE consistently matches or improves reconstruction (MSE~$\downarrow$) and delivers robust gains in calibration (NLL/Brier/ECE~$\downarrow$) and unsupervised structure (NMI/ARI~$\uparrow$) relative to diagonal-covariance VAEs with matched capacity, especially at mid-range latent sizes. Latent-plane visualizations further indicate smoother, more coherent factor traversals and sharper local detail. We release a fully reproducible implementation with training/evaluation scripts and sweep utilities to facilitate fair comparison and reuse.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [30] [Bi-Objective Evolutionary Optimization for Large-Scale Open Pit Mine Scheduling Problem under Uncertainty with Chance Constraints](https://arxiv.org/abs/2511.08275)
*Ishara Hewa Pathiranage,Aneta Neumann*

Main category: cs.NE

TL;DR: The paper tackles the open-pit mine scheduling problem using a bi-objective approach to improve robustness and balance between economic value and risk.


<details>
  <summary>Details</summary>
Motivation: Traditional deterministic methods for mine scheduling overlook geological uncertainty, leading to suboptimal and infeasible schedules.

Method: Introduces a bi-objective formulation with evolutionary algorithms to maximize economic value and minimize risk, incorporating domain-specific operators.

Result: Demonstrated superior performance of the bi-objective approach compared to confidence-dependent single-objective methods, for deposits with up to 112,687 blocks.

Conclusion: The bi-objective formulation enhances scheduling by accounting for both economic and risk considerations, providing more robust solutions.

Abstract: The open-pit mine scheduling problem (OPMSP) is a complex, computationally expensive process in long-term mine planning, constrained by operational and geological dependencies. Traditional deterministic approaches often ignore geological uncertainty, leading to suboptimal and potentially infeasible production schedules. Chance constraints allow modeling of stochastic components by ensuring probabilistic constraints are satisfied with high probability. This paper presents a bi-objective formulation of the OPMSP that simultaneously maximizes expected net present value and minimizes scheduling risk, independent of the confidence level required for the constraint. Solutions are represented using integer encoding, inherently satisfying reserve constraints. We introduce a domain-specific greedy randomized initialization and a precedence-aware period-swap mutation operator. We integrate these operators into three multi-objective evolutionary algorithms: the global simple evolutionary multi-objective optimizer (GSEMO), a mutation-only variant of multi-objective evolutionary algorithm based on decomposition (MOEA/D), and non-dominated sorting genetic algorithm II (NSGA-II). We compare our bi-objective formulation against the single-objective approach, which depends on a specific confidence level, by analyzing mine deposits consisting of up to 112 687 blocks. Results demonstrate that the proposed bi-objective formulation yields more robust and balanced trade-offs between economic value and risk compared to single-objective, confidence-dependent approach.

</details>


### [31] [Understanding Electro-communication and Electro-sensing in Weakly Electric Fish using Multi-Agent Deep Reinforcement Learning](https://arxiv.org/abs/2511.08436)
*Satpreet H. Singh,Sonja Johnson-Yu,Zhouyang Lu,Aaron Walsman,Federico Pedraja,Denis Turcu,Pratyusha Sharma,Naomi Saphra,Nathaniel B. Sawtell,Kanaka Rajan*

Main category: cs.NE

TL;DR: The study introduces a computational framework using RNNs and MARL to model the electrosensory activities and behaviors of weakly electric fish.


<details>
  <summary>Details</summary>
Motivation: Studying the intricate behaviors and neural activities of weakly electric fish like Gnathonemus petersii is challenging in natural conditions, requiring an alternative approach.

Method: The authors designed artificial agents based on recurrent neural networks, trained through multi-agent reinforcement learning to simulate foraging and electrocommunication in virtual environments.

Result: Emergent behaviors in trained agents mirrored real fish patterns, such as context-dependent EOD intervals and social strategies like freeloading.

Conclusion: This framework aids in understanding weakly electric fish behaviors and can be applied to modeling complex interactions in other animal collectives where traditional methods are difficult.

Abstract: Weakly electric fish, like Gnathonemus petersii, use a remarkable electrical modality for active sensing and communication, but studying their rich electrosensing and electrocommunication behavior and associated neural activity in naturalistic settings remains experimentally challenging. Here, we present a novel biologically-inspired computational framework to study these behaviors, where recurrent neural network (RNN) based artificial agents trained via multi-agent reinforcement learning (MARL) learn to modulate their electric organ discharges (EODs) and movement patterns to collectively forage in virtual environments. Trained agents demonstrate several emergent features consistent with real fish collectives, including heavy tailed EOD interval distributions, environmental context dependent shifts in EOD interval distributions, and social interaction patterns like freeloading, where agents reduce their EOD rates while benefiting from neighboring agents' active sensing. A minimal two-fish assay further isolates the role of electro-communication, showing that access to conspecific EODs and relative dominance jointly shape foraging success. Notably, these behaviors emerge through evolution-inspired rewards for individual fitness and emergent inter-agent interactions, rather than through rewarding agents explicitly for social interactions. Our work has broad implications for the neuroethology of weakly electric fish, as well as other social, communicating animals in which extensive recordings from multiple individuals, and thus traditional data-driven modeling, are infeasible.

</details>


### [32] [Spatio-Temporal Cluster-Triggered Encoding for Spiking Neural Networks](https://arxiv.org/abs/2511.08469)
*Lingyun Ke,Minchi Hu*

Main category: cs.NE

TL;DR: The paper proposes a new encoding method for Spiking Neural Networks (SNNs) to process visual information, improving spatial and temporal consistency while reducing computation costs.


<details>
  <summary>Details</summary>
Motivation: Existing encoding schemes in SNNs often ignore spatial relationships and produce temporally inconsistent patterns, limiting efficiency in processing visual information.

Method: A cluster-based encoding method, utilizing 2D spatial cluster triggers and a 3D spatio-temporal (ST3D) framework, is introduced to preserve semantic structures and improve temporal consistency.

Result: Experiments on the N-MNIST dataset show that the proposed ST3D encoder achieves 98.17% classification accuracy, outperforming standard methods while significantly reducing the number of spikes per sample.

Conclusion: The proposed ST3D encoding strategy is interpretable, efficient, and enhances SNN performance, proving useful for neuromorphic computing applications.

Abstract: Encoding static images into spike trains is a crucial step for enabling Spiking Neural Networks (SNNs) to process visual information efficiently. However, existing schemes such as rate coding, Poisson encoding, and time-to-first-spike (TTFS) often ignore spatial relationships and yield temporally inconsistent spike patterns. In this article, a novel cluster-based encoding approach is proposed, which leverages local density computation to preserve semantic structure in both spatial and temporal domains. This method introduces a 2D spatial cluster trigger that identifies foreground regions through connected component analysis and local density estimation. Then, extend to a 3D spatio-temporal (ST3D) framework that jointly considers temporal neighborhoods, producing spike trains with improved temporal consistency. Experiments on the N-MNIST dataset demonstrate that our ST3D encoder achieves 98.17% classification accuracy with a simple single-layer SNN, outperforming standard TTFS encoding (97.58%) and matching the performance of more complex deep architectures while using significantly fewer spikes (~3800 vs ~5000 per sample). The results demonstrate that this approach provides an interpretable and efficient encoding strategy for neuromorphic computing applications.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [33] [Machine Learning-Guided Memory Optimization for DLRM Inference on Tiered Memory](https://arxiv.org/abs/2511.08568)
*Jie Ren,Bin Ma,Shuangyan Yang,Benjamin Francis,Ehsan K. Ardestani,Min Si,Dong Li*

Main category: cs.PF

TL;DR: RecMG is an ML-guided system designed to improve caching and prefetching in tiered memory architectures for deep learning recommendation models (DLRMs).


<details>
  <summary>Details</summary>
Motivation: DLRMs require large memory capacity, making cost-effective tiered memory architectures vital. Challenges arise in efficiently managing embedding-vector placements.

Method: RecMG uses machine learning models to predict access patterns and optimize embedding-vector placement through advanced caching and prefetching techniques, including a novel differentiable loss function.

Result: RecMG reduces on-demand fetches by up to 2.8x compared to alternatives and improves DLRM inference time by up to 43% in industrial scenarios.

Conclusion: RecMG demonstrates the potential of machine learning-enabled systems for efficient tiered memory management in DLRM applications, significantly boosting performance and cost-effectiveness.

Abstract: Deep learning recommendation models (DLRMs) are widely used in industry, and their memory capacity requirements reach the terabyte scale. Tiered memory architectures provide a cost-effective solution but introduce challenges in embedding-vector placement due to complex embedding-access patterns. We propose RecMG, a machine learning (ML)-guided system for vector caching and prefetching on tiered memory. RecMG accurately predicts accesses to embedding vectors with long reuse distances or few reuses. The design of RecMG focuses on making ML feasible in the context of DLRM inference by addressing unique challenges in data labeling and navigating the search space for embedding-vector placement. By employing separate ML models for caching and prefetching, plus a novel differentiable loss function, RecMG narrows the prefetching search space and minimizes on-demand fetches. Compared to state-of-the-art temporal, spatial, and ML-based prefetchers, RecMG reduces on-demand fetches by 2.2x, 2.8x, and 1.5x, respectively. In industrial-scale DLRM inference scenarios, RecMG effectively reduces end-to-end DLRM inference time by up to 43%.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [34] [Dynamic Stability of LLM-Generated Code](https://arxiv.org/abs/2511.07463)
*Prateek Rajput,Abdoul Aziz Bonkoungou,Yewei Song,Abdoul Kader Kabore,Iyiola E. Olatunji,Jacques Klein,Tegewende Bissyande*

Main category: cs.PL

TL;DR: The paper critiques current evaluation methods of LLMs for code generation, introducing metrics to assess algorithmic and runtime stability and revealing trade-offs between correctness and stability.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods focus only on functional correctness of code generated by LLMs, neglecting algorithmic and runtime diversity, which significantly affect real-world performance.

Method: The authors propose two metrics: Static Canonical Trace Divergence (SCTD) for algorithmic diversity and Dynamic Canonical Trace Divergence (DCTD) for runtime behavioral variance. They also define the Behavioral Expression Factor (BEF) to quantify runtime instability and redundancy.

Result: Experiments with BigOBench and CodeContests reveal that LLMs often generate functionally correct code with significant algorithmic variance. Higher sampling temperatures increase solution correctness but reduce runtime stability.

Conclusion: The work highlights the need for stability-aware evaluation methods and new benchmarks to better assess the real-world applicability and performance of generated code.

Abstract: Current evaluations of LLMs for code generation emphasize functional correctness, overlooking the fact that functionally correct solutions can differ significantly in algorithmic complexity. For instance, an $(O(n^2))$ versus $(O(n \log n))$ sorting algorithm may yield similar output but incur vastly different performance costs in production. This discrepancy reveals a critical limitation in current evaluation methods: they fail to capture the behavioral and performance diversity among correct solutions. To address this, we introduce a principled framework for evaluating the dynamic stability of generated code. We propose two metrics derived from opcode distributions: Static Canonical Trace Divergence (SCTD), which captures algorithmic structure diversity across generated solutions, and Dynamic Canonical Trace Divergence (DCTD), which quantifies runtime behavioral variance. Their ratio, the Behavioral Expression Factor (BEF), serves as a diagnostic signal: it indicates critical runtime instability when BEF $\ll$ 1 and functional redundancy when BEF $\gg$ 1. Empirical results on BigOBench and CodeContests show that state-of-the-art LLMs exhibit significant algorithmic variance even among functionally correct outputs. Notably, increasing sampling temperature improves pass@1 rates but degrades stability, revealing an unrecognized trade-off: searching for correct solutions in diverse output spaces introduces a "penalty of instability" between correctness and behavioral consistency. Our findings call for stability-aware objectives in code generation and new benchmarks with asymptotic test cases for robust, real-world LLM evaluation.

</details>


### [35] [Streaming Tensor Program: A streaming abstraction for dynamic parallelism](https://arxiv.org/abs/2511.07776)
*Gina Sohn,Genghan Zhang,Konstantin Hossfeld,Jungwoo Kim,Nathan Sobotka,Nathan Zhang,Olivia Hsu,Kunle Olukotun*

Main category: cs.PL

TL;DR: This paper introduces Streaming Tensor Program (STeP), a novel abstraction to efficiently handle dynamic tensor workloads on spatial dataflow accelerators, achieving notable performance improvements.


<details>
  <summary>Details</summary>
Motivation: Existing programming abstractions for spatial dataflow accelerators struggle to efficiently manage dynamic behaviors like dynamically shaped tensors and data-dependent control flows.

Method: The authors propose STeP, featuring flexible routing operators, explicit memory hierarchy, and symbolic shape semantics to expose dynamic data patterns and enable optimizations like dynamic tiling, parallelization, and multiplexing.

Result: Using simulations on real-world Large Language Model datasets, STeP demonstrates significant improvements: memory requirement reduced by 2.18x, latency improved by 1.5x, and compute utilization enhanced by 2.57x.

Conclusion: STeP successfully addresses the inefficiencies of prior abstractions, enabling dynamic workloads with robust optimizations, making it highly impactful for spatial dataflow accelerators in dynamic tensor applications.

Abstract: Dynamic behaviors are becoming prevalent in many tensor applications. In machine learning, for example, the input tensors are dynamically shaped or ragged, and data-dependent control flow is widely used in many models. However, the limited expressiveness of prior programming abstractions for spatial dataflow accelerators forces the dynamic behaviors to be implemented statically or lacks the visibility for performance-critical decisions. To address these challenges, we present the Streaming Tensor Program (STeP), a new streaming abstraction that enables dynamic tensor workloads to run efficiently on spatial dataflow accelerators. STeP introduces flexible routing operators, an explicit memory hierarchy, and symbolic shape semantics that expose dynamic data rates and tensor dimensions. These capabilities unlock new optimizations-dynamic tiling, dynamic parallelization, and configuration time-multiplexing-that adapt to dynamic behaviors while preserving dataflow efficiency. Using a cycle-approximate simulator on representative LLM layers with real-world traces, dynamic tiling reduces on-chip memory requirement by 2.18x, dynamic parallelization improves latency by 1.5x, and configuration time-multiplexing improves compute utilization by 2.57x over implementations available in prior abstractions.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [36] [CAVER: Curious Audiovisual Exploring Robot](https://arxiv.org/abs/2511.07619)
*Luca Macesanu,Boueny Folefack,Samik Singh,Ruchira Ray,Ben Abbatematteo,Roberto MartÃ­n-MartÃ­n*

Main category: cs.RO

TL;DR: CAVER is a robot system utilizing audiovisual perception to help robotic manipulation, focusing on enriching audiovisual representations through a novel end-effector, representation method, and exploration algorithm, leading to advancements in material classification and audio imitation tasks.


<details>
  <summary>Details</summary>
Motivation: To enable better robotic manipulation and imitation capabilities by learning correlations between visual appearances and sounds generated by objects.

Method: CAVER integrates a 3D printed end-effector for sound excitation, a representation combining appearance and sound features, and a curiosity-driven exploration algorithm for efficient data collection.

Result: CAVER's auditory-visual representations improve material classification and facilitate imitation of audio-only human demonstrations.

Conclusion: The system outperforms exploration baselines and demonstrates practical benefits in multimodal tasks, making it a valuable tool for robotics.

Abstract: Multimodal audiovisual perception can enable new avenues for robotic manipulation, from better material classification to the imitation of demonstrations for which only audio signals are available (e.g., playing a tune by ear). However, to unlock such multimodal potential, robots need to learn the correlations between an object's visual appearance and the sound it generates when they interact with it. Such an active sensorimotor experience requires new interaction capabilities, representations, and exploration methods to guide the robot in efficiently building increasingly rich audiovisual knowledge. In this work, we present CAVER, a novel robot that builds and utilizes rich audiovisual representations of objects. CAVER includes three novel contributions: 1) a novel 3D printed end-effector, attachable to parallel grippers, that excites objects' audio responses, 2) an audiovisual representation that combines local and global appearance information with sound features, and 3) an exploration algorithm that uses and builds the audiovisual representation in a curiosity-driven manner that prioritizes interacting with high uncertainty objects to obtain good coverage of surprising audio with fewer interactions. We demonstrate that CAVER builds rich representations in different scenarios more efficiently than several exploration baselines, and that the learned audiovisual representation leads to significant improvements in material classification and the imitation of audio-only human demonstrations. https://caver-bot.github.io/

</details>


### [37] [Time-Aware Policy Learning for Adaptive and Punctual Robot Control](https://arxiv.org/abs/2511.07654)
*Yinsen Jia,Boyuan Chen*

Main category: cs.RO

TL;DR: Introducing "time-aware policy learning," a reinforcement learning framework enabling robots to perceive and reason with time as a core variable, optimizing efficiency, robustness, and adaptability across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Robot learning algorithms tend to lack temporal awareness, which is critical for intelligent behavior and decision-making in dynamic environments.

Method: The framework incorporates two temporal signals: remaining time and a time ratio, facilitating adaptive behavior across varied conditions. Policies are optimized jointly for punctuality and stability.

Result: Experimental results show up to 48% improved efficiency, 8x higher robustness in sim-to-real transfer, and 90% reduced acoustic noise, while maintaining high success rates.

Conclusion: Time-aware policy learning enhances robots' autonomy by treating time as a controllable variable, allowing efficient, robust, and human-aligned adaptability in real-world tasks.

Abstract: Temporal awareness underlies intelligent behavior in both animals and humans, guiding how actions are sequenced, paced, and adapted to changing goals and environments. Yet most robot learning algorithms remain blind to time. We introduce time-aware policy learning, a reinforcement learning framework that enables robots to explicitly perceive and reason with time as a first-class variable. The framework augments conventional reinforcement policies with two complementary temporal signals, the remaining time and a time ratio, which allow a single policy to modulate its behavior continuously from rapid and dynamic to cautious and precise execution. By jointly optimizing punctuality and stability, the robot learns to balance efficiency, robustness, resiliency, and punctuality without re-training or reward adjustment. Across diverse manipulation domains from long-horizon pick and place, to granular-media pouring, articulated-object handling, and multi-agent object delivery, the time-aware policy produces adaptive behaviors that outperform standard reinforcement learning baselines by up to 48% in efficiency, 8 times more robust in sim-to-real transfer, and 90% in acoustic quietness while maintaining near-perfect success rates. Explicit temporal reasoning further enables real-time human-in-the-loop control and multi-agent coordination, allowing robots to recover from disturbances, re-synchronize after delays, and align motion tempo with human intent. By treating time not as a constraint but as a controllable dimension of behavior, time-aware policy learning provides a unified foundation for efficient, robust, resilient, and human-aligned robot autonomy.

</details>


### [38] [Testing and Evaluation of Underwater Vehicle Using Hardware-In-The-Loop Simulation with HoloOcean](https://arxiv.org/abs/2511.07687)
*Braden Meyers,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: This paper addresses challenges in testing marine robotics systems by utilizing the HoloOcean 2.0 simulator with improved dynamics and ROS 2 interface for Hardware-in-the-Loop (HIL) and Software-in-the-Loop (SIL) testing of a torpedo AUV.


<details>
  <summary>Details</summary>
Motivation: Testing marine robotics systems in indoor tanks or pools is limited by space and other constraints, making it difficult to evaluate control, navigation, and perception algorithms effectively at scale.

Method: The authors demonstrate the use of the HoloOcean 2.0 simulator for HIL and SIL testing by interfacing it with the CougUV torpedo AUV through a ROS 2 bridge. Simulated sensor data and control commands are exchanged between the simulator and the actual AUV system.

Result: Simulated results from the HoloOcean 2.0 were compared with real-world field trials of the CougUV torpedo AUV, validating the effectiveness of the simulation setup.

Conclusion: The HoloOcean 2.0 simulator proves to be a valuable tool for testing and validating the functionality of underwater AUVs, addressing the constraints of physical indoor testing environments.

Abstract: Testing marine robotics systems in controlled environments before field tests is challenging, especially when acoustic-based sensors and control surfaces only function properly underwater. Deploying robots in indoor tanks and pools often faces space constraints that complicate testing of control, navigation, and perception algorithms at scale. Recent developments of high-fidelity underwater simulation tools have the potential to address these problems. We demonstrate the utility of the recently released HoloOcean 2.0 simulator with improved dynamics for torpedo AUV vehicles and a new ROS 2 interface. We have successfully demonstrated a Hardware-in-the-Loop (HIL) and Software-in-the-Loop (SIL) setup for testing and evaluating a CougUV torpedo autonomous underwater vehicle (AUV) that was built and developed in our lab. With this HIL and SIL setup, simulations are run in HoloOcean using a ROS 2 bridge such that simulated sensor data is sent to the CougUV (mimicking sensor drivers) and control surface commands are sent back to the simulation, where vehicle dynamics and sensor data are calculated. We compare our simulated results to real-world field trial results.

</details>


### [39] [RoboTAG: End-to-end Robot Configuration Estimation via Topological Alignment Graph](https://arxiv.org/abs/2511.07717)
*Yifan Liu,Fangneng Zhan,Wanhua Li,Haowen Sun,Katerina Fragkiadaki,Hanspeter Pfister*

Main category: cs.RO

TL;DR: The paper proposes RoboTAG, a novel framework to estimate robot pose from monocular RGB images, integrating 3D priors while reducing reliance on labels.


<details>
  <summary>Details</summary>
Motivation: To bridge the sim-to-real gap in robot pose estimation from monocular RGB images by leveraging scarce labeled data and incorporating 3D priors.

Method: The RoboTAG framework consists of 2D and 3D branches interconnected in a graph structure where nodes represent states and edges denote dependencies or alignments, applying consistency supervision across branches.

Result: RoboTAG effectively handles robot pose estimation using in-the-wild images without annotations, demonstrating generalizability across robot types.

Conclusion: RoboTAG reduces data bottleneck in robotics by integrating 3D priors and utilizing unlabeled real-world images, offering robust performance across different robots.

Abstract: Estimating robot pose from a monocular RGB image is a challenge in robotics and computer vision. Existing methods typically build networks on top of 2D visual backbones and depend heavily on labeled data for training, which is often scarce in real-world scenarios, causing a sim-to-real gap. Moreover, these approaches reduce the 3D-based problem to 2D domain, neglecting the 3D priors. To address these, we propose Robot Topological Alignment Graph (RoboTAG), which incorporates a 3D branch to inject 3D priors while enabling co-evolution of the 2D and 3D representations, alleviating the reliance on labels. Specifically, the RoboTAG consists of a 3D branch and a 2D branch, where nodes represent the states of the camera and robot system, and edges capture the dependencies between these variables or denote alignments between them. Closed loops are then defined in the graph, on which a consistency supervision across branches can be applied. This design allows us to utilize in-the-wild images as training data without annotations. Experimental results demonstrate that our method is effective across robot types, highlighting its potential to alleviate the data bottleneck in robotics.

</details>


### [40] [A QP Framework for Improving Data Collection: Quantifying Device-Controller Performance in Robot Teleoperation](https://arxiv.org/abs/2511.07720)
*Yuxuan Zhao,Yuanchen Tang,Jindi Zhang,Hongyu Yu*

Main category: cs.RO

TL;DR: This paper develops a teleoperation pipeline for robots, focusing on data quality in manipulation tasks, optimal control methods, and compliance tracking to enhance LLMs for embodied intelligence.


<details>
  <summary>Details</summary>
Motivation: To achieve human-like adaptability in robots and enhance embodied intelligence in LLMs, this study focuses on improving data quality for training foundational models with diverse robot skills.

Method: A teleoperation pipeline was developed, incorporating various teleoperation devices and control strategies such as position-based IK, torque-based ID, and optimization-based compliance control. Optimal QP formulation was employed for compliant pose tracking and singularity avoidance.

Result: The pipeline's experimental analysis demonstrated improved trajectory data quality in tracking error, singularity occurrence, and joint trajectory smoothness across differing device-controller combinations.

Conclusion: The proposed teleoperation pipeline enhances compliant tracking and data quality, benefiting foundational robot skill modeling and embodied intelligence training in LLMs.

Abstract: Robot learning empowers the robot system with human brain-like intelligence to autonomously acquire and adapt skills through experience, enhancing flexibility and adaptability in various environments. Aimed at achieving a similar level of capability in large language models (LLMs) for embodied intelligence, data quality plays a crucial role in training a foundational model with diverse robot skills. In this study, we investigate the collection of data for manipulation tasks using teleoperation devices. Different devices yield varying effects when paired with corresponding controller strategies, including position-based inverse kinematics (IK) control, torque-based inverse dynamics (ID) control, and optimization-based compliance control. In this paper, we develop a teleoperation pipeline that is compatible with different teleoperation devices and manipulator controllers. Within the pipeline, we construct the optimal QP formulation with the dynamic nullspace and the impedance tracking as the novel optimal controller to achieve compliant pose tracking and singularity avoidance. Regarding the optimal controller, it adaptively adjusts the weights assignment depending on the robot joint manipulability that reflects the state of joint configuration for the pose tracking in the form of impedance control and singularity avoidance with nullspace tracking. Analysis of quantitative experimental results suggests the quality of the teleoperated trajectory data, including tracking error, occurrence of singularity, and the smoothness of the joints' trajectory, with different combinations of teleoperation interface and the motion controller.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [41] [A Service Suite for Specifying Digital Twins for Industry 5.0](https://arxiv.org/abs/2511.07506)
*Izaque Esteves,Regina Braga,JosÃ© Maria David,Victor Stroele*

Main category: cs.SE

TL;DR: This paper presents DT-Create, a suite for creating Digital Twins to enhance predictive maintenance by leveraging intelligent and real-time data processing.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of making agile and assertive decisions in predictive maintenance, leveraging connected sensors and operational data.

Method: A suite called DT-Create was developed using the Design Science Research methodology through two development cycles and evaluated with case studies.

Result: DT-Create was found feasible in its ability to process data from sensors, enrich information via machine learning and ontologies, apply predictive modeling, and provide decision support alongside self-adaptation capabilities.

Conclusion: The DT-Create suite enriches predictive maintenance by employing intelligent processing and self-adaptive techniques, enhancing decision-making and model prediction capabilities.

Abstract: One of the challenges of predictive maintenance is making decisions based on data in an agile and assertive way. Connected sensors and operational data favor intelligent processing techniques to enrich information and enable decision-making. Digital Twins (DTs) can be used to process information and support decision-making. DTs are a real-time representation of physical machines and generate data that predictive maintenance can use to make assertive and quick decisions. The main contribution of this work is the specification of a suite of services for specifying DTs, called DT-Create, focused on decision support in predictive maintenance. DT-Create suite is based on intelligent techniques, semantic data processing, and self-adaptation. This suite was developed using the Design Science Research (DSR) methodology through two development cycles and evaluated through case studies. The results demonstrate the feasibility of using DT-Create in specifying DTs considering the following aspects: (i) collection, storage, and intelligent processing of data generated by sensors, (ii) enrichment of information through machine learning and ontologies, (iii) use of intelligent techniques to select predictive models that adhere to the available data set, and (iv) decision support and self-adaptation.

</details>


### [42] [SemanticForge: Repository-Level Code Generation through Semantic Knowledge Graphs and Constraint Satisfaction](https://arxiv.org/abs/2511.07584)
*Wuyang Zhang,Chenkai Zhang,Zhen Luo,Jianming Ma,Wangming Yuan,Chuqiao Gu,Chenwei Feng*

Main category: cs.SE

TL;DR: SemanticForge enhances the accuracy of code generation in LLMs by integrating semantically-aware approaches and addressing hallucination errors.


<details>
  <summary>Details</summary>
Motivation: Existing large language models frequently generate errors in software development tasks due to the lack of mechanisms to handle repository-wide semantics.

Method: SemanticForge introduces four algorithmic advances: reconciliation of knowledge graphs, neural-based query generation, beam search with SMT solving, and efficient incremental graph updates.

Result: It achieves significant improvements in precision for structured graph queries (73% vs. 51%) and enables real-time constraint verification for code generation.

Conclusion: A semantically-aware system like SemanticForge can mitigate systematic errors, enhancing the reliability and deployment of LLM-based code generation tools.

Abstract: Large language models (LLMs) have transformed software development by enabling automated code generation, yet they frequently suffer from systematic errors that limit practical deployment. We identify two critical failure modes: \textit{logical hallucination} (incorrect control/data-flow reasoning) and \textit{schematic hallucination} (type mismatches, signature violations, and architectural inconsistencies). These errors stem from the absence of explicit, queryable representations of repository-wide semantics.
  This paper presents \textbf{SemanticForge}, which introduces four fundamental algorithmic advances for semantically-aware code generation: (1) a novel automatic reconciliation algorithm for dual static-dynamic knowledge graphs, unifying compile-time and runtime program semantics; (2) a neural approach that learns to generate structured graph queries from natural language, achieving 73\% precision versus 51\% for traditional retrieval; (3) a novel beam search algorithm with integrated SMT solving, enabling real-time constraint verification during generation rather than post-hoc validation; and (4) an incremental maintenance algorithm that updates knowledge graphs in $O(|Î”R| \cdot \log n)$ time while maintaining semantic equivalence.

</details>


### [43] [An Exploratory Eye Tracking Study on How Developers Classify and Debug Python Code in Different Paradigms](https://arxiv.org/abs/2511.07612)
*Samuel W. Flint,Jigyasa Chauhan,Niloofar Mansoor,Bonita Sharif,Robert Dyer*

Main category: cs.SE

TL;DR: The study explores how paradigm-specific features in Python impact code comprehension and debugging using an eye-tracking study with 29 participants.


<details>
  <summary>Details</summary>
Motivation: To investigate how features of various programming paradigms in modern languages like Python influence developers' comprehension and debugging skills.

Method: An eye-tracking study with 29 participants performing classification and debugging tasks on Python code rooted in Object-Oriented, Functional, and Procedural paradigms.

Result: Participants struggled to classify Functional and Procedural paradigms, and required more time for Functional code. Debugging success was not affected by paradigm, but Functional code caused lower confidence and different reading patterns.

Conclusion: Certain language features and paradigms (especially Functional) influence cognitive processes like classification, confidence, and reading approaches, requiring deeper research for improving coding practices and tools.

Abstract: Modern programming languages, such as Python, support language features from several paradigms, such as object-oriented, procedural, and functional. Research has shown that code written in some paradigms can be harder to comprehend, but to date, no research has looked at which paradigm-specific language features impact comprehension. To this end, this study seeks to uncover which paradigm-specific features impactcomprehension and debugging of code or how multi-paradigm code might affect a developer's ability to do so. We present an exploratory empirical eye-tracking study to investigate 1) how developers classify the predominant paradigm in Python code and 2) how the paradigm affects their ability to debug Python code. The goal is to uncover if specific language features are looked at more often while classifying and debugging code with a predominant paradigm. Twenty-nine developers (primarily students) were recruited for the study and were each given four classification and four debugging tasks in Python. Eye movements were recorded during all the tasks. The results indicate confusion in labeling Functional and Procedural paradigms, but not Object-Oriented. The code with predominantly functional paradigms also took the longest to complete. Changing the predominant paradigm did not affect the ability to debug the code, though developers did rate themselves with lower confidence for Functional code. We report significant differences in reading patterns during debugging, especially in the Functional code. During classification, results show that developers do not necessarily read paradigm-relevant token types.

</details>


### [44] [A Self-Improving Architecture for Dynamic Safety in Large Language Models](https://arxiv.org/abs/2511.07645)
*Tyler Slater*

Main category: cs.SE

TL;DR: The paper presents the Self-Improving Safety Framework (SISF), a dynamic architecture that enhances runtime AI safety by enabling autonomous policy adaptation based on detected breaches.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of static architecture patterns and unscalable safety assurances, leaving AI systems vulnerable to novel adversarial threats.

Method: The researchers developed the Self-Improving Safety Framework (SISF), combining a base LLM with a dynamic feedback loop involving AI adjudicators for breach detection and modules for policy synthesis.

Result: The SISF reduced the Attack Success Rate (ASR) from 100% to 45.58% after detecting breaches and synthesizing new policies. It achieved a 0.00% False Positive Rate (FPR) on benign prompts, showcasing effective and adaptive performance.

Conclusion: A self-adaptive architectural approach for AI safety can create more robust and scalable systems, transforming safety assurance into an automated, runtime process.

Abstract: Context: The integration of Large Language Models (LLMs) into core software systems is accelerating. However, existing software architecture patterns are static, while current safety assurance methods are not scalable, leaving systems vulnerable to novel adversarial threats.
  Objective: To design, implement, and evaluate a novel software architecture that enables an AI-driven system to autonomously and continuously adapt its own safety protocols at runtime.
  Method: We propose the Self-Improving Safety Framework (SISF), a runtime architecture that couples an unprotected, unaligned base LLM (mistralai/Mistral-7B-v0.1) with a dynamic feedback loop. This loop consists of an AI Adjudicator (GPT-4o) for breach detection and a Policy Synthesis Module (GPT-4 Turbo) that autonomously generates new, generalized safety policies (both heuristic and semantic) in response to failures.
  Results: We conducted a dynamic learning evaluation using the 520-prompt AdvBench dataset. The unprotected model was 100% vulnerable. Our SISF, starting from zero policies, demonstrated a clear learning curve: it detected 237 breaches, autonomously synthesized 234 new policies, and reduced the overall Attack Success Rate (ASR) to 45.58%. In a subsequent test on 520 benign prompts, the SISF achieved a 0.00% False Positive Rate (FPR), proving its ability to adapt without compromising user utility.
  Conclusion: An architectural approach to AI safety, based on the principles of self-adaptation, is a viable and effective strategy. Our framework demonstrates a practical path towards building more robust, resilient, and scalable AI-driven systems, shifting safety assurance from a static, pre-deployment activity to an automated, runtime process.

</details>


### [45] [Can Large Language Models Simulate Symbolic Execution Output Like KLEE?](https://arxiv.org/abs/2511.08530)
*Rong Feng,Vanisha Gupta,Vivek Patel,Viroopaksh Reddy Ernampati,Suman Saha*

Main category: cs.SE

TL;DR: The paper explores using GPT-4o to simulate KLEE's symbolic execution outputs to address KLEE's inefficiency with complex programs. Initial results show 20% accuracy, indicating LLMs' current limitations in this area.


<details>
  <summary>Details</summary>
Motivation: The research aims to address KLEE's inefficiency in handling programs with many branching paths by exploring whether large language models like GPT-4o can simulate KLEE's outputs, potentially saving computational resources.

Method: The study tested GPT-4o's ability to predict KLEE's outputs and identify the most constrained execution paths by using a dataset of 100 C programs. These paths are crucial as they often have deep bugs, but determining them generally requires resource-intensive operations by KLEE.

Result: GPT-4o achieved 20% accuracy in mimicking KLEE's outputs and identifying the most constrained paths in the tested dataset.

Conclusion: While the results aren't highly accurate, the research provides insights into the current capabilities and limitations of large language models in simulating aspects of symbolic execution, paving the way for future advancements.

Abstract: Symbolic execution helps check programs by exploring different paths based on symbolic inputs. Tools like KLEE are commonly used because they can automatically detect bugs and create test cases. But one of KLEE's biggest issues is how slow it can get when programs have lots of branching paths-it often becomes too resource-heavy to run on large or complex code. In this project, we wanted to see if a large language model like GPT-4o could simulate the kinds of outputs that KLEE generates. The idea was to explore whether LLMs could one day replace parts of symbolic execution to save time and resources.
  One specific goal was to have GPT-4o identify the most constrained path in a program, this is the execution path with the most symbolic conditions. These paths are especially important because they often represent edge cases that are harder to test and more likely to contain deep bugs. However, figuring this out usually requires fully running KLEE, which can be expensive. So, we tested whether GPT-4o could predict the KLEE outputs and the most complex path using a dataset of 100 C programs. Our results showed about 20% accuracy in generating KLEE-like outputs and identifying the most constrained path. While not highly accurate, this early work helps show what current LLMs can and can't do when it comes to simulating symbolic execution.

</details>


### [46] [Smart but Costly? Benchmarking LLMs on Functional Accuracy and Energy Efficiency](https://arxiv.org/abs/2511.07698)
*Mohammadjavad Mehditabar,Saurabhsingh Rajput,Antonio Mastropaolo,Tushar Sharma*

Main category: cs.SE

TL;DR: The paper introduces BRACE, a framework to evaluate Code Language Models (CLMs) based on both energy efficiency and accuracy. Two rating methods are proposed to perform this evaluation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to systematically assess the environmental impact and accuracy of AI models for software development tasks, addressing the shortcomings of existing approaches for evaluating Code Language Models.

Method: The authors propose the BRACE framework, which benchmarks CLMs for energy efficiency and accuracy using two distinct evaluation methods: CIRC (deterministic, robust rankings) and OTER (trend-aware, dynamic trade-offs).

Result: The study benchmarks 22 models, showing that summarization tasks are less constrained than code generation tasks, and model size does not heavily impact ratings if parameters are efficiently utilized.

Conclusion: BRACE provides a useful tool for assessing CLMs' performance in terms of sustainability and correctness, enabling informed model selection based on deployment priorities.

Abstract: The rapid advancement of AI technologies and their accelerated adoption in software development necessitates a systematic evaluation of their environmental impact alongside functional correctness. While prior studies have examined sustainability in large language models, existing approaches lack systematic frameworks for evaluating accuracy-energy trade-offs in Code Language Models (CLMs). In this paper, we present a framework, BRACE, to benchmark CLMs on a unified scale of energy efficiency and functional correctness (referred to as accuracy). We benchmark 22 state-of-the-art models on code generation and summarization tasks, proposing two rating methods: Concentric Incremental Rating Circles (CIRC) and Observation to Expectation Rating (OTER). CIRC provides deterministic Euclidean-based rankings with static trade-offs that are robust to outliers, and OTER offers trend-aware evaluation with dynamic trade-offs that capture the complex correlation between energy and accuracy, each offering a distinct perspective and addressing the problem in a unique way. These rating methods enable us to rate LLMs on a 1-5 scale reflecting their combined capabilities in terms of energy efficiency and functional correctness. Our analysis reveals models generally perform better in the code summarization tasks as they are not enforced to generate a grammar-based and syntactically correct output. Also, we find that models' size does not have a significant impact on their ratings, indicating that if models utilize their parameters efficiently, they can be ranked higher on these scales. The proposed BRACE framework empowers practitioners to make evidence-based model selections that balance sustainability with task requirements, guiding rating choice -- CIRC for deterministic comparisons or OTER for trend-aware evaluation -- based on deployment priorities.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [47] [Inferring Hidden Motives: Bayesian Models of Preference Learning in Repeated Dictator Games](https://arxiv.org/abs/2511.07825)
*Gregory Stanley,Jun Zhang,Rick Lewis*

Main category: q-bio.NC

TL;DR: The study introduces a Bayesian model to understand human cooperation and social preference learning in binary dictator games.


<details>
  <summary>Details</summary>
Motivation: The paper aims to clarify how humans infer others' social motives and learn whom to trust for promoting cooperation.

Method: The study employs Bayesian belief updating over continuous social preference parameters, testing 476 utility functions in dictator games using The Morality Game platform.

Result: The Utility Bayesian Model (UBM) outperforms other models, revealing moderate altruism, strong inequality aversion, and variability in moral phenotypes.

Conclusion: The findings provide a computational framework and insights on social motives, aiding cooperation in both human and artificial systems.

Abstract: Human cooperation depends on how accurately we infer others' motives--how much they value fairness, generosity, or self-interest from the choices they make. We model that process in binary dictator games, which isolate moral trade-offs between self and other stripped of strategic complexity. Participants observed others' allocation decisions and predicted their future behavior while playing through an exhaustive, randomized payoff space implemented on The Morality Game platform. We formalize social-preference learning as Bayesian belief updating over continuous parameters such as self-interest, altruism, envy, and guilt. The resulting Utility Bayesian Model (UBM) outperformed non-Bayesian alternatives and Bayesian models that categorize others into discrete social types. Because Bayesian updating requires a utility function in its likelihood term, we conducted the largest utility-function comparison to date--476 candidate forms differing in psychologically meaningful properties (e.g., payoff exponents, reference dependence, payoff ratios, and envy-guilt asymmetries). Exploring this joint space of payoffs and models allowed us to identify the function that unifies prior theories and generalizes across payoff conditions. Parameter estimation revealed moderate altruism, strong inequality aversion, and nonlinear payoff valuation (exponent > 1). Altruism and social-comparison motives were largely independent, revealing diverse moral phenotypes from cooperative to competitive or sadistic. Together, these findings provide a computational framework and a map of social motives, clarifying how humans learn whom to trust and offering quantitative foundations for promoting cooperation in social and artificial systems.

</details>


### [48] [Advancing credibility and transparency in brain-to-image reconstruction research: Reanalysis of Koide-Majima, Nishimoto, and Majima (Neural Networks, 2024)](https://arxiv.org/abs/2511.07960)
*Ken Shirakawa,Yoshihiro Nagano,Misato Tanaka,Fan L. Cheng,Yukiyasu Kamitani*

Main category: q-bio.NC

TL;DR: This paper critically evaluates the claims of Koide-Majima et al. (2024), highlighting methodological flaws that challenge the validity of their generative AI approach to reconstructing visual imagery from brain activity.


<details>
  <summary>Details</summary>
Motivation: To reanalyze and validate the methodology and conclusions presented by Koide-Majima et al. (2024), given their high-profile claims of advancements in brain activity-based visual reconstruction.

Method: The authors conducted an independent reanalysis of the methods and results from Koide-Majima et al.'s study, using fair baseline comparisons and scrutinizing the statistical and functional components of the approach.

Result: The reanalysis identified several issues: biased result reporting, inflated performance metrics, lack of advantage over existing techniques, redundancy in the central Bayesian component, and unsubstantiated claims of Bayesian novelty.

Conclusion: The study calls for a reassessment of the contributions claimed by Koide-Majima et al., emphasizing the importance of methodological rigor and transparency in brain decoding research.

Abstract: A recent high-profile study by Koide-Majima et al. (2024) claimed a major advance in reconstructing visual imagery from brain activity using a novel variant of a generative AI-based method. However, our independent reanalysis reveals multiple methodological concerns that raise questions about the validity of their conclusions. Specifically, our evaluation demonstrates that: (1) the reconstruction results are biased by selective reporting of only the best-performing examples at multiple levels; (2) performance is artificially inflated by circular metrics that fail to reflect perceptual accuracy; (3) fair baseline comparisons reveal no discernible advantages of the study's key innovations over existing techniques; (4) the central "Bayesian" sampling component is functionally inert, producing outcomes identical to the standard optimization result; and (5) even if the component were successfully implemented, the claims of Bayesian novelty are unsubstantiated, as the proposed method does not leverage the principles of a proper Bayesian framework. These systemic issues necessitate a critical reassessment of the study's contributions. This commentary dissects these deficiencies to underscore the need for greater credibility and transparency in the rapidly advancing field of brain decoding.

</details>


### [49] [Direction and speed selectivity properties for spatio-temporal receptive fields according to the generalized Gaussian derivative model for visual receptive fields](https://arxiv.org/abs/2511.08101)
*Tony Lindeberg*

Main category: q-bio.NC

TL;DR: This paper presents theoretical analysis on the direction and speed selectivity of visual neurons, modeling receptive fields as velocity-adapted Gaussian derivatives, and comparing outcomes with neurophysiological measurements.


<details>
  <summary>Details</summary>
Motivation: To understand and model the direction and speed selectivity properties of visual neurons in the primary visual cortex, using idealized models based on Gaussian derivatives.

Method: Receptive fields were modeled as velocity-adapted affine Gaussian derivatives and tested using moving sine waves with different angular frequencies and image velocities. The results were compared to neurophysiological measurements.

Result: The model outputs were found consistent with neurophysiological observations: velocity-tuned neurons sensitive to motion directions/speeds, and variability in degree of selectivity across different neurons.

Conclusion: The findings support a hypothesis that simple cells in the primary visual cortex are covariant under Galilean transformations, aiding in visual motion processing across different speeds and directions.

Abstract: This paper gives an in-depth theoretical analysis of the direction and speed selectivity properties of idealized models of the spatio-temporal receptive fields of simple cells and complex cells, based on the generalized Gaussian derivative model for visual receptive fields. According to this theory, the receptive fields are modelled as velocity-adapted affine Gaussian derivatives for different image velocities and different degrees of elongation. By probing such idealized receptive field models of visual neurons to moving sine waves with different angular frequencies and image velocities, we characterize the computational models to a structurally similar probing method as is used for characterizing the direction and speed selective properties of biological neurons.
  By comparison to results of neurophysiological measurements of direction and speed selectivity for biological neurons in the primary visual cortex, we find that our theoretical results are qualitatively consistent with (i) velocity-tuned visual neurons that are sensitive to particular motion directions and speeds, and (ii)~different visual neurons having broader {\em vs.\/}\ sharper direction and speed selective properties. Our theoretical results in combination with results from neurophysiological characterizations of motion-sensitive visual neurons are also consistent with a previously formulated hypothesis that the simple cells in the primary visual cortex ought to be covariant under local Galilean transformations, so as to enable processing of visual stimuli with different motion directions and speeds.

</details>


### [50] [Distance by de-correlation: Computing distance with heterogeneous grid cells](https://arxiv.org/abs/2511.08292)
*Pritipriya Dasbehera,Akshunna S. Dogra,William T. Redman*

Main category: q-bio.NC

TL;DR: The paper proposes a simpler mechanism for distance encoding by grid cells through the de-correlation of population activity, highlighting the role of heterogeneity in grid cell properties.


<details>
  <summary>Details</summary>
Motivation: Understanding how grid cells encode spatial distance efficiently without relying on complex coding mechanisms or previously assumed anatomical constraints.

Method: Development of a mathematical theory describing de-correlation in one-dimension, simulations of noisy grid cells in one and two dimensions, and comparisons with rodent behavioral data.

Result: The study predicts a 'sweet spot' where certain farther distances are better encoded than nearer ones, confirms this prediction in rodent behavior, and identifies a trade-off between distance encoding range and distinguishability, influenced by variability in grid cell properties.

Conclusion: The findings suggest grid cells encode distances using de-correlation, leveraging small heterogeneity in grid properties for efficient spatial navigation.

Abstract: Encoding the distance between locations in space is essential for accurate navigation. Grid cells, a functional class of neurons in medial entorhinal cortex, are believed to support this computation. However, existing theories of how populations of grid cells code distance rely on complex coding schemes, with assumptions that may not be met by anatomical constraints. Inspired by recent work finding grid cells to have small, but robust heterogeneity in their grid properties, we hypothesize that distance coding can be achieved by a simple de-correlation of population activity. We develop a mathematical theory for describing this de-correlation in one-dimension, showing that its predictions are consistent with simulations of noisy grid cells. Our simulations highlight a non-intuitive prediction of such a distance by de-correlation framework. Namely, that some further distances are better encoded than some nearer distances. We find evidence of this "sweet spot" in previously published rodent behavioral experiments and demonstrate that a decoder which estimates distance from the de-correlation of populations of simulated noisy grid cells leads to a similar pattern of errors. Finally, by simulating noisy grid cells in two-dimensions, we find that there exists a trade-off between the range of distances that can be encoded by de-correlation of population activity and the distinguishability of different distances, which is controlled by the amount of variability in grid properties. We show that the previously observed average amount of grid property variability strikes a balance between the two, enabling the encoding of distances up to several meters. Our work provides new insight on how grid cells can underlie the coding of distance, without the assumptions previously needed, and why grid cells may have small amounts of heterogeneity in their grid properties.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [51] [Tractable Instances of Bilinear Maximization: Implementing LinUCB on Ellipsoids](https://arxiv.org/abs/2511.07504)
*Raymond Zhang,HÃ©di Hadiji,Richard Combes*

Main category: stat.ML

TL;DR: The paper focuses on solving a maximization problem of $x^\top Î¸$ over convex sets and ellipsoids, a task central to linear bandits. It proves inefficiency for certain sets and introduces efficient algorithms for specific cases.


<details>
  <summary>Details</summary>
Motivation: Linear bandit optimization requires efficient algorithms to maximize $x^\top Î¸$ at every step as part of optimistic strategies, particularly in high-dimensional setups.

Method: The study analyzes efficiency in solving the problem for various convex sets, proving NP-hardness for some cases. It introduces two novel algorithms for centered ellipsoidal sets.

Result: The algorithms provide efficient solutions for the problem in high-dimensional cases with centered ellipsoidal sets.

Conclusion: The paper establishes the conditions under which the maximization problem is efficient and provides the first efficient method for linear bandit use in high dimensions.

Abstract: We consider the maximization of $x^\top Î¸$ over $(x,Î¸) \in \mathcal{X} \times Î˜$, with $\mathcal{X} \subset \mathbb{R}^d$ convex and $Î˜\subset \mathbb{R}^d$ an ellipsoid. This problem is fundamental in linear bandits, as the learner must solve it at every time step using optimistic algorithms. We first show that for some sets $\mathcal{X}$ e.g. $\ell_p$ balls with $p>2$, no efficient algorithms exist unless $\mathcal{P} = \mathcal{NP}$. We then provide two novel algorithms solving this problem efficiently when $\mathcal{X}$ is a centered ellipsoid. Our findings provide the first known method to implement optimistic algorithms for linear bandits in high dimensions.

</details>


### [52] [Infinite-Dimensional Operator/Block Kaczmarz Algorithms: Regret Bounds and $Î»$-Effectiveness](https://arxiv.org/abs/2511.07604)
*Halyun Jeong,Palle E. T. Jorgensen,Hyun-Kyoung Kwon,Myung-Sin Song*

Main category: stat.ML

TL;DR: The paper studies projection-based linear regression focusing on the relaxation parameter's role and deriving regret bounds for Kaczmarz algorithms, with applications to machine learning, noisy data, and infinite-dimensional spaces.


<details>
  <summary>Details</summary>
Motivation: Understanding how relaxation parameters influence algorithmic performance in Kaczmarz methods and their applications in modern machine-learning scenarios.

Method: Analyzing Kaczmarz algorithms with relaxation parameters and establishing explicit regret bounds related to their performance deviation. Extending the analysis to infinite-dimensional spaces with bounded operators.

Result: Presented results show explicit regret bounds for Kaczmarz algorithms and their applications to noisy data and modern machine learning models.

Conclusion: The study provides a comprehensive analysis of relaxation parameters, regret bounds, and introduces new insights into Kaczmarz algorithm frameworks across diverse applications.

Abstract: We present a variety of projection-based linear regression algorithms with a focus on modern machine-learning models and their algorithmic performance. We study the role of the relaxation parameter in generalized Kaczmarz algorithms and establish a priori regret bounds with explicit $Î»$-dependence to quantify how much an algorithm's performance deviates from its optimal performance. A detailed analysis of relaxation parameter is also provided. Applications include: explicit regret bounds for the framework of Kaczmarz algorithm models, non-orthogonal Fourier expansions, and the use of regret estimates in modern machine learning models, including for noisy data, i.e., regret bounds for the noisy Kaczmarz algorithms. Motivated by machine-learning practice, our wider framework treats bounded operators (on infinite-dimensional Hilbert spaces), with updates realized as (block) Kaczmarz algorithms, leading to new and versatile results.

</details>


### [53] [Robust Experimental Design via Generalised Bayesian Inference](https://arxiv.org/abs/2511.07671)
*Yasir Zubayr Barlas,Sabina J. Sloman,Samuel Kaski*

Main category: stat.ML

TL;DR: The paper introduces Generalised Bayesian Optimal Experimental Design (GBOED), combining robust Gibbs inference with experimental design to address limitations in standard Bayesian methods.


<details>
  <summary>Details</summary>
Motivation: To provide a more robust framework for experimental design when statistical models are misspecified or outliers are present.

Method: Generalised Bayesian inference is extended to experimental design using a new information-theoretic acquisition function called Gibbs expected information gain (Gibbs EIG).

Result: The proposed GBOED method improves robustness to model misspecifications and outliers, outperforming traditional Bayesian approaches.

Conclusion: GBOED demonstrates the potential of extending Gibbs inference to experimental design, offering enhanced robustness in both experimental designs and subsequent inferences.

Abstract: Bayesian optimal experimental design is a principled framework for conducting experiments that leverages Bayesian inference to quantify how much information one can expect to gain from selecting a certain design. However, accurate Bayesian inference relies on the assumption that one's statistical model of the data-generating process is correctly specified. If this assumption is violated, Bayesian methods can lead to poor inference and estimates of information gain. Generalised Bayesian (or Gibbs) inference is a more robust probabilistic inference framework that replaces the likelihood in the Bayesian update by a suitable loss function. In this work, we present Generalised Bayesian Optimal Experimental Design (GBOED), an extension of Gibbs inference to the experimental design setting which achieves robustness in both design and inference. Using an extended information-theoretic framework, we derive a new acquisition function, the Gibbs expected information gain (Gibbs EIG). Our empirical results demonstrate that GBOED enhances robustness to outliers and incorrect assumptions about the outcome noise distribution.

</details>


### [54] [Distributionally Robust Online Markov Game with Linear Function Approximation](https://arxiv.org/abs/2511.07831)
*Zewu Zheng,Yuanyuan Lin*

Main category: stat.ML

TL;DR: The paper addresses the sim-to-real gap in reinforcement learning by proposing a sample-efficient algorithm, DR-CCE-LSI, to find robust equilibriums in multi-agent settings.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the fundamental challenge in reinforcement learning (sim-to-real gap) where there is performance degradation due to an environment shift when transitioning from simulated training to real-world applications.

Method: The method involves developing the DR-CCE-LSI algorithm, which incorporates a minimum value assumption, exploration bonuses, and least square value iteration type approaches to achieve robust coarse correlated equilibrium, considering multi-agent settings and leveraging feature mapping properties.

Result: The paper achieves a sample-efficient learning approach with a regret bound of O{dHmin{H,1/min{Ïƒ_i}}âˆšK}, marking it as the first sample-efficient algorithm for this setting. It matches the best results in single-agent cases while achieving minimax optimal sample complexity in terms of feature dimensions.

Conclusion: This work introduces a novel algorithm that effectively addresses the sim-to-real gap in multi-agent reinforcement learning settings, paving the way for robust and sample-efficient solutions.

Abstract: The sim-to-real gap, where agents trained in a simulator face significant performance degradation during testing, is a fundamental challenge in reinforcement learning. Extansive works adopt the framework of distributionally robust RL, to learn a policy that acts robustly under worst case environment shift. Within this framework, our objective is to devise algorithms that are sample efficient with interactive data collection and large state spaces. By assuming d-rectangularity of environment dynamic shift, we identify a fundamental hardness result for learning in online Markov game, and address it by adopting minimum value assumption. Then, a novel least square value iteration type algorithm, DR-CCE-LSI, with exploration bonus devised specifically for multiple agents, is proposed to find an \episilon-approximate robust Coarse Correlated Equilibrium(CCE). To obtain sample efficient learning, we find that: when the feature mapping function satisfies certain properties, our algorithm, DR-CCE-LSI, is able to achieve Îµ-approximate CCE with a regret bound of O{dHmin{H,1/min{Ïƒ_i}}\sqrt{K}}, where K is the number of interacting episodes, H is the horizon length, d is the feature dimension, and \simga_i represents the uncertainty level of player i. Our work introduces the first sample-efficient algorithm for this setting, matches the best result so far in single agent setting, and achieves minimax optimalsample complexity in terms of the feature dimension d. Meanwhile, we also conduct simulation study to validate the efficacy of our algorithm in learning a robust equilibrium.

</details>


### [55] [PrAda-GAN: A Private Adaptive Generative Adversarial Network with Bayes Network Structure](https://arxiv.org/abs/2511.07997)
*Ke Jia,Yuheng Ma,Yang Li,Feifei Wang*

Main category: stat.ML

TL;DR: This paper introduces PrAda-GAN, a novel method combining GAN-based and marginal-based approaches to generate synthetic data under differential privacy, showing improved privacy-utility trade-off.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address key limitations in marginal-based synthetic data generation methods and leverage the strengths of GAN-based models, ensuring better adherence to differential privacy constraints.

Method: The proposed method, PrAda-GAN, uses a sequential generator architecture to capture variable dependencies and promotes sparsity in the Bayes network. It establishes theoretical bounds related to parameter distances and error rates for optimal differential privacy.

Result: PrAda-GAN significantly improves convergence rates by leveraging dependency sparsity and demonstrates superior privacy-utility balance when tested on synthetic and real-world datasets.

Conclusion: The integration of GAN-based and marginal-based approaches in PrAda-GAN leads to more effective synthetic data generation, improving both theoretical performance and empirical outcomes under differential privacy.

Abstract: We revisit the problem of generating synthetic data under differential privacy. To address the core limitations of marginal-based methods, we propose the Private Adaptive Generative Adversarial Network with Bayes Network Structure (PrAda-GAN), which integrates the strengths of both GAN-based and marginal-based approaches. Our method adopts a sequential generator architecture to capture complex dependencies among variables, while adaptively regularizing the learned structure to promote sparsity in the underlying Bayes network. Theoretically, we establish diminishing bounds on the parameter distance, variable selection error, and Wasserstein distance. Our analysis shows that leveraging dependency sparsity leads to significant improvements in convergence rates. Empirically, experiments on both synthetic and real-world datasets demonstrate that PrAda-GAN outperforms existing tabular data synthesis methods in terms of the privacy-utility trade-off.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [56] [Blockly2Hooks: Smart Contracts for Everyone with the XRP Ledger and Google Blockly](https://arxiv.org/abs/2511.08403)
*Lucian Trestioreanu,Wazen Shbair,Flaviene Scheidt de Cristo,Radu State*

Main category: cs.CR

TL;DR: This paper presents Blockly2Hooks, a platform designed to make smart contract development accessible for non-expert users using visual programming techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in smart contract adoption, particularly usability and creating tools for non-experts amid rising demand for knowledgeable users and accessible resources.

Method: The paper uses the Blockly Visual Programming library to simplify smart contract creation. It employs Visual Programming Languages to lower the learning curve for building contracts on the XRP Ledger.

Result: Blockly2Hooks was developed and tested, showing promising results in facilitating smoother learning of smart contract technology.

Conclusion: Blockly2Hooks fills a usability gap in smart contract development, enabling non-experts to adopt this technology more easily by leveraging accessible teaching methodologies.

Abstract: Recent technologies such as inter-ledger payments, non-fungible tokens, and smart contracts are all fruited from the ongoing development of Distributed Ledger Technologies. The foreseen trend is that they will play an increasingly visible role in daily life, which will have to be backed by appropriate operational resources. For example, due to increasing demand, smart contracts could soon face a shortage of knowledgeable users and tools to handle them in practice. Widespread smart contract adoption is currently limited by security, usability and costs aspects. Because of a steep learning curve, the handling of smart contracts is currently performed by specialised developers mainly, and most of the research effort is focusing on smart contract security, while other aspects like usability being somewhat neglected. Specific tools would lower the entry barrier, enabling interested non-experts to create smart contracts.
  In this paper we designed, developed and tested Blockly2Hooks, a solution towards filling this gap even in challenging scenarios such as when the smart contracts are written in an advanced language like C. With the XRP Ledger as a concrete working case, Blockly2Hooks helps interested non-experts from the community to learn smart contracts easily and adopt the technology, through leveraging well-proven teaching methodologies like Visual Programming Languages, and more specifically, the Blockly Visual Programming library from Google. The platform was developed and tested and the results are promising to make learning smart contract development smoother.

</details>


### [57] [QLCoder: A Query Synthesizer For Static Analysis of Security Vulnerabilities](https://arxiv.org/abs/2511.08462)
*Claire Wang,Ziyang Li,Saikat Dutta,Mayur Naik*

Main category: cs.CR

TL;DR: The paper introduces QLCoder, a framework to automatically synthesize CodeQL queries for detecting security vulnerabilities using CVE metadata.


<details>
  <summary>Details</summary>
Motivation: Writing vulnerability detection queries for static analysis tools is challenging due to the need for diverse expertise in security and program analysis.

Method: QLCoder employs an LLM-based synthesis loop with execution feedback and constraints via a custom MCP interface supporting structured interaction with a Language Server Protocol and a RAG database.

Result: QLCoder successfully generated correct CodeQL queries for detecting vulnerabilities in 53.4% of CVEs across 111 Java projects, significantly outperforming an alternative method achieving only 10%.

Conclusion: The framework demonstrates substantial improvements in automating the generation of effective security queries, enhancing static analysis for vulnerability detection.

Abstract: Static analysis tools provide a powerful means to detect security vulnerabilities by specifying queries that encode vulnerable code patterns. However, writing such queries is challenging and requires diverse expertise in security and program analysis. To address this challenge, we present QLCoder - an agentic framework that automatically synthesizes queries in CodeQL, a powerful static analysis engine, directly from a given CVE metadata. QLCode embeds an LLM in a synthesis loop with execution feedback, while constraining its reasoning using a custom MCP interface that allows structured interaction with a Language Server Protocol (for syntax guidance) and a RAG database (for semantic retrieval of queries and documentation). This approach allows QLCoder to generate syntactically and semantically valid security queries. We evaluate QLCode on 176 existing CVEs across 111 Java projects. Building upon the Claude Code agent framework, QLCoder synthesizes correct queries that detect the CVE in the vulnerable but not in the patched versions for 53.4% of CVEs. In comparison, using only Claude Code synthesizes 10% correct queries.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [58] [Hyperellipsoid Density Sampling: Exploitative Sequences to Accelerate High-Dimensional Optimization](https://arxiv.org/abs/2511.07836)
*Julian Soltes*

Main category: math.NA

TL;DR: This paper introduces Hyperellipsoid Density Sampling (HDS), an adaptive sampling strategy for high-dimensional optimization problems, outperforming traditional quasi-Monte Carlo (QMC) methods.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies caused by the curse of dimensionality in optimization problems, which make traditional methods infeasible due to the exponential increase in search space.

Method: HDS employs hyperellipsoids and three unsupervised learning algorithms to generate intelligent, non-uniform sample sequences that focus on statistically promising regions of the parameter space. It includes optional Gaussian weights to adaptively concentrate the sampling.

Result: HDS demonstrated statistically significant improvements in optimization performance compared to the Sobol QMC method, with gains ranging from 3% to 37% over varying dimensions on benchmark tests.

Conclusion: HDS serves as a robust and versatile alternative to QMC sampling, improving solution quality in high-dimensional optimization and expanding its utility to applications requiring focused sampling in non-uniform parameter spaces.

Abstract: The curse of dimensionality presents a pervasive challenge in optimization problems, with exponential expansion of the search space rapidly causing traditional algorithms to become inefficient or infeasible. An adaptive sampling strategy is presented to accelerate optimization in this domain as an alternative to uniform quasi-Monte Carlo (QMC) methods.
  This method, referred to as Hyperellipsoid Density Sampling (HDS), generates its sequences by defining multiple hyperellipsoids throughout the search space. HDS uses three types of unsupervised learning algorithms to circumvent high-dimensional geometric calculations, producing an intelligent, non-uniform sample sequence that exploits statistically promising regions of the parameter space and improves final solution quality in high-dimensional optimization problems.
  A key feature of the method is optional Gaussian weights, which may be provided to influence the sample distribution towards known locations of interest. This capability makes HDS versatile for applications beyond optimization, providing a focused, denser sample distribution where models need to concentrate their efforts on specific, non-uniform regions of the parameter space.
  The method was evaluated against Sobol, a standard QMC method, using differential evolution (DE) on the 29 CEC2017 benchmark test functions. The results show statistically significant improvements in solution geometric mean error (p < 0.05), with average performance gains ranging from 3% in 30D to 37% in 10D. This paper demonstrates the efficacy of HDS as a robust alternative to QMC sampling for high-dimensional optimization.

</details>
