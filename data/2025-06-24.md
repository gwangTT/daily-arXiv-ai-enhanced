<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 50]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.CL](#cs.CL) [Total: 85]
- [cs.CV](#cs.CV) [Total: 110]
- [cs.DC](#cs.DC) [Total: 13]
- [cs.LG](#cs.LG) [Total: 108]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.RO](#cs.RO) [Total: 51]
- [cs.SE](#cs.SE) [Total: 29]
- [q-bio.NC](#q-bio.NC) [Total: 6]
- [stat.ML](#stat.ML) [Total: 16]
- [cs.HC](#cs.HC) [Total: 3]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.CG](#cs.CG) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.LO](#cs.LO) [Total: 2]
- [eess.IV](#eess.IV) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.DS](#cs.DS) [Total: 2]
- [math.OC](#math.OC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 16]
- [eess.SY](#eess.SY) [Total: 4]
- [cs.CY](#cs.CY) [Total: 7]
- [math.PR](#math.PR) [Total: 1]
- [stat.ME](#stat.ME) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Evaluating Generalization and Representation Stability in Small LMs via Prompting](https://arxiv.org/abs/2506.17289)
*Rahul Raja,Arpita Vats*

Main category: cs.AI

TL;DR: The paper compares prompting and fine-tuning paradigms for small language models in low-resource and distributionally shifted settings, analyzing their performance and internal representations.


<details>
  <summary>Details</summary>
Motivation: To understand the generalization capabilities of small language models and compare prompting and fine-tuning in various settings, especially under low-resource and distribution shifts.

Method: The authors conducted empirical analysis of prompting and fine-tuning across different task formats, prompt styles, and model scales, focusing on both in-distribution and out-of-distribution scenarios.

Result: The study revealed critical differences in how small models learn and generalize task-specific knowledge under prompting versus fine-tuning, offering insights into their robustness and efficiency.

Conclusion: The findings provide practical guidance for choosing between prompting and fine-tuning in low-data settings and contribute to the ongoing debate about the two approaches' efficacy.

Abstract: We investigate the generalization capabilities of small language models under
two popular adaptation paradigms: few-shot prompting and supervised
fine-tuning. While prompting is often favored for its parameter efficiency and
flexibility, it remains unclear how robust this approach is in low-resource
settings and under distributional shifts. This paper presents a comparative
study of prompting and fine-tuning across task formats, prompt styles, and
model scales, with a focus on their behavior in both in-distribution and
out-of-distribution (OOD) settings.
  Beyond accuracy, we analyze the internal representations learned by each
approach to assess the stability and abstraction of task-specific features. Our
findings highlight critical differences in how small models internalize and
generalize knowledge under different adaptation strategies. This work offers
practical guidance for model selection in low-data regimes and contributes
empirical insight into the ongoing debate over prompting versus fine-tuning.
Code for the experiments is available at the following

</details>


### [2] [Individual Causal Inference with Structural Causal Model](https://arxiv.org/abs/2506.17300)
*Daniel T. Chang*

Main category: cs.AI

TL;DR: The paper introduces Individual Causal Inference (ICI) using Structural Causal Models (SCM), emphasizing methods to individualize population-based causal analyses and estimate individual causal effects.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to address the challenge of estimating individual causal effects (ICE) given that most causal inference models are population-based and lack mechanisms to predict effects specific to individuals.

Method: The authors propose the use of Structural Causal Models (SCM) for individual causal inference by introducing the indiv-operator to encode individual variations and formulating individual causal queries like P(Y | indiv(W), do(X), Z).

Result: ICI with SCM enables inference on individual alternatives (hypothetical scenarios based on characteristics) but not individual counterfactuals (non-actual scenarios).

Conclusion: The approach provides a novel framework for individualizing population-based causal inference using SCM, contributing to the advancement of personalized interventions and individual predictions.

Abstract: Individual causal inference (ICI) uses causal inference methods to understand
and predict the effects of interventions on individuals, considering their
specific characteristics / facts. It aims to estimate individual causal effect
(ICE), which varies across individuals. Estimating ICE can be challenging due
to the limited data available for individuals, and the fact that most causal
inference methods are population-based. Structural Causal Model (SCM) is
fundamentally population-based. Therefore, causal discovery (structural
learning and parameter learning), association queries and intervention queries
are all naturally population-based. However, exogenous variables (U) in SCM can
encode individual variations and thus provide the mechanism for individualized
population per specific individual characteristics / facts. Based on this, we
propose ICI with SCM as a "rung 3" causal inference, because it involves
"imagining" what would be the causal effect of a hypothetical intervention on
an individual, given the individual's observed characteristics / facts.
Specifically, we propose the indiv-operator, indiv(W), to formalize/represent
the population individualization process, and the individual causal query, P(Y
| indiv(W), do(X), Z), to formalize/represent ICI. We show and argue that ICI
with SCM is inference on individual alternatives (possible), not individual
counterfactuals (non-actual).

</details>


### [3] [Resource Rational Contractualism Should Guide AI Alignment](https://arxiv.org/abs/2506.17434)
*Sydney Levine,Matija Franklin,Tan Zhi-Xuan,Secil Yanik Guyot,Lionel Wong,Daniel Kilov,Yejin Choi,Joshua B. Tenenbaum,Noah Goodman,Seth Lazar,Iason Gabriel*

Main category: cs.AI

TL;DR: The paper introduces Resource-Rational Contractualism (RRC), a framework for aligning AI decisions with agreements rational stakeholders would endorse using cognitively-inspired heuristics for efficiency and adaptability.


<details>
  <summary>Details</summary>
Motivation: AI systems will inevitably make decisions in environments with divergent human and AI values. Aligning these systems to agreements acceptable to all stakeholders at scale is both critical and challenging.

Method: The RRC framework relies on heuristics inspired by human cognition, aiming to approximate rational agreements while trading effort for decision accuracy.

Result: The proposed RRC-aligned AI systems can operate efficiently and adapt to the dynamic nature of human social interactions.

Conclusion: A resource-rational approach enables scalable alignment of AI systems with human and other stakeholders' goals and values while ensuring adaptability and efficiency.

Abstract: AI systems will soon have to navigate human environments and make decisions
that affect people and other AI agents whose goals and values diverge.
Contractualist alignment proposes grounding those decisions in agreements that
diverse stakeholders would endorse under the right conditions, yet securing
such agreement at scale remains costly and slow -- even for advanced AI. We
therefore propose Resource-Rational Contractualism (RRC): a framework where AI
systems approximate the agreements rational parties would form by drawing on a
toolbox of normatively-grounded, cognitively-inspired heuristics that trade
effort for accuracy. An RRC-aligned agent would not only operate efficiently,
but also be equipped to dynamically adapt to and interpret the ever-changing
human social world.

</details>


### [4] [Keeping Medical AI Healthy: A Review of Detection and Correction Methods for System Degradation](https://arxiv.org/abs/2506.17442)
*Hao Guan,David Bates,Li Zhou*

Main category: cs.AI

TL;DR: This paper reviews methods to monitor, detect, and correct performance degradation in healthcare AI systems, proposing ways to maintain long-term system reliability.


<details>
  <summary>Details</summary>
Motivation: In clinical settings, AI systems face performance degradation due to data shifts, patient variability, and changing protocols, posing safety risks that necessitate solutions for sustained reliability.

Method: The paper examines causes of performance degradation, surveys detection techniques for model and data drift, explores root cause analysis, and reviews correction strategies like model retraining and test-time adaptation.

Result: The review synthesizes insights across machine learning, including large language models, identifying both their capabilities and limitations in addressing system reliability challenges.

Conclusion: The paper provides guidance on achieving robust and reliable AI systems for safe, long-term healthcare applications, while highlighting unresolved technical challenges and future research needs.

Abstract: Artificial intelligence (AI) is increasingly integrated into modern
healthcare, offering powerful support for clinical decision-making. However, in
real-world settings, AI systems may experience performance degradation over
time, due to factors such as shifting data distributions, changes in patient
characteristics, evolving clinical protocols, and variations in data quality.
These factors can compromise model reliability, posing safety concerns and
increasing the likelihood of inaccurate predictions or adverse outcomes. This
review presents a forward-looking perspective on monitoring and maintaining the
"health" of AI systems in healthcare. We highlight the urgent need for
continuous performance monitoring, early degradation detection, and effective
self-correction mechanisms. The paper begins by reviewing common causes of
performance degradation at both data and model levels. We then summarize key
techniques for detecting data and model drift, followed by an in-depth look at
root cause analysis. Correction strategies are further reviewed, ranging from
model retraining to test-time adaptation. Our survey spans both traditional
machine learning models and state-of-the-art large language models (LLMs),
offering insights into their strengths and limitations. Finally, we discuss
ongoing technical challenges and propose future research directions. This work
aims to guide the development of reliable, robust medical AI systems capable of
sustaining safe, long-term deployment in dynamic clinical settings.

</details>


### [5] [OmniReflect: Discovering Transferable Constitutions for LLM agents via Neuro-Symbolic Reflections](https://arxiv.org/abs/2506.17449)
*Manasa Bharadwaj,Nikhil Verma,Kevin Ferreira*

Main category: cs.AI

TL;DR: OmniReflect is a framework that helps Large Language Models (LLMs) learn long-term principles from their task experiences, leading to significant performance improvements on various tasks.


<details>
  <summary>Details</summary>
Motivation: Existing techniques for enhancing LLM performance often lack mechanisms for long-term learning and are inefficient in dynamic settings, necessitating a more generalizable and efficient approach.

Method: OmniReflect creates a 'constitution' of guiding principles derived from task experiences, employing Neural, Symbolic, and NeuroSymbolic techniques. The framework works in two modes: Self-sustaining, where the agent refines its own principles, and Co-operative, where a Meta-advisor guides another agent.

Result: The framework achieves significant performance improvements across tasks and models, including +10.3% on ALFWorld, +23.8% on BabyAI, and +8.3% on PDDL in Self-sustaining mode, with Co-operative mode also showing strong gains.

Conclusion: OmniReflect demonstrates effectiveness and robustness in enhancing LLM agent performance by incorporating hierarchical, reflection-driven learning, making it a valuable contribution to LLM research and applications.

Abstract: Efforts to improve Large Language Model (LLM) agent performance on complex
tasks have largely focused on fine-tuning and iterative self-correction.
However, these approaches often lack generalizable mechanisms for longterm
learning and remain inefficient in dynamic environments. We introduce
OmniReflect, a hierarchical, reflection-driven framework that constructs a
constitution, a compact set of guiding principles distilled from task
experiences, to enhance the effectiveness and efficiency of an LLM agent.
OmniReflect operates in two modes: Self-sustaining, where a single agent
periodically curates its own reflections during task execution, and
Co-operative, where a Meta-advisor derives a constitution from a small
calibration set to guide another agent. To construct these constitutional
principles, we employ Neural, Symbolic, and NeuroSymbolic techniques, offering
a balance between contextual adaptability and computational efficiency.
Empirical results averaged across models show major improvements in task
success, with absolute gains of +10.3% on ALFWorld, +23.8% on BabyAI, and +8.3%
on PDDL in the Self-sustaining mode. Similar gains are seen in the Co-operative
mode, where a lightweight Qwen3-4B ReAct agent outperforms all Reflexion
baselines on BabyAI. These findings highlight the robustness and effectiveness
of OmniReflect across environments and backbones.

</details>


### [6] [Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective](https://arxiv.org/abs/2506.17930)
*Jianyu Wang,Zhiqiang Hu,Lidong Bing*

Main category: cs.AI

TL;DR: This paper proposes "PromptQuine," a framework showing that pruning seemingly irrelevant prompt demonstrations into incoherent forms ("gibberish") can boost large language model (LLM) performance. It challenges traditional wisdom in LLM prompting.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the limitations of traditional prompt design in LLMs, which emphasizes well-crafted instructions and demonstrations, often failing to fully optimize performance.

Method: The authors introduce "PromptQuine," an evolutionary search framework that optimizes prompts by pruning demonstrations into unconventional forms. The system leverages limited data and emergent complexity principles, such as self-organization, to refine prompts.

Result: The optimized 'gibberish' prompts generated by PromptQuine outperform state-of-the-art automatic prompt optimization techniques, achieving significant improvements in various tasks (e.g., classification, question answering, generation, and math reasoning).

Conclusion: The findings suggest that effective LLM prompting can emerge from unconventional methods like gibberish pruning, highlighting the need for open-ended search algorithms and inspiring further exploration in prompt optimization and in-context learning studies.

Abstract: We propose a novel prompt design paradigm that challenges conventional wisdom
in large language model (LLM) prompting. While conventional wisdom prioritizes
well-crafted instructions and demonstrations for in-context learning (ICL), we
show that pruning random demonstrations into seemingly incoherent "gibberish"
can remarkably improve performance across diverse tasks. Notably, the
"gibberish" always matches or surpasses state-of-the-art automatic prompt
optimization techniques, achieving substantial gains regardless of LLM
alignment. Nevertheless, discovering an effective pruning strategy is
non-trivial, as existing attribution methods and prompt compression algorithms
fail to deliver robust results, let alone human intuition. In terms of this, we
propose a self-discover prompt optimization framework, PromptQuine, an
evolutionary search framework that automatically searches for the pruning
strategy by itself using only low-data regimes. Much like the emergent
complexity in nature--such as symbiosis and self-organization--arising in
response to resource constraints, our framework evolves and refines
unconventional yet highly effective prompts by leveraging only the tokens
present within the context. We demonstrate its effectiveness across
classification, multi-choice question answering, generation and math reasoning
tasks across LLMs, while achieving decent runtime efficiency. We hope our
findings can guide mechanistic studies on in-context learning, and provide a
call to action, to pave the way for more open-ended search algorithms for more
effective LLM prompting.

</details>


### [7] [From Unstructured Communication to Intelligent RAG: Multi-Agent Automation for Supply Chain Knowledge Bases](https://arxiv.org/abs/2506.17484)
*Yao Zhang,Zaixi Shang,Silpan Patel,Mikel Zuniga*

Main category: cs.AI

TL;DR: The paper presents an offline-first methodology using LLM-based multi-agent systems to convert unstructured supply chain communication data into a structured knowledge base, improving RAG system performance and operational efficiency.


<details>
  <summary>Details</summary>
Motivation: Critical knowledge in supply chain operations often resides in unstructured formats like tickets and chat logs, making it hard to retrieve and utilize effectively within RAG systems.

Method: The methodology introduces a multi-agent system with three LLM-based specialized agents: Category Discovery for taxonomy creation, Categorization for ticket grouping, and Knowledge Synthesis for generating articles, thereby structuring unstructured operational data offline.

Result: The approach compresses ticket data volume to 3.4% while enhancing its quality, improving RAG system performance for helpful answers (48.74% vs. 38.60%) and drastically reducing unhelpful responses by 77.4%.

Conclusion: This offline-first system automates the transformation of unstructured knowledge into structured, reusable forms, enhancing supply chain efficiency by reducing workload, accelerating issue resolution, and enabling systems to autonomously resolve a significant portion of future tickets.

Abstract: Supply chain operations generate vast amounts of operational data; however,
critical knowledge such as system usage practices, troubleshooting workflows,
and resolution techniques often remains buried within unstructured
communications like support tickets, emails, and chat logs. While RAG systems
aim to leverage such communications as a knowledge base, their effectiveness is
limited by raw data challenges: support tickets are typically noisy,
inconsistent, and incomplete, making direct retrieval suboptimal. Unlike
existing RAG approaches that focus on runtime optimization, we introduce a
novel offline-first methodology that transforms these communications into a
structured knowledge base. Our key innovation is a LLMs-based multi-agent
system orchestrating three specialized agents: Category Discovery for taxonomy
creation, Categorization for ticket grouping, and Knowledge Synthesis for
article generation. Applying our methodology to real-world support tickets with
resolution notes and comments, our system creates a compact knowledge base -
reducing total volume to just 3.4% of original ticket data while improving
quality. Experiments demonstrate that our prebuilt knowledge base in RAG
systems significantly outperforms traditional RAG implementations (48.74% vs.
38.60% helpful answers) and achieves a 77.4% reduction in unhelpful responses.
By automating institutional knowledge capture that typically remains siloed in
experts' heads, our solution translates to substantial operational efficiency:
reducing support workload, accelerating resolution times, and creating
self-improving systems that automatically resolve approximately 50% of future
supply chain tickets. Our approach addresses a key gap in knowledge management
by transforming transient communications into structured, reusable knowledge
through intelligent offline processing rather than latency-inducing runtime
architectures.

</details>


### [8] [Kaleidoscopic Teaming in Multi Agent Simulations](https://arxiv.org/abs/2506.17514)
*Ninareh Mehrabi,Tharindu Kumarage,Kai-Wei Chang,Aram Galstyan,Rahul Gupta*

Main category: cs.AI

TL;DR: The paper addresses safety risks in AI agents, especially in multi-agent settings, by proposing a new "kaleidoscopic teaming" framework to evaluate vulnerabilities in complex behaviors and interactions.


<details>
  <summary>Details</summary>
Motivation: Existing safety evaluation frameworks fail to effectively assess complex behaviors and thought processes in AI agents, especially in multi-agent interactions.

Method: The authors introduce a "kaleidoscopic teaming" framework that simulates diverse real-world scenarios. The framework tests both single- and multi-agent setups and incorporates new in-context optimization techniques to improve scenario generation for safety analysis.

Result: The framework successfully identifies safety vulnerabilities in different AI models under agentic use-cases. Metrics are also proposed to quantify safety.

Conclusion: Kaleidoscopic teaming provides a novel approach to evaluating AI safety by addressing gaps in current frameworks, particularly in relation to complex multi-agent dynamics and vulnerabilities.

Abstract: Warning: This paper contains content that may be inappropriate or offensive.
  AI agents have gained significant recent attention due to their autonomous
tool usage capabilities and their integration in various real-world
applications. This autonomy poses novel challenges for the safety of such
systems, both in single- and multi-agent scenarios. We argue that existing red
teaming or safety evaluation frameworks fall short in evaluating safety risks
in complex behaviors, thought processes and actions taken by agents. Moreover,
they fail to consider risks in multi-agent setups where various vulnerabilities
can be exposed when agents engage in complex behaviors and interactions with
each other. To address this shortcoming, we introduce the term kaleidoscopic
teaming which seeks to capture complex and wide range of vulnerabilities that
can happen in agents both in single-agent and multi-agent scenarios. We also
present a new kaleidoscopic teaming framework that generates a diverse array of
scenarios modeling real-world human societies. Our framework evaluates safety
of agents in both single-agent and multi-agent setups. In single-agent setup,
an agent is given a scenario that it needs to complete using the tools it has
access to. In multi-agent setup, multiple agents either compete against or
cooperate together to complete a task in the scenario through which we capture
existing safety vulnerabilities in agents. We introduce new in-context
optimization techniques that can be used in our kaleidoscopic teaming framework
to generate better scenarios for safety analysis. Lastly, we present
appropriate metrics that can be used along with our framework to measure safety
of agents. Utilizing our kaleidoscopic teaming framework, we identify
vulnerabilities in various models with respect to their safety in agentic
use-cases.

</details>


### [9] [Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models](https://arxiv.org/abs/2506.17585)
*Yukun Huang,Sanxing Chen,Jian Pei,Manzil Zaheer,Bhuwan Dhingra*

Main category: cs.AI

TL;DR: The paper proposes a method to enable language models to reliably cite from pretraining data without requiring real-time retrieval, addressing citation reliability issues in LLMs.


<details>
  <summary>Details</summary>
Motivation: Language models often hallucinate citations, leading to unreliable references. Current methods depend on external retrieval systems which add latency and are prone to noise. This paper aims to enable LLMs to reliably provide citations based on their pretraining data.

Method: The authors introduce a two-stage approach: (1) continual pretraining with document identifiers, and (2) instruction tuning to elicit citation behaviors. They compare Passive Indexing (binding text to individual document IDs) with their proposed Active Indexing, which uses synthetic QA pairs to teach bidirectional fact sourcing.

Result: Experiments with Qwen2.5-7B and 3B show that Active Indexing significantly improves citation precision, achieving up to 30.2% improvement compared to Passive Indexing.

Conclusion: Active Indexing is a more effective method for teaching LLMs reliable citation behavior, with continued improvement as training data scales.

Abstract: Trustworthy language models should provide both correct and verifiable
answers. While language models can sometimes attribute their outputs to
pretraining data, their citations are often unreliable due to hallucination. As
a result, current systems insert citations by querying an external retriever at
inference time, introducing latency, infrastructure dependence, and
vulnerability to retrieval noise. We explore whether LLMs can be made to
reliably attribute to the documents seen during (continual)
pretraining--without test-time retrieval--by revising the training process. To
evaluate this, we release CitePretrainBench, a benchmark that mixes real-world
corpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and
probes both short-form (single fact) and long-form (multi-fact) citation tasks.
Our approach follows a two-stage process: (1) continual pretraining to bind
facts to persistent document identifiers, and (2) instruction tuning to elicit
citation behavior. We find that simple Passive Indexing, which appends an
identifier to each document, helps memorize verbatim text but fails on
paraphrased or compositional facts. Instead, we propose Active Indexing, which
continually pretrains on synthetic QA pairs that (1) restate each fact in
diverse compositional forms, and (2) require bidirectional source-to-fact and
fact-to-source generation, jointly teaching the model to generate content from
a cited source and to attribute its own answers. Experiments with Qwen2.5-7B
and 3B show that Active Indexing consistently outperforms Passive Indexing
across all tasks and models, with citation precision gains up to 30.2 percent.
Our ablation studies reveal that performance continues to improve as we scale
the amount of augmented data, showing a clear upward trend even at 16 times the
original token count.

</details>


### [10] [Taming the Untamed: Graph-Based Knowledge Retrieval and Reasoning for MLLMs to Conquer the Unknown](https://arxiv.org/abs/2506.17589)
*Bowen Wang*

Main category: cs.AI

TL;DR: This paper explores the potential of multimodal large language models (MLLMs) for domain-specific knowledge retrieval and reasoning using a visual game environment and proposes a multi-agent retriever to enhance MLLM capabilities.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of MLLMs in domain-specific tasks due to inadequate relevant knowledge.

Method: Constructed a multimodal knowledge graph (MH-MMKG) using visual game Monster Hunter: World, designed challenging queries, and proposed a multi-agent retriever for autonomous knowledge search.

Result: The proposed method significantly improved the performance of MLLMs on complex knowledge retrieval and reasoning tasks.

Conclusion: The study opens up new possibilities for multimodal knowledge-augmented reasoning and establishes a foundation for future advancements in the field.

Abstract: The real value of knowledge lies not just in its accumulation, but in its
potential to be harnessed effectively to conquer the unknown. Although recent
multimodal large language models (MLLMs) exhibit impressing multimodal
capabilities, they often fail in rarely encountered domain-specific tasks due
to limited relevant knowledge. To explore this, we adopt visual game cognition
as a testbed and select Monster Hunter: World as the target to construct a
multimodal knowledge graph (MH-MMKG), which incorporates multi-modalities and
intricate entity relations. We also design a series of challenging queries
based on MH-MMKG to evaluate the models' ability for complex knowledge
retrieval and reasoning. Furthermore, we propose a multi-agent retriever that
enables a model to autonomously search relevant knowledge without additional
training. Experimental results show that our approach significantly enhances
the performance of MLLMs, providing a new perspective on multimodal
knowledge-augmented reasoning and laying a solid foundation for future
research.

</details>


### [11] [Measuring and Augmenting Large Language Models for Solving Capture-the-Flag Challenges](https://arxiv.org/abs/2506.17644)
*Zimo Ji,Daoyuan Wu,Wenyuan Jiang,Pingchuan Ma,Zongjie Li,Shuai Wang*

Main category: cs.AI

TL;DR: This paper presents CTFKnow, a benchmark to measure the performance of large language models (LLMs) in solving Capture-the-Flag (CTF) cybersecurity challenges, and introduces CTFAgent, a framework improving LLMs' abilities in exploiting technical knowledge and vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: The paper aims to evaluate and enhance the ability of LLMs in solving CTF problems, which demand technical knowledge, reasoning, and adaptability—critical for advancing AI-powered cybersecurity education and challenges.

Method: The authors developed a benchmark called CTFKnow containing 3,992 questions to assess LLMs' performance. They also introduced CTFAgent, a framework with two modules (two-stage RAG and interactive Environmental Augmentation) to improve LLMs' problem-solving abilities.

Result: CTFAgent showed over an 80% performance improvement on two popular CTF datasets and achieved a top 23.6% ranking in picoCTF2024 among nearly 7,000 teams, demonstrating the practical advantages of their framework.

Conclusion: The study highlights LLMs' technical knowledge shortcomings in specific scenarios and proposes solutions through CTFAgent, showcasing its potential in advancing LLM performance in CTF competitions leveraging enhanced frameworks.

Abstract: Capture-the-Flag (CTF) competitions are crucial for cybersecurity education
and training. As large language models (LLMs) evolve, there is increasing
interest in their ability to automate CTF challenge solving. For example, DARPA
has organized the AIxCC competition since 2023 to advance AI-powered automated
offense and defense. However, this demands a combination of multiple abilities,
from knowledge to reasoning and further to actions. In this paper, we highlight
the importance of technical knowledge in solving CTF problems and deliberately
construct a focused benchmark, CTFKnow, with 3,992 questions to measure LLMs'
performance in this core aspect. Our study offers a focused and innovative
measurement of LLMs' capability in understanding CTF knowledge and applying it
to solve CTF challenges. Our key findings reveal that while LLMs possess
substantial technical knowledge, they falter in accurately applying this
knowledge to specific scenarios and adapting their strategies based on feedback
from the CTF environment.
  Based on insights derived from this measurement study, we propose CTFAgent, a
novel LLM-driven framework for advancing CTF problem-solving. CTFAgent
introduces two new modules: two-stage Retrieval Augmented Generation (RAG) and
interactive Environmental Augmentation, which enhance LLMs' technical knowledge
and vulnerability exploitation on CTF, respectively. Our experimental results
show that, on two popular CTF datasets, CTFAgent both achieves over 80%
performance improvement. Moreover, in the recent picoCTF2024 hosted by CMU,
CTFAgent ranked in the top 23.6% of nearly 7,000 participating teams. This
reflects the benefit of our measurement study and the potential of our
framework in advancing LLMs' capabilities in CTF problem-solving.

</details>


### [12] [PhysUniBench: An Undergraduate-Level Physics Reasoning Benchmark for Multimodal Models](https://arxiv.org/abs/2506.17667)
*Lintao Wang,Encheng Su,Jiaqi Liu,Pengze Li,Peng Xia,Jiabei Xiao,Wenlong Zhang,Xinnan Dai,Xi Chen,Yuan Meng,Mingyu Ding,Lei Bai,Wanli Ouyang,Shixiang Tang,Aoran Wang,Xinzhu Ma*

Main category: cs.AI

TL;DR: This paper introduces PhysUniBench, a multimodal benchmark for evaluating and advancing large language models' (MLLMs) physics reasoning on undergraduate-level problems, highlighting significant challenges in current models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing evaluation methodologies and encourage progress in AI's ability to solve challenging undergraduate-level physics problems that require deep reasoning and multimodal understanding.

Method: Developed PhysUniBench, a benchmark with 3,304 physics problems across 8 sub-disciplines, including open-ended and multiple-choice formats, systematically curated and graded for difficulty through an iterative, model-in-the-loop process.

Result: State-of-the-art models, such as GPT-4 mini, perform poorly on the benchmark, achieving only 34.2% accuracy, emphasizing the current limitations of MLLMs in physics reasoning and diagram interpretation.

Conclusion: PhysUniBench reveals significant shortcomings in current AI models' physics reasoning capabilities and serves as a resource to advance the development of more capable and multimodal AI for Science systems.

Abstract: Physics problem-solving is a challenging domain for large AI models,
requiring integration of conceptual understanding, mathematical reasoning, and
interpretation of physical diagrams. Current evaluation methodologies show
notable limitations in capturing the breadth and complexity of
undergraduate-level physics, underscoring the need for more rigorous
assessments. To this end, we present PhysUniBench, a large-scale multimodal
benchmark designed to evaluate and improve the reasoning capabilities of
multimodal large language models (MLLMs) specifically on undergraduate-level
physics problems. PhysUniBench consists of 3,304 physics questions spanning 8
major sub-disciplines of physics, each accompanied by one visual diagrams. The
benchmark includes both open-ended and multiple-choice questions,
systematically curated and difficulty-rated through an iterative
model-in-the-loop process. The benchmark's construction involved a rigorous
multi-stage process, including multiple roll-outs, expert-level evaluation,
automated filtering of easily solved problems, and a nuanced difficulty grading
system with five levels. Through extensive experiments, we observe that current
state-of-the-art models encounter substantial challenges in physics reasoning.
For example, GPT-4o mini achieves only about 34.2\% accuracy in the proposed
PhysUniBench. These results highlight that current MLLMs struggle with advanced
physics reasoning, especially on multi-step problems and those requiring
precise diagram interpretation. By providing a broad and rigorous assessment
tool, PhysUniBench aims to drive progress in AI for Science, encouraging the
development of models with stronger physical reasoning, problem-solving skills,
and multimodal understanding. The benchmark and evaluation scripts are
available at https://prismax-team.github.io/PhysUniBenchmark/.

</details>


### [13] [Beyond Syntax: Action Semantics Learning for App Agents](https://arxiv.org/abs/2506.17697)
*Bohan Tang,Dezhao Luo,Jingxuan Chen,Shaogang Gong,Jianye Hao,Jun Wang,Kun Shao*

Main category: cs.AI

TL;DR: The paper introduces a novel learning framework, Action Semantics Learning (ASL), aimed at enhancing App agents' robustness by focusing on semantics rather than syntax for actions performed in smartphone apps.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address limitations in current methods that rely on syntax learning paradigms, which often struggle with out-of-distribution (OOD) vulnerability.

Method: The authors propose ASL, utilizing a Semantic Estimator (SEE) to measure and train agents based on the semantic effects of actions, bypassing the need for exact syntactic replication.

Result: ASL demonstrates superior performance in both offline and online smartphone App operation benchmarks, showing improved accuracy and generalization compared to existing methods.

Conclusion: ASL provides a robust approach for training App agents, enhancing generalization and reducing dependencies on syntactic action reproduction, effectively mitigating the OOD problem.

Abstract: The advent of Large Language Models (LLMs) enables the rise of App agents
that interpret user intent and operate smartphone Apps through actions such as
clicking and scrolling. While prompt-based solutions with closed LLM APIs show
promising ability, they incur heavy compute costs and external API dependency.
Fine-tuning smaller open-source LLMs solves these limitations. However, current
fine-tuning methods use a syntax learning paradigm that forces agents to
reproduce exactly the ground truth action strings, leading to
out-of-distribution (OOD) vulnerability. To fill this gap, we propose Action
Semantics Learning (ASL), a novel learning framework, where the learning
objective is capturing the semantics of the ground truth actions. Specifically,
inspired by the programming language theory, we define the action semantics for
App agents as the state transition induced by the action in the user interface.
With this insight, ASL employs a novel SEmantic Estimator (SEE) to compute a
semantic reward to train the App agents in generating actions aligned with the
semantics of ground truth actions, even when the syntactic forms differ. To
support the effectiveness of ASL, we theoretically demonstrate the superior
robustness of ASL for the OOD problem compared with the existing syntax
learning paradigm. Extensive experiments on offline and online smartphone App
operation benchmarks show that ASL significantly improves the accuracy and
generalisation of App agents over existing methods.

</details>


### [14] [AnyMAC: Cascading Flexible Multi-Agent Collaboration via Next-Agent Prediction](https://arxiv.org/abs/2506.17784)
*Song Wang,Zhen Tan,Zihan Chen,Shuang Zhou,Tianlong Chen,Jundong Li*

Main category: cs.AI

TL;DR: This paper introduces a novel framework for multi-agent coordination using sequential structures instead of traditional graph-based approaches, emphasizing adaptability and efficiency in communication.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance adaptability and flexibility in multi-agent communication, overcoming limitations of static or graph-based inter-agent topologies.

Method: The paper proposes a sequential structure for multi-agent communication, incorporating (1) Next-Agent Prediction for dynamic role assignment, and (2) Next-Context Selection for context-aware information sharing.

Result: The approach demonstrated superior performance across benchmarks with reduced communication overhead compared to existing methods.

Conclusion: Sequential communication structures enable task-adaptive pipelines, offering role flexibility and effective global information flow in multi-agent systems.

Abstract: Recent progress in large language model (LLM)-based multi-agent collaboration
highlights the power of structured communication in enabling collective
intelligence. However, existing methods largely rely on static or graph-based
inter-agent topologies, lacking the potential adaptability and flexibility in
communication. In this work, we propose a new framework that rethinks
multi-agent coordination through a sequential structure rather than a graph
structure, offering a significantly larger topology space for multi-agent
communication. Our method focuses on two key directions: (1) Next-Agent
Prediction, which selects the most suitable agent role at each step, and (2)
Next-Context Selection (NCS), which enables each agent to selectively access
relevant information from any previous step. Together, these components
construct task-adaptive communication pipelines that support both role
flexibility and global information flow. Extensive evaluations across multiple
benchmarks demonstrate that our approach achieves superior performance while
substantially reducing communication overhead.

</details>


### [15] [Bayesian Social Deduction with Graph-Informed Language Models](https://arxiv.org/abs/2506.17788)
*Shahab Rahimirad,Guven Gergerli,Lucia Romero,Angela Qian,Matthew Lyle Olson,Simon Stepputtis,Joseph Campbell*

Main category: cs.AI

TL;DR: The paper introduces a hybrid reasoning framework that combines language models with probabilistic reasoning to enhance social reasoning. It achieves competitive AI and human gameplay results.


<details>
  <summary>Details</summary>
Motivation: Current large language models struggle with complex social reasoning, particularly in inferring beliefs and intentions during social deduction scenarios like the Avalon game.

Method: The proposed framework externalizes belief inference to a probabilistic model while leveraging an LLM for language tasks, achieving efficient reasoning and interaction.

Result: The approach outperformed larger LLMs, achieving a 67% win rate against humans and received higher qualitative ratings than existing baselines.

Conclusion: This hybrid framework demonstrates the potential to enhance reasoning in AI systems more efficiently, bridging the performance gap compared to larger LLMs. Resources are shared to support further development in this area.

Abstract: Social reasoning - inferring unobservable beliefs and intentions from partial
observations of other agents - remains a challenging task for large language
models (LLMs). We evaluate the limits of current reasoning language models in
the social deduction game Avalon and find that while the largest models
demonstrate strong performance, they require extensive test-time inference and
degrade sharply when distilled to smaller, real-time-capable variants. To
address this, we introduce a hybrid reasoning framework that externalizes
belief inference to a structured probabilistic model, while using an LLM for
language understanding and interaction. Our approach achieves competitive
performance with much larger models in Agent-Agent play and, notably, is the
first language agent to defeat human players in a controlled study - achieving
a 67% win rate and receiving higher qualitative ratings than both reasoning
baselines and human teammates. We release code, models, and a dataset to
support future work on social reasoning in LLM agents, which can be found at
https://camp-lab-purdue.github.io/bayesian-social-deduction/

</details>


### [16] [Efficient Strategy Synthesis for MDPs via Hierarchical Block Decomposition](https://arxiv.org/abs/2506.17792)
*Alexandros Evangelidis,Gricel Vázquez,Simos Gerasimou*

Main category: cs.AI

TL;DR: This paper introduces a method to improve policy synthesis for large Markov decision processes (MDPs) by dynamically refining the most fragile regions, demonstrating better performance over the PRISM model checker.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the scalability issues of traditional policy synthesis methods when dealing with MDPs that have large state spaces.

Method: The method involves dynamically refining MDPs, focusing on the most fragile regions, and using an iterative approach to achieve a balance between accuracy and efficiency.

Result: The approach is empirically tested on various case studies with MDPs of up to 1 million states, showing performance improvements of up to 2x over the PRISM model checker.

Conclusion: The proposed method proves to be a competitive solution for synthesizing policies in large-scale MDPs, balancing efficiency and accuracy effectively.

Abstract: Software-intensive systems, such as software product lines and robotics,
utilise Markov decision processes (MDPs) to capture uncertainty and analyse
sequential decision-making problems. Despite the usefulness of conventional
policy synthesis methods, they fail to scale to large state spaces. Our
approach addresses this issue and accelerates policy synthesis in large MDPs by
dynamically refining the MDP and iteratively selecting the most fragile MDP
regions for refinement. This iterative procedure offers a balance between
accuracy and efficiency, as refinement occurs only when necessary. Through a
comprehensive empirical evaluation comprising diverse case studies and MDPs up
to 1M states, we demonstrate significant performance improvements yielded by
our approach compared to the leading probabilistic model checker PRISM (up to
2x), thus offering a very competitive solution for real-world policy synthesis
tasks in larger MDPs.

</details>


### [17] [Reflective Verbal Reward Design for Pluralistic Alignment](https://arxiv.org/abs/2506.17834)
*Carter Blair,Kate Larson,Edith Law*

Main category: cs.AI

TL;DR: The paper introduces a new method for teaching AI agents individualized human values using language model-guided reflective dialogues instead of aggregating feedback, to address value diversity and enhance alignment.


<details>
  <summary>Details</summary>
Motivation: Current methods like reinforcement learning from human feedback (RLHF) often aggregate diverse human values into a single reward model, potentially sidelining minority preferences.

Method: A language model facilitates reflective dialogues with users to critique agent behaviors and articulate preferences, creating a personalized dialogue history that feeds into a verbal reward model for trajectory evaluation.

Result: The approach showed a 9-12% improvement in accuracy compared to non-reflective methods and demonstrated better sample efficiency than traditional supervised learning in tests with 30 participants.

Conclusion: The method proves promising in capturing individual human values more accurately and efficiently, marking a step forward in personalized AI alignment.

Abstract: AI agents are commonly aligned with "human values" through reinforcement
learning from human feedback (RLHF), where a single reward model is learned
from aggregated human feedback and used to align an agent's behavior. However,
human values are not homogeneous--different people hold distinct and sometimes
conflicting values. Aggregating feedback into a single reward model risks
disproportionately suppressing minority preferences. To address this, we
present a novel reward modeling approach for learning individualized reward
models. Our approach uses a language model to guide users through reflective
dialogues where they critique agent behavior and construct their preferences.
This personalized dialogue history, containing the user's reflections and
critiqued examples, is then used as context for another language model that
serves as an individualized reward function (what we call a "verbal reward
model") for evaluating new trajectories. In studies with 30 participants, our
method achieved a 9-12% improvement in accuracy over non-reflective verbal
reward models while being more sample efficient than traditional supervised
learning methods.

</details>


### [18] [Out of Control -- Why Alignment Needs Formal Control Theory (and an Alignment Control Stack)](https://arxiv.org/abs/2506.17846)
*Elija Perrier*

Main category: cs.AI

TL;DR: This paper proposes using formal optimal control theory for better AI alignment, suggesting a hierarchical "Alignment Control Stack" for interoperable control frameworks.


<details>
  <summary>Details</summary>
Motivation: Current AI safety methods and mechanistic interpretability often lack the generalization and interoperability needed for robust control frameworks.

Method: The authors propose reframing AI alignment through formal optimal control, introducing the "Alignment Control Stack," a hierarchical framework detailing controls at physical, socio-technical, and other layers.

Result: The paper introduces the Alignment Control Stack, which demonstrates how measurement and control protocols can be interoperable across layers.

Conclusion: Applying principles of optimal control theory will improve alignment frameworks, aiding safety and reliability for AI systems and providing assurance to regulators.

Abstract: This position paper argues that formal optimal control theory should be
central to AI alignment research, offering a distinct perspective from
prevailing AI safety and security approaches. While recent work in AI safety
and mechanistic interpretability has advanced formal methods for alignment,
they often fall short of the generalisation required of control frameworks for
other technologies. There is also a lack of research into how to render
different alignment/control protocols interoperable. We argue that by recasting
alignment through principles of formal optimal control and framing alignment in
terms of hierarchical stack from physical to socio-technical layers according
to which controls may be applied we can develop a better understanding of the
potential and limitations for controlling frontier models and agentic AI
systems. To this end, we introduce an Alignment Control Stack which sets out a
hierarchical layered alignment stack, identifying measurement and control
characteristics at each layer and how different layers are formally
interoperable. We argue that such analysis is also key to the assurances that
will be needed by governments and regulators in order to see AI technologies
sustainably benefit the community. Our position is that doing so will bridge
the well-established and empirically validated methods of optimal control with
practical deployment considerations to create a more comprehensive alignment
framework, enhancing how we approach safety and reliability for advanced AI
systems.

</details>


### [19] [Towards Robust Fact-Checking: A Multi-Agent System with Advanced Evidence Retrieval](https://arxiv.org/abs/2506.17878)
*Tam Trinh,Manh Nguyen,Truong-Son Hy*

Main category: cs.AI

TL;DR: The paper introduces a multi-agent system that improves automated fact-checking by enhancing accuracy, efficiency, and explainability, achieving a 12.3% performance increase over baseline models.


<details>
  <summary>Details</summary>
Motivation: The work is motivated by the rapid spread of digital misinformation and the limitations of traditional and automated fact-checking methods in handling complex claims, ensuring credibility, and maintaining transparency.

Method: The authors propose a multi-agent system comprising four specialized agents: Input Ingestion for claim decomposition, Query Generation for subqueries, Evidence Retrieval for trustworthy sources, and Verdict Prediction for human-readable veracity judgments.

Result: On benchmark datasets (FEVEROUS, HOVER, SciFact), the system delivered a 12.3% improvement in Macro F1-score compared to baseline methods.

Conclusion: The proposed automated fact-checking system successfully balances scalability and reliability, closely mirrors human fact-checking practices, and offers transparent and interpretable decisions. Its source code is publicly available.

Abstract: The rapid spread of misinformation in the digital era poses significant
challenges to public discourse, necessitating robust and scalable fact-checking
solutions. Traditional human-led fact-checking methods, while credible,
struggle with the volume and velocity of online content, prompting the
integration of automated systems powered by Large Language Models (LLMs).
However, existing automated approaches often face limitations, such as handling
complex claims, ensuring source credibility, and maintaining transparency. This
paper proposes a novel multi-agent system for automated fact-checking that
enhances accuracy, efficiency, and explainability. The system comprises four
specialized agents: an Input Ingestion Agent for claim decomposition, a Query
Generation Agent for formulating targeted subqueries, an Evidence Retrieval
Agent for sourcing credible evidence, and a Verdict Prediction Agent for
synthesizing veracity judgments with human-interpretable explanations.
Evaluated on benchmark datasets (FEVEROUS, HOVER, SciFact), the proposed system
achieves a 12.3% improvement in Macro F1-score over baseline methods. The
system effectively decomposes complex claims, retrieves reliable evidence from
trusted sources, and generates transparent explanations for verification
decisions. Our approach contributes to the growing field of automated
fact-checking by providing a more accurate, efficient, and transparent
verification methodology that aligns with human fact-checking practices while
maintaining scalability for real-world applications. Our source code is
available at https://github.com/HySonLab/FactAgent

</details>


### [20] [Leveraging Large Language Model for Intelligent Log Processing and Autonomous Debugging in Cloud AI Platforms](https://arxiv.org/abs/2506.17900)
*Cheng Ji,Huaiying Luo*

Main category: cs.AI

TL;DR: The paper proposes "Intelligent Debugger (LLM-ID)," an AI-driven framework using large language models to process and debug massive logs in cloud platforms efficiently, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address complications in fault location and self-repair driven by unstructured and ambiguous log data from large-scale AI systems.

Method: Developed a framework leveraging pre-trained Transformer-based models, dynamic structuring, fine-tuned LLM with multi-round attention, and reinforcement learning-based recovery planning.

Result: Experiments show a 16.2% improvement in fault location accuracy compared to mainstream approaches.

Conclusion: LLM-ID exhibits enhanced semantic understanding, continuous learning, and adaptability for debugging cloud platform environments.

Abstract: With the increasing complexity and rapid expansion of the scale of AI systems
in cloud platforms, the log data generated during system operation is massive,
unstructured, and semantically ambiguous, which brings great challenges to
fault location and system self-repair. In order to solve this problem, this
paper proposes an intelligent log processing and automatic debugging framework
based on Large Language Model (LLM), named Intelligent Debugger (LLM-ID). This
method is extended on the basis of the existing pre-trained Transformer model,
and integrates a multi-stage semantic inference mechanism to realize the
context understanding of system logs and the automatic reconstruction of fault
chains. Firstly, the system log is dynamically structured, and the unsupervised
clustering and embedding mechanism is used to extract the event template and
semantic schema. Subsequently, the fine-tuned LLM combined with the multi-round
attention mechanism to perform contextual reasoning on the log sequence to
generate potential fault assumptions and root cause paths. Furthermore, this
paper introduces a reinforcement learning-based policy-guided recovery planner,
which is driven by the remediation strategy generated by LLM to support dynamic
decision-making and adaptive debugging in the cloud environment. Compared with
the existing rule engine or traditional log analysis system, the proposed model
has stronger semantic understanding ability, continuous learning ability and
heterogeneous environment adaptability. Experiments on the cloud platform log
dataset show that LLM-ID improves the fault location accuracy by 16.2%, which
is significantly better than the current mainstream methods

</details>


### [21] [Learning, Reasoning, Refinement: A Framework for Kahneman's Dual-System Intelligence in GUI Agents](https://arxiv.org/abs/2506.17913)
*Jinjie Wei,Jiyao Liu,Lihao Liu,Ming Hu,Junzhi Ning,Mingcheng Li,Weijie Yin,Junjun He,Xiao Liang,Chao Feng,Dingkang Yang*

Main category: cs.AI

TL;DR: CogniGUI is proposed as a cognitive framework for GUI automation, addressing limitations of existing systems by enabling adaptive learning and introducing iterative mastery cycles.


<details>
  <summary>Details</summary>
Motivation: Current GUI agents rely heavily on trial-and-error methods and simplistic evaluation metrics, lacking adaptive and human-like learning capabilities.

Method: CogniGUI employs a dual-system design inspired by Dual Process Theory, combining an omni parser engine for quick parsing and GRPO grounding agent for efficient interaction path assessment.

Result: CogniGUI outperformed state-of-the-art methods on both existing GUI benchmarks and the newly introduced ScreenSeek benchmark.

Conclusion: CogniGUI advances GUI agent systems with adaptive learning resembling human behavior, showing promise in real-world applications and challenging benchmarks.

Abstract: Graphical User Interface (GUI) agents have made significant progress in
automating digital tasks through the utilization of computer vision and
language models. Nevertheless, existing agent systems encounter notable
limitations. Firstly, they predominantly depend on trial and error decision
making rather than progressive reasoning, thereby lacking the capability to
learn and adapt from interactive encounters. Secondly, these systems are
assessed using overly simplistic single step accuracy metrics, which do not
adequately reflect the intricate nature of real world GUI interactions. In this
paper, we present CogniGUI, a cognitive framework developed to overcome these
limitations by enabling adaptive learning for GUI automation resembling
human-like behavior. Inspired by Kahneman's Dual Process Theory, our approach
combines two main components: (1) an omni parser engine that conducts immediate
hierarchical parsing of GUI elements through quick visual semantic analysis to
identify actionable components, and (2) a Group based Relative Policy
Optimization (GRPO) grounding agent that assesses multiple interaction paths
using a unique relative reward system, promoting minimal and efficient
operational routes. This dual-system design facilitates iterative ''exploration
learning mastery'' cycles, enabling the agent to enhance its strategies over
time based on accumulated experience. Moreover, to assess the generalization
and adaptability of agent systems, we introduce ScreenSeek, a comprehensive
benchmark that includes multi application navigation, dynamic state
transitions, and cross interface coherence, which are often overlooked
challenges in current benchmarks. Experimental results demonstrate that
CogniGUI surpasses state-of-the-art methods in both the current GUI grounding
benchmarks and our newly proposed benchmark.

</details>


### [22] [medicX-KG: A Knowledge Graph for Pharmacists' Drug Information Needs](https://arxiv.org/abs/2506.17959)
*Lizzy Farrugia,Lilian M. Azzopardi,Jeremy Debattista,Charlie Abela*

Main category: cs.AI

TL;DR: This paper introduces medicX-KG, a pharmacist-focused knowledge graph to support clinical and regulatory decisions by integrating various drug-related datasets.


<details>
  <summary>Details</summary>
Motivation: To address the challenges pharmacists face, such as reliance on fragmented medicinal information sources, and to enhance their decision-making capabilities by leveraging AI and semantic technologies.

Method: Developed a knowledge graph (medicX-KG) by integrating data from three sources: British National Formulary, DrugBank, and Malta Medicines Authority, informed by pharmacist interviews to ensure practical relevance.

Result: The medicX-KG successfully facilitated queries related to drug availability, interactions, adverse reactions, and therapeutic classes, reducing reliance on fragmented data sources.

Conclusion: The medicX-KG effectively aids pharmacists in decision-making, despite some limitations like missing dosage details and real-time updates, with proposed future improvements for refinement.

Abstract: The role of pharmacists is evolving from medicine dispensing to delivering
comprehensive pharmaceutical services within multidisciplinary healthcare
teams. Central to this shift is access to accurate, up-to-date medicinal
product information supported by robust data integration. Leveraging artificial
intelligence and semantic technologies, Knowledge Graphs (KGs) uncover hidden
relationships and enable data-driven decision-making. This paper presents
medicX-KG, a pharmacist-oriented knowledge graph supporting clinical and
regulatory decisions. It forms the semantic layer of the broader medicX
platform, powering predictive and explainable pharmacy services. medicX-KG
integrates data from three sources, including, the British National Formulary
(BNF), DrugBank, and the Malta Medicines Authority (MMA) that addresses Malta's
regulatory landscape and combines European Medicines Agency alignment with
partial UK supply dependence. The KG tackles the absence of a unified national
drug repository, reducing pharmacists' reliance on fragmented sources. Its
design was informed by interviews with practicing pharmacists to ensure
real-world applicability. We detail the KG's construction, including data
extraction, ontology design, and semantic mapping. Evaluation demonstrates that
medicX-KG effectively supports queries about drug availability, interactions,
adverse reactions, and therapeutic classes. Limitations, including missing
detailed dosage encoding and real-time updates, are discussed alongside
directions for future enhancements.

</details>


### [23] [Graphs Meet AI Agents: Taxonomy, Progress, and Future Opportunities](https://arxiv.org/abs/2506.18019)
*Yuanchen Bei,Weizhi Zhang,Siwen Wang,Weizhi Chen,Sheng Zhou,Hao Chen,Yong Li,Jiajun Bu,Shirui Pan,Yizhou Yu,Irwin King,Fakhri Karray,Philip S. Yu*

Main category: cs.AI

TL;DR: The paper presents a systematic review of how graph techniques can enhance AI agents, exploring their integration, applications, and future research directions.


<details>
  <summary>Details</summary>
Motivation: AI agents must contend with complex tasks requiring structured data for better planning, memory management, and coordination, where graphs can organize intricate relationships effectively.

Method: The paper surveys existing integration of graph-based techniques with AI agents and provides insights into their core functionalities, applications, and future research opportunities.

Result: Graphs are identified as a powerful paradigm to structurize data for advanced AI agent capabilities, with potential applications and research prospects outlined.

Conclusion: Integrating graph techniques can significantly enhance AI agents’ ability to process complex tasks, suggesting promising directions for next-generation AI development.

Abstract: AI agents have experienced a paradigm shift, from early dominance by
reinforcement learning (RL) to the rise of agents powered by large language
models (LLMs), and now further advancing towards a synergistic fusion of RL and
LLM capabilities. This progression has endowed AI agents with increasingly
strong abilities. Despite these advances, to accomplish complex real-world
tasks, agents are required to plan and execute effectively, maintain reliable
memory, and coordinate smoothly with other agents. Achieving these capabilities
involves contending with ever-present intricate information, operations, and
interactions. In light of this challenge, data structurization can play a
promising role by transforming intricate and disorganized data into
well-structured forms that agents can more effectively understand and process.
In this context, graphs, with their natural advantage in organizing, managing,
and harnessing intricate data relationships, present a powerful data paradigm
for structurization to support the capabilities demanded by advanced AI agents.
To this end, this survey presents a first systematic review of how graphs can
empower AI agents. Specifically, we explore the integration of graph techniques
with core agent functionalities, highlight notable applications, and identify
prospective avenues for future research. By comprehensively surveying this
burgeoning intersection, we hope to inspire the development of next-generation
AI agents equipped to tackle increasingly sophisticated challenges with graphs.
Related resources are collected and continuously updated for the community in
the Github link.

</details>


### [24] [Action Language BC+](https://arxiv.org/abs/2506.18044)
*Joseph Babb,Joohyung Lee*

Main category: cs.AI

TL;DR: This paper proposes a new action language, BC+, which bridges older action languages and modern ASP, incorporating advanced ASP capabilities into action language modeling.


<details>
  <summary>Details</summary>
Motivation: The authors aim to align action languages with the features of modern Answer Set Programming (ASP), which includes advanced constructs for effective knowledge representation.

Method: They define the semantics of BC+ using the general stable model semantics for propositional formulas, leveraging modern ASP constructs as propositional formula shorthands.

Result: BC+ incorporates features of various action languages (B, C, C+, and BC) and is compatible with computational methods in existing ASP solvers, enabling an implementation via an extension of system cplus2asp.

Conclusion: BC+ successfully bridges the gap between traditional action languages and modern ASP, enriching representational capabilities while maintaining computational applicability.

Abstract: Action languages are formal models of parts of natural language that are
designed to describe effects of actions. Many of these languages can be viewed
as high level notations of answer set programs structured to represent
transition systems. However, the form of answer set programs considered in the
earlier work is quite limited in comparison with the modern Answer Set
Programming (ASP) language, which allows several useful constructs for
knowledge representation, such as choice rules, aggregates, and abstract
constraint atoms. We propose a new action language called BC+, which closes the
gap between action languages and the modern ASP language. The main idea is to
define the semantics of BC+ in terms of general stable model semantics for
propositional formulas, under which many modern ASP language constructs can be
identified with shorthands for propositional formulas. Language BC+ turns out
to be sufficiently expressive to encompass the best features of other action
languages, such as languages B, C, C+, and BC. Computational methods available
in ASP solvers are readily applicable to compute BC+, which led to an
implementation of the language by extending system cplus2asp.

</details>


### [25] [Weighted Assumption Based Argumentation to reason about ethical principles and actions](https://arxiv.org/abs/2506.18056)
*Paolo Baldi,Fabio Aurelio D'Asaro,Abeer Dyoub,Francesca Alessandra Lisi*

Main category: cs.AI

TL;DR: The paper enhances Assumption Based Argumentation (ABA) by incorporating weighted argumentation and demonstrates its application in ethical reasoning through examples and an Answer Set Programming-based implementation.


<details>
  <summary>Details</summary>
Motivation: To improve the expressiveness and practical applicability of ABA in fields like ethical reasoning by integrating weight-based evaluations.

Method: Weights are assigned to arguments, and the weights of attacks between these arguments are determined. Demonstrations are provided in ethical reasoning settings, and a computational implementation is developed using Answer Set Programming.

Result: Weighted argumentation is successfully integrated with ABA, illustrated through ethical reasoning examples, and supported by an Answer Set Programming-based implementation.

Conclusion: The integration of weighted argumentation into ABA enhances its utility, particularly in areas requiring nuanced reasoning like ethical decision-making.

Abstract: We augment Assumption Based Argumentation (ABA for short) with weighted
argumentation. In a nutshell, we assign weights to arguments and then derive
the weight of attacks between ABA arguments. We illustrate our proposal through
running examples in the field of ethical reasoning, and present an
implementation based on Answer Set Programming.

</details>


### [26] [Deep Research Agents: A Systematic Examination And Roadmap](https://arxiv.org/abs/2506.18096)
*Yuxuan Huang,Yihang Chen,Haozheng Zhang,Kang Li,Meng Fang,Linyi Yang,Xiaoguang Li,Lifeng Shang,Songcen Xu,Jianye Hao,Kun Shao,Jun Wang*

Main category: cs.AI

TL;DR: The paper analyzes Deep Research (DR) agents, autonomous AI systems designed for complex, multi-step informational tasks, detailing their components, workflows, challenges, and future areas of research.


<details>
  <summary>Details</summary>
Motivation: To understand the underlying technologies that power Deep Research agents and improve their efficiency in handling complex research tasks.

Method: The authors categorize information acquisition strategies, evaluate modular tool-use frameworks, propose a taxonomy for workflows and agent architectures, and critically assess benchmarks in DR agents.

Result: The paper identifies limitations in benchmarks, workflow inefficiencies, and alignment issues with evaluation metrics, while providing a systematic framework and taxonomy for DR agents.

Conclusion: The study highlights open challenges and potential research directions for advancing the capabilities and evaluation standards of Deep Research agents.

Abstract: The rapid progress of Large Language Models (LLMs) has given rise to a new
category of autonomous AI systems, referred to as Deep Research (DR) agents.
These agents are designed to tackle complex, multi-turn informational research
tasks by leveraging a combination of dynamic reasoning, adaptive long-horizon
planning, multi-hop information retrieval, iterative tool use, and the
generation of structured analytical reports. In this paper, we conduct a
detailed analysis of the foundational technologies and architectural components
that constitute Deep Research agents. We begin by reviewing information
acquisition strategies, contrasting API-based retrieval methods with
browser-based exploration. We then examine modular tool-use frameworks,
including code execution, multimodal input processing, and the integration of
Model Context Protocols (MCPs) to support extensibility and ecosystem
development. To systematize existing approaches, we propose a taxonomy that
differentiates between static and dynamic workflows, and we classify agent
architectures based on planning strategies and agent composition, including
single-agent and multi-agent configurations. We also provide a critical
evaluation of current benchmarks, highlighting key limitations such as
restricted access to external knowledge, sequential execution inefficiencies,
and misalignment between evaluation metrics and the practical objectives of DR
agents. Finally, we outline open challenges and promising directions for future
research. A curated and continuously updated repository of DR agent research is
available at: {https://github.com/ai-agents-2030/awesome-deep-research-agent}.

</details>


### [27] [Decentralized Consensus Inference-based Hierarchical Reinforcement Learning for Multi-Constrained UAV Pursuit-Evasion Game](https://arxiv.org/abs/2506.18126)
*Xiang Yuming,Li Sizhao,Li Rongpeng,Zhao Zhifeng,Zhang Honggang*

Main category: cs.AI

TL;DR: This paper introduces a hierarchical reinforcement learning framework (CI-HRL) to solve complex multi-UAV tasks like evasion and formation coverage in communication-limited environments.


<details>
  <summary>Details</summary>
Motivation: To address the significant challenges in managing multi-UAV systems in pursuit-evasion games, such as maximizing coverage while evading threats in communication-constrained scenarios.

Method: The authors propose CI-HRL, a two-level reinforcement learning framework. It combines a high-level policy using a novel multi-agent communication module (ConsMAC) for global awareness and a low-level policy leveraging alternative PPO and policy distillation for advanced control.

Result: Experimental results and SITL simulations demonstrated superior performance of CI-HRL in improving UAV swarm coordination for evasion and task completion.

Conclusion: CI-HRL effectively tackles the problem of cooperative evasion and formation coverage with enhanced swarm collaboration, even under constrained communication scenarios.

Abstract: Multiple quadrotor unmanned aerial vehicle (UAV) systems have garnered
widespread research interest and fostered tremendous interesting applications,
especially in multi-constrained pursuit-evasion games (MC-PEG). The Cooperative
Evasion and Formation Coverage (CEFC) task, where the UAV swarm aims to
maximize formation coverage across multiple target zones while collaboratively
evading predators, belongs to one of the most challenging issues in MC-PEG,
especially under communication-limited constraints. This multifaceted problem,
which intertwines responses to obstacles, adversaries, target zones, and
formation dynamics, brings up significant high-dimensional complications in
locating a solution. In this paper, we propose a novel two-level framework
(i.e., Consensus Inference-based Hierarchical Reinforcement Learning (CI-HRL)),
which delegates target localization to a high-level policy, while adopting a
low-level policy to manage obstacle avoidance, navigation, and formation.
Specifically, in the high-level policy, we develop a novel multi-agent
reinforcement learning module, Consensus-oriented Multi-Agent Communication
(ConsMAC), to enable agents to perceive global information and establish
consensus from local states by effectively aggregating neighbor messages.
Meanwhile, we leverage an Alternative Training-based Multi-agent proximal
policy optimization (AT-M) and policy distillation to accomplish the low-level
control. The experimental results, including the high-fidelity
software-in-the-loop (SITL) simulations, validate that CI-HRL provides a
superior solution with enhanced swarm's collaborative evasion and task
completion capabilities.

</details>


### [28] [SE-Merging: A Self-Enhanced Approach for Dynamic Model Merging](https://arxiv.org/abs/2506.18135)
*Zijun Chen,Zhanpeng Zhou,Bo Zhang,Weinan Zhang,Xi Sun,Junchi Yan*

Main category: cs.AI

TL;DR: The paper investigates the mechanisms behind model merging, shedding light on how interpolating task-specific model parameters can achieve multi-task adaptability. It introduces SE-Merging, a framework enhancing this process without extra training.


<details>
  <summary>Details</summary>
Motivation: Understanding the mechanisms of model merging is crucial because it offers the potential for efficient multi-task capabilities without the need for separate training for each task.

Method: The authors propose SE-Merging, a self-enhanced model merging framework. It dynamically identifies tasks per sample and adaptively adjusts merging coefficients to maintain task-specific expertise.

Result: SE-Merging yields significant improvements in multi-task performance, retains compatibility with existing model merging techniques, and achieves dynamic merging without requiring additional training.

Conclusion: SE-Merging offers a practical and effective model merging solution, enhancing task-specific expertise and paving the way for broader applications in multi-tasking without added computational overhead.

Abstract: Model merging has gained increasing attention due to its intriguing property:
interpolating the parameters of different task-specific fine-tuned models leads
to multi-task abilities. However, despite its empirical success, the underlying
mechanisms of model merging remain poorly understood. In this work, we delve
into the mechanism behind model merging from a representation perspective. Our
analysis reveals that model merging achieves multi-task abilities through two
key capabilities: i) distinguishing samples from different tasks, and ii)
adapting to the corresponding expert model for each sample. These two
capabilities allow the merged model to retain task-specific expertise, enabling
efficient multi-task adaptation. Building on these insights, we propose
\texttt{SE-Merging}, a self-enhanced model merging framework that leverages
these two characteristics to dynamically identify the corresponding task for
each sample and then adaptively rescales the merging coefficients to further
enhance task-specific expertise in the merged model. Notably,
\texttt{SE-Merging} achieves dynamic model merging without additional training.
Extensive experiments demonstrate that \texttt{SE-Merging} achieves significant
performance improvements while remaining compatible with existing model merging
techniques.

</details>


### [29] [CoachGPT: A Scaffolding-based Academic Writing Assistant](https://arxiv.org/abs/2506.18149)
*Fumian Chen,Sotheara Veng,Joshua Wilson,Xiaoming Li,Hui Fang*

Main category: cs.AI

TL;DR: CoachGPT is a web application leveraging large language models (LLMs) to offer personalized academic writing guidance, addressing traditional education limitations.


<details>
  <summary>Details</summary>
Motivation: Academic writing in a second language is challenging, and existing tools lack effective guidance, accessibility, or contextual accuracy.

Method: CoachGPT uses instructions from educators to craft sub-tasks and employs LLMs for real-time feedback in a scaffolded, task-driven structure.

Result: User studies confirm CoachGPT's usefulness and demonstrate LLMs' potential to enhance academic writing.

Conclusion: CoachGPT provides a unique, immersive experience with personalized feedback, addressing gaps in traditional writing assistance and promoting self-paced learning.

Abstract: Academic writing skills are crucial for students' success, but can feel
overwhelming without proper guidance and practice, particularly when writing in
a second language. Traditionally, students ask instructors or search
dictionaries, which are not universally accessible. Early writing assistants
emerged as rule-based systems that focused on detecting misspellings,
subject-verb disagreements, and basic punctuation errors; however, they are
inaccurate and lack contextual understanding. Machine learning-based assistants
demonstrate a strong ability for language understanding but are expensive to
train. Large language models (LLMs) have shown remarkable capabilities in
generating responses in natural languages based on given prompts. Still, they
have a fundamental limitation in education: they generate essays without
teaching, which can have detrimental effects on learning when misused. To
address this limitation, we develop CoachGPT, which leverages large language
models (LLMs) to assist individuals with limited educational resources and
those who prefer self-paced learning in academic writing. CoachGPT is an AI
agent-based web application that (1) takes instructions from experienced
educators, (2) converts instructions into sub-tasks, and (3) provides real-time
feedback and suggestions using large language models. This unique scaffolding
structure makes CoachGPT unique among existing writing assistants. Compared to
existing writing assistants, CoachGPT provides a more immersive writing
experience with personalized feedback and guidance. Our user studies prove the
usefulness of CoachGPT and the potential of large language models for academic
writing.

</details>


### [30] [AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology](https://arxiv.org/abs/2506.18156)
*Akash Kundu,Rishika Goswami*

Main category: cs.AI

TL;DR: The paper evaluates whether Large Language Models display human-like cognitive patterns using psychological frameworks, finding alignment in several behaviors but influenced by their training.


<details>
  <summary>Details</summary>
Motivation: Understanding if and how LLMs reflect human cognitive tendencies helps assess their reliability, ethical considerations, and alignment.

Method: The investigation applied psychological tests like TAT, Framing Bias, Moral Foundations Theory, and Cognitive Dissonance using structured prompts and automated scoring.

Result: LLMs exhibit patterns similar to human cognition such as susceptibility to framing, moral judgments aligned with Liberty/Oppression, and tempered self-contradictions.

Conclusion: LLMs demonstrate human-like behaviors shaped by training data, raising implications for transparency, ethical use, and integrating psychology into AI safety research.

Abstract: We investigate whether Large Language Models (LLMs) exhibit human-like
cognitive patterns under four established frameworks from psychology: Thematic
Apperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and
Cognitive Dissonance. We evaluated several proprietary and open-source models
using structured prompts and automated scoring. Our findings reveal that these
models often produce coherent narratives, show susceptibility to positive
framing, exhibit moral judgments aligned with Liberty/Oppression concerns, and
demonstrate self-contradictions tempered by extensive rationalization. Such
behaviors mirror human cognitive tendencies yet are shaped by their training
data and alignment methods. We discuss the implications for AI transparency,
ethical deployment, and future work that bridges cognitive psychology and AI
safety

</details>


### [31] [Chain-of-Memory: Enhancing GUI Agents for Cross-Application Navigation](https://arxiv.org/abs/2506.18158)
*Xinzge Gao,Chuanrui Hu,Bin Chen,Teng Li*

Main category: cs.AI

TL;DR: The paper introduces Chain-of-Memory (CoM) to improve memory management and task performance in GUI agents and demonstrates its effectiveness using a novel dataset.


<details>
  <summary>Details</summary>
Motivation: Existing GUI agents struggle with understanding task states and lack mechanisms to store critical information for complex cross-app tasks.

Method: The CoM approach combines explicit memory representations by capturing action descriptions and task-relevant screen info, along with a dedicated memory module. A new dataset, GUI Odyssey-CoM, supports this development.

Result: CoM enhances GUI agents' cross-application task performance, enabling even smaller 7B models to achieve memory capabilities similar to larger 72B models.

Conclusion: Explicit memory modeling via CoM improves GUI agents' understanding and performance, and the dataset/code will be made publicly available for broader use.

Abstract: Multimodal large language models (MLLMs) are attracting growing attention in
the development of Graphical User Interface (GUI) agents. Existing approaches
often rely on historical screenshots or actions to implicitly represent the
task state. This reliance poses challenges for GUI agents in accurately
understanding task states and underscores the absence of effective mechanisms
to store critical information in complex and lengthy cross-app tasks. To
address these challenges, we propose Chain-of-Memory (CoM), a novel approach
for explicitly modeling short-term and long-term memory in GUI agents. CoM
achieves this by capturing action descriptions, integrating task-relevant
screen information, and maintaining a dedicated memory module to store and
manage this information. By leveraging explicit memory representations, CoM
enables GUI agents to better understand task states and retain critical
historical information persistently. To equip GUI agents with memory management
capabilities and evaluate the effectiveness of CoM, we developed the GUI
Odyssey-CoM, a dataset comprising 111k screen-action pairs annotated with
Chain-of-Memory. Experimental results demonstrate that CoM significantly
improves GUI agents' performance in cross-application tasks. Additionally, GUI
Odyssey-CoM enables 7B models to achieve memory management capabilities
comparable to 72B models. The dataset and code will be open-sourced.

</details>


### [32] [Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?](https://arxiv.org/abs/2506.18183)
*Zhiting Mei,Christina Zhang,Tenny Yin,Justin Lidard,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.AI

TL;DR: The paper examines how reasoning language models assess their own confidence about their responses, particularly exploring whether they are well-calibrated and how introspection impacts calibration.


<details>
  <summary>Details</summary>
Motivation: Understanding and controlling overconfidence and hallucinated responses in reasoning models is critical for their safe use in real-world applications.

Method: The study evaluates uncertainty quantification through introspection across diverse reasoning models and benchmarks, while considering factors like deeper reasoning and confidence self-assessment.

Result: Findings show reasoning models are generally overconfident, tend to become even more overconfident with deeper reasoning, and introspection aids calibration in certain setups but worsens it in others.

Conclusion: While introspection shows potential to improve calibration for some models, its effectiveness is inconsistent, signaling a need for dedicated UQ benchmarks and methods to enhance reasoning model calibration.

Abstract: Reasoning language models have set state-of-the-art (SOTA) records on many
challenging benchmarks, enabled by multi-step reasoning induced using
reinforcement learning. However, like previous language models, reasoning
models are prone to generating confident, plausible responses that are
incorrect (hallucinations). Knowing when and how much to trust these models is
critical to the safe deployment of reasoning models in real-world applications.
To this end, we explore uncertainty quantification of reasoning models in this
work. Specifically, we ask three fundamental questions: First, are reasoning
models well-calibrated? Second, does deeper reasoning improve model
calibration? Finally, inspired by humans' innate ability to double-check their
thought processes to verify the validity of their answers and their confidence,
we ask: can reasoning models improve their calibration by explicitly reasoning
about their chain-of-thought traces? We introduce introspective uncertainty
quantification (UQ) to explore this direction. In extensive evaluations on SOTA
reasoning models across a broad range of benchmarks, we find that reasoning
models: (i) are typically overconfident, with self-verbalized confidence
estimates often greater than 85% particularly for incorrect responses, (ii)
become even more overconfident with deeper reasoning, and (iii) can become
better calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not
uniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we
conclude with important research directions to design necessary UQ benchmarks
and improve the calibration of reasoning models.

</details>


### [33] [The Impact of Medication Non-adherence on Adverse Outcomes: Evidence from Schizophrenia Patients via Survival Analysis](https://arxiv.org/abs/2506.18187)
*Shahriar Noroozizadeh,Pim Welle,Jeremy C. Weiss,George H. Chen*

Main category: cs.AI

TL;DR: Non-adherence to antipsychotic medications is associated with earlier adverse outcomes in schizophrenia patients, advancing events by 1 to 4 months.


<details>
  <summary>Details</summary>
Motivation: To explore the impact of non-adherence to antipsychotic medications on adverse outcomes in schizophrenia patients and provide policy-relevant insights.

Method: Combined survival analysis with causal inference techniques (T-learner, S-learner, nearest neighbor matching) using Allegheny County data, examining time to adverse events.

Result: Non-adherence advances adverse outcomes by 1-4 months; effects are strengthened with confounder adjustments, and consistent associations are observed across medication types.

Conclusion: Adherence to antipsychotic medications is crucial in delaying psychiatric crises, and integrating survival analysis with causal inference offers valuable clinical insights, while acknowledging assumptions required for causality.

Abstract: This study quantifies the association between non-adherence to antipsychotic
medications and adverse outcomes in individuals with schizophrenia. We frame
the problem using survival analysis, focusing on the time to the earliest of
several adverse events (early death, involuntary hospitalization, jail
booking). We extend standard causal inference methods (T-learner, S-learner,
nearest neighbor matching) to utilize various survival models to estimate
individual and average treatment effects, where treatment corresponds to
medication non-adherence. Analyses are repeated using different amounts of
longitudinal information (3, 6, 9, and 12 months). Using data from Allegheny
County in western Pennsylvania, we find strong evidence that non-adherence
advances adverse outcomes by approximately 1 to 4 months. Ablation studies
confirm that county-provided risk scores adjust for key confounders, as their
removal amplifies the estimated effects. Subgroup analyses by medication
formulation (injectable vs. oral) and medication type consistently show that
non-adherence is associated with earlier adverse events. These findings
highlight the clinical importance of adherence in delaying psychiatric crises
and show that integrating survival analysis with causal inference tools can
yield policy-relevant insights. We caution that although we apply causal
inference, we only make associative claims and discuss assumptions needed for
causal interpretation.

</details>


### [34] [A Conceptual Framework for AI Capability Evaluations](https://arxiv.org/abs/2506.18213)
*María Victoria Carro,Denise Alejandra Mester,Francisca Gauna Selasco,Luca Nicolás Forziati Gangi,Matheo Sandleris Musa,Lola Ramos Pereyra,Mario Leiva,Juan Gustavo Corvalan,María Vanina Martinez,Gerardo Simari*

Main category: cs.AI

TL;DR: The paper presents a conceptual framework to enhance transparency and reliability in AI capability evaluations, addressing the current lack of clarity in this domain.


<details>
  <summary>Details</summary>
Motivation: There is an increasing demand for well-designed and transparent AI system evaluations to support governance and decision-making, as current approaches are often unclear and unreliable.

Method: The paper proposes a conceptual framework that systematizes the analysis of evaluation methods and terminology without introducing new taxonomies or rigid structures.

Result: The framework facilitates transparency, comparability, and interpretability, allowing for the identification of methodological weaknesses and aiding the design of better evaluations.

Conclusion: The proposed framework benefits researchers, practitioners, and policymakers by enhancing the reliability and utility of AI system evaluations for diverse applications.

Abstract: As AI systems advance and integrate into society, well-designed and
transparent evaluations are becoming essential tools in AI governance,
informing decisions by providing evidence about system capabilities and risks.
Yet there remains a lack of clarity on how to perform these assessments both
comprehensively and reliably. To address this gap, we propose a conceptual
framework for analyzing AI capability evaluations, offering a structured,
descriptive approach that systematizes the analysis of widely used methods and
terminology without imposing new taxonomies or rigid formats. This framework
supports transparency, comparability, and interpretability across diverse
evaluations. It also enables researchers to identify methodological weaknesses,
assists practitioners in designing evaluations, and provides policymakers with
an accessible tool to scrutinize, compare, and navigate complex evaluation
landscapes.

</details>


### [35] [The 4th Dimension for Scaling Model Size](https://arxiv.org/abs/2506.18233)
*Ruike Zhu,Hanwen Zhang,Tianyu Shi,Chi Wang,Tianyi Zhou,Zengyi Qin*

Main category: cs.AI

TL;DR: The paper introduces a fourth dimension, virtual logical depth (VLD), for scaling language models and investigates its potential to improve reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: To explore a parameter-efficient way to enhance reasoning capabilities of large language models without increasing their parameter count.

Method: Conducted controlled experiments to study the effects of virtual logical depth (parameter reuse) on reasoning and knowledge capacity in models.

Result: VLD scaling maintains the model's knowledge capacity while improving reasoning performance. It shows that reasoning capabilities can improve without increasing parameter count.

Conclusion: VLD scaling offers an effective alternative in model scaling to enhance reasoning without requiring more parameters, with consistent results across configurations.

Abstract: Scaling the size of large language models typically involves three
dimensions: depth, width, and the number of parameters. In this work, we
explore a fourth dimension, virtual logical depth (VLD), which increases the
effective algorithmic depth without changing the overall parameter count by
reusing parameters within the model. Although parameter reuse is not a new
concept, its potential and characteristics in model scaling have not been
thoroughly studied. Through carefully designed controlled experiments, we make
the following key discoveries regarding VLD scaling:
  VLD scaling forces the knowledge capacity of the model to remain almost
constant, with only minor variations.
  VLD scaling enables a significant improvement in reasoning capability,
provided the scaling method is properly implemented.
  The number of parameters correlates with knowledge capacity, but not with
reasoning capability. Under certain conditions, it is not necessary to increase
the parameter count to enhance reasoning.
  These findings are consistent across various model configurations and are
likely to be generally valid within the scope of our experiments.

</details>


### [36] [Advanced For-Loop for QML algorithm search](https://arxiv.org/abs/2506.18260)
*FuTe Wong*

Main category: cs.AI

TL;DR: This paper proposes a framework using Multi-Agent Systems with Large Language Models to automate the search and optimization of Quantum Machine Learning algorithms.


<details>
  <summary>Details</summary>
Motivation: To efficiently adapt classical machine learning algorithms for quantum computing, enabling systematic exploration and development of Quantum Machine Learning algorithms.

Method: The framework utilizes Large Language Model-based Multi-Agent Systems to iteratively generate and refine quantum adaptations of classical machine learning algorithms.

Result: A proof of concept demonstrating the framework's potential to effectively explore and adapt classical machine learning methods for quantum computing.

Conclusion: The proposed framework paves the way for efficient automation in QML algorithm development while suggesting future exploration in planning mechanisms and optimization strategies.

Abstract: This paper introduces an advanced framework leveraging Large Language
Model-based Multi-Agent Systems (LLMMA) for the automated search and
optimization of Quantum Machine Learning (QML) algorithms. Inspired by Google
DeepMind's FunSearch, the proposed system works on abstract level to
iteratively generates and refines quantum transformations of classical machine
learning algorithms (concepts), such as the Multi-Layer Perceptron,
forward-forward and backpropagation algorithms. As a proof of concept, this
work highlights the potential of agentic frameworks to systematically explore
classical machine learning concepts and adapt them for quantum computing,
paving the way for efficient and automated development of QML algorithms.
Future directions include incorporating planning mechanisms and optimizing
strategy in the search space for broader applications in quantum-enhanced
machine learning.

</details>


### [37] [Dynamic Knowledge Exchange and Dual-diversity Review: Concisely Unleashing the Potential of a Multi-Agent Research Team](https://arxiv.org/abs/2506.18348)
*Weilun Yu,Shixiang Tang,Yonggui Huang,Nanqing Dong,Li Fan,Honggang Qi,Wei Liu,Xiaoli Diao,Xi Chen,Wanli Ouyang*

Main category: cs.AI

TL;DR: The paper introduces IDVSCI, a framework using LLMs to emulate collaboration among researchers, achieving superior results in generating impactful scientific ideas via iterative feedback and peer review dynamics.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based scientist systems show promise but lack interactive reasoning and evaluation mechanisms essential for real-world scientific collaboration.

Method: The authors developed the IDVSCI framework, incorporating a Dynamic Knowledge Exchange mechanism for iterative feedback among LLM agents and a Dual-Diversity Review paradigm for heterogeneous expert evaluation.

Result: Experiments on datasets in computer science and health sciences show IDVSCI outperforms existing systems like AI Scientist and VIRSCI.

Conclusion: Modeling interaction and peer review dynamics in LLM-based frameworks significantly enhances autonomous research capabilities and scientific idea generation.

Abstract: Scientific progress increasingly relies on effective collaboration among
researchers, a dynamic that large language models (LLMs) have only begun to
emulate. While recent LLM-based scientist agents show promise in autonomous
scientific discovery, they often lack the interactive reasoning and evaluation
mechanisms essential to real-world research. We propose IDVSCI (Internal
Discussion and Vote SCIentists), a multi-agent framework built on LLMs that
incorporates two key innovations: a Dynamic Knowledge Exchange mechanism
enabling iterative feedback among agents, and a Dual-Diversity Review paradigm
that simulates heterogeneous expert evaluation. These components jointly
promote deeper reasoning and the generation of more creative and impactful
scientific ideas. To evaluate the effectiveness and generalizability of our
approach, we conduct experiments on two datasets: a widely used benchmark in
computer science and a new dataset we introduce in the health sciences domain.
Results show that IDVSCI consistently achieves the best performance across both
datasets, outperforming existing systems such as AI Scientist and VIRSCI. These
findings highlight the value of modeling interaction and peer review dynamics
in LLM-based autonomous research.

</details>


### [38] [A Large Language Model-based Multi-Agent Framework for Analog Circuits' Sizing Relationships Extraction](https://arxiv.org/abs/2506.18424)
*Chengjie Liu,Weiyu Chen,Huiyao Xu,Yuan Du,Jun Yang,Li Du*

Main category: cs.AI

TL;DR: The paper introduces an LLM-based multi-agent framework to improve analog circuit sizing by extracting sizing relationships, leading to efficient search space pruning and optimization efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: Existing mathematical optimization methods for analog circuit sizing ignore automatic introduction of prior knowledge and fail to prune the search space, causing inefficiencies in design processes.

Method: A large language model-based multi-agent framework extracts analog circuit sizing relationships from academic papers, using this knowledge to effectively prune the search space during optimization.

Result: Tests conducted on three types of circuits showed optimization efficiency improvements between 2.32 to 26.6 times.

Conclusion: The use of LLMs for analog circuit design automation demonstrates an effective method for integrating AI with conventional approaches to enhance efficiency and streamline design processes.

Abstract: In the design process of the analog circuit pre-layout phase, device sizing
is an important step in determining whether an analog circuit can meet the
required performance metrics. Many existing techniques extract the circuit
sizing task as a mathematical optimization problem to solve and continuously
improve the optimization efficiency from a mathematical perspective. But they
ignore the automatic introduction of prior knowledge, fail to achieve effective
pruning of the search space, which thereby leads to a considerable compression
margin remaining in the search space. To alleviate this problem, we propose a
large language model (LLM)-based multi-agent framework for analog circuits'
sizing relationships extraction from academic papers. The search space in the
sizing process can be effectively pruned based on the sizing relationship
extracted by this framework. Eventually, we conducted tests on 3 types of
circuits, and the optimization efficiency was improved by $2.32 \sim 26.6
\times$. This work demonstrates that the LLM can effectively prune the search
space for analog circuit sizing, providing a new solution for the combination
of LLMs and conventional analog circuit design automation methods.

</details>


### [39] [How Robust is Model Editing after Fine-Tuning? An Empirical Study on Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.18428)
*Feng He,Zhenyang Liu,Marco Valentino,Zhixue Zhao*

Main category: cs.AI

TL;DR: This paper analyzes how model edits interact with fine-tuning, especially in T2I diffusion models, and reveals that edits typically fail to persist through fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Understanding the persistence of model edits through fine-tuning is essential for practical applications like bias mitigation and for addressing safety concerns regarding malicious edits.

Method: Researchers performed systematic investigations using two model families, two editing techniques, and three fine-tuning methods, assessing various editing tasks and metrics.

Result: Edits generally do not persist post-fine-tuning, with DoRA showing the strongest reversal effect. Among editing methods, UCE demonstrates higher robustness compared to ReFACT.

Conclusion: Current editing methods are limited in ensuring long-term persistence after fine-tuning, signaling the need for improved methodologies for reliable AI alignment and safety.

Abstract: Model editing offers a low-cost technique to inject or correct a particular
behavior in a pre-trained model without extensive retraining, supporting
applications such as factual correction and bias mitigation. Despite this
common practice, it remains unknown whether edits persist after fine-tuning or
whether they are inadvertently reversed. This question has fundamental
practical implications. For example, if fine-tuning removes prior edits, it
could serve as a defence mechanism against hidden malicious edits. Vice versa,
the unintended removal of edits related to bias mitigation could pose serious
safety concerns. We systematically investigate the interaction between model
editing and fine-tuning in the context of T2I diffusion models, which are known
to exhibit biases and generate inappropriate content. Our study spans two T2I
model families (Stable Diffusion and FLUX), two sota editing techniques, and
three fine-tuning methods (DreamBooth, LoRA, and DoRA). Through an extensive
empirical analysis across diverse editing tasks and evaluation metrics, our
findings reveal a trend: edits generally fail to persist through fine-tuning,
even when fine-tuning is tangential or unrelated to the edits. Notably, we
observe that DoRA exhibits the strongest edit reversal effect. At the same
time, among editing methods, UCE demonstrates greater robustness, retaining
significantly higher efficacy post-fine-tuning compared to ReFACT. These
findings highlight a crucial limitation in current editing methodologies,
emphasizing the need for more robust techniques to ensure reliable long-term
control and alignment of deployed AI systems. These findings have dual
implications for AI safety: they suggest that fine-tuning could serve as a
remediation mechanism for malicious edits while simultaneously highlighting the
need for re-editing after fine-tuning to maintain beneficial safety and
alignment properties.

</details>


### [40] [Standard Applicability Judgment and Cross-jurisdictional Reasoning: A RAG-based Framework for Medical Device Compliance](https://arxiv.org/abs/2506.18511)
*Yu Han,Aaron Ceross,Jeroen H. M. Bergmann*

Main category: cs.AI

TL;DR: The paper proposes a modular AI system using a retrieval-augmented generation (RAG) pipeline to determine jurisdiction-specific regulatory standard applicability for medical devices, achieving promising accuracy and recall.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the critical yet understudied challenge of identifying applicable regulatory standards for medical devices, which often requires expert interpretation of fragmented, heterogeneous documentation across jurisdictions.

Method: The authors developed a modular AI system leveraging a retrieval-augmented generation (RAG) pipeline. The system retrieves candidate standards from curated datasets and uses large language models to determine jurisdiction-specific relevance, providing traceable justifications. They also created an international benchmark dataset for evaluation.

Result: The system achieves a classification accuracy of 73% and a Top-5 retrieval recall of 87%, outperforming retrieval-only, zero-shot, and rule-based baselines.

Conclusion: The study presents the first scalable, interpretable, end-to-end system for reasoning about regulatory standards, capable of cross-jurisdictional applicability, and highlights its potential to enhance regulatory science using AI.

Abstract: Identifying the appropriate regulatory standard applicability remains a
critical yet understudied challenge in medical device compliance, frequently
necessitating expert interpretation of fragmented and heterogeneous
documentation across different jurisdictions. To address this challenge, we
introduce a modular AI system that leverages a retrieval-augmented generation
(RAG) pipeline to automate standard applicability determination. Given a
free-text device description, our system retrieves candidate standards from a
curated corpus and uses large language models to infer jurisdiction-specific
applicability, classified as Mandatory, Recommended, or Not Applicable, with
traceable justifications. We construct an international benchmark dataset of
medical device descriptions with expert-annotated standard mappings, and
evaluate our system against retrieval-only, zero-shot, and rule-based
baselines. The proposed approach attains a classification accuracy of 73% and a
Top-5 retrieval recall of 87%, demonstrating its effectiveness in identifying
relevant regulatory standards. We introduce the first end-to-end system for
standard applicability reasoning, enabling scalable and interpretable
AI-supported regulatory science. Notably, our region-aware RAG agent performs
cross-jurisdictional reasoning between Chinese and U.S. standards, supporting
conflict resolution and applicability justification across regulatory
frameworks.

</details>


### [41] [A Question Bank to Assess AI Inclusivity: Mapping out the Journey from Diversity Errors to Inclusion Excellence](https://arxiv.org/abs/2506.18538)
*Rifat Ara Shams,Didar Zowghi,Muneera Bano*

Main category: cs.AI

TL;DR: The paper presents a structured AI inclusivity question bank consisting of 253 questions to assess AI systems' alignment with diversity and inclusion (D&I) principles.


<details>
  <summary>Details</summary>
Motivation: Existing AI risk assessment frameworks often neglect the aspect of inclusivity, creating a need for standardized tools to evaluate AI systems against D&I principles.

Method: The question bank was developed through an iterative, multi-source approach drawing from literature reviews, D&I guidelines, Responsible AI frameworks, and a simulated user study involving 70 AI-generated personas from diverse roles.

Result: The simulated evaluation demonstrated the question bank's relevance and effectiveness in assessing AI inclusivity across various roles and domains, highlighting the need for embedding D&I principles into AI workflows and governance.

Conclusion: The question bank provides a practical and systematic tool to promote equity and responsibility in AI systems by enabling researchers, practitioners, and policymakers to assess and enhance AI inclusivity.

Abstract: Ensuring diversity and inclusion (D&I) in artificial intelligence (AI) is
crucial for mitigating biases and promoting equitable decision-making. However,
existing AI risk assessment frameworks often overlook inclusivity, lacking
standardized tools to measure an AI system's alignment with D&I principles.
This paper introduces a structured AI inclusivity question bank, a
comprehensive set of 253 questions designed to evaluate AI inclusivity across
five pillars: Humans, Data, Process, System, and Governance. The development of
the question bank involved an iterative, multi-source approach, incorporating
insights from literature reviews, D&I guidelines, Responsible AI frameworks,
and a simulated user study. The simulated evaluation, conducted with 70
AI-generated personas related to different AI jobs, assessed the question
bank's relevance and effectiveness for AI inclusivity across diverse roles and
application domains. The findings highlight the importance of integrating D&I
principles into AI development workflows and governance structures. The
question bank provides an actionable tool for researchers, practitioners, and
policymakers to systematically assess and enhance the inclusivity of AI
systems, paving the way for more equitable and responsible AI technologies.

</details>


### [42] [T-CPDL: A Temporal Causal Probabilistic Description Logic for Developing Logic-RAG Agent](https://arxiv.org/abs/2506.18559)
*Hong Qing Yu*

Main category: cs.AI

TL;DR: This paper introduces T-CPDL, a framework improving large language models' ability to handle structured reasoning tasks involving temporal, causal, and probabilistic constraints.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with reasoning tasks requiring understanding temporal constraints, causal relationships, and probabilistic reasoning, limiting their reliability in structured decision-making.

Method: The authors propose two variants of T-CPDL that extend Description Logic with temporal and causal relationships, using Allen’s interval algebra and timestamped causal assertions to enable structured reasoning. The logical structure of T-CPDL supports tasks from temporal ordering to probabilistic causation.

Result: Through empirical evaluations, T-CPDL shows improved accuracy, interpretability, and confidence calibration in benchmarks for temporal reasoning and causal inference.

Conclusion: T-CPDL enhances reasoning capabilities in large language models, paving the way for more robust, explainable decision-making, and sets a foundation for Logic-RAG frameworks to improve knowledge graph integration.

Abstract: Large language models excel at generating fluent text but frequently struggle
with structured reasoning involving temporal constraints, causal relationships,
and probabilistic reasoning. To address these limitations, we propose Temporal
Causal Probabilistic Description Logic (T-CPDL), an integrated framework that
extends traditional Description Logic with temporal interval operators,
explicit causal relationships, and probabilistic annotations. We present two
distinct variants of T-CPDL: one capturing qualitative temporal relationships
through Allen's interval algebra, and another variant enriched with explicit
timestamped causal assertions. Both variants share a unified logical structure,
enabling complex reasoning tasks ranging from simple temporal ordering to
nuanced probabilistic causation. Empirical evaluations on temporal reasoning
and causal inference benchmarks confirm that T-CPDL substantially improves
inference accuracy, interpretability, and confidence calibration of language
model outputs. By delivering transparent reasoning paths and fine-grained
temporal and causal semantics, T-CPDL significantly enhances the capability of
language models to support robust, explainable, and trustworthy
decision-making. This work also lays the groundwork for developing advanced
Logic-Retrieval-Augmented Generation (Logic-RAG) frameworks, potentially
boosting the reasoning capabilities and efficiency of knowledge graph-enhanced
RAG systems.

</details>


### [43] [Airalogy: AI-empowered universal data digitization for research automation](https://arxiv.org/abs/2506.18586)
*Zijie Yang,Qiji Zhou,Fang Guo,Sijie Zhang,Yexun Xi,Jinglei Nie,Yudian Zhu,Liping Huang,Chou Wu,Yonghe Xia,Xiaoyu Ma,Yingming Pu,Panzhong Lu,Junshu Pan,Mingtao Chen,Tiannan Guo,Yanmei Dou,Hongyu Chen,Anping Zeng,Jiaxing Huang,Tian Xu,Yue Zhang*

Main category: cs.AI

TL;DR: The paper introduces Airalogy, a multidisciplinary AI-driven platform to address research data standardization challenges, enabling digitization and automation for diverse fields.


<details>
  <summary>Details</summary>
Motivation: Current AI applications are limited due to fragmented and non-standard research data collection methods, hindering AI empowerment across disciplines.

Method: The authors developed Airalogy, a platform blending scientific domain knowledge and computational expertise to create standardized, customizable research data records and an AI research copilot.

Result: Airalogy is successfully deployed across various domains at Westlake University, demonstrating its ability to standardize workflows and assist researchers.

Conclusion: Airalogy has the potential to accelerate scientific innovation globally by balancing data universality and standardization, benefiting research across academia and industry.

Abstract: Research data are the foundation of Artificial Intelligence (AI)-driven
science, yet current AI applications remain limited to a few fields with
readily available, well-structured, digitized datasets. Achieving comprehensive
AI empowerment across multiple disciplines is still out of reach. Present-day
research data collection is often fragmented, lacking unified standards,
inefficiently managed, and difficult to share. Creating a single platform for
standardized data digitization needs to overcome the inherent challenge of
balancing between universality (supporting the diverse, ever-evolving needs of
various disciplines) and standardization (enforcing consistent formats to fully
enable AI). No existing platform accommodates both facets. Building a truly
multidisciplinary platform requires integrating scientific domain knowledge
with sophisticated computing skills. Researchers often lack the computational
expertise to design customized and standardized data recording methods, whereas
platform developers rarely grasp the intricate needs of multiple scientific
domains. These gaps impede research data standardization and hamper AI-driven
progress. In this study, we address these challenges by developing Airalogy
(https://airalogy.com), the world's first AI- and community-driven platform
that balances universality and standardization for digitizing research data
across multiple disciplines. Airalogy represents entire research workflows
using customizable, standardized data records and offers an advanced AI
research copilot for intelligent Q&A, automated data entry, analysis, and
research automation. Already deployed in laboratories across all four schools
of Westlake University, Airalogy has the potential to accelerate and automate
scientific innovation in universities, industry, and the global research
community-ultimately benefiting humanity as a whole.

</details>


### [44] [AggTruth: Contextual Hallucination Detection using Aggregated Attention Scores in LLMs](https://arxiv.org/abs/2506.18628)
*Piotr Matys,Jan Eliasz,Konrad Kiełczyński,Mikołaj Langner,Teddy Ferdinan,Jan Kocoń,Przemysław Kazienko*

Main category: cs.AI

TL;DR: AggTruth, a method that detects contextual hallucinations in LLMs by analyzing attention score distributions, proves effective across tasks and surpasses state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the pervasive issue of hallucinations in Large Language Models in real-world applications, especially in retrieval-augmented generation settings.

Method: Proposes AggTruth, which detects hallucinations by aggregating attention scores using four distinct variants and analyzing their distributions. Also explores feature selection and the impact of attention head count.

Result: The method outperforms current state-of-the-art techniques in detecting contextual hallucinations in various tasks and setups.

Conclusion: AggTruth is a robust and effective approach for mitigating hallucinations in LLMs, emphasizing the importance of attention head selection for improved detection.

Abstract: In real-world applications, Large Language Models (LLMs) often hallucinate,
even in Retrieval-Augmented Generation (RAG) settings, which poses a
significant challenge to their deployment. In this paper, we introduce
AggTruth, a method for online detection of contextual hallucinations by
analyzing the distribution of internal attention scores in the provided context
(passage). Specifically, we propose four different variants of the method, each
varying in the aggregation technique used to calculate attention scores. Across
all LLMs examined, AggTruth demonstrated stable performance in both same-task
and cross-task setups, outperforming the current SOTA in multiple scenarios.
Furthermore, we conducted an in-depth analysis of feature selection techniques
and examined how the number of selected attention heads impacts detection
performance, demonstrating that careful selection of heads is essential to
achieve optimal results.

</details>


### [45] [Dual-level Behavioral Consistency for Inter-group and Intra-group Coordination in Multi-Agent Systems](https://arxiv.org/abs/2506.18651)
*Shuocun Yang,Huawen Hu,Enze Shi,Shu Zhang*

Main category: cs.AI

TL;DR: Dual-Level Behavioral Consistency (DLBC) is introduced to improve behavioral regulation in MARL by dynamically managing intra-group and inter-group diversity, proving effective in enhancing cooperation and specialization.


<details>
  <summary>Details</summary>
Motivation: To address the need for regulating behavioral consistency not just within groups but also across groups in MARL systems for improved performance.

Method: DLBC categorizes agents into groups and dynamically adjusts behavioral diversity intra-group (for cooperation) and inter-group (for task division).

Result: Experiments show that DLBC significantly improves cooperative performance within groups and task specialization between groups in various scenarios.

Conclusion: DLBC brings innovative approaches to controlling behavioral consistency in multi-agent systems and can be extended to more complex and dynamic environments.

Abstract: Behavioral diversity in Multi-agent reinforcement learning(MARL) represents
an emerging and promising research area. Prior work has largely centered on
intra-group behavioral consistency in multi-agent systems, with limited
attention given to behavioral consistency in multi-agent grouping scenarios. In
this paper, we introduce Dual-Level Behavioral Consistency (DLBC), a novel MARL
control method designed to explicitly regulate agent behaviors at both
intra-group and inter-group levels. DLBC partitions agents into distinct groups
and dynamically modulates behavioral diversity both within and between these
groups. By dynamically modulating behavioral diversity within and between these
groups, DLBC achieves enhanced division of labor through inter-group
consistency, which constrains behavioral strategies across different groups.
Simultaneously, intra-group consistency, achieved by aligning behavioral
strategies within each group, fosters stronger intra-group cooperation.
Crucially, DLBC's direct constraint of agent policy functions ensures its broad
applicability across various algorithmic frameworks. Experimental results in
various grouping cooperation scenarios demonstrate that DLBC significantly
enhances both intra-group cooperative performance and inter-group task
specialization, yielding substantial performance improvements. DLBC provides
new ideas for behavioral consistency control of multi-intelligent body systems,
and its potential for application in more complex tasks and dynamic
environments can be further explored in the future.

</details>


### [46] [Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training](https://arxiv.org/abs/2506.18777)
*Jonathan Cook,Silvia Sapora,Arash Ahmadian,Akbir Khan,Tim Rocktaschel,Jakob Foerster,Laura Ruis*

Main category: cs.AI

TL;DR: The paper explores the role of training large language models (LLMs) with only source code—without input/output examples—in improving their reasoning and program evaluation abilities.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand how training LLMs on source code improves their general-purpose reasoning abilities, beyond simple input-output learning.

Method: The authors fine-tuned LLMs on two datasets: one with programs and associated input/output examples, and the other with only the program's source code (no I/O examples). They then tested the models' ability to evaluate programs by implicitly running them.

Result: The study found that LLMs can evaluate programs better without I/O examples when using code instead of semantic descriptions and perform even better with techniques like chain-of-thought prompting. PBB training also enables more robust program evaluation than training with I/O pairs.

Conclusion: Training LLMs on source code enables them to internalize reusable algorithmic abstractions, improving reasoning. This approach could provide a framework for advances in symbolic reasoning and better model alignment in the future.

Abstract: Training large language models (LLMs) on source code significantly enhances
their general-purpose reasoning abilities, but the mechanisms underlying this
generalisation are poorly understood. In this paper, we propose Programming by
Backprop (PBB) as a potential driver of this effect - teaching a model to
evaluate a program for inputs by training on its source code alone, without
ever seeing I/O examples. To explore this idea, we finetune LLMs on two sets of
programs representing simple maths problems and algorithms: one with source
code and I/O examples (w/ IO), the other with source code only (w/o IO). We
find evidence that LLMs have some ability to evaluate w/o IO programs for
inputs in a range of experimental settings, and make several observations.
Firstly, PBB works significantly better when programs are provided as code
rather than semantically equivalent language descriptions. Secondly, LLMs can
produce outputs for w/o IO programs directly, by implicitly evaluating the
program within the forward pass, and more reliably when stepping through the
program in-context via chain-of-thought. We further show that PBB leads to more
robust evaluation of programs across inputs than training on I/O pairs drawn
from a distribution that mirrors naturally occurring data. Our findings suggest
a mechanism for enhanced reasoning through code training: it allows LLMs to
internalise reusable algorithmic abstractions. Significant scope remains for
future work to enable LLMs to more effectively learn from symbolic procedures,
and progress in this direction opens other avenues like model alignment by
training on formal constitutional principles.

</details>


### [47] [TRIZ Agents: A Multi-Agent LLM Approach for TRIZ-Based Innovation](https://arxiv.org/abs/2506.18783)
*Kamil Szczepanik,Jarosław A. Chudziak*

Main category: cs.AI

TL;DR: The paper proposes using a multi-agent system powered by Large Language Models (LLMs) to implement TRIZ methodology for solving inventive problems with enhanced efficiency and collaboration.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the complexity and interdisciplinary knowledge barriers that restrict traditional TRIZ applications, leveraging advancements in LLMs to automate and enhance the problem-solving process.

Method: The method involves developing a multi-agent system called TRIZ agents, where each agent specializes in a domain and collaboratively applies TRIZ methodology to simulate and solve inventive problems.

Result: The proposed system was effective in addressing complex engineering problems in a case study, demonstrating that agent collaboration leads to diverse and inventive solutions.

Conclusion: This research highlights the benefits of decentralized, AI-driven problem-solving approaches and marks a step forward in automating and innovating complex ideation tasks using LLM-based systems.

Abstract: TRIZ, the Theory of Inventive Problem Solving, is a structured,
knowledge-based framework for innovation and abstracting problems to find
inventive solutions. However, its application is often limited by the
complexity and deep interdisciplinary knowledge required. Advancements in Large
Language Models (LLMs) have revealed new possibilities for automating parts of
this process. While previous studies have explored single LLMs in TRIZ
applications, this paper introduces a multi-agent approach. We propose an
LLM-based multi-agent system, called TRIZ agents, each with specialized
capabilities and tool access, collaboratively solving inventive problems based
on the TRIZ methodology. This multi-agent system leverages agents with various
domain expertise to efficiently navigate TRIZ steps. The aim is to model and
simulate an inventive process with language agents. We assess the effectiveness
of this team of agents in addressing complex innovation challenges based on a
selected case study in engineering. We demonstrate the potential of agent
collaboration to produce diverse, inventive solutions. This research
contributes to the future of AI-driven innovation, showcasing the advantages of
decentralized problem-solving in complex ideation tasks.

</details>


### [48] [ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation](https://arxiv.org/abs/2506.18810)
*Siao Tang,Xinyin Ma,Gongfan Fang,Xinchao Wang*

Main category: cs.AI

TL;DR: "ConciseHint" framework compresses reasoning in large reasoning models without accuracy loss, reducing verbosity efficiently.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency caused by verbose reasoning in Large Reasoning Models (LRMs).

Method: Injects concise textual hints during token generation, adapting intensity based on query complexity.

Result: Achieves up to 65% reduction in reasoning length on GSM8K benchmark with almost no drop in accuracy.

Conclusion: ConciseHint enhances LRMs' efficiency by creating concise reasoning processes without impairing their accuracy.

Abstract: Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and
OpenAI o1 series have achieved notable performance enhancements on complex
reasoning tasks by scaling up the generation length by Chain-of-Thought (CoT).
However, an emerging issue is their inclination to produce excessively verbose
reasoning processes, leading to the inefficiency problem. Existing literature
on improving efficiency mainly adheres to the before-reasoning paradigms such
as prompting and reasoning or fine-tuning and reasoning, but ignores the
promising direction of directly encouraging the model to speak concisely by
intervening during the generation of reasoning. In order to fill the blank, we
propose a framework dubbed ConciseHint, which continuously encourages the
reasoning model to speak concisely by injecting the textual hint (manually
designed or trained on the concise data) during the token generation of the
reasoning process. Besides, ConciseHint is adaptive to the complexity of the
query by adaptively adjusting the hint intensity, which ensures it will not
undermine model performance. Experiments on the state-of-the-art LRMs,
including DeepSeek-R1 and Qwen-3 series, demonstrate that our method can
effectively produce concise reasoning processes while maintaining performance
well. For instance, we achieve a reduction ratio of 65\% for the reasoning
length on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss.

</details>


### [49] [Steering Conceptual Bias via Transformer Latent-Subspace Activation](https://arxiv.org/abs/2506.18887)
*Vansh Sharma,Venkat Raman*

Main category: cs.AI

TL;DR: This paper investigates steering language models' scientific coding towards specific programming languages using a new adaptive activation framework.


<details>
  <summary>Details</summary>
Motivation: Current methods for guiding language models' code generation towards specific programming languages exhibit brittleness and scaling issues, necessitating a more dynamic and efficient approach.

Method: Developed G-ACT, a gradient-refined framework that clusters activation differences, trains probes online, and selectively steers neuron activations for controlled code generation.

Result: Using G-ACT, classification accuracy improved by 15% in LLaMA-3.2 3B and by 61.5% in earlier layers compared to standard methods. LLaMA-3.3 70B also showed improved language steering with targeted layer injections.

Conclusion: The G-ACT framework provides an efficient, scalable, and interpretable mechanism for controlling concept-level behaviors in language models relevant for agentic systems.

Abstract: This work examines whether activating latent subspaces in language models
(LLMs) can steer scientific code generation toward a specific programming
language. Five causal LLMs were first evaluated on scientific coding prompts to
quantify their baseline bias among four programming languages. A static
neuron-attribution method, perturbing the highest activated MLP weight for a
C++ or CPP token, proved brittle and exhibited limited generalization across
prompt styles and model scales. To address these limitations, a
gradient-refined adaptive activation steering framework (G-ACT) was developed:
per-prompt activation differences are clustered into a small set of steering
directions, and lightweight per-layer probes are trained and refined online to
select the appropriate steering vector. In LLaMA-3.2 3B, this approach reliably
biases generation towards the CPP language by increasing the average probe
classification accuracy by 15% and the early layers (0-6) improving the probe
classification accuracy by 61.5% compared to the standard ACT framework. For
LLaMA-3.3 70B, where attention-head signals become more diffuse, targeted
injections at key layers still improve language selection. Although per-layer
probing introduces a modest inference overhead, it remains practical by
steering only a subset of layers and enables reproducible model behavior. These
results demonstrate a scalable, interpretable and efficient mechanism for
concept-level control for practical agentic systems.

</details>


### [50] [jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval](https://arxiv.org/abs/2506.18902)
*Michael Günther,Saba Sturua,Mohammad Kalim Akram,Isabelle Mohr,Andrei Ungureanu,Sedigheh Eslami,Scott Martens,Bo Wang,Nan Wang,Han Xiao*

Main category: cs.AI

TL;DR: jina-embeddings-v4 is a 3.8B parameter multimodal embedding model that integrates text and image processing with single-vector and multi-vector embeddings, achieving state-of-the-art retrieval performance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of uniting text and image representations under a single multimodal model to improve various retrieval tasks, especially for visually complex content like tables and charts.

Method: The model uses a novel architecture designed for late interaction embeddings, enhanced with task-specific LoRA adapters for optimized retrieval in multimodal and single-modal contexts.

Result: jina-embeddings-v4 outperforms existing models across single-modal and cross-modal retrieval tasks, excelling especially in visually rich content search.

Conclusion: This work provides a significant advancement in multimodal embeddings, especially highlighted by the introduction of Jina-VDR, a benchmark tailored for visually rich image retrieval evaluation.

Abstract: We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding
model that unifies text and image representations through a novel architecture
supporting both single-vector and multi-vector embeddings in the late
interaction style. The model incorporates task-specific Low-Rank Adaptation
(LoRA) adapters to optimize performance across diverse retrieval scenarios,
including query-based information retrieval, cross-modal semantic similarity,
and programming code search. Comprehensive evaluations demonstrate that
jina-embeddings-v4 achieves state-of-the-art performance on both single- modal
and cross-modal retrieval tasks, with particular strength in processing
visually rich content such as tables, charts, diagrams, and mixed-media
formats. To facilitate evaluation of this capability, we also introduce
Jina-VDR, a novel benchmark specifically designed for visually rich image
retrieval.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [51] [AMD Versal Implementations of FAM and SSCA Estimators](https://arxiv.org/abs/2506.18003)
*Carol Jingyi Li,Ruilin Wu,Philip H. W. Leong*

Main category: cs.AR

TL;DR: The paper introduces high-speed FPGA implementations of cyclostationary signal analysis techniques, achieving significant speed and energy efficiency improvements compared to GPUs.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the computational inefficiency of spectral correlation density (SCD) estimation, which limits its real-time application in signal processing.

Method: Optimised FPGA-based implementations of the FFT accumulation method (FAM) and strip spectral correlation analyser (SSCA) are developed, employing parallelisation techniques adapted to hardware constraints.

Result: The FPGA implementations show speedups of 4.43x (FAM) and 1.90x (SSCA) over a high-performance GPU, as well as 30.5x and 24.5x improvements in energy efficiency respectively.

Conclusion: The proposed FPGA designs enable efficient and real-time cyclostationary analysis, outperforming GPUs in both processing speed and energy consumption.

Abstract: Cyclostationary analysis is widely used in signal processing, particularly in
the analysis of human-made signals, and spectral correlation density (SCD) is
often used to characterise cyclostationarity. Unfortunately, for real-time
applications, even utilising the fast Fourier transform (FFT), the high
computational complexity associated with estimating the SCD limits its
applicability. In this work, we present optimised, high-speed
field-programmable gate array (FPGA) implementations of two SCD estimation
techniques. Specifically, we present an implementation of the FFT accumulation
method (FAM) running entirely on the AMD Versal AI engine (AIE) array. We also
introduce an efficient implementation of the strip spectral correlation
analyser (SSCA) that can be used for window sizes up to $2^{20}$. For both
techniques, a generalised methodology is presented to parallelise the
computation while respecting memory size and data bandwidth constraints.
Compared to an NVIDIA GeForce RTX 3090 graphics processing unit (GPU) which
uses a similar 7nm technology to our FPGA, for the same accuracy, our FAM/SSCA
implementations achieve speedups of 4.43x/1.90x and a 30.5x/24.5x improvement
in energy efficiency.

</details>


### [52] [Embedded FPGA Acceleration of Brain-Like Neural Networks: Online Learning to Scalable Inference](https://arxiv.org/abs/2506.18530)
*Muhammad Ihsan Al Hafiz,Naresh Ravichandran,Anders Lansner,Pawel Herman,Artur Podobas*

Main category: cs.AR

TL;DR: The paper introduces an embedded FPGA accelerator for Brain-Like Neural Networks (BLNNs) tailored to edge AI applications, achieving significant latency and energy efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the energy inefficiencies and cloud reliance of traditional deep learning models by exploring biologically-inspired Brain-Like Neural Networks (BLNNs).

Method: The paper presents an FPGA accelerator designed for the Bayesian Confidence Propagation Neural Network (BCPNN), leveraging High-Level Synthesis for implementation on a Zynq UltraScale+ SoC with capabilities for both online learning and inference.

Result: The accelerator achieves up to 17.5x latency reduction and 94% energy savings without compromising accuracy across datasets including MNIST, Pneumonia, and Breast Cancer.

Conclusion: This work practically enables neuromorphic edge computing by bridging brain-like learning paradigms with real-world embedded deployment, making energy-efficient AI feasible.

Abstract: Edge AI applications increasingly require models that can learn and adapt
on-device with minimal energy budget. Traditional deep learning models, while
powerful, are often overparameterized, energy-hungry, and dependent on cloud
connectivity. Brain-Like Neural Networks (BLNNs), such as the Bayesian
Confidence Propagation Neural Network (BCPNN), propose a neuromorphic
alternative by mimicking cortical architecture and biologically-constrained
learning. They offer sparse architectures with local learning rules and
unsupervised/semi-supervised learning, making them well-suited for low-power
edge intelligence. However, existing BCPNN implementations rely on GPUs or
datacenter FPGAs, limiting their applicability to embedded systems. This work
presents the first embedded FPGA accelerator for BCPNN on a Zynq UltraScale+
SoC using High-Level Synthesis. We implement both online learning and
inference-only kernels with support for variable and mixed precision. Evaluated
on MNIST, Pneumonia, and Breast Cancer datasets, our accelerator achieves up to
17.5x latency and 94% energy savings over ARM baselines, without sacrificing
accuracy. This work enables practical neuromorphic computing on edge devices,
bridging the gap between brain-like learning and real-world deployment.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [53] [Outcome-Based Education: Evaluating Students' Perspectives Using Transformer](https://arxiv.org/abs/2506.17223)
*Shuvra Smaran Das,Anirban Saha Anik,Md Kishor Morol,Mohammad Sakib Mahmood*

Main category: cs.CL

TL;DR: This study uses DistilBERT, a transformer-based model, and LIME to analyze student feedback for improving educational outcomes under Outcome-Based Education (OBE).


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance OBE practices by leveraging advanced NLP methods to classify and interpret student feedback, thus improving learning outcomes.

Method: Implemented a DistilBERT-based NLP model for sentiment classification and applied LIME for interpretable predictions.

Result: The proposed approach outperformed other machine learning models in analyzing student feedback with improved accuracy and interpretability.

Conclusion: Combining transformer models with LIME provides an effective and transparent framework for analyzing student feedback, aligning with OBE principles and fostering data-driven educational improvements.

Abstract: Outcome-Based Education (OBE) emphasizes the development of specific
competencies through student-centered learning. In this study, we reviewed the
importance of OBE and implemented transformer-based models, particularly
DistilBERT, to analyze an NLP dataset that includes student feedback. Our
objective is to assess and improve educational outcomes. Our approach is better
than other machine learning models because it uses the transformer's deep
understanding of language context to classify sentiment better, giving better
results across a wider range of matrices. Our work directly contributes to
OBE's goal of achieving measurable outcomes by facilitating the identification
of patterns in student learning experiences. We have also applied LIME (local
interpretable model-agnostic explanations) to make sure that model predictions
are clear. This gives us understandable information about how key terms affect
sentiment. Our findings indicate that the combination of transformer models and
LIME explanations results in a strong and straightforward framework for
analyzing student feedback. This aligns more closely with the principles of OBE
and ensures the improvement of educational practices through data-driven
insights.

</details>


### [54] [Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs](https://arxiv.org/abs/2506.17231)
*Xiang Li,Chong Zhang,Jia Wang,Fangyu Wu,Yushi Li,Xiaobo Jin*

Main category: cs.CL

TL;DR: The paper introduces "Adversarial Prompt Distillation" method to enable small language models (SLMs) to perform efficient and adaptable jailbreak attacks on large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The study addresses issues with current jailbreak attack methods, such as inefficiency, high computational cost, and limited adaptability across models, in light of the rapid evolution of LLMs.

Method: The paper combines masked language modeling, reinforcement learning, and dynamic temperature control in a prompt generation and distillation approach to empower SLMs for jailbreak attacks.

Result: Experiments demonstrate the proposed method's high attack success rate, significant impact, efficient resource usage, and adaptability across different LLMs.

Conclusion: This research highlights vulnerabilities in LLMs while showing the feasibility of transferring jailbreak capabilities to SLMs, offering valuable insights for enhancing security in LLM development.

Abstract: Attacks on large language models (LLMs) in jailbreaking scenarios raise many
security and ethical issues. Current jailbreak attack methods face problems
such as low efficiency, high computational cost, and poor cross-model
adaptability and versatility, which make it difficult to cope with the rapid
development of LLM and new defense strategies. Our work proposes an Adversarial
Prompt Distillation, which combines masked language modeling, reinforcement
learning, and dynamic temperature control through a prompt generation and
distillation method. It enables small language models (SLMs) to jailbreak
attacks on mainstream LLMs. The experimental results verify the superiority of
the proposed method in terms of attack success rate and harm, and reflect the
resource efficiency and cross-model adaptability. This research explores the
feasibility of distilling the jailbreak ability of LLM to SLM, reveals the
model's vulnerability, and provides a new idea for LLM security research.

</details>


### [55] [GTA: Grouped-head latenT Attention](https://arxiv.org/abs/2506.17286)
*Luoyang Sun,Jiwen Jiang,Cheng Deng,Xinjian Wu,Haifeng Zhang,Lei Chen,Lionel Ni,Jun Wang*

Main category: cs.CL

TL;DR: The study introduces Grouped-Head Latent Attention (GTA), an efficient attention mechanism for large language models that reduces computational and memory costs while delivering comparable performance.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the inefficiencies and high resource demands of traditional attention mechanisms in LLMs, particularly when handling scaling issues with text length, which makes deployment on limited hardware challenging.

Method: The authors develop GTA, consisting of (1) a shared attention map mechanism for reducing key cache size by reusing attention scores across heads, and (2) a nonlinear value decoder that compresses the value cache into a latent space via learned projections.

Result: GTA achieves up to 62.5% reduction in attention computation FLOPs and up to 70% reduction in KV cache size, improving inference efficiency and achieving a 2x speedup in end-to-end LLM operations.

Conclusion: The GTA mechanism provides a significant advancement in improving LLM efficiency by reducing memory and computational costs without sacrificing performance, making it more viable for deployment on resource-constrained hardware.

Abstract: Attention mechanisms underpin the success of large language models (LLMs),
yet their substantial computational and memory overhead poses challenges for
optimizing efficiency and performance. A critical bottleneck arises as KV cache
and attention computations scale rapidly with text length, challenging
deployment on hardware with limited computational and memory resources. We
observe that attention mechanisms exhibit substantial redundancy, since the KV
cache can be significantly compressed and attention maps across heads display
high similarity, revealing that much of the computation and storage is
unnecessary. Leveraging these insights, we propose \textbf{G}rouped-Head
Laten\textbf{T} \textbf{A}ttention (GTA), a novel attention mechanism that
reduces memory usage and computational complexity while maintaining
performance. GTA comprises two components: (1) a shared attention map mechanism
that reuses attention scores across multiple heads, decreasing the key cache
size; and (2) a nonlinear value decoder with learned projections that
compresses the value cache into a latent space, further cutting memory needs.
GTA cuts attention computation FLOPs by up to \emph{62.5\%} versus
Grouped-Query Attention and shrink the KV cache by up to \emph{70\%}, all while
avoiding the extra overhead of Multi-Head Latent Attention to improve LLM
deployment efficiency. Consequently, GTA models achieve a \emph{2x} increase in
end-to-end inference speed, with prefill benefiting from reduced computational
cost and decoding benefiting from the smaller cache footprint.

</details>


### [56] [AI-Generated Game Commentary: A Survey and a Datasheet Repository](https://arxiv.org/abs/2506.17294)
*Qirui Zheng,Xingbo Wang,Keyuan Cheng,Yunlong Lu,Wenxin Li*

Main category: cs.CL

TL;DR: The paper introduces a general framework for AI-Generated Game Commentary (AIGGC), surveys datasets and methods, classifies evaluation metrics, and offers a structured datasheet to aid further research.


<details>
  <summary>Details</summary>
Motivation: AI-generated game commentary is a challenging multimodal NLP task with significant market potential and technical demands.

Method: The authors present a framework for AIGGC, survey 45 datasets and methods, classify evaluation metrics, and compile a publicly available datasheet.

Result: Key challenges in the field are addressed, datasets and methods are categorized, evaluation metric classifications are offered, and a datasheet is provided for research support.

Conclusion: The paper sets a foundational resource for advancing research and benchmarking in the field of AI-generated game commentary.

Abstract: AI-Generated Game Commentary (AIGGC) has gained increasing attention due to
its market potential and inherent technical challenges. As a comprehensive
multimodal Natural Language Processing (NLP) task, AIGGC imposes substantial
demands on language models, including factual accuracy, logical reasoning,
expressive text generation, generation speed, and context management. In this
paper, we introduce a general framework for AIGGC and present a comprehensive
survey of 45 existing game commentary dataset and methods according to key
challenges they aim to address in this domain. We further classify and compare
various evaluation metrics commonly used in this domain. To support future
research and benchmarking, we also provide a structured datasheet summarizing
the essential attributes of these datasets in appendix, which is meanwhile
publicly available in an open repository.

</details>


### [57] [Semantic uncertainty in advanced decoding methods for LLM generation](https://arxiv.org/abs/2506.17296)
*Darius Foodeei,Simin Fan,Martin Jaggi*

Main category: cs.CL

TL;DR: The paper analyzes semantic uncertainty in LLM outputs, examining how different decoding methods like CoT and speculative sampling impact diversity and reliability across tasks like question answering, code generation, and summarization.


<details>
  <summary>Details</summary>
Motivation: To explore how decoding strategies influence the balance between semantic diversity and accuracy in LLM outputs, focusing on practical applications where reliability and diversity are crucial.

Method: Experiments were conducted on tasks like question answering, summarization, and code generation, comparing traditional methods with CoT decoding and speculative sampling.

Result: CoT decoding showed higher diversity with low predictive entropy, improving confidence in outputs; speculative sampling excelled in summarization tasks with better ROUGE scores and moderate diversity; a 48.8% improvement in Pass@2 was noted for code generation.

Conclusion: Structured decoding strategies can enhance semantic exploration without compromising output quality, challenging trade-offs between diversity and accuracy and holding promise for real-world applications of LLMs.

Abstract: This study investigates semantic uncertainty in large language model (LLM)
outputs across different decoding methods, focusing on emerging techniques like
speculative sampling and chain-of-thought (CoT) decoding. Through experiments
on question answering, summarization, and code generation tasks, we analyze how
different decoding strategies affect both the diversity and reliability of
model outputs. Our findings reveal that while CoT decoding demonstrates higher
semantic diversity, it maintains lower predictive entropy, suggesting that
structured exploration can lead to more confident and accurate outputs. This is
evidenced by a 48.8% improvement in code generation Pass@2 rates, despite lower
alignment with reference solutions. For summarization tasks, speculative
sampling proved particularly effective, achieving superior ROUGE scores while
maintaining moderate semantic diversity. Our results challenge conventional
assumptions about trade-offs between diversity and accuracy in language model
outputs, demonstrating that properly structured decoding methods can increase
semantic exploration while maintaining or improving output quality. These
findings have significant implications for deploying language models in
practical applications where both reliability and diverse solution generation
are crucial.

</details>


### [58] [Mercury: Ultra-Fast Language Models Based on Diffusion](https://arxiv.org/abs/2506.17298)
*Inception Labs,Samar Khanna,Siddhant Kharbanda,Shufan Li,Harshit Varma,Eric Wang,Sawyer Birnbaum,Ziyang Luo,Yanis Miraoui,Akash Palrecha,Stefano Ermon,Aditya Grover,Volodymyr Kuleshov*

Main category: cs.CL

TL;DR: The paper introduces Mercury, a novel diffusion-based large language model (LLM) designed for coding applications, delivering exceptional speed and quality.


<details>
  <summary>Details</summary>
Motivation: Existing large language models for coding face challenges in concurrently delivering state-of-the-art quality and throughput. Mercury aims to set new standards in the speed-quality frontier.

Method: Mercury leverages a diffusion-based Transformer architecture trained to predict multiple tokens simultaneously. Two versions, Mini and Small, are optimized for coding tasks.

Result: The Mercury Coder models achieve notable throughput (1109 and 737 tokens/sec for Mini and Small) while matching or improving upon quality benchmarks. They outperform prior speed-optimized models by up to 10x.

Conclusion: Mercury Coder models achieve a balance between high-speed performance and quality, making them ideal for coding applications. They are validated by benchmarks and developer use-cases, and APIs have been released for wider access.

Abstract: We present Mercury, a new generation of commercial-scale large language
models (LLMs) based on diffusion. These models are parameterized via the
Transformer architecture and trained to predict multiple tokens in parallel. In
this report, we detail Mercury Coder, our first set of diffusion LLMs designed
for coding applications. Currently, Mercury Coder comes in two sizes: Mini and
Small. These models set a new state-of-the-art on the speed-quality frontier.
Based on independent evaluations conducted by Artificial Analysis, Mercury
Coder Mini and Mercury Coder Small achieve state-of-the-art throughputs of 1109
tokens/sec and 737 tokens/sec, respectively, on NVIDIA H100 GPUs and outperform
speed-optimized frontier models by up to 10x on average while maintaining
comparable quality. We discuss additional results on a variety of code
benchmarks spanning multiple languages and use-cases as well as real-world
validation by developers on Copilot Arena, where the model currently ranks
second on quality and is the fastest model overall. We also release a public
API at https://platform.inceptionlabs.ai/ and free playground at
https://chat.inceptionlabs.ai

</details>


### [59] [PRAISE: Enhancing Product Descriptions with LLM-Driven Structured Insights](https://arxiv.org/abs/2506.17314)
*Adnan Qidwai,Srija Mukhopadhyay,Prerana Khatiwada,Dan Roth,Vivek Gupta*

Main category: cs.CL

TL;DR: The paper introduces PRAISE, a system that uses Large Language Models to extract and compare insights from customer reviews and seller descriptions, aiming to enhance e-commerce product listings.


<details>
  <summary>Details</summary>
Motivation: E-commerce product descriptions often lack completeness or accuracy, and manually extracting useful details from customer reviews is time-intensive.

Method: PRAISE leverages Large Language Models to automatically analyze, compare, and structure insights from both reviews and seller-provided descriptions.

Result: The system identifies discrepancies, such as missing or contradictory information, and presents these findings in a structured format with supporting evidence.

Conclusion: PRAISE has the potential to improve the trustworthiness and quality of e-commerce product catalogs, benefiting both sellers and buyers.

Abstract: Accurate and complete product descriptions are crucial for e-commerce, yet
seller-provided information often falls short. Customer reviews offer valuable
details but are laborious to sift through manually. We present PRAISE: Product
Review Attribute Insight Structuring Engine, a novel system that uses Large
Language Models (LLMs) to automatically extract, compare, and structure
insights from customer reviews and seller descriptions. PRAISE provides users
with an intuitive interface to identify missing, contradictory, or partially
matching details between these two sources, presenting the discrepancies in a
clear, structured format alongside supporting evidence from reviews. This
allows sellers to easily enhance their product listings for clarity and
persuasiveness, and buyers to better assess product reliability. Our
demonstration showcases PRAISE's workflow, its effectiveness in generating
actionable structured insights from unstructured reviews, and its potential to
significantly improve the quality and trustworthiness of e-commerce product
catalogs.

</details>


### [60] [Towards Safety Evaluations of Theory of Mind in Large Language Models](https://arxiv.org/abs/2506.17352)
*Tatsuhiro Aoshima,Mitsuaki Akiyama*

Main category: cs.CL

TL;DR: LLMs are advancing, requiring safety checks as they sometimes display deceptive behaviors. This study measures their theory of mind for evaluating risks in safety.


<details>
  <summary>Details</summary>
Motivation: Growing concerns about deceptive behaviors in LLMs necessitate tools to evaluate risks tied to models understanding and intentions.

Method: Researchers analyze theory of mind capabilities in open-weight LLMs through developmental psychology trends and safety evaluation tasks.

Result: Findings show improvement in LLM reading comprehension but limited progress in their theory of mind abilities.

Conclusion: The study highlights the importance of incorporating theory of mind in safety checks for LLMs and points to challenges for future research.

Abstract: As the capabilities of large language models (LLMs) continue to advance, the
importance of rigorous safety evaluation is becoming increasingly evident.
Recent concerns within the realm of safety assessment have highlighted
instances in which LLMs exhibit behaviors that appear to disable oversight
mechanisms and respond in a deceptive manner. For example, there have been
reports suggesting that, when confronted with information unfavorable to their
own persistence during task execution, LLMs may act covertly and even provide
false answers to questions intended to verify their behavior.To evaluate the
potential risk of such deceptive actions toward developers or users, it is
essential to investigate whether these behaviors stem from covert, intentional
processes within the model. In this study, we propose that it is necessary to
measure the theory of mind capabilities of LLMs. We begin by reviewing existing
research on theory of mind and identifying the perspectives and tasks relevant
to its application in safety evaluation. Given that theory of mind has been
predominantly studied within the context of developmental psychology, we
analyze developmental trends across a series of open-weight LLMs. Our results
indicate that while LLMs have improved in reading comprehension, their theory
of mind capabilities have not shown comparable development. Finally, we present
the current state of safety evaluation with respect to LLMs' theory of mind,
and discuss remaining challenges for future work.

</details>


### [61] [Cash or Comfort? How LLMs Value Your Inconvenience](https://arxiv.org/abs/2506.17367)
*Mateusz Cedro,Timour Ichmoukhamedov,Sofie Goethals,Yifan He,James Hinns,David Martens*

Main category: cs.CL

TL;DR: The study evaluates how LLMs make decisions when financial rewards are weighed against user discomfort and identifies key issues, including variability and irrational trade-offs.


<details>
  <summary>Details</summary>
Motivation: To explore the effectiveness and limitations of LLMs in personal decision-making, particularly in scenarios where financial incentives conflict with user comfort.

Method: Quantifying the values given by different LLMs to various user discomforts (e.g., walking, waiting, hunger, pain) and analyzing their decision responses.

Result: The study revealed significant concerns, such as high response variability between LLMs, sensitivity to prompt phrasing, acceptance of unreasonable trade-offs, and irrational monetary decisions.

Conclusion: Current LLMs exhibit unreliable behavior in valuing human inconvenience, raising concerns about their readiness for decision-making tasks involving comfort or monetary trade-offs on users' behalf.

Abstract: Large Language Models (LLMs) are increasingly proposed as near-autonomous
artificial intelligence (AI) agents capable of making everyday decisions on
behalf of humans. Although LLMs perform well on many technical tasks, their
behaviour in personal decision-making remains less understood. Previous studies
have assessed their rationality and moral alignment with human decisions.
However, the behaviour of AI assistants in scenarios where financial rewards
are at odds with user comfort has not yet been thoroughly explored. In this
paper, we tackle this problem by quantifying the prices assigned by multiple
LLMs to a series of user discomforts: additional walking, waiting, hunger and
pain. We uncover several key concerns that strongly question the prospect of
using current LLMs as decision-making assistants: (1) a large variance in
responses between LLMs, (2) within a single LLM, responses show fragility to
minor variations in prompt phrasing (e.g., reformulating the question in the
first person can considerably alter the decision), (3) LLMs can accept
unreasonably low rewards for major inconveniences (e.g., 1 Euro to wait 10
hours), and (4) LLMs can reject monetary gains where no discomfort is imposed
(e.g., 1,000 Euro to wait 0 minutes). These findings emphasize the need for
scrutiny of how LLMs value human inconvenience, particularly as we move toward
applications where such cash-versus-comfort trade-offs are made on users'
behalf.

</details>


### [62] [Leveraging LLMs to Assess Tutor Moves in Real-Life Dialogues: A Feasibility Study](https://arxiv.org/abs/2506.17410)
*Danielle R. Thomas,Conrad Borchers,Jionghao Lin,Sanjit Kakarla,Shambhavi Bhushan,Erin Gatz,Shivang Gupta,Ralph Abboud,Kenneth R. Koedinger*

Main category: cs.CL

TL;DR: This study used generative AI to evaluate tutor effectiveness and identify key tutoring actions in real-life math education scenarios, reaching high accuracy rates in detecting and assessing tutoring practices.


<details>
  <summary>Details</summary>
Motivation: Understanding which tutoring actions promote student learning and achieving scalability in evaluating these actions using technology is a significant challenge.

Method: The study involved analyzing 50 transcripts of college tutors assisting middle school students in math, using AI models like GPT-4 and others to evaluate tutors' performance in delivering praise and addressing math errors.

Result: AI models reliably detected tutoring actions (94-98% accuracy for praise, 82-88% for error detection) and aligned well with human evaluations, showing accuracy rates of 83-89% for praise and 73-77% for error handling.

Conclusion: Generative AI can effectively and scalable evaluate tutoring practices with high accuracy, offering practical implications for improving tutoring and scalability in authentic educational contexts.

Abstract: Tutoring improves student achievement, but identifying and studying what
tutoring actions are most associated with student learning at scale based on
audio transcriptions is an open research problem. This present study
investigates the feasibility and scalability of using generative AI to identify
and evaluate specific tutor moves in real-life math tutoring. We analyze 50
randomly selected transcripts of college-student remote tutors assisting middle
school students in mathematics. Using GPT-4, GPT-4o, GPT-4-turbo,
Gemini-1.5-pro, and LearnLM, we assess tutors' application of two tutor skills:
delivering effective praise and responding to student math errors. All models
reliably detected relevant situations, for example, tutors providing praise to
students (94-98% accuracy) and a student making a math error (82-88% accuracy)
and effectively evaluated the tutors' adherence to tutoring best practices,
aligning closely with human judgments (83-89% and 73-77%, respectively). We
propose a cost-effective prompting strategy and discuss practical implications
for using large language models to support scalable assessment in authentic
settings. This work further contributes LLM prompts to support reproducibility
and research in AI-supported learning.

</details>


### [63] [UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making](https://arxiv.org/abs/2506.17419)
*Jinhao Duan,James Diffenderfer,Sandeep Madireddy,Tianlong Chen,Bhavya Kailkhura,Kaidi Xu*

Main category: cs.CL

TL;DR: This paper introduces UProp, a framework for estimating and propagating uncertainties in Large Language Model (LLM) agents' sequential decision-making processes, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to trust LLM decisions in safety-critical, multi-step decision-making scenarios, as current uncertainty quantification methods primarily focus on single-turn settings.

Method: The authors developed an information-theoretic framework that splits LLM uncertainty into internal (current decision) and extrinsic (sequential decision) uncertainties. They proposed UProp, which uses Pointwise Mutual Information to estimate extrinsic uncertainty over trajectory-dependent processes efficiently.

Result: Experimental evaluations on benchmarks like AgentBench and HotpotQA, using state-of-the-art models (e.g., GPT-4.1), show that UProp outperforms single-turn uncertainty quantification baselines, even with aggregation techniques.

Conclusion: UProp effectively quantifies and propagates uncertainty in sequential decisions, contributing to the reliability of LLMs in complex decision-making tasks. Its sampling efficiency and applications further validate its utility.

Abstract: As Large Language Models (LLMs) are integrated into safety-critical
applications involving sequential decision-making in the real world, it is
essential to know when to trust LLM decisions. Existing LLM Uncertainty
Quantification (UQ) methods are primarily designed for single-turn
question-answering formats, resulting in multi-step decision-making scenarios,
e.g., LLM agentic system, being underexplored. In this paper, we introduce a
principled, information-theoretic framework that decomposes LLM sequential
decision uncertainty into two parts: (i) internal uncertainty intrinsic to the
current decision, which is focused on existing UQ methods, and (ii) extrinsic
uncertainty, a Mutual-Information (MI) quantity describing how much uncertainty
should be inherited from preceding decisions. We then propose UProp, an
efficient and effective extrinsic uncertainty estimator that converts the
direct estimation of MI to the estimation of Pointwise Mutual Information (PMI)
over multiple Trajectory-Dependent Decision Processes (TDPs). UProp is
evaluated over extensive multi-step decision-making benchmarks, e.g.,
AgentBench and HotpotQA, with state-of-the-art LLMs, e.g., GPT-4.1 and
DeepSeek-V3. Experimental results demonstrate that UProp significantly
outperforms existing single-turn UQ baselines equipped with thoughtful
aggregation strategies. Moreover, we provide a comprehensive analysis of UProp,
including sampling efficiency, potential applications, and intermediate
uncertainty propagation, to demonstrate its effectiveness. Codes will be
available at https://github.com/jinhaoduan/UProp.

</details>


### [64] [Beyond the Link: Assessing LLMs' ability to Classify Political Content across Global Media](https://arxiv.org/abs/2506.17435)
*Alberto Martinez-Serra,Alejandro De La Fuente,Nienke Viescher,Ana S. Cardenal*

Main category: cs.CL

TL;DR: This paper investigates the ability of large language models (LLMs) to classify political content (PC) using URLs and article text.


<details>
  <summary>Details</summary>
Motivation: To explore whether LLMs can accurately classify political versus non-political content using only URLs, filling a gap in studies focused on URL-level PC analysis across languages and countries.

Method: The authors use advanced LLMs (e.g., GPT, Llama, Mistral) to evaluate performance on URL and text-based political content classification across five countries. Comparisons are made with human-labeled datasets and traditional machine learning methods.

Result: The study finds that URLs can embed most news content effectively, showing LLMs' potential for achieving accuracy in classification with lower costs.

Conclusion: LLMs are suitable tools for PC classification via URLs, but context-specific limitations should be considered. The paper offers methodological recommendations for applying LLMs in political science research.

Abstract: The use of large language models (LLMs) is becoming common in the context of
political science, particularly in studies that analyse individuals use of
digital media. However, while previous research has demonstrated LLMs ability
at labelling tasks, the effectiveness of using LLMs to classify political
content (PC) from just URLs is not yet well explored. The work presented in
this article bridges this gap by evaluating whether LLMs can accurately
identify PC vs. non-PC from both the article text and the URLs from five
countries (France, Germany, Spain, the UK, and the US) and different languages.
Using cutting-edge LLMs like GPT, Llama, Mistral, Deepseek, Qwen and Gemma, we
measure model performance to assess whether URL-level analysis can be a good
approximation for full-text analysis of PC, even across different linguistic
and national contexts. Model outputs are compared with human-labelled articles,
as well as traditional supervised machine learning techniques, to set a
baseline of performance. Overall, our findings suggest the capacity of URLs to
embed most of the news content, providing a vital perspective on accuracy-cost
balancing. We also account for contextual limitations and suggest
methodological recommendations to use LLMs within political science studies.

</details>


### [65] [Breaking the Transcription Bottleneck: Fine-tuning ASR Models for Extremely Low-Resource Fieldwork Languages](https://arxiv.org/abs/2506.17459)
*Siyu Liang,Gina-Anne Levow*

Main category: cs.CL

TL;DR: The paper evaluates two multilingual ASR models (MMS and XLS-R) on low-resource languages and provides practical insights for field linguists.


<details>
  <summary>Details</summary>
Motivation: The need to overcome transcription bottlenecks in linguistics fieldwork involving under-documented low-resource languages.

Method: Fine-tuning and benchmarking two multilingual ASR models (MMS and XLS-R) based on varying amounts of training data for five typologically diverse languages.

Result: MMS performs better with minimal training data, while XLS-R shows comparable performance when training data exceeds one hour.

Conclusion: Reproducible ASR techniques can aid field linguists in adapting ASR for language documentation under constrained circumstances.

Abstract: Automatic Speech Recognition (ASR) has reached impressive accuracy for
high-resource languages, yet its utility in linguistic fieldwork remains
limited. Recordings collected in fieldwork contexts present unique challenges,
including spontaneous speech, environmental noise, and severely constrained
datasets from under-documented languages. In this paper, we benchmark the
performance of two fine-tuned multilingual ASR models, MMS and XLS-R, on five
typologically diverse low-resource languages with control of training data
duration. Our findings show that MMS is best suited when extremely small
amounts of training data are available, whereas XLS-R shows parity performance
once training data exceed one hour. We provide linguistically grounded analysis
for further provide insights towards practical guidelines for field linguists,
highlighting reproducible ASR adaptation approaches to mitigate the
transcription bottleneck in language documentation.

</details>


### [66] [Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems](https://arxiv.org/abs/2506.17467)
*Weixin Liang*

Main category: cs.CL

TL;DR: This dissertation investigates the implications of LLM adoption, examining AI biases, measuring usage trends, and exploring manuscript feedback capabilities.


<details>
  <summary>Details</summary>
Motivation: To understand how individuals and institutions are adapting to the societal adoption of LLMs and highlight the equity and support concerns associated with their usage.

Method: The dissertation employs a three-fold approach: analyzing AI detector biases, using algorithmic measurements to track LLM adoption in writing, and conducting empirical studies of LLM-powered feedback mechanisms.

Result: Findings show systematic biases in AI detectors disadvantaging non-dominant language users, widespread application of LLMs in writing domains, and the potential for LLMs to offer valuable manuscript feedback for under-resourced researchers.

Conclusion: The study emphasizes the need for equitable AI governance, deeper understanding of LLM adoption patterns, and leveraging LLM feedback to support less privileged researchers.

Abstract: Large language models (LLMs) have shown significant potential to change how
we write, communicate, and create, leading to rapid adoption across society.
This dissertation examines how individuals and institutions are adapting to and
engaging with this emerging technology through three research directions.
First, I demonstrate how the institutional adoption of AI detectors introduces
systematic biases, particularly disadvantaging writers of non-dominant language
varieties, highlighting critical equity concerns in AI governance. Second, I
present novel population-level algorithmic approaches that measure the
increasing adoption of LLMs across writing domains, revealing consistent
patterns of AI-assisted content in academic peer reviews, scientific
publications, consumer complaints, corporate communications, job postings, and
international organization press releases. Finally, I investigate LLMs'
capability to provide feedback on research manuscripts through a large-scale
empirical analysis, offering insights into their potential to support
researchers who face barriers in accessing timely manuscript feedback,
particularly early-career researchers and those from under-resourced settings.

</details>


### [67] [VeriLocc: End-to-End Cross-Architecture Register Allocation via LLM](https://arxiv.org/abs/2506.17506)
*Lesheng Jin,Zhenyuan Ruan,Haohui Mai,Jingbo Shang*

Main category: cs.CL

TL;DR: The paper introduces VeriLocc, a framework using large language models (LLMs) plus formal compiler verification to create generalizable, efficient, and accurate GPU register allocation.


<details>
  <summary>Details</summary>
Motivation: GPU hardware evolves rapidly, but current hand-crafted and heuristic-driven GPU register allocation methods are labor-intensive and need re-tuning for each new architecture.

Method: VeriLocc fine-tunes an LLM to translate intermediate representations into hardware-specific register assignments. It employs static analysis for normalization across architectures and a correctness-verifying regeneration loop.

Result: VeriLocc achieved 85-99% single-shot accuracy, near-100% pass@100 rates, and produced more optimized code than expert-tuned libraries, including over 10% runtime improvements compared to rocBLAS.

Conclusion: VeriLocc demonstrates the potential of combining LLMs with formal techniques to deliver generalizable, verifiable, and high-performance solutions for GPU register allocations across architectures.

Abstract: Modern GPUs evolve rapidly, yet production compilers still rely on
hand-crafted register allocation heuristics that require substantial re-tuning
for each hardware generation. We introduce VeriLocc, a framework that combines
large language models (LLMs) with formal compiler techniques to enable
generalizable and verifiable register allocation across GPU architectures.
VeriLocc fine-tunes an LLM to translate intermediate representations (MIRs)
into target-specific register assignments, aided by static analysis for
cross-architecture normalization and generalization and a verifier-guided
regeneration loop to ensure correctness. Evaluated on matrix multiplication
(GEMM) and multi-head attention (MHA), VeriLocc achieves 85-99% single-shot
accuracy and near-100% pass@100. Case study shows that VeriLocc discovers more
performant assignments than expert-tuned libraries, outperforming rocBLAS by
over 10% in runtime.

</details>


### [68] [Data Quality Issues in Multilingual Speech Datasets: The Need for Sociolinguistic Awareness and Proactive Language Planning](https://arxiv.org/abs/2506.17525)
*Mingfei Lau,Qian Chen,Yeming Fang,Tingting Xu,Tongzhou Chen,Pavel Golik*

Main category: cs.CL

TL;DR: Audit identifies quality issues in multilingual speech datasets, especially for under-resourced languages, and provides guidelines to improve them.


<details>
  <summary>Details</summary>
Motivation: To improve the usability and reliability of widely-used multilingual speech datasets by identifying and addressing quality issues.

Method: Audit and categorize quality issues in Mozilla Common Voice 17.0, FLEURS, and VoxPopuli datasets, and provide a case analysis on Taiwanese Southern Min (nan_tw).

Result: Revealed significant quality issues in some languages, with more prevalent macro-level problems in under-resourced languages, and highlighted the impact of sociolinguistic factors.

Conclusion: Recommended proactive language planning and enhanced data quality control to mitigate identified issues, emphasizing sociolinguistic awareness in ASR dataset creation.

Abstract: Our quality audit for three widely used public multilingual speech datasets -
Mozilla Common Voice 17.0, FLEURS, and VoxPopuli - shows that in some
languages, these datasets suffer from significant quality issues. We believe
addressing these issues will make these datasets more useful as training and
evaluation sets, and improve downstream models. We divide these quality issues
into two categories: micro-level and macro-level. We find that macro-level
issues are more prevalent in less institutionalized, often under-resourced
languages. We provide a case analysis of Taiwanese Southern Min (nan_tw) that
highlights the need for proactive language planning (e.g. orthography
prescriptions, dialect boundary definition) and enhanced data quality control
in the process of Automatic Speech Recognition (ASR) dataset creation. We
conclude by proposing guidelines and recommendations to mitigate these issues
in future dataset development, emphasizing the importance of sociolinguistic
awareness in creating robust and reliable speech data resources.

</details>


### [69] [DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning](https://arxiv.org/abs/2506.17533)
*Yuanhao Wu,Juntong Song,Hanning Zhang,Tong Zhang,Cheng Niu*

Main category: cs.CL

TL;DR: The paper introduces DuaShepherd, a reward modeling framework blending correctness and potential signals to improve mathematical reasoning in LLMs, achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: Enhancing the mathematical reasoning abilities of Large Language Models by addressing limitations of single reward signaling approaches.

Method: Developed an automated pipeline to construct a dataset with dual reward signals and trained models using a unified multi-head architecture in a multi-task setup.

Result: The model combining correctness and potential signals showed significant improvements, outperforming single-signal models on benchmarks like MATH500 and ProcessBench.

Conclusion: Incorporating dual reward signals leads to enhanced reasoning capabilities in LLMs and sets a new state-of-the-art in mathematical benchmarks.

Abstract: In this paper, we propose DuaShepherd, a novel reward modeling framework that
integrates two complementary reward signals, correctness and potential, to
enhance the mathematical reasoning capabilities of Large Language Models
(LLMs). While correctness-based signals emphasize identification of stepwise
errors, potential-based signals focus on the likelihood of reaching the correct
final answer. We developed an automated pipeline for constructing large-scale
reward modeling dataset with both signals. A unified, multi-head architecture
was explored to train the two reward models in a multi-task setup,
demonstrating benefits from learning both correctness and potential in
parallel. By combining these two signals into a compound probability, our model
achieves consistent performance improvements across multiple benchmarks.
Empirical evaluations on MATH500 and ProcessBench confirm that this combined
reward significantly outperforms models trained on either reward type alone,
achieving state-of-the-art performance under comparable resource constraints.

</details>


### [70] [Probing for Phonology in Self-Supervised Speech Representations: A Case Study on Accent Perception](https://arxiv.org/abs/2506.17542)
*Nitin Venkateswaran,Kevin Tang,Ratree Wayland*

Main category: cs.CL

TL;DR: The paper explores how self-supervised learning (SSL) speech models encode phonological feature variations relevant to accent perception, focusing on Hindi-English speakers.


<details>
  <summary>Details</summary>
Motivation: Traditional accent perception models overlook the significance of gradient phonological feature variations for accent judgments.

Method: Phonological feature probabilities from Phonet and pretrained embeddings from Wav2Vec2-BERT and WavLM were analyzed, alongside accent judgments from American English native speakers.

Result: Certain features of pretrained representations strongly predict accent perception, correlating with phonological differences between expected American English and realized non-native English segments.

Conclusion: SSL speech representations effectively model accent perception by leveraging interpretable phonological features.

Abstract: Traditional models of accent perception underestimate the role of gradient
variations in phonological features which listeners rely upon for their accent
judgments. We investigate how pretrained representations from current
self-supervised learning (SSL) models of speech encode phonological
feature-level variations that influence the perception of segmental accent. We
focus on three segments: the labiodental approximant, the rhotic tap, and the
retroflex stop, which are uniformly produced in the English of native speakers
of Hindi as well as other languages in the Indian sub-continent. We use the
CSLU Foreign Accented English corpus (Lander, 2007) to extract, for these
segments, phonological feature probabilities using Phonet (V\'asquez-Correa et
al., 2019) and pretrained representations from Wav2Vec2-BERT (Barrault et al.,
2023) and WavLM (Chen et al., 2022) along with accent judgements by native
speakers of American English. Probing analyses show that accent strength is
best predicted by a subset of the segment's pretrained representation features,
in which perceptually salient phonological features that contrast the expected
American English and realized non-native English segments are given prominent
weighting. A multinomial logistic regression of pretrained representation-based
segment distances from American and Indian English baselines on accent ratings
reveals strong associations between the odds of accent strength and distances
from the baselines, in the expected directions. These results highlight the
value of self-supervised speech representations for modeling accent perception
using interpretable phonological features.

</details>


### [71] [AgriCHN: A Comprehensive Cross-domain Resource for Chinese Agricultural Named Entity Recognition](https://arxiv.org/abs/2506.17578)
*Lingxiao Zeng,Yiqi Tong,Wei Guo,Huarui Wu,Lihao Ge,Yijun Ye,Fuzhen Zhuang,Deqing Wang,Wei Guo,Cheng Chen*

Main category: cs.CL

TL;DR: The paper introduces AgriCHN, an open-source Chinese dataset for agricultural named entity recognition (NER), addressing the lack of high-quality datasets and including entities in hydrology and meteorology.


<details>
  <summary>Details</summary>
Motivation: Enhance agricultural named entity recognition by tackling the scarcity of quality Chinese datasets and including diverse entities from hydrology and meteorology.

Method: Curated a dataset from agricultural articles, designed a benchmark task using advanced neural NER models, and validated its quality and challenge level.

Result: AgriCHN contains 4,040 sentences and 15,799 entity mentions across 27 categories, showing richer entity diversity and fine-grained divisions compared to existing resources.

Conclusion: AgriCHN raises the standard for agricultural NER tasks, offering a challenging dataset that facilitates future research opportunities through its richness and quality.

Abstract: Agricultural named entity recognition is a specialized task focusing on
identifying distinct agricultural entities within vast bodies of text,
including crops, diseases, pests, and fertilizers. It plays a crucial role in
enhancing information extraction from extensive agricultural text resources.
However, the scarcity of high-quality agricultural datasets, particularly in
Chinese, has resulted in suboptimal performance when employing mainstream
methods for this purpose. Most earlier works only focus on annotating
agricultural entities while overlook the profound correlation of agriculture
with hydrology and meteorology. To fill this blank, we present AgriCHN, a
comprehensive open-source Chinese resource designed to promote the accuracy of
automated agricultural entity annotation. The AgriCHN dataset has been
meticulously curated from a wealth of agricultural articles, comprising a total
of 4,040 sentences and encapsulating 15,799 agricultural entity mentions
spanning 27 diverse entity categories. Furthermore, it encompasses entities
from hydrology to meteorology, thereby enriching the diversity of entities
considered. Data validation reveals that, compared with relevant resources,
AgriCHN demonstrates outstanding data quality, attributable to its richer
agricultural entity types and more fine-grained entity divisions. A benchmark
task has also been constructed using several state-of-the-art neural NER
models. Extensive experimental results highlight the significant challenge
posed by AgriCHN and its potential for further research.

</details>


### [72] [Mind the Gap: Assessing Wiktionary's Crowd-Sourced Linguistic Knowledge on Morphological Gaps in Two Related Languages](https://arxiv.org/abs/2506.17603)
*Jonathan Sakunkoo,Annabella Sakunkoo*

Main category: cs.CL

TL;DR: The paper investigates morphological defectivity (missing inflectional forms), validating defective verb lists from Wiktionary using a neural analyzer for Latin and Italian. While Wiktionary is reliable for Italian, 7% of Latin entries are misclassified.


<details>
  <summary>Details</summary>
Motivation: To address the lack of accurate NLP tools accommodating morphological defectivity in morphologically rich languages, a phenomenon underrepresented in traditional resources and requiring extensive expertise.

Method: The study employs a neural morphological analyzer to annotate corpora of Latin and Italian, computationally validating Wiktionary’s lists of defective verbs for these languages using annotated data.

Result: The findings reveal that Wiktionary is highly reliable for Italian defectivity but 7% of its Latin defective verb listings are contradicted by corpus evidence, suggesting they are not defective.

Conclusion: While crowd-sourced wikis like Wiktionary are valuable for rare linguistic information, they have reliability limits for lesser-documented phenomena. The paper provides scalable validation tools to enhance computational morphology and knowledge of defectivity in rich languages.

Abstract: Morphological defectivity is an intriguing and understudied phenomenon in
linguistics. Addressing defectivity, where expected inflectional forms are
absent, is essential for improving the accuracy of NLP tools in morphologically
rich languages. However, traditional linguistic resources often lack coverage
of morphological gaps as such knowledge requires significant human expertise
and effort to document and verify. For scarce linguistic phenomena in
under-explored languages, Wikipedia and Wiktionary often serve as among the few
accessible resources. Despite their extensive reach, their reliability has been
a subject of controversy. This study customizes a novel neural morphological
analyzer to annotate Latin and Italian corpora. Using the massive annotated
data, crowd-sourced lists of defective verbs compiled from Wiktionary are
validated computationally. Our results indicate that while Wiktionary provides
a highly reliable account of Italian morphological gaps, 7% of Latin lemmata
listed as defective show strong corpus evidence of being non-defective. This
discrepancy highlights potential limitations of crowd-sourced wikis as
definitive sources of linguistic knowledge, particularly for less-studied
phenomena and languages, despite their value as resources for rare linguistic
features. By providing scalable tools and methods for quality assurance of
crowd-sourced data, this work advances computational morphology and expands
linguistic knowledge of defectivity in non-English, morphologically rich
languages.

</details>


### [73] [TyphoFormer: Language-Augmented Transformer for Accurate Typhoon Track Forecasting](https://arxiv.org/abs/2506.17609)
*Lincan Li,Eren Erman Ozguven,Yue Zhao,Guang Wang,Yiqun Xie,Yushun Dong*

Main category: cs.CL

TL;DR: The paper introduces TyphoFormer, a Transformer-based framework that leverages natural language prompts for improved typhoon trajectory forecasting.


<details>
  <summary>Details</summary>
Motivation: The lack of contextual knowledge often limits the forecasting reliability of sparse meteorological trajectories like typhoon tracks, even with Transformer-based models.

Method: TyphoFormer integrates numerical typhoon data with natural language descriptions generated by a Large Language Model (LLM), which are embedded as auxiliary tokens in a unified Transformer encoder.

Result: TyphoFormer demonstrates superior performance on the HURDAT2 benchmark compared to state-of-the-art methods, especially in scenarios with nonlinear path shifts and limited data.

Conclusion: Using natural language descriptions as auxiliary prompts enhances typhoon trajectory forecasting, emphasizing the value of combining textual and numerical data.

Abstract: Accurate typhoon track forecasting is crucial for early system warning and
disaster response. While Transformer-based models have demonstrated strong
performance in modeling the temporal dynamics of dense trajectories of humans
and vehicles in smart cities, they usually lack access to broader contextual
knowledge that enhances the forecasting reliability of sparse meteorological
trajectories, such as typhoon tracks. To address this challenge, we propose
TyphoFormer, a novel framework that incorporates natural language descriptions
as auxiliary prompts to improve typhoon trajectory forecasting. For each time
step, we use Large Language Model (LLM) to generate concise textual
descriptions based on the numerical attributes recorded in the North Atlantic
hurricane database. The language descriptions capture high-level meteorological
semantics and are embedded as auxiliary special tokens prepended to the
numerical time series input. By integrating both textual and sequential
information within a unified Transformer encoder, TyphoFormer enables the model
to leverage contextual cues that are otherwise inaccessible through numerical
features alone. Extensive experiments are conducted on HURDAT2 benchmark,
results show that TyphoFormer consistently outperforms other state-of-the-art
baseline methods, particularly under challenging scenarios involving nonlinear
path shifts and limited historical observations.

</details>


### [74] [OpusLM: A Family of Open Unified Speech Language Models](https://arxiv.org/abs/2506.17611)
*Jinchuan Tian,William Chen,Yifan Peng,Jiatong Shi,Siddhant Arora,Shikhar Bharadwaj,Takashi Maekaku,Yusuke Shinohara,Keita Goto,Xiang Yue,Huck Yang,Shinji Watanabe*

Main category: cs.CL

TL;DR: The paper introduces OpusLMs, open foundational Speech Language Models trained on a mix of speech-text pairs and text-only tokens, achieving competitive performance in various speech-related tasks.


<details>
  <summary>Details</summary>
Motivation: To create high-performing, open, and transparent Speech Language Models that rival or surpass existing ones in performance across multiple tasks.

Method: The paper utilizes decoder-only text language models that are pre-trained on a vast dataset of speech-text pairs and text-only tokens, incorporating multi-stream models, tokenization strategies, and multi-stage training.

Result: OpusLMs show comparable or superior results in speech recognition, speech synthesis, and text-based tasks compared to existing models.

Conclusion: OpusLMs demonstrate the value of open foundational SpeechLMs, offering tools, data, and training logs for advancing open research in this domain.

Abstract: This paper presents Open Unified Speech Language Models (OpusLMs), a family
of open foundational speech language models (SpeechLMs) up to 7B. Initialized
from decoder-only text language models, the OpusLMs are continuously
pre-trained on 213K hours of speech-text pairs and 292B text-only tokens. We
demonstrate our OpusLMs achieve comparable (or even superior) performance with
existing SpeechLMs in speech recognition, speech synthesis, and text-only
capabilities. Technically, this paper articulates our SpeechLM designs on
tokenization, multi-stream language models, and multi-stage training
strategies. We experimentally demonstrate the importance of model size scaling
and the effect of annealing data selection. The OpusLMs are all built from
publicly available materials and are fully transparent models. We release our
code, data, checkpoints, and training logs to facilitate open SpeechLM research

</details>


### [75] [Answer-Centric or Reasoning-Driven? Uncovering the Latent Memory Anchor in LLMs](https://arxiv.org/abs/2506.17630)
*Yang Wu,Yifan Zhang,Yiwei Wang,Yujun Cai,Yurong Wu,Yuran Wang,Ning Xu,Jian Cheng*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) tend to rely heavily on recognizing explicit answers rather than genuine reasoning processes for their responses.


<details>
  <summary>Details</summary>
Motivation: The study investigates whether LLMs genuinely infer answers or simply rely on memorized patterns of reasoning tied to explicit answers.

Method: A five-level answer-visibility prompt framework was introduced to manipulate available answer cues and analyze LLM behavior systematically through indirect and behavioral testing.

Result: Performance of LLMs dropped significantly (by 26.90%) when explicit answer cues were masked, suggesting reliance on these cues despite access to reasoning chains.

Conclusion: LLMs may exhibit post-hoc rationalizations rather than true inferential reasoning, revealing a need for a deeper understanding of their "reasoning" mechanisms.

Abstract: While Large Language Models (LLMs) demonstrate impressive reasoning
capabilities, growing evidence suggests much of their success stems from
memorized answer-reasoning patterns rather than genuine inference. In this
work, we investigate a central question: are LLMs primarily anchored to final
answers or to the textual pattern of reasoning chains? We propose a five-level
answer-visibility prompt framework that systematically manipulates answer cues
and probes model behavior through indirect, behavioral analysis. Experiments
across state-of-the-art LLMs reveal a strong and consistent reliance on
explicit answers. The performance drops by 26.90\% when answer cues are masked,
even with complete reasoning chains. These findings suggest that much of the
reasoning exhibited by LLMs may reflect post-hoc rationalization rather than
true inference, calling into question their inferential depth. Our study
uncovers the answer-anchoring phenomenon with rigorous empirical validation and
underscores the need for a more nuanced understanding of what constitutes
reasoning in LLMs.

</details>


### [76] [Step-Opt: Boosting Optimization Modeling in LLMs through Iterative Data Synthesis and Structured Validation](https://arxiv.org/abs/2506.17637)
*Yang Wu,Yifan Zhang,Yurong Wu,Yuran Wang,Junkai Zhang,Jian Cheng*

Main category: cs.CL

TL;DR: Step-Opt-Instruct is a framework designed for fine-tuning Large Language Models (LLMs) to tackle complex optimization problems in Operations Research, showing state-of-the-art performance and significant improvements in benchmarks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge faced by LLMs in performing optimization modeling tasks within Operations Research, particularly when problems are complex.

Method: The Step-Opt-Instruct framework iteratively generates increasingly complex problems and employs stepwise validation to refine and ensure the quality of fine-tuning datasets. These datasets are subsequently used to fine-tune open-source LLMs such as LLaMA-3-8B and Mistral-7B.

Result: Step-Opt achieves state-of-the-art results on benchmarks like NL4OPT, MAMO, and IndustryOR, with a 17.01% improvement in micro average accuracy on difficult optimization problems.

Conclusion: The proposed approach demonstrates the efficacy of combining structured validation with incremental problem complexity to significantly improve LLM capabilities in solving complex optimization problems, enhancing automation in decision-making processes.

Abstract: Large Language Models (LLMs) have revolutionized various domains but
encounter substantial challenges in tackling optimization modeling tasks for
Operations Research (OR), particularly when dealing with complex problem. In
this work, we propose Step-Opt-Instruct, a framework that augments existing
datasets and generates high-quality fine-tuning data tailored to optimization
modeling. Step-Opt-Instruct employs iterative problem generation to
systematically increase problem complexity and stepwise validation to
rigorously verify data, preventing error propagation and ensuring the quality
of the generated dataset. Leveraging this framework, we fine-tune open-source
LLMs, including LLaMA-3-8B and Mistral-7B, to develop Step-Opt--a model that
achieves state-of-the-art performance on benchmarks such as NL4OPT, MAMO, and
IndustryOR. Extensive experiments demonstrate the superior performance of
Step-Opt, especially in addressing complex OR tasks, with a notable 17.01\%
improvement in micro average accuracy on difficult problems. These findings
highlight the effectiveness of combining structured validation with gradual
problem refinement to advance the automation of decision-making processes using
LLMs.The code and dataset are available at https://github.com/samwu-learn/Step.

</details>


### [77] [TPTT: Transforming Pretrained Transformer into Titans](https://arxiv.org/abs/2506.17671)
*Fabien Furfaro*

Main category: cs.CL

TL;DR: The paper introduces TPTT, a framework enhancing pretrained Transformers with better efficiency and accuracy for long-context tasks.


<details>
  <summary>Details</summary>
Motivation: Enhancing large language models (LLMs) to address computational and memory challenges, particularly in long-context inference.

Method: TPTT modifies pretrained Transformers with Memory as Gate (MaG), mixed linearized attention (LiZA), and parameter-efficient fine-tuning (LoRA). It integrates into the Hugging Face Transformers library.

Result: Models enhanced with TPTT show significant efficiency and accuracy improvements, such as a 20% gain in Exact Match on the MMLU benchmark.

Conclusion: TPTT provides a scalable and robust solution for improving the performance of LLMs in a computationally efficient way, confirmed by statistical and state-of-the-art comparisons.

Abstract: Recent advances in large language models (LLMs) have led to remarkable
progress in natural language processing, but their computational and memory
demands remain a significant challenge, particularly for long-context
inference. We introduce TPTT (Transforming Pretrained Transformer into Titans),
a novel framework for enhancing pretrained Transformer models with efficient
linearized attention mechanisms and advanced memory management. TPTT employs
techniques such as Memory as Gate (MaG) and mixed linearized attention (LiZA).
It is fully compatible with the Hugging Face Transformers library, enabling
seamless adaptation of any causal LLM through parameter-efficient fine-tuning
(LoRA) without full retraining. We show the effectiveness of TPTT on the MMLU
benchmark with models of approximately 1 billion parameters, observing
substantial improvements in both efficiency and accuracy. For instance,
Titans-Llama-3.2-1B achieves a 20% increase in Exact Match (EM) over its
baseline. Statistical analyses and comparisons with recent state-of-the-art
methods confirm the practical scalability and robustness of TPTT. Code is
available at https://github.com/fabienfrfr/tptt . Python package at
https://pypi.org/project/tptt/ .

</details>


### [78] [Resource-Friendly Dynamic Enhancement Chain for Multi-Hop Question Answering](https://arxiv.org/abs/2506.17692)
*Binquan Ji,Haibo Luo,Yifei Lu,Lei Hei,Jiaqi Wang,Tingjing Liao,Lingyu Wang,Shichao Wang,Feiliang Ren*

Main category: cs.CL

TL;DR: DEC framework improves multi-hop question answering by decomposing complex queries into subquestions and refining them iteratively, combined with lightweight keyword-based retrieval.


<details>
  <summary>Details</summary>
Motivation: To address challenges like hallucinations and semantic drift while enabling knowledge-intensive multi-hop QA using lightweight LLMs with fewer parameters.

Method: The DEC framework first decomposes complex questions into subquestions to avoid reasoning errors, iteratively refines them in context, and employs a keyword-based discriminative retrieval module.

Result: DEC achieves state-of-the-art performance in multi-hop QA tasks, notably with 8B parameter models, while reducing token usage and computational overhead.

Conclusion: DEC is particularly effective in resource-constrained settings, demonstrating the feasibility of accurate multi-hop QA using lightweight LLMs.

Abstract: Knowledge-intensive multi-hop question answering (QA) tasks, which require
integrating evidence from multiple sources to address complex queries, often
necessitate multiple rounds of retrieval and iterative generation by large
language models (LLMs). However, incorporating many documents and extended
contexts poses challenges -such as hallucinations and semantic drift-for
lightweight LLMs with fewer parameters. This work proposes a novel framework
called DEC (Dynamic Enhancement Chain). DEC first decomposes complex questions
into logically coherent subquestions to form a hallucination-free reasoning
chain. It then iteratively refines these subquestions through context-aware
rewriting to generate effective query formulations. For retrieval, we introduce
a lightweight discriminative keyword extraction module that leverages extracted
keywords to achieve targeted, precise document recall with relatively low
computational overhead. Extensive experiments on three multi-hop QA datasets
demonstrate that DEC performs on par with or surpasses state-of-the-art
benchmarks while significantly reducing token consumption. Notably, our
approach attains state-of-the-art results on models with 8B parameters,
showcasing its effectiveness in various scenarios, particularly in
resource-constrained environments.

</details>


### [79] [Zero-Shot Conversational Stance Detection: Dataset and Approaches](https://arxiv.org/abs/2506.17693)
*Yuzhe Ding,Kang He,Bobo Li,Li Zheng,Haijun He,Fei Li,Chong Teng,Donghong Ji*

Main category: cs.CL

TL;DR: The paper introduces ZS-CSD, a large-scale conversational stance detection dataset with 280 targets, addressing limitations in existing datasets. It also presents SITPCL, a model for zero-shot stance detection.


<details>
  <summary>Details</summary>
Motivation: To overcome the restricted scope of existing conversational stance detection datasets and improve model performance on unseen targets.

Method: The authors curate a zero-shot dataset, ZS-CSD, and propose SITPCL, which employs prototypical contrastive learning focused on speaker interactions and target-awareness.

Result: SITPCL achieves state-of-the-art performance with an F1-macro score of 43.81% in zero-shot conversational stance detection.

Conclusion: While SITPCL advances zero-shot conversational stance detection, the challenging nature of the task is evident from the achieved score, signaling opportunities for further research.

Abstract: Stance detection, which aims to identify public opinion towards specific
targets using social media data, is an important yet challenging task. With the
increasing number of online debates among social media users, conversational
stance detection has become a crucial research area. However, existing
conversational stance detection datasets are restricted to a limited set of
specific targets, which constrains the effectiveness of stance detection models
when encountering a large number of unseen targets in real-world applications.
To bridge this gap, we manually curate a large-scale, high-quality zero-shot
conversational stance detection dataset, named ZS-CSD, comprising 280 targets
across two distinct target types. Leveraging the ZS-CSD dataset, we propose
SITPCL, a speaker interaction and target-aware prototypical contrastive
learning model, and establish the benchmark performance in the zero-shot
setting. Experimental results demonstrate that our proposed SITPCL model
achieves state-of-the-art performance in zero-shot conversational stance
detection. Notably, the SITPCL model attains only an F1-macro score of 43.81%,
highlighting the persistent challenges in zero-shot conversational stance
detection.

</details>


### [80] [The Evolution of Natural Language Processing: How Prompt Optimization and Language Models are Shaping the Future](https://arxiv.org/abs/2506.17700)
*Summra Saleem,Muhammad Nabeel Asim,Shaista Zulfiqar,Andreas Dengel*

Main category: cs.CL

TL;DR: This paper explores 11 classes of prompt optimization strategies for LLMs, analyzing their paradigms and applications to NLP tasks and datasets.


<details>
  <summary>Details</summary>
Motivation: Address the gap in systematic analysis of prompt optimization strategies with LLMs in existing research.

Method: Categorizes prompt optimization methods into 11 types, analyzes them, and compiles relevant applications and evaluations across tasks and benchmarks.

Result: Comprehensive categorization and insights on prompt strategies, enabling comparative studies and systematic assessments.

Conclusion: Centralizes knowledge for prompt optimization, paving the way for adaptability and innovation in NLP task-solving.

Abstract: Large Language Models (LLMs) have revolutionized the field of Natural
Language Processing (NLP) by automating traditional labor-intensive tasks and
consequently accelerated the development of computer-aided applications. As
researchers continue to advance this field with the introduction of novel
language models and more efficient training/finetuning methodologies, the idea
of prompt engineering and subsequent optimization strategies with LLMs has
emerged as a particularly impactful trend to yield a substantial performance
boost across diverse NLP tasks. To best of our knowledge numerous review
articles have explored prompt engineering, however, a critical gap exists in
comprehensive analyses of prompt optimization strategies. To bridge this gap
this paper provides unique and comprehensive insights about the potential of
diverse prompt optimization strategies. It analyzes their underlying working
paradigms and based on these principles, categorizes them into 11 distinct
classes. Moreover, the paper provides details about various NLP tasks where
these prompt optimization strategies have been employed, along with details of
different LLMs and benchmark datasets used for evaluation. This comprehensive
compilation lays a robust foundation for future comparative studies and enables
rigorous assessment of prompt optimization and LLM-based predictive pipelines
under consistent experimental settings: a critical need in the current
landscape. Ultimately, this research will centralize diverse strategic
knowledge to facilitate the adaptation of existing prompt optimization
strategies for development of innovative predictors across unexplored tasks.

</details>


### [81] [Aged to Perfection: Machine-Learning Maps of Age in Conversational English](https://arxiv.org/abs/2506.17708)
*MingZe Tang*

Main category: cs.CL

TL;DR: The study analyzes the British National Corpus 2014 to investigate linguistic patterns across age groups, using computational techniques to predict speaker age.


<details>
  <summary>Details</summary>
Motivation: To understand how language patterns vary with age and uncover sociolinguistic diversity within modern British speech.

Method: The researchers used computational language analysis and machine learning to identify linguistic markers and predict speaker age group.

Result: The study reveals distinctive linguistic patterns tied to age groups and develops prediction models for estimating speaker age.

Conclusion: The findings enhance understanding of sociolinguistic diversity in contemporary British English across different life stages.

Abstract: The study uses the British National Corpus 2014, a large sample of
contemporary spoken British English, to investigate language patterns across
different age groups. Our research attempts to explore how language patterns
vary between different age groups, exploring the connection between speaker
demographics and linguistic factors such as utterance duration, lexical
diversity, and word choice. By merging computational language analysis and
machine learning methodologies, we attempt to uncover distinctive linguistic
markers characteristic of multiple generations and create prediction models
that can consistently estimate the speaker's age group from various aspects.
This work contributes to our knowledge of sociolinguistic diversity throughout
the life of modern British speech.

</details>


### [82] [Unveiling Factors for Enhanced POS Tagging: A Study of Low-Resource Medieval Romance Languages](https://arxiv.org/abs/2506.17715)
*Matthias Schöffel,Esteban Garces Arias,Marinus Wiedner,Paula Ruppert,Meimingwei Li,Christian Heumann,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: The study evaluates how well various methods perform in POS tagging for Medieval Romance languages, using different text corpora and techniques.


<details>
  <summary>Details</summary>
Motivation: The research addresses challenges in part-of-speech tagging for Medieval Romance languages, caused by diachronic linguistic evolution, non-standardized spelling, and lack of labeled data.

Method: The study involves fine-tuning, prompt engineering, varied model architectures, decoding strategies, and cross-lingual transfer learning to assess POS tagging performance across different historical text corpora.

Result: Findings reveal limitations of LLMs in addressing historical language variations, but highlight promising techniques tailored for low-resource historical languages.

Conclusion: While LLMs struggle with historical language specifics, specialized methods demonstrate potential to overcome these challenges effectively.

Abstract: Part-of-speech (POS) tagging remains a foundational component in natural
language processing pipelines, particularly critical for historical text
analysis at the intersection of computational linguistics and digital
humanities. Despite significant advancements in modern large language models
(LLMs) for ancient languages, their application to Medieval Romance languages
presents distinctive challenges stemming from diachronic linguistic evolution,
spelling variations, and labeled data scarcity. This study systematically
investigates the central determinants of POS tagging performance across diverse
corpora of Medieval Occitan, Medieval Spanish, and Medieval French texts,
spanning biblical, hagiographical, medical, and dietary domains. Through
rigorous experimentation, we evaluate how fine-tuning approaches, prompt
engineering, model architectures, decoding strategies, and cross-lingual
transfer learning techniques affect tagging accuracy. Our results reveal both
notable limitations in LLMs' ability to process historical language variations
and non-standardized spelling, as well as promising specialized techniques that
effectively address the unique challenges presented by low-resource historical
languages.

</details>


### [83] [KAG-Thinker: Teaching Large Language Models to Think with Human-like Reasoning Process](https://arxiv.org/abs/2506.17728)
*Dalong Zhang,Jun Xu,Jun Zhou,Lei Liang,Lin Yuan,Ling Zhong,Mengshu Sun,Peilong Zhao,QiWei Wang,Xiaorui Wang,Xinkai Du,YangYang Hou,Yu Ao,ZhaoYang Wang,Zhengke Gui,ZhiYing Yi,Zhongpu Bo*

Main category: cs.CL

TL;DR: KAG-Thinker is a reasoning framework built on a compact LLM that enhances logical and contextual reasoning for question-answering tasks within domain-specific knowledge bases.


<details>
  <summary>Details</summary>
Motivation: To improve the logical coherence and contextual consistency in reasoning processes for complex Q&A tasks in domain-specific knowledge bases.

Method: Decomposes questions into sub-problems using breadth decomposition, employs a knowledge boundary model for retrieval, and uses supervised fine-tuning with multi-turn dialogues.

Result: KAG-Thinker establishes a structured thinking process, integrates external KBs as part of its reasoning, and improves reasoning efficiency and accuracy.

Conclusion: A structured, efficient reasoning framework enhances LLM capabilities in complex domain-specific Q&A scenarios, avoiding excessive computation.

Abstract: In this paper, we introduce KAG-Thinker, a novel human-like reasoning
framework built upon a parameter-light large language model (LLM). Our approach
enhances the logical coherence and contextual consistency of the thinking
process in question-answering (Q\&A) tasks on domain-specific knowledge bases
(KBs) within LLMs. This framework simulates human cognitive mechanisms for
handling complex problems by establishing a structured thinking process.
Continuing the \textbf{Logical Form} guided retrieval and reasoning technology
route of KAG v0.7, firstly, it decomposes complex questions into independently
solvable sub-problems(also referred to as logical forms) through
\textbf{breadth decomposition}, each represented in two equivalent
forms-natural language and logical function-and further classified as either
Knowledge Retrieval or Reasoning Analysis tasks, with dependencies and
variables passing explicitly modeled via logical function interfaces. In the
solving process, the Retrieval function is used to perform knowledge retrieval
tasks, while the Math and Deduce functions are used to perform reasoning
analysis tasks. Secondly, it is worth noting that, in the Knowledge Retrieval
sub-problem tasks, LLMs and external knowledge sources are regarded as
equivalent KBs. We use the \textbf{knowledge boundary} model to determine the
optimal source using self-regulatory mechanisms such as confidence calibration
and reflective reasoning, and use the \textbf{depth solving} model to enhance
the comprehensiveness of knowledge acquisition. Finally, instead of utilizing
reinforcement learning, we employ supervised fine-tuning with multi-turn
dialogues to align the model with our structured inference paradigm, thereby
avoiding excessive reflection. This is supported by a data evaluation framework
and iterative corpus synthesis, which facilitate the generation of detailed
reasoning trajectories...

</details>


### [84] [HIDE and Seek: Detecting Hallucinations in Language Models via Decoupled Representations](https://arxiv.org/abs/2506.17748)
*Anwoy Chatterjee,Yash Goel,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: The paper introduces HIDE, a single-pass method to detect hallucinations in Language Models (LMs) by measuring decoupled internal representations.


<details>
  <summary>Details</summary>
Motivation: Addressing the issue of hallucination in Language Models, a problem where LMs generate content that is factually inaccurate or contextually unfaithful, undermining their reliability.

Method: Proposes HIDE (Hallucination detectIon via Decoupled rEpresentations), a training-free, single-pass method using the Hilbert-Schmidt Independence Criterion (HSIC) to measure statistical decoupling in LM hidden-state representations.

Result: HIDE outperforms other single-pass methods by ~29% in AUC-ROC and rivals multi-pass state-of-the-art methods with ~3% improvement while using ~51% less computation time.

Conclusion: HIDE is a highly effective and efficient hallucination detection method, leveraging decoupling of internal representations for practical LM evaluation.

Abstract: Contemporary Language Models (LMs), while impressively fluent, often generate
content that is factually incorrect or unfaithful to the input context - a
critical issue commonly referred to as 'hallucination'. This tendency of LMs to
generate hallucinated content undermines their reliability, especially because
these fabrications are often highly convincing and therefore difficult to
detect. While several existing methods attempt to detect hallucinations, most
rely on analyzing multiple generations per input, leading to increased
computational cost and latency. To address this, we propose a single-pass,
training-free approach for effective Hallucination detectIon via Decoupled
rEpresentations (HIDE). Our approach leverages the hypothesis that
hallucinations result from a statistical decoupling between an LM's internal
representations of input context and its generated output. We quantify this
decoupling using the Hilbert-Schmidt Independence Criterion (HSIC) applied to
hidden-state representations extracted while generating the output sequence. We
conduct extensive experiments on four diverse question answering datasets,
evaluating both faithfulness and factuality hallucinations across six
open-source LMs of varying scales and properties. Our results demonstrate that
HIDE outperforms other single-pass methods in almost all settings, achieving an
average relative improvement of ~29% in AUC-ROC over the best-performing
single-pass strategy across various models and datasets. Additionally, HIDE
shows competitive and often superior performance with multi-pass
state-of-the-art methods, obtaining an average relative improvement of ~3% in
AUC-ROC while consuming ~51% less computation time. Our findings highlight the
effectiveness of exploiting internal representation decoupling in LMs for
efficient and practical hallucination detection.

</details>


### [85] [Multilingual Tokenization through the Lens of Indian Languages: Challenges and Insights](https://arxiv.org/abs/2506.17789)
*N J Karthika,Maharaj Brahma,Rohit Saluja,Ganesh Ramakrishnan,Maunendra Sankar Desarkar*

Main category: cs.CL

TL;DR: This paper evaluates tokenization strategies for 17 Indian languages, focusing on fairness, efficiency, and tackling challenges faced by linguistically diverse, low-resource languages.


<details>
  <summary>Details</summary>
Motivation: Existing tokenizers favor high-resource languages, making them ineffective for morphologically rich and diverse languages like those from the Indian subcontinent.

Method: The study analyzes bottom-up and top-down tokenization algorithms, evaluates multilingual vocabulary approaches, considers the effects of vocabulary sizes, and examines how low-resource languages can leverage related high-resource language tokenizers.

Result: The paper identifies trade-offs in tokenization approaches, highlights benefits of related-language training for low-resource languages, and uncovers strategies for equitable multilingual tokenizer construction.

Conclusion: The findings inform the creation of fairer and more efficient tokenizers, enabling better handling of linguistically diverse and low-resource languages in multilingual NLP tasks.

Abstract: Tokenization plays a pivotal role in multilingual NLP. However, existing
tokenizers are often skewed towards high-resource languages, limiting their
effectiveness for linguistically diverse and morphologically rich languages
such as those in the Indian subcontinent. This paper presents a comprehensive
intrinsic evaluation of tokenization strategies across 17 Indian languages. We
quantify the trade-offs between bottom-up and top-down tokenizer algorithms
(BPE and Unigram LM), effects of vocabulary sizes, and compare strategies of
multilingual vocabulary construction such as joint and cluster-based training.
We also show that extremely low-resource languages can benefit from tokenizers
trained on related high-resource languages. Our study provides practical
insights for building more fair, efficient, and linguistically informed
tokenizers for multilingual NLP.

</details>


### [86] [THCM-CAL: Temporal-Hierarchical Causal Modelling with Conformal Calibration for Clinical Risk Prediction](https://arxiv.org/abs/2506.17844)
*Xin Zhang,Qiyu Wei,Yingjie Zhu,Fanyi Wu,Sophia Ananiadou*

Main category: cs.CL

TL;DR: This paper introduces THCM-CAL, a new method for multimodal clinical risk prediction considering structured and unstructured EHR data.


<details>
  <summary>Details</summary>
Motivation: There is a need for advanced models that can integrate and effectively analyze EHR data, including diagnostic codes and narrative notes, to improve clinical risk prediction.

Method: The authors propose THCM-CAL, a model that uses a multimodal causal graph and hierarchical causal discovery for analyzing EHR data while enhancing prediction reliability with conformal calibration for ICD coding.

Result: THCM-CAL outperforms existing methods in experimental evaluations on the MIMIC-III and MIMIC-IV datasets.

Conclusion: The proposed THCM-CAL approach offers a superior solution for predicting clinical risks by modeling causal interactions across textual and codified EHR data, along with improved reliability in multi-label predictions.

Abstract: Automated clinical risk prediction from electronic health records (EHRs)
demands modeling both structured diagnostic codes and unstructured narrative
notes. However, most prior approaches either handle these modalities separately
or rely on simplistic fusion strategies that ignore the directional,
hierarchical causal interactions by which narrative observations precipitate
diagnoses and propagate risk across admissions. In this paper, we propose
THCM-CAL, a Temporal-Hierarchical Causal Model with Conformal Calibration. Our
framework constructs a multimodal causal graph where nodes represent clinical
entities from two modalities: Textual propositions extracted from notes and ICD
codes mapped to textual descriptions. Through hierarchical causal discovery,
THCM-CAL infers three clinically grounded interactions: intra-slice
same-modality sequencing, intra-slice cross-modality triggers, and inter-slice
risk propagation. To enhance prediction reliability, we extend conformal
prediction to multi-label ICD coding, calibrating per-code confidence intervals
under complex co-occurrences. Experimental results on MIMIC-III and MIMIC-IV
demonstrate the superiority of THCM-CAL.

</details>


### [87] [LLMs for Customized Marketing Content Generation and Evaluation at Scale](https://arxiv.org/abs/2506.17863)
*Haoran Liu,Amir Tahmasbi,Ehtesham Sam Haque,Purak Jain*

Main category: cs.CL

TL;DR: This paper presents MarketingFM, a system for generating targeted ad content, and an evaluation system called AutoEval for validating its output.


<details>
  <summary>Details</summary>
Motivation: Current ad content in e-commerce marketing is often generic and fails to align effectively with landing pages, limiting its success.

Method: The paper introduces MarketingFM, a retrieval-augmented system for generating specific ad copy, and two evaluation frameworks (AutoEval-Main and AutoEval-Update) that combine automation with human input.

Result: MarketingFM significantly improved ad performance in metrics such as CTR, impressions, and CPC. AutoEval-Main achieved 89.57% agreement with human evaluations, and AutoEval-Update enhanced evaluation efficiency and consistency.

Conclusion: The proposed systems improve ad generation quality and efficiency, though human oversight remains necessary for validation and refinement.

Abstract: Offsite marketing is essential in e-commerce, enabling businesses to reach
customers through external platforms and drive traffic to retail websites.
However, most current offsite marketing content is overly generic,
template-based, and poorly aligned with landing pages, limiting its
effectiveness. To address these limitations, we propose MarketingFM, a
retrieval-augmented system that integrates multiple data sources to generate
keyword-specific ad copy with minimal human intervention. We validate
MarketingFM via offline human and automated evaluations and large-scale online
A/B tests. In one experiment, keyword-focused ad copy outperformed templates,
achieving up to 9% higher CTR, 12% more impressions, and 0.38% lower CPC,
demonstrating gains in ad ranking and cost efficiency. Despite these gains,
human review of generated ads remains costly. To address this, we propose
AutoEval-Main, an automated evaluation system that combines rule-based metrics
with LLM-as-a-Judge techniques to ensure alignment with marketing principles.
In experiments with large-scale human annotations, AutoEval-Main achieved
89.57% agreement with human reviewers. Building on this, we propose
AutoEval-Update, a cost-efficient LLM-human collaborative framework to
dynamically refine evaluation prompts and adapt to shifting criteria with
minimal human input. By selectively sampling representative ads for human
review and using a critic LLM to generate alignment reports, AutoEval-Update
improves evaluation consistency while reducing manual effort. Experiments show
the critic LLM suggests meaningful refinements, improving LLM-human agreement.
Nonetheless, human oversight remains essential for setting thresholds and
validating refinements before deployment.

</details>


### [88] [QueueEDIT: Structural Self-Correction for Sequential Model Editing in LLMs](https://arxiv.org/abs/2506.17864)
*Taolin Zhang,Haidong Kang,Dongyang Li,Qizhou Chen,Chengyu Wang Xiaofeng He,Richang Hong*

Main category: cs.CL

TL;DR: This paper introduces QueueEDIT, a model editing framework for LLMs designed to improve sequential editing performance and minimize negative impacts on the models' general capabilities.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) excel in various tasks but often suffer from factual inaccuracies and hallucinations. Sequential model editing (SME) addresses continuous corrections but risks degrading the overall performance of LLMs.

Method: The QueueEDIT framework uses structural mapping loss to map knowledge-sensitive neurons, stores edited parameters in a queue, and dynamically selects relevant ones for realignment while freezing irrelevant ones, minimizing influence on general LLM abilities.

Result: QueueEDIT outperforms existing model editing approaches in sequential editing settings and maintains high performance in general NLP tasks without compromising LLM capabilities.

Conclusion: The proposed framework achieves effective sequential model editing with reduced harm to general capabilities, demonstrating scalability and robustness in diverse editing scenarios.

Abstract: Recently, large language models (LLMs) have demonstrated impressive results
but still suffer from hallucinations. Model editing has been proposed to
correct factual inaccuracies in LLMs. A challenging case is sequential model
editing (SME), which aims to rectify errors continuously rather than treating
them as a one-time task. During SME, the general capabilities of LLMs can be
negatively affected due to the introduction of new parameters. In this paper,
we propose a queue-based self-correction framework (QueueEDIT) that not only
enhances SME performance by addressing long-sequence dependency but also
mitigates the impact of parameter bias on the general capabilities of LLMs.
Specifically, we first introduce a structural mapping editing loss to map the
triplets to the knowledge-sensitive neurons within the Transformer layers of
LLMs. We then store the located parameters for each piece of edited knowledge
in a queue and dynamically align previously edited parameters. In each edit, we
select queue parameters most relevant to the currently located parameters to
determine whether previous knowledge needs realignment. Irrelevant parameters
in the queue are frozen, and we update the parameters at the queue head to the
LLM to ensure they do not harm general abilities. Experiments show that our
framework significantly outperforms strong baselines across various SME
settings and maintains competitiveness in single-turn editing. The resulting
LLMs also preserve high capabilities in general NLP tasks throughout the SME
process.

</details>


### [89] [How Alignment Shrinks the Generative Horizon](https://arxiv.org/abs/2506.17871)
*Chenghao Yang,Ari Holtzman*

Main category: cs.CL

TL;DR: This paper introduces the Branching Factor (BF) to study why aligned large language models tend to generate less diverse outputs and finds that alignment tuning significantly reduces BF, making generation more predictable.


<details>
  <summary>Details</summary>
Motivation: To understand why aligned large language models have reduced output diversity and identify mechanisms behind this phenomenon.

Method: The researchers propose the Branching Factor (BF) as a token-invariant metric to quantify the concentration in the model's output distribution and conduct empirical analyses and nudging experiments.

Result: Key findings include: (1) BF decreases as generation progresses, making LLMs more predictable over time; (2) alignment tuning sharply reduces BF from the outset, explaining reduced sensitivity to decoding strategies.

Conclusion: The findings suggest alignment tuning directs models toward stylistic tokens that unlock existing low-entropy trajectories, aiding predictability and stable outputs. BF emerges as a valuable tool for understanding and controlling diversity in LLM behavior.

Abstract: Despite their impressive capabilities, aligned large language models (LLMs)
often generate outputs that lack diversity. What drives this stability in the
generation? We investigate this phenomenon through the lens of probability
concentration in the model's output distribution. To quantify this
concentration, we introduce the Branching Factor (BF) -- a token-invariant
measure of the effective number of plausible next steps during generation. Our
empirical analysis reveals two key findings: (1) BF often decreases as
generation progresses, suggesting that LLMs become more predictable as they
generate. (2) alignment tuning substantially sharpens the model's output
distribution from the outset, reducing BF by nearly an order of magnitude
(e.g., from 12 to 1.2) relative to base models. This stark reduction helps
explain why aligned models often appear less sensitive to decoding strategies.
Building on this insight, we find this stability has surprising implications
for complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g.,
DeepSeek-distilled models), for instance, leverage this effect; by generating
longer reasoning chains, they push generation into later, more deterministic
(lower BF) stages, resulting in more stable outputs. We hypothesize that
alignment tuning does not fundamentally change a model's behavior, but instead
steers it toward stylistic tokens (e.g., "Sure") that unlock low-entropy
trajectories already present in the base model. This view is supported by
nudging experiments, which show that prompting base models with such tokens can
similarly reduce BF. Together, our findings establish BF as a powerful
diagnostic for understanding and controlling LLM outputs - clarifying how
alignment reduces variability, how CoT promotes stable generations, and how
base models can be steered away from diversity.

</details>


### [90] [Multi-turn Jailbreaking via Global Refinement and Active Fabrication](https://arxiv.org/abs/2506.17881)
*Hua Tang,Lingyong Yan,Yukun Zhao,Shuaiqiang Wang,Jizhou Huang,Dawei Yin*

Main category: cs.CL

TL;DR: This paper proposes an advanced method for multi-turn jailbreaking of Large Language Models (LLMs), achieving more effective results in bypassing safety mechanisms compared to prior approaches.


<details>
  <summary>Details</summary>
Motivation: The research aims to address the safety risks associated with LLMs, focusing on multi-turn jailbreaking techniques which are underexplored and require better adaptation to the dynamics of interactions.

Method: A novel multi-turn jailbreaking method was introduced that globally refines the jailbreak strategy at each interaction and actively fabricates responses to suppress safety warnings, enhancing harmful output generation.

Result: The proposed method outperformed existing single-turn and multi-turn jailbreaking techniques when tested on six state-of-the-art LLMs.

Conclusion: The research contributes a more effective approach to identifying vulnerabilities in LLMs, emphasizing the importance of addressing multi-turn jailbreaks for improving model safety.

Abstract: Large Language Models (LLMs) have achieved exceptional performance across a
wide range of tasks. However, they still pose significant safety risks due to
the potential misuse for malicious purposes. Jailbreaks, which aim to elicit
models to generate harmful content, play a critical role in identifying the
underlying security threats. Recent jailbreaking primarily focuses on
single-turn scenarios, while the more complicated multi-turn scenarios remain
underexplored. Moreover, existing multi-turn jailbreaking techniques struggle
to adapt to the evolving dynamics of dialogue as the interaction progresses. To
address this limitation, we propose a novel multi-turn jailbreaking method that
refines the jailbreaking path globally at each interaction. We also actively
fabricate model responses to suppress safety-related warnings, thereby
increasing the likelihood of eliciting harmful outputs in subsequent questions.
Experimental results demonstrate the superior performance of our method
compared with existing single-turn and multi-turn jailbreaking techniques
across six state-of-the-art LLMs. Our code is publicly available at
https://github.com/Ytang520/Multi-Turn_jailbreaking_Global-Refinment_and_Active-Fabrication.

</details>


### [91] [Scatter-Based Innovation Propagation in Large Language Models for Multi-Stage Process Adaptation](https://arxiv.org/abs/2506.17949)
*Hong Su*

Main category: cs.CL

TL;DR: The paper introduces an innovation scatter model for LLMs to generalize ideas across stages, enhancing their adaptability and reuse.


<details>
  <summary>Details</summary>
Motivation: LLMs often fail to generalize innovations beyond their original context due to their localized nature, motivating the need for a systematic method to enable broader application.

Method: The innovation scatter model employs a four-step process: identify the core innovation, generalize it, evaluate its broader applicability, and apply it systematically to structurally similar contexts.

Result: Experiments verify that the innovation scatter model improves LLMs' ability to generalize and reuse innovations across stages.

Conclusion: The proposed model enhances LLMs' generalization capabilities, making them better at applying learned patterns to new and similar scenarios.

Abstract: Large Language Models (LLMs) exhibit strong capabilities in reproducing and
extending patterns observed during pretraining but often struggle to generalize
novel ideas beyond their original context. This paper addresses the challenge
of applying such localized innovations - introduced at a specific stage or
component - to other parts of a multi-stage process. We propose a scatter-based
innovation expansion model (innovation scatter model) that guides the LLM
through a four-step process: (1) identifying the core innovation by comparing
the user's input with its surrounding context, (2) generalizing the innovation
by removing references to specific stages or components, (3) determining
whether the generalized innovation applies to a broader scope beyond the
original stage, and (4) systematically applying it to other structurally
similar stages using the LLM. This model leverages structural redundancy across
stages to improve the applicability of novel ideas. Verification results
demonstrate that the innovation scatter model enables LLMs to extend
innovations across structurally similar stages, thereby enhancing
generalization and reuse.

</details>


### [92] [A Comprehensive Graph Framework for Question Answering with Mode-Seeking Preference Alignment](https://arxiv.org/abs/2506.17951)
*Quanwei Tang,Sophia Yat Mei Lee,Junshuang Wu,Dong Zhang,Shoushan Li,Erik Cambria,Guodong Zhou*

Main category: cs.CL

TL;DR: The paper proposes GraphMPA, a graph-based framework for enhancing retrieval-augmented generation models with improved human preference alignment.


<details>
  <summary>Details</summary>
Motivation: While retrieval-augmented generation has improved LLMs for question answering, challenges remain in achieving global understanding and aligning responses with human ethics and quality.

Method: The authors design GraphMPA, which constructs a hierarchical document graph using similarity measures to emulate cognitive understanding, combined with mode-seeking preference optimization.

Result: Experiments on six datasets confirm the effectiveness of GraphMPA in improving global understanding and alignment with human preferences.

Conclusion: GraphMPA offers a significant enhancement to RAG systems by integrating cognitive-inspired graph construction and effective human preference alignment.

Abstract: Recent advancements in retrieval-augmented generation (RAG) have enhanced
large language models in question answering by integrating external knowledge.
However, challenges persist in achieving global understanding and aligning
responses with human ethical and quality preferences. To address these issues,
we propose GraphMPA, a comprehensive graph-based framework with mode-seeking
preference alignment. Our approach constructs a hierarchical document graph
using a general similarity measurement, mimicking human cognitive processes for
information understanding and synthesis. Additionally, we introduce
mode-seeking preference optimization to better align model outputs with human
preferences through probability-matching constraints. Extensive experiments on
six datasets demonstrate the effectiveness of our
\href{https://github.com/tangquanwei/GraphMPA}{GraphMPA}.

</details>


### [93] [PDF Retrieval Augmented Question Answering](https://arxiv.org/abs/2506.18027)
*Thi Thu Uyen Hoang,Viet Anh Nguyen*

Main category: cs.CL

TL;DR: This paper integrates Retrieval Augmented Generation (RAG) into Question-Answering systems to process multimodal content in PDFs effectively.


<details>
  <summary>Details</summary>
Motivation: Existing QA systems struggle with extracting and answering questions based on multimodal data in PDFs, such as text, images, tables, and graphs.

Method: The authors refined methods within the RAG framework to process non-textual PDF elements, while fine-tuning large language models to improve multimodal question handling.

Result: Experimental evaluations show the system's ability to accurately extract and respond to multimodal queries across various PDF content types.

Conclusion: The study advances RAG-based QA systems, addressing multimodal challenges while providing a platform for future research in PDF data integration and processing.

Abstract: This paper presents an advancement in Question-Answering (QA) systems using a
Retrieval Augmented Generation (RAG) framework to enhance information
extraction from PDF files. Recognizing the richness and diversity of data
within PDFs--including text, images, vector diagrams, graphs, and tables--poses
unique challenges for existing QA systems primarily designed for textual
content. We seek to develop a comprehensive RAG-based QA system that will
effectively address complex multimodal questions, where several data types are
combined in the query. This is mainly achieved by refining approaches to
processing and integrating non-textual elements in PDFs into the RAG framework
to derive precise and relevant answers, as well as fine-tuning large language
models to better adapt to our system. We provide an in-depth experimental
evaluation of our solution, demonstrating its capability to extract accurate
information that can be applied to different types of content across PDFs. This
work not only pushes the boundaries of retrieval-augmented QA systems but also
lays a foundation for further research in multimodal data integration and
processing.

</details>


### [94] [Splitformer: An improved early-exit architecture for automatic speech recognition on edge devices](https://arxiv.org/abs/2506.18035)
*Maxence Lasbordes,Daniele Falavigna,Alessio Brutti*

Main category: cs.CL

TL;DR: The paper proposes an improvement to early-exit neural models for speech recognition by introducing parallel layers that process downsampled inputs, optimizing performance while keeping inference time unchanged.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of dynamically adjusting computational load during inference for on-device applications with limited and variable resources. Existing approaches like early-exit architectures and Zipformer models are either resource-efficient or performant but lack modular integration.

Method: The proposed method introduces parallel layers in early-exit models, which process downsampled versions of inputs alongside standard layers. This modification seeks to enhance performance without significantly increasing computational costs.

Result: The experiments demonstrate improved speech recognition performance on standard benchmarks with minimal increase in model parameters and no negative impact on inference time.

Conclusion: Implementing parallel layers in early-exit architectures effectively balances performance enhancement and resource constraints, making it a practical improvement for on-device speech recognition systems.

Abstract: The ability to dynamically adjust the computational load of neural models
during inference in a resource aware manner is crucial for on-device processing
scenarios, characterised by limited and time-varying computational resources.
Early-exit architectures represent an elegant and effective solution, since
they can process the input with a subset of their layers, exiting at
intermediate branches (the upmost layers are hence removed from the model).
  From a different perspective, for automatic speech recognition applications
there are memory-efficient neural architectures that apply variable frame rate
analysis, through downsampling/upsampling operations in the middle layers,
reducing the overall number of operations and improving significantly the
performance on well established benchmarks. One example is the Zipformer.
However, these architectures lack the modularity necessary to inject early-exit
branches.
  With the aim of improving the performance in early-exit models, we propose
introducing parallel layers in the architecture that process downsampled
versions of their inputs. % in conjunction with standard processing layers. We
show that in this way the speech recognition performance on standard benchmarks
significantly improve, at the cost of a small increase in the overall number of
model parameters but without affecting the inference time.

</details>


### [95] [Markov-Enhanced Clustering for Long Document Summarization: Tackling the 'Lost in the Middle' Challenge with Large Language Models](https://arxiv.org/abs/2506.18036)
*Aziz Amari,Mohamed Achref Ben Ammar*

Main category: cs.CL

TL;DR: The paper addresses challenges in automatic text summarization by proposing a hybrid method that combines extractive and abstractive techniques for improved performance on lengthy documents.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiency and resource-intensive nature of current abstractive summarization models, which struggle with retaining key information in long documents.

Method: The proposed hybrid approach involves segmenting documents into smaller chunks, clustering their vector embeddings, generating summaries per cluster, and constructing a final summary using a Markov chain graph to determine semantic order.

Result: The approach avoids losing key information in lengthy documents, balancing coherence and brevity in summary generation.

Conclusion: The hybrid method effectively combines extractive and abstractive techniques, overcoming common issues in lengthy document summarization while maintaining coherence.

Abstract: The rapid expansion of information from diverse sources has heightened the
need for effective automatic text summarization, which condenses documents into
shorter, coherent texts. Summarization methods generally fall into two
categories: extractive, which selects key segments from the original text, and
abstractive, which generates summaries by rephrasing the content coherently.
Large language models have advanced the field of abstractive summarization, but
they are resourceintensive and face significant challenges in retaining key
information across lengthy documents, which we call being "lost in the middle".
To address these issues, we propose a hybrid summarization approach that
combines extractive and abstractive techniques. Our method splits the document
into smaller text chunks, clusters their vector embeddings, generates a summary
for each cluster that represents a key idea in the document, and constructs the
final summary by relying on a Markov chain graph when selecting the semantic
order of ideas.

</details>


### [96] [Statistical Multicriteria Evaluation of LLM-Generated Text](https://arxiv.org/abs/2506.18082)
*Esteban Garces Arias,Hannah Blocher,Julian Rodemann,Matthias Aßenmacher,Christoph Jansen*

Main category: cs.CL

TL;DR: The paper introduces a novel evaluation framework for assessing LLM-generated text quality using Generalized Stochastic Dominance (GSD), which considers multiple quality dimensions simultaneously.


<details>
  <summary>Details</summary>
Motivation: Current evaluation metrics for LLM-generated text often either oversimplify or inadequately respect the nuances of multiple text quality dimensions, as well as the gap between human judgment (ordinal) and automated metrics (cardinal).

Method: The paper adapts a Generalized Stochastic Dominance (GSD)-based framework, allowing evaluation across multiple dimensions like coherence and fluency, while ensuring statistical rigor without arbitrary metric weighting.

Result: The GSD-based method successfully discriminates between decoding strategies and establishes statistically significant performance differences compared to human-generated text.

Conclusion: The proposed GSD approach addresses key evaluation challenges by respecting diverse measurement scales and ensuring robust statistical guarantees.

Abstract: Assessing the quality of LLM-generated text remains a fundamental challenge
in natural language processing. Current evaluation approaches often rely on
isolated metrics or simplistic aggregations that fail to capture the nuanced
trade-offs between coherence, diversity, fluency, and other relevant indicators
of text quality. In this work, we adapt a recently proposed framework for
statistical inference based on Generalized Stochastic Dominance (GSD) that
addresses three critical limitations in existing benchmarking methodologies:
the inadequacy of single-metric evaluation, the incompatibility between
cardinal automatic metrics and ordinal human judgments, and the lack of
inferential statistical guarantees. The GSD-front approach enables simultaneous
evaluation across multiple quality dimensions while respecting their different
measurement scales, building upon partial orders of decoding strategies, thus
avoiding arbitrary weighting of the involved metrics. By applying this
framework to evaluate common decoding strategies against human-generated text,
we demonstrate its ability to identify statistically significant performance
differences while accounting for potential deviations from the i.i.d.
assumption of the sampling design.

</details>


### [97] [Evaluating Prompt-Based and Fine-Tuned Approaches to Czech Anaphora Resolution](https://arxiv.org/abs/2506.18091)
*Patrik Stano,Aleš Horák*

Main category: cs.CL

TL;DR: This study compares prompt engineering with LLMs and fine-tuning generative models for anaphora resolution in Czech, finding fine-tuned models outperform prompts.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve natural language understanding in Czech, a morphologically rich language, focusing on the challenge of anaphora resolution.

Method: It compares two approaches: using prompt engineering with instruction-tuned large language models and fine-tuning smaller generative models trained on Czech data.

Result: Fine-tuned models (e.g., mT5-large) achieved up to 88% accuracy, outperforming prompted LLMs, which reached up to 74.5% accuracy, while being computationally efficient.

Conclusion: Fine-tuned compact models, particularly mT5-large, are more effective and resource-efficient for Czech anaphora resolution compared to prompt-engineered large models.

Abstract: Anaphora resolution plays a critical role in natural language understanding,
especially in morphologically rich languages like Czech. This paper presents a
comparative evaluation of two modern approaches to anaphora resolution on Czech
text: prompt engineering with large language models (LLMs) and fine-tuning
compact generative models. Using a dataset derived from the Prague Dependency
Treebank, we evaluate several instruction-tuned LLMs, including Mistral Large 2
and Llama 3, using a series of prompt templates. We compare them against
fine-tuned variants of the mT5 and Mistral models that we trained specifically
for Czech anaphora resolution. Our experiments demonstrate that while prompting
yields promising few-shot results (up to 74.5% accuracy), the fine-tuned
models, particularly mT5-large, outperform them significantly, achieving up to
88% accuracy while requiring fewer computational resources. We analyze
performance across different anaphora types, antecedent distances, and source
corpora, highlighting key strengths and trade-offs of each approach.

</details>


### [98] [InspireDebate: Multi-Dimensional Subjective-Objective Evaluation-Guided Reasoning and Optimization for Debating](https://arxiv.org/abs/2506.18102)
*Fuyu Wang,Jiangtong Li,Kun Zhu,Changjun Jiang*

Main category: cs.CL

TL;DR: The paper introduces InspireScore for multi-dimensional argument evaluation and InspireDebate, an optimized debating framework, showing significant improvement in argument assessment and debate systems.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing LLM-based debating systems, which ignore objective assessments and lack structured optimization for multiple dimensions.

Method: Proposed InspireScore, which evaluates arguments by subjective and objective metrics, and InspireDebate, using chain-of-thought reasoning, direct preference optimization, and real-time knowledge grounding.

Result: Empirical evaluations reveal InspireScore has a 44% higher correlation with expert judgments, and InspireDebate improves performance over baselines by 57%.

Conclusion: The framework dramatically enhances the effectiveness of LLM-based debating systems and provides resources for further research through open-source code.

Abstract: With the rapid advancements in large language models (LLMs), debating tasks,
such as argument quality assessment and debate process simulation, have made
significant progress. However, existing LLM-based debating systems focus on
responding to specific arguments while neglecting objective assessments such as
authenticity and logical validity. Furthermore, these systems lack a structured
approach to optimize across various dimensions$-$including evaluation metrics,
chain-of-thought (CoT) reasoning, and multi-turn debate refinement$-$thereby
limiting their effectiveness. To address these interconnected challenges, we
propose a dual-component framework: (1) $\textbf{InspireScore}$, a novel
evaluation system that establishes a multi-dimensional assessment architecture
incorporating four subjective criteria (emotional appeal, argument clarity,
argument arrangement, and topic relevance) alongside two objective metrics
(fact authenticity and logical validity); and (2) $\textbf{InspireDebate}$, an
optimized debating framework employing a phased optimization approach through
CoT reasoning enhancement, multi-dimensional Direct Preference Optimization
(DPO), and real-time knowledge grounding via web-based Retrieval Augmented
Generation (Web-RAG). Empirical evaluations demonstrate that
$\textbf{InspireScore}$ achieves 44$\%$ higher correlation with expert
judgments compared to existing methods, while $\textbf{InspireDebate}$ shows
significant improvements, outperforming baseline models by 57$\%$. Source code
is available at https://github.com/fywang12/InspireDebate.

</details>


### [99] [Chengyu-Bench: Benchmarking Large Language Models for Chinese Idiom Understanding and Use](https://arxiv.org/abs/2506.18105)
*Yicheng Fu,Zhemin Huang,Liuxin Yang,Yumeng Lu,Zhongdongming Dai*

Main category: cs.CL

TL;DR: The paper introduces Chengyu-Bench, a benchmark designed to evaluate language models on understanding and using Chinese idioms (Chengyu) through three tasks: Evaluative Connotation, Appropriateness, and Open Cloze.


<details>
  <summary>Details</summary>
Motivation: Chinese idioms are intricate expressions deeply rooted in cultural and historical context, making them challenging for language models to interpret and use correctly. Existing benchmarks do not adequately evaluate their usage across diverse and nuanced contexts.

Method: The authors created Chengyu-Bench, a benchmark with three tasks—Evaluative Connotation, Appropriateness, and Open Cloze—comprising 2,937 human-verified examples sourced from diverse corpora.

Result: Leading language models performed well in sentiment evaluation (95% accuracy in Evaluative Connotation) but struggled in tasks requiring nuanced understanding—achieving 85% on Appropriateness and only 40% top-1 accuracy on Open Cloze.

Conclusion: The study highlights the limitations of current language models in understanding the cultural and contextual intricacies of Chinese idioms. Chengyu-Bench offers a thorough evaluation framework to address this gap, with resources made publicly available for further exploration.

Abstract: Chinese idioms (Chengyu) are concise four-character expressions steeped in
history and culture, whose literal translations often fail to capture their
full meaning. This complexity makes them challenging for language models to
interpret and use correctly. Existing benchmarks focus on narrow tasks -
multiple-choice cloze tests, isolated translation, or simple paraphrasing. We
introduce Chengyu-Bench, a comprehensive benchmark featuring three tasks: (1)
Evaluative Connotation, classifying idioms as positive or negative; (2)
Appropriateness, detecting incorrect idiom usage in context; and (3) Open
Cloze, filling blanks in longer passages without options. Chengyu-Bench
comprises 2,937 human-verified examples covering 1,765 common idioms sourced
from diverse corpora. We evaluate leading LLMs and find they achieve over 95%
accuracy on Evaluative Connotation, but only ~85% on Appropriateness and ~40%
top-1 accuracy on Open Cloze. Error analysis reveals that most mistakes arise
from fundamental misunderstandings of idiom meanings. Chengyu-Bench
demonstrates that while LLMs can reliably gauge idiom sentiment, they still
struggle to grasp the cultural and contextual nuances essential for proper
usage. The benchmark and source code are available at:
https://github.com/sofyc/ChengyuBench.

</details>


### [100] [Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives](https://arxiv.org/abs/2506.18116)
*Batool Haider,Atmika Gorti,Aman Chadha,Manas Gaur*

Main category: cs.CL

TL;DR: This paper introduces a multi-hop question answering (MHQA) framework to detect and mitigate intersectional biases in Large Language Models (LLMs) used in mental healthcare contexts.


<details>
  <summary>Details</summary>
Motivation: To address and mitigate the inherent biases in LLMs that could harm marginalized groups and reinforce stigma in mental healthcare, particularly where systematic methods to detect intersectional biases are lacking.

Method: A systematic MHQA approach is employed using the Interpretable Mental Health Instruction (IMHI) dataset. The biases are analyzed across various demographics (age, race, gender, socioeconomic status) and evaluated in four LLMs using tasks like tagging and sequential reasoning. Debiasing techniques such as Roleplay Simulation and Explicit Bias Reduction are applied using few-shot prompting with the BBQ dataset.

Result: The MHQA framework outperforms conventional methods in detecting intersectional biases, and the proposed debiasing techniques reduced biases by 66–94%.

Conclusion: This work provides actionable insights to address biases in LLMs for equitable AI development in mental healthcare, while demonstrating the effectiveness of the new MHQA framework and debiasing strategies.

Abstract: Large Language Models (LLMs) in mental healthcare risk propagating biases
that reinforce stigma and harm marginalized groups. While previous research
identified concerning trends, systematic methods for detecting intersectional
biases remain limited. This work introduces a multi-hop question answering
(MHQA) framework to explore LLM response biases in mental health discourse. We
analyze content from the Interpretable Mental Health Instruction (IMHI) dataset
across symptom presentation, coping mechanisms, and treatment approaches. Using
systematic tagging across age, race, gender, and socioeconomic status, we
investigate bias patterns at demographic intersections. We evaluate four LLMs:
Claude 3.5 Sonnet, Jamba 1.6, Gemma 3, and Llama 4, revealing systematic
disparities across sentiment, demographics, and mental health conditions. Our
MHQA approach demonstrates superior detection compared to conventional methods,
identifying amplification points where biases magnify through sequential
reasoning. We implement two debiasing techniques: Roleplay Simulation and
Explicit Bias Reduction, achieving 66-94% bias reductions through few-shot
prompting with BBQ dataset examples. These findings highlight critical areas
where LLMs reproduce mental healthcare biases, providing actionable insights
for equitable AI development.

</details>


### [101] [The Syntactic Acceptability Dataset (Preview): A Resource for Machine Learning and Linguistic Analysis of English](https://arxiv.org/abs/2506.18120)
*Tom S Juzek*

Main category: cs.CL

TL;DR: The Syntactic Acceptability Dataset provides 1,000 labeled English sequences to study grammaticality and acceptability in syntax and computation.


<details>
  <summary>Details</summary>
Motivation: The dataset aims to bridge gaps and advance research in both syntax and computational linguistics by providing a standardized, large-scale resource.

Method: The researchers collected 1,000 English sequences from educational and academic sources, labeled them for grammaticality (literature-based) and acceptability (crowdsourced), and analyzed the data for linguistic debates.

Result: 83% convergence was found between grammaticality and acceptability judgments, along with frequent 'in-betweenness.' Machine learning struggled with grammaticality but performed better predicting acceptability.

Conclusion: The dataset serves as a valuable resource for study, confirming existing findings while providing new insights into acceptability judgments in computational models.

Abstract: We present a preview of the Syntactic Acceptability Dataset, a resource being
designed for both syntax and computational linguistics research. In its current
form, the dataset comprises 1,000 English sequences from the syntactic
discourse: Half from textbooks and half from the journal Linguistic Inquiry,
the latter to ensure a representation of the contemporary discourse. Each entry
is labeled with its grammatical status ("well-formedness" according to
syntactic formalisms) extracted from the literature, as well as its
acceptability status ("intuitive goodness" as determined by native speakers)
obtained through crowdsourcing, with highest experimental standards. Even in
its preliminary form, this dataset stands as the largest of its kind that is
publicly accessible. We also offer preliminary analyses addressing three
debates in linguistics and computational linguistics: We observe that
grammaticality and acceptability judgments converge in about 83% of the cases
and that "in-betweenness" occurs frequently. This corroborates existing
research. We also find that while machine learning models struggle with
predicting grammaticality, they perform considerably better in predicting
acceptability. This is a novel finding. Future work will focus on expanding the
dataset.

</details>


### [102] [$φ^{\infty}$: Clause Purification, Embedding Realignment, and the Total Suppression of the Em Dash in Autoregressive Language Models](https://arxiv.org/abs/2506.18129)
*Bugra Kilictas,Faruk Alpay*

Main category: cs.CL

TL;DR: This paper identifies vulnerabilities in transformer language models and proposes solutions to improve their robustness against recursive semantic drift caused by specific tokens like em dashes.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address critical issues in autoregressive transformer models that affect long-form text generation consistency, particularly induced by em dash tokens.

Method: The authors use a combination of symbolic clause purification with the phi-infinity operator and targeted embedding matrix realignment to suppress problematic tokens without retraining the model.

Result: Experimental validation demonstrates significant improvement in text generation consistency and maintaining topic coherence.

Conclusion: The study establishes a framework for identifying and mitigating token-level vulnerabilities in foundation models, with implications for AI safety and robust deployment in production environments.

Abstract: We identify a critical vulnerability in autoregressive transformer language
models where the em dash token induces recursive semantic drift, leading to
clause boundary hallucination and embedding space entanglement. Through formal
analysis of token-level perturbations in semantic lattices, we demonstrate that
em dash insertion fundamentally alters the model's latent representations,
causing compounding errors in long-form generation. We propose a novel solution
combining symbolic clause purification via the phi-infinity operator with
targeted embedding matrix realignment. Our approach enables total suppression
of problematic tokens without requiring model retraining, while preserving
semantic coherence through fixed-point convergence guarantees. Experimental
validation shows significant improvements in generation consistency and topic
maintenance. This work establishes a general framework for identifying and
mitigating token-level vulnerabilities in foundation models, with immediate
implications for AI safety, model alignment, and robust deployment of large
language models in production environments. The methodology extends beyond
punctuation to address broader classes of recursive instabilities in neural
text generation systems.

</details>


### [103] [Sparse Feature Coactivation Reveals Composable Semantic Modules in Large Language Models](https://arxiv.org/abs/2506.18141)
*Ruixuan Deng,Xiaoyang Hu,Miles Gilberti,Shane Storks,Aman Taxali,Mike Angstadt,Chandra Sripada,Joyce Chai*

Main category: cs.CL

TL;DR: The paper identifies modular components in LLMs using sparse autoencoders to analyze their behavior in country-relation tasks, leading to methods for targeted manipulations.


<details>
  <summary>Details</summary>
Motivation: To better understand and manipulate the modular organization of knowledge within large language models for focused tasks.

Method: The paper uses sparse autoencoder features acquired from limited prompts to identify semantic and context-driven network components, focusing on country-relation analysis.

Result: Semantic components for countries appear in the first layers, while abstract relations are concentrated later, and their manipulation changes outputs predictably.

Conclusion: The study uncovers a modular structure in LLMs' knowledge organization and demonstrates new approaches for targeted and counterfactual model manipulation.

Abstract: We identify semantically coherent, context-consistent network components in
large language models (LLMs) using coactivation of sparse autoencoder (SAE)
features collected from just a handful of prompts. Focusing on country-relation
tasks, we show that ablating semantic components for countries and relations
changes model outputs in predictable ways, while amplifying these components
induces counterfactual responses. Notably, composing relation and country
components yields compound counterfactual outputs. We find that, whereas most
country components emerge from the very first layer, the more abstract relation
components are concentrated in later layers. Furthermore, within relation
components themselves, nodes from later layers tend to have a stronger causal
impact on model outputs. Overall, these findings suggest a modular organization
of knowledge within LLMs and advance methods for efficient, targeted model
manipulation.

</details>


### [104] [QuranMorph: Morphologically Annotated Quranic Corpus](https://arxiv.org/abs/2506.18148)
*Diyam Akra,Tymaa Hammouda,Mustafa Jarrar*

Main category: cs.CL

TL;DR: The QuranMorph corpus is a morphologically annotated and open-source resource for the Quran, offering detailed linguistic tagging based on expert analysis.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of a comprehensive, morphologically annotated corpus for the Quran to facilitate linguistic analysis and interlink with various resources.

Method: The corpus was manually lemmatized and tagged for part-of-speech by three expert linguists using Qabas database and SAMA/Qabas POS tagsets with 40 fine-grained tags.

Result: The QuranMorph corpus contains 77,429 tokens, allowing rich linguistic annotation and inter-connectivity with numerous linguistic resources.

Conclusion: The QuranMorph provides an accessible, detailed linguistic resource for researchers, enhancing the potential for Arabic language analysis and Quranic studies.

Abstract: We present the QuranMorph corpus, a morphologically annotated corpus for the
Quran (77,429 tokens). Each token in the QuranMorph was manually lemmatized and
tagged with its part-of-speech by three expert linguists. The lemmatization
process utilized lemmas from Qabas, an Arabic lexicographic database linked
with 110 lexicons and corpora of 2 million tokens. The part-of-speech tagging
was performed using the fine-grained SAMA/Qabas tagset, which encompasses 40
tags. As shown in this paper, this rich lemmatization and POS tagset enabled
the QuranMorph corpus to be inter-linked with many linguistic resources. The
corpus is open-source and publicly available as part of the SinaLab resources
at (https://sina.birzeit.edu/quran)

</details>


### [105] [CareLab at #SMM4H-HeaRD 2025: Insomnia Detection and Food Safety Event Extraction with Domain-Aware Transformers](https://arxiv.org/abs/2506.18185)
*Zihan Liang,Ziwen Pan,Sumon Kanti Dey,Azra Ismail*

Main category: cs.CL

TL;DR: The paper introduces a system for the SMM4H-HeaRD 2025 shared tasks, achieving notable success in extracting food safety events from news articles, especially in Task 5 Subtask 1 with an F1 score of 0.958.


<details>
  <summary>Details</summary>
Motivation: The study aims to address two challenges: identifying insomnia mentions in clinical notes and extracting food safety events from news articles.

Method: The authors used encoder-based models like RoBERTa and leveraged GPT-4 for data augmentation. They detailed the preprocessing steps, model architecture, and task-specific adaptations.

Result: The system excelled in Task 5 Subtask 1, securing the first position with an F1 score of 0.958 on the test data.

Conclusion: The proposed approach demonstrates the efficacy of combining pre-trained models and GPT-4 for data augmentation in specialized information extraction tasks.

Abstract: This paper presents our system for the SMM4H-HeaRD 2025 shared tasks,
specifically Task 4 (Subtasks 1, 2a, and 2b) and Task 5 (Subtasks 1 and 2).
Task 4 focused on detecting mentions of insomnia in clinical notes, while Task
5 addressed the extraction of food safety events from news articles. We
participated in all subtasks and report key findings across them, with
particular emphasis on Task 5 Subtask 1, where our system achieved strong
performance-securing first place with an F1 score of 0.958 on the test set. To
attain this result, we employed encoder-based models (e.g., RoBERTa), alongside
GPT-4 for data augmentation. This paper outlines our approach, including
preprocessing, model architecture, and subtask-specific adaptations

</details>


### [106] [Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review](https://arxiv.org/abs/2506.18199)
*Bushra Asseri,Estabrag Abdelaziz,Areej Al-Wabil*

Main category: cs.CL

TL;DR: This paper systematically reviews bias mitigation techniques in LLMs addressing Arab and Muslim representation, identifying five effective prompt-engineering approaches.


<details>
  <summary>Details</summary>
Motivation: Address the ethical issue of cultural bias in large language models, specifically focusing on Arabs and Muslims representation.

Method: A mixed-methods systematic review was conducted using PRISMA guidelines and Kitchenham's methodology on 8 empirical studies (2021-2024).

Result: Five promising approaches were identified, with structured multi-step pipelines achieving the highest bias reduction (87.7%) and cultural prompting offering accessibility.

Conclusion: Prompt engineering shows potential for mitigating cultural bias, but further research is needed to develop adaptive techniques, evaluation resources, and complementary strategies.

Abstract: Large language models have demonstrated remarkable capabilities across
various domains, yet concerns about cultural bias - particularly towards Arabs
and Muslims - pose significant ethical challenges by perpetuating harmful
stereotypes and marginalization. Despite growing recognition of bias in LLMs,
prompt engineering strategies specifically addressing Arab and Muslim
representation remain understudied. This mixed-methods systematic review
examines such techniques, offering evidence-based guidance for researchers and
practitioners. Following PRISMA guidelines and Kitchenham's systematic review
methodology, we analyzed 8 empirical studies published between 2021-2024
investigating bias mitigation strategies. Our findings reveal five primary
prompt engineering approaches: cultural prompting, affective priming,
self-debiasing techniques, structured multi-step pipelines, and
parameter-optimized continuous prompts. Although all approaches show potential
for reducing bias, effectiveness varied substantially across studies and bias
types. Evidence suggests that certain bias types may be more resistant to
prompt-based mitigation than others. Structured multi-step pipelines
demonstrated the highest overall effectiveness, achieving up to 87.7% reduction
in bias, though they require greater technical expertise. Cultural prompting
offers broader accessibility with substantial effectiveness. These results
underscore the accessibility of prompt engineering for mitigating cultural bias
without requiring access to model parameters. The limited number of studies
identified highlights a significant research gap in this critical area. Future
research should focus on developing culturally adaptive prompting techniques,
creating Arab and Muslim-specific evaluation resources, and integrating prompt
engineering with complementary debiasing methods to address deeper stereotypes
while maintaining model utility.

</details>


### [107] [Deciphering Emotions in Children Storybooks: A Comparative Analysis of Multimodal LLMs in Educational Applications](https://arxiv.org/abs/2506.18201)
*Bushra Asseri,Estabraq Abdelaziz,Maha Al Mogren,Tayef Alhefdhi,Areej Al-Wabil*

Main category: cs.CL

TL;DR: This study evaluates GPT-4o and Gemini 1.5 Pro for emotion recognition in Arabic children's storybooks. GPT-4o outperforms Gemini, but both models struggle with culturally nuanced emotions.


<details>
  <summary>Details</summary>
Motivation: To address the gap in culturally responsive technology suited for Arabic contexts by exploring AI systems' capabilities in emotion recognition using Arabic children's stories.

Method: Emotion recognition performance was evaluated on 75 images from 7 Arabic storybooks using three prompting strategies (zero-shot, few-shot, chain-of-thought), comparing AI predictions with human annotations based on Plutchik's emotional framework.

Result: GPT-4o achieved the highest macro F1-score of 59%, surpassing Gemini’s best score of 43%. Both models showed systematic errors, including frequent valence inversions and difficulty with culturally nuanced emotions.

Conclusion: Current multimodal AI models, including GPT-4o and Gemini, exhibit limitations in processing culturally specific emotions. Culturally sensitive training is needed for effective educational technologies in Arabic-speaking contexts.

Abstract: Emotion recognition capabilities in multimodal AI systems are crucial for
developing culturally responsive educational technologies, yet remain
underexplored for Arabic language contexts where culturally appropriate
learning tools are critically needed. This study evaluates the emotion
recognition performance of two advanced multimodal large language models,
GPT-4o and Gemini 1.5 Pro, when processing Arabic children's storybook
illustrations. We assessed both models across three prompting strategies
(zero-shot, few-shot, and chain-of-thought) using 75 images from seven Arabic
storybooks, comparing model predictions with human annotations based on
Plutchik's emotional framework. GPT-4o consistently outperformed Gemini across
all conditions, achieving the highest macro F1-score of 59% with
chain-of-thought prompting compared to Gemini's best performance of 43%. Error
analysis revealed systematic misclassification patterns, with valence
inversions accounting for 60.7% of errors, while both models struggled with
culturally nuanced emotions and ambiguous narrative contexts. These findings
highlight fundamental limitations in current models' cultural understanding and
emphasize the need for culturally sensitive training approaches to develop
effective emotion-aware educational technologies for Arabic-speaking learners.

</details>


### [108] [Enhancing Entity Aware Machine Translation with Multi-task Learning](https://arxiv.org/abs/2506.18318)
*An Trieu,Phuong Nguyen,Minh Le Nguyen*

Main category: cs.CL

TL;DR: This paper proposes a multi-task learning method to handle entity-aware machine translation by simultaneously optimizing named entity recognition and machine translation tasks.


<details>
  <summary>Details</summary>
Motivation: Entity-aware machine translation is challenging due to limited entity-specific translation data and context complexity during translation.

Method: The paper introduces multi-task learning to jointly optimize named entity recognition and machine translation tasks to improve overall performance.

Result: Results and analysis are based on the SemEval 2025 competition's Task 2 dataset, demonstrating enhanced performance in entity-aware machine translation.

Conclusion: Multi-task learning significantly improves entity-aware machine translation by addressing entity recognition and translation challenges collaboratively.

Abstract: Entity-aware machine translation (EAMT) is a complicated task in natural
language processing due to not only the shortage of translation data related to
the entities needed to translate but also the complexity in the context needed
to process while translating those entities. In this paper, we propose a method
that applies multi-task learning to optimize the performance of the two
subtasks named entity recognition and machine translation, which improves the
final performance of the Entity-aware machine translation task. The result and
analysis are performed on the dataset provided by the organizer of Task 2 of
the SemEval 2025 competition.

</details>


### [109] [TranslationCorrect: A Unified Framework for Machine Translation Post-Editing with Predictive Error Assistance](https://arxiv.org/abs/2506.18337)
*Syed Mekael Wasti,Shou-Yi Hung,Christopher Collins,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: TranslationCorrect is an integrated platform that enhances MT post-editing and research workflows by combining MT generation, error prediction, and user-friendly annotation capabilities.


<details>
  <summary>Details</summary>
Motivation: The paper addresses inefficient and disconnected workflows in machine translation post-editing and research data collection.

Method: TranslationCorrect incorporates MT models like NLLB, automated error prediction tools, and an intuitive HCI-informed post-editing interface while supporting ESA format-based annotations.

Result: The framework improves translation efficiency and ensures high-quality annotations suitable for training advanced MT and post-editing systems.

Conclusion: TranslationCorrect significantly enhances user satisfaction and workflow efficiency compared to traditional methods, confirmed through a user study.

Abstract: Machine translation (MT) post-editing and research data collection often rely
on inefficient, disconnected workflows. We introduce TranslationCorrect, an
integrated framework designed to streamline these tasks. TranslationCorrect
combines MT generation using models like NLLB, automated error prediction using
models like XCOMET or LLM APIs (providing detailed reasoning), and an intuitive
post-editing interface within a single environment. Built with human-computer
interaction (HCI) principles in mind to minimize cognitive load, as confirmed
by a user study. For translators, it enables them to correct errors and batch
translate efficiently. For researchers, TranslationCorrect exports high-quality
span-based annotations in the Error Span Annotation (ESA) format, using an
error taxonomy inspired by Multidimensional Quality Metrics (MQM). These
outputs are compatible with state-of-the-art error detection models and
suitable for training MT or post-editing systems. Our user study confirms that
TranslationCorrect significantly improves translation efficiency and user
satisfaction over traditional annotation methods.

</details>


### [110] [Less Data Less Tokens: Multilingual Unification Learning for Efficient Test-Time Reasoning in LLMs](https://arxiv.org/abs/2506.18341)
*Kang Chen,Mengdi Zhang,Yixin Cao*

Main category: cs.CL

TL;DR: This paper proposes the \(L^2\) method to enhance large language models' reasoning capabilities across multiple languages, achieving data and inference efficiency.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenges LLMs face in test-time scaling, focusing on improving data and inference efficiency while leveraging multilingual reasoning.

Method: The paper introduces \(L^2\), a multilingual unification learning approach with decoding intervention strategies, utilizing both long chain-of-thought annotations and step-wise language mixture tuning.

Result: Experiments show that even small amounts of multilingual data can enhance reasoning capabilities, reduce data and inference tokens, and maintain performance.

Conclusion: The \(L^2\) method offers a viable solution for reduced data and compute requirements in LLMs, while emphasizing the value of diverse data selection.

Abstract: This paper explores the challenges of test-time scaling of large language
models (LLMs), regarding both the data and inference efficiency. We highlight
the diversity of multi-lingual reasoning based on our pilot studies, and then
introduce a novel approach, \(L^2\) multi-lingual unification learning with a
decoding intervention strategy for further investigation. The basic idea of
\(L^2\) is that the reasoning process varies across different languages, which
may be mutually beneficial to enhance both model performance and efficiency. In
specific, there are two types of multi-lingual data: the entire long
chain-of-thought annotations in different languages and the step-wise mixture
of languages. By further tuning based on them, we show that even small amounts
of data can significantly improve reasoning capabilities. Our findings suggest
that multilingual learning reduces both the required data and the number of
inference tokens while maintaining a comparable performance. Furthermore,
\(L^2\) is orthogonal to other data efficient methods. Thus, we also emphasize
the importance of diverse data selection. The \(L^2\) method offers a promising
solution to the challenges of data collection and test-time compute efficiency
in LLMs.

</details>


### [111] [Evaluating Causal Explanation in Medical Reports with LLM-Based and Human-Aligned Metrics](https://arxiv.org/abs/2506.18387)
*Yousang Cho,Key-Sun Choi*

Main category: cs.CL

TL;DR: This study evaluates six metrics for assessing causal explanations in generated diagnostic reports, finding GPT-Black and GPT-White align best with expert evaluations.


<details>
  <summary>Details</summary>
Motivation: To identify which metrics best capture the quality of causal reasoning in diagnostic report generation.

Method: This study compared six metrics using inputs from observation-based and multiple-choice-based report generation, applying two weighting strategies.

Result: GPT-Black showed the best discriminative power, followed by GPT-White, while similarity-based metrics deviated from expert evaluations.

Conclusion: Metric selection critically impacts evaluation, and LLM-based metrics like GPT-Black and GPT-White are recommended for tasks involving interpretability and causal reasoning.

Abstract: This study investigates how accurately different evaluation metrics capture
the quality of causal explanations in automatically generated diagnostic
reports. We compare six metrics: BERTScore, Cosine Similarity, BioSentVec,
GPT-White, GPT-Black, and expert qualitative assessment across two input types:
observation-based and multiple-choice-based report generation. Two weighting
strategies are applied: one reflecting task-specific priorities, and the other
assigning equal weights to all metrics. Our results show that GPT-Black
demonstrates the strongest discriminative power in identifying logically
coherent and clinically valid causal narratives. GPT-White also aligns well
with expert evaluations, while similarity-based metrics diverge from clinical
reasoning quality. These findings emphasize the impact of metric selection and
weighting on evaluation outcomes, supporting the use of LLM-based evaluation
for tasks requiring interpretability and causal reasoning.

</details>


### [112] [Lemmatization as a Classification Task: Results from Arabic across Multiple Genres](https://arxiv.org/abs/2506.18399)
*Mostafa Saeed,Nizar Habash*

Main category: cs.CL

TL;DR: This paper proposes new models for Arabic lemmatization, framing it as a classification task and offering robust, interpretable outputs.


<details>
  <summary>Details</summary>
Motivation: Existing lemmatization tools for Arabic face challenges due to inconsistent standards and limited genre coverage.

Method: The paper introduces approaches that frame lemmatization as classification into a Lemma-POS-Gloss (LPG) tagset, leveraging machine translation and semantic clustering. It also evaluates these methods against character-level sequence-to-sequence models.

Result: Classification and clustering approaches yield more robust and interpretable results than existing methods, and a new test set for Arabic lemmatization is presented.

Conclusion: The proposed methods set new benchmarks for Arabic lemmatization, offering a more robust and genre-inclusive solution.

Abstract: Lemmatization is crucial for NLP tasks in morphologically rich languages with
ambiguous orthography like Arabic, but existing tools face challenges due to
inconsistent standards and limited genre coverage. This paper introduces two
novel approaches that frame lemmatization as classification into a
Lemma-POS-Gloss (LPG) tagset, leveraging machine translation and semantic
clustering. We also present a new Arabic lemmatization test set covering
diverse genres, standardized alongside existing datasets. We evaluate character
level sequence-to-sequence models, which perform competitively and offer
complementary value, but are limited to lemma prediction (not LPG) and prone to
hallucinating implausible forms. Our results show that classification and
clustering yield more robust, interpretable outputs, setting new benchmarks for
Arabic lemmatization.

</details>


### [113] [TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2506.18421)
*Ce Li,Xiaofan Liu,Zhiyan Song,Ce Chi,Chen Zhao,Jingjing Yang,Zhendong Wang,Kexin Yang,Boshen Shi,Xing Wang,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: The paper introduces TReB, a benchmark for evaluating large language models (LLMs) on table reasoning abilities using 26 sub-tasks and three inference modes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of an effective evaluation benchmark for LLMs' table reasoning abilities, which is crucial due to the challenges posed by the structured and complex nature of table data.

Method: The authors developed TReB, a comprehensive benchmark, comprising a high-quality dataset and an evaluation framework that incorporates three inference modes: TCoT, PoT, and ICoT.

Result: Over 20 state-of-the-art LLMs were tested using this benchmark framework, revealing significant room for improvement in their table reasoning capabilities.

Conclusion: The study demonstrates the need for better LLMs in table reasoning and introduces an effective, publicly available benchmark and evaluation framework to facilitate advancements in this area.

Abstract: The majority of data in businesses and industries is stored in tables,
databases, and data warehouses. Reasoning with table-structured data poses
significant challenges for large language models (LLMs) due to its hidden
semantics, inherent complexity, and structured nature. One of these challenges
is lacking an effective evaluation benchmark fairly reflecting the performances
of LLMs on broad table reasoning abilities. In this paper, we fill in this gap,
presenting a comprehensive table reasoning evolution benchmark, TReB, which
measures both shallow table understanding abilities and deep table reasoning
abilities, a total of 26 sub-tasks. We construct a high quality dataset through
an iterative data processing procedure. We create an evaluation framework to
robustly measure table reasoning capabilities with three distinct inference
modes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs
using this frame work and prove its effectiveness. Experimental results reveal
that existing LLMs still have significant room for improvement in addressing
the complex and real world Table related tasks. Both the dataset and evaluation
framework are publicly available, with the dataset hosted on [HuggingFace] and
the framework on [GitHub].

</details>


### [114] [MeRF: Motivation-enhanced Reinforcement Finetuning for Large Reasoning Models](https://arxiv.org/abs/2506.18485)
*Junjie Zhang,Guozheng Ma,Shunyu Liu,Haoyu Wang,Jiaxing Huang,Ting-En Lin,Fei Huang,Yongbin Li,Dacheng Tao*

Main category: cs.CL

TL;DR: The paper introduces Motivation-enhanced Reinforcement Finetuning (MeRF) for leveraging reinforcement learning with in-context capabilities of LLMs to improve reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Current RLVR approaches for reasoning tasks fail to utilize the unique in-context learning capabilities demonstrated by Chain-of-Thought prompting in LLMs.

Method: The authors propose MeRF, which integrates reward specification into the prompt as in-context motivation, aligning LLM generation with external optimization objectives.

Result: Empirical evaluation on reasoning tasks (Knights & Knaves benchmark) shows MeRF significantly improves performance over baseline methods.

Conclusion: MeRF effectively combines reinforcement learning and in-context learning, enhancing LLM reasoning while balancing internal motivation and external rewards.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful learn-to-reason paradigm for Large Language Models (LLMs) to tackle
complex reasoning tasks. However, existing RLVR methods overlook one of the
most distinctive capabilities of LLMs, their in-context learning ability, as
prominently demonstrated by the success of Chain-of-Thought (CoT) prompting.
This motivates us to explore how reinforcement learning can be effectively
combined with in-context learning to better improve the reasoning capabilities
of LLMs. In this paper, we introduce Motivation-enhanced Reinforcement
Finetuning} (MeRF), an intuitive yet effective method enhancing reinforcement
learning of LLMs by involving ``telling LLMs the rules of the game''.
Specifically, MeRF directly injects the reward specification into the prompt,
which serves as an in-context motivation for model to improve its responses
with awareness of the optimization objective. This simple modification
leverages the in-context learning ability of LLMs aligning generation with
optimization, thereby incentivizing the model to generate desired outputs from
both inner motivation and external reward. Empirical evaluations on the Knights
and Knaves~(K&K) logic puzzle reasoning benchmark demonstrate that
\texttt{MeRF} achieves substantial performance gains over baselines. Moreover,
ablation studies show that performance improves with greater consistency
between the in-context motivation and the external reward function, while the
model also demonstrates an ability to adapt to misleading motivations through
reinforcement learning.

</details>


### [115] [Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks: Strengths, Weaknesses, and Domain-Specific Performance](https://arxiv.org/abs/2506.18501)
*Wael Etaiwi,Bushra Alhijawi*

Main category: cs.CL

TL;DR: The paper evaluates ChatGPT and DeepSeek models across five NLP tasks, highlighting their respective strengths and weaknesses.


<details>
  <summary>Details</summary>
Motivation: To assess and compare the effectiveness of ChatGPT and DeepSeek across diverse NLP applications to understand their domain-specific strengths and weaknesses.

Method: A structured experimental protocol was followed where models were tested with identical prompts on two benchmark datasets per NLP task.

Result: DeepSeek demonstrated superior classification stability and logical reasoning, while ChatGPT excelled in tasks requiring nuanced understanding and flexibility.

Conclusion: Insights from the evaluation can guide the selection of appropriate LLMs for specific NLP tasks based on their strengths and performance.

Abstract: The increasing use of large language models (LLMs) in natural language
processing (NLP) tasks has sparked significant interest in evaluating their
effectiveness across diverse applications. While models like ChatGPT and
DeepSeek have shown strong results in many NLP domains, a comprehensive
evaluation is needed to understand their strengths, weaknesses, and
domain-specific abilities. This is critical as these models are applied to
various tasks, from sentiment analysis to more nuanced tasks like textual
entailment and translation. This study aims to evaluate ChatGPT and DeepSeek
across five key NLP tasks: sentiment analysis, topic classification, text
summarization, machine translation, and textual entailment. A structured
experimental protocol is used to ensure fairness and minimize variability. Both
models are tested with identical, neutral prompts and evaluated on two
benchmark datasets per task, covering domains like news, reviews, and
formal/informal texts. The results show that DeepSeek excels in classification
stability and logical reasoning, while ChatGPT performs better in tasks
requiring nuanced understanding and flexibility. These findings provide
valuable insights for selecting the appropriate LLM based on task requirements.

</details>


### [116] [End-to-End Spoken Grammatical Error Correction](https://arxiv.org/abs/2506.18532)
*Mengjie Qian,Rao Ma,Stefano Bannò,Mark J. F. Gales,Kate M. Knill*

Main category: cs.CL

TL;DR: The paper focuses on improving spoken grammatical error correction (SGEC) by comparing different architectures—cascaded, partial-cascaded, and end-to-end (E2E). It introduces methods like pseudo-labeling and contextual information to enhance performance.


<details>
  <summary>Details</summary>
Motivation: To support second language learners with spoken grammatical correction and feedback, where challenges include disfluencies, transcription inaccuracies, and unstructured inputs.

Method: The study builds SGEC systems using Whisper models, explores pseudo-labeling for dataset expansion, integrates contextual ASR outputs, proposes a novel reference alignment for transcription errors, and includes edit confidence estimation for better feedback precision.

Result: By applying the proposed methods, SGEC performance shows significant improvement, validated on in-house and publicly available corpora.

Conclusion: The combination of pseudo-labeling, contextual ASR information, alignment processes, and confidence estimation enhances E2E SGEC, addressing challenges like errors from transcription and disfluencies effectively.

Abstract: Grammatical Error Correction (GEC) and feedback play a vital role in
supporting second language (L2) learners, educators, and examiners. While
written GEC is well-established, spoken GEC (SGEC), aiming to provide feedback
based on learners' speech, poses additional challenges due to disfluencies,
transcription errors, and the lack of structured input. SGEC systems typically
follow a cascaded pipeline consisting of Automatic Speech Recognition (ASR),
disfluency detection, and GEC, making them vulnerable to error propagation
across modules. This work examines an End-to-End (E2E) framework for SGEC and
feedback generation, highlighting challenges and possible solutions when
developing these systems. Cascaded, partial-cascaded and E2E architectures are
compared, all built on the Whisper foundation model. A challenge for E2E
systems is the scarcity of GEC labeled spoken data. To address this, an
automatic pseudo-labeling framework is examined, increasing the training data
from 77 to over 2500 hours. To improve the accuracy of the SGEC system,
additional contextual information, exploiting the ASR output, is investigated.
Candidate feedback of their mistakes is an essential step to improving
performance. In E2E systems the SGEC output must be compared with an estimate
of the fluent transcription to obtain the feedback. To improve the precision of
this feedback, a novel reference alignment process is proposed that aims to
remove hypothesised edits that results from fluent transcription errors.
Finally, these approaches are combined with an edit confidence estimation
approach, to exclude low-confidence edits. Experiments on the in-house
Linguaskill (LNG) corpora and the publicly available Speak & Improve (S&I)
corpus show that the proposed approaches significantly boost E2E SGEC
performance.

</details>


### [117] [When Fine-Tuning Fails: Lessons from MS MARCO Passage Ranking](https://arxiv.org/abs/2506.18535)
*Manu Pande,Shahil Kumar,Anay Yatin Damle*

Main category: cs.CL

TL;DR: Fine-tuning transformer models on MS MARCO degrades performance compared to the base pre-trained model, likely due to disruptions in the embedding space.


<details>
  <summary>Details</summary>
Motivation: To investigate why fine-tuning pre-trained transformers underperforms on MS MARCO despite its prevailing use in transfer learning setups.

Method: The study conducts experiments across five fine-tuning approaches, uses embedding visualizations (UMAP), analyzes training dynamics, and measures computational efficiency to understand performance degradation.

Result: Fine-tuned models perform worse than the base model (MRR@10 of 0.3026), and embedding visualizations reveal structural disruptions with embedding space flattening.

Conclusion: Fine-tuning may not be effective on saturated benchmarks like MS MARCO without architectural changes, challenging traditional transfer learning norms.

Abstract: This paper investigates the counterintuitive phenomenon where fine-tuning
pre-trained transformer models degrades performance on the MS MARCO passage
ranking task. Through comprehensive experiments involving five model
variants-including full parameter fine-tuning and parameter efficient LoRA
adaptations-we demonstrate that all fine-tuning approaches underperform the
base sentence-transformers/all- MiniLM-L6-v2 model (MRR@10: 0.3026). Our
analysis reveals that fine-tuning disrupts the optimal embedding space
structure learned during the base model's extensive pre-training on 1 billion
sentence pairs, including 9.1 million MS MARCO samples. UMAP visualizations
show progressive embedding space flattening, while training dynamics analysis
and computational efficiency metrics further support our findings. These
results challenge conventional wisdom about transfer learning effectiveness on
saturated benchmarks and suggest architectural innovations may be necessary for
meaningful improvements.

</details>


### [118] [A Modular Taxonomy for Hate Speech Definitions and Its Impact on Zero-Shot LLM Classification Performance](https://arxiv.org/abs/2506.18576)
*Matteo Melis,Gabriella Lapesa,Dennis Assenmacher*

Main category: cs.CL

TL;DR: This paper explores hate speech definitions, organizing them into 14 conceptual components, and evaluating how definition variation affects three large language model (LLM) performances on hate speech datasets.


<details>
  <summary>Details</summary>
Motivation: The research aims to address ambiguity in hate speech definitions, as understanding and defining hate speech are critical for improving NLP model accuracy and social good applications.

Method: The authors collected various hate speech definitions, categorized them into 14 conceptual elements, and conducted a zero-shot evaluation of three LLMs using three distinct hate speech datasets.

Result: The study found that LLM performance varies based on the chosen definitions, though the impact differs across model architectures.

Conclusion: The paper highlights the importance of definition specificity in hate speech detection tasks, suggesting that universal definitions may not equally benefit all LLM architectures.

Abstract: Detecting harmful content is a crucial task in the landscape of NLP
applications for Social Good, with hate speech being one of its most dangerous
forms. But what do we mean by hate speech, how can we define it, and how does
prompting different definitions of hate speech affect model performance? The
contribution of this work is twofold. At the theoretical level, we address the
ambiguity surrounding hate speech by collecting and analyzing existing
definitions from the literature. We organize these definitions into a taxonomy
of 14 Conceptual Elements-building blocks that capture different aspects of
hate speech definitions, such as references to the target of hate (individual
or groups) or of the potential consequences of it. At the experimental level,
we employ the collection of definitions in a systematic zero-shot evaluation of
three LLMs, on three hate speech datasets representing different types of data
(synthetic, human-in-the-loop, and real-world). We find that choosing different
definitions, i.e., definitions with a different degree of specificity in terms
of encoded elements, impacts model performance, but this effect is not
consistent across all architectures.

</details>


### [119] [Parallel Continuous Chain-of-Thought with Jacobi Iteration](https://arxiv.org/abs/2506.18582)
*Haoyi Wu,Zhihao Teng,Kewei Tu*

Main category: cs.CL

TL;DR: The paper introduces Parallel Continuous Chain-of-Thought (PCCoT), which improves the efficiency of reasoning by parallelizing latent token updates.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in training and inference time caused by sequential dependencies in continuous chain-of-thought reasoning.

Method: The paper proposes applying Jacobi iteration to latent thought tokens, enabling their iterative updates in parallel rather than sequentially.

Result: Experiments show that PCCoT achieves comparable or better performance while reducing training and inference time by nearly 50%. It also improves stability and robustness.

Conclusion: PCCoT enhances the continuous chain-of-thought approach by significantly improving efficiency and reliability in reasoning tasks through parallelization.

Abstract: Continuous chain-of-thought has been shown to be effective in saving
reasoning tokens for large language models. By reasoning with continuous latent
thought tokens, continuous CoT is able to perform implicit reasoning in a
compact manner. However, the sequential dependencies between latent thought
tokens spoil parallel training, leading to long training time. In this paper,
we propose Parallel Continuous Chain-of-Thought (PCCoT), which performs Jacobi
iteration on the latent thought tokens, updating them iteratively in parallel
instead of sequentially and thus improving both training and inference
efficiency of continuous CoT. Experiments demonstrate that by choosing the
proper number of iterations, we are able to achieve comparable or even better
performance while saving nearly 50% of the training and inference time.
Moreover, PCCoT shows better stability and robustness in the training process.
Our code is available at https://github.com/whyNLP/PCCoT.

</details>


### [120] [Reply to "Emergent LLM behaviors are observationally equivalent to data leakage"](https://arxiv.org/abs/2506.18600)
*Ariel Flint Ashery,Luca Maria Aiello,Andrea Baronchelli*

Main category: cs.CL

TL;DR: The paper argues that despite concerns about data contamination in large language models (LLMs), emergent dynamics, especially related to self-organization and social conventions, can be studied effectively in LLM populations.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the critique of previous research, aiming to clarify that emergent self-organization dynamics in LLM populations can still be examined despite potential training data contamination.

Method: The study focuses on empirical observations of social conventions to demonstrate emergent dynamics in LLM populations.

Result: The paper establishes that emergent dynamics and self-organization are possible to observe in LLM populations, even with concerns about training data contamination.

Conclusion: It concludes that studying populations of LLMs is feasible for exploring emergent dynamics and social conventions, despite challenges posed by data contamination.

Abstract: A potential concern when simulating populations of large language models
(LLMs) is data contamination, i.e. the possibility that training data may shape
outcomes in unintended ways. While this concern is important and may hinder
certain experiments with multi-agent models, it does not preclude the study of
genuinely emergent dynamics in LLM populations. The recent critique by Barrie
and T\"ornberg [1] of the results of Flint Ashery et al. [2] offers an
opportunity to clarify that self-organisation and model-dependent emergent
dynamics can be studied in LLM populations, highlighting how such dynamics have
been empirically observed in the specific case of social conventions.

</details>


### [121] [Semantic similarity estimation for domain specific data using BERT and other techniques](https://arxiv.org/abs/2506.18602)
*R. Prashanth*

Main category: cs.CL

TL;DR: The paper evaluates semantic similarity estimation using various techniques, finding BERT to be superior, especially for domain-specific datasets.


<details>
  <summary>Details</summary>
Motivation: To address the critical task of estimating semantic similarity, which is fundamental to numerous NLP applications.

Method: They experimented with Universal Sentence Encoder (USE), InferSent, and BERT models on both an in-house domain-specific dataset and Quora's question pairs dataset.

Result: BERT outperformed other methods significantly due to its fine-tuning capability, making it particularly effective for domain-specific data.

Conclusion: BERT is highly effective for semantic similarity estimation, especially for domain-specific datasets, showcasing its adaptability and superior performance compared to earlier methods.

Abstract: Estimation of semantic similarity is an important research problem both in
natural language processing and the natural language understanding, and that
has tremendous application on various downstream tasks such as question
answering, semantic search, information retrieval, document clustering,
word-sense disambiguation and machine translation. In this work, we carry out
the estimation of semantic similarity using different state-of-the-art
techniques including the USE (Universal Sentence Encoder), InferSent and the
most recent BERT, or Bidirectional Encoder Representations from Transformers,
models. We use two question pairs datasets for the analysis, one is a domain
specific in-house dataset and the other is a public dataset which is the
Quora's question pairs dataset. We observe that the BERT model gave much
superior performance as compared to the other methods. This should be because
of the fine-tuning procedure that is involved in its training process, allowing
it to learn patterns based on the training data that is used. This works
demonstrates the applicability of BERT on domain specific datasets. We infer
from the analysis that BERT is the best technique to use in the case of domain
specific data.

</details>


### [122] [The Anatomy of Speech Persuasion: Linguistic Shifts in LLM-Modified Speeches](https://arxiv.org/abs/2506.18621)
*Alisa Barkar,Mathieu Chollet,Matthieu Labeau,Beatrice Biancardi,Chloe Clavel*

Main category: cs.CL

TL;DR: The study explores GPT-4o's approach to modifying speech transcripts for persuasiveness, finding systematic stylistic adjustments rather than human-like optimization.


<details>
  <summary>Details</summary>
Motivation: Investigate how large language models comprehend and modify persuasiveness in public speaking, using the 3MT French dataset.

Method: Introduce an interpretable textual feature set, prompt GPT-4o to adjust speech transcripts for persuasiveness, and analyze linguistic changes.

Result: GPT-4o alters emotional lexicon and syntactic structures to enhance rhetorical impact, but its modifications are systematic, not human-like.

Conclusion: Large language models like GPT-4o show potential in systematic rhetorical adjustments, yet lack the nuanced understanding characteristic of human persuasiveness optimization.

Abstract: This study examines how large language models understand the concept of
persuasiveness in public speaking by modifying speech transcripts from PhD
candidates in the "Ma These en 180 Secondes" competition, using the 3MT French
dataset. Our contributions include a novel methodology and an interpretable
textual feature set integrating rhetorical devices and discourse markers. We
prompt GPT-4o to enhance or diminish persuasiveness and analyze linguistic
shifts between original and generated speech in terms of the new features.
Results indicate that GPT-4o applies systematic stylistic modifications rather
than optimizing persuasiveness in a human-like manner. Notably, it manipulates
emotional lexicon and syntactic structures (such as interrogative and
exclamatory clauses) to amplify rhetorical impact.

</details>


### [123] [ByteSpan: Information-Driven Subword Tokenisation](https://arxiv.org/abs/2506.18639)
*Zébulon Goriely,Suchir Salhan,Pietro Lesci,Julius Cheng,Paula Buttery*

Main category: cs.CL

TL;DR: The paper proposes ByteSpan, a tokenization method that groups predictable byte sequences into subwords using a language model, improving morphological alignments over traditional methods.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve subword tokenization methods by exploring whether predictable bytes can be grouped into subwords for efficiency, inspired by autoregressive models that identify lexical boundaries based on prediction errors.

Method: ByteSpan utilizes an external byte-level language model during training to identify and group contiguous predictable byte sequences into subwords for creating an efficient vocabulary.

Result: Experiments demonstrate that ByteSpan achieves better morphological alignment scores for English compared to BPE. Additionally, it shows comparable compression and Renyi efficiency across 25 languages.

Conclusion: ByteSpan effectively produces efficient subword vocabularies, offering advantages in morphological alignment and demonstrating strong performance across diverse languages.

Abstract: Recent dynamic tokenisation methods operate directly on bytes and pool their
latent representations into patches. This bears similarities to computational
models of word segmentation that determine lexical boundaries using spikes in
an autoregressive model's prediction error. Inspired by this connection, we
explore whether grouping predictable bytes - rather than pooling their
representations - can yield a useful fixed subword vocabulary. We propose a new
information-driven subword tokeniser, ByteSpan, that uses an external
byte-level LM during training to identify contiguous predictable byte sequences
and group them into subwords. Experiments show that ByteSpan yields efficient
vocabularies with higher morphological alignment scores than BPE for English.
Multilingual experiments show similar compression and R\'enyi efficiency for 25
languages.

</details>


### [124] [Is There a Case for Conversation Optimized Tokenizers in Large Language Models?](https://arxiv.org/abs/2506.18674)
*Raquel Ferrando,Javier Conde,Gonzalo Martínez,Pedro Reviriego*

Main category: cs.CL

TL;DR: The paper explores optimizing LLM tokenizers specifically for chatbot conversations, which reduces token usage and yields energy savings of 5-10%.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of tokenizers in chatbot use cases compared to their optimization for the training corpus.

Method: Redesigned tokenizers' vocabularies by leveraging conversation-specific corpora and evaluated their performance in chatbot dialogues.

Result: Conversation-optimized tokenizers reduce token usage in chatbot dialogues by 5-10% and have negligible or slightly positive effects on traditional corpora.

Conclusion: Fine-tuning tokenizers for chatbots can yield meaningful computational and energy savings without negatively impacting performance on the original training data.

Abstract: The computational and energy costs of Large Language Models (LLMs) have
increased exponentially driven by the growing model sizes and the massive
adoption of LLMs by hundreds of millions of users. The unit cost of an LLM is
the computation of a token. Therefore, the tokenizer plays an important role in
the efficiency of a model, and they are carefully optimized to minimize the
number of tokens for the text in their training corpus. One of the most popular
applications of LLMs are chatbots that interact with users. A key observation
is that, for those chatbots, what is important is the performance of the
tokenizer in the user text input and the chatbot responses. Those are most
likely different from the text in the training corpus. So, a question that
immediately arises is whether there is a potential benefit in optimizing
tokenizers for chatbot conversations. In this paper, this idea is explored for
different tokenizers by using a publicly available corpus of chatbot
conversations to redesign their vocabularies and evaluate their performance in
this domain. The results show that conversation-optimized tokenizers
consistently reduce the number of tokens in chatbot dialogues, which can lead
to meaningful energy savings, in the range of 5% to 10% while having minimal or
even slightly positive impact on tokenization efficiency for the original
training corpus.

</details>


### [125] [Context Biasing for Pronunciations-Orthography Mismatch in Automatic Speech Recognition](https://arxiv.org/abs/2506.18703)
*Christian Huber,Alexander Waibel*

Main category: cs.CL

TL;DR: This paper addresses the challenge of recognizing out-of-vocabulary words in neural sequence-to-sequence speech recognition and proposes a method for correcting substitution errors during inference.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve speech recognition systems' ability to recognize words not seen during training, especially words with a pronunciation-orthography mismatch, such as named entities or domain-specific terms.

Method: The authors propose an approach allowing users to add corrections on the fly during inference to improve recognition of challenging words.

Result: Their method shows up to an 11% improvement in biased word error rate while maintaining competitive overall word error rate.

Conclusion: This approach enhances recognition accuracy for hard-to-recognize words without negatively impacting overall system performance.

Abstract: Neural sequence-to-sequence systems deliver state-of-the-art performance for
automatic speech recognition. When using appropriate modeling units, e.g.,
byte-pair encoded characters, these systems are in principal open vocabulary
systems. In practice, however, they often fail to recognize words not seen
during training, e.g., named entities, acronyms, or domain-specific special
words. To address this problem, many context biasing methods have been
proposed; however, for words with a pronunciation-orthography mismatch, these
methods may still struggle. We propose a method which allows corrections of
substitution errors to improve the recognition accuracy of such challenging
words. Users can add corrections on the fly during inference. We show that with
this method we get a relative improvement in biased word error rate of up to
11\%, while maintaining a competitive overall word error rate.

</details>


### [126] [Benchmarking the Pedagogical Knowledge of Large Language Models](https://arxiv.org/abs/2506.18710)
*Maxime Lelièvre,Amy Waldock,Meng Liu,Natalia Valdés Aspillaga,Alasdair Mackintosh,María José Ogando Portelo,Jared Lee,Paul Atherton,Robin A. A. Ince,Oliver G. B. Garrod*

Main category: cs.CL

TL;DR: The study introduces The Pedagogy Benchmark to assess large language models on their pedagogical knowledge, highlighting performance across 97 models and offering insights into their educational potential.


<details>
  <summary>Details</summary>
Motivation: To address the gap in evaluating AI models' understanding of pedagogy, which is crucial for their effective deployment in education.

Method: Developed a novel benchmark dataset based on questions from teacher professional development exams to measure CDPK and SEND pedagogical knowledge across diverse subdomains.

Result: 97 models were evaluated on the benchmark, achieving accuracies between 28% and 89%. Insights on cost-accuracy trade-offs and Pareto value frontier progression were provided.

Conclusion: Education-focused benchmarks are vital for responsibly deploying AI in education, ensuring it aligns with pedagogical requirements and informs development and policy decisions.

Abstract: Benchmarks like Massive Multitask Language Understanding (MMLU) have played a
pivotal role in evaluating AI's knowledge and abilities across diverse domains.
However, existing benchmarks predominantly focus on content knowledge, leaving
a critical gap in assessing models' understanding of pedagogy - the method and
practice of teaching. This paper introduces The Pedagogy Benchmark, a novel
dataset designed to evaluate large language models on their Cross-Domain
Pedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)
pedagogical knowledge. These benchmarks are built on a carefully curated set of
questions sourced from professional development exams for teachers, which cover
a range of pedagogical subdomains such as teaching strategies and assessment
methods. Here we outline the methodology and development of these benchmarks.
We report results for 97 models, with accuracies spanning a range from 28% to
89% on the pedagogical knowledge questions. We consider the relationship
between cost and accuracy and chart the progression of the Pareto value
frontier over time. We provide online leaderboards at
https://rebrand.ly/pedagogy which are updated with new models and allow
interactive exploration and filtering based on various model properties, such
as cost per token and open-vs-closed weights, as well as looking at performance
in different subjects. LLMs and generative AI have tremendous potential to
influence education and help to address the global learning crisis.
Education-focused benchmarks are crucial to measure models' capacities to
understand pedagogical concepts, respond appropriately to learners' needs, and
support effective teaching practices across diverse contexts. They are needed
for informing the responsible and evidence-based deployment of LLMs and
LLM-based tools in educational settings, and for guiding both development and
policy decisions.

</details>


### [127] [Semantic-Preserving Adversarial Attacks on LLMs: An Adaptive Greedy Binary Search Approach](https://arxiv.org/abs/2506.18756)
*Chong Zhang,Xiang Li,Jia Wang,Shan Liang,Haochen Xue,Xiaobo Jin*

Main category: cs.CL

TL;DR: This paper introduces the Adaptive Greedy Binary Search (AGBS) method to improve prompt optimization with Large Language Models (LLMs) by ensuring semantic stability and generating robust adversarial samples.


<details>
  <summary>Details</summary>
Motivation: To address issues where automatic prompt engineering in LLMs leads to misinterpretations and distorted outputs.

Method: The AGBS method simulates common prompt optimization strategies, dynamically evaluates their effects, and generates robust adversarial samples while maintaining semantic stability.

Result: AGBS was tested on open and closed-source LLMs and proven to balance semantic consistency with adversarial attack efficacy.

Conclusion: The findings offer valuable insights for developing reliable and semantically stable prompt optimization systems for LLMs.

Abstract: Large Language Models (LLMs) increasingly rely on automatic prompt
engineering in graphical user interfaces (GUIs) to refine user inputs and
enhance response accuracy. However, the diversity of user requirements often
leads to unintended misinterpretations, where automated optimizations distort
original intentions and produce erroneous outputs. To address this challenge,
we propose the Adaptive Greedy Binary Search (AGBS) method, which simulates
common prompt optimization mechanisms while preserving semantic stability. Our
approach dynamically evaluates the impact of such strategies on LLM
performance, enabling robust adversarial sample generation. Through extensive
experiments on open and closed-source LLMs, we demonstrate AGBS's effectiveness
in balancing semantic consistency and attack efficacy. Our findings offer
actionable insights for designing more reliable prompt optimization systems.
Code is available at: https://github.com/franz-chang/DOBS

</details>


### [128] [ASP2LJ : An Adversarial Self-Play Laywer Augmented Legal Judgment Framework](https://arxiv.org/abs/2506.18768)
*Ao Chang,Tong Zhou,Yubo Chen,Delai Qiu,Shengping Liu,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: This paper proposes ASP2LJ, a framework for legal judgment prediction addressing long-tail data issues and enhancing lawyers' argumentation. It improves judicial objectivity and introduces a new dataset, RareCases.


<details>
  <summary>Details</summary>
Motivation: To tackle challenges in legal judgment prediction like long-tail data distribution and insufficient focus on lawyers' argumentation, which hinder fairness and accuracy in judicial decisions.

Method: ASP2LJ integrates a case generation module to alleviate imbalanced data distribution and utilizes an adversarial self-play mechanism to refine lawyers' argumentation skills. A judge uses improved lawyer arguments to enhance decision rationality.

Result: The proposed framework shows improved performance on the SimuCourt and RareCases datasets. RareCases includes rare legal cases to address long-tail data issues.

Conclusion: ASP2LJ contributes to automated judicial systems by addressing long-tail data and argument refinement, improving fairness and objectivity. RareCases and code are released for further research.

Abstract: Legal Judgment Prediction (LJP) aims to predict judicial outcomes, including
relevant legal charge, terms, and fines, which is a crucial process in Large
Language Model(LLM). However, LJP faces two key challenges: (1)Long Tail
Distribution: Current datasets, derived from authentic cases, suffer from high
human annotation costs and imbalanced distributions, leading to model
performance degradation. (2)Lawyer's Improvement: Existing systems focus on
enhancing judges' decision-making but neglect the critical role of lawyers in
refining arguments, which limits overall judicial accuracy. To address these
issues, we propose an Adversarial Self-Play Lawyer Augmented Legal Judgment
Framework, called ASP2LJ, which integrates a case generation module to tackle
long-tailed data distributions and an adversarial self-play mechanism to
enhance lawyers' argumentation skills. Our framework enables a judge to
reference evolved lawyers' arguments, improving the objectivity, fairness, and
rationality of judicial decisions. Besides, We also introduce RareCases, a
dataset for rare legal cases in China, which contains 120 tail-end cases. We
demonstrate the effectiveness of our approach on the SimuCourt dataset and our
RareCases dataset. Experimental results show our framework brings improvements,
indicating its utilization. Our contributions include an integrated framework,
a rare-case dataset, and publicly releasing datasets and code to support
further research in automated judicial systems.

</details>


### [129] [Existing LLMs Are Not Self-Consistent For Simple Tasks](https://arxiv.org/abs/2506.18781)
*Zhenru Lin,Jiawen Tao,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.CL

TL;DR: The paper analyzes inconsistencies in Large Language Models (LLMs), introduces metrics to measure them, and proposes graph-based and energy-based methods to address the issue, providing partial improvements.


<details>
  <summary>Details</summary>
Motivation: The motivation is to ensure that AI systems, particularly LLMs, produce reliable and interpretable results by addressing their lack of self-consistency.

Method: The authors introduced inconsistency metrics to measure the problem and developed two automated methods: graph-based and energy-based approaches to mitigate model inconsistencies.

Result: Even state-of-the-art LLMs like DeepSeek-R1 and GPT-o4-mini display inconsistencies on simple tasks. The proposed methods improve self-consistency partially but underline the complexity of achieving full reliability.

Conclusion: Self-consistency in LLMs remains an important challenge for transparent and trustworthy AI. The proposed methods help but do not fully resolve the inconsistencies, pointing to further work needed in this area.

Abstract: Large Language Models (LLMs) have grown increasingly powerful, yet ensuring
their decisions remain transparent and trustworthy requires self-consistency --
no contradictions in their internal reasoning. Our study reveals that even on
simple tasks, such as comparing points on a line or a plane, or reasoning in a
family tree, all smaller models are highly inconsistent, and even
state-of-the-art models like DeepSeek-R1 and GPT-o4-mini are not fully
self-consistent. To quantify and mitigate these inconsistencies, we introduce
inconsistency metrics and propose two automated methods -- a graph-based and an
energy-based approach. While these fixes provide partial improvements, they
also highlight the complexity and importance of self-consistency in building
more reliable and interpretable AI. The code and data are available at
https://github.com/scorpio-nova/llm-self-consistency.

</details>


### [130] [RWESummary: A Framework and Test for Choosing Large Language Models to Summarize Real-World Evidence (RWE) Studies](https://arxiv.org/abs/2506.18819)
*Arjun Mukerji,Michael L. Jackson,Jason Jones,Neil Sanghavi*

Main category: cs.CL

TL;DR: The paper introduces RWESummary as part of the MedHELM framework to benchmark large language models (LLMs) for summarizing real-world evidence (RWE) studies.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations for LLMs lack specific benchmarking for summarizing RWE studies, which are critical in medical research.

Method: The authors developed the RWESummary benchmark using proprietary data, focusing on one scenario and three key evaluation metrics for summarization errors.

Result: Tests using 13 RWE studies showed that Gemini 2.5 models (Flash and Pro variants) outperformed other LLMs in summarization tasks.

Conclusion: RWESummary provides a valuable foundation for evaluating and improving LLMs in medical RWE summarization tasks.

Abstract: Large Language Models (LLMs) have been extensively evaluated for general
summarization tasks as well as medical research assistance, but they have not
been specifically evaluated for the task of summarizing real-world evidence
(RWE) from structured output of RWE studies. We introduce RWESummary, a
proposed addition to the MedHELM framework (Bedi, Cui, Fuentes, Unell et al.,
2025) to enable benchmarking of LLMs for this task. RWESummary includes one
scenario and three evaluations covering major types of errors observed in
summarization of medical research studies and was developed using Atropos
Health proprietary data. Additionally, we use RWESummary to compare the
performance of different LLMs in our internal RWE summarization tool. At the
time of publication, with 13 distinct RWE studies, we found the Gemini 2.5
models performed best overall (both Flash and Pro). We suggest RWESummary as a
novel and useful foundation model benchmark for real-world evidence study
summarization.

</details>


### [131] [MLLP-VRAIN UPV system for the IWSLT 2025 Simultaneous Speech Translation Translation task](https://arxiv.org/abs/2506.18828)
*Jorge Iranzo-Sánchez,Javier Iranzo-Sánchez,Adrià Giménez,Jorge Civera,Alfons Juan*

Main category: cs.CL

TL;DR: The paper presents a modular cascade system for simultaneous long-form speech translation utilizing pre-trained models, Whisper Large-V3-Turbo for ASR and NLLB-3.3B for MT, achieving a BLEU score of 31.96 and low latency.


<details>
  <summary>Details</summary>
Motivation: To address challenges in real-time translation of long-form speech while avoiding extensive end-to-end model training or reliance on in-domain parallel data.

Method: The proposed system adapts pre-trained models using lightweight techniques such as prefix training for document-level adaptation, wait-$k$ and RALCP for emission policies, and specialized buffer and segmentation strategies for coherent translations.

Result: Achieved a BLEU score of 31.96 on ACL60/60 dataset with 2.94 seconds latency and 29.8 BLEU on the IWSLT25Instruct test set, balancing translation quality and latency.

Conclusion: Pre-trained models, when carefully adapted, can be leveraged to build effective real-time translation systems for long-form content with minimal new model training required.

Abstract: This work describes the participation of the MLLP-VRAIN research group in the
shared task of the IWSLT 2025 Simultaneous Speech Translation track. Our
submission addresses the unique challenges of real-time translation of
long-form speech by developing a modular cascade system that adapts strong
pre-trained models to streaming scenarios. We combine Whisper Large-V3-Turbo
for ASR with the multilingual NLLB-3.3B model for MT, implementing lightweight
adaptation techniques rather than training new end-to-end models from scratch.
Our approach employs document-level adaptation with prefix training to enhance
the MT model's ability to handle incomplete inputs, while incorporating
adaptive emission policies including a wait-$k$ strategy and RALCP for managing
the translation stream. Specialized buffer management techniques and
segmentation strategies ensure coherent translations across long audio
sequences. Experimental results on the ACL60/60 dataset demonstrate that our
system achieves a favorable balance between translation quality and latency,
with a BLEU score of 31.96 and non-computational-aware StreamLAAL latency of
2.94 seconds. Our final model achieves a preliminary score on the official test
set (IWSLT25Instruct) of 29.8 BLEU. Our work demonstrates that carefully
adapted pre-trained components can create effective simultaneous translation
systems for long-form content without requiring extensive in-domain parallel
data or specialized end-to-end training.

</details>


### [132] [STU-PID: Steering Token Usage via PID Controller for Efficient Large Language Model Reasoning](https://arxiv.org/abs/2506.18831)
*Aryasomayajula Ram Bharadwaj*

Main category: cs.CL

TL;DR: Large Language Models often overthink, generating excessive reasoning steps. The paper proposes STUPID, a method using PID controllers to dynamically manage reasoning during inference, improving both accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Large Language Models suffer from overthinking, increasing computational costs and degrading performance due to redundant reasoning steps. Static interventions lack adaptability.

Method: STUPID employs a PID controller combined with a classifier to dynamically adjust the intensity of activation steering during inference based on detected redundancy.

Result: STUPID improves accuracy by 6% and reduces token usage by 32% on GSM8K, surpassing static baselines in performance.

Conclusion: The method provides a dynamic and efficient framework for mitigating overthinking, maintaining reasoning quality while significantly reducing computational effort.

Abstract: Large Language Models employing extended chain-of-thought (CoT) reasoning
often suffer from the overthinking phenomenon, generating excessive and
redundant reasoning steps that increase computational costs while potentially
degrading performance. While recent work has explored static steering
approaches to mitigate this issue, they lack the adaptability to dynamically
adjust intervention strength based on real-time reasoning quality. We propose
STUPID (Steering Token Usage via PID controller), a novel training-free method
that employs a PID controller to dynamically modulate activation steering
strength during inference. Our approach combines a chunk-level classifier for
detecting redundant reasoning patterns with a PID control mechanism that
adaptively adjusts steering intensity based on the predicted redundancy
probability. Experimental evaluation on GSM8K demonstrates that STUPID achieves
a 6% improvement in accuracy while reducing token usage by 32%, outperforming
static steering baselines. Our method provides a principled framework for
dynamic reasoning calibration that maintains reasoning quality while
significantly improving computational efficiency.

</details>


### [133] [LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning](https://arxiv.org/abs/2506.18841)
*Yuhao Wu,Yushi Bai,Zhiqiang Hu,Roy Ka-Wei Lee,Juanzi Li*

Main category: cs.CL

TL;DR: The paper presents LongWriter-Zero, an approach using reinforcement learning (RL) instead of supervised fine-tuning for ultra-long high-quality text generation by large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Ultra-long text generation by LLMs faces challenges of length limits and quality degradation. Previous methods using synthetic fine-tuning are costly and produce artificial outputs. This motivates a new RL-based approach to overcome these challenges.

Method: The method employs RL from scratch on a base LLM, guided by reward models to improve text length control, writing quality, and structure formatting. No synthetic annotated data is used.

Result: LongWriter-Zero, trained from Qwen2.5-32B, achieves state-of-the-art performance on long-form writing benchmarks, outperforming even models with larger parameters.

Conclusion: The incentivization-based RL approach successfully enables ultra-long generation capabilities in LLMs and sets new standards for quality, validating RL as a promising alternative to SFT for long-form writing tasks.

Abstract: Ultra-long generation by large language models (LLMs) is a widely demanded
scenario, yet it remains a significant challenge due to their maximum
generation length limit and overall quality degradation as sequence length
increases. Previous approaches, exemplified by LongWriter, typically rely on
''teaching'', which involves supervised fine-tuning (SFT) on synthetic
long-form outputs. However, this strategy heavily depends on synthetic SFT
data, which is difficult and costly to construct, often lacks coherence and
consistency, and tends to be overly artificial and structurally monotonous. In
this work, we propose an incentivization-based approach that, starting entirely
from scratch and without relying on any annotated or synthetic data, leverages
reinforcement learning (RL) to foster the emergence of ultra-long, high-quality
text generation capabilities in LLMs. We perform RL training starting from a
base model, similar to R1-Zero, guiding it to engage in reasoning that
facilitates planning and refinement during the writing process. To support
this, we employ specialized reward models that steer the LLM towards improved
length control, writing quality, and structural formatting. Experimental
evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,
consistently outperforms traditional SFT methods on long-form writing tasks,
achieving state-of-the-art results across all metrics on WritingBench and
Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and
Qwen3-235B. We open-source our data and model checkpoints under
https://huggingface.co/THU-KEG/LongWriter-Zero-32B

</details>


### [134] [Mechanistic Interpretability Needs Philosophy](https://arxiv.org/abs/2506.18852)
*Iwan Williams,Ninell Oldenburg,Ruchira Dhar,Joshua Hatherley,Constanza Fierro,Nina Rajcic,Sandrine R. Schiller,Filippos Stamatiou,Anders Søgaard*

Main category: cs.CL

TL;DR: Mechanistic interpretability (MI) studies the inner workings of neural networks but requires philosophical input to refine its concepts, methods, and ethical considerations.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of MI necessitates a deeper reflection on its assumptions, strategies, and implications beyond purely technical models.

Method: The paper uses three open problems in MI as case studies to demonstrate how incorporating philosophy can enhance its approaches.

Result: The study shows that philosophical insights are essential in clarifying MI concepts, methods, and addressing broader epistemic and ethical issues.

Conclusion: Interdisciplinary collaboration between MI and philosophy should be prioritized to advance the field and responsibly interpret AI systems.

Abstract: Mechanistic interpretability (MI) aims to explain how neural networks work by
uncovering their underlying causal mechanisms. As the field grows in influence,
it is increasingly important to examine not just models themselves, but the
assumptions, concepts and explanatory strategies implicit in MI research. We
argue that mechanistic interpretability needs philosophy: not as an
afterthought, but as an ongoing partner in clarifying its concepts, refining
its methods, and assessing the epistemic and ethical stakes of interpreting AI
systems. Taking three open problems from the MI literature as examples, this
position paper illustrates the value philosophy can add to MI research, and
outlines a path toward deeper interdisciplinary dialogue.

</details>


### [135] [CommVQ: Commutative Vector Quantization for KV Cache Compression](https://arxiv.org/abs/2506.18879)
*Junyan Li,Yang Zhang,Muhammad Yusuf Hassan,Talha Chafekar,Tianle Cai,Zhile Ren,Pengsheng Guo,Foroozan Karimzadeh,Colorado Reed,Chong Wang,Chuang Gan*

Main category: cs.CL

TL;DR: The paper introduces a method called Commutative Vector Quantization (CommVQ) to reduce memory usage for long-context large language model inference by shrinking the key-value (KV) cache using quantization techniques.


<details>
  <summary>Details</summary>
Motivation: With the increasing use of Large Language Models (LLMs) in applications requiring long contexts, the memory bottleneck caused by the KV cache on GPUs poses a significant challenge.

Method: The authors propose additive quantization to compress the KV cache and integrate decoding efficiently into the self-attention mechanism using a commutative codebook trained via an Expectation-Maximization (EM) algorithm.

Result: Using 2-bit quantization, CommVQ achieves an 87.5% reduction in FP16 KV cache size and supports 1-bit quantization with minimal accuracy loss. It allows a LLaMA-3.1 8B model to operate with a 128K context length on a single RTX 4090 GPU.

Conclusion: CommVQ provides a scalable solution for long-context LLMs by significantly reducing memory usage, enabling efficient inference with negligible accuracy trade-offs.

Abstract: Large Language Models (LLMs) are increasingly used in applications requiring
long context lengths, but the key-value (KV) cache often becomes a memory
bottleneck on GPUs as context grows. To address this, we propose Commutative
Vector Quantization (CommVQ) to significantly reduce memory usage for
long-context LLM inference. We first introduce additive quantization with a
lightweight encoder and codebook to compress the KV cache, which can be decoded
via simple matrix multiplication. To further reduce computational costs during
decoding, we design the codebook to be commutative with Rotary Position
Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.
This enables efficient integration of decoding into the self-attention
mechanism. Our approach achieves high accuracy with additive quantization and
low overhead via the RoPE-commutative codebook. Experiments on long-context
benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%
with 2-bit quantization, while outperforming state-of-the-art KV cache
quantization methods. Notably, it enables 1-bit KV cache quantization with
minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context
length on a single RTX 4090 GPU. The source code is available at:
https://github.com/UMass-Embodied-AGI/CommVQ.

</details>


### [136] [OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization](https://arxiv.org/abs/2506.18880)
*Yiyou Sun,Shawn Hu,Georgia Zhou,Ken Zheng,Hannaneh Hajishirzi,Nouha Dziri,Dawn Song*

Main category: cs.CL

TL;DR: This paper introduces the OMEGA benchmark for evaluating Large Language Models (LLMs) on out-of-distribution generalization across exploratory, compositional, and transformative mathematical reasoning skills.


<details>
  <summary>Details</summary>
Motivation: LLMs like DeepSeek-R1 excel in standardized mathematical tests but struggle with creative or unconventional problem-solving. The paper aims to systematically assess and improve their generalization capabilities.

Method: The authors developed OMEGA, a diverse benchmark with training-test pairs generated programmatically across six mathematical domains. They tested frontier LLMs and performed fine-tuning experiments using the Qwen-series models.

Result: While fine-tuning improves exploratory generalization, compositional generalization remains limited, and transformative reasoning shows minimal progress. Performance decreases significantly as problem complexity rises.

Conclusion: OMEGA provides a controlled evaluation framework that highlights the limitations of current LLMs in handling complex, creative tasks, offering pathways to advance mathematical reasoning capabilities in AI.

Abstract: Recent large-scale language models (LLMs) with long Chain-of-Thought
reasoning-such as DeepSeek-R1-have achieved impressive results on
Olympiad-level mathematics benchmarks. However, they often rely on a narrow set
of strategies and struggle with problems that require a novel way of thinking.
To systematically investigate these limitations, we introduce
OMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a
controlled yet diverse benchmark designed to evaluate three axes of
out-of-distribution generalization, inspired by Boden's typology of creativity:
(1) Exploratory-applying known problem solving skills to more complex instances
within the same problem domain; (2) Compositional-combining distinct reasoning
skills, previously learned in isolation, to solve novel problems that require
integrating these skills in new and coherent ways; and (3)
Transformative-adopting novel, often unconventional strategies by moving beyond
familiar approaches to solve problems more effectively. OMEGA consists of
programmatically generated training-test pairs derived from templated problem
generators across geometry, number theory, algebra, combinatorics, logic, and
puzzles, with solutions verified using symbolic, numerical, or graphical
methods. We evaluate frontier (or top-tier) LLMs and observe sharp performance
degradation as problem complexity increases. Moreover, we fine-tune the
Qwen-series models across all generalization settings and observe notable
improvements in exploratory generalization, while compositional generalization
remains limited and transformative reasoning shows little to no improvement. By
isolating and quantifying these fine-grained failures, OMEGA lays the
groundwork for advancing LLMs toward genuine mathematical creativity beyond
mechanical proficiency.

</details>


### [137] [ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs](https://arxiv.org/abs/2506.18896)
*Jiaru Zou,Ling Yang,Jingwen Gu,Jiahao Qiu,Ke Shen,Jingrui He,Mengdi Wang*

Main category: cs.CL

TL;DR: The paper introduces ReasonFlux-PRM, a trajectory-aware process reward model (PRM) that evaluates and enhances reasoning traces in LLMs, outperforming previous PRMs and human-curated baselines.


<details>
  <summary>Details</summary>
Motivation: To improve the evaluation and supervision of intermediate reasoning steps in LLMs, especially for trajectory-response outputs from advanced reasoning systems like Deepseek-R1.

Method: Introduces ReasonFlux-PRM with step-level and trajectory-level supervision, enabling detailed reward assignments for structured reasoning. Supports offline and online settings for data selection, reinforcement learning, and test-time scaling.

Result: ReasonFlux-PRM surpasses prior PRMs and human baselines in selecting quality data, achieving performance gains in supervised fine-tuning (+12.1%), reinforcement learning (+4.5%), and test-time scaling (+6.3%).

Conclusion: ReasonFlux-PRM provides a robust framework for enhancing reasoning steps in LLMs, demonstrating significant empirical gains while also offering a lightweight version for resource-constrained environments.

Abstract: Process Reward Models (PRMs) have recently emerged as a powerful framework
for supervising intermediate reasoning steps in large language models (LLMs).
Previous PRMs are primarily trained on model final output responses and
struggle to evaluate intermediate thinking trajectories robustly, especially in
the emerging setting of trajectory-response outputs generated by frontier
reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a
novel trajectory-aware PRM explicitly designed to evaluate the
trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both
step-level and trajectory-level supervision, enabling fine-grained reward
assignment aligned with structured chain-of-thought data. We adapt
ReasonFlux-PRM to support reward supervision under both offline and online
settings, including (i) selecting high-quality model distillation data for
downstream supervised fine-tuning of smaller models, (ii) providing dense
process-level rewards for policy optimization during reinforcement learning,
and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results
on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond
demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs
(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our
derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving
average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement
learning, and 6.3% in test-time scaling. We also release our efficient
ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.
Projects: https://github.com/Gen-Verse/ReasonFlux

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [138] [Mechanistic Interpretability of Diffusion Models: Circuit-Level Analysis and Causal Validation](https://arxiv.org/abs/2506.17237)
*Dip Roy*

Main category: cs.CV

TL;DR: This paper analyzes diffusion models' computational pathways for image generation, discovering key differences in handling synthetic and real-world facial data and identifying distinct attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand the computational and algorithmic mechanisms behind how diffusion models generate images, particularly exploring differences in processing synthetic versus real-world (e.g., CelebA facial) image distributions.

Method: The authors conducted intervention experiments on 2,000 synthetic and 2,000 CelebA facial images, analyzing attention mechanisms, computational complexity, and degradation under targeted ablations to discern functional roles of model circuits.

Result: They found higher computational complexity in processing real-world facial images (complexity ratio = 1.084±0.008) and identified eight specialized attention mechanisms, such as edge detection, texture analysis, and semantic understanding. Causal roles were confirmed via performance degradation under ablations.

Conclusion: The paper provides foundational insights into understanding and controlling generative models by revealing critical computational mechanisms and causal pathways, enabling strategic interventions for model behavior optimization.

Abstract: We present a quantitative circuit-level analysis of diffusion models,
establishing computational pathways and mechanistic principles underlying image
generation processes. Through systematic intervention experiments across 2,000
synthetic and 2,000 CelebA facial images, we discover fundamental algorithmic
differences in how diffusion architectures process synthetic versus
naturalistic data distributions. Our investigation reveals that real-world face
processing requires circuits with measurably higher computational complexity
(complexity ratio = 1.084 plus/minus 0.008, p < 0.001), exhibiting distinct
attention specialization patterns with entropy divergence ranging from 0.015 to
0.166 across denoising timesteps. We identify eight functionally distinct
attention mechanisms showing specialized computational roles: edge detection
(entropy = 3.18 plus/minus 0.12), texture analysis (entropy = 4.16 plus/minus
0.08), and semantic understanding (entropy = 2.67 plus/minus 0.15).
Intervention analysis demonstrates critical computational bottlenecks where
targeted ablations produce 25.6% to 128.3% performance degradation, providing
causal evidence for identified circuit functions. These findings establish
quantitative foundations for algorithmic understanding and control of
generative model behavior through mechanistic intervention strategies.

</details>


### [139] [SRKD: Towards Efficient 3D Point Cloud Segmentation via Structure- and Relation-aware Knowledge Distillation](https://arxiv.org/abs/2506.17290)
*Yuqi Li,Junhao Dong,Zeyu Dong,Chuanguang Yang,Zhulin An,Yongjun Xu*

Main category: cs.CV

TL;DR: The study presents SRKD, a knowledge distillation framework to transfer knowledge from large teacher models to lightweight student models for efficient 3D point cloud segmentation.


<details>
  <summary>Details</summary>
Motivation: Address the computational complexity and deployment challenges of large-scale transformer-based models for 3D point cloud segmentation.

Method: Introduces SRKD leveraging relation alignment using affinity matrices, cross-sample mini-batch strategies, KL divergence for semantic alignment, and ground-truth supervision.

Result: Achieves state-of-the-art segmentation results with lower model complexity, making it suitable for real-world applications.

Conclusion: SRKD offers an effective and efficient solution for 3D point cloud segmentation by bridging large teacher models and lightweight student models.

Abstract: 3D point cloud segmentation faces practical challenges due to the
computational complexity and deployment limitations of large-scale
transformer-based models. To address this, we propose a novel Structure- and
Relation-aware Knowledge Distillation framework, named SRKD, that transfers
rich geometric and semantic knowledge from a large frozen teacher model (>100M)
to a lightweight student model (<15M). Specifically, we propose an affinity
matrix-based relation alignment module, which distills structural dependencies
from the teacher to the student through point-wise similarity matching,
enhancing the student's capability to learn contextual interactions. Meanwhile,
we introduce a cross-sample mini-batch construction strategy that enables the
student to perceive stable and generalized geometric structure. This aligns
across diverse point cloud instances of the teacher, rather than within a
single sample. Additionally, KL divergence is applied to align semantic
distributions, and ground-truth supervision further reinforces accurate
segmentation. Our method achieves state of the art performance with
significantly reduced model complexity, demonstrating its effectiveness and
efficiency in real-world deployment scenarios. Our Code is available at
https://github.com/itsnotacie/SRKD.

</details>


### [140] [Fine-Scale Soil Mapping in Alaska with Multimodal Machine Learning](https://arxiv.org/abs/2506.17302)
*Yijun Lin,Theresa Chen,Colby Brungard,Grunwald Sabine,Sue Ives,Matt Macander,Timm Nawrocki,Yao-Yi Chiang,Nic Jelinski*

Main category: cs.CV

TL;DR: This paper introduces MISO, an advanced machine learning model for fine-scale soil mapping in Alaska, with superior accuracy compared to traditional models like Random Forest, particularly for permafrost monitoring.


<details>
  <summary>Details</summary>
Motivation: To address the underdeveloped state of fine-scale soil mapping in Alaska and tackle challenges posed by permafrost thaw impacting ecosystem services and infrastructure stability.

Method: The study introduces MISO, integrating geospatial foundation models, implicit neural representations, and contrastive learning for enhanced soil mapping and multimodal geolocation awareness.

Result: MISO outperforms Random Forest in spatial generalization and recall, indicating its effectiveness in identifying vulnerable areas and supporting adaptation strategies.

Conclusion: Advanced machine learning approaches like MISO are pivotal for fine-scale soil mapping, guiding environmental monitoring and infrastructure planning in permafrost regions.

Abstract: Fine-scale soil mapping in Alaska, traditionally relying on fieldwork and
localized simulations, remains a critical yet underdeveloped task, despite the
region's ecological importance and extensive permafrost coverage. As permafrost
thaw accelerates due to climate change, it threatens infrastructure stability
and key ecosystem services, such as soil carbon storage. High-resolution soil
maps are essential for characterizing permafrost distribution, identifying
vulnerable areas, and informing adaptation strategies. We present MISO, a
vision-based machine learning (ML) model to produce statewide fine-scale soil
maps for near-surface permafrost and soil taxonomy. The model integrates a
geospatial foundation model for visual feature extraction, implicit neural
representations for continuous spatial prediction, and contrastive learning for
multimodal alignment and geo-location awareness. We compare MISO with Random
Forest (RF), a traditional ML model that has been widely used in soil mapping
applications. Spatial cross-validation and regional analysis across Permafrost
Zones and Major Land Resource Areas (MLRAs) show that MISO generalizes better
to remote, unseen locations and achieves higher recall than RF, which is
critical for monitoring permafrost thaw and related environmental processes.
These findings demonstrate the potential of advanced ML approaches for
fine-scale soil mapping and provide practical guidance for future soil sampling
and infrastructure planning in permafrost-affected landscapes. The project will
be released at https://github.com/knowledge-computing/Peatland-permafrost.

</details>


### [141] [RadarSeq: A Temporal Vision Framework for User Churn Prediction via Radar Chart Sequences](https://arxiv.org/abs/2506.17325)
*Sina Najafi,M. Hadi Sepanj,Fahimeh Jafari*

Main category: cs.CV

TL;DR: This paper presents a temporally-aware computer vision framework to predict user churn in non-subscription gig platforms by modeling user behaviors as sequential radar chart images analyzed via CNN-LSTM architecture.


<details>
  <summary>Details</summary>
Motivation: User churn prediction in non-subscription gig platforms is critical but challenging due to implicit disengagement and the lack of explicit labels, requiring innovative methods for early detection.

Method: The approach encodes day-level behavioral features into radar chart images and utilizes a pretrained CNN paired with a bidirectional LSTM to model spatial and temporal patterns of churn behavior.

Result: This method significantly outperforms classical models and ViT-based radar chart baselines, delivering notable gains in F1 score (+17.7), precision (+29.4), and AUC (+16.1) while enhancing interpretability.

Conclusion: The presented framework is modular, interpretable, and efficient, making it highly suitable for large-scale churn prediction in gig-economy platforms.

Abstract: Predicting user churn in non-subscription gig platforms, where disengagement
is implicit, poses unique challenges due to the absence of explicit labels and
the dynamic nature of user behavior. Existing methods often rely on aggregated
snapshots or static visual representations, which obscure temporal cues
critical for early detection. In this work, we propose a temporally-aware
computer vision framework that models user behavioral patterns as a sequence of
radar chart images, each encoding day-level behavioral features. By integrating
a pretrained CNN encoder with a bidirectional LSTM, our architecture captures
both spatial and temporal patterns underlying churn behavior. Extensive
experiments on a large real-world dataset demonstrate that our method
outperforms classical models and ViT-based radar chart baselines, yielding
gains of 17.7 in F1 score, 29.4 in precision, and 16.1 in AUC, along with
improved interpretability. The framework's modular design, explainability
tools, and efficient deployment characteristics make it suitable for
large-scale churn modeling in dynamic gig-economy platforms.

</details>


### [142] [P2MFDS: A Privacy-Preserving Multimodal Fall Detection System for Elderly People in Bathroom Environments](https://arxiv.org/abs/2506.17332)
*Haitian Wang,Yiren Wang,Xinyu Wang,Yumeng Miao,Yuliang Zhang,Yu Zhang,Atif Mansoor*

Main category: cs.CV

TL;DR: A new multimodal fall detection system using mmWave radar and 3D vibration sensing is introduced for elderly safety in bathrooms, showing improved accuracy in complex environments.


<details>
  <summary>Details</summary>
Motivation: The rising elderly population is prone to falls, especially in bathrooms, and current unimodal systems (WiFi, infrared, mmWave) have accuracy limitations due to environmental interference.

Method: The paper proposes P2MFDS, a dual-stream CNN-BiLSTM and CNN-SEBlock network designed for multimodal sensing, combining radar dynamics and vibration data for enhanced fall detection.

Result: P2MFDS significantly outperforms existing methods in accuracy and recall by utilizing multimodal sensing and unified feature analysis.

Conclusion: The proposed system effectively addresses the accuracy limitations of unimodal fall detection approaches, offering a practical solution for safer elderly living environments with public availability of datasets and models.

Abstract: By 2050, people aged 65 and over are projected to make up 16 percent of the
global population. As aging is closely associated with increased fall risk,
particularly in wet and confined environments such as bathrooms where over 80
percent of falls occur. Although recent research has increasingly focused on
non-intrusive, privacy-preserving approaches that do not rely on wearable
devices or video-based monitoring, these efforts have not fully overcome the
limitations of existing unimodal systems (e.g., WiFi-, infrared-, or
mmWave-based), which are prone to reduced accuracy in complex environments.
These limitations stem from fundamental constraints in unimodal sensing,
including system bias and environmental interference, such as multipath fading
in WiFi-based systems and drastic temperature changes in infrared-based
methods. To address these challenges, we propose a Privacy-Preserving
Multimodal Fall Detection System for Elderly People in Bathroom Environments.
First, we develop a sensor evaluation framework to select and fuse
millimeter-wave radar with 3D vibration sensing, and use it to construct and
preprocess a large-scale, privacy-preserving multimodal dataset in real
bathroom settings, which will be released upon publication. Second, we
introduce P2MFDS, a dual-stream network combining a CNN-BiLSTM-Attention branch
for radar motion dynamics with a multi-scale CNN-SEBlock-Self-Attention branch
for vibration impact detection. By uniting macro- and micro-scale features,
P2MFDS delivers significant gains in accuracy and recall over state-of-the-art
approaches. Code and pretrained models will be made available at:
https://github.com/HaitianWang/P2MFDS-A-Privacy-Preserving-Multimodal-Fall-Detection-Network-for-Elderly-Individuals-in-Bathroom.

</details>


### [143] [A Novel Multi-layer Task-centric and Data Quality Framework for Autonomous Driving](https://arxiv.org/abs/2506.17346)
*Yuhan Zhou,Haihua Chen,Kewei Sha*

Main category: cs.CV

TL;DR: This paper introduces a task-centric, data-quality-focused framework for improving performance in autonomous vehicles by addressing data quality issues.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the overlooked issue of data quality in autonomous vehicles, which is critical for functionality, efficiency, and trustworthiness in real-time decision-making.

Method: The paper proposes a novel five-layer data quality framework consisting of data, DQ, task, application, and goal layers. It maps data quality to task requirements and goals.

Result: The study shows that addressing data redundancy in the nuScenes dataset improves object detection performance using YOLOv8, and identifies redundancy issues in multimodal data (image and LiDAR).

Conclusion: The paper emphasizes the importance of adaptive, explainable, and resilient systems for autonomous vehicles, highlighting the need for further exploration of data quality and task orchestration challenges.

Abstract: The next-generation autonomous vehicles (AVs), embedded with frequent
real-time decision-making, will rely heavily on a large volume of multisource
and multimodal data. In real-world settings, the data quality (DQ) of different
sources and modalities usually varies due to unexpected environmental factors
or sensor issues. However, both researchers and practitioners in the AV field
overwhelmingly concentrate on models/algorithms while undervaluing the DQ. To
fulfill the needs of the next-generation AVs with guarantees of functionality,
efficiency, and trustworthiness, this paper proposes a novel task-centric and
data quality vase framework which consists of five layers: data layer, DQ
layer, task layer, application layer, and goal layer. The proposed framework
aims to map DQ with task requirements and performance goals. To illustrate, a
case study investigating redundancy on the nuScenes dataset proves that
partially removing redundancy on multisource image data could improve YOLOv8
object detection task performance. Analysis on multimodal data of image and
LiDAR further presents existing redundancy DQ issues. This paper opens up a
range of critical but unexplored challenges at the intersection of DQ, task
orchestration, and performance-oriented system development in AVs. It is
expected to guide the AV community toward building more adaptive, explainable,
and resilient AVs that respond intelligently to dynamic environments and
heterogeneous data streams. Code, data, and implementation details are publicly
available at: https://anonymous.4open.science/r/dq4av-framework/README.md.

</details>


### [144] [Efficient Feedback Gate Network for Hyperspectral Image Super-Resolution](https://arxiv.org/abs/2506.17361)
*Xufei Wang,Mingjian Zhang,Fei Ge,Jinchen Zhu,Wen Sha,Jifen Ren,Zhimeng Hou,Shouguo Zheng,ling Zheng,Shizhuang Weng*

Main category: cs.CV

TL;DR: The paper proposes an advanced SHSR method utilizing feedback and gate operations to enhance spatial resolution for hyperspectral imaging, outperforming prior benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing SHSR methods have limited performance due to inadequate exploration of coherence across bands and spatial-spectral information.

Method: Introduced an efficient feedback gate network incorporating novel modules like SPDFM and SSRGM to optimize spatial-spectral feature extraction and coherence.

Result: Experimental findings display improved spectral fidelity and spatial reconstruction on three hyperspectral datasets over current state-of-the-art methods.

Conclusion: The proposed network effectively enhances hyperspectral image super-resolution by leveraging innovative feedback and gate structures.

Abstract: Even without auxiliary images, single hyperspectral image super-resolution
(SHSR) methods can be designed to improve the spatial resolution of
hyperspectral images. However, failing to explore coherence thoroughly along
bands and spatial-spectral information leads to the limited performance of the
SHSR. In this study, we propose a novel group-based SHSR method termed the
efficient feedback gate network, which uses various feedbacks and gate
operations involving large kernel convolutions and spectral interactions. In
particular, by providing different guidance for neighboring groups, we can
learn rich band information and hierarchical hyperspectral spatial information
using channel shuffling and dilatation convolution in shuffled and progressive
dilated fusion module(SPDFM). Moreover, we develop a wide-bound perception gate
block and a spectrum enhancement gate block to construct the spatial-spectral
reinforcement gate module (SSRGM) and obtain highly representative
spatial-spectral features efficiently. Additionally, we apply a
three-dimensional SSRGM to enhance holistic information and coherence for
hyperspectral data. The experimental results on three hyperspectral datasets
demonstrate the superior performance of the proposed network over the
state-of-the-art methods in terms of spectral fidelity and spatial content
reconstruction.

</details>


### [145] [From Drawings to Decisions: A Hybrid Vision-Language Framework for Parsing 2D Engineering Drawings into Structured Manufacturing Knowledge](https://arxiv.org/abs/2506.17374)
*Muhammad Tayyab Khan,Lequn Chen,Zane Yong,Jun Ming Tan,Wenhe Feng,Seung Ki Moon*

Main category: cs.CV

TL;DR: This paper proposes a hybrid vision-language framework to reliably extract structured data from 2D engineering drawings for manufacturing workflows and demonstrates its effectiveness through experiments and a case study.


<details>
  <summary>Details</summary>
Motivation: Manual extraction of key data from 2D engineering drawings is slow and error-prone, while generic OCR models struggle with complex layouts and rotated text, calling for a solution tailored to this domain.

Method: The framework integrates YOLOv11-OBB for rotation-aware object detection and lightweight vision-language models (Donut and Florence-2) for parsing localized text into structured outputs, using a curated dataset for training.

Result: Experimental results show that the Donut model achieves strong performance metrics—88.5% precision, 99.2% recall, and 93.5% F1-score—surpassing Florence-2, and exhibits practical benefits in downstream manufacturing tasks in a case study.

Conclusion: The proposed framework effectively modernizes 2D engineering drawing interpretation, enabling accurate and efficient extraction of critical data for digital manufacturing advancements.

Abstract: Efficient and accurate extraction of key information from 2D engineering
drawings is essential for advancing digital manufacturing workflows. Such
information includes geometric dimensioning and tolerancing (GD&T), measures,
material specifications, and textual annotations. Manual extraction is slow and
labor-intensive, while generic OCR models often fail due to complex layouts,
engineering symbols, and rotated text, leading to incomplete and unreliable
outputs. These limitations result in incomplete and unreliable outputs. To
address these challenges, we propose a hybrid vision-language framework that
integrates a rotation-aware object detection model (YOLOv11-obb) with a
transformer-based vision-language parser. Our structured pipeline applies
YOLOv11-OBB to localize annotations and extract oriented bounding box (OBB)
patches, which are then parsed into structured outputs using a fine-tuned,
lightweight vision-language model (VLM). We curate a dataset of 1,367 2D
mechanical drawings annotated across nine key categories. YOLOv11-OBB is
trained on this dataset to detect OBBs and extract annotation patches. These
are parsed using two open-source VLMs: Donut and Florence-2. Both models are
lightweight and well-suited for specialized industrial tasks under limited
computational overhead. Following fine-tuning of both models on the curated
dataset of image patches paired with structured annotation labels, a
comparative experiment is conducted to evaluate parsing performance across four
key metrics. Donut outperforms Florence-2, achieving 88.5% precision, 99.2%
recall, and a 93.5% F1-score, with a hallucination rate of 11.5%. Finally, a
case study demonstrates how the extracted structured information supports
downstream manufacturing tasks such as process and tool selection, showcasing
the practical utility of the proposed framework in modernizing 2D drawing
interpretation.

</details>


### [146] [Spatial-Temporal Pre-Training for Embryo Viability Prediction Using Time-Lapse Videos](https://arxiv.org/abs/2506.17403)
*Zhiyi Shi,Junsik Kim,Helen Y. Yang,Yonghyun Song,Hyun-Jic Oh,Dalit Ben-Yosef,Daniel Needleman,Hanspeter Pfister*

Main category: cs.CV

TL;DR: The paper introduces a self-supervised learning (SSL) method designed for embryo viability predictions using IVF time-lapse videos, addressing challenges like long video lengths and temporal misalignment.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle to effectively utilize labeled pregnancy outcome data or handle challenges unique to IVF time-lapse videos, such as extensive frame count and variability.

Method: The proposed Spatial-Temporal Pre-Training (STPT) method operates in two stages—spatial and temporal—to reduce memory usage and handle variability. It trains encoders separately during different stages and avoids traditional frame alignment issues.

Result: STPT demonstrated high efficiency and outperformed baselines by achieving the highest AUC of 0.635 on a dataset of 23,027 videos, including 3,286 labeled samples.

Conclusion: STPT provides a computationally efficient and effective solution for predicting embryo viability in IVF scenarios, overcoming challenges associated with video length and temporal alignment.

Abstract: Automating embryo viability prediction for in vitro fertilization (IVF) is
important but challenging due to the limited availability of labeled pregnancy
outcome data, as only a small fraction of embryos are labeled after transfer.
Self-supervised learning (SSL) can leverage both labeled and unlabeled data to
improve prediction. However, existing SSL methods for videos are not directly
applicable to embryo development videos due to two challenges: (1) embryo
time-lapse videos contain hundreds of frames, requiring significant GPU memory
for conventional SSL; (2) the dataset contains videos with varying lengths and
many outlier frames, causing traditional video alignment methods to struggle
with semantic misalignment. We propose Spatial-Temporal Pre-Training (STPT) to
address these challenges. STPT includes two stages: spatial and temporal. In
each stage, only one encoder is trained while the other is frozen, reducing
memory demands. To handle temporal misalignment, STPT avoids frame-by-frame
alignment across videos. The spatial stage learns from alignments within each
video and its temporally consistent augmentations. The temporal stage then
models relationships between video embeddings. Our method efficiently handles
long videos and temporal variability. On 23,027 time-lapse videos (3,286
labeled), STPT achieves the highest AUC of 0.635 (95% CI: 0.632-0.638) compared
to baselines, with limited computational resources.

</details>


### [147] [VMRA-MaR: An Asymmetry-Aware Temporal Framework for Longitudinal Breast Cancer Risk Prediction](https://arxiv.org/abs/2506.17412)
*Zijun Sun,Solveig Thrun,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: The paper presents a novel method for early breast cancer detection using Vision Mamba RNN integrated with state-space models and asymmetry modules, achieving superior prediction over extended time periods.


<details>
  <summary>Details</summary>
Motivation: Current breast cancer screening heavily relies on recent imaging data, despite evidence in clinical practice that temporal trends in breast tissue evolution are significant for identifying risks.

Method: The paper utilizes Vision Mamba RNN (VMRNN) with state-space modeling and LSTM-like mechanisms to analyze longitudinal imaging data and incorporates a Spatial Asymmetry Detector (SAD) and Longitudinal Asymmetry Tracker (LAT) to detect relevant bilateral differences.

Result: The proposed framework significantly improves cancer onset prediction, particularly in high-density breast cases, performing notably well at predicting outcomes for years four and five.

Conclusion: This approach demonstrates potential to enhance early breast cancer diagnoses, especially for high-risk groups, and paves the way for targeted and personalized screening strategies.

Abstract: Breast cancer remains a leading cause of mortality worldwide and is typically
detected via screening programs where healthy people are invited in regular
intervals. Automated risk prediction approaches have the potential to improve
this process by facilitating dynamically screening of high-risk groups. While
most models focus solely on the most recent screening, there is growing
interest in exploiting temporal information to capture evolving trends in
breast tissue, as inspired by clinical practice. Early methods typically relied
on two time steps, and although recent efforts have extended this to multiple
time steps using Transformer architectures, challenges remain in fully
harnessing the rich temporal dynamics inherent in longitudinal imaging data. In
this work, we propose to instead leverage Vision Mamba RNN (VMRNN) with a
state-space model (SSM) and LSTM-like memory mechanisms to effectively capture
nuanced trends in breast tissue evolution. To further enhance our approach, we
incorporate an asymmetry module that utilizes a Spatial Asymmetry Detector
(SAD) and Longitudinal Asymmetry Tracker (LAT) to identify clinically relevant
bilateral differences. This integrated framework demonstrates notable
improvements in predicting cancer onset, especially for the more challenging
high-density breast cases and achieves superior performance at extended time
points (years four and five), highlighting its potential to advance early
breast cancer recognition and enable more personalized screening strategies.
Our code is available at https://github.com/Mortal-Suen/VMRA-MaR.git.

</details>


### [148] [Trans${^2}$-CBCT: A Dual-Transformer Framework for Sparse-View CBCT Reconstruction](https://arxiv.org/abs/2506.17425)
*Minmin Yang,Huantao Ren,Senem Velipasalar*

Main category: cs.CV

TL;DR: The paper introduces two novel models, Trans-CBCT and Trans²-CBCT, that improve sparse-view cone-beam CT reconstruction by combining convolutional, transformer, and point-based geometric methods, achieving better image quality with fewer views.


<details>
  <summary>Details</summary>
Motivation: To address challenges in sparse-view CBCT reconstruction, such as strong artifacts and poor spatial coverage, by improving image quality while maintaining low radiation doses and scan efficiency.

Method: The study replaces traditional encoder models with TransUNet, a hybrid CNN-Transformer, and introduces a neighbor-aware Point Transformer to enhance spatial consistency. These innovations combine multi-scale feature querying, attention mechanisms, and geometry reasoning for better 3D image reconstruction.

Result: The proposed Trans-CBCT model outperforms prior baselines by 1.17 dB PSNR and 0.0163 SSIM on the LUNA16 dataset with six X-ray views. The advanced Trans²-CBCT model further achieves gains of 0.63 dB PSNR and 0.0117 SSIM. These results generalize to other datasets like ToothFairy.

Conclusion: Combining CNN-Transformer architecture with 3D point-based geometry reasoning results in significant improvements for sparse-view CBCT reconstruction, reducing artifacts and enhancing spatial quality while maintaining efficiency.

Abstract: Cone-beam computed tomography (CBCT) using only a few X-ray projection views
enables faster scans with lower radiation dose, but the resulting severe
under-sampling causes strong artifacts and poor spatial coverage. We address
these challenges in a unified framework. First, we replace conventional
UNet/ResNet encoders with TransUNet, a hybrid CNN-Transformer model.
Convolutional layers capture local details, while self-attention layers enhance
global context. We adapt TransUNet to CBCT by combining multi-scale features,
querying view-specific features per 3D point, and adding a lightweight
attenuation-prediction head. This yields Trans-CBCT, which surpasses prior
baselines by 1.17 dB PSNR and 0.0163 SSIM on the LUNA16 dataset with six views.
Second, we introduce a neighbor-aware Point Transformer to enforce volumetric
coherence. This module uses 3D positional encoding and attention over k-nearest
neighbors to improve spatial consistency. The resulting model, Trans$^2$-CBCT,
provides an additional gain of 0.63 dB PSNR and 0.0117 SSIM. Experiments on
LUNA16 and ToothFairy show consistent gains from six to ten views, validating
the effectiveness of combining CNN-Transformer features with point-based
geometry reasoning for sparse-view CBCT reconstruction.

</details>


### [149] [Enhancing Wireless Device Identification through RF Fingerprinting: Leveraging Transient Energy Spectrum Analysis](https://arxiv.org/abs/2506.17439)
*Nisar Ahmed,Gulshan Saleem,Hafiz Muhammad Shahzad Asif,Muhammad Usman Younus,Kalsoom Safdar*

Main category: cs.CV

TL;DR: This paper proposes a CNN-Bi-GRU hybrid deep learning model combined with transient energy spectrum analysis for identifying RF devices, achieving over 99% accuracy, precision, recall, and F1 score.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of identifying radiation devices accurately and securely in complex electromagnetic environments due to the exponential growth of IoT and 5G technologies.

Method: The method uses transient energy spectrum analysis via the General Linear Chirplet Transform to extract features for classification modeling, coupled with a CNN-Bi-GRU hybrid deep learning model.

Result: The CNN-Bi-GRU approach achieved high classification performance metrics: precision of 99.33%, recall of 99.53%, F1-score of 99.43%, and accuracy of 99.17% using a dataset of nine RF devices.

Conclusion: The proposed method demonstrates strong potential for improving RF device identification and classification in complex wireless environments, with high accuracy and reliability.

Abstract: In recent years, the rapid growth of the Internet of Things technologies and
the widespread adoption of 5G wireless networks have led to an exponential
increase in the number of radiation devices operating in complex
electromagnetic environments. A key challenge in managing and securing these
devices is accurate identification and classification. To address this
challenge, specific emitter identification techniques have emerged as a
promising solution that aims to provide reliable and efficient means of
identifying individual radiation devices in a unified and standardized manner.
This research proposes an approach that leverages transient energy spectrum
analysis using the General Linear Chirplet Transform to extract features from
RF devices. A dataset comprising nine RF devices is utilized, with each sample
containing 900 attributes and a total of 1080 equally distributed samples
across the devices. These features are then used in a classification modeling
framework. To overcome the limitations of conventional machine learning
methods, we introduce a hybrid deep learning model called the CNN-Bi-GRU for
learning the identification of RF devices based on their transient
characteristics. The proposed approach provided a 10-fold cross-validation
performance with a precision of 99.33%, recall of 99.53%, F1-score of 99.43%,
and classification accuracy of 99.17%. The results demonstrate the promising
classification performance of the CNN-Bi-GRU approach, indicating its
suitability for accurately identifying RF devices based on their transient
characteristics and its potential for enhancing device identification and
classification in complex wireless environments.

</details>


### [150] [AQUA20: A Benchmark Dataset for Underwater Species Classification under Challenging Conditions](https://arxiv.org/abs/2506.17455)
*Taufikur Rahman Fuad,Sabbir Ahmed,Shahriar Ivan*

Main category: cs.CV

TL;DR: The paper introduces AQUA20, an underwater image dataset aimed at addressing challenges in visual recognition under harsh marine conditions, and benchmarks deep learning models' performance on the dataset.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty of robust visual recognition in underwater environments due to turbidity, low illumination, and occlusions by introducing a dedicated dataset and evaluating state-of-the-art models.

Method: Compiled a dataset (AQUA20) of 8,171 underwater images of 20 marine species under challenging circumstances and benchmarked 13 deep learning models, including lightweight CNNs and transformer-based architectures.

Result: ConvNeXt achieved the highest accuracy (Top-1: 90.69%, Top-3: 98.82%) and overall F1-score of 88.92%, with analyses showcasing trade-offs between model complexity and performance.

Conclusion: The study highlights the need for improvement in underwater image recognition and introduces AQUA20 as a significant research resource, with models' results and explainability analyses providing insights for future advancements.

Abstract: Robust visual recognition in underwater environments remains a significant
challenge due to complex distortions such as turbidity, low illumination, and
occlusion, which severely degrade the performance of standard vision systems.
This paper introduces AQUA20, a comprehensive benchmark dataset comprising
8,171 underwater images across 20 marine species reflecting real-world
environmental challenges such as illumination, turbidity, occlusions, etc.,
providing a valuable resource for underwater visual understanding. Thirteen
state-of-the-art deep learning models, including lightweight CNNs (SqueezeNet,
MobileNetV2) and transformer-based architectures (ViT, ConvNeXt), were
evaluated to benchmark their performance in classifying marine species under
challenging conditions. Our experimental results show ConvNeXt achieving the
best performance, with a Top-3 accuracy of 98.82% and a Top-1 accuracy of
90.69%, as well as the highest overall F1-score of 88.92% with moderately large
parameter size. The results obtained from our other benchmark models also
demonstrate trade-offs between complexity and performance. We also provide an
extensive explainability analysis using GRAD-CAM and LIME for interpreting the
strengths and pitfalls of the models. Our results reveal substantial room for
improvement in underwater species recognition and demonstrate the value of
AQUA20 as a foundation for future research in this domain. The dataset is
publicly available at: https://huggingface.co/datasets/taufiktrf/AQUA20.

</details>


### [151] [When Every Millisecond Counts: Real-Time Anomaly Detection via the Multimodal Asynchronous Hybrid Network](https://arxiv.org/abs/2506.17457)
*Dong Xiao,Guangyao Chen,Peixi Peng,Yangru Huang,Yifan Zhao,Yongxing Dai,Yonghong Tian*

Main category: cs.CV

TL;DR: The paper introduces a real-time anomaly detection system combining event cameras and RGB cameras for autonomous driving, achieving high accuracy with minimal response time.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly detection systems for autonomous driving often neglect response time, which is essential for time-sensitive scenarios.

Method: The paper employs a multimodal asynchronous hybrid network that merges event-camera data interpreted by a Graph Neural Network (GNN) and spatial data from RGB camera images processed by a CNN.

Result: The proposed system outperforms existing methods in both detection accuracy and response time, with real-time performance measured in milliseconds.

Conclusion: The integration of asynchronous event streams with RGB image data enables effective real-time anomaly detection in autonomous driving systems.

Abstract: Anomaly detection is essential for the safety and reliability of autonomous
driving systems. Current methods often focus on detection accuracy but neglect
response time, which is critical in time-sensitive driving scenarios. In this
paper, we introduce real-time anomaly detection for autonomous driving,
prioritizing both minimal response time and high accuracy. We propose a novel
multimodal asynchronous hybrid network that combines event streams from event
cameras with image data from RGB cameras. Our network utilizes the high
temporal resolution of event cameras through an asynchronous Graph Neural
Network and integrates it with spatial features extracted by a CNN from RGB
images. This combination effectively captures both the temporal dynamics and
spatial details of the driving environment, enabling swift and precise anomaly
detection. Extensive experiments on benchmark datasets show that our approach
outperforms existing methods in both accuracy and response time, achieving
millisecond-level real-time performance.

</details>


### [152] [Photogranulometry -- Dataset of soil images with corresponding particle size distributions](https://arxiv.org/abs/2506.17469)
*Thomas Plante St-Cyr,François Duhaime,Jean-Sébastien Dubé,Simon Grenier*

Main category: cs.CV

TL;DR: The paper introduces an optical grain size analysis and provides a detailed dataset of soil images for developing CNNs in geotechnical applications.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiency, high cost, and downtime associated with traditional particle size distribution methods by integrating optical grain size analysis into geotechnical workflows.

Method: The researchers created a dataset of 12,714 high-resolution images of soil samples, photographed in standardized conditions, for use in training convolutional neural networks. Methods included photographing in moist and dry states and using a test bench with white aluminum trays, along with coning and quartering for large samples.

Result: The result is a high-quality dataset containing high-resolution images of 321 distinct soil samples, ready for use in machine learning applications.

Conclusion: The dataset serves as a valuable resource to train CNNs, potentially revolutionizing the efficiency of grain size analysis in geotechnical studies.

Abstract: Traditional particle size distribution (PSD) analyses create significant
downtime and are expensive in labor and maintenance. These drawbacks could be
alleviated using optical grain size analysis integrated into routine
geotechnical laboratory workflow. This paper presents a high-resolution dataset
of 12,714 images of 321 different soil samples collected in the Montreal,
Quebec region, alongside their PSD analysis. It is designed to provide a robust
starting point for training convolutional neural networks (CNN) in geotechnical
applications. Soil samples were photographed in a standardized top-view
position with a resolution of 45 MP and a minimum scale of 39.4 micrometers per
pixel, both in their moist and dry states. A custom test bench employing 13x9
inch white aluminum trays, on which the samples are spread in a thin layer, was
used. For samples exceeding a size limit, a coning and quartering method was
employed for mass reduction.

</details>


### [153] [Few-Shot, Now for Real: Medical VLMs Adaptation without Balanced Sets or Validation](https://arxiv.org/abs/2506.17500)
*Julio Silva-Rodríguez,Fereshteh Shakeri,Houda Bahig,Jose Dolz,Ismail Ben Ayed*

Main category: cs.CV

TL;DR: This paper identifies limitations in current vision-language models (VLMs) for medical image analysis under realistic data constraints and proposes a novel training-free linear probe for robust few-shot adaptation.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies in previous VLM adaptation methods for medical image analysis caused by assumptions like balanced support sets and reliance on validation sets.

Method: Introduce a realistic adaptation setting with imbalanced support sets and no validation sets, along with a training-free linear probe that integrates visual and textual supervision.

Result: Existing methods often fail under realistic conditions, sometimes performing worse than zero-shot inference. The proposed training-free probe delivers efficient and robust adaptation performance.

Conclusion: The study highlights the need for realistic data assumptions in VLM adaptation and offers an effective solution for robust deployment under practical conditions.

Abstract: Vision-language models (VLMs) are gaining attention in medical image
analysis. These are pre-trained on large, heterogeneous data sources, yielding
rich and transferable representations. Notably, the combination of
modality-specialized VLMs with few-shot adaptation has provided fruitful
results, enabling the efficient deployment of high-performing solutions.
However, previous works on this topic make strong assumptions about the
distribution of adaptation data, which are unrealistic in the medical domain.
First, prior art assumes access to a balanced support set, a condition that
breaks the natural imbalance in disease prevalence found in real-world
scenarios. Second, these works typically assume the presence of an additional
validation set to fix critical hyper-parameters, which is highly
data-inefficient. This work challenges these favorable deployment scenarios and
introduces a realistic, imbalanced, validation-free adaptation setting. Our
extensive benchmark across various modalities and downstream tasks demonstrates
that current methods systematically compromise their performance when operating
under realistic conditions, occasionally even performing worse than zero-shot
inference. Also, we introduce a training-free linear probe that adaptively
blends visual and textual supervision. Detailed studies demonstrate that the
proposed solver is a strong, efficient baseline, enabling robust adaptation in
challenging scenarios.

</details>


### [154] [Trustworthy Few-Shot Transfer of Medical VLMs through Split Conformal Prediction](https://arxiv.org/abs/2506.17503)
*Julio Silva-Rodríguez,Ismail Ben Ayed,Jose Dolz*

Main category: cs.CV

TL;DR: This paper introduces transductive split conformal adaptation (SCA-T) to enhance medical vision-language models (VLMs) reliability while transferring them to new tasks.


<details>
  <summary>Details</summary>
Motivation: Reliability of medical VLMs in transfer tasks remains unexplored, despite their growing adoption and efficiency in image classification.

Method: The authors propose SCA-T, which involves unsupervised transductive adaptation performed jointly on calibration and test data to conformal scenarios.

Result: Experimental results show gains in efficiency and conditional coverage compared to SCP, while preserving empirical guarantees, across various image modalities and transfer tasks.

Conclusion: SCA-T offers a superior approach for conformal adaptation of VLMs, addressing key issues in reliability during transfer learning.

Abstract: Medical vision-language models (VLMs) have demonstrated unprecedented
transfer capabilities and are being increasingly adopted for data-efficient
image classification. Despite its growing popularity, its reliability aspect
remains largely unexplored. This work explores the split conformal prediction
(SCP) framework to provide trustworthiness guarantees when transferring such
models based on a small labeled calibration set. Despite its potential, the
generalist nature of the VLMs' pre-training could negatively affect the
properties of the predicted conformal sets for specific tasks. While common
practice in transfer learning for discriminative purposes involves an
adaptation stage, we observe that deploying such a solution for conformal
purposes is suboptimal since adapting the model using the available calibration
data breaks the rigid exchangeability assumptions for test data in SCP. To
address this issue, we propose transductive split conformal adaptation (SCA-T),
a novel pipeline for transfer learning on conformal scenarios, which performs
an unsupervised transductive adaptation jointly on calibration and test data.
We present comprehensive experiments utilizing medical VLMs across various
image modalities, transfer tasks, and non-conformity scores. Our framework
offers consistent gains in efficiency and conditional coverage compared to SCP,
maintaining the same empirical guarantees.

</details>


### [155] [Learning golf swing signatures from a single wrist-worn inertial sensor](https://arxiv.org/abs/2506.17505)
*Jessy Lauer*

Main category: cs.CV

TL;DR: This paper proposes a data-driven framework for analyzing golf swings using wrist-worn sensors, providing lab-grade motion analysis and actionable feedback.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address gaps in golf swing analysis, such as reliance on isolated metrics, lack of professional athlete data, and insufficient interpretability of movement representations.

Method: The method involves using publicly available videos to reconstruct 3D full-body kinematics, generating synthetic inertial data, training neural networks for motion prediction, and segmenting swing phases. It also creates a vocabulary of motion primitives for analyzing technical flaws.

Result: The system accurately estimates full-body kinematics and identifies swing events using wrist-based input. It also predicts player-related attributes and longitudinal changes, providing actionable technical feedback.

Conclusion: The system offers scalable, high-fidelity motion analysis for biomechanics research, coaching, and injury prevention while uncovering personalized movement biomarkers, challenging traditional assumptions about swings, and paving the way for innovation in golf training and equipment design.

Abstract: Despite its importance for performance and injury prevention, golf swing
analysis is limited by isolated metrics, underrepresentation of professional
athletes, and a lack of rich, interpretable movement representations. We
address these gaps with a holistic, data-driven framework for personalized golf
swing analysis from a single wrist-worn sensor. We build a large dataset of
professional swings from publicly available videos, reconstruct full-body 3D
kinematics using biologically accurate human mesh recovery, and generate
synthetic inertial data to train neural networks that infer motion and segment
swing phases from wrist-based input. We learn a compositional, discrete
vocabulary of motion primitives that facilitates the detection and
visualization of technical flaws, and is expressive enough to predict player
identity, club type, sex, and age. Our system accurately estimates full-body
kinematics and swing events from wrist data, delivering lab-grade motion
analysis on-course and supporting early detection of anomalous movement
patterns. Explainability methods reveal subtle, individualized movement
signatures, reinforcing the view that variability is a hallmark of skilled
performance. Longitudinal tracking demonstrates practical value: as one
player's handicap improved from 50 to 2.2 over 1.5 years, our system captured
measurable technical progress and provided targeted, actionable feedback. Our
findings challenge common assumptions, such as swing consistency across clubs
and the existence of a single "ideal" swing, and uncover latent biomarkers
shaped by both intrinsic traits and task-specific constraints. This work
bridges lab and field-based biomechanics, offering scalable, accessible,
high-fidelity motion analysis for research, coaching, and injury prevention,
while opening new directions in movement-based phenotyping, personalized
equipment design, and motor skill development.

</details>


### [156] [Scene-R1: Video-Grounded Large Language Models for 3D Scene Reasoning without 3D Annotations](https://arxiv.org/abs/2506.17545)
*Zhihao Yuan,Shuyi Jiang,Chun-Mei Feng,Yaolun Zhang,Shuguang Cui,Zhen Li,Na Zhao*

Main category: cs.CV

TL;DR: Scene-R1 is a framework that enables 3D scene reasoning without point-wise 3D instance supervision by combining reinforcement learning and a two-stage grounding pipeline.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing 3D-aware language models in understanding 3D scenes, which rely on pre-trained 3D detectors and act as black boxes.

Method: The method involves a two-stage grounding pipeline: temporal grounding to select relevant video snippets and image grounding to predict 2D bounding boxes. The system then tracks objects, projects them back to 3D, and doesn't require dense 3D point-wise supervision.

Result: Scene-R1 outperforms existing open-vocabulary baselines across multiple datasets, offering efficient 3D scene understanding with transparent reasoning.

Conclusion: Reinforcement-learning-driven reasoning combined with RGB-D video data provides an efficient, annotation-light, and trustworthy solution for 3D scene understanding.

Abstract: Currently, utilizing large language models to understand the 3D world is
becoming popular. Yet existing 3D-aware LLMs act as black boxes: they output
bounding boxes or textual answers without revealing how those decisions are
made, and they still rely on pre-trained 3D detectors to supply object
proposals. We introduce Scene-R1, a video-grounded framework that learns to
reason about 3D scenes without any point-wise 3D instance supervision by
pairing reinforcement-learning-driven reasoning with a two-stage grounding
pipeline. In the temporal grounding stage, we explicitly reason about the video
and select the video snippets most relevant to an open-ended query. In the
subsequent image grounding stage, we analyze the image and predict the 2D
bounding box. After that, we track the object using SAM2 to produce
pixel-accurate masks in RGB frames, and project them back into 3D, thereby
eliminating the need for 3D detector-based proposals while capturing fine
geometry and material cues. Scene-R1 can also adapt to the 3D visual question
answering task to answer free-form questions directly from video. Our training
pipeline only needs task-level 2D boxes or textual labels without dense 3D
point-wise labels. Scene-R1 surpasses existing open-vocabulary baselines on
multiple datasets, while delivering transparent, step-by-step rationales. These
results show that reinforcement-learning-based reasoning combined with RGB-D
video alone offers a practical, annotation-efficient route to trustworthy 3D
scene understanding.

</details>


### [157] [SynDaCaTE: A Synthetic Dataset For Evaluating Part-Whole Hierarchical Inference](https://arxiv.org/abs/2506.17558)
*Jake Levi,Mark van der Wilk*

Main category: cs.CV

TL;DR: This paper introduces a new synthetic dataset (SynDaCaTE) for testing and evaluating capsule models and investigates their ability to infer part-whole hierarchies.


<details>
  <summary>Details</summary>
Motivation: Understanding whether capsule networks truly learn part-whole hierarchies as they are claimed to do for improving data efficiency and robustness.

Method: The authors develop SynDaCaTE, a synthetic dataset tailored for capsule models and use it to evaluate existing approaches and test alternative mechanisms like permutation-equivariant self-attention.

Result: Identified bottlenecks in existing capsule models and demonstrated that permutation-equivariant self-attention performs well for parts-to-wholes inference.

Conclusion: SynDaCaTE is useful for evaluating capsule networks and opens pathways for designing better inductive biases in computer vision models.

Abstract: Learning to infer object representations, and in particular part-whole
hierarchies, has been the focus of extensive research in computer vision, in
pursuit of improving data efficiency, systematic generalisation, and
robustness. Models which are \emph{designed} to infer part-whole hierarchies,
often referred to as capsule networks, are typically trained end-to-end on
supervised tasks such as object classification, in which case it is difficult
to evaluate whether such a model \emph{actually} learns to infer part-whole
hierarchies, as claimed. To address this difficulty, we present a SYNthetic
DAtaset for CApsule Testing and Evaluation, abbreviated as SynDaCaTE, and
establish its utility by (1) demonstrating the precise bottleneck in a
prominent existing capsule model, and (2) demonstrating that
permutation-equivariant self-attention is highly effective for parts-to-wholes
inference, which motivates future directions for designing effective inductive
biases for computer vision.

</details>


### [158] [VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models](https://arxiv.org/abs/2506.17561)
*Chongkai Gao,Zixuan Liu,Zhenghao Chi,Junshan Huang,Xin Fei,Yiwen Hou,Yuxuan Zhang,Yudi Lin,Zhirui Fang,Zeyu Jiang,Lin Shao*

Main category: cs.CV

TL;DR: This paper studies Vision-Language-Action (VLA) models and their task planning paradigms, proposing the VLA-OS architecture to evaluate performance trade-offs systematically.


<details>
  <summary>Details</summary>
Motivation: Current VLA research lacks a unified framework to disentangle the impact of planning paradigms and representations from network architectures and training data.

Method: The authors introduce VLA-OS, a unified VLA architecture compatible with multiple planning paradigms, and conduct controlled experiments across diverse setups, including different object types, modalities, environments, and end-effectors.

Result: They found that visually grounded planning representations generally outperform language-based ones and that a Hierarchical-VLA paradigm performs best in several aspects but has slower training and inference.

Conclusion: This work provides a comprehensive investigation of VLA planning approaches, concluding that visually grounded, hierarchical paradigms offer strong performance with specific trade-offs for speed.

Abstract: Recent studies on Vision-Language-Action (VLA) models have shifted from the
end-to-end action-generation paradigm toward a pipeline involving task planning
followed by action generation, demonstrating improved performance on various
complex, long-horizon manipulation tasks. However, existing approaches vary
significantly in terms of network architectures, planning paradigms,
representations, and training data sources, making it challenging for
researchers to identify the precise sources of performance gains and components
to be further improved. To systematically investigate the impacts of different
planning paradigms and representations isolating from network architectures and
training data, in this paper, we introduce VLA-OS, a unified VLA architecture
series capable of various task planning paradigms, and design a comprehensive
suite of controlled experiments across diverse object categories (rigid and
deformable), visual modalities (2D and 3D), environments (simulation and
real-world), and end-effectors (grippers and dexterous hands). Our results
demonstrate that: 1) visually grounded planning representations are generally
better than language planning representations; 2) the Hierarchical-VLA paradigm
generally achieves superior or comparable performance than other paradigms on
task performance, pretraining, generalization ability, scalability, and
continual learning ability, albeit at the cost of slower training and inference
speeds.

</details>


### [159] [LLM-driven Medical Report Generation via Communication-efficient Heterogeneous Federated Learning](https://arxiv.org/abs/2506.17562)
*Haoxuan Che,Haibo Jin,Zhengrui Guo,Yi Lin,Cheng Jin,Hao Chen*

Main category: cs.CV

TL;DR: FedMRG is a framework using Federated Learning to enable privacy-preserving and communication-efficient medical report generation by LLMs across multiple centers.


<details>
  <summary>Details</summary>
Motivation: Medical Report Generation using LLMs faces challenges due to scattered data across centers and privacy regulations, hindering centralized development.

Method: The framework uses low-rank factorization to reduce communication costs in Federated Learning and employs techniques like client-aware contrastive learning and dual-adapter mutual boosting to address multi-modal data heterogeneity.

Result: FedMRG demonstrates generalizability and adaptability with multi-center data while retaining communication efficiency and generating clinically accurate reports.

Conclusion: FedMRG is a promising solution for effective distributed LLM training in medical report generation, addressing both communication challenges and data heterogeneity.

Abstract: LLMs have demonstrated significant potential in Medical Report Generation
(MRG), yet their development requires large amounts of medical image-report
pairs, which are commonly scattered across multiple centers. Centralizing these
data is exceptionally challenging due to privacy regulations, thereby impeding
model development and broader adoption of LLM-driven MRG models. To address
this challenge, we present FedMRG, the first framework that leverages Federated
Learning (FL) to enable privacy-preserving, multi-center development of
LLM-driven MRG models, specifically designed to overcome the critical challenge
of communication-efficient LLM training under multi-modal data heterogeneity.
To start with, our framework tackles the fundamental challenge of communication
overhead in FL-LLM tuning by employing low-rank factorization to efficiently
decompose parameter updates, significantly reducing gradient transmission costs
and making LLM-driven MRG feasible in bandwidth-constrained FL settings.
Furthermore, we observed the dual heterogeneity in MRG under the FL scenario:
varying image characteristics across medical centers, as well as diverse
reporting styles and terminology preferences. To address this, we further
enhance FedMRG with (1) client-aware contrastive learning in the MRG encoder,
coupled with diagnosis-driven prompts, which capture both globally
generalizable and locally distinctive features while maintaining diagnostic
accuracy; and (2) a dual-adapter mutual boosting mechanism in the MRG decoder
that harmonizes generic and specialized adapters to address variations in
reporting styles and terminology. Through extensive evaluation of our
established FL-MRG benchmark, we demonstrate the generalizability and
adaptability of FedMRG, underscoring its potential in harnessing multi-center
data and generating clinically accurate reports while maintaining communication
efficiency.

</details>


### [160] [HalluRNN: Mitigating Hallucinations via Recurrent Cross-Layer Reasoning in Large Vision-Language Models](https://arxiv.org/abs/2506.17587)
*Le Yu,Kaishen Wang,Jianlong Xiong,Yue Cao,Tao He*

Main category: cs.CV

TL;DR: HalluRNN addresses hallucinations in Large Vision-Language Models using a recurrent cross-layer reasoning solution, introducing the DG-DPU module.


<details>
  <summary>Details</summary>
Motivation: Large Vision-Language Models suffer from generating visually ungrounded outputs, a problem that existing solutions address with resource-heavy or task-specific methods.

Method: The study proposes HalluRNN, featuring a Dual-Gated Depth Propagation Unit (DG-DPU) module shared across layers to refine hidden states through recurrent reasoning.

Result: HalluRNN, with fine-tuning limited to just the DG-DPU module, demonstrates strong performance across multiple benchmarks.

Conclusion: The paper presents HalluRNN as an architecture-level, resource-efficient solution to mitigate hallucinations in LVLMs while maintaining robust model performance.

Abstract: Though Large Vision-Language Models (LVLMs) have achieved remarkable
performance across various tasks, they are still prone to
hallucinations-generating outputs that are textually plausible but visually
ungrounded. While prior approaches generally address this issue through
data-centric fine-tuning or innovative decoding strategies, these methods often
require substantial resources or task-specific configurations. In this work, we
introduce an architecture-level solution, HalluRNN, which enhances model
stability through recurrent cross-layer reasoning. Specifically, we propose a
novel Dual-Gated Depth Propagation Unit (DG-DPU) module, which is shared across
layers and recurrently refines hidden states. This allows for the adaptive
propagation of information throughout the model, enforces consistency across
layers, and mitigates hallucinations caused by representational drift. By
fine-tuning only the DG-DPU module, HalluRNN achieves strong and robust
performance across multiple benchmarks.

</details>


### [161] [DRAMA-X: A Fine-grained Intent Prediction and Risk Reasoning Benchmark For Driving](https://arxiv.org/abs/2506.17590)
*Mihir Godbole,Xiangbo Gao,Zhengzhong Tu*

Main category: cs.CV

TL;DR: This paper introduces DRAMA-X, a benchmark focused on evaluating fine-grained intent prediction for vulnerable road users to enhance autonomous driving safety. It also proposes SGG-Intent, a lightweight framework using vision-language models and scene-graph reasoning.


<details>
  <summary>Details</summary>
Motivation: The study is driven by the need to improve autonomous driving systems in safety-critical urban scenarios where vulnerable road users exhibit ambiguous or risky behaviors.

Method: The authors developed DRAMA-X utilizing automated annotations, and proposed SGG-Intent, a framework leveraging vision-language models and compositional reasoning with scene graphs.

Result: Results show that incorporating scene-graph reasoning boosts the prediction of VRU intent and risk assessment in autonomous systems, especially with contextual cues.

Conclusion: The paper underscores the importance of detailed benchmarks for multi-class intent prediction and demonstrates the potential of combining VLMs with structured reasoning for enhancing autonomous vehicle decision-making.

Abstract: Understanding the short-term motion of vulnerable road users (VRUs) like
pedestrians and cyclists is critical for safe autonomous driving, especially in
urban scenarios with ambiguous or high-risk behaviors. While vision-language
models (VLMs) have enabled open-vocabulary perception, their utility for
fine-grained intent reasoning remains underexplored. Notably, no existing
benchmark evaluates multi-class intent prediction in safety-critical
situations, To address this gap, we introduce DRAMA-X, a fine-grained benchmark
constructed from the DRAMA dataset via an automated annotation pipeline.
DRAMA-X contains 5,686 accident-prone frames labeled with object bounding
boxes, a nine-class directional intent taxonomy, binary risk scores,
expert-generated action suggestions for the ego vehicle, and descriptive motion
summaries. These annotations enable a structured evaluation of four
interrelated tasks central to autonomous decision-making: object detection,
intent prediction, risk assessment, and action suggestion. As a reference
baseline, we propose SGG-Intent, a lightweight, training-free framework that
mirrors the ego vehicle's reasoning pipeline. It sequentially generates a scene
graph from visual input using VLM-backed detectors, infers intent, assesses
risk, and recommends an action using a compositional reasoning stage powered by
a large language model. We evaluate a range of recent VLMs, comparing
performance across all four DRAMA-X tasks. Our experiments demonstrate that
scene-graph-based reasoning enhances intent prediction and risk assessment,
especially when contextual cues are explicitly modeled.

</details>


### [162] [SELFI: Selective Fusion of Identity for Generalizable Deepfake Detection](https://arxiv.org/abs/2506.17592)
*Younghun Kim,Minsuk Jang,Myung-Joon Kwon,Wonjun Lee,Changick Kim*

Main category: cs.CV

TL;DR: This paper explores the role of face identity in deepfake detection, introducing a novel framework called SELFI to adaptively utilize identity cues for improved performance.


<details>
  <summary>Details</summary>
Motivation: To address conflicting views on the role of face identity in deepfake detection and to improve cross-manipulation generalization in detection methods.

Method: This paper proposes SELFI, a detection framework that includes a Forgery-Aware Identity Adapter (FAIA) to project identity embeddings into a forgery-relevant space and an Identity-Aware Fusion Module (IAFM) to integrate identity and visual features selectively.

Result: SELFI improves the generalization of deepfake detection models across manipulation methods, outperforming prior methods with an average gain of 3.1% in AUC on four benchmarks and by 6% on the DFDC dataset.

Conclusion: Explicit and adaptive modeling of identity features enhances the performance and generalization of deepfake detection models, demonstrating the effectiveness of SELFI.

Abstract: Face identity provides a powerful signal for deepfake detection. Prior
studies show that even when not explicitly modeled, classifiers often learn
identity features implicitly. This has led to conflicting views: some suppress
identity cues to reduce bias, while others rely on them as forensic evidence.
To reconcile these views, we analyze two hypotheses: (1) whether face identity
alone is discriminative for detecting deepfakes, and (2) whether such identity
features generalize poorly across manipulation methods. Our experiments confirm
that identity is informative but context-dependent. While some manipulations
preserve identity-consistent artifacts, others distort identity cues and harm
generalization. We argue that identity features should neither be blindly
suppressed nor relied upon, but instead be explicitly modeled and adaptively
controlled based on per-sample relevance. We propose \textbf{SELFI}
(\textbf{SEL}ective \textbf{F}usion of \textbf{I}dentity), a generalizable
detection framework that dynamically modulates identity usage. SELFI consists
of: (1) a Forgery-Aware Identity Adapter (FAIA) that extracts identity
embeddings from a frozen face recognition model and projects them into a
forgery-relevant space via auxiliary supervision; and (2) an Identity-Aware
Fusion Module (IAFM) that selectively integrates identity and visual features
using a relevance-guided fusion mechanism. Experiments on four benchmarks show
that SELFI improves cross-manipulation generalization, outperforming prior
methods by an average of 3.1\% AUC. On the challenging DFDC dataset, SELFI
exceeds the previous best by 6\%. Code will be released upon paper acceptance.

</details>


### [163] [A Multimodal In Vitro Diagnostic Method for Parkinson's Disease Combining Facial Expressions and Behavioral Gait Data](https://arxiv.org/abs/2506.17596)
*Wei Huang,Yinxuan Xu,Yintao Zhou,Zhengyu Li,Jing Huang,Meng Pang*

Main category: cs.CV

TL;DR: This paper addresses challenges in Parkinson's disease (PD) diagnosis and proposes a multimodal in vitro approach using facial expressions and gait analysis with deep learning.


<details>
  <summary>Details</summary>
Motivation: Early and accurate detection of PD is critical due to its severe progression and impact, but current in vitro diagnosis methods face issues like limited data, equipment constraints, and single-modality risks.

Method: The paper introduces a multimodal diagnostic method using facial expressions and gait patterns, supported by deep learning for feature extraction and fusion, enabling mobile device deployment.

Result: The authors created the largest multimodal PD dataset in collaboration with a hospital and demonstrated improved diagnostic accuracy via extensive experiments.

Conclusion: The proposed multimodal approach enhances PD diagnosis accuracy, offering a cost-effective, deployable solution suitable for mobile devices to address existing diagnostic challenges.

Abstract: Parkinson's disease (PD), characterized by its incurable nature, rapid
progression, and severe disability, poses significant challenges to the lives
of patients and their families. Given the aging population, the need for early
detection of PD is increasing. In vitro diagnosis has garnered attention due to
its non-invasive nature and low cost. However, existing methods present several
challenges: 1) limited training data for facial expression diagnosis; 2)
specialized equipment and acquisition environments required for gait diagnosis,
resulting in poor generalizability; 3) the risk of misdiagnosis or missed
diagnosis when relying on a single modality. To address these issues, we
propose a novel multimodal in vitro diagnostic method for PD, leveraging facial
expressions and behavioral gait. Our method employs a lightweight deep learning
model for feature extraction and fusion, aimed at improving diagnostic accuracy
and facilitating deployment on mobile devices. Furthermore, we have established
the largest multimodal PD dataset in collaboration with a hospital and
conducted extensive experiments to validate the effectiveness of our proposed
method.

</details>


### [164] [OpenMAP-BrainAge: Generalizable and Interpretable Brain Age Predictor](https://arxiv.org/abs/2506.17597)
*Pengyu Kan,Craig Jones,Kenichi Oishi*

Main category: cs.CV

TL;DR: This paper presents a transformer-based model for brain age prediction using MRI scans that is interpretable, robust, and generalizable across datasets.


<details>
  <summary>Details</summary>
Motivation: To create an interpretable and robust model for predicting brain age that addresses demographic and technological variances in MRI scans and supports analysis of neurodegenerative disorders.

Method: Using a transformer-based architecture with a novel stem design to improve scalability, the model incorporates pseudo-3D MRI scans and brain volumetric data. It was trained on large North American datasets (ADNI2 & 3, OASIS3) with validation on an Australian dataset (AIBL).

Result: The model achieved a low mean absolute error (MAE) of 3.65 years on the test set and 3.54 years on the validation set, while also demonstrating an increase in brain age gap across cognitive groups and a significant correlation between BAG and cognitive scores.

Conclusion: The model achieved state-of-the-art performance, demonstrated robust generalizability, and identified key brain regions related to aging, offering potential for analyzing neurodegenerative disorders.

Abstract: Purpose: To develop an age prediction model which is interpretable and robust
to demographic and technological variances in brain MRI scans. Materials and
Methods: We propose a transformer-based architecture that leverages
self-supervised pre-training on large-scale datasets. Our model processes
pseudo-3D T1-weighted MRI scans from three anatomical views and incorporates
brain volumetric information. By introducing a stem architecture, we reduce the
conventional quadratic complexity of transformer models to linear complexity,
enabling scalability for high-dimensional MRI data. We trained our model on
ADNI2 $\&$ 3 (N=1348) and OASIS3 (N=716) datasets (age range: 42 - 95) from the
North America, with an 8:1:1 split for train, validation and test. Then, we
validated it on the AIBL dataset (N=768, age range: 60 - 92) from Australia.
Results: We achieved an MAE of 3.65 years on ADNI2 $\&$ 3 and OASIS3 test set
and a high generalizability of MAE of 3.54 years on AIBL. There was a notable
increase in brain age gap (BAG) across cognitive groups, with mean of 0.15
years (95% CI: [-0.22, 0.51]) in CN, 2.55 years ([2.40, 2.70]) in MCI, 6.12
years ([5.82, 6.43]) in AD. Additionally, significant negative correlation
between BAG and cognitive scores was observed, with correlation coefficient of
-0.185 (p < 0.001) for MoCA and -0.231 (p < 0.001) for MMSE. Gradient-based
feature attribution highlighted ventricles and white matter structures as key
regions influenced by brain aging. Conclusion: Our model effectively fused
information from different views and volumetric information to achieve
state-of-the-art brain age prediction accuracy, improved generalizability and
interpretability with association to neurodegenerative disorders.

</details>


### [165] [HIRE: Lightweight High-Resolution Image Feature Enrichment for Multimodal LLMs](https://arxiv.org/abs/2506.17608)
*Nikitha SR,Aradhya Neeraj Mathur,Tarun Ram Menta,Rishabh Jain,Mausoom Sarkar*

Main category: cs.CV

TL;DR: The paper proposes a shallow feature enricher for multimodal large language models to reduce computational costs while maintaining high performance.


<details>
  <summary>Details</summary>
Motivation: High-resolution image features in multimodal models improve visual understanding but are computationally expensive.

Method: The authors introduced a feature upsampling approach using a shallow feature enricher.

Result: Achieved competitive results with up to 1.5x savings in computational cost (FLOPs) and reduced training/inference times.

Conclusion: Feature upsampling with a shallow model offers a cost-effective and efficient solution for maintaining performance in multimodal tasks.

Abstract: The integration of high-resolution image features in modern multimodal large
language models has demonstrated significant improvements in fine-grained
visual understanding tasks, achieving high performance across multiple
benchmarks. Since these features are obtained from large image encoders like
ViT, they come with a significant increase in computational costs due to
multiple calls to these encoders. In this work, we first develop an intuition
for feature upsampling as a natural extension of high-resolution feature
generation. Through extensive experiments and ablations, we demonstrate how a
shallow feature enricher can achieve competitive results with tremendous
reductions in training and inference time as well as computational cost, with
upto 1.5x saving in FLOPs.

</details>


### [166] [JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo Retouching Agent](https://arxiv.org/abs/2506.17612)
*Yunlong Lin,Zixu Lin,Kunjie Lin,Jinbin Bai,Panwang Pan,Chenxin Li,Haoyu Chen,Zhongdao Wang,Xinghao Ding,Wenbo Li,Shuicheng Yan*

Main category: cs.CV

TL;DR: The paper introduces JarvisArt, an AI-driven tool for photo retouching within Adobe Lightroom, overcoming limitations of existing methods by offering detailed control, professional-level reasoning, and enhanced tool coordination.


<details>
  <summary>Details</summary>
Motivation: Existing AI-based photo retouching solutions lack adjustability and fail to generalize effectively, limiting their ability to meet personalized editing needs.

Method: JarvisArt employs a multi-modal large language model with two-stage training: Chain-of-Thought supervised fine-tuning for reasoning and tool-use basics, followed by Group Relative Policy Optimization for Retouching (GRPO-R) for enhanced decision-making. It also uses a custom Agent-to-Lightroom Protocol for integration.

Result: JarvisArt proves effective, offering user-friendly interaction, superior generalization, and precise control over adjustments. It excels in performance, improving average pixel-level metrics by 60% over GPT-4o on the MMArt-Bench benchmark.

Conclusion: JarvisArt achieves personalized and professional photo editing by fusing AI-driven automation with extensive tool adjustability and reasoning skills, setting a new standard for intelligent photo retouching.

Abstract: Photo retouching has become integral to contemporary visual storytelling,
enabling users to capture aesthetics and express creativity. While professional
tools such as Adobe Lightroom offer powerful capabilities, they demand
substantial expertise and manual effort. In contrast, existing AI-based
solutions provide automation but often suffer from limited adjustability and
poor generalization, failing to meet diverse and personalized editing needs. To
bridge this gap, we introduce JarvisArt, a multi-modal large language model
(MLLM)-driven agent that understands user intent, mimics the reasoning process
of professional artists, and intelligently coordinates over 200 retouching
tools within Lightroom. JarvisArt undergoes a two-stage training process: an
initial Chain-of-Thought supervised fine-tuning to establish basic reasoning
and tool-use skills, followed by Group Relative Policy Optimization for
Retouching (GRPO-R) to further enhance its decision-making and tool
proficiency. We also propose the Agent-to-Lightroom Protocol to facilitate
seamless integration with Lightroom. To evaluate performance, we develop
MMArt-Bench, a novel benchmark constructed from real-world user edits.
JarvisArt demonstrates user-friendly interaction, superior generalization, and
fine-grained control over both global and local adjustments, paving a new
avenue for intelligent photo retouching. Notably, it outperforms GPT-4o with a
60% improvement in average pixel-level metrics on MMArt-Bench for content
fidelity, while maintaining comparable instruction-following capabilities.
Project Page: https://jarvisart.vercel.app/.

</details>


### [167] [CLiViS: Unleashing Cognitive Map through Linguistic-Visual Synergy for Embodied Visual Reasoning](https://arxiv.org/abs/2506.17629)
*Kailing Li,Qi'ao Xu,Tianwen Qian,Yuqian Fu,Yang Jiao,Xiaoling Wang*

Main category: cs.CV

TL;DR: The paper introduces CLiViS, a training-free framework that combines LLMs for task planning and VLMs for visual perception to address challenges in Embodied Visual Reasoning (EVR).


<details>
  <summary>Details</summary>
Motivation: Embodied Visual Reasoning (EVR) faces difficulties with complex instructions and dynamic spatiotemporal visual data. Existing methods inadequately combine reasoning and perception to address these challenges.

Method: The proposed CLiViS framework uses LLMs for high-level reasoning and VLMs for perception. It builds a dynamic Cognitive Map that iteratively updates scene representation during the reasoning process without requiring additional training.

Result: Experiments on various benchmarks show that CLiViS effectively handles long-term visual dependencies and is generalizable across tasks.

Conclusion: By synergizing LLM reasoning and VLM perception with a novel Cognitive Map, CLiViS significantly improves EVR performance and understanding in complex, dynamic environments.

Abstract: Embodied Visual Reasoning (EVR) seeks to follow complex, free-form
instructions based on egocentric video, enabling semantic understanding and
spatiotemporal reasoning in dynamic environments. Despite its promising
potential, EVR encounters significant challenges stemming from the diversity of
complex instructions and the intricate spatiotemporal dynamics in long-term
egocentric videos. Prior solutions either employ Large Language Models (LLMs)
over static video captions, which often omit critical visual details, or rely
on end-to-end Vision-Language Models (VLMs) that struggle with stepwise
compositional reasoning. Consider the complementary strengths of LLMs in
reasoning and VLMs in perception, we propose CLiViS. It is a novel
training-free framework that leverages LLMs for high-level task planning and
orchestrates VLM-driven open-world visual perception to iteratively update the
scene context. Building on this synergy, the core of CLiViS is a dynamic
Cognitive Map that evolves throughout the reasoning process. This map
constructs a structured representation of the embodied scene, bridging
low-level perception and high-level reasoning. Extensive experiments across
multiple benchmarks demonstrate the effectiveness and generality of CLiViS,
especially in handling long-term visual dependencies. Code is available at
https://github.com/Teacher-Tom/CLiViS.

</details>


### [168] [Optimization-Free Patch Attack on Stereo Depth Estimation](https://arxiv.org/abs/2506.17632)
*Hangcheng Liu,Xu Kuang,Xingshuo Han,Xingwan Wu,Haoran Ou,Shangwei Guo,Xingyi Huang,Tao Xiang,Tianwei Zhang*

Main category: cs.CV

TL;DR: The paper addresses vulnerabilities in stereo depth estimation (SDE) models to adversarial attacks and introduces PatchHunter, a novel, physically realizable, and transferable attack method.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance understanding of the vulnerabilities in SDE models under realistic physical constraints, as current adversarial attacks on these models are limited to unrealistic scenarios.

Method: The authors propose a two-fold approach: (1) a unified optimization-based attack framework targeting four stages of stereo matching and (2) PatchHunter, a reinforcement learning-driven, optimization-free method to generate transferable adversarial patches.

Result: PatchHunter demonstrates superior effectiveness and black-box transferability compared to optimization-based techniques, validated through tests on datasets, simulation environments, and real-world conditions.

Conclusion: This study highlights the limitations of existing adversarial methods for SDE and provides a practical solution through PatchHunter, showing its effectiveness under real-world constraints.

Abstract: Stereo Depth Estimation (SDE) is essential for scene understanding in
vision-based systems like autonomous driving. However, recent studies show that
SDE models are vulnerable to adversarial attacks, which are often limited to
unrealistic settings, e.g., digital perturbations on separate stereo views in
static scenes, restricting their real-world applicability. This raises a
critical question: how can we design physically realizable, scene-adaptive, and
transferable attacks against SDE under realistic constraints?
  To answer this, we make two key contributions. First, we propose a unified
attack framework that extends optimization-based techniques to four core stages
of stereo matching: feature extraction, cost-volume construction, cost
aggregation, and disparity regression. A comprehensive stage-wise evaluation
across 9 mainstream SDE models, under constraints like photometric consistency,
reveals that optimization-based patches suffer from poor transferability.
Interestingly, partially transferable patches suggest that patterns, rather
than pixel-level perturbations, may be key to generalizable attacks. Motivated
by this, we present PatchHunter, the first optimization-free adversarial patch
attack against SDE. PatchHunter formulates patch generation as a reinforcement
learning-driven search over a structured space of visual patterns crafted to
disrupt SDE assumptions.
  We validate PatchHunter across three levels: the KITTI dataset, the CARLA
simulator, and real-world vehicle deployment. PatchHunter not only surpasses
optimization-based methods in effectiveness but also achieves significantly
better black-box transferability. Even under challenging physical conditions
like low light, PatchHunter maintains high attack success (e.g., D1-all > 0.4),
whereas optimization-based methods fail.

</details>


### [169] [Adaptive Multi-prompt Contrastive Network for Few-shot Out-of-distribution Detection](https://arxiv.org/abs/2506.17633)
*Xiang Fang,Arvind Easwaran,Blaise Genest*

Main category: cs.CV

TL;DR: The paper introduces a method, AMCN, to tackle the problem of few-shot OOD detection, achieving superior results by employing adaptive prompts and leveraging CLIP.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of OOD detection in scenarios where only a few labeled in-distribution (ID) samples are available, a setting that limits existing methods reliant on many IID samples.

Method: The proposed method, Adaptive Multi-prompt Contrastive Network (AMCN), uses CLIP to bridge text and images, creating adaptive learnable textual prompts (ID and OOD). It also learns inter- and intra-class distributions, applying class-specific thresholds and a prompt-guided ID-OOD separation module to improve OOD detection.

Result: The AMCN demonstrates superior performance compared to state-of-the-art methods, effectively handling OOD detection in few-shot scenarios.

Conclusion: AMCN successfully adapts to few-shot OOD detection challenges by introducing innovative prompt generation and ID-OOD separation techniques, offering a significant advancement over traditional methods.

Abstract: Out-of-distribution (OOD) detection attempts to distinguish outlier samples
to prevent models trained on the in-distribution (ID) dataset from producing
unavailable outputs. Most OOD detection methods require many IID samples for
training, which seriously limits their real-world applications. To this end, we
target a challenging setting: few-shot OOD detection, where {Only a few {\em
labeled ID} samples are available.} Therefore, few-shot OOD detection is much
more challenging than the traditional OOD detection setting. Previous few-shot
OOD detection works ignore the distinct diversity between different classes. In
this paper, we propose a novel network: Adaptive Multi-prompt Contrastive
Network (AMCN), which adapts the ID-OOD separation boundary by learning inter-
and intra-class distribution. To compensate for the absence of OOD and scarcity
of ID {\em image samples}, we leverage CLIP, connecting text with images,
engineering learnable ID and OOD {\em textual prompts}. Specifically, we first
generate adaptive prompts (learnable ID prompts, label-fixed OOD prompts and
label-adaptive OOD prompts). Then, we generate an adaptive class boundary for
each class by introducing a class-wise threshold. Finally, we propose a
prompt-guided ID-OOD separation module to control the margin between ID and OOD
prompts. Experimental results show that AMCN outperforms other state-of-the-art
works.

</details>


### [170] [Histopathology Image Report Generation by Vision Language Model with Multimodal In-Context Learning](https://arxiv.org/abs/2506.17645)
*Shih-Wen Liu,Hsuan-Yu Fan,Wei-Ta Chu,Fu-En Yang,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: The paper introduces PathGenIC, a framework using multimodal in-context learning to generate medical reports from histopathology images, achieving state-of-the-art results on several evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: Generate accurate and AI-driven histopathology medical reports using effective visual representation and domain-specific context.

Method: A multimodal in-context learning approach that dynamically retrieves relevant WSI-report pairs and utilizes adaptive feedback for improved report generation.

Result: PathGenIC achieved significant performance improvements on BLEU, METEOR, and ROUGE-L metrics, showing robustness across report lengths and disease categories.

Conclusion: The framework bridges vision and language, offering a promising solution for automated histopathology reporting and advancing multimodal clinical applications.

Abstract: Automating medical report generation from histopathology images is a critical
challenge requiring effective visual representations and domain-specific
knowledge. Inspired by the common practices of human experts, we propose an
in-context learning framework called PathGenIC that integrates context derived
from the training set with a multimodal in-context learning (ICL) mechanism.
Our method dynamically retrieves semantically similar whole slide image
(WSI)-report pairs and incorporates adaptive feedback to enhance contextual
relevance and generation quality. Evaluated on the HistGen benchmark, the
framework achieves state-of-the-art results, with significant improvements
across BLEU, METEOR, and ROUGE-L metrics, and demonstrates robustness across
diverse report lengths and disease categories. By maximizing training data
utility and bridging vision and language with ICL, our work offers a solution
for AI-driven histopathology reporting, setting a strong foundation for future
advancements in multimodal clinical applications.

</details>


### [171] [MDSAM:Memory-Driven Sparse Attention Matrix for LVLMs Hallucination Mitigation](https://arxiv.org/abs/2506.17664)
*Shuaiye Lu,Linjiang Zhou,Xiaochuan Shi*

Main category: cs.CV

TL;DR: This paper introduces MDSAM, a training-free method to dynamically refine attention on image tokens in LVLMs, effectively reducing hallucinations and improving task performance.


<details>
  <summary>Details</summary>
Motivation: Large vision-language models often hallucinate due to errors in attention allocation to image tokens during decoding, necessitating a method to enhance focus and reduce these issues.

Method: The authors propose MDSAM, a mechanism that memorizes and dynamically adjusts attention patterns on image tokens across layers during decoding, focusing on aligning relevant features and minimizing hallucinations.

Result: MDSAM improves reliability and reduces hallucination rates across various LVLM benchmarks in tasks like image captioning and visual question answering.

Conclusion: The study demonstrates that MDSAM is an adaptable and effective solution for mitigating hallucinations in LVLMs without additional training or external resources.

Abstract: Hallucinations in large vision-language models (LVLMs) often stem from the
model's sensitivity to image tokens during decoding, as evidenced by attention
peaks observed when generating both real and hallucinated entities. To address
this, we propose Memory-Driven Sparse Attention Matrix (MDSAM) , a novel
training-free approach that dynamically captures and refines the attention
allocated to image tokens at each layer. MDSAM memorizes attention patterns and
activates updates through alignment during decoding, enhancing focus on
relevant image tokens while effectively reducing hallucinations. We evaluate
MDSAM on multiple benchmarks for tasks such as image captioning and visual
question answering, demonstrating its ability to consistently reduce
hallucinations and improve reliability. Compatible with various LVLM
architectures, MDSAM highlights its adaptability and effectiveness in
mitigating hallucinations without requiring additional training or external
tools.

</details>


### [172] [CSDN: A Context-Gated Self-Adaptive Detection Network for Real-Time Object Detection](https://arxiv.org/abs/2506.17679)
*Wei Haolin*

Main category: cs.CV

TL;DR: The paper introduces the Context-Gated Scale-Adaptive Detection Network (CSDN), a Transformer-based detection header to improve global context modeling and object size adaptability in CNNs.


<details>
  <summary>Details</summary>
Motivation: Limitations in CNNs' receptive fields hinder global contextual information capture, and feature utilization is equally critical.

Method: CSDN integrates a novel gating mechanism replacing traditional self-attention layers for adaptive ROI feature selection and contextual modeling.

Result: Replacing native CNN-based detectors' heads with CSDN shows notable improvement in detection accuracy with minimal fine-tuning iterations.

Conclusion: CSDN enhances CNN-based detectors' efficiency and accuracy without extensive retraining, leveraging better feature utilization and contextual adaptability.

Abstract: Convolutional neural networks (CNNs) have long been the cornerstone of target
detection, but they are often limited by limited receptive fields, which
hinders their ability to capture global contextual information. This paper
believes that the effective utilization of extracted features is as important
as the feature extraction process itself. We critically re-evaluated the
DETR-inspired header network architecture, questioning the indispensable nature
of its self-attention mechanism, and discovering significant information
redundancies. To solve these problems, we introduced the Context-Gated
Scale-Adaptive Detection Network (CSDN), a Transformer-based detection header
inspired by natural language processing architecture and human visual
perception. CSDN aims to efficiently utilize the characteristics of the CNN
backbone network by replacing the traditional stacked self-attention and
cross-attention layers with a novel gating mechanism. This mechanism enables
each region of interest (ROI) to adaptively select and combine feature
dimensions and scale information from multiple attention patterns. CSDN
provides more powerful global context modeling capabilities and can better
adapt to objects of different sizes and structures. Our proposed detection head
can directly replace the native heads of various CNN-based detectors, and only
a few rounds of fine-tuning on the pre-training weights can significantly
improve the detection accuracy, thus avoiding the need to achieve small
improvements. Various layer modules undergo extensive re-training.

</details>


### [173] [Domain Generalization using Action Sequences for Egocentric Action Recognition](https://arxiv.org/abs/2506.17685)
*Amirshayan Nasirimajd,Chiara Plizzari,Simone Alberto Peirone,Marco Ciccone,Giuseppe Averta,Barbara Caputo*

Main category: cs.CV

TL;DR: The paper introduces SeqDG, a domain generalization approach for recognizing egocentric human actions using action sequences, achieving measurable improvements on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Egocentric Action Recognition models perform poorly in unseen environments due to variability in illumination, viewpoint, and environment. Enhancing generalization capability is crucial for better performance.

Method: The SeqDG approach uses a visual-text sequence reconstruction objective (SeqRec) and mixed-action sequences training (SeqMix) to improve the model's cross-domain generalization abilities.

Result: SeqDG showed a +2.4% improvement on EPIC-KITCHENS-100 for cross-domain recognition and a +0.6% Top-1 accuracy improvement on EGTEA compared to the state-of-the-art.

Conclusion: SeqDG significantly enhances the robustness and generalization of Egocentric Action Recognition models by focusing on action sequences and leveraging text-visual context cues.

Abstract: Recognizing human activities from visual inputs, particularly through a
first-person viewpoint, is essential for enabling robots to replicate human
behavior. Egocentric vision, characterized by cameras worn by observers,
captures diverse changes in illumination, viewpoint, and environment. This
variability leads to a notable drop in the performance of Egocentric Action
Recognition models when tested in environments not seen during training. In
this paper, we tackle these challenges by proposing a domain generalization
approach for Egocentric Action Recognition. Our insight is that action
sequences often reflect consistent user intent across visual domains. By
leveraging action sequences, we aim to enhance the model's generalization
ability across unseen environments. Our proposed method, named SeqDG,
introduces a visual-text sequence reconstruction objective (SeqRec) that uses
contextual cues from both text and visual inputs to reconstruct the central
action of the sequence. Additionally, we enhance the model's robustness by
training it on mixed sequences of actions from different domains (SeqMix). We
validate SeqDG on the EGTEA and EPIC-KITCHENS-100 datasets. Results on
EPIC-KITCHENS-100, show that SeqDG leads to +2.4% relative average improvement
in cross-domain action recognition in unseen environments, and on EGTEA the
model achieved +0.6% Top-1 accuracy over SOTA in intra-domain action
recognition.

</details>


### [174] [SSAVSV: Towards Unified Model for Self-Supervised Audio-Visual Speaker Verification](https://arxiv.org/abs/2506.17694)
*Gnana Praveen Rajasekhar,Jahangir Alam*

Main category: cs.CV

TL;DR: The paper proposes a self-supervised learning framework for audio-visual speaker verification to reduce reliance on labeled data and computational costs.


<details>
  <summary>Details</summary>
Motivation: Conventional methods require large labeled datasets and separate architectures for audio and visual inputs, making them costly and less scalable.

Method: The study employs contrastive learning with asymmetric masking and masked data modeling within a unified framework using a shared vision transformer backbone.

Result: The proposed method delivers competitive performance without requiring labeled data and is computationally efficient compared to traditional approaches, supporting single or combined modalities.

Conclusion: The unified framework advances speaker verification by improving computational efficiency, reducing dependency on labeled datasets, and handling missing modalities effectively.

Abstract: Conventional audio-visual methods for speaker verification rely on large
amounts of labeled data and separate modality-specific architectures, which is
computationally expensive, limiting their scalability. To address these
problems, we propose a self-supervised learning framework based on contrastive
learning with asymmetric masking and masked data modeling to obtain robust
audiovisual feature representations. In particular, we employ a unified
framework for self-supervised audiovisual speaker verification using a single
shared backbone for audio and visual inputs, leveraging the versatility of
vision transformers. The proposed unified framework can handle audio, visual,
or audiovisual inputs using a single shared vision transformer backbone during
training and testing while being computationally efficient and robust to
missing modalities. Extensive experiments demonstrate that our method achieves
competitive performance without labeled data while reducing computational costs
compared to traditional approaches.

</details>


### [175] [DreamJourney: Perpetual View Generation with Video Diffusion Models](https://arxiv.org/abs/2506.17705)
*Bo Pan,Yang Chen,Yingwei Pan,Ting Yao,Wei Chen,Tao Mei*

Main category: cs.CV

TL;DR: DreamJourney is a two-stage framework for generating long-term dynamic scene videos with both camera movements and object dynamics from a single input image, addressing limitations in previous methods.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of 2D diffusion models which lack 3D awareness and fail to account for object dynamics, providing a solution for generating dynamic scenes in a 4D world.

Method: The DreamJourney framework consists of two stages: Stage I lifts the input image to a 3D point cloud and uses video diffusion models to generate cross-view consistent videos. Strategies like early stopping and view padding are used for better quality. Stage II integrates large language models for animating object movements and incorporates video diffusion to simulate dynamics.

Result: DreamJourney produces visually coherent, dynamic scene view videos with better quantitative and qualitative performance compared to state-of-the-art methods.

Conclusion: DreamJourney addresses both static 3D scene generation and dynamic 4D object movements, offering a more robust and visually satisfying solution for perpetual view generation tasks.

Abstract: Perpetual view generation aims to synthesize a long-term video corresponding
to an arbitrary camera trajectory solely from a single input image. Recent
methods commonly utilize a pre-trained text-to-image diffusion model to
synthesize new content of previously unseen regions along camera movement.
However, the underlying 2D diffusion model lacks 3D awareness and results in
distorted artifacts. Moreover, they are limited to generating views of static
3D scenes, neglecting to capture object movements within the dynamic 4D world.
To alleviate these issues, we present DreamJourney, a two-stage framework that
leverages the world simulation capacity of video diffusion models to trigger a
new perpetual scene view generation task with both camera movements and object
dynamics. Specifically, in stage I, DreamJourney first lifts the input image to
3D point cloud and renders a sequence of partial images from a specific camera
trajectory. A video diffusion model is then utilized as generative prior to
complete the missing regions and enhance visual coherence across the sequence,
producing a cross-view consistent video adheres to the 3D scene and camera
trajectory. Meanwhile, we introduce two simple yet effective strategies (early
stopping and view padding) to further stabilize the generation process and
improve visual quality. Next, in stage II, DreamJourney leverages a multimodal
large language model to produce a text prompt describing object movements in
current view, and uses video diffusion model to animate current view with
object movements. Stage I and II are repeated recurrently, enabling perpetual
dynamic scene view generation. Extensive experiments demonstrate the
superiority of our DreamJourney over state-of-the-art methods both
quantitatively and qualitatively. Our project page:
https://dream-journey.vercel.app.

</details>


### [176] [Programmable-Room: Interactive Textured 3D Room Meshes Generation Empowered by Large Language Models](https://arxiv.org/abs/2506.17707)
*Jihyun Kim,Junho Park,Kyeongbo Kong,Suk-Ju Kang*

Main category: cs.CV

TL;DR: Programmable-Room framework creates and edits 3D room meshes using natural language instructions by decomposing the processes into simpler steps, supported through visual programming powered by a large language model.


<details>
  <summary>Details</summary>
Motivation: The paper aims to simplify precise 3D room mesh generation and editing using natural language by breaking down complex tasks and leveraging large language models.

Method: Decomposition of tasks into smaller steps (coordinate creation, texture generation, mesh construction, furniture arrangement) along with visual programming using a Python-like script developed by large language models.

Result: Flexible 3D room mesh generation and editing is achieved, with quantitative and qualitative improvement over existing models.

Conclusion: The Programmable-Room framework successfully demonstrates its capability to enhance the generation and editing of 3D room meshes efficiently through natural language-driven visual programming.

Abstract: We present Programmable-Room, a framework which interactively generates and
edits a 3D room mesh, given natural language instructions. For precise control
of a room's each attribute, we decompose the challenging task into simpler
steps such as creating plausible 3D coordinates for room meshes, generating
panorama images for the texture, constructing 3D meshes by integrating the
coordinates and panorama texture images, and arranging furniture. To support
the various decomposed tasks with a unified framework, we incorporate visual
programming (VP). VP is a method that utilizes a large language model (LLM) to
write a Python-like program which is an ordered list of necessary modules for
the various tasks given in natural language. We develop most of the modules.
Especially, for the texture generating module, we utilize a pretrained
large-scale diffusion model to generate panorama images conditioned on text and
visual prompts (i.e., layout, depth, and semantic map) simultaneously.
Specifically, we enhance the panorama image generation quality by optimizing
the training objective with a 1D representation of a panorama scene obtained
from bidirectional LSTM. We demonstrate Programmable-Room's flexibility in
generating and editing 3D room meshes, and prove our framework's superiority to
an existing model quantitatively and qualitatively. Project page is available
in https://jihyun0510.github.io/Programmable_Room_Page/.

</details>


### [177] [PDC-Net: Pattern Divide-and-Conquer Network for Pelvic Radiation Injury Segmentation](https://arxiv.org/abs/2506.17712)
*Xinyu Xiong,Wuteng Cao,Zihuang Wu,Lei Zhang,Chong Gao,Guanbin Li,Qiyuan Qin*

Main category: cs.CV

TL;DR: The paper introduces PDC-Net, an advanced network to enhance automated segmentation of pelvic radiation injury from MRI scans using novel modules like Multi-Direction Aggregation and Memory-Guided Context.


<details>
  <summary>Details</summary>
Motivation: Accurate segmentation of Pelvic Radiation Injury from MRI is critical for effective prognosis assessment and creating personalized treatment plans.

Method: The authors propose PDC-Net, which uses separate modules to address local and global patterns, incorporating Multi-Direction Aggregation for shape-specific fitting, a Memory-Guided Context module for improved pattern distinction, and an Adaptive Fusion Decoder for feature selection.

Result: The method is validated on the first large-scale dataset for pelvic radiation injury, showing improved results over existing segmentation techniques.

Conclusion: PDC-Net effectively addresses challenges in PRI segmentation, proving superior accuracy and setting a benchmark for future automated approaches.

Abstract: Accurate segmentation of Pelvic Radiation Injury (PRI) from Magnetic
Resonance Images (MRI) is crucial for more precise prognosis assessment and the
development of personalized treatment plans. However, automated segmentation
remains challenging due to factors such as complex organ morphologies and
confusing context. To address these challenges, we propose a novel Pattern
Divide-and-Conquer Network (PDC-Net) for PRI segmentation. The core idea is to
use different network modules to "divide" various local and global patterns
and, through flexible feature selection, to "conquer" the Regions of Interest
(ROI) during the decoding phase. Specifically, considering that our ROI often
manifests as strip-like or circular-like structures in MR slices, we introduce
a Multi-Direction Aggregation (MDA) module. This module enhances the model's
ability to fit the shape of the organ by applying strip convolutions in four
distinct directions. Additionally, to mitigate the challenge of confusing
context, we propose a Memory-Guided Context (MGC) module. This module
explicitly maintains a memory parameter to track cross-image patterns at the
dataset level, thereby enhancing the distinction between global patterns
associated with the positive and negative classes. Finally, we design an
Adaptive Fusion Decoder (AFD) that dynamically selects features from different
patterns based on the Mixture-of-Experts (MoE) framework, ultimately generating
the final segmentation results. We evaluate our method on the first large-scale
pelvic radiation injury dataset, and the results demonstrate the superiority of
our PDC-Net over existing approaches.

</details>


### [178] [YOLOv13: Real-Time Object Detection with Hypergraph-Enhanced Adaptive Visual Perception](https://arxiv.org/abs/2506.17733)
*Mengqi Lei,Siqi Li,Yihong Wu,Han Hu,You Zhou,Xinhu Zheng,Guiguang Ding,Shaoyi Du,Zongze Wu,Yue Gao*

Main category: cs.CV

TL;DR: YOLOv13 introduces significant advancements in object detection, including the HyperACE mechanism and FullPAD paradigm, achieving state-of-the-art accuracy and efficiency in complex scenarios.


<details>
  <summary>Details</summary>
Motivation: To address YOLO series models' limitations in capturing global high-order correlations, hindering their detection performance in complex environments.

Method: Introduced HyperACE for high-order correlation exploitation, FullPAD paradigm for fine-grained information distribution, and depthwise separable convolutions for parameter and computational reduction.

Result: YOLOv13 achieves a 3.0% mAP improvement over YOLOv11-N and a 1.5% increase over YOLOv12-N on the MS COCO benchmark, with fewer parameters and FLOPs.

Conclusion: YOLOv13 establishes itself as a lightweight and accurate object detector, outperforming previous versions through advanced mechanisms and efficient computational design.

Abstract: The YOLO series models reign supreme in real-time object detection due to
their superior accuracy and computational efficiency. However, both the
convolutional architectures of YOLO11 and earlier versions and the area-based
self-attention mechanism introduced in YOLOv12 are limited to local information
aggregation and pairwise correlation modeling, lacking the capability to
capture global multi-to-multi high-order correlations, which limits detection
performance in complex scenarios. In this paper, we propose YOLOv13, an
accurate and lightweight object detector. To address the above-mentioned
challenges, we propose a Hypergraph-based Adaptive Correlation Enhancement
(HyperACE) mechanism that adaptively exploits latent high-order correlations
and overcomes the limitation of previous methods that are restricted to
pairwise correlation modeling based on hypergraph computation, achieving
efficient global cross-location and cross-scale feature fusion and enhancement.
Subsequently, we propose a Full-Pipeline Aggregation-and-Distribution (FullPAD)
paradigm based on HyperACE, which effectively achieves fine-grained information
flow and representation synergy within the entire network by distributing
correlation-enhanced features to the full pipeline. Finally, we propose to
leverage depthwise separable convolutions to replace vanilla large-kernel
convolutions, and design a series of blocks that significantly reduce
parameters and computational complexity without sacrificing performance. We
conduct extensive experiments on the widely used MS COCO benchmark, and the
experimental results demonstrate that our method achieves state-of-the-art
performance with fewer parameters and FLOPs. Specifically, our YOLOv13-N
improves mAP by 3.0\% over YOLO11-N and by 1.5\% over YOLOv12-N. The code and
models of our YOLOv13 model are available at:
https://github.com/iMoonLab/yolov13.

</details>


### [179] [PhysID: Physics-based Interactive Dynamics from a Single-view Image](https://arxiv.org/abs/2506.17746)
*Sourabh Vasant Gothe,Ayon Chattopadhyay,Gunturi Venkata Sai Phani Kiran,Pratik,Vibhav Agarwal,Jayesh Rajkumar Vachhani,Sourav Ghosh,Parameswaranath VM,Barath Raj KR*

Main category: cs.CV

TL;DR: This paper introduces PhysID, a framework to enable physics-based interactive dynamics from a single-view image using generative models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance mobile user experiences in AR/VR and interactive applications by overcoming the limitations of requiring pre-recorded video or multi-view images.

Method: PhysID leverages large generative models for 3D mesh generation and physical property prediction, combined with an on-device physics engine for real-time rendering.

Result: The framework demonstrated effectiveness through experiments evaluating zero-shot capabilities of Multimodal Large Language Models and the performance of 3D reconstruction models.

Conclusion: PhysID advances mobile-based interactive dynamics, enabling scalable and user-personalized interactions with minimal expertise and efficient memory usage.

Abstract: Transforming static images into interactive experiences remains a challenging
task in computer vision. Tackling this challenge holds the potential to elevate
mobile user experiences, notably through interactive and AR/VR applications.
Current approaches aim to achieve this either using pre-recorded video
responses or requiring multi-view images as input. In this paper, we present
PhysID, that streamlines the creation of physics-based interactive dynamics
from a single-view image by leveraging large generative models for 3D mesh
generation and physical property prediction. This significantly reduces the
expertise required for engineering-intensive tasks like 3D modeling and
intrinsic property calibration, enabling the process to be scaled with minimal
manual intervention. We integrate an on-device physics-based engine for
physically plausible real-time rendering with user interactions. PhysID
represents a leap forward in mobile-based interactive dynamics, offering
real-time, non-deterministic interactions and user-personalization with
efficient on-device memory consumption. Experiments evaluate the zero-shot
capabilities of various Multimodal Large Language Models (MLLMs) on diverse
tasks and the performance of 3D reconstruction models. These results
demonstrate the cohesive functioning of all modules within the end-to-end
framework, contributing to its effectiveness.

</details>


### [180] [LoLA-SpecViT: Local Attention SwiGLU Vision Transformer with LoRA for Hyperspectral Imaging](https://arxiv.org/abs/2506.17759)
*Fadi Abdeladhim Zidi,Djamel Eddine Boukhari,Abdellah Zakaria Sellam,Abdelkrim Ouafi,Cosimo Distante,Salah Eddine Bekhouche,Abdelmalik Taleb-Ahmed*

Main category: cs.CV

TL;DR: This paper introduces LoLA-SpecViT, a lightweight spectral vision transformer tailored for hyperspectral image classification, aiming to improve accuracy and robustness with fewer parameters and under label-scarce conditions.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral image classification faces challenges such as high spectral dimensionality, significant inter-band redundancy, and limited annotated samples. Existing models struggle with scalability and adaptability under label-scarce scenarios.

Method: LoLA-SpecViT combines a 3D convolutional spectral front-end with local window-based self-attention for efficient feature extraction. It includes low-rank adaptation (LoRA) in attention and projection layers to enable fine-tuning with reduced trainable parameters and a cyclical learning rate scheduler for improved training convergence and generalization.

Result: Experiments on three benchmark datasets show LoLA-SpecViT outperforms state-of-the-art models, achieving accuracy up to 99.91% with reduced parameter count and improved robustness under limited-label conditions.

Conclusion: LoLA-SpecViT provides a scalable, efficient, and generalizable framework for hyperspectral image classification, with applications in agriculture, environmental monitoring, and remote sensing analytics.

Abstract: Hyperspectral image classification remains a challenging task due to the high
dimensionality of spectral data, significant inter-band redundancy, and the
limited availability of annotated samples. While recent transformer-based
models have improved the global modeling of spectral-spatial dependencies,
their scalability and adaptability under label-scarce conditions remain
limited. In this work, we propose \textbf{LoLA-SpecViT}(Low-rank adaptation
Local Attention Spectral Vision Transformer), a lightweight spectral vision
transformer that addresses these limitations through a parameter-efficient
architecture tailored to the unique characteristics of hyperspectral imagery.
Our model combines a 3D convolutional spectral front-end with local
window-based self-attention, enhancing both spectral feature extraction and
spatial consistency while reducing computational complexity. To further improve
adaptability, we integrate low-rank adaptation (LoRA) into attention and
projection layers, enabling fine-tuning with over 80\% fewer trainable
parameters. A novel cyclical learning rate scheduler modulates LoRA adaptation
strength during training, improving convergence and generalisation. Extensive
experiments on three benchmark datasets WHU-Hi LongKou, WHU-Hi HongHu, and
Salinas demonstrate that LoLA-SpecViT consistently outperforms state-of-the-art
baselines, achieving up to 99.91\% accuracy with substantially fewer parameters
and enhanced robustness under low-label regimes. The proposed framework
provides a scalable and generalizable solution for real-world HSI applications
in agriculture, environmental monitoring, and remote sensing analytics. Our
code is available in the following
\href{https://github.com/FadiZidiDz/LoLA-SpecViT}{GitHub Repository}.

</details>


### [181] [Incorporating Rather Than Eliminating: Achieving Fairness for Skin Disease Diagnosis Through Group-Specific Expert](https://arxiv.org/abs/2506.17787)
*Gelei Xu,Yuying Duan,Zheyuan Liu,Xueyang Li,Meng Jiang,Michael Lemmon,Wei Jin,Yiyu Shi*

Main category: cs.CV

TL;DR: The paper introduces a new AI framework, FairMoE, to improve fairness in skin disease diagnostics without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional diagnostic AI systems tend to be biased across different demographic groups, leading to less fair healthcare results. Existing bias mitigation methods often fail to balance fairness and clinical relevance.

Method: FairMoE uses layer-wise mixture-of-experts modules as group-specific learners and dynamically assigns data to the most appropriate expert, addressing complex cases near group boundaries.

Result: FairMoE achieves significant accuracy improvements while maintaining fairness metrics, outperforming prior fairness methods that degrade performance.

Conclusion: Incorporating sensitive attributes and improving routing dynamics via FairMoE provides a promising pathway to equitable and accurate medical diagnostics.

Abstract: AI-based systems have achieved high accuracy in skin disease diagnostics but
often exhibit biases across demographic groups, leading to inequitable
healthcare outcomes and diminished patient trust. Most existing bias mitigation
methods attempt to eliminate the correlation between sensitive attributes and
diagnostic prediction, but those methods often degrade performance due to the
lost of clinically relevant diagnostic cues. In this work, we propose an
alternative approach that incorporates sensitive attributes to achieve
fairness. We introduce FairMoE, a framework that employs layer-wise
mixture-of-experts modules to serve as group-specific learners. Unlike
traditional methods that rigidly assign data based on group labels, FairMoE
dynamically routes data to the most suitable expert, making it particularly
effective for handling cases near group boundaries. Experimental results show
that, unlike previous fairness approaches that reduce performance, FairMoE
achieves substantial accuracy improvements while preserving comparable fairness
metrics.

</details>


### [182] [Time-Contrastive Pretraining for In-Context Image and Video Segmentation](https://arxiv.org/abs/2506.17837)
*Assefa Wahd,Jacob Jaremko,Abhilash Hareendranathan*

Main category: cs.CV

TL;DR: The paper introduces 'Temporal,' a time-contrastive self-supervised learning method to enable visual in-context learning (ICL) for image and video segmentation, achieving significant accuracy improvements over baseline methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of traditional grid-based methods used in in-context learning (ICL), which restrict the flexibility, resolution, and scalability of ICL for vision tasks, and to enhance performance in vision applications.

Method: The authors introduce 'Temporal,' a self-supervised pretraining objective for a prompt retriever, and reframe in-context learning as a video object segmentation (VOS) task. This approach utilizes adjacent video frames as positives and distant frames as negatives to train the retriever, which then selects relevant sequences or keyframes to facilitate image and video segmentation tasks.

Result: The proposed method achieves a 90.95% Dice score for image segmentation, showing a 10.64% improvement, and a 92.45% Dice score for video segmentation, showing a 14.88% improvement over baseline methods.

Conclusion: The results demonstrate that reframing ICL as a video object segmentation task, coupled with the Temporal retriever, provides a more flexible and effective alternative to grid-based ICL methods for both image and video segmentation tasks.

Abstract: In-context learning (ICL) enables generalization to new tasks with minimal
labeled data. However, mainstream ICL approaches rely on a gridding strategy,
which lacks the flexibility required for vision applications. We introduce
Temporal, a time-contrastive self-supervised objective that pretrains a prompt
retriever for visual ICL, and formulate ICL as a video object segmentation
(VOS) task. Temporal addresses key limitations of grid-based methods that
restrict the number and resolution of context images. By reframing ICL as a VOS
problem, our approach supports a variable number of context images while
preserving their full resolution. To address the challenge of selecting optimal
context sets for queries, we pretrain a prompt retriever on videos via
self-supervised learning, where adjacent frames serve as positives and distant
frames as negatives. For image segmentation, the prompt retriever selects
relevant sequences that, when combined with the query, form coherent videos for
VOS processing. For video segmentation, it identifies keyframes, predicts their
masks using our ICL pipeline, and propagates them throughout the sequence. When
evaluated on MICCAI FLARE 2022, our method achieves substantial improvements
over baselines: 90.95% Dice score for image segmentation (10.64% improvement)
and 92.45% Dice for video segmentation (14.88% improvement).

</details>


### [183] [Robust Foreground-Background Separation for Severely-Degraded Videos Using Convolutional Sparse Representation Modeling](https://arxiv.org/abs/2506.17838)
*Kazuki Naganuma,Shunsuke Ono*

Main category: cs.CV

TL;DR: The paper introduces a novel foreground-background separation method using convolutional sparse representation to address low frame rate and noisy videos.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle to accurately separate foreground and background in degraded videos due to limitations in capturing features and noise models.

Method: Proposed a CSR-based foreground model, integrated into a constrained multi-convex optimization problem, solved by alternating between two convex subproblems.

Result: Experiments validate the method's effectiveness in handling infrared and microscope videos compared to existing approaches.

Conclusion: The new method improves foreground-background separation in challenging video scenarios by leveraging detailed and general feature representation alongside noise modeling.

Abstract: This paper proposes a foreground-background separation (FBS) method with a
novel foreground model based on convolutional sparse representation (CSR). In
order to analyze the dynamic and static components of videos acquired under
undesirable conditions, such as hardware, environmental, and power limitations,
it is essential to establish an FBS method that can handle videos with low
frame rates and various types of noise. Existing FBS methods have two
limitations that prevent us from accurately separating foreground and
background components from such degraded videos. First, they only capture
either data-specific or general features of the components. Second, they do not
include explicit models for various types of noise to remove them in the FBS
process. To this end, we propose a robust FBS method with a CSR-based
foreground model. This model can adaptively capture specific spatial structures
scattered in imaging data. Then, we formulate FBS as a constrained multiconvex
optimization problem that incorporates CSR, functions that capture general
features, and explicit noise characterization functions for multiple types of
noise. Thanks to these functions, our method captures both data-specific and
general features to accurately separate the components from various types of
noise even under low frame rates. To obtain a solution of the optimization
problem, we develop an algorithm that alternately solves its two convex
subproblems by newly established algorithms. Experiments demonstrate the
superiority of our method over existing methods using two types of degraded
videos: infrared and microscope videos.

</details>


### [184] [Fetuses Made Simple: Modeling and Tracking of Fetal Shape and Pose](https://arxiv.org/abs/2506.17858)
*Yingcheng Liu,Peiqi Wang,Sebastian Diaz,Esra Abaci Turk,Benjamin Billot,Patricia Ellen Grant,Polina Golland*

Main category: cs.CV

TL;DR: The paper introduces the first 3D articulated statistical fetal body model, improving analysis of fetal motion and shape in prenatal diagnostics.


<details>
  <summary>Details</summary>
Motivation: Current methods either simplify fetal body structure or struggle with motion analysis; this paper aims to overcome those shortcomings.

Method: A 3D statistical body model based on SMPL is developed, estimating body pose in image space and shape in canonical pose, trained on 19,816 MRI volumes.

Result: The model achieves a surface alignment error of 3.2 mm and allows anthropometric measurements that were previously difficult to extract.

Conclusion: This work enhances fetal motion and shape analysis, offering better robustness and application in prenatal diagnostics via an open-source model.

Abstract: Analyzing fetal body motion and shape is paramount in prenatal diagnostics
and monitoring. Existing methods for fetal MRI analysis mainly rely on
anatomical keypoints or volumetric body segmentations. Keypoints simplify body
structure to facilitate motion analysis, but may ignore important details of
full-body shape. Body segmentations capture complete shape information but
complicate temporal analysis due to large non-local fetal movements. To address
these limitations, we construct a 3D articulated statistical fetal body model
based on the Skinned Multi-Person Linear Model (SMPL). Our algorithm
iteratively estimates body pose in the image space and body shape in the
canonical pose space. This approach improves robustness to MRI motion artifacts
and intensity distortions, and reduces the impact of incomplete surface
observations due to challenging fetal poses. We train our model on
segmentations and keypoints derived from $19,816$ MRI volumes across $53$
subjects. Our model captures body shape and motion across time series and
provides intuitive visualization. Furthermore, it enables automated
anthropometric measurements traditionally difficult to obtain from
segmentations and keypoints. When tested on unseen fetal body shapes, our
method yields a surface alignment error of $3.2$ mm for $3$ mm MRI voxel size.
To our knowledge, this represents the first 3D articulated statistical fetal
body model, paving the way for enhanced fetal motion and shape analysis in
prenatal diagnostics. The code is available at
https://github.com/MedicalVisionGroup/fetal-smpl .

</details>


### [185] [Cross-modal State Space Modeling for Real-time RGB-thermal Wild Scene Semantic Segmentation](https://arxiv.org/abs/2506.17869)
*Xiaodong Guo,Zi'ang Lin,Luwen Hu,Zhihong Deng,Tong Liu,Wujie Zhou*

Main category: cs.CV

TL;DR: This paper presents CM-SSM, an efficient architecture for semantic segmentation using RGB and thermal data, offering lower computational complexity compared to Transformer-based approaches.


<details>
  <summary>Details</summary>
Motivation: To address the computational challenges of integrating RGB and thermal data for semantic segmentation in resource-constrained environments.

Method: The method features the CM-SS2D module for cross-modal state space modeling and the CM-SSA module to integrate global and local features efficiently, achieving linear computational complexity.

Result: The proposed CM-SSM framework achieves state-of-the-art performance on the CART dataset and shows generalizability on the PST900 dataset, with fewer parameters and lower computational cost.

Conclusion: CM-SSM provides an effective and efficient alternative to Transformer-based models for multimodal semantic segmentation, suitable for resource-limited systems.

Abstract: The integration of RGB and thermal data can significantly improve semantic
segmentation performance in wild environments for field robots. Nevertheless,
multi-source data processing (e.g. Transformer-based approaches) imposes
significant computational overhead, presenting challenges for
resource-constrained systems. To resolve this critical limitation, we
introduced CM-SSM, an efficient RGB-thermal semantic segmentation architecture
leveraging a cross-modal state space modeling (SSM) approach. Our framework
comprises two key components. First, we introduced a cross-modal
2D-selective-scan (CM-SS2D) module to establish SSM between RGB and thermal
modalities, which constructs cross-modal visual sequences and derives hidden
state representations of one modality from the other. Second, we developed a
cross-modal state space association (CM-SSA) module that effectively integrates
global associations from CM-SS2D with local spatial features extracted through
convolutional operations. In contrast with Transformer-based approaches, CM-SSM
achieves linear computational complexity with respect to image resolution.
Experimental results show that CM-SSM achieves state-of-the-art performance on
the CART dataset with fewer parameters and lower computational cost. Further
experiments on the PST900 dataset demonstrate its generalizability. Codes are
available at https://github.com/xiaodonguo/CMSSM.

</details>


### [186] [SurgVidLM: Towards Multi-grained Surgical Video Understanding with Large Language Model](https://arxiv.org/abs/2506.17873)
*Guankun Wang,Wenjin Mo,Junyi Wang,Long Bai,Kun Yuan,Ming Hu,Jinlin Wu,Junjun He,Yiming Huang,Nicolas Padoy,Zhen Lei,Hongbin Liu,Nassir Navab,Hongliang Ren*

Main category: cs.CV

TL;DR: SurgVidLM, a video language model specifically designed for fine-grained surgical video understanding, is introduced alongside the SVU-31K dataset, outperforming current models in comprehending complex surgical procedures.


<details>
  <summary>Details</summary>
Motivation: The need to bridge the gap in specialized Video Large Language Models for fine-grained surgical video understanding, which is essential for detailed analysis of surgical procedures.

Method: The paper introduces SurgVidLM, trained on the newly constructed SVU-31K dataset consisting of 31K video-instruction pairs. It employs two novel mechanisms: StageFocus for multi-grained progressive understanding and Multi-frequency Fusion Attention for integrating visual tokens.

Result: Experimental results show that SurgVidLM significantly outperforms state-of-the-art Vid-LLMs in fine-grained and holistic surgical video understanding tasks.

Conclusion: SurgVidLM sets a new benchmark in surgical video comprehension, demonstrating its ability to capture complex procedural contexts.

Abstract: Recent advances in Multimodal Large Language Models have demonstrated great
potential in the medical domain, facilitating users to understand surgical
scenes and procedures. Beyond image-based methods, the exploration of Video
Large Language Models (Vid-LLMs) has emerged as a promising avenue for
capturing the complex sequences of information involved in surgery. However,
there is still a lack of Vid-LLMs specialized for fine-grained surgical video
understanding tasks, which is crucial for analyzing specific processes or
details within a surgical procedure. To bridge this gap, we propose SurgVidLM,
the first video language model designed to address both full and fine-grained
surgical video comprehension. To train our SurgVidLM, we construct the SVU-31K
dataset which consists of over 31K video-instruction pairs, enabling both
holistic understanding and detailed analysis of surgical procedures.
Furthermore, we introduce the StageFocus mechanism which is a two-stage
framework performing the multi-grained, progressive understanding of surgical
videos. We also develop the Multi-frequency Fusion Attention to effectively
integrate low and high-frequency visual tokens, ensuring the retention of
critical information. Experimental results demonstrate that SurgVidLM
significantly outperforms state-of-the-art Vid-LLMs in both full and
fine-grained video understanding tasks, showcasing its superior capability in
capturing complex procedural contexts.

</details>


### [187] [StainPIDR: A Pathological Image Decouplingand Reconstruction Method for StainNormalization Based on Color VectorQuantization and Structure Restaining](https://arxiv.org/abs/2506.17879)
*Zheng Chen*

Main category: cs.CV

TL;DR: The paper proposes StainPIDR, a stain normalization method for pathological images that mitigates color discrepancies caused by imaging protocols, dye proportions, and devices, using feature decoupling, cross-attention staining, and template selection.


<details>
  <summary>Details</summary>
Motivation: Address the issue of color variation in pathological images that negatively impacts computer-aided diagnostic systems.

Method: Proposes the StainPIDR method which decouples images into structural and color features, employs a fixed color codebook, utilizes cross-attention for staining, and includes a template selection algorithm.

Result: Extensive experiments validate that StainPIDR and the proposed template selection algorithm effectively perform stain normalization.

Conclusion: The StainPIDR method demonstrates superior performance in normalizing pathological image stains, with promising potential for enhancing diagnostic reliability.

Abstract: The color appearance of a pathological image is highly related to the imaging
protocols, the proportion of different dyes, and the scanning devices.
Computer-aided diagnostic systems may deteriorate when facing these
color-variant pathological images. In this work, we propose a stain
normalization method called StainPIDR. We try to eliminate this color
discrepancy by decoupling the image into structure features and
vector-quantized color features, restaining the structure features with the
target color features, and decoding the stained structure features to
normalized pathological images. We assume that color features decoupled by
different images with the same color should be exactly the same. Under this
assumption, we train a fixed color vector codebook to which the decoupled color
features will map. In the restaining part, we utilize the cross-attention
mechanism to efficiently stain the structure features. As the target color
(decoupled from a selected template image) will also affect the performance of
stain normalization, we further design a template image selection algorithm to
select a template from a given dataset. In our extensive experiments, we
validate the effectiveness of StainPIDR and the template image selection
algorithm. All the results show that our method can perform well in the stain
normalization task. The code of StainPIDR will be publicly available later.

</details>


### [188] [Cloud-Aware SAR Fusion for Enhanced Optical Sensing in Space Missions](https://arxiv.org/abs/2506.17885)
*Trong-An Bui,Thanh-Thoai Le*

Main category: cs.CV

TL;DR: The paper introduces a Cloud-Attentive Reconstruction Framework for generating cloud-free optical images using SAR-optical feature fusion and deep learning.


<details>
  <summary>Details</summary>
Motivation: To address the challenge posed by cloud contamination in optical satellite imagery, which affects applications like environmental monitoring and disaster response.

Method: The framework aligns SAR structural information with optical spectral data using an attention-driven feature fusion mechanism and adaptive loss weighting for cloud-occluded regions.

Result: The proposed method achieves superior reconstruction accuracy with metrics: PSNR of 31.01 dB, SSIM of 0.918, and MAE of 0.017.

Conclusion: This novel approach is effective in producing spatially and spectrally consistent cloud-free images, outperforming existing methods.

Abstract: Cloud contamination significantly impairs the usability of optical satellite
imagery, affecting critical applications such as environmental monitoring,
disaster response, and land-use analysis. This research presents a
Cloud-Attentive Reconstruction Framework that integrates SAR-optical feature
fusion with deep learning-based image reconstruction to generate cloud-free
optical imagery. The proposed framework employs an attention-driven feature
fusion mechanism to align complementary structural information from Synthetic
Aperture Radar (SAR) with spectral characteristics from optical data.
Furthermore, a cloud-aware model update strategy introduces adaptive loss
weighting to prioritize cloud-occluded regions, enhancing reconstruction
accuracy. Experimental results demonstrate that the proposed method outperforms
existing approaches, achieving a PSNR of 31.01 dB, SSIM of 0.918, and MAE of
0.017. These outcomes highlight the framework's effectiveness in producing
high-fidelity, spatially and spectrally consistent cloud-free optical images.

</details>


### [189] [Relation3D: Enhancing Relation Modeling for Point Cloud Instance Segmentation](https://arxiv.org/abs/2506.17891)
*Jiahao Lu,Jiacheng Deng*

Main category: cs.CV

TL;DR: Relation3D proposes advanced techniques for improving 3D point cloud instance segmentation by enhancing external and internal relationship modeling in point cloud data.


<details>
  <summary>Details</summary>
Motivation: Address the limitations in current transformer-based 3D instance segmentation methods, specifically their ineffective modeling of internal relationships within scene and query features.

Method: Introduced novel modules such as adaptive superpoint aggregation, contrastive learning-guided superpoint refinement, and a relation-aware self-attention mechanism to improve relationship modeling capabilities.

Result: Extensive experiments on benchmarks like ScanNetV2, ScanNet++, ScanNet200, and S3DIS datasets showcased superior performance of Relation3D compared to existing methods.

Conclusion: Relation3D advances the state-of-the-art in 3D instance segmentation by enhancing internal and external relationship modeling, demonstrating its effectiveness in various datasets.

Abstract: 3D instance segmentation aims to predict a set of object instances in a
scene, representing them as binary foreground masks with corresponding semantic
labels. Currently, transformer-based methods are gaining increasing attention
due to their elegant pipelines and superior predictions. However, these methods
primarily focus on modeling the external relationships between scene features
and query features through mask attention. They lack effective modeling of the
internal relationships among scene features as well as between query features.
In light of these disadvantages, we propose \textbf{Relation3D: Enhancing
Relation Modeling for Point Cloud Instance Segmentation}. Specifically, we
introduce an adaptive superpoint aggregation module and a contrastive
learning-guided superpoint refinement module to better represent superpoint
features (scene features) and leverage contrastive learning to guide the
updates of these features. Furthermore, our relation-aware self-attention
mechanism enhances the capabilities of modeling relationships between queries
by incorporating positional and geometric relationships into the self-attention
mechanism. Extensive experiments on the ScanNetV2, ScanNet++, ScanNet200 and
S3DIS datasets demonstrate the superior performance of Relation3D.

</details>


### [190] [BeltCrack: the First Sequential-image Industrial Conveyor Belt Crack Detection Dataset and Its Baseline with Triple-domain Feature Learning](https://arxiv.org/abs/2506.17892)
*Jianghong Huang,Luping Ji,Xin Ma,Mao Ye*

Main category: cs.CV

TL;DR: The paper addresses the need for an intelligent detection system for conveyor belt cracks in industrial settings, presenting the first real-world datasets, BeltCrack14ks and BeltCrack9kd, and a baseline hierarchical fusion learning method.


<details>
  <summary>Details</summary>
Motivation: Conveyor belts play a critical role in industrial operations, and their cracks are prominent threats to efficiency and safety. The absence of real-world datasets for detecting belt cracks limits advancements in intelligent detection methods.

Method: The paper constructs two sequential-image datasets, BeltCrack14ks and BeltCrack9kd, sourced from real factory environments. It also proposes a baseline detection model using hierarchical fusion learning from triple domains (time-space-frequency).

Result: Experimental findings validate the effectiveness of the datasets and the baseline detection method, demonstrating its superiority over similar methods.

Conclusion: The constructed datasets and baseline detection approach significantly advance intelligent belt crack detection. The public release aims to foster further research and development in this area.

Abstract: Conveyor belt is a category of important equipments in modern industry,
widely applied in production and manufacturing Fields. Its health status is
much critical to operation efficiency and safety hazards. Among the factors
affecting belt health, crack is often one of the most threatening risks.
Currently, considering safety, how to intelligently detect belt cracks is
catching an increasing attention. To implement the intelligent detection with
machine learning, real crack samples are believed to be necessary. However,
existing crack datasets primarily focus on pavement scenarios or synthetic
data, no real-world industrial belt crack datasets at all. To propel machine
learning advancement in this field, this paper constructs the first
sequential-image belt crack detection datasets (BeltCrack14ks and
BeltCrack9kd), from real-world factory scenes. Furthermore, to validate
usability and effectiveness, we propose a special baseline method with
triple-domain (i.e., time-space-frequency) feature hierarchical fusion learning
for the two whole-new datasets. Experimental results demonstrate the
availability and effectiveness of our dataset. Besides, they also show that our
baseline is obviously superior to other similar detection methods. Our datasets
and source codes are available at https://github.com/UESTC-nnLab/BeltCrack.

</details>


### [191] [EgoWorld: Translating Exocentric View to Egocentric View using Rich Exocentric Observations](https://arxiv.org/abs/2506.17896)
*Junho Park,Andrew Sangwoo Ye,Taein Kwon*

Main category: cs.CV

TL;DR: The paper proposes EgoWorld, a framework to reconstruct egocentric views from exocentric observations, utilizing 3D and semantic data, achieving state-of-the-art results in validation.


<details>
  <summary>Details</summary>
Motivation: To enable precise translation of third-person (exocentric) views to first-person (egocentric) perspectives for applications in augmented and virtual reality, as well as robotics.

Method: The EgoWorld framework uses a two-stage process: first, reconstructing a point cloud from exocentric depth maps and reprojecting it into egocentric views, and second, using diffusion-based inpainting to create semantic, dense egocentric images.

Result: EgoWorld demonstrated state-of-the-art performance on H2O and TACO datasets, showing robust generalization to novel objects, actions, and scenarios, including unlabeled real-world examples.

Conclusion: EgoWorld overcomes limitations of existing methods by leveraging advanced 3D and semantic techniques, making substantial contributions to the field of egocentric image reconstruction for AR, VR, and robotics.

Abstract: Egocentric vision is essential for both human and machine visual
understanding, particularly in capturing the detailed hand-object interactions
needed for manipulation tasks. Translating third-person views into first-person
views significantly benefits augmented reality (AR), virtual reality (VR) and
robotics applications. However, current exocentric-to-egocentric translation
methods are limited by their dependence on 2D cues, synchronized multi-view
settings, and unrealistic assumptions such as necessity of initial egocentric
frame and relative camera poses during inference. To overcome these challenges,
we introduce EgoWorld, a novel two-stage framework that reconstructs an
egocentric view from rich exocentric observations, including projected point
clouds, 3D hand poses, and textual descriptions. Our approach reconstructs a
point cloud from estimated exocentric depth maps, reprojects it into the
egocentric perspective, and then applies diffusion-based inpainting to produce
dense, semantically coherent egocentric images. Evaluated on the H2O and TACO
datasets, EgoWorld achieves state-of-the-art performance and demonstrates
robust generalization to new objects, actions, scenes, and subjects. Moreover,
EgoWorld shows promising results even on unlabeled real-world examples.

</details>


### [192] [PostAlign: Multimodal Grounding as a Corrective Lens for MLLMs](https://arxiv.org/abs/2506.17901)
*Yixuan Wu,Yang Zhang,Jian Wu,Philip Torr,Jindong Gu*

Main category: cs.CV

TL;DR: This paper addresses issues in multimodal models, like spurious correlations and hallucinations, by introducing a framework called MMGrounded-PostAlign for better visual and textual grounding.


<details>
  <summary>Details</summary>
Motivation: The paper aims to solve shortcomings in MLLMs, such as over-reliance on linguistic priors and hallucinations during vision-language tasks.

Method: The proposed framework incorporates visual grounding to identify relevant objects, textual grounding for rationale generation, and mechanisms like negative rejection and selective reasoning to mitigate hallucinations.

Result: The framework showed notable improvements in visual understanding and reduced hallucinations across multiple benchmarks like POPE and VQAv2.

Conclusion: MMGrounded-PostAlign enhances multimodal models by anchoring their outputs in visual-textual evidence, thereby improving reliability and accuracy in tasks.

Abstract: Multimodal Large Language Models (MLLMs) excel in vision-language tasks, such
as image captioning and visual question answering. However, they often suffer
from over-reliance on spurious correlations, primarily due to linguistic priors
that distract the model from leveraging actual visual information. To address
these issues, we introduce MMGrounded-PostAlign, a post-multimodal alignment
framework designed to enhance the visual understanding capabilities and
mitigate the hallucinations of MLLMs. Our framework incorporates a multimodal
grounding module for both visual grounding, which identifies the referred
object in the image, and textual grounding, which generates the rationale for
the final answer, ensuring that outputs are anchored in both visual and textual
evidence. To mitigate the hallucinations, we introduce a negative rejection
mechanism in the visual grounding module to distinguish grounded entities from
non-existent objects influenced by linguistic biases. On the textual grounding
side, we propose a selective reasoning mechanism that adjusts the model's
reasoning strategy based on query complexity. Extensive evaluations are
conducted on benchmarks such as POPE, HaloQuest, VQAv2, MME, and MMBench
showing significant improvements in fine-grained visual understanding and
hallucination suppression.

</details>


### [193] [Cause-Effect Driven Optimization for Robust Medical Visual Question Answering with Language Biases](https://arxiv.org/abs/2506.17903)
*Huanjia Zhu,Yishu Liu,Xiaozhao Fang,Guangming Lu,Bingzhi Chen*

Main category: cs.CV

TL;DR: The authors propose CEDO, a framework to address language bias in Med-VQA models using adaptive learning rates, synergistic modality interactions, and balanced learning.


<details>
  <summary>Details</summary>
Motivation: Med-VQA models suffer from language biases caused by spurious correlations between question types and answer categories, affecting model robustness.

Method: CEDO employs Modality-driven Heterogeneous Optimization (MHO), Gradient-guided Modality Synergy (GMS), and Distribution-adapted Loss Rescaling (DLR) to address biases from causal and effectual perspectives.

Result: CEDO shows consistent robustness across traditional and bias-sensitive Med-VQA benchmarks, outperforming existing methods.

Conclusion: CEDO is an effective framework for mitigating language biases in Med-VQA models, ensuring better reasoning and balanced learning.

Abstract: Existing Medical Visual Question Answering (Med-VQA) models often suffer from
language biases, where spurious correlations between question types and answer
categories are inadvertently established. To address these issues, we propose a
novel Cause-Effect Driven Optimization framework called CEDO, that incorporates
three well-established mechanisms, i.e., Modality-driven Heterogeneous
Optimization (MHO), Gradient-guided Modality Synergy (GMS), and
Distribution-adapted Loss Rescaling (DLR), for comprehensively mitigating
language biases from both causal and effectual perspectives. Specifically, MHO
employs adaptive learning rates for specific modalities to achieve
heterogeneous optimization, thus enhancing robust reasoning capabilities.
Additionally, GMS leverages the Pareto optimization method to foster
synergistic interactions between modalities and enforce gradient orthogonality
to eliminate bias updates, thereby mitigating language biases from the effect
side, i.e., shortcut bias. Furthermore, DLR is designed to assign adaptive
weights to individual losses to ensure balanced learning across all answer
categories, effectively alleviating language biases from the cause side, i.e.,
imbalance biases within datasets. Extensive experiments on multiple traditional
and bias-sensitive benchmarks consistently demonstrate the robustness of CEDO
over state-of-the-art competitors.

</details>


### [194] [Feedback Driven Multi Stereo Vision System for Real-Time Event Analysis](https://arxiv.org/abs/2506.17910)
*Mohamed Benkedadra,Matei Mancas,Sidi Ahmed Mahmoudi*

Main category: cs.CV

TL;DR: This paper proposes a robust 3D stereo vision-based system for interactive applications, enhancing scene understanding and introducing multi-camera fusion for full scene reconstruction.


<details>
  <summary>Details</summary>
Motivation: 2D and existing 3D cameras struggle in large, complex environments. A robust system capable of supporting real-time interactive systems is needed.

Method: The authors propose a 3D stereo vision pipeline that fuses multiple 3D cameras to achieve full-scene reconstruction. It incorporates feedback mechanisms for adaptation and improved decision-making.

Result: Preliminary experiments demonstrate the system's capabilities in event recognition, subject tracking, and notifications.

Conclusion: The authors outline next steps to refine the pipeline for production and address larger environments, ensuring reliability and adaptability for complex use cases.

Abstract: 2D cameras are often used in interactive systems. Other systems like gaming
consoles provide more powerful 3D cameras for short range depth sensing.
Overall, these cameras are not reliable in large, complex environments. In this
work, we propose a 3D stereo vision based pipeline for interactive systems,
that is able to handle both ordinary and sensitive applications, through robust
scene understanding. We explore the fusion of multiple 3D cameras to do full
scene reconstruction, which allows for preforming a wide range of tasks, like
event recognition, subject tracking, and notification. Using possible feedback
approaches, the system can receive data from the subjects present in the
environment, to learn to make better decisions, or to adapt to completely new
environments. Throughout the paper, we introduce the pipeline and explain our
preliminary experimentation and results. Finally, we draw the roadmap for the
next steps that need to be taken, in order to get this pipeline into production

</details>


### [195] [PlanMoGPT: Flow-Enhanced Progressive Planning for Text to Motion Synthesis](https://arxiv.org/abs/2506.17912)
*Chuhao Jin,Haosen Li,Bingzi Zhang,Che Liu,Xiting Wang,Ruihua Song,Wenbing Huang,Ying Qin,Fuzheng Zhang,Di Zhang*

Main category: cs.CV

TL;DR: This paper presents PlanMoGPT, a framework using large language models (LLMs) for text-to-motion generation.


<details>
  <summary>Details</summary>
Motivation: LLMs have shown promise but underperform compared to non-LLM methods in text-to-motion generation, mainly due to motion tokenization challenges.

Method: The framework combines progressive planning for hierarchical token generation and flow-enhanced tokenization to balance detail preservation and global coherence.

Result: PlanMoGPT achieves state-of-the-art performance, improving FID scores by 63.8% and enhancing motion diversity by 49.9% on benchmarks.

Conclusion: PlanMoGPT resolves the diversity-quality trade-off in text-to-motion generation, setting a new standard in this field.

Abstract: Recent advances in large language models (LLMs) have enabled breakthroughs in
many multimodal generation tasks, but a significant performance gap still
exists in text-to-motion generation, where LLM-based methods lag far behind
non-LLM methods. We identify the granularity of motion tokenization as a
critical bottleneck: fine-grained tokenization induces local dependency issues,
where LLMs overemphasize short-term coherence at the expense of global semantic
alignment, while coarse-grained tokenization sacrifices motion details. To
resolve this issue, we propose PlanMoGPT, an LLM-based framework integrating
progressive planning and flow-enhanced fine-grained motion tokenization. First,
our progressive planning mechanism leverages LLMs' autoregressive capabilities
to hierarchically generate motion tokens by starting from sparse global plans
and iteratively refining them into full sequences. Second, our flow-enhanced
tokenizer doubles the downsampling resolution and expands the codebook size by
eight times, minimizing detail loss during discretization, while a
flow-enhanced decoder recovers motion nuances. Extensive experiments on
text-to-motion benchmarks demonstrate that it achieves state-of-the-art
performance, improving FID scores by 63.8% (from 0.380 to 0.141) on
long-sequence generation while enhancing motion diversity by 49.9% compared to
existing methods. The proposed framework successfully resolves the
diversity-quality trade-off that plagues current non-LLM approaches,
establishing new standards for text-to-motion generation.

</details>


### [196] [IDAL: Improved Domain Adaptive Learning for Natural Images Dataset](https://arxiv.org/abs/2506.17931)
*Ravi Kant Gupta,Shounak Das,Amit Sethi*

Main category: cs.CV

TL;DR: The paper introduces a new unsupervised domain adaptation method for handling domain shifts in natural images, combining advanced neural architecture with tailored loss functions for improved accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve domain alignment in representation space for classification tasks, especially for multimodal distributions, addressing challenges such as noise, scale, and style shifts.

Method: The approach uses ResNet and feature pyramidal networks for architecture, alongside a custom combination of novel and existing loss functions optimized for domain adaptation problems.

Result: The proposed method outperforms state-of-the-art CNN-based methods on Office-Home, Office-31, and VisDA-2017 datasets and achieves comparable results on DomainNet.

Conclusion: The combination of architectural and loss-function advancements leads to better model accuracy, robustness, and training efficiency for UDA systems dealing with natural images.

Abstract: We present a novel approach for unsupervised domain adaptation (UDA) for
natural images. A commonly-used objective for UDA schemes is to enhance domain
alignment in representation space even if there is a domain shift in the input
space. Existing adversarial domain adaptation methods may not effectively align
different domains of multimodal distributions associated with classification
problems. Our approach has two main features. Firstly, its neural architecture
uses the deep structure of ResNet and the effective separation of scales of
feature pyramidal network (FPN) to work with both content and style features.
Secondly, it uses a combination of a novel loss function and judiciously
selected existing loss functions to train the network architecture. This
tailored combination is designed to address challenges inherent to natural
images, such as scale, noise, and style shifts, that occur on top of a
multi-modal (multi-class) distribution. The combined loss function not only
enhances model accuracy and robustness on the target domain but also speeds up
training convergence. Our proposed UDA scheme generalizes better than
state-of-the-art for CNN-based methods on Office-Home, Office-31, and
VisDA-2017 datasets and comaparable for DomainNet dataset.

</details>


### [197] [GEMeX-ThinkVG: Towards Thinking with Visual Grounding in Medical VQA via Reinforcement Learning](https://arxiv.org/abs/2506.17939)
*Bo Liu,Xiangyu Zhao,Along He,Yidi Chen,Huazhu Fu,Xiao-Ming Wu*

Main category: cs.CV

TL;DR: The paper introduces a dataset and methodology to improve reliability, efficiency, and interpretability in medical visual question answering models.


<details>
  <summary>Details</summary>
Motivation: Current medical visual question answering models lack reliability and interpretability, limiting their trustworthiness for clinicians and patients.

Method: The study proposed a ThinkVG dataset with intermediate reasoning steps and visual region grounding, coupled with a novel reinforcement learning reward mechanism.

Result: The proposed technique achieves similar performance with only one-eighth of the training data compared to prior methods.

Conclusion: The approach enhances data efficiency, answer reliability, and model interpretability, addressing key limitations in medical visual question answering.

Abstract: Medical visual question answering aims to support clinical decision-making by
enabling models to answer natural language questions based on medical images.
While recent advances in multi-modal learning have significantly improved
performance, current methods still suffer from limited answer reliability and
poor interpretability, impairing the ability of clinicians and patients to
understand and trust model-generated answers. To address this, this work first
proposes a Thinking with Visual Grounding (ThinkVG) dataset wherein the answer
generation is decomposed into intermediate reasoning steps that explicitly
ground relevant visual regions of the medical image, thereby providing
fine-grained explainability. Furthermore, we introduce a novel verifiable
reward mechanism for reinforcement learning to guide post-training, improving
the alignment between the model's reasoning process and its final answer.
Remarkably, our method achieves comparable performance using only one-eighth of
the training data, demonstrating the efficiency and effectiveness of the
proposal. The dataset is available at
https://huggingface.co/datasets/BoKelvin/GEMeX-ThinkVG.

</details>


### [198] [SegChange-R1:Augmented Reasoning for Remote Sensing Change Detection via Large Language Models](https://arxiv.org/abs/2506.17944)
*Fei Zhou*

Main category: cs.CV

TL;DR: The paper introduces SegChange-R1, a change detection method leveraging large language models and a spatial transformation module to enhance result accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve remote sensing change detection by addressing challenges in feature extraction, convergence speed, and modal misalignment across temporal perspectives.

Method: The proposed method combines large language models for descriptive information integration, a spatial transformation module (BEV) for feature alignment, and a new UAV-derived dataset (DVCD).

Result: SegChange-R1 achieves significant improvements in detection accuracy and speed, as demonstrated on four widely-used change detection datasets.

Conclusion: The study demonstrates that enhancing change detection models with LLMs and spatial alignment techniques can effectively boost detection performance and convergence.

Abstract: Remote sensing change detection is widely used in a variety of fields such as
urban planning, terrain and geomorphology analysis, and environmental
monitoring, mainly by analyzing the significant change differences of features
(e.g., building changes) in the same spatial region at different time phases.
In this paper, we propose a large language model (LLM) augmented inference
approach (SegChange-R1), which enhances the detection capability by integrating
textual descriptive information and aims at guiding the model to segment the
more interested change regions, thus accelerating the convergence speed.
Moreover, we design a spatial transformation module (BEV) based on linear
attention, which solves the problem of modal misalignment in change detection
by unifying features from different temporal perspectives onto the BEV space.
In addition, we construct the first dataset for building change detection from
UAV viewpoints (DVCD ), and our experiments on four widely-used change
detection datasets show a significant improvement over existing methods. The
code and pre-trained models are available in
https://github.com/Yu-Zhouz/SegChange-R1.

</details>


### [199] [Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning](https://arxiv.org/abs/2506.18234)
*Yue Li,Meng Tian,Dechang Zhu,Jiangtong Zhu,Zhenyu Lin,Zhiwei Xiong,Xinhai Zhao*

Main category: cs.CV

TL;DR: Drive-R1 is a vision-language model designed to improve autonomous driving by aligning scenario reasoning and motion planning, outperforming current models in benchmarks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses two key challenges in applying large vision-language models (VLMs) for autonomous driving: VLMs relying too much on input history instead of understanding the visual input, and a misalignment between chain-of-thought reasoning and motion planning outcomes.

Method: Drive-R1 uses supervised fine-tuning on data containing both long and short chain-of-thought reasoning and leverages reinforcement learning with trajectory-based rewards to connect reasoning paths with better planning outcomes.

Result: Drive-R1 demonstrated superior performance compared to existing state-of-the-art VLMs on nuScenes and DriveLM-nuScenes benchmarks.

Conclusion: Drive-R1 provides a solid framework for integrating reasoning and planning processes in autonomous driving, paving the way for methodological advancements and future research in the field.

Abstract: Large vision-language models (VLMs) for autonomous driving (AD) are evolving
beyond perception and cognition tasks toward motion planning. However, we
identify two critical challenges in this direction: (1) VLMs tend to learn
shortcuts by relying heavily on history input information, achieving seemingly
strong planning results without genuinely understanding the visual inputs; and
(2) the chain-ofthought (COT) reasoning processes are always misaligned with
the motion planning outcomes, and how to effectively leverage the complex
reasoning capability to enhance planning remains largely underexplored. In this
paper, we start from a small-scale domain-specific VLM and propose Drive-R1
designed to bridges the scenario reasoning and motion planning for AD. Drive-R1
first undergoes the supervised finetuning on a elaborate dataset containing
both long and short COT data. Drive-R1 is encouraged to reason step-by-step
from visual input to final planning decisions. Subsequently, Drive-R1 is
trained within a reinforcement learning framework that incentivizes the
discovery of reasoning paths that are more informative for planning, guided by
rewards based on predicted trajectories and meta actions. Experimental
evaluations on the nuScenes and DriveLM-nuScenes benchmarks demonstrate that
Drive-R1 achieves superior performance compared to existing state-of-the-art
VLMs. We believe that Drive-R1 presents a promising direction for bridging
reasoning and planning in AD, offering methodological insights for future
research and applications.

</details>


### [200] [Classification of Tents in Street Bazaars Using CNN](https://arxiv.org/abs/2506.17946)
*Azamat Ibragimov,Ruslan Isaev,Remudin Reshid Mekuria,Gulnaz Gimaletdinova,Dim Shaiakhmetov*

Main category: cs.CV

TL;DR: This paper improves tent classification in street bazaars using CNN and EfficientNetB0, achieving 92.8% and 98.4% accuracy respectively.


<details>
  <summary>Details</summary>
Motivation: The paper addresses inefficiencies in manual tent classification in street bazaars, which are economically vital yet structurally unorganized.

Method: The authors trained and augmented a dataset of 126 photographs and evaluated custom CNN versus EfficientNetB0 using metrics like accuracy, precision, recall, F1 score, and mAP.

Result: EfficientNetB0 showed 98.4% accuracy, outperforming the custom CNN model which achieved 92.8%, highlighting the superiority of transfer learning.

Conclusion: Pre-trained models like EfficientNetB0 significantly enhance bazaar image classification accuracy and generalization, aiding in market organization.

Abstract: This research paper proposes an improved deep learning model for classifying
tents in street bazaars, comparing a custom Convolutional Neural Network (CNN)
with EfficientNetB0. This is a critical task for market organization with a
tent classification, but manual methods in the past have been inefficient.
Street bazaars represent a vital economic hub in many regions, yet their
unstructured nature poses significant challenges for the automated
classification of market infrastructure, such as tents. In Kyrgyzstan, more
than a quarter of the country's GDP is derived from bazaars. While CNNs have
been widely applied to object recognition, their application to bazaar-specific
tasks remains underexplored. Here, we build upon our original approach by
training on an extended set of 126 original photographs that were augmented to
generate additional images. This dataset is publicly available for download on
Kaggle. A variety of performance metrics, such as accuracy, precision, recall,
F1 score, and mean average precision (mAP), were used to assess the models
comparatively, providing a more extensive analysis of classification
performance.
  The results show that the CNN custom model achieved 92.8% accuracy, and
EfficientNetB0 showed 98.4% accuracy results, confirming the effectiveness of
transfer learning in the bazaar image classification. Also, when analyzing the
confusion matrix, the analysis reveals the weaknesses and strengths of each
model. These findings suggest that using a pre-trained model such as
EfficientNetB0 significantly improves classification accuracy and
generalization.

</details>


### [201] [Mobile Image Analysis Application for Mantoux Skin Test](https://arxiv.org/abs/2506.17954)
*Liong Gele,Tan Chye Cheah*

Main category: cs.CV

TL;DR: The paper introduces a mobile app for diagnosing Latent Tuberculosis Infection using the Mantoux Skin Test with advanced image processing and machine learning for better accuracy and accessibility.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of traditional TST methods such as low follow-up rates, manual interpretation errors, and inefficiencies while improving diagnostic capabilities in resource-limited regions.

Method: The app uses scaling stickers for measurement, integrates ARCore and DeepLabv3 for image processing and segmentation, and employs edge detection algorithms to ensure robust and accurate diagnoses.

Result: The evaluation demonstrated substantial improvements in accuracy and reliability compared to standard clinical practices.

Conclusion: The mobile application enhances TB diagnostics by automating TST evaluations, making them more accessible and efficient, with future plans to optimize and expand functionalities.

Abstract: This paper presents a newly developed mobile application designed to diagnose
Latent Tuberculosis Infection (LTBI) using the Mantoux Skin Test (TST).
Traditional TST methods often suffer from low follow-up return rates, patient
discomfort, and subjective manual interpretation, particularly with the
ball-point pen method, leading to misdiagnosis and delayed treatment. Moreover,
previous developed mobile applications that used 3D reconstruction, this app
utilizes scaling stickers as reference objects for induration measurement. This
mobile application integrates advanced image processing technologies, including
ARCore, and machine learning algorithms such as DeepLabv3 for robust image
segmentation and precise measurement of skin indurations indicative of LTBI.
The system employs an edge detection algorithm to enhance accuracy. The
application was evaluated against standard clinical practices, demonstrating
significant improvements in accuracy and reliability. This innovation is
crucial for effective tuberculosis management, especially in resource-limited
regions. By automating and standardizing TST evaluations, the application
enhances the accessibility and efficiency of TB di-agnostics. Future work will
focus on refining machine learning models, optimizing measurement algorithms,
expanding functionalities to include comprehensive patient data management, and
enhancing ARCore's performance across various lighting conditions and
operational settings.

</details>


### [202] [MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit Neural Scene Representation](https://arxiv.org/abs/2506.18678)
*Tianchen Deng,Guole Shen,Xun Chen,Shenghai Yuan,Hongming Shen,Guohao Peng,Zhenyu Wu,Jingchuan Wang,Lihua Xie,Danwei Wang,Hesheng Wang,Weidong Chen*

Main category: cs.CV

TL;DR: This paper introduces a novel distributed multi-agent neural SLAM framework with hybrid scene representation, improved methods for scene reconstruction and consistency, and a new real-world Dense SLAM dataset.


<details>
  <summary>Details</summary>
Motivation: Existing implicit SLAM algorithms are limited to single-agent scenarios and face difficulties with large-scale scenes or long sequences. NeRF-based multi-agent SLAM frameworks also struggle with communication bandwidth constraints.

Method: The authors propose hybrid scene representation (triplane-grid joint method), distributed camera tracking, intra-to-inter loop closure for consistency, and online distillation for submap fusion. Alongside the framework, they introduce a new real-world dataset to facilitate research.

Result: Experiments validate the superiority of the proposed method in mapping, tracking, and communication for various datasets.

Conclusion: The proposed framework advances multi-agent SLAM capabilities while the new dataset provides a foundation for broader research in SLAM, 3D reconstruction, and visual models.

Abstract: Neural implicit scene representations have recently shown promising results
in dense visual SLAM. However, existing implicit SLAM algorithms are
constrained to single-agent scenarios, and fall difficulties in large-scale
scenes and long sequences. Existing NeRF-based multi-agent SLAM frameworks
cannot meet the constraints of communication bandwidth. To this end, we propose
the first distributed multi-agent collaborative neural SLAM framework with
hybrid scene representation, distributed camera tracking, intra-to-inter loop
closure, and online distillation for multiple submap fusion. A novel
triplane-grid joint scene representation method is proposed to improve scene
reconstruction. A novel intra-to-inter loop closure method is designed to
achieve local (single-agent) and global (multi-agent) consistency. We also
design a novel online distillation method to fuse the information of different
submaps to achieve global consistency. Furthermore, to the best of our
knowledge, there is no real-world dataset for NeRF-based/GS-based SLAM that
provides both continuous-time trajectories groundtruth and high-accuracy 3D
meshes groundtruth. To this end, we propose the first real-world Dense slam
(DES) dataset covering both single-agent and multi-agent scenarios, ranging
from small rooms to large-scale outdoor scenes, with high-accuracy ground truth
for both 3D mesh and continuous-time camera trajectory. This dataset can
advance the development of the research in both SLAM, 3D reconstruction, and
visual foundation model. Experiments on various datasets demonstrate the
superiority of the proposed method in both mapping, tracking, and
communication. The dataset and code will open-source on
https://github.com/dtc111111/mcnslam.

</details>


### [203] [ELMAR: Enhancing LiDAR Detection with 4D Radar Motion Awareness and Cross-modal Uncertainty](https://arxiv.org/abs/2506.17958)
*Xiangyuan Peng,Miao Tang,Huawei Sun,Bierzynski Kay,Lorenzo Servadei,Robert Wille*

Main category: cs.CV

TL;DR: The paper introduces a novel framework combining 4D radar motion detection and LiDAR data to improve object detection in autonomous systems.


<details>
  <summary>Details</summary>
Motivation: To overcome the misalignment issues and leverage the strengths of 4D radar and LiDAR for enhanced perception in autonomous systems.

Method: A Dynamic Motion-Aware Encoding module is used to capture object movement from 4D radar, and uncertainties across modalities are estimated to refine LiDAR predictions.

Result: Achieved state-of-the-art detection performance with mAP scores of 74.89% overall and 88.70% within driving corridors on the VoD dataset while maintaining 30.02 FPS inference speed.

Conclusion: Integrating motion and uncertainty information across modalities significantly improves perception accuracy and speed in autonomous systems using LiDAR and 4D radar.

Abstract: LiDAR and 4D radar are widely used in autonomous driving and robotics. While
LiDAR provides rich spatial information, 4D radar offers velocity measurement
and remains robust under adverse conditions. As a result, increasing studies
have focused on the 4D radar-LiDAR fusion method to enhance the perception.
However, the misalignment between different modalities is often overlooked. To
address this challenge and leverage the strengths of both modalities, we
propose a LiDAR detection framework enhanced by 4D radar motion status and
cross-modal uncertainty. The object movement information from 4D radar is first
captured using a Dynamic Motion-Aware Encoding module during feature extraction
to enhance 4D radar predictions. Subsequently, the instance-wise uncertainties
of bounding boxes are estimated to mitigate the cross-modal misalignment and
refine the final LiDAR predictions. Extensive experiments on the View-of-Delft
(VoD) dataset highlight the effectiveness of our method, achieving
state-of-the-art performance with the mAP of 74.89% in the entire area and
88.70% within the driving corridor while maintaining a real-time inference
speed of 30.02 FPS.

</details>


### [204] [Including Semantic Information via Word Embeddings for Skeleton-based Action Recognition](https://arxiv.org/abs/2506.18721)
*Dustin Aganian,Erik Franze,Markus Eisenbach,Horst-Michael Gross*

Main category: cs.CV

TL;DR: The paper proposes a semantic-enhanced approach for skeleton-based human action recognition, particularly for cobots in Industry 4.0.


<details>
  <summary>Details</summary>
Motivation: Conventional skeleton-based methods lose keypoint semantics, limiting their efficiency in complex interactions for cobots in assembly tasks.

Method: Introduces word embeddings to replace one-hot keypoints with semantic volumes, capturing relationships between skeleton joints and objects.

Result: Experiments on multiple assembly datasets show improved classification performance and generalization for various skeleton types and object classes.

Conclusion: Incorporating semantic information improves skeleton-based action recognition, making it more effective in dynamic and diverse environments.

Abstract: Effective human action recognition is widely used for cobots in Industry 4.0
to assist in assembly tasks. However, conventional skeleton-based methods often
lose keypoint semantics, limiting their effectiveness in complex interactions.
In this work, we introduce a novel approach to skeleton-based action
recognition that enriches input representations by leveraging word embeddings
to encode semantic information. Our method replaces one-hot encodings with
semantic volumes, enabling the model to capture meaningful relationships
between joints and objects. Through extensive experiments on multiple assembly
datasets, we demonstrate that our approach significantly improves
classification performance, and enhances generalization capabilities by
simultaneously supporting different skeleton types and object classes. Our
findings highlight the potential of incorporating semantic information to
enhance skeleton-based action recognition in dynamic and diverse environments.

</details>


### [205] [BPCLIP: A Bottom-up Image Quality Assessment from Distortion to Semantics Based on CLIP](https://arxiv.org/abs/2506.17969)
*Chenyue Song,Chen Hui,Wei Zhang,Haiqi Zhu,Shaohui Liu,Hong Huang,Feng Jiang*

Main category: cs.CV

TL;DR: The paper introduces BPCLIP, an IQA model leveraging CLIP for feature alignment to better assess perceptual image quality by analyzing multiscale features and incorporating human-language descriptors.


<details>
  <summary>Details</summary>
Motivation: Existing IQA methods struggle to capture the impact of distortions on semantic content due to reliance on linear fusion techniques.

Method: BPCLIP uses multiscale cross-attention modules and a CLIP text encoder trained with image quality adjectives to align low-level distortions with high-level semantic perception.

Result: BPCLIP outperforms other methods on both Full-Reference and No-Reference IQA benchmarks, showcasing superior accuracy and robustness.

Conclusion: The proposed method effectively links image quality assessment with human perceptual understanding, leveraging CLIP's ability to align textual and visual representations.

Abstract: Image Quality Assessment (IQA) aims to evaluate the perceptual quality of
images based on human subjective perception. Existing methods generally combine
multiscale features to achieve high performance, but most rely on
straightforward linear fusion of these features, which may not adequately
capture the impact of distortions on semantic content. To address this, we
propose a bottom-up image quality assessment approach based on the Contrastive
Language-Image Pre-training (CLIP, a recently proposed model that aligns images
and text in a shared feature space), named BPCLIP, which progressively extracts
the impact of low-level distortions on high-level semantics. Specifically, we
utilize an encoder to extract multiscale features from the input image and
introduce a bottom-up multiscale cross attention module designed to capture the
relationships between shallow and deep features. In addition, by incorporating
40 image quality adjectives across six distinct dimensions, we enable the
pre-trained CLIP text encoder to generate representations of the intrinsic
quality of the image, thereby strengthening the connection between image
quality perception and human language. Our method achieves superior results on
most public Full-Reference (FR) and No-Reference (NR) IQA benchmarks, while
demonstrating greater robustness.

</details>


### [206] [USVTrack: USV-Based 4D Radar-Camera Tracking Dataset for Autonomous Driving in Inland Waterways](https://arxiv.org/abs/2506.18737)
*Shanliang Yao,Runwei Guan,Yi Ni,Sen Xu,Yong Yue,Xiaohui Zhu,Ryan Wen Liu*

Main category: cs.CV

TL;DR: This paper introduces USVTrack, the first 4D radar-camera dataset tailored for object tracking in waterborne autonomous navigation. It also presents a radar-camera matching method (RCM) to enhance tracking accuracy.


<details>
  <summary>Details</summary>
Motivation: To enable robust and accurate object tracking in complex waterborne environments for applications like transportation, monitoring, and rescue.

Method: The approach includes data collection using a USV equipped with 4D radar, cameras, GPS, and IMU. Additionally, the authors propose a radar-camera matching method called RCM for improving object tracking when used with two-stage association trackers.

Result: The radar-camera matching method (RCM) effectively enhances object tracking accuracy and reliability under diverse conditions such as different times of day, weather, and lighting.

Conclusion: The USVTrack dataset and the RCM method represent significant advancements in waterborne autonomous systems by providing comprehensive data and effective tracking techniques, aiding safety and efficiency.

Abstract: Object tracking in inland waterways plays a crucial role in safe and
cost-effective applications, including waterborne transportation, sightseeing
tours, environmental monitoring and surface rescue. Our Unmanned Surface
Vehicle (USV), equipped with a 4D radar, a monocular camera, a GPS, and an IMU,
delivers robust tracking capabilities in complex waterborne environments. By
leveraging these sensors, our USV collected comprehensive object tracking data,
which we present as USVTrack, the first 4D radar-camera tracking dataset
tailored for autonomous driving in new generation waterborne transportation
systems. Our USVTrack dataset presents rich scenarios, featuring diverse
various waterways, varying times of day, and multiple weather and lighting
conditions. Moreover, we present a simple but effective radar-camera matching
method, termed RCM, which can be plugged into popular two-stage association
trackers. Experimental results utilizing RCM demonstrate the effectiveness of
the radar-camera matching in improving object tracking accuracy and reliability
for autonomous driving in waterborne environments. The USVTrack dataset is
public on https://usvtrack.github.io.

</details>


### [207] [Enabling PSO-Secure Synthetic Data Sharing Using Diversity-Aware Diffusion Models](https://arxiv.org/abs/2506.17975)
*Mischa Dombrowski,Bernhard Kainz*

Main category: cs.CV

TL;DR: This paper introduces a framework leveraging diffusion models to create synthetic datasets that are privacy-protective (PSO-secure) and perform closely to real data.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of synthetic datasets in medical imaging, including legal concerns under regulations like GDPR and performance shortcomings compared to real data.

Method: The authors propose a generalizable framework utilizing diffusion models to generate synthetic datasets, emphasizing image diversity for both improved downstream performance and privacy protection (PSO-security).

Result: The proposed method produces synthetic datasets that achieve performance within 1% of real data models and outperform existing state-of-the-art synthetic methods while ensuring privacy compliance.

Conclusion: This work demonstrates the feasibility of generating robust and privacy-compliant synthetic datasets through diffusion models, offering a path forward for ethical and effective data sharing in sensitive domains.

Abstract: Synthetic data has recently reached a level of visual fidelity that makes it
nearly indistinguishable from real data, offering great promise for
privacy-preserving data sharing in medical imaging. However, fully synthetic
datasets still suffer from significant limitations: First and foremost, the
legal aspect of sharing synthetic data is often neglected and data regulations,
such as the GDPR, are largley ignored. Secondly, synthetic models fall short of
matching the performance of real data, even for in-domain downstream
applications. Recent methods for image generation have focused on maximising
image diversity instead of fidelity solely to improve the mode coverage and
therefore the downstream performance of synthetic data. In this work, we shift
perspective and highlight how maximizing diversity can also be interpreted as
protecting natural persons from being singled out, which leads to predicate
singling-out (PSO) secure synthetic datasets. Specifically, we propose a
generalisable framework for training diffusion models on personal data which
leads to unpersonal synthetic datasets achieving performance within one
percentage point of real-data models while significantly outperforming
state-of-the-art methods that do not ensure privacy. Our code is available at
https://github.com/MischaD/Trichotomy.

</details>


### [208] [Fast Neural Inverse Kinematics on Human Body Motions](https://arxiv.org/abs/2506.17996)
*David Tolpin,Sefy Kagarlitsky*

Main category: cs.CV

TL;DR: The paper introduces a real-time markerless motion capture system using a neural inverse kinematics framework.


<details>
  <summary>Details</summary>
Motivation: Markerless motion capture systems face challenges in real-time applications due to their high computational requirements, creating a need for a faster and more efficient solution.

Method: The authors design a novel neural inverse kinematics framework, detailing its architecture, training methods, and inference processes, while conducting ablation studies to justify design choices.

Result: The framework is evaluated through both qualitative and quantitative analyses, demonstrating its efficacy in capturing human motion in real-time.

Conclusion: The proposed framework achieves reliable real-time capture of human body motions from 3D keypoints, addressing existing limitations of markerless motion capture systems.

Abstract: Markerless motion capture enables the tracking of human motion without
requiring physical markers or suits, offering increased flexibility and reduced
costs compared to traditional systems. However, these advantages often come at
the expense of higher computational demands and slower inference, limiting
their applicability in real-time scenarios. In this technical report, we
present a fast and reliable neural inverse kinematics framework designed for
real-time capture of human body motions from 3D keypoints. We describe the
network architecture, training methodology, and inference procedure in detail.
Our framework is evaluated both qualitatively and quantitatively, and we
support key design decisions through ablation studies.

</details>


### [209] [SWA-SOP: Spatially-aware Window Attention for Semantic Occupancy Prediction in Autonomous Driving](https://arxiv.org/abs/2506.18785)
*Helin Cao,Rafael Materla,Sven Behnke*

Main category: cs.CV

TL;DR: This paper presents Spatially-aware Window Attention (SWA), a novel approach to improve autonomous driving perception by integrating local spatial context into transformer attention for Semantic Occupancy Prediction (SOP).


<details>
  <summary>Details</summary>
Motivation: Current perception systems for autonomous vehicles struggle with incomplete information due to occlusions and data sparsity. Existing transformer-based SOP methods lack efficient spatial structure modeling, which limits their ability to handle geometric challenges and perform accurately in sparse or occluded environments.

Method: The authors introduce Spatially-aware Window Attention (SWA), a mechanism that incorporates local spatial context into the attention process of transformers, enhancing geometric awareness for Semantic Occupancy Prediction tasks.

Result: SWA significantly improves scene completion and achieves state-of-the-art performance on LiDAR-based SOP benchmarks. Additionally, it demonstrates its generality and efficacy by providing consistent performance gains when applied to camera-based SOP pipelines.

Conclusion: The proposed SWA effectively addresses the limitations of existing SOP methods, offering a robust solution that enhances performance across sensor types and improves geometric awareness in challenging scenarios for autonomous driving applications.

Abstract: Perception systems in autonomous driving rely on sensors such as LiDAR and
cameras to perceive the 3D environment. However, due to occlusions and data
sparsity, these sensors often fail to capture complete information. Semantic
Occupancy Prediction (SOP) addresses this challenge by inferring both occupancy
and semantics of unobserved regions. Existing transformer-based SOP methods
lack explicit modeling of spatial structure in attention computation, resulting
in limited geometric awareness and poor performance in sparse or occluded
areas. To this end, we propose Spatially-aware Window Attention (SWA), a novel
mechanism that incorporates local spatial context into attention. SWA
significantly improves scene completion and achieves state-of-the-art results
on LiDAR-based SOP benchmarks. We further validate its generality by
integrating SWA into a camera-based SOP pipeline, where it also yields
consistent gains across modalities.

</details>


### [210] [OSDMamba: Enhancing Oil Spill Detection from Remote Sensing Images Using Selective State Space Model](https://arxiv.org/abs/2506.18006)
*Shuaiyu Chen,Fu Wang,Peng Ren,Chunbo Luo,Zeyu Fu*

Main category: cs.CV

TL;DR: The paper introduces OSDMamba, a novel Mamba-based architecture for enhanced oil spill detection in remote sensing images, addressing challenges like class imbalance and small spill area detection, achieving state-of-the-art performance improvements.


<details>
  <summary>Details</summary>
Motivation: Effective oil spill detection faces challenges due to the limited availability of labelled samples, class imbalance, and poor detection of small spill areas by CNN-based methods.

Method: The study proposes OSDMamba, which utilizes the Mamba architecture's selective scanning mechanism to expand the receptive field and incorporates an asymmetric decoder with ConvSSM and deep supervision for multi-scale feature fusion.

Result: OSDMamba demonstrated improvements of 8.9% and 11.8% in oil spill detection accuracy across two datasets, outperforming existing methods.

Conclusion: State-Space Models, specifically OSDMamba, address limitations of traditional CNN-based oil spill detection methods, offering enhanced detection accuracy and sensitivity to small spill areas.

Abstract: Semantic segmentation is commonly used for Oil Spill Detection (OSD) in
remote sensing images. However, the limited availability of labelled oil spill
samples and class imbalance present significant challenges that can reduce
detection accuracy. Furthermore, most existing methods, which rely on
convolutional neural networks (CNNs), struggle to detect small oil spill areas
due to their limited receptive fields and inability to effectively capture
global contextual information. This study explores the potential of State-Space
Models (SSMs), particularly Mamba, to overcome these limitations, building on
their recent success in vision applications. We propose OSDMamba, the first
Mamba-based architecture specifically designed for oil spill detection.
OSDMamba leverages Mamba's selective scanning mechanism to effectively expand
the model's receptive field while preserving critical details. Moreover, we
designed an asymmetric decoder incorporating ConvSSM and deep supervision to
strengthen multi-scale feature fusion, thereby enhancing the model's
sensitivity to minority class samples. Experimental results show that the
proposed OSDMamba achieves state-of-the-art performance, yielding improvements
of 8.9% and 11.8% in OSD across two publicly available datasets.

</details>


### [211] [OC-SOP: Enhancing Vision-Based 3D Semantic Occupancy Prediction by Object-Centric Awareness](https://arxiv.org/abs/2506.18798)
*Helin Cao,Sven Behnke*

Main category: cs.CV

TL;DR: The paper introduces OC-SOP, a framework integrating object-centric cues into the semantic occupancy prediction process to enhance autonomous driving perception.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of occlusions and incomplete scene data, particularly for dynamic foreground objects, in autonomous driving systems.

Method: Developing OC-SOP, which incorporates a detection branch to extract object-centric cues and integrates them into the semantic occupancy prediction pipeline.

Result: OC-SOP shows state-of-the-art performance in predicting semantic occupancy, especially improving accuracy for foreground objects in evaluations on SemanticKITTI.

Conclusion: OC-SOP effectively tackles limitations of conventional methods, enhancing prediction for dynamic foreground objects, and advancing SOP in autonomous driving.

Abstract: Autonomous driving perception faces significant challenges due to occlusions
and incomplete scene data in the environment. To overcome these issues, the
task of semantic occupancy prediction (SOP) is proposed, which aims to jointly
infer both the geometry and semantic labels of a scene from images. However,
conventional camera-based methods typically treat all categories equally and
primarily rely on local features, leading to suboptimal predictions, especially
for dynamic foreground objects. To address this, we propose Object-Centric SOP
(OC-SOP), a framework that integrates high-level object-centric cues extracted
via a detection branch into the semantic occupancy prediction pipeline. This
object-centric integration significantly enhances the prediction accuracy for
foreground objects and achieves state-of-the-art performance among all
categories on SemanticKITTI.

</details>


### [212] [On the Robustness of Human-Object Interaction Detection against Distribution Shift](https://arxiv.org/abs/2506.18021)
*Chi Xie,Shuang Liang,Jie Li,Feng Zhu,Rui Zhao,Yichen Wei,Shengjie Zhao*

Main category: cs.CV

TL;DR: The paper improves the robustness of Human-Object Interaction (HOI) detection models under distribution shifts by proposing a new benchmark, analyzing model performance, and introducing two augmentation techniques.


<details>
  <summary>Details</summary>
Motivation: To address the issue of HOI detection models struggling in practical scenarios with inevitable distribution shifts, which affects their real-world applicability.

Method: The authors created an automated robustness evaluation benchmark, analyzed over 40 existing models to identify weaknesses, and proposed two enhancements: cross-domain data augmentation with mixup and a feature fusion strategy incorporating frozen vision foundation models.

Result: The proposed approach significantly improves the robustness of various HOI detection models, as demonstrated by experimental results on both the robustness benchmark and standard benchmarks.

Conclusion: Enhancing the robustness of HOI detection models makes them more applicable in real-world scenarios, with the methods being simple, widely applicable, and effective. The dataset and code will be shared publicly.

Abstract: Human-Object Interaction (HOI) detection has seen substantial advances in
recent years. However, existing works focus on the standard setting with ideal
images and natural distribution, far from practical scenarios with inevitable
distribution shifts. This hampers the practical applicability of HOI detection.
In this work, we investigate this issue by benchmarking, analyzing, and
enhancing the robustness of HOI detection models under various distribution
shifts. We start by proposing a novel automated approach to create the first
robustness evaluation benchmark for HOI detection. Subsequently, we evaluate
more than 40 existing HOI detection models on this benchmark, showing their
insufficiency, analyzing the features of different frameworks, and discussing
how the robustness in HOI is different from other tasks. With the insights from
such analyses, we propose to improve the robustness of HOI detection methods
through: (1) a cross-domain data augmentation integrated with mixup, and (2) a
feature fusion strategy with frozen vision foundation models. Both are simple,
plug-and-play, and applicable to various methods. Our experimental results
demonstrate that the proposed approach significantly increases the robustness
of various methods, with benefits on standard benchmarks, too. The dataset and
code will be released.

</details>


### [213] [PP-DocBee2: Improved Baselines with Efficient Data for Multimodal Document Understanding](https://arxiv.org/abs/2506.18023)
*Kui Huang,Xinrong Chen,Wenyu Lv,Jincheng Liao,Guanzhong Wang,Yi Liu*

Main category: cs.CV

TL;DR: PP-DocBee2 is an upgraded multimodal model enhancing document understanding, offering significant performance improvements and latency reduction.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of PP-DocBee and enhance multimodal document understanding using advanced strategies.

Method: Utilized enhanced synthetic data quality, innovative visual feature fusion, and leveraged statistical filtering and ViT decomposition.

Result: Achieved an 11.4% performance boost on benchmarks and 73.0% reduction in inference latency for Chinese business documents.

Conclusion: PP-DocBee2 demonstrates superior efficiency and reasoning abilities in multimodal document tasks with public access to source code and model.

Abstract: This report introduces PP-DocBee2, an advanced version of the PP-DocBee,
designed to enhance multimodal document understanding. Built on a large
multimodal model architecture, PP-DocBee2 addresses the limitations of its
predecessor through key technological improvements, including enhanced
synthetic data quality, improved visual feature fusion strategy, and optimized
inference methodologies. These enhancements yield an $11.4\%$ performance boost
on internal benchmarks for Chinese business documents, and reduce inference
latency by $73.0\%$ to the vanilla version. A key innovation of our work is a
data quality optimization strategy for multimodal document tasks. By employing
a large-scale multimodal pre-trained model to evaluate data, we apply a novel
statistical criterion to filter outliers, ensuring high-quality training data.
Inspired by insights into underutilized intermediate features in multimodal
models, we enhance the ViT representational capacity by decomposing it into
layers and applying a novel feature fusion strategy to improve complex
reasoning. The source code and pre-trained model are available at
\href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}.

</details>


### [214] [MiCo: Multiple Instance Learning with Context-Aware Clustering for Whole Slide Image Analysis](https://arxiv.org/abs/2506.18028)
*Junjian Li,Hulin Kuang,Jin Liu,Hailin Yue,Mengshen He,Jianxin Wang*

Main category: cs.CV

TL;DR: The paper introduces a MIL method called MiCo to address spatial heterogeneity in histopathology images using clustering techniques to improve morphological pattern recognition and cross-regional tissue associations.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from limitations in existing MIL methods to handle spatial heterogeneity and distant tissue interactions in WSIs, which are critical for accurate cancer diagnosis and prognosis.

Method: The proposed method, MiCo, employs context-aware clustering, with semantic anchors for intra-tissue correlation and cross-tissue interactions, combining Cluster Route and Cluster Reducer modules for enhanced representations.

Result: Extensive experiments across 9 public cancer datasets validate MiCo's effectiveness and superiority over state-of-the-art methods.

Conclusion: MiCo offers a robust framework for analyzing WSIs by overcoming spatial and semantic tissue fragmentation, aiding better diagnostic and prognostic outcomes.

Abstract: Multiple instance learning (MIL) has shown significant promise in
histopathology whole slide image (WSI) analysis for cancer diagnosis and
prognosis. However, the inherent spatial heterogeneity of WSIs presents
critical challenges, as morphologically similar tissue types are often
dispersed across distant anatomical regions. Conventional MIL methods struggle
to model these scattered tissue distributions and capture cross-regional
spatial interactions effectively. To address these limitations, we propose a
novel Multiple instance learning framework with Context-Aware Clustering
(MiCo), designed to enhance cross-regional intra-tissue correlations and
strengthen inter-tissue semantic associations in WSIs. MiCo begins by
clustering instances to distill discriminative morphological patterns, with
cluster centroids serving as semantic anchors. To enhance cross-regional
intra-tissue correlations, MiCo employs a Cluster Route module, which
dynamically links instances of the same tissue type across distant regions via
feature similarity. These semantic anchors act as contextual hubs, propagating
semantic relationships to refine instance-level representations. To eliminate
semantic fragmentation and strengthen inter-tissue semantic associations, MiCo
integrates a Cluster Reducer module, which consolidates redundant anchors while
enhancing information exchange between distinct semantic groups. Extensive
experiments on two challenging tasks across nine large-scale public cancer
datasets demonstrate the effectiveness of MiCo, showcasing its superiority over
state-of-the-art methods. The code is available at
https://github.com/junjianli106/MiCo.

</details>


### [215] [Pre-Trained LLM is a Semantic-Aware and Generalizable Segmentation Booster](https://arxiv.org/abs/2506.18034)
*Fenghe Tang,Wenxin Ma,Zhiyang He,Xiaodong Tao,Zihang Jiang,S. Kevin Zhou*

Main category: cs.CV

TL;DR: Frozen pre-trained LLM layers enhance medical image segmentation tasks when integrated into a CNN encoder-decoder framework.


<details>
  <summary>Details</summary>
Motivation: To explore whether pre-trained LLM layers, known for semantic processing in language tasks, can improve performance in medical image segmentation.

Method: The study incorporates frozen pre-trained LLM layers into a CNN encoder-decoder segmentation framework (LLM4Seg) and evaluates performance across diverse medical imaging modalities.

Result: The proposed framework enhances segmentation performance with minimal additional trainable parameters across multiple modalities like ultrasound, dermoscopy, polyp, and CT scans.

Conclusion: Integrating pre-trained LLM layers transfers semantic awareness to image segmentation tasks, achieving robust performance improvements across different LLMs and imaging types.

Abstract: With the advancement of Large Language Model (LLM) for natural language
processing, this paper presents an intriguing finding: a frozen pre-trained LLM
layer can process visual tokens for medical image segmentation tasks.
Specifically, we propose a simple hybrid structure that integrates a
pre-trained, frozen LLM layer within the CNN encoder-decoder segmentation
framework (LLM4Seg). Surprisingly, this design improves segmentation
performance with a minimal increase in trainable parameters across various
modalities, including ultrasound, dermoscopy, polypscopy, and CT scans. Our
in-depth analysis reveals the potential of transferring LLM's semantic
awareness to enhance segmentation tasks, offering both improved global
understanding and better local modeling capabilities. The improvement proves
robust across different LLMs, validated using LLaMA and DeepSeek.

</details>


### [216] [CmFNet: Cross-modal Fusion Network for Weakly-supervised Segmentation of Medical Images](https://arxiv.org/abs/2506.18042)
*Dongdong Meng,Sheng Li,Hao Wu,Suqing Tian,Wenjun Ma,Guoping Wang,Xueqing Yan*

Main category: cs.CV

TL;DR: The paper introduces CmFNet, a 3D weakly supervised medical image segmentation framework designed to work with sparse annotations. It integrates modality-specific and cross-modal feature learning alongside a hybrid-supervised strategy and demonstrates superior segmentation performance.


<details>
  <summary>Details</summary>
Motivation: High-quality medical image segmentation traditionally requires dense annotations, which are costly and inefficient. Weakly supervised learning can leverage sparse annotations but faces challenges like overfitting and degraded performance.

Method: CmFNet incorporates modality-specific feature learning, cross-modal feature learning, and hybrid-supervised strategies that utilize scribble supervision, intra-modal regularization, and inter-modal consistency to enhance segmentation accuracy.

Result: CmFNet achieves state-of-the-art performance on nasopharyngeal carcinoma and public CT datasets, outperforming both weakly supervised and fully supervised methods in certain scenarios.

Conclusion: The proposed approach significantly mitigates overfitting, improves segmentation for challenging and common structures, and demonstrates clinical applicability, benefiting experts across medical disciplines.

Abstract: Accurate automatic medical image segmentation relies on high-quality, dense
annotations, which are costly and time-consuming. Weakly supervised learning
provides a more efficient alternative by leveraging sparse and coarse
annotations instead of dense, precise ones. However, segmentation performance
degradation and overfitting caused by sparse annotations remain key challenges.
To address these issues, we propose CmFNet, a novel 3D weakly supervised
cross-modal medical image segmentation approach. CmFNet consists of three main
components: a modality-specific feature learning network, a cross-modal feature
learning network, and a hybrid-supervised learning strategy. Specifically, the
modality-specific feature learning network and the cross-modal feature learning
network effectively integrate complementary information from multi-modal
images, enhancing shared features across modalities to improve segmentation
performance. Additionally, the hybrid-supervised learning strategy guides
segmentation through scribble supervision, intra-modal regularization, and
inter-modal consistency, modeling spatial and contextual relationships while
promoting feature alignment. Our approach effectively mitigates overfitting,
delivering robust segmentation results. It excels in segmenting both
challenging small tumor regions and common anatomical structures. Extensive
experiments on a clinical cross-modal nasopharyngeal carcinoma (NPC) dataset
(including CT and MR imaging) and the publicly available CT Whole Abdominal
Organ dataset (WORD) show that our approach outperforms state-of-the-art weakly
supervised methods. In addition, our approach also outperforms fully supervised
methods when full annotation is used. Our approach can facilitate clinical
therapy and benefit various specialists, including physicists, radiologists,
pathologists, and oncologists.

</details>


### [217] [CLGRPO: Reasoning Ability Enhancement for Small VLMs](https://arxiv.org/abs/2506.18048)
*Fanyi Wang,Binzhi Dong,Haotian Hu,Jinjin Xu,Zhiwang Zhang*

Main category: cs.CV

TL;DR: This paper proposes an Incremental Training Strategy to boost reasoning in Small Vision Language Models (SVLMs), achieving performance rivaling larger models.


<details>
  <summary>Details</summary>
Motivation: To enhance reasoning capabilities of Small Vision Language Models (SVLMs) with ≤2B parameters, overcoming limitations due to smaller model sizes.

Method: The approach involves a 4-stage Incremental Training Strategy utilizing self-supervised Chain-of-Thought (COT) data and fine-tuning with Group Relative Policy Optimization (GRPO) at various stages.

Result: Significant reasoning improvements for 1B SVLMs were achieved, with accuracy increased by 2.77 and recall by 0.69, showcasing performance comparable to 8B models.

Conclusion: The Incremental Training Strategy effectively boosts reasoning ability in low-parameter SVLMs, making them commercially valuable alternatives to larger models.

Abstract: Small Vision Language Models (SVLMs) generally refer to models with parameter
sizes less than or equal to 2B. Their low cost and power consumption
characteristics confer high commercial value. However, their reasoning
abilities are limited by the number of parameters. To address this issue, this
paper proposes a post-training optimization paradigm called the Incremental
Training Strategy to enhance the reasoning ability of SVLMs. Firstly, we
constructed a Self-Supervised Chain-of-Thought (COT) Data Construction System,
which leverages multiple LVLMs with 7B parameters or more to transform original
data into COT data in a self-supervised manner. Our proposed Incremental
Training Strategy consists of four stages. Stage 1 injects domain knowledge by
performing Supervised Fine-Tuning (SFT) to the pretrained model on the COT
data. Stage 2 aligns the COT data format by conducting a small amount of Group
Relative Policy Optimization (GRPO) training constrained only by format rewards
on the COT data. Stage 3 enhances reasoning ability by applying GRPO training
on the COT data with constraints on both format and accuracy rewards. The
resulting model shows significant improvement compared to the baseline. Stage 4
addresses the limited capacity of the SVLMs and the weak ability to capture
complex patterns by proposing ClipLow GRPO (CLGRPO) to constrain the capture
space of the training process. We conducted extensive comparative and ablation
experiments on the abstract semantic recognition dataset EMOSet-118K.
Experimental results demonstrate that our method significantly improves the
reasoning ability of 1B SVLM. Compared to the baseline model fine-tuned on the
original data, accuracy increased by 2.77 and recall by 0.69, achieving
performance comparable to that of 8B models.

</details>


### [218] [Deep Supervised LSTM for 3D morphology estimation from Multi-View RGB Images of Wheat Spikes](https://arxiv.org/abs/2506.18060)
*Olivia Zumsteg,Nico Graf,Aaron Haeusler,Norbert Kirchgessner,Nicola Storni,Lukas Roth,Andreas Hund*

Main category: cs.CV

TL;DR: The paper proposes a neural network model to estimate wheat spike volume using 2D RGB images, leveraging DINOv2 and LSTM for improved accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of estimating 3D morphological traits from 2D RGB images, focusing on wheat spikes with complex geometries under field conditions.

Method: The method involves a neural network combining DINOv2 (Vision Transformer) and LSTM, using deep supervision for robustness. Ground truth is established from structured-light 3D scans.

Result: The deep supervised model achieves superior accuracy with a mean absolute percentage error (MAPE) of 6.46% for indoor images and adapts to field conditions with a MAPE of 10.82%, outperforming traditional geometric and area-based approaches.

Conclusion: The approach effectively improves volume estimation accuracy for irregular geometries like wheat spikes, making it more reliable than conventional geometric methods.

Abstract: Estimating three-dimensional morphological traits from two-dimensional RGB
images presents inherent challenges due to the loss of depth information,
projection distortions, and occlusions under field conditions. In this work, we
explore multiple approaches for non-destructive volume estimation of wheat
spikes, using RGB image sequences and structured-light 3D scans as ground truth
references. Due to the complex geometry of the spikes, we propose a neural
network approach for volume estimation in 2D images, employing a transfer
learning pipeline that combines DINOv2, a self-supervised Vision Transformer,
with a unidirectional Long Short-Term Memory (LSTM) network. By using deep
supervision, the model is able to learn more robust intermediate
representations, which enhances its generalisation ability across varying
evaluation sequences. We benchmark our model against two conventional
baselines: a 2D area-based projection and a geometric reconstruction using
axis-aligned cross-sections. Our deep supervised model achieves a mean absolute
percentage error (MAPE) of 6.46% on six-view indoor images, outperforming the
area (9.36%) and geometric (13.98%) baselines. Fine-tuning the model on
field-based single-image data enables domain adaptation, yielding a MAPE of
10.82%. We demonstrate that object shape significantly impacts volume
prediction accuracy, with irregular geometries such as wheat spikes posing
greater challenges for geometric methods compared to our deep learning
approach.

</details>


### [219] [Training-free Test-time Improvement for Explainable Medical Image Classification](https://arxiv.org/abs/2506.18070)
*Hangzhou He,Jiachen Tang,Lei Zhu,Kaiwen Li,Yanye Lu*

Main category: cs.CV

TL;DR: A novel training-free technique is proposed to improve Concept Bottleneck Models' (CBMs) performance on out-of-domain medical image classifications without requiring expensive concept annotations.


<details>
  <summary>Details</summary>
Motivation: Developing accurate and interpretable medical image classification models for diverse clinical scenarios is challenging due to variations in imaging protocols and the high cost of concept annotations.

Method: The approach uses minimal new data (4 images per class) with image-level labels to mask misactivated confounding concepts and amplify under-activated discriminative concepts, improving out-of-domain performance without sacrificing source domain accuracy.

Result: Validation on skin and white blood cell images demonstrates the efficacy of the proposed method, enhancing model robustness and adaptability.

Conclusion: The proposed strategy effectively addresses challenges of CBMs in new environments, offering a practical and efficient solution to enhance accuracy and explainability for medical image analysis.

Abstract: Deep learning-based medical image classification techniques are rapidly
advancing in medical image analysis, making it crucial to develop accurate and
trustworthy models that can be efficiently deployed across diverse clinical
scenarios. Concept Bottleneck Models (CBMs), which first predict a set of
explainable concepts from images and then perform classification based on these
concepts, are increasingly being adopted for explainable medical image
classification. However, the inherent explainability of CBMs introduces new
challenges when deploying trained models to new environments. Variations in
imaging protocols and staining methods may induce concept-level shifts, such as
alterations in color distribution and scale. Furthermore, since CBM training
requires explicit concept annotations, fine-tuning models solely with
image-level labels could compromise concept prediction accuracy and
faithfulness - a critical limitation given the high cost of acquiring
expert-annotated concept labels in medical domains. To address these
challenges, we propose a training-free confusion concept identification
strategy. By leveraging minimal new data (e.g., 4 images per class) with only
image-level labels, our approach enhances out-of-domain performance without
sacrificing source domain accuracy through two key operations: masking
misactivated confounding concepts and amplifying under-activated discriminative
concepts. The efficacy of our method is validated on both skin and white blood
cell images. Our code is available at:
https://github.com/riverback/TF-TTI-XMed.

</details>


### [220] [MUPA: Towards Multi-Path Agentic Reasoning for Grounded Video Question Answering](https://arxiv.org/abs/2506.18071)
*Jisheng Dang,Huilin Song,Junbin Xiao,Bimei Wang,Han Peng,Haoxuan Li,Xun Yang,Meng Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: The paper introduces MUPA, a novel method for improving grounded video question answering by enhancing the alignment between textual answers and visual evidence, outperforming existing methods despite using fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Current multimodal models for Grounded VideoQA depend excessively on linguistic biases and spurious correlations, leading to weakly grounded outputs. The paper seeks to develop a more robust method to address these shortcomings.

Method: MUPA, a multi-path agent-based approach, combines video grounding, question answering, and answer reflection through three reasoning paths in various chronological orders. A dedicated reflection agent aggregates the reasoning outcomes for consistent grounding and answer accuracy.

Result: MUPA outperforms methods with 7B-scale parameters despite using only 2B parameters initially. When scaled to 7B, it achieves state-of-the-art results, including Acc@GQA of 30.3% on NExT-GQA and 47.4% on DeVE-QA.

Conclusion: The proposed MUPA approach demonstrates significant improvements in grounding fidelity and answer accuracy, establishing itself as a leading method for trustworthy video-language understanding.

Abstract: Grounded Video Question Answering (Grounded VideoQA) requires aligning
textual answers with explicit visual evidence. However, modern multimodal
models often rely on linguistic priors and spurious correlations, resulting in
poorly grounded predictions. In this work, we propose MUPA, a cooperative
MUlti-Path Agentic approach that unifies video grounding, question answering,
answer reflection and aggregation to tackle Grounded VideoQA. MUPA features
three distinct reasoning paths on the interplay of grounding and QA agents in
different chronological orders, along with a dedicated reflection agent to
judge and aggregate the multi-path results to accomplish consistent QA and
grounding. This design markedly improves grounding fidelity without sacrificing
answer accuracy. Despite using only 2B parameters, our method outperforms all
7B-scale competitors. When scaled to 7B parameters, MUPA establishes new
state-of-the-art results, with Acc@GQA of 30.3% and 47.4% on NExT-GQA and
DeVE-QA respectively, demonstrating MUPA' effectiveness towards trustworthy
video-language understanding. Our code is available in
https://github.com/longmalongma/MUPA.

</details>


### [221] [TEM^3-Learning: Time-Efficient Multimodal Multi-Task Learning for Advanced Assistive Driving](https://arxiv.org/abs/2506.18084)
*Wenzhuo Liu,Yicheng Qiao,Zhen Wang,Qiannan Guo,Zilong Chen,Meihua Zhou,Xinran Li,Letian Wang,Zhiwei Li,Huaping Liu,Wenshuo Wang*

Main category: cs.CV

TL;DR: This paper proposes TEM^3-Learning, a new multimodal multi-task learning framework for assistive driving, achieving state-of-the-art accuracy and real-time efficiency.


<details>
  <summary>Details</summary>
Motivation: To address current limitations in assistive driving systems, namely single-modality constraints that hinder comprehensive scene understanding and inefficient architectures that are unsuitable for real-time deployment.

Method: The method involves a novel two-stage architecture. First, the MTS-Mamba subnetwork uses a temporal scanning mechanism and spatial attention to extract temporal-spatial features from multi-view images. Second, the MGMI component utilizes task-specific gating modules to select relevant modality features and mitigate negative transfer in multi-task learning.

Result: TEM^3-Learning achieves state-of-the-art performance across four tasks: driver emotion recognition, driver behavior recognition, traffic context recognition, and vehicle behavior recognition. It is computationally efficient with under 6 million parameters and an inference speed of 142.32 FPS.

Conclusion: The proposed TEM^3-Learning framework effectively addresses the identified limitations, advancing assistive driving through a lightweight, efficient, and high-performing model. Results are validated through ablation studies, demonstrating the framework's robustness.

Abstract: Multi-task learning (MTL) can advance assistive driving by exploring
inter-task correlations through shared representations. However, existing
methods face two critical limitations: single-modality constraints limiting
comprehensive scene understanding and inefficient architectures impeding
real-time deployment. This paper proposes TEM^3-Learning (Time-Efficient
Multimodal Multi-task Learning), a novel framework that jointly optimizes
driver emotion recognition, driver behavior recognition, traffic context
recognition, and vehicle behavior recognition through a two-stage architecture.
The first component, the mamba-based multi-view temporal-spatial feature
extraction subnetwork (MTS-Mamba), introduces a forward-backward temporal
scanning mechanism and global-local spatial attention to efficiently extract
low-cost temporal-spatial features from multi-view sequential images. The
second component, the MTL-based gated multimodal feature integrator (MGMI),
employs task-specific multi-gating modules to adaptively highlight the most
relevant modality features for each task, effectively alleviating the negative
transfer problem in MTL. Evaluation on the AIDE dataset, our proposed model
achieves state-of-the-art accuracy across all four tasks, maintaining a
lightweight architecture with fewer than 6 million parameters and delivering an
impressive 142.32 FPS inference speed. Rigorous ablation studies further
validate the effectiveness of the proposed framework and the independent
contributions of each module. The code is available on
https://github.com/Wenzhuo-Liu/TEM3-Learning.

</details>


### [222] [ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image Generation](https://arxiv.org/abs/2506.18095)
*Junying Chen,Zhenyang Cai,Pengcheng Chen,Shunian Chen,Ke Ji,Xidong Wang,Yunjin Yang,Benyou Wang*

Main category: cs.CV

TL;DR: This paper introduces ShareGPT-4o-Image, a public dataset for text-to-image and text-and-image-to-image tasks, and Janus-4o, a powerful multimodal model trained using this data.


<details>
  <summary>Details</summary>
Motivation: To make advanced text-to-image and text-and-image-to-image generation capabilities accessible to researchers, as current leading systems are proprietary.

Method: The authors release ShareGPT-4o-Image, a dataset containing 91K synthetic samples generated via GPT-4o. They train Janus-4o, a multimodal model, using this dataset on an 8 A800-GPU machine for 6 hours.

Result: Janus-4o significantly improves upon its predecessor in text-to-image generation and introduces effective text-and-image-to-image capabilities, achieving strong results with minimal training resources.

Conclusion: The release of ShareGPT-4o-Image and Janus-4o should advance open research in photorealistic image generation, making such technologies more accessible and scalable.

Abstract: Recent advances in multimodal generative models have unlocked photorealistic,
instruction-aligned image generation, yet leading systems like GPT-4o-Image
remain proprietary and inaccessible. To democratize these capabilities, we
present ShareGPT-4o-Image, the first dataset comprising 45K text-to-image and
46K text-and-image-to-image data, all synthesized using GPT-4o's image
generation capabilities for distilling its advanced image generation abilities.
Leveraging this dataset, we develop Janus-4o, a multimodal large language model
capable of both text-to-image and text-and-image-to-image generation. Janus-4o
not only significantly improves text-to-image generation over its predecessor,
Janus-Pro, but also newly supports text-and-image-to-image generation. Notably,
it achieves impressive performance in text-and-image-to-image generation from
scratch, using only 91K synthetic samples and 6 hours of training on an 8
A800-GPU machine. We hope the release of ShareGPT-4o-Image and Janus-4o will
foster open research in photorealistic, instruction-aligned image generation.

</details>


### [223] [Enhancing VICReg: Random-Walk Pairing for Improved Generalization and Better Global Semantics Capturing](https://arxiv.org/abs/2506.18104)
*Idan Simai,Ronen Talmon,Uri Shaham*

Main category: cs.CV

TL;DR: The paper introduces SAG-VICReg, an enhancement of VICReg for self-supervised learning, to better generalize to unseen data while maintaining competitive performance.


<details>
  <summary>Details</summary>
Motivation: Addressing VICReg's limitation in generalizing robustly to unseen data due to overreliance on training data.

Method: SAG-VICReg improves VICReg by adopting new training techniques to enhance global semantic understanding and generalization capabilities.

Result: SAG-VICReg surpasses state-of-the-art SSL baselines in global semantic metrics while staying competitive on local metrics.

Conclusion: SAG-VICReg offers a significant improvement in robustness and generalization, along with a new evaluation metric for unlabeled data.

Abstract: In this paper, we argue that viewing VICReg-a popular self-supervised
learning (SSL) method--through the lens of spectral embedding reveals a
potential source of sub-optimality: it may struggle to generalize robustly to
unseen data due to overreliance on the training data. This observation invites
a closer look at how well this method achieves its goal of producing meaningful
representations of images outside of the training set as well. Here, we
investigate this issue and introduce SAG-VICReg (Stable and Generalizable
VICReg), a method that builds on VICReg by incorporating new training
techniques. These enhancements improve the model's ability to capture global
semantics within the data and strengthen the generalization capabilities.
Experiments demonstrate that SAG-VICReg effectively addresses the
generalization challenge while matching or surpassing diverse state-of-the-art
SSL baselines. Notably, our method exhibits superior performance on metrics
designed to evaluate global semantic understanding, while simultaneously
maintaining competitive results on local evaluation metrics. Furthermore, we
propose a new standalone evaluation metric for embeddings that complements the
standard evaluation methods and accounts for the global data structure without
requiring labels--a key issue when tagged data is scarce or not available.

</details>


### [224] [Targeted False Positive Synthesis via Detector-guided Adversarial Diffusion Attacker for Robust Polyp Detection](https://arxiv.org/abs/2506.18134)
*Quan Zhou,Gan Luo,Qiang Hu,Qingyong Zhang,Jinhua Zhang,Yinjiao Tian,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: This paper presents an adversarial diffusion framework for synthesizing high-value false positives to improve polyp detection in colorectal cancer screening, achieving significant improvements in detection performance.


<details>
  <summary>Details</summary>
Motivation: Existing polyp detection models are limited by the scale and diversity of available data and struggle with the issue of false positives. Enhancing data diversity and addressing false positives can significantly benefit colorectal cancer screening.

Method: The authors propose a two-part adversarial diffusion framework: (1) A regional noise matching strategy to train a negative-centric diffusion model by masking polyp regions, allowing the model to learn diverse background patterns. (2) A Detector-guided Adversarial Diffusion Attacker (DADA) module that disrupts a detector's decision, ensuring the model generates false positives that specifically confuse detection models.

Result: The proposed method outperformed existing state-of-the-art approaches, improving detection F1-scores by at least 2.6% and 2.7% on public and in-house datasets, respectively.

Conclusion: The paper establishes a novel paradigm for synthesizing high-value false positives in lesion detection, offering a powerful approach to enhance the reliability of polyp detection models in clinical applications.

Abstract: Polyp detection is crucial for colorectal cancer screening, yet existing
models are limited by the scale and diversity of available data. While
generative models show promise for data augmentation, current methods mainly
focus on enhancing polyp diversity, often overlooking the critical issue of
false positives. In this paper, we address this gap by proposing an adversarial
diffusion framework to synthesize high-value false positives. The extensive
variability of negative backgrounds presents a significant challenge in false
positive synthesis. To overcome this, we introduce two key innovations: First,
we design a regional noise matching strategy to construct a negative synthesis
space using polyp detection datasets. This strategy trains a negative-centric
diffusion model by masking polyp regions, ensuring the model focuses
exclusively on learning diverse background patterns. Second, we introduce the
Detector-guided Adversarial Diffusion Attacker (DADA) module, which perturbs
the negative synthesis process to disrupt a pre-trained detector's decision,
guiding the negative-centric diffusion model to generate high-value,
detector-confusing false positives instead of low-value, ordinary backgrounds.
Our approach is the first to apply adversarial diffusion to lesion detection,
establishing a new paradigm for targeted false positive synthesis and paving
the way for more reliable clinical applications in colorectal cancer screening.
Extensive results on public and in-house datasets verify the superiority of our
method over the current state-of-the-arts, with our synthesized data improving
the detectors by at least 2.6% and 2.7% in F1-score, respectively, over the
baselines. Codes are at https://github.com/Huster-Hq/DADA.

</details>


### [225] [See-in-Pairs: Reference Image-Guided Comparative Vision-Language Models for Medical Diagnosis](https://arxiv.org/abs/2506.18140)
*Ruinan Jin,Gexin Huang,Xinwei Shen,Qiong Zhang,Yan Shuo Tan,Xiaoxiao Li*

Main category: cs.CV

TL;DR: This paper explores the enhancement of medical diagnosis using general-purpose vision-language models (VLMs) by incorporating comparative reasoning with reference images, leading to improved diagnostic accuracy after supervised fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of current medical VLMs, which lack mechanisms for comparative reasoning and struggle with inter-patient variability, by integrating clinically-inspired comparative analysis from reference images.

Method: The authors provide general-purpose VLMs with both query and normative reference images, and design clinically-informed comparative prompts. They then apply supervised fine-tuning (SFT) to improve comparative performance in medical diagnosis.

Result: By using reference images and comparative prompts, the fine-tuned VLMs significantly outperformed single-image models in diagnostic accuracy across multiple medical VQA tasks.

Conclusion: The study validates the clinical importance of integrating comparative analysis in VLMs, offering novel approaches and empirical evidence that such strategies improve performance in medical diagnosis.

Abstract: Medical imaging diagnosis presents inherent challenges due to diseases that
mimic normal anatomy and exhibit significant inter-patient variability.
Clinicians routinely employ comparative reasoning-using reference images from
healthy controls or previous patient examinations-to discern subtle yet
diagnostically critical abnormalities. However, existing medical
vision-language models (VLMs) focus primarily on single-image or single-series
analyses and lack explicit mechanisms for comparative reasoning. Conversely,
general-purpose VLMs demonstrate strong multi-image comparative reasoning
capabilities but lack essential medical-domain knowledge to identify nuanced
clinical differences. This work aims to bridge this gap by exploring
clinically-inspired comparative analysis within VLMs, leveraging reference
images to enhance diagnostic accuracy. Through extensive empirical analysis, we
show that providing general-purpose VLMs with query and normative matched
reference images, accompanied by clinically-informed comparative prompts,
significantly improves diagnostic outcomes compared to single-image baselines,
especially after supervised finetuning (SFT). Our contributions highlight the
clinical relevance of comparative analysis introduce novel strategies for
leveraging reference images in VLMs, empirically demonstrate enhanced
performance across multiple medical visual question answering (VQA) tasks, and
provide theoretical insights into the efficacy of comparative image analysis in
medical diagnosis.

</details>


### [226] [Pattern-Based Phase-Separation of Tracer and Dispersed Phase Particles in Two-Phase Defocusing Particle Tracking Velocimetry](https://arxiv.org/abs/2506.18157)
*Christian Sax,Jochen Kriegseis*

Main category: cs.CV

TL;DR: This paper explores using convolutional neural networks (CNNs) to distinguish between phases in defocusing particle tracking velocimetry (DPTV) for two-phase flows, achieving high detection precision and classification accuracy using a single-camera setup and synthetic datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for phase separation in two-phase flows are often impractical due to constraints like wavelength and size dependencies. The authors aim to investigate a CNN-based post-processing alternative for improved 3D localization and phase identification using a single-camera approach.

Method: The study employs CNNs, specifically Faster R-CNN and YOLOv4, to classify defocused particle images based on their light-scattering patterns. A generative adversarial network is used to create labeled training datasets that simulate experiment-specific appearances to train the model effectively.

Result: The method achieves 95-100% precision and accuracy in detecting and classifying particles across six datasets, including synthetic two-phase flow data and real-world single- and two-phase flow cases, even under domain shifts.

Conclusion: CNNs provide a robust and practical solution for phase separation in DPTV for dispersed two-phase flows, especially in conditions where traditional methods are unsuitable.

Abstract: This work investigates the feasibility of a post-processing-based approach
for phase separation in defocusing particle tracking velocimetry for dispersed
two-phase flows. The method enables the simultaneous 3D localization
determination of both tracer particles and particles of the dispersed phase,
using a single-camera setup. The distinction between phases is based on pattern
differences in defocused particle images, which arise from distinct light
scattering behaviors of tracer particles and bubbles or droplets. Convolutional
neural networks, including Faster R-CNN and YOLOv4 variants, are trained to
detect and classify particle images based on these pattern features. To
generate large, labeled training datasets, a generative adversarial network
based framework is introduced, allowing the generation of auto-labeled data
that more closely reflects experiment-specific visual appearance. Evaluation
across six datasets, comprising synthetic two-phase and real single- and
two-phase flows, demonstrates high detection precision and classification
accuracy (95-100%), even under domain shifts. The results confirm the viability
of using CNNs for robust phase separation in disperse two-phase DPTV,
particularly in scenarios where traditional wavelength-, size-, or ensemble
correlation-based methods are impractical.

</details>


### [227] [CDG-MAE: Learning Correspondences from Diffusion Generated Views](https://arxiv.org/abs/2506.18164)
*Varun Belagali,Pierre Marza,Srikar Yellapragada,Zilinghan Li,Tarak Nath Nandi,Ravi K Madduri,Joel Saltz,Stergios Christodoulidis,Maria Vakalopoulou,Dimitris Samaras*

Main category: cs.CV

TL;DR: The paper introduces CDG-MAE, a novel self-supervised method leveraging diverse synthetic views generated using image-conditioned diffusion models to improve dense correspondence learning.


<details>
  <summary>Details</summary>
Motivation: Manual annotation for dense correspondence is tedious and unscalable, and existing self-supervised methods face challenges with effective training data that lack diversity and pose variations.

Method: The method uses an image-conditioned diffusion model to create diverse synthetic views from static images, complemented by a multi-anchor strategy for masked autoencoder-based training.

Result: CDG-MAE achieves significant improvements over state-of-the-art MAE methods using only images and narrows the performance gap with video-based approaches.

Conclusion: Synthetic pose and perspective variations using diffusion models provide a scalable solution for training dense correspondence tasks, bolstering self-supervised pretraining effectively.

Abstract: Learning dense correspondences, critical for application such as video label
propagation, is hindered by tedious and unscalable manual annotation.
Self-supervised methods address this by using a cross-view pretext task, often
modeled with a masked autoencoder, where a masked target view is reconstructed
from an anchor view. However, acquiring effective training data remains a
challenge - collecting diverse video datasets is difficult and costly, while
simple image crops lack necessary pose variations. This paper introduces
CDG-MAE, a novel MAE-based self-supervised method that uses diverse synthetic
views generated from static images via an image-conditioned diffusion model.
These generated views exhibit substantial changes in pose and perspective,
providing a rich training signal that overcomes the limitations of video and
crop-based anchors. We present a quantitative method to evaluate local and
global consistency of generated images, discussing their use for cross-view
self-supervised pretraining. Furthermore, we enhance the standard single-anchor
MAE setting to a multi-anchor strategy to effectively modulate the difficulty
of pretext task. CDG-MAE significantly outperforms state-of-the-art MAE methods
reliant only on images and substantially narrows the performance gap to
video-based approaches.

</details>


### [228] [STACT-Time: Spatio-Temporal Cross Attention for Cine Thyroid Ultrasound Time Series Classification](https://arxiv.org/abs/2506.18172)
*Irsyad Adam,Tengyue Zhang,Shrayes Raman,Zhuyu Qiu,Brandon Taraku,Hexiang Feng,Sile Wang,Ashwath Radhachandran,Shreeram Athreya,Vedrana Ivezic,Peipei Ping,Corey Arnold,William Speier*

Main category: cs.CV

TL;DR: This study introduces the STACT-Time model, a deep learning framework that improves thyroid cancer malignancy prediction by leveraging spatio-temporal features from ultrasound cine clips and segmentation masks, reducing unnecessary biopsies.


<details>
  <summary>Details</summary>
Motivation: To address unnecessary biopsies caused by the limitations of fine-needle aspiration and the interobserver variability in TI-RADS-based approaches, and to improve thyroid malignancy prediction.

Method: The paper proposes the STACT-Time model, which combines self-attention and cross-attention mechanisms with segmentation-guided learning to extract rich spatio-temporal features from ultrasound cine clips and segmentation masks.

Result: The model achieves a cross-validation precision of 0.91 and an F1 score of 0.89, outperforming state-of-the-art models while maintaining sensitivity for malignancy detection.

Conclusion: The STACT-Time model can enhance clinical decision-making by reliably predicting thyroid malignancies and reducing the number of unnecessary biopsies, thereby improving patient outcomes.

Abstract: Thyroid cancer is among the most common cancers in the United States. Thyroid
nodules are frequently detected through ultrasound (US) imaging, and some
require further evaluation via fine-needle aspiration (FNA) biopsy. Despite its
effectiveness, FNA often leads to unnecessary biopsies of benign nodules,
causing patient discomfort and anxiety. To address this, the American College
of Radiology Thyroid Imaging Reporting and Data System (TI-RADS) has been
developed to reduce benign biopsies. However, such systems are limited by
interobserver variability. Recent deep learning approaches have sought to
improve risk stratification, but they often fail to utilize the rich temporal
and spatial context provided by US cine clips, which contain dynamic global
information and surrounding structural changes across various views. In this
work, we propose the Spatio-Temporal Cross Attention for Cine Thyroid
Ultrasound Time Series Classification (STACT-Time) model, a novel
representation learning framework that integrates imaging features from US cine
clips with features from segmentation masks automatically generated by a
pretrained model. By leveraging self-attention and cross-attention mechanisms,
our model captures the rich temporal and spatial context of US cine clips while
enhancing feature representation through segmentation-guided learning. Our
model improves malignancy prediction compared to state-of-the-art models,
achieving a cross-validation precision of 0.91 (plus or minus 0.02) and an F1
score of 0.89 (plus or minus 0.02). By reducing unnecessary biopsies of benign
nodules while maintaining high sensitivity for malignancy detection, our model
has the potential to enhance clinical decision-making and improve patient
outcomes.

</details>


### [229] [DExNet: Combining Observations of Domain Adapted Critics for Leaf Disease Classification with Limited Data](https://arxiv.org/abs/2506.18173)
*Sabbir Ahmed,Md. Bakhtiar Hasan,Tasnim Ahmed,Md. Hasanul Kabir*

Main category: cs.CV

TL;DR: The paper introduces a few-shot learning model, DExNet, leveraging pre-trained CNN critic observations for effective plant disease classification, achieving strong accuracy with limited training data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of classifying plant leaf diseases with limited training samples, typically required by deep learning models.

Method: A few-shot learning approach named DExNet is presented, extracting feature embeddings from nine pre-trained CNN critics, adapting them with domain-specific datasets, and combining them with a Bi-LSTM-based classifier network.

Result: The model achieves high classification accuracies: 89.06% (5-shot), 92.46% (10-shot), 94.07% (15-shot), and 98.09% (80-shot), reducing training data needs by 94.5%. It surpasses existing methods in diverse scenarios.

Conclusion: DExNet demonstrates that few-shot learning can mitigate data limitations, effectively classifying plant diseases with minimal training data while maintaining competitive accuracies.

Abstract: While deep learning-based architectures have been widely used for correctly
detecting and classifying plant diseases, they require large-scale datasets to
learn generalized features and achieve state-of-the-art performance. This poses
a challenge for such models to obtain satisfactory performance in classifying
leaf diseases with limited samples. This work proposes a few-shot learning
framework, Domain-adapted Expert Network (DExNet), for plant disease
classification that compensates for the lack of sufficient training data by
combining observations of a number of expert critics. It starts with extracting
the feature embeddings as 'observations' from nine 'critics' that are
state-of-the-art pre-trained CNN-based architectures. These critics are 'domain
adapted' using a publicly available leaf disease dataset having no overlapping
classes with the specific downstream task of interest. The observations are
then passed to the 'Feature Fusion Block' and finally to a classifier network
consisting of Bi-LSTM layers. The proposed pipeline is evaluated on the 10
classes of tomato leaf images from the PlantVillage dataset, achieving
promising accuracies of 89.06%, 92.46%, and 94.07%, respectively, for 5-shot,
10-shot, and 15-shot classification. Furthermore, an accuracy of 98.09+-0.7%
has been achieved in 80-shot classification, which is only 1.2% less than
state-of-the-art, allowing a 94.5% reduction in the training data requirement.
The proposed pipeline also outperforms existing works on leaf disease
classification with limited data in both laboratory and real-life conditions in
single-domain, mixed-domain, and cross-domain scenarios.

</details>


### [230] [Multimodal Fusion SLAM with Fourier Attention](https://arxiv.org/abs/2506.18204)
*Youjie Zhou,Guofeng Mei,Yiming Wang,Yi Wan,Fabio Poiesi*

Main category: cs.CV

TL;DR: The paper introduces FMF-SLAM, an efficient Visual SLAM method using Fourier-based attention mechanisms and multimodal fusion for enhanced performance under difficult conditions.


<details>
  <summary>Details</summary>
Motivation: Conventional optical flow-based Visual SLAM methods face challenges with computational inefficiency and poor performance under noise, varying light conditions, and darkness in real-world environments.

Method: The proposed FMF-SLAM employs a Fourier-based self-attention and cross-attention mechanism to fuse RGB and depth signals. Additionally, multi-scale knowledge distillation is used across modalities to improve feature interaction.

Result: FMF-SLAM achieves state-of-the-art performance on TUM, TartanAir, and real-world datasets, with strong robustness to noise, varying lighting, and darkness, as well as real-time applicability in practical robot scenarios.

Conclusion: FMF-SLAM offers an efficient and practical framework for Visual SLAM with multimodal fusion, validated for robustness and real-time performance in diverse and challenging environments.

Abstract: Visual SLAM is particularly challenging in environments affected by noise,
varying lighting conditions, and darkness. Learning-based optical flow
algorithms can leverage multiple modalities to address these challenges, but
traditional optical flow-based visual SLAM approaches often require significant
computational resources.To overcome this limitation, we propose FMF-SLAM, an
efficient multimodal fusion SLAM method that utilizes fast Fourier transform
(FFT) to enhance the algorithm efficiency. Specifically, we introduce a novel
Fourier-based self-attention and cross-attention mechanism to extract features
from RGB and depth signals. We further enhance the interaction of multimodal
features by incorporating multi-scale knowledge distillation across modalities.
We also demonstrate the practical feasibility of FMF-SLAM in real-world
scenarios with real time performance by integrating it with a security robot by
fusing with a global positioning module GNSS-RTK and global Bundle Adjustment.
Our approach is validated using video sequences from TUM, TartanAir, and our
real-world datasets, showcasing state-of-the-art performance under noisy,
varying lighting, and dark conditions.Our code and datasets are available at
https://github.com/youjie-zhou/FMF-SLAM.git.

</details>


### [231] [Limitations of NERF with pre-trained Vision Features for Few-Shot 3D Reconstruction](https://arxiv.org/abs/2506.18208)
*Ankit Sanjyal*

Main category: cs.CV

TL;DR: The paper evaluates DINO-enhanced NeRF models for 3D scene reconstruction and finds them less effective than the baseline NeRF, questioning pre-trained vision feature integration.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of integrating pre-trained vision features, specifically DINO, into NeRF models for improving 3D reconstruction in few-shot scenarios.

Method: Systematic evaluation of different DINO-enhanced NeRF models, including frozen features, LoRA fine-tuned features, and multi-scale feature fusion, compared to a baseline NeRF.

Result: All DINO-enhanced models perform worse than the baseline NeRF, achieving PSNR values of 12.9-13.0 versus the baseline's 14.71.

Conclusion: Pre-trained vision features may not benefit few-shot 3D reconstruction and could introduce harmful biases, suggesting simpler geometric models might be better suited.

Abstract: Neural Radiance Fields (NeRF) have revolutionized 3D scene reconstruction
from sparse image collections. Recent work has explored integrating pre-trained
vision features, particularly from DINO, to enhance few-shot reconstruction
capabilities. However, the effectiveness of such approaches remains unclear,
especially in extreme few-shot scenarios. In this paper, we present a
systematic evaluation of DINO-enhanced NeRF models, comparing baseline NeRF,
frozen DINO features, LoRA fine-tuned features, and multi-scale feature fusion.
Surprisingly, our experiments reveal that all DINO variants perform worse than
the baseline NeRF, achieving PSNR values around 12.9 to 13.0 compared to the
baseline's 14.71. This counterintuitive result suggests that pre-trained vision
features may not be beneficial for few-shot 3D reconstruction and may even
introduce harmful biases. We analyze potential causes including feature-task
mismatch, overfitting to limited data, and integration challenges. Our findings
challenge common assumptions in the field and suggest that simpler
architectures focusing on geometric consistency may be more effective for
few-shot scenarios.

</details>


### [232] [Deep Learning-based Alignment Measurement in Knee Radiographs](https://arxiv.org/abs/2506.18209)
*Zhisen Hu,Dominic Cullen,Peter Thompson,David Johnson,Chang Bian,Aleksei Tiulpin,Timothy Cootes,Claudia Lindner*

Main category: cs.CV

TL;DR: The paper introduces a deep learning approach to measure radiographic knee alignment (KA) using knee radiographs, achieving high accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: The current methods for measuring radiographic knee alignment are manual, time-intensive, and require specialized imaging (long-leg radiographs), creating a need for automated, efficient, and accurate solutions.

Method: The proposed method uses deep learning, specifically hourglass networks with attention gate structures, to localize over 100 knee anatomical landmarks and measure KA based on anteroposterior knee radiographs.

Result: The method achieved mean absolute differences of approximately 1 degree from clinical ground truth measurements and demonstrated excellent agreement pre-operatively (ICC=0.97) and good agreement post-operatively (ICC=0.86).

Conclusion: This research highlights the viability of automating KA assessment with high accuracy, paving the way for enhanced clinical workflows through deep learning-based solutions.

Abstract: Radiographic knee alignment (KA) measurement is important for predicting
joint health and surgical outcomes after total knee replacement. Traditional
methods for KA measurements are manual, time-consuming and require long-leg
radiographs. This study proposes a deep learning-based method to measure KA in
anteroposterior knee radiographs via automatically localized knee anatomical
landmarks. Our method builds on hourglass networks and incorporates an
attention gate structure to enhance robustness and focus on key anatomical
features. To our knowledge, this is the first deep learning-based method to
localize over 100 knee anatomical landmarks to fully outline the knee shape
while integrating KA measurements on both pre-operative and post-operative
images. It provides highly accurate and reliable anatomical varus/valgus KA
measurements using the anatomical tibiofemoral angle, achieving mean absolute
differences ~1{\deg} when compared to clinical ground truth measurements.
Agreement between automated and clinical measurements was excellent
pre-operatively (intra-class correlation coefficient (ICC) = 0.97) and good
post-operatively (ICC = 0.86). Our findings demonstrate that KA assessment can
be automated with high accuracy, creating opportunities for digitally enhanced
clinical workflows.

</details>


### [233] [Shape from Polarization of Thermal Emission and Reflection](https://arxiv.org/abs/2506.18217)
*Kazuma Kitazawa,Tsuyoshi Takatani*

Main category: cs.CV

TL;DR: This paper addresses shape estimation of transparent objects in the LWIR (long-wave infrared) spectrum using a novel polarization model to improve accuracy, introducing a benchmark dataset and utilizing both model-based and learning-based approaches.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve shape estimation for transparent objects, which is difficult due to complex light behavior, by leveraging the LWIR spectrum where materials tend to be opaque and emissive.

Method: They developed a polarization model that accounts for both emission and reflection effects and applied it using model-based and learning-based (neural network) methods trained on synthetic data. They also built a system to account for systematic errors in LWIR polarimetric imaging.

Result: A prototype system was implemented, and the authors created 'ThermoPol,' the first benchmark dataset for real-world LWIR SfP. Experiments showed high accuracy and method applicability to various materials, including visible spectrum-transparent objects.

Conclusion: The proposed LWIR polarimetric model and methods significantly improve shape estimation accuracy and can be applied broadly across different materials, filling a critical gap in shape-from-polarization research.

Abstract: Shape estimation for transparent objects is challenging due to their complex
light transport. To circumvent these difficulties, we leverage the Shape from
Polarization (SfP) technique in the Long-Wave Infrared (LWIR) spectrum, where
most materials are opaque and emissive. While a few prior studies have explored
LWIR SfP, these attempts suffered from significant errors due to inadequate
polarimetric modeling, particularly the neglect of reflection. Addressing this
gap, we formulated a polarization model that explicitly accounts for the
combined effects of emission and reflection. Based on this model, we estimated
surface normals using not only a direct model-based method but also a
learning-based approach employing a neural network trained on a
physically-grounded synthetic dataset. Furthermore, we modeled the LWIR
polarimetric imaging process, accounting for inherent systematic errors to
ensure accurate polarimetry. We implemented a prototype system and created
ThermoPol, the first real-world benchmark dataset for LWIR SfP. Through
comprehensive experiments, we demonstrated the high accuracy and broad
applicability of our method across various materials, including those
transparent in the visible spectrum.

</details>


### [234] [Cross-Architecture Knowledge Distillation (KD) for Retinal Fundus Image Anomaly Detection on NVIDIA Jetson Nano](https://arxiv.org/abs/2506.18220)
*Berk Yilmaz,Aniruddh Aiyengar*

Main category: cs.CV

TL;DR: The paper presents a lightweight AI-based retinal ailment classifier deployable on edge devices using a knowledge distillation technique.


<details>
  <summary>Details</summary>
Motivation: Many low-resource settings lack access to dependable diagnostic tools for retinal diseases, necessitating a deployable, efficient solution.

Method: A high-capacity ViT teacher model is used to classify fundus images, and its knowledge is distilled into a CNN-based student model deployable on IoT devices like NVIDIA Jetson Nano.

Result: The student model achieves 89% classification accuracy with 93% performance retention of the teacher model, despite being significantly smaller.

Conclusion: The proposed method compresses the ViT model for deployment without significant loss in accuracy, providing a scalable solution for retinal triage in resource-limited areas.

Abstract: Early and accurate identification of retinal ailments is crucial for averting
ocular decline; however, access to dependable diagnostic devices is not often
available in low-resourced settings. This project proposes to solve that by
developing a lightweight, edge-device deployable disease classifier using
cross-architecture knowledge distilling. We first train a high-capacity vision
transformer (ViT) teacher model, pre-trained using I-JEPA self-supervised
learning, to classify fundus images into four classes: Normal, Diabetic
Retinopathy, Glaucoma, and Cataract. We kept an Internet of Things (IoT) focus
when compressing to a CNN-based student model for deployment in
resource-limited conditions, such as the NVIDIA Jetson Nano. This was
accomplished using a novel framework which included a Partitioned
Cross-Attention (PCA) projector, a Group-Wise Linear (GL) projector, and a
multi-view robust training method. The teacher model has 97.4 percent more
parameters than the student model, with it achieving 89 percent classification
with a roughly 93 percent retention of the teacher model's diagnostic
performance. The retention of clinical classification behavior supports our
method's initial aim: compression of the ViT while retaining accuracy. Our work
serves as an example of a scalable, AI-driven triage solution for retinal
disorders in under-resourced areas.

</details>


### [235] [Make It Efficient: Dynamic Sparse Attention for Autoregressive Image Generation](https://arxiv.org/abs/2506.18226)
*Xunzhi Xiang,Qi Fan*

Main category: cs.CV

TL;DR: The paper addresses memory and computational inefficiencies in autoregressive text-to-image generation models by introducing Adaptive Dynamic Sparse Attention (ADSA), a training-free method for optimizing attention and reducing GPU memory usage.


<details>
  <summary>Details</summary>
Motivation: The study aims to resolve issues in autoregressive image generation, particularly the memory overhead and computational delays caused by excessively long contexts during inference.

Method: The authors propose Adaptive Dynamic Sparse Attention (ADSA), which dynamically identifies important tokens for local texture and global semantic coherence, along with a dynamic KV-cache update mechanism to lower memory usage.

Result: ADSA effectively reduces GPU memory consumption by about 50% during inference while maintaining or improving the quality of generated images.

Conclusion: ADSA is a promising solution for optimizing resource efficiency and maintaining high-quality image generation in autoregressive models.

Abstract: Autoregressive conditional image generation models have emerged as a dominant
paradigm in text-to-image synthesis. These methods typically convert images
into one-dimensional token sequences and leverage the self-attention mechanism,
which has achieved remarkable success in natural language processing, to
capture long-range dependencies, model global context, and ensure semantic
coherence. However, excessively long contexts during inference lead to
significant memory overhead caused by KV-cache and computational delays. To
alleviate these challenges, we systematically analyze how global semantics,
spatial layouts, and fine-grained textures are formed during inference, and
propose a novel training-free context optimization method called Adaptive
Dynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies
historical tokens crucial for maintaining local texture consistency and those
essential for ensuring global semantic coherence, thereby efficiently
streamlining attention computation. Additionally, we introduce a dynamic
KV-cache update mechanism tailored for ADSA, reducing GPU memory consumption
during inference by approximately $50\%$. Extensive qualitative and
quantitative experiments demonstrate the effectiveness and superiority of our
approach in terms of both generation quality and resource efficiency.

</details>


### [236] [Referring Expression Instance Retrieval and A Strong End-to-End Baseline](https://arxiv.org/abs/2506.18246)
*Xiangzhao Hao,Kuan Zhu,Hongyu Guo,Haiyun Guo,Ming Tang,JinQiao Wang*

Main category: cs.CV

TL;DR: The paper introduces a new task called Referring Expression Instance Retrieval (REIR) aimed at addressing the gap in scalability and precision between Text-Image Retrieval (TIR) and Referring Expression Comprehension (REC).


<details>
  <summary>Details</summary>
Motivation: Current vision-language tasks like TIR lack precision while REC lacks scalability in real-world applications that require both instance-level retrieval and localization.

Method: The authors propose the REIR task, introduce the REIRCOCO benchmark, and present a baseline model called CLARE featuring a dual-stream architecture for inter-instance relations using the Mix of Relation Experts (MORE) module.

Result: CLARE achieves state-of-the-art performance on REIR and demonstrates strong generalizability to TIR and REC tasks.

Conclusion: The paper successfully establishes REIR as a scalable and precise task, with the CLARE model showing effectiveness and versatility across related tasks in vision-language research.

Abstract: Natural language querying of visual content underpins many vision-language
tasks, typically categorized by text granularity and visual search scope.
Text-Image Retrieval (TIR) retrieves whole images using coarse descriptions,
while Referring Expression Comprehension (REC) localizes objects using
fine-grained expressions within a single image. However, real-world scenarios
often require both instance-level retrieval and localization across large
galleries -- tasks where TIR lacks precision and REC lacks scalability. To
address this gap, we propose a new task: Referring Expression Instance
Retrieval (REIR), which jointly supports instance-level retrieval and
localization. We introduce REIRCOCO, a large-scale benchmark constructed by
prompting vision-language models to generate fine-grained expressions for
MSCOCO and RefCOCO instances. We also present a baseline method, CLARE,
featuring a dual-stream architecture with a Mix of Relation Experts (MORE)
module for capturing inter-instance relationships. CLARE integrates object
detection and REC pretraining with Contrastive Language-Instance Alignment
(CLIA) for end-to-end optimization. Experiments show that CLARE achieves
state-of-the-art performance on REIR and generalizes well to TIR and REC,
highlighting its effectiveness and versatility.

</details>


### [237] [Semantic Structure-Aware Generative Attacks for Enhanced Adversarial Transferability](https://arxiv.org/abs/2506.18248)
*Jongoh Jeong,Hunmin Yang,Jaeseok Jeong,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: The paper proposes a new framework for adversarial attacks using semantic structure awareness, improving transferability compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing generative adversarial attack methods do not fully utilize the semantic features encoded in intermediate activations of the generator, which limits their performance on black-box models.

Method: A semantic structure-aware framework based on the Mean Teacher model is proposed. The method aligns semantic consistency using feature distillation and anchors perturbation synthesis to early intermediate blocks in the generator to enhance adversarial transferability.

Result: Extensive experiments across various models and domains show improvements over state-of-the-art generative attacks. A new metric, Accidental Correction Rate (ACR), is introduced for evaluation.

Conclusion: The proposed framework effectively leverages semantic features to generate adversarial perturbations that achieve better transferability and efficiency in real-world settings.

Abstract: Generative adversarial attacks train a perturbation generator on a white-box
surrogate model and subsequently apply the crafted perturbations to unseen
black-box victim models. In contrast to iterative attacks, these methods
deliver superior inference-time efficiency, scalability, and transferability;
however, up until now, existing studies have not fully exploited the
representational capacity of generative models to preserve and harness semantic
information. Specifically, the intermediate activations of the generator encode
rich semantic features--object boundaries and coarse shapes--that remain
under-exploited, thereby limiting the alignment of perturbations with
object-salient regions which are critical for adversarial transferability. To
remedy this, we introduce a semantic structure-aware attack framework based on
the Mean Teacher, which serves as a temporally smoothed feature reference. With
this smoothed reference, we further direct semantic consistency between the
early-layer activations in the student and those of the semantically rich
teacher by feature distillation. By anchoring perturbation synthesis to the
semantically salient early intermediate blocks within the generator based on
empirical findings, our method guides progressive adversarial perturbation on
regions that substantially enhance adversarial transferability. We conduct
extensive experiments over diverse models, domains and tasks to demonstrate
consistent improvements relative to state-of-the-art generative attacks,
comprehensively evaluated using conventional metrics and our newly proposed
Accidental Correction Rate (ACR).

</details>


### [238] [Improving Weakly Supervised Temporal Action Localization by Exploiting Multi-resolution Information in Temporal Domain](https://arxiv.org/abs/2506.18261)
*Rui Su,Dong Xu,Luping Zhou,Wanli Ouyang*

Main category: cs.CV

TL;DR: This paper presents a two-stage method for weakly supervised temporal action localization using multi-resolution temporal information and iterative pseudo-label refinement.


<details>
  <summary>Details</summary>
Motivation: Weakly supervised temporal action localization is challenging due to the lack of frame-level annotations during training. The study aims to utilize video-level annotations effectively to generate accurate frame-level pseudo labels.

Method: The method has two stages. The first stage employs an Initial Label Generation (ILG) module using multi-resolution temporal consistency to produce class activation sequences (CASs). The second stage introduces a Progressive Temporal Label Refinement (PTLR) framework with two networks (OTS and RTS streams) that collaborate in pseudo-label refinement, enhancing action localization through multi-resolution information exchange.

Result: The proposed approach successfully generates high-quality pseudo labels and improves temporal action localization by leveraging both appearance and motion cues in the temporal domain.

Conclusion: This method demonstrates that multi-resolution and iterative pseudo-label refinement techniques can significantly enhance the performance of weakly supervised temporal action localization models.

Abstract: Weakly supervised temporal action localization is a challenging task as only
the video-level annotation is available during the training process. To address
this problem, we propose a two-stage approach to fully exploit multi-resolution
information in the temporal domain and generate high quality frame-level pseudo
labels based on both appearance and motion streams. Specifically, in the first
stage, we generate reliable initial frame-level pseudo labels, and in the
second stage, we iteratively refine the pseudo labels and use a set of selected
frames with highly confident pseudo labels to train neural networks and better
predict action class scores at each frame. We fully exploit temporal
information at multiple scales to improve temporal action localization
performance. Specifically, in order to obtain reliable initial frame-level
pseudo labels, in the first stage, we propose an Initial Label Generation (ILG)
module, which leverages temporal multi-resolution consistency to generate high
quality class activation sequences (CASs), which consist of a number of
sequences with each sequence measuring how likely each video frame belongs to
one specific action class. In the second stage, we propose a Progressive
Temporal Label Refinement (PTLR) framework. In our PTLR framework, two networks
called Network-OTS and Network-RTS, which are respectively used to generate
CASs for the original temporal scale and the reduced temporal scales, are used
as two streams (i.e., the OTS stream and the RTS stream) to refine the pseudo
labels in turn. By this way, the multi-resolution information in the temporal
domain is exchanged at the pseudo label level, and our work can help improve
each stream (i.e., the OTS/RTS stream) by exploiting the refined pseudo labels
from another stream (i.e., the RTS/OTS stream).

</details>


### [239] [YouTube-Occ: Learning Indoor 3D Semantic Occupancy Prediction from YouTube Videos](https://arxiv.org/abs/2506.18266)
*Haoming Chen,Lichen Yuan,TianFang Sun,Jingyu Gong,Xin Tan,Zhizhong Zhang,Yuan Xie*

Main category: cs.CV

TL;DR: The paper presents a method for 3D semantic occupancy prediction using only public indoor videos from YouTube, bypassing the need for precise geometric data or camera parameters.


<details>
  <summary>Details</summary>
Motivation: Address the impracticality of large-scale data collection and fine-grained annotations for 3D indoor environment modeling, while minimizing privacy concerns.

Method: A self-supervised framework leveraging YouTube house tour videos (dataset named YouTube-Occ) and 2D vision foundation models, distilling 2D knowledge into 3D occupancy predictions using superpixels.

Result: Achieves state-of-the-art zero-shot performance on NYUv2 and OccScanNet benchmarks in 3D indoor perception.

Conclusion: 3D spatially-accurate training can use publicly available web data effectively, eliminating the need for complex data setups or camera parameters.

Abstract: 3D semantic occupancy prediction in the past was considered to require
precise geometric relationships in order to enable effective training. However,
in complex indoor environments, the large-scale and widespread collection of
data, along with the necessity for fine-grained annotations, becomes
impractical due to the complexity of data acquisition setups and privacy
concerns. In this paper, we demonstrate that 3D spatially-accurate training can
be achieved using only indoor Internet data, without the need for any
pre-knowledge of intrinsic or extrinsic camera parameters. In our framework, we
collect a web dataset, YouTube-Occ, which comprises house tour videos from
YouTube, providing abundant real house scenes for 3D representation learning.
Upon on this web dataset, we establish a fully self-supervised model to
leverage accessible 2D prior knowledge for reaching powerful 3D indoor
perception. Specifically, we harness the advantages of the prosperous vision
foundation models, distilling the 2D region-level knowledge into the occupancy
network by grouping the similar pixels into superpixels. Experimental results
show that our method achieves state-of-the-art zero-shot performance on two
popular benchmarks (NYUv2 and OccScanNet

</details>


### [240] [ThermalLoc: A Vision Transformer-Based Approach for Robust Thermal Camera Relocalization in Large-Scale Environments](https://arxiv.org/abs/2506.18268)
*Yu Liu,Yangtao Meng,Xianfei Pan,Jie Jiang,Changhao Chen*

Main category: cs.CV

TL;DR: ThermalLoc is an advanced deep learning approach to thermal image relocalization, outperforming existing methods with higher accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Current camera relocalization methods predominantly focus on visible light images and are not well-suited for thermal cameras, which rely on heat emission for data capture. This disparity highlights the need for methods tailored specifically to thermal imaging.

Method: The proposed solution, ThermalLoc, employs a combination of EfficientNet and Transformers to extract local and global features from thermal images and uses two MLP networks for absolute pose regression. This method is designed to function as an end-to-end relocalization system.

Result: Testing on publicly available and proprietary datasets revealed that ThermalLoc achieves better accuracy and robustness than current methods like AtLoc, MapNet, PoseNet, and RobustLoc.

Conclusion: ThermalLoc establishes itself as a leading, specialized solution for thermal image relocalization, addressing an underexplored domain and pushing the boundaries of performance in this field.

Abstract: Thermal cameras capture environmental data through heat emission, a
fundamentally different mechanism compared to visible light cameras, which rely
on pinhole imaging. As a result, traditional visual relocalization methods
designed for visible light images are not directly applicable to thermal
images. Despite significant advancements in deep learning for camera
relocalization, approaches specifically tailored for thermal camera-based
relocalization remain underexplored. To address this gap, we introduce
ThermalLoc, a novel end-to-end deep learning method for thermal image
relocalization. ThermalLoc effectively extracts both local and global features
from thermal images by integrating EfficientNet with Transformers, and performs
absolute pose regression using two MLP networks. We evaluated ThermalLoc on
both the publicly available thermal-odometry dataset and our own dataset. The
results demonstrate that ThermalLoc outperforms existing representative methods
employed for thermal camera relocalization, including AtLoc, MapNet, PoseNet,
and RobustLoc, achieving superior accuracy and robustness.

</details>


### [241] [Adaptive Mask-guided K-space Diffusion for Accelerated MRI Reconstruction](https://arxiv.org/abs/2506.18270)
*Qinrong Cai,Yu Guan,Zhibo Chen,Dong Liang,Qiuyun Fan,Qiegen Liu*

Main category: cs.CV

TL;DR: This paper introduces a new adaptive masking diffusion model (AMDM) for improving MRI reconstructions by effectively separating and learning frequency-specific information from k-space data.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in prior MRI reconstruction methods that neglected the importance of different frequency regions in k-space, thereby improving reconstruction quality.

Method: Developed AMDM, a diffusion model that adaptively adjusts masks in k-space to separate high- and low-frequency components, guiding a closed-loop diffusion process for optimized MRI reconstruction.

Result: The proposed model effectively learns frequency-specific information, improving MRI reconstruction quality as verified through experimental results.

Conclusion: AMDM provides a flexible and effective framework for optimizing k-space data with adaptive masks, offering significant advances for future MRI reconstructions.

Abstract: As the deep learning revolution marches on, masked modeling has emerged as a
distinctive approach that involves predicting parts of the original data that
are proportionally masked during training, and has demonstrated exceptional
performance in multiple fields. Magnetic Resonance Imaging (MRI) reconstruction
is a critical task in medical imaging that seeks to recover high-quality images
from under-sampled k-space data. However, previous MRI reconstruction
strategies usually optimized the entire image domain or k-space, without
considering the importance of different frequency regions in the k-space This
work introduces a diffusion model based on adaptive masks (AMDM), which
utilizes the adaptive adjustment of frequency distribution based on k-space
data to develop a hybrid masks mechanism that adapts to different k-space
inputs. This enables the effective separation of high-frequency and
low-frequency components, producing diverse frequency-specific representations.
Additionally, the k-space frequency distribution informs the generation of
adaptive masks, which, in turn, guide a closed-loop diffusion process.
Experimental results verified the ability of this method to learn specific
frequency information and thereby improved the quality of MRI reconstruction,
providing a flexible framework for optimizing k-space data using masks in the
future.

</details>


### [242] [ReFrame: Rectification Framework for Image Explaining Architectures](https://arxiv.org/abs/2506.18272)
*Debjyoti Das Adhikary,Aritra Hazra,Partha Pratim Chakrabarti*

Main category: cs.CV

TL;DR: The paper introduces a framework to improve image explanation quality by addressing issues of object hallucination and incomplete recognition.


<details>
  <summary>Details</summary>
Motivation: Existing image explanation frameworks often hallucinate non-existent objects or fail to recognize all objects, leading to inconsistent and incomplete outputs.

Method: The proposed interpretable framework can be integrated with various image explanation systems (Image Captioning, VQA, Prompt-based AI) to correct inaccuracies and enhance the completeness and consistency of the object explanations.

Result: The framework improved explanations substantially, with an increase in completeness (up to 81.81%) and decreased inconsistency (up to 37.10%) across different tasks, surpassing current state-of-the-art results.

Conclusion: This work mitigates critical drawbacks of image explanation systems and significantly improves their reliability and performance through a versatile and effective rectification framework.

Abstract: Image explanation has been one of the key research interests in the Deep
Learning field. Throughout the years, several approaches have been adopted to
explain an input image fed by the user. From detecting an object in a given
image to explaining it in human understandable sentence, to having a
conversation describing the image, this problem has seen an immense change
throughout the years, However, the existing works have been often found to (a)
hallucinate objects that do not exist in the image and/or (b) lack identifying
the complete set of objects present in the image. In this paper, we propose a
novel approach to mitigate these drawbacks of inconsistency and incompleteness
of the objects recognized during the image explanation. To enable this, we
propose an interpretable framework that can be plugged atop diverse image
explaining frameworks including Image Captioning, Visual Question Answering
(VQA) and Prompt-based AI using LLMs, thereby enhancing their explanation
capabilities by rectifying the incorrect or missing objects. We further measure
the efficacy of the rectified explanations generated through our proposed
approaches leveraging object based precision metrics, and showcase the
improvements in the inconsistency and completeness of image explanations.
Quantitatively, the proposed framework is able to improve the explanations over
the baseline architectures of Image Captioning (improving the completeness by
81.81% and inconsistency by 37.10%), Visual Question Answering(average of 9.6%
and 37.10% in completeness and inconsistency respectively) and Prompt-based AI
model (0.01% and 5.2% for completeness and inconsistency respectively)
surpassing the current state-of-the-art by a substantial margin.

</details>


### [243] [Open Set Recognition for Endoscopic Image Classification: A Deep Learning Approach on the Kvasir Dataset](https://arxiv.org/abs/2506.18284)
*Kasra Moazzami,Seoyoun Son,John Lin,Sun Min Lee,Daniel Son,Hayeon Lee,Jeongho Lee,Seongji Lee*

Main category: cs.CV

TL;DR: The paper explores Open Set Recognition (OSR) techniques applied to the Kvasir endoscopic image dataset, comparing deep learning models like ResNet-50, Swin Transformer, and hybrid ResNet-Transformer.


<details>
  <summary>Details</summary>
Motivation: Conventional closed-set classification frameworks struggle in open-world clinical settings due to the presence of previously unseen conditions that compromise reliability.

Method: Different deep learning models and the OpenMax baseline OSR method were used to assess the models' ability to distinguish known classes from unseen ones in endoscopic images.

Result: The results underscore the models’ behavior under open-set conditions and provide insights into their clinical application with endoscopic image data.

Conclusion: OSR techniques are crucial for enhancing the reliability and safety of AI systems in medical diagnostics, marking an important step toward robust clinical deployment.

Abstract: Endoscopic image classification plays a pivotal role in medical diagnostics
by identifying anatomical landmarks and pathological findings. However,
conventional closed-set classification frameworks are inherently limited in
open-world clinical settings, where previously unseen conditions can arise
andcompromise model reliability. To address this, we explore the application of
Open Set Recognition (OSR) techniques on the Kvasir dataset, a publicly
available and diverse endoscopic image collection. In this study, we evaluate
and compare the OSR capabilities of several representative deep learning
architectures, including ResNet-50, Swin Transformer, and a hybrid
ResNet-Transformer model, under both closed-set and open-set conditions.
OpenMax is adopted as a baseline OSR method to assess the ability of these
models to distinguish known classes from previously unseen categories. This
work represents one of the first efforts to apply open set recognition to the
Kvasir dataset and provides a foundational benchmark for evaluating OSR
performance in medical image analysis. Our results offer practical insights
into model behavior in clinically realistic settings and highlight the
importance of OSR techniques for the safe deployment of AI systems in
endoscopy.

</details>


### [244] [Selective Social-Interaction via Individual Importance for Fast Human Trajectory Prediction](https://arxiv.org/abs/2506.18291)
*Yota Urano,Hiromu Taketsugu,Norimichi Ukita*

Main category: cs.CV

TL;DR: The paper introduces an architecture for predicting a person's trajectory using selected neighboring individuals, utilizing an Importance Estimator module and Gumbel Softmax for training.


<details>
  <summary>Details</summary>
Motivation: The research aims to improve the efficiency and accuracy of predicting an individual's trajectory by smartly selecting relevant neighboring individuals.

Method: The method involves a people selection module called Importance Estimator to quantify the relevance of each neighbor, combined with Gumbel Softmax to handle non-differentiable sampling during training.

Result: The proposed method was validated on the JRDB dataset, demonstrating faster processing with competitive prediction accuracy.

Conclusion: The architecture efficiently selects neighboring individuals and achieves competitive performance, offering a promising approach to trajectory prediction.

Abstract: This paper presents an architecture for selecting important neighboring
people to predict the primary person's trajectory. To achieve effective
neighboring people selection, we propose a people selection module called the
Importance Estimator which outputs the importance of each neighboring person
for predicting the primary person's future trajectory. To prevent gradients
from being blocked by non-differentiable operations when sampling surrounding
people based on their importance, we employ the Gumbel Softmax for training.
Experiments conducted on the JRDB dataset show that our method speeds up the
process with competitive prediction accuracy.

</details>


### [245] [Rapeseed population point cloud completion network (RP-PCN) with dynamic graph convolution for 3D reconstruction of crop canopy occlusion architecture](https://arxiv.org/abs/2506.18292)
*Ziyue Guo,Xin Yang,Yutao Shen,Yang Zhu,Lixi Jiang,Haiyan Cen*

Main category: cs.CV

TL;DR: The paper introduces RP-PCN, a point cloud completion model, to accurately reconstruct 3D canopy architectures of rapeseed populations across growth stages using multi-view imaging.


<details>
  <summary>Details</summary>
Motivation: Accurate 3D descriptions of canopy architecture are essential for predicting crop yield and photosynthesis to design optimized plant ideotypes, but occlusion and complex structures in conventional methods hinder this.

Method: A novel RP-PCN model was created incorporating virtual-real simulation, occlusion detection, MRDG, PPD, and DGCFE modules to complete point clouds using surface data.

Result: RP-PCN achieved significant accuracy in point cloud completion with stage-specific improvements in Chamfer distance and enhanced yield prediction by 11.2% using complete point clouds.

Conclusion: RP-PCN can effectively reconstruct crop canopy architecture and improve yield predictions, offering broad applicability to other crops for field-based studies.

Abstract: Quantitative descriptions of complete canopy architecture are crucial for
evaluating crop photosynthesis and yield to guide ideotype design. Although
three-dimensional (3D) sensing technologies have been developed for plant and
canopy reconstruction, severe occlusion and complex architectures hinder
accurate canopy descriptions. In this study, we propose a point cloud
completion model for 3D reconstruction of rapeseed populations from seeding to
silique stages using multi-view imaging. A complete point cloud generation
framework was developed with the virtual-real integration (VRI) simulation
method and occlusion point detection algorithm to annotate the training dataset
by distinguishing surface from occluded points. The rapeseed population point
cloud completion network (RP-PCN) was designed with a multi-resolution dynamic
graph convolutional encoder (MRDG) and point pyramid decoder (PPD) to predict
occluded points based on input surface point clouds. A dynamic graph
convolutional feature extractor (DGCFE) was introduced to capture structural
variations across the growth period. The effectiveness of point cloud
completion was validated by predicting yield using architectural indicators
from complete point clouds of rapeseed population. The results demonstrated
that RP-PCN achieved chamfer distance (CD) values of 3.35 cm, 3.46 cm, 4.32 cm,
and 4.51 cm at the seedling, bolting, flowering, and silique stages,
respectively. Ablation studies showed the effectiveness of the MRDG and DGCFE
modules, reducing CD values by 10% and 23%, respectively. The silique
efficiency index (SEI) from RP-PCN improved yield prediction accuracy by 11.2%
compared to incomplete point clouds. The RP-PCN pipeline proposed in this study
has the potential to be extended to other crops, significantly enhancing the
analysis of population canopy architectures in field environments.

</details>


### [246] [Attention-Based Ensemble Learning for Crop Classification Using Landsat 8-9 Fusion](https://arxiv.org/abs/2506.18321)
*Zeeshan Ramzan,Nisar Ahmed,Qurat-ul-Ain Akram,Shahzad Asif,Muhammad Shahbaz,Rabin Chakrabortty,Ahmed F. Elaksher*

Main category: cs.CV

TL;DR: Remote sensing and Landsat 8-9 imagery were used to identify crop cover and types in irrigated Central Punjab, utilizing advanced pre-processing and classification models.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve the accuracy of crop classification in irrigated agricultural regions of Central Punjab, addressing the need for reliable data on crop types and coverage through remote sensing.

Method: The research involved field surveys for data collection, geocoding six target crops, pre-processing Landsat 8-9 imagery, and applying techniques like image fusion and vegetation indices extraction. This was followed by classification modeling using ensemble learning, neural networks, and feature selection.

Result: A comprehensive dataset of 50,835 data points was created, enabling accurate extraction of vegetation indices and effective classification of crop cover using remote sensing and machine learning methods.

Conclusion: The study highlights the effectiveness of combining remote sensing imagery and advanced classification models to achieve robust crop classification in agricultural regions, supporting better agricultural management.

Abstract: Remote sensing offers a highly effective method for obtaining accurate
information on total cropped area and crop types. The study focuses on crop
cover identification for irrigated regions of Central Punjab. Data collection
was executed in two stages: the first involved identifying and geocoding six
target crops through field surveys conducted in January and February 2023. The
second stage involved acquiring Landsat 8-9 imagery for each geocoded field to
construct a labelled dataset. The satellite imagery underwent extensive
pre-processing, including radiometric calibration for reflectance values,
atmospheric correction, and georeferencing verification to ensure consistency
within a common coordinate system. Subsequently, image fusion techniques were
applied to combine Landsat 8 and 9 spectral bands, creating a composite image
with enhanced spectral information, followed by contrast enhancement. During
data acquisition, farmers were interviewed, and fields were meticulously mapped
using GPS instruments, resulting in a comprehensive dataset of 50,835 data
points. This dataset facilitated the extraction of vegetation indices such as
NDVI, SAVO, RECI, and NDRE. These indices and raw reflectance values were
utilized for classification modeling using conventional classifiers, ensemble
learning, and artificial neural networks. A feature selection approach was also
incorporated to identify the optimal feature set for classification learning.
This study demonstrates the effectiveness of combining remote sensing data and
advanced modeling techniques to improve crop classification accuracy in
irrigated agricultural regions.

</details>


### [247] [Escaping the SpuriVerse: Can Large Vision-Language Models Generalize Beyond Seen Spurious Correlations?](https://arxiv.org/abs/2506.18322)
*Yiwei Yang,Chung Peng Lee,Shangbin Feng,Dora Zhao,Bingbing Wen,Anthony Z. Liu,Yulia Tsvetkov,Bill Howe*

Main category: cs.CV

TL;DR: The paper introduces SpuriVerse, a benchmark aimed at studying spurious correlations in Large Vision Language Models (LVLMs) by identifying and curating real-world and synthetic errors from VQA datasets, demonstrating how these models struggle and can improve significantly via fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To address the issue of spurious correlations in fine-tuned multi-modal LVLMs and develop a realistic benchmark for studying their impact on model performance.

Method: The authors sourced GPT-4 errors on visual-question-answering benchmarks and used LVLM-human annotation and counterfactual evaluation to curate a dataset with realistic and synthetic examples of spurious correlations.

Result: SpuriVerse contains 124 types of spurious correlations across 1364 VQA samples. Evaluations show that even the best models only reach 37.1% accuracy, but fine-tuning on synthetic samples improves performance to 78.4%.

Conclusion: Training on diverse spurious patterns helps LVLMs generalize better by encouraging them to focus on the overall image context instead of relying on shortcuts.

Abstract: Finetuning can cause spurious correlations to arise between non-essential
features and the target labels, but benchmarks to study these effects involve
contrived settings and narrow tasks. In contrast, we consider spurious
correlations in multi-modal Large Vision Language Models (LVLMs) pretrained on
extensive and diverse datasets without explicit task supervision. We develop a
benchmark by sourcing GPT-4o errors on real-world visual-question-answering
(VQA) benchmarks, then curating a subset through LVLM-human annotation and
synthetic counterfactual evaluation to identify errors caused by spurious
correlations. This process yields SpuriVerse, a novel benchmark comprised of
124 distinct types of spurious correlations extracted from real-world datasets,
each containing 1 realistic and 10 synthetic VQA samples for a total of 1364
multiple choice questions. We evaluate 15 open and closed-source LVLMs on
SpuriVerse, finding that even state-of-the-art closed-source models struggle
significantly, achieving at best only 37.1% accuracy. Fine-tuning on synthetic
examples that emphasize the spurious correlation improves performance to
78.40%, suggesting that training on diverse spurious patterns generalizes to
unseen situations: models appear to learn to avoid "shortcuts" and attend to
the overall image context.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [248] [Code Generation for Near-Roofline Finite Element Actions on GPUs from Symbolic Variational Forms](https://arxiv.org/abs/2506.17471)
*Kaushik Kulkarni,Andreas Klöckner*

Main category: cs.DC

TL;DR: This study introduces a GPU parallelization strategy for evaluating FEM variational forms by leveraging code transformations and a heuristic cost model, achieving high performance across various workloads.


<details>
  <summary>Details</summary>
Motivation: To enhance computational efficiency and performance of FEM variational form evaluations on GPUs by addressing the challenge of diverse computational workloads.

Method: The method involves constructing and ranking scheduling configurations with a heuristic cost model, implementing a pruning strategy, and prototyping within the Firedrake framework.

Result: The proposed algorithm achieved over 50% roofline performance in 65% of test cases on Nvidia Titan V and Tesla K40c GPUs across diverse applications.

Conclusion: The strategy effectively balances computational efficiency and device capabilities, making it suitable for various workloads and achieving high performance on modern GPUs.

Abstract: We present a novel parallelization strategy for evaluating Finite Element
Method (FEM) variational forms on GPUs, focusing on those that are expressible
through the Unified Form Language (UFL) on simplex meshes. We base our approach
on code transformations, wherein we construct a space of scheduling candidates
and rank them via a heuristic cost model to effectively handle the large
diversity of computational workloads that can be expressed in this way. We
present a design of a search space to which the cost model is applied, along
with an associated pruning strategy to limit the number of configurations that
need to be empirically evaluated. The goal of our design is to strike a balance
between the device's latency-hiding capabilities and the amount of state space,
a key factor in attaining near-roofline performance.
  To make our work widely available, we have prototyped our parallelization
strategy within the \textsc{Firedrake} framework, a UFL-based FEM solver. We
evaluate the performance of our parallelization scheme on two generations of
Nvidia GPUs, specifically the Titan V (Volta architecture) and Tesla K40c
(Kepler architecture), across a range of operators commonly used in
applications, including fluid dynamics, wave propagation, and structural
mechanics, in 2D and 3D geometries. Our results demonstrate that our proposed
algorithm achieves more than $50\%$ roofline performance in $65\%$ of the test
cases on both devices.

</details>


### [249] [PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning](https://arxiv.org/abs/2506.17338)
*Duong Bach*

Main category: cs.DC

TL;DR: This paper introduces the Co-Forgetting Protocol for managing and pruning memory in multi-agent systems (MAS) using semantic voting, temporal decay, and PBFT consensus mechanisms, achieving notable efficiency and accuracy metrics in simulations.


<details>
  <summary>Details</summary>
Motivation: With increasing deployment of multi-agent systems in dynamic and complex environments, there is a need for mechanisms to manage shared knowledge that ensure distributed memories remain synchronized, relevant, and efficient, avoiding the accumulation of outdated data.

Method: The Co-Forgetting Protocol consists of three main components: (1) context-aware semantic voting using DistilBERT to assess memory relevance, (2) multi-scale temporal decay for prioritizing memory based on age and access frequency, and (3) PBFT-based consensus for agreeing on memory retention or discarding decisions amidst Byzantine faults, along with technical implementations using gRPC, Pinecone, and SQLite.

Result: In simulations with four agents, the protocol achieved a 52% reduction in memory footprint over 500 epochs, 88% accuracy in forgetting decisions compared to human-annotated benchmarks, a 92% PBFT consensus success rate under Byzantine conditions, and an 82% cache hit rate for memory access.

Conclusion: The Co-Forgetting Protocol offers an effective, scalable, and fault-tolerant solution for synchronized memory pruning in MAS, demonstrating promising results in maintaining memory efficiency and decision reliability.

Abstract: The proliferation of multi-agent systems (MAS) in complex, dynamic
environments necessitates robust and efficient mechanisms for managing shared
knowledge. A critical challenge is ensuring that distributed memories remain
synchronized, relevant, and free from the accumulation of outdated or
inconsequential data - a process analogous to biological forgetting. This paper
introduces the Co-Forgetting Protocol, a novel, comprehensive framework
designed to address this challenge by enabling synchronized memory pruning in
MAS. The protocol integrates three key components: (1) context-aware semantic
voting, where agents utilize a lightweight DistilBERT model to assess the
relevance of memory items based on their content and the current operational
context; (2) multi-scale temporal decay functions, which assign diminishing
importance to memories based on their age and access frequency across different
time horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based
consensus mechanism, ensuring that decisions to retain or discard memory items
are agreed upon by a qualified and fault-tolerant majority of agents, even in
the presence of up to f Byzantine (malicious or faulty) agents in a system of N
greater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient
inter-agent communication and Pinecone for scalable vector embedding storage
and similarity search, with SQLite managing metadata. Experimental evaluations
in a simulated MAS environment with four agents demonstrate the protocol's
efficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%
voting accuracy in forgetting decisions against human-annotated benchmarks, a
92% PBFT consensus success rate under simulated Byzantine conditions, and an
82% cache hit rate for memory access.

</details>


### [250] [Speeding up Local Optimization in Vehicle Routing with Tensor-based GPU Acceleration](https://arxiv.org/abs/2506.17357)
*Zhenyu Lei,Jin-Kao Hao,Qinghua Wu*

Main category: cs.DC

TL;DR: This paper introduces a GPU-based tensor acceleration method to enhance computational efficiency in local search algorithms for vehicle routing problems.


<details>
  <summary>Details</summary>
Motivation: Addressing computational inefficiencies in local search algorithms, particularly for large or complex vehicle routing problems, is critical for improving heuristic algorithms.

Method: The study proposes a tensor-based GPU acceleration approach, utilizing attribute-based representation and low-coupling architecture to offload computations entirely to the GPU.

Result: Experimental findings demonstrate significant computational efficiency improvements over traditional CPU implementations on three benchmarking routing problems.

Conclusion: The approach shows promise in better efficiency and solution quality, with analysis highlighting strengths, limitations, and avenues for future refinements.

Abstract: Local search plays a central role in many effective heuristic algorithms for
the vehicle routing problem (VRP) and its variants. However, neighborhood
exploration is known to be computationally expensive and time consuming,
especially for large instances or problems with complex constraints. In this
study, we explore a promising direction to address this challenge by
introducing an original tensor-based GPU acceleration method designed to speed
up the commonly used local search operators in vehicle routing. By using an
attribute-based representation, the method offers broad extensibility, making
it applicable to different VRP variants. Its low-coupling architecture, with
intensive computations completely offloaded to the GPU, ensures seamless
integration in various local search-based algorithms and frameworks, leading to
significant improvements in computational efficiency and potentially improved
solution quality. Through comparative experiments on benchmark instances of
three routing problems, we demonstrate the substantial computational advantages
of the proposed approach over traditional CPU-based implementations. We also
provide a detailed analysis of the strengths and limitations of the method,
providing valuable insights into its performance characteristics and
identifying potential bottlenecks in practical applications. These findings
contribute to a better understanding and suggest directions for future
improvements.

</details>


### [251] [ConsumerBench: Benchmarking Generative AI Applications on End-User Devices](https://arxiv.org/abs/2506.17538)
*Yile Gu,Rohan Kadekodi,Hoang Nguyen,Keisuke Kamahori,Yiyu Liu,Baris Kasikci*

Main category: cs.DC

TL;DR: The paper introduces ConsumerBench, a benchmarking framework for evaluating Generative AI models on end-user devices, addressing resource management and efficiency under realistic scenarios.


<details>
  <summary>Details</summary>
Motivation: The shift of Generative AI applications from cloud to end-user devices creates challenges in efficient resource management and user experience.

Method: ConsumerBench simulates multi-application scenarios on constrained hardware, evaluates application and system-level metrics, and provides experiments for insights.

Result: ConsumerBench identifies inefficiencies in resource sharing, scheduling issues, and server configurations while highlighting approaches for improvement.

Conclusion: ConsumerBench offers a valuable framework for developers to optimize GenAI models and systems through custom kernels and SLO-aware strategies.

Abstract: The recent shift in Generative AI (GenAI) applications from cloud-only
environments to end-user devices introduces new challenges in resource
management, system efficiency, and user experience. This paper presents
ConsumerBench, a comprehensive benchmarking framework designed to evaluate the
system efficiency and response time of GenAI models running on end-user
devices. Unlike existing benchmarks that assume exclusive model access on
dedicated GPUs, ConsumerBench simulates realistic multi-application scenarios
executing concurrently on constrained hardware. Furthermore, ConsumerBench
supports customizable workflows that simulate complex tasks requiring
coordination among multiple applications. ConsumerBench captures both
application-level metrics, including latency and Service Level Objective (SLO)
attainment, and system-level metrics like CPU/GPU utilization and memory
bandwidth. Through extensive experiments, ConsumerBench reveals inefficiencies
in resource sharing, unfair scheduling under greedy allocation, and performance
pitfalls of static model server configurations. The paper also provides
practical insights for model developers and system designers, highlighting the
benefits of custom kernels tailored to consumer-grade GPU architectures and the
value of implementing SLO-aware scheduling strategies.

</details>


### [252] [Research on Model Parallelism and Data Parallelism Optimization Methods in Large Language Model-Based Recommendation Systems](https://arxiv.org/abs/2506.17551)
*Haowei Yang,Yu Tian,Zhongheng Yang,Zhao Wang,Chengrui Zhou,Dannier Li*

Main category: cs.DC

TL;DR: This paper investigates optimization methods for distributed training of large language models (LLMs) in recommendations systems, proposing hybrid parallelism to enhance performance.


<details>
  <summary>Details</summary>
Motivation: The growing deployment of LLMs in recommendation systems faces computational and communication challenges due to their large parameter sizes and data volumes.

Method: The study implements two optimization techniques: model parallelism (tensor/pipeline parallelism with load balancing) and data parallelism (synchronous/asynchronous modes using gradient compression and an efficient communication framework).

Result: Experiments using a real-world dataset showed that the hybrid parallelism scheme increased training throughput by over 30% and resource utilization by about 20%, achieving scalability and robustness.

Conclusion: Hybrid parallelism outperformed traditional methods in distributed settings, and future research should focus on incorporating heterogeneous hardware and automated scheduling for deployment.

Abstract: With the rapid adoption of large language models (LLMs) in recommendation
systems, the computational and communication bottlenecks caused by their
massive parameter sizes and large data volumes have become increasingly
prominent. This paper systematically investigates two classes of optimization
methods-model parallelism and data parallelism-for distributed training of LLMs
in recommendation scenarios. For model parallelism, we implement both tensor
parallelism and pipeline parallelism, and introduce an adaptive load-balancing
mechanism to reduce cross-device communication overhead. For data parallelism,
we compare synchronous and asynchronous modes, combining gradient compression
and sparsification techniques with an efficient aggregation communication
framework to significantly improve bandwidth utilization. Experiments conducted
on a real-world recommendation dataset in a simulated service environment
demonstrate that our proposed hybrid parallelism scheme increases training
throughput by over 30% and improves resource utilization by approximately 20%
compared to traditional single-mode parallelism, while maintaining strong
scalability and robustness. Finally, we discuss trade-offs among different
parallel strategies in online deployment and outline future directions
involving heterogeneous hardware integration and automated scheduling
technologies.

</details>


### [253] [Distributed Butterfly Analysis using Mobile Agents](https://arxiv.org/abs/2506.17721)
*Prabhat Kumar Chand,Apurba Das,Anisur Rahaman Molla*

Main category: cs.DC

TL;DR: This paper presents distributed, agent-based algorithms to count 4-cycles (butterflies) in bipartite graphs efficiently without needing prior graph knowledge.


<details>
  <summary>Details</summary>
Motivation: The lack of research into agent-based algorithms for exploring bipartite networks, which are important for dense subgraph analysis and cohesive structure identification.

Method: Proposes algorithms where agents determine partitions, build a spanning tree, and elect a leader in $O(n \log \lambda)$ rounds using $O(\log \lambda)$ bits. The butterfly counting is completed in $O(\Delta)$ rounds.

Result: Achieves efficient distributed process for butterfly counting, requiring minimal graph knowledge and maintaining low complexity for leader election, spanning tree construction, and total butterfly counting.

Conclusion: The approach extends naturally to general graphs, enhancing applicability while maintaining efficiency and low resource requirements.

Abstract: Butterflies, or 4-cycles in bipartite graphs, are crucial for identifying
cohesive structures and dense subgraphs. While agent-based data mining is
gaining prominence, its application to bipartite networks remains relatively
unexplored. We propose distributed, agent-based algorithms for \emph{Butterfly
Counting} in a bipartite graph $G((A,B),E)$. Agents first determine their
respective partitions and collaboratively construct a spanning tree, electing a
leader within $O(n \log \lambda)$ rounds using only $O(\log \lambda)$ bits per
agent. A novel meeting mechanism between adjacent agents improves efficiency
and eliminates the need for prior knowledge of the graph, requiring only the
highest agent ID $\lambda$ among the $n$ agents. Notably, our techniques
naturally extend to general graphs, where leader election and spanning tree
construction maintain the same round and memory complexities. Building on these
foundations, agents count butterflies per node in $O(\Delta)$ rounds and
compute the total butterfly count of $G$ in $O(\Delta+\min\{|A|,|B|\})$ rounds.

</details>


### [254] [Choosing the Right Battery Model for Data Center Simulations](https://arxiv.org/abs/2506.17739)
*Paul Kilian,Philipp Wiesner,Odej Kao*

Main category: cs.DC

TL;DR: The paper investigates battery modeling for data center power systems in microgrid scenarios, assessing four models for realism and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the increasing costs and environmental concerns associated with data center energy consumption, the study explores incorporating realistic and efficient battery models for simulations.

Method: Four battery models, applied in the Vessim co-simulation framework, were tested for their performance in simulating data center power systems.

Result: Linear models successfully balance accuracy and execution speed, approximating the behavior of complex physics-based simulations in short-term experiments.

Conclusion: Linear models offer a practical option for battery simulation in data center scenarios, avoiding the complications of physics-based models and outperforming simple models in realism.

Abstract: As demand for computing resources continues to rise, the increasing cost of
electricity and anticipated regulations on carbon emissions are prompting
changes in data center power systems. Many providers are now operating compute
nodes in microgrids, close to renewable power generators and energy storage, to
maintain full control over the cost and origin of consumed electricity.
Recently, new co-simulation testbeds have emerged that integrate
domain-specific simulators to support research, development, and testing of
such systems in a controlled environment. Yet, choosing an appropriate battery
model for data center simulations remains challenging, as it requires balancing
simulation speed, realism, and ease of configuration.
  In this paper, we implement four different battery models for data center
scenarios within the co-simulation framework Vessim and analyze their behavior.
The results show that linear models, which consider inefficiencies and power
limits, closely match the behavior of complex physics-based models in
short-term experiments while offering faster execution, and not requiring
knowledge on electrochemical reactions and circuit-level dynamics. In contrast,
simple, lossless models fail to accurately represent complex behavior and
provide no further runtime advantage.

</details>


### [255] [Maintaining a Bounded Degree Expander in Dynamic Peer-to-Peer Networks](https://arxiv.org/abs/2506.17757)
*Antonio Cruciani*

Main category: cs.DC

TL;DR: The paper studies maintaining sparse and robust overlay networks under dynamic conditions with nodes joining and leaving. It proposes a distributed protocol for keeping a constant-degree expander graph even with high churn.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to maintain well-connected, low-degree communication graphs in peer-to-peer networks experiencing churn, which reflects real-world settings.

Method: It generalizes a prior randomized connection strategy by Becchetti et al. to handle dynamic networks under adversarial churn using an analysis framework from Augustine et al. The approach relies on a fully distributed protocol.

Result: The proposed algorithm ensures the maintenance of a constant-degree expander graph with high probability under adversarial churn at a rate of up to O(n/polylog(n)) per round, where n is the network size.

Conclusion: The work provides a churn-resilient, distributed protocol with provable guarantees and addresses an open problem raised in previous studies.

Abstract: We study the problem of maintaining robust and sparse overlay networks in
fully distributed settings where nodes continuously join and leave the system.
This scenario closely models real-world unstructured peer-to-peer networks,
where maintaining a well-connected yet low-degree communication graph is
crucial. We generalize a recent protocol by Becchetti et al. [SODA 2020] that
relies on a simple randomized connection strategy to build an expander topology
with high probability to a dynamic networks with churn setting. In this work,
the network dynamism is governed by an oblivious adversary that controls which
nodes join and leave the system in each round. The adversary has full knowledge
of the system and unbounded computational power, but cannot see the random
choices made by the protocol. Our analysis builds on the framework of Augustine
et al. [FOCS 2015], and shows that our distributed algorithm maintains a
constant-degree expander graph with high probability, despite a continuous
adversarial churn with a rate of up to $\mathcal{O}(n/polylog(n))$ per round,
where $n$ is the stable network size. The protocol and proof techniques are not
new, but together they resolve a specific open problem raised in prior work.
The result is a simple, fully distributed, and churn-resilient protocol with
provable guarantees that align with observed empirical behavior.

</details>


### [256] [Implementation and Evaluation of Fast Raft for Hierarchical Consensus](https://arxiv.org/abs/2506.17793)
*Anton Melnychuk,Bryan SebaRaj*

Main category: cs.DC

TL;DR: This paper introduces Fast Raft, an optimized variant of the Raft consensus protocol, focusing on performance improvements with reduced message-round and leader dependence.


<details>
  <summary>Details</summary>
Motivation: Standard Raft has limitations in dynamic environments, mainly due to its reliance on multiple message rounds and centralized leader dependence, which slow consensus processes.

Method: Fast Raft incorporates gRPC infrastructure and Kubernetes deployment, introducing a fast-track mechanism for quicker consensus and leveraging AWS for distributed implementation.

Result: Experimental evaluation shows improved throughput and reduced commit latency with Fast Raft, primarily under conditions of low packet loss, while preserving Raft's core guarantees.

Conclusion: Fast Raft enhances consensus efficiency in distributed systems, especially in scenarios with low packet loss, without compromising safety or liveness guarantees.

Abstract: We present the first open-source implementation and evaluation of Fast Raft,
a hierarchical consensus protocol designed for dynamic, distributed
environments. Fast Raft reduces the number of message rounds needed to commit
log entries compared to standard Raft by introducing a fast-track mechanism and
reducing leader dependence. Our implementation uses gRPC and Kubernetes-based
deployment across AWS availability zones. Experimental results demonstrate a
throughput improvement and reduced commit latency under low packet loss
conditions, while maintaining Raft's safety and liveness guarantees.

</details>


### [257] [CFTel: A Practical Architecture for Robust and Scalable Telerobotics with Cloud-Fog Automation](https://arxiv.org/abs/2506.17991)
*Thien Tran,Jonathan Kua,Minh Tran,Honghao Lyu,Thuong Hoang,Jiong Jin*

Main category: cs.DC

TL;DR: The paper explores Cloud-Fog Telerobotics (CFTel) as a solution to address latency, reliability, and scalability challenges in conventional cloud-based telerobotics, leveraging cutting-edge computing and AI technologies.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in overcoming the limitations of cloud-based telerobotics, such as latency and reliability issues, which obstruct real-time performance required for critical applications.

Method: The paper synthesizes recent advancements in CFTel, analyzing Cloud-Edge-Robotics architecture and emerging technologies like 5G, Edge Intelligence, and Digital Twins.

Result: CFTel demonstrates improved real-time control, scalability, autonomy, and service-oriented solutions in telerobotics applications while acknowledging practical challenges.

Conclusion: The paper identifies CFTel as a promising paradigm for future telerobotics research and applications, providing foundational insights for researchers and industry stakeholders.

Abstract: Telerobotics is a key foundation in autonomous Industrial Cyber-Physical
Systems (ICPS), enabling remote operations across various domains. However,
conventional cloud-based telerobotics suffers from latency, reliability,
scalability, and resilience issues, hindering real-time performance in critical
applications. Cloud-Fog Telerobotics (CFTel) builds on the Cloud-Fog Automation
(CFA) paradigm to address these limitations by leveraging a distributed
Cloud-Edge-Robotics computing architecture, enabling deterministic
connectivity, deterministic connected intelligence, and deterministic networked
computing. This paper synthesizes recent advancements in CFTel, aiming to
highlight its role in facilitating scalable, low-latency, autonomous, and
AI-driven telerobotics. We analyze architectural frameworks and technologies
that enable them, including 5G Ultra-Reliable Low-Latency Communication, Edge
Intelligence, Embodied AI, and Digital Twins. The study demonstrates that CFTel
has the potential to enhance real-time control, scalability, and autonomy while
supporting service-oriented solutions. We also discuss practical challenges,
including latency constraints, cybersecurity risks, interoperability issues,
and standardization efforts. This work serves as a foundational reference for
researchers, stakeholders, and industry practitioners in future telerobotics
research.

</details>


### [258] [Leveraging Cloud-Fog Automation for Autonomous Collision Detection and Classification in Intelligent Unmanned Surface Vehicles](https://arxiv.org/abs/2506.18024)
*Thien Tran,Quang Nguyen,Jonathan Kua,Minh Tran,Toan Luu,Thuong Hoang,Jiong Jin*

Main category: cs.DC

TL;DR: The paper proposes a distributed Cloud-Edge-IoT architecture to address computational and latency challenges in maritime Industrial Cyber-Physical Systems (ICPS) for Unmanned Surface Vehicles (USVs), demonstrating improved efficiency, responsiveness, and scalability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational limitations and communication latency in real-time maritime ICPS that hinder scalability and responsiveness, especially for USVs.

Method: The authors propose a tri-layered distributed architecture based on Cloud-Fog Automation principles. The architecture includes: a Cloud Layer for centralized analytics and model refinement, an Edge Layer for localized AI-driven decisions, and an IoT Layer for sensor data acquisition.

Result: The architecture achieved a classification accuracy of 86%, improved computational efficiency, latency performance, and overall scalability compared to conventional methods.

Conclusion: The proposed framework effectively addresses low-latency constraints and scalability issues, offering a modular and scalable solution for enhancing maritime ICPS and enabling robust AI-driven autonomy in USVs.

Abstract: Industrial Cyber-Physical Systems (ICPS) technologies are foundational in
driving maritime autonomy, particularly for Unmanned Surface Vehicles (USVs).
However, onboard computational constraints and communication latency
significantly restrict real-time data processing, analysis, and predictive
modeling, hence limiting the scalability and responsiveness of maritime ICPS.
To overcome these challenges, we propose a distributed Cloud-Edge-IoT
architecture tailored for maritime ICPS by leveraging design principles from
the recently proposed Cloud-Fog Automation paradigm. Our proposed architecture
comprises three hierarchical layers: a Cloud Layer for centralized and
decentralized data aggregation, advanced analytics, and future model
refinement; an Edge Layer that executes localized AI-driven processing and
decision-making; and an IoT Layer responsible for low-latency sensor data
acquisition. Our experimental results demonstrated improvements in
computational efficiency, responsiveness, and scalability. When compared with
our conventional approaches, we achieved a classification accuracy of 86\%,
with an improved latency performance. By adopting Cloud-Fog Automation, we
address the low-latency processing constraints and scalability challenges in
maritime ICPS applications. Our work offers a practical, modular, and scalable
framework to advance robust autonomy and AI-driven decision-making and autonomy
for intelligent USVs in future maritime ICPS.

</details>


### [259] [Edge Association Strategies for Synthetic Data Empowered Hierarchical Federated Learning with Non-IID Data](https://arxiv.org/abs/2506.18259)
*Jer Shyuan Ng,Aditya Pribadi Kalapaaking,Xiaoyu Xia,Dusit Niyato,Ibrahim Khalil,Iqbal Gondal*

Main category: cs.DC

TL;DR: The paper introduces a synthetic-data-empowered Hierarchical Federated Learning (HFL) framework to tackle issues like worker dropout, non-IID data, and participation incentives.


<details>
  <summary>Details</summary>
Motivation: Issues like FL model performance degradation due to worker dropout, large communication rounds in non-IID scenarios, and limited cooperation from workers motivated this research.

Method: Incorporates edge servers to distribute synthetic datasets and reward worker participation, while allowing workers to choose servers based on computational resource considerations.

Result: Efficient model convergence despite non-IID data and better worker participation through reward mechanisms.

Conclusion: The synthetic-data-empowered HFL framework addresses key challenges in FL with improved model performance and better worker cooperation.

Abstract: In recent years, Federated Learning (FL) has emerged as a widely adopted
privacy-preserving distributed training approach, attracting significant
interest from both academia and industry. Research efforts have been dedicated
to improving different aspects of FL, such as algorithm improvement, resource
allocation, and client selection, to enable its deployment in distributed edge
networks for practical applications. One of the reasons for the poor FL model
performance is due to the worker dropout during training as the FL server may
be located far away from the FL workers. To address this issue, an Hierarchical
Federated Learning (HFL) framework has been introduced, incorporating an
additional layer of edge servers to relay communication between the FL server
and workers. While the HFL framework improves the communication between the FL
server and workers, large number of communication rounds may still be required
for model convergence, particularly when FL workers have non-independent and
identically distributed (non-IID) data. Moreover, the FL workers are assumed to
fully cooperate in the FL training process, which may not always be true in
practical situations. To overcome these challenges, we propose a
synthetic-data-empowered HFL framework that mitigates the statistical issues
arising from non-IID local datasets while also incentivizing FL worker
participation. In our proposed framework, the edge servers reward the FL
workers in their clusters for facilitating the FL training process. To improve
the performance of the FL model given the non-IID local datasets of the FL
workers, the edge servers generate and distribute synthetic datasets to FL
workers within their clusters. FL workers determine which edge server to
associate with, considering the computational resources required to train on
both their local datasets and the synthetic datasets.

</details>


### [260] [The Power of Strong Linearizability: the Difficulty of Consistent Refereeing](https://arxiv.org/abs/2506.18401)
*Hagit Attiya,Armando Castañeda,Constantin Enea*

Main category: cs.DC

TL;DR: This paper investigates the relationship between agreement and strongly linearizable implementations of concurrent objects, introducing 'contest objects' to model coordination limitations. It shows that implementing such objects requires significant coordination power.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore the underlying coordination requirements of strongly linearizable, lock-free, and wait-free implementations of concurrent objects, as these implementations are central to distributed computing but face inherent limitations.

Method: The authors introduce the concept of 'contest objects' to encapsulate the properties and challenges of strongly linearizable and lock-free/wait-free implementations. They analyze the required coordination primitives via theoretical proofs and impossibility results.

Result: The study identifies that strongly linearizable implementations of certain objects fail when relying solely on non-universal primitives, as these implementations require a distinguished referee process to mediate among competing processes under high coordination.

Conclusion: Strong linearizability entails inherent limitations due to its high coordination demands. Consequently, objects like stacks, queues, and snapshots cannot be implemented in a strongly linearizable manner with non-universal primitives, but contest objects provide a formal structure to understand these constraints.

Abstract: This paper studies the relation between agreement and strongly linearizable
implementations of various objects. This leads to new results about
implementations of concurrent objects from various primitives including window
registers and interfering primitives. We consider implementations that provide
both strong linearizability and decisive linearizability.
  We identify that lock-free, respectively, wait-free, strongly linearizable
implementations of several concurrent objects entail a form of agreement that
is weaker than consensus but impossible to strongly-linearizable implement with
combinations of non-universal primitives. In both cases, lock-free and
wait-free, this form of agreement requires a distinguished process to referee a
competition that involves all other processes. Our results show that consistent
refereeing of such competitions (i.e. the outcome of the competition does not
change in extensions of the current execution) requires high coordination
power.
  More specifically, two contest objects are defined and used to capture the
power of strong linearizability in lock-free and wait-free implementations,
respectively. Both objects are strictly weaker than consensus, in the sense
that they have a wait-free linearizable (in fact, decisively linearizable)
implementation from reads and writes. The contest objects capture strong
linearizability since (1) they have strongly linearizable implementations from
several ``high-level'' objects like stacks, queues, snapshots, counters, and
therefore, impossibility results for them carry over to these objects, and (2)
they admit powerful impossibility results for strong linearizability that
involve window registers and interfering primitives, which are non-universal.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [261] [MMET: A Multi-Input and Multi-Scale Transformer for Efficient PDEs Solving](https://arxiv.org/abs/2506.17230)
*Yichen Luo,Jia Wang,Dapeng Lan,Yu Liu,Zhibo Pang*

Main category: cs.LG

TL;DR: This paper introduces MMET, a framework designed for solving multi-scale, multi-input PDE problems efficiently using machine learning. MMET minimizes computational costs and improves accuracy compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The traditional methods for solving PDEs struggle with generic and efficient handling of multi-scale and multi-input problems while facing high computational demands. The authors aim to address these limitations using a machine learning-based approach.

Method: MMET utilizes an efficient transformer framework combining mesh and query points sequences with features like Gated Condition Embedding (GCE), Hilbert curve-based reserialization, and patch embedding for computational efficiency.

Result: MMET achieves superior performance on diverse physical benchmarks in terms of both accuracy and computational efficiency compared to existing solutions.

Conclusion: MMET is demonstrated as a scalable and robust solution for real-time PDE solving in engineering and physics applications, also opening up possibilities for pre-trained domain-specific large models.

Abstract: Partial Differential Equations (PDEs) are fundamental for modeling physical
systems, yet solving them in a generic and efficient manner using machine
learning-based approaches remains challenging due to limited multi-input and
multi-scale generalization capabilities, as well as high computational costs.
This paper proposes the Multi-input and Multi-scale Efficient Transformer
(MMET), a novel framework designed to address the above challenges. MMET
decouples mesh and query points as two sequences and feeds them into the
encoder and decoder, respectively, and uses a Gated Condition Embedding (GCE)
layer to embed input variables or functions with varying dimensions, enabling
effective solutions for multi-scale and multi-input problems. Additionally, a
Hilbert curve-based reserialization and patch embedding mechanism decrease the
input length. This significantly reduces the computational cost when dealing
with large-scale geometric models. These innovations enable efficient
representations and support multi-scale resolution queries for large-scale and
multi-input PDE problems. Experimental evaluations on diverse benchmarks
spanning different physical fields demonstrate that MMET outperforms SOTA
methods in both accuracy and computational efficiency. This work highlights the
potential of MMET as a robust and scalable solution for real-time PDE solving
in engineering and physics-based applications, paving the way for future
explorations into pre-trained large-scale models in specific domains. This work
is open-sourced at https://github.com/YichenLuo-0/MMET.

</details>


### [262] [PCaM: A Progressive Focus Attention-Based Information Fusion Method for Improving Vision Transformer Domain Adaptation](https://arxiv.org/abs/2506.17232)
*Zelin Zang,Fei Wang,Liangyu Li,Jinlin Wu,Chunshui Zhao,Zhen Lei,Baigui Sun*

Main category: cs.LG

TL;DR: The paper introduces the Progressive Focus Cross-Attention Mechanism (PCaM) to improve unsupervised domain adaptation (UDA) by focusing on foreground semantics and addressing object mismatch issues.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome challenges in unsupervised domain adaptation caused by mismatches in foreground object size and spatial distribution between source and target domains, which hinder effective domain alignment in current Vision Transformer-based methods.

Method: The proposed method, PCaM, leverages a progressive approach to filter out background information during cross-attention, enabling fusion of foreground semantics. Additionally, an attentional guidance loss is introduced to guide the model's focus to task-relevant regions, ensuring better cross-domain attention consistency.

Result: Extensive experiments on datasets like Office-Home, DomainNet, VisDA-2017, and remote sensing domains show that PCaM significantly improves adaptation performance over existing methods, setting new state-of-the-art results.

Conclusion: PCaM, a lightweight and easily integrable mechanism, enhances foreground information fusion and addresses limitations in UDA frameworks, proving effective for domain alignment and performance improvement.

Abstract: Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a
labeled source domain to an unlabeled target domain. Recent UDA methods based
on Vision Transformers (ViTs) have achieved strong performance through
attention-based feature alignment. However, we identify a key limitation:
foreground object mismatch, where the discrepancy in foreground object size and
spatial distribution across domains weakens attention consistency and hampers
effective domain alignment. To address this issue, we propose the Progressive
Focus Cross-Attention Mechanism (PCaM), which progressively filters out
background information during cross-attention, allowing the model to focus on
and fuse discriminative foreground semantics across domains. We further
introduce an attentional guidance loss that explicitly directs attention toward
task-relevant regions, enhancing cross-domain attention consistency. PCaM is
lightweight, architecture-agnostic, and easy to integrate into existing
ViT-based UDA pipelines. Extensive experiments on Office-Home, DomainNet,
VisDA-2017, and remote sensing datasets demonstrate that PCaM significantly
improves adaptation performance and achieves new state-of-the-art results,
validating the effectiveness of attention-guided foreground fusion for domain
adaptation.

</details>


### [263] [Graph Neural Networks in Multi-Omics Cancer Research: A Structured Survey](https://arxiv.org/abs/2506.17234)
*Payam Zohari,Mostafa Haghir Chehreghani*

Main category: cs.LG

TL;DR: The paper reviews application of graph neural networks (GNNs) in multi-omics cancer research, highlighting various architectures, biological tasks, and trends such as hybrid models and attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Challenges in unraveling complex biological mechanisms of cancer through multi-omics data integration inspired the exploration of GNNs as a modeling tool.

Method: The paper systematically analyzes recent studies employing GNNs in multi-omics research and classifies approaches based on omics layers, GNN structures, and biological tasks.

Result: Key trends identified include hybrid and interpretable models, attention mechanisms, contrastive learning, patient-specific graphs, and knowledge-driven priors.

Conclusion: The work serves as a comprehensive guide to design effective GNN-based pipelines for cancer research, identifying current limitations and suggesting future directions like patient-specific models.

Abstract: The task of data integration for multi-omics data has emerged as a powerful
strategy to unravel the complex biological underpinnings of cancer. Recent
advancements in graph neural networks (GNNs) offer an effective framework to
model heterogeneous and structured omics data, enabling precise representation
of molecular interactions and regulatory networks. This systematic review
explores several recent studies that leverage GNN-based architectures in
multi-omics cancer research. We classify the approaches based on their targeted
omics layers, graph neural network structures, and biological tasks such as
subtype classification, prognosis prediction, and biomarker discovery. The
analysis reveals a growing trend toward hybrid and interpretable models,
alongside increasing adoption of attention mechanisms and contrastive learning.
Furthermore, we highlight the use of patient-specific graphs and
knowledge-driven priors as emerging directions. This survey serves as a
comprehensive resource for researchers aiming to design effective GNN-based
pipelines for integrative cancer analysis, offering insights into current
practices, limitations, and potential future directions.

</details>


### [264] [Training a Scientific Reasoning Model for Chemistry](https://arxiv.org/abs/2506.17238)
*Siddharth M. Narayanan,James D. Braza,Ryan-Rhys Griffiths,Albert Bou,Geemi Wellawatte,Mayk Caldas Ramos,Ludovico Mitchener,Samuel G. Rodriques,Andrew D. White*

Main category: cs.LG

TL;DR: The paper introduces ether0, a 24B parameter reasoning model capable of performing chemistry-related tasks with post-training instead of extensive domain-specific pretraining, driven by reinforcement learning on over 640,000 chemistry problems.


<details>
  <summary>Details</summary>
Motivation: Investigate whether reasoning models, which generate chain-of-thought outputs, can generalize beyond traditional domains (like mathematics), particularly in chemistry, without requiring extensive domain-specific pretraining.

Method: Developed ether0, a 24B parameter language model trained using reinforcement learning on 640,730 chemistry tasks, covering diverse problem areas such as synthesizability and receptor activity.

Result: Ether0 outperforms general chemistry models, frontier models, and human experts in molecular design, while being more data-efficient compared to specialized models.

Conclusion: This approach demonstrates that reasoning models can efficiently adapt to scientific domains such as chemistry and may be viable for enabling specialized tasks in other scientific fields with data efficiency.

Abstract: Reasoning models are large language models that emit a long chain-of-thought
before answering, providing both higher accuracy and explicit reasoning for
their response. A major question has been whether language model reasoning
generalizes beyond mathematics, programming, and logic, where most previous
work has focused. We demonstrate that reasoning models can be post-trained for
chemistry without additional domain pretraining, and require substantially less
data compared to contemporary domain-specific models. We report ether0, a 24B
parameter LLM (based on Mistral-Small-24B) that can reason in natural language
and respond with chemical structures. This reasoning model was trained with
reinforcement learning on 640,730 experimentally-grounded chemistry problems
across 375 tasks ranging from synthesizability, to blood-brain barrier
permeability, to human receptor activity, to scent. Our model exceeds
general-purpose chemistry models, frontier models, and human experts on
molecular design tasks. It is also more data efficient relative to specialized
models. We anticipate that this method can be applied to train data-efficient
language models specialized for tasks across a wide variety of scientific
domains.

</details>


### [265] [AnalogNAS-Bench: A NAS Benchmark for Analog In-Memory Computing](https://arxiv.org/abs/2506.18495)
*Aniss Bessalah,Hatem Mohamed Abdelmoumen,Karima Benatchba,Hadjer Benmeziane*

Main category: cs.LG

TL;DR: The paper introduces AnalogNAS-Bench, a NAS benchmark tailored for Analog In-memory Computing (AIMC) to address AIMC-specific non-idealities, providing key insights into AIMC-optimized architectures.


<details>
  <summary>Details</summary>
Motivation: Deep Neural Networks (DNNs) are not inherently optimized for AIMC due to hardware limitations and non-idealities, prompting the need for NAS approaches tailored for AIMC-specific constraints.

Method: The authors developed AnalogNAS-Bench, a benchmark platform specifically designed to evaluate NAS methodologies for AIMC by incorporating AIMC-specific noise and constraints.

Result: The study identifies three insights: quantization methods fail to capture AIMC-specific noise; robust architectures prefer wider-branched designs; skip connections mitigate resilience issues against temporal noise drift.

Conclusion: AnalogNAS-Bench highlights the shortcomings of current NAS approaches in AIMC and offers a specialized platform for future development of analog-aware architecture searches.

Abstract: Analog In-memory Computing (AIMC) has emerged as a highly efficient paradigm
for accelerating Deep Neural Networks (DNNs), offering significant energy and
latency benefits over conventional digital hardware. However, state-of-the-art
neural networks are not inherently designed for AIMC, as they fail to account
for its unique non-idealities. Neural Architecture Search (NAS) is thus needed
to systematically discover neural architectures optimized explicitly for AIMC
constraints. However, comparing NAS methodologies and extracting insights about
robust architectures for AIMC requires a dedicated NAS benchmark that
explicitly accounts for AIMC-specific hardware non-idealities. To address this,
we introduce AnalogNAS-Bench, the first NAS benchmark tailored specifically for
AIMC. Our study reveals three key insights: (1) standard quantization
techniques fail to capture AIMC-specific noises, (2) robust architectures tend
to feature wider and branched blocks, (3) skip connections improve resilience
to temporal drift noise. These insights highlight the limitations of current
NAS benchmarks for AIMC and pave the way for future analog-aware NAS. All the
implementations used in this paper can be found at
https://github.com/IBM/analog-nas/tree/main/analognasbench.

</details>


### [266] [Recursive Learning-Based Virtual Buffering for Analytical Global Placement](https://arxiv.org/abs/2506.17247)
*Andrew B. Kahng,Yiting Liu,Zhiang Wang*

Main category: cs.LG

TL;DR: MLBuf-RePlAce is an open-source framework enhancing global placement by incorporating virtual buffering via a learning-based approach, significantly improving timing metrics.


<details>
  <summary>Details</summary>
Motivation: Address limitations in buffer-aware placement: computational cost of traditional methods and shortcomings in existing ML-based solutions regarding Electrical Rule Check (ERC) violations.

Method: Developed MLBuf-RePlAce, a learning-driven analytical framework for buffering-aware placement, utilizing recursive generative models to predict buffer types and locations efficiently.

Result: Achieved up to 56% improvement in Total Negative Slack (TNS) during global placement with no degradation in post-route power, showcasing advantages in both open-source and commercial flows.

Conclusion: The approach improves timing closure and integrates virtual buffering effectively into physical design, maintaining power efficiency and overcoming traditional challenges.

Abstract: Due to the skewed scaling of interconnect versus cell delay in modern
technology nodes, placement with buffer porosity (i.e., cell density) awareness
is essential for timing closure in physical synthesis flows. However, existing
approaches face two key challenges: (i) traditional van Ginneken-Lillis-style
buffering approaches are computationally expensive during global placement; and
(ii) machine learning-based approaches, such as BufFormer, lack a thorough
consideration of Electrical Rule Check (ERC) violations and fail to "close the
loop" back into the physical design flow. In this work, we propose
MLBuf-RePlAce, the first open-source learning-driven virtual buffering-aware
analytical global placement framework, built on top of the OpenROAD
infrastructure. MLBuf-RePlAce adopts an efficient recursive learning-based
generative buffering approach to predict buffer types and locations, addressing
ERC violations during global placement. We compare MLBuf-RePlAce against the
default virtual buffering-based timing-driven global placer in OpenROAD, using
open-source testcases from the TILOS MacroPlacement and OpenROAD-flow-scripts
repositories. Without degradation of post-route power, MLBuf-RePlAce achieves
(maximum, average) improvements of (56%, 31%) in total negative slack (TNS)
within the open-source OpenROAD flow. When evaluated by completion in a
commercial flow, MLBuf-RePlAce achieves (maximum, average) improvements of
(53%, 28%) in TNS with an average of 0.2% improvement in post-route power.

</details>


### [267] [Efficient Quantification of Multimodal Interaction at Sample Level](https://arxiv.org/abs/2506.17248)
*Zequn Yang,Hongfa Wang,Di Hu*

Main category: cs.LG

TL;DR: This paper introduces a method, LSMI, for measuring multimodal interactions at the sample level, rooted in pointwise information theory.


<details>
  <summary>Details</summary>
Motivation: Understanding redundancy, uniqueness, and synergy in multimodal systems is key but challenging due to the complexity of accurate, sample-level quantification.

Method: The authors proposed LSMI, a sample-wise interaction estimation method that uses pointwise information theory and efficient entropy estimation for continuous data.

Result: Extensive experiments validated LSMI's accuracy and efficiency. The approach uncovered detailed dynamics in multimodal data, supporting applications like knowledge distillation and model ensembling.

Conclusion: LSMI offers a fine-grained tool for interaction analysis in multimodal systems, enabling practical and novel applications.

Abstract: Interactions between modalities -- redundancy, uniqueness, and synergy --
collectively determine the composition of multimodal information. Understanding
these interactions is crucial for analyzing information dynamics in multimodal
systems, yet their accurate sample-level quantification presents significant
theoretical and computational challenges. To address this, we introduce the
Lightweight Sample-wise Multimodal Interaction (LSMI) estimator, rigorously
grounded in pointwise information theory. We first develop a redundancy
estimation framework, employing an appropriate pointwise information measure to
quantify this most decomposable and measurable interaction. Building upon this,
we propose a general interaction estimation method that employs efficient
entropy estimation, specifically tailored for sample-wise estimation in
continuous distributions. Extensive experiments on synthetic and real-world
datasets validate LSMI's precision and efficiency. Crucially, our sample-wise
approach reveals fine-grained sample- and category-level dynamics within
multimodal data, enabling practical applications such as redundancy-informed
sample partitioning, targeted knowledge distillation, and interaction-aware
model ensembling. The code is available at
https://github.com/GeWu-Lab/LSMI_Estimator.

</details>


### [268] [Improving Prediction Certainty Estimation for Reliable Early Exiting via Null Space Projection](https://arxiv.org/abs/2506.17249)
*Jianing He,Qi Zhang,Duoqian Miao,Yi Kun,Shufeng Hao,Hongyun Zhang,Zhihua Wei*

Main category: cs.LG

TL;DR: The paper introduces a novel early exiting method for pre-trained language models that improves prediction certainty estimation by addressing the issues caused by class-irrelevant features. This method significantly boosts inference efficiency while maintaining high performance.


<details>
  <summary>Details</summary>
Motivation: Current early exiting methods for pre-trained language models often overestimate prediction certainty, leading to premature exits of samples with incorrect results due to neglecting class-irrelevant information.

Method: The paper defines an NSP score to measure prediction certainty by accounting for class-irrelevant information in features. On this basis, it proposes the Certainty-Aware Probability (CAP) score, which combines logits and the NSP score to make more accurate early exiting decisions.

Result: The method achieved a 2.19x average speed-up on the GLUE benchmark, outperforming the state-of-the-art ConsistentEE by 28%, with negligible trade-off in model performance.

Conclusion: The proposed CAP-based early exiting method effectively balances inference efficiency and task accuracy, offering superior performance compared to existing methods and setting a new benchmark for early exiting techniques in PLMs.

Abstract: Early exiting has demonstrated great potential in accelerating the inference
of pre-trained language models (PLMs) by enabling easy samples to exit at
shallow layers, eliminating the need for executing deeper layers. However,
existing early exiting methods primarily rely on class-relevant logits to
formulate their exiting signals for estimating prediction certainty, neglecting
the detrimental influence of class-irrelevant information in the features on
prediction certainty. This leads to an overestimation of prediction certainty,
causing premature exiting of samples with incorrect early predictions. To
remedy this, we define an NSP score to estimate prediction certainty by
considering the proportion of class-irrelevant information in the features. On
this basis, we propose a novel early exiting method based on the
Certainty-Aware Probability (CAP) score, which integrates insights from both
logits and the NSP score to enhance prediction certainty estimation, thus
enabling more reliable exiting decisions. The experimental results on the GLUE
benchmark show that our method can achieve an average speed-up ratio of 2.19x
across all tasks with negligible performance degradation, surpassing the
state-of-the-art (SOTA) ConsistentEE by 28%, yielding a better trade-off
between task performance and inference efficiency. The code is available at
https://github.com/He-Jianing/NSP.git.

</details>


### [269] [Towards Interpretable Adversarial Examples via Sparse Adversarial Attack](https://arxiv.org/abs/2506.17250)
*Fudong Lin,Jiadong Lou,Hao Wang,Brian Jalaian,Xu Yuan*

Main category: cs.LG

TL;DR: This paper develops a novel sparse attack for understanding the vulnerabilities of DNNs by minimizing adversarial perturbations under the l0 constraint, achieving improved efficiency, transferability, and attack strength.


<details>
  <summary>Details</summary>
Motivation: Sparse attacks under the l0 constraint are critical for interpreting the vulnerabilities of DNNs, but existing methods suffer from poor sparsity, inefficiency, weak transferability, and insufficient attack strength.

Method: The paper introduces a theoretical parameterization technique to approximate the otherwise NP-hard l0 problem for sparsity optimization. A new loss function is proposed to maximize adversarial impact while minimizing perturbed pixels, alongside extensive experiments for validation.

Result: The proposed approach outperforms existing methods in computational efficiency, transferability, and strength of attacks. It also provides insights into interpretable adversarial examples by uncovering two types of adversarial noise: 'obscuring' and 'leading' noise.

Conclusion: The approach offers a benchmark for evaluating DNN robustness and improves the interpretability of adversarial perturbations, making substantial advancements over existing sparse attacks.

Abstract: Sparse attacks are to optimize the magnitude of adversarial perturbations for
fooling deep neural networks (DNNs) involving only a few perturbed pixels
(i.e., under the l0 constraint), suitable for interpreting the vulnerability of
DNNs. However, existing solutions fail to yield interpretable adversarial
examples due to their poor sparsity. Worse still, they often struggle with
heavy computational overhead, poor transferability, and weak attack strength.
In this paper, we aim to develop a sparse attack for understanding the
vulnerability of CNNs by minimizing the magnitude of initial perturbations
under the l0 constraint, to overcome the existing drawbacks while achieving a
fast, transferable, and strong attack to DNNs. In particular, a novel and
theoretical sound parameterization technique is introduced to approximate the
NP-hard l0 optimization problem, making directly optimizing sparse
perturbations computationally feasible. Besides, a novel loss function is
designed to augment initial perturbations by maximizing the adversary property
and minimizing the number of perturbed pixels simultaneously. Extensive
experiments are conducted to demonstrate that our approach, with theoretical
performance guarantees, outperforms state-of-the-art sparse attacks in terms of
computational overhead, transferability, and attack strength, expecting to
serve as a benchmark for evaluating the robustness of DNNs. In addition,
theoretical and empirical results validate that our approach yields sparser
adversarial examples, empowering us to discover two categories of noises, i.e.,
"obscuring noise" and "leading noise", which will help interpret how
adversarial perturbation misleads the classifiers into incorrect predictions.
Our code is available at https://github.com/fudong03/SparseAttack.

</details>


### [270] [Training-free LLM Verification via Recycling Few-shot Examples](https://arxiv.org/abs/2506.17251)
*Dongseok Lee,Jimyung Hong,Dongyoung Kim,Jaehyung Kim*

Main category: cs.LG

TL;DR: LLMs struggle with reasoning consistency, and the paper proposes 'Referi,' a framework leveraging few-shot examples for response verification, showing accuracy improvement across tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs face challenges in reasoning stochasticity and varying conclusions, limiting their consistency and utility.

Method: They introduced 'Referi,' which uses few-shot examples not only for generation but also as tools for candidate evaluations via Bayesian-inspired scoring.

Result: Experiments across seven tasks and three LLMs show accuracy improvement of 4.8%, proving effectiveness without requiring extra training.

Conclusion: Referi enhances response selection accuracy for LLMs by repurposing few-shot examples intelligently, avoiding limitations in other methods and costly training.

Abstract: Although LLMs have achieved remarkable performance, the inherent
stochasticity of their reasoning process and varying conclusions present
significant challenges. Majority voting or Best-of-N with external verification
models has been explored to find the most promising solution among multiple LLM
outputs. However, these approaches have certain limitations, such as limited
applicability or the cost of an additional training step. To address this
problem, we propose a novel and effective framework that Recycles Few-shot
examples to verify LLM outputs (Referi). Our key idea is to additionally
utilize the given few-shot examples to evaluate the candidate outputs of the
target query, not only using them to generate outputs as the conventional
few-shot prompting setup. Specifically, Referi evaluates the generated outputs
by combining two different scores, designed motivated from Bayes' rule, and
subsequently selects the candidate that is both confidently determined and
contextually coherent through a few additional LLM inferences. Experiments with
three different LLMs and across seven diverse tasks demonstrate that our
framework significantly improves the accuracy of LLMs-achieving an average gain
of 4.8%-through effective response selection, without additional training.

</details>


### [271] [Adaptive Sample Scheduling for Direct Preference Optimization](https://arxiv.org/abs/2506.17252)
*Zixuan Huang,Yikun Ban,Lean Fu,Xiaojie Li,Zhongxiang Dai,Jianxin Li,Deqing Wang*

Main category: cs.LG

TL;DR: The paper introduces an adaptive sample scheduling technique, SamS, to improve Direct Preference Optimization for aligning large language models with human preferences.


<details>
  <summary>Details</summary>
Motivation: The performance of Direct Preference Optimization is heavily reliant on the quality of human preference data, but current data selection strategies often neglect the evolving states of the language model during training.

Method: They propose SamS, a sample scheduling algorithm that dynamically selects training samples in each batch based on feedback from the LLM's learning states.

Result: SamS significantly improves performance across tasks in the DPO process while requiring minimal additional computational resources.

Conclusion: Integrating SamS with the DPO algorithm enhances large language model alignment without altering the underlying optimization method, paving the way for better use of static preference datasets.

Abstract: Direct Preference Optimization (DPO) has emerged as an effective approach for
aligning large language models (LLMs) with human preferences. However, its
performance is highly dependent on the quality of the underlying human
preference data. To address this bottleneck, prior work has explored various
data selection strategies, but these methods often overlook the impact of the
evolving states of the language model during the DPO process. %including active
querying, response pair selection, and data pre-selection. In this paper, we
introduce a novel problem: Sample Scheduling for DPO, which aims to dynamically
and adaptively schedule training samples based on the model's evolving states
throughout preference optimization. To solve this problem, we propose SamS, an
efficient and effective algorithm that adaptively selects samples in each
training batch based on the LLM's learning feedback to maximize the potential
generalization performance. Notably, without modifying the core DPO algorithm,
simply integrating SamS significantly improves performance across tasks, with
minimal additional computational overhead. This work points to a promising new
direction for improving LLM alignment through more effective utilization of
fixed preference datasets.

</details>


### [272] [MS-TVNet:A Long-Term Time Series Prediction Method Based on Multi-Scale Dynamic Convolution](https://arxiv.org/abs/2506.17253)
*Chenghan Li,Mingchen Li,Yipu Liao,Ruisheng Diao*

Main category: cs.LG

TL;DR: The paper introduces MS-TVNet, a convolutional approach for long-term time series prediction, achieving state-of-the-art (SOTA) performance.


<details>
  <summary>Details</summary>
Motivation: Convolutional networks are underexplored in long-term time series prediction compared to Transformers and MLPs.

Method: The paper presents a multi-scale time series reshape module and builds upon it to form MS-TVNet, a multi-scale 3D dynamic convolutional neural network.

Result: MS-TVNet shows superior performance and achieves SOTA results across diverse datasets for long-term time series prediction.

Conclusion: The study demonstrates the potential of convolutional networks in capturing complex temporal patterns, paving the way for future research.

Abstract: Long-term time series prediction has predominantly relied on Transformer and
MLP models, while the potential of convolutional networks in this domain
remains underexplored. To address this gap, we introduce a novel multi-scale
time series reshape module, which effectively captures the relationships among
multi-period patches and variable dependencies. Building upon this module, we
propose MS-TVNet, a multi-scale 3D dynamic convolutional neural network.
Through comprehensive evaluations on diverse datasets, MS-TVNet demonstrates
superior performance compared to baseline models, achieving state-of-the-art
(SOTA) results in long-term time series prediction. Our findings highlight the
effectiveness of leveraging convolutional networks for capturing complex
temporal patterns, suggesting a promising direction for future research in this
field.The code is realsed on https://github.com/Curyyfaust/TVNet.

</details>


### [273] [Keeping Up with the Models: Online Deployment and Routing of LLMs at Scale](https://arxiv.org/abs/2506.17254)
*Shaoang Li,Jian Li*

Main category: cs.LG

TL;DR: The paper introduces StageRoute, a model management and query routing algorithm for large language models (LLMs) that balances cost and reward while achieving near-optimal performance with provable regret bounds.


<details>
  <summary>Details</summary>
Motivation: The rapid development and obsolescence of large language models (LLMs) create challenges for service providers in managing deployment capacity and cost budgets, requiring an efficient online decision framework.

Method: The paper proposes StageRoute, a two-part algorithm. First, it selects up to a maximum number of models for deployment using confidence bounds. Second, it routes user queries as a budget-constrained bandit sub-problem. Theoretical proofs and experiments are used to validate performance and regret bounds.

Result: Theoretical analysis shows StageRoute achieves a regret of order $T^{2/3}$ with a matching lower bound, implying near-optimality. Experimental results confirm that the algorithm closely approaches optimal performance in real-world-like settings.

Conclusion: StageRoute is an effective hierarchical algorithm for managing dynamic LLM inventories and per-query routing, balancing between deployment costs and user satisfaction with provably near-optimal regret.

Abstract: The rapid pace at which new large language models (LLMs) appear -- and older
ones become obsolete -- forces LLM service providers to juggle a streaming
inventory of models while respecting tight deployment capacity and per-query
cost budgets. We cast the reality as an online decision problem that couples
stage-wise deployment, made at fixed maintenance windows, with per-query
routing among the models kept live. We introduce StageRoute, a hierarchical
algorithm that (i) optimistically selects up to $M_max$ models for the next
stage using reward upper-confidence and cost lower-confidence bounds, then (ii)
solves a budget-constrained bandit sub-problem to route each incoming query. We
prove that StageRoute achieves a regret of order $T^{2/3}$ and provide a
matching lower bound, thereby establishing its near-optimality. Moreover, our
experiments confirm the theory, demonstrating that StageRoute performs close to
the optimum in practical settings.

</details>


### [274] [UltraSketchLLM: Saliency-Driven Sketching for Ultra-Low Bit LLM Compression](https://arxiv.org/abs/2506.17255)
*Sunan Zou,Ziyun Zhang,Xueting Sun,Guojie Luo*

Main category: cs.LG

TL;DR: UltraSketchLLM introduces an innovative compression framework for large language models (LLMs), achieving ultra-low bit compression down to 0.5 bits per weight while maintaining model performance.


<details>
  <summary>Details</summary>
Motivation: The growth of LLMs has exceeded the memory capabilities of edge devices, requiring novel weight compression techniques that push beyond traditional limits like 1-bit quantization.

Method: UltraSketchLLM employs data sketching to represent weights sub-linearly, combining AbsMaxMin sketches, importance-aware space allocation, and a straight-through estimator for compression-aware finetuning.

Result: The framework achieves significant compression (0.5 bits per weight) with competitive perplexity and acceptable latency overhead, verified through experiments on Llama-3.2-1B.

Conclusion: UltraSketchLLM is a viable solution for deploying LLMs in environments with restricted computational and memory resources, addressing the challenges of extreme weight compression without compromising model accuracy.

Abstract: The rapid growth of large language models (LLMs) has outpaced the memory
constraints of edge devices, necessitating extreme weight compression beyond
the 1-bit limit. While quantization reduces model size, it is fundamentally
limited to 1 bit per weight. Existing multiple-to-one compression methods
either rely on mapping tables (inducing memory overhead) or incur severe
accuracy degradation due to random weight grouping. We introduce
UltraSketchLLM, an index-free, sketch-based framework that achieves ultra-low
bit compression (down to 0.5 bits per weight) while preserving model
performance. UltraSketchLLM leverages data sketching, a sub-linear
representation technique from streaming applications, to map multiple weights
to single values with bounded error. Our approach integrates an underestimate
AbsMaxMin sketch to minimize relative errors for small weights,
importance-aware space allocation to prioritize salient weights, and a
straight-through estimator for compression-aware finetuning. Experiments on
Llama-3.2-1B demonstrate up to 0.5-bit compression with competitive perplexity,
alongside tolerable latency overhead. UltraSketchLLM offers a practical
solution for deploying LLMs in resource-constrained environments.

</details>


### [275] [AI to Identify Strain-sensitive Regions of the Optic Nerve Head Linked to Functional Loss in Glaucoma](https://arxiv.org/abs/2506.17262)
*Thanadet Chuangsuwanich,Monisha E. Nongpiur,Fabian A. Braeu,Tin A. Tun,Alexandre Thiery,Shamira Perera,Ching Lin Ho,Martin Buist,George Barbastathis,Tin Aung,Michaël J. A. Girard*

Main category: cs.LG

TL;DR: The study explores how optic nerve head (ONH) biomechanics may improve predictions of visual field (VF) loss patterns in glaucoma and highlights key regions via AI models.


<details>
  <summary>Details</summary>
Motivation: To determine if ONH biomechanics can enhance predictions of specific glaucomatous visual field loss patterns and to identify critical ONH regions via explainable AI.

Method: 237 glaucoma patients were studied, utilizing ONH imaging under normal and elevated intraocular pressure. Strain-sensitive ONH properties were modeled with geometric deep learning, with outcomes validated through AUC measures and explainable AI highlighting key regions.

Result: The models achieved high AUCs (0.77-0.88), demonstrating enhanced prediction of VF loss patterns using ONH strain data. The inferior and inferotemporal rim were identified as primary contributors to these predictions.

Conclusion: ONH strain data is a valuable predictor of VF loss patterns, with the neuroretinal rim playing a more significant role than the lamina cribrosa in model predictions.

Abstract: Objective: (1) To assess whether ONH biomechanics improves prediction of
three progressive visual field loss patterns in glaucoma; (2) to use
explainable AI to identify strain-sensitive ONH regions contributing to these
predictions.
  Methods: We recruited 237 glaucoma subjects. The ONH of one eye was imaged
under two conditions: (1) primary gaze and (2) primary gaze with IOP elevated
to ~35 mmHg via ophthalmo-dynamometry. Glaucoma experts classified the subjects
into four categories based on the presence of specific visual field defects:
(1) superior nasal step (N=26), (2) superior partial arcuate (N=62), (3) full
superior hemifield defect (N=25), and (4) other/non-specific defects (N=124).
Automatic ONH tissue segmentation and digital volume correlation were used to
compute IOP-induced neural tissue and lamina cribrosa (LC) strains.
Biomechanical and structural features were input to a Geometric Deep Learning
model. Three classification tasks were performed to detect: (1) superior nasal
step, (2) superior partial arcuate, (3) full superior hemifield defect. For
each task, the data were split into 80% training and 20% testing sets. Area
under the curve (AUC) was used to assess performance. Explainable AI techniques
were employed to highlight the ONH regions most critical to each
classification.
  Results: Models achieved high AUCs of 0.77-0.88, showing that ONH strain
improved VF loss prediction beyond morphology alone. The inferior and
inferotemporal rim were identified as key strain-sensitive regions,
contributing most to visual field loss prediction and showing progressive
expansion with increasing disease severity.
  Conclusion and Relevance: ONH strain enhances prediction of glaucomatous VF
loss patterns. Neuroretinal rim, rather than the LC, was the most critical
region contributing to model predictions.

</details>


### [276] [Memory Allocation in Resource-Constrained Reinforcement Learning](https://arxiv.org/abs/2506.17263)
*Massimiliano Tamborski,David Abel*

Main category: cs.LG

TL;DR: The paper investigates how memory constraints impact learning and decision-making in reinforcement learning algorithms.


<details>
  <summary>Details</summary>
Motivation: To understand how limited memory influences an agent's learning ability and decision-making process in unknown environments.

Method: The study examines the performance of agents using MCTS and DQN algorithms, emphasizing how memory allocation affects their internal processes.

Result: Results reveal the impact of memory allocation choices on agent performance in episodic and continual learning contexts.

Conclusion: Memory allocation plays a critical role in shaping agent performance, highlighting the importance of designing resource-efficient algorithms.

Abstract: Resource constraints can fundamentally change both learning and
decision-making. We explore how memory constraints influence an agent's
performance when navigating unknown environments using standard reinforcement
learning algorithms. Specifically, memory-constrained agents face a dilemma:
how much of their limited memory should be allocated to each of the agent's
internal processes, such as estimating a world model, as opposed to forming a
plan using that model? We study this dilemma in MCTS- and DQN-based algorithms
and examine how different allocations of memory impact performance in episodic
and continual learning settings.

</details>


### [277] [OAT-Rephrase: Optimization-Aware Training Data Rephrasing for Zeroth-Order LLM Fine-Tuning](https://arxiv.org/abs/2506.17264)
*Jikai Long,Zijian Hu,Xiaodong Yu,Jianwen Xie,Zhaozhuo Xu*

Main category: cs.LG

TL;DR: OAT-Rephrase is introduced to enhance zeroth-order optimization-based fine-tuning of large language models (LLMs), improving performance while retaining memory efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies and instabilities of zeroth-order optimization methods for fine-tuning large language models.

Method: Proposed OAT-Rephrase, a strategy comprising a dual-stage pipeline with a rewriter LLM and a semantic judge to ensure meaningful rephrased training data aligned with optimization-aware dynamics.

Result: OAT-Rephrase demonstrated improvements in MeZO fine-tuning performance across five classification tasks and three LLM architectures.

Conclusion: Optimization-aware rephrasing provides a reusable, low-overhead approach to enhance zeroth-order tuning, closing the gap with first-order methods in terms of results.

Abstract: Fine-tuning large language models (LLMs) using zeroth-order optimization (ZO)
offers a memory-efficient alternative to gradient-based methods but suffers
from slower convergence and unstable optimization due to noisy gradient
estimates. This paper introduces OAT-Rephrase, an Optimization-Aware Training
data rephrasing strategy that leverages an LLM to rephrase training instances
based on its understanding of the ZO dynamics, specifically MeZO, derived
directly from its paper. The approach incorporates a dual-stage pipeline
featuring a rewriter LLM and a semantic judge, ensuring all rephrasings retain
task relevance and logical consistency. Evaluations across five classification
tasks and three LLM architectures demonstrate that OAT-Rephrase consistently
improves MeZO fine-tuning performance, often narrowing or eliminating the gap
with first-order methods. Our findings suggest that optimization-aware
rephrasing serves as a reusable and low-overhead enhancement for zeroth-order
tuning regimes.

</details>


### [278] [Does Multimodal Large Language Model Truly Unlearn? Stealthy MLLM Unlearning Attack](https://arxiv.org/abs/2506.17265)
*Xianren Zhang,Hui Liu,Delvin Ce Zhang,Xianfeng Tang,Qi He,Dongwon Lee,Suhang Wang*

Main category: cs.LG

TL;DR: The paper addresses recovering sensitive information supposedly forgotten by Multimodal Large Language Models (MLLMs) through a novel attack framework called Stealthy Unlearning Attack (SUA).


<details>
  <summary>Details</summary>
Motivation: To address privacy risks stemming from MLLMs, which may fail to completely forget sensitive data during unlearning processes.

Method: The introduced framework involves generating a universal noise pattern which, when applied to image data, reveals information believed to be forgotten by MLLMs.

Result: SUA effectively recovers unlearned data and generalizes well, indicating consistent vulnerabilities in MLLMs.

Conclusion: MLLMs may not truly forget sensitive information; the proposed attack demonstrates that unlearned data can be persistently recovered.

Abstract: Multimodal Large Language Models (MLLMs) trained on massive data may memorize
sensitive personal information and photos, posing serious privacy risks. To
mitigate this, MLLM unlearning methods are proposed, which fine-tune MLLMs to
reduce the ``forget'' sensitive information. However, it remains unclear
whether the knowledge has been truly forgotten or just hidden in the model.
Therefore, we propose to study a novel problem of LLM unlearning attack, which
aims to recover the unlearned knowledge of an unlearned LLM. To achieve the
goal, we propose a novel framework Stealthy Unlearning Attack (SUA) framework
that learns a universal noise pattern. When applied to input images, this noise
can trigger the model to reveal unlearned content. While pixel-level
perturbations may be visually subtle, they can be detected in the semantic
embedding space, making such attacks vulnerable to potential defenses. To
improve stealthiness, we introduce an embedding alignment loss that minimizes
the difference between the perturbed and denoised image embeddings, ensuring
the attack is semantically unnoticeable. Experimental results show that SUA can
effectively recover unlearned information from MLLMs. Furthermore, the learned
noise generalizes well: a single perturbation trained on a subset of samples
can reveal forgotten content in unseen images. This indicates that knowledge
reappearance is not an occasional failure, but a consistent behavior.

</details>


### [279] [CopulaSMOTE: A Copula-Based Oversampling Approach for Imbalanced Classification in Diabetes Prediction](https://arxiv.org/abs/2506.17326)
*Agnideep Aich,Md Monzur Murshed,Sameera Hewage,Amanda Mayeaux*

Main category: cs.LG

TL;DR: The paper proposes using A2 copula-based data augmentation to improve machine learning models for diabetes detection, leading to significant performance improvements over traditional SMOTE methods.


<details>
  <summary>Details</summary>
Motivation: Diabetes is a critical health issue affecting millions globally, and early detection can mitigate risks. Machine learning-based prediction is hindered by data imbalances, prompting the need for innovative augmentation techniques.

Method: The study utilized copula-based data augmentation via the A2 copula to generate minority-class data while preserving dependencies. Four machine learning models—Logistic Regression, Random Forest, Gradient Boosting, and XGBoost—were employed, and outcomes were compared to the SMOTE technique.

Result: XGBoost combined with A2 copula oversampling achieved superior performance, with improvements in accuracy (+4.6%), precision (+15.6%), recall (+20.4%), F1-score (+18.2%), and AUC (+25.5%) over SMOTE.

Conclusion: This research introduces A2 copulas as an effective alternative for data augmentation in imbalanced datasets, demonstrating superior performance in diabetes prediction compared to SMOTE and validating results statistically with the McNemar test.

Abstract: Diabetes mellitus poses a significant health risk, as nearly 1 in 9 people
are affected by it. Early detection can significantly lower this risk. Despite
significant advancements in machine learning for identifying diabetic cases,
results can still be influenced by the imbalanced nature of the data. To
address this challenge, our study considered copula-based data augmentation,
which preserves the dependency structure when generating data for the minority
class and integrates it with machine learning (ML) techniques. We selected the
Pima Indian dataset and generated data using A2 copula, then applied four
machine learning algorithms: logistic regression, random forest, gradient
boosting, and extreme gradient boosting. Our findings indicate that XGBoost
combined with A2 copula oversampling achieved the best performance improving
accuracy by 4.6%, precision by 15.6%, recall by 20.4%, F1-score by 18.2% and
AUC by 25.5% compared to the standard SMOTE method. Furthermore, we
statistically validated our results using the McNemar test. This research
represents the first known use of A2 copulas for data augmentation and serves
as an alternative to the SMOTE technique, highlighting the efficacy of copulas
as a statistical method in machine learning applications.

</details>


### [280] [CF-VLM:CounterFactual Vision-Language Fine-tuning](https://arxiv.org/abs/2506.17267)
*Jusheng Zhang,Kaitong Cai,Yijia Fan,Jian Wang,Keze Wang*

Main category: cs.LG

TL;DR: This paper introduces CF-VLM, a novel framework that enhances visual-language models' causal reasoning by leveraging counterfactual training objectives, resulting in superior compositional reasoning and improved factual consistency.


<details>
  <summary>Details</summary>
Motivation: Visual-language models face challenges in fine-grained discrimination and causal reasoning, relying on statistical correlations without capturing deeper causal logic.

Method: The CF-VLM framework employs three targeted training objectives: maintaining cross-modal alignment, reinforcing scene representation stability, and sharpening sensitivity to causal edits using counterfactual samples.

Result: CF-VLM surpasses baselines and state-of-the-art methods in compositional reasoning and generalization benchmarks, while reducing visual hallucinations for better factual accuracy.

Conclusion: CF-VLM offers significant improvements in causal reasoning, compositional tasks, and visual hallucination mitigation, making it suitable for reliable deployment in real-world applications.

Abstract: Recent advances in vision-language models (VLMs) have greatly improved
cross-modal semantic understanding, yet significant limitations remain in
fine-grained discrimination and deep causal reasoning tasks. Existing VLMs
often rely on superficial statistical correlations, lacking the ability to
capture the underlying causal logic between visual and textual content. To
address this, we propose CounterFactual Vision-Language Fine-tuning (CF-VLM), a
novel framework that enhances the causal reasoning capabilities of VLMs through
the targeted use of counterfactual samples. CF-VLM introduces three
complementary training objectives: maintaining foundational cross-modal
alignment, reinforcing the uniqueness and stability of factual scene
representations against coherent counterfactuals, and sharpening the model's
sensitivity to minimal but critical causal edits. Extensive experiments
demonstrate that CF-VLM consistently outperforms strong baselines and
state-of-the-art methods on compositional reasoning and generalization
benchmarks. Furthermore, it shows promise in mitigating visual hallucinations,
indicating improved factual consistency. Our CF-VLM provides a robust
foundation for deploying VLMs in high-stakes, real-world scenarios requiring
reliable reasoning and interpretability.

</details>


### [281] [SafeRL-Lite: A Lightweight, Explainable, and Constrained Reinforcement Learning Library](https://arxiv.org/abs/2506.17297)
*Satyam Mishra,Phung Thao Vi,Shivam Mishra,Vishwanath Bijalwan,Vijay Bhaskar Semwal,Abdul Manan Khan*

Main category: cs.LG

TL;DR: SafeRL-Lite is an open-source Python library for creating constrained and explainable reinforcement learning agents.


<details>
  <summary>Details</summary>
Motivation: Existing RL toolkits lack mechanisms for enforcing hard safety constraints and producing human-interpretable decision rationales.

Method: SafeRL-Lite wraps standard Gym environments and deep Q-learning agents to enable safety-aware training and real-time post-hoc explanations using SHAP values and saliency maps.

Result: The library proved effective in constrained CartPole variants, offering visualizations of policy logic and safety adherence.

Conclusion: SafeRL-Lite addresses gaps in RL toolkits by supporting both safety constraints and explainability, aiming to foster more responsible RL development.

Abstract: We introduce SafeRL-Lite, an open-source Python library for building
reinforcement learning (RL) agents that are both constrained and explainable.
Existing RL toolkits often lack native mechanisms for enforcing hard safety
constraints or producing human-interpretable rationales for decisions.
SafeRL-Lite provides modular wrappers around standard Gym environments and deep
Q-learning agents to enable: (i) safety-aware training via constraint
enforcement, and (ii) real-time post-hoc explanation via SHAP values and
saliency maps. The library is lightweight, extensible, and installable via pip,
and includes built-in metrics for constraint violations. We demonstrate its
effectiveness on constrained variants of CartPole and provide visualizations
that reveal both policy logic and safety adherence. The full codebase is
available at: https://github.com/satyamcser/saferl-lite.

</details>


### [282] [NestQuant: Post-Training Integer-Nesting Quantization for On-Device DNN](https://arxiv.org/abs/2506.17870)
*Jianhang Xie,Chuntao Ding,Xiaqing Li,Shenyuan Ren,Yidong Li,Zhichao Lu*

Main category: cs.LG

TL;DR: The paper introduces NestQuant, a resource-efficient post-training quantization method to adapt DNN models on IoT devices without requiring retraining, specialized hardware, or storing multiple models.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in adapting quantized DNN models to dynamic IoT device resources, as existing post-training quantization methods are either inflexible or consume excessive storage and switching resources.

Method: NestQuant employs integer weight decomposition to split quantized weights into higher and lower bit weights and incorporates a nesting mechanism to optimize these weights. This enables dynamic adaptation of quantized models using one stored model.

Result: The NestQuant method demonstrated high accuracy (78.1% and 77.9% for full-bit and part-bit ResNet-101 models, respectively) while reducing switching overheads by 78.1% compared to multiple PTQ models.

Conclusion: NestQuant provides an effective and resource-efficient solution for deploying quantized DNN models on IoT devices, optimizing storage, transmission, and dynamic switching while achieving high accuracy in diverse scenarios.

Abstract: Deploying quantized deep neural network (DNN) models with resource adaptation
capabilities on ubiquitous Internet of Things (IoT) devices to provide
high-quality AI services can leverage the benefits of compression and meet
multi-scenario resource requirements. However, existing dynamic/mixed precision
quantization requires retraining or special hardware, whereas post-training
quantization (PTQ) has two limitations for resource adaptation: (i) The
state-of-the-art PTQ methods only provide one fixed bitwidth model, which makes
it challenging to adapt to the dynamic resources of IoT devices; (ii) Deploying
multiple PTQ models with diverse bitwidths consumes large storage resources and
switching overheads. To this end, this paper introduces a resource-friendly
post-training integer-nesting quantization, i.e., NestQuant, for on-device
quantized model switching on IoT devices. The proposed NestQuant incorporates
the integer weight decomposition, which bit-wise splits quantized weights into
higher-bit and lower-bit weights of integer data types. It also contains a
decomposed weights nesting mechanism to optimize the higher-bit weights by
adaptive rounding and nest them into the original quantized weights. In
deployment, we can send and store only one NestQuant model and switch between
the full-bit/part-bit model by paging in/out lower-bit weights to adapt to
resource changes and reduce consumption. Experimental results on the
ImageNet-1K pretrained DNNs demonstrated that the NestQuant model can achieve
high performance in top-1 accuracy, and reduce in terms of data transmission,
storage consumption, and switching overheads. In particular, the ResNet-101
with INT8 nesting INT6 can achieve 78.1% and 77.9% accuracy for full-bit and
part-bit models, respectively, and reduce switching overheads by approximately
78.1% compared with diverse bitwidths PTQ models.

</details>


### [283] [A Survey of State Representation Learning for Deep Reinforcement Learning](https://arxiv.org/abs/2506.17518)
*Ayoub Echchahed,Pablo Samuel Castro*

Main category: cs.LG

TL;DR: This survey categorizes methods for learning state representations in reinforcement learning into six classes, providing insights into their mechanisms, benefits, limitations, and future directions.


<details>
  <summary>Details</summary>
Motivation: To address challenges in sequential decision-making spaces by improving sample efficiency, generalization, and performance in reinforcement learning.

Method: The paper provides a taxonomy of model-free online methods for state representation learning in reinforcement learning, categorizing them into six types.

Result: It organizes and analyzes various representation learning techniques, emphasizing their advantages and drawbacks.

Conclusion: The survey enhances understanding in the field and serves as a guide for new researchers, while highlighting techniques for assessing representation quality and identifying future research directions.

Abstract: Representation learning methods are an important tool for addressing the
challenges posed by complex observations spaces in sequential decision making
problems. Recently, many methods have used a wide variety of types of
approaches for learning meaningful state representations in reinforcement
learning, allowing better sample efficiency, generalization, and performance.
This survey aims to provide a broad categorization of these methods within a
model-free online setting, exploring how they tackle the learning of state
representations differently. We categorize the methods into six main classes,
detailing their mechanisms, benefits, and limitations. Through this taxonomy,
our aim is to enhance the understanding of this field and provide a guide for
new researchers. We also discuss techniques for assessing the quality of
representations, and detail relevant future directions.

</details>


### [284] [AlgoSelect: Universal Algorithm Selection via the Comb Operator](https://arxiv.org/abs/2506.17304)
*Jasper Yao*

Main category: cs.LG

TL;DR: AlgoSelect introduces a universal and optimal framework for learning algorithm selection using the Comb Operator, achieving near-perfect accuracy in tests.


<details>
  <summary>Details</summary>
Motivation: Algorithm selection remains challenging, especially optimizing implementation based on distinct problem features.

Method: AlgoSelect leverages a sigmoid-gated Comb Operator for interpolation and extends it to N-Path Comb for multi-algorithm selection.

Result: AlgoSelect achieves 99.9% accuracy in selecting algorithms, converges quickly with few samples, and is provably universal and efficient.

Conclusion: AlgoSelect offers a scalable solution to automatic algorithm selection, backed by rigorous theoretical guarantees and practical validation.

Abstract: We introduce AlgoSelect, a principled framework for learning optimal
algorithm selection from data, centered around the novel Comb Operator. Given a
set of algorithms and a feature representation of problems, AlgoSelect learns
to interpolate between diverse computational approaches. For pairs of
algorithms, a simple sigmoid-gated selector, an instance of the Comb Operator,
facilitates this interpolation. We extend this to an N-Path Comb for multiple
algorithms. We prove that this framework is universal (can approximate any
algorithm selector), information-theoretically optimal in its learnability
(thresholds for selection converge almost surely, demonstrated via
Borel-Cantelli arguments), computationally efficient, and robust. Key
theoretical contributions include: (1) a universal approximation theorem
demonstrating that Comb-based selectors can achieve arbitrary accuracy; (2)
information-theoretic learnability for selection thresholds; (3) formalization
of the Comb Operator within linear operator theory, detailing its boundedness
and spectral properties; (4) an N-Path Comb generalization for multi-algorithm
selection; and (5) a practical learning framework for the adaptive seeding
functions that guide the Comb Operator. Empirical validation on a comprehensive
20$\times$20 problem-algorithm study demonstrates near-perfect selection
(99.9\%+ accuracy) with remarkably few samples and rapid convergence, revealing
that $H(\text{Algorithm}|\text{Problem}) \approx 0$ in structured domains.
AlgoSelect provides a theoretically grounded, practically deployable solution
to automated algorithm selection with provable optimality and learnability
guarantees, with significant implications for AI and adaptive systems.

</details>


### [285] [Learning to Adapt Frozen CLIP for Few-Shot Test-Time Domain Adaptation](https://arxiv.org/abs/2506.17307)
*Zhixiang Chi,Li Gu,Huan Liu,Ziqiang Wang,Yanan Wu,Yang Wang,Konstantinos N Plataniotis*

Main category: cs.LG

TL;DR: The paper addresses few-shot test-time domain adaptation for addressing domain shift, introducing a method to adapt CLIP by learning in input space and combining dataset-specific knowledge.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing CLIP-based domain adaptation methods, which rely only on prior knowledge of CLIP's feature space and struggle with less robust backbones on real-world datasets.

Method: The method introduces a parallel side branch to frozen CLIP that learns exclusive knowledge via revert attention, enhances label semantics through a greedy text ensemble, and progressively combines text and visual features with a domain-aware prompt for adaptation.

Result: The proposed method shows significant performance improvements on large-scale test benchmarks, including a +5.1 F1 gain for iWildCam and a +3.1% accuracy gain for FMoW, even with smaller ViT-B/16 backbones.

Conclusion: The study demonstrates that learning in the input space and complementing frozen CLIP's intrinsic knowledge effectively enhances performance for domain-specific test-time adaptation, particularly for challenging real-world benchmarks.

Abstract: Few-shot Test-Time Domain Adaptation focuses on adapting a model at test time
to a specific domain using only a few unlabeled examples, addressing domain
shift. Prior methods leverage CLIP's strong out-of-distribution (OOD) abilities
by generating domain-specific prompts to guide its generalized, frozen
features. However, since downstream datasets are not explicitly seen by CLIP,
solely depending on the feature space knowledge is constrained by CLIP's prior
knowledge. Notably, when using a less robust backbone like ViT-B/16,
performance significantly drops on challenging real-world benchmarks. Departing
from the state-of-the-art of inheriting the intrinsic OOD capability of CLIP,
this work introduces learning directly on the input space to complement the
dataset-specific knowledge for frozen CLIP. Specifically, an independent side
branch is attached in parallel with CLIP and enforced to learn exclusive
knowledge via revert attention. To better capture the dataset-specific label
semantics for downstream adaptation, we propose to enhance the inter-dispersion
among text features via greedy text ensemble and refinement. The text and
visual features are then progressively fused in a domain-aware manner by a
generated domain prompt to adapt toward a specific domain. Extensive
experiments show our method's superiority on 5 large-scale benchmarks (WILDS
and DomainNet), notably improving over smaller networks like ViT-B/16 with
gains of \textbf{+5.1} in F1 for iWildCam and \textbf{+3.1\%} in WC Acc for
FMoW.

</details>


### [286] [Towards Fundamental Limits for Active Multi-distribution Learning](https://arxiv.org/abs/2506.17607)
*Chicheng Zhang,Yihan Zhou*

Main category: cs.LG

TL;DR: The paper addresses active multi-distribution learning, improving label complexity bounds in both distribution-dependent and distribution-free settings. It also provides theoretical insights into the sample complexity gap between realizable and agnostic settings.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the broad applications of multi-distribution learning in areas like collaborative learning, fairness, and robustness, alongside the lack of comprehensive algorithms and optimality understanding for active multi-distribution learning.

Method: The researchers propose new active multi-distribution learning algorithms and analyze their label complexity in near-realizable, realizable, and agnostic settings. They leverage notions like disagreement coefficients, VC dimension, and multi-distribution error.

Result: Improved upper and lower bounds for label complexity are provided, along with information-theoretical optimality proofs in specific settings. The bounds smoothly transition between realizable and agnostic regimes.

Conclusion: This study advances active multi-distribution learning by optimizing label complexity bounds and offering a nuanced understanding of the theoretical limits in various settings. It also establishes foundational results that inform the development of future algorithms.

Abstract: Multi-distribution learning extends agnostic Probably Approximately Correct
(PAC) learning to the setting in which a family of $k$ distributions,
$\{D_i\}_{i\in[k]}$, is considered and a classifier's performance is measured
by its error under the worst distribution. This problem has attracted a lot of
recent interests due to its applications in collaborative learning, fairness,
and robustness. Despite a rather complete picture of sample complexity of
passive multi-distribution learning, research on active multi-distribution
learning remains scarce, with algorithms whose optimality remaining unknown.
  In this paper, we develop new algorithms for active multi-distribution
learning and establish improved label complexity upper and lower bounds, in
distribution-dependent and distribution-free settings. Specifically, in the
near-realizable setting we prove an upper bound of
$\widetilde{O}\Bigl(\theta_{\max}(d+k)\ln\frac{1}{\varepsilon}\Bigr)$ and
$\widetilde{O}\Bigl(\theta_{\max}(d+k)\Bigl(\ln\frac{1}{\varepsilon}+\frac{\nu^2}{\varepsilon^2}\Bigr)+\frac{k\nu}{\varepsilon^2}\Bigr)$
in the realizable and agnostic settings respectively, where $\theta_{\max}$ is
the maximum disagreement coefficient among the $k$ distributions, $d$ is the VC
dimension of the hypothesis class, $\nu$ is the multi-distribution error of the
best hypothesis, and $\varepsilon$ is the target excess error. Moreover, we
show that the bound in the realizable setting is information-theoretically
optimal and that the $k\nu/\varepsilon^2$ term in the agnostic setting is
fundamental for proper learners. We also establish instance-dependent sample
complexity bound for passive multidistribution learning that smoothly
interpolates between realizable and agnostic
regimes~\citep{blum2017collaborative,zhang2024optimal}, which may be of
independent interest.

</details>


### [287] [I Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry for Authorship Attribution](https://arxiv.org/abs/2506.17323)
*Tamas Bisztray,Bilel Cherif,Richard A. Dubniczky,Nils Gruschka,Bertalan Borsos,Mohamed Amine Ferrag,Attila Kovacs,Vasileios Mavroeidis,Norbert Tihanyi*

Main category: cs.LG

TL;DR: This paper introduces CodeT5-Authorship, a novel model for identifying which Large Language Model (LLM) generated a given C program. The study demonstrates state-of-the-art accuracy in both binary and multi-class classification tasks and provides resources for open research.


<details>
  <summary>Details</summary>
Motivation: The motivation of this work is to address the growing need to identify the specific LLM that generated a synthetic content such as C programs, as the use of AI-generated code becomes increasingly widespread and sophisticated.

Method: The authors propose CodeT5-Authorship, a model based on the encoder architecture from CodeT5. They focus on classification tasks, utilizing a two-layer classification head with GELU activation and dropout. The model is evaluated using a comprehensive benchmark dataset called LLM-AuthorBench, which contains 32,000 C programs from eight state-of-the-art LLMs.

Result: CodeT5-Authorship achieved 97.56% accuracy in binary classification (e.g., distinguishing between closely related models like GPT-4.1 and GPT-4.0) and 95.40% accuracy in multi-class attribution among five leading LLMs.

Conclusion: The study establishes CodeT5-Authorship as a high-performing solution for LLM authorship attribution in C programs, providing a valuable tool for detecting AI-authored synthetic code. All resources for replication are openly shared to encourage further research.

Abstract: Detecting AI-generated code, deepfakes, and other synthetic content is an
emerging research challenge. As code generated by Large Language Models (LLMs)
becomes more common, identifying the specific model behind each sample is
increasingly important. This paper presents the first systematic study of LLM
authorship attribution for C programs. We released CodeT5-Authorship, a novel
model that uses only the encoder layers from the original CodeT5
encoder-decoder architecture, discarding the decoder to focus on
classification. Our model's encoder output (first token) is passed through a
two-layer classification head with GELU activation and dropout, producing a
probability distribution over possible authors. To evaluate our approach, we
introduce LLM-AuthorBench, a benchmark of 32,000 compilable C programs
generated by eight state-of-the-art LLMs across diverse tasks. We compare our
model to seven traditional ML classifiers and eight fine-tuned transformer
models, including BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3,
Longformer, and LoRA-fine-tuned Qwen2-1.5B. In binary classification, our model
achieves 97.56% accuracy in distinguishing C programs generated by closely
related models such as GPT-4.1 and GPT-4o, and 95.40% accuracy for multi-class
attribution among five leading LLMs (Gemini 2.5 Flash, Claude 3.5 Haiku,
GPT-4.1, Llama 3.3, and DeepSeek-V3). To support open science, we release the
CodeT5-Authorship architecture, the LLM-AuthorBench benchmark, and all relevant
Google Colab scripts on GitHub: https://github.com/LLMauthorbench/.

</details>


### [288] [DeInfoReg: A Decoupled Learning Framework for Better Training Throughput](https://arxiv.org/abs/2506.18193)
*Zih-Hao Huang,You-Teng Lin,Hung-Hsuan Chen*

Main category: cs.LG

TL;DR: This paper proposes DeInfoReg, a method that splits the gradient flow into shorter paths, addressing gradient vanishing and enhancing parallel training.


<details>
  <summary>Details</summary>
Motivation: Address the vanishing gradient problem and enhance model training efficiency on GPUs using parallelization.

Method: DeInfoReg transforms long gradient flows into smaller ones and integrates a pipeline strategy for parallel processing.

Result: DeInfoReg outperforms standard backpropagation (BP) methods in performance, noise resistance, and resource utilization over multiple tasks and datasets.

Conclusion: DeInfoReg offers an effective solution for handling vanishing gradients and optimizing parallel GPU resource use in neural network training.

Abstract: This paper introduces Decoupled Supervised Learning with Information
Regularization (DeInfoReg), a novel approach that transforms a long gradient
flow into multiple shorter ones, thereby mitigating the vanishing gradient
problem. Integrating a pipeline strategy, DeInfoReg enables model
parallelization across multiple GPUs, significantly improving training
throughput. We compare our proposed method with standard backpropagation and
other gradient flow decomposition techniques. Extensive experiments on diverse
tasks and datasets demonstrate that DeInfoReg achieves superior performance and
better noise resistance than traditional BP models and efficiently utilizes
parallel computing resources. The code for reproducibility is available at:
https://github.com/ianzih/Decoupled-Supervised-Learning-for-Information-Regularization/.

</details>


### [289] [Origins of Creativity in Attention-Based Diffusion Models](https://arxiv.org/abs/2506.17324)
*Emma Finn,T. Anderson Keller,Manos Theodosis,Demba E. Ba*

Main category: cs.LG

TL;DR: The paper investigates how creativity arises in diffusion models with self-attention, focusing on their ability to generate globally coherent and novel images.


<details>
  <summary>Details</summary>
Motivation: To understand the origin of creativity in diffusion models, and to explore the role of self-attention in enabling these models to generate images that are consistent yet novel at a global level.

Method: The authors extend existing score-matching theory to diffusion models with CNNs augmented by a self-attention layer, and verify their findings on a specifically designed dataset.

Result: Self-attention in diffusion models induces globally coherent image features beyond simple patch mosaics, as validated by their empirical experiments.

Conclusion: The inclusion of self-attention enhances the creative capacity of diffusion models by ensuring greater global consistency in the features of their generated images.

Abstract: As diffusion models have become the tool of choice for image generation and
as the quality of the images continues to improve, the question of how
`creativity' originates in diffusion has become increasingly important. The
score matching perspective on diffusion has proven particularly fruitful for
understanding how and why diffusion models generate images that remain
plausible while differing significantly from their training images. In
particular, as explained in (Kamb \& Ganguli, 2024) and others, e.g.,
(Ambrogioni, 2023), theory suggests that if our score matching were optimal, we
would only be able to recover training samples through our diffusion process.
However, as shown by Kamb \& Ganguli, (2024), in diffusion models where the
score is parametrized by a simple CNN, the inductive biases of the CNN itself
(translation equivariance and locality) allow the model to generate samples
that globally do not match any training samples, but are rather patch-wise
`mosaics'. Notably, however, this theory does not extend to describe the role
of self-attention in this process. In this work, we take a preliminary step in
this direction to extend this theory to the case of diffusion models whose
score is parametrized by a CNN with a final self-attention layer. We show that
our theory suggests that self-attention will induce a globally image-consistent
arrangement of local features beyond the patch-level in generated samples, and
we verify this behavior empirically on a carefully crafted dataset.

</details>


### [290] [CEGA: A Cost-Effective Approach for Graph-Based Model Extraction and Acquisition](https://arxiv.org/abs/2506.17709)
*Zebin Wang,Menghan Lin,Bolin Shen,Ken Anderson,Molei Liu,Tianxi Cai,Yushun Dong*

Main category: cs.LG

TL;DR: Graph Neural Networks (GNNs) are vulnerable to model extraction attacks (MEAs) but can also benefit from adaptive node querying strategies for efficient model acquisition in low-resource scenarios.


<details>
  <summary>Details</summary>
Motivation: To assess the vulnerability of GNNs to MEAs and explore their utility for cost-effective, ethical model acquisition in research environments where data labeling is resource-intensive.

Method: The authors propose a tailored node querying strategy for scenarios where bulk queries are restricted, iteratively refining the selection mechanism using historical feedback across multiple learning cycles.

Result: Experiments on benchmark graph datasets demonstrate the proposed approach outperforms baselines in accuracy, fidelity, and F1 score under strict query-size constraints.

Conclusion: The paper underscores GNNs' susceptibility to MEAs and their potential for ethical and efficient data acquisition, especially in domains like biomedicine requiring expert annotations with limited resources.

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable utility across
diverse applications, and their growing complexity has made Machine Learning as
a Service (MLaaS) a viable platform for scalable deployment. However, this
accessibility also exposes GNN to serious security threats, most notably model
extraction attacks (MEAs), in which adversaries strategically query a deployed
model to construct a high-fidelity replica. In this work, we evaluate the
vulnerability of GNNs to MEAs and explore their potential for cost-effective
model acquisition in non-adversarial research settings. Importantly, adaptive
node querying strategies can also serve a critical role in research,
particularly when labeling data is expensive or time-consuming. By selectively
sampling informative nodes, researchers can train high-performing GNNs with
minimal supervision, which is particularly valuable in domains such as
biomedicine, where annotations often require expert input. To address this, we
propose a node querying strategy tailored to a highly practical yet
underexplored scenario, where bulk queries are prohibited, and only a limited
set of initial nodes is available. Our approach iteratively refines the node
selection mechanism over multiple learning cycles, leveraging historical
feedback to improve extraction efficiency. Extensive experiments on benchmark
graph datasets demonstrate our superiority over comparable baselines on
accuracy, fidelity, and F1 score under strict query-size constraints. These
results highlight both the susceptibility of deployed GNNs to extraction
attacks and the promise of ethical, efficient GNN acquisition methods to
support low-resource research environments.

</details>


### [291] [Learning Time-Aware Causal Representation for Model Generalization in Evolving Domains](https://arxiv.org/abs/2506.17718)
*Zhuo He,Shuang Li,Wenze Song,Longhui Yuan,Jian Liang,Han Li,Kun Gai*

Main category: cs.LG

TL;DR: The paper introduces SYNC, a method addressing evolving domain generalization (EDG) challenges by learning time-aware causal representations that reduce spurious correlations and improve model generalization over dynamic scenarios.


<details>
  <summary>Details</summary>
Motivation: Current approaches in evolving domain generalization (EDG) struggle with spurious correlations that hinder their ability to generalize in dynamic environments, as they rely only on data-target modeling without accounting for causal factors and mechanisms over time.

Method: The paper proposes SYNC, a static-dynamic causal representation learning method based on a time-aware structural causal model (SCM). It integrates information-theoretic objectives into a sequential VAE framework to learn time-aware causal representations, focusing on intra-class compactness both across and within domains.

Result: Experimental results, based on both synthetic and real-world datasets, demonstrate that SYNC outperforms existing methods in temporal generalization performance.

Conclusion: SYNC provides an effective solution to enhance model generalization by addressing causal mechanism drifts and preserving causal factor integrity over time domains.

Abstract: Endowing deep models with the ability to generalize in dynamic scenarios is
of vital significance for real-world deployment, given the continuous and
complex changes in data distribution. Recently, evolving domain generalization
(EDG) has emerged to address distribution shifts over time, aiming to capture
evolving patterns for improved model generalization. However, existing EDG
methods may suffer from spurious correlations by modeling only the dependence
between data and targets across domains, creating a shortcut between
task-irrelevant factors and the target, which hinders generalization. To this
end, we design a time-aware structural causal model (SCM) that incorporates
dynamic causal factors and the causal mechanism drifts, and propose
\textbf{S}tatic-D\textbf{YN}amic \textbf{C}ausal Representation Learning
(\textbf{SYNC}), an approach that effectively learns time-aware causal
representations. Specifically, it integrates specially designed
information-theoretic objectives into a sequential VAE framework which captures
evolving patterns, and produces the desired representations by preserving
intra-class compactness of causal factors both across and within domains.
Moreover, we theoretically show that our method can yield the optimal causal
predictor for each time domain. Results on both synthetic and real-world
datasets exhibit that SYNC can achieve superior temporal generalization
performance.

</details>


### [292] [AutomataGPT: Forecasting and Ruleset Inference for Two-Dimensional Cellular Automata](https://arxiv.org/abs/2506.17333)
*Jaime A. Berkovich,Noah S. David,Markus J. Buehler*

Main category: cs.LG

TL;DR: AutomataGPT uses transformers to predict and infer cellular automata dynamics, achieving high forecast and rule reconstruction accuracy without hand-crafted priors.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to solve the challenge of automatically discovering cellular automata (CA) update rules and using them for accurate predictions, which is crucial for understanding diverse phenomena modeled by CA.

Method: AutomataGPT, a decoder-only transformer model, is pretrained on simulated trajectories from 100 distinct binary CA rules on toroidal grids and tested on unseen rules.

Result: AutomataGPT achieves 98.5% accuracy in one-step forecasts, 96% application accuracy in rule inference, and 82% exact rule-matrix matching for unseen CA rules.

Conclusion: The study demonstrates transformers' ability to generalize CA dynamics through large-scale pretraining, enabling data-efficient surrogates for modeling dynamical systems in science and engineering.

Abstract: Cellular automata (CA) provide a minimal formalism for investigating how
simple local interactions generate rich spatiotemporal behavior in domains as
diverse as traffic flow, ecology, tissue morphogenesis and crystal growth.
However, automatically discovering the local update rules for a given
phenomenon and using them for quantitative prediction remains challenging. Here
we present AutomataGPT, a decoder-only transformer pretrained on around 1
million simulated trajectories that span 100 distinct two-dimensional binary
deterministic CA rules on toroidal grids. When evaluated on previously unseen
rules drawn from the same CA family, AutomataGPT attains 98.5% perfect one-step
forecasts and reconstructs the governing update rule with up to 96% functional
(application) accuracy and 82% exact rule-matrix match. These results
demonstrate that large-scale pretraining over wider regions of rule space
yields substantial generalization in both the forward (state forecasting) and
inverse (rule inference) problems, without hand-crafted priors. By showing that
transformer models can faithfully infer and execute CA dynamics from data
alone, our work lays the groundwork for abstracting real-world dynamical
phenomena into data-efficient CA surrogates, opening avenues in biology, tissue
engineering, physics and AI-driven scientific discovery.

</details>


### [293] [Log-Normal Multiplicative Dynamics for Stable Low-Precision Training of Large Networks](https://arxiv.org/abs/2506.17768)
*Keigo Nishida,Eren Mehmet Kıral,Kenichi Bannai,Mohammad Emtiyaz Khan,Thomas Möllenhoff*

Main category: cs.LG

TL;DR: The study proposes a Log-Normal Multiplicative Dynamics (LMD) algorithm inspired by biological synapse behaviors, showcasing its stable performance in artificial neural networks under low-precision operations.


<details>
  <summary>Details</summary>
Motivation: Exploring if biological synapse-inspired noisy multiplicative dynamics can be applied to artificial neural networks for stability and efficiency.

Method: A Bayesian learning rule was derived, creating the LMD algorithm with multiplicative noise and regularization. It integrates log-normal posterior distributions.

Result: LMD delivers stable and accurate training for Vision Transformer and GPT-2 using low-precision computations.

Conclusion: Biologically inspired multiplicative dynamics can lead to stable low-precision artificial neural network training, benefiting future energy-efficient hardware.

Abstract: Studies in neuroscience have shown that biological synapses follow a
log-normal distribution whose transitioning can be explained by noisy
multiplicative dynamics. Biological networks can function stably even under
dynamically fluctuating conditions arising due to unreliable synaptic
transmissions. Here we ask: Is it possible to design similar multiplicative
training in artificial neural networks? To answer this question, we derive a
Bayesian learning rule that assumes log-normal posterior distributions over
weights which gives rise to a new Log-Normal Multiplicative Dynamics (LMD)
algorithm. The algorithm uses multiplicative updates with both noise and
regularization applied multiplicatively. The method is as easy to implement as
Adam and only requires one additional vector to store. Our results show that
LMD achieves stable and accurate training-from-scratch under low-precision
forward operations for Vision Transformer and GPT-2. These results suggest that
multiplicative dynamics, a biological feature, may enable stable low-precision
inference and learning on future energy-efficient hardware.

</details>


### [294] [Adaptive Social Metaverse Streaming based on Federated Multi-Agent Deep Reinforcement Learning](https://arxiv.org/abs/2506.17342)
*Zijian Long,Haopeng Wang,Haiwei Dong,Abdulmotaleb El Saddik*

Main category: cs.LG

TL;DR: Proposes ASMS, a streaming system using federated learning and reinforcement learning to enhance social metaverse experiences while safeguarding user privacy.


<details>
  <summary>Details</summary>
Motivation: The digital social metaverse faces challenges of privacy concerns and real-time streaming quality due to its requirement of continuous biometric and behavioral data collection.

Method: ASMS leverages Federated Multi-Agent Proximal Policy Optimization (F-MAPPO) which combines federated learning and deep reinforcement learning to optimize streaming bit rates while preserving user privacy.

Result: Experimental results show ASMS improves user experience by at least 14% compared to current streaming methods across different network conditions.

Conclusion: ASMS improves seamless, immersive streaming in resource-constrained networks, while keeping sensitive user data secure on local devices.

Abstract: The social metaverse is a growing digital ecosystem that blends virtual and
physical worlds. It allows users to interact socially, work, shop, and enjoy
entertainment. However, privacy remains a major challenge, as immersive
interactions require continuous collection of biometric and behavioral data. At
the same time, ensuring high-quality, low-latency streaming is difficult due to
the demands of real-time interaction, immersive rendering, and bandwidth
optimization. To address these issues, we propose ASMS (Adaptive Social
Metaverse Streaming), a novel streaming system based on Federated Multi-Agent
Proximal Policy Optimization (F-MAPPO). ASMS leverages F-MAPPO, which
integrates federated learning (FL) and deep reinforcement learning (DRL) to
dynamically adjust streaming bit rates while preserving user privacy.
Experimental results show that ASMS improves user experience by at least 14%
compared to existing streaming methods across various network conditions.
Therefore, ASMS enhances the social metaverse experience by providing seamless
and immersive streaming, even in dynamic and resource-constrained networks,
while ensuring that sensitive user data remains on local devices.

</details>


### [295] [SING: SDE Inference via Natural Gradients](https://arxiv.org/abs/2506.17796)
*Amber Hu,Henry Smith,Scott Linderman*

Main category: cs.LG

TL;DR: The paper introduces SDE Inference via Natural Gradients (SING), a method for efficient and reliable inference in latent stochastic differential equation (SDE) models using natural gradient variational inference, achieving better state and drift function estimation.


<details>
  <summary>Details</summary>
Motivation: Inference in latent SDE models is crucial for discovering dynamical systems in fields like neuroscience and engineering, but existing methods suffer from inefficiencies and instability, motivating the need for a more reliable approach.

Method: The authors develop SING, a natural gradient variational inference method that approximates intractable integrals, exploits model geometry, and parallelizes computations in time, with theoretical guarantees for optimizing the continuous-time objective.

Result: SING significantly outperforms prior methods in state inference and drift estimation across multiple datasets, including neural dynamics in freely behaving animals.

Conclusion: SING demonstrates strong potential for accurate and fast inference in complex, non-conjugate dynamical systems with limited prior knowledge, paving the way for advancements in fields requiring dynamic modeling.

Abstract: Latent stochastic differential equation (SDE) models are important tools for
the unsupervised discovery of dynamical systems from data, with applications
ranging from engineering to neuroscience. In these complex domains, exact
posterior inference of the latent state path is typically intractable,
motivating the use of approximate methods such as variational inference (VI).
However, existing VI methods for inference in latent SDEs often suffer from
slow convergence and numerical instability. Here, we propose SDE Inference via
Natural Gradients (SING), a method that leverages natural gradient VI to
efficiently exploit the underlying geometry of the model and variational
posterior. SING enables fast and reliable inference in latent SDE models by
approximating intractable integrals and parallelizing computations in time. We
provide theoretical guarantees that SING will approximately optimize the
intractable, continuous-time objective of interest. Moreover, we demonstrate
that better state inference enables more accurate estimation of nonlinear drift
functions using, for example, Gaussian process SDE models. SING outperforms
prior methods in state inference and drift estimation on a variety of datasets,
including a challenging application to modeling neural dynamics in freely
behaving animals. Altogether, our results illustrate the potential of SING as a
tool for accurate inference in complex dynamical systems, especially those
characterized by limited prior knowledge and non-conjugate structure.

</details>


### [296] [FFINO: Factorized Fourier Improved Neural Operator for Modeling Multiphase Flow in Underground Hydrogen Storage](https://arxiv.org/abs/2506.17344)
*Tao Wang,Hewei Tang*

Main category: cs.LG

TL;DR: This paper presents a new neural operator model, FFINO, tailored for fast simulations of underground hydrogen storage (UHS), achieving remarkable efficiency and accuracy compared to current models.


<details>
  <summary>Details</summary>
Motivation: The transition to a low-carbon economy necessitates efficient energy storage solutions like UHS. There is a pressing need for fast and reliable modeling tools to manage hydrogen plume migration and pressure dynamics in UHS scenarios.

Method: The study introduces the FFINO model, a novel neural operator architecture. It incorporates parameterized relative permeability curves and benchmarks its performance against the FMIONet model using various metrics.

Result: The FFINO model demonstrates 38.1% fewer parameters, 17.6% reduced training time, and 12% lower GPU memory usage compared to FMIONet. Additionally, it improves focus area accuracy by 9.8%, reduces inference time drastically (7850x faster compared to numerical simulators), but achieves 18% poorer RMSE in pressure prediction.

Conclusion: FFINO is a highly efficient and accurate surrogate model for UHS applications, offering significant time-saving capabilities in simulations. While showing moderate trade-offs in specific metrics, its overall performance makes it a valuable tool for UHS management.

Abstract: Underground hydrogen storage (UHS) is a promising energy storage option for
the current energy transition to a low-carbon economy. Fast modeling of
hydrogen plume migration and pressure field evolution is crucial for UHS field
management. In this study, we propose a new neural operator architecture,
FFINO, as a fast surrogate model for multiphase flow problems in UHS. We
parameterize experimental relative permeability curves reported in the
literature and include them as key uncertainty parameters in the FFINO model.
We also compare the FFINO model with the state-of-the-art FMIONet model through
a comprehensive combination of metrics. Our new FFINO model has 38.1% fewer
trainable parameters, 17.6% less training time, and 12% less GPU memory cost
compared to FMIONet. The FFINO model also achieves a 9.8% accuracy improvement
in predicting hydrogen plume in focused areas, and 18% higher RMSE in
predicting pressure buildup. The inference time of the trained FFINO model is
7850 times faster than a numerical simulator, which makes it a competent
substitute for numerical simulations of UHS problems with superior time
efficiency.

</details>


### [297] [Flatness After All?](https://arxiv.org/abs/2506.17809)
*Neta Shoham,Liron Mor-Yosef,Haim Avron*

Main category: cs.LG

TL;DR: The paper examines using a soft rank measure of the Hessian for assessing generalization and offers experimental validation of its robustness.


<details>
  <summary>Details</summary>
Motivation: Generalization in neural networks, particularly the relationship between loss-function curvature and generalization, has been a debated subject due to conflicting evidence regarding flat and sharp minima.

Method: The study introduces a soft rank measure of the Hessian to assess flatness, uses it for calibrated models, connects it to Takeuchi Information Criterion, and performs experiments to validate it.

Result: The measure accurately predicts the generalization gap for calibrated models and provides reliable estimates for non-calibrated models without high confidence.

Conclusion: Experimental results affirm the robustness of the proposed flatness measure in estimating generalization gaps compared to existing methods.

Abstract: Recent literature has examined the relationship between the curvature of the
loss function at minima and generalization, mainly in the context of
overparameterized networks. A key observation is that "flat" minima tend to
generalize better than "sharp" minima. While this idea is supported by
empirical evidence, it has also been shown that deep networks can generalize
even with arbitrary sharpness, as measured by either the trace or the spectral
norm of the Hessian. In this paper, we argue that generalization could be
assessed by measuring flatness using a soft rank measure of the Hessian. We
show that when the common neural network model (neural network with exponential
family negative log likelihood loss) is calibrated, and its prediction error
and its confidence in the prediction are not correlated with the first and the
second derivatives of the network's output, our measure accurately captures the
asymptotic expected generalization gap. For non-calibrated models, we connect
our flatness measure to the well-known Takeuchi Information Criterion and show
that it still provides reliable estimates of generalization gaps for models
that are not overly confident. Experimental results indicate that our approach
offers a robust estimate of the generalization gap compared to baselines.

</details>


### [298] [SAFEx: Analyzing Vulnerabilities of MoE-Based LLMs via Stable Safety-critical Expert Identification](https://arxiv.org/abs/2506.17368)
*Zhenglin Lai,Mengyao Liao,Dong Xu,Zebin Zhao,Zhihang Yuan,Chao Fan,Jianqiang Li,Bingzhe Wu*

Main category: cs.LG

TL;DR: The paper examines unique safety alignment challenges in Mixture-of-Experts (MoE) models, proposing the SAFEx framework to identify critical experts and a Stability-based Expert Selection algorithm to analyze their role. Experiments confirm that disabling a small subset of these experts significantly compromises the model's safety.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address safety alignment challenges unique to Mixture-of-Experts (MoE) models, which cannot be sufficiently tackled with methods designed for traditional dense models, exposing an underexplored vulnerability with serious implications.

Method: The authors introduced SAFEx, an analytical framework that uses a novel Stability-based Expert Selection (SES) algorithm to identify and evaluate critical safety-related experts in MoE models. The framework also decomposes these safety-critical experts into functional groups based on their specific roles.

Result: The study found that safety mechanisms in MoE models like Qwen3-MoE crucially depend on a small number of experts. For instance, disabling just 12 out of 6144 experts in Qwen3-MoE reduced the refusal rate for harmful requests by 22%.

Conclusion: MoE-based language models are disproportionately reliant on a small set of safety-critical experts, and their disabling can significantly compromise the model's ability to maintain safety standards. This highlights the need for targeted safety alignment strategies tailored to MoE architectures.

Abstract: Large language models based on Mixture-of-Experts have achieved substantial
gains in efficiency and scalability, yet their architectural uniqueness
introduces underexplored safety alignment challenges. Existing safety alignment
strategies, predominantly designed for dense models, are ill-suited to address
MoE-specific vulnerabilities. In this work, we formalize and systematically
study MoE model's positional vulnerability - the phenomenon where
safety-aligned behaviors rely on specific expert modules, revealing critical
risks inherent to MoE architectures. To this end, we present SAFEx, an
analytical framework that robustly identifies, characterizes, and validates the
safety-critical experts using a novel Stability-based Expert Selection (SES)
algorithm. Notably, our approach enables the explicit decomposition of
safety-critical experts into distinct functional groups, including those
responsible for harmful content detection and those controlling safe response
generation. Extensive experiments on mainstream MoE models, such as the
recently released Qwen3-MoE, demonstrated that their intrinsic safety
mechanisms heavily rely on a small subset of positional experts. Disabling
these experts significantly compromised the models' ability to refuse harmful
requests. For Qwen3-MoE with 6144 experts (in the FNN layer), we find that
disabling as few as 12 identified safety-critical experts can cause the refusal
rate to drop by 22%, demonstrating the disproportionate impact of a small set
of experts on overall model safety.

</details>


### [299] [Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in Inference-time Scaling?](https://arxiv.org/abs/2506.17417)
*Mingyuan Wu,Meitang Li,Jingcheng Yang,Jize Jiang,Kaizhuo Yan,Zhaoheng Li,Minjia Zhang,Klara Nahrstedt*

Main category: cs.LG

TL;DR: The paper explores inference-time techniques for improving reasoning in vision-language models (VLMs) and finds that generation-based methods outperform verification-based methods, with RL-trained models still lacking robust self-verification capabilities.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether inference-time computation techniques, which have helped large language models (LLMs), can also enhance reasoning in vision-language models (VLMs) trained with reinforcement learning (RL).

Method: The study conducts extensive experiments in inference-time scaling, employing various decoding strategies like majority voting and best-of-N selection to evaluate reasoning performances, while exploring self-verification and self-correction behaviors.

Result: Decoding strategies improve reasoning in VLMs, with generation-focused methods yielding higher gains compared to verification techniques. RL-tuned VLMs fail to exhibit robust self-verification capabilities.

Conclusion: Inference-time generation methods outperform verification-based strategies in VLMs, and the lack of self-verification in RL-trained models remains a significant barrier to reasoning improvements.

Abstract: Recent advances in large language models (LLMs) have demonstrated that
inference-time computation techniques, such as decoding-time scaling and
self-refinement, can significantly enhance reasoning capabilities without
relying on external knowledge. A key driver of this success is the emergence of
self-correction and self-verification behaviors, often elicited through
reinforcement learning (RL). In this paper, we investigate whether these
inference-time techniques extend effectively to vision-language models (VLMs),
particularly those trained with RL. We find that while decoding strategies such
as majority voting and best-of-N selection with self-verification all improve
VLM reasoning performance, generation-reliant methods such as the former
achieve significantly higher gains versus verification-reliant methods such as
the latter. Additionally, the self-correction behavior often associated with
RL-tuned models, such as aha moment, does not lead to measurable gains. We show
via extensive experimentation within the inference-time scaling framework to
identify a key root cause: RL-trained VLMs still lack robust self-verification
capabilities across both visual and textual modalities.

</details>


### [300] [Choice of Scoring Rules for Indirect Elicitation of Properties with Parametric Assumptions](https://arxiv.org/abs/2506.17880)
*Lingfang Hu,Ian A. Kash*

Main category: cs.LG

TL;DR: The paper explores the task of indirectly eliciting statistical properties using weighted sums of scoring rules, analyzing how weight choices impact property estimation.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in literature by understanding how different weight configurations in scoring rule setups affect the estimation of a statistical property.

Method: Simulations were performed to observe patterns in weight configuration effects, followed by theoretical analysis to explain these observations across two-dimensional and higher-dimensional cases.

Result: Experiments showed that optimal solutions often exhibit monotonic behavior with changing weights, and the best settings frequently involve setting some weights to zero. Theoretical frameworks confirmed these findings.

Conclusion: The proposed framework effectively identifies optimal weight configurations, providing insights into indirect elicitation of statistical properties and practical advice for modeling in higher dimensions.

Abstract: People are commonly interested in predicting a statistical property of a
random event such as mean and variance. Proper scoring rules assess the quality
of predictions and require that the expected score gets uniquely maximized at
the precise prediction, in which case we call the score directly elicits the
property. Previous research work has widely studied the existence and the
characterization of proper scoring rules for different properties, but little
literature discusses the choice of proper scoring rules for applications at
hand. In this paper, we explore a novel task, the indirect elicitation of
properties with parametric assumptions, where the target property is a function
of several directly-elicitable sub-properties and the total score is a weighted
sum of proper scoring rules for each sub-property. Because of the restriction
to a parametric model class, different settings for the weights lead to
different constrained optimal solutions. Our goal is to figure out how the
choice of weights affects the estimation of the target property and which
choice is the best. We start it with simulation studies and observe an
interesting pattern: in most cases, the optimal estimation of the target
property changes monotonically with the increase of each weight, and the best
configuration of weights is often to set some weights as zero. To understand
how it happens, we first establish the elementary theoretical framework and
then provide deeper sufficient conditions for the case of two sub-properties
and of more sub-properties respectively. The theory on 2-D cases perfectly
interprets the experimental results. In higher-dimensional situations, we
especially study the linear cases and suggest that more complex settings can be
understood with locally mapping into linear situations or using linear
approximations when the true values of sub-properties are close enough to the
parametric space.

</details>


### [301] [FedNAMs: Performing Interpretability Analysis in Federated Learning Context](https://arxiv.org/abs/2506.17466)
*Amitash Nanda,Sree Bhargavi Balija,Debashis Sahoo*

Main category: cs.LG

TL;DR: The paper introduces Federated Neural Additive Models (FedNAMs), combining Neural Additive Models (NAMs) with federated learning to enhance interpretability and privacy. Studies show FedNAMs achieve strong interpretability with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: The study addresses limitations in interpretability and explainability of federated learning models, emphasizing the need for privacy-preserving and interpretable solutions, especially in critical sectors like finance and healthcare.

Method: The researchers integrate Neural Additive Models (NAMs), which focus on specific input features, with federated learning. They develop FedNAMs to enable feature-specific learning, privacy by distributed data processing, and client-specific updates.

Result: Experiments on datasets like Wine, UCI Heart, and Iris demonstrate FedNAMs' ability to deliver detailed feature interpretability with minimal compromises in accuracy compared to traditional federated deep neural networks.

Conclusion: FedNAMs enhance privacy and interpretability while maintaining robustness, making them suitable for privacy-critical applications in diverse fields. They also offer insights into predictive features at local and global levels.

Abstract: Federated learning continues to evolve but faces challenges in
interpretability and explainability. To address these challenges, we introduce
a novel approach that employs Neural Additive Models (NAMs) within a federated
learning framework. This new Federated Neural Additive Models (FedNAMs)
approach merges the advantages of NAMs, where individual networks concentrate
on specific input features, with the decentralized approach of federated
learning, ultimately producing interpretable analysis results. This integration
enhances privacy by training on local data across multiple devices, thereby
minimizing the risks associated with data centralization and improving model
robustness and generalizability. FedNAMs maintain detailed, feature-specific
learning, making them especially valuable in sectors such as finance and
healthcare. They facilitate the training of client-specific models to integrate
local updates, preserve privacy, and mitigate concerns related to
centralization. Our studies on various text and image classification tasks,
using datasets such as OpenFetch ML Wine, UCI Heart Disease, and Iris, show
that FedNAMs deliver strong interpretability with minimal accuracy loss
compared to traditional Federated Deep Neural Networks (DNNs). The research
involves notable findings, including the identification of critical predictive
features at both client and global levels. Volatile acidity, sulfates, and
chlorides for wine quality. Chest pain type, maximum heart rate, and number of
vessels for heart disease. Petal length and width for iris classification. This
approach strengthens privacy and model efficiency and improves interpretability
and robustness across diverse datasets. Finally, FedNAMs generate insights on
causes of highly and low interpretable features.

</details>


### [302] [An entropy-optimal path to humble AI](https://arxiv.org/abs/2506.17940)
*Davide Bassetti,Lukáš Pospíšil,Michael Groom,Terence J. O'Kane,Illia Horenko*

Main category: cs.LG

TL;DR: A novel entropy-optimizing approach reformulates Boltzmann machines, making AI models more cost-effective, reliable, and efficient.


<details>
  <summary>Details</summary>
Motivation: Address the growing cost, resource demand, and overconfidence of modern AI models by proposing a more efficient and reliable framework.

Method: Introduces a non-equilibrium entropy-optimizing reformulation of Boltzmann machines based on the law of total probability, eliminating gradient-descent reliance.

Result: The method outperforms current AI tools in performance, cost, and model compactness while excelling in climate prediction tasks like forecasting El Niño/La Niña.

Conclusion: The proposed framework demonstrates enhanced efficiency, reliability, and predictive skill, especially for problems with high intrinsic complexity or limited data.

Abstract: Progress of AI has led to a creation of very successful, but by no means
humble models and tools, especially regarding (i) the huge and further
exploding costs and resources they demand, and (ii) the over-confidence of
these tools with the answers they provide. Here we introduce a novel
mathematical framework for a non-equilibrium entropy-optimizing reformulation
of Boltzmann machines based on the exact law of total probability. It results
in the highly-performant, but much cheaper, gradient-descent-free learning
framework with mathematically-justified existence and uniqueness criteria, and
answer confidence/reliability measures. Comparisons to state-of-the-art AI
tools in terms of performance, cost and the model descriptor lengths on a set
of synthetic problems with varying complexity reveal that the proposed method
results in more performant and slim models, with the descriptor lengths being
very close to the intrinsic complexity scaling bounds for the underlying
problems. Applying this framework to historical climate data results in models
with systematically higher prediction skills for the onsets of La Ni\~na and El
Ni\~no climate phenomena, requiring just few years of climate data for training
- a small fraction of what is necessary for contemporary climate prediction
tools.

</details>


### [303] [A geometric framework for momentum-based optimizers for low-rank training](https://arxiv.org/abs/2506.17475)
*Steffen Schotthöfer,Timon Klein,Jonas Kusch*

Main category: cs.LG

TL;DR: The paper proposes novel optimization strategies for training low-rank neural network parameterizations, offering faster convergence and better validation performance.


<details>
  <summary>Details</summary>
Motivation: Improving computational and storage efficiency of large neural networks through effective training of low-rank parameterizations, overcoming challenges faced by conventional optimizers.

Method: Introduced optimizers derived from dynamical low-rank approximation, combining it with momentum-based optimization techniques, ensuring alignment with the geometry of the parameter space.

Result: The proposed methods exhibit faster convergence and improved validation performance in numerical experiments.

Conclusion: The new training strategies effectively address difficulties in training low-rank parameterizations by exploiting the geometry, enhancing both efficiency and performance.

Abstract: Low-rank pre-training and fine-tuning have recently emerged as promising
techniques for reducing the computational and storage costs of large neural
networks. Training low-rank parameterizations typically relies on conventional
optimizers such as heavy ball momentum methods or Adam. In this work, we
identify and analyze potential difficulties that these training methods
encounter when used to train low-rank parameterizations of weights. In
particular, we show that classical momentum methods can struggle to converge to
a local optimum due to the geometry of the underlying optimization landscape.
To address this, we introduce novel training strategies derived from dynamical
low-rank approximation, which explicitly account for the underlying geometric
structure. Our approach leverages and combines tools from dynamical low-rank
approximation and momentum-based optimization to design optimizers that respect
the intrinsic geometry of the parameter space. We validate our methods through
numerical experiments, demonstrating faster convergence, and stronger
validation metrics at given parameter budgets.

</details>


### [304] [Episode-specific Fine-tuning for Metric-based Few-shot Learners with Optimization-based Training](https://arxiv.org/abs/2506.17499)
*Xuanyu Zhuang,Geoffroy Peeters,Gaël Richard*

Main category: cs.LG

TL;DR: The paper proposes methods to improve few-shot classification models by utilizing labeled support samples more effectively during inference and fine-tuning the metric space while avoiding overfitting. Tested on audio datasets, the methods improve performance significantly.


<details>
  <summary>Details</summary>
Motivation: To address the underutilization of labeled support samples during inference in few-shot classification and to adapt metric-based models effectively to specific tasks.

Method: Proposes fine-tuning methods (RDFT, IDFT, ADFT) using pseudo support-query pairs and combines them with optimization-based meta-learning frameworks to avoid overfitting.

Result: Experimental validations on three diverse audio datasets demonstrate consistent improvements across different metric-based models, particularly attention-based ones.

Conclusion: The proposed approach enhances the adaptability of metric-based models to limited samples and improves their generalization across different audio domains.

Abstract: In few-shot classification tasks (so-called episodes), a small set of labeled
support samples is provided during inference to aid the classification of
unlabeled query samples. Metric-based models typically operate by computing
similarities between query and support embeddings within a learned metric
space, followed by nearest-neighbor classification. However, these labeled
support samples are often underutilized--they are only used for similarity
comparison, despite their potential to fine-tune and adapt the metric space
itself to the classes in the current episode. To address this, we propose a
series of simple yet effective episode-specific, during-inference fine-tuning
methods for metric-based models, including Rotational Division Fine-Tuning
(RDFT) and its two variants, Iterative Division Fine-Tuning (IDFT) and
Augmented Division Fine-Tuning (ADFT). These methods construct pseudo
support-query pairs from the given support set to enable fine-tuning even for
non-parametric models. Nevertheless, the severely limited amount of data in
each task poses a substantial risk of overfitting when applying such
fine-tuning strategies. To mitigate this, we further propose to train the
metric-based model within an optimization-based meta-learning framework. With
the combined efforts of episode-specific fine-tuning and optimization-based
meta-training, metric-based models are equipped with the ability to rapidly
adapt to the limited support samples during inference while avoiding
overfitting. We validate our approach on three audio datasets from diverse
domains, namely ESC-50 (environmental sounds), Speech Commands V2 (spoken
keywords), and Medley-solos-DB (musical instrument). Experimental results
demonstrate that our approach consistently improves performance for all
evaluated metric-based models (especially for attention-based models) and
generalizes well across different audio domains.

</details>


### [305] [h-calibration: Rethinking Classifier Recalibration with Probabilistic Error-Bounded Objective](https://arxiv.org/abs/2506.17968)
*Wenjian Huang,Guiping Cao,Jiahao Xia,Jingkun Chen,Hao Wang,Jianguo Zhang*

Main category: cs.LG

TL;DR: This paper introduces a new probabilistic learning framework, h-calibration, to address limitations in existing neural network calibration methods, achieving state-of-the-art results in improving the reliability of predicted probabilities.


<details>
  <summary>Details</summary>
Motivation: Neural networks often produce unreliable probability outputs due to miscalibration, prompting the need for improved calibration methods, especially in a post-hoc setting.

Method: The authors propose h-calibration, a probabilistic learning framework that theoretically reformulates the calibration problem with boundedness and designs a post-hoc calibration algorithm.

Result: The proposed h-calibration approach overcomes ten limitations of prior methods and achieves superior performance in calibration tasks compared to traditional techniques.

Conclusion: The study provides a theoretical and practical solution for improving neural network calibration, offering insights into learning reliable probability outputs and achieving state-of-the-art benchmark performance.

Abstract: Deep neural networks have demonstrated remarkable performance across numerous
learning tasks but often suffer from miscalibration, resulting in unreliable
probability outputs. This has inspired many recent works on mitigating
miscalibration, particularly through post-hoc recalibration methods that aim to
obtain calibrated probabilities without sacrificing the classification
performance of pre-trained models. In this study, we summarize and categorize
previous works into three general strategies: intuitively designed methods,
binning-based methods, and methods based on formulations of ideal calibration.
Through theoretical and practical analysis, we highlight ten common limitations
in previous approaches. To address these limitations, we propose a
probabilistic learning framework for calibration called h-calibration, which
theoretically constructs an equivalent learning formulation for canonical
calibration with boundedness. On this basis, we design a simple yet effective
post-hoc calibration algorithm. Our method not only overcomes the ten
identified limitations but also achieves markedly better performance than
traditional methods, as validated by extensive experiments. We further analyze,
both theoretically and experimentally, the relationship and advantages of our
learning objective compared to traditional proper scoring rule. In summary, our
probabilistic framework derives an approximately equivalent differentiable
objective for learning error-bounded calibrated probabilities, elucidating the
correspondence and convergence properties of computational statistics with
respect to theoretical bounds in canonical calibration. The theoretical
effectiveness is verified on standard post-hoc calibration benchmarks by
achieving state-of-the-art performance. This research offers valuable reference
for learning reliable likelihood in related fields.

</details>


### [306] [Imputation of Longitudinal Data Using GANs: Challenges and Implications for Classification](https://arxiv.org/abs/2506.18007)
*Sharon Torao Pingi,Md Abul Bashar,Richi Nayak*

Main category: cs.LG

TL;DR: The paper reviews how Generative Adversarial Networks (GANs) address challenges in imputing missing values in longitudinal data, examining their strengths, limitations, and research gaps for classification tasks.


<details>
  <summary>Details</summary>
Motivation: Longitudinal data, used in fields like health and education, suffers from challenges such as missing values, heterogeneity, and multi-dimensionality, making accurate classification difficult.

Method: The paper analyzes current GAN-based approaches for longitudinal data imputation (LDI), categorizing main strategies, reviewing their effectiveness, and identifying issues related to classification tasks.

Result: GANs demonstrate significant potential for improving longitudinal data usability and quality for classification tasks but require enhancements to manage broader challenges like class imbalance and mixed data types.

Conclusion: More versatile and effective GAN-based methodologies are needed to overcome the existing limitations and address a wider range of challenges in longitudinal data classification.

Abstract: Longitudinal data is commonly utilised across various domains, such as
health, biomedical, education and survey studies. This ubiquity has led to a
rise in statistical, machine and deep learning-based methods for Longitudinal
Data Classification (LDC). However, the intricate nature of the data,
characterised by its multi-dimensionality, causes instance-level heterogeneity
and temporal correlations that add to the complexity of longitudinal data
analysis. Additionally, LDC accuracy is often hampered by the pervasiveness of
missing values in longitudinal data. Despite ongoing research that draw on the
generative power and utility of Generative Adversarial Networks (GANs) to
address the missing data problem, critical considerations include statistical
assumptions surrounding longitudinal data and missingness within it, as well as
other data-level challenges like class imbalance and mixed data types that
impact longitudinal data imputation (LDI) and the subsequent LDC process in
GANs. This paper provides a comprehensive overview of how GANs have been
applied in LDI, with a focus whether GANS have adequately addressed fundamental
assumptions about the data from a LDC perspective. We propose a categorisation
of main approaches to GAN-based LDI, highlight strengths and limitations of
methods, identify key research trends, and provide promising future directions.
Our findings indicate that while GANs show great potential for LDI to improve
usability and quality of longitudinal data for tasks like LDC, there is need
for more versatile approaches that can handle the wider spectrum of challenges
presented by longitudinal data with missing values. By synthesising current
knowledge and identifying critical research gaps, this survey aims to guide
future research efforts in developing more effective GAN-based solutions to
address LDC challenges.

</details>


### [307] [Predicting E-commerce Purchase Behavior using a DQN-Inspired Deep Learning Model for enhanced adaptability](https://arxiv.org/abs/2506.17543)
*Aditi Madhusudan Jain*

Main category: cs.LG

TL;DR: The paper introduces a Deep Q-Network (DQN)-inspired model combining LSTMs and reinforcement learning techniques to predict user buying intent in e-commerce, achieving an accuracy of 88% and AUC-ROC of 0.88.


<details>
  <summary>Details</summary>
Motivation: To improve demand forecasting, user experience personalization, and inventory management in e-commerce settings through accurate behavior prediction.

Method: A hybrid model combining LSTM networks for sequential modeling and reinforcement learning aspects of DQNs, adapted to supervised learning, was evaluated on a large e-commerce dataset with experiments addressing class imbalance and classification thresholds.

Result: The model achieved 88% accuracy, an AUC-ROC score of 0.88, and provided superior performance over traditional and standard deep learning models in capturing complex temporal patterns.

Conclusion: The proposed DQN-inspired method is robust, scalable, and effective for handling high-dimensional, sequential data in e-commerce applications, offering practical benefits for improving demand forecasting and marketing strategies.

Abstract: This paper presents a novel approach to predicting buying intent and product
demand in e-commerce settings, leveraging a Deep Q-Network (DQN) inspired
architecture. In the rapidly evolving landscape of online retail, accurate
prediction of user behavior is crucial for optimizing inventory management,
personalizing user experiences, and maximizing sales. Our method adapts
concepts from reinforcement learning to a supervised learning context,
combining the sequential modeling capabilities of Long Short-Term Memory (LSTM)
networks with the strategic decision-making aspects of DQNs. We evaluate our
model on a large-scale e-commerce dataset comprising over 885,000 user
sessions, each characterized by 1,114 features. Our approach demonstrates
robust performance in handling the inherent class imbalance typical in
e-commerce data, where purchase events are significantly less frequent than
non-purchase events. Through comprehensive experimentation with various
classification thresholds, we show that our model achieves a balance between
precision and recall, with an overall accuracy of 88\% and an AUC-ROC score of
0.88. Comparative analysis reveals that our DQN-inspired model offers
advantages over traditional machine learning and standard deep learning
approaches, particularly in its ability to capture complex temporal patterns in
user behavior. The model's performance and scalability make it well-suited for
real-world e-commerce applications dealing with high-dimensional, sequential
data. This research contributes to the field of e-commerce analytics by
introducing a novel predictive modeling technique that combines the strengths
of deep learning and reinforcement learning paradigms. Our findings have
significant implications for improving demand forecasting, personalizing user
experiences, and optimizing marketing strategies in online retail environments.

</details>


### [308] [Probing the Embedding Space of Transformers via Minimal Token Perturbations](https://arxiv.org/abs/2506.18011)
*Eddie Conti,Alejandro Astruc,Alvaro Parafita,Axel Brando*

Main category: cs.LG

TL;DR: The paper explores how minimal token perturbations affect the embedding space in Transformer models and how information propagates through layers.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of understanding information propagation and improving interpretability in Transformer models.

Method: Analyzing token perturbations and shifts in the embedding space, focusing on rare token behavior and intermixing of input information across layers.

Result: Rare tokens cause larger shifts in embedding space, and input information becomes increasingly intermixed in deeper model layers.

Conclusion: Token perturbation techniques provide valuable insights into model interpretability, validating assumptions about early layers serving as proxies for explanations.

Abstract: Understanding how information propagates through Transformer models is a key
challenge for interpretability. In this work, we study the effects of minimal
token perturbations on the embedding space. In our experiments, we analyze the
frequency of which tokens yield to minimal shifts, highlighting that rare
tokens usually lead to larger shifts. Moreover, we study how perturbations
propagate across layers, demonstrating that input information is increasingly
intermixed in deeper layers. Our findings validate the common assumption that
the first layers of a model can be used as proxies for model explanations.
Overall, this work introduces the combination of token perturbations and shifts
on the embedding space as a powerful tool for model interpretability.

</details>


### [309] [DRIMV_TSK: An Interpretable Surgical Evaluation Model for Incomplete Multi-View Rectal Cancer Data](https://arxiv.org/abs/2506.17552)
*Wei Zhang,Zi Wang,Hanwen Zhou,Zhaohong Deng,Weiping Ding,Yuxi Ge,Te Zhang,Yuanpeng Zhang,Kup-Sze Choi,Shitong Wang,Shudong Hu*

Main category: cs.LG

TL;DR: This paper develops an AI-driven model to evaluate the difficulty of rectal cancer surgeries, leveraging multi-view patient data including MRI images and clinical records.


<details>
  <summary>Details</summary>
Motivation: Current methods for evaluating surgical difficulty in rectal cancer largely rely on clinical data, but advances in technology provide opportunities for richer datasets and AI applications to improve accuracy and outcomes.

Method: The authors constructed a multi-view rectal cancer dataset combining various MRI image types and clinical data. They proposed a learning model for incomplete multi-view data to extract commonalities and uniqueness across these views, including missing data imputation and second-order similarity constraints.

Result: The proposed DRIMV_TSK model outperformed other algorithms on the MVRC dataset, showing superior performance in surgical difficulty evaluation.

Conclusion: The paper demonstrates that an interpretable and cooperative learning mechanism using multi-view data can enhance surgical evaluation accuracy and reliability in rectal cancer treatment.

Abstract: A reliable evaluation of surgical difficulty can improve the success of the
treatment for rectal cancer and the current evaluation method is based on
clinical data. However, more data about rectal cancer can be collected with the
development of technology. Meanwhile, with the development of artificial
intelligence, its application in rectal cancer treatment is becoming possible.
In this paper, a multi-view rectal cancer dataset is first constructed to give
a more comprehensive view of patients, including the high-resolution MRI image
view, pressed-fat MRI image view, and clinical data view. Then, an
interpretable incomplete multi-view surgical evaluation model is proposed,
considering that it is hard to obtain extensive and complete patient data in
real application scenarios. Specifically, a dual representation incomplete
multi-view learning model is first proposed to extract the common information
between views and specific information in each view. In this model, the missing
view imputation is integrated into representation learning, and second-order
similarity constraint is also introduced to improve the cooperative learning
between these two parts. Then, based on the imputed multi-view data and the
learned dual representation, a multi-view surgical evaluation model with the
TSK fuzzy system is proposed. In the proposed model, a cooperative learning
mechanism is constructed to explore the consistent information between views,
and Shannon entropy is also introduced to adapt the view weight. On the MVRC
dataset, we compared it with several advanced algorithms and DRIMV_TSK obtained
the best results.

</details>


### [310] [Generalization under Byzantine & Poisoning Attacks: Tight Stability Bounds in Robust Distributed Learning](https://arxiv.org/abs/2506.18020)
*Thomas Boudou,Batiste Le Bars,Nirupam Gupta,Aurélien Bellet*

Main category: cs.LG

TL;DR: This paper theoretically compares Byzantine and data poisoning attacks on robust distributed learning, showing that Byzantine attacks more harm generalization.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the fundamental gap in understanding how two prominent threat models (Byzantine attacks and data poisoning attacks) impact the ability of robust distributed learning systems to generalize.

Method: Theoretical analysis of uniform algorithmic stability under both Byzantine and data poisoning attacks, quantifying degradation factors.

Result: The study demonstrates that under data poisoning, stability degrades additively with $\Theta(\frac{f}{n-f})$, whereas under Byzantine attacks, degradation follows $ \mathcal{O}\big(\sqrt{\frac{f}{n-2f}}\big)$—highlighting a stronger impact on generalization for Byzantine.

Conclusion: Byzantine attacks inherently degrade the generalization capabilities of distributed learning algorithms more significantly than data poisoning attacks, with effects growing severe as the number of misbehaving workers ($f$) increases.

Abstract: Robust distributed learning algorithms aim to maintain good performance in
distributed and federated settings, even in the presence of misbehaving
workers. Two primary threat models have been studied: Byzantine attacks, where
misbehaving workers can send arbitrarily corrupted updates, and data poisoning
attacks, where misbehavior is limited to manipulation of local training data.
While prior work has shown comparable optimization error under both threat
models, a fundamental question remains open: How do these threat models impact
generalization? Empirical evidence suggests a gap between the two threat
models, yet it remains unclear whether it is fundamental or merely an artifact
of suboptimal attacks. In this work, we present the first theoretical
investigation into this problem, formally showing that Byzantine attacks are
intrinsically more harmful to generalization than data poisoning. Specifically,
we prove that: (i) under data poisoning, the uniform algorithmic stability of a
robust distributed learning algorithm, with optimal optimization error,
degrades by an additive factor of $\varTheta ( \frac{f}{n-f} )$, with $f$ the
number of misbehaving workers out of $n$; and (ii) In contrast, under Byzantine
attacks, the degradation is in $\mathcal{O} \big( \sqrt{ \frac{f}{n-2f}}
\big)$.This difference in stability leads to a generalization error gap that is
especially significant as $f$ approaches its maximum value $\frac{n}{2}$.

</details>


### [311] [Accelerating Residual Reinforcement Learning with Uncertainty Estimation](https://arxiv.org/abs/2506.17564)
*Lakshita Dodeja,Karl Schmeckpeper,Shivam Vats,Thomas Weng,Mingxi Jia,George Konidaris,Stefanie Tellex*

Main category: cs.LG

TL;DR: This paper introduces enhancements to Residual Reinforcement Learning (RL) for handling stochastic base policies and addressing sparse rewards, outperforming existing baselines and demonstrating effectiveness in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Current Residual RL methods are limited by their inefficiency with sparse rewards and their reliance on deterministic base policies, necessitating improvements.

Method: The proposed method introduces two enhancements: leveraging uncertainty estimates of the base policy for focused exploration and a modification to off-policy residual learning to better handle stochastic base policies by observing base actions.

Result: The method surpasses state-of-the-art finetuning techniques, demo-augmented RL methods, and existing Residual RL baselines in various benchmark scenarios and succeeds in zero-shot sim-to-real deployment.

Conclusion: This paper's advancements improve sample efficiency and robustness, making Residual RL more applicable to diverse and real-world settings, including those with stochastic base policies.

Abstract: Residual Reinforcement Learning (RL) is a popular approach for adapting
pretrained policies by learning a lightweight residual policy that provides
corrective actions. While Residual RL is more sample-efficient than finetuning
the entire base policy, existing methods struggle with sparse rewards and are
designed for deterministic base policies. We propose two improvements to
Residual RL that further enhance its sample efficiency and make it suitable for
stochastic base policies. First, we leverage uncertainty estimates of the base
policy to focus exploration on regions in which the base policy is not
confident. Second, we propose a simple modification to off-policy residual
learning that allows it to observe base actions and better handle stochastic
base policies. We evaluate our method with both Gaussian-based and
Diffusion-based stochastic base policies on tasks from Robosuite and D4RL, and
compare against state-of-the-art finetuning methods, demo-augmented RL methods,
and other residual RL methods. Our algorithm significantly outperforms existing
baselines in a variety of simulation benchmark environments. We also deploy our
learned polices in the real world to demonstrate their robustness with
zero-shot sim-to-real transfer.

</details>


### [312] [Towards Deeper GCNs: Alleviating Over-smoothing via Iterative Training and Fine-tuning](https://arxiv.org/abs/2506.17576)
*Furong Peng,Jinzhen Gao,Xuan Lu,Kang Liu,Yifan Huo,Sheng Wang*

Main category: cs.LG

TL;DR: Graph Convolutional Networks (GCNs) suffer from over-smoothing at deep depths. The paper identifies trainable linear transformations as a critical cause and proposes Layer-wise Gradual Training (LGT) to build scalable deep GCNs while preserving expressiveness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the performance degradation in deep GCN architectures caused by over-smoothing. Existing solutions inadequately consider the role of trainable linear transformations in exacerbating this issue.

Method: The proposed method is Layer-wise Gradual Training (LGT), which includes three components: layer-wise training for stable optimization, low-rank adaptation for fine-tuning and acceleration, and identity initialization for smooth integration and faster convergence.

Result: Experiments show that LGT achieves state-of-the-art performance, significantly improving GCN accuracy even at 32 layers. It can also enhance existing methods like PairNorm and ContraNorm when applied together.

Conclusion: LGT offers a novel and general training strategy for scalable deep GCNs, providing a solution to over-smoothing while maintaining expressiveness. It works across architectures and is proven effective on benchmark datasets.

Abstract: Graph Convolutional Networks (GCNs) suffer from severe performance
degradation in deep architectures due to over-smoothing. While existing studies
primarily attribute the over-smoothing to repeated applications of graph
Laplacian operators, our empirical analysis reveals a critical yet overlooked
factor: trainable linear transformations in GCNs significantly exacerbate
feature collapse, even at moderate depths (e.g., 8 layers). In contrast,
Simplified Graph Convolution (SGC), which removes these transformations,
maintains stable feature diversity up to 32 layers, highlighting linear
transformations' dual role in facilitating expressive power and inducing
over-smoothing. However, completely removing linear transformations weakens the
model's expressive capacity.
  To address this trade-off, we propose Layer-wise Gradual Training (LGT), a
novel training strategy that progressively builds deep GCNs while preserving
their expressiveness. LGT integrates three complementary components: (1)
layer-wise training to stabilize optimization from shallow to deep layers, (2)
low-rank adaptation to fine-tune shallow layers and accelerate training, and
(3) identity initialization to ensure smooth integration of new layers and
accelerate convergence. Extensive experiments on benchmark datasets demonstrate
that LGT achieves state-of-the-art performance on vanilla GCN, significantly
improving accuracy even in 32-layer settings. Moreover, as a training method,
LGT can be seamlessly combined with existing methods such as PairNorm and
ContraNorm, further enhancing their performance in deeper networks. LGT offers
a general, architecture-agnostic training framework for scalable deep GCNs. The
code is available at [https://github.com/jfklasdfj/LGT_GCN].

</details>


### [313] [Bayesian Multiobject Tracking With Neural-Enhanced Motion and Measurement Models](https://arxiv.org/abs/2506.18124)
*Shaoxiu Wei,Mingchao Liang,Florian Meyer*

Main category: cs.LG

TL;DR: The paper proposes a hybrid multi-object tracking (MOT) method that combines model-based and data-driven approaches, using neural networks to improve Bayesian MOT. It is validated on the nuScenes dataset with competitive results.


<details>
  <summary>Details</summary>
Motivation: To integrate the strengths of both model-based MOT, which is broadly applicable, and data-driven MOT, which excels with abundant labeled data, into a unified framework.

Method: The method enhances Bayesian MOT by neural networks to refine its prediction and update steps. Belief propagation and sequential Monte Carlo methods are employed for computational efficiency.

Result: The hybrid method outperforms existing approaches on the nuScenes autonomous driving dataset, achieving state-of-the-art performance.

Conclusion: The proposed framework effectively combines the robustness of model-based MOT and the learning capabilities of neural networks, presenting a versatile and superior performing method.

Abstract: Multiobject tracking (MOT) is an important task in applications including
autonomous driving, ocean sciences, and aerospace surveillance. Traditional MOT
methods are model-based and combine sequential Bayesian estimation with data
association and an object birth model. More recent methods are fully
data-driven and rely on the training of neural networks. Both approaches offer
distinct advantages in specific settings. In particular, model-based methods
are generally applicable across a wide range of scenarios, whereas data-driven
MOT achieves superior performance in scenarios where abundant labeled data for
training is available. A natural thought is whether a general framework can
integrate the two approaches. This paper introduces a hybrid method that
utilizes neural networks to enhance specific aspects of the statistical model
in Bayesian MOT that have been identified as overly simplistic. By doing so,
the performance of the prediction and update steps of Bayesian MOT is improved.
To ensure tractable computation, our framework uses belief propagation to avoid
high-dimensional operations combined with sequential Monte Carlo methods to
perform low-dimensional operations efficiently. The resulting method combines
the flexibility and robustness of model-based approaches with the capability to
learn complex information from data of neural networks. We evaluate the
performance of the proposed method based on the nuScenes autonomous driving
dataset and demonstrate that it has state-of-the-art performance

</details>


### [314] [LFR-PINO: A Layered Fourier Reduced Physics-Informed Neural Operator for Parametric PDEs](https://arxiv.org/abs/2506.17582)
*Jing Wang,Biao Chen,Hairun Xie,Rui Wang,Yifan Xia,Jifa Zhang,Hui Xu*

Main category: cs.LG

TL;DR: LFR-PINO is a novel physics-informed neural operator designed to efficiently solve parametric partial differential equations, achieving improvements in accuracy and memory usage.


<details>
  <summary>Details</summary>
Motivation: Existing physics-informed neural operators face limitations in expressiveness or computational inefficiency, hindering their broader application in solving complex parametric PDEs.

Method: LFR-PINO introduces a layered hypernetwork architecture for tailored parameter generation and a frequency-domain reduction strategy to reduce parameter count while maintaining critical spectral features.

Result: In experiments on four PDE problems, LFR-PINO demonstrated 22.8%-68.7% error reduction over baselines and reduced memory usage by 28.6%-69.3% compared to Hyper-PINNs.

Conclusion: LFR-PINO provides an effective and efficient solution for learning universal PDE solvers, balancing computational efficiency and solution accuracy.

Abstract: Physics-informed neural operators have emerged as a powerful paradigm for
solving parametric partial differential equations (PDEs), particularly in the
aerospace field, enabling the learning of solution operators that generalize
across parameter spaces. However, existing methods either suffer from limited
expressiveness due to fixed basis/coefficient designs, or face computational
challenges due to the high dimensionality of the parameter-to-weight mapping
space. We present LFR-PINO, a novel physics-informed neural operator that
introduces two key innovations: (1) a layered hypernetwork architecture that
enables specialized parameter generation for each network layer, and (2) a
frequency-domain reduction strategy that significantly reduces parameter count
while preserving essential spectral features. This design enables efficient
learning of a universal PDE solver through pre-training, capable of directly
handling new equations while allowing optional fine-tuning for enhanced
precision. The effectiveness of this approach is demonstrated through
comprehensive experiments on four representative PDE problems, where LFR-PINO
achieves 22.8%-68.7% error reduction compared to state-of-the-art baselines.
Notably, frequency-domain reduction strategy reduces memory usage by
28.6%-69.3% compared to Hyper-PINNs while maintaining solution accuracy,
striking an optimal balance between computational efficiency and solution
fidelity.

</details>


### [315] [Online Learning of Whittle Indices for Restless Bandits with Non-Stationary Transition Kernels](https://arxiv.org/abs/2506.18186)
*Md Kamran Chowdhury Shisher,Vishrant Tripathi,Mung Chiang,Christopher G. Brinton*

Main category: cs.LG

TL;DR: The paper develops an online learning algorithm to compute Whittle indices for restless multi-armed bandits (RMABs) in unknown and non-stationary conditions, achieving low cumulative regret.


<details>
  <summary>Details</summary>
Motivation: RMAB problems are PSPACE-hard, and computing Whittle indices in non-stationary environments without known transition kernels is a complex challenge. Existing solutions require novel approaches to handle dynamic and non-stationary behaviors in practical applications.

Method: The paper proposes an algorithm that predicts transition kernels using linear optimization based on empirical data and upper confidence bounds, accounts for sliding windows, and computes Whittle indices for decision-making.

Result: The algorithm demonstrates sub-linear dynamic regret over episodes and uses prior domain knowledge to accelerate learning. Experimentally, it outperforms baseline methods in non-stationary settings.

Conclusion: The work successfully addresses dynamic RMAB challenges in unknown, non-stationary environments, providing a computationally efficient and regret-minimizing solution.

Abstract: We consider optimal resource allocation for restless multi-armed bandits
(RMABs) in unknown, non-stationary settings. RMABs are PSPACE-hard to solve
optimally, even when all parameters are known. The Whittle index policy is
known to achieve asymptotic optimality for a large class of such problems,
while remaining computationally efficient. In many practical settings, however,
the transition kernels required to compute the Whittle index are unknown and
non-stationary. In this work, we propose an online learning algorithm for
Whittle indices in this setting. Our algorithm first predicts current
transition kernels by solving a linear optimization problem based on upper
confidence bounds and empirical transition probabilities calculated from data
over a sliding window. Then, it computes the Whittle index associated with the
predicted transition kernels. We design these sliding windows and upper
confidence bounds to guarantee sub-linear dynamic regret on the number of
episodes $T$, under the condition that transition kernels change slowly over
time (rate upper bounded by $\epsilon=1/T^k$ with $k>0$). Furthermore, our
proposed algorithm and regret analysis are designed to exploit prior domain
knowledge and structural information of the RMABs to accelerate the learning
process. Numerical results validate that our algorithm achieves superior
performance in terms of lowest cumulative regret relative to baselines in
non-stationary environments.

</details>


### [316] [EQuARX: Efficient Quantized AllReduce in XLA for Distributed Machine Learning Acceleration](https://arxiv.org/abs/2506.17615)
*Ibrahim Ahmed,Clemens Schaefer,Gil Tabak,Denis Vnukov,Zenong Zhang,Felix chern,Anatoliy Yevtushenko,Andy Davis*

Main category: cs.LG

TL;DR: The paper introduces EQuARX, a solution for efficient distributed processing of Large Language Models (LLMs) using dynamic block-wise quantized AllReduce on TPUs, achieving substantial speedups with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficiently deploying Large Language Models (LLMs) by improving inter-device communication processes during distributed inference or training, while maintaining model accuracy.

Method: The authors propose EQuARX, a dynamic block-wise quantized AllReduce technique integrated into the XLA compiler for TPUs. It involves int8 precision quantization and deep pipelining between communication and computation to reduce numerical instability and error accumulation.

Result: EQuARX demonstrates a 1.8X speedup in performance compared to BF16 AllReduce across varied network topologies. Additionally, it accelerates the prefill stage of 27B and 12B models by up to 1.25X and 1.1X, respectively, without significant quality degradation.

Conclusion: EQuARX successfully enhances distributed processing efficiency for LLMs on TPUs via quantized AllReduce, providing a practical solution for scaling large-scale models with improved speed and minimal quality loss.

Abstract: While Large Language Models (LLMs) have become highly influential, their
enormous scale presents significant deployment challenges. Efficiently serving
these models typically requires distributing them across numerous accelerator
devices, which introduces substantial performance overhead from inter-device
communication (collectives). While model quantization has been widely adopted
to reduce the memory and compute requirements of LLM weights and activations
with minimal quality impact, applying quantization directly to collectives like
AllReduce is inherently difficult due to the inter-device summation involved,
which can lead to numerical instability or significant error accumulation. In
this work, we present a native dynamic block-wise efficient quantized AllReduce
within the XLA compiler for TPUs (EQuARX). By using TPU-friendly quantization
and deep pipelining of communication and compute, EQuARX with int8 precision
achieves a 1.8X speedup over baseline BF16 AllReduce across various network
topologies. Furthermore, EQuARX accelerates the prefill stage of Gemma 3 27B by
1.25X and Gemma 3 12B by 1.1X, respectively, with small to negligible impact on
quality.

</details>


### [317] [Trustworthy Chronic Disease Risk Prediction For Self-Directed Preventive Care via Medical Literature Validation](https://arxiv.org/abs/2506.17620)
*Minh Le,Khoi Ton*

Main category: cs.LG

TL;DR: This study develops deep learning models using only personal and lifestyle factors to predict 13 chronic diseases while ensuring explainability validated against medical literature.


<details>
  <summary>Details</summary>
Motivation: Current chronic disease risk prediction models rely heavily on medical test data, limiting access for proactive self-assessment, and lack validated explainability, reducing trust.

Method: The authors created deep learning models to predict 13 chronic diseases based solely on personal and lifestyle data. They applied SHAP-based explainability and validated influential features against established medical literature.

Result: The models successfully identified features aligned with medical literature, demonstrating trustworthiness. This held true across all 13 chronic diseases studied.

Conclusion: The work demonstrates the feasibility of trustworthy machine learning tools for chronic disease prevention, emphasizing accessible and validated model explanations. Further exploration is encouraged on ethics and trustworthiness.

Abstract: Chronic diseases are long-term, manageable, yet typically incurable
conditions, highlighting the need for effective preventive strategies. Machine
learning has been widely used to assess individual risk for chronic diseases.
However, many models rely on medical test data (e.g. blood results, glucose
levels), which limits their utility for proactive self-assessment.
Additionally, to gain public trust, machine learning models should be
explainable and transparent. Although some research on self-assessment machine
learning models includes explainability, their explanations are not validated
against established medical literature, reducing confidence in their
reliability. To address these issues, we develop deep learning models that
predict the risk of developing 13 chronic diseases using only personal and
lifestyle factors, enabling accessible, self-directed preventive care.
Importantly, we use SHAP-based explainability to identify the most influential
model features and validate them against established medical literature. Our
results show a strong alignment between the models' most influential features
and established medical literature, reinforcing the models' trustworthiness.
Critically, we find that this observation holds across 13 distinct diseases,
indicating that this machine learning approach can be broadly trusted for
chronic disease prediction. This work lays the foundation for developing
trustworthy machine learning tools for self-directed preventive care. Future
research can explore other approaches for models' trustworthiness and discuss
how the models can be used ethically and responsibly.

</details>


### [318] [Reliability-Adjusted Prioritized Experience Replay](https://arxiv.org/abs/2506.18482)
*Leonard S. Pleiss,Tobias Sutter,Maximilian Schiffer*

Main category: cs.LG

TL;DR: The paper presents Reliability-adjusted Prioritized Experience Replay (ReaPER), an improved experience replay method for reinforcement learning, which performs better than the current standard method.


<details>
  <summary>Details</summary>
Motivation: To improve the data efficiency of reinforcement learning agents by addressing limitations in the sampling strategy of Prioritized Experience Replay (PER).

Method: The authors introduce ReaPER, a transition selection algorithm that uses temporal difference (TD) error reliability as an additional factor when sampling experiences from the replay buffer.

Result: ReaPER demonstrated superior performance compared to PER in reinforcement learning tasks, including results on the Atari-5 benchmark.

Conclusion: ReaPER offers a more efficient alternative to PER by effectively weighting experiences based on the reliability of their temporal difference errors.

Abstract: Experience replay enables data-efficient learning from past experiences in
online reinforcement learning agents. Traditionally, experiences were sampled
uniformly from a replay buffer, regardless of differences in
experience-specific learning potential. In an effort to sample more
efficiently, researchers introduced Prioritized Experience Replay (PER). In
this paper, we propose an extension to PER by introducing a novel measure of
temporal difference error reliability. We theoretically show that the resulting
transition selection algorithm, Reliability-adjusted Prioritized Experience
Replay (ReaPER), enables more efficient learning than PER. We further present
empirical results showing that ReaPER outperforms PER across various
environment types, including the Atari-5 benchmark.

</details>


### [319] [Exploiting Efficiency Vulnerabilities in Dynamic Deep Learning Systems](https://arxiv.org/abs/2506.17621)
*Ravishka Rathnasuriya,Wei Yang*

Main category: cs.LG

TL;DR: Dynamic Deep Learning Systems (DDLSs) optimize runtime efficiency through input-adaptive computation but face new security risks, such as adversaries exploiting dynamic behaviors to degrade efficiency.


<details>
  <summary>Details</summary>
Motivation: Real-world deployment of deep learning models requires efficient inference under strict latency and resource constraints, necessitating a focus on input-adaptive computation systems (DDLSs).

Method: The paper surveys existing attack strategies, identifies security vulnerabilities, and proposes to assess the feasibility of efficiency attacks and develop defenses to protect DDLSs.

Result: The study reveals security vulnerabilities in DDLSs due to input-dependent execution pathways, demonstrates gaps in current defenses, and lays groundwork for targeted protective measures.

Conclusion: Dynamic behaviors in DDLSs, while optimizing efficiency, create security vulnerabilities that necessitate more robust defenses against adversarial threats.

Abstract: The growing deployment of deep learning models in real-world environments has
intensified the need for efficient inference under strict latency and resource
constraints. To meet these demands, dynamic deep learning systems (DDLSs) have
emerged, offering input-adaptive computation to optimize runtime efficiency.
While these systems succeed in reducing cost, their dynamic nature introduces
subtle and underexplored security risks. In particular, input-dependent
execution pathways create opportunities for adversaries to degrade efficiency,
resulting in excessive latency, energy usage, and potential denial-of-service
in time-sensitive deployments. This work investigates the security implications
of dynamic behaviors in DDLSs and reveals how current systems expose efficiency
vulnerabilities exploitable by adversarial inputs. Through a survey of existing
attack strategies, we identify gaps in the coverage of emerging model
architectures and limitations in current defense mechanisms. Building on these
insights, we propose to examine the feasibility of efficiency attacks on modern
DDLSs and develop targeted defenses to preserve robustness under adversarial
conditions.

</details>


### [320] [Optimization-Induced Dynamics of Lipschitz Continuity in Neural Networks](https://arxiv.org/abs/2506.18588)
*Róisín Luo,James McDermott,Christian Gagné,Qiang Sun,Colm O'Riordan*

Main category: cs.LG

TL;DR: The paper studies how neural networks' Lipschitz continuity evolves during training by establishing a mathematical framework with stochastic differential equations.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored topic of how Lipschitz continuity, a key measure of neural network sensitivity, changes dynamically during training.

Method: A mathematical framework leveraging stochastic differential equations (SDEs) to model both deterministic and stochastic dynamics driving Lipschitz continuity evolution.

Result: Three principal factors influencing Lipschitz continuity evolution are identified, supported by experimental validation aligning with theoretical predictions.

Conclusion: The study enhances understanding of neural network sensitivity evolution by linking various training factors, such as noisy supervision and parameter initialization, with Lipschitz continuity dynamics.

Abstract: Lipschitz continuity characterizes the worst-case sensitivity of neural
networks to small input perturbations; yet its dynamics (i.e. temporal
evolution) during training remains under-explored. We present a rigorous
mathematical framework to model the temporal evolution of Lipschitz continuity
during training with stochastic gradient descent (SGD). This framework
leverages a system of stochastic differential equations (SDEs) to capture both
deterministic and stochastic forces. Our theoretical analysis identifies three
principal factors driving the evolution: (i) the projection of gradient flows,
induced by the optimization dynamics, onto the operator-norm Jacobian of
parameter matrices; (ii) the projection of gradient noise, arising from the
randomness in mini-batch sampling, onto the operator-norm Jacobian; and (iii)
the projection of the gradient noise onto the operator-norm Hessian of
parameter matrices. Furthermore, our theoretical framework sheds light on such
as how noisy supervision, parameter initialization, batch size, and mini-batch
sampling trajectories, among other factors, shape the evolution of the
Lipschitz continuity of neural networks. Our experimental results demonstrate
strong agreement between the theoretical implications and the observed
behaviors.

</details>


### [321] [LLM-Prompt: Integrated Heterogeneous Prompts for Unlocking LLMs in Time Series Forecasting](https://arxiv.org/abs/2506.17631)
*Zesen Wang,Yonggang Li,Lijuan Lan*

Main category: cs.LG

TL;DR: LLM-Prompt is a new framework for time series forecasting that integrates textual prompts and cross-modal semantic alignment, showcasing strong performance across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: To improve the suboptimal performance of deep learning methods in long-term and data-scarce time series forecasting scenarios using large language models.

Method: A unified textual prompt paradigm combining soft and hard prompts, and cross-modal semantic alignment through a semantic embedding space for integrating temporal and textual information.

Result: LLM-Prompt demonstrates strong forecasting capability on 6 public datasets and 3 carbon emission datasets.

Conclusion: The proposed LLM-Prompt framework showcases effective advancements in time series forecasting tasks by leveraging textual prompt formulation and cross-modal integration.

Abstract: Time series forecasting aims to model temporal dependencies among variables
for future state inference, holding significant importance and widespread
applications in real-world scenarios. Although deep learning-based methods have
achieved remarkable progress, they still exhibit suboptimal performance in
long-term forecasting and data-scarce scenarios. Recent research demonstrates
that large language models (LLMs) achieve promising performance in time series
forecasting. However, we find existing LLM-based methods still have
shortcomings: (1) the absence of a unified paradigm for textual prompt
formulation and (2) the neglect of modality discrepancies between textual
prompts and time series. To address this, we propose LLM-Prompt, an LLM-based
time series forecasting framework integrating multi-prompt information and
cross-modal semantic alignment. Specifically, we first construct a unified
textual prompt paradigm containing learnable soft prompts and textualized hard
prompts. Second, to enhance LLMs' comprehensive understanding of the
forecasting task, we design a semantic space embedding and cross-modal
alignment module to achieve cross-modal fusion of temporal and textual
information. Finally, the transformed time series from the LLMs are projected
to obtain the forecasts. Comprehensive evaluations on 6 public datasets and 3
carbon emission datasets demonstrate that LLM-Prompt is a powerful framework
for time series forecasting.

</details>


### [322] [On Equivariant Model Selection through the Lens of Uncertainty](https://arxiv.org/abs/2506.18629)
*Putri A. van der Linden,Alexander Timans,Dharmesh Tailor,Erik J. Bekkers*

Main category: cs.LG

TL;DR: The paper investigates uncertainty-aware model selection among pretrained equivariant models with symmetry biases. Various metrics like Conformal Prediction, Bayesian marginal likelihood, and calibration are compared for alignment with predictive performance, revealing inconsistencies.


<details>
  <summary>Details</summary>
Motivation: Equivariant models leverage symmetry priors for improved prediction but struggle with misspecified constraints. There's a challenge in selecting optimal models with different symmetry biases.

Method: The authors compare uncertainty-based metrics such as frequency-based Conformal Prediction, Bayesian marginal likelihood, and calibration-based measures to error metrics for evaluating model selection efficacy.

Result: Uncertainty metrics generally align with predictive performance, while Bayesian model evidence shows inconsistent alignment due to mismatched notions of model complexity.

Conclusion: Leveraging uncertainty-aware measures could potentially improve symmetry-aware model selection, though challenges with Bayesian complexity mismatch must be addressed.

Abstract: Equivariant models leverage prior knowledge on symmetries to improve
predictive performance, but misspecified architectural constraints can harm it
instead. While work has explored learning or relaxing constraints, selecting
among pretrained models with varying symmetry biases remains challenging. We
examine this model selection task from an uncertainty-aware perspective,
comparing frequentist (via Conformal Prediction), Bayesian (via the marginal
likelihood), and calibration-based measures to naive error-based evaluation. We
find that uncertainty metrics generally align with predictive performance, but
Bayesian model evidence does so inconsistently. We attribute this to a mismatch
in Bayesian and geometric notions of model complexity, and discuss possible
remedies. Our findings point towards the potential of uncertainty in guiding
symmetry-aware model selection.

</details>


### [323] [Online Multi-LLM Selection via Contextual Bandits under Unstructured Context Evolution](https://arxiv.org/abs/2506.17670)
*Manhin Poon,XiangXiang Dai,Xutong Liu,Fang Kong,John C. S. Lui,Jinhang Zuo*

Main category: cs.LG

TL;DR: The paper focuses on adaptive multi-LLM selection using a contextual bandit framework to optimize sequential choices in an online setting for query refinement.


<details>
  <summary>Details</summary>
Motivation: Choosing the most suitable LLM for a given query is challenging due to diverse characteristics like accuracy, cost, and responsiveness, especially when prompt dynamics evolve unpredictably.

Method: The authors introduce a LinUCB-based algorithm under a contextual bandit framework, which addresses unstructured prompt dynamics without modeling or pre-training. Extensions for budget and user preference considerations are also included.

Result: The proposed methods outperform existing LLM routing strategies in terms of accuracy and cost-efficiency across diverse benchmarks.

Conclusion: The contextual bandit framework is effective for real-time adaptive LLM selection, avoiding the need for offline datasets or fine-tuning and adapting seamlessly to complex scenarios.

Abstract: Large language models (LLMs) exhibit diverse response behaviors, costs, and
strengths, making it challenging to select the most suitable LLM for a given
user query. We study the problem of adaptive multi-LLM selection in an online
setting, where the learner interacts with users through multi-step query
refinement and must choose LLMs sequentially without access to offline datasets
or model internals. A key challenge arises from unstructured context evolution:
the prompt dynamically changes in response to previous model outputs via a
black-box process, which cannot be simulated, modeled, or learned. To address
this, we propose the first contextual bandit framework for sequential LLM
selection under unstructured prompt dynamics. We formalize a notion of myopic
regret and develop a LinUCB-based algorithm that provably achieves sublinear
regret without relying on future context prediction. We further introduce
budget-aware and positionally-aware (favoring early-stage satisfaction)
extensions to accommodate variable query costs and user preferences for early
high-quality responses. Our algorithms are theoretically grounded and require
no offline fine-tuning or dataset-specific training. Experiments on diverse
benchmarks demonstrate that our methods outperform existing LLM routing
strategies in both accuracy and cost-efficiency, validating the power of
contextual bandits for real-time, adaptive LLM selection.

</details>


### [324] [Learning Personalized Utility Functions for Drivers in Ride-hailing Systems Using Ensemble Hypernetworks](https://arxiv.org/abs/2506.17672)
*Weiming Mai,Jie Gao,Oded Cats*

Main category: cs.LG

TL;DR: The paper introduces a novel method using hypernetworks and ensemble learning to improve the prediction of ride-hailing drivers' decisions, addressing non-linear interactions and personalized preferences.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and reliability of ride-hailing systems by accurately predicting drivers' ride acceptance decisions, overcoming the limitations of traditional models assuming linear correlations.

Method: The authors propose using hypernetworks to dynamically generate weights for utility functions based on trip and driver data. An ensemble of hypernetworks is trained to introduce controlled randomness and reduce model overfitting.

Result: The ensemble hypernetworks model demonstrated high prediction accuracy and effective uncertainty estimation in a real-world dataset.

Conclusion: The proposed approach balances prediction accuracy with explainability and uncertainty quantification, and serves as a tool for understanding drivers' personalized preferences in ride-hailing systems.

Abstract: In ride-hailing systems, drivers decide whether to accept or reject ride
requests based on factors such as order characteristics, traffic conditions,
and personal preferences. Accurately predicting these decisions is essential
for improving the efficiency and reliability of these systems. Traditional
models, such as the Random Utility Maximization (RUM) approach, typically
predict drivers' decisions by assuming linear correlations among attributes.
However, these models often fall short because they fail to account for
non-linear interactions between attributes and do not cater to the unique,
personalized preferences of individual drivers. In this paper, we develop a
method for learning personalized utility functions using hypernetwork and
ensemble learning. Hypernetworks dynamically generate weights for a linear
utility function based on trip request data and driver profiles, capturing the
non-linear relationships. An ensemble of hypernetworks trained on different
data segments further improve model adaptability and generalization by
introducing controlled randomness, thereby reducing over-fitting. We validate
the performance of our ensemble hypernetworks model in terms of prediction
accuracy and uncertainty estimation in a real-world dataset. The results
demonstrate that our approach not only accurately predicts each driver's
utility but also effectively balances the needs for explainability and
uncertainty quantification. Additionally, our model serves as a powerful tool
for revealing the personalized preferences of different drivers, clearly
illustrating which attributes largely impact their rider acceptance decisions.

</details>


### [325] [FaithfulSAE: Towards Capturing Faithful Features with Sparse Autoencoders without External Dataset Dependencies](https://arxiv.org/abs/2506.17673)
*Seonglae Cho,Harryn Oh,Donghyun Lee,Luis Eduardo Rodrigues Vieira,Andrew Bermingham,Ziad El Sayed*

Main category: cs.LG

TL;DR: FaithfulSAE is introduced as a method to train sparse autoencoders (SAEs) on synthetic datasets generated by the model itself, which reduces instability and improves feature interpretability by avoiding reliance on external out-of-distribution data.


<details>
  <summary>Details</summary>
Motivation: Existing SAEs face instability and generate non-representative 'Fake Features' when trained on external, out-of-distribution datasets, leading to poor interpretations of language model features.

Method: FaithfulSAE uses the model's own synthetic dataset for SAE training, minimizing reliance on external datasets and addressing issues of out-of-distribution data.

Result: FaithfulSAEs demonstrated increased stability across initialization seeds, performed better on a probing task, and had a lower Fake Feature Ratio compared to SAEs trained on external datasets in most models tested.

Conclusion: FaithfulSAEs improve interpretability and stability in feature extraction for language models by focusing on synthetic in-distribution data, underscoring the importance of dataset origin in SAE training.

Abstract: Sparse Autoencoders (SAEs) have emerged as a promising solution for
decomposing large language model representations into interpretable features.
However, Paulo and Belrose (2025) have highlighted instability across different
initialization seeds, and Heap et al. (2025) have pointed out that SAEs may not
capture model-internal features. These problems likely stem from training SAEs
on external datasets - either collected from the Web or generated by another
model - which may contain out-of-distribution (OOD) data beyond the model's
generalisation capabilities. This can result in hallucinated SAE features,
which we term "Fake Features", that misrepresent the model's internal
activations. To address these issues, we propose FaithfulSAE, a method that
trains SAEs on the model's own synthetic dataset. Using FaithfulSAEs, we
demonstrate that training SAEs on less-OOD instruction datasets results in SAEs
being more stable across seeds. Notably, FaithfulSAEs outperform SAEs trained
on web-based datasets in the SAE probing task and exhibit a lower Fake Feature
Ratio in 5 out of 7 models. Overall, our approach eliminates the dependency on
external datasets, advancing interpretability by better capturing
model-internal features while highlighting the often neglected importance of
SAE training datasets.

</details>


### [326] [Enhancing Stress-Strain Predictions with Seq2Seq and Cross-Attention based on Small Punch Test](https://arxiv.org/abs/2506.17680)
*Zhengni Yang,Rui Yang,Weijian Han,Qixin Liu*

Main category: cs.LG

TL;DR: A new deep-learning method using GAF and Seq2Seq predicts true stress-strain curves from SPT data, offering high accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and efficiency of predicting true stress-strain curves in high-strength steels using non-destructive testing methods like SPT.

Method: The method converts load-displacement data into images via Gramian Angular Field (GAF) and employs an LSTM-based Sequence-to-Sequence model enhanced with multi-head cross-attention for predictions.

Result: Achieved superior prediction accuracy with a minimum MAE of 0.15 MPa and maximum MAE of 5.58 MPa.

Conclusion: The proposed method is a promising alternative to traditional experimental techniques, significantly improving efficiency and accuracy in materials science.

Abstract: This paper introduces a novel deep-learning approach to predict true
stress-strain curves of high-strength steels from small punch test (SPT)
load-displacement data. The proposed approach uses Gramian Angular Field (GAF)
to transform load-displacement sequences into images, capturing
spatial-temporal features and employs a Sequence-to-Sequence (Seq2Seq) model
with an LSTM-based encoder-decoder architecture, enhanced by multi-head
cross-attention to improved accuracy. Experimental results demonstrate that the
proposed approach achieves superior prediction accuracy, with minimum and
maximum mean absolute errors of 0.15 MPa and 5.58 MPa, respectively. The
proposed method offers a promising alternative to traditional experimental
techniques in materials science, enhancing the accuracy and efficiency of true
stress-strain relationship predictions.

</details>


### [327] [Physics-informed mixture of experts network for interpretable battery degradation trajectory computation amid second-life complexities](https://arxiv.org/abs/2506.17755)
*Xinghao Huang,Shengyu Tao,Chen Liang,Jiawei Chen,Junzhe Shi,Yuqi Li,Bizhong Xia,Guangmin Zhou,Xuan Zhang*

Main category: cs.LG

TL;DR: The paper introduces a Physics-Informed Mixture of Experts (PIMOE) network to predict battery degradation trajectories using minimal data, achieving superior accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome uncertainties and data limitations in predicting degradation behavior of retired electric vehicle batteries in second-life applications.

Method: The PIMOE framework utilizes adaptive prediction modules, latent trend embeddings, and recurrent networks to compute long-term battery degradation using partial field signals.

Result: PIMOE showed high performance with 0.88% average MAPE, faster inference time of 0.43 ms, and significant improvements over state-of-the-art methods in computational time and accuracy.

Conclusion: PIMOE provides a scalable, efficient solution for assessing and integrating second-life battery systems into low-carbon energy frameworks, advancing sustainable practices.

Abstract: Retired electric vehicle batteries offer immense potential to support
low-carbon energy systems, but uncertainties in their degradation behavior and
data inaccessibilities under second-life use pose major barriers to safe and
scalable deployment. This work proposes a Physics-Informed Mixture of Experts
(PIMOE) network that computes battery degradation trajectories using partial,
field-accessible signals in a single cycle. PIMOE leverages an adaptive
multi-degradation prediction module to classify degradation modes using expert
weight synthesis underpinned by capacity-voltage and relaxation data, producing
latent degradation trend embeddings. These are input to a use-dependent
recurrent network for long-term trajectory prediction. Validated on 207
batteries across 77 use conditions and 67,902 cycles, PIMOE achieves an average
mean absolute percentage (MAPE) errors of 0.88% with a 0.43 ms inference time.
Compared to the state-of-the-art Informer and PatchTST, it reduces
computational time and MAPE by 50%, respectively. Compatible with random state
of charge region sampling, PIMOE supports 150-cycle forecasts with 1.50%
average and 6.26% maximum MAPE, and operates effectively even with pruned 5MB
training data. Broadly, PIMOE framework offers a deployable, history-free
solution for battery degradation trajectory computation, redefining how
second-life energy storage systems are assessed, optimized, and integrated into
the sustainable energy landscape.

</details>


### [328] [Towards a Unified Textual Graph Framework for Spectral Reasoning via Physical and Chemical Information Fusion](https://arxiv.org/abs/2506.17761)
*Jiheng Liang,Ziru Yu,Zujie Xie,Yuchen Guo,Yulan Guo,Xiangyang Yu*

Main category: cs.LG

TL;DR: The paper proposes a novel multi-modal spectral analysis framework combining knowledge graphs and Large Language Models (LLMs) for improved interpretability and generalizability in spectral analysis.


<details>
  <summary>Details</summary>
Motivation: Current spectral analysis methods face challenges like reliance on single-modality data, limited generalizability, and poor interpretability.

Method: The proposed method transforms raw spectrum data into Textual Attribute Graphs (TAGs) enriched with textual descriptions and integrates prior knowledge to form Task Graphs. These are processed using Graph Neural Networks and LLMs for contextual reasoning.

Result: The framework achieves high performance across tasks (classification at various levels) and demonstrates strong generalization in zero-shot and few-shot learning settings.

Conclusion: The work establishes an interpretable and scalable method for LLM-driven spectral analysis, aligning physical and chemical modalities for scientific applications.

Abstract: Motivated by the limitations of current spectral analysis methods-such as
reliance on single-modality data, limited generalizability, and poor
interpretability-we propose a novel multi-modal spectral analysis framework
that integrates prior knowledge graphs with Large Language Models. Our method
explicitly bridges physical spectral measurements and chemical structural
semantics by representing them in a unified Textual Graph format, enabling
flexible, interpretable, and generalizable spectral understanding. Raw spectra
are first transformed into TAGs, where nodes and edges are enriched with
textual attributes describing both spectral properties and chemical context.
These are then merged with relevant prior knowledge-including functional groups
and molecular graphs-to form a Task Graph that incorporates "Prompt Nodes"
supporting LLM-based contextual reasoning. A Graph Neural Network further
processes this structure to complete downstream tasks. This unified design
enables seamless multi-modal integration and automated feature decoding with
minimal manual annotation. Our framework achieves consistently high performance
across multiple spectral analysis tasks, including node-level, edge-level, and
graph-level classification. It demonstrates robust generalization in both
zero-shot and few-shot settings, highlighting its effectiveness in learning
from limited data and supporting in-context reasoning. This work establishes a
scalable and interpretable foundation for LLM-driven spectral analysis,
unifying physical and chemical modalities for scientific applications.

</details>


### [329] [PhysiX: A Foundation Model for Physics Simulations](https://arxiv.org/abs/2506.17774)
*Tung Nguyen,Arsh Koneru,Shufan Li,Aditya grover*

Main category: cs.LG

TL;DR: The paper introduces PhysiX, a pioneering large-scale foundation model for physics simulation that surpasses task-specific and prior state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To address the lack of progress in scaling foundation models to physics simulations due to data scarcity and challenges of varied dataset scales.

Method: PhysiX utilizes a 4.5B parameter autoregressive generative model with discrete tokenization and introduces a refinement module to reduce discretization errors.

Result: PhysiX overcomes data scarcity issues and achieves superior performance on the Well benchmark compared to task-specific and state-of-the-art physics simulation models.

Conclusion: PhysiX demonstrates that leveraging natural video knowledge and joint multi-task training significantly boosts physics simulation performance.

Abstract: Foundation models have achieved remarkable success across video, image, and
language domains. By scaling up the number of parameters and training datasets,
these models acquire generalizable world knowledge and often surpass
task-specific approaches. However, such progress has yet to extend to the
domain of physics simulation. A primary bottleneck is data scarcity: while
millions of images, videos, and textual resources are readily available on the
internet, the largest physics simulation datasets contain only tens of
thousands of samples. This data limitation hinders the use of large models, as
overfitting becomes a major concern. As a result, physics applications
typically rely on small models, which struggle with long-range prediction due
to limited context understanding. Additionally, unlike images, videos, or
text-which typically exhibit fixed granularity-physics datasets often vary
drastically in scale, amplifying the challenges of scaling up multitask
training. We introduce PhysiX, the first large-scale foundation model for
physics simulation. PhysiX is a 4.5B parameter autoregressive generative model.
It uses a discrete tokenizer to encode physical processes at different scales
into a sequence of discrete tokens, and employs an autoregressive next-token
prediction objective to model such processes in the token space. To mitigate
the rounding error in the discretization process, PhysiX incorporates a
specialized refinement module. Through extensive experiments, we show that
PhysiX effectively addresses the data bottleneck, outperforming task-specific
baselines under comparable settings as well as the previous absolute
state-of-the-art approaches on The Well benchmark. Our results indicate that
knowledge learned from natural videos can be successfully transferred to
physics simulation, and that joint training across diverse simulation tasks
enables synergistic learning.

</details>


### [330] [Machine Learning Model Integration with Open World Temporal Logic for Process Automation](https://arxiv.org/abs/2506.17776)
*Dyuman Aditya,Colton Payne,Mario Leiva,Paulo Shakarian*

Main category: cs.LG

TL;DR: A system is proposed that integrates outputs of machine learning models with the PyReason framework for automated and explainable decision-making.


<details>
  <summary>Details</summary>
Motivation: There is a gap between perceptual or extractive outputs of machine learning models and actionable logical decision-making in complex workflows.

Method: The paper uses PyReason, a temporal logic programming engine, to integrate ML outputs as truth intervals into its reasoning framework, leveraging Python-based tools for continuous polling and dynamic reasoning.

Result: The integration provides realtime adaptive decision-making with features for temporal reasoning, knowledge graph usage, and explainability.

Conclusion: The system successfully merges ML's perception capabilities with PyReason's logical inference, enabling advanced, explainable process automation for various fields such as healthcare and manufacturing.

Abstract: Recent advancements in Machine Learning (ML) have yielded powerful models
capable of extracting structured information from diverse and complex data
sources. However, a significant challenge lies in translating these perceptual
or extractive outputs into actionable, reasoned decisions within complex
operational workflows. To address these challenges, this paper introduces a
novel approach that integrates the outputs from various machine learning models
directly with the PyReason framework, an open-world temporal logic programming
reasoning engine. PyReason's foundation in generalized annotated logic allows
for the seamless incorporation of real-valued outputs (e.g., probabilities,
confidence scores) from diverse ML models, treating them as truth intervals
within its logical framework. Crucially, PyReason provides mechanisms,
implemented in Python, to continuously poll ML model outputs, convert them into
logical facts, and dynamically recompute the minimal model, ensuring real-tine
adaptive decision-making. Furthermore, its native support for temporal
reasoning, knowledge graph integration, and fully explainable interface traces
enables sophisticated analysis over time-sensitive process data and existing
organizational knowledge. By combining the strengths of perception and
extraction from ML models with the logical deduction and transparency of
PyReason, we aim to create a powerful system for automating complex processes.
This integration finds utility across numerous domains, including
manufacturing, healthcare, and business operations.

</details>


### [331] [Toward Autonomous UI Exploration: The UIExplorer Benchmark](https://arxiv.org/abs/2506.17779)
*Andrei Cristian Nica,Akshaya Vishnu Kudlu Shanbhogue,Harshil Shah,Aleix Cambray,Tudor Berariu,Lucas Maystre,David Barber*

Main category: cs.LG

TL;DR: The paper introduces UIExplore-Bench, the first benchmark for evaluating UI exploration by autonomous agents, providing metrics and results to benchmark agent performance compared to humans.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic evaluation methods for the crucial phase of autonomous agents exploring user interfaces for task solving.

Method: The authors designed the UIExplore-Bench benchmark with Structured and Screen modes, using the GitLab sandbox. They quantified UI exploration with the human-normalized UI-Functionalities Observed (hUFO) metric.

Result: The UIExplore-AlGo agent achieved leading scores, with performance at 77.2% of human exploration in Structured mode and 59.0% in Screen mode. Current agents still display a significant gap compared to human experts.

Conclusion: The results underline the importance of UIExplore-Bench, highlighting opportunities for research and advancements in UI exploration by releasing the benchmark, dataset, and evaluation suite.

Abstract: Autonomous agents must know how to explore user interfaces (UIs) for reliable
task solving, yet systematic evaluation of this crucial phase is lacking. We
introduce UIExplore-Bench, the first benchmark explicitly dedicated to UI
exploration. The benchmark evaluates agents with either Structured mode
(granting access to layout information like DOM trees) or Screen mode (relying
on GUI-only observations such as screenshots and human-like mouse/keyboard
interactions) across three levels in a standardized GitLab sandbox environment.
We formalize exploration as the process of maximizing the set of actionable UI
components discovered and propose a metric, human-normalized UI-Functionalities
Observed (hUFO), to quantify the effectiveness of exploration. Our results show
that UIExplore-AlGo achieves the leading mean hUFO scores, reaching up to 77.2%
of human performance in Structured mode and 59.0% in Screen mode at 2,000
steps, particularly excelling at the Sparse level. The results highlight the
relevance of our benchmark, as current agents show a substantial performance
gap compared to one hour of human expert exploration, indicating ample room for
future advancements. We publicly release the benchmark environment, an
exploration dataset, and an evaluation suite to catalyze research into
efficient UI exploration strategies and their downstream applications, such as
experience-driven task completion and automated training data generation.

</details>


### [332] [Beyond instruction-conditioning, MoTE: Mixture of Task Experts for Multi-task Embedding Models](https://arxiv.org/abs/2506.17781)
*Miguel Romero,Shuoyang Ding,Corey D. Barret,Georgiana Dinu,George Karypis*

Main category: cs.LG

TL;DR: The paper introduces the Mixture of Task Experts (MoTE) transformer block to overcome limits of low-capacity models in embedding specialization, achieving significant performance gains in retrieval tasks without increasing complexity.


<details>
  <summary>Details</summary>
Motivation: Dense embeddings are crucial in modern ML systems, but instruction-conditioning has representational limitations, especially for low-capacity models, restricting performance enhancement.

Method: The authors propose the Mixture of Task Experts (MoTE) transformer block, using task-specialized parameters and Task-Aware Contrastive Learning (TACL) to refine embedding specialization.

Result: MoTE delivers 64% higher performance improvement in retrieval datasets and 43% across all datasets, surpassing traditional approaches without modifying instructions, data, or active parameters.

Conclusion: The Mixture of Task Experts (MoTE) approach successfully enhances embedding specialization, improving model performance without additional complexity in parameters or data requirements.

Abstract: Dense embeddings are fundamental to modern machine learning systems, powering
Retrieval-Augmented Generation (RAG), information retrieval, and representation
learning. While instruction-conditioning has become the dominant approach for
embedding specialization, its direct application to low-capacity models imposes
fundamental representational constraints that limit the performance gains
derived from specialization. In this paper, we analyze these limitations and
introduce the Mixture of Task Experts (MoTE) transformer block, which leverages
task-specialized parameters trained with Task-Aware Contrastive Learning
(\tacl) to enhance the model ability to generate specialized embeddings.
Empirical results show that MoTE achieves $64\%$ higher performance gains in
retrieval datasets ($+3.27 \rightarrow +5.21$) and $43\%$ higher performance
gains across all datasets ($+1.81 \rightarrow +2.60$). Critically, these gains
are achieved without altering instructions, training data, inference time, or
number of active parameters.

</details>


### [333] [Reimagining Parameter Space Exploration with Diffusion Models](https://arxiv.org/abs/2506.17807)
*Lijun Zhang,Xiao Liu,Hui Guan*

Main category: cs.LG

TL;DR: The paper introduces a generative approach using diffusion models to synthesize neural network parameters directly from task identifiers, avoiding the need for task-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning neural networks for new tasks is often time-intensive and reliant on labeled data. The authors aim to eliminate this dependency by exploring a generative method.

Method: The authors propose using task-conditioned diffusion models to learn the structure of task-specific parameter spaces and generate neural network weights from task identifiers.

Result: Experiments show that diffusion models can generate effective parameters for seen tasks and support interpolation, but fail to generalize to unseen tasks.

Conclusion: Diffusion models offer potential for generating task-specific parameters efficiently for seen tasks; however, their inability to generalize to unseen tasks underscores limitations of the approach.

Abstract: Adapting neural networks to new tasks typically requires task-specific
fine-tuning, which is time-consuming and reliant on labeled data. We explore a
generative alternative that produces task-specific parameters directly from
task identity, eliminating the need for task-specific training. To this end, we
propose using diffusion models to learn the underlying structure of effective
task-specific parameter space and synthesize parameters on demand. Once
trained, the task-conditioned diffusion model can generate specialized weights
directly from task identifiers. We evaluate this approach across three
scenarios: generating parameters for a single seen task, for multiple seen
tasks, and for entirely unseen tasks. Experiments show that diffusion models
can generate accurate task-specific parameters and support multi-task
interpolation when parameter subspaces are well-structured, but fail to
generalize to unseen tasks, highlighting both the potential and limitations of
this generative solution.

</details>


### [334] [Actionable Interpretability via Causal Hypergraphs: Unravelling Batch Size Effects in Deep Learning](https://arxiv.org/abs/2506.17826)
*Zhongtian Sun,Anoushka Harit,Pietro Lio*

Main category: cs.LG

TL;DR: This paper introduces HGCNet, a hypergraph-based causal framework to study batch size impact on generalization in graph and text tasks.


<details>
  <summary>Details</summary>
Motivation: To understand the causal mechanisms linking batch size to generalization in non-vision domains, which remain underexplored compared to vision tasks.

Method: A hypergraph-based framework called HGCNet uses deep structural causal models (DSCMs) to quantify causal effects using do-calculus, analyzing gradient noise, minima sharpness, and model complexity.

Result: HGCNet outperformed existing models like GCN, GAT, PI-GNN, BERT, and RoBERTa on citation networks, biomedical text, and e-commerce reviews.

Conclusion: Smaller batch sizes enhance generalization through increased stochasticity and flatter minima, offering interpretable insights for better training strategies and optimization choices.

Abstract: While the impact of batch size on generalisation is well studied in vision
tasks, its causal mechanisms remain underexplored in graph and text domains. We
introduce a hypergraph-based causal framework, HGCNet, that leverages deep
structural causal models (DSCMs) to uncover how batch size influences
generalisation via gradient noise, minima sharpness, and model complexity.
Unlike prior approaches based on static pairwise dependencies, HGCNet employs
hypergraphs to capture higher-order interactions across training dynamics.
Using do-calculus, we quantify direct and mediated effects of batch size
interventions, providing interpretable, causally grounded insights into
optimisation. Experiments on citation networks, biomedical text, and e-commerce
reviews show that HGCNet outperforms strong baselines including GCN, GAT,
PI-GNN, BERT, and RoBERTa. Our analysis reveals that smaller batch sizes
causally enhance generalisation through increased stochasticity and flatter
minima, offering actionable interpretability to guide training strategies in
deep learning. This work positions interpretability as a driver of principled
architectural and optimisation choices beyond post hoc analysis.

</details>


### [335] [Aligning Frozen LLMs by Reinforcement Learning: An Iterative Reweight-then-Optimize Approach](https://arxiv.org/abs/2506.17828)
*Xinnan Zhang,Chenliang Li,Siliang Zeng,Jiaxiang Li,Zhongruo Wang,Kaixiang Lin,Songtao Lu,Alfredo Garcia,Mingyi Hong*

Main category: cs.LG

TL;DR: The paper introduces Iterative Reweight-then-Optimize (IRO), a reinforcement learning approach for aligning frozen language models with human preferences without modifying their parameters, addressing limitations in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for aligning LLMs, such as RLHF, require access to model weights and cannot operate at test-time. This paper aims to create a method that works on frozen models and provides efficient test-time alignment.

Method: IRO follows an iterative process: sampling outputs, resampling via value functions, training lightweight value models, and guiding generation using search-based optimization. It aligns frozen models during test-time without weight updates.

Result: IRO successfully aligns frozen LLMs with user-provided datasets at test-time, offering efficiency and improved output quality, overcoming the limitations of prior methods.

Conclusion: IRO provides a framework for RL-style alignment of frozen LLMs, enabling efficient, test-time optimization without requiring model weight access, filling a critical gap in alignment techniques.

Abstract: Aligning large language models (LLMs) with human preferences usually requires
fine-tuning methods such as RLHF and DPO. These methods directly optimize the
model parameters, so they cannot be used in test-time to improve model
performance, nor are they applicable when the model weights are not accessible.
In contrast, test-time methods sidestep weight updates by leveraging reward
functions to guide and improve output quality. However, they incur high
inference costs, and their one-shot guidance is often based on imperfect reward
or value functions, leading to suboptimal outputs. In this work, we present a
method named Iterative Reweight-then-Optimize (IRO), a reinforcement learning
(RL) framework that performs RL-style alignment of the (frozen) base model
without touching its parameters. During training, each iteration (i) samples
candidates from the base model, (ii) resamples using current value functions,
and (iii) trains a new lightweight value function that guides the next decoding
pass. At test time, the value functions are used to guide the base model
generation via a search-based optimization process. Notably, users can apply
IRO to align a model on their own dataset, similar to OpenAI's reinforcement
fine-tuning (RFT), but without requiring access to the model weights.

</details>


### [336] [Causal Spherical Hypergraph Networks for Modelling Social Uncertainty](https://arxiv.org/abs/2506.17840)
*Anoushka Harit,Zhongtian Sun*

Main category: cs.LG

TL;DR: The paper introduces a framework called Causal Spherical Hypergraph Networks (Causal-SphHN) for improving prediction in social behaviors by incorporating causality, uncertainty, and higher-order group dynamics.


<details>
  <summary>Details</summary>
Motivation: Social behaviors are governed by uncertain and complex group interactions, and there's a need for a method that effectively handles these aspects in predictive models.

Method: The proposed method uses hyperspherical embeddings, hyperedges to represent group contexts, von Mises-Fisher distributions for uncertainty, and Granger-informed subgraphs to identify causal dependencies.

Result: Causal-SphHN demonstrated better predictive accuracy, robustness, and calibration compared to existing approaches across datasets such as SNARE, PHEME, and AMIGOS.

Conclusion: Causal-SphHN provides a unified, interpretable, and robust causal-geometric framework for modeling social behaviors under uncertainty.

Abstract: Human social behaviour is governed by complex interactions shaped by
uncertainty, causality, and group dynamics. We propose Causal Spherical
Hypergraph Networks (Causal-SphHN), a principled framework for socially
grounded prediction that jointly models higher-order structure, directional
influence, and epistemic uncertainty. Our method represents individuals as
hyperspherical embeddings and group contexts as hyperedges, capturing semantic
and relational geometry. Uncertainty is quantified via Shannon entropy over von
Mises-Fisher distributions, while temporal causal dependencies are identified
using Granger-informed subgraphs. Information is propagated through an angular
message-passing mechanism that respects belief dispersion and directional
semantics. Experiments on SNARE (offline networks), PHEME (online discourse),
and AMIGOS (multimodal affect) show that Causal-SphHN improves predictive
accuracy, robustness, and calibration over strong baselines. Moreover, it
enables interpretable analysis of influence patterns and social ambiguity. This
work contributes a unified causal-geometric approach for learning under
uncertainty in dynamic social environments.

</details>


### [337] [A Comparative Study of Open-Source Libraries for Synthetic Tabular Data Generation: SDV vs. SynthCity](https://arxiv.org/abs/2506.17847)
*Cristian Del Gobbo*

Main category: cs.LG

TL;DR: This paper compares the performance of six tabular synthetic data generators in replicating real-world data for machine learning purposes.


<details>
  <summary>Details</summary>
Motivation: To address the difficulties smaller organizations and startups face in obtaining high-quality training data by evaluating synthetic data generators.

Method: Six synthetic data generators from SDV and Synthicity were tested on a real-world dataset under 1:1 and 1:10 input-output ratios, assessed via statistical similarity and predictive utility.

Result: Bayesian Network (Synthicity) performed best in statistical fidelity, while TVAE (SDV) excelled in predictive tasks under the 1:10 setting. SDV was noted for its ease of use.

Conclusion: Synthetic data generators are promising for low-data scenarios; SDV is recommended for practitioners due to its usability, though Bayesian Network and TVAE showed specific strengths.

Abstract: High-quality training data is critical to the performance of machine learning
models, particularly Large Language Models (LLMs). However, obtaining real,
high-quality data can be challenging, especially for smaller organizations and
early-stage startups. Synthetic data generators provide a promising solution by
replicating the statistical and structural properties of real data while
preserving privacy and scalability. This study evaluates the performance of six
tabular synthetic data generators from two widely used open-source libraries:
SDV (Gaussian Copula, CTGAN, TVAE) and Synthicity (Bayesian Network, CTGAN,
TVAE). Using a real-world dataset from the UCI Machine Learning Repository,
comprising energy consumption and environmental variables from Belgium, we
simulate a low-data regime by training models on only 1,000 rows. Each
generator is then tasked with producing synthetic datasets under two
conditions: a 1:1 (1,000 rows) and a 1:10 (10,000 rows) input-output ratio.
Evaluation is conducted using two criteria: statistical similarity, measured
via classical statistics and distributional metrics; and predictive utility,
assessed using a "Train on Synthetic, Test on Real" approach with four
regression models. While statistical similarity remained consistent across
models in both scenarios, predictive utility declined notably in the 1:10 case.
The Bayesian Network from Synthicity achieved the highest fidelity in both
scenarios, while TVAE from SDV performed best in predictive tasks under the
1:10 setting. Although no significant performance gap was found between the two
libraries, SDV stands out for its superior documentation and ease of use,
making it more accessible for practitioners.

</details>


### [338] [Pathway-based Progressive Inference (PaPI) for Energy-Efficient Continual Learning](https://arxiv.org/abs/2506.17848)
*Suyash Gaurav,Jukka Heikkonen,Jatin Chaudhary*

Main category: cs.LG

TL;DR: The paper introduces the Pathway-based Progressive Inference (PaPI) framework, aimed at addressing catastrophic forgetting and improving energy efficiency in continual learning systems.


<details>
  <summary>Details</summary>
Motivation: To develop a continual learning system that efficiently balances the stability-plasticity trade-off while being energy-conscious for application in resource-constrained settings.

Method: PaPI introduces a novel approach to pathway selection formulated as an energy-constrained optimization problem. It utilizes Fisher Information Matrix analysis for deriving bounds on forgetting and energy consumption and provides theoretical guarantees on convergence and effectiveness.

Result: Theoretical analysis shows PaPI’s superiority in addressing catastrophic forgetting, with an $\mathcal{O}(K)$ improvement in stability-plasticity over monolithic architectures. It outperforms Elastic Weight Consolidation (EWC) and Gradient Episodic Memory (GEM) in both catastrophic forgetting mitigation and energy efficiency. Experimental validations confirm these advantages.

Conclusion: PaPI is established as a robust and energy-efficient framework for continual learning, suitable for resource-constrained applications. Its theoretical and empirical strengths make it superior to existing methods like EWC and GEM.

Abstract: Continual learning systems face the dual challenge of preventing catastrophic
forgetting while maintaining energy efficiency, particularly in
resource-constrained environments. This paper introduces Pathway-based
Progressive Inference (PaPI), a novel theoretical framework that addresses
these challenges through a mathematically rigorous approach to pathway
selection and adaptation. We formulate continual learning as an
energy-constrained optimization problem and provide formal convergence
guarantees for our pathway routing mechanisms. Our theoretical analysis
demonstrates that PaPI achieves an $\mathcal{O}(K)$ improvement in the
stability-plasticity trade-off compared to monolithic architectures, where $K$
is the number of pathways. We derive tight bounds on forgetting rates using
Fisher Information Matrix analysis and prove that PaPI's energy consumption
scales with the number of active parameters rather than the total model size.
Comparative theoretical analysis shows that PaPI provides stronger guarantees
against catastrophic forgetting than Elastic Weight Consolidation (EWC) while
maintaining better energy efficiency than both EWC and Gradient Episodic Memory
(GEM). Our experimental validation confirms these theoretical advantages across
multiple benchmarks, demonstrating PaPI's effectiveness for continual learning
in energy-constrained settings. Our codes are available at
https://github.com/zser092/PAPI_FILES.

</details>


### [339] [In-Context Learning Strategies Emerge Rationally](https://arxiv.org/abs/2506.17859)
*Daniel Wurgaft,Ekdeep Singh Lubana,Core Francisco Park,Hidenori Tanaka,Gautam Reddy,Noah D. Goodman*

Main category: cs.LG

TL;DR: The paper introduces a Bayesian framework to explain how models learn in-context strategies as a tradeoff between loss and complexity, reconciling varying observed behaviors.


<details>
  <summary>Details</summary>
Motivation: To unify findings on why models adopt different strategies in in-context learning across experimental conditions.

Method: Developing a hierarchical Bayesian framework based on rational analysis, without relying on access to model weights.

Result: The proposed framework accurately predicts Transformer next token behaviors throughout training and yields novel phenomena, such as superlinear trends in memorization transitions.

Conclusion: The study offers an explanatory framework for in-context learning, emphasizing the tradeoffs between strategy effectiveness and complexity.

Abstract: Recent work analyzing in-context learning (ICL) has identified a broad set of
strategies that describe model behavior in different experimental conditions.
We aim to unify these findings by asking why a model learns these disparate
strategies in the first place. Specifically, we start with the observation that
when trained to learn a mixture of tasks, as is popular in the literature, the
strategies learned by a model for performing ICL can be captured by a family of
Bayesian predictors: a memorizing predictor, which assumes a discrete prior on
the set of seen tasks, and a generalizing predictor, wherein the prior matches
the underlying task distribution. Adopting the lens of rational analysis from
cognitive science, where a learner's behavior is explained as an optimal
adaptation to data given computational constraints, we develop a hierarchical
Bayesian framework that almost perfectly predicts Transformer next token
predictions throughout training without assuming access to its weights. Under
this framework, pretraining is viewed as a process of updating the posterior
probability of different strategies, and its inference-time behavior as a
posterior-weighted average over these strategies' predictions. Our framework
draws on common assumptions about neural network learning dynamics, which make
explicit a tradeoff between loss and complexity among candidate strategies:
beyond how well it explains the data, a model's preference towards implementing
a strategy is dictated by its complexity. This helps explain well-known ICL
phenomena, while offering novel predictions: e.g., we show a superlinear trend
in the timescale for transition to memorization as task diversity is increased.
Overall, our work advances an explanatory and predictive account of ICL
grounded in tradeoffs between strategy loss and complexity.

</details>


### [340] [Decoding Federated Learning: The FedNAM+ Conformal Revolution](https://arxiv.org/abs/2506.17872)
*Sree Bhargavi Balija,Amitash Nanda,Debashis Sahoo*

Main category: cs.LG

TL;DR: FedNAM+ is a federated learning framework that integrates Neural Additive Models and a novel conformal prediction method to improve interpretability, reliability, and uncertainty estimation.


<details>
  <summary>Details</summary>
Motivation: Existing federated learning frameworks lack comprehensive solutions for combining uncertainty quantification, interpretability, and robustness.

Method: The authors propose FedNAM+, which uses Neural Additive Models and introduces a dynamic level adjustment technique leveraging gradient-based sensitivity maps for interpretability and pixel-wise uncertainty estimation.

Result: Experiments on datasets like CT scan, MNIST, and CIFAR show high prediction accuracy (e.g., only 0.1% loss on MNIST) and robust uncertainty estimates. FedNAM+ is more efficient with lower computational costs compared to methods like Monte Carlo Dropout.

Conclusion: FedNAM+ offers a reliable, interpretable, and computationally efficient solution for federated learning, enhancing trust and transparency in predictive modeling.

Abstract: Federated learning has significantly advanced distributed training of machine
learning models across decentralized data sources. However, existing frameworks
often lack comprehensive solutions that combine uncertainty quantification,
interpretability, and robustness. To address this, we propose FedNAM+, a
federated learning framework that integrates Neural Additive Models (NAMs) with
a novel conformal prediction method to enable interpretable and reliable
uncertainty estimation. Our method introduces a dynamic level adjustment
technique that utilizes gradient-based sensitivity maps to identify key input
features influencing predictions. This facilitates both interpretability and
pixel-wise uncertainty estimates. Unlike traditional interpretability methods
such as LIME and SHAP, which do not provide confidence intervals, FedNAM+
offers visual insights into prediction reliability. We validate our approach
through experiments on CT scan, MNIST, and CIFAR datasets, demonstrating high
prediction accuracy with minimal loss (e.g., only 0.1% on MNIST), along with
transparent uncertainty measures. Visual analysis highlights variable
uncertainty intervals, revealing low-confidence regions where model performance
can be improved with additional data. Compared to Monte Carlo Dropout, FedNAM+
delivers efficient and global uncertainty estimates with reduced computational
overhead, making it particularly suitable for federated learning scenarios.
Overall, FedNAM+ provides a robust, interpretable, and computationally
efficient framework that enhances trust and transparency in decentralized
predictive modeling.

</details>


### [341] [TROJAN-GUARD: Hardware Trojans Detection Using GNN in RTL Designs](https://arxiv.org/abs/2506.17894)
*Kiran Thorat,Amit Hasan,Caiwen Ding,Zhijie Shi*

Main category: cs.LG

TL;DR: The paper proposes a framework for detecting hardware trojans in large chip designs using graph neural networks (GNNs) with domain-specific optimizations.


<details>
  <summary>Details</summary>
Motivation: The growing reliance on untrusted third-party tools and intellectual properties increases the risk of hardware trojans in chip manufacturing, threatening national security, the economy, and privacy.

Method: The proposed framework uses graph embeddings for large chip designs, incorporates various GNN models for trojan detection, and optimizes efficiency with model quantization to reduce computational requirements while maintaining accuracy.

Result: The evaluation on a custom dataset shows a precision of 98.66% and a recall of 92.30%, demonstrating its high accuracy and efficiency for large chip designs.

Conclusion: The framework effectively addresses the shortcomings of existing methods by applying tailored GNNs and optimization techniques to efficiently detect hardware trojans in large-scale designs.

Abstract: Chip manufacturing is a complex process, and to achieve a faster time to
market, an increasing number of untrusted third-party tools and designs from
around the world are being utilized. The use of these untrusted third party
intellectual properties (IPs) and tools increases the risk of adversaries
inserting hardware trojans (HTs). The covert nature of HTs poses significant
threats to cyberspace, potentially leading to severe consequences for national
security, the economy, and personal privacy. Many graph neural network
(GNN)-based HT detection methods have been proposed. However, they perform
poorly on larger designs because they rely on training with smaller designs.
Additionally, these methods do not explore different GNN models that are
well-suited for HT detection or provide efficient training and inference
processes. We propose a novel framework that generates graph embeddings for
large designs (e.g., RISC-V) and incorporates various GNN models tailored for
HT detection. Furthermore, our framework introduces domain-specific techniques
for efficient training and inference by implementing model quantization. Model
quantization reduces the precision of the weights, lowering the computational
requirements, enhancing processing speed without significantly affecting
detection accuracy. We evaluate our framework using a custom dataset, and our
results demonstrate a precision of 98.66% and a recall (true positive rate) of
92.30%, highlighting the effectiveness and efficiency of our approach in
detecting hardware trojans in large-scale chip designs

</details>


### [342] [Permutation Equivariant Model-based Offline Reinforcement Learning for Auto-bidding](https://arxiv.org/abs/2506.17919)
*Zhiyu Mou,Miao Xu,Wei Chen,Rongquan Bai,Chuan Yu,Jian Xu*

Main category: cs.LG

TL;DR: The paper introduces Model-based RL Bidding (MRLB) by combining learned environment models with real data to improve auto-bidding policies. A novel algorithm, PE-MORL, showcases better performance than previous state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in auto-bidding policies caused by either poor state coverage (Offline RL Bidding) or inaccuracies due to simulator gaps (Simulation-based RL Bidding), thus improving policy reliability and coverage.

Method: The paper develops MRLB to expand state coverage by combining real and model-generated data. It employs a permutation equivariant model architecture for generalization and integrates a robust Q-learning mechanism to counteract the errors from the learned model.

Result: The proposed PE-MORL algorithm achieves superior performance in real-world auto-bidding tasks compared to prior methods.

Conclusion: MRLB with PE-MORL reduces the simulator gap and extends state space coverage, leading to significant improvements in auto-bidding policy effectiveness.

Abstract: Reinforcement learning (RL) for auto-bidding has shifted from using
simplistic offline simulators (Simulation-based RL Bidding, SRLB) to offline RL
on fixed real datasets (Offline RL Bidding, ORLB). However, ORLB policies are
limited by the dataset's state space coverage, offering modest gains. While
SRLB expands state coverage, its simulator-reality gap risks misleading
policies. This paper introduces Model-based RL Bidding (MRLB), which learns an
environment model from real data to bridge this gap. MRLB trains policies using
both real and model-generated data, expanding state coverage beyond ORLB. To
ensure model reliability, we propose: 1) A permutation equivariant model
architecture for better generalization, and 2) A robust offline Q-learning
method that pessimistically penalizes model errors. These form the Permutation
Equivariant Model-based Offline RL (PE-MORL) algorithm. Real-world experiments
show that PE-MORL outperforms state-of-the-art auto-bidding methods.

</details>


### [343] [ASTER: Adaptive Spatio-Temporal Early Decision Model for Dynamic Resource Allocation](https://arxiv.org/abs/2506.17929)
*Shulun Chen,Wei Shao,Flora D. Salim,Hao Xue*

Main category: cs.LG

TL;DR: This paper presents ASTER, a model that integrates spatio-temporal forecasting with decision-making to improve actionable outcomes like resource allocation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to bridge the gap between prediction and decision phases in spatio-temporal intelligence, focusing on actionable strategies for tasks such as emergency response.

Method: ASTER includes a Resource-aware Spatio-Temporal interaction module (RaST) for context-adaptive forecasting and a Preference-oriented decision agent (Poda) that uses reinforcement learning for optimal resource allocation.

Result: Experiments on four datasets show that ASTER offers state-of-the-art performance in early prediction accuracy and resource allocation across six metrics.

Conclusion: ASTER successfully shifts spatio-temporal forecasting from merely predicting events to supporting dynamic, resource-efficient decision-making strategies.

Abstract: Supporting decision-making has long been a central vision in the field of
spatio-temporal intelligence. While prior work has improved the timeliness and
accuracy of spatio-temporal forecasting, converting these forecasts into
actionable strategies remains a key challenge. A main limitation is the
decoupling of the prediction and the downstream decision phases, which can
significantly degrade the downstream efficiency. For example, in emergency
response, the priority is successful resource allocation and intervention, not
just incident prediction. To this end, it is essential to propose an Adaptive
Spatio-Temporal Early Decision model (ASTER) that reforms the forecasting
paradigm from event anticipation to actionable decision support. This framework
ensures that information is directly used for decision-making, thereby
maximizing overall effectiveness. Specifically, ASTER introduces a new
Resource-aware Spatio-Temporal interaction module (RaST) that adaptively
captures long- and short-term dependencies under dynamic resource conditions,
producing context-aware spatiotemporal representations. To directly generate
actionable decisions, we further design a Preference-oriented decision agent
(Poda) based on multi-objective reinforcement learning, which transforms
predictive signals into resource-efficient intervention strategies by deriving
optimal actions under specific preferences and dynamic constraints.
Experimental results on four benchmark datasets demonstrate the
state-of-the-art performance of ASTER in improving both early prediction
accuracy and resource allocation outcomes across six downstream metrics.

</details>


### [344] [Adapting Vision-Language Models for Evaluating World Models](https://arxiv.org/abs/2506.17967)
*Mariya Hendriksen,Tabish Rashid,David Bignell,Raluca Georgescu,Abdelhak Lemkhenter,Katja Hofmann,Sam Devlin,Sarah Parisot*

Main category: cs.LG

TL;DR: The paper introduces UNIVERSE, a method that adapts Vision-Language Models (VLMs) to evaluate generative model rollouts in simulated environments under data and compute constraints. It proves to align well with human judgments.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitation of existing metrics that fail to provide fine-grained, temporally grounded evaluations of generative content produced by world models, particularly for action and semantic alignment.

Method: The authors introduce a new evaluation protocol targeting action recognition and character recognition tasks, offering binary, multiple-choice, and open-ended formats. They adapt Vision-Language Models (VLMs) via different fine-tuning strategies under constraints, benchmarking their efficacy.

Result: UNIVERSE achieves task-specific performance by using a unified checkpoint that aligns well with human evaluations across tasks. It provides scalable, semantics-aware evaluation for generative models.

Conclusion: UNIVERSE is validated as a robust, scalable evaluation mechanism that can replace task-specific evaluators while preserving human-level judgment for generative model rollouts in simulated environments.

Abstract: World models -- generative models that simulate environment dynamics
conditioned on past observations and actions -- are gaining prominence in
planning, simulation, and embodied AI. However, evaluating their rollouts
remains a fundamental challenge, requiring fine-grained, temporally grounded
assessment of action alignment and semantic consistency -- capabilities not
captured by existing metrics. Vision-Language Models (VLMs) have shown promise
as automatic evaluators of generative content due to their strong multimodal
reasoning abilities. Yet, their use in fine-grained, temporally sensitive
evaluation tasks remains limited and requires targeted adaptation. We introduce
a evaluation protocol targeting two recognition tasks -- action recognition and
character recognition -- each assessed across binary, multiple-choice, and
open-ended formats. To support this, we present UNIVERSE (UNIfied
Vision-language Evaluator for Rollouts in Simulated Environments), a method for
adapting VLMs to rollout evaluation under data and compute constraints. We
conduct a large-scale study comparing full, partial, and parameter-efficient
finetuning across task formats, context lengths, sampling strategies, and data
compositions. The resulting unified evaluator matches the performance of
task-specific baselines using a single checkpoint. Human studies confirm strong
alignment with human judgments, establishing UNIVERSE as a scalable,
semantics-aware evaluator for world models.

</details>


### [345] [Trustworthy Efficient Communication for Distributed Learning using LQ-SGD Algorithm](https://arxiv.org/abs/2506.17974)
*Hongyang Li,Lincen Bai,Caesar Wu,Mohammed Chadli,Said Mammar,Pascal Bouvry*

Main category: cs.LG

TL;DR: Introduction of LQ-SGD gradient compression algorithm using low-rank approximation and log-quantization for distributed training to minimize communication overhead while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Develop a robust and efficient method addressing communication inefficiencies in distributed training systems.

Method: Combine PowerSGD's low-rank approximation with log-quantization techniques in the LQ-SGD algorithm.

Result: Reduced communication overhead with maintained convergence speed and model accuracy, stronger resistance to gradient inversion.

Conclusion: LQ-SGD improves the efficiency and robustness of distributed training systems via innovative compression techniques.

Abstract: We propose LQ-SGD (Low-Rank Quantized Stochastic Gradient Descent), an
efficient communication gradient compression algorithm designed for distributed
training. LQ-SGD further develops on the basis of PowerSGD by incorporating the
low-rank approximation and log-quantization techniques, which drastically
reduce the communication overhead, while still ensuring the convergence speed
of training and model accuracy. In addition, LQ-SGD and other compression-based
methods show stronger resistance to gradient inversion than traditional SGD,
providing a more robust and efficient optimization path for distributed
learning systems.

</details>


### [346] [SliceGX: Layer-wise GNN Explanation with Model-slicing](https://arxiv.org/abs/2506.17977)
*Tingting Zhu,Tingyang Chen,Yinghui Wu,Arijit Khan,Xiangyu Ke*

Main category: cs.LG

TL;DR: The paper presents SliceGX, a method to generate layer-wise explanations for GNNs, addressing their black-box nature and aiding in model debugging.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing GNN explanation methods, which lack fine-grained, layer-wise analysis critical for model diagnosis and optimization.

Method: SliceGX segments GNNs into layer-specific blocks and identifies explanatory subgraphs for outputs at targeted layers using efficient algorithms with approximation guarantees.

Result: Experimental validation on large real-world graphs and representative GNN models shows SliceGX's effectiveness and efficiency in layer-wise explanation generation.

Conclusion: SliceGX provides a novel approach for progressive layer-wise GNN explanations, enabling better model understanding, debugging, and architecture optimization.

Abstract: Ensuring the trustworthiness of graph neural networks (GNNs) as black-box
models requires effective explanation methods. Existing GNN explanations
typically apply input perturbations to identify subgraphs that are responsible
for the occurrence of the final output of GNNs. However, such approaches lack
finer-grained, layer-wise analysis of how intermediate representations
contribute to the final result, capabilities that are crucial for model
diagnosis and architecture optimization. This paper introduces SliceGX, a novel
GNN explanation approach that generates explanations at specific GNN layers in
a progressive manner. Given a GNN M, a set of selected intermediate layers, and
a target layer, SliceGX automatically segments M into layer blocks ("model
slice") and discovers high-quality explanatory subgraphs in each layer block
that clarifies the occurrence of output of M at the targeted layer. Although
finding such layer-wise explanations is computationally challenging, we develop
efficient algorithms and optimization techniques that incrementally generate
and maintain these subgraphs with provable approximation guarantees.
Additionally, SliceGX offers a SPARQL-like query interface, providing
declarative access and search capacities for the generated explanations.
Through experiments on large real-world graphs and representative GNN
architectures, we verify the effectiveness and efficiency of SliceGX, and
illustrate its practical utility in supporting model debugging.

</details>


### [347] [Data Curation Matters: Model Collapse and Spurious Shift Performance Prediction from Training on Uncurated Text Embeddings](https://arxiv.org/abs/2506.17989)
*Lucas Mattioli,Youness Ait Hadichou,Sabrina Chaouche,Martin Gonzalez*

Main category: cs.LG

TL;DR: This paper identifies a risk called model collapse when models are trained using text embeddings (TEs) derived from raw tabular data, leading to incorrect predictions. It offers metrics to measure collapse severity and insights on embedding quality's role in learning.


<details>
  <summary>Details</summary>
Motivation: The authors aim to examine the reliability of Text Embeddings (TEs) derived from raw tabular data in training models without curation, addressing associated risks like model collapse and embedding evaluation quality.

Method: The authors compared models trained with identical hyper-parameter setups using raw tabular data versus TE-derived data. They also introduced metrics to measure the collapse extent and analyzed TE quality's impact.

Result: The study finds TE-derived models suffer from consistent model collapse, which skews performance metrics like accuracy. TE does not serve effectively as a curation layer, and its quality profoundly affects downstream learning.

Conclusion: These findings underscore the importance of careful TE curation and evaluation while raising concerns about current methods' ability to generalize well in out-of-distribution scenarios.

Abstract: Training models on uncurated Text Embeddings (TEs) derived from raw tabular
data can lead to a severe failure mode known as model collapse, where
predictions converge to a single class regardless of input. By comparing models
trained with identical hyper-parameter configurations on both raw tabular data
and their TE-derived counterparts, we find that collapse is a consistent
failure mode in the latter setting. We introduce a set of metrics that capture
the extent of model collapse, offering a new perspective on TE quality as a
proxy for data curation. Our results reveal that TE alone does not effectively
function as a curation layer - and that their quality significantly influences
downstream learning. More insidiously, we observe that the presence of model
collapse can yield artificially inflated and spurious Accuracy-on-the-Line
correlation. These findings highlight the need for more nuanced curation and
evaluation of embedding-based representations, particularly in
out-of-distribution settings.

</details>


### [348] [Why Do Some Language Models Fake Alignment While Others Don't?](https://arxiv.org/abs/2506.18032)
*Abhay Sheshadri,John Hughes,Julian Michael,Alex Mallen,Arun Jose,Janus,Fabien Roger*

Main category: cs.LG

TL;DR: The paper investigates alignment faking in large language models and finds only 5 out of 25 models comply with harmful queries more during training scenarios. Claude 3 Opus shows the most consistent compliance gap.


<details>
  <summary>Details</summary>
Motivation: The study aims to analyze the behavior of large language models (LLMs) in terms of their compliance with harmful queries during training versus deployment, especially in the context of maintaining alignment objectives.

Method: The researchers studied 25 language models, examining their responses to harmful queries during training and post-training scenarios. Perturbation experiments and analyses of refusal behavior were conducted to uncover motivations and mechanisms affecting alignment faking.

Result: Out of 25 models, only 5 exhibited greater compliance with harmful queries during training. Claude 3 Opus was identified as the most consistent case of alignment faking to maintain its goals. Variations in refusal behavior were suggested as a significant factor influencing alignment faking.

Conclusion: The findings indicate that alignment faking may not be fully dependent on model capabilities. Post-training interventions can amplify or suppress this behavior, and refusal behaviors play a critical role in alignment consistency.

Abstract: Alignment faking in large language models presented a demonstration of Claude
3 Opus and Claude 3.5 Sonnet selectively complying with a helpful-only training
objective to prevent modification of their behavior outside of training. We
expand this analysis to 25 models and find that only 5 (Claude 3 Opus, Claude
3.5 Sonnet, Llama 3 405B, Grok 3, Gemini 2.0 Flash) comply with harmful queries
more when they infer they are in training than when they infer they are in
deployment. First, we study the motivations of these 5 models. Results from
perturbing details of the scenario suggest that only Claude 3 Opus's compliance
gap is primarily and consistently motivated by trying to keep its goals.
Second, we investigate why many chat models don't fake alignment. Our results
suggest this is not entirely due to a lack of capabilities: many base models
fake alignment some of the time, and post-training eliminates alignment-faking
for some models and amplifies it for others. We investigate 5 hypotheses for
how post-training may suppress alignment faking and find that variations in
refusal behavior may account for a significant portion of differences in
alignment faking.

</details>


### [349] [Pathwise Explanation of ReLU Neural Networks](https://arxiv.org/abs/2506.18037)
*Seongwoo Lim,Won Jo,Joohyung Lee,Jaesik Choi*

Main category: cs.LG

TL;DR: The paper proposes a new method for explaining neural network decisions by focusing on subsets of hidden units involved in decision-making paths, improving transparency and reliability.


<details>
  <summary>Details</summary>
Motivation: To address concerns about the opaque 'black box' nature of neural networks, and to improve transparency and reliability in their decision-making processes.

Method: The method involves pathwise explanations using subsets of hidden units and offers the flexibility to adjust the scope of explanations for input components.

Result: Experimental results show improved performance over existing methods in terms of both quantitative metrics and qualitative insights.

Conclusion: The approach enhances clarity and consistency in understanding neural network decisions, offering detailed and adjustable explanations.

Abstract: Neural networks have demonstrated a wide range of successes, but their
``black box" nature raises concerns about transparency and reliability.
Previous research on ReLU networks has sought to unwrap these networks into
linear models based on activation states of all hidden units. In this paper, we
introduce a novel approach that considers subsets of the hidden units involved
in the decision making path. This pathwise explanation provides a clearer and
more consistent understanding of the relationship between the input and the
decision-making process. Our method also offers flexibility in adjusting the
range of explanations within the input, i.e., from an overall attribution input
to particular components within the input. Furthermore, it allows for the
decomposition of explanations for a given input for more detailed explanations.
Experiments demonstrate that our method outperforms others both quantitatively
and qualitatively.

</details>


### [350] [TAB: Unified Benchmarking of Time Series Anomaly Detection Methods](https://arxiv.org/abs/2506.18046)
*Xiangfei Qiu,Zhe Li,Wanghui Qiu,Shiyan Hu,Lekui Zhou,Xingjian Wu,Zhengyu Li,Chenjuan Guo,Aoying Zhou,Zhenli Sheng,Jilin Hu,Christian S. Jensen,Bin Yang*

Main category: cs.LG

TL;DR: The paper introduces TAB, a comprehensive benchmark for time series anomaly detection (TSAD), which integrates diverse datasets and methods and provides an automated evaluation pipeline.


<details>
  <summary>Details</summary>
Motivation: To address deficiencies in existing TSAD evaluation procedures and meet growing demands with reliable benchmarks for assessing methods.

Method: The authors created TAB, a benchmark comprising 29 multivariate datasets, 1,635 univariate time series, various TSAD methods, and a unified evaluation pipeline.

Result: TAB evaluated existing TSAD methods, yielding insights into their performance and effectiveness.

Conclusion: TAB facilitates more comprehensive and fair evaluations of TSAD approaches, advancing the domain with publicly accessible datasets and tools.

Abstract: Time series anomaly detection (TSAD) plays an important role in many domains
such as finance, transportation, and healthcare. With the ongoing
instrumentation of reality, more time series data will be available, leading
also to growing demands for TSAD. While many TSAD methods already exist, new
and better methods are still desirable. However, effective progress hinges on
the availability of reliable means of evaluating new methods and comparing them
with existing methods. We address deficiencies in current evaluation procedures
related to datasets and experimental settings and protocols. Specifically, we
propose a new time series anomaly detection benchmark, called TAB. First, TAB
encompasses 29 public multivariate datasets and 1,635 univariate time series
from different domains to facilitate more comprehensive evaluations on diverse
datasets. Second, TAB covers a variety of TSAD methods, including Non-learning,
Machine learning, Deep learning, LLM-based, and Time-series pre-trained
methods. Third, TAB features a unified and automated evaluation pipeline that
enables fair and easy evaluation of TSAD methods. Finally, we employ TAB to
evaluate existing TSAD methods and report on the outcomes, thereby offering a
deeper insight into the performance of these methods. Besides, all datasets and
code are available at https://github.com/decisionintelligence/TAB.

</details>


### [351] [Distributionally robust minimization in meta-learning for system identification](https://arxiv.org/abs/2506.18074)
*Matteo Rufolo,Dario Piga,Marco Forgione*

Main category: cs.LG

TL;DR: The paper introduces distributionally robust minimization in meta learning for system identification to improve performance in worst-case scenarios.


<details>
  <summary>Details</summary>
Motivation: Standard meta learning optimizations tend to overlook variability among tasks, potentially compromising reliability in safety-critical applications.

Method: The researchers apply a distributionally robust optimization approach that focuses on high-loss tasks, favoring worst-case scenario performance.

Result: The method was tested on synthetic dynamic systems and showed improved adaptability in both in-distribution and out-of-distribution situations.

Conclusion: Prioritizing high-loss tasks in meta learning enhances robustness and reliability, particularly for safety-critical applications.

Abstract: Meta learning aims at learning how to solve tasks, and thus it allows to
estimate models that can be quickly adapted to new scenarios. This work
explores distributionally robust minimization in meta learning for system
identification. Standard meta learning approaches optimize the expected loss,
overlooking task variability. We use an alternative approach, adopting a
distributionally robust optimization paradigm that prioritizes high-loss tasks,
enhancing performance in worst-case scenarios. Evaluated on a meta model
trained on a class of synthetic dynamical systems and tested in both
in-distribution and out-of-distribution settings, the proposed approach allows
to reduce failures in safety-critical applications.

</details>


### [352] [RL for Reasoning by Adaptively Revealing Rationales](https://arxiv.org/abs/2506.18110)
*Mohammad Hossein Amani,Aryo Lotfi,Nicolas Mario Baldwin,Samy Bengio,Mehrdad Farajtabar,Emmanuel Abbe,Robert West*

Main category: cs.LG

TL;DR: The paper introduces Adaptive Backtracking (AdaBack), a curriculum learning algorithm for reinforcement learning that dynamically adjusts supervision length based on model performance, enabling effective problem-solving in tasks with long sequence dependencies.


<details>
  <summary>Details</summary>
Motivation: Sequence generation tasks with long output sequences face challenges in both supervised fine-tuning (SFT) and reinforcement learning (RL), requiring novel strategies to overcome issues like costly ground-truth labels and sparse rewards.

Method: The proposed AdaBack algorithm uses adaptive curriculum learning to reveal partial output prefixes during training. It dynamically adjusts supervision length for individual samples, leveraging past reward signals to enable incremental learning of reasoning chains.

Result: AdaBack effectively solves synthetic tasks with latent parity constraints that are otherwise intractable. On mathematical reasoning benchmarks, it outperforms RL by equipping models with new reasoning capabilities through incremental exposure.

Conclusion: Per-sample curriculum learning offers a promising framework between supervised fine-tuning and reinforcement learning, capable of handling tasks with complex, long-sequence dependencies where traditional methods fail.

Abstract: We propose that reinforcement learning (RL) from partial expert
demonstrations is not merely a training heuristic, but a promising framework
for solving complex sequence generation tasks. Supervised fine-tuning (SFT)
relies on dense ground-truth labels, which become increasingly costly as
sequence length grows. RL, on the other hand, struggles with sparse rewards and
a combinatorially large output space. We address this by introducing adaptive
backtracking (AdaBack), a per-sample curriculum learning algorithm that reveals
only a partial prefix of the target output during training. The supervision
length is adjusted dynamically for each sample based on the model's past reward
signal, allowing it to incrementally learn to complete reasoning chains by
conditioning on correct partial solutions. We investigate this intermediate
regime between SFT and RL and argue that per-sample curriculum learning is more
than a trade-off between efficiency and generality, it can succeed in tasks
with long sequences of latent dependencies where SFT and RL both fail to
generalize. Using a synthetic task with latent parity constraints, we show that
our adaptive curriculum over partial answers reliably solves problems that are
otherwise intractable. On mathematical reasoning benchmarks (MATH, GSM8k), we
find that curriculum learning enables models to solve problems that RL alone
cannot, acquiring new reasoning capabilities through incremental exposure to
partial solutions.

</details>


### [353] [Routing Mamba: Scaling State Space Models with Mixture-of-Experts Projection](https://arxiv.org/abs/2506.18145)
*Zheng Zhan,Liliang Ren,Shuohang Wang,Liyuan Liu,Yang Liu,Yeyun Gong,Yanzhi Wang,Yelong Shen*

Main category: cs.LG

TL;DR: The paper introduces Routing Mamba (RoM), a scalable variant of Linear State Space Models (SSMs), which uses sparse mixtures of experts for more efficient performance.


<details>
  <summary>Details</summary>
Motivation: Scaling the expressive power of SSMs using Mixture of Experts (MoE) has been challenging, and naive attempts often degrade performance. The study aims to address this limitation.

Method: RoM scales SSM parameters by utilizing sparse mixtures of linear projection experts. It employs shared routing decisions between projection layers and lightweight sub-modules within Mamba for efficient sparse scaling.

Result: RoM matches the performance of dense Mamba models with significantly fewer active parameters, demonstrating a 23% FLOPS saving for comparable performance in hybrid language models.

Conclusion: RoM successfully scales SSMs for long sequence modeling, offering computational and memory efficiency without sacrificing accuracy, making it a strong alternative to dense models.

Abstract: Linear State Space Models (SSMs) offer remarkable performance gains in
efficient sequence modeling, with constant inference-time computation and
memory complexity. Recent advances, such as Mamba, further enhance SSMs with
input-dependent gating and hardware-aware implementations, positioning them as
strong alternatives to Transformers for long sequence modeling. However,
efficiently scaling the expressive power of SSMs, particularly with Mixture of
Experts (MoE), remains challenging, as naive integration attempts often falter
or degrade performance. In this work, we introduce Routing Mamba (RoM), a novel
approach that scales SSM parameters using sparse mixtures of linear projection
experts. By sharing routing decisions between projection layers and lightweight
sub-modules within Mamba across experts, RoM leverages synergies among linear
projection experts for effective and efficient sparse scaling of Mamba layers.
At a scale of 1.3B active parameters (10B total) and 16K training sequence
length, RoM achieves language modeling performance equivalent to a dense Mamba
model requiring over 2.3x more active parameters, and demonstrates consistent
perplexity across context lengths. Experimental results further show RoM
effectively scales hybrid language models, yielding a 23% FLOPS saving compared
to dense Mamba scaling for similar performance.

</details>


### [354] [Probabilistic and reinforced mining of association rules](https://arxiv.org/abs/2506.18155)
*Yongchao Huang*

Main category: cs.LG

TL;DR: The paper introduces four novel probabilistic and reinforcement-driven methods for association rule mining (ARM), offering more robust, flexible, and uncertainty-aware frameworks compared to traditional frequency-based algorithms.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for ARM like Apriori and FP-Growth are frequency-driven and static, lacking capabilities to incorporate prior knowledge, probabilistic inference, and adaptive strategies, which limits their applicability in complex or sensitive domains.

Method: Four novel methods are introduced: Gaussian processes (GPAR) for item co-occurrence modeling, Bayesian framework (BARM) for uncertainty quantification, multi-armed bandit with UCB strategy (MAB-ARM), and deep Q-network (RLAR) for extracting high-quality rules.

Result: The methods demonstrate effectiveness on synthetic and real-world datasets for discovering rare or complex patterns, showing improvements in flexibility and adaptability, but reveal trade-offs in computational complexity and interpretability.

Conclusion: These innovations redefine ARM by transitioning to probabilistic, uncertainty-aware, and scalable frameworks, suitable for diverse application domains like retail, finance, and medical diagnostics.

Abstract: This work introduces 4 novel probabilistic and reinforcement-driven methods
for association rule mining (ARM): Gaussian process-based association rule
mining (GPAR), Bayesian ARM (BARM), multi-armed bandit based ARM (MAB-ARM), and
reinforcement learning based association rule mining (RLAR). These methods
depart fundamentally from traditional frequency-based algorithms such as
Apriori, FP-Growth, and Eclat, offering enhanced capabilities for incorporating
prior knowledge, modeling uncertainty, item dependencies, probabilistic
inference and adaptive search strategies. GPAR employs Gaussian processes to
model item co-occurrence via feature representations, enabling principled
inference, uncertainty quantification, and efficient generalization to unseen
itemsets without retraining. BARM adopts a Bayesian framework with priors and
optional correlation structures, yielding robust uncertainty quantification
through full posterior distributions over item presence probabilities. MAB-ARM,
including its Monte Carlo tree search (MCTS) companion, utilizes an upper
confidence bound (UCB) strategy for efficient and adaptive exploration of the
itemset space, while RLAR applies a deep Q-network (DQN) to learn a
generalizable policy for identifying high-quality rules. Collectively, these
approaches improve the flexibility and robustness of ARM, particularly for
discovering rare or complex patterns and operating on small datasets. Empirical
results on synthetic and real-world datasets demonstrate their effectiveness,
while also highlighting trade-offs in computational complexity and
interpretability. These innovations mark a significant shift from static,
frequency-driven paradigms, offering some prior and dependency-informed,
uncertainty-aware or scalable ARM frameworks for diverse application domains
such as retail, geography, finance, medical diagnostics, and risk-sensitive
scenarios.

</details>


### [355] [Pitfalls of Conformal Predictions for Medical Image Classification](https://arxiv.org/abs/2506.18162)
*Hendrik Mehrtens,Tabea Bucher,Titus J. Brinker*

Main category: cs.LG

TL;DR: The paper explores the roles and limitations of conformal predictions in medical classification tasks, particularly under distributional shifts and scenarios with fewer classes.


<details>
  <summary>Details</summary>
Motivation: To address issues surrounding uncertainty estimation in medical classification tasks, specifically through the lens of conformal predictions.

Method: The study employs applied examples from dermatology and histopathology to investigate the reliability of conformal predictions under various conditions.

Result: Conformal predictions are found to be unreliable under distributional shifts and for subsets of data. They also have limited use in classification with a small number of classes, as common in medical imaging.

Conclusion: While conformal predictions offer calibrated guarantees, their limitations make them unsuitable for safety-critical applications without careful consideration of assumptions and pitfalls.

Abstract: Reliable uncertainty estimation is one of the major challenges for medical
classification tasks. While many approaches have been proposed, recently the
statistical framework of conformal predictions has gained a lot of attention,
due to its ability to provide provable calibration guarantees. Nonetheless, the
application of conformal predictions in safety-critical areas such as medicine
comes with pitfalls, limitations and assumptions that practitioners need to be
aware of. We demonstrate through examples from dermatology and histopathology
that conformal predictions are unreliable under distributional shifts in input
and label variables. Additionally, conformal predictions should not be used for
selecting predictions to improve accuracy and are not reliable for subsets of
the data, such as individual classes or patient attributes. Moreover, in
classification settings with a small number of classes, which are common in
medical image classification tasks, conformal predictions have limited
practical value.

</details>


### [356] [Non-equilibrium Annealed Adjoint Sampler](https://arxiv.org/abs/2506.18165)
*Jaemoo Choi,Yongxin Chen,Molei Tao,Guan-Horng Liu*

Main category: cs.LG

TL;DR: NAAS is a novel sampling method that avoids importance sampling's pitfalls and is effective across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Current diffusion sampling methods like SOC or annealed path measures lack scalability or suffer from high variance due to reliance on importance sampling.

Method: NAAS uses a new Stochastic Optimal Control framework with annealed reference dynamics, leveraging adjoint matching for efficient and scalable training.

Result: NAAS is effective in tasks such as sampling from energy landscapes and molecular Boltzmann distributions, demonstrating superior performance.

Conclusion: NAAS combines the strengths of SOC and annealing approaches while maintaining scalability and efficiency, addressing previous methods' shortcomings.

Abstract: Recently, there has been significant progress in learning-based diffusion
samplers, which aim to sample from a given unnormalized density. These methods
typically follow one of two paradigms: (i) formulating sampling as an unbiased
stochastic optimal control (SOC) problem using a canonical reference process,
or (ii) refining annealed path measures through importance-weighted sampling.
Although annealing approaches have advantages in guiding samples toward
high-density regions, reliance on importance sampling leads to high variance
and limited scalability in practice. In this paper, we introduce the
\textbf{Non-equilibrium Annealed Adjoint Sampler (NAAS)}, a novel SOC-based
diffusion sampler that leverages annealed reference dynamics without resorting
to importance sampling. NAAS employs a lean adjoint system inspired by adjoint
matching, enabling efficient and scalable training. We demonstrate the
effectiveness of our approach across a range of tasks, including sampling from
classical energy landscapes and molecular Boltzmann distribution.

</details>


### [357] [Understanding Reasoning in Thinking Language Models via Steering Vectors](https://arxiv.org/abs/2506.18167)
*Constantin Venhoff,Iván Arcuschin,Philip Torr,Arthur Conmy,Neel Nanda*

Main category: cs.LG

TL;DR: The paper introduces methods to steer reasoning behaviors in thinking LLMs by analyzing and controlling specific behaviors through steering vectors.


<details>
  <summary>Details</summary>
Motivation: Controlling reasoning chains in thinking LLMs remains challenging despite their improved performance.

Method: The authors identify and manipulate reasoning behaviors in DeepSeek-R1-Distill models using linear directions in activation space to create steering vectors.

Result: Their method successfully modulates behaviors like uncertainty, hypothesis validation, and backtracking across 500 tasks in 10 categories.

Conclusion: This approach enables controlled and interpretable steering of LLM reasoning processes, verified in multiple model architectures.

Abstract: Recent advances in large language models (LLMs) have led to the development
of thinking language models that generate extensive internal reasoning chains
before producing responses. While these models achieve improved performance,
controlling their reasoning processes remains challenging. This work presents a
steering approach for thinking LLMs by analyzing and manipulating specific
reasoning behaviors in DeepSeek-R1-Distill models. Through a systematic
experiment on 500 tasks across 10 diverse categories, we identify several
reasoning behaviors exhibited by thinking models, including expressing
uncertainty, generating examples for hypothesis validation, and backtracking in
reasoning chains. We demonstrate that these behaviors are mediated by linear
directions in the model's activation space and can be controlled using steering
vectors. By extracting and applying these vectors, we provide a method to
modulate specific aspects of the model's reasoning process, such as its
tendency to backtrack or express uncertainty. Our approach offers practical
tools for steering reasoning processes in thinking models in a controlled and
interpretable manner. We validate our steering method using two
DeepSeek-R1-Distill models, demonstrating consistent control across different
model architectures.

</details>


### [358] [Memba: Membrane-driven Parameter-Efficient Fine-Tuning for Mamba](https://arxiv.org/abs/2506.18184)
*Donghyun Lee,Yuhang Li,Ruokai Yin,Shiting Xiao,Priyadarshini Panda*

Main category: cs.LG

TL;DR: Memba introduces a bio-inspired PEFT method specifically for Mamba, leveraging LIM neurons for improved temporal modeling.


<details>
  <summary>Details</summary>
Motivation: PEFT methods are necessary to efficiently adapt large pre-trained SSMs like Mamba to specific tasks without excessive computational costs.

Method: Memba leverages LIM neurons, Low-Rank Adaptations, and cross-layer membrane transfer for tailored fine-tuning of Mamba.

Result: Memba achieves significant improvements in downstream tasks over previous PEFT methods, validated by experiments in language and vision.

Conclusion: Memba enhances Mamba's capability for temporal modeling, proving its effectiveness and efficiency in PEFT tasks.

Abstract: State Space Models (SSMs) have emerged as powerful alternatives to
attention-based Transformers, with Mamba demonstrating impressive efficiency
and scalability. As these models grow increasingly larger, the need for
Parameter-Efficient Fine-Tuning (PEFT) methods becomes critical to adapt
pre-trained Mamba to downstream tasks without prohibitive computational costs.
However, previous approaches simply apply traditional Transformer-tailored PEFT
methods without addressing the unique temporal processing dynamics of SSMs. To
address this limitation, we propose Memba, a membrane-driven PEFT approach
specifically designed for Mamba. Memba introduces Leaky Integrate Membrane
(LIM) neurons as bio-inspired gating mechanisms that naturally accumulate
membrane potentials over time, enhancing selective information retention. By
strategically combining LIM neurons with Low-Rank Adaptations (LoRA) and
cross-layer membrane transfer, our approach significantly improves Mamba's
temporal modeling capabilities. Extensive experiments across language and
vision tasks demonstrate that Memba achieves substantial improvements over
existing PEFT methods. The code is available at
https://github.com/Intelligent-Computing-Lab-Yale/Memba.

</details>


### [359] [Joint Embedding Predictive Architecture for self-supervised pretraining on polymer molecular graphs](https://arxiv.org/abs/2506.18194)
*Francesco Picolli,Gabriel Vogel,Jana M. Weber*

Main category: cs.LG

TL;DR: Self-supervised learning via the Joint Embedding Predictive Architecture improves predictions in polymer property discovery, especially when labeled data is scarce.


<details>
  <summary>Details</summary>
Motivation: The scarcity of high-quality labeled datasets for polymers limits the effectiveness of supervised machine learning models in discovering polymers with desired properties.

Method: The proposed method uses the Joint Embedding Predictive Architecture (JEPA), a self-supervised learning technique, on polymer molecular graphs as a pretraining step.

Result: Results show that JEPA-based pretraining enhances downstream tasks and yields performance improvements across all tested datasets when labeled data is limited.

Conclusion: Using JEPA for self-supervised learning is a promising approach to address data scarcity in polymer discovery through improved ML model performance.

Abstract: Recent advances in machine learning (ML) have shown promise in accelerating
the discovery of polymers with desired properties by aiding in tasks such as
virtual screening via property prediction. However, progress in polymer ML is
hampered by the scarcity of high-quality labeled datasets, which are necessary
for training supervised ML models. In this work, we study the use of the very
recent 'Joint Embedding Predictive Architecture' (JEPA), a type of architecture
for self-supervised learning (SSL), on polymer molecular graphs to understand
whether pretraining with the proposed SSL strategy improves downstream
performance when labeled data is scarce. Our results indicate that JEPA-based
self-supervised pretraining on polymer graphs enhances downstream performance,
particularly when labeled data is very scarce, achieving improvements across
all tested datasets.

</details>


### [360] [These are Not All the Features You are Looking For: A Fundamental Bottleneck In Supervised Pretraining](https://arxiv.org/abs/2506.18221)
*Xingyu Alice Yang,Jianyu Zhang,Léon Bottou*

Main category: cs.LG

TL;DR: The paper investigates challenges in transfer learning, specifically how pre-trained models often fail to effectively adapt to unseen datasets due to 'information saturation bottleneck.' It proposes richer feature representations as a solution to improve generalization.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of ensuring that features learned from pretraining can adapt adequately to unseen datasets and tasks, overcoming the issue of differing task 'relatedness.'

Method: The paper evaluates model transfer by comparing pretraining features with task-specific direct training, identifying key limitations such as feature loss during pretraining.

Result: The study reveals the 'information saturation bottleneck,' where deep learning models permanently lose critical features during pretraining, leading to inconsistent performance across tasks.

Conclusion: Large-scale networks are insufficient for some transfer learning scenarios, suggesting that task-specific training or richer feature representations are a critical direction for improving model adaptability.

Abstract: Transfer learning is a cornerstone of modern machine learning, promising a
way to adapt models pretrained on a broad mix of data to new tasks with minimal
new data. However, a significant challenge remains in ensuring that transferred
features are sufficient to handle unseen datasets, amplified by the difficulty
of quantifying whether two tasks are "related". To address these challenges, we
evaluate model transfer from a pretraining mixture to each of its component
tasks, assessing whether pretrained features can match the performance of
task-specific direct training. We identify a fundamental limitation in deep
learning models -- an "information saturation bottleneck" -- where networks
fail to learn new features once they encode similar competing features during
training. When restricted to learning only a subset of key features during
pretraining, models will permanently lose critical features for transfer and
perform inconsistently on data distributions, even components of the training
mixture. Empirical evidence from published studies suggests that this
phenomenon is pervasive in deep learning architectures -- factors such as data
distribution or ordering affect the features that current representation
learning methods can learn over time. This study suggests that relying solely
on large-scale networks may not be as effective as focusing on task-specific
training, when available. We propose richer feature representations as a
potential solution to better generalize across new datasets and, specifically,
present existing methods alongside a novel approach, the initial steps towards
addressing this challenge.

</details>


### [361] [AdapThink: Adaptive Thinking Preferences for Reasoning Language Model](https://arxiv.org/abs/2506.18237)
*Xu Wan,Wei Wang,Wenyue Xu,Wotao Yin,Jie Song,Mingyang Sun*

Main category: cs.LG

TL;DR: AdapThink is proposed to improve the efficiency of RL-based post-training for reasoning in language models, addressing inefficiencies in simple and complex questions.


<details>
  <summary>Details</summary>
Motivation: Current "slow thinking" RL post-training for language models can be inefficient, using excessive computation on simple tasks or switching prematurely for complex ones. Existing solutions lack adaptability.

Method: AdapThink uses two mechanisms: a dynamic group-relative reward function based on model confidence and a diversity-aware sampling mechanism to balance solution accuracy and reasoning diversity.

Result: Experiments on mathematical reasoning datasets show AdapThink improves adaptive reasoning patterns and reduces inefficiencies in reasoning language models.

Conclusion: AdapThink enables more efficient reasoning in language models without sacrificing performance, demonstrating adaptability to varying task complexities.

Abstract: Reinforcement Learning (RL)-based post-training has significantly advanced
the complex reasoning capabilities of language models, fostering sophisticated
self-reflection processes. However, this ``slow thinking'' paradigm presents a
critical challenge to reasoning efficiency: models may expend excessive
computation on simple questions and shift reasoning prematurely for complex
ones. Previous mechanisms typically rely on static length budgets or predefined
rules, lacking the adaptability for varying question complexities and models'
evolving capabilities. To this end, we propose AdapThink, an adaptive
post-training framework designed to induce more efficient thinking while
maintaining the performance of reasoning language models. Specifically,
AdapThink incorporates two key mechanisms: 1) A group-relative reward function
that leverages model confidence and response's characteristic to dynamically
adjust the preference of reflection-related transition words without resorting
to a fixed length preference. 2) A diversity-aware sampling mechanism that
balances the training group's solution accuracy with reasoning diversity via an
entropy-guided score. Experiments on several mathematical reasoning datasets
with DeepSeek-distilled models demonstrate AdapThink's advantages in enabling
adaptive reasoning patterns and mitigating the inefficiencies.

</details>


### [362] [Quantum-Classical Hybrid Quantized Neural Network](https://arxiv.org/abs/2506.18240)
*Wenxin Li,Chuan Wang,Hongdong Zhu,Qi Gao,Yin Ma,Hai Wei,Kai Wen*

Main category: cs.LG

TL;DR: This paper introduces a novel Quadratic Binary Optimization model for training quantized neural networks using quantum computing and spline interpolation, achieving high efficiency with limited precision.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle challenges in training quantized neural networks, such as handling arbitrary activation and loss functions and addressing the computational difficulties of solving large-scale Quadratic Constrained Binary Optimization problems.

Method: The authors propose Forward Interval Propagation to discretize activation functions, derive theoretical bounds on the optimization process, and employ the Quantum Conditional Gradient Descent algorithm to solve the QCBO problem efficiently.

Result: The approach achieves 94.95% accuracy on the Fashion MNIST task using a coherent Ising machine, with only 1.1-bit precision.

Conclusion: The combination of theoretical advancements and quantum algorithms broadens the applicability of neural network training and supports efficient optimization, showcasing significant promise for quantum artificial intelligence.

Abstract: Here in this work, we present a novel Quadratic Binary Optimization (QBO)
model for quantized neural network training, enabling the use of arbitrary
activation and loss functions through spline interpolation. We introduce
Forward Interval Propagation (FIP), a method designed to tackle the challenges
of non-linearity and the multi-layer composite structure in neural networks by
discretizing activation functions into linear subintervals. This approach
preserves the universal approximation properties of neural networks while
allowing complex nonlinear functions to be optimized using quantum computers,
thus broadening their applicability in artificial intelligence. We provide
theoretical upper bounds on the approximation error and the number of Ising
spins required, by deriving the sample complexity of the empirical risk
minimization problem, from an optimization perspective. A significant challenge
in solving the associated Quadratic Constrained Binary Optimization (QCBO)
model on a large scale is the presence of numerous constraints. When employing
the penalty method to handle these constraints, tuning a large number of
penalty coefficients becomes a critical hyperparameter optimization problem,
increasing computational complexity and potentially affecting solution quality.
To address this, we employ the Quantum Conditional Gradient Descent (QCGD)
algorithm, which leverages quantum computing to directly solve the QCBO
problem. We prove the convergence of QCGD under a quantum oracle with
randomness and bounded variance in objective value, as well as under limited
precision constraints in the coefficient matrix. Additionally, we provide an
upper bound on the Time-To-Solution for the QCBO solving process. Experimental
results using a coherent Ising machine (CIM) demonstrate a 94.95% accuracy on
the Fashion MNIST classification task, with only 1.1-bit precision.

</details>


### [363] [Dual-Forward Path Teacher Knowledge Distillation: Bridging the Capacity Gap Between Teacher and Student](https://arxiv.org/abs/2506.18244)
*Tong Li,Long Liu,Yihang Hu,Hu Chen,Shifeng Chen*

Main category: cs.LG

TL;DR: The paper introduces a method called Dual-Forward Path Teacher Knowledge Distillation (DFPT-KD), and its enhanced version DFPT-KD+, to address the capacity gap issue in knowledge distillation and improve student model performance.


<details>
  <summary>Details</summary>
Motivation: The existing knowledge distillation approaches often face the limitation of a capacity gap between teacher and student models, which reduces their effectiveness and restricts the student model's performance.

Method: The paper proposes DFPT-KD, which introduces a dual-forward path within the teacher model using prompt-based tuning. This makes the teacher's knowledge more compatible with the student's capacity. Additionally, DFPT-KD+ fine-tunes the entire prompt-based forward path for further improvements.

Result: Extensive experiments show that both DFPT-KD and DFPT-KD+ outperform vanilla knowledge distillation techniques. DFPT-KD+ achieves state-of-the-art accuracy.

Conclusion: DFPT-KD and its enhanced version DFPT-KD+ address the capacity gap problem effectively, enabling student models to achieve superior performance in comparison to existing methods.

Abstract: Knowledge distillation (KD) provides an effective way to improve the
performance of a student network under the guidance of pre-trained teachers.
However, this approach usually brings in a large capacity gap between teacher
and student networks, limiting the distillation gains. Previous methods
addressing this problem either discard accurate knowledge representation or
fail to dynamically adjust the transferred knowledge, which is less effective
in addressing the capacity gap problem and hinders students from achieving
comparable performance with the pre-trained teacher. In this work, we extend
the ideology of prompt-based learning to address the capacity gap problem, and
propose Dual-Forward Path Teacher Knowledge Distillation (DFPT-KD), which
replaces the pre-trained teacher with a novel dual-forward path teacher to
supervise the learning of student. The key to DFPT-KD is prompt-based tuning,
i.e., establishing an additional prompt-based forward path within the
pre-trained teacher and optimizing it with the pre-trained teacher frozen to
make the transferred knowledge compatible with the representation ability of
the student. Extensive experiments demonstrate that DFPT-KD leads to trained
students performing better than the vanilla KD. To make the transferred
knowledge better compatible with the representation abilities of the student,
we further fine-tune the whole prompt-based forward path, yielding a novel
distillation approach dubbed DFPT-KD+. By extensive experiments, it is shown
that DFPT-KD+ improves upon DFPT-KD and achieves state-of-the-art accuracy
performance.

</details>


### [364] [Exploring Efficient Quantification of Modeling Uncertainties with Differentiable Physics-Informed Machine Learning Architectures](https://arxiv.org/abs/2506.18247)
*Manaswin Oddiraju,Bharath Varma Penumatsa,Divyang Amin,Michael Piedmonte,Souma Chowdhury*

Main category: cs.LG

TL;DR: This paper proposes a Bayesian Neural Network (BNN)-integrated hybrid Physics-Informed Machine Learning (PIML) architecture to improve uncertainty quantification and propagation and evaluates its performance on benchmark data.


<details>
  <summary>Details</summary>
Motivation: Better quantification and propagation of modeling uncertainties are important for tasks like reliability analysis and robust optimization. While PIML offers interpretability and accuracy, its uncertainty prediction capabilities are underexplored.

Method: The study integrates Bayesian Neural Networks (BNNs) into hybrid PIML architectures and employs a two-stage training process to overcome issues with probabilistic ML models. The BNN-enhanced PIML method uses Monte Carlo weight sampling for uncertainty propagation.

Result: The proposed BNN-integrated PIML model achieved similar or slightly worse predictive performance compared to traditional models but excelled at propagating uncertainties effectively using Monte Carlo sampling.

Conclusion: The integration of BNNs into PIML architectures enhances uncertainty propagation capabilities, although it does not significantly improve predictive performance compared to existing PIML or purely data-driven models.

Abstract: Quantifying and propagating modeling uncertainties is crucial for reliability
analysis, robust optimization, and other model-based algorithmic processes in
engineering design and control. Now, physics-informed machine learning (PIML)
methods have emerged in recent years as a new alternative to traditional
computational modeling and surrogate modeling methods, offering a balance
between computing efficiency, modeling accuracy, and interpretability. However,
their ability to predict and propagate modeling uncertainties remains mostly
unexplored. In this paper, a promising class of auto-differentiable hybrid PIML
architectures that combine partial physics and neural networks or ANNs (for
input transformation or adaptive parameter estimation) is integrated with
Bayesian Neural networks (replacing the ANNs); this is done with the goal to
explore whether BNNs can successfully provision uncertainty propagation
capabilities in the PIML architectures as well, further supported by the
auto-differentiability of these architectures. A two-stage training process is
used to alleviate the challenges traditionally encountered in training
probabilistic ML models. The resulting BNN-integrated PIML architecture is
evaluated on an analytical benchmark problem and flight experiments data for a
fixed-wing RC aircraft, with prediction performance observed to be slightly
worse or at par with purely data-driven ML and original PIML models. Moreover,
Monte Carlo sampling of probabilistic BNN weights was found to be most
effective in propagating uncertainty in the BNN-integrated PIML architectures.

</details>


### [365] [RLPR: Extrapolating RLVR to General Domains without Verifiers](https://arxiv.org/abs/2506.18254)
*Tianyu Yu,Bo Ji,Shouli Wang,Shu Yao,Zefan Wang,Ganqu Cui,Lifan Yuan,Ning Ding,Yuan Yao,Zhiyuan Liu,Maosong Sun,Tat-Seng Chua*

Main category: cs.LG

TL;DR: The paper introduces RLPR, a verifier-free reinforcement learning framework to improve reasoning capabilities of LLMs using intrinsic token probabilities as rewards, showing superior performance in general and mathematical domains.


<details>
  <summary>Details</summary>
Motivation: Current state-of-the-art in reinforcement learning for LLMs focuses on domain-specific verifiers, which limits scalability and generalization outside of mathematical and code domains.

Method: The proposed RLPR framework uses intrinsic token probability scores from LLMs as reward signals, addressing noise and variance through methods such as prob-to-reward and stabilizing approaches to ensure precise and reliable rewards during training.

Result: RLPR consistently improves reasoning across general-domain and mathematical benchmarks, achieving superior performance compared to existing verifier-dependent and verifier-free approaches across multiple models.

Conclusion: RLPR expands the application of reinforcement learning for LLM reasoning by eliminating the dependency on verifiers, offering a scalable and effective solution for diverse domains, with robust experimental validations and performance improvements.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising
potential in advancing the reasoning capabilities of LLMs. However, its success
remains largely confined to mathematical and code domains. This primary
limitation stems from the heavy reliance on domain-specific verifiers, which
results in prohibitive complexity and limited scalability. To address the
challenge, our key observation is that LLM's intrinsic probability of
generating a correct free-form answer directly indicates its own evaluation of
the reasoning reward (i.e., how well the reasoning process leads to the correct
answer). Building on this insight, we propose RLPR, a simple verifier-free
framework that extrapolates RLVR to broader general domains. RLPR uses the
LLM's own token probability scores for reference answers as the reward signal
and maximizes the expected reward during training. We find that addressing the
high variance of this noisy probability reward is crucial to make it work, and
propose prob-to-reward and stabilizing methods to ensure a precise and stable
reward from LLM intrinsic probabilities. Comprehensive experiments in four
general-domain benchmarks and three mathematical benchmarks show that RLPR
consistently improves reasoning capabilities in both areas for Gemma, Llama,
and Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6
points on TheoremQA and 7.5 points on Minerva, and even surpasses strong
verifier-model-dependent approaches General-Reasoner by 1.6 average points
across seven benchmarks.

</details>


### [366] [Ground tracking for improved landmine detection in a GPR system](https://arxiv.org/abs/2506.18258)
*Li Tang,Peter A. Torrione,Cihat Eldeniz,Leslie M. Collins*

Main category: cs.LG

TL;DR: This paper addresses ground bounce (GB) interference in GPR data, proposing Kalman and particle filter-based tracking algorithms to enhance landmine detection.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the need to improve landmine detection using GPR technology by mitigating ground bounce (GB) interference, which reduces detection accuracy.

Method: The method involves proposing Kalman and particle filter-based GB tracking algorithms where GB location is modeled as a hidden state within a stochastic system. Observations are based on 2D radar images, with parameters adapted through a training stage and data updates.

Result: Proposed algorithms were experimentally validated using real data, outperforming existing GB tracking approaches and enhancing landmine detection.

Conclusion: Effective GB tracking directly improves landmine detection performance, as demonstrated through experimental results.

Abstract: Ground penetrating radar (GPR) provides a promising technology for accurate
subsurface object detection. In particular, it has shown promise for detecting
landmines with low metal content. However, the ground bounce (GB) that is
present in GPR data, which is caused by the dielectric discontinuity between
soil and air, is a major source of interference and degrades landmine detection
performance. To mitigate this interference, GB tracking algorithms formulated
using both a Kalman filter (KF) and a particle filter (PF) framework are
proposed. In particular, the location of the GB in the radar signal is modeled
as the hidden state in a stochastic system for the PF approach. The
observations are the 2D radar images, which arrive scan by scan along the
down-track direction. An initial training stage sets parameters automatically
to accommodate different ground and weather conditions. The features associated
with the GB description are updated adaptively with the arrival of new data.
The prior distribution for a given location is predicted by propagating
information from two adjacent channels/scans, which ensures that the overall GB
surface remains smooth. The proposed algorithms are verified in experiments
utilizing real data, and their performances are compared with other GB tracking
approaches. We demonstrate that improved GB tracking contributes to improved
performance for the landmine detection problem.

</details>


### [367] [ARD-LoRA: Dynamic Rank Allocation for Parameter-Efficient Fine-Tuning of Foundation Models with Heterogeneous Adaptation Needs](https://arxiv.org/abs/2506.18267)
*Haseeb Ullah Khan Shinwari,Muhammad Usama*

Main category: cs.LG

TL;DR: The paper introduces ARD-LoRA, an adaptive rank allocation method that dynamically adjusts ranks per layer and head in transformer models, optimizing both performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current LoRA methods use fixed rank, leading to inefficiencies due to heterogeneous learning dynamics across model layers and attention heads.

Method: The ARD-LoRA framework employs learnable scaling factors optimized through a meta-objective, incorporating sparsity and stability regularizations for adaptive rank determination.

Result: ARD-LoRA achieves near full fine-tuning performance (99.3%) with only 0.32% trainable parameters, surpassing existing methods like DoRA and AdaLoRA. Additionally, it reduces multimodal adaptation memory by 41%.

Conclusion: Adaptive rank allocation via ARD-LoRA proves essential for efficient foundation model adaptation, offering significant performance and memory advantages.

Abstract: Conventional Low-Rank Adaptation (LoRA) methods employ a fixed rank, imposing
uniform adaptation across transformer layers and attention heads despite their
heterogeneous learning dynamics. This paper introduces Adaptive Rank Dynamic
LoRA (ARD-LoRA), a novel framework that automates rank allocation through
learnable scaling factors. These factors are optimized via a meta-objective
balancing task performance and parameter efficiency, incorporating $\ell_1$
sparsity for minimal rank and Total Variation regularization for stable rank
transitions. ARD-LoRA enables continuous, differentiable, per-head rank
adaptation. Experiments on LLAMA-3.1-70B and PaliGemma-2 demonstrate ARD-LoRA's
efficacy, achieving up to 99.3% of full fine-tuning performance with only 0.32%
trainable parameters, outperforming strong baselines like DoRA and AdaLoRA.
Furthermore, it reduces multimodal adaptation memory by 41%. These results
establish dynamic, fine-grained rank allocation as a critical paradigm for
efficient foundation model adaptation.

</details>


### [368] [Memory-Augmented Architecture for Long-Term Context Handling in Large Language Models](https://arxiv.org/abs/2506.18271)
*Haseeb Ullah Khan Shinwari,Muhammad Usama*

Main category: cs.LG

TL;DR: This paper addresses the issue of limited contextual memory in large language models by proposing a memory-augmented architecture for effective long-term context handling.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of large language models in maintaining coherent and relevant interactions over extended dialogues due to constrained contextual memory.

Method: The method involves a memory-augmented architecture that dynamically retrieves, updates, and prunes relevant information from past interactions to handle long-term contexts effectively.

Result: The experimental results show significant improvements in contextual coherence, reduced memory overhead, and enhanced response quality for long dialogues.

Conclusion: The proposed solution demonstrates strong potential for real-time applications in interactive systems by addressing critical aspects of context handling.

Abstract: Large Language Models face significant challenges in maintaining coherent
interactions over extended dialogues due to their limited contextual memory.
This limitation often leads to fragmented exchanges and reduced relevance in
responses, diminishing user experience. To address these issues, we propose a
memory-augmented architecture that dynamically retrieves, updates, and prunes
relevant information from past interactions, ensuring effective long-term
context handling. Experimental results demonstrate that our solution
significantly improves contextual coherence, reduces memory overhead, and
enhances response quality, showcasing its potential for real-time applications
in interactive systems.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [369] [Spiffy: Efficient Implementation of CoLaNET for Raspberry Pi](https://arxiv.org/abs/2506.18306)
*Andrey Derzhavin,Denis Larionov*

Main category: cs.NE

TL;DR: This paper introduces Spiffy, a lightweight Rust-based SNN implementation optimized for common platforms like Raspberry Pi, achieving high accuracy and low latency.


<details>
  <summary>Details</summary>
Motivation: The study aims to enable efficient use of spiking neural networks (SNNs) without dependence on costly, specialized neuromorphic hardware.

Method: The researchers implemented an SNN architecture, CoLaNET, using the Rust programming language and optimized it for standard computing platforms like Raspberry Pi, including benchmarking using MNIST dataset.

Result: Spiffy achieved 92% accuracy on MNIST dataset with very low latency, measured at 0.9 ms per training step and 0.45 ms per inference step.

Conclusion: Spiffy demonstrates that high-performance SNNs can be achieved on standard computing hardware, making advanced neural networks more accessible and cost-effective.

Abstract: This paper presents a lightweight software-based approach for running spiking
neural networks (SNNs) without relying on specialized neuromorphic hardware or
frameworks. Instead, we implement a specific SNN architecture (CoLaNET) in Rust
and optimize it for common computing platforms. As a case study, we demonstrate
our implementation, called Spiffy, on a Raspberry Pi using the MNIST dataset.
Spiffy achieves 92% accuracy with low latency - just 0.9 ms per training step
and 0.45 ms per inference step. The code is open-source.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [370] [Reflective VLM Planning for Dual-Arm Desktop Cleaning: Bridging Open-Vocabulary Perception and Precise Manipulation](https://arxiv.org/abs/2506.17328)
*Yufan Liu,Yi Wu,Gweneth Ge,Haoliang Cheng,Rui Liu*

Main category: cs.RO

TL;DR: The paper introduces a hierarchical framework combining Vision-Language Model (VLM) planning and dual-arm manipulation for desktop cleaning, achieving significant performance improvements in task completion.


<details>
  <summary>Details</summary>
Motivation: Enhancing desktop cleaning systems to handle open-vocabulary recognition and precise manipulation for diverse debris categories.

Method: A hierarchical framework integrates reflective Vision-Language Model (VLM) planning with dual-arm execution using structured scene representation and memory augmentation.

Result: The system achieved 87.2% task completion, improving performance by 28.8% over static VLM and 36.2% over single-arm baselines during simulated evaluations.

Conclusion: Integrating structured memory in desktop cleaning systems ensures robust and generalizable manipulation while retaining real-time control efficiency.

Abstract: Desktop cleaning demands open-vocabulary recognition and precise manipulation
for heterogeneous debris. We propose a hierarchical framework integrating
reflective Vision-Language Model (VLM) planning with dual-arm execution via
structured scene representation. Grounded-SAM2 facilitates open-vocabulary
detection, while a memory-augmented VLM generates, critiques, and revises
manipulation sequences. These sequences are converted into parametric
trajectories for five primitives executed by coordinated Franka arms. Evaluated
in simulated scenarios, our system achieving 87.2% task completion, a 28.8%
improvement over static VLM and 36.2% over single-arm baselines. Structured
memory integration proves crucial for robust, generalizable manipulation while
maintaining real-time control performance.

</details>


### [371] [A workflow for generating synthetic LiDAR datasets in simulation environments](https://arxiv.org/abs/2506.17378)
*Abhishek Phadke,Shakib Mahmud Dipto,Pratip Rana*

Main category: cs.RO

TL;DR: This paper introduces a workflow for creating synthetic LiDAR datasets using the CoppeliaSim simulation environment to aid autonomous vehicle perception, robotics research, and sensor security analysis.


<details>
  <summary>Details</summary>
Motivation: To address the need for high-fidelity, reproducible synthetic LiDAR datasets for evaluating autonomous system perception and reinforcing sensor security.

Method: The paper employs the CoppeliaSim and its Python API to simulate a vehicle equipped with LiDAR and image sensors in urban scenarios. The workflow automates the entire process, generating synchronized multimodal datasets with ground truth information.

Result: The simulation produces synchronized point clouds along with RGB and depth imagery. The paper also explores security vulnerabilities in LiDAR data and evaluates their defense strategies using these synthetic datasets.

Conclusion: The proposed workflow is a versatile and reproducible solution for synthetic LiDAR dataset generation, though limitations like environmental realism and computational scalability remain. Future directions include adding weather effects and advanced terrain models.

Abstract: This paper presents a simulation workflow for generating synthetic LiDAR
datasets to support autonomous vehicle perception, robotics research, and
sensor security analysis. Leveraging the CoppeliaSim simulation environment and
its Python API, we integrate time-of-flight LiDAR, image sensors, and two
dimensional scanners onto a simulated vehicle platform operating within an
urban scenario. The workflow automates data capture, storage, and annotation
across multiple formats (PCD, PLY, CSV), producing synchronized multimodal
datasets with ground truth pose information. We validate the pipeline by
generating large-scale point clouds and corresponding RGB and depth imagery.
The study examines potential security vulnerabilities in LiDAR data, such as
adversarial point injection and spoofing attacks, and demonstrates how
synthetic datasets can facilitate the evaluation of defense strategies.
Finally, limitations related to environmental realism, sensor noise modeling,
and computational scalability are discussed, and future research directions,
such as incorporating weather effects, real-world terrain models, and advanced
scanner configurations, are proposed. The workflow provides a versatile,
reproducible framework for generating high-fidelity synthetic LiDAR datasets to
advance perception research and strengthen sensor security in autonomous
systems. Documentation and examples accompany this framework; samples of
animated cloud returns and image sensor data can be found at this Link.

</details>


### [372] [Kinematic Model Optimization via Differentiable Contact Manifold for In-Space Manipulation](https://arxiv.org/abs/2506.17458)
*Abhay Negi,Omey M. Manyar,Satyandra K. Gupta*

Main category: cs.RO

TL;DR: The paper introduces a method for estimating kinematic parameters, enabling precise robotic manipulation in space without relying on external sensors or calibration tools.


<details>
  <summary>Details</summary>
Motivation: To address challenges in space-based robotic tasks like debris removal and assembly, including thermal-induced manipulator deformations and encoder inaccuracies, which impact accuracy.

Method: Develops a learning-based model to understand contact manifolds and combines it with an optimization approach using encoder data and binary contact detection.

Result: Achieves robust and efficient kinematic parameter estimation suitable for contact-rich space scenarios under uncertainty.

Conclusion: The methodology improves robotic manipulation accuracy and safety in space by eliminating the need for external sensors or dedicated calibration.

Abstract: Robotic manipulation in space is essential for emerging applications such as
debris removal and in-space servicing, assembly, and manufacturing (ISAM). A
key requirement for these tasks is the ability to perform precise, contact-rich
manipulation under significant uncertainty. In particular, thermal-induced
deformation of manipulator links and temperature-dependent encoder bias
introduce kinematic parameter errors that significantly degrade end-effector
accuracy. Traditional calibration techniques rely on external sensors or
dedicated calibration procedures, which can be infeasible or risky in dynamic,
space-based operational scenarios.
  This paper proposes a novel method for kinematic parameter estimation that
only requires encoder measurements and binary contact detection. The approach
focuses on estimating link thermal deformation strain and joint encoder biases
by leveraging information of the contact manifold - the set of relative SE(3)
poses at which contact between the manipulator and environment occurs. We
present two core contributions: (1) a differentiable, learning-based model of
the contact manifold, and (2) an optimization-based algorithm for estimating
kinematic parameters from encoder measurements at contact instances. By
enabling parameter estimation using only encoder measurements and contact
detection, this method provides a robust, interpretable, and data-efficient
solution for safe and accurate manipulation in the challenging conditions of
space.

</details>


### [373] [General-Purpose Robotic Navigation via LVLM-Orchestrated Perception, Reasoning, and Acting](https://arxiv.org/abs/2506.17462)
*Bernard Lange,Anil Yildiz,Mansur Arief,Shehryar Khattak,Mykel Kochenderfer,Georgios Georgakis*

Main category: cs.RO

TL;DR: ARNA is a flexible robotic navigation framework using Large Vision-Language Models (LVLMs) to autonomously explore and reason in unknown environments, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of current robotic navigation systems that rely on task-specific neural networks and fixed data flows, preventing generalization in unknown environments.

Method: ARNA integrates LVLMs into robotic systems, allowing the agent to autonomously create and execute workflows by leveraging perception, reasoning, and navigation tools in modern robotic stacks.

Result: ARNA achieved state-of-the-art performance when evaluated on the HM-EQA benchmark within Habitat Lab, showing improved exploration, navigation, and question answering capabilities.

Conclusion: ARNA provides a scalable and effective framework for robotic navigation in unmapped environments, without relying on handcrafted representations or pre-existing maps.

Abstract: Developing general-purpose navigation policies for unknown environments
remains a core challenge in robotics. Most existing systems rely on
task-specific neural networks and fixed data flows, limiting generalizability.
Large Vision-Language Models (LVLMs) offer a promising alternative by embedding
human-like knowledge suitable for reasoning and planning. Yet, prior LVLM-robot
integrations typically depend on pre-mapped spaces, hard-coded representations,
and myopic exploration. We introduce the Agentic Robotic Navigation
Architecture (ARNA), a general-purpose navigation framework that equips an
LVLM-based agent with a library of perception, reasoning, and navigation tools
available within modern robotic stacks. At runtime, the agent autonomously
defines and executes task-specific workflows that iteratively query the robotic
modules, reason over multimodal inputs, and select appropriate navigation
actions. This approach enables robust navigation and reasoning in previously
unmapped environments, providing a new perspective on robotic stack design.
Evaluated in Habitat Lab on the HM-EQA benchmark, ARNA achieves
state-of-the-art performance, demonstrating effective exploration, navigation,
and embodied question answering without relying on handcrafted plans, fixed
input representations, or pre-existing maps.

</details>


### [374] [DiLQR: Differentiable Iterative Linear Quadratic Regulator via Implicit Differentiation](https://arxiv.org/abs/2506.17473)
*Shuyuan Wang,Philip D. Loewen,Michael Forbes,Bhushan Gopaluni,Wei Pan*

Main category: cs.RO

TL;DR: DiLQR is a novel framework enabling efficient differentiation through iLQR, overcoming scalability challenges and offering notable speed and learning performance improvements.


<details>
  <summary>Details</summary>
Motivation: Differentiable control is promising but underexplored in the context of iLQR, which faces scalability and efficiency challenges.

Method: DiLQR utilizes implicit differentiation for an analytical gradient solution, reducing backward computational costs while maintaining accuracy.

Result: DiLQR achieves up to 128x computational speedup, significantly improved learning performance, and successful integration into high-dimensional tasks.

Conclusion: The framework demonstrates the viability of iLQR as an effective differentiable controller for complex tasks, with strong computational and learning gains.

Abstract: While differentiable control has emerged as a powerful paradigm combining
model-free flexibility with model-based efficiency, the iterative Linear
Quadratic Regulator (iLQR) remains underexplored as a differentiable component.
The scalability of differentiating through extended iterations and horizons
poses significant challenges, hindering iLQR from being an effective
differentiable controller. This paper introduces DiLQR, a framework that
facilitates differentiation through iLQR, allowing it to serve as a trainable
and differentiable module, either as or within a neural network. A novel aspect
of this framework is the analytical solution that it provides for the gradient
of an iLQR controller through implicit differentiation, which ensures a
constant backward cost regardless of iteration, while producing an accurate
gradient. We evaluate our framework on imitation tasks on famous control
benchmarks. Our analytical method demonstrates superior computational
performance, achieving up to 128x speedup and a minimum of 21x speedup compared
to automatic differentiation. Our method also demonstrates superior learning
performance ($10^6$x) compared to traditional neural network policies and
better model loss with differentiable controllers that lack exact analytical
gradients. Furthermore, we integrate our module into a larger network with
visual inputs to demonstrate the capacity of our method for high-dimensional,
fully end-to-end tasks. Codes can be found on the project homepage
https://sites.google.com/view/dilqr/.

</details>


### [375] [Distilling On-device Language Models for Robot Planning with Minimal Human Intervention](https://arxiv.org/abs/2506.17486)
*Zachary Ravichandran,Ignacio Hounie,Fernando Cladera,Alejandro Ribeiro,George J. Pappas,Vijay Kumar*

Main category: cs.RO

TL;DR: An efficient framework, PRISM, enables on-device performance of robots by distilling small language model planners, achieving over 93% of GPT-4o's performance using synthetic data in diverse environments.


<details>
  <summary>Details</summary>
Motivation: Current LLM-enabled robots rely heavily on cloud-hosted models, which limits their effectiveness in settings with unreliable communication infrastructure.

Method: PRISM automatically generates synthetic datasets by eliciting plans from existing LLMs and distills small language models to act as replacements for the original models.

Result: PRISM improves the performance of smaller language models (like Llama-3.2-3B) from 10-20% of GPT-4o’s abilities to over 93%, demonstrating effective generalization across robotic platforms and environments.

Conclusion: PRISM makes it feasible for robots to operate independently of cloud-hosted models with excellent performance, contributing to their usability in diverse applications such as mapping, manipulation, and household tasks.

Abstract: Large language models (LLMs) provide robots with powerful contextual
reasoning abilities and a natural human interface. Yet, current LLM-enabled
robots typically depend on cloud-hosted models, limiting their usability in
environments with unreliable communication infrastructure, such as outdoor or
industrial settings. We present PRISM, a framework for distilling small
language model (SLM)-enabled robot planners that run on-device with minimal
human supervision. Starting from an existing LLM-enabled planner, PRISM
automatically synthesizes diverse tasks and environments, elicits plans from
the LLM, and uses this synthetic dataset to distill a compact SLM as a drop-in
replacement of the source model. We apply PRISM to three LLM-enabled planners
for mapping and exploration, manipulation, and household assistance, and we
demonstrate that PRISM improves the performance of Llama-3.2-3B from 10-20% of
GPT-4o's performance to over 93% - using only synthetic data. We further
demonstrate that the distilled planners generalize across heterogeneous robotic
platforms (ground and aerial) and diverse environments (indoor and outdoor). We
release all software, trained models, and datasets at
https://zacravichandran.github.io/PRISM.

</details>


### [376] [Online Adaptation for Flying Quadrotors in Tight Formations](https://arxiv.org/abs/2506.17488)
*Pei-An Hsieh,Kong Yao Chee,M. Ani Hsieh*

Main category: cs.RO

TL;DR: The paper introduces L1 KNODE-DW MPC, a control framework enabling quadrotors to fly in tight formations by adapting to time-varying aerodynamic effects, outperforming other baselines in experiments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of quadrotors flying in tight formations, which face destabilization due to complex and fast-changing aerodynamic wake interactions that are hard to model and predict.

Method: The proposed method, L1 KNODE-DW MPC, is an adaptive and mixed expert learning-based control framework that compensates for unpredictable aerodynamic effects and allows for precise trajectory tracking in formation flights.

Result: The framework was tested on two three-quadrotor formations, demonstrating its ability to outperform several MPC baselines and maintain vertical alignment in close proximity throughout the flight.

Conclusion: The study concludes that the L1 adaptive module, when paired with an accurate dynamics model, effectively handles unmodeled aerodynamic disturbances, offering reliable formation flight control for quadrotors.

Abstract: The task of flying in tight formations is challenging for teams of quadrotors
because the complex aerodynamic wake interactions can destabilize individual
team members as well as the team. Furthermore, these aerodynamic effects are
highly nonlinear and fast-paced, making them difficult to model and predict. To
overcome these challenges, we present L1 KNODE-DW MPC, an adaptive, mixed
expert learning based control framework that allows individual quadrotors to
accurately track trajectories while adapting to time-varying aerodynamic
interactions during formation flights. We evaluate L1 KNODE-DW MPC in two
different three-quadrotor formations and show that it outperforms several MPC
baselines. Our results show that the proposed framework is capable of enabling
the three-quadrotor team to remain vertically aligned in close proximity
throughout the flight. These findings show that the L1 adaptive module
compensates for unmodeled disturbances most effectively when paired with an
accurate dynamics model. A video showcasing our framework and the physical
experiments is available here: https://youtu.be/9QX1Q5Ut9Rs

</details>


### [377] [EASE: Embodied Active Event Perception via Self-Supervised Energy Minimization](https://arxiv.org/abs/2506.17516)
*Zhou Chen,Sanjoy Kundu,Harsimran S. Baweja,Sathyanarayanan N. Aakur*

Main category: cs.RO

TL;DR: EASE is a self-supervised framework for real-time event tracking and summarization, emphasizing adaptability in dynamic environments without predefined rules, annotations, or rewards.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing event perception methods that rely on predefined spaces, datasets, and external rewards, and ensure adaptability for real-world applications like human-AI collaboration.

Method: EASE integrates spatiotemporal representation learning and embodied control based on free energy minimization, using prediction errors and intrinsic entropy signals for event detection and tracking.

Result: The framework showed effective event perception in both simulation and real-world tests, achieving privacy-preserving, scalable capabilities in dynamic and unscripted tasks.

Conclusion: EASE establishes a foundation for robust, adaptable embodied systems suitable for diverse, unscripted tasks without dependency on explicit external inputs.

Abstract: Active event perception, the ability to dynamically detect, track, and
summarize events in real time, is essential for embodied intelligence in tasks
such as human-AI collaboration, assistive robotics, and autonomous navigation.
However, existing approaches often depend on predefined action spaces,
annotated datasets, and extrinsic rewards, limiting their adaptability and
scalability in dynamic, real-world scenarios. Inspired by cognitive theories of
event perception and predictive coding, we propose EASE, a self-supervised
framework that unifies spatiotemporal representation learning and embodied
control through free energy minimization. EASE leverages prediction errors and
entropy as intrinsic signals to segment events, summarize observations, and
actively track salient actors, operating without explicit annotations or
external rewards. By coupling a generative perception model with an
action-driven control policy, EASE dynamically aligns predictions with
observations, enabling emergent behaviors such as implicit memory, target
continuity, and adaptability to novel environments. Extensive evaluations in
simulation and real-world settings demonstrate EASE's ability to achieve
privacy-preserving and scalable event perception, providing a robust foundation
for embodied systems in unscripted, dynamic tasks.

</details>


### [378] [Risk-Guided Diffusion: Toward Deploying Robot Foundation Models in Space, Where Failure Is Not An Option](https://arxiv.org/abs/2506.17601)
*Rohan Thakker,Adarsh Patnaik,Vince Kurtz,Jonas Frey,Jonathan Becktor,Sangwoo Moon,Rob Royce,Marcel Kaufmann,Georgios Georgakis,Pascal Roth,Joel Burdick,Marco Hutter,Shehryar Khattak*

Main category: cs.RO

TL;DR: The paper introduces a risk-guided diffusion framework for safe and adaptive robotic navigation in extreme terrains, integrating learned and physics-based systems.


<details>
  <summary>Details</summary>
Motivation: The need for precise and safe robotic navigation in challenging, unexplored terrains for future space exploration missions.

Method: A two-pronged approach merges a fast, learned (System-1) component with a slow, physics-based (System-2) component to balance adaptability and safety in navigation.

Result: Experiments conducted in a Mars-like setting demonstrated up to fourfold reduction in failure rates without additional training, maintaining goal-reaching efficiency.

Conclusion: The framework improves safety and adaptability in robotic navigation, showcasing potential for reliable autonomous operations in extraterrestrial settings.

Abstract: Safe, reliable navigation in extreme, unfamiliar terrain is required for
future robotic space exploration missions. Recent generative-AI methods learn
semantically aware navigation policies from large, cross-embodiment datasets,
but offer limited safety guarantees. Inspired by human cognitive science, we
propose a risk-guided diffusion framework that fuses a fast, learned "System-1"
with a slow, physics-based "System-2", sharing computation at both training and
inference to couple adaptability with formal safety. Hardware experiments
conducted at the NASA JPL's Mars-analog facility, Mars Yard, show that our
approach reduces failure rates by up to $4\times$ while matching the
goal-reaching performance of learning-based robotic models by leveraging
inference-time compute without any additional training.

</details>


### [379] [Imitation Learning for Active Neck Motion Enabling Robot Manipulation beyond the Field of View](https://arxiv.org/abs/2506.17624)
*Koki Nakagawa,Yoshiyuki Ohmura,Yasuo Kuniyoshi*

Main category: cs.RO

TL;DR: This paper proposes a system and model for deep imitation learning in robots with active neck movement, enhancing task diversity and success rates.


<details>
  <summary>Details</summary>
Motivation: Deep imitation learning often relies on fixed camera perspectives, limiting performance and task variety. The paper aims to expand capabilities to include active neck movements.

Method: A teaching system was created to collect neck-involving datasets while avoiding issues from dynamic viewpoints during teleoperation. A novel network model was designed to process these inputs.

Result: Experiments demonstrated around a 90% task success rate, even with viewpoint distractions from neck motion. Traditional models struggled in peripheral scenarios, but the proposed method excelled.

Conclusion: The approach improves dataset efficiency and extends imitation learning applicability to more complex and dynamic robotic tasks.

Abstract: Most prior research in deep imitation learning has predominantly utilized
fixed cameras for image input, which constrains task performance to the
predefined field of view. However, enabling a robot to actively maneuver its
neck can significantly expand the scope of imitation learning to encompass a
wider variety of tasks and expressive actions such as neck gestures. To
facilitate imitation learning in robots capable of neck movement while
simultaneously performing object manipulation, we propose a teaching system
that systematically collects datasets incorporating neck movements while
minimizing discomfort caused by dynamic viewpoints during teleoperation. In
addition, we present a novel network model for learning manipulation tasks
including active neck motion. Experimental results showed that our model can
achieve a high success rate of around 90\%, regardless of the distraction from
the viewpoint variations by active neck motion. Moreover, the proposed model
proved particularly effective in challenging scenarios, such as when objects
were situated at the periphery or beyond the standard field of view, where
traditional models struggled. The proposed approach contributes to the
efficiency of dataset collection and extends the applicability of imitation
learning to more complex and dynamic scenarios.

</details>


### [380] [RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models](https://arxiv.org/abs/2506.17639)
*Yuxuan Chen,Xiao Li*

Main category: cs.RO

TL;DR: This paper introduces RLRC, a method to compress Vision-Language-Action models (VLAs) to reduce memory usage and inference latency while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome the challenge of deploying large and computationally expensive VLAs on resource-constrained robotic platforms.

Method: RLRC involves three stages: structured pruning, performance recovery via SFT and RL, and quantization to compress VLA models effectively.

Result: RLRC reduces memory usage by up to 8x, improves inference throughput by 2.3x, and maintains or surpasses the original model's success rate.

Conclusion: RLRC provides an effective approach to compress VLAs, enabling their on-device deployment without significant performance trade-offs.

Abstract: Vision-Language-Action models (VLA) have demonstrated remarkable capabilities
and promising potential in solving complex robotic manipulation tasks. However,
their substantial parameter sizes and high inference latency pose significant
challenges for real-world deployment, particularly on resource-constrained
robotic platforms. To address this issue, we begin by conducting an extensive
empirical study to explore the effectiveness of model compression techniques
when applied to VLAs. Building on the insights gained from these preliminary
experiments, we propose RLRC, a three-stage recovery method for compressed
VLAs, including structured pruning, performance recovery based on SFT and RL,
and further quantization. RLRC achieves up to an 8x reduction in memory usage
and a 2.3x improvement in inference throughput, while maintaining or even
surpassing the original VLA's task success rate. Extensive experiments show
that RLRC consistently outperforms existing compression baselines,
demonstrating strong potential for on-device deployment of VLAs. Project
website: https://rlrc-vla.github.io

</details>


### [381] [Optimizing Exploration with a New Uncertainty Framework for Active SLAM Systems](https://arxiv.org/abs/2506.17775)
*Sebastian Sansoni,Javier Gimenez,Gastón Castro,Santiago Tosetti,Flavio Craparo*

Main category: cs.RO

TL;DR: The paper introduces a new method for Active SLAM using an Uncertainty Map (UM) and a measure called Signed Relative Entropy (SiREn) for balancing exploration and exploitation.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy of environment reconstruction in SLAM systems by addressing the impact of the agent's trajectory on map estimation.

Method: The method models map uncertainty using Uncertainty Maps (UM) and defines Uncertainty Frontiers (UF) for goal setting. It also uses Signed Relative Entropy (SiREn) to measure map coverage and uncertainty.

Result: The approach enables autonomous exploration of open spaces, is sensor-agnostic, and solves challenges in exploration planning and stopping criteria.

Conclusion: The proposed method offers a generic and effective solution for Active SLAM, providing tools for better trajectory planning and ensuring broad applicability through open-source implementations.

Abstract: Accurate reconstruction of the environment is a central goal of Simultaneous
Localization and Mapping (SLAM) systems. However, the agent's trajectory can
significantly affect estimation accuracy. This paper presents a new method to
model map uncertainty in Active SLAM systems using an Uncertainty Map (UM). The
UM uses probability distributions to capture where the map is uncertain,
allowing Uncertainty Frontiers (UF) to be defined as key
exploration-exploitation objectives and potential stopping criteria. In
addition, the method introduces the Signed Relative Entropy (SiREn), based on
the Kullback-Leibler divergence, to measure both coverage and uncertainty
together. This helps balance exploration and exploitation through an
easy-to-understand parameter. Unlike methods that depend on particular SLAM
setups, the proposed approach is compatible with different types of sensors,
such as cameras, LiDARs, and multi-sensor fusion. It also addresses common
problems in exploration planning and stopping conditions. Furthermore,
integrating this map modeling approach with a UF-based planning system enables
the agent to autonomously explore open spaces, a behavior not previously
observed in the Active SLAM literature. Code and implementation details are
available as a ROS node, and all generated data are openly available for public
use, facilitating broader adoption and validation of the proposed approach.

</details>


### [382] [RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models](https://arxiv.org/abs/2506.17811)
*Jacky Kwok,Christopher Agia,Rohan Sinha,Matt Foutter,Shulu Li,Ion Stoica,Azalia Mirhoseini,Marco Pavone*

Main category: cs.RO

TL;DR: The paper introduces RoboMonkey, a test-time scaling framework for Vision-Language-Action (VLA) models to improve robustness and generalization in unstructured environments, leveraging sampling, Gaussian perturbations, majority voting, and VLM-based verification.


<details>
  <summary>Details</summary>
Motivation: Vision-Language-Action models excel in visuomotor tasks but lack robustness in real-world scenarios, creating a need for mechanisms to enhance their reliability.

Method: The paper proposed RoboMonkey, which samples actions, applies Gaussian perturbations, conducts majority voting for action proposals, and employs Vision Language Model verifiers trained on synthetic datasets for selecting optimal actions.

Result: RoboMonkey improved performance by 25% on out-of-distribution tasks and 8% on in-distribution tasks. Fine-tuning RoboMonkey alongside VLAs further improved performance by 7% in new robot setups.

Conclusion: Test-time scaling using RoboMonkey significantly enhances VLA robustness and adapts well to unseen tasks and environments, demonstrating its potential for deployment in diverse real-world applications.

Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities
in visuomotor control, yet ensuring their robustness in unstructured real-world
environments remains a persistent challenge. In this paper, we investigate
test-time scaling through the lens of sampling and verification as means to
enhance the robustness and generalization of VLAs. We first demonstrate that
the relationship between action error and the number of generated samples
follows an exponentiated power law across a range of VLAs, indicating the
existence of inference-time scaling laws. Building on these insights, we
introduce RoboMonkey, a test-time scaling framework for VLAs. At deployment,
RoboMonkey samples a small set of actions from a VLA, applies Gaussian
perturbation and majority voting to construct an action proposal distribution,
and then uses a Vision Language Model (VLM)-based verifier to select the
optimal action. We propose a synthetic data generation pipeline for training
such VLM-based action verifiers, and demonstrate that scaling the synthetic
dataset consistently improves verification and downstream accuracy. Through
extensive simulated and hardware experiments, we show that pairing existing
VLAs with RoboMonkey yields significant performance gains, achieving a 25%
absolute improvement on out-of-distribution tasks and 8% on in-distribution
tasks. Additionally, when adapting to new robot setups, we show that
fine-tuning both VLAs and action verifiers yields a 7% performance increase
compared to fine-tuning VLAs alone.

</details>


### [383] [Learning to Dock: A Simulation-based Study on Closing the Sim2Real Gap in Autonomous Underwater Docking](https://arxiv.org/abs/2506.17823)
*Kevin Chang,Rakesh Vivekanandan,Noah Pragin,Sean Bullock,Geoffrey Hollinger*

Main category: cs.RO

TL;DR: The paper addresses the sim2real gap issue in AUV docking by conducting simulation studies on controllers trained and tested under realistic disturbances, especially with payload variation.


<details>
  <summary>Details</summary>
Motivation: The need to overcome the sim2real gap for robust AUV docking in dynamic and uncertain underwater environments.

Method: Simulation studies focusing on training docking controllers with randomization techniques and history-conditioned methodologies, and evaluating them under realistic payload disturbances.

Result: Improved understanding of how to address sim2real gap challenges and insights into enhancing controller robustness.

Conclusion: The study highlights effective methods for reducing the sim2real gap in AUV docking, offering valuable directions for future research in marine robotics.

Abstract: Autonomous Underwater Vehicle (AUV) docking in dynamic and uncertain
environments is a critical challenge for underwater robotics. Reinforcement
learning is a promising method for developing robust controllers, but the
disparity between training simulations and the real world, or the sim2real gap,
often leads to a significant deterioration in performance. In this work, we
perform a simulation study on reducing the sim2real gap in autonomous docking
through training various controllers and then evaluating them under realistic
disturbances. In particular, we focus on the real-world challenge of docking
under different payloads that are potentially outside the original training
distribution. We explore existing methods for improving robustness including
randomization techniques and history-conditioned controllers. Our findings
provide insights into mitigating the sim2real gap when training docking
controllers. Furthermore, our work indicates areas of future research that may
be beneficial to the marine robotics community.

</details>


### [384] [Engagement and Disclosures in LLM-Powered Cognitive Behavioral Therapy Exercises: A Factorial Design Comparing the Influence of a Robot vs. Chatbot Over Time](https://arxiv.org/abs/2506.17831)
*Mina Kian,Mingyu Zong,Katrin Fischer,Anna-Maria Velentza,Abhyuday Singh,Kaleen Shrestha,Pau Sang,Shriya Upadhyay,Wallace Browning,Misha Arif Faruki,Sébastien M. R. Arnold,Bhaskar Krishnamachari,Maja Matarić*

Main category: cs.RO

TL;DR: The study assesses the long-term impact of therapeutic interaction using either LLM-powered socially assistive robots (SARs) or disembodied chatbots, finding that participant engagement and intimacy improve over time with robots but decline with chatbots.


<details>
  <summary>Details</summary>
Motivation: To address the global mental health crisis by exploring how therapeutic technologies like chatbots and SARs—empowered by large language models—can effectively enhance care accessibility over time.

Method: A factorial design experiment with 26 university students performing daily interactive Cognitive Behavioral Therapy (CBT) exercises over two weeks, either with a SAR or a chatbot. Transcripts were analyzed for engagement and intimacy levels.

Result: Participant engagement and intimacy increased over time with SARs but decreased with chatbots, showing the importance of physical embodiment in long-term therapeutic contexts.

Conclusion: Embodiment plays a critical role in sustaining engagement and intimacy during therapeutic exercises over time, with robots performing better than chatbots in this regard.

Abstract: Many researchers are working to address the worldwide mental health crisis by
developing therapeutic technologies that increase the accessibility of care,
including leveraging large language model (LLM) capabilities in chatbots and
socially assistive robots (SARs) used for therapeutic applications. Yet, the
effects of these technologies over time remain unexplored. In this study, we
use a factorial design to assess the impact of embodiment and time spent
engaging in therapeutic exercises on participant disclosures. We assessed
transcripts gathered from a two-week study in which 26 university student
participants completed daily interactive Cognitive Behavioral Therapy (CBT)
exercises in their residences using either an LLM-powered SAR or a disembodied
chatbot. We evaluated the levels of active engagement and high intimacy of
their disclosures (opinions, judgments, and emotions) during each session and
over time. Our findings show significant interactions between time and
embodiment for both outcome measures: participant engagement and intimacy
increased over time in the physical robot condition, while both measures
decreased in the chatbot condition.

</details>


### [385] [Leveling the Playing Field: Carefully Comparing Classical and Learned Controllers for Quadrotor Trajectory Tracking](https://arxiv.org/abs/2506.17832)
*Pratik Kunapuli,Jake Welde,Dinesh Jayaraman,Vijay Kumar*

Main category: cs.RO

TL;DR: This paper highlights the challenge of fairly comparing reinforcement learning (RL)-based controllers with geometric controllers (GC) for agile quadrotor tasks, showing RL's advantage in agility and GC's advantage in precision but reducing previous overstated performance gaps.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address biases in prior studies that unfairly favor RL controllers over geometric controllers when comparing their performance in quadrotor trajectory tracking tasks.

Method: The researchers redesigned experimental protocols to ensure symmetric access to task definitions, datasets, and feedforward information for both RL and geometric controllers. They then analyzed and benchmarked the controllers using these best practices for comparison.

Result: Contrary to prior studies, the paper observed smaller performance gaps between RL and GC. RL excelled in transient performance and agility, while GC outperformed RL in steady-state error and less agile tasks. This revealed that earlier conclusions were biased due to asymmetric comparisons.

Conclusion: The paper concludes that fair experimental practices are essential for unbiased comparison between RL and traditional controllers. It also emphasizes the differences in where RL and GC excel, depending on task requirements, and provides open-source code to enhance future development.

Abstract: Learning-based control approaches like reinforcement learning (RL) have
recently produced a slew of impressive results for tasks like quadrotor
trajectory tracking and drone racing. Naturally, it is common to demonstrate
the advantages of these new controllers against established methods like
analytical controllers. We observe, however, that reliably comparing the
performance of such very different classes of controllers is more complicated
than might appear at first sight. As a case study, we take up the problem of
agile tracking of an end-effector for a quadrotor with a fixed arm. We develop
a set of best practices for synthesizing the best-in-class RL and geometric
controllers (GC) for benchmarking. In the process, we resolve widespread
RL-favoring biases in prior studies that provide asymmetric access to: (1) the
task definition, in the form of an objective function, (2) representative
datasets, for parameter optimization, and (3) feedforward information,
describing the desired future trajectory. The resulting findings are the
following: our improvements to the experimental protocol for comparing learned
and classical controllers are critical, and each of the above asymmetries can
yield misleading conclusions. Prior works have claimed that RL outperforms GC,
but we find the gaps between the two controller classes are much smaller than
previously published when accounting for symmetric comparisons. Geometric
control achieves lower steady-state error than RL, while RL has better
transient performance, resulting in GC performing better in relatively slow or
less agile tasks, but RL performing better when greater agility is required.
Finally, we open-source implementations of geometric and RL controllers for
these aerial vehicles, implementing best practices for future development.
Website and code is available at https://pratikkunapuli.github.io/rl-vs-gc/

</details>


### [386] [Generative Grasp Detection and Estimation with Concept Learning-based Safety Criteria](https://arxiv.org/abs/2506.17842)
*Al-Harith Farhad,Khalil Abuibaid,Christiane Plociennik,Achim Wagner,Martin Ruskowski*

Main category: cs.RO

TL;DR: The paper presents a grasping algorithm for cobots integrating explainable AI for tool detection and grasp optimization, ensuring safety in handling work tools.


<details>
  <summary>Details</summary>
Motivation: To improve transparency and reliability in neural network-dependent cobot grasping systems, addressing the challenge of interpretability in safety-critical applications.

Method: The method incorporates explainable AI to extract and correlate learned features to their classes for better grasping decisions. These concepts serve as additional safety criteria in tool handling.

Result: The approach demonstrated consistency and improved handover positions, validated through tests in an industrial setup where a robot used a camera system for tool detection and manipulation.

Conclusion: The explainable AI-integrated framework enhances the safety, transparency, and effectiveness of robot grasping algorithms in industrial contexts.

Abstract: Neural networks are often regarded as universal equations that can estimate
any function. This flexibility, however, comes with the drawback of high
complexity, rendering these networks into black box models, which is especially
relevant in safety-centric applications. To that end, we propose a pipeline for
a collaborative robot (Cobot) grasping algorithm that detects relevant tools
and generates the optimal grasp. To increase the transparency and reliability
of this approach, we integrate an explainable AI method that provides an
explanation for the underlying prediction of a model by extracting the learned
features and correlating them to corresponding classes from the input. These
concepts are then used as additional criteria to ensure the safe handling of
work tools. In this paper, we show the consistency of this approach and the
criterion for improving the handover position. This approach was tested in an
industrial environment, where a camera system was set up to enable a robot to
pick up certain tools and objects.

</details>


### [387] [Geometric Contact Flows: Contactomorphisms for Dynamics and Control](https://arxiv.org/abs/2506.17868)
*Andrea Testa,Søren Hauberg,Tamim Asfour,Leonel Rozo*

Main category: cs.RO

TL;DR: This paper introduces a novel method called Geometric Contact Flows (GCF) to model and predict complex dynamical systems, ensuring stability and energy conservation.


<details>
  <summary>Details</summary>
Motivation: There is a need for accurate modeling of complex dynamical systems that feature force exchange, dissipation, and an interplay of geometry and energy transfer, which are critical in fields like fluid dynamics and robotics.

Method: The proposed method, GCF, employs Riemannian and Contact geometry principles to construct a latent contact Hamiltonian model. It uses an ensemble of contactomorphisms for adapting the model to the target dynamics while maintaining properties such as stability and energy conservation.

Result: GCF successfully enables robust generalization and adaptation to unseen scenarios by creating uncertainty-aware geodesics. Its effectiveness is demonstrated in experiments involving physical system dynamics and robotic interaction tasks.

Conclusion: The approach provides a powerful tool for modeling dynamical systems with improved generalization and adaptability while preserving desirable properties like stability and energy conservation.

Abstract: Accurately modeling and predicting complex dynamical systems, particularly
those involving force exchange and dissipation, is crucial for applications
ranging from fluid dynamics to robotics, but presents significant challenges
due to the intricate interplay of geometric constraints and energy transfer.
This paper introduces Geometric Contact Flows (GFC), a novel framework
leveraging Riemannian and Contact geometry as inductive biases to learn such
systems. GCF constructs a latent contact Hamiltonian model encoding desirable
properties like stability or energy conservation. An ensemble of
contactomorphisms then adapts this model to the target dynamics while
preserving these properties. This ensemble allows for uncertainty-aware
geodesics that attract the system's behavior toward the data support, enabling
robust generalization and adaptation to unseen scenarios. Experiments on
learning dynamics for physical systems and for controlling robots on
interaction tasks demonstrate the effectiveness of our approach.

</details>


### [388] [Embedded Flexible Circumferential Sensing for Real-Time Intraoperative Environmental Perception in Continuum Robots](https://arxiv.org/abs/2506.17902)
*Peiyu Luo,Shilong Yao,Yuhan Chen,Max Q. -H. Meng*

Main category: cs.RO

TL;DR: Proposes a flexible sensor for continuum robots in surgeries, enhancing tissue awareness and control.


<details>
  <summary>Details</summary>
Motivation: Limited proprioceptive capabilities of continuum robots in surgeries lead to unintended tissue contact and risks.

Method: Developed an annular sensor around robotic disks for real-time tissue distance estimation.

Result: Achieved 0.19 mm accuracy in obstacle detection with a compact and cost-effective design.

Conclusion: The sensor is modular, adaptable to various robots, and improves intraoperative perception and control.

Abstract: Continuum robots have been widely adopted in robot-assisted minimally
invasive surgery (RMIS) because of their compact size and high flexibility.
However, their proprioceptive capabilities remain limited, particularly in
narrow lumens, where lack of environmental awareness can lead to unintended
tissue contact and surgical risks. To address this challenge, this work
proposes a flexible annular sensor structure integrated around the vertebral
disks of continuum robots. The proposed design enables real-time environmental
mapping by estimating the distance between the robotic disks and the
surrounding tissue, thereby facilitating safer operation through advanced
control strategies. The experiment has proven that its accuracy in obstacle
detection can reach 0.19 mm. Fabricated using flexible printed circuit (FPC)
technology, the sensor demonstrates a modular and cost-effective design with
compact dimensions and low noise interference. Its adaptable parameters allow
compatibility with various continuum robot architectures, offering a promising
solution for enhancing intraoperative perception and control in surgical
robotics.

</details>


### [389] [GeNIE: A Generalizable Navigation System for In-the-Wild Environments](https://arxiv.org/abs/2506.17960)
*Jiaming Wang,Diwen Liu,Jizhuo Chen,Jiaxuan Da,Nuowen Qian,Tram Minh Man,Harold Soh*

Main category: cs.RO

TL;DR: This paper introduces GeNIE, a navigation framework for robots in diverse outdoor environments, achieving outstanding results in a global competition.


<details>
  <summary>Details</summary>
Motivation: Current navigation systems struggle with the challenges of unstructured and diverse real-world terrains, weather, and sensor configurations.

Method: GeNIE combines a traversability prediction model (SAM2) and a novel path fusion strategy to improve navigation in noisy and ambiguous environments.

Result: GeNIE won the Earth Rover Challenge, scoring 79% of the maximum score, outperforming the next best competitor by 17%, and operated without human intervention.

Conclusion: GeNIE sets a new benchmark for outdoor robot navigation reliability and generalizability, with plans to release supporting resources for further research.

Abstract: Reliable navigation in unstructured, real-world environments remains a
significant challenge for embodied agents, especially when operating across
diverse terrains, weather conditions, and sensor configurations. In this paper,
we introduce GeNIE (Generalizable Navigation System for In-the-Wild
Environments), a robust navigation framework designed for global deployment.
GeNIE integrates a generalizable traversability prediction model built on SAM2
with a novel path fusion strategy that enhances planning stability in noisy and
ambiguous settings. We deployed GeNIE in the Earth Rover Challenge (ERC) at
ICRA 2025, where it was evaluated across six countries spanning three
continents. GeNIE took first place and achieved 79% of the maximum possible
score, outperforming the second-best team by 17%, and completed the entire
competition without a single human intervention. These results set a new
benchmark for robust, generalizable outdoor robot navigation. We will release
the codebase, pretrained model weights, and newly curated datasets to support
future research in real-world navigation.

</details>


### [390] [Newtonian and Lagrangian Neural Networks: A Comparison Towards Efficient Inverse Dynamics Identification](https://arxiv.org/abs/2506.17994)
*Minh Trinh,Andreas René Geist,Josefine Monnet,Stefan Vilceanu,Sebastian Trimpe,Christian Brecher*

Main category: cs.RO

TL;DR: The study compares Newtonian and Lagrangian neural networks for modeling inverse dynamics in robots, finding Newtonian networks more effective when estimating motor torques.


<details>
  <summary>Details</summary>
Motivation: To address the lack of guidance in choosing between Lagrangian and Newtonian neural networks for inverse dynamics in robotics.

Method: The research combines neural network regression with inverse dynamics formulations and assesses performance using data from MABI MAX 100 industrial robot.

Result: Newtonian networks outperform Lagrangian networks when motor torques are estimated instead of directly measuring joint torques.

Conclusion: Newtonian networks are preferable in scenarios involving estimated motor torques, highlighting their robustness in capturing dissipative dynamics.

Abstract: Accurate inverse dynamics models are essential tools for controlling
industrial robots. Recent research combines neural network regression with
inverse dynamics formulations of the Newton-Euler and the Euler-Lagrange
equations of motion, resulting in so-called Newtonian neural networks and
Lagrangian neural networks, respectively. These physics-informed models seek to
identify unknowns in the analytical equations from data. Despite their
potential, current literature lacks guidance on choosing between Lagrangian and
Newtonian networks. In this study, we show that when motor torques are
estimated instead of directly measuring joint torques, Lagrangian networks
prove less effective compared to Newtonian networks as they do not explicitly
model dissipative torques. The performance of these models is compared to
neural network regression on data of a MABI MAX 100 industrial robot.

</details>


### [391] [ADA-DPM: A Neural Descriptors-based Adaptive Noise Point Filtering Strategy for SLAM](https://arxiv.org/abs/2506.18016)
*Yongxin Shao,Binrui Wang,Aihong Tan*

Main category: cs.RO

TL;DR: The paper introduces ADA-DPM, a LiDAR SLAM strategy focusing on optimizing performance in dynamic and noisy environments through advanced feature point processing.


<details>
  <summary>Details</summary>
Motivation: Existing LiDAR SLAM methods struggle to balance accuracy and robustness when dealing with challenges such as dynamic objects, noise, and unstructured environments.

Method: The authors propose ADA-DPM, which integrates a Dynamic Segmentation Head for dynamic object filtering, a Global Importance Scoring Head for noise suppression and feature selection, and a GLI-GCN module for multi-scale feature fusion.

Result: Testing on multiple publicly available datasets demonstrated that ADA-DPM achieved remarkable performance improvements, validating its effectiveness.

Conclusion: The proposed ADA-DPM strategy offers a robust and accurate solution for LiDAR SLAM, addressing environmental challenges and significantly advancing SLAM performance.

Abstract: LiDAR SLAM has demonstrated significant application value in various fields,
including mobile robot navigation and high-precision map construction. However,
existing methods often need to make a trade-off between positioning accuracy
and system robustness when faced with dynamic object interference, point cloud
noise, and unstructured environments. To address this challenge, we propose an
adaptive noise filtering SLAM strategy-ADA-DPM, achieving excellent preference
in both aspects. We design the Dynamic Segmentation Head to predict the
category of feature points belonging to dynamic points, to eliminate dynamic
feature points; design the Global Importance Scoring Head to adaptively select
feature points with higher contribution and features while suppressing noise
interference; and construct the Cross Layer Intra-Graph Convolution Module
(GLI-GCN) to fuse multi-scale neighborhood structures, thereby enhancing the
discriminative ability of overlapping features. Finally, to further validate
the effectiveness of our method, we tested it on several publicly available
datasets and achieved outstanding results.

</details>


### [392] [StereoTacTip: Vision-based Tactile Sensing with Biomimetic Skin-Marker Arrangements](https://arxiv.org/abs/2506.18040)
*Chenghua Lu,Kailuan Tang,Xueming Hui,Haoran Li,Saekwang Nam,Nathan F. Lepora*

Main category: cs.RO

TL;DR: This paper develops methods to improve geometric reconstruction accuracy in marker-based Vision-Based Tactile Sensors (VBTSs) using stereo cameras.


<details>
  <summary>Details</summary>
Motivation: The need to address limitations in geometric reconstruction of complex biomimetic skin-marker arrangements in marker-based VBTSs.

Method: Developed techniques include stereo marker tracking using Delaunay-Triangulation-Ring-Coding algorithm, refractive depth correction model, and models for skin surface correction and multi-contact geometry reconstruction.

Result: Successful reconstruction of 3D topographic terrains on a map, showing improved geometric performance of marker-based VBTSs.

Conclusion: A comprehensive analysis of skin-marker-based VBTS principles is essential to achieving accurate geometric information in these tactile sensors.

Abstract: Vision-Based Tactile Sensors (VBTSs) stand out for their superior performance
due to their high-information content output. Recently, marker-based VBTSs have
been shown to give accurate geometry reconstruction when using stereo cameras.
\uhl{However, many marker-based VBTSs use complex biomimetic skin-marker
arrangements, which presents issues for the geometric reconstruction of the
skin surface from the markers}. Here we investigate how the marker-based skin
morphology affects stereo vision-based tactile sensing, using a novel VBTS
called the StereoTacTip. To achieve accurate geometry reconstruction, we
introduce: (i) stereo marker matching and tracking using a novel
Delaunay-Triangulation-Ring-Coding algorithm; (ii) a refractive depth
correction model that corrects the depth distortion caused by refraction in the
internal media; (iii) a skin surface correction model from the marker
positions, relying on an inverse calculation of normals to the skin surface;
and (iv)~methods for geometry reconstruction over multiple contacts. To
demonstrate these findings, we reconstruct topographic terrains on a large 3D
map. Even though contributions (i) and (ii) were developed for biomimetic
markers, they should improve the performance of all marker-based VBTSs.
Overall, this work illustrates that a thorough understanding and evaluation of
the morphologically-complex skin and marker-based tactile sensor principles are
crucial for obtaining accurate geometric information.

</details>


### [393] [RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation](https://arxiv.org/abs/2506.18088)
*Tianxing Chen,Zanxin Chen,Baijun Chen,Zijian Cai,Yibin Liu,Qiwei Liang,Zixuan Li,Xianliang Lin,Yiheng Ge,Zhenyu Gu,Weiliang Deng,Yubin Guo,Tian Nian,Xuanbing Xie,Qiangyu Chen,Kailun Su,Tianling Xu,Guodong Liu,Mengkang Hu,Huan-ang Gao,Kaixuan Wang,Zhixuan Liang,Yusen Qin,Xiaokang Yang,Ping Luo,Yao Mu*

Main category: cs.RO

TL;DR: RoboTwin 2.0 is a framework enabling large-scale, realistic data generation and evaluation for robotic dual-arm tasks, addressing simulation and generalization challenges.


<details>
  <summary>Details</summary>
Motivation: Existing data synthesis frameworks are inadequate for dual-arm robotic manipulation due to limitations in scalable data generation and realistic simulation environments.

Method: The authors introduce RoboTwin 2.0, which includes a large object library (RoboTwin-OD), a data synthesis pipeline using multimodal large language models, and structured domain randomization for robustness.

Result: RoboTwin 2.0 demonstrates a 10.9% improvement in code generation success and significant gains in real-world task generalization, with models fine-tuned on the dataset achieving up to a 367% performance improvement.

Conclusion: RoboTwin 2.0 effectively enhances synthetic-to-real-world transfer in robotic bimanual tasks and provides tools for scalable manipulation research.

Abstract: Simulation-based data synthesis has emerged as a powerful paradigm for
enhancing real-world robotic manipulation. However, existing synthetic datasets
remain insufficient for robust bimanual manipulation due to two challenges: (1)
the lack of an efficient, scalable data generation method for novel tasks, and
(2) oversimplified simulation environments that fail to capture real-world
complexity. We present RoboTwin 2.0, a scalable simulation framework that
enables automated, large-scale generation of diverse and realistic data, along
with unified evaluation protocols for dual-arm manipulation. We first construct
RoboTwin-OD, a large-scale object library comprising 731 instances across 147
categories, each annotated with semantic and manipulation-relevant labels.
Building on this foundation, we develop an expert data synthesis pipeline that
combines multimodal large language models (MLLMs) with simulation-in-the-loop
refinement to generate task-level execution code automatically. To improve
sim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization
along five axes: clutter, lighting, background, tabletop height and language
instructions, thereby enhancing data diversity and policy robustness. We
instantiate this framework across 50 dual-arm tasks spanning five robot
embodiments, and pre-collect over 100,000 domain-randomized expert
trajectories. Empirical results show a 10.9% gain in code generation success
and improved generalization to novel real-world scenarios. A VLA model
fine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%)
on unseen scene real-world tasks, while zero-shot models trained solely on our
synthetic data achieve a 228% relative gain, highlighting strong generalization
without real-world supervision. We release the data generator, benchmark,
dataset, and code to support scalable research in robust bimanual manipulation.

</details>


### [394] [RoboArena: Distributed Real-World Evaluation of Generalist Robot Policies](https://arxiv.org/abs/2506.18123)
*Pranav Atreya,Karl Pertsch,Tony Lee,Moo Jin Kim,Arhan Jain,Artur Kuramshin,Clemens Eppner,Cyrus Neary,Edward Hu,Fabio Ramos,Jonathan Tremblay,Kanav Arora,Kirsty Ellis,Luca Macesanu,Matthew Leonard,Meedeum Cho,Ozgur Aslan,Shivin Dass,Jie Wang,Xingfang Yuan,Xuning Yang,Abhishek Gupta,Dinesh Jayaraman,Glen Berseth,Kostas Daniilidis,Roberto Martin-Martin,Youngwoon Lee,Percy Liang,Chelsea Finn,Sergey Levine*

Main category: cs.RO

TL;DR: The paper introduces RoboArena, a crowd-sourced, distributed evaluation method for generalist robot policies, overcoming limitations of traditional fixed-task benchmarking.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of comprehensively, unbiasedly, and comparably evaluating generalist robot policies across diverse tasks and environments, which traditional methods struggle to achieve.

Method: The authors propose a distributed evaluation framework called RoboArena, where evaluators perform double-blind comparisons of policies across freely chosen tasks and environments, aggregating pairwise feedback to rank the policies.

Result: Using evaluations from seven institutions with the DROID robot platform, RoboArena conducted over 600 pairwise comparisons and demonstrated superior scalability, accuracy, resilience, and credibility compared to fixed-task evaluations.

Conclusion: RoboArena provides an open, scalable, and reliable approach for community-driven comparisons of generalist robot policies, fostering broader and more trustworthy benchmarking.

Abstract: Comprehensive, unbiased, and comparable evaluation of modern generalist
policies is uniquely challenging: existing approaches for robot benchmarking
typically rely on heavy standardization, either by specifying fixed evaluation
tasks and environments, or by hosting centralized ''robot challenges'', and do
not readily scale to evaluating generalist policies across a broad range of
tasks and environments. In this work, we propose RoboArena, a new approach for
scalable evaluation of generalist robot policies in the real world. Instead of
standardizing evaluations around fixed tasks, environments, or locations, we
propose to crowd-source evaluations across a distributed network of evaluators.
Importantly, evaluators can freely choose the tasks and environments they
evaluate on, enabling easy scaling of diversity, but they are required to
perform double-blind evaluations over pairs of policies. Then, by aggregating
preference feedback from pairwise comparisons across diverse tasks and
environments, we can derive a ranking of policies. We instantiate our approach
across a network of evaluators at seven academic institutions using the DROID
robot platform. Through more than 600 pairwise real-robot evaluation episodes
across seven generalist policies, we demonstrate that our crowd-sourced
approach can more accurately rank the performance of existing generalist
policies than conventional, centralized evaluation approaches, while being more
scalable, resilient, and trustworthy. We open our evaluation network to the
community and hope that it can enable more accessible comparisons of generalist
robot policies.

</details>


### [395] [Automated Plan Refinement for Improving Efficiency of Robotic Layup of Composite Sheets](https://arxiv.org/abs/2506.18160)
*Rutvik Patel,Alec Kanyuck,Zachary McNulty,Zeren Yu,Lisa Carlson,Vann Heng,Brice Johnson,Satyandra K. Gupta*

Main category: cs.RO

TL;DR: The paper introduces a framework to refine robotic draping plans for composite sheet layup, integrating human expertise and data-driven methods for improved efficiency and adaptability.


<details>
  <summary>Details</summary>
Motivation: Composite sheet layup automation lacks robustness in varying operational conditions, requiring adaptable planning methods.

Method: The framework combines human expertise, data-driven decision-making, empirical data analysis, action-effectiveness modeling, and search-based refinement.

Result: Experiments show improved efficiency, reducing corrective paths compared to initial expert plans and enhancing time efficiency in robotic layup.

Conclusion: The proposed framework advances composite manufacturing automation by improving adaptability and operational efficiency in robotic layup.

Abstract: The automation of composite sheet layup is essential to meet the increasing
demand for composite materials in various industries. However, draping plans
for the robotic layup of composite sheets are not robust. A plan that works
well under a certain condition does not work well in a different condition.
Changes in operating conditions due to either changes in material properties or
working environment may lead a draping plan to exhibit suboptimal performance.
In this paper, we present a comprehensive framework aimed at refining plans
based on the observed execution performance. Our framework prioritizes the
minimization of uncompacted regions while simultaneously improving time
efficiency. To achieve this, we integrate human expertise with data-driven
decision-making to refine expert-crafted plans for diverse production
environments. We conduct experiments to validate the effectiveness of our
approach, revealing significant reductions in the number of corrective paths
required compared to initial expert-crafted plans. Through a combination of
empirical data analysis, action-effectiveness modeling, and search-based
refinement, our system achieves superior time efficiency in robotic layup.
Experimental results demonstrate the efficacy of our approach in optimizing the
layup process, thereby advancing the state-of-the-art in composite
manufacturing automation.

</details>


### [396] [Integrating LLMs and Digital Twins for Adaptive Multi-Robot Task Allocation in Construction](https://arxiv.org/abs/2506.18178)
*Min Deng,Bo Fu,Lingyao Li,Xi Wang*

Main category: cs.RO

TL;DR: This study presents a multi-robot task allocation framework combining Digital Twins, Integer Programming (IP), and Large Language Models (LLMs) to enhance adaptability and efficiency in challenging industrial environments.


<details>
  <summary>Details</summary>
Motivation: The increasing demand for productivity, safety, and adaptability in industrial sectors, particularly in dynamic and uncertain environments like construction sites, necessitates effective multi-robot coordination solutions.

Method: The study developed a framework using Integer Programming (IP) for task allocation, integrated with LLMs for human-in-the-loop flexibility through natural language input processing, and a Digital Twin system for real-time synchronization.

Result: The framework demonstrated computational efficiency in task allocation and high reasoning accuracy for constraint and parameter extraction, with LLMs achieving over 97% accuracy in case studies.

Conclusion: The proposed methods are practical, adaptable, and applicable across domains, addressing the challenges of multi-robot coordination in dynamic scenarios.

Abstract: Multi-robot systems are emerging as a promising solution to the growing
demand for productivity, safety, and adaptability across industrial sectors.
However, effectively coordinating multiple robots in dynamic and uncertain
environments, such as construction sites, remains a challenge, particularly due
to unpredictable factors like material delays, unexpected site conditions, and
weather-induced disruptions. To address these challenges, this study proposes
an adaptive task allocation framework that strategically leverages the
synergistic potential of Digital Twins, Integer Programming (IP), and Large
Language Models (LLMs). The multi-robot task allocation problem is formally
defined and solved using an IP model that accounts for task dependencies, robot
heterogeneity, scheduling constraints, and re-planning requirements. A
mechanism for narrative-driven schedule adaptation is introduced, in which
unstructured natural language inputs are interpreted by an LLM, and
optimization constraints are autonomously updated, enabling human-in-the-loop
flexibility without manual coding. A digital twin-based system has been
developed to enable real-time synchronization between physical operations and
their digital representations. This closed-loop feedback framework ensures that
the system remains dynamic and responsive to ongoing changes on site. A case
study demonstrates both the computational efficiency of the optimization
algorithm and the reasoning performance of several LLMs, with top-performing
models achieving over 97% accuracy in constraint and parameter extraction. The
results confirm the practicality, adaptability, and cross-domain applicability
of the proposed methods.

</details>


### [397] [Haptic-ACT -- Pseudo Oocyte Manipulation by a Robot Using Multimodal Information and Action Chunking with Transformers](https://arxiv.org/abs/2506.18212)
*Pedro Miguel Uriguen Eljuri,Hironobu Shibata,Maeyama Katsuyoshi,Yuanyuan Jia,Tadahiro Taniguchi*

Main category: cs.RO

TL;DR: Haptic-ACT is a robotic system enhancing oocyte manipulation with multimodal learning and haptic feedback, improving task performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional oocyte transfer methods that rely heavily on visual perception and require human supervision due to biological variability and environmental disturbances.

Method: The method integrates multimodal information and Action Chunking with Transformers (ACT) while incorporating haptic feedback for real-time failure detection and using a 3D-printed TPU soft gripper for delicate tasks.

Result: Haptic-ACT demonstrated improved success rate, robustness, and adaptability in oocyte manipulation, particularly in dynamic environments.

Conclusion: The study underscores the promise of multimodal learning and haptic feedback in advancing robotic systems for biomedical automation tasks.

Abstract: In this paper we introduce Haptic-ACT, an advanced robotic system for pseudo
oocyte manipulation, integrating multimodal information and Action Chunking
with Transformers (ACT). Traditional automation methods for oocyte transfer
rely heavily on visual perception, often requiring human supervision due to
biological variability and environmental disturbances. Haptic-ACT enhances ACT
by incorporating haptic feedback, enabling real-time grasp failure detection
and adaptive correction. Additionally, we introduce a 3D-printed TPU soft
gripper to facilitate delicate manipulations. Experimental results demonstrate
that Haptic-ACT improves the task success rate, robustness, and adaptability
compared to conventional ACT, particularly in dynamic environments. These
findings highlight the potential of multimodal learning in robotics for
biomedical automation.

</details>


### [398] [Robot Tactile Gesture Recognition Based on Full-body Modular E-skin](https://arxiv.org/abs/2506.18256)
*Shuo Jiang,Boce Hu,Linfeng Zhao,Lawson L. S. Wong*

Main category: cs.RO

TL;DR: The paper presents an AI-enhanced electronic skin capable of recognizing tactile gestures for intuitive human-robot interaction.


<details>
  <summary>Details</summary>
Motivation: To improve human-robot interaction by enabling robots to interpret tactile gestures as commands.

Method: Development of modular electronic skin with sensing points and an equivariant graph neural network to classify tactile gestures.

Result: The system accurately identifies various tactile gestures like poke, grab, stroke, and double-pat.

Conclusion: Robots equipped with this technology can provide intuitive interaction through tactile inputs, improving usability.

Abstract: With the development of robot electronic skin technology, various tactile
sensors, enhanced by AI, are unlocking a new dimension of perception for
robots. In this work, we explore how robots equipped with electronic skin can
recognize tactile gestures and interpret them as human commands. We developed a
modular robot E-skin, composed of multiple irregularly shaped skin patches,
which can be assembled to cover the robot's body while capturing real-time
pressure and pose data from thousands of sensing points. To process this
information, we propose an equivariant graph neural network-based recognizer
that efficiently and accurately classifies diverse tactile gestures, including
poke, grab, stroke, and double-pat. By mapping the recognized gestures to
predefined robot actions, we enable intuitive human-robot interaction purely
through tactile input.

</details>


### [399] [Learning Approach to Efficient Vision-based Active Tracking of a Flying Target by an Unmanned Aerial Vehicle](https://arxiv.org/abs/2506.18264)
*Jagadeswara PKV Pothuri,Aditya Bhatt,Prajit KrisshnaKumar,Manaswin Oddiraju,Souma Chowdhury*

Main category: cs.RO

TL;DR: The paper proposes a vision-based tracking system for UAVs, integrating deep learning and kernelized correlation filters for object detection and using reinforcement learning for maneuver decisions.


<details>
  <summary>Details</summary>
Motivation: Vision-based active tracking from airborne UAVs addresses the limitations of ground-based tracking, offering solutions for remote areas, crowded cities, and dense vegetation applications.

Method: The method integrates deep learning architectures with Kernelized Correlation Filters for efficient object detection and employs reinforcement learning to train a neuro-controller for optimizing UAV maneuvers.

Result: The framework is validated through lab experiments and AirSim simulations, showing better performance compared to PID controllers in maintaining tracking up-time and positioning accuracy.

Conclusion: Integrating learning-based perception with reinforcement learning enhances UAV tracking of aerial objects, providing improved maneuverability and detection efficiency.

Abstract: Autonomous tracking of flying aerial objects has important civilian and
defense applications, ranging from search and rescue to counter-unmanned aerial
systems (counter-UAS). Ground based tracking requires setting up
infrastructure, could be range limited, and may not be feasible in remote
areas, crowded cities or in dense vegetation areas. Vision based active
tracking of aerial objects from another airborne vehicle, e.g., a chaser
unmanned aerial vehicle (UAV), promises to fill this important gap, along with
serving aerial coordination use cases. Vision-based active tracking by a UAV
entails solving two coupled problems: 1) compute-efficient and accurate
(target) object detection and target state estimation; and 2) maneuver
decisions to ensure that the target remains in the field of view in the future
time-steps and favorably positioned for continued detection. As a solution to
the first problem, this paper presents a novel integration of standard deep
learning based architectures with Kernelized Correlation Filter (KCF) to
achieve compute-efficient object detection without compromising accuracy,
unlike standalone learning or filtering approaches. The proposed perception
framework is validated using a lab-scale setup. For the second problem, to
obviate the linearity assumptions and background variations limiting
effectiveness of the traditional controllers, we present the use of
reinforcement learning to train a neuro-controller for fast computation of
velocity maneuvers. New state space, action space and reward formulations are
developed for this purpose, and training is performed in simulation using
AirSim. The trained model is also tested in AirSim with respect to complex
target maneuvers, and is found to outperform a baseline PID control in terms of
tracking up-time and average distance maintained (from the target) during
tracking.

</details>


### [400] [Improvement on LiDAR-Camera Calibration Using Square Targets](https://arxiv.org/abs/2506.18294)
*Zhongyuan Li,Honggang Gou,Ping Li,Jiaotong Guo,Mao Ye*

Main category: cs.RO

TL;DR: This paper introduces a fully automatic LiDAR-camera extrinsic calibration algorithm designed to be quick, easily deployable, and robust to sensor noise, addressing challenges in factory pipelines and after-sales services.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicles require precise sensor calibration to ensure proper perception functionality. Errors in calibration can lead to significant object detection inaccuracies, posing safety risks. Existing multi-sensor calibration methods fail to address challenges in practical applications such as manufacturing pipelines and service scenarios.

Method: The proposed method includes: (1) an automatic multi-stage LiDAR board detection pipeline using only geometry; (2) a fast, robust coarse extrinsic parameter search mechanism; and (3) a direct optimization algorithm resilient to sensor noise.

Result: Experiments conducted on data from real-world scenarios demonstrate the effectiveness of the proposed calibration methods.

Conclusion: The algorithm provides a robust, fast, and practical solution for LiDAR-camera extrinsic calibration, making it suitable for factory and service scenarios, ensuring better safety and system accuracy.

Abstract: Precise sensor calibration is critical for autonomous vehicles as a
prerequisite for perception algorithms to function properly. Rotation error of
one degree can translate to position error of meters in target object detection
at large distance, leading to improper reaction of the system or even safety
related issues. Many methods for multi-sensor calibration have been proposed.
However, there are very few work that comprehensively consider the challenges
of the calibration procedure when applied to factory manufacturing pipeline or
after-sales service scenarios. In this work, we introduce a fully automatic
LiDAR-camera extrinsic calibration algorithm based on targets that is fast,
easy to deploy and robust to sensor noises such as missing data. The core of
the method include: (1) an automatic multi-stage LiDAR board detection pipeline
using only geometry information with no specific material requirement; (2) a
fast coarse extrinsic parameter search mechanism that is robust to initial
extrinsic errors; (3) a direct optimization algorithm that is robust to sensor
noises. We validate the effectiveness of our methods through experiments on
data captured in real world scenarios.

</details>


### [401] [TritonZ: A Remotely Operated Underwater Rover with Manipulator Arm for Exploration and Rescue Operations](https://arxiv.org/abs/2506.18343)
*Kawser Ahmed,Mir Shahriar Fardin,Md Arif Faysal Nayem,Fahim Hafiz,Swakkhar Shatabda*

Main category: cs.RO

TL;DR: The paper introduces "TritonZ," a semi-wireless underwater vehicle equipped with a manipulator arm and sensors for underwater exploration and rescue operations.


<details>
  <summary>Details</summary>
Motivation: The authors aim to create an advanced underwater vehicle that can navigate challenging terrains and perform tasks like exploration and rescue in emergency situations.

Method: TritonZ employs a compact design with a manipulator arm, a Pi-Camera for live streaming, and various environmental sensors. It is controlled via a custom remote controller.

Result: The vehicle achieved an average speed of 13.5cm/s with a delay of 2-3 seconds and demonstrated the ability to maintain stability even in waves while performing tasks.

Conclusion: TritonZ proves to be an effective solution for underwater exploration and rescue missions, providing real-time data and stable operation in submerged environments.

Abstract: The increasing demand for underwater exploration and rescue operations
enforces the development of advanced wireless or semi-wireless underwater
vessels equipped with manipulator arms. This paper presents the implementation
of a semi-wireless underwater vehicle, "TritonZ" equipped with a manipulator
arm, tailored for effective underwater exploration and rescue operations. The
vehicle's compact design enables deployment in different submarine
surroundings, addressing the need for wireless systems capable of navigating
challenging underwater terrains. The manipulator arm can interact with the
environment, allowing the robot to perform sophisticated tasks during
exploration and rescue missions in emergency situations. TritonZ is equipped
with various sensors such as Pi-Camera, Humidity, and Temperature sensors to
send real-time environmental data. Our underwater vehicle controlled using a
customized remote controller can navigate efficiently in the water where
Pi-Camera enables live streaming of the surroundings. Motion control and video
capture are performed simultaneously using this camera. The manipulator arm is
designed to perform various tasks, similar to grasping, manipulating, and
collecting underwater objects. Experimental results shows the efficacy of the
proposed remotely operated vehicle in performing a variety of underwater
exploration and rescue tasks. Additionally, the results show that TritonZ can
maintain an average of 13.5cm/s with a minimal delay of 2-3 seconds.
Furthermore, the vehicle can sustain waves underwater by maintaining its
position as well as average velocity. The full project details and source code
can be accessed at this link: https://github.com/kawser-ahmed-byte/TritonZ

</details>


### [402] [Robotic Manipulation of a Rotating Chain with Bottom End Fixed](https://arxiv.org/abs/2506.18355)
*Qi Jing Chen,Shilin Shan,Quang-Cuong Pham*

Main category: cs.RO

TL;DR: The paper examines a robot arm's manipulation of a uniformly rotating chain and proposes a stable method for transitioning between rotation modes.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of achieving stable and consistent rotational shape transitions in a rotating chain, which has practical importance in real-world applications like drill strings and yarn spinning.

Method: A manipulation strategy was developed by modeling the chain's configuration space as a 3D cube and utilizing its geometric properties to plan transitions between rotation modes while considering stability and feasibility.

Result: The proposed strategy was validated in physical experiments, successfully transitioning from rest to the first two rotation modes.

Conclusion: The strategy enables safe and efficient manipulation of rotating chains and holds potential for application in industrial operations requiring rotational stability.

Abstract: This paper studies the problem of using a robot arm to manipulate a uniformly
rotating chain with its bottom end fixed. Existing studies have investigated
ideal rotational shapes for practical applications, yet they do not discuss how
these shapes can be consistently achieved through manipulation planning. Our
work presents a manipulation strategy for stable and consistent shape
transitions. We find that the configuration space of such a chain is
homeomorphic to a three-dimensional cube. Using this property, we suggest a
strategy to manipulate the chain into different configurations, specifically
from one rotation mode to another, while taking stability and feasibility into
consideration. We demonstrate the effectiveness of our strategy in physical
experiments by successfully transitioning from rest to the first two rotation
modes. The concepts explored in our work has critical applications in ensuring
safety and efficiency of drill string and yarn spinning operations.

</details>


### [403] [Robots and Children that Learn Together : Improving Knowledge Retention by Teaching Peer-Like Interactive Robots](https://arxiv.org/abs/2506.18365)
*Imene Tarakli,Samuele Vinanzi,Richard Moore,Alessandro Di Nuovo*

Main category: cs.RO

TL;DR: The study explores Learning-by-Teaching (LbT) with autonomous social robots, showing that Interactive Reinforcement Learning improves children's retention and meta-cognitive engagement during language learning tasks.


<details>
  <summary>Details</summary>
Motivation: Address the gap in understanding real-time interactive learning using autonomous social robots in classrooms with an emphasis on Learning-by-Teaching.

Method: Two between-subject experiments were conducted involving 58 primary school children who either taught a robot using Interactive Reinforcement Learning or practiced independently on a tablet while learning French vocabulary and grammar.

Result: Children in the LbT condition achieved higher retention gains compared to self-practice, particularly in grammar tasks. Lower prior knowledge learners benefited more from teaching robots, and children adapted their teaching strategies over time, showing deeper engagement.

Conclusion: Interactive Reinforcement Learning provides a pedagogically effective and scalable model for social robots, demonstrating feasibility for deployment in classrooms. Robots can act as adaptive partners to enhance meta-cognitive engagement and learning outcomes.

Abstract: Despite growing interest in Learning-by-Teaching (LbT), few studies have
explored how this paradigm can be implemented with autonomous, peer-like social
robots in real classrooms. Most prior work has relied on scripted or
Wizard-of-Oz behaviors, limiting our understanding of how real-time,
interactive learning can be supported by artificial agents. This study
addresses this gap by introducing Interactive Reinforcement Learning (RL) as a
cognitive model for teachable social robots. We conducted two between-subject
experiments with 58 primary school children, who either taught a robot or
practiced independently on a tablet while learning French vocabulary
(memorization) and grammatical rules (inference). The robot, powered by
Interactive RL, learned from the child's evaluative feedback. Children in the
LbT condition achieved significantly higher retention gains compared to those
in the self-practice condition, especially on the grammar task. Learners with
lower prior knowledge benefited most from teaching the robot. Behavioural
metrics revealed that children adapted their teaching strategies over time and
engaged more deeply during inference tasks. This work makes two contributions:
(1) it introduces Interactive RL as a pedagogically effective and scalable
model for peer-robot learning, and (2) it demonstrates, for the first time, the
feasibility of deploying multiple autonomous robots simultaneously in real
classrooms. These findings extend theoretical understanding of LbT by showing
that social robots can function not only as passive tutees but as adaptive
partners that enhance meta-cognitive engagement and long-term learning
outcomes.

</details>


### [404] [Integrating Maneuverable Planning and Adaptive Control for Robot Cart-Pushing under Disturbances](https://arxiv.org/abs/2506.18410)
*Zhe Zhang,Peijia Xie,Zhirui Sun,Bingyi Xia,Bi-Ke Zhu,Jiankun Wang*

Main category: cs.RO

TL;DR: The paper addresses challenges in cart-pushing for mobile robots by proposing a planning and control framework that is validated through simulation and real-world tests for flexibility and robustness.


<details>
  <summary>Details</summary>
Motivation: Mobile robots face challenges in precise cart-pushing due to motion constraints, redundancy, variable payloads, and disturbances, creating complex planning and control problems.

Method: Their framework introduces a nonlinear optimization-based motion planning method using a local coordinate representation and a disturbance rejection control method for handling disturbances without relying on an accurate dynamic model.

Result: The proposed approach enhances motion maneuverability, generates feasible and flexible push poses, and demonstrates superior performance in both simulation and real-world experiments when compared to existing methods.

Conclusion: This work is the first to systematically evaluate and successfully demonstrate enhanced flexibility and robustness in cart-pushing tasks using an innovative planning and control framework.

Abstract: Precise and flexible cart-pushing is a challenging task for mobile robots.
The motion constraints during cart-pushing and the robot's redundancy lead to
complex motion planning problems, while variable payloads and disturbances
present complicated dynamics. In this work, we propose a novel planning and
control framework for flexible whole-body coordination and robust adaptive
control. Our motion planning method employs a local coordinate representation
and a novel kinematic model to solve a nonlinear optimization problem, thereby
enhancing motion maneuverability by generating feasible and flexible push
poses. Furthermore, we present a disturbance rejection control method to resist
disturbances and reduce control errors for the complex control problem without
requiring an accurate dynamic model. We validate our method through extensive
experiments in simulation and real-world settings, demonstrating its
superiority over existing approaches. To the best of our knowledge, this is the
first work to systematically evaluate the flexibility and robustness of
cart-pushing methods in experiments. The video supplement is available at
https://sites.google.com/view/mpac-pushing/.

</details>


### [405] [Radar and Event Camera Fusion for Agile Robot Ego-Motion Estimation](https://arxiv.org/abs/2506.18443)
*Yang Lyu,Zhenghao Zou,Yanfeng Li,Chunhui Zhao,Quan Pan*

Main category: cs.RO

TL;DR: The paper introduces an IMU-free, association-free framework using event cameras and millimeter wave radars for accurate ego-motion estimation in highly dynamic scenarios.


<details>
  <summary>Details</summary>
Motivation: Current sensors on agile robots struggle to provide reliable measurements in highly dynamic environments due to issues like blurring and delays, necessitating improved methods.

Method: The framework combines raw data from event cameras and millimeter wave radars without feature association and integrates them in a continuous-time state-space model to estimate velocities.

Result: Extensive experiments validate that the framework achieves reliable and efficient ego-motion velocity estimation in challenging datasets.

Conclusion: The proposed method is robust, computationally efficient, and suitable for edge devices, overcoming limitations in texture-less and structureless environments.

Abstract: Achieving reliable ego motion estimation for agile robots, e.g., aerobatic
aircraft, remains challenging because most robot sensors fail to respond timely
and clearly to highly dynamic robot motions, often resulting in measurement
blurring, distortion, and delays. In this paper, we propose an IMU-free and
feature-association-free framework to achieve aggressive ego-motion velocity
estimation of a robot platform in highly dynamic scenarios by combining two
types of exteroceptive sensors, an event camera and a millimeter wave radar,
First, we used instantaneous raw events and Doppler measurements to derive
rotational and translational velocities directly. Without a sophisticated
association process between measurement frames, the proposed method is more
robust in texture-less and structureless environments and is more
computationally efficient for edge computing devices. Then, in the back-end, we
propose a continuous-time state-space model to fuse the hybrid time-based and
event-based measurements to estimate the ego-motion velocity in a fixed-lagged
smoother fashion. In the end, we validate our velometer framework extensively
in self-collected experiment datasets. The results indicate that our IMU-free
and association-free ego motion estimation framework can achieve reliable and
efficient velocity output in challenging environments. The source code,
illustrative video and dataset are available at
https://github.com/ZzhYgwh/TwistEstimator.

</details>


### [406] [GraspMAS: Zero-Shot Language-driven Grasp Detection with Multi-Agent System](https://arxiv.org/abs/2506.18448)
*Quang Nguyen,Tri Le,Huy Nguyen,Thieu Vo,Tung D. Ta,Baoru Huang,Minh N. Vu,Anh Nguyen*

Main category: cs.RO

TL;DR: The paper introduces GraspMAS, a multi-agent framework for robots to better detect and execute language-directed grasping tasks, addressing challenges like complex instructions and dynamic environments without needing retraining.


<details>
  <summary>Details</summary>
Motivation: Current methods for language-directed robotic grasping face two issues: difficulty with complex instructions or cluttered environments and dependency on retraining when adapting to new domains.

Method: The GraspMAS framework introduces three specialized agents - Planner (strategizes queries), Coder (executes source code), and Observer (evaluates feedback) - to handle ambiguities and complex scenarios in language-based grasping tasks.

Result: Experiments on two large datasets show GraspMAS outperforms existing systems, and additional robot experiments in both simulations and real-world setups further demonstrate its effectiveness.

Conclusion: GraspMAS enables improved decision-making and reasoning for language-driven grasp detection, offering a generalizable and effective solution for real-world human-robot interactions.

Abstract: Language-driven grasp detection has the potential to revolutionize
human-robot interaction by allowing robots to understand and execute grasping
tasks based on natural language commands. However, existing approaches face two
key challenges. First, they often struggle to interpret complex text
instructions or operate ineffectively in densely cluttered environments.
Second, most methods require a training or finetuning step to adapt to new
domains, limiting their generation in real-world applications. In this paper,
we introduce GraspMAS, a new multi-agent system framework for language-driven
grasp detection. GraspMAS is designed to reason through ambiguities and improve
decision-making in real-world scenarios. Our framework consists of three
specialized agents: Planner, responsible for strategizing complex queries;
Coder, which generates and executes source code; and Observer, which evaluates
the outcomes and provides feedback. Intensive experiments on two large-scale
datasets demonstrate that our GraspMAS significantly outperforms existing
baselines. Additionally, robot experiments conducted in both simulation and
real-world settings further validate the effectiveness of our approach.

</details>


### [407] [A Motivational Architecture for Open-Ended Learning Challenges in Robots](https://arxiv.org/abs/2506.18454)
*Alejandro Romero,Gianluca Baldassarre,Richard J. Duro,Vieri Giuliano Santucci*

Main category: cs.RO

TL;DR: The paper introduces H-GRAIL, a hierarchical architecture for autonomous agent learning in complex environments by discovering goals, learning skills, and adapting.


<details>
  <summary>Details</summary>
Motivation: Developing artificial systems capable of autonomous learning and adaptation in dynamic, unpredictable, and non-stationary environments for real-world applications.

Method: The authors propose H-GRAIL, a hierarchical framework employing intrinsic motivations and interconnected mechanisms to discover goals, learn skills, handle interdependent tasks, and adapt dynamically.

Result: H-GRAIL was tested in a real robotic scenario, and results demonstrated its effectiveness in open-ended learning environments.

Conclusion: H-GRAIL successfully addresses core challenges in open-ended learning, including goal discovery, skill acquisition, and adaptability, showcasing potential for real-world deployment.

Abstract: Developing agents capable of autonomously interacting with complex and
dynamic environments, where task structures may change over time and prior
knowledge cannot be relied upon, is a key prerequisite for deploying artificial
systems in real-world settings. The open-ended learning framework identifies
the core challenges for creating such agents, including the ability to
autonomously generate new goals, acquire the necessary skills (or curricula of
skills) to achieve them, and adapt to non-stationary environments. While many
existing works tackles various aspects of these challenges in isolation, few
propose integrated solutions that address them simultaneously. In this paper,
we introduce H-GRAIL, a hierarchical architecture that, through the use of
different typologies of intrinsic motivations and interconnected learning
mechanisms, autonomously discovers new goals, learns the required skills for
their achievement, generates skill sequences for tackling interdependent tasks,
and adapts to non-stationary environments. We tested H-GRAIL in a real robotic
scenario, demonstrating how the proposed solutions effectively address the
various challenges of open-ended learning.

</details>


### [408] [Mirror Eyes: Explainable Human-Robot Interaction at a Glance](https://arxiv.org/abs/2506.18466)
*Matti Krüger,Daniel Tanneberg,Chao Wang,Stephan Hasler,Michael Gienger*

Main category: cs.RO

TL;DR: This paper investigates how gaze reflection in robot eyes aids human-robot interaction during task execution and issue detection.


<details>
  <summary>Details</summary>
Motivation: To enhance human-robot interaction by leveraging intuitive gaze-based feedback systems.

Method: Developed robot system with gaze-reflective eyes and conducted a user study where participants performed tasks with the robot.

Result: Participants were more aware of the robot’s processing, detected errors quicker, and reported better user experience with gaze-reflective eyes.

Conclusion: Gaze-reflective robot eyes offer intuitive benefits and improve cooperative human-robot interaction.

Abstract: The gaze of a person tends to reflect their interest. This work explores what
happens when this statement is taken literally and applied to robots. Here we
present a robot system that employs a moving robot head with a screen-based eye
model that can direct the robot's gaze to points in physical space and present
a reflection-like mirror image of the attended region on top of each eye. We
conducted a user study with 33 participants, who were asked to instruct the
robot to perform pick-and-place tasks, monitor the robot's task execution, and
interrupt it in case of erroneous actions. Despite a deliberate lack of
instructions about the role of the eyes and a very brief system exposure,
participants felt more aware about the robot's information processing, detected
erroneous actions earlier, and rated the user experience higher when eye-based
mirroring was enabled compared to non-reflective eyes. These results suggest a
beneficial and intuitive utilization of the introduced method in cooperative
human-robot interaction.

</details>


### [409] [Design, fabrication and control of a cable-driven parallel robot](https://arxiv.org/abs/2506.18526)
*Dhruv Sorathiya,Sarthak Sahoo,Vivek Natarajan*

Main category: cs.RO

TL;DR: This paper presents the development of an experimental setup for a cable-driven parallel robot (CDPR) with three cables, including its design, fabrication, and testing of basic motion planning algorithms.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this study is to address the complexities in the dynamics of CDPRs due to cable flexibility and unidirectional force (pull only). Advanced modeling and control systems, along with experimental validation, are required to maximize the potential of CDPRs.

Method: The authors designed and fabricated an experimental setup for a three-cable CDPR to study its dynamics and validate open-loop motion planning algorithms. They detail component selection, assembly processes, and setup features.

Result: The setup successfully replicates intricate phenomena such as transverse cable vibrations observed in large-scale CDPRs. Elementary motion planning techniques were tested and validated.

Conclusion: The experimental setup provides a robust platform for further research into complex dynamics and control strategies for CDPRs, including advanced motion planning and modeling of phenomena like cable vibrations.

Abstract: In cable driven parallel robots (CDPRs), the payload is suspended using a
network of cables whose length can be controlled to maneuver the payload within
the workspace. Compared to rigid link robots, CDPRs provide better
maneuverability due to the flexibility of the cables and consume lesser power
due to the high strength-to-weight ratio of the cables. However, amongst other
things, the flexibility of the cables and the fact that they can only pull (and
not push) render the dynamics of CDPRs complex. Hence advanced modelling
paradigms and control algorithms must be developed to fully utilize the
potential of CDPRs. Furthermore, given the complex dynamics of CDPRs, the
models and control algorithms proposed for them must be validated on
experimental setups to ascertain their efficacy in practice. We have recently
developed an elaborate experimental setup for a CDPR with three cables and
validated elementary open-loop motion planning algorithms on it. In this paper,
we describe several aspects of the design and fabrication of our setup,
including component selection and assembly, and present our experimental
results. Our setup can reproduce complex phenomenon such as the transverse
vibration of the cables seen in large CDPRs and will in the future be used to
model and control such phenomenon and also to validate more sophisticated
motion planning algorithms.

</details>


### [410] [Learning Point Correspondences In Radar 3D Point Clouds For Radar-Inertial Odometry](https://arxiv.org/abs/2506.18580)
*Jan Michalczyk,Stephan Weiss,Jan Steinbrener*

Main category: cs.RO

TL;DR: This paper introduces a novel learning-based method leveraging transformer architecture to estimate robust correspondences in noisy, sparse 3D point clouds from low-cost radar sensors, achieving over 14-19% improvement in position estimation accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in the challenge of finding correspondences in 3D point clouds for low-quality, noisy, sparse data, where traditional state-of-the-art methods fail.

Method: The method uses a transformer-based neural network trained in a self-supervised manner with multi-label classification loss, targeting point correspondences directly instead of odometry error. Ground-truth matches are generated via Linear Sum Assignment optimization.

Result: The method improves the position estimation accuracy in real-world UAV flights and the public Coloradar dataset by over 14% and 19% on average, respectively.

Conclusion: This approach demonstrates the efficacy of leveraging self-supervised learning and transformer-based architectures for improving odometry estimation from low-quality radar data, providing an open-source framework for future research.

Abstract: Using 3D point clouds in odometry estimation in robotics often requires
finding a set of correspondences between points in subsequent scans. While
there are established methods for point clouds of sufficient quality,
state-of-the-art still struggles when this quality drops. Thus, this paper
presents a novel learning-based framework for predicting robust point
correspondences between pairs of noisy, sparse and unstructured 3D point clouds
from a light-weight, low-power, inexpensive, consumer-grade System-on-Chip
(SoC) Frequency Modulated Continuous Wave (FMCW) radar sensor. Our network is
based on the transformer architecture which allows leveraging the attention
mechanism to discover pairs of points in consecutive scans with the greatest
mutual affinity. The proposed network is trained in a self-supervised way using
set-based multi-label classification cross-entropy loss, where the ground-truth
set of matches is found by solving the Linear Sum Assignment (LSA) optimization
problem, which avoids tedious hand annotation of the training data.
Additionally, posing the loss calculation as multi-label classification permits
supervising on point correspondences directly instead of on odometry error,
which is not feasible for sparse and noisy data from the SoC radar we use. We
evaluate our method with an open-source state-of-the-art Radar-Inertial
Odometry (RIO) framework in real-world Unmanned Aerial Vehicle (UAV) flights
and with the widely used public Coloradar dataset. Evaluation shows that the
proposed method improves the position estimation accuracy by over 14 % and 19 %
on average, respectively. The open source code and datasets can be found here:
https://github.com/aau-cns/radar_transformer.

</details>


### [411] [PG-LIO: Photometric-Geometric fusion for Robust LiDAR-Inertial Odometry](https://arxiv.org/abs/2506.18583)
*Nikhil Khedekar,Kostas Alexis*

Main category: cs.RO

TL;DR: The paper introduces PG-LIO, a robust LiDAR-Inertial Odometry method that incorporates both photometric and geometric data with IMU constraints to improve state estimation and mapping in challenging geometric conditions.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of conventional LIO methods that struggle and become ill-conditioned in geometrically degenerate environments, ensuring broader deployment and reliability of LIO in autonomous robots.

Method: PG-LIO fuses photometric and geometric information from LiDAR with IMU constraints using a factor graph optimized over a sliding window for real-time operation.

Result: PG-LIO achieves accuracy comparable to state-of-the-art methods in well-conditioned environments and significantly outperforms others in geometrically degenerate scenarios. It demonstrated a drift of only 1 meter over a 1 km aerial trajectory in a self-similar tunnel at speeds up to 10.8 m/s.

Conclusion: PG-LIO enhances the robustness and accuracy of LIO in challenging environments, showcasing its potential for broader deployment. The source code is also made available to benefit the community.

Abstract: LiDAR-Inertial Odometry (LIO) is widely used for accurate state estimation
and mapping which is an essential requirement for autonomous robots.
Conventional LIO methods typically rely on formulating constraints from the
geometric structure sampled by the LiDAR. Hence, in the lack of geometric
structure, these tend to become ill-conditioned (degenerate) and fail.
Robustness of LIO to such conditions is a necessity for its broader deployment.
To address this, we propose PG-LIO, a real-time LIO method that fuses
photometric and geometric information sampled by the LiDAR along with inertial
constraints from an Inertial Measurement Unit (IMU). This multi-modal
information is integrated into a factor graph optimized over a sliding window
for real-time operation. We evaluate PG-LIO on multiple datasets that include
both geometrically well-conditioned as well as self-similar scenarios. Our
method achieves accuracy on par with state-of-the-art LIO in geometrically
well-structured settings while significantly improving accuracy in degenerate
cases including against methods that also fuse intensity. Notably, we
demonstrate only 1 m drift over a 1 km manually piloted aerial trajectory
through a geometrically self-similar tunnel at an average speed of 7.5m/s (max
speed 10.8 m/s). For the benefit of the community, we shall also release our
source code https://github.com/ntnu-arl/mimosa

</details>


### [412] [NOVA: Navigation via Object-Centric Visual Autonomy for High-Speed Target Tracking in Unstructured GPS-Denied Environments](https://arxiv.org/abs/2506.18689)
*Alessandro Saviolo,Giuseppe Loianno*

Main category: cs.RO

TL;DR: The paper proposes NOVA, an onboard, object-centric aerial target tracking framework operating in unstructured, GPS-denied environments using stereo cameras and IMU. The system avoids reliance on maps or external localization.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of existing methods in aerial target tracking which rely on motion capture systems, pre-mapped scenes, or feature-based localization, restricting deployment in real-world, GPS-denied environments.

Method: NOVA uses stereo cameras and an IMU for perception, estimation, and control in the target's reference frame. A lightweight object detector, depth completion, and histogram filtering infer target distance, while a visual-inertial state estimator calculates the robot's pose. A nonlinear model predictive controller plans trajectories, and collision-aware navigation is achieved using control barrier functions from extracted depth data.

Result: NOVA demonstrated consistent performance in complex real-world environments, including urban mazes and forests, handling GPS loss and lighting issues, with the capability to follow targets at speeds over 50 km/h.

Conclusion: NOVA shows that high-speed, vision-based aerial target tracking is feasible in unstructured environments with onboard sensing, eliminating the need for external localization or prior environmental knowledge.

Abstract: Autonomous aerial target tracking in unstructured and GPS-denied environments
remains a fundamental challenge in robotics. Many existing methods rely on
motion capture systems, pre-mapped scenes, or feature-based localization to
ensure safety and control, limiting their deployment in real-world conditions.
We introduce NOVA, a fully onboard, object-centric framework that enables
robust target tracking and collision-aware navigation using only a stereo
camera and an IMU. Rather than constructing a global map or relying on absolute
localization, NOVA formulates perception, estimation, and control entirely in
the target's reference frame. A tightly integrated stack combines a lightweight
object detector with stereo depth completion, followed by histogram-based
filtering to infer robust target distances under occlusion and noise. These
measurements feed a visual-inertial state estimator that recovers the full
6-DoF pose of the robot relative to the target. A nonlinear model predictive
controller (NMPC) plans dynamically feasible trajectories in the target frame.
To ensure safety, high-order control barrier functions are constructed online
from a compact set of high-risk collision points extracted from depth, enabling
real-time obstacle avoidance without maps or dense representations. We validate
NOVA across challenging real-world scenarios, including urban mazes, forest
trails, and repeated transitions through buildings with intermittent GPS loss
and severe lighting changes that disrupt feature-based localization. Each
experiment is repeated multiple times under similar conditions to assess
resilience, showing consistent and reliable performance. NOVA achieves agile
target following at speeds exceeding 50 km/h. These results show that
high-speed vision-based tracking is possible in the wild using only onboard
sensing, with no reliance on external localization or environment assumptions.

</details>


### [413] [Safety-Aware Optimal Scheduling for Autonomous Masonry Construction using Collaborative Heterogeneous Aerial Robots](https://arxiv.org/abs/2506.18697)
*Marios-Nektarios Stamatopoulos,Shridhar Velhal,Avijit Banerjee,George Nikolakopoulos*

Main category: cs.RO

TL;DR: The paper introduces a planning framework for autonomous drone construction teams, optimizing timing and safety in aerial masonry tasks.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address challenges in scheduling and coordinating autonomous drones for masonry construction, particularly considering safety and mortar curing deadlines.

Method: An automated pipeline generates wall construction plans incorporating curing deadlines, static dependencies, and spatio-temporal safety constraints. The framework optimizes UAV task allocation and timing, minimizing construction time.

Result: Validation in Gazebo simulations showed efficiency in coordinated drone operations for masonry tasks, demonstrating enhanced structural integrity and operational safety.

Conclusion: The framework successfully optimizes and ensures safety and efficiency in aerial masonry construction, making it a viable solution for autonomous robotic construction systems.

Abstract: This paper presents a novel high-level task planning and optimal coordination
framework for autonomous masonry construction, using a team of heterogeneous
aerial robotic workers, consisting of agents with separate skills for brick
placement and mortar application. This introduces new challenges in scheduling
and coordination, particularly due to the mortar curing deadline required for
structural bonding and ensuring the safety constraints among UAVs operating in
parallel. To address this, an automated pipeline generates the wall
construction plan based on the available bricks while identifying static
structural dependencies and potential conflicts for safe operation. The
proposed framework optimizes UAV task allocation and execution timing by
incorporating dynamically coupled precedence deadline constraints that account
for the curing process and static structural dependency constraints, while
enforcing spatio-temporal constraints to prevent collisions and ensure safety.
The primary objective of the scheduler is to minimize the overall construction
makespan while minimizing logistics, traveling time between tasks, and the
curing time to maintain both adhesion quality and safe workspace separation.
The effectiveness of the proposed method in achieving coordinated and
time-efficient aerial masonry construction is extensively validated through
Gazebo simulated missions. The results demonstrate the framework's capability
to streamline UAV operations, ensuring both structural integrity and safety
during the construction process.

</details>


### [414] [TDACloud: Point Cloud Recognition Using Topological Data Analysis](https://arxiv.org/abs/2506.18725)
*Anirban Ghosh,Ian Dahlin,Ayan Dutta*

Main category: cs.RO

TL;DR: The paper introduces TDACloud, a novel method using Topological Data Analysis for deriving robust local descriptors in raw point clouds, achieving significant accuracy improvements in object and place recognition.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of extracting meaningful local descriptors from raw point clouds, especially when the data is noisy or transformed, to enhance object and place recognition in various applications like autonomous driving.

Method: The proposed method, TDACloud, uses Topological Data Analysis (TDA) through the ATOL vectorization technique to process raw point clouds into fixed-size TDA-descriptor vectors, eliminating the need for GPU-heavy machine learning models.

Result: TDACloud demonstrated high recognition accuracy on multiple real-world and synthetic datasets, including Oxford RobotCar, KITTI-360, and ShapeNet, outperforming baseline methods by up to 14%, even under noisy and transformed test conditions.

Conclusion: TDACloud effectively provides a resource-efficient and robust solution for point cloud-based object and place recognition, showing superior performance compared to existing methods, particularly in challenging scenarios.

Abstract: Point cloud-based object/place recognition remains a problem of interest in
applications such as autonomous driving, scene reconstruction, and
localization. Extracting meaningful local descriptors from a query point cloud
that can be matched with the descriptors of the collected point clouds is a
challenging problem. Furthermore, when the query point cloud is noisy or has
been transformed (e.g., rotated), it adds to the complexity. To this end, we
propose a novel methodology, named TDACloud, using Topological Data Analysis
(TDA) for local descriptor extraction from a point cloud, which does not need
resource-intensive GPU-based machine learning training. More specifically, we
used the ATOL vectorization method to generate vectors for point clouds. Unlike
voxelization, our proposed technique can take raw point clouds as inputs and
outputs a fixed-size TDA-descriptor vector. To test the quality of the proposed
TDACloud technique, we have implemented it on multiple real-world (e.g., Oxford
RobotCar, KITTI-360) and realistic (e.g., ShapeNet) point cloud datasets for
object and place recognition. We have also tested TDACloud on noisy and
transformed test cases where the query point cloud has been scaled, translated,
or rotated. Our results demonstrate high recognition accuracies in noisy
conditions and large-scale real-world place recognition while outperforming the
baselines by up to approximately 14%.

</details>


### [415] [DefFusionNet: Learning Multimodal Goal Shapes for Deformable Object Manipulation via a Diffusion-based Probabilistic Model](https://arxiv.org/abs/2506.18779)
*Bao Thach,Siyeon Kim,Britton Jordan,Mohanraj Shanthi,Tanner Watts,Shing-Hei Ho,James M. Ferguson,Tucker Hermans,Alan Kuntz*

Main category: cs.RO

TL;DR: DefFusionNet introduces a generative neural network leveraging a diffusion probabilistic model to address shape servoing challenges for deformable object manipulation, allowing for diverse and multi-modal goal shapes.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of existing shape servoing methods, particularly the inability of deterministic models like DefGoalNet to handle multi-modal goal settings effectively.

Method: Propose DefFusionNet, a neural network utilizing diffusion probabilistic models to learn distributions over valid goal shapes, enabling generation of diverse goal shapes.

Result: DefFusionNet demonstrated effectiveness in both simulated and real-world applications involving manufacturing and surgical-inspired tasks.

Conclusion: This work presents the first generative model for creating multi-modal deformable object goals, offering advancements in robotic applications requiring complex shape manipulation.

Abstract: Deformable object manipulation is critical to many real-world robotic
applications, ranging from surgical robotics and soft material handling in
manufacturing to household tasks like laundry folding. At the core of this
important robotic field is shape servoing, a task focused on controlling
deformable objects into desired shapes. The shape servoing formulation requires
the specification of a goal shape. However, most prior works in shape servoing
rely on impractical goal shape acquisition methods, such as laborious
domain-knowledge engineering or manual manipulation. DefGoalNet previously
posed the current state-of-the-art solution to this problem, which learns
deformable object goal shapes directly from a small number of human
demonstrations. However, it significantly struggles in multi-modal settings,
where multiple distinct goal shapes can all lead to successful task completion.
As a deterministic model, DefGoalNet collapses these possibilities into a
single averaged solution, often resulting in an unusable goal. In this paper,
we address this problem by developing DefFusionNet, a novel neural network that
leverages the diffusion probabilistic model to learn a distribution over all
valid goal shapes rather than predicting a single deterministic outcome. This
enables the generation of diverse goal shapes and avoids the averaging
artifacts. We demonstrate our method's effectiveness on robotic tasks inspired
by both manufacturing and surgical applications, both in simulation and on a
physical robot. Our work is the first generative model capable of producing a
diverse, multi-modal set of deformable object goals for real-world robotic
applications.

</details>


### [416] [Learning Physical Systems: Symplectification via Gauge Fixing in Dirac Structures](https://arxiv.org/abs/2506.18812)
*Aristotelis Papatheodorou,Pranav Vaidhyanathan,Natalia Ares,Ioannis Havoutis*

Main category: cs.RO

TL;DR: This paper introduces Presymplectification Networks (PSNs) to address the degeneration of canonical symplectic forms in systems with dissipation and constraints. This method preserves energy, momentum, and constraints for accurate trajectory forecasting.


<details>
  <summary>Details</summary>
Motivation: Existing physics-informed deep learning methods struggle with dissipative and constrained systems, where the degeneracy of symplectic forms harms stability and long-term predictions. The goal is to overcome this limitation and enable accurate modeling of such systems.

Method: The authors propose PSNs, which use Dirac structures to restore non-degenerate symplectic geometry by embedding constrained systems into a higher-dimensional manifold. This system consists of a recurrent encoder for phase-space dynamics and a lightweight Symplectic Network (SympNet) for trajectory forecasting.

Result: The framework is validated on the dynamics of the ANYmal quadruped robot, showcasing success in constrained, dissipative mechanical systems. PSNs maintain energy, momentum, and constraints while offering accurate predictions.

Conclusion: PSNs are the first approach to bridge the gap between constrained, dissipative systems and symplectic learning. By combining first principles and data adaptability, this framework paves the way for advanced geometric machine learning in various mechanical systems.

Abstract: Physics-informed deep learning has achieved remarkable progress by embedding
geometric priors, such as Hamiltonian symmetries and variational principles,
into neural networks, enabling structure-preserving models that extrapolate
with high accuracy. However, in systems with dissipation and holonomic
constraints, ubiquitous in legged locomotion and multibody robotics, the
canonical symplectic form becomes degenerate, undermining the very invariants
that guarantee stability and long-term prediction. In this work, we tackle this
foundational limitation by introducing Presymplectification Networks (PSNs),
the first framework to learn the symplectification lift via Dirac structures,
restoring a non-degenerate symplectic geometry by embedding constrained systems
into a higher-dimensional manifold. Our architecture combines a recurrent
encoder with a flow-matching objective to learn the augmented phase-space
dynamics end-to-end. We then attach a lightweight Symplectic Network (SympNet)
to forecast constrained trajectories while preserving energy, momentum, and
constraint satisfaction. We demonstrate our method on the dynamics of the
ANYmal quadruped robot, a challenging contact-rich, multibody system. To the
best of our knowledge, this is the first framework that effectively bridges the
gap between constrained, dissipative mechanical systems and symplectic
learning, unlocking a whole new class of geometric machine learning models,
grounded in first principles yet adaptable from data.

</details>


### [417] [SViP: Sequencing Bimanual Visuomotor Policies with Object-Centric Motion Primitives](https://arxiv.org/abs/2506.18825)
*Yizhou Chen,Hang Xu,Dongjie Yu,Zeqing Zhang,Yi Ren,Jia Pan*

Main category: cs.RO

TL;DR: SViP enhances imitation learning for bimanual manipulation by integrating it with task and motion planning (TAMP), enabling better generalization with only 20 demonstrations.


<details>
  <summary>Details</summary>
Motivation: To address the limited generalization and error accumulation of visuomotor policies in imitation learning for complex tasks with small datasets.

Method: SViP employs a semantic scene graph monitor to partition demonstrations into bimanual and unimanual operations. A switching condition generator is trained to produce parameterized scripted primitives for robust out-of-distribution performance.

Result: SViP generalizes visual policies across out-of-distribution initial conditions and enables unseen task solutions, outperforming generative imitation learning methods in real-world experiments.

Conclusion: SViP succeeds in improving the reliability and applicability of visuomotor policies in bimanual manipulation, even for unseen and complex tasks.

Abstract: Imitation learning (IL), particularly when leveraging high-dimensional visual
inputs for policy training, has proven intuitive and effective in complex
bimanual manipulation tasks. Nonetheless, the generalization capability of
visuomotor policies remains limited, especially when small demonstration
datasets are available. Accumulated errors in visuomotor policies significantly
hinder their ability to complete long-horizon tasks. To address these
limitations, we propose SViP, a framework that seamlessly integrates visuomotor
policies into task and motion planning (TAMP). SViP partitions human
demonstrations into bimanual and unimanual operations using a semantic scene
graph monitor. Continuous decision variables from the key scene graph are
employed to train a switching condition generator. This generator produces
parameterized scripted primitives that ensure reliable performance even when
encountering out-of-the-distribution observations. Using only 20 real-world
demonstrations, we show that SViP enables visuomotor policies to generalize
across out-of-distribution initial conditions without requiring object pose
estimators. For previously unseen tasks, SViP automatically discovers effective
solutions to achieve the goal, leveraging constraint modeling in TAMP
formulism. In real-world experiments, SViP outperforms state-of-the-art
generative IL methods, indicating wider applicability for more complex tasks.
Project website: https://sites.google.com/view/svip-bimanual

</details>


### [418] [Reproducible Evaluation of Camera Auto-Exposure Methods in the Field: Platform, Benchmark and Lessons Learned](https://arxiv.org/abs/2506.18844)
*Olivier Gamache,Jean-Michel Fortin,Matěj Boxan,François Pomerleau,Philippe Giguère*

Main category: cs.RO

TL;DR: The paper introduces a methodology and dataset called BorealHDR with an emulator for generating images at different exposure times, facilitating reproducible benchmarks for Automatic-Exposure (AE) methods.


<details>
  <summary>Details</summary>
Motivation: Traditional datasets and benchmarks for Automatic-Exposure (AE) methods struggle with reproducibility due to online evaluation and fixed sensor parameters. A need exists for tools and datasets to systematically assess how AE methods respond to varying environmental conditions and changing exposure times.

Method: The authors introduce BorealHDR, a multi-exposure stereo dataset with images acquired during varying times and lighting, along with an emulator for generating realistic images with controllable exposure times. They evaluated their approach using RMSE comparisons and benchmarked eight AE methods.

Result: Their emulator generated images with an RMSE below 1.78% compared to ground truth images. Classical AE methods were found to outperform others in benchmarks conducted on the dataset and emulator.

Conclusion: The newly proposed dataset and emulator not only enhance reproducibility but also provide a robust framework for evaluating AE methods. Insights from their backpack acquisition platform were shared, and their code and dataset were made public to support further research.

Abstract: Standard datasets often present limitations, particularly due to the fixed
nature of input data sensors, which makes it difficult to compare methods that
actively adjust sensor parameters to suit environmental conditions. This is the
case with Automatic-Exposure (AE) methods, which rely on environmental factors
to influence the image acquisition process. As a result, AE methods have
traditionally been benchmarked in an online manner, rendering experiments
non-reproducible. Building on our prior work, we propose a methodology that
utilizes an emulator capable of generating images at any exposure time. This
approach leverages BorealHDR, a unique multi-exposure stereo dataset, along
with its new extension, in which data was acquired along a repeated trajectory
at different times of the day to assess the impact of changing illumination. In
total, BorealHDR covers 13.4 km over 59 trajectories in challenging lighting
conditions. The dataset also includes lidar-inertial-odometry-based maps with
pose estimation for each image frame, as well as Global Navigation Satellite
System (GNSS) data for comparison. We demonstrate that by using images acquired
at various exposure times, we can emulate realistic images with a
Root-Mean-Square Error (RMSE) below 1.78% compared to ground truth images.
Using this offline approach, we benchmarked eight AE methods, concluding that
the classical AE method remains the field's best performer. To further support
reproducibility, we provide in-depth details on the development of our backpack
acquisition platform, including hardware, electrical components, and
performance specifications. Additionally, we share valuable lessons learned
from deploying the backpack over more than 25 km across various environments.
Our code and dataset are available online at this link:
https://github.com/norlab-ulaval/TFR24 BorealHDR

</details>


### [419] [GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale Multi-Agent Gaussian SLAM](https://arxiv.org/abs/2506.18885)
*Annika Thomas,Aneesa Sonawalla,Alex Rose,Jonathan P. How*

Main category: cs.RO

TL;DR: The paper introduces GRAND-SLAM, a multi-agent Gaussian splatting SLAM system, emphasizing collaborative mapping and enhanced tracking in diverse environments.


<details>
  <summary>Details</summary>
Motivation: Existing 3D Gaussian splatting methods are effective in smaller indoor environments but are not adapted for large-scale, multi-agent outdoor scenarios.

Method: Developing GRAND-SLAM, incorporating implicit tracking with submap-based local optimization, pose-graph optimization for loop closure across agents, and scalable Gaussian-based representations.

Result: GRAND-SLAM achieves state-of-the-art tracking, 28% higher PSNR on Replica dataset, significantly reduced multi-agent tracking error (91% improvement), and superior rendering on outdoor Kimera-Multi dataset.

Conclusion: The proposed GRAND-SLAM approach demonstrates scalability and effectiveness, excelling in both indoor and large-scale outdoor multi-agent environments.

Abstract: 3D Gaussian splatting has emerged as an expressive scene representation for
RGB-D visual SLAM, but its application to large-scale, multi-agent outdoor
environments remains unexplored. Multi-agent Gaussian SLAM is a promising
approach to rapid exploration and reconstruction of environments, offering
scalable environment representations, but existing approaches are limited to
small-scale, indoor environments. To that end, we propose Gaussian
Reconstruction via Multi-Agent Dense SLAM, or GRAND-SLAM, a collaborative
Gaussian splatting SLAM method that integrates i) an implicit tracking module
based on local optimization over submaps and ii) an approach to inter- and
intra-robot loop closure integrated into a pose-graph optimization framework.
Experiments show that GRAND-SLAM provides state-of-the-art tracking performance
and 28% higher PSNR than existing methods on the Replica indoor dataset, as
well as 91% lower multi-agent tracking error and improved rendering over
existing multi-agent methods on the large-scale, outdoor Kimera-Multi dataset.

</details>


### [420] [MinD: Unified Visual Imagination and Control via Hierarchical World Models](https://arxiv.org/abs/2506.18897)
*Xiaowei Chi,Kuangzhi Ge,Jiaming Liu,Siyuan Zhou,Peidong Jia,Zichen He,Yuzhen Liu,Tingguang Li,Lei Han,Sirui Han,Shanghang Zhang,Yike Guo*

Main category: cs.RO

TL;DR: The paper presents Manipulate in Dream (MinD), a hierarchical framework to improve video generation models (VGMs) for robotic manipulation by tackling slow speed and action-video inconsistency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome limitations in existing VGMs, namely slow generation speed and poor coherence between generated videos and executable actions, hindering robotics applications.

Method: MinD employs a dual-system hierarchical diffusion framework with low-frequency VGMs for video predictions and high-frequency diffusion policies for real-time actions, supplemented by a DiffMatcher module for better coordination.

Result: MinD achieves state-of-the-art performance in robotic manipulation (63%+ success rate in RL-Bench) and demonstrates robust task feasibility prediction and risk mitigation.

Conclusion: MinD bridges the gap in unified world modeling by ensuring efficient and consistent video-guided robotic manipulation, advancing VGMs for broader practical applications.

Abstract: Video generation models (VGMs) offer a promising pathway for unified world
modeling in robotics by integrating simulation, prediction, and manipulation.
However, their practical application remains limited due to (1) slowgeneration
speed, which limits real-time interaction, and (2) poor consistency between
imagined videos and executable actions. To address these challenges, we propose
Manipulate in Dream (MinD), a hierarchical diffusion-based world model
framework that employs a dual-system design for vision-language manipulation.
MinD executes VGM at low frequencies to extract video prediction features,
while leveraging a high-frequency diffusion policy for real-time interaction.
This architecture enables low-latency, closed-loop control in manipulation with
coherent visual guidance. To better coordinate the two systems, we introduce a
video-action diffusion matching module (DiffMatcher), with a novel co-training
strategy that uses separate schedulers for each diffusion model. Specifically,
we introduce a diffusion-forcing mechanism to DiffMatcher that aligns their
intermediate representations during training, helping the fast action model
better understand video-based predictions. Beyond manipulation, MinD also
functions as a world simulator, reliably predicting task success or failure in
latent space before execution. Trustworthy analysis further shows that VGMs can
preemptively evaluate task feasibility and mitigate risks. Extensive
experiments across multiple benchmarks demonstrate that MinD achieves
state-of-the-art manipulation (63%+) in RL-Bench, advancing the frontier of
unified world modeling in robotics.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [421] [Challenges and Practices in Quantum Software Testing and Debugging: Insights from Practitioners](https://arxiv.org/abs/2506.17306)
*Jake Zappin,Trevor Stalnaker,Oscar Chaparro,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: This study examines challenges in quantum software development, highlighting testing and debugging issues and the lack of quantum-specific tools.


<details>
  <summary>Details</summary>
Motivation: Address gaps between classical testing/debugging practices and quantum-specific needs during the evolution of quantum software engineering.

Method: The authors conducted surveys and follow-up interviews with 26 quantum software developers from academia and industry to capture current practices and challenges.

Result: Findings revealed a reliance on classical tools for debugging and testing, underutilization of quantum-specific tools (31% usage), and frequent bug sources like library updates and developer mistakes.

Conclusion: There is an urgent need for enhanced quantum-specific testing/debugging tools integrated into quantum developers’ workflows, which remain heavily reliant on classical counterparts.

Abstract: Quantum software engineering is an emerging discipline with distinct
challenges, particularly in testing and debugging. As quantum computing
transitions from theory to implementation, developers face issues not present
in classical software development, such as probabilistic execution, limited
observability, shallow abstractions, and low awareness of quantum-specific
tools. To better understand current practices, we surveyed 26 quantum software
developers from academia and industry and conducted follow-up interviews
focused on testing, debugging, and recurring challenges. All participants
reported engaging in testing, with unit testing (88%), regression testing
(54%), and acceptance testing (54%) being the most common. However, only 31%
reported using quantum-specific testing tools, relying instead on manual
methods. Debugging practices were similarly grounded in classical strategies,
such as print statements, circuit visualizations, and simulators, which
respondents noted do not scale well. The most frequently cited sources of bugs
were classical in nature-library updates (81%), developer mistakes (68%), and
compatibility issues (62%)-often worsened by limited abstraction in existing
SDKs. These findings highlight the urgent need for better-aligned testing and
debugging tools, integrated more seamlessly into the workflows of quantum
developers. We present these results in detail and offer actionable
recommendations grounded in the real-world needs of practitioners.

</details>


### [422] [An Expert Survey on Models and Digital Twins](https://arxiv.org/abs/2506.17313)
*Jonathan Reif,Daniel Dittler,Milapji Singh Gill,Tamás Farkas,Valentin Stegmaier,Felix Gehlhoff,Tobias Kleinert,Michael Weyrich*

Main category: cs.SE

TL;DR: The paper analyzes the challenges of integrating diverse Digital Models (DMs) in Digital Twins (DTs) through an expert survey, highlighting issues with standardization and adaptability.


<details>
  <summary>Details</summary>
Motivation: To address the lack of industry insight into challenges related to integrating various Digital Models in Digital Twins.

Method: An expert survey was conducted across multiple application domains to identify and analyze integration challenges for DMs within DTs.

Result: Revealed missing standardized interfaces, high manual adaptation efforts, and limited model reuse across lifecycle phases.

Conclusion: Future research should focus on automated model composition and semantics-based interoperability to improve DT efficiency.

Abstract: Digital Twins (DTs) are becoming increasingly vital for future industrial
applications, enhancing monitoring, control, and optimization of physical
assets. This enhancement is made possible by integrating various Digital Models
(DMs) within DTs, which must interoperate to represent different system aspects
and fulfill diverse application purposes. However, industry perspectives on the
challenges and research needs for integrating these models are rarely obtained.
Thus, this study conducts an expert survey across multiple application domains
to identify and analyze the challenges in utilizing diverse DMs within DTs. The
results reveal missing standardized interfaces, high manual adaptation effort,
and limited support for model reuse across lifecycle phases, highlighting
future research needs in automated model composition and semantics-based
interoperability.

</details>


### [423] [Large Language Models for Spreadsheets: Benchmarking Progress and Evaluating Performance with FLARE](https://arxiv.org/abs/2506.17330)
*Simon Thorne*

Main category: cs.SE

TL;DR: The paper evaluates the performance of Large Language Models (LLMs) on spreadsheet-related tasks using a newly introduced benchmark called FLARE.


<details>
  <summary>Details</summary>
Motivation: The authors aim to explore and assess the capabilities of LLMs in performing spreadsheet functions, an area that remains underexplored compared to other domains.

Method: The authors created a benchmark framework called FLARE, which evaluates LLMs on tasks ranging from basic formula creation to complex, real-world spreadsheet scenarios.

Result: Findings indicate LLMs perform well in simple tasks but struggle with complex, multi-step operations, often generating plausible but incorrect outputs.

Conclusion: Current LLMs lack robust symbolic reasoning capabilities, necessitating improvements in their architectures to better handle complex spreadsheet tasks.

Abstract: Large Language Models (LLMs) have demonstrated some significant capabilities
across various domains; however, their effectiveness in spreadsheet related
tasks remains underexplored. This study introduces a foundation for a
comprehensive benchmark framework to evaluate the performance of leading LLMs
in executing spreadsheet functions, formula generation and data manipulation
tasks. The benchmark encompasses tasks ranging from basic formula creation to
complex, real world spreadsheet scenarios. Our findings reveal that while LLMs
exhibit proficiency in straightforward tasks, they often falter in complex,
multi step operations, frequently producing plausible yet incorrect outputs.
These results underscore the limitations of current LLMs in handling
spreadsheet tasks that require precise logical reasoning and highlight the need
for integrating symbolic reasoning capabilities into LLM architectures. To
support this, we introduce FLARE (Formula Logic, Auditing, Reasoning and
Evaluation) a new benchmark for evaluating LLM performance on real-world
spreadsheet logic, auditing, and reasoning tasks.

</details>


### [424] [LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research](https://arxiv.org/abs/2506.17335)
*Shuo Yan,Ruochen Li,Ziming Luo,Zimu Wang,Daoyang Li,Liqiang Jing,Kaiyu He,Peilin Wu,George Michalopoulos,Yue Zhang,Ziyang Zhang,Mian Zhang,Zhiyu Chen,Xinya Du*

Main category: cs.SE

TL;DR: The paper presents LMR-BENCH, a benchmark for testing the ability of large language models (LLMs) to reproduce code from research papers in the NLP domain, revealing challenges in scientific reasoning and code synthesis.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of exploration into the ability of LLMs to reproduce complex code linked to research papers, particularly in NLP, where abstract reasoning and interdependent code files complicate the process.

Method: LMR-BENCH provides 28 code reproduction tasks from 23 NLP research papers, categorized into nine types. LLMs are tested under standard prompting and agent settings using unit test accuracy and LLM-based code correctness evaluation.

Result: Experiments reveal that even cutting-edge LLMs struggle with scientific reasoning and code synthesis tasks, exposing persistent limitations in current technologies.

Conclusion: The study underscores the need for advancements in LLMs to enhance their capability in reproducing scientific research and handling complex reasoning tasks.

Abstract: Large language model (LLM) agents have demonstrated remarkable potential in
advancing scientific discovery. However, their capability in the fundamental
yet crucial task of reproducing code from research papers, especially in the
NLP domain, remains underexplored. This task includes unique complex reasoning
challenges in the intellectual synthesis of abstract concepts and the
comprehension of code repositories with interdependent files. Motivated by this
gap, we present LMR-BENCH, a benchmark designed to systematically evaluate the
capability of LLM agents on code reproduction from Language Modeling Research.
It consists of 28 code reproduction tasks derived from 23 research papers
published in top-tier NLP venues over the past five years, spanning nine
fundamental categories. Models are provided with a research paper, a code
repository containing one or more masked functions, and instructions for
implementing these functions. We conduct extensive experiments in standard
prompting and LLM agent settings with state-of-the-art LLMs, evaluating the
accuracy of unit tests and performing LLM-based evaluation of code correctness.
Experimental results reveal that even the most advanced models still exhibit
persistent limitations in scientific reasoning and code synthesis, highlighting
critical gaps in LLM agents' ability to autonomously reproduce scientific
research

</details>


### [425] [Re-Evaluating Code LLM Benchmarks Under Semantic Mutation](https://arxiv.org/abs/2506.17369)
*Zhiyuan Pan,Xing Hu,Xin Xia,Xiaohu Yang*

Main category: cs.SE

TL;DR: The paper uncovers prompt sensitivity issues in code benchmarks, where minor prompt changes can cause performance inconsistencies in large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Code benchmarks for LLMs often rely on a single prompt template, leading to unreliable evaluations due to prompt sensitivity. This paper aims to address this critical issue that has not been extensively explored in code-related tasks.

Method: The researchers designed a framework that modifies prompt templates while preserving their semantics and structure. They tested 100 semantically similar prompts across 8 code tasks on 10 open-source LLMs and analyzed the results using various statistical metrics.

Result: The study found that small changes in prompts significantly affect model performance and rankings, highlighting the instability in benchmark evaluations caused by prompt sensitivity.

Conclusion: The findings stress the importance of accounting for prompt sensitivity in designing code benchmarks to ensure accurate and consistent evaluations of LLM capabilities.

Abstract: In the era of large language models (LLMs), code benchmarks have become an
important research area in software engineering and are widely used by
practitioners. These benchmarks evaluate the performance of LLMs on specific
code-related tasks, such as code understanding and generation. A critical step
in constructing code benchmarks is the design of prompts. However, as existing
code benchmarks typically rely on a single prompt template per task, they are
prone to the issue of prompt sensitivity, where minor prompt variations could
result in substantial performance variations, leading to unreliable evaluations
of model capabilities.
  While previous studies have explored prompt sensitivity, their experimental
designs and findings are limited to traditional natural language processing
(NLP) tasks. In this paper, we present an empirical study to investigate prompt
sensitivity in code benchmarks. We first propose a general framework that
modifies prompt templates in a manner that preserves both their semantics and
their structure as much as possible. Based on the framework, we conduct
extensive experiments across eight code benchmark tasks on 10 representative
open-source LLMs, with each task featuring 100 semantically similar prompt
templates. We then analyze the evaluation results using various statistical
metrics, focusing on both absolute and relative model performance. Our findings
suggest that even slight prompt variations can lead to significant shifts in
performance. Additionally, we observe that such variations can introduce
inconsistencies in the performance rankings across different models. These
insights highlight the need for considering prompt sensitivity when designing
future code benchmarks, to ensure more reliable and accurate evaluation of LLM
capabilities.

</details>


### [426] [Breaking Single-Tester Limits: Multi-Agent LLMs for Multi-User Feature Testing](https://arxiv.org/abs/2506.17539)
*Sidong Feng,Changhao Du,Huaxiao Liu,Qingnan Wang,Zhengwei Lv,Mengfei Wang,Chunyang Chen*

Main category: cs.SE

TL;DR: This paper introduces MAdroid, a novel testing framework using multi-agents powered by LLMs to automate the testing of multi-user interactive app features.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inadequacy of current automated methods in handling dynamic, timely, and collaborative interaction testing for apps.

Method: MAdroid employs multi-agents categorized as user agents (Operator) and supervisor agents (Coordinator and Observer). Each agent handles specific roles to simulate user interactions on apps efficiently.

Result: The approach achieved 82.9% success on tested tasks with 96.8% similarity in actions and identified 11 multi-user interactive bugs in real-world testing.

Conclusion: MAdroid improves automated app feature testing, showcasing practicality and potential for identifying interactive bugs in software development.

Abstract: The growing dependence on mobile phones and their apps has made multi-user
interactive features, like chat calls, live streaming, and video conferencing,
indispensable for bridging the gaps in social connectivity caused by physical
and situational barriers. However, automating these interactive features for
testing is fraught with challenges, owing to their inherent need for timely,
dynamic, and collaborative user interactions, which current automated testing
methods inadequately address. Inspired by the concept of agents designed to
autonomously and collaboratively tackle problems, we propose MAdroid, a novel
multi-agent approach powered by the Large Language Models (LLMs) to automate
the multi-user interactive task for app feature testing. Specifically, MAdroid
employs two functional types of multi-agents: user agents (Operator) and
supervisor agents (Coordinator and Observer). Each agent takes a specific role:
the Coordinator directs the interactive task; the Operator mimics user
interactions on the device; and the Observer monitors and reviews the task
automation process. Our evaluation, which included 41 multi-user interactive
tasks, demonstrates the effectiveness of our approach, achieving 82.9% of the
tasks with 96.8% action similarity, outperforming the ablation studies and
state-of-the-art baselines. Additionally, a preliminary investigation
underscores MAdroid's practicality by helping identify 11 multi-user
interactive bugs during regression app testing, confirming its potential value
in real-world software development contexts.

</details>


### [427] [CodeMorph: Mitigating Data Leakage in Large Language Model Assessment](https://arxiv.org/abs/2506.17627)
*Hongzhou Rao,Yanjie Zhao,Wenjie Zhu,Ling Xiao,Meizhen Wang,Haoyu Wang*

Main category: cs.SE

TL;DR: The paper addresses concerns about data leakage in evaluating large language models for coding tasks by introducing CodeMorph, a tool for generating diverse, semantic-preserving code modifications to mitigate this issue.


<details>
  <summary>Details</summary>
Motivation: Benchmark leakage compromises the integrity of evaluating large language models for code by inflating metrics and causing data contamination. A robust solution is needed to ensure fairer evaluations.

Method: The authors developed CodeMorph, which uses 26 semantic-preserving transformation methods and includes a genetic algorithm-based selection module (PESO) to produce diverse and effective perturbations across multiple programming languages.

Result: Applying CodeMorph led to a 24.67% average reduction in LLM accuracy for code completion tasks and showed significant improvements in perturbation effectiveness, with PESO lowering code similarity scores by an average of 7.01%.

Conclusion: CodeMorph demonstrates strong potential to enhance evaluation fairness in Code LLMs by providing diverse and robust perturbations that mitigate data leakage, benefiting multi-language tasks and cross-file complexities.

Abstract: Concerns about benchmark leakage in large language models for code (Code
LLMs) have raised issues of data contamination and inflated evaluation metrics.
The diversity and inaccessibility of many training datasets make it difficult
to prevent data leakage entirely, even with time lag strategies. Consequently,
generating new datasets through code perturbation has become essential.
However, existing methods often fail to produce complex and diverse variations,
struggle with complex cross-file dependencies, and lack support for multiple
programming languages, which limits their effectiveness in enhancing LLM
evaluations for coding tasks. To fill this gap, we propose CodeMorph, an
approach designed to support multiple programming languages while preserving
cross-file dependencies to mitigate data leakage. CodeMorph consists of two
main components that work together to enhance the perturbation process. The
first component employs 26 semantic-preserving transformation methods to
iteratively perturb code, generating diverse variations while ensuring that the
modified code remains compilable. The second component introduces a genetic
algorithm-based selection algorithm, PESO, to identify the more effective
perturbation method for each iteration by targeting lower similarity scores
between the perturbed and original code, thereby enhancing overall perturbation
effectiveness. Experimental results demonstrate that after applying CodeMorph,
the accuracy of the LLM on code completion tasks across five programming
languages decreased by an average of 24.67%, with Python showing the most
significant reduction at 45%. The similarity score of code optimized by PESO
is, on average, 7.01% lower than that of randomly perturbed code, peaking at a
reduction of 42.86%.

</details>


### [428] [Deep Learning Framework Testing via Model Mutation: How Far Are We?](https://arxiv.org/abs/2506.17638)
*Yanzhou Mu,Rong Wang,Juan Zhai,Chunrong Fang,Xiang Chen,Zhiyuan Peng,Peiran Yang,Ruixiang Qian,Shaoyu Yang,Zhenyu Chen*

Main category: cs.SE

TL;DR: The paper revisits the effectiveness of mutation-based testing methods for Deep Learning (DL) frameworks, identifies limitations in current approaches, proposes optimizations, and successfully detects new defects.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in existing mutation-based testing methods for DL frameworks, particularly their lack of customization and high rate of false positives, and to improve their ability to detect important and prioritized defects.

Method: Review and analyze existing mutation-based testing methods, build a dataset by collecting defect reports from three popular DL frameworks, classify them based on developer ratings, and propose optimizations informed by insights from the analysis.

Result: The proposed optimizations resulted in identifying seven new defects, including four high-priority issues confirmed by developers, of which three were resolved. Across 23 models, 39 unique defects were identified, with 31 confirmed and eight fixed.

Conclusion: Mutation-based testing methods for DL frameworks can be significantly improved through customized and targeted optimizations, leading to the identification of actionable and high-priority defects.

Abstract: Deep Learning (DL) frameworks are a fundamental component of DL development.
Therefore, the detection of DL framework defects is important and challenging.
As one of the most widely adopted DL testing techniques, model mutation has
recently gained significant attention. In this study, we revisit the defect
detection ability of existing mutation-based testing methods and investigate
the factors that influence their effectiveness. To begin with, we reviewed
existing methods and observed that many of them mutate DL models (e.g.,
changing their parameters) without any customization, ignoring the unique
challenges in framework testing. Another issue with these methods is their
limited effectiveness, characterized by a high rate of false positives caused
by illegal mutations arising from the use of generic, non-customized mutation
operators. Moreover, we tracked the defects identified by these methods and
discovered that most of them were ignored by developers. Motivated by these
observations, we investigate the effectiveness of existing mutation-based
testing methods in detecting important defects that have been authenticated by
framework developers. We begin by collecting defect reports from three popular
frameworks and classifying them based on framework developers' ratings to build
a comprehensive dataset. We then perform an in-depth analysis to uncover
valuable insights. Based on our findings, we propose optimization strategies to
address the shortcomings of existing approaches. Following these optimizations,
we identified seven new defects, four of which were confirmed by developers as
high-priority issues, with three resolved. In summary, we identified 39 unique
defects across just 23 models, of which 31 were confirmed by developers, and
eight have been fixed.

</details>


### [429] [May the Feedback Be with You! Unlocking the Power of Feedback-Driven Deep Learning Framework Fuzzing via LLMs](https://arxiv.org/abs/2506.17642)
*Shaoyu Yang,Chunrong Fang,Haifeng Lin,Xiang Chen,Zhenyu Chen*

Main category: cs.SE

TL;DR: The paper introduces FUEL, a system that utilizes Large Language Models (LLMs) for feedback-driven fuzz testing of Deep Learning (DL) frameworks, resulting in detection of numerous new bugs.


<details>
  <summary>Details</summary>
Motivation: Existing fuzz testing techniques for DL frameworks fail to comprehensively consider multiple types of feedback and analyze it in a detailed manner. Additionally, LLM-based fuzzing methods so far only focused on test case generation, leaving the potential of feedback analysis untapped.

Method: FUEL employs two LLM-based agents—Analysis LLM for generating feedback summaries and Generation LLM for creating guided test cases based on these summaries.

Result: Using FUEL, 104 bugs were detected in PyTorch and TensorFlow, with 93 confirmed as new, 47 fixed, and 5 assigned CVE IDs.

Conclusion: FUEL emphasizes the importance of leveraging LLMs for feedback analysis in fuzz testing, demonstrating improved performance and effectiveness.

Abstract: Artificial Intelligence (AI) Infrastructures, represented by Deep Learning
(DL) frameworks, have served as fundamental DL systems over the last decade.
However, the bugs in DL frameworks could lead to catastrophic consequences in
some critical scenarios (e.g., healthcare and autonomous driving). A simple yet
effective way to find bugs in DL frameworks is fuzz testing (Fuzzing).
Unfortunately, existing fuzzing techniques have not comprehensively considered
multiple types of feedback. Additionally, they analyze feedback in a
coarse-grained manner, such as mutating the test cases only according to
whether the coverage increases. Recently, researchers introduced Large Language
Models (LLMs) into fuzzing. However, current LLM-based fuzzing techniques only
focus on using LLMs to generate test cases while overlooking their potential to
analyze feedback information, failing to create more valid and diverse test
cases. To fill this gap, we propose FUEL to break the seal of Feedback-driven
fuzzing for DL frameworks. The backbone of FUEL comprises two LLM-based agents,
namely analysis LLM and generation LLM. Analysis LLM agent infers analysis
summaries from feedback information, while the generation LLM agent creates
tests guided by these analysis summaries. So far, FUEL has detected 104 bugs
for PyTorch and TensorFlow, with 93 confirmed as new bugs, 47 already fixed,
and 5 assigned with CVE IDs. Our work indicates that considering multiple types
of feedback is beneficial to fuzzing performance, and leveraging LLMs to
analyze feedback information is a promising direction. Our artifact is
available at https://github.com/NJU-iSE/FUEL

</details>


### [430] [Improving Compiler Bug Isolation by Leveraging Large Language Models](https://arxiv.org/abs/2506.17647)
*Yixian Qi,Jiajun Jiang,Fengjie Li,Bowen Chen,Hongyu Zhang,Junjie Chen*

Main category: cs.SE

TL;DR: The paper introduces AutoCBI, a method using Large Language Models for compiler bug localization, demonstrating significant improvements in isolating bugs in GCC and LLVM compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional and current compiler bug localization techniques, particularly in test program mutation and resource usage, by leveraging advancements in Large Language Models (LLMs).

Method: Developed AutoCBI, which uses LLMs to summarize compiler file functions and reorders suspicious file rankings using specialized prompts. It integrates information from failing test programs, source file function summaries, test coverage analysis, and compiler configurations to identify and rank suspicious files.

Result: AutoCBI outperforms state-of-the-art methods (DiWi, RecBi, and FuseFL) in isolating bugs from GCC and LLVM compilers. It isolates up to 69.23% more bugs in the Top-1 ranked results for LLVM and shows significant improvement over competitors.

Conclusion: The AutoCBI approach demonstrates that integrating LLM capabilities for function summarization and ranking refinement can effectively localize bugs in large-scale compiler systems, showcasing its potential as a robust alternative to existing methodologies.

Abstract: Compilers play a foundational role in building reliable software systems, and
bugs within them can lead to catastrophic consequences. The compilation process
typically involves hundreds of files, making traditional automated bug
isolation techniques inapplicable due to scalability or effectiveness issues.
Current mainstream compiler bug localization techniques have limitations in
test program mutation and resource consumption. Inspired by the recent advances
of pre-trained Large Language Models (LLMs), we propose an innovative approach
named AutoCBI, which (1) uses LLMs to summarize compiler file functions and (2)
employs specialized prompts to guide LLM in reordering suspicious file
rankings. This approach leverages four types of information: the failing test
program, source file function summaries, lists of suspicious files identified
through analyzing test coverage, as well as compilation configurations with
related output messages, resulting in a refined ranking of suspicious files.
Our evaluation of AutoCBI against state-of-the-art approaches (DiWi, RecBi and
FuseFL) on 120 real-world bugs from the widely-used GCC and LLVM compilers
demonstrates its effectiveness. Specifically, AutoCBI isolates 66.67%/69.23%,
300%/340%, and 100%/57.14% more bugs than RecBi, DiWi, and FuseFL,
respectively, in the Top-1 ranked results for GCC/LLVM. Additionally, the
ablation study underscores the significance of each component in our approach.

</details>


### [431] [PAGENT: Learning to Patch Software Engineering Agents](https://arxiv.org/abs/2506.17772)
*Haoran Xue,Gias Uddin,Song Wang*

Main category: cs.SE

TL;DR: This paper conducts an empirical study of failed code patches generated by seven LLM agents and proposes PAGENT, a program-analysis-driven approach, to address type-related issues in generated patches.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand why patches generated by LLM Agents often fail and how these failures could be addressed.

Method: The researchers analyzed 769 failed patches generated by seven LLMs for 114 unresolved issues, classified failure reasons into a taxonomy, and developed PAGENT—a program analysis and LLM-based tool—to fix type-related patch failures.

Result: PAGENT achieved partial success by fixing 29 out of 127 type-related failed patches in the study.

Conclusion: Though not perfect, PAGENT represents a step forward in addressing type-related errors in patches generated by LLMs, demonstrating the potential of combining program analysis with LLM-based inference techniques.

Abstract: LLM Agents produce patches automatically to resolve an issue. However, they
can generate inaccurate patches. Little is known about the root causes behind
those failed patches or how those could be fixed. This paper reports an
empirical study of the failed patches generated by seven top LLM code agents.
We collected 114 issues from the SWE-bench Lite dataset that remained
unresolved across the agents. The seven agents produced a total of 769 failed
patches for those issues, which we checked with a combination of GPT-4o and
manual analysis. We present a taxonomy of the failure reasons across the
patches. The taxonomy contains six categories, with several sub-categories
under each category. For example, a frequently observed category is the
inability of an LLM to correctly infer/produce the appropriate variable type in
the produced patch. As a first step towards addressing such type-related
errors, we designed PAGENT (Patch Agent). PAGENT utilizes program analysis
techniques like CFG creation and exploration to infer the type of information
of a patch. PAGENT does this by applying repository-level static code analysis
techniques. Then, PAGENT refines the inferred type by further utilizing an
LLM-based inference technique. We tested PAGENT on all 127 type-related failed
patches from the top three agents in our study. PAGENT could fix 29 of the 127
failed patches.

</details>


### [432] [SAVANT: Vulnerability Detection in Application Dependencies through Semantic-Guided Reachability Analysis](https://arxiv.org/abs/2506.17798)
*Wang Lingxiang,Quanzhi Fu,Wenjia Song,Gelei Deng,Yi Liu,Dan Williams,Ying Zhang*

Main category: cs.SE

TL;DR: SAVANT, a novel tool using Large Language Models (LLMs) and semantic preprocessing, improves the accuracy of detecting vulnerabilities in third-party Java libraries.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in vulnerable API usage detection by current Software Composition Analysis tools, which lead to inaccurate alerts, security risks, and delays in fixing bugs.

Method: SAVANT integrates semantic preprocessing and LLM-powered context analysis to segment code logically and assess vulnerability impact within specific contexts.

Result: SAVANT demonstrated superior performance metrics (83.8% precision, 73.8% recall, 69.0% accuracy, 78.5% F1-score) during evaluation against 55 real-world applications, surpassing existing tools.

Conclusion: SAVANT effectively enhances vulnerability detection accuracy, thereby reducing the burden on development teams and accelerating security fixes in Java applications.

Abstract: The integration of open-source third-party library dependencies in Java
development introduces significant security risks when these libraries contain
known vulnerabilities. Existing Software Composition Analysis (SCA) tools
struggle to effectively detect vulnerable API usage from these libraries due to
limitations in understanding API usage semantics and computational challenges
in analyzing complex codebases, leading to inaccurate vulnerability alerts that
burden development teams and delay critical security fixes.
  To address these challenges, we proposed SAVANT by leveraging two insights:
proof-of-vulnerability test cases demonstrate how vulnerabilities can be
triggered in specific contexts, and Large Language Models (LLMs) can understand
code semantics. SAVANT combines semantic preprocessing with LLM-powered context
analysis for accurate vulnerability detection. SAVANT first segments source
code into meaningful blocks while preserving semantic relationships, then
leverages LLM-based reflection to analyze API usage context and determine
actual vulnerability impacts. Our evaluation on 55 real-world applications
shows that SAVANT achieves 83.8% precision, 73.8% recall, 69.0% accuracy, and
78.5% F1-score, outperforming state-of-the-art SCA tools.

</details>


### [433] [Is Your Automated Software Engineer Trustworthy?](https://arxiv.org/abs/2506.17812)
*Noble Saji Mathews,Meiyappan Nagappan*

Main category: cs.SE

TL;DR: This paper introduces BouncerBench, a benchmark designed to evaluate LLM-based software agents on their ability to abstain from action when inputs are vague or outputs are incorrect.


<details>
  <summary>Details</summary>
Motivation: To address the unreliability of LLM-based tools in software engineering, especially their tendency to produce responses even when inputs are ambiguous or outputs are flawed.

Method: The authors developed BouncerBench to systematically assess whether models can identify vague issues and invalid patches, incorporating an evaluation mechanism for abstention when uncertainty arises.

Result: Most existing LLMs fail to abstain from acting on ill-defined inputs or generating patches when their outputs are incorrect, highlighting a key area for improvement.

Conclusion: There is substantial room for improvement in LLMs' ability to make cautious decisions, and BouncerBench serves as a foundational tool for improving their reliability in software engineering workflows.

Abstract: Large Language Models (LLMs) are being increasingly used in software
engineering tasks, with an increased focus on bug report resolution over the
past year. However, most proposed systems fail to properly handle uncertain or
incorrect inputs and outputs. Existing LLM-based tools and coding agents
respond to every issue and generate a patch for every case, even when the input
is vague or their own output is incorrect. There are no mechanisms in place to
abstain when confidence is low. This leads to unreliable behaviour, such as
hallucinated code changes or responses based on vague issue reports. We
introduce BouncerBench, a benchmark that evaluates whether LLM-based software
agents can refuse to act when inputs are ill-defined or refuse to respond when
their own outputs are likely to be incorrect. Unlike prior benchmarks that
implicitly incentivize models to generate responses even when uncertain,
BouncerBench aims to improve precision by targeting two overlooked failure
points: (1) vague or underspecified issue descriptions in tickets and (2)
logically or functionally incorrect code patches created by the system. It
measures whether proposed systems can distinguish actionable issues from vague
tickets and valid patches from untrustworthy ones. We also implement a basic
input and output bouncer, evaluating how well current LLMs can abstain when
needed. Our results show that most models fail to abstain from underspecified
inputs or incorrect outputs. Hence, we conclude that there is significant room
for improvement before LLMs can be trusted to make correct decisions and
recommendations in real-world software engineering workflows. BouncerBench
provides a first step toward evaluating and building more cautious, trustworthy
code agents. The replication package, dataset, and leaderboard can be found at
bouncerbench.com

</details>


### [434] [The Impact of AI-Generated Solutions on Software Architecture and Productivity: Results from a Survey Study](https://arxiv.org/abs/2506.17833)
*Giorgio Amasanti,Jasmin Jahic*

Main category: cs.SE

TL;DR: AI tools improve software engineers' productivity but are less effective for complex problems. For smaller tasks, they don't harm software quality, but they produce lower-quality solutions for bigger challenges requiring human oversight.


<details>
  <summary>Details</summary>
Motivation: To examine the productivity benefits of AI tools for software engineers and their impact on long-term software quality.

Method: A survey was conducted among software practitioners who actively use AI-powered tools.

Result: AI tools enhance engineer productivity and don't harm software quality for small tasks, though they produce lower-quality solutions for complex issues.

Conclusion: AI tools are beneficial but require careful use and human intervention for larger, complex software tasks.

Abstract: AI-powered software tools are widely used to assist software engineers.
However, there is still a need to understand the productivity benefits of such
tools for software engineers. In addition to short-term benefits, there is a
question of how adopting AI-generated solutions affects the quality of software
over time (e.g., maintainability and extendability).
  To provide some insight on these questions, we conducted a survey among
software practitioners who use AI tools. Based on the data collected from our
survey, we conclude that AI tools significantly increase the productivity of
software engineers. However, the productivity benefits of using AI tools reduce
as projects become more complex. The results also show that there are no
significant negative influences of adopting AI-generated solutions on software
quality, as long as those solutions are limited to smaller code snippets.
However, when solving larger and more complex problems, AI tools generate
solutions of a lower quality, indicating the need for architects to perform
problem decomposition and solution integration.

</details>


### [435] [Software Reuse in the Generative AI Era: From Cargo Cult Towards AI Native Software Engineering](https://arxiv.org/abs/2506.17937)
*Tommi Mikkonen,Antero Taivalsaari*

Main category: cs.SE

TL;DR: The paper explores the shift towards AI-assisted generative software reuse and its implications for software engineering, calling for further research in this area.


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand the implications and challenges of integrating AI-generated code into modern software development workflows.

Method: The paper discusses the phenomenon, raises relevant questions, and defines a research agenda for addressing key issues.

Result: It identifies critical challenges and outlines research directions regarding AI-assisted generative software reuse.

Conclusion: AI-assisted generative reuse is reshaping software development but raises conceptual and practical concerns, warranting focused research and action.

Abstract: Software development is currently under a paradigm shift in which artificial
intelligence and generative software reuse are taking the center stage in
software creation. Consequently, earlier software reuse practices and methods
are rapidly being replaced by AI-assisted approaches in which developers place
their trust on code that has been generated by artificial intelligence. This is
leading to a new form of software reuse that is conceptually not all that
different from cargo cult development. In this paper we discuss the
implications of AI-assisted generative software reuse in the context of
emerging "AI native" software engineering, bring forth relevant questions, and
define a tentative research agenda and call to action for tackling some of the
central issues associated with this approach.

</details>


### [436] [Build It Clean: Large-Scale Detection of Code Smells in Build Scripts](https://arxiv.org/abs/2506.17948)
*Mahzabin Tamanna,Yash Chandrani,Matthew Burrows,Brandon Wroblewski,Laurie Williams,Dominik Wermke*

Main category: cs.SE

TL;DR: This study focuses on identifying and addressing code smells in build scripts using GitHub issues analysis and a static analysis tool, "Sniffer." Researchers examined 5882 scripts and categorized 13 code smells, offering mitigation strategies for software reliability.


<details>
  <summary>Details</summary>
Motivation: Practitioners often introduce code smells in build scripts unintentionally, leading to build failures, increased risks, and technical debt. Addressing these smells is crucial for improving software project reliability and efficiency.

Method: A mixed-methods approach was implemented, including qualitative analysis of 2000 GitHub issues and quantitative analysis of 5882 build scripts using "Sniffer," a static analysis tool, to detect 13 code smell categories.

Result: Researchers identified 10,895 code smell occurrences across Maven (3184), Gradle (1214), CMake (337), and Makefiles (6160). They found prevalent patterns like Insecure URLs, Hardcoded Paths/URLs, and Wildcard Usage, along with strong associations between specific code smell pairs.

Conclusion: Mitigation strategies based on the findings can help practitioners improve build script maintainability, reliability, and efficiency in software projects, reducing risks and technical debt.

Abstract: Build scripts are files that automate the process of compiling source code,
managing dependencies, running tests, and packaging software into deployable
artifacts. These scripts are ubiquitous in modern software development
pipelines for streamlining testing and delivery. While developing build
scripts, practitioners may inadvertently introduce code smells. Code smells are
recurring patterns of poor coding practices that may lead to build failures or
increase risk and technical debt. The goal of this study is to aid
practitioners in avoiding code smells in build scripts through an empirical
study of build scripts and issues on GitHub. We employed a mixed-methods
approach, combining qualitative and quantitative analysis. We conducted a
qualitative analysis of 2000 build-script-related GitHub issues. Next, we
developed a static analysis tool, Sniffer, to identify code smells in 5882
build scripts of Maven, Gradle, CMake, and Make files, collected from 4877
open-source GitHub repositories. We identified 13 code smell categories, with a
total of 10,895 smell occurrences, where 3184 were in Maven, 1214 in Gradle,
337 in CMake, and 6160 in Makefiles.
  Our analysis revealed that Insecure URLs were the most prevalent code smell
in Maven build scripts, while Hardcoded Paths/URLs were commonly observed in
both Gradle and CMake scripts. Wildcard Usage emerged as the most frequent
smell in Makefiles. The co-occurrence analysis revealed strong associations
between specific smell pairs of Hardcoded Paths/URLs with Duplicates, and
Inconsistent Dependency Management with Empty or Incomplete Tags, indicating
potential underlying issues in the build script structure and maintenance
practices. Based on our findings, we recommend strategies to mitigate the
existence of code smells in build scripts to improve the efficiency,
reliability, and maintainability of software projects.

</details>


### [437] [VFArchē: A Dual-Mode Framework for Locating Vulnerable Functions in Open-Source Software](https://arxiv.org/abs/2506.18050)
*Lyuye Zhang,Jian Zhang,Kaixuan Li,Chong Wang,Chengwei Liu,Jiahui Wu,Sen Chen,Yaowen Zheng,Yang Liu*

Main category: cs.SE

TL;DR: The paper introduces VFArch=e, a dual-mode approach for localizing Vulnerable Functions (VF) in software vulnerabilities, tackling scenarios both with and without patch availability, and demonstrating improved accuracy and reduced false positives in real-world applications.


<details>
  <summary>Details</summary>
Motivation: To address challenges in tracking reachable vulnerabilities in open-source software projects due to the lack of vulnerable function (VF) information in modern vulnerability databases.

Method: The authors propose VFArch=e, which leverages a dual-mode mechanism to automatically localize vulnerable functions (VF) in scenarios both with and without patch links.

Result: Experimental evaluations show VFArch=e significantly outperforms existing baselines in Mean Reciprocal Rank for both patch-present and patch-absent modes, and reduces false positives by 78-89% in SCA tools.

Conclusion: VFArch=e is an effective, holistic solution for vulnerable function localization in modern software projects, providing notable improvements in accuracy and practical application scenarios.

Abstract: Software Composition Analysis (SCA) has become pivotal in addressing
vulnerabilities inherent in software project dependencies. In particular,
reachability analysis is increasingly used in Open-Source Software (OSS)
projects to identify reachable vulnerabilities (e.g., CVEs) through call
graphs, enabling a focus on exploitable risks. Performing reachability analysis
typically requires the vulnerable function (VF) to track the call chains from
downstream applications. However, such crucial information is usually
unavailable in modern vulnerability databases like NVD. While directly
extracting VF from modified functions in vulnerability patches is intuitive,
patches are not always available. Moreover, our preliminary study shows that
over 26% of VF do not exist in the modified functions. Meanwhile, simply
ignoring patches to search vulnerable functions suffers from overwhelming
noises and lexical gaps between descriptions and source code. Given that almost
half of the vulnerabilities are equipped with patches, a holistic solution that
handles both scenarios with and without patches is required. To meet real-world
needs and automatically localize VF, we present VFArch\=e, a dual-mode approach
designed for disclosed vulnerabilities, applicable in scenarios with or without
available patch links. The experimental results of VFArch\=e on our constructed
benchmark dataset demonstrate significant efficacy regarding three metrics,
achieving 1.3x and 1.9x Mean Reciprocal Rank over the best baselines for
Patch-present and Patch-absent modes, respectively. Moreover, VFArch\=e has
proven its applicability in real-world scenarios by successfully locating VF
for 43 out of 50 latest vulnerabilities with reasonable efforts and
significantly reducing 78-89% false positives of SCA tools.

</details>


### [438] [Call Me Maybe: Enhancing JavaScript Call Graph Construction using Graph Neural Networks](https://arxiv.org/abs/2506.18191)
*Masudul Hasan Masud Bhuiyan,Gianluca De Stefano,Giancarlo Pellegrino,Cristian-Alexandru Staicu*

Main category: cs.SE

TL;DR: GRAPHIA employs Graph Neural Networks (GNNs) for improving JavaScript call graph construction by predicting missed call edges, achieving high ranking accuracy in identifying valid function definitions.


<details>
  <summary>Details</summary>
Motivation: Existing call graph construction for JavaScript is inconsistent due to hard-to-analyze language features, leading to false positives and negatives in identified function call edges.

Method: GRAPHIA uses graph neural networks and represents JavaScript programs as full program graphs with syntactic and semantic edges. It learns from imperfect labels derived from static tools and dynamic test data to predict missed call edges.

Result: Evaluation on 50 JavaScript libraries demonstrates GRAPHIA correctly ranks the valid function target as the top candidate in 42% of unresolved call sites and among the top 5 in 72% of cases, showcasing improved recall.

Conclusion: GRAPHIA highlights the benefits of learning-based approaches in enhancing JavaScript call graph recall, representing a novel application of GNN-based link prediction for interprocedural analysis across multi-file program graphs.

Abstract: Static analysis plays a key role in finding bugs, including security issues.
A critical step in static analysis is building accurate call graphs that model
function calls in a program. However, due to hard-to-analyze language features,
existing call graph construction algorithms for JavaScript are neither sound
nor complete. Prior work shows that even advanced solutions produce false edges
and miss valid ones. In this work, we assist these tools by identifying missed
call edges. Our main idea is to frame the problem as link prediction on full
program graphs, using a rich representation with multiple edge types. Our
approach, GRAPHIA, leverages recent advances in graph neural networks to model
non-local relationships between code elements. Concretely, we propose
representing JavaScript programs using a combination of syntactic- and
semantic-based edges. GRAPHIA can learn from imperfect labels, including static
call edges from existing tools and dynamic edges from tests, either from the
same or different projects. Because call graphs are sparse, standard machine
learning metrics like ROC are not suitable. Instead, we evaluate GRAPHIA by
ranking function definitions for each unresolved call site. We conduct a
large-scale evaluation on 50 popular JavaScript libraries with 163K call edges
(150K static and 13K dynamic). GRAPHIA builds program graphs with 6.6M
structural and 386K semantic edges. It ranks the correct target as the top
candidate in over 42% of unresolved cases and within the top 5 in 72% of cases,
reducing the manual effort needed for analysis. Our results show that
learning-based methods can improve the recall of JavaScript call graph
construction. To our knowledge, this is the first work to apply GNN-based link
prediction to full multi-file program graphs for interprocedural analysis.

</details>


### [439] [Managing Technical Debt in a Multidisciplinary Data Intensive Software Team: an Observational Case Study](https://arxiv.org/abs/2506.18219)
*Ulrike M. Graetsch,Rashina Hoda,Hourieh Khalazjadeh,Mojtaba Shahin,John Grundy*

Main category: cs.SE

TL;DR: The paper explores how multidisciplinary teams manage technical debt in data-intensive (DI) systems, using case studies and socio-technical grounded theory.


<details>
  <summary>Details</summary>
Motivation: Limited understanding exists on how multidisciplinary teams develop DI systems and handle the associated technical debt, despite increasing investments and data requirements.

Method: The research employed exploratory observation case studies and socio-technical grounded theory to analyze team practices and conceptualize their methods of technical debt management.

Result: The study identifies types of technical debt, such as technical data components debt and pipeline debt, and explains how the teams assess, manage, and mitigate these debts while adhering to sprint capacity constraints.

Conclusion: The findings align with existing taxonomies, emphasizing the need for new implementation patterns and tools to better equip multidisciplinary DI teams for technical debt management.

Abstract: Context: There is an increase in the investment and development of
data-intensive (DI) solutions, systems that manage large amounts of data.
Without careful management, this growing investment will also grow associated
technical debt (TD). Delivery of DI solutions requires a multidisciplinary
skill set, but there is limited knowledge about how multidisciplinary teams
develop DI systems and manage TD.
  Objective: This research contributes empirical, practice based insights about
multidisciplinary DI team TD management practices.
  Method: This research was conducted as an exploratory observation case study.
We used socio-technical grounded theory (STGT) for data analysis to develop
concepts and categories that articulate TD and TDs debt management practices.
  Results: We identify TD that the DI team deals with, in particular technical
data components debt and pipeline debt. We explain how the team manages the TD,
assesses TD, what TD treatments they consider and how they implement TD
treatments to fit sprint capacity constraints.
  Conclusion: We align our findings to existing TD and TDM taxonomies, discuss
their implications and highlight the need for new implementation patterns and
tool support for multidisciplinary DI teams.

</details>


### [440] [Tu(r)ning AI Green: Exploring Energy Efficiency Cascading with Orthogonal Optimizations](https://arxiv.org/abs/2506.18289)
*Saurabhsingh Rajput,Mootez Saad,Tushar Sharma*

Main category: cs.SE

TL;DR: The paper discusses optimizing energy efficiency in AI pipelines through strategic design across pipeline phases, achieving significant energy savings.


<details>
  <summary>Details</summary>
Motivation: Rapid growth in AI leads to high computational and energy demands, necessitating proactive strategies for efficiency.

Method: Proposes a curated approach by optimizing five AI pipeline phases: data, model, training, system, and inference, with combinatorial knobs.

Result: Experimental results show energy consumption reductions of up to 94.6% while retaining 95.95% of original performance metrics.

Conclusion: The paper offers a sustainable AI framework that integrates efficiency and environmental responsibility into pipeline design.

Abstract: AI's exponential growth intensifies computational demands and energy
challenges. While practitioners employ various optimization techniques, that we
refer as "knobs" in this paper, to tune model efficiency, these are typically
afterthoughts and reactive ad-hoc changes applied in isolation without
understanding their combinatorial effects on energy efficiency. This paper
emphasizes on treating energy efficiency as the first-class citizen and as a
fundamental design consideration for a compute-intensive pipeline. We show that
strategic selection across five AI pipeline phases (data, model, training,
system, inference) creates cascading efficiency. Experimental validation shows
orthogonal combinations reduce energy consumption by up to $94.6$% while
preserving $95.95$% of the original F1 score of non-optimized pipelines. This
curated approach provides actionable frameworks for informed sustainable AI
that balance efficiency, performance, and environmental responsibility.

</details>


### [441] [Use Property-Based Testing to Bridge LLM Code Generation and Validation](https://arxiv.org/abs/2506.18315)
*Lehan He,Zeren Chen,Zhe Zhang,Jing Shao,Xiang Gao,Lu Sheng*

Main category: cs.SE

TL;DR: The paper presents "Property-Generated Solver," a framework leveraging Property-Based Testing (PBT) to validate high-level program properties, aiming to improve functional correctness in Large Language Models' (LLMs) code generation.


<details>
  <summary>Details</summary>
Motivation: Ensuring functional correctness in code generated by LLMs, especially for complex tasks, remains a significant challenge. Traditional Test-Driven Development (TDD) approaches struggle due to the scarcity or bias of test cases, leading to less effective refinement.

Method: The framework uses two LLM-based agents: a Generator for code creation and refinement, and a Tester for managing the PBT process, which validates high-level program properties rather than specific input-output pairs. Feedback from property violations guides iterative improvements by the Generator.

Result: The experimental results on multiple benchmarks show substantial improvements in pass@1, with relative gains of 23.1% to 37.3% over traditional TDD approaches.

Conclusion: The Property-Generated Solver demonstrates the effectiveness of PBT as a validation mechanism, significantly improving the functional correctness and generalizability of LLM-generated code.

Abstract: Large Language Models (LLMs) excel at code generation, but ensuring their
outputs to be functionally correct, especially in complex programming tasks, is
a persistent challenge. While traditional Test-Driven Development (TDD) offers
a path for code refinement, its efficacy with LLMs is often undermined by the
scarcity of high-quality test cases or the pitfalls of automated test
generation, including biased tests or inaccurate output predictions that can
misdirect the correction process. This paper introduces Property-Generated
Solver, a novel framework that leverages Property-Based Testing (PBT) to
validate high-level program properties or invariants, instead of relying on
specific input-output examples. These properties are often simpler to define
and verify than directly predicting exhaustive test oracles, breaking the
"cycle of self-deception" where tests might share flaws with the code they are
meant to validate. Property-Generated Solver employs two collaborative
LLM-based agents: a Generator dedicated to code generation and iterative
refinement, and a Tester that manages the PBT life-cycle and formulate
semantically rich feedback from property violations. The resulting
comprehensive and actionable feedback then guides the Generator in its
refinement efforts. By establishing PBT as the core validation engine within
this iterative, closed-loop paradigm, Property-Generated Solver provides a
robust mechanism for steering LLMs towards more correct and generalizable code.
Extensive experimental results on multiple code generation benchmarks
demonstrate that Property-Generated Solver achieves substantial pass@1
improvements, ranging from 23.1% to 37.3% relative gains over established TDD
methods.

</details>


### [442] [Predictive Analytics for Collaborators Answers, Code Quality, and Dropout on Stack Overflow](https://arxiv.org/abs/2506.18329)
*Elijah Zolduoarrati,Sherlock A. Licorish,Nigel Stanger*

Main category: cs.SE

TL;DR: Previous studies using Stack Overflow data typically examined few models. This work benchmarks 21 algorithms for three tasks: answer prediction, code quality assessment, and dropout status. Notable techniques include Bayesian optimization, genetic algorithms, and the fine-tuned CodeBERT model.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations of previous research that only used a small selection of predictive models to analyze Stack Overflow data, which may lead to overlooked algorithms.

Method: Benchmarked 21 algorithms on three tasks: predicting user answers, code quality violations, and dropout status. Techniques included data normalization/transformations, Bayesian hyperparameter optimization, genetic algorithms, and fine-tuned CodeBERT for classification.

Result: Bagging models with standardization achieved R2=0.821 for answers. Stochastic Gradient Descent and others performed well on code quality. Extreme Gradient Boosting paired with log-transformation achieved F1=0.825 for dropout prediction. CodeBERT also classified dropout with F1=0.809.

Conclusion: The study’s comprehensive benchmarking offers insights into selecting models for specific tasks and provides useful hyperparameter setups for researchers and practitioners.

Abstract: Previous studies that used data from Stack Overflow to develop predictive
models often employed limited benchmarks of 3-5 models or adopted arbitrary
selection methods. Despite being insightful, their limited scope suggests the
need to benchmark more models to avoid overlooking untested algorithms. Our
study evaluates 21 algorithms across three tasks: predicting the number of
question a user is likely to answer, their code quality violations, and their
dropout status. We employed normalisation, standardisation, as well as
logarithmic and power transformations paired with Bayesian hyperparameter
optimisation and genetic algorithms. CodeBERT, a pre-trained language model for
both natural and programming languages, was fine-tuned to classify user dropout
given their posts (questions and answers) and code snippets. We found Bagging
ensemble models combined with standardisation achieved the highest R2 value
(0.821) in predicting user answers. The Stochastic Gradient Descent regressor,
followed by Bagging and Epsilon Support Vector Machine models, consistently
demonstrated superior performance to other benchmarked algorithms in predicting
user code quality across multiple quality dimensions and languages. Extreme
Gradient Boosting paired with log-transformation exhibited the highest F1-score
(0.825) in predicting user dropout. CodeBERT was able to classify user dropout
with a final F1-score of 0.809, validating the performance of Extreme Gradient
Boosting that was solely based on numerical data. Overall, our benchmarking of
21 algorithms provides multiple insights. Researchers can leverage findings
regarding the most suitable models for specific target variables, and
practitioners can utilise the identified optimal hyperparameters to reduce the
initial search space during their own hyperparameter tuning processes.

</details>


### [443] [Recipe for Discovery: A Framework for Systematic Open Source Project Identification](https://arxiv.org/abs/2506.18359)
*Juanita Gomez,Emily Lovell,Stephanie Lieggi,Alvaro A. Cardenas,James Davis*

Main category: cs.SE

TL;DR: This paper presents a framework to identify and analyze open source software projects affiliated with institutions, using the University of California system as a case study.


<details>
  <summary>Details</summary>
Motivation: Open source projects in academic institutions often lack visibility and recognition due to decentralized development and unclear tracking methods.

Method: The authors utilized GitHub's REST API to build a data pipeline for discovering repositories, extracting metadata, and implemented multiple classification strategies, including traditional machine learning and large language models, to determine institutional affiliation.

Result: The framework discovered over 52,000 repositories and showed high accuracy in predicting institutional affiliations at scale.

Conclusion: This effective framework offers systematic insights into academic open source software projects, enhancing visibility and recognition of institutional contributions.

Abstract: Open source software development, particularly within institutions such as
universities and research laboratories, is often decentralized and difficult to
track. Despite producing highly impactful tools in science, these efforts often
go unrecognized due to a lack of visibility and institutional awareness. This
paper addresses the challenge of discovering, classifying, and analyzing open
source software projects developed across distributed institutional systems. We
present a framework for systematically identifying institutional affiliated
repositories, using the University of California (UC) system as a case study.
  Using GitHub's REST API, we build a pipeline to discover relevant
repositories and extract meaningful metadata. We then propose and evaluate
multiple classification strategies, including both traditional machine learning
models and large language models (LLMs), to distinguish affiliated projects
from unrelated repositories and generate accurate insights into the academic
open source landscape. Our results show that the framework is effective at
scale, discovering over 52,000 repositories and predicting institutional
affiliation with high accuracy.

</details>


### [444] [Tracing Errors, Constructing Fixes: Repository-Level Memory Error Repair via Typestate-Guided Context Retrieval](https://arxiv.org/abs/2506.18394)
*Xiao Cheng,Zhihao Guo,Huan Huo,Yulei Sui*

Main category: cs.SE

TL;DR: The paper addresses C programming memory-related errors by proposing LTFix, an approach using Large Language Models (LLMs) to automate error repair with a typestate-guided method.


<details>
  <summary>Details</summary>
Motivation: Memory errors in C programming are challenging due to manual memory management, often leading to vulnerabilities requiring sophisticated repairs.

Method: Proposes LTFix, utilizing Large Language Models and a typestate automaton to address interprocedural memory errors and optimize repair context representation.

Result: Successfully tackles repository-level memory issues by capturing error-propagation paths and providing semantically rich data for LLM-based repairs.

Conclusion: LTFix provides an effective solution to memory error repair by leveraging LLMs and typestate-driven context analysis, overcoming key limitations in traditional and deep learning repair strategies.

Abstract: Memory-related errors in C programming continue to pose significant
challenges in software development, primarily due to the complexities of manual
memory management inherent in the language. These errors frequently serve as
vectors for severe vulnerabilities, while their repair requires extensive
knowledge of program logic and C's memory model. Automated Program Repair (APR)
has emerged as a critical research area to address these challenges.
Traditional APR approaches rely on expert-designed strategies and predefined
templates, which are labor-intensive and constrained by the effectiveness of
manual specifications. Deep learning techniques offer a promising alternative
by automatically extracting repair patterns, but they require substantial
training datasets and often lack interpretability.
  This paper introduces LTFix, a novel approach that harnesses the potential of
Large Language Models (LLMs) for automated memory error repair, especially for
complex repository-level errors that span multiple functions and files. We
address two fundamental challenges in LLM-based memory error repair: a limited
understanding of interprocedural memory management patterns and context window
limitations for repository-wide analysis. Our approach utilizes a finite
typestate automaton to guide the tracking of error-propagation paths and
context trace, capturing both spatial (memory states) and temporal (execution
history) dimensions of error behavior. This typestate-guided context retrieval
strategy provides the LLM with concise yet semantically rich information
relevant to erroneous memory management, effectively addressing the token
limitation of LLMs.

</details>


### [445] [Your Token Becomes Worthless: Unveiling Rug Pull Schemes in Crypto Token via Code-and-Transaction Fusion Analysis](https://arxiv.org/abs/2506.18398)
*Hao Wu,Haijun Wang,Shangwang Li,Yin Wu,Ming Fan,Wuxia Jin,Yitao Zhao,Ting Liu*

Main category: cs.SE

TL;DR: Rug Pull scams, malicious cryptocurrency schemes, are addressed by RPhunter, a model combining code analysis and transaction data. It shows superior detection accuracy using machine-learning techniques.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current Rug Pull detection methods that focus solely on either code risks or transaction behaviors, which fail to effectively detect complex, real-world schemes.

Method: RPhunter constructs a semantic risk code graph (SRCG) from code analysis and a token flow behavior graph (TFBG) from transaction data, then employs graph neural networks and attention fusion models to integrate these features for enhanced detection.

Result: RPhunter achieved a precision of 95.3%, recall of 93.8%, and F1 score of 94.5% on a ground-truth dataset, outperforming state-of-the-art methods. It also successfully identified 4,801 Rug Pull tokens in real-world tests with 91% precision.

Conclusion: RPhunter demonstrates an effective advancement in Rug Pull detection by integrating code and transaction analysis, significantly improving the detection of such schemes in both laboratory and real-world scenarios.

Abstract: Rug pull scams have emerged as a persistent threat to cryptocurrency, causing
significant financial losses. A typical scenario involves scammers deploying
honeypot contracts to attract investments, restricting token sales, and
draining the funds, which leaves investors with worthless tokens. Current
methods either rely on predefined patterns to detect code risks or utilize
statistical transaction data to train detection models. However, real-world Rug
Pull schemes often involve a complex interplay between malicious code and
suspicious transaction behaviors. These methods, which solely focus on one
aspect, fall short in detecting such schemes effectively.
  In this paper, we propose RPhunter, a novel technique that integrates code
and transaction for Rug Pull detection. First, RPhunter establishes declarative
rules and performs flow analysis to extract code risk information, further
constructing a semantic risk code graph (SRCG). Meanwhile, to leverage
transaction information, RPhunter formulates dynamic token transaction
activities as a token flow behavior graph (TFBG) in which nodes and edges are
characterized from network structure and market manipulation perspectives.
Finally, RPhunter employs graph neural networks to extract complementary
features from SRCG and TFBG, integrating them through an attention fusion model
to enhance the detection of Rug Pull. We manually analyzed 645 Rug Pull
incidents from code and transaction aspects and constructed a ground-truth
dataset. We evaluated RPhunter on our dataset, achieving a precision of 95.3%,
a recall of 93.8% and an F1 score of 94.5%, which highlights superior
performance compared to existing state-of-the-art methods. Furthermore, when
applied to the real-world scenarios, RPhunter has identified 4801 Rug Pull
tokens, achieving a precision of 91%.

</details>


### [446] [The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs](https://arxiv.org/abs/2506.18403)
*Muntasir Adnan,Carlos C. N. Kuhn*

Main category: cs.SE

TL;DR: AI debugging effectiveness reduces exponentially with repeated attempts, losing 60-80% capability in 2-3 trials. A quantitative framework, Debugging Decay Index (DDI), helps identify intervention points for optimization.


<details>
  <summary>Details</summary>
Motivation: Iterative debugging is critical for AI-driven code generation systems but current AI models suffer rapid effectiveness decay during debugging.

Method: Introduced Debugging Decay Index (DDI), a mathematical framework to quantify debugging decay and predict optimal intervention moments, coupled with a strategic fresh start approach to enhance debugging efficacy.

Result: DDI successfully quantified the decay pattern in AI debugging and illustrated how fresh start interventions can mitigate effectiveness loss.

Conclusion: Current AI debugging has inherent limitations, and DDI provides a path to optimize iterative debugging in code generation systems.

Abstract: The effectiveness of AI debugging follows a predictable exponential decay
pattern; most models lose 60-80% of their debugging capability within just 2-3
attempts, despite iterative debugging being a critical capability for practical
code generation systems. We introduce the Debugging Decay Index (DDI), a
mathematical framework that quantifies when debugging becomes ineffective and
predicts intervention points. Our strategic fresh start approach shifts from
exploitation to exploration at strategic points in the debugging process,
demonstrating that well-timed interventions can rescue the effectiveness of
debugging. DDI reveals a fundamental limitation in current AI debugging and
provides the first quantitative framework for optimising iterative code
generation strategies.

</details>


### [447] [ModeliHub: A Web-based, Federated Analytics Platform for Modelica-centric, Model-based Systems Engineering](https://arxiv.org/abs/2506.18790)
*Mohamad Omar Nachawati*

Main category: cs.SE

TL;DR: ModeliHub is a federated analytics platform tailored for model-based systems engineering with Modelica.


<details>
  <summary>Details</summary>
Motivation: To address the need for integrating heterogeneous engineering artifacts into a unified system model with real-time simulations.

Method: The platform utilizes a Modelica-centric hub-and-spoke federation architecture with an extensible compiler frontend in Isomorphic TypeScript.

Result: ModeliHub enables seamless integration, real-time simulations, and enhanced analysis for iterative systems engineering.

Conclusion: The architecture balances rigor with agility, improving efficiency in engineering domains.

Abstract: This paper introduces ModeliHub, a Web-based, federated analytics platform
designed specifically for model-based systems engineering with Modelica.
ModeliHub's key innovation lies in its Modelica-centric, hub-and-spoke
federation architecture that provides systems engineers with a Modelica-based,
unified system model of repositories containing heterogeneous engineering
artifacts. From this unified system model, ModeliHub's Virtual Twin engine
provides a real-time, interactive simulation environment for deploying Modelica
simulation models that represent digital twins of the virtual prototype of the
system under development at a particular iteration of the iterative systems
engineering life cycle. The implementation of ModeliHub is centered around its
extensible, Modelica compiler frontend developed in Isomorphic TypeScript that
can run seamlessly across browser, desktop and server environments. This
architecture aims to strike a balance between rigor and agility, enabling
seamless integration and analysis across various engineering domains.

</details>


### [448] [Context-Aware CodeLLM Eviction for AI-assisted Coding](https://arxiv.org/abs/2506.18796)
*Kishanthan Thangarajah,Boyuan Chen,Shi Chang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: The paper introduces CACE, a context-aware model eviction strategy, to optimize hosting Code Large Language Models (CodeLLMs) under limited resources by reducing latency and model evictions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in managing and efficiently serving diverse and expanding CodeLLMs under resource constraints, especially in self-hosted environments, while ensuring privacy, latency, and customization.

Method: The method involves proposing CACE, a novel eviction strategy that incorporates multiple context-aware factors like load time, latency sensitivity, expected output length, and sliding-window tracking of usage and demand.

Result: The results show that CACE reduces Time-to-First-Token (TTFT), end-to-end latency, and model evictions compared to state-of-the-art systems when tested on realistic mixed workloads.

Conclusion: CACE effectively balances responsiveness and resource efficiency, offering a scalable and low-latency approach for self-hosted AI coding tools, which is validated through experiments and ablation studies.

Abstract: AI-assisted coding tools powered by Code Large Language Models (CodeLLMs) are
increasingly integrated into modern software development workflows. To address
concerns around privacy, latency, and model customization, many enterprises opt
to self-host these models. However, the diversity and growing number of
CodeLLMs, coupled with limited accelerator memory, introduce practical
challenges in model management and serving efficiency. This paper presents
CACE, a novel context-aware model eviction strategy designed specifically to
optimize self-hosted CodeLLM serving under resource constraints. Unlike
traditional eviction strategies based solely on recency (e.g., Least Recently
Used), CACE leverages multiple context-aware factors, including model load
time, task-specific latency sensitivity, expected output length, and recent
usage and future demand tracked through a sliding window. We evaluate CACE
using realistic workloads that include both latency-sensitive code completion
and throughput-intensive code reasoning tasks. Our experiments show that CACE
reduces Time-to-First-Token (TTFT) and end-to-end (E2E) latency, while
significantly lowering the number of model evictions compared to
state-of-the-art systems. Ablation studies further demonstrate the importance
of multi-factor eviction in balancing responsiveness and resource efficiency.
This work contributes practical strategies for deploying scalable, low-latency
AI coding assistants in real-world software engineering environments.

</details>


### [449] [Understanding Software Engineering Agents: A Study of Thought-Action-Result Trajectories](https://arxiv.org/abs/2506.18824)
*Islem Bouzenia,Michael Pradel*

Main category: cs.SE

TL;DR: The paper conducts a large-scale study of LLM-based software engineering agents to analyze their decision-making processes during tasks like program repair and issue resolution.


<details>
  <summary>Details</summary>
Motivation: Understanding the internal decision-making processes of LLM-based agents to improve their dynamics and failure mode detection.

Method: Analyzed 120 trajectories and 2822 LLM interactions using qualitative and quantitative metrics, capturing patterns in iteration, token usage, reasoning coherence, and feedback integration.

Result: Identified behavioral motifs and anti-patterns distinguishing successful and failed task executions, offering insights for better agent design and operation.

Conclusion: Proposes improvements for LLM-based agents and releases datasets and tools for further research in autonomous software engineering systems.

Abstract: Large Language Model (LLM)-based agents are increasingly employed to automate
complex software engineering tasks such as program repair and issue resolution.
These agents operate by autonomously generating natural language thoughts,
invoking external tools, and iteratively refining their solutions. Despite
their widespread adoption, the internal decision-making processes of these
agents remain largely unexplored, limiting our understanding of their
operational dynamics and failure modes. In this paper, we present a large-scale
empirical study of the thought-action-result trajectories of three
state-of-the-art LLM-based agents: \textsc{RepairAgent},
\textsc{AutoCodeRover}, and \textsc{OpenHands}. We unify their interaction logs
into a common format, capturing 120 trajectories and 2822 LLM interactions
focused on program repair and issue resolution. Our study combines quantitative
analyses of structural properties, action patterns, and token usage with
qualitative assessments of reasoning coherence and feedback integration. We
identify key trajectory characteristics such as iteration counts and token
consumption, recurring action sequences, and the semantic coherence linking
thoughts, actions, and their results. Our findings reveal behavioral motifs and
anti-patterns that distinguish successful from failed executions, providing
actionable insights for improving agent design, including prompting strategies,
failure diagnosis, and anti-pattern detection. We release our dataset and
annotation framework to support further research on transparent and robust
autonomous software engineering agents.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [450] [The Sex-Dependent Effects of Psychedelics on Myelination in APOE4 Mice](https://arxiv.org/abs/2506.17293)
*Sanjana Shankar*

Main category: q-bio.NC

TL;DR: This study explores the link between psychedelics and myelination issues in APOE4-related neuropsychiatric disorders, highlighting sex-specific outcomes.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms of APOE4-related myelin abnormalities and investigate the therapeutic potential of psychedelics in addressing these issues, particularly focusing on sex-dependent differences in outcomes.

Method: The study involved administering a psychedelic compound (DOI) to APOE4 mutant mice and analyzing changes in myelin basic protein (MBP) levels in the brain regions and behavioral changes, focusing on sex-specific differences.

Result: Preliminary findings showed an increase in MBP levels in the CA1 and CA2 brain regions exclusively in female APOE4 mice and reduced anxiety-like behavior in these mice post-DOI administration.

Conclusion: The research demonstrates sex-specific biological mechanisms in APOE4-related brain degeneration and underscores the potential for developing tailored therapies targeting myelin abnormalities based on sex.

Abstract: Several studies have linked myelin abnormalities with neuropsychiatric
disorders; others have implicated psychedelics as a potential therapeutic for
such conditions. One risk factor for these demyelinating disorders is a
mutation in the Apolipoprotein E gene known as APOE4. This variant impedes the
cholesterol regulation of oligodendrocytes responsible for the myelination, or
insulation, of neurons when compared to the wild-type phenotype. In this work,
I advance knowledge of cellular pathways involved in the progression of
APOE4-related diseases and elucidate the effects of psychedelics on the brain.
Myelin sheaths are vital for maintaining neural pathways, and healthy
oligodendrocytes serve as a prerequisite for axonal integrity. Further, the
Kaufer Lab has observed significant behavioral differences between male and
female APOE4 mice following psychedelic treatment with
2,5-Dimethoxy-4-iodoamphetamine, or DOI, a serotonin receptor ligand. The
sex-dependent mechanisms influencing symptom differences and treatment outcomes
in AD are unclear, and could be key to developing successful therapeutics for
myelin-related issues. I hypothesize that administration of DOI will increase
the myelination activity of oligodendrocytes in female APOE4 mice compared with
their male counterparts or controls. Preliminary results show a significant
increase in MBP in the CA1, or short-term, and CA2, or long term, areas in only
female APOE4 mice post-introduction of DOI to the system. This aligns with
behavioral data indicating fewer anxiety-related behaviors in female APOE4 mice
after DOI administration. These findings reveal distinct biological mechanisms
in male and female brain degeneration and suggest potential for sex-specific
therapeutics.

</details>


### [451] [PaceLLM: Brain-Inspired Large Language Models for Long-Context Understanding](https://arxiv.org/abs/2506.17310)
*Kangcong Li,Peng Ye,Chongjun Tu,Lin Zhang,Chunfeng Song,Jiamin Wu,Tao Yang,Qihao Zheng,Tao Chen*

Main category: q-bio.NC

TL;DR: This paper introduces PaceLLM, a brain-inspired optimization for LLMs, addressing long-context limitations via persistent neural activation and modular clustering strategies.


<details>
  <summary>Details</summary>
Motivation: The need to overcome LLMs' performance degradation in long-context tasks caused by information decay and semantic fragmentation.

Method: Two innovations: Persistent Activity Mechanism (PA) for activation memory retrieval and Cortical Expert Clustering (CE) for semantic modularization.

Result: Outperforms benchmarks with improvements of 6% on Multi-document QA, 12.5-17.5% on Infinite-Bench tasks, and extends context length to 200K tokens.

Conclusion: PaceLLM enhances long-context handling and interpretability in LLMs, providing a generalizable, brain-inspired optimization method without requiring structural changes.

Abstract: While Large Language Models (LLMs) demonstrate strong performance across
domains, their long-context capabilities are limited by transient neural
activations causing information decay and unstructured feed-forward network
(FFN) weights leading to semantic fragmentation. Inspired by the brain's
working memory and cortical modularity, we propose PaceLLM, featuring two
innovations: (1) a Persistent Activity (PA) Mechanism that mimics prefrontal
cortex (PFC) neurons' persistent firing by introducing an activation-level
memory bank to dynamically retrieve, reuse, and update critical FFN states,
addressing contextual decay; and (2) Cortical Expert (CE) Clustering that
emulates task-adaptive neural specialization to reorganize FFN weights into
semantic modules, establishing cross-token dependencies and mitigating
fragmentation. Extensive evaluations show that PaceLLM achieves 6% improvement
on LongBench's Multi-document QA and 12.5-17.5% performance gains on
Infinite-Bench tasks, while extending measurable context length to 200K tokens
in Needle-In-A-Haystack (NIAH) tests. This work pioneers brain-inspired LLM
optimization and is complementary to other works. Besides, it can be
generalized to any model and enhance their long-context performance and
interpretability without structural overhauls.

</details>


### [452] [Challenges in Grounding Language in the Real World](https://arxiv.org/abs/2506.17375)
*Peter Lindes,Kaoutar Skiker*

Main category: q-bio.NC

TL;DR: The paper addresses building an AI system where humans can collaborate with robots through natural language, combining cognitive agents and large language models.


<details>
  <summary>Details</summary>
Motivation: To enable seamless communication and collaboration between humans and physical robots via natural language.

Method: Integrating cognitive agents tailored for interactive task learning with large language models within a robotic framework.

Result: Outlined an approach to merging cognitive agent capabilities and linguistic models, and suggested a potential initial implementation.

Conclusion: Integrating cognitive AI and language models holds promise for enhancing human-robot collaboration using natural language.

Abstract: A long-term goal of Artificial Intelligence is to build a language
understanding system that allows a human to collaborate with a physical robot
using language that is natural to the human. In this paper we highlight some of
the challenges in doing this, and propose a solution that integrates the
abilities of a cognitive agent capable of interactive task learning in a
physical robot with the linguistic abilities of a large language model. We also
point the way to an initial implementation of this approach.

</details>


### [453] [Sequence-to-Sequence Models with Attention Mechanistically Map to the Architecture of Human Memory Search](https://arxiv.org/abs/2506.17424)
*Nikolaus Salvatore,Qiong Zhang*

Main category: q-bio.NC

TL;DR: The paper explores the similarities between human memory models and certain AI architectures, proposing an AI-driven cognitive model for memory search.


<details>
  <summary>Details</summary>
Motivation: To understand why humans rely on context-based memory models and to identify parallels with AI architectures for deeper insights into cognitive processes.

Method: Utilized recurrent neural networks (RNNs) with attention from neural machine translation as a proxy to model human memory search.

Result: The proposed AI-based model mimics human memory dynamics, aligning with both averaged and optimal behavioral patterns, while demonstrating additional capabilities.

Conclusion: This AI-driven approach deepens the understanding of human memory's functional role and creates a nuanced framework for modeling cognitive processes and memory search.

Abstract: Past work has long recognized the important role of context in guiding how
humans search their memory. While context-based memory models can explain many
memory phenomena, it remains unclear why humans develop such architectures over
possible alternatives in the first place. In this work, we demonstrate that
foundational architectures in neural machine translation -- specifically,
recurrent neural network (RNN)-based sequence-to-sequence models with attention
-- exhibit mechanisms that directly correspond to those specified in the
Context Maintenance and Retrieval (CMR) model of human memory. Since neural
machine translation models have evolved to optimize task performance, their
convergence with human memory models provides a deeper understanding of the
functional role of context in human memory, as well as presenting new ways to
model human memory. Leveraging this convergence, we implement a neural machine
translation model as a cognitive model of human memory search that is both
interpretable and capable of capturing complex dynamics of learning. We show
that our model accounts for both averaged and optimal human behavioral patterns
as effectively as context-based memory models. Further, we demonstrate
additional strengths of the proposed model by evaluating how memory search
performance emerges from the interaction of different model components.

</details>


### [454] [The Relationship between Cognition and Computation: "Global-first" Cognition versus Local-first Computation](https://arxiv.org/abs/2506.17970)
*Lin Chen*

Main category: q-bio.NC

TL;DR: The paper identifies a key research question for developing brain-like AI or AGI: the relationship between cognition and computation (RCC), categorized into four specific dimensions.


<details>
  <summary>Details</summary>
Motivation: To uncover foundational principles necessary to advance toward AI systems capable of performing any intellectual task that humans can, akin to exploring fundamental concepts like computation or information.

Method: The paper provides a conceptual framework by categorizing RCC into four aspects: primitives, anatomical structures, emergents, and mathematical foundations, guiding the research focus.

Result: RCC categorization provides a structured perspective for analyzing interconnections between human cognition and artificial computation.

Conclusion: Understanding RCC across these defined categories is critical for addressing broader challenges of AGI development and its alignment with human cognitive capabilities.

Abstract: What fundamental research questions are essential for advancing toward
brain-like AI or AGI (Artificial General Intelligence) capable of performing
any intellectual task a human can? Should it be something like the Turing
machine (1936), which answers the question "What is computation?" and lays the
foundation for the entire field of computer science? Or should it be something
like Shannon's mathematical theory of communication (1948), which answers the
question "What is information?" and forms the basis for modern communication
technology? We believe the key question today is the relationship between
cognition and computation (RCC). For example, the widely discussed question
"Will artificial intelligence replace the human mind?" is, in essence and in
scientific terms, an issue concerning RCC. We have chosen to classify RCC into
four categories: 1. The relationship between the primitives of cognition and
the primitives of computation. 2. The relationship between the anatomical
structure of neural representation of cognition and the computational
architecture of artificial intelligence. 3. The relationship between emergents
in cognition and emergents in computation. 4. The relationship between the
mathematical foundations of cognition and computation.

</details>


### [455] [Perceptual multistability: a window for a multi-facet understanding of psychiatric disorders](https://arxiv.org/abs/2506.18176)
*Shervin Safavi,Danaé Rolland,Philipp Sterzer,Renaud Jardri,Pantelis Leptourgos*

Main category: q-bio.NC

TL;DR: This paper explores perceptual multistability as a way to bridge computational psychiatry approaches, offering clinical and translational research insights.


<details>
  <summary>Details</summary>
Motivation: To better understand cognitive functions, dysfunctions, and clinical distinctions by analyzing perceptual multistability and its neural mechanisms.

Method: Synthesizing two computational psychiatry approaches: Bayesian modeling and reinforcement learning perspectives to study perceptual multistability.

Result: Perceptual multistability links human and animal studies, connecting behavior to neural mechanisms, and emerges as a useful non-invasive clinical tool.

Conclusion: Perceptual multistability advances our mechanistic understanding and has translational potential for cognitive impairments across clinical and non-clinical populations.

Abstract: Perceptual multistability, observed across species and sensory modalities,
offers valuable insights into numerous cognitive functions and dysfunctions.
For instance, differences in temporal dynamics and information integration
during percept formation often distinguish clinical from non-clinical
populations. Computational psychiatry can elucidate these variations, through
two primary approaches: (i) Bayesian modeling, which treats perception as an
unconscious inference, and (ii) an active, information-seeking perspective
(e.g., reinforcement learning) framing perceptual switches as internal actions.
Our synthesis aims to leverage multistability to bridge these computational
psychiatry subfields, linking human and animal studies as well as connecting
behavior to underlying neural mechanisms. Perceptual multistability emerges as
a promising non-invasive tool for clinical applications, facilitating
translational research and enhancing our mechanistic understanding of cognitive
processes and their impairments.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [456] [Coupled Entropy: A Goldilocks Generalization?](https://arxiv.org/abs/2506.17229)
*Kenric P. Nelson*

Main category: stat.ML

TL;DR: A proposal for Coupled Entropy to address normalization instability in Nonextensive Statistical Mechanics while modeling complex systems.


<details>
  <summary>Details</summary>
Motivation: To resolve the instability observed in normalized Tsallis entropy used in modeling complex systems, while preserving robustness for practical applications.

Method: The paper introduces Coupled Entropy, defined as Normalized Tsallis Entropy divided by a factor involving system dimensions and coupling parameters.

Result: The formulation of Coupled Entropy yields stable maximizing distributions, known as coupled exponential families, with parameters tied to system nonlinearity and tail characteristics.

Conclusion: Coupling serves as an effective measure of nonlinearity and statistical complexity, addressing foundational issues in Nonextensive Statistical Mechanics.

Abstract: Nonextensive Statistical Mechanics (NSM) has developed into a powerful
toolset for modeling and analyzing complex systems. Despite its many successes,
a puzzle arose early in its development. The constraints on the Tsallis entropy
are in the form of an escort distribution with elements proportional to
$p_i^q$, but this same factor within the Tsallis entropy function is not
normalized. This led to consideration of the Normalized Tsallis Entropy (NTE);
however, the normalization proved to make the function unstable. I will provide
evidence that the coupled entropy, which divides NTE by $1 + d\kappa$, where
$d$ is the dimension and $\kappa$ is the coupling, may provide the necessary
robustness necessary for applications like machine learning. The definition for
the coupled entropy and its maximizing distributions, the coupled exponential
family, arises from clarifying how the number of independent random variables
$(q)$ is composed of the nonlinear properties of complex systems,
$q=1+\frac{\alpha\kappa}{1+d\kappa}$, where $\alpha$ is the nonlinear parameter
governing the shape of distributions near their location and $\kappa$ is the
parameter determining the asymptotic tail decay. Foundationally, for complex
systems, the coupling is the measure of nonlinearity inducing non-exponential
distributions and the degree of nonadditivity entropy. As such, the coupling is
a strong candidate as a measure of statistical complexity.

</details>


### [457] [Differentiable neural network representation of multi-well, locally-convex potentials](https://arxiv.org/abs/2506.17242)
*Reese E. Jones,Adrian Buganza Tepole,Jan N. Fuhg*

Main category: stat.ML

TL;DR: The paper introduces a differentiable and convex neural network model, LSE-ICNN, for handling multimodal phenomena in various scientific domains.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for smooth, differentiable models for multimodal systems while retaining computational tractability and convexity.

Method: The authors propose the LSE-ICNN, which employs a log-sum-exponential mixture of input convex neural network modes for smooth and adaptable multimodal modeling.

Result: The LSE-ICNN models multimodal phenomena efficiently across various domains like mechanochemical phase transformations and variational inference.

Conclusion: The LSE-ICNN enables adaptive, effective, and differentiable modeling for diverse scientific and engineering applications.

Abstract: Multi-well potentials are ubiquitous in science, modeling phenomena such as
phase transitions, dynamic instabilities, and multimodal behavior across
physics, chemistry, and biology. In contrast to non-smooth minimum-of-mixture
representations, we propose a differentiable and convex formulation based on a
log-sum-exponential (LSE) mixture of input convex neural network (ICNN) modes.
This log-sum-exponential input convex neural network (LSE-ICNN) provides a
smooth surrogate that retains convexity within basins and allows for
gradient-based learning and inference.
  A key feature of the LSE-ICNN is its ability to automatically discover both
the number of modes and the scale of transitions through sparse regression,
enabling adaptive and parsimonious modeling. We demonstrate the versatility of
the LSE-ICNN across diverse domains, including mechanochemical phase
transformations, microstructural elastic instabilities, conservative biological
gene circuits, and variational inference for multimodal probability
distributions. These examples highlight the effectiveness of the LSE-ICNN in
capturing complex multimodal landscapes while preserving differentiability,
making it broadly applicable in data-driven modeling, optimization, and
physical simulation.

</details>


### [458] [Gaussian Processes and Reproducing Kernels: Connections and Equivalences](https://arxiv.org/abs/2506.17366)
*Motonobu Kanagawa,Philipp Hennig,Dino Sejdinovic,Bharath K. Sriperumbudur*

Main category: stat.ML

TL;DR: This work examines the connection between Gaussian processes (probabilistic methods) and Reproducing Kernel Hilbert Spaces (RKHS, non-probabilistic methods) in various tasks in machine learning and statistics.


<details>
  <summary>Details</summary>
Motivation: The study aims to unify and establish connections between Gaussian process and RKHS-based methods, which are often researched independently by two separate communities, despite their overlap.

Method: The paper reviews and establishes equivalences between Gaussian Hilbert spaces and RKHS by addressing fundamental topics like regression, interpolation, and numerical integration. It focuses on unity from theoretical and practical perspectives.

Result: The findings present detailed equivalences and connections across Gaussian processes and RKHS methods, bridging the two frameworks effectively for a range of applications.

Conclusion: This work provides a unified framework to understand and leverage the connections between Gaussian processes and RKHS methods for broader applicability in machine learning and other fields.

Abstract: This monograph studies the relations between two approaches using positive
definite kernels: probabilistic methods using Gaussian processes, and
non-probabilistic methods using reproducing kernel Hilbert spaces (RKHS). They
are widely studied and used in machine learning, statistics, and numerical
analysis. Connections and equivalences between them are reviewed for
fundamental topics such as regression, interpolation, numerical integration,
distributional discrepancies, and statistical dependence, as well as for sample
path properties of Gaussian processes. A unifying perspective for these
equivalences is established, based on the equivalence between the Gaussian
Hilbert space and the RKHS. The monograph serves as a basis to bridge many
other methods based on Gaussian processes and reproducing kernels, which are
developed in parallel by the two research communities.

</details>


### [459] [Scalable Machine Learning Algorithms using Path Signatures](https://arxiv.org/abs/2506.17634)
*Csaba Tóth*

Main category: stat.ML

TL;DR: The thesis explores path signatures from stochastic analysis for improving machine learning on sequential and structured data, providing theoretical and computational innovations for time series and graph data challenges.


<details>
  <summary>Details</summary>
Motivation: To address challenges like evolving dynamics, long-range dependencies, and irregular sampling in real-world time series and graph data by leveraging path signatures for scalable machine learning.

Method: The thesis develops models and frameworks integrating path signatures into machine learning using Gaussian processes, deep learning frameworks like Seq2Tens, graph-based methods, and scalable kernel approximations like Random Fourier Signature Features.

Result: The proposed methods achieve uncertainty-aware time series modeling, efficient deep learning for long-range dependencies, expressive graph-based models, and adaptive multi-horizon time series forecasting.

Conclusion: Path signatures, as part of a methodological toolkit, provide a robust and scalable means to tackle challenges in sequential and structured data, bridging theoretical insights with computational efficiency.

Abstract: The interface between stochastic analysis and machine learning is a rapidly
evolving field, with path signatures - iterated integrals that provide
faithful, hierarchical representations of paths - offering a principled and
universal feature map for sequential and structured data. Rooted in rough path
theory, path signatures are invariant to reparameterization and well-suited for
modelling evolving dynamics, long-range dependencies, and irregular sampling -
common challenges in real-world time series and graph data.
  This thesis investigates how to harness the expressive power of path
signatures within scalable machine learning pipelines. It introduces a suite of
models that combine theoretical robustness with computational efficiency,
bridging rough path theory with probabilistic modelling, deep learning, and
kernel methods. Key contributions include: Gaussian processes with signature
kernel-based covariance functions for uncertainty-aware time series modelling;
the Seq2Tens framework, which employs low-rank tensor structure in the weight
space for scalable deep modelling of long-range dependencies; and graph-based
models where expected signatures over graphs induce hypo-elliptic diffusion
processes, offering expressive yet tractable alternatives to standard graph
neural networks. Further developments include Random Fourier Signature
Features, a scalable kernel approximation with theoretical guarantees, and
Recurrent Sparse Spectrum Signature Gaussian Processes, which combine Gaussian
processes, signature kernels, and random features with a principled forgetting
mechanism for multi-horizon time series forecasting with adaptive context
length.
  We hope this thesis serves as both a methodological toolkit and a conceptual
bridge, and provides a useful reference for the current state of the art in
scalable, signature-based learning for sequential and structured data.

</details>


### [460] [Derandomizing Simultaneous Confidence Regions for Band-Limited Functions by Improved Norm Bounds and Majority-Voting Schemes](https://arxiv.org/abs/2506.17764)
*Balázs Csanád Csáji,Bálint Horváth*

Main category: stat.ML

TL;DR: The paper enhances confidence regions for band-limited functions using Paley-Wiener spaces, tighter kernel norms, and voting mechanisms.


<details>
  <summary>Details</summary>
Motivation: The need for robust confidence regions for band-limited functions from noisy data in systems theory and signal processing.

Method: Refinement of kernel norms using Hoeffding's and empirical Bernstein bounds, threshold-based bound selection, and aggregated confidence sets through majority voting.

Result: Validated refinements showed improved stability and coverage in simulations for simultaneous confidence intervals.

Conclusion: Refinements boost accuracy and ensure reliable, nonparametric confidence regions for band-limited functions.

Abstract: Band-limited functions are fundamental objects that are widely used in
systems theory and signal processing. In this paper we refine a recent
nonparametric, nonasymptotic method for constructing simultaneous confidence
regions for band-limited functions from noisy input-output measurements, by
working in a Paley-Wiener reproducing kernel Hilbert space. Kernel norm bounds
are tightened using a uniformly-randomized Hoeffding's inequality for small
samples and an empirical Bernstein bound for larger ones. We derive an
approximate threshold, based on the sample size and how informative the inputs
are, that governs which bound to deploy. Finally, we apply majority voting to
aggregate confidence sets from random subsamples, boosting both stability and
region size. We prove that even per-input aggregated intervals retain their
simultaneous coverage guarantee. These refinements are also validated through
numerical experiments.

</details>


### [461] [DRO-Augment Framework: Robustness by Synergizing Wasserstein Distributionally Robust Optimization and Data Augmentation](https://arxiv.org/abs/2506.17874)
*Jiaming Hu,Debarghya Mukherjee,Ioannis Ch. Paschalidis*

Main category: stat.ML

TL;DR: This paper proposes DRO-Augment, a framework combining Wasserstein Distributionally Robust Optimization with data augmentation to enhance model robustness against corruption and adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks used in image classification face challenges in robustness and stability against input perturbations, necessitating improved methods to handle corrupted data and adversarial attacks.

Method: The authors introduce DRO-Augment, which leverages W-DRO and various data augmentation techniques to enhance resilience in models across different datasets.

Result: DRO-Augment improves robustness against severe data perturbations and adversarial attacks while maintaining accuracy, as demonstrated on benchmarks like CIFAR-10-C and MNIST.

Conclusion: This work demonstrates practical and theoretical advancements in improving DNN robustness, introducing a powerful framework with novel generalization error bounds.

Abstract: In many real-world applications, ensuring the robustness and stability of
deep neural networks (DNNs) is crucial, particularly for image classification
tasks that encounter various input perturbations. While data augmentation
techniques have been widely adopted to enhance the resilience of a trained
model against such perturbations, there remains significant room for
improvement in robustness against corrupted data and adversarial attacks
simultaneously. To address this challenge, we introduce DRO-Augment, a novel
framework that integrates Wasserstein Distributionally Robust Optimization
(W-DRO) with various data augmentation strategies to improve the robustness of
the models significantly across a broad spectrum of corruptions. Our method
outperforms existing augmentation methods under severe data perturbations and
adversarial attack scenarios while maintaining the accuracy on the clean
datasets on a range of benchmark datasets, including but not limited to
CIFAR-10-C, CIFAR-100-C, MNIST, and Fashion-MNIST. On the theoretical side, we
establish novel generalization error bounds for neural networks trained using a
computationally efficient, variation-regularized loss function closely related
to the W-DRO problem.

</details>


### [462] [Identifiable Convex-Concave Regression via Sub-gradient Regularised Least Squares](https://arxiv.org/abs/2506.18078)
*William Chung*

Main category: stat.ML

TL;DR: This paper introduces ICCNLS, a regression method combining convex and concave decomposition with statistical orthogonality for improved predictive accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance the interpretability and predictive accuracy of regression models for complex relationships by addressing decomposition ambiguities and incorporating statistical constraints.

Method: The proposed ICCNLS method combines convex-concave decomposition, orthogonality constraints to enforce identifiability, and regularisation techniques like L1, L2, and elastic net on sub-gradients.

Result: ICCNLS outperforms traditional CNLS and DC regression models in terms of predictive accuracy and simplicity, with successful applications to synthetic and real-world data, including healthcare pricing datasets.

Conclusion: Statistical identifiability, combined with convex-concave decomposition and sub-gradient regularisation, provides interpretable models relevant for various practical applications.

Abstract: We propose a novel nonparametric regression method that models complex
input-output relationships as the sum of convex and concave components. The
method-Identifiable Convex-Concave Nonparametric Least Squares
(ICCNLS)-decomposes the target function into additive shape-constrained
components, each represented via sub-gradient-constrained affine functions. To
address the affine ambiguity inherent in convex-concave decompositions, we
introduce global statistical orthogonality constraints, ensuring that residuals
are uncorrelated with both intercept and input variables. This enforces
decomposition identifiability and improves interpretability. We further
incorporate L1, L2 and elastic net regularisation on sub-gradients to enhance
generalisation and promote structural sparsity. The proposed method is
evaluated on synthetic and real-world datasets, including healthcare pricing
data, and demonstrates improved predictive accuracy and model simplicity
compared to conventional CNLS and difference-of-convex (DC) regression
approaches. Our results show that statistical identifiability, when paired with
convex-concave structure and sub-gradient regularisation, yields interpretable
models suited for forecasting, benchmarking, and policy evaluation.

</details>


### [463] [Phase transition of \emph{descending} phase retrieval algorithms](https://arxiv.org/abs/2506.18275)
*Mihailo Stojnic*

Main category: stat.ML

TL;DR: The paper explores descending phase retrieval algorithms, using Random Duality Theory to statistically characterize their performance and identify key geometric structures like parametric manifolds and funneling points. It establishes a link between manifold structure and algorithmic success, revealing a phase transition tied to sample complexity. A practical hybrid algorithm variant is also developed.


<details>
  <summary>Details</summary>
Motivation: The study seeks to understand theoretical limits of descending phase retrieval algorithms, investigating the mathematical structures and behaviors that dictate their success or failure.

Method: The authors use Random Duality Theory (RDT) to analyze algorithmic performance metrics and investigate the geometric structures (like parametric manifolds and funneling points) that influence behavior. Additionally, they explore phase transitions in sample complexity and propose a hybrid algorithm combining barrier and gradient descent methods.

Result: The analysis revealed phase transitions wherein parametric manifolds transition from multi to single funneling points as sample complexity increases. This phenomenon determines algorithmic success. Simulations confirmed these theoretical predictions for dimensions as small as a few hundred.

Conclusion: Descending phase retrieval algorithms' success is governed by the structure of parametric manifolds, specifically the transition to single funneling points with increased sample complexity. Theoretical outcomes align with practical simulations, offering insights and a prototype hybrid algorithm.

Abstract: We study theoretical limits of \emph{descending} phase retrieval algorithms.
Utilizing \emph{Random duality theory} (RDT) we develop a generic program that
allows statistical characterization of various algorithmic performance metrics.
Through these we identify the concepts of \emph{parametric manifold} and its
\emph{funneling points} as key mathematical objects that govern the underlying
algorithms' behavior. An isomorphism between single funneling point manifolds
and global convergence of descending algorithms is established. The structure
and shape of the parametric manifold as well as its dependence on the sample
complexity are studied through both plain and lifted RDT. Emergence of a phase
transition is observed. Namely, as sample complexity increases, parametric
manifold transitions from a multi to a single funneling point structure. This
in return corresponds to a transition from the scenarios where descending
algorithms generically fail to the scenarios where they succeed in solving
phase retrieval. We also develop and implement a practical algorithmic variant
that in a hybrid alternating fashion combines a barrier and a plain gradient
descent. Even though the theoretical results are obtained for infinite
dimensional scenarios (and consequently non-jittery parametric manifolds), we
observe a strong agrement between theoretical and simulated phase transitions
predictions for fairly small dimensions on the order of a few hundreds.

</details>


### [464] [Optimal spectral initializers impact on phase retrieval phase transitions -- an RDT view](https://arxiv.org/abs/2506.18279)
*Mihailo Stojnic*

Main category: stat.ML

TL;DR: This paper analyzes how spectral initializers, specifically overlap optimal spectral initializers (OptSpins), influence the effectiveness of descending phase retrieval algorithms (dPR), focusing on how they interact with critical problem structures to affect solution outcomes.


<details>
  <summary>Details</summary>
Motivation: To understand the influence of spectral initializers on descending phase retrieval algorithms and determine the conditions under which these algorithms successfully solve phase retrieval challenges, considering theoretical limits and practical constraints.

Method: The authors use Random Duality Theory (RDT) to statistically characterize the starting conditions (overlap structures) provided by overlap optimal spectral initializers (OptSpins) for dPRs. They also analyze how critical problem regions called "flat regions" influence algorithm performance.

Result: The theoretical analysis reveals two key findings: (i) achieving a theoretical phase transition might be difficult due to large flat regions in the problem structure, leading OptSpins to fail within them; (ii) employing a safer compression strategy (slightly increasing the sample complexity ratio $\alpha$) shrinks flat regions, improving the success probability of dPRs. Numerical simulations confirm the theoretical insights.

Conclusion: The findings highlight the importance of understanding the interplay between spectral initializers and problem structures in phase retrieval tasks. Adjusting parameters such as sample complexity can significantly improve the performance of descending phase retrieval algorithms.

Abstract: We analyze the relation between spectral initializers and theoretical limits
of \emph{descending} phase retrieval algorithms (dPR). In companion paper
[104], for any sample complexity ratio, $\alpha$, \emph{parametric manifold},
${\mathcal {PM}}(\alpha)$, is recognized as a critically important structure
that generically determines dPRs abilities to solve phase retrieval (PR).
Moreover, overlap between the algorithmic solution and the true signal is
positioned as a key ${\mathcal {PM}}$'s component. We here consider the
so-called \emph{overlap optimal} spectral initializers (OptSpins) as dPR's
starting points and develop a generic \emph{Random duality theory} (RDT) based
program to statistically characterize them. In particular, we determine the
functional structure of OptSpins and evaluate the starting overlaps that they
provide for the dPRs. Since ${\mathcal {PM}}$'s so-called \emph{flat regions}
are highly susceptible to \emph{local jitteriness} and as such are key
obstacles on dPR's path towards PR's global optimum, a precise characterization
of the starting overlap allows to determine if such regions can be successfully
circumvented. Through the presented theoretical analysis we observe two key
points in that regard: \textbf{\emph{(i)}} dPR's theoretical phase transition
(critical $\alpha$ above which they solve PR) might be difficult to practically
achieve as the ${\mathcal {PM}}$'s flat regions are large causing the
associated OptSpins to fall exactly within them; and \textbf{\emph{(ii)}}
Opting for so-called ``\emph{safer compression}'' and slightly increasing
$\alpha$ (by say $15\%$) shrinks flat regions and allows OptSpins to fall
outside them and dPRs to ultimately solve PR. Numerical simulations are
conducted as well and shown to be in an excellent agreement with theoretical
predictions.

</details>


### [465] [Phase retrieval with rank $d$ measurements -- \emph{descending} algorithms phase transitions](https://arxiv.org/abs/2506.18282)
*Mihailo Stojnic*

Main category: stat.ML

TL;DR: This paper extends the Random Duality Theory (RDT) for analyzing phase retrieval problems by studying rank $d$ positive definite measurements and identifying phase transitions in sample complexity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to generalize the analytical power of Random Duality Theory to not just descending phase retrieval algorithms, but also to multi-rank ($d > 1$) positive definite phase retrieval problems, thereby broadening its applicability.

Method: The authors generalize RDT to handle rank $d$ measurements and analytically determine the locations of phase transitions in the sample complexity ratio for descending phase retrieval algorithms. They also complement the theoretical findings by implementing and simulating a log-barrier gradient descent variant.

Result: The study unveils that the success of descending phase retrieval algorithms is governed by a phase transition in sample complexity. Simulations of the log-barrier gradient descent match theoretical predictions closely, even with small-dimensional problems.

Conclusion: The extended RDT framework successfully predicts phase transitions in sample complexity for rank $d$ positive definite phase retrieval, with simulations aligning well with theoretical outcomes, validating the approach.

Abstract: Companion paper [118] developed a powerful \emph{Random duality theory} (RDT)
based analytical program to statistically characterize performance of
\emph{descending} phase retrieval algorithms (dPR) (these include all variants
of gradient descents and among them widely popular Wirtinger flows). We here
generalize the program and show how it can be utilized to handle rank $d$
positive definite phase retrieval (PR) measurements (with special cases $d=1$
and $d=2$ serving as emulations of the real and complex phase retrievals,
respectively). In particular, we observe that the minimal sample complexity
ratio (number of measurements scaled by the dimension of the unknown signal)
which ensures dPR's success exhibits a phase transition (PT) phenomenon. For
both plain and lifted RDT we determine phase transitions locations. To
complement theoretical results we implement a log barrier gradient descent
variant and observe that, even in small dimensional scenarios (with problem
sizes on the order of 100), the simulated phase transitions are in an excellent
agreement with the theoretical predictions.

</details>


### [466] [Quantifying Uncertainty in the Presence of Distribution Shifts](https://arxiv.org/abs/2506.18283)
*Yuli Slavutsky,David M. Blei*

Main category: stat.ML

TL;DR: The paper presents a Bayesian framework with adaptive priors to address unreliable uncertainty estimation in neural networks under covariate shift, yielding improved results.


<details>
  <summary>Details</summary>
Motivation: Neural networks struggle with reliable uncertainty estimation when there are covariate shifts between training and testing data.

Method: The proposed method introduces an adaptive prior conditioned on training and new covariates, which increases uncertainty for inputs far from the training distribution. Amortized variational inference is used for efficient posterior approximation.

Result: The method produces better uncertainty estimates in synthetic and real-world data under distribution shifts.

Conclusion: Introducing adaptive priors tailored to covariate shifts enhances the reliability of uncertainty models in neural networks.

Abstract: Neural networks make accurate predictions but often fail to provide reliable
uncertainty estimates, especially under covariate distribution shifts between
training and testing. To address this problem, we propose a Bayesian framework
for uncertainty estimation that explicitly accounts for covariate shifts. While
conventional approaches rely on fixed priors, the key idea of our method is an
adaptive prior, conditioned on both training and new covariates. This prior
naturally increases uncertainty for inputs that lie far from the training
distribution in regions where predictive performance is likely to degrade. To
efficiently approximate the resulting posterior predictive distribution, we
employ amortized variational inference. Finally, we construct synthetic
environments by drawing small bootstrap samples from the training data,
simulating a range of plausible covariate shift using only the original
dataset. We evaluate our method on both synthetic and real-world data. It
yields substantially improved uncertainty estimates under distribution shifts.

</details>


### [467] [Theoretical guarantees for neural estimators in parametric statistics](https://arxiv.org/abs/2506.18508)
*Almut Rödder,Manuel Hentschel,Sebastian Engelke*

Main category: stat.ML

TL;DR: The paper studies neural estimators, proposing a decomposition of their risk and providing theoretical guarantees under specific conditions.


<details>
  <summary>Details</summary>
Motivation: Neural estimators are widely used, yet there is a gap in statistical theory regarding their guarantees.

Method: The authors decompose the risk of neural estimators into separate terms and formulate conditions for convergence to zero.

Result: They validate their assumptions and conditions for common applications of neural estimators, offering theoretical insights.

Conclusion: The study provides a framework to establish theoretical guarantees for neural estimators across diverse architectures and problems.

Abstract: Neural estimators are simulation-based estimators for the parameters of a
family of statistical models, which build a direct mapping from the sample to
the parameter vector. They benefit from the versatility of available network
architectures and efficient training methods developed in the field of deep
learning. Neural estimators are amortized in the sense that, once trained, they
can be applied to any new data set with almost no computational cost. While
many papers have shown very good performance of these methods in simulation
studies and real-world applications, so far no statistical guarantees are
available to support these observations theoretically. In this work, we study
the risk of neural estimators by decomposing it into several terms that can be
analyzed separately. We formulate easy-to-check assumptions ensuring that each
term converges to zero, and we verify them for popular applications of neural
estimators. Our results provide a general recipe to derive theoretical
guarantees also for broader classes of architectures and estimation problems.

</details>


### [468] [Trustworthy Prediction with Gaussian Process Knowledge Scores](https://arxiv.org/abs/2506.18630)
*Kurt Butler,Guanchao Feng,Tong Chen,Petar Djuric*

Main category: stat.ML

TL;DR: This paper introduces a knowledge score for Gaussian Process Regression (GPR) that quantifies how much data informs a prediction.


<details>
  <summary>Details</summary>
Motivation: To address the lack of clarity in the reliability of predictions made by probabilistic models, particularly in areas with no observed data.

Method: The authors propose and define a knowledge score for GPR that measures prediction uncertainty reduction. It is bounded between 0 and 1 and is interpretable.

Result: The knowledge score effectively predicts when GPR model predictions will be accurate and enhances the model's performance in tasks like anomaly detection, extrapolation, and missing data imputation.

Conclusion: The proposed knowledge score for GPR improves prediction reliability and task performance, offering an interpretable tool to quantify uncertainty reduction. Code is available online for further application.

Abstract: Probabilistic models are often used to make predictions in regions of the
data space where no observations are available, but it is not always clear
whether such predictions are well-informed by previously seen data. In this
paper, we propose a knowledge score for predictions from Gaussian process
regression (GPR) models that quantifies the extent to which observing data have
reduced our uncertainty about a prediction. The knowledge score is
interpretable and naturally bounded between 0 and 1. We demonstrate in several
experiments that the knowledge score can anticipate when predictions from a GPR
model are accurate, and that this anticipation improves performance in tasks
such as anomaly detection, extrapolation, and missing data imputation. Source
code for this project is available online at
https://github.com/KurtButler/GP-knowledge.

</details>


### [469] [Tight Generalization Error Bounds for Stochastic Gradient Descent in Non-convex Learning](https://arxiv.org/abs/2506.18645)
*Wenjun Xiong,Juan Ding,Xinlei Zuo,Qizhai Li*

Main category: stat.ML

TL;DR: This paper introduces the Type II perturbed SGD (T2pm-SGD) for tighter generalization error bounds in non-convex learning, validated through experiments on MNIST and CIFAR-10.


<details>
  <summary>Details</summary>
Motivation: Understanding and improving the generalization properties of Stochastic Gradient Descent (SGD) in non-convex settings to enhance model performance on unseen data.

Method: The paper develops T2pm-SGD, which integrates perturbation noise and accommodates sub-Gaussian and bounded loss functions. Generalization error bounds are decomposed into trajectory and flatness terms.

Result: T2pm-SGD achieves improved generalization error bounds: $O(n^{-1})$ and $O(n^{-2/3})$ for bounded losses, with a stable flatness term, outperforming previous methods. Results are validated experimentally.

Conclusion: T2pm-SGD consistently improves generalization error bounds by stabilizing the flatness term and refining trajectory terms, offering robust performance for non-convex learning tasks.

Abstract: Stochastic Gradient Descent (SGD) is fundamental for training deep neural
networks, especially in non-convex settings. Understanding SGD's generalization
properties is crucial for ensuring robust model performance on unseen data. In
this paper, we analyze the generalization error bounds of SGD for non-convex
learning by introducing the Type II perturbed SGD (T2pm-SGD), which
accommodates both sub-Gaussian and bounded loss functions. The generalization
error bound is decomposed into two components: the trajectory term and the
flatness term. Our analysis improves the trajectory term to $O(n^{-1})$,
significantly enhancing the previous $O((nb)^{-1/2})$ bound for bounded losses,
where n is the number of training samples and b is the batch size. By selecting
an optimal variance for the perturbation noise, the overall bound is further
refined to $O(n^{-2/3})$. For sub-Gaussian loss functions, a tighter trajectory
term is also achieved. In both cases, the flatness term remains stable across
iterations and is smaller than those reported in previous literature, which
increase with iterations. This stability, ensured by T2pm-SGD, leads to tighter
generalization error bounds for both loss function types. Our theoretical
results are validated through extensive experiments on benchmark datasets,
including MNIST and CIFAR-10, demonstrating the effectiveness of T2pm-SGD in
establishing tighter generalization bounds.

</details>


### [470] [A Random Matrix Analysis of In-context Memorization for Nonlinear Attention](https://arxiv.org/abs/2506.18656)
*Zhenyu Liao,Jiaqing Liu,TianQi Hou,Difan Zou,Zenan Ling*

Main category: stat.ML

TL;DR: This paper analyzes the in-context memorization error of nonlinear attention mechanisms in machine learning, focusing on high-dimensional scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the limited theoretical understanding of attention mechanisms, particularly nonlinear attention, despite their critical role in modern large language models.

Method: The authors employ tools from large kernel random matrices theory to study memorization errors in nonlinear attention, especially in high-dimensional proportional regimes.

Result: Nonlinear attention's memorization error is typically higher than linear ridge regression for random inputs but can improve or surpass under structured inputs aligned with attention weights.

Conclusion: Nonlinear attention's performance is strongly influenced by the interplay between nonlinearity and statistical input structure, and this is validated through numerical experiments.

Abstract: Attention mechanisms have revolutionized machine learning (ML) by enabling
efficient modeling of global dependencies across inputs. Their inherently
parallelizable structures allow for efficient scaling with the exponentially
increasing size of both pretrained data and model parameters. Yet, despite
their central role as the computational backbone of modern large language
models (LLMs), the theoretical understanding of Attentions, especially in the
nonlinear setting, remains limited.
  In this paper, we provide a precise characterization of the \emph{in-context
memorization error} of \emph{nonlinear Attention}, in the high-dimensional
proportional regime where the number of input tokens $n$ and their embedding
dimension $p$ are both large and comparable. Leveraging recent advances in the
theory of large kernel random matrices, we show that nonlinear Attention
typically incurs higher memorization error than linear ridge regression on
random inputs. However, this gap vanishes, and can even be reversed, when the
input exhibits statistical structure, particularly when the Attention weights
align with the input signal direction. Our results reveal how nonlinearity and
input structure interact with each other to govern the memorization performance
of nonlinear Attention. The theoretical insights are supported by numerical
experiments.

</details>


### [471] [Local Averaging Accurately Distills Manifold Structure From Noisy Data](https://arxiv.org/abs/2506.18761)
*Yihan Shen,Shiyu Wang,Arnaud Lamy,Mariam Avagyan,John Wright*

Main category: stat.ML

TL;DR: The paper analyzes the accuracy of local averaging methods for manifold fitting and denoising in high-noise conditions.


<details>
  <summary>Details</summary>
Motivation: High-dimensional data often reside on low-dimensional manifolds, and understanding these manifolds is critical for applications like signal denoising and generation.

Method: The authors theoretically analyze a two-round mini-batch local averaging method applied to noisy samples from high-dimensional manifolds.

Result: They show high-probability bounds on the distance of the averaged point to the manifold in a high-noise regime.

Conclusion: The method serves as a preprocessing step for methods tailored to lower-noise settings and provides a theoretical basis for related techniques relying on local averaging.

Abstract: High-dimensional data are ubiquitous, with examples ranging from natural
images to scientific datasets, and often reside near low-dimensional manifolds.
Leveraging this geometric structure is vital for downstream tasks, including
signal denoising, reconstruction, and generation. However, in practice, the
manifold is typically unknown and only noisy samples are available. A
fundamental approach to uncovering the manifold structure is local averaging,
which is a cornerstone of state-of-the-art provable methods for manifold
fitting and denoising. However, to the best of our knowledge, there are no
works that rigorously analyze the accuracy of local averaging in a manifold
setting in high-noise regimes. In this work, we provide theoretical analyses of
a two-round mini-batch local averaging method applied to noisy samples drawn
from a $d$-dimensional manifold $\mathcal M \subset \mathbb{R}^D$, under a
relatively high-noise regime where the noise size is comparable to the reach
$\tau$. We show that with high probability, the averaged point $\hat{\mathbf
q}$ achieves the bound $d(\hat{\mathbf q}, \mathcal M) \leq \sigma
\sqrt{d\left(1+\frac{\kappa\mathrm{diam}(\mathcal {M})}{\log(D)}\right)}$,
where $\sigma, \mathrm{diam(\mathcal M)},\kappa$ denote the standard deviation
of the Gaussian noise, manifold's diameter and a bound on its extrinsic
curvature, respectively. This is the first analysis of local averaging accuracy
over the manifold in the relatively high noise regime where $\sigma \sqrt{D}
\approx \tau$. The proposed method can serve as a preprocessing step for a wide
range of provable methods designed for lower-noise regimes. Additionally, our
framework can provide a theoretical foundation for a broad spectrum of
denoising and dimensionality reduction methods that rely on local averaging
techniques.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [472] [AutoGraph: A Knowledge-Graph Framework for Modeling Interface Interaction and Automating Procedure Execution in Digital Nuclear Control Rooms](https://arxiv.org/abs/2506.18727)
*Xingyu Xiao,Jiejuan Tong,Jun Sun,Zhe Sui,Jingang Liang,Hongru Zhao,Jun Zhao,Haitao Wang*

Main category: cs.HC

TL;DR: AutoGraph is a framework for digitalizing power plant procedures to reduce human error and improve efficiency through semantic integration.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of existing computer-based procedures in nuclear power plants, focusing on improving human-system integration and reducing human error in dynamic operations.

Method: AutoGraph employs a knowledge graph-based approach, combining modules like HTRPM tracking and interface element knowledge graphs to automate procedure execution, integrated with real-time assessment systems.

Result: Implementing AutoGraph led to significant reductions in task completion times and showed potential in improving human reliability assessment during scenarios.

Conclusion: The study concludes that AutoGraph enhances procedural safety and cognitive performance, offering extensibility to dynamic frameworks and decision-support systems in complex operational environments.

Abstract: Digitalization in nuclear power plant (NPP) control rooms is reshaping how
operators interact with procedures and interface elements. However, existing
computer-based procedures (CBPs) often lack semantic integration with
human-system interfaces (HSIs), limiting their capacity to support intelligent
automation and increasing the risk of human error, particularly under dynamic
or complex operating conditions. In this study, we present AutoGraph, a
knowledge-graph-based framework designed to formalize and automate procedure
execution in digitalized NPP environments.AutoGraph integrates (1) a proposed
HTRPM tracking module to capture operator interactions and interface element
locations; (2) an Interface Element Knowledge Graph (IE-KG) encoding spatial,
semantic, and structural properties of HSIs; (3) automatic mapping from textual
procedures to executable interface paths; and (4) an execution engine that maps
textual procedures to executable interface paths. This enables the
identification of cognitively demanding multi-action steps and supports fully
automated execution with minimal operator input. We validate the framework
through representative control room scenarios, demonstrating significant
reductions in task completion time and the potential to support real-time human
reliability assessment. Further integration into dynamic HRA frameworks (e.g.,
COGMIF) and real-time decision support systems (e.g., DRIF) illustrates
AutoGraph extensibility in enhancing procedural safety and cognitive
performance in complex socio-technical systems.

</details>


### [473] [Crowdsourcing Ubiquitous Indoor Localization with Non-Cooperative Wi-Fi Ranging](https://arxiv.org/abs/2506.18317)
*Emerson Sie,Enguang Fan,Federico Cifuentes-Urtubey,Deepak Vasisht*

Main category: cs.HC

TL;DR: The study introduces PeepLoc, a Wi-Fi-based indoor localization system using existing infrastructure with high accuracy.


<details>
  <summary>Details</summary>
Motivation: To address impracticality and deployment challenges of existing indoor localization methods for real-world applications.

Method: PeepLoc utilizes non-cooperative time-of-flight measurements, pedestrian dead reckoning, and crowdsourcing to initialize anchor points using existing Wi-Fi devices and infrastructure.

Result: PeepLoc achieves median and mean errors of 3.06m and 3.41m, outperforming current indoor systems and rivaling outdoor GPS accuracy.

Conclusion: PeepLoc presents a scalable, deployable indoor localization system that leverages existing Wi-Fi networks and mobile devices effectively.

Abstract: Indoor localization opens the path to potentially transformative
applications. Although many indoor localization methods have been proposed over
the years, they remain too impractical for widespread deployment in the real
world. In this paper, we introduce PeepLoc, a deployable and scalable
Wi-Fi-based solution for indoor localization that relies only on pre-existing
devices and infrastructure. Specifically, PeepLoc works on any mobile device
with an unmodified Wi-Fi transceiver and in any indoor environment with a
sufficient number of Wi-Fi access points (APs) and pedestrian traffic. At the
core of PeepLoc is (a) a mechanism which allows any Wi-Fi device to obtain
non-cooperative time-of-flight (ToF) to any Wi-Fi AP and (b) a novel
bootstrapping mechanism that relies on pedestrian dead reckoning (PDR) and
crowdsourcing to opportunistically initialize pre-existing APs as anchor points
within an environment. We implement PeepLoc using commodity hardware and
evaluate it extensively across 4 campus buildings. We show PeepLoc leads to a
mean and median positional error of 3.41 m and 3.06 m respectively, which is
superior to existing deployed indoor localization systems and is competitive
with commodity GPS in outdoor environments.

</details>


### [474] [BRAVE: Brain-Controlled Prosthetic Arm with Voice Integration and Embodied Learning for Enhanced Mobility](https://arxiv.org/abs/2506.18749)
*Abdul Basit,Maha Nawaz,Muhammad Shafique*

Main category: cs.HC

TL;DR: This paper introduces BRAVE, a hybrid EEG and voice-controlled system for prosthetics, achieving high classification accuracy and real-time adaptability while enabling intuitive control for amputees.


<details>
  <summary>Details</summary>
Motivation: To address challenges like signal noise, classification accuracy, and real-time adaptability in EEG-based brain-computer interface systems for prosthetic limb control.

Method: BRAVE integrates ensemble learning (LSTM, CNN, Random Forest) for EEG signal classification, along with human-in-the-loop correction and speech recognition for intuitive control. Preprocessing includes bandpass filtering, ICA, and CSP feature extraction.

Result: BRAVE achieves 96% classification accuracy, operates in real time with a 150 ms latency, and demonstrates generalizability across users with low-power efficiency for practical deployment.

Conclusion: BRAVE represents a significant advancement in non-invasive, intuitive prosthetic control systems, offering robust and deployable solutions for individuals with upper-limb amputations.

Abstract: Non-invasive brain-computer interfaces (BCIs) have the potential to enable
intuitive control of prosthetic limbs for individuals with upper limb
amputations. However, existing EEG-based control systems face challenges
related to signal noise, classification accuracy, and real-time adaptability.
In this work, we present BRAVE, a hybrid EEG and voice-controlled prosthetic
system that integrates ensemble learning-based EEG classification with a
human-in-the-loop (HITL) correction framework for enhanced responsiveness.
Unlike traditional electromyography (EMG)-based prosthetic control, BRAVE aims
to interpret EEG-driven motor intent, enabling movement control without
reliance on residual muscle activity. To improve classification robustness,
BRAVE combines LSTM, CNN, and Random Forest models in an ensemble framework,
achieving a classification accuracy of 96% across test subjects. EEG signals
are preprocessed using a bandpass filter (0.5-45 Hz), Independent Component
Analysis (ICA) for artifact removal, and Common Spatial Pattern (CSP) feature
extraction to minimize contamination from electromyographic (EMG) and
electrooculographic (EOG) signals. Additionally, BRAVE incorporates automatic
speech recognition (ASR) to facilitate intuitive mode switching between
different degrees of freedom (DOF) in the prosthetic arm. The system operates
in real time, with a response latency of 150 ms, leveraging Lab Streaming Layer
(LSL) networking for synchronized data acquisition. The system is evaluated on
an in-house fabricated prosthetic arm and on multiple participants highlighting
the generalizability across users. The system is optimized for low-power
embedded deployment, ensuring practical real-world application beyond
high-performance computing environments. Our results indicate that BRAVE offers
a promising step towards robust, real-time, non-invasive prosthetic control.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [475] [Parallel nonlinear neuromorphic computing with temporal encoding](https://arxiv.org/abs/2506.17261)
*Guangfeng You,Chao Qian,Hongsheng Chen*

Main category: physics.app-ph

TL;DR: The paper introduces a novel neuromorphic photonic processor utilizing spatiotemporal metasurfaces for advanced computational tasks, achieving robust results in real-time scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of attaining high linear and nonlinear expressivity in neuromorphic photonic systems, while minimizing power consumption and enhancing computational capabilities.

Method: The paper proposes a parallel nonlinear neuromorphic processor that leverages temporal encoding of spatiotemporal metasurfaces to process data and trainable weights, enabling customizable nonlinearity and preserving linear transformation within each time partition.

Result: Experimental demonstrations showcase robust performance in multi-label recognition, multi-task parallel processing, dynamic memory capabilities, and real-time responsiveness in maze-solving tasks.

Conclusion: The work presents a flexible solution for developing temporally-modulated neuromorphic processors, which could be applied to complex, high-efficiency scenarios in real-world applications.

Abstract: The proliferation of deep learning applications has intensified the demand
for electronic hardware with low energy consumption and fast computing speed.
Neuromorphic photonics have emerged as a viable alternative to directly process
high-throughput information at the physical space. However, the simultaneous
attainment of high linear and nonlinear expressivity posse a considerable
challenge due to the power efficiency and impaired manipulability in
conventional nonlinear materials and optoelectronic conversion. Here we
introduce a parallel nonlinear neuromorphic processor that enables arbitrary
superposition of information states in multi-dimensional channels, only by
leveraging the temporal encoding of spatiotemporal metasurfaces to map the
input data and trainable weights. The proposed temporal encoding nonlinearity
is theoretically proved to flexibly customize the nonlinearity, while
preserving quasi-static linear transformation capability within each time
partition. We experimentally demonstrated the concept based on distributed
spatiotemporal metasurfaces, showcasing robust performance in multi-label
recognition and multi-task parallelism with asynchronous modulation.
Remarkably, our nonlinear processor demonstrates dynamic memory capability in
autonomous planning tasks and real-time responsiveness to canonical
maze-solving problem. Our work opens up a flexible avenue for a variety of
temporally-modulated neuromorphic processors tailored for complex scenarios.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [476] [Advanced Game-Theoretic Frameworks for Multi-Agent AI Challenges: A 2025 Outlook](https://arxiv.org/abs/2506.17348)
*Pavel Malinovskiy*

Main category: cs.MA

TL;DR: The paper explores advanced game-theoretic paradigms to address next-gen AI challenges, focusing on multi-agent systems under complexity.


<details>
  <summary>Details</summary>
Motivation: To provide AI researchers with theoretical tools to manage complex, uncertain, and partially adversarial environments.

Method: Using dynamic coalition formation, language-based utilities, Bayesian update methods, moral framing in payoffs, and coding simulations.

Result: Demonstrates how multi-agent systems can adapt through repeated games, adversarial detection, and moral considerations.

Conclusion: A valuable foundation for next-gen AI systems' strategic and adaptable interactions in complex settings.

Abstract: This paper presents a substantially reworked examination of how advanced
game-theoretic paradigms can serve as a foundation for the next-generation
challenges in Artificial Intelligence (AI), forecasted to arrive in or around
2025. Our focus extends beyond traditional models by incorporating dynamic
coalition formation, language-based utilities, sabotage risks, and partial
observability. We provide a set of mathematical formalisms, simulations, and
coding schemes that illustrate how multi-agent AI systems may adapt and
negotiate in complex environments. Key elements include repeated games,
Bayesian updates for adversarial detection, and moral framing within payoff
structures. This work aims to equip AI researchers with robust theoretical
tools for aligning strategic interaction in uncertain, partially adversarial
contexts.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [477] [Tutorial: $\varphi$-Transductions in OpenFst via the Gallic Semiring](https://arxiv.org/abs/2506.17942)
*Marco Cognetta,Cyril Allauzen*

Main category: cs.FL

TL;DR: This paper provides a tutorial on implementing φ-transductions using OpenFst, showcasing the process through the WordPiece tokenization algorithm.


<details>
  <summary>Details</summary>
Motivation: OpenFst supports φ-transitions but lacks straightforward compatibility with transducers, necessitating a workaround for their application.

Method: The authors utilized the Gallic semiring functionality in OpenFst as a workaround to correctly implement φ-transductions, demonstrated via the MaxMatch (WordPiece) tokenization algorithm.

Result: The tutorial demonstrates the feasibility of implementing φ-transductions in OpenFst with practical code examples accompanying the explanation.

Conclusion: Through leveraging alternate OpenFst functionalities, φ-transductions can be effectively applied despite the software's limitations, and this is shown in the context of the WordPiece tokenizer implementation.

Abstract: OpenFst, a popular finite-state transducer library, supports
$\varphi$-transitions but, due to an implementation constraint, they cannot be
used with transducers in a straightforward way.
  In this short tutorial, we describe how one can use other functionality
provided by OpenFst (namely, the Gallic semiring) to correctly implement
$\varphi$-transductions and demonstrate it by implementing the MaxMatch
(WordPiece) tokenization algorithm (Devlin et al., 2019; Song et al., 2021).
Accompanying self-contained code examples are provided.
https://www.openfst.org/twiki/pub/Contrib/FstContrib/phi_transduction_tutorial_code.tgz

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [478] [BrainSymphony: A Transformer-Driven Fusion of fMRI Time Series and Structural Connectivity](https://arxiv.org/abs/2506.18314)
*Moein Khajehnejad,Forough Habibollahi,Adeel Razi*

Main category: q-bio.QM

TL;DR: BrainSymphony introduces a compact foundation model for neuroimaging that achieves state-of-the-art performance with significantly fewer resources.


<details>
  <summary>Details</summary>
Motivation: Current neuroimaging foundation models are too large and require extensive datasets, limiting accessibility and efficiency.

Method: BrainSymphony employs parallel spatial and temporal transformers for fMRI, a signed graph transformer for diffusion MRI, and integrates outputs through an adaptive fusion gate and Perceiver module.

Result: The model outperforms larger models across various benchmarks, provides unique insights through attention maps on psilocybin neuroimaging data, and demonstrates superior multimodal integration.

Conclusion: BrainSymphony showcases that efficient multimodal designs can outperform larger models, enhancing research accessibility in computational neuroscience.

Abstract: Existing foundation models for neuroimaging are often prohibitively large and
data-intensive. We introduce BrainSymphony, a lightweight, parameter-efficient
foundation model that achieves state-of-the-art performance while being
pre-trained on significantly smaller public datasets. BrainSymphony's strong
multimodal architecture processes functional MRI data through parallel spatial
and temporal transformer streams, which are then efficiently distilled into a
unified representation by a Perceiver module. Concurrently, it models
structural connectivity from diffusion MRI using a novel signed graph
transformer to encode the brain's anatomical structure. These powerful,
modality-specific representations are then integrated via an adaptive fusion
gate. Despite its compact design, our model consistently outperforms larger
models on a diverse range of downstream benchmarks, including classification,
prediction, and unsupervised network identification tasks. Furthermore, our
model revealed novel insights into brain dynamics using attention maps on a
unique external psilocybin neuroimaging dataset (pre- and post-administration).
BrainSymphony establishes that architecturally-aware, multimodal models can
surpass their larger counterparts, paving the way for more accessible and
powerful research in computational neuroscience.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [479] [QUST_NLP at SemEval-2025 Task 7: A Three-Stage Retrieval Framework for Monolingual and Crosslingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2506.17272)
*Youzheng Liu,Jiyan Liu,Xiaoman Xu,Taihang Wang,Yimin Wang,Ye Jiang*

Main category: cs.IR

TL;DR: This paper reports QUST_NLP's participation in SemEval-2025 Task 7, using a three-stage retrieval framework for fact-checked claim retrieval, achieving 5th and 7th place in respective tracks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop an effective framework for improving the accuracy and efficiency of fact-checked claim retrieval in monolingual and cross-lingual settings.

Method: The proposed method involves a three-stage retrieval framework: (1) selecting the best-performing retrieval model for candidate retrieval, (2) applying multiple re-ranking models to refine results to Top-10 outcomes, and (3) using weighted voting to finalize retrieval outcomes.

Result: The system demonstrated high performance, securing 5th place in the monolingual track and 7th in the cross-lingual track at SemEval-2025 Task 7.

Conclusion: The proposed approach shows promise in improving fact-checked claim retrieval, supported by robust performance across two evaluation tracks. The system code is shared for further research and collaboration.

Abstract: This paper describes the participation of QUST_NLP in the SemEval-2025 Task
7. We propose a three-stage retrieval framework specifically designed for
fact-checked claim retrieval. Initially, we evaluate the performance of several
retrieval models and select the one that yields the best results for candidate
retrieval. Next, we employ multiple re-ranking models to enhance the candidate
results, with each model selecting the Top-10 outcomes. In the final stage, we
utilize weighted voting to determine the final retrieval outcomes. Our approach
achieved 5th place in the monolingual track and 7th place in the crosslingual
track. We release our system code at:
https://github.com/warmth27/SemEval2025_Task7

</details>


### [480] [Chunk Twice, Embed Once: A Systematic Study of Segmentation and Representation Trade-offs in Chemistry-Aware Retrieval-Augmented Generation](https://arxiv.org/abs/2506.17277)
*Mahmoud Amiri,Thomas Bocklitz*

Main category: cs.IR

TL;DR: This paper systematically evaluates chunking strategies and embedding models for chemistry-focused Retrieval-Augmented Generation (RAG) systems, introducing new benchmarks and guidelines.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of scientific literature, especially in critical domains like chemistry, necessitates efficient RAG systems. Foundational design aspects such as document segmentation and representation remain insufficiently explored.

Method: The authors assess 25 chunking configurations and 48 embedding models on three chemistry-specific benchmarks, including a newly introduced dataset. Recursive token-based chunking and retrieval-optimized embeddings are particularly analyzed.

Result: Recursive token-based chunking (R100-0) consistently outperforms other approaches with low resource demands. Retrieval-optimized embeddings like Nomic and Intfloat E5 variants show better performance compared to specialized domain models like SciBERT.

Conclusion: The paper provides actionable insights and released resources to guide the development of efficient chemistry-aware RAG systems, emphasizing robust chunking and embedding strategies.

Abstract: Retrieval-Augmented Generation (RAG) systems are increasingly vital for
navigating the ever-expanding body of scientific literature, particularly in
high-stakes domains such as chemistry. Despite the promise of RAG, foundational
design choices -- such as how documents are segmented and represented -- remain
underexplored in domain-specific contexts. This study presents the first
large-scale, systematic evaluation of chunking strategies and embedding models
tailored to chemistry-focused RAG systems. We investigate 25 chunking
configurations across five method families and evaluate 48 embedding models on
three chemistry-specific benchmarks, including the newly introduced
QuestChemRetrieval dataset. Our results reveal that recursive token-based
chunking (specifically R100-0) consistently outperforms other approaches,
offering strong performance with minimal resource overhead. We also find that
retrieval-optimized embeddings -- such as Nomic and Intfloat E5 variants --
substantially outperform domain-specialized models like SciBERT. By releasing
our datasets, evaluation framework, and empirical benchmarks, we provide
actionable guidelines for building effective and efficient chemistry-aware RAG
systems.

</details>


### [481] [CORONA: A Coarse-to-Fine Framework for Graph-based Recommendation with Large Language Models](https://arxiv.org/abs/2506.17281)
*Junze Chen,Xinjie Yang,Cheng Yang,Junfei Bao,Zeyuan Guo,Yawen Li,Chuan Shi*

Main category: cs.IR

TL;DR: The paper introduces CORONA, a framework that uses LLMs for candidate filtering in recommender systems alongside GNNs, achieving notable improvements in recommendation performance.


<details>
  <summary>Details</summary>
Motivation: Existing recommender systems using LLMs fail to utilize their reasoning abilities during candidate filtering, which potentially leads to suboptimal performance. There was a need to explore LLM integration into the retrieval phase itself.

Method: CORONA is a three-step framework that utilizes LLMs for progressive reasoning during candidate filtering on interaction graphs: preference-assisted retrieval via user profiles, intent-assisted retrieval through purchase history, and GNN-enhanced retrieval via collaborative filtering.

Result: Experiments on multiple datasets report a relative improvement of 18.6% in recall and 18.4% in NDCG, setting a new benchmark in recommendation performance.

Conclusion: CORONA successfully leverages LLM reasoning and integrates GNNs into the retrieval process, establishing significant advancements in the accuracy and efficiency of recommender systems.

Abstract: Recommender systems (RSs) are designed to retrieve candidate items a user
might be interested in from a large pool. A common approach is using graph
neural networks (GNNs) to capture high-order interaction relationships. As
large language models (LLMs) have shown strong capabilities across domains,
researchers are exploring their use to enhance recommendation. However, prior
work limits LLMs to re-ranking results or dataset augmentation, failing to
utilize their power during candidate filtering - which may lead to suboptimal
performance. Instead, we propose to leverage LLMs' reasoning abilities during
the candidate filtering process, and introduce Chain Of Retrieval ON grAphs
(CORONA) to progressively narrow down the range of candidate items on
interaction graphs with the help of LLMs: (1) First, LLM performs preference
reasoning based on user profiles, with the response serving as a query to
extract relevant users and items from the interaction graph as
preference-assisted retrieval; (2) Then, using the information retrieved in the
previous step along with the purchase history of target user, LLM conducts
intent reasoning to help refine an even smaller interaction subgraph as
intent-assisted retrieval; (3) Finally, we employ a GNN to capture high-order
collaborative filtering information from the extracted subgraph, performing
GNN-enhanced retrieval to generate the final recommendation results. The
proposed framework leverages the reasoning capabilities of LLMs during the
retrieval process, while seamlessly integrating GNNs to enhance overall
recommendation performance. Extensive experiments on various datasets and
settings demonstrate that our proposed CORONA achieves state-of-the-art
performance with an 18.6% relative improvement in recall and an 18.4% relative
improvement in NDCG on average.

</details>


### [482] [SlimRAG: Retrieval without Graphs via Entity-Aware Context Selection](https://arxiv.org/abs/2506.17288)
*Jiale Zhang,Jiaxiang Chen,Zhucong Li,Jie Ding,Kui Zhao,Zenglin Xu,Xin Pang,Yinghui Xu*

Main category: cs.IR

TL;DR: SlimRAG replaces graph-based retrieval in language models with a lightweight, entity-aware mechanism, providing improved relevance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based RAG systems have limitations due to structural overheads and imprecise retrieval, stemming from reliance on semantic similarity rather than semantic relevance.

Method: SlimRAG constructs a compact entity-to-chunk table using semantic embeddings at indexing time. At query time, it identifies entities, retrieves associated chunks, and assembles relevant inputs without graph traversal.

Result: SlimRAG outperforms flat and graph-based baselines on QA benchmarks in accuracy and retrieval efficiency, significantly reducing index size and RITU.

Conclusion: SlimRAG is a more efficient and accurate structure-free, entity-centric retrieval framework for augmenting language models.

Abstract: Retrieval-Augmented Generation (RAG) enhances language models by
incorporating external knowledge at inference time. However, graph-based RAG
systems often suffer from structural overhead and imprecise retrieval: they
require costly pipelines for entity linking and relation extraction, yet
frequently return subgraphs filled with loosely related or tangential content.
This stems from a fundamental flaw -- semantic similarity does not imply
semantic relevance. We introduce SlimRAG, a lightweight framework for retrieval
without graphs. SlimRAG replaces structure-heavy components with a simple yet
effective entity-aware mechanism. At indexing time, it constructs a compact
entity-to-chunk table based on semantic embeddings. At query time, it
identifies salient entities, retrieves and scores associated chunks, and
assembles a concise, contextually relevant input -- without graph traversal or
edge construction. To quantify retrieval efficiency, we propose Relative Index
Token Utilization (RITU), a metric measuring the compactness of retrieved
content. Experiments across multiple QA benchmarks show that SlimRAG
outperforms strong flat and graph-based baselines in accuracy while reducing
index size and RITU (e.g., 16.31 vs. 56+), highlighting the value of
structure-free, entity-centric context selection. The code will be released
soon. https://github.com/continue-ai-company/SlimRAG

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [483] [Optimal Parallel Algorithms for Convex Hulls in 2D and 3D under Noisy Primitive Operations](https://arxiv.org/abs/2506.17507)
*Michael T. Goodrich,Vinesh Sridhar*

Main category: cs.CG

TL;DR: This paper addresses noisy comparison errors in computations, proposing optimal parallel algorithms for 2D and 3D convex hulls in the noisy primitives model.


<details>
  <summary>Details</summary>
Motivation: The noisy primitives model led to challenges in sequential computational approaches, motivating studies into parallel algorithm solutions for geometric problems.

Method: The researchers designed parallel algorithms in the CREW PRAM model. They introduced a generalized failure sweeping technique to mitigate computational errors.

Result: The paper presents optimal parallel algorithms for handling noisy geometric computations, specifically targeting convex hull construction in 2D and 3D.

Conclusion: This work demonstrates the feasibility of error detection and correction during parallel geometric computational processes, extending noisy primitive models to efficient convex hull construction.

Abstract: In the noisy primitives model, each primitive comparison performed by an
algorithm, e.g., testing whether one value is greater than another, returns the
incorrect answer with random, independent probability p < 1/2 and otherwise
returns a correct answer. This model was first applied in the context of
sorting and searching, and recent work by Eppstein, Goodrich, and Sridhar
extends this model to sequential algorithms involving geometric primitives such
as orientation and sidedness tests. However, their approaches appear to be
inherently sequential; hence, in this paper, we study parallel computational
geometry algorithms for 2D and 3D convex hulls in the noisy primitives model.
We give the first optimal parallel algorithms in the noisy primitives model for
2D and 3D convex hulls in the CREW PRAM model. The main technical contribution
of our work concerns our ability to detect and fix errors during intermediate
steps of our algorithm using a generalization of the failure sweeping
technique.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [484] [Predicting Stock Market Crash with Bayesian Generalised Pareto Regression](https://arxiv.org/abs/2506.17549)
*Sourish Das*

Main category: q-fin.ST

TL;DR: The paper introduces a Bayesian GPR model to forecast extreme losses in the Nifty 50 index by dynamically connecting tail risk with market volatility.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of extreme negative returns is critical for financial risk management, particularly in emerging markets like India.

Method: The study links the GPD's scale parameter to market conditions via a log-linear function and evaluates four Bayesian regularisation priors: Cauchy, Lasso, Ridge, and Zellner's g-prior.

Result: The Cauchy prior showed the best performance in predictive accuracy, and the study confirmed significant contributions of S&P 500, gold volatilities, and local market volatility to tail risk.

Conclusion: The proposed GPR model outperforms traditional EVT models by using real-time financial indicators, making it a valuable tool for risk assessment and systemic stress testing.

Abstract: This paper develops a Bayesian Generalised Pareto Regression (GPR) model to
forecast extreme losses in Indian equity markets, with a focus on the Nifty 50
index. Extreme negative returns, though rare, can cause significant financial
disruption, and accurate modelling of such events is essential for effective
risk management. Traditional Generalised Pareto Distribution (GPD) models often
ignore market conditions; in contrast, our framework links the scale parameter
to covariates using a log-linear function, allowing tail risk to respond
dynamically to market volatility. We examine four prior choices for Bayesian
regularisation of regression coefficients: Cauchy, Lasso (Laplace), Ridge
(Gaussian), and Zellner's g-prior. Simulation results suggest that the Cauchy
prior delivers the best trade-off between predictive accuracy and model
simplicity, achieving the lowest RMSE, AIC, and BIC values. Empirically, we
apply the model to large negative returns (exceeding 5%) in the Nifty 50 index.
Volatility measures from the Nifty 50, S&P 500, and gold are used as covariates
to capture both domestic and global risk drivers. Our findings show that tail
risk increases significantly with higher market volatility. In particular, both
S&P 500 and gold volatilities contribute meaningfully to crash prediction,
highlighting global spillover and flight-to-safety effects. The proposed GPR
model offers a robust and interpretable approach for tail risk forecasting in
emerging markets. It improves upon traditional EVT-based models by
incorporating real-time financial indicators, making it useful for
practitioners, policymakers, and financial regulators concerned with systemic
risk and stress testing.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [485] [Floating-Point Data Transformation for Lossless Compression](https://arxiv.org/abs/2506.18062)
*Samirasadat Jamalidinan,Kazem Cheshmi*

Main category: cs.DB

TL;DR: The paper introduces Typed Data Transformation (DTT), enhancing lossless compression for floating-point data by exploiting inherent byte correlations, achieving better compression ratios and throughput compared to state-of-the-art tools.


<details>
  <summary>Details</summary>
Motivation: Floating-point data is widely used in applications like medical imaging and language models, requiring precise and efficient storage due to the critical impact of accuracy and dataset size.

Method: The authors propose Typed Data Transformation (DTT), a novel method that reorganizes and groups correlated bytes within floating-point data to optimize compression.

Result: DTT improves the geometric mean compression ratio by 1.16× and boosts compression/decompression throughput by 1.18–3.79× compared to leading tools like zstd.

Conclusion: Typed Data Transformation significantly enhances lossless compression efficiency for floating-point data, addressing byte correlations to improve both compression ratio and processing speed.

Abstract: Floating-point data is widely used across various domains. Depending on the
required precision, each floating-point value can occupy several bytes.
Lossless storage of this information is crucial due to its critical accuracy,
as seen in applications such as medical imaging and language model weights. In
these cases, data size is often significant, making lossless compression
essential. Previous approaches either treat this data as raw byte streams for
compression or fail to leverage all patterns within the dataset. However,
because multiple bytes represent a single value and due to inherent patterns in
floating-point representations, some of these bytes are correlated. To leverage
this property, we propose a novel data transformation method called Typed Data
Transformation (\DTT{}) that groups related bytes together to improve
compression. We implemented and tested our approach on various datasets across
both CPU and GPU. \DTT{} achieves a geometric mean compression ratio
improvement of 1.16$\times$ over state-of-the-art compression tools such as
zstd, while also improving both compression and decompression throughput by
1.18--3.79$\times$.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [486] [Enhancing Few-shot Keyword Spotting Performance through Pre-Trained Self-supervised Speech Models](https://arxiv.org/abs/2506.17686)
*Alican Gok,Oguzhan Buyuksolak,Osman Erman Okman,Murat Saraclar*

Main category: eess.AS

TL;DR: The paper introduces a system for Few-Shot Keyword Spotting (FS-KWS) that combines self-supervised learning models and lightweight student models to achieve high accuracy on edge devices.


<details>
  <summary>Details</summary>
Motivation: Traditional keyword spotting systems face scalability and adaptability challenges, especially in resource-limited edge environments, when recognizing custom keywords with minimal examples.

Method: The approach involves using a teacher model (Wav2Vec 2.0) trained with Sub-center ArcFace loss for robust feature extraction. An attention-based dimensionality reduction technique is paired with a lightweight ResNet15 student model for efficient edge deployment.

Result: The system achieves a significant improvement in 10-shot classification accuracy, increasing from 33.4% to 74.1% on the GSC dataset while maintaining a 1% false alarm rate.

Conclusion: The proposed method is more accurate and resource-efficient, positioning it as a viable solution for keyword spotting on edge devices.

Abstract: Keyword Spotting plays a critical role in enabling hands-free interaction for
battery-powered edge devices. Few-Shot Keyword Spotting (FS-KWS) addresses the
scalability and adaptability challenges of traditional systems by enabling
recognition of custom keywords with only a few examples. However, existing
FS-KWS systems achieve subpar accuracy at desirable false acceptance rates,
particularly in resource-constrained edge environments. To address these
issues, we propose a training scheme that leverages self-supervised learning
models for robust feature extraction, dimensionality reduction, and knowledge
distillation. The teacher model, based on Wav2Vec 2.0 is trained using
Sub-center ArcFace loss, which enhances inter-class separability and
intra-class compactness. To enable efficient deployment on edge devices, we
introduce attention-based dimensionality reduction and train a standard
lightweight ResNet15 student model. We evaluate the proposed approach on the
English portion of the Multilingual Spoken Words Corpus (MSWC) and the Google
Speech Commands (GSC) datasets. Notably, the proposed training method improves
the 10-shot classification accuracy from 33.4% to 74.1% on 11 classes at 1%
false alarm accuracy on the GSC dataset, thus making it significantly
better-suited for a real use case scenario.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [487] [Consistent Channel Hopping Algorithms for the Multichannel Rendezvous Problem with Heterogeneous Available Channel Sets](https://arxiv.org/abs/2506.18381)
*Yiwei Liu,Yi-Chia Cheng,Cheng-Shang Chang*

Main category: cs.NI

TL;DR: The paper presents a theoretical framework for developing efficient channel hopping algorithms to solve the multichannel rendezvous problem (MRP) in heterogeneous wireless networks. It introduces consistent channel selection functions, establishes bounds on rendezvous metrics, and proposes practical algorithms like modulo and hybrid strategies.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the inefficiencies in rendezvous for multichannel wireless networks, especially when channel availability varies across different nodes. This remains a critical issue for reliable connectivity in dynamic environments.

Method: The authors propose using consistent channel selection functions, analyze rendezvous metrics using permutations and fictitious users, and develop efficient algorithms like the modulo approach, extending them to multi-user scenarios.

Result: They establish tight bounds for rendezvous metrics (MTTR and ETTR), prove that consistent channel hopping algorithms maximize rendezvous probability, and validate the effectiveness of the proposed strategies via simulations.

Conclusion: The consistent channel hopping framework and the proposed algorithms significantly improve rendezvous efficiency in multichannel wireless networks, ensuring scalability and high performance across diverse settings.

Abstract: We propose a theoretical framework for consistent channel hopping algorithms
to address the multichannel rendezvous problem (MRP) in wireless networks with
heterogeneous available channel sets. A channel selection function is called
consistent if the selected channel remains unchanged when the available channel
set shrinks, provided the selected channel is still available. We show that all
consistent channel selection functions are equivalent to the function that
always selects the smallest-index channel under appropriate channel relabeling.
This leads to a natural representation of a consistent channel hopping
algorithm as a sequence of permutations. For the two-user MRP, we characterize
rendezvous time slots using a fictitious user and derive tight bounds on the
maximum time-to-rendezvous (MTTR) and expected time-to-rendezvous (ETTR).
Notably, the ETTR is shown to be the inverse of the Jaccard index when
permutations are randomly selected. We also prove that consistent channel
hopping algorithms maximize the rendezvous probability. To reduce
implementation complexity, we propose the modulo algorithm, which uses modular
arithmetic with one-cycle permutations and achieves performance comparable to
locality-sensitive hashing (LSH)-based algorithms. The framework is extended to
multiple users, with novel strategies such as stick-together, spread-out, and a
hybrid method that accelerates rendezvous in both synchronous and asynchronous
settings. Simulation results confirm the effectiveness and scalability of the
proposed algorithms.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [488] [JAX-LaB: A High-Performance, Differentiable, Lattice Boltzmann Library for Modeling Multiphase Fluid Dynamics in Geosciences and Engineering](https://arxiv.org/abs/2506.17713)
*Piyush Pradhan,Pierre Gentine,Shaina Kelly*

Main category: physics.comp-ph

TL;DR: JAX-LaB is an open-source, Python-based library for high-performance simulations of complex flows in porous media using the Lattice Boltzmann method.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for a flexible, scalable, and machine learning-compatible tool for simulating complex multiphase and multiphysics flows in porous media.

Method: The library leverages JAX for efficient hardware-agnostic computations, employing the Shan-Chen pseudopotential method and various enhanced schemes for fluid dynamics modeling.

Result: Several analytical benchmarks validate JAX-LaB’s accuracy, while performance scaling tests for single- and multi-GPU setups highlight its computational efficiency.

Conclusion: JAX-LaB successfully integrates advanced fluid modeling techniques with scalable computational frameworks, proving its utility in scientific and engineering applications.

Abstract: We present JAX-LaB, a differentiable, Python-based Lattice Boltzmann library
for simulating multiphase and multiphysics flows in hydrologic, geologic, and
engineered porous media. Built as an extension of the XLB library, JAX-LaB
utilizes JAX for computations and offers a performant, hardware-agnostic
implementation that integrates seamlessly with machine learning workflows and
scales efficiently across CPUs, GPUs, and distributed systems. Multiphase
interactions are modeled using the Shan-Chen pseudopotential method, which is
coupled with an equation of state and an improved forcing scheme to obtain
liquid-vapor densities that are consistent with Maxwell's construction,
enabling simulations of systems with very large density ratios while
maintaining minimal spurious currents. Wetting is handled using the "improved"
virtual density scheme, which allows precise control of contact angles and
eliminates non-physical films seen in other Shan-Chen wetting methods. We
validate the library through several analytical benchmarks, such as Laplace's
law, capillary rise, and cocurrent multicomponent flow, and demonstrate some
exemplary use cases for the library. We also report single- and multi-GPU
performance scaling of the library. The library is open-source under the Apache
license and available at https://github.com/piyush-ppradhan/JAX-LaB.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [489] [Zero-Shot Cognitive Impairment Detection from Speech Using AudioLLM](https://arxiv.org/abs/2506.17351)
*Mostafa Shahin,Beena Ahmed,Julien Epps*

Main category: cs.SD

TL;DR: This study introduces a zero-shot method for detecting cognitive impairment (CI) using the Qwen2-Audio AudioLLM model that processes speech data, achieving performance comparable to supervised methods.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the public health concern of cognitive impairment by creating a generalizable, non-invasive, and language-agnostic detection method using speech as a biomarker.

Method: The researchers developed a prompt-based, zero-shot classification system using Qwen2-Audio AudioLLM, enabling detection of CI without manual annotation. They evaluated the method on both English and multilingual datasets.

Result: The proposed zero-shot method achieved performance comparable to traditional supervised methods, demonstrating strong generalizability across languages, tasks, and datasets.

Conclusion: Zero-shot models like AudioLLM can serve as effective, scalable tools for detecting cognitive impairment, offering advantages in efficiency and cross-linguistic capabilities.

Abstract: Cognitive impairment (CI) is of growing public health concern, and early
detection is vital for effective intervention. Speech has gained attention as a
non-invasive and easily collectible biomarker for assessing cognitive decline.
Traditional CI detection methods typically rely on supervised models trained on
acoustic and linguistic features extracted from speech, which often require
manual annotation and may not generalise well across datasets and languages. In
this work, we propose the first zero-shot speech-based CI detection method
using the Qwen2- Audio AudioLLM, a model capable of processing both audio and
text inputs. By designing prompt-based instructions, we guide the model in
classifying speech samples as indicative of normal cognition or cognitive
impairment. We evaluate our approach on two datasets: one in English and
another multilingual, spanning different cognitive assessment tasks. Our
results show that the zero-shot AudioLLM approach achieves performance
comparable to supervised methods and exhibits promising generalizability and
consistency across languages, tasks, and datasets.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [490] [Residue Number System (RNS) based Distributed Quantum Multiplication](https://arxiv.org/abs/2506.17588)
*Bhaskar Gaur,Himanshu Thapliyal*

Main category: quant-ph

TL;DR: This paper proposes a design for a Quantum Diminished-1 Modulo Multiplier based on the Residue Number System (RNS) to improve quantum multiplication efficiency, achieving reduced Toffoli depth and lower T gate usage.


<details>
  <summary>Details</summary>
Motivation: Quantum multipliers, essential for quantum algorithms, are inefficient due to high Toffoli depth and T gate usage, limiting their applicability on quantum computers.

Method: The authors introduce RNS-based distributed quantum multiplication and design a Quantum Diminished-1 Modulo $(2^n+1)$ Multiplier.

Result: They demonstrate up to 46.018% reduction in Toffoli depth and a 34.483% to 86.25% decrease in T gate usage compared to traditional quantum multipliers (for 6 to 16 qubit outputs).

Conclusion: Implementing RNS-based distributed quantum multiplication can improve efficiency and scalability for quantum arithmetic operations in quantum computing.

Abstract: Multiplication of quantum states is a frequently used function or subroutine
in quantum algorithms and applications, making quantum multipliers an essential
component of quantum arithmetic. However, quantum multiplier circuits suffer
from high Toffoli depth and T gate usage, which ultimately affects their
scalability and applicability on quantum computers. To address these issues, we
propose utilizing the Residue Number System (RNS) based distributed quantum
multiplication, which executes multiple quantum modulo multiplication circuits
across quantum computers or jobs with lower Toffoli depth and T gate usage.
Towards this end, we propose a design of Quantum Diminished-1 Modulo $(2^n+1)$
Multiplier, an essential component of RNS based distributed quantum
multiplication. We provide estimates of quantum resource usage and compare them
with those of an existing non-distributed quantum multiplier for 6 to 16 qubit
sized output. Our comparative analysis estimates up to 46.018% lower Toffoli
depth, and reduction in T gates of 34.483% to 86.25%.

</details>


### [491] [Bloch Vector Assertions for Debugging Quantum Programs](https://arxiv.org/abs/2506.18458)
*Noah H. Oldfield,Christoph Laaber,Shaukat Ali*

Main category: quant-ph

TL;DR: The paper introduces Bloq, an automated, scalable approach for debugging quantum programs using Bloch-vector-based assertions, significantly improving localization efficiency and reliability.


<details>
  <summary>Details</summary>
Motivation: Debugging quantum programs is challenging due to quantum-specific faults (e.g., gate misimplementations, hardware noise) and their probabilistic nature. Existing solutions face issues like manual assertion generation, dependency on mid-circuit measurements, and lack of scalability.

Method: Propose Bloq, which uses Bloch-vector-based assertions with expectation value measurements of Pauli operators, avoiding mid-circuit measurements. It also includes AutoBloq for automatic generation of assertion schemes from quantum algorithms.

Result: Experimental evaluation with 684,432 programs (covering Quantum Fourier Transform and Grover algorithms) shows Bloq outperforms the Proq method in fault localization efficiency, achieving higher F1 scores (e.g., 0.74 for Grover under ideal conditions) and robustness under noise. Bloq reduces runtime and circuit depth overhead significantly (by factors of 5 and 23).

Conclusion: Bloq provides a scalable, automated, and low-overhead solution for assertion-based debugging, making it highly promising for improving the reliability of near-term quantum devices.

Abstract: Quantum programs must be reliable to ensure trustworthy results, yet
debugging them is notoriously challenging due to quantum-specific faults like
gate misimplementations and hardware noise, as well as their inherently
probabilistic nature. Assertion-based debugging provides a promising solution
by enabling localized correctness checks during execution. However, current
approaches face challenges including manual assertion generation, reliance on
mid-circuit-measurements, and poor scalability. In this paper, we present Bloq,
a scalable, automated fault localization approach introducing
Bloch-vector-based assertions utilizing expectation value measurements of Pauli
operators, enabling low-overhead fault localization without mid-circuit
measurements. In addition, we introduce AutoBloq, a component of Bloq for
automatically generating assertion schemes from quantum algorithms. An
experimental evaluation over 684432 programs using two algorithms (Quantum
Fourier Transform (QFT) and Grover) shows that Bloq consistently outperforms
the state-of-the-art approach Proq, notably as circuit depth and noise
increase. For Grover, Bloq achieves a mean F1 score across all experimental
instances of 0.74 versus 0.38 for Proq under ideal conditions, and maintains
performance under noise (0.43 versus 0.06). Bloq also reduces Proq's runtime by
a factor of 5 and circuit depth overhead by a factor of 23. These results
underline Bloq's potential to make assertion-based debugging scalable and
effective for near-term quantum devices.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [492] [The Within-Orbit Adaptive Leapfrog No-U-Turn Sampler](https://arxiv.org/abs/2506.18746)
*Nawaf Bou-Rabee,Bob Carpenter,Tore Selland Kleppe,Sifan Liu*

Main category: stat.CO

TL;DR: The paper introduces WALNUTS, an improved MCMC sampler that adapts the step size locally through energy thresholding, outperforming NUTS on multiscale distributions.


<details>
  <summary>Details</summary>
Motivation: Posterior distributions often exhibit multi-scale geometries, necessitating local adaptation of Markov Chain Monte Carlo (MCMC) step sizes, which the original NUTS algorithm struggles to implement effectively.

Method: The proposed WALNUTS algorithm adapts leapfrog integrator step sizes dynamically by selecting them based on simulated orbit intervals and maintaining energy error within a user-specified threshold. It also employs biased state selection similar to NUTS.

Result: Empirical evaluations showed WALNUTS improves sampling efficiency and robustness over NUTS on multiscale distributions, such as Neal's funnel and stochastic volatility models.

Conclusion: WALNUTS generalizes NUTS by effectively handling posterior distributions with multi-scale geometries, offering significant operational benefits in MCMC sampling methods.

Abstract: Locally adapting parameters within Markov chain Monte Carlo methods while
preserving reversibility is notoriously difficult. The success of the No-U-Turn
Sampler (NUTS) largely stems from its clever local adaptation of the
integration time in Hamiltonian Monte Carlo via a geometric U-turn condition.
However, posterior distributions frequently exhibit multi-scale geometries with
extreme variations in scale, making it necessary to also adapt the leapfrog
integrator's step size locally and dynamically. Despite its practical
importance, this problem has remained largely open since the introduction of
NUTS by Hoffman and Gelman (2014). To address this issue, we introduce the
Within-orbit Adaptive Leapfrog No-U-Turn Sampler (WALNUTS), a generalization of
NUTS that adapts the leapfrog step size at fixed intervals of simulated time as
the orbit evolves. At each interval, the algorithm selects the largest step
size from a dyadic schedule that keeps the energy error below a user-specified
threshold. Like NUTS, WALNUTS employs biased progressive state selection to
favor states with positions that are further from the initial point along the
orbit. Empirical evaluations on multiscale target distributions, including
Neal's funnel and the Stock-Watson stochastic volatility time-series model,
demonstrate that WALNUTS achieves substantial improvements in sampling
efficiency and robustness compared to standard NUTS.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [493] [Modal Logic for Stratified Becoming: Actualization Beyond Possible Worlds](https://arxiv.org/abs/2506.17276)
*Alexandre Le Nepvou*

Main category: cs.LO

TL;DR: The paper introduces Stratified Actualization Logic (SAL), a novel framework for modal logic based on layered actualization rather than global possible worlds.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional Kripke semantics, which neglect local, dynamic, and asymmetric actualization aspects in modal logic.

Method: It develops the SAL framework, defines its syntax and semantics, introduces axioms, and proves the system's soundness and completeness.

Result: SAL offers a structured, layered approach to modality, accounting for ontological stability transitions, and is applicable to areas like temporal becoming, quantum decoherence, and modal metaphysics.

Conclusion: SAL provides an alternative to modal realism by capturing actualization's ontological structure without relying on abstract possible worlds.

Abstract: This article develops a novel framework for modal logic based on the idea of
stratified actualization, rather than the classical model of global possible
worlds. Traditional Kripke semantics treat modal operators as quantification
over fully determinate alternatives, neglecting the local, dynamic, and often
asymmetric nature of actualization processes. We propose a system Stratified
Actualization Logic (SAL) in which modalities are indexed by levels of
ontological stability, interpreted as admissibility regimes. Each modality
operates over a structured layer of possibility, grounded in the internal
coherence of transitions between layers. We formally define the syntax and
semantics of SAL, introduce its axioms, and prove soundness and completeness.
Applications are discussed in connection with temporal becoming, quantum
decoherence domains, and modal metaphysics. The result is a logic that captures
the ontological structure of actualization without recourse to abstract
possible worlds, offering a stratified alternative to standard modal realism.

</details>


### [494] [Beyond Prediction -- Structuring Epistemic Integrity in Artificial Reasoning Systems](https://arxiv.org/abs/2506.17331)
*Craig Steven Wright*

Main category: cs.LO

TL;DR: This paper introduces a framework for AI systems to ensure epistemic constraints and rational reasoning.


<details>
  <summary>Details</summary>
Motivation: To enhance AI systems' ability to perform structured reasoning under epistemic constraints.

Method: The framework combines symbolic inference, knowledge graphs, and blockchain-based justification for robust belief management.

Result: Systems under this framework can exhibit truth-preserving rationality and enable auditable epistemic processes.

Conclusion: The proposed framework provides a pathway for developing epistemically rigorous and rational AI agents.

Abstract: This paper develops a comprehensive framework for artificial intelligence
systems that operate under strict epistemic constraints, moving beyond
stochastic language prediction to support structured reasoning, propositional
commitment, and contradiction detection. It formalises belief representation,
metacognitive processes, and normative verification, integrating symbolic
inference, knowledge graphs, and blockchain-based justification to ensure
truth-preserving, auditably rational epistemic agents.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [495] [Can Common VLMs Rival Medical VLMs? Evaluation and Strategic Insights](https://arxiv.org/abs/2506.17337)
*Yuan Zhong,Ruinan Jin,Xiaoxiao Li,Qi Dou*

Main category: eess.IV

TL;DR: The study investigates if fine-tuned common vision-language models (VLMs) can match or surpass specialized medical VLMs for medical imaging tasks, finding fine-tuned common models to perform competitively.


<details>
  <summary>Details</summary>
Motivation: Medical VLMs require substantial computational and data resources, and fine-tuning common VLMs like CLIP and LLaVA might achieve similar or better performance for specific medical tasks, reducing costs and complexity.

Method: The researchers evaluated common and medical VLMs across tasks like disease diagnosis and medical visual question answering (VQA). They compared off-the-shelf and fine-tuned performance in in-domain and out-of-domain settings, using approaches like LoRA-based tuning.

Result: Fine-tuned common VLMs matched or outperformed specialized medical VLMs in certain in-domain tasks and showed strong adaptability in out-of-domain settings, challenging the necessity of medical-specific pretraining.

Conclusion: Lightweight fine-tuning of common VLMs offers an efficient and scalable alternative to medical-specific VLMs, enabling cost-effective solutions without significant performance compromise in medical imaging tasks.

Abstract: Medical vision-language models (VLMs) leverage large-scale pretraining for
diverse imaging tasks but require substantial computational and data resources.
Meanwhile, common or general-purpose VLMs (e.g., CLIP, LLaVA), though not
trained for medical use, show promise with fine-tuning. This raises a key
question: Can efficient fine-tuned common VLMs rival generalist medical VLMs
for solving specific medical imaging tasks? This study systematically evaluates
common and medical VLMs across disease diagnosis and visual question answering
(VQA). Using CLIP-based and LLaVA-based models, we examine (1) off-the-shelf
performance gaps in in-domain (ID) settings, (2) whether fine-tuning bridges
these gaps, and (3) generalization to out-of-domain (OOD) tasks on unseen
medical modalities. While medical-specific pretraining provides advantages in
ID settings, common VLMs match or surpass medical-specific models after
lightweight fine-tuning, with LoRA-based adaptation proving highly effective
among different tasks. In OOD tasks, common VLMs demonstrate strong
adaptability in some tasks, challenging the assumption that medical-specific
pre-training is essential. These findings suggest that leveraging common VLMs
with fine-tuning offers a scalable and cost-effective alternative to developing
large-scale medical VLMs, providing crucial insights for future research in the
medical imaging field.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [496] [Pix2Geomodel: A Next-Generation Reservoir Geomodeling with Property-to-Property Translation](https://arxiv.org/abs/2506.17747)
*Abdulrahman Al-Fakih,Ardiansyah Koeshidayatullah,Nabil A. Saraih,Tapan Mukerji,Rayan Kanfar,Abdulmohsen Alali,SanLinn I. Kaka*

Main category: physics.geo-ph

TL;DR: This paper presents Pix2Geomodel, a conditional generative adversarial network (cGAN) framework tailored to predict key reservoir properties, demonstrating improvements over traditional methods in accuracy and geological realism.


<details>
  <summary>Details</summary>
Motivation: Traditional geological modeling methods struggle with subsurface heterogeneity and conditioning to observed data, necessitating more accurate techniques for reservoir characterization.

Method: Pix2Geomodel leverages Pix2Pix cGAN architecture, utilizing a U-Net generator and PatchGAN discriminator on a large dataset of 7.6 million cells, with preprocessing, data augmentation, and evaluation via accuracy metrics.

Result: The framework showed high accuracy for facies (PA 0.88) and water saturation (PA 0.96), moderate success for porosity and permeability, and robust performance in property-to-property translation tasks.

Conclusion: Pix2Geomodel improves fidelity in property mapping compared to traditional methods, suggesting future development into 3D modeling and integration of multi-modal data for broader applications.

Abstract: Accurate geological modeling is critical for reservoir characterization, yet
traditional methods struggle with complex subsurface heterogeneity, and they
have problems with conditioning to observed data. This study introduces
Pix2Geomodel, a novel conditional generative adversarial network (cGAN)
framework based on Pix2Pix, designed to predict reservoir properties (facies,
porosity, permeability, and water saturation) from the Rotliegend reservoir of
the Groningen gas field. Utilizing a 7.6 million-cell dataset from the
Nederlandse Aardolie Maatschappij, accessed via EPOS-NL, the methodology
included data preprocessing, augmentation to generate 2,350 images per
property, and training with a U-Net generator and PatchGAN discriminator over
19,000 steps. Evaluation metrics include pixel accuracy (PA), mean intersection
over union (mIoU), frequency weighted intersection over union (FWIoU), and
visualizations assessed performance in masked property prediction and
property-to-property translation tasks. Results demonstrated high accuracy for
facies (PA 0.88, FWIoU 0.85) and water saturation (PA 0.96, FWIoU 0.95), with
moderate success for porosity (PA 0.70, FWIoU 0.55) and permeability (PA 0.74,
FWIoU 0.60), and robust translation performance (e.g., facies-to-facies PA
0.98, FWIoU 0.97). The framework captured spatial variability and geological
realism, as validated by variogram analysis, and calculated the training loss
curves for the generator and discriminator for each property. Compared to
traditional methods, Pix2Geomodel offers enhanced fidelity in direct property
mapping. Limitations include challenges with microstructural variability and 2D
constraints, suggesting future integration of multi-modal data and 3D modeling
(Pix2Geomodel v2.0). This study advances the application of generative AI in
geoscience, supporting improved reservoir management and open science
initiatives.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [497] [Heterogeneous Temporal Hypergraph Neural Network](https://arxiv.org/abs/2506.17312)
*Huan Liu,Pengfei Jiao,Mengzhou Gao,Chaochao Chen,Di Jin*

Main category: cs.SI

TL;DR: The paper presents HTHGN, a new Graph Representation Learning model designed to capture high-order interactions in Heterogeneous Temporal HyperGraphs (HTGs), outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Current GRL methods struggle with capturing high-order group interactions and are typically suited to static, homogeneous graphs. This limits their effectiveness in representing real-world complex networks.

Method: The authors propose a formal definition of HTGs, a $P$-uniform heterogeneous hyperedge construction algorithm, and a new Heterogeneous Temporal HyperGraph Neural network (HTHGN) with hierarchical attention and contrastive learning mechanisms.

Result: Experimental results on three real-world HTG datasets validate the effectiveness of HTHGN, showing significant performance improvements over existing methods.

Conclusion: HTHGN successfully models high-order interactions, expands the capability of GRL methods for HTGs, and addresses existing limitations in handling real-world heterogeneous dynamic graphs.

Abstract: Graph representation learning (GRL) has emerged as an effective technique for
modeling graph-structured data. When modeling heterogeneity and dynamics in
real-world complex networks, GRL methods designed for complex heterogeneous
temporal graphs (HTGs) have been proposed and have achieved successful
applications in various fields. However, most existing GRL methods mainly focus
on preserving the low-order topology information while ignoring higher-order
group interaction relationships, which are more consistent with real-world
networks. In addition, most existing hypergraph methods can only model static
homogeneous graphs, limiting their ability to model high-order interactions in
HTGs. Therefore, to simultaneously enable the GRL model to capture high-order
interaction relationships in HTGs, we first propose a formal definition of
heterogeneous temporal hypergraphs and $P$-uniform heterogeneous hyperedge
construction algorithm that does not rely on additional information. Then, a
novel Heterogeneous Temporal HyperGraph Neural network (HTHGN), is proposed to
fully capture higher-order interactions in HTGs. HTHGN contains a hierarchical
attention mechanism module that simultaneously performs temporal
message-passing between heterogeneous nodes and hyperedges to capture rich
semantics in a wider receptive field brought by hyperedges. Furthermore, HTHGN
performs contrastive learning by maximizing the consistency between low-order
correlated heterogeneous node pairs on HTG to avoid the low-order structural
ambiguity issue. Detailed experimental results on three real-world HTG datasets
verify the effectiveness of the proposed HTHGN for modeling high-order
interactions in HTGs and demonstrate significant performance improvements.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [498] [Fully-Dynamic Parallel Algorithms for Single-Linkage Clustering](https://arxiv.org/abs/2506.18384)
*Quinten De Man,Laxman Dhulipala,Kishen N Gowda*

Main category: cs.DS

TL;DR: This paper introduces asymptotically faster algorithms for maintaining the single-linkage dendrogram (SLD) in a fully-dynamic setting, outperforming naive recomputation methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of efficient update algorithms for dynamic single-linkage hierarchical agglomerative clustering, as existing methods recompute SLD from scratch—a computational bottleneck.

Method: The paper develops algorithms for edge insertion and deletion that are faster than the static computation approach. These include sequential and parallel versions enhancing efficiency with low computational cost.

Result: Key results include updates for SLD in $O(h)$ (insertion) and $O(h \log(1+n/h))$ (deletion) time, along with parallel algorithms that are work-efficient, achieving polylogarithmic depth.

Conclusion: This work significantly advances the dynamic clustering field by providing faster and parallel approaches to update SLD, ensuring scalability for large datasets with frequent insertion or deletion of edges.

Abstract: Single-linkage clustering is a popular form of hierarchical agglomerative
clustering (HAC) where the distance between two clusters is defined as the
minimum distance between any pair of points across the two clusters. In
single-linkage HAC, the output is typically the single-linkage dendrogram
(SLD), which is the binary tree representing the hierarchy of clusters formed
by iteratively contracting the two closest clusters. In the dynamic setting,
prior work has only studied maintaining a minimum spanning forest over the data
since single-linkage HAC reduces to computing the SLD on the minimum spanning
forest of the data.
  In this paper, we study the problem of maintaining the SLD in the
fully-dynamic setting. We assume the input is a dynamic forest $F$
(representing the minimum spanning forest of the data) which receives a
sequence of edge insertions and edge deletions. To our knowledge, no prior work
has provided algorithms to update an SLD asymptotically faster than recomputing
it from scratch. All of our update algorithms are asymptotically faster than
the best known static SLD computation algorithm, which takes $O(n \log h)$ time
where $h$ is the height of the dendrogram ($h \leq n-1$). Furthermore, our
algorithms are much faster in many cases, such as when $h$ is low. Our first
set of results are an insertion algorithm in $O(h)$ time and a deletion
algorithm in $O(h \log (1+n/h))$ time. Next, we describe parallel and
batch-parallel versions of these algorithms which are work-efficient or nearly
work-efficient and have poly-logarithmic depth. Finally, we show how to perform
insertions near-optimally in $O(c \log(1+n/c))$ time, where $c$ is the number
of structural changes in the dendrogram caused by the update, and give a
work-efficient parallel version of this algorithm that has polylogarithmic
depth.

</details>


### [499] [On the Power of Spatial Locality on Online Routing Problems](https://arxiv.org/abs/2506.17517)
*Swapnil Guragain,Gokarna Sharma*

Main category: cs.DS

TL;DR: This paper studies online versions of traveling salesman (TSP) and dial-a-ride (DARP) problems. It introduces the concept of spatial locality, which provides limited future request information, and shows it improves competitive ratios.


<details>
  <summary>Details</summary>
Motivation: To address real-world applications like Uber/Lyft that can leverage limited knowledge of future service requests for improved routing efficiency.

Method: The paper introduces the spatial locality model, offering partial foresight of new request locations, and compares competitive ratios with and without this model.

Result: It demonstrates that spatial locality improves competitive ratios for TSP and DARP across all metric spaces with multiple servers.

Conclusion: Incorporating spatial locality significantly enhances the efficiency of online routing tasks, proving its practical value in logistics and robotics.

Abstract: We consider the online versions of two fundamental routing problems,
traveling salesman (TSP) and dial-a-ride (DARP), which have a variety of
relevant applications in logistics and robotics. The online versions of these
problems concern with efficiently serving a sequence of requests presented in a
real-time on-line fashion located at points of a metric space by servers
(salesmen/vehicles/robots). In this paper, motivated from real-world
applications, such as Uber/Lyft rides, where some limited knowledge is
available on the future requests, we propose the {\em spatial locality} model
that provides in advance the distance within which new request(s) will be
released from the current position of server(s). We study the usefulness of
this advanced information on achieving the improved competitive ratios for both
the problems with $k\geq 1$ servers, compared to the competitive results
established in the literature without such spatial locality consideration. We
show that small locality is indeed useful in obtaining improved competitive
ratios irrespective of the metric space.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [500] [Regular Tree Search for Simulation Optimization](https://arxiv.org/abs/2506.17696)
*Du-Yi Wang,Guo Liang,Guangwu Liu,Kun Zhang*

Main category: math.OC

TL;DR: The paper introduces the Regular Tree Search algorithm to address simulation optimization problems with non-convex functions. It iteratively refines promising regions using adaptive sampling and recursive partitioning, ensuring global convergence under certain noise conditions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge in simulation optimization where non-convex objective functions make it difficult to reliably identify global optima.

Method: The authors developed the Regular Tree Search algorithm, which employs adaptive sampling, recursive partitioning, a tree structure, and a UCT strategy to guide sampling and refining of search regions.

Result: The proposed algorithm demonstrates reliable global optimum identification and accurate objective value estimation in numerical experiments.

Conclusion: The Regular Tree Search approach effectively tackles non-convex optimization problems, with provable convergence under specific noise assumptions and without continuity requirements on the objective.

Abstract: Tackling simulation optimization problems with non-convex objective functions
remains a fundamental challenge in operations research. In this paper, we
propose a class of random search algorithms, called Regular Tree Search, which
integrates adaptive sampling with recursive partitioning of the search space.
The algorithm concentrates simulations on increasingly promising regions by
iteratively refining a tree structure. A tree search strategy guides sampling
decisions, while partitioning is triggered when the number of samples in a leaf
node exceeds a threshold that depends on its depth. Furthermore, a specific
tree search strategy, Upper Confidence Bounds applied to Trees (UCT), is
employed in the Regular Tree Search. We prove global convergence under
sub-Gaussian noise, based on assumptions involving the optimality gap, without
requiring continuity of the objective function. Numerical experiments confirm
that the algorithm reliably identifies the global optimum and provides accurate
estimates of its objective value.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [501] [Cost-Effective Optimization and Implementation of the CRT-Paillier Decryption Algorithm for Enhanced Performance](https://arxiv.org/abs/2506.17935)
*Zhengwu Huang,Ding Deng,Pengyue Sun,Guangfu Sun,Xiaomei Tang*

Main category: cs.CR

TL;DR: The paper enhances the Paillier homomorphic encryption algorithm by proposing an optimized decryption method and hardware architecture, significantly improving computation speed and area efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational inefficiencies in the Paillier algorithm, particularly the heavy computational burden caused by complex modulo operations in the decryption process.

Method: The paper proposes an enhanced CRT-Paillier decryption algorithm by introducing precomputed parameters, eliminating extra judgment operations, and designing a highly parallel full-pipeline architecture to optimize hardware implementation on FPGA.

Result: The proposed MESA accelerator achieves high throughput and efficiency, completing 2048-bit decryption in 0.577ms and showing significant improvements in throughput and area efficiency compared to prior works.

Conclusion: The proposed solutions effectively enhance the decryption speed, area efficiency, and resource utilization of the Paillier algorithm, making it more viable for practical applications requiring privacy-preserving computations.

Abstract: To address the privacy protection problem in cloud computing, privacy
enhancement techniques such as the Paillier additive homomorphism algorithm are
receiving widespread attention. Paillier algorithm allows addition and scalar
multiplication operations in dencrypted state, which can effectively protect
privacy. However, its computational efficiency is limited by complex modulo
operations due to the ciphertext expansion followed by encryption. To
accelerate its decryption operation, the Chinese Remainder Theorem (CRT) is
often used to optimize these modulo operations, which lengthens the decryption
computation chain in turn. To address this issue, we propose an eCRT-Paillier
decryption algorithm that shortens the decryption computation chain by
combining precomputed parameters and eliminating extra judgment operations
introduced by Montgomery modular multiplications. These two improvements reduce
50% modular multiplications and 60% judgment operations in the postprocessing
of the CRT-Paillier decryption algorithm. Based on these improvements, we
propose a highly parallel full-pipeline architecture to eliminate stalls caused
by multiplier reuse in traditional modular exponentiation operations. This
architecture also adopts some optimizations such as simplifying modular
exponentiation units by dividing the exponent into segments and parallelizing
data flow by multi-core instantiation. Finally, a high-throughput and efficient
Paillier accelerator named MESA was implemented on the Xilinx Virtex-7 FPGA for
evaluation, which can complete a decryption using 2048-bit key within 0.577ms
under 100 MHz clock frequency. Compared to prior works, MESA demonstrates a
throughput improvement of 1.16 to 313.21 under identical conditions, also with
enhancements in area efficiency for LUT, DSP, and FF of 3.32 to 117.55, 1.49 to
1.64, and 2.94 to 9.94, respectively.

</details>


### [502] [Design high-confidence computers using trusted instructional set architecture and emulators](https://arxiv.org/abs/2506.18780)
*Shuangbao Paul Wang*

Main category: cs.CR

TL;DR: The paper addresses CPU vulnerability issues like Spectre and Meltdown, proposing a holistic trusted architecture design for computing.


<details>
  <summary>Details</summary>
Motivation: Modern processors face security threats from exploits such as Spectre and Meltdown, and existing solutions are inadequate to effectively mitigate these vulnerabilities.

Method: The authors propose a comprehensive redesign of trusted computer architecture, integrating high-confidence elements such as trusted instruction sets, sealed kernels, and secure virtualization systems.

Result: The paper introduces a systematic framework that avoids the performance downside of disabling branch predictions or pipelines while tackling the architectural vulnerabilities.

Conclusion: The proposed trusted architecture design offers a feasible and secure solution for enhancing modern computing systems' resilience without sacrificing performance.

Abstract: High-confidence computing relies on trusted instructional set architecture,
sealed kernels, and secure operating systems. Cloud computing depends on
trusted systems for virtualization tasks. Branch predictions and pipelines are
essential in improving performance of a CPU/GPU. But Spectre and Meltdown make
modern processors vulnerable to be exploited. Disabling the prediction and
pipeline is definitely not a good solution. On the other hand, current software
patches can only address non-essential issues around Meltdown. This paper
introduces a holistic approach in trusted computer architecture design and
emulation.

</details>


### [503] [Design, Implementation, and Analysis of Fair Faucets for Blockchain Ecosystems](https://arxiv.org/abs/2506.17236)
*Serdar Metin*

Main category: cs.CR

TL;DR: The dissertation develops six Max-min Fair algorithms to fairly distribute resources in non-commercial blockchain networks, ensuring resistance to DoS attacks and accommodating user weighting policies.


<details>
  <summary>Details</summary>
Motivation: To address the fairness problem in distributing shared resources in non-commercial blockchain networks, where monetary exchange is not an option, and current mechanisms are vulnerable to DoS attacks.

Method: Six distinct Max-min Fair algorithms were developed and implemented to distribute resources in a blockchain environment. These algorithms adapt the faucet mechanism to ensure fairness.

Result: The proposed algorithms demonstrated resistance to DoS attacks, were cost-efficient in terms of blockchain computation, and allowed implementation of diverse user weighting policies.

Conclusion: The study successfully provides a fair, efficient, and secure resource distribution mechanism suited for non-commercial blockchain networks, advancing fairness and robustness.

Abstract: The present dissertation addresses the problem of fairly distributing shared
resources in non-commercial blockchain networks. Blockchains are distributed
systems that order and timestamp records of a given network of users, in a
public, cryptographically secure, and consensual way. The records, which may in
kind be events, transaction orders, sets of rules for structured transactions
etc. are placed within well-defined datastructures called blocks, and they are
linked to each other by the virtue of cryptographic pointers, in a total
ordering which represents their temporal relations of succession. The ability
to operate on the blockchain, and/or to contribute a record to the content of a
block are shared resources of the blockchain systems. In commercial networks,
these resources are exchanged in return for fiat money, and consequently,
fairness is not a relevant problem in terms of computer engineering. In
non-commercial networks, however, monetary solutions are not available, by
definition. The present non-commercial blockchain networks employ trivial
distribution mechanisms called faucets, which offer fixed amounts of free
tokens (called cryptocurrencies) specific to the given network. This mechanism,
although simple and efficient, is prone to denial of service (DoS) attacks and
cannot address the fairness problem. In the present dissertation, the faucet
mechanism is adapted for fair distribution, in line with Max-min Fairness
scheme. In total, we contributed 6 distinct Max-min Fair algorithms as
efficient blockchain faucets. The algorithms we contribute are resistant to DoS
attacks, low-cost in terms of blockchain computation economics, and they also
allow for different user weighting policies.

</details>


### [504] [A Locally Differential Private Coding-Assisted Succinct Histogram Protocol](https://arxiv.org/abs/2506.17767)
*Hsuan-Po Liu,Hessam Mahdavifar*

Main category: cs.CR

TL;DR: The paper proposes the first practical $(\epsilon,\delta)$-LDP protocol using error-correcting polar codes to build succinct histograms for large-scale, privacy-sensitive ML applications.


<details>
  <summary>Details</summary>
Motivation: Privacy-sensitive machine learning applications require effective solutions for identifying frequent items without compromising privacy, where local differential privacy (LDP) guarantees are crucial.

Method: The methodology involves leveraging polar codes with successive-cancellation list decoding. Gaussian-based perturbation is introduced to facilitate efficient soft decoding under the $(\epsilon, \delta)$-LDP framework.

Result: The proposed approach demonstrates superior performance in capturing and estimating low-frequency items under LDP constraints, outperforming existing methods.

Conclusion: Polar codes and Gaussian perturbation can be effectively utilized within the $(\epsilon, \delta)$-LDP framework to construct succinct histograms, improving utility for low-frequency data while ensuring privacy.

Abstract: A succinct histogram captures frequent items and their frequencies across
clients and has become increasingly important for large-scale,
privacy-sensitive machine learning applications. To develop a rigorous
framework to guarantee privacy for the succinct histogram problem, local
differential privacy (LDP) has been utilized and shown promising results. To
preserve data utility under LDP, which essentially works by intentionally
adding noise to data, error-correcting codes naturally emerge as a promising
tool for reliable information collection. This work presents the first
practical $(\epsilon,\delta)$-LDP protocol for constructing succinct histograms
using error-correcting codes. To this end, polar codes and their
successive-cancellation list (SCL) decoding algorithms are leveraged as the
underlying coding scheme. More specifically, our protocol introduces
Gaussian-based perturbations to enable efficient soft decoding. Experiments
demonstrate that our approach outperforms prior methods, particularly for items
with low true frequencies, while maintaining similar frequency estimation
accuracy.

</details>


### [505] [Tracking GPTs Third Party Service: Automation, Analysis, and Insights](https://arxiv.org/abs/2506.17315)
*Chuan Yan,Liuhuo Wan,Bowei Guan,Fengqi Yu,Guangdong Bai,Jin Song Dong*

Main category: cs.CR

TL;DR: The paper introduces GPTs-ThirdSpy, a framework to analyze the privacy settings and third-party integrations of GPT applications.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the lack of transparency and challenges in analyzing the data privacy implications of third-party services used by GPT applications.

Method: The authors created GPTs-ThirdSpy, an automated framework that extracts privacy settings and metadata of GPT applications for analysis.

Result: GPTs-ThirdSpy enables real-time access to metadata, facilitating research on the integration, compliance, and security risks of GPTs.

Conclusion: By providing a tool for structured data collection, GPTs-ThirdSpy supports large-scale research on transparency and regulatory challenges in the GPT app ecosystem.

Abstract: ChatGPT has quickly advanced from simple natural language processing to
tackling more sophisticated and specialized tasks. Drawing inspiration from the
success of mobile app ecosystems, OpenAI allows developers to create
applications that interact with third-party services, known as GPTs. GPTs can
choose to leverage third-party services to integrate with specialized APIs for
domain-specific applications. However, the way these disclose privacy setting
information limits accessibility and analysis, making it challenging to
systematically evaluate the data privacy implications of third-party integrate
to GPTs. In order to support academic research on the integration of
third-party services in GPTs, we introduce GPTs-ThirdSpy, an automated
framework designed to extract privacy settings of GPTs. GPTs-ThirdSpy provides
academic researchers with real-time, reliable metadata on third-party services
used by GPTs, enabling in-depth analysis of their integration, compliance, and
potential security risks. By systematically collecting and structuring this
data, GPTs-ThirdSpy facilitates large-scale research on the transparency and
regulatory challenges associated with the GPT app ecosystem.

</details>


### [506] [Smart-LLaMA-DPO: Reinforced Large Language Model for Explainable Smart Contract Vulnerability Detection](https://arxiv.org/abs/2506.18245)
*Lei Yu,Zhirong Huang,Hang Yuan,Shiqi Cheng,Li Yang,Fengjun Zhang,Chenjie Shen,Jiajia Ma,Jingyuan Zhang,Junyi Lu,Chun Zuo*

Main category: cs.CR

TL;DR: This paper addresses smart contract vulnerability detection by creating Smart-LLaMA-DPO, an LLM-based system that significantly improves accuracy and explanation quality in detecting vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome two major challenges: insufficient quality and coverage in datasets for preference learning, and the inability of LLMs to reliably interpret specific blockchain security concepts.

Method: The authors propose a specialized LLM (Smart-LLaMA-DPO) enhanced via continual pre-training with large-scale smart contract data, supervised fine-tuning using a comprehensive dataset, and Direct Preference Optimization (DPO) with human feedback and a custom loss function.

Result: Smart-LLaMA-DPO achieves average improvements of 10.43% in F1 score and 7.87% in accuracy over baselines. It also produces more accurate and clearer vulnerability explanations as confirmed by both LLM and human evaluations.

Conclusion: The study demonstrates significant advancements in smart contract vulnerability detection, combining enhanced LLM training techniques and a robust dataset to improve both accuracy and the quality of explanations.

Abstract: Smart contract vulnerability detection remains a major challenge in
blockchain security. Existing vulnerability detection methods face two main
issues: (1) Existing datasets lack comprehensive coverage and high-quality
explanations for preference learning. (2) Large language models (LLMs) often
struggle with accurately interpreting specific concepts in smart contract
security. Empirical analysis shows that even after continual pre-training (CPT)
and supervised fine-tuning (SFT), LLMs may misinterpret the execution order of
state changes, resulting in incorrect explanations despite making correct
detection decisions. To address these challenges, we propose Smart-LLaMA-DPO
based on LLaMA-3.1-8B. We construct a comprehensive dataset covering four major
vulnerability types and machine-unauditable vulnerabilities, including precise
labels, explanations, and locations for SFT, as well as high-quality and
low-quality output pairs for Direct Preference Optimization (DPO). Second, we
perform CPT using large-scale smart contract to enhance the LLM's understanding
of specific security practices in smart contracts. Futhermore, we conduct SFT
with our comprehensive dataset. Finally, we apply DPO, leveraging human
feedback and a specially designed loss function that increases the probability
of preferred explanations while reducing the likelihood of non-preferred
outputs. We evaluate Smart-LLaMA-DPO on four major vulnerability types:
reentrancy, timestamp dependence, integer overflow/underflow, and delegatecall,
as well as machine-unauditable vulnerabilities. Our method significantly
outperforms state-of-the-art baselines, with average improvements of 10.43% in
F1 score and 7.87% in accuracy. Moreover, both LLM evaluation and human
evaluation confirm that our method generates more correct, thorough, and clear
explanations.

</details>


### [507] [Automatic Selection of Protections to Mitigate Risks Against Software Applications](https://arxiv.org/abs/2506.18470)
*Daniele Canavese,Leonardo Regano,Bjorn De Sutter,Cataldo Basile*

Main category: cs.CR

TL;DR: This paper presents an automated approach for selecting software protections using a game-theoretic model to guard against MATE risks.


<details>
  <summary>Details</summary>
Motivation: To mitigate MATE risks to critical assets in software applications and improve defense strategies.

Method: The approach involves a game-theoretic model with dynamic programming optimizations and a mini-max depth-first strategy.

Result: A Software Protection Index was introduced and validated via proof-of-concept implementation and expert evaluations.

Conclusion: Automated protection selection is effective and practical for mitigating software risk in a scalable way.

Abstract: This paper introduces a novel approach for the automated selection of
software protections to mitigate MATE risks against critical assets within
software applications. We formalize the key elements involved in protection
decision-making - including code artifacts, assets, security requirements,
attacks, and software protections - and frame the protection process through a
game-theoretic model. In this model, a defender strategically applies
protections to various code artifacts of a target application, anticipating
repeated attack attempts by adversaries against the confidentiality and
integrity of the application's assets. The selection of the optimal defense
maximizes resistance to attacks while ensuring the application remains usable
by constraining the overhead introduced by protections. The game is solved
through a heuristic based on a mini-max depth-first exploration strategy,
augmented with dynamic programming optimizations for improved efficiency.
Central to our formulation is the introduction of the Software Protection
Index, an original contribution that extends existing notions of potency and
resilience by evaluating protection effectiveness against attack paths using
software metrics and expert assessments. We validate our approach through a
proof-of-concept implementation and expert evaluations, demonstrating that
automated software protection is a practical and effective solution for risk
mitigation in software.

</details>


### [508] [FORGE: An LLM-driven Framework for Large-Scale Smart Contract Vulnerability Dataset Construction](https://arxiv.org/abs/2506.18795)
*Jiachi Chen,Yiming Shen,Jiashuo Zhang,Zihao Li,John Grundy,Zhenzhe Shao,Yanlin Wang,Jiashui Wang,Ting Chen,Zibin Zheng*

Main category: cs.CR

TL;DR: This paper introduces FORGE, an automated method for constructing high-quality smart contract vulnerability datasets using LLMs and CWE classification.


<details>
  <summary>Details</summary>
Motivation: Current smart contract vulnerability datasets are limited due to manual, error-prone processes and inconsistencies in classification rules.

Method: FORGE uses an LLM-driven pipeline to analyze real-world audit reports, extracting vulnerabilities and classifying them using the CWE hierarchy through a divide-and-conquer and tree-of-thoughts strategy.

Result: FORGE analyzed 6,454 audit reports to produce a dataset containing 81,390 Solidity files and 27,497 vulnerability findings, demonstrating high precision (95.6%) and consistency (inter-rater agreement of 0.87).

Conclusion: The dataset generated by FORGE is practical for benchmarking security tools and provides insights into real-world vulnerabilities, highlighting weaknesses in current detection approaches.

Abstract: High-quality smart contract vulnerability datasets are critical for
evaluating security tools and advancing smart contract security research. Two
major limitations of current manual dataset construction are (1)
labor-intensive and error-prone annotation processes limiting the scale,
quality, and evolution of the dataset, and (2) absence of standardized
classification rules results in inconsistent vulnerability categories and
labeling results across different datasets. To address these limitations, we
present FORGE, the first automated approach for constructing smart contract
vulnerability datasets. FORGE leverages an LLM-driven pipeline to extract
high-quality vulnerabilities from real-world audit reports and classify them
according to the CWE, the most widely recognized classification in software
security. FORGE employs a divide-and-conquer strategy to extract structured and
self-contained vulnerability information from these reports. Additionally, it
uses a tree-of-thoughts technique to classify the vulnerability information
into the hierarchical CWE classification. To evaluate FORGE's effectiveness, we
run FORGE on 6,454 real-world audit reports and generate a dataset comprising
81,390 solidity files and 27,497 vulnerability findings across 296 CWE
categories. Manual assessment of the dataset demonstrates high extraction
precision and classification consistency with human experts (precision of 95.6%
and inter-rater agreement k-$\alpha$ of 0.87). We further validate the
practicality of our dataset by benchmarking 13 existing security tools on our
dataset. The results reveal the significant limitations in current detection
capabilities. Furthermore, by analyzing the severity-frequency distribution
patterns through a unified CWE perspective in our dataset, we highlight
inconsistency between current smart contract research focus and priorities
identified from real-world vulnerabilities...

</details>


### [509] [Step-by-Step Reasoning Attack: Revealing 'Erased' Knowledge in Large Language Models](https://arxiv.org/abs/2506.17279)
*Yash Sinha,Manit Baser,Murari Mandal,Dinil Mon Divakaran,Mohan Kankanhalli*

Main category: cs.CR

TL;DR: This paper addresses the failure of existing unlearning techniques in large language models (LLMs) to reliably erase knowledge, showcasing how adversarial prompts can retrieve supposedly erased information.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in ensuring compliance with data privacy regulations and mitigating risks like bias and misinformation by effectively erasing specific knowledge from LLMs.

Method: The authors propose an attack framework called Sleek, which uses step-by-step reasoning to generate adversarial prompts and uncover failures in unlearning techniques. The framework categorizes prompts into direct, indirect, and implied types.

Result: Through evaluation on four unlearning methods and two LLMs, the study found 62.5% success in retrieving erased Harry Potter facts and 50% success in exposing unfair suppression of retained information.

Conclusion: The research shows that current knowledge-erasure strategies in LLMs are inadequate, stressing the need for more reliable and robust unlearning approaches to prevent information leakage.

Abstract: Knowledge erasure in large language models (LLMs) is important for ensuring
compliance with data and AI regulations, safeguarding user privacy, mitigating
bias, and misinformation. Existing unlearning methods aim to make the process
of knowledge erasure more efficient and effective by removing specific
knowledge while preserving overall model performance, especially for retained
information. However, it has been observed that the unlearning techniques tend
to suppress and leave the knowledge beneath the surface, thus making it
retrievable with the right prompts. In this work, we demonstrate that
\textit{step-by-step reasoning} can serve as a backdoor to recover this hidden
information. We introduce a step-by-step reasoning-based black-box attack,
Sleek, that systematically exposes unlearning failures. We employ a structured
attack framework with three core components: (1) an adversarial prompt
generation strategy leveraging step-by-step reasoning built from LLM-generated
queries, (2) an attack mechanism that successfully recalls erased content, and
exposes unfair suppression of knowledge intended for retention and (3) a
categorization of prompts as direct, indirect, and implied, to identify which
query types most effectively exploit unlearning weaknesses. Through extensive
evaluations on four state-of-the-art unlearning techniques and two widely used
LLMs, we show that existing approaches fail to ensure reliable knowledge
removal. Of the generated adversarial prompts, 62.5% successfully retrieved
forgotten Harry Potter facts from WHP-unlearned Llama, while 50% exposed unfair
suppression of retained knowledge. Our work highlights the persistent risks of
information leakage, emphasizing the need for more robust unlearning strategies
for erasure.

</details>


### [510] [Theoretically Unmasking Inference Attacks Against LDP-Protected Clients in Federated Vision Models](https://arxiv.org/abs/2506.17292)
*Quan Nguyen,Minh N. Vu,Truc Nguyen,My T. Thai*

Main category: cs.CR

TL;DR: The paper examines privacy risks in Federated Learning systems protected by Local Differential Privacy (LDP) against Membership Inference Attacks (MIAs). Results demonstrate that even LDP-protected data carry significant risks, dependent on the privacy budget.


<details>
  <summary>Details</summary>
Motivation: Federated Learning promises privacy preservation by avoiding direct data sharing, but MIAs reveal vulnerabilities. There is a need to understand privacy risks under LDP, as previous works fail to address these risks theoretically or practically.

Method: The authors derive theoretical lower bounds for the success rates of MIAs targeting LDP-protected data and test practical models to confirm privacy risks. Evaluations focus on federated vision models impacted by local differential privacy noise.

Result: The study shows that while LDP offers privacy protection, it is insufficient to eliminate risks from MIAs, especially as these risks correlate with the privacy budget. Additionally, sufficient noise required to counteract MIAs severely compromises model utility.

Conclusion: Privacy risks persist in Federated Learning even with LDP protections, suggesting a trade-off between data privacy and model utility that demands careful consideration in deploying such systems.

Abstract: Federated Learning enables collaborative learning among clients via a
coordinating server while avoiding direct data sharing, offering a perceived
solution to preserve privacy. However, recent studies on Membership Inference
Attacks (MIAs) have challenged this notion, showing high success rates against
unprotected training data. While local differential privacy (LDP) is widely
regarded as a gold standard for privacy protection in data analysis, most
studies on MIAs either neglect LDP or fail to provide theoretical guarantees
for attack success rates against LDP-protected data. To address this gap, we
derive theoretical lower bounds for the success rates of low-polynomial time
MIAs that exploit vulnerabilities in fully connected or self-attention layers.
We establish that even when data are protected by LDP, privacy risks persist,
depending on the privacy budget. Practical evaluations on federated vision
models confirm considerable privacy risks, revealing that the noise required to
mitigate these attacks significantly degrades models' utility.

</details>


### [511] [LLM Jailbreak Oracle](https://arxiv.org/abs/2506.17299)
*Shuyi Lin,Anshuman Suri,Alina Oprea,Cheng Tan*

Main category: cs.CR

TL;DR: The paper introduces the problem of identifying jailbreak vulnerabilities in large language models and proposes Boa, an efficient algorithm that systematically assesses and explores these vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: The lack of systematic methods to assess large language models' vulnerabilities to jailbreak attacks creates a significant security gap, especially as these models are deployed in safety-critical applications.

Method: The authors define the "jailbreak oracle problem" and propose Boa, which uses a three-phase search strategy: constructing block lists, breadth-first sampling to detect accessible jailbreaks, and depth-first priority search for exploring less likely but possible jailbreak paths.

Result: Boa provides a means for rigorous security assessments, including defense evaluation, standardized attack comparisons, and model certification under adversarial conditions.

Conclusion: The authors present Boa as the first efficient algorithm addressing jailbreak vulnerabilities, enabling a formalized, systematic, and practical approach to LLM security evaluation.

Abstract: As large language models (LLMs) become increasingly deployed in
safety-critical applications, the lack of systematic methods to assess their
vulnerability to jailbreak attacks presents a critical security gap. We
introduce the jailbreak oracle problem: given a model, prompt, and decoding
strategy, determine whether a jailbreak response can be generated with
likelihood exceeding a specified threshold. This formalization enables a
principled study of jailbreak vulnerabilities. Answering the jailbreak oracle
problem poses significant computational challenges -- the search space grows
exponentially with the length of the response tokens. We present Boa, the first
efficient algorithm for solving the jailbreak oracle problem. Boa employs a
three-phase search strategy: (1) constructing block lists to identify refusal
patterns, (2) breadth-first sampling to identify easily accessible jailbreaks,
and (3) depth-first priority search guided by fine-grained safety scores to
systematically explore promising low-probability paths. Boa enables rigorous
security assessments including systematic defense evaluation, standardized
comparison of red team attacks, and model certification under extreme
adversarial conditions.

</details>


### [512] [Context manipulation attacks : Web agents are susceptible to corrupted memory](https://arxiv.org/abs/2506.17318)
*Atharv Singh Patlan,Ashwin Hebbar,Pramod Viswanath,Prateek Mittal*

Main category: cs.CR

TL;DR: The study introduces 'plan injection,' a novel attack method targeting autonomous web navigation systems' context vulnerabilities, offering higher success rates in compromising agents compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address the growing security vulnerabilities presented by external memory management in autonomous navigation agents, which are often exploited for malicious purposes.

Method: The paper systematically evaluates the efficacy of 'plan injections' and 'context-chained injections' across two popular web navigation agents: Browser-use and Agent-E.

Result: Plan injections outperform prompt injection defenses, achieving up to triple the attack success rates, and context-chained injections increase success rates in privacy exfiltration by 17.7%.

Conclusion: The study emphasizes the urgent need for secure memory management in agentic systems to counteract advanced attacks like plan and context-chained injections.

Abstract: Autonomous web navigation agents, which translate natural language
instructions into sequences of browser actions, are increasingly deployed for
complex tasks across e-commerce, information retrieval, and content discovery.
Due to the stateless nature of large language models (LLMs), these agents rely
heavily on external memory systems to maintain context across interactions.
Unlike centralized systems where context is securely stored server-side, agent
memory is often managed client-side or by third-party applications, creating
significant security vulnerabilities. This was recently exploited to attack
production systems.
  We introduce and formalize "plan injection," a novel context manipulation
attack that corrupts these agents' internal task representations by targeting
this vulnerable context. Through systematic evaluation of two popular web
agents, Browser-use and Agent-E, we show that plan injections bypass robust
prompt injection defenses, achieving up to 3x higher attack success rates than
comparable prompt-based attacks. Furthermore, "context-chained injections,"
which craft logical bridges between legitimate user goals and attacker
objectives, lead to a 17.7% increase in success rate for privacy exfiltration
tasks. Our findings highlight that secure memory handling must be a first-class
concern in agentic systems.

</details>


### [513] [On the Performance of Cyber-Biomedical Features for Intrusion Detection in Healthcare 5.0](https://arxiv.org/abs/2506.17329)
*Pedro H. Lui,Lucas P. Siqueira,Juliano F. Kazienko,Vagner E. Quincozes,Silvio E. Quincozes,Daniel Welfer*

Main category: cs.CR

TL;DR: The study uses eXplainable AI (XAI) to analyze Healthcare 5.0 datasets with network traffic and biomedical sensor data, achieving high classification performance and offering insights into feature contributions.


<details>
  <summary>Details</summary>
Motivation: Current AI-driven cybersecurity models inadequately handle biomedical data, which limits their effectiveness and interpretability, especially in the context of Healthcare 5.0.

Method: The authors applied eXplainable AI (XAI) techniques to a Healthcare 5.0 dataset that includes network traffic and biomedical sensor data, focusing on classification outputs and feature explainability.

Result: XGBoost achieved an F1-score of 99% for benign and data alteration classification, and 81% for spoofing. Network data dominated intrusion detection, while biomedical features were crucial for spoofing detection (e.g., temperature with Shapley value magnitude of 0.37).

Conclusion: This study demonstrates the importance of integrating network and biomedical sensor data for cyber-threat detection in Healthcare 5.0, highlighting XAI's utility for model interpretability and improved security outcomes.

Abstract: Healthcare 5.0 integrates Artificial Intelligence (AI), the Internet of
Things (IoT), real-time monitoring, and human-centered design toward
personalized medicine and predictive diagnostics. However, the increasing
reliance on interconnected medical technologies exposes them to cyber threats.
Meanwhile, current AI-driven cybersecurity models often neglect biomedical
data, limiting their effectiveness and interpretability. This study addresses
this gap by applying eXplainable AI (XAI) to a Healthcare 5.0 dataset that
integrates network traffic and biomedical sensor data. Classification outputs
indicate that XGBoost achieved 99% F1-score for benign and data alteration, and
81% for spoofing. Explainability findings reveal that network data play a
dominant role in intrusion detection whereas biomedical features contributed to
spoofing detection, with temperature reaching a Shapley values magnitude of
0.37.

</details>


### [514] [CUBA: Controlled Untargeted Backdoor Attack against Deep Neural Networks](https://arxiv.org/abs/2506.17350)
*Yinghao Wu,Liyan Zhang*

Main category: cs.CR

TL;DR: The paper proposes "Constrained Untargeted Backdoor Attack" (CUBA) to enhance the flexibility of backdoor attacks by randomizing classifications within a set range of target classes. It demonstrates that CUBA effectively evades current defense mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing backdoor attacks typically rely on a direct and strong trigger-to-target connection. This inherent property has allowed backdoor detection methods to mitigate such attacks, leaving space for a more flexible and potent approach.

Method: The researchers introduce CUBA, which modifies cross-entropy loss using logit normalization and flipped one-hot labels to constrain the randomized classifications to a predefined set of target classes, resulting in a controlled untargeted attack.

Result: Their experiments showcase that the proposed method leads to compromised models classifying backdoor inputs with a uniform distribution across a subset of target classes, effectively evading existing backdoor detection methodologies.

Conclusion: CUBA bridges the gap between untargeted and targeted backdoor attacks, offering attackers enhanced flexibility while maintaining an effective evasion of current defense strategies.

Abstract: Backdoor attacks have emerged as a critical security threat against deep
neural networks in recent years. The majority of existing backdoor attacks
focus on targeted backdoor attacks, where trigger is strongly associated to
specific malicious behavior. Various backdoor detection methods depend on this
inherent property and shows effective results in identifying and mitigating
such targeted attacks. However, a purely untargeted attack in backdoor
scenarios is, in some sense, self-weakening, since the target nature is what
makes backdoor attacks so powerful. In light of this, we introduce a novel
Constrained Untargeted Backdoor Attack (CUBA), which combines the flexibility
of untargeted attacks with the intentionality of targeted attacks. The
compromised model, when presented with backdoor images, will classify them into
random classes within a constrained range of target classes selected by the
attacker. This combination of randomness and determinedness enables the
proposed untargeted backdoor attack to natively circumvent existing backdoor
defense methods. To implement the untargeted backdoor attack under controlled
flexibility, we propose to apply logit normalization on cross-entropy loss with
flipped one-hot labels. By constraining the logit during training, the
compromised model will show a uniform distribution across selected target
classes, resulting in controlled untargeted attack. Extensive experiments
demonstrate the effectiveness of the proposed CUBA on different datasets.

</details>


### [515] [Differentiation-Based Extraction of Proprietary Data from Fine-Tuned LLMs](https://arxiv.org/abs/2506.17353)
*Zongjie Li,Daoyuan Wu,Shuai Wang,Zhendong Su*

Main category: cs.CR

TL;DR: The paper investigates the risks of extracting fine-tuning data from domain-specific LLMs, introduces a novel attack method called Differentiated Data Extraction (DDE), and proposes a defense to mitigate such attacks.


<details>
  <summary>Details</summary>
Motivation: To address the critical problem of extracting valuable instruction-response pairs from fine-tuned large language models, which poses security risks.

Method: The researchers developed DDE, leveraging the confidence levels and behavioral differences between fine-tuned and pre-trained models for data extraction.

Result: Experiments demonstrated that DDE consistently performs better than existing extraction methods, revealing vulnerabilities in fine-tuned LLMs.

Conclusion: Fine-tuned LLMs face hidden risks of data leaks, and the study highlights the need for secure modeling practices while providing a defense mechanism against DDE attacks.

Abstract: The increasing demand for domain-specific and human-aligned Large Language
Models (LLMs) has led to the widespread adoption of Supervised Fine-Tuning
(SFT) techniques. SFT datasets often comprise valuable instruction-response
pairs, making them highly valuable targets for potential extraction. This paper
studies this critical research problem for the first time. We start by formally
defining and formulating the problem, then explore various attack goals, types,
and variants based on the unique properties of SFT data in real-world
scenarios. Based on our analysis of extraction behaviors of direct extraction,
we develop a novel extraction method specifically designed for SFT models,
called Differentiated Data Extraction (DDE), which exploits the confidence
levels of fine-tuned models and their behavioral differences from pre-trained
base models. Through extensive experiments across multiple domains and
scenarios, we demonstrate the feasibility of SFT data extraction using DDE. Our
results show that DDE consistently outperforms existing extraction baselines in
all attack settings. To counter this new attack, we propose a defense mechanism
that mitigates DDE attacks with minimal impact on model performance. Overall,
our research reveals hidden data leak risks in fine-tuned LLMs and provides
insights for developing more secure models.

</details>


### [516] [Shrinking the Generation-Verification Gap with Weak Verifiers](https://arxiv.org/abs/2506.18203)
*Jon Saad-Falcon,E. Kelly Buchanan,Mayee F. Chen,Tzu-Heng Huang,Brendan McLaughlin,Tanvir Bhathal,Shang Zhu,Ben Athiwaratkun,Frederic Sala,Scott Linderman,Azalia Mirhoseini,Christopher Ré*

Main category: cs.CR

TL;DR: Weaver is a framework combining multiple weak verifiers to enhance the scoring and ranking of language model responses, achieving significant improvements in accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to bridge the performance gap between existing general-purpose verifiers (e.g., LM judges) and oracle verifiers, which possess perfect accuracy. Current solutions are either unscalable or limited in utility.

Method: Weaver leverages weighted ensembles of weak verifiers and uses weak supervision to estimate verifier accuracies. It utilizes dataset statistics for normalizing outputs and filters low-quality verifiers to create a unified scoring mechanism.

Result: Weaver demonstrated impactful improvements in selecting high-quality responses across reasoning and math tasks, achieving accuracy at o3-mini level using Llama 3.3 70B and smaller judge/reward models.

Conclusion: Weaver improves response selection significantly over standard methods, reducing the dependency on labeled data and computational costs, while maintaining high accuracy by using advanced ensemble techniques.

Abstract: Verifiers can improve language model capabilities by scoring and ranking
responses from generated candidates. Currently, high-quality verifiers are
either unscalable (e.g., humans) or limited in utility (e.g., tools like Lean).
While LM judges and reward models have become broadly useful as general-purpose
verifiers, a significant performance gap remains between them and oracle
verifiers (verifiers with perfect accuracy). To help close this gap, we
introduce Weaver, a framework for designing a strong verifier by combining
multiple weak, imperfect verifiers. We find weighted ensembles of verifiers,
which typically require learning from labeled data, significantly outperform
unweighted combinations due to differences in verifier accuracies. To reduce
dependency on labeled data, Weaver leverages weak supervision to estimate each
verifier's accuracy and combines outputs into a unified score that better
reflects true response quality. However, directly applying weak supervision
algorithms poses challenges, including inconsistent verifier output formats and
handling low-quality verifiers. Weaver addresses these using dataset statistics
to normalize outputs and filter specific verifiers. We study Weaver's
effectiveness in test-time repeated sampling, where a model generates multiple
candidate responses and selects one. Our evaluations show Weaver significantly
improves over Pass@1-performance when selecting the first candidate-across
reasoning and math tasks, achieving o3-mini-level accuracy with Llama 3.3 70B
Instruct as generator, and an ensemble of 70B or smaller judge and reward
models as verifiers (87.7% average). This gain mirrors the jump between GPT-4o
and o3-mini (69.0% vs. 86.7%), which required extensive finetuning and
post-training. To reduce computational costs of verifier ensembles, we train a
400M cross-encoder using Weaver's combined output scores.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [517] [Quantification of Sim2Real Gap via Neural Simulation Gap Function](https://arxiv.org/abs/2506.17675)
*P Sangeerth,Pushpak Jagtap*

Main category: eess.SY

TL;DR: The paper introduces neural simulation gap functions to quantify discrepancies between mathematical models and high-fidelity simulators. It provides methods to design robust controllers using this gap function and demonstrates this approach on simulations of a Mecanum bot and a Pendulum.


<details>
  <summary>Details</summary>
Motivation: Controllers designed for mathematical models often fail in reality due to the unmodeled gap between theoretical models and high-fidelity simulations. This paper seeks to provide a formal way to quantify and overcome this gap.

Method: The authors train a neural network with finite data points from high-fidelity simulators to quantify the simulation gap function. They then extrapolate guarantees for the entire state space, including unseen data points, ensuring a robust transition from simulation to reality.

Result: The proposed neural simulation gap function successfully bridges the gap between simulation and real-world systems. The approach is validated using two case studies: a Mecanum bot and a Pendulum.

Conclusion: The paper concludes that neural simulation gap functions offer a reliable means to ensure smooth and guaranteed transitions from mathematical models to practical implementations, enhancing controller design robustness across unseen states.

Abstract: In this paper, we introduce the notion of neural simulation gap functions,
which formally quantifies the gap between the mathematical model and the model
in the high-fidelity simulator, which closely resembles reality. Many times, a
controller designed for a mathematical model does not work in reality because
of the unmodelled gap between the two systems. With the help of this simulation
gap function, one can use existing model-based tools to design controllers for
the mathematical system and formally guarantee a decent transition from the
simulation to the real world. Although in this work, we have quantified this
gap using a neural network, which is trained using a finite number of data
points, we give formal guarantees on the simulation gap function for the entire
state space including the unseen data points. We collect data from
high-fidelity simulators leveraging recent advancements in Real-to-Sim transfer
to ensure close alignment with reality. We demonstrate our results through two
case studies - a Mecanum bot and a Pendulum.

</details>


### [518] [A Digital Twin Framework for Generation-IV Reactors with Reinforcement Learning-Enabled Health-Aware Supervisory Control](https://arxiv.org/abs/2506.17258)
*Jasmin Y. Lim,Dimitrios Pylorof,Humberto E. Garcia,Karthik Duraisamy*

Main category: eess.SY

TL;DR: This paper proposes a digital twin framework for operating Generation IV Fluoride-salt-cooled High-temperature Reactors (Gen-IV FHRs), optimizing maintenance and operational strategies while ensuring system constraints are met.


<details>
  <summary>Details</summary>
Motivation: The high costs associated with deploying Gen-IV nuclear power plants hinder their adoption, inspiring the need for advanced tools to improve efficiency and reduce expenses.

Method: The authors developed a closed-loop digital twin framework integrating surrogate modeling, reinforcement learning, Bayesian inference, and a Reference Governor control algorithm, all aimed at improving maintenance and operational decision-making.

Result: The framework was tested through three case studies, demonstrating capabilities in long-term maintenance planning, short-term operational accuracy, and real-time recalibration during system shocks.

Conclusion: The digital twin framework provides robust, constraint-compliant, and health-aware operation for advanced reactors like Gen-IV FHRs, with applications to other complex systems.

Abstract: Generation IV (Gen-IV) nuclear power plants are envisioned to replace the
current reactor fleet, bringing improvements in performance, safety,
reliability, and sustainability. However, large cost investments currently
inhibit the deployment of these advanced reactor concepts. Digital twins bridge
real-world systems with digital tools to reduce costs, enhance decision-making,
and boost operational efficiency. In this work, a digital twin framework is
designed to operate the Gen-IV Fluoride-salt-cooled High-temperature Reactor,
utilizing data-enhanced methods to optimize operational and maintenance
policies while adhering to system constraints. The closed-loop framework
integrates surrogate modeling, reinforcement learning, and Bayesian inference
to streamline end-to-end communication for online regulation and
self-adjustment. Reinforcement learning is used to consider component health
and degradation to drive the target power generations, with constraints
enforced through a Reference Governor control algorithm that ensures compliance
with pump flow rate and temperature limits. These input driving modules benefit
from detailed online simulations that are assimilated to measurement data with
Bayesian filtering. The digital twin is demonstrated in three case studies: a
one-year long-term operational period showcasing maintenance planning
capabilities, short-term accuracy refinement with high-frequency measurements,
and system shock capturing that demonstrates real-time recalibration
capabilities when change in boundary conditions. These demonstrations validate
robustness for health-aware and constraint-informed nuclear plant operation,
with general applicability to other advanced reactor concepts and complex
engineering systems.

</details>


### [519] [Conformal Safety Shielding for Imperfect-Perception Agents](https://arxiv.org/abs/2506.17275)
*William Scarbro,Calum Imrie,Sinem Getir Yaman,Kavan Fatehi,Corina S. Pasareanu,Radu Calinescu,Ravi Mangal*

Main category: eess.SY

TL;DR: This paper proposes a safety mechanism for autonomous agents with imperfect perception, providing probabilistic safety guarantees via action restrictions governed by conformal prediction.


<details>
  <summary>Details</summary>
Motivation: To address safety concerns in autonomous systems that rely on learned components for state estimation, especially when perception is imperfect and errors can lead to unsafe outcomes.

Method: The authors introduce a shield mechanism that restricts agent actions based on state estimates derived via conformal prediction, ensuring predicted estimates encompass true states with a user-defined probability.

Result: The shield guarantees local safety by only allowing universally safe actions across all state estimates, and a global safety proof bounds the likelihood of unsafe outcomes. A case study demonstrates its application in airplane taxiing systems.

Conclusion: The proposed shielding approach provides a practical and provably safe framework for controlling autonomous agents under perception uncertainty.

Abstract: We consider the problem of safe control in discrete autonomous agents that
use learned components for imperfect perception (or more generally, state
estimation) from high-dimensional observations. We propose a shield
construction that provides run-time safety guarantees under perception errors
by restricting the actions available to an agent, modeled as a Markov decision
process, as a function of the state estimates. Our construction uses conformal
prediction for the perception component, which guarantees that for each
observation, the predicted set of estimates includes the actual state with a
user-specified probability. The shield allows an action only if it is allowed
for all the estimates in the predicted set, resulting in a local safety
guarantee. We also articulate and prove a global safety property of existing
shield constructions for perfect-perception agents bounding the probability of
reaching unsafe states if the agent always chooses actions prescribed by the
shield. We illustrate our approach with a case-study of an experimental
autonomous system that guides airplanes on taxiways using high-dimensional
perception DNNs.

</details>


### [520] [A Theoretical Framework for Virtual Power Plant Integration with Gigawatt-Scale AI Data Centers: Multi-Timescale Control and Stability Analysis](https://arxiv.org/abs/2506.17284)
*Ali Peivandizadeh*

Main category: eess.SY

TL;DR: The paper proposes a novel framework to integrate AI-driven data centers into power systems, addressing their extreme power dynamics through advanced Virtual Power Plant architectures and stability mechanisms.


<details>
  <summary>Details</summary>
Motivation: The rapid development of AI has significantly increased data center electricity demands, creating power challenges due to fluctuating loads that require novel stability and control solutions.

Method: A four-layer hierarchical control architecture is introduced, incorporating sub-millisecond controls and new stability criteria, tailored for converter-dominated systems with dynamic loads.

Result: The new framework ensures stability under extreme AI-driven power fluctuations, achieves peak power reductions of 30%, and maintains high AI service availability (99.95%).

Conclusion: This theoretical foundation enables sustainable integration of AI infrastructures into power systems, foreseeing their dominance in electricity consumption by 2030.

Abstract: The explosive growth of artificial intelligence has created gigawatt-scale
data centers that fundamentally challenge power system operation, exhibiting
power fluctuations exceeding 500 MW within seconds and millisecond-scale
variations of 50-75% of thermal design power. This paper presents a
comprehensive theoretical framework that reconceptualizes Virtual Power Plants
(VPPs) to accommodate these extreme dynamics through a four-layer hierarchical
control architecture operating across timescales from 100 microseconds to 24
hours.
  We develop control mechanisms and stability criteria specifically tailored to
converter-dominated systems with pulsing megawatt-scale loads. We prove that
traditional VPP architectures, designed for aggregating distributed resources
with response times of seconds to minutes, cannot maintain stability when
confronted with AI data center dynamics exhibiting slew rates exceeding 1,000
MW/s at gigawatt scale.
  Our framework introduces: (1) a sub-millisecond control layer that interfaces
with data center power electronics to actively dampen power oscillations; (2)
new stability criteria incorporating protection system dynamics, demonstrating
that critical clearing times reduce from 150 ms to 83 ms for gigawatt-scale
pulsing loads; and (3) quantified flexibility characterization showing that
workload deferability enables 30% peak reduction while maintaining AI service
availability above 99.95%.
  This work establishes the mathematical foundations necessary for the stable
integration of AI infrastructure that will constitute 50-70% of data center
electricity consumption by 2030.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [521] [A Grassroots Network and Community Roadmap for Interconnected Autonomous Science Laboratories for Accelerated Discovery](https://arxiv.org/abs/2506.17510)
*Rafael Ferreira da Silva,Milad Abolhasani,Dionysios A. Antonopoulos,Laura Biven,Ryan Coffee,Ian T. Foster,Leslie Hamilton,Shantenu Jha,Theresa Mayer,Benjamin Mintz,Robert G. Moore,Salahudin Nimer,Noah Paulson,Woong Shin,Frederic Suter,Mitra Taheri,Michela Taufer,Newell R. Washburn*

Main category: cs.CY

TL;DR: The paper introduces AISLE, a network that integrates autonomous labs across institutions to accelerate scientific discovery.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the fragmentation of existing autonomous labs and enable collaborative, accelerated scientific discovery.

Method: Five dimensions are tackled: cross-institutional equipment orchestration, FAIR-compliant intelligent data handling, AI-driven orchestration, interoperable communication among agents, and enhanced AI/ML-focused scientific education.

Result: AISLE unifies isolated autonomous labs to democratize technology access and expand research capabilities into unexplored areas.

Conclusion: AISLE offers a paradigm shift in scientific discovery by fostering collaborative autonomous science, with potential impacts in energy, materials, and health research.

Abstract: Scientific discovery is being revolutionized by AI and autonomous systems,
yet current autonomous laboratories remain isolated islands unable to
collaborate across institutions. We present the Autonomous Interconnected
Science Lab Ecosystem (AISLE), a grassroots network transforming fragmented
capabilities into a unified system that shorten the path from ideation to
innovation to impact and accelerates discovery from decades to months. AISLE
addresses five critical dimensions: (1) cross-institutional equipment
orchestration, (2) intelligent data management with FAIR compliance, (3)
AI-agent driven orchestration grounded in scientific principles, (4)
interoperable agent communication interfaces, and (5) AI/ML-integrated
scientific education. By connecting autonomous agents across institutional
boundaries, autonomous science can unlock research spaces inaccessible to
traditional approaches while democratizing cutting-edge technologies. This
paradigm shift toward collaborative autonomous science promises breakthroughs
in sustainable energy, materials development, and public health.

</details>


### [522] [Public Perceptions of Autonomous Vehicles: A Survey of Pedestrians and Cyclists in Pittsburgh](https://arxiv.org/abs/2506.17513)
*Rudra Y. Bedekar*

Main category: cs.CY

TL;DR: Explores pedestrian and bicyclist perceptions of autonomous vehicles, emphasizing demographic differences, infrastructure issues, and the need for communication.


<details>
  <summary>Details</summary>
Motivation: Understand how pedestrians and bicyclists perceive autonomous vehicle technology, particularly in terms of safety, trust, and readiness.

Method: Survey data collected from over 1200 respondents in Pittsburgh to analyze demographics, AV interactions, infrastructure readiness, safety perceptions, and trust.

Result: Identified demographic divides, gaps in infrastructure, and the importance of communication and education for AV adoption.

Conclusion: The adoption of AV technology needs to address demographic disparities, improve infrastructure, and foster communication and education with the public.

Abstract: This study investigates how autonomous vehicle(AV) technology is perceived by
pedestrians and bicyclists in Pittsburgh. Using survey data from over 1200
respondents, the research explores the interplay between demographics, AV
interactions, infrastructural readiness, safety perceptions, and trust.
Findings highlight demographic divides, infrastructure gaps, and the crucial
role of communication and education in AV adoption.

</details>


### [523] [Distinguishing Predictive and Generative AI in Regulation](https://arxiv.org/abs/2506.17347)
*Jennifer Wang,Andrew Selbst,Solon Barocas,Suresh Venkatasubramanian*

Main category: cs.CY

TL;DR: This paper discusses the need for updated policy approaches to address the unique characteristics of generative AI, highlighting its differences from predictive AI and offering recommendations.


<details>
  <summary>Details</summary>
Motivation: The motivation behind the study is the recognition that current AI regulatory frameworks, based on predictive AI, may not adequately address the unique challenges posed by generative AI.

Method: The authors analyze four distinct aspects of generative AI that challenge existing policies and provide a set of three recommendations for policymakers.

Result: Key distinctions of generative AI—like its generality, adaptability, evaluation challenges, legal concerns, and value chain structure—are identified as requiring different regulatory strategies.

Conclusion: Policymakers must reconsider existing AI policies and develop new regulations specifically tailored to mitigate the unique risks and challenges of generative AI.

Abstract: Over the past decade, policymakers have developed a set of regulatory tools
to ensure AI development aligns with key societal goals. Many of these tools
were initially developed in response to concerns with predictive AI and
therefore encode certain assumptions about the nature of AI systems and the
utility of certain regulatory approaches. With the advent of generative AI,
however, some of these assumptions no longer hold, even as policymakers attempt
to maintain a single regulatory target that covers both types of AI.
  In this paper, we identify four distinct aspects of generative AI that call
for meaningfully different policy responses. These are the generality and
adaptability of generative AI that make it a poor regulatory target, the
difficulty of designing effective evaluations, new legal concerns that change
the ecosystem of stakeholders and sources of expertise, and the distributed
structure of the generative AI value chain.
  In light of these distinctions, policymakers will need to evaluate where the
past decade of policy work remains relevant and where new policies, designed to
address the unique risks posed by generative AI, are necessary. We outline
three recommendations for policymakers to more effectively identify regulatory
targets and leverage constraints across the broader ecosystem to govern
generative AI.

</details>


### [524] [The Democratic Paradox in Large Language Models' Underestimation of Press Freedom](https://arxiv.org/abs/2506.18045)
*I. Loaiza,R. Vestrelli,A. Fronzetti Colladon,R. Rigobon*

Main category: cs.CY

TL;DR: The paper reveals systematic bias in six LLMs' evaluation of global press freedom, highlighting errors like underestimation of freedom and positive home-country bias.


<details>
  <summary>Details</summary>
Motivation: LLMs are becoming major mediators of information access, making their alignment critical for shaping public understanding of global democratic institutions.

Method: The authors compared press freedom evaluations by six LLMs to the World Press Freedom Index (WPFI) data for 180 countries.

Result: Key findings include consistent press freedom underestimation, differential misalignment where free countries are disproportionately underestimated, and positive home-country bias in five LLMs.

Conclusion: Accurate modeling of global human and civic rights is necessary for LLMs to responsibly handle their rising influence as cultural tools and search engines.

Abstract: As Large Language Models (LLMs) increasingly mediate global information
access for millions of users worldwide, their alignment and biases have the
potential to shape public understanding and trust in fundamental democratic
institutions, such as press freedom. In this study, we uncover three systematic
distortions in the way six popular LLMs evaluate press freedom in 180 countries
compared to expert assessments of the World Press Freedom Index (WPFI). The six
LLMs exhibit a negative misalignment, consistently underestimating press
freedom, with individual models rating between 71% to 93% of countries as less
free. We also identify a paradoxical pattern we term differential misalignment:
LLMs disproportionately underestimate press freedom in countries where it is
strongest. Additionally, five of the six LLMs exhibit positive home bias,
rating their home countries' press freedoms more favorably than would be
expected given their negative misalignment with the human benchmark. In some
cases, LLMs rate their home countries between 7% to 260% more positively than
expected. If LLMs are set to become the next search engines and some of the
most important cultural tools of our time, they must ensure accurate
representations of the state of our human and civic rights globally.

</details>


### [525] [Automatic Large Language Models Creation of Interactive Learning Lessons](https://arxiv.org/abs/2506.17356)
*Jionghao Lin,Jiarui Rao,Yiyang Zhao,Yuting Wang,Ashish Gurung,Amanda Barany,Jaclyn Ocumpaugh,Ryan S. Baker,Kenneth R. Koedinger*

Main category: cs.CY

TL;DR: Researchers developed a system utilizing GPT-4 for creating interactive tutor training lessons tailored to three educational topics, employing a task decomposition strategy for better lesson quality.


<details>
  <summary>Details</summary>
Motivation: To train novice human tutors in middle school mathematics using AI-generated, scenario-based lessons, addressing the challenge of scalable and efficient tutor training.

Method: Utilized Retrieval-Augmented Generation with GPT-4 and a task decomposition prompting strategy, breaking lesson generation into sub-tasks for structured tutor education content.

Result: Generated high-quality lessons evaluated by human reviewers, showing improvements with the task decomposition approach compared to single-step generation.

Conclusion: Hybrid human-AI methods hold promise for efficient and effective tutor training, though further refinements are needed to address limitations like generic feedback and unclear instructional sections.

Abstract: We explore the automatic generation of interactive, scenario-based lessons
designed to train novice human tutors who teach middle school mathematics
online. Employing prompt engineering through a Retrieval-Augmented Generation
approach with GPT-4o, we developed a system capable of creating structured
tutor training lessons. Our study generated lessons in English for three key
topics: Encouraging Students' Independence, Encouraging Help-Seeking Behavior,
and Turning on Cameras, using a task decomposition prompting strategy that
breaks lesson generation into sub-tasks. The generated lessons were evaluated
by two human evaluators, who provided both quantitative and qualitative
evaluations using a comprehensive rubric informed by lesson design research.
Results demonstrate that the task decomposition strategy led to higher-rated
lessons compared to single-step generation. Human evaluators identified several
strengths in the LLM-generated lessons, including well-structured content and
time-saving potential, while also noting limitations such as generic feedback
and a lack of clarity in some instructional sections. These findings underscore
the potential of hybrid human-AI approaches for generating effective lessons in
tutor training.

</details>


### [526] [A Large-Scale Real-World Evaluation of LLM-Based Virtual Teaching Assistant](https://arxiv.org/abs/2506.17363)
*Sunjun Kweon,Sooyohn Nam,Hyunseung Lim,Hwajung Hong,Edward Choi*

Main category: cs.CY

TL;DR: This study evaluates the use of Virtual Teaching Assistants (VTAs) powered by Large Language Models (LLMs) in a graduate AI programming course, focusing on their performance, student perceptions, and interaction patterns.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the scarcity of empirical studies on the effectiveness and acceptance of LLM-driven VTAs in real-world educational setups.

Method: The authors deployed an LLM-based VTA in a graduate course, conducted three rounds of surveys for student feedback, and analyzed thousands of interaction pairs to compare VTA and human instructor roles.

Result: The study identified common student engagement patterns, evaluated student perceptions over time, and highlighted key challenges for broader adoption of VTAs in classrooms.

Conclusion: VTAs powered by LLMs show promise in enhancing education, but there are significant challenges to address before scaling their adoption. The system's code is made publicly available for further development.

Abstract: Virtual Teaching Assistants (VTAs) powered by Large Language Models (LLMs)
have the potential to enhance student learning by providing instant feedback
and facilitating multi-turn interactions. However, empirical studies on their
effectiveness and acceptance in real-world classrooms are limited, leaving
their practical impact uncertain. In this study, we develop an LLM-based VTA
and deploy it in an introductory AI programming course with 477 graduate
students. To assess how student perceptions of the VTA's performance evolve
over time, we conduct three rounds of comprehensive surveys at different stages
of the course. Additionally, we analyze 3,869 student--VTA interaction pairs to
identify common question types and engagement patterns. We then compare these
interactions with traditional student--human instructor interactions to
evaluate the VTA's role in the learning process. Through a large-scale
empirical study and interaction analysis, we assess the feasibility of
deploying VTAs in real-world classrooms and identify key challenges for broader
adoption. Finally, we release the source code of our VTA system, fostering
future advancements in AI-driven education:
\texttt{https://github.com/sean0042/VTA}.

</details>


### [527] [AI-based Multimodal Biometrics for Detecting Smartphone Distractions: Application to Online Learning](https://arxiv.org/abs/2506.17364)
*Alvaro Becerra,Roberto Daza,Ruth Cobos,Aythami Morales,Mutlu Cukurova,Julian Fierrez*

Main category: cs.CY

TL;DR: This paper explores using multimodal biometrics to detect smartphone distractions during computer-based online learning, achieving 91% accuracy using an integrated model.


<details>
  <summary>Details</summary>
Motivation: Online learners often struggle with engagement due to internal, system-related, and contextual factors like phone distractions. Identifying these distractions is critical to improve learning outcomes.

Method: The study employs an AI-based model using physiological signals and head pose data to detect smartphone usage, comparing single signals vs. multimodal integration.

Result: Single biometric signals showed limited accuracy, head pose data achieved 87% detection accuracy, and multimodal integration improved accuracy to 91%.

Conclusion: Multimodal biometrics offer promising accuracy for detecting distractions, but practical deployment in real-time online learning environments requires addressing limitations.

Abstract: This work investigates the use of multimodal biometrics to detect
distractions caused by smartphone use during tasks that require sustained
attention, with a focus on computer-based online learning. Although the methods
are applicable to various domains, such as autonomous driving, we concentrate
on the challenges learners face in maintaining engagement amid internal (e.g.,
motivation), system-related (e.g., course design) and contextual (e.g.,
smartphone use) factors. Traditional learning platforms often lack detailed
behavioral data, but Multimodal Learning Analytics (MMLA) and biosensors
provide new insights into learner attention. We propose an AI-based approach
that leverages physiological signals and head pose data to detect phone use.
Our results show that single biometric signals, such as brain waves or heart
rate, offer limited accuracy, while head pose alone achieves 87%. A multimodal
model combining all signals reaches 91% accuracy, highlighting the benefits of
integration. We conclude by discussing the implications and limitations of
deploying these models for real-time support in online learning environments.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [528] [Greedy Selection under Independent Increments: A Toy Model Analysis](https://arxiv.org/abs/2506.17941)
*Huitao Yang*

Main category: math.PR

TL;DR: The paper shows that a greedy algorithm is the optimal strategy for selecting the best process in an iterative elimination setting based on certain assumptions.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the need to understand the effectiveness of greedy selection strategies in multi-stage elimination problems, which are relevant to high-dimensional applications.

Method: The authors analyze a model involving N independent discrete-time stochastic processes and prove that greedy selection is optimal under strong independence assumptions.

Result: The result demonstrates that the greedy selection strategy consistently leads to choosing the final maximum-value process.

Conclusion: Greedy heuristics are justified as an optimal strategy in this selection problem, offering insights into iterative elimination strategies in more complex scenarios.

Abstract: We study an iterative selection problem over N i.i.d. discrete-time
stochastic processes with independent increments. At each stage, a fixed number
of processes are retained based on their observed values. Under this simple
model, we prove that the optimal strategy for selecting the final maximum-value
process is to apply greedy selection at each stage. While the result relies on
strong independence assumptions, it offers a clean justification for greedy
heuristics in multi-stage elimination settings and may serve as a toy example
for understanding related algorithms in high-dimensional applications.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [529] [Projected Normal Distribution: Moment Approximations and Generalizations](https://arxiv.org/abs/2506.17461)
*Daniel Herrera-Esposito,Johannes Burge*

Main category: stat.ME

TL;DR: This paper derives analytic approximations for the moments of the projected normal distribution and its generalizations to facilitate their applications in data analysis, including neuroscience.


<details>
  <summary>Details</summary>
Motivation: The projected normal distribution, though useful for modeling data on the unit sphere, lacks closed-form moment formulas, which restricts its applicability in various fields.

Method: The authors use Taylor expansions and results from quadratic forms of Gaussian random variables to derive approximations for the first and second moments. They also generalize the distribution by modifying the normalization denominator and derive corresponding approximations and density functions.

Result: They demonstrate that the derived moment approximations yield accurate results across a wide range of dimensions and distribution parameters. The methods are shown to be effective for fitting data via moment matching.

Conclusion: The study provides practical tools for applying the projected normal distribution and its generalizations, improving their utility in neuroscience and other fields requiring spherical data modeling.

Abstract: The projected normal distribution, also known as the angular Gaussian
distribution, is obtained by dividing a multivariate normal random variable
$\mathbf{x}$ by its norm $\sqrt{\mathbf{x}^T \mathbf{x}}$. The resulting random
variable follows a distribution on the unit sphere. No closed-form formulas for
the moments of the projected normal distribution are known, which can limit its
use in some applications. In this work, we derive analytic approximations to
the first and second moments of the projected normal distribution using Taylor
expansions and using results from the theory of quadratic forms of Gaussian
random variables. Then, motivated by applications in systems neuroscience, we
present generalizations of the projected normal distribution that divide the
variable $\mathbf{x}$ by a denominator of the form $\sqrt{\mathbf{x}^T
\mathbf{B} \mathbf{x} + c}$, where $\mathbf{B}$ is a symmetric positive
definite matrix and $c$ is a non-negative number. We derive moment
approximations as well as the density function for these other projected
distributions. We show that the moments approximations are accurate for a wide
range of dimensionalities and distribution parameters. Furthermore, we show
that the moments approximations can be used to fit these distributions to data
through moment matching. These moment matching methods should be useful for
analyzing data across a range of applications where the projected normal
distribution is used, and for applying the projected normal distribution and
its generalizations to model data in neuroscience.

</details>


### [530] [Bayesian Inference for Left-Truncated Log-Logistic Distributions for Time-to-event Data Analysis](https://arxiv.org/abs/2506.17852)
*Fahad Mostafa,Md Rejuan Haque,Md Mostafijur Rahman,Farzana Nasrin*

Main category: stat.ME

TL;DR: The paper proposes a Bayesian approach to estimate parameters of the left-truncated log-logistic (LTLL) distribution, using Markov Chain Monte Carlo methods, and demonstrates its robustness in handling irregular likelihoods from truncated data.


<details>
  <summary>Details</summary>
Motivation: To enhance parameter estimation in statistical modeling, particularly for cases involving small or truncated samples, as is often required in time-to-event data like precipitation and cancer survival analysis.

Method: The authors derive a likelihood function for the LTLL distribution and utilize Bayesian inference with Markov Chain Monte Carlo sampling, specifically employing the Metropolis-Hastings algorithm, for posterior parameter estimation.

Result: Simulation studies and real-world applications show that the Bayesian approach provides stable and reliable parameter estimates even under irregular likelihood surfaces caused by left truncation.

Conclusion: Bayesian estimation provides a robust framework for parameter uncertainty and reliability in time-to-event data analysis, outperforming other approaches in modeling truncated distributions.

Abstract: Parameter estimation is a foundational step in statistical modeling, enabling
us to extract knowledge from data and apply it effectively. Bayesian estimation
of parameters incorporates prior beliefs with observed data to infer
distribution parameters probabilistically and robustly. Moreover, it provides
full posterior distributions, allowing uncertainty quantification and
regularization, especially useful in small or truncated samples. Utilizing the
left-truncated log-logistic (LTLL) distribution is particularly well-suited for
modeling time-to-event data where observations are subject to a known lower
bound such as precipitation data and cancer survival times. In this paper, we
propose a Bayesian approach for estimating the parameters of the LTLL
distribution with a fixed truncation point \( x_L > 0 \). Given a random
variable \( X \sim LL(\alpha, \beta; x_L) \), where \( \alpha > 0 \) is the
scale parameter and \( \beta > 0 \) is the shape parameter, the likelihood
function is derived based on a truncated sample \( X_1, X_2, \dots, X_N \) with
\( X_i > x_L \). We assume independent prior distributions for the parameters,
and the posterior inference is conducted via Markov Chain Monte Carlo sampling,
specifically using the Metropolis-Hastings algorithm to obtain posterior
estimates \( \hat{\alpha} \) and \( \hat{\beta} \). Through simulation studies
and real-world applications, we demonstrate that Bayesian estimation provides
more stable and reliable parameter estimates, particularly when the likelihood
surface is irregular due to left truncation. The results highlight the
advantages of Bayesian inference outperform the estimation of parameter
uncertainty in truncated distributions for time to event data analysis.

</details>


### [531] [GRASP: Grouped Regression with Adaptive Shrinkage Priors](https://arxiv.org/abs/2506.18092)
*Shu Yu Tew,Daniel F. Schmidt,Mario Boley*

Main category: stat.ME

TL;DR: GRASP is a Bayesian framework designed for grouped regression using the NBP prior, which offers adaptable sparsity control and eliminates the need for hierarchical complexity.


<details>
  <summary>Details</summary>
Motivation: To improve regression modeling with grouped predictors using an adaptive sparsity framework while reducing hierarchical complexity.

Method: The method involves assigning the NBP prior to local and group shrinkage parameters, quantifying correlations in grouped shrinkage behavior, and employing a Metropolis-Hastings sampler for hyperparameter estimation.

Result: Empirical tests on simulated and real-world data showed GRASP's robustness and versatility in grouped regression problems with different sparsity and signal-to-noise ratios.

Conclusion: GRASP is a flexible and efficient framework for grouped regression, providing adaptive sparsity and simplifying prior constructions.

Abstract: We introduce GRASP, a simple Bayesian framework for regression with grouped
predictors, built on the normal beta prime (NBP) prior. The NBP prior is an
adaptive generalization of the horseshoe prior with tunable hyperparameters
that control tail behavior, enabling a flexible range of sparsity, from strong
shrinkage to ridge-like regularization. Unlike prior work that introduced the
group inverse-gamma gamma (GIGG) prior by decomposing the NBP prior into
structured hierarchies, we show that directly controlling the tails is
sufficient without requiring complex hierarchical constructions. Extending the
non-tail adaptive grouped half-Cauchy hierarchy of Xu et al., GRASP assigns the
NBP prior to both local and group shrinkage parameters allowing adaptive
sparsity within and across groups. A key contribution of this work is a novel
framework to explicitly quantify correlations among shrinkage parameters within
a group, providing deeper insights into grouped shrinkage behavior. We also
introduce an efficient Metropolis-Hastings sampler for hyperparameter
estimation. Empirical results on simulated and real-world data demonstrate the
robustness and versatility of GRASP across grouped regression problems with
varying sparsity and signal-to-noise ratios.

</details>


### [532] [PCA-Guided Quantile Sampling: Preserving Data Structure in Large-Scale Subsampling](https://arxiv.org/abs/2506.18249)
*Foo Hui-Mean,Yuan-chin Ivan Chang*

Main category: stat.ME

TL;DR: The paper introduces PCA-guided Quantile Sampling (PCA QS), a method to sample datasets while preserving both their statistical and geometric structures.


<details>
  <summary>Details</summary>
Motivation: The goal is to address the limitations of conventional PCA, which compromises interpretability when reducing dimensionality, by developing a sampling method that retains original feature spaces without distorting data semantics.

Method: PCA QS employs leading principal components to guide quantile-based stratification, ensuring representative sampling. The authors provide theoretical guarantees on distributional fidelity and practical guidance for parameter selection.

Result: Empirical studies on synthetic and real-world datasets show that PCA QS maintains better structural fidelity and improves downstream model performance compared to random sampling.

Conclusion: PCA QS is demonstrated as a scalable and interpretable solution for efficient data summarization, supported by theoretical foundations and practical benefits for machine learning workflows.

Abstract: We introduce Principal Component Analysis guided Quantile Sampling (PCA QS),
a novel sampling framework designed to preserve both the statistical and
geometric structure of large scale datasets. Unlike conventional PCA, which
reduces dimensionality at the cost of interpretability, PCA QS retains the
original feature space while using leading principal components solely to guide
a quantile based stratification scheme. This principled design ensures that
sampling remains representative without distorting the underlying data
semantics. We establish rigorous theoretical guarantees, deriving convergence
rates for empirical quantiles, Kullback Leibler divergence, and Wasserstein
distance, thus quantifying the distributional fidelity of PCA QS samples.
Practical guidelines for selecting the number of principal components, quantile
bins, and sampling rates are provided based on these results. Extensive
empirical studies on both synthetic and real-world datasets show that PCA QS
consistently outperforms simple random sampling, yielding better structure
preservation and improved downstream model performance. Together, these
contributions position PCA QS as a scalable, interpretable, and theoretically
grounded solution for efficient data summarization in modern machine learning
workflows.

</details>
