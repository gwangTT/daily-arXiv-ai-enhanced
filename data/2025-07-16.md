<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 36]
- [cs.AR](#cs.AR) [Total: 8]
- [cs.CL](#cs.CL) [Total: 47]
- [cs.CV](#cs.CV) [Total: 65]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.LG](#cs.LG) [Total: 83]
- [cs.NE](#cs.NE) [Total: 6]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 34]
- [cs.SE](#cs.SE) [Total: 20]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [stat.ML](#stat.ML) [Total: 8]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.DS](#cs.DS) [Total: 2]
- [cs.CR](#cs.CR) [Total: 9]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [q-bio.OT](#q-bio.OT) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.GT](#cs.GT) [Total: 3]
- [math.PR](#math.PR) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.GR](#cs.GR) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.IR](#cs.IR) [Total: 3]
- [math.OC](#math.OC) [Total: 3]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.SI](#cs.SI) [Total: 3]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [eess.IV](#eess.IV) [Total: 6]
- [gr-qc](#gr-qc) [Total: 1]
- [cs.CY](#cs.CY) [Total: 5]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.HC](#cs.HC) [Total: 3]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [SAMEP: A Secure Protocol for Persistent Context Sharing Across AI Agents](https://arxiv.org/abs/2507.10562)
*Hari Masoor*

Main category: cs.AI

TL;DR: SAMEP (Secure Agent Memory Exchange Protocol) is designed to enable AI agents to share memory securely, persistently, and effectively, improving collaboration and context preservation across sessions.


<details>
  <summary>Details</summary>
Motivation: The aim is to address critical limitations in current AI systems related to ephemeral memory and lack of context retention and collaborative capabilities.

Method: SAMEP utilizes a distributed memory repository, vector-based semantic search, cryptographic access controls (AES-256-GCM), and APIs compatible with existing agent communication protocols like MCP and A2A.

Result: SAMEP demonstrated a 73% reduction in redundant computations, 89% enhancement in context relevance scores, and ensured compliance with regulatory requirements across tested domains.

Conclusion: The framework enables a secure, persistent, and collaborative AI ecosystem that enhances efficiency and context-sharing while maintaining security and privacy requirements.

Abstract: Current AI agent architectures suffer from ephemeral memory limitations,
preventing effective collaboration and knowledge sharing across sessions and
agent boundaries. We introduce SAMEP (Secure Agent Memory Exchange Protocol), a
novel framework that enables persistent, secure, and semantically searchable
memory sharing among AI agents. Our protocol addresses three critical
challenges: (1) persistent context preservation across agent sessions, (2)
secure multi-agent collaboration with fine-grained access control, and (3)
efficient semantic discovery of relevant historical context. SAMEP implements a
distributed memory repository with vector-based semantic search, cryptographic
access controls (AES-256-GCM), and standardized APIs compatible with existing
agent communication protocols (MCP, A2A). We demonstrate SAMEP's effectiveness
across diverse domains including multi-agent software development, healthcare
AI with HIPAA compliance, and multi-modal processing pipelines. Experimental
results show 73% reduction in redundant computations, 89% improvement in
context relevance scores, and complete compliance with regulatory requirements
including audit trail generation. SAMEP enables a new paradigm of persistent,
collaborative AI agent ecosystems while maintaining security and privacy
guarantees.

</details>


### [2] [AI Mother Tongue: Self-Emergent Communication in MARL via Endogenous Symbol Systems](https://arxiv.org/abs/2507.10566)
*Hung Ming Liu*

Main category: cs.AI

TL;DR: This paper investigates whether emergent communication in decentralized MARL can evolve naturally without external inductive biases, using the AI Mother Tongue (AIM) framework based on VQ-VAE. It finds that agents can achieve semantic compression and convergence, leading to effective symbolic communication.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches in decentralized MARL rely on artificial inductive biases to solve the Joint Exploration Dilemma. The study aims to explore whether such biases are over-engineered and if natural emergent communication can arise intrinsically.

Method: The study employs the AI Mother Tongue (AIM) framework, leveraging a Vector Quantized Variational Autoencoder (VQ-VAE), to enable agents to develop endogenous symbol systems. It evaluates the framework's impact on semantic compression, Nash equilibrium-driven convergence, and interpretability.

Result: Agents using the AIM framework naturally demonstrate semantic compression and symbolic communication without external biases. Symbol usage follows a power-law distribution, validating three theoretical insights: Neural Communication Hypothesis, Tool-First Principle, and Semantic Interpretability Paradigm.

Conclusion: Inductive biases may be unnecessary for emergent communication in MARL. The AIM framework offers a more general and efficient approach, aligning with neuroscience findings and LLM research. Future enhancements with HQ-VAE could further develop its symbolic expressiveness.

Abstract: In Decentralized Multi-Agent Reinforcement Learning (MARL), the development
of Emergent Communication has long been constrained by the ``Joint Exploration
Dilemma'', leading agents to fall into a ``Communication Vacuum Equilibrium'' .
Traditional methods address this by introducing inductive biases to facilitate
communication emergence . This study fundamentally questions whether such
artificial inductive biases are, in fact, over-engineering. Through experiments
with the ``AI Mother Tongue'' (AIM) framework, based on a Vector Quantized
Variational Autoencoder (VQ-VAE), we demonstrate that when agents possess an
endogenous symbol system, their neural representations naturally exhibit
spontaneous semantic compression and Nash equilibrium-driven semantic
convergence, achieving effective symbolic communication without external
inductive biases. This aligns with recent neuroscience findings suggesting that
the human brain does not directly use human language for internal thought , and
resonates with research on ``soft thinking'' capabilities in Large Language
Models (LLMs) . Compared to traditional explicit communication methods, AIM
demonstrates stronger generality and efficiency. The interpretable analysis
toolkit developed in this study confirms that symbol usage exhibits a
significant power-law distribution, leading to three major theoretical
insights: the ``Neural Communication Hypothesis'', the ``Tool-First
Principle'', and the ``Semantic Interpretability Paradigm''. Future research
will explore the integration of Hierarchical Quantized Variational Autoencoders
(HQ-VAE) to enhance AIM's complex expressive capabilities and investigate the
potential for ``Reinforcement Learning (RL) Low-Level Pre-training''. This
discovery offers new avenues for bridging symbolism and connectionism.

</details>


### [3] [Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning](https://arxiv.org/abs/2507.10571)
*Konstantinos I. Roumeliotis,Ranjan Sapkota,Manoj Karkee,Nikolaos D. Tselikas*

Main category: cs.AI

TL;DR: This paper proposes a modular framework combining multimodal AI agents with reasoning and retrieval modules to improve trust and accuracy in zero-shot visual classification tasks, achieving 77.94% accuracy improvement.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the issue of trust and reliability in AI, particularly in zero-shot visual classification tasks where no fine-tuning is involved, to enable confident decision-making.

Method: A novel Agentic AI framework integrates generalist multimodal agents with a non-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG) module, tested across three configurations including zero-shot and fine-tuned settings.

Result: The framework achieved a 77.94% improvement in zero-shot accuracy and 85.63% overall accuracy using trust-aware orchestration mechanisms.

Conclusion: The proposed system successfully separates perception and reasoning, is trust-calibrated, and is extensible to broader diagnostics and critical domains, with a focus on scalability and interpretability.

Abstract: Modern Artificial Intelligence (AI) increasingly relies on multi-agent
architectures that blend visual and language understanding. Yet, a pressing
challenge remains: How can we trust these agents especially in zero-shot
settings with no fine-tuning? We introduce a novel modular Agentic AI visual
classification framework that integrates generalist multimodal agents with a
non-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG)
module. Applied to apple leaf disease diagnosis, we benchmark three
configurations: (I) zero-shot with confidence-based orchestration, (II)
fine-tuned agents with improved performance, and (III) trust-calibrated
orchestration enhanced by CLIP-based image retrieval and re-evaluation loops.
Using confidence calibration metrics (ECE, OCR, CCC), the orchestrator
modulates trust across agents. Our results demonstrate a 77.94\% accuracy
improvement in the zero-shot setting using trust-aware orchestration and RAG,
achieving 85.63\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL
displayed overconfidence. Furthermore, image-RAG grounded predictions with
visually similar cases, enabling correction of agent overconfidence via
iterative re-evaluation. The proposed system separates perception (vision
agents) from meta-reasoning (orchestrator), enabling scalable and interpretable
multi-agent AI. This blueprint is extensible to diagnostics, biology, and other
trust-critical domains. All models, prompts, results, and system components
including the complete software source code are openly released to support
reproducibility, transparency, and community benchmarking at Github:
https://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust

</details>


### [4] [Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning](https://arxiv.org/abs/2507.10624)
*Zheng Zhang*

Main category: cs.AI

TL;DR: LLMs excel in fluency but fail in symbolic reasoning due to architectural gaps between principle articulation and application, termed 'computational split-brain syndrome.'


<details>
  <summary>Details</summary>
Motivation: To investigate why LLMs struggle with logical and symbolic tasks, despite their surface-level fluency.

Method: Conducting controlled experiments and architectural analysis to examine the discrepancy between comprehension and competence in LLMs.

Result: The study identifies a geometric and functional dissociation between instruction and execution pathways in LLMs, leading to inconsistencies in performance across domains.

Conclusion: Current LLMs are optimized for pattern completion but lack structures necessary for compositional reasoning. Addressing these limitations may improve their capabilities in principled reasoning and introspection.

Abstract: Large Language Models (LLMs) display striking surface fluency yet
systematically fail at tasks requiring symbolic reasoning, arithmetic accuracy,
and logical consistency. This paper offers a structural diagnosis of such
failures, revealing a persistent gap between \textit{comprehension} and
\textit{competence}. Through controlled experiments and architectural analysis,
we demonstrate that LLMs often articulate correct principles without reliably
applying them--a failure rooted not in knowledge access, but in computational
execution. We term this phenomenon the computational \textit{split-brain
syndrome}, where instruction and action pathways are geometrically and
functionally dissociated. This core limitation recurs across domains, from
mathematical operations to relational inferences, and explains why model
behavior remains brittle even under idealized prompting. We argue that LLMs
function as powerful pattern completion engines, but lack the architectural
scaffolding for principled, compositional reasoning. Our findings delineate the
boundary of current LLM capabilities and motivate future models with
metacognitive control, principle lifting, and structurally grounded execution.
This diagnosis also clarifies why mechanistic interpretability findings may
reflect training-specific pattern coordination rather than universal
computational principles, and why the geometric separation between instruction
and execution pathways suggests limitations in neural introspection and
mechanistic analysis.

</details>


### [5] [Enhancing the Capabilities of Large Language Models for API calls through Knowledge Graphs](https://arxiv.org/abs/2507.10630)
*Ye Yang,Xue Xiao,Ping Yin,Taotao Xie*

Main category: cs.AI

TL;DR: KG2data enhances API-call-based data analysis for meteorology, outperforming existing models like RAG2data and chat2data by integrating knowledge graphs and LLMs.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of utilizing large language models (LLMs) effectively in knowledge-intensive domains such as meteorology, especially in handling domain-specific knowledge and complex queries.

Method: The study introduces KG2data, a system based on knowledge graphs, LLMs, and ReAct agents, using virtual APIs to evaluate API call effectiveness via three metrics: name recognition failure, hallucination failure, and call correctness.

Result: KG2data outperformed baseline systems, achieving superior scores for name recognition failure (1.43%), hallucination failure (0%), and call correctness (88.57%).

Conclusion: KG2data successfully enhances domain-specific question answering and data analysis while mitigating fine-tuning costs and improving adaptability to evolving domain knowledge and APIs.

Abstract: API calls by large language models (LLMs) offer a cutting-edge approach for
data analysis. However, their ability to effectively utilize tools via API
calls remains underexplored in knowledge-intensive domains like meteorology.
This paper introduces KG2data, a system that integrates knowledge graphs, LLMs,
ReAct agents, and tool-use technologies to enable intelligent data acquisition
and query handling in the meteorological field. Using a virtual API, we
evaluate API call accuracy across three metrics: name recognition failure,
hallucination failure, and call correctness. KG2data achieves superior
performance (1.43%, 0%, 88.57%) compared to RAG2data (16%, 10%, 72.14%) and
chat2data (7.14%, 8.57%, 71.43%). KG2data differs from typical LLM-based
systems by addressing their limited access to domain-specific knowledge, which
hampers performance on complex or terminology-rich queries. By using a
knowledge graph as persistent memory, our system enhances content retrieval,
complex query handling, domain-specific reasoning, semantic relationship
resolution, and heterogeneous data integration. It also mitigates the high cost
of fine-tuning LLMs, making the system more adaptable to evolving domain
knowledge and API structures. In summary, KG2data provides a novel solution for
intelligent, knowledge-based question answering and data analysis in domains
with high knowledge demands.

</details>


### [6] [From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents](https://arxiv.org/abs/2507.10644)
*Tatiana Petrova,Aleksandr Puzikov,Boris Bliznukov,Radu State*

Main category: cs.AI

TL;DR: The paper investigates the evolution and future challenges of the Web of Agents (WoA), highlighting a paradigm shift in intelligence location and proposing a unified analytical framework.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the fragmented understanding across agent systems research while bridging the historical gap between Multi-Agent Systems (MAS), the Semantic Web, and modern LLM-powered frameworks.

Method: The authors introduce a four-axis taxonomy (semantic foundation, communication paradigm, locus of intelligence, discovery mechanism) to unify agent architecture analysis and trace intellectual lineage.

Result: The study identifies a paradigm shift in intelligence—from external data or platforms to being embedded in agents' core models via LLMs—and proposes a research agenda for socio-technical challenges in WoA.

Conclusion: The authors emphasize that although updated protocols are necessary, they are insufficient alone; broader socio-technical issues need addressing to create a secure, open, and adaptable WoA ecosystem.

Abstract: The concept of the Web of Agents (WoA), which transforms the static,
document-centric Web into an environment of autonomous agents acting on users'
behalf, has attracted growing interest as large language models (LLMs) become
more capable. However, research in this area is still fragmented across
different communities. Contemporary surveys catalog the latest LLM-powered
frameworks, while the rich histories of Multi-Agent Systems (MAS) and the
Semantic Web are often treated as separate, legacy domains. This fragmentation
obscures the intellectual lineage of modern systems and hinders a holistic
understanding of the field's trajectory. We present the first comprehensive
evolutionary overview of the WoA. We show that modern protocols like A2A and
the MCP, are direct evolutionary responses to the well-documented limitations
of earlier standards like FIPA standards and OWL-based semantic agents. To
systematize this analysis, we introduce a four-axis taxonomy (semantic
foundation, communication paradigm, locus of intelligence, discovery
mechanism). This framework provides a unified analytical lens for comparing
agent architectures across all generations, revealing a clear line of descent
where others have seen a disconnect. Our analysis identifies a paradigm shift
in the 'locus of intelligence': from being encoded in external data (Semantic
Web) or the platform (MAS) to being embedded within the agent's core model
(LLM). This shift is foundational to modern Agentic AI, enabling the scalable
and adaptive systems the WoA has long envisioned. We conclude that while new
protocols are essential, they are insufficient for building a robust, open,
trustworthy ecosystem. Finally, we argue that the next research frontier lies
in solving persistent socio-technical challenges, and we map out a new agenda
focused on decentralized identity, economic models, security, and governance
for the emerging WoA.

</details>


### [7] [Parsing Musical Structure to Enable Meaningful Variations](https://arxiv.org/abs/2507.10740)
*Maziar Kanani,Sean O Leary,James McDermott*

Main category: cs.AI

TL;DR: The study introduces a rule-based method to create variations of musical tunes by mutating grammars derived using the Sequitur algorithm and analyzing the effects of iterative mutations based on Irish traditional tunes.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this study is to explore a systematic and rule-based method to generate new tunes related to existing ones by leveraging grammar-based representations, as opposed to directly altering musical notes.

Method: The paper uses the Sequitur algorithm to derive grammars from musical tunes, performs one of 19 mutation types on those grammars, generates new tunes by expanding the mutated grammars, and evaluates the mutations using edit distances, structural complexity, tune length, and their musical aspects.

Result: The study demonstrates how tunes transform over iterative mutations and analyzes the impact of each type of mutation based on metrics like edit distance, complexity, and length.

Conclusion: The method effectively generates new, related musical sequences from existing tunes, offering insights into systematic musical variation, but it focuses on pitch sequences, excluding other musical elements.

Abstract: This paper presents a novel rule-based approach for generating music by
varying existing tunes. We parse each tune to find the Pathway Assembly (PA) [
1], that is a structure representing all repetitions in the tune. The Sequitur
algorithm [2 ] is used for this. The result is a grammar. We then carry out
mutation on the grammar, rather than on a tune directly. There are potentially
19 types of mutations such as adding, removing, swapping or reversing parts of
the grammar that can be applied to the grammars. The system employs one of the
mutations randomly in this step to automatically manipulate the grammar.
Following the mutation, we need to expand the grammar which returns a new tune.
The output after 1 or more mutations will be a new tune related to the original
tune. Our study examines how tunes change gradually over the course of multiple
mutations. Edit distances, structural complexity and length of the tunes are
used to show how a tune is changed after multiple mutations. In addition, the
size of effect of each mutation type is analyzed. As a final point, we review
the musical aspect of the output tunes. It should be noted that the study only
focused on generating new pitch sequences. The study is based on an Irish
traditional tune dataset and a list of integers has been used to represent each
tune's pitch values.

</details>


### [8] [AI and the Net-Zero Journey: Energy Demand, Emissions, and the Potential for Transition](https://arxiv.org/abs/2507.10750)
*Pandu Devarakota,Nicolas Tsesmetzis,Faruk O. Alpak,Apurva Gala,Detlef Hohl*

Main category: cs.AI

TL;DR: This paper explores energy consumption and GHG emissions related to AI-driven data centers, analyzing both near-term and long-term impacts on CO2 emissions.


<details>
  <summary>Details</summary>
Motivation: Understand AI's impact on energy consumption and CO2 emissions, and assess its potential in automating and optimizing energy-related processes.

Method: Examine scenarios of data center energy use and CO2 emissions up to 2035 and assess AI's role in energy optimization and disruption.

Result: Near-term: AI increases energy consumption and CO2 emissions; Long-term: AI optimizes processes, possibly reducing overall CO2 emissions.

Conclusion: AI may initially increase CO2 emissions but holds significant long-term potential for reducing carbon footprints and aiding climate mitigation.

Abstract: Thanks to the availability of massive amounts of data, computing resources,
and advanced algorithms, AI has entered nearly every sector. This has sparked
significant investment and interest, particularly in building data centers with
the necessary hardware and software to develop and operate AI models and
AI-based workflows. In this technical review article, we present energy
consumption scenarios of data centers and impact on GHG emissions, considering
both near-term projections (up to 2030) and long-term outlook (2035 and
beyond). We address the quintessential question of whether AI will have a net
positive, neutral, or negative impact on CO2 emissions by 2035. Additionally,
we discuss AI's potential to automate, create efficient and disruptive
workflows across various fields related to energy production, supply and
consumption. In the near-term scenario, the growing demand for AI will likely
strain computing resources, lead to increase in electricity consumption and
therefore associated CO2 emissions. This is due to the power-hungry nature of
big data centers and the requirements for training and running of large and
complex AI models, as well as the penetration of AI assistant search and
applications for public use. However, the long-term outlook could be more
promising. AI has the potential to be a game-changer in CO2 reduction. Its
ability to further automate and optimize processes across industries, from
energy production to logistics, could significantly decrease our carbon
footprint. This positive impact is anticipated to outweigh the initial
emissions bump, creating value for businesses and society in areas where
traditional solutions have fallen short. In essence, AI might cause some
initial growing pains for the environment, but it has the potential to support
climate mitigation efforts.

</details>


### [9] [IoT Malware Network Traffic Detection using Deep Learning and GraphSAGE Models](https://arxiv.org/abs/2507.10758)
*Nikesh Prajapati,Bimal Karki,Saroj Gopali,Akbar Siami Namin*

Main category: cs.AI

TL;DR: The paper explores deep learning models for detecting IoT malicious attacks, finding BERT achieves the highest performance with 99.94% accuracy.


<details>
  <summary>Details</summary>
Motivation: The study seeks to effectively and accurately detect malicious traffic in IoT systems, leveraging deep learning's ability to analyze temporal and diverse data patterns.

Method: The paper evaluates GraphSAGE, BERT, TCN, Multi-Head Attention with BI-LSTM variants, and standard LSTM models to detect sequential and temporal IoT traffic patterns.

Result: BERT demonstrated the best performance with 99.94% accuracy and high precision, recall, F1, and AUC-ROC scores. Other models varied in performance and computational efficiency.

Conclusion: BERT is an extremely effective model for malicious IoT detection due to its temporal dependency capture abilities, while other models offer varying trade-offs between detection accuracy and computational efficiency.

Abstract: This paper intends to detect IoT malicious attacks through deep learning
models and demonstrates a comprehensive evaluation of the deep learning and
graph-based models regarding malicious network traffic detection. The models
particularly are based on GraphSAGE, Bidirectional encoder representations from
transformers (BERT), Temporal Convolutional Network (TCN) as well as Multi-Head
Attention, together with Bidirectional Long Short-Term Memory (BI-LSTM)
Multi-Head Attention and BI-LSTM and LSTM models. The chosen models
demonstrated great performance to model temporal patterns and detect feature
significance. The observed performance are mainly due to the fact that IoT
system traffic patterns are both sequential and diverse, leaving a rich set of
temporal patterns for the models to learn. Experimental results showed that
BERT maintained the best performance. It achieved 99.94% accuracy rate
alongside high precision and recall, F1-score and AUC-ROC score of 99.99% which
demonstrates its capabilities through temporal dependency capture. The
Multi-Head Attention offered promising results by providing good detection
capabilities with interpretable results. On the other side, the Multi-Head
Attention model required significant processing time like BI-LSTM variants. The
GraphSAGE model achieved good accuracy while requiring the shortest training
time but yielded the lowest accuracy, precision, and F1 score compared to the
other models

</details>


### [10] [Detecting AI Assistance in Abstract Complex Tasks](https://arxiv.org/abs/2507.10761)
*Tyler King,Nikolos Gurney,John H. Miller,Volkan Ustun*

Main category: cs.AI

TL;DR: This paper explores detecting AI assistance in tasks and proposes new data preprocessing techniques to enable effective classification using neural networks.


<details>
  <summary>Details</summary>
Motivation: The ubiquity of AI assistance across tasks like text generation and autonomous driving makes it critical to detect AI involvement, a challenging task particularly for humans when dealing with abstract data.

Method: The authors reframed AI assistance detection as a classification problem and investigated five data formulations, including four image-based formats and one time-series format, to encode user behavior. They benchmarked these approaches using classical deep learning architectures and a CNN-RNN model.

Result: Experimental results demonstrated that preprocessing techniques and combining temporal and spatial features improve detection of AI assistance in abstract tasks, with significant performance gains using the CNN-RNN framework.

Conclusion: Appropriate data preprocessing and tailored neural network architectures enable effective classification in AI assistance detection tasks, potentially generalizable to various abstract domains.

Abstract: Detecting assistance from artificial intelligence is increasingly important
as they become ubiquitous across complex tasks such as text generation, medical
diagnosis, and autonomous driving. Aid detection is challenging for humans,
especially when looking at abstract task data. Artificial neural networks excel
at classification thanks to their ability to quickly learn from and process
large amounts of data -- assuming appropriate preprocessing. We posit detecting
help from AI as a classification task for such models. Much of the research in
this space examines the classification of complex but concrete data classes,
such as images. Many AI assistance detection scenarios, however, result in data
that is not machine learning-friendly. We demonstrate that common models can
effectively classify such data when it is appropriately preprocessed. To do so,
we construct four distinct neural network-friendly image formulations along
with an additional time-series formulation that explicitly encodes the
exploration/exploitation of users, which allows for generalizability to other
abstract tasks. We benchmark the quality of each image formulation across three
classical deep learning architectures, along with a parallel CNN-RNN
architecture that leverages the additional time series to maximize testing
performance, showcasing the importance of encoding temporal and spatial
quantities for detecting AI aid in abstract tasks.

</details>


### [11] [Uncertainty-Informed Scheduling of Decision Points for Intelligent Mobile Health Interventions](https://arxiv.org/abs/2507.10798)
*Asim H. Gazi,Bhanu T. Gullapalli,Daiqi Gao,Benjamin M. Marlin,Vivek Shetty,Susan A. Murphy*

Main category: cs.AI

TL;DR: The paper introduces SigmaScheduling, a dynamic method to improve the timing of mobile health interventions by accounting for uncertainty in user behavior predictions.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the inefficiencies of fixed-interval scheduling in mHealth interventions, especially for users with irregular routines, where such interventions often miss the optimal time window.

Method: The authors propose SigmaScheduling, a method that adjusts decision point timings dynamically based on prediction uncertainty about user behaviors, and evaluate its effectiveness using real-world data from a 10-week trial with 68 participants.

Result: SigmaScheduling demonstrated improved scheduling performance by ensuring that intervention opportunities preceded brushing events in at least 70% of cases, thus maintaining chances to influence behavior effectively.

Conclusion: SigmaScheduling can enhance the precision of mobile health interventions, particularly for time-sensitive habitual behaviors, improving their effectiveness in varied contexts.

Abstract: Timely decision making is critical to the effectiveness of mobile health
(mHealth) interventions. At predefined timepoints called "decision points,"
intelligent mHealth systems such as just-in-time adaptive interventions
(JITAIs) estimate an individual's biobehavioral context from sensor or survey
data and determine whether and how to intervene. For interventions targeting
habitual behavior (e.g., oral hygiene), effectiveness often hinges on
delivering support shortly before the target behavior is likely to occur.
Current practice schedules decision points at a fixed interval (e.g., one hour)
before user-provided behavior times, and the fixed interval is kept the same
for all individuals. However, this one-size-fits-all approach performs poorly
for individuals with irregular routines, often scheduling decision points after
the target behavior has already occurred, rendering interventions ineffective.
In this paper, we propose SigmaScheduling, a method to dynamically schedule
decision points based on uncertainty in predicted behavior times. When behavior
timing is more predictable, SigmaScheduling schedules decision points closer to
the predicted behavior time; when timing is less certain, SigmaScheduling
schedules decision points earlier, increasing the likelihood of timely
intervention. We evaluated SigmaScheduling using real-world data from 68
participants in a 10-week trial of Oralytics, a JITAI designed to improve daily
toothbrushing. SigmaScheduling increased the likelihood that decision points
preceded brushing events in at least 70% of cases, preserving opportunities to
intervene and impact behavior. Our results indicate that SigmaScheduling can
advance precision mHealth, particularly for JITAIs targeting time-sensitive,
habitual behaviors such as oral hygiene or dietary habits.

</details>


### [12] [Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social Media Chatter Use Case](https://arxiv.org/abs/2507.10803)
*JaMor Hairston,Ritvik Ranjan,Sahithi Lakamana,Anthony Spadaro,Selen Bozkurt,Jeanmarie Perrone,Abeed Sarker*

Main category: cs.AI

TL;DR: This paper evaluates the ability of large language models (LLMs), particularly GPT-4, to conduct expert-driven thematic analysis of social media data.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by challenges faced by LLMs in performing deep interpretive and domain-specific tasks, such as thematic analysis, which traditionally require expert knowledge.

Method: The researchers used Reddit datasets on xylazine, modeled the task as binary classifications, applied zero-, single-, and few-shot prompting, and evaluated five LLMs based on metrics like accuracy and F1-score.

Result: GPT-4 using two-shot prompting achieved the highest performance, closely mirroring expert thematic classifications for high-prevalence themes.

Conclusion: Few-shot LLM-based approaches can effectively and scalably automate thematic analyses, providing valuable support to qualitative research.

Abstract: Background Large language models (LLMs) face challenges in inductive thematic
analysis, a task requiring deep interpretive and domain-specific expertise. We
evaluated the feasibility of using LLMs to replicate expert-driven thematic
analysis of social media data. Methods Using two temporally non-intersecting
Reddit datasets on xylazine (n=286 and n=686, for model optimization and
validation, respectively) with twelve expert-derived themes, we evaluated five
LLMs against expert coding. We modeled the task as a series of binary
classifications, rather than a single, multi-label classification, employing
zero-, single-, and few-shot prompting strategies and measuring performance via
accuracy, precision, recall, and F1-score. Results On the validation set,
GPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score:
0.71). For high-prevalence themes, model-derived thematic distributions closely
mirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use:
16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based
approaches can automate thematic analyses, offering a scalable supplement for
qualitative research. Keywords: thematic analysis, large language models,
natural language processing, qualitative analysis, social media, prompt
engineering, public health

</details>


### [13] [AF-XRAY: Visual Explanation and Resolution of Ambiguity in Legal Argumentation Frameworks](https://arxiv.org/abs/2507.10831)
*Yilin Xia,Heng Zheng,Shawn Bowers,Bertram Ludäscher*

Main category: cs.AI

TL;DR: AF-XRAY is an open-source toolkit designed to simplify the exploration and resolution of ambiguities in argumentation frameworks (AFs) for legal reasoning. It offers layered visualizations, classification of attack edges, and tools to resolve undecided arguments.


<details>
  <summary>Details</summary>
Motivation: Legal reasoning often involves complex argument structures that are hard for non-experts to interpret. There is a need for tools to identify ambiguity sources and explain argument acceptance effectively.

Method: AF-XRAY uses layered visualizations based on game-theoretic principles, semantic classification of attack edges, alternative solution overlays, and systematic generation of critical attack sets to resolve ambiguous scenarios within AFs.

Result: The toolkit successfully resolves undecided arguments by transforming ambiguous scenarios into more grounded solutions. It also clarifies how differing assumptions impact the resolution of legal cases.

Conclusion: AF-XRAY enhances transparency in legal reasoning by enabling users to pinpoint ambiguity causes and explore alternative resolutions effectively, supporting clearer teleological reasoning in legal cases.

Abstract: Argumentation frameworks (AFs) provide formal approaches for legal reasoning,
but identifying sources of ambiguity and explaining argument acceptance remains
challenging for non-experts. We present AF-XRAY, an open-source toolkit for
exploring, analyzing, and visualizing abstract AFs in legal reasoning. AF-XRAY
introduces: (i) layered visualizations based on game-theoretic argument length
revealing well-founded derivation structures; (ii) classification of attack
edges by semantic roles (primary, secondary, blunders); (iii) overlay
visualizations of alternative 2-valued solutions on ambiguous 3-valued grounded
semantics; and (iv) identification of critical attack sets whose suspension
resolves undecided arguments. Through systematic generation of critical attack
sets, AF-XRAY transforms ambiguous scenarios into grounded solutions, enabling
users to pinpoint specific causes of ambiguity and explore alternative
resolutions. We use real-world legal cases (e.g., Wild Animals as modeled by
Bench-Capon) to show that our tool supports teleological legal reasoning by
revealing how different assumptions lead to different justified conclusions.

</details>


### [14] [NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization](https://arxiv.org/abs/2507.10894)
*Zongtao He,Liuyi Wang,Lu Chen,Chengju Liu,Qijun Chen*

Main category: cs.AI

TL;DR: NavComposer generates high-quality navigation instructions by decomposing and recomposing semantic elements, while NavInstrCritic evaluates without expert annotations, enabling scalable research in embodied AI navigation.


<details>
  <summary>Details</summary>
Motivation: The field of embodied AI requires high-quality language navigation instructions for research, but the current reliance on limited expert annotations and poor-quality synthetic annotations hampers scalability.

Method: NavComposer generates instructions by decomposing actions, scenes, and objects into semantic entities and recomposing them into language instructions, while NavInstrCritic evaluates instructions based on match quality, consistency, and diversity without expert reliance.

Result: The approach demonstrated enriched and accurate instruction generation adaptable to diverse environments and provided holistic evaluation metrics to effectively assess instruction quality.

Conclusion: NavComposer and NavInstrCritic advance embodied AI research by offering scalable, adaptable, and generalizable tools for instruction generation and evaluation, overcoming dependency on expert annotations.

Abstract: Language-guided navigation is a cornerstone of embodied AI, enabling agents
to interpret language instructions and navigate complex environments. However,
expert-provided instructions are limited in quantity, while synthesized
annotations often lack quality, making them insufficient for large-scale
research. To address this, we propose NavComposer, a novel framework for
automatically generating high-quality navigation instructions. NavComposer
explicitly decomposes semantic entities such as actions, scenes, and objects,
and recomposes them into natural language instructions. Its modular
architecture allows flexible integration of state-of-the-art techniques, while
the explicit use of semantic entities enhances both the richness and accuracy
of instructions. Moreover, it operates in a data-agnostic manner, supporting
adaptation to diverse navigation trajectories without domain-specific training.
Complementing NavComposer, we introduce NavInstrCritic, a comprehensive
annotation-free evaluation system that assesses navigation instructions on
three dimensions: contrastive matching, semantic consistency, and linguistic
diversity. NavInstrCritic provides a holistic evaluation of instruction
quality, addressing limitations of traditional metrics that rely heavily on
expert annotations. By decoupling instruction generation and evaluation from
specific navigation agents, our method enables more scalable and generalizable
research. Extensive experiments provide direct and practical evidence for the
effectiveness of our method.

</details>


### [15] [Lessons Learned from Evaluation of LLM based Multi-agents in Safer Therapy Recommendation](https://arxiv.org/abs/2507.10911)
*Yicong Wu,Ting Chen,Irit Hochberg,Zhoujian Sun,Ruth Edry,Zhengxing Huang,Mor Peleg*

Main category: cs.AI

TL;DR: This paper explores using a Large Language Model-based multi-agent system to improve therapy recommendations for patients with multiple chronic conditions, addressing treatment conflicts and offering an alternative to multidisciplinary team decision-making.


<details>
  <summary>Details</summary>
Motivation: Managing treatment for patients with multimorbidity is complex due to treatment conflicts, necessitating scalable decision support systems.

Method: The study developed single-agent and multi-agent frameworks using LLMs, simulating multidisciplinary team collaboration to tackle medical conflicts. Evaluation metrics were introduced to assess clinical goals and medication burden.

Result: Single-agent systems performed comparably to multi-agent systems and successfully addressed clinical goals. However, issues such as incomplete recommendations and unnecessary medications leading to conflicts were identified.

Conclusion: While LLM-based systems show promise, current models still need improvement to ensure therapy recommendations are complete and minimize unnecessary medication-related interactions.

Abstract: Therapy recommendation for chronic patients with multimorbidity is
challenging due to risks of treatment conflicts. Existing decision support
systems face scalability limitations. Inspired by the way in which general
practitioners (GP) manage multimorbidity patients, occasionally convening
multidisciplinary team (MDT) collaboration, this study investigated the
feasibility and value of using a Large Language Model (LLM)-based multi-agent
system (MAS) for safer therapy recommendations. We designed a single agent and
a MAS framework simulating MDT decision-making by enabling discussion among LLM
agents to resolve medical conflicts. The systems were evaluated on therapy
planning tasks for multimorbidity patients using benchmark cases. We compared
MAS performance with single-agent approaches and real-world benchmarks. An
important contribution of our study is the definition of evaluation metrics
that go beyond the technical precision and recall and allow the inspection of
clinical goals met and medication burden of the proposed advices to a gold
standard benchmark. Our results show that with current LLMs, a single agent GP
performs as well as MDTs. The best-scoring models provide correct
recommendations that address all clinical goals, yet the advices are
incomplete. Some models also present unnecessary medications, resulting in
unnecessary conflicts between medication and conditions or drug-drug
interactions.

</details>


### [16] [Enhancing Safe and Controllable Protein Generation via Knowledge Preference Optimization](https://arxiv.org/abs/2507.10923)
*Yuhao Wang,Keyan Ding,Kehua Feng,Zeyuan Wang,Ming Qin,Xiaotong Li,Qiang Zhang,Huajun Chen*

Main category: cs.AI

TL;DR: This paper introduces a Knowledge-guided Preference Optimization (KPO) framework to safely optimize protein sequence generation using protein language models.


<details>
  <summary>Details</summary>
Motivation: Protein language models have significant potential in functional optimization and protein design but also pose biosafety risks by potentially generating harmful sequences. This paper addresses these biosafety and ethical concerns.

Method: The authors propose a Knowledge-guided Preference Optimization framework that incorporates a Protein Safety Knowledge Graph. It uses a graph pruning strategy to identify safe sequences and reinforcement learning to minimize risks associated with harmful protein generation.

Result: The KPO framework demonstrated experimental success in reducing the generation of hazardous protein sequences while maintaining their functional effectiveness.

Conclusion: KPO provides a robust safety framework, reducing risks associated with protein language model use in biotechnology applications while preserving functionality.

Abstract: Protein language models have emerged as powerful tools for sequence
generation, offering substantial advantages in functional optimization and
denovo design. However, these models also present significant risks of
generating harmful protein sequences, such as those that enhance viral
transmissibility or evade immune responses. These concerns underscore critical
biosafety and ethical challenges. To address these issues, we propose a
Knowledge-guided Preference Optimization (KPO) framework that integrates prior
knowledge via a Protein Safety Knowledge Graph. This framework utilizes an
efficient graph pruning strategy to identify preferred sequences and employs
reinforcement learning to minimize the risk of generating harmful proteins.
Experimental results demonstrate that KPO effectively reduces the likelihood of
producing hazardous sequences while maintaining high functionality, offering a
robust safety assurance framework for applying generative models in
biotechnology.

</details>


### [17] [Modeling Habitat Shifts: Integrating Convolutional Neural Networks and Tabular Data for Species Migration Prediction](https://arxiv.org/abs/2507.10993)
*Emir Durakovic,Min-Hong Shih*

Main category: cs.AI

TL;DR: The paper introduces a method that combines Convolutional Neural Networks and tabular data to predict bird species’ presence in habitats, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Climate change causes habitats to shift geographically, necessitating models to accurately predict bird species presence.

Method: The paper combines CNNs for spatial landscape analysis of satellite images and tabular data for ecological and geographic features.

Result: The integrated system predicted bird distributions across climates with an average accuracy of 85%.

Conclusion: This method is scalable and provides reliable insights into bird migration patterns across shifting habitats.

Abstract: Due to climate-induced changes, many habitats are experiencing range shifts
away from their traditional geographic locations (Piguet, 2011). We propose a
solution to accurately model whether bird species are present in a specific
habitat through the combination of Convolutional Neural Networks (CNNs)
(O'Shea, 2015) and tabular data. Our approach makes use of satellite imagery
and environmental features (e.g., temperature, precipitation, elevation) to
predict bird presence across various climates. The CNN model captures spatial
characteristics of landscapes such as forestation, water bodies, and
urbanization, whereas the tabular method uses ecological and geographic data.
Both systems predict the distribution of birds with an average accuracy of 85%,
offering a scalable but reliable method to understand bird migration.

</details>


### [18] [Personalized Exercise Recommendation with Semantically-Grounded Knowledge Tracing](https://arxiv.org/abs/2507.11060)
*Yilmazcan Ozyurt,Tunaberk Almaci,Stefan Feuerriegel,Mrinmaya Sachan*

Main category: cs.AI

TL;DR: ExRec is a framework combining knowledge tracing and reinforcement learning for personalized exercise recommendations. It incorporates semantic question understanding and accounts for learning progression.


<details>
  <summary>Details</summary>
Motivation: Existing exercise recommendation systems neglect key aspects such as semantic content of questions and structured student progression in learning.

Method: ExRec combines semantic representation of question knowledge components with knowledge tracing models and reinforcement learning techniques, enhancing RL methods through tailored model-based value estimation.

Result: Validated on four real-world online math learning tasks, ExRec demonstrates robust generalization, interpretable student trajectories, and effective personalization.

Conclusion: ExRec highlights the potential of combining knowledge tracing and reinforcement learning for personalized and impactful education solutions.

Abstract: We introduce ExRec, a general framework for personalized exercise
recommendation with semantically-grounded knowledge tracing. Our method builds
on the observation that existing exercise recommendation approaches simulate
student performance via knowledge tracing (KT) but they often overlook two key
aspects: (a) the semantic content of questions and (b) the sequential,
structured progression of student learning. To address this, our ExRec presents
an end-to-end pipeline, from annotating the KCs of questions and learning their
semantic representations to training KT models and optimizing several
reinforcement learning (RL) methods. Moreover, we improve standard
Q-learning-based continuous RL methods via a tailored model-based value
estimation (MVE) approach that directly leverages the components of KT model in
estimating cumulative knowledge improvement. We validate the effectiveness of
our ExRec using various RL methods across four real-world tasks with different
educational goals in online math learning. We further show that ExRec
generalizes robustly to new, unseen questions and that it produces
interpretable student learning trajectories. Together, our findings highlight
the promise of KT-guided RL for effective personalization in education.

</details>


### [19] [Tactical Decision for Multi-UGV Confrontation with a Vision-Language Model-Based Commander](https://arxiv.org/abs/2507.11079)
*Li Wang,Qizhen Wu,Lei Chen*

Main category: cs.AI

TL;DR: The paper presents a vision-language model-based commander for decision-making in unmanned ground vehicle confrontations, achieving an 80% win rate over baselines.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of autonomous multi-agent tactical decision-making in complex battlefield scenarios where traditional and reinforcement learning methods have limitations.

Method: Utilizes a vision-language model for scene understanding and a lightweight large language model for strategic reasoning, facilitating unified perception and decision-making in a shared semantic space.

Result: Achieved over 80% win rate in simulation and ablation experiments compared to baseline models, highlighting adaptability and interpretability.

Conclusion: The proposed method reflects human-like cognitive processes for intelligent perception-to-decision reasoning, offering a significant advancement over traditional approaches.

Abstract: In multiple unmanned ground vehicle confrontations, autonomously evolving
multi-agent tactical decisions from situational awareness remain a significant
challenge. Traditional handcraft rule-based methods become vulnerable in the
complicated and transient battlefield environment, and current reinforcement
learning methods mainly focus on action manipulation instead of strategic
decisions due to lack of interpretability. Here, we propose a vision-language
model-based commander to address the issue of intelligent
perception-to-decision reasoning in autonomous confrontations. Our method
integrates a vision language model for scene understanding and a lightweight
large language model for strategic reasoning, achieving unified perception and
decision within a shared semantic space, with strong adaptability and
interpretability. Unlike rule-based search and reinforcement learning methods,
the combination of the two modules establishes a full-chain process, reflecting
the cognitive process of human commanders. Simulation and ablation experiments
validate that the proposed approach achieves a win rate of over 80% compared
with baseline models.

</details>


### [20] [Function-to-Style Guidance of LLMs for Code Translation](https://arxiv.org/abs/2507.11083)
*Longhui Zhang,Bin Wang,Jiahao Wang,Xiaofeng Zhao,Min Zhang,Hao Yang,Meishan Zhang,Yu Li,Jing Li,Jun Yu,Min Zhang*

Main category: cs.AI

TL;DR: The paper introduces F2STrans, a two-stage framework for enhancing large language models (LLMs) in code translation tasks, focusing on correctness and readability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges faced by LLMs in ensuring correctness and readability in code translation to improve their utility in real-world software development.

Method: The F2STrans framework uses two stages: Functional learning for optimizing correctness via high-quality source-target code pairs, and Style learning for improving readability through positive and negative style examples. It also introduces a code translation benchmark for robust evaluation.

Result: The proposed approach significantly boosts code translation performance, enabling a smaller model (Qwen-1.5B) to outperform much larger models like Qwen-32B and GPT-4 on average across 20 diverse scenarios.

Conclusion: The F2STrans framework is effective in improving both correctness and readability in code translation, demonstrating its potential to advance the adoption of LLMs in software development.

Abstract: Large language models (LLMs) have made significant strides in code
translation tasks. However, ensuring both the correctness and readability of
translated code remains a challenge, limiting their effective adoption in
real-world software development. In this work, we propose F2STrans, a
function-to-style guiding paradigm designed to progressively improve the
performance of LLMs in code translation. Our approach comprises two key stages:
(1) Functional learning, which optimizes translation correctness using
high-quality source-target code pairs mined from online programming platforms,
and (2) Style learning, which improves translation readability by incorporating
both positive and negative style examples. Additionally, we introduce a novel
code translation benchmark that includes up-to-date source code, extensive test
cases, and manually annotated ground-truth translations, enabling comprehensive
functional and stylistic evaluations. Experiments on both our new benchmark and
existing datasets demonstrate that our approach significantly improves code
translation performance. Notably, our approach enables Qwen-1.5B to outperform
prompt-enhanced Qwen-32B and GPT-4 on average across 20 diverse code
translation scenarios.

</details>


### [21] [AI Agent Architecture for Decentralized Trading of Alternative Assets](https://arxiv.org/abs/2507.11117)
*Ailiya Borjigin,Cong He,Charles CC Lee,Wei Zhou*

Main category: cs.AI

TL;DR: GoldMine OS uses AI agents and blockchain to tokenize and trade physical gold as stablecoin ('OZ'), offering high speed, liquidity, and security.


<details>
  <summary>Details</summary>
Motivation: To enable decentralized trading of physical alternative assets like gold while meeting compliance, liquidity, and risk management requirements.

Method: GoldMine OS integrates on-chain smart contracts with off-chain AI agents (Compliance, Token Issuance, Market Making, Risk Control) to automate tokenization and trading processes.

Result: Simulations and deployments show 1.2-second token issuance, tight liquidity with <0.5% spreads, resilience to attacks, and scalability to 5000 TPS with 10000 users.

Conclusion: The system demonstrates practicality for democratizing access to illiquid assets, with a governance model ensuring transparency, adaptability, and system integrity.

Abstract: Decentralized trading of real-world alternative assets (e.g., gold) requires
bridging physical asset custody with blockchain systems while meeting strict
requirements for compliance, liquidity, and risk management. We present
GoldMine OS, a research oriented architecture that employs multiple specialized
AI agents to automate and secure the tokenization and exchange of physical gold
into a blockchain based stablecoin ("OZ"). Our approach combines on chain smart
contracts for critical risk controls with off chain AI agents for decision
making, blending the transparency and reliability of blockchains with the
flexibility of AI driven automation. We describe four cooperative agents
(Compliance, Token Issuance, Market Making, and Risk Control) and a
coordinating core, and evaluate the system through simulation and a controlled
pilot deployment. In experiments the prototype delivers on demand token
issuance in under 1.2 s, more than 100 times faster than manual workflows. The
Market Making agent maintains tight liquidity with spreads often below 0.5
percent even under volatile conditions. Fault injection tests show resilience:
an oracle price spoofing attack is detected and mitigated within 10 s, and a
simulated vault mis reporting halts issuance immediately with minimal user
impact. The architecture scales to 5000 transactions per second with 10000
concurrent users in benchmarks. These results indicate that an AI agent based
decentralized exchange for alternative assets can satisfy rigorous performance
and safety requirements. We discuss broader implications for democratizing
access to traditionally illiquid assets and explain how our governance model --
multi signature agent updates and on chain community voting on risk parameters
-- provides ongoing transparency, adaptability, and formal assurance of system
integrity.

</details>


### [22] [Defining neurosymbolic AI](https://arxiv.org/abs/2507.11127)
*Lennert De Smet,Luc De Raedt*

Main category: cs.AI

TL;DR: The paper introduces a formal definition of neurosymbolic AI, unifying logic and neural computing, through a conceptual framework based on integrals and belief functions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a formal and universally accepted definition in the growing field of neurosymbolic AI.

Method: Defined neurosymbolic inference as computing an integral over a product of logical and belief functions, abstracting key ingredients of the field.

Result: Demonstrated the applicability of the new definition to encompass representative neurosymbolic AI systems.

Conclusion: The proposed definition provides a standardized conceptual framework for understanding and developing neurosymbolic AI systems.

Abstract: Neurosymbolic AI focuses on integrating learning and reasoning, in
particular, on unifying logical and neural representations. Despite the
existence of an alphabet soup of neurosymbolic AI systems, the field is lacking
a generally accepted formal definition of what neurosymbolic models and
inference really are. We introduce a formal definition for neurosymbolic AI
that makes abstraction of its key ingredients. More specifically, we define
neurosymbolic inference as the computation of an integral over a product of a
logical and a belief function. We show that our neurosymbolic AI definition
makes abstraction of key representative neurosymbolic AI systems.

</details>


### [23] [Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety](https://arxiv.org/abs/2507.11473)
*Tomek Korbak,Mikita Balesni,Elizabeth Barnes,Yoshua Bengio,Joe Benton,Joseph Bloom,Mark Chen,Alan Cooney,Allan Dafoe,Anca Dragan,Scott Emmons,Owain Evans,David Farhi,Ryan Greenblatt,Dan Hendrycks,Marius Hobbhahn,Evan Hubinger,Geoffrey Irving,Erik Jenner,Daniel Kokotajlo,Victoria Krakovna,Shane Legg,David Lindner,David Luan,Aleksander Mądry,Julian Michael,Neel Nanda,Dave Orr,Jakub Pachocki,Ethan Perez,Mary Phuong,Fabien Roger,Joshua Saxe,Buck Shlegeris,Martín Soto,Eric Steinberger,Jasmine Wang,Wojciech Zaremba,Bowen Baker,Rohin Shah,Vlad Mikulik*

Main category: cs.AI

TL;DR: This paper explores using AI's chain of thought (CoT) as a tool for monitoring intent and ensuring AI safety.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is to explore how monitoring AI's internal thought processes in human-like language can improve AI safety and detect potential misbehavior.

Method: The researchers propose using Chain of Thought (CoT) monitoring to track and evaluate the intents of AI systems by analyzing their language-based reasoning processes.

Result: They found that CoT monitoring is not perfect but shows significant potential to complement existing AI oversight methods.

Conclusion: The paper concludes that further research and investment in CoT monitoring are needed, emphasizing the importance of making development choices that maintain CoT monitorability.

Abstract: AI systems that "think" in human language offer a unique opportunity for AI
safety: we can monitor their chains of thought (CoT) for the intent to
misbehave. Like all other known AI oversight methods, CoT monitoring is
imperfect and allows some misbehavior to go unnoticed. Nevertheless, it shows
promise and we recommend further research into CoT monitorability and
investment in CoT monitoring alongside existing safety methods. Because CoT
monitorability may be fragile, we recommend that frontier model developers
consider the impact of development decisions on CoT monitorability.

</details>


### [24] [Collaborative Trustworthiness for Good Decision Making in Autonomous Systems](https://arxiv.org/abs/2507.11135)
*Selma Saidi,Omar Laimona,Christoph Schmickler,Dirk Ziegenbein*

Main category: cs.AI

TL;DR: The paper proposes a collaborative approach to enhance trustworthiness and improve decision-making in autonomous systems by introducing a quality-based aggregation method using Binary Decision Diagrams.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by ensuring safe and reliable decision-making in autonomous systems operating in dynamic and complex environments.

Method: The approach leverages quality attributes of autonomous systems, borrowing concepts from social epistemology for aggregation and propagation rules. Binary Decision Diagrams are used to model beliefs and formulate reduction rules for efficient computation.

Result: An innovative aggregation framework based on perception quality was developed, demonstrating better trustworthiness in autonomous systems' decision-making processes.

Conclusion: The proposed method enhances reliability and trustworthiness in autonomous systems by improving aggregation methods and computational efficiency for collaborative reasoning.

Abstract: Autonomous systems are becoming an integral part of many application domains,
like in the mobility sector. However, ensuring their safe and correct behaviour
in dynamic and complex environments remains a significant challenge, where
systems should autonomously make decisions e.g., about manoeuvring. We propose
in this paper a general collaborative approach for increasing the level of
trustworthiness in the environment of operation and improve reliability and
good decision making in autonomous system. In the presence of conflicting
information, aggregation becomes a major issue for trustworthy decision making
based on collaborative data sharing. Unlike classical approaches in the
literature that rely on consensus or majority as aggregation rule, we exploit
the fact that autonomous systems have different quality attributes like
perception quality. We use this criteria to determine which autonomous systems
are trustworthy and borrow concepts from social epistemology to define
aggregation and propagation rules, used for automated decision making. We use
Binary Decision Diagrams (BDDs) as formal models for beliefs aggregation and
propagation, and formulate reduction rules to reduce the size of the BDDs and
allow efficient computation structures for collaborative automated reasoning.

</details>


### [25] [Fine-grained Timing Analysis of Digital Integrated Circuits in Answer Set Programming](https://arxiv.org/abs/2507.11150)
*Alessandro Bertagnon,Marcello Dalpasso,Michele Favalli,Marco Gavanelli*

Main category: cs.AI

TL;DR: This paper addresses the problem of computing the actual maximum delay in hardware circuits using Answer Set Programming (ASP), instead of relying on approximations from Static Timing Analysis.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to calculate maximum delay often rely on approximations, which may lead to suboptimal processor speeds due to conservative upper bounds. Accurate computation of maximum delay is critical for optimizing the clock frequency in synchronous systems like CPUs.

Method: The authors model the maximum delay computation problem using Answer Set Programming (ASP), which is known for its efficient solvers. They propose novel encodings of this problem into ASP.

Result: Experimental results indicate that ASP provides a viable and effective solution for solving complex delay computation problems in hardware design.

Conclusion: ASP demonstrates potential as a powerful tool for accurately computing maximum delays in combinational circuits, improving hardware performance by overcoming limitations of existing methods.

Abstract: In the design of integrated circuits, one critical metric is the maximum
delay introduced by combinational modules within the circuit. This delay is
crucial because it represents the time required to perform a computation: in an
Arithmetic-Logic Unit it represents the maximum time taken by the circuit to
perform an arithmetic operation. When such a circuit is part of a larger,
synchronous system, like a CPU, the maximum delay directly impacts the maximum
clock frequency of the entire system. Typically, hardware designers use Static
Timing Analysis to compute an upper bound of the maximum delay because it can
be determined in polynomial time. However, relying on this upper bound can lead
to suboptimal processor speeds, thereby missing performance opportunities. In
this work, we tackle the challenging task of computing the actual maximum
delay, rather than an approximate value. Since the problem is computationally
hard, we model it in Answer Set Programming (ASP), a logic language featuring
extremely efficient solvers. We propose non-trivial encodings of the problem
into ASP. Experimental results show that ASP is a viable solution to address
complex problems in hardware design.

</details>


### [26] [DuetGraph: Coarse-to-Fine Knowledge Graph Reasoning with Dual-Pathway Global-Local Fusion](https://arxiv.org/abs/2507.11229)
*Jin Li,Zezhong Ding,Xike Xie*

Main category: cs.AI

TL;DR: The paper proposes a dual-pathway knowledge graph reasoning method called DuetGraph to address over-smoothing issues and achieve state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing KG reasoning methods are hindered by score over-smoothing, which reduces the distinction between correct and incorrect answers and negatively impacts reasoning effectiveness.

Method: DuetGraph employs a dual-pathway mechanism that separates global and local information processing, along with a coarse-to-fine optimization strategy for narrowing candidate space and sharpening score distinctions.

Result: DuetGraph achieves up to 8.7% improvement in reasoning quality and 1.8× acceleration in training efficiency over prior methods.

Conclusion: DuetGraph successfully mitigates over-smoothing and enhances both the quality and efficiency of reasoning in knowledge graphs, establishing new state-of-the-art performance.

Abstract: Knowledge graphs (KGs) are vital for enabling knowledge reasoning across
various domains. Recent KG reasoning methods that integrate both global and
local information have achieved promising results. However, existing methods
often suffer from score over-smoothing, which blurs the distinction between
correct and incorrect answers and hinders reasoning effectiveness. To address
this, we propose DuetGraph, a coarse-to-fine KG reasoning mechanism with
dual-pathway global-local fusion. DuetGraph tackles over-smoothing by
segregating -- rather than stacking -- the processing of local (via message
passing) and global (via attention) information into two distinct pathways,
preventing mutual interference and preserving representational discrimination.
In addition, DuetGraph introduces a coarse-to-fine optimization, which
partitions entities into high- and low-score subsets. This strategy narrows the
candidate space and sharpens the score gap between the two subsets, which
alleviates over-smoothing and enhances inference quality. Extensive experiments
on various datasets demonstrate that DuetGraph achieves state-of-the-art (SOTA)
performance, with up to an 8.7% improvement in reasoning quality and a
1.8$\times$ acceleration in training efficiency.

</details>


### [27] [Modeling Code: Is Text All You Need?](https://arxiv.org/abs/2507.11467)
*Daniel Nichols,Konstantinos Parasyris,Harshitha Menon,Brian R. Bartoldson,Giorgis Georgakoudis,Tal Ben-Nun,Abhinav Bhatele*

Main category: cs.AI

TL;DR: The paper aims to integrate structured code representation and text-based representation to overcome limitations of current code LLMs.


<details>
  <summary>Details</summary>
Motivation: Transformers struggle with structured, analytical code properties, prompting the need for enhanced modeling techniques.

Method: A hybrid approach combining LLM's text-based representation with structured data for code analysis.

Result: Novel approach enables analysis of structured code properties while retaining generative power of LLMs.

Conclusion: Combining strengths of text-based and structured representation compensates for limitations of modern code LLMs.

Abstract: Code LLMs have become extremely popular recently for modeling source code
across a variety of tasks, such as generation, translation, and summarization.
However, transformer-based models are limited in their capabilities to reason
through structured, analytical properties of code, such as control and data
flow. Previous work has explored the modeling of these properties with
structured data and graph neural networks. However, these approaches lack the
generative capabilities and scale of modern LLMs. In this work, we introduce a
novel approach to combine the strengths of modeling both code as text and more
structured forms.

</details>


### [28] [Taming Uncertainty via Automation: Observing, Analyzing, and Optimizing Agentic AI Systems](https://arxiv.org/abs/2507.11277)
*Dany Moshkovich,Sergey Zeltyn*

Main category: cs.AI

TL;DR: This paper introduces AgentOps, a framework addressing the challenges of operating agentic AI systems by enabling observation, analysis, optimization, and automation.


<details>
  <summary>Details</summary>
Motivation: The growing complexity of agentic systems powered by LLMs brings unique uncertainties, and current observability and operations practices are insufficient to manage these challenges.

Method: The authors propose a six-stage AgentOps Automation Pipeline focusing on observation, metric collection, issue detection, root cause analysis, recommendations, and runtime automation, tailored for developers, testers, SREs, and business users.

Result: The framework emphasizes automation to manage uncertainty, ensuring the adaptive, safe, and effective operation of agentic systems.

Conclusion: AgentOps serves as a vital toolkit for operating complex LLM-powered systems, fostering self-improving capabilities while managing uncertainty effectively.

Abstract: Large Language Models (LLMs) are increasingly deployed within agentic
systems-collections of interacting, LLM-powered agents that execute complex,
adaptive workflows using memory, tools, and dynamic planning. While enabling
powerful new capabilities, these systems also introduce unique forms of
uncertainty stemming from probabilistic reasoning, evolving memory states, and
fluid execution paths. Traditional software observability and operations
practices fall short in addressing these challenges.
  This paper introduces AgentOps: a comprehensive framework for observing,
analyzing, optimizing, and automating operation of agentic AI systems. We
identify distinct needs across four key roles-developers, testers, site
reliability engineers (SREs), and business users-each of whom engages with the
system at different points in its lifecycle. We present the AgentOps Automation
Pipeline, a six-stage process encompassing behavior observation, metric
collection, issue detection, root cause analysis, optimized recommendations,
and runtime automation. Throughout, we emphasize the critical role of
automation in managing uncertainty and enabling self-improving AI systems-not
by eliminating uncertainty, but by taming it to ensure safe, adaptive, and
effective operation.

</details>


### [29] [Opus: A Prompt Intention Framework for Complex Workflow Generation](https://arxiv.org/abs/2507.11288)
*Théo Fagnoni,Mahsun Altin,Chia En Chung,Phillip Kingston,Alan Tuning,Dana O. Mohamed,Inès Adnani*

Main category: cs.AI

TL;DR: This paper presents the Opus Prompt Intention Framework, which improves Workflow Generation using LLMs by introducing an intermediate Intention Capture layer.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the logical and meaningful output generation of complex Workflow Generation tasks by addressing challenges like scaling reliability with query complexity.

Method: The paper proposes an intermediate layer called the Intention Capture layer, including extracting Workflow Signals, structuring these into Workflow Intention objects, and generating workflows through these Intentions.

Result: Experiments on a synthetic benchmark demonstrate consistent improvements in semantic Workflow similarity metrics when using this framework, especially for complex, multi-intent queries.

Conclusion: The proposed framework significantly improves workflow generation quality and offers a customizable system for better Intention Capture with LLMs, particularly for challenging cases like Mixed Intention Elicitation.

Abstract: This paper introduces the Opus Prompt Intention Framework, designed to
improve complex Workflow Generation with instruction-tuned Large Language
Models (LLMs). We propose an intermediate Intention Capture layer between user
queries and Workflow Generation, implementing the Opus Workflow Intention
Framework, which consists of extracting Workflow Signals from user queries,
interpreting them into structured Workflow Intention objects, and generating
Workflows based on these Intentions. Our results show that this layer enables
LLMs to produce logical and meaningful outputs that scale reliably as query
complexity increases. On a synthetic benchmark of 1,000 multi-intent
query-Workflow(s) pairs, applying the Opus Prompt Intention Framework to
Workflow Generation yields consistent improvements in semantic Workflow
similarity metrics. In this paper, we introduce the Opus Prompt Intention
Framework by applying the concepts of Workflow Signal and Workflow Intention to
LLM-driven Workflow Generation. We present a reproducible, customizable
LLM-based Intention Capture system to extract Workflow Signals and Workflow
Intentions from user queries. Finally, we provide empirical evidence that the
proposed system significantly improves Workflow Generation quality compared to
direct generation from user queries, particularly in cases of Mixed Intention
Elicitation.

</details>


### [30] [Contestability in Quantitative Argumentation](https://arxiv.org/abs/2507.11323)
*Xiang Yin,Nico Potyka,Antonio Rago,Timotheus Kampik,Francesca Toni*

Main category: cs.AI

TL;DR: Introduced a method to align AI-driven decisions with human preferences using Edge-Weighted Quantitative Bipolar Argumentation Frameworks (EW-QBAFs), with an effective experimental demonstration.


<details>
  <summary>Details</summary>
Motivation: The paper aims to foster contestable AI by ensuring that decisions align with human preferences, focusing on the underexplored area of EW-QBAFs.

Method: It presents a problem definition for contestability in EW-QBAFs, proposing gradient-based relation attribution explanations (G-RAEs) to interpret and adjust edge weights. An iterative algorithm then fine-tunes these weights to achieve desired argument strengths.

Result: Experiments on synthetic frameworks, resembling personalized recommender systems and multi-layer perceptrons, show that the proposed method effectively achieves contestability.

Conclusion: EW-QBAFs can support contestable AI by integrating G-RAEs and iterative weight adjustment methods, providing interpretable and practical solutions for alignment with human preferences.

Abstract: Contestable AI requires that AI-driven decisions align with human
preferences. While various forms of argumentation have been shown to support
contestability, Edge-Weighted Quantitative Bipolar Argumentation Frameworks
(EW-QBAFs) have received little attention. In this work, we show how EW-QBAFs
can be deployed for this purpose. Specifically, we introduce the contestability
problem for EW-QBAFs, which asks how to modify edge weights (e.g., preferences)
to achieve a desired strength for a specific argument of interest (i.e., a
topic argument). To address this problem, we propose gradient-based relation
attribution explanations (G-RAEs), which quantify the sensitivity of the topic
argument's strength to changes in individual edge weights, thus providing
interpretable guidance for weight adjustments towards contestability. Building
on G-RAEs, we develop an iterative algorithm that progressively adjusts the
edge weights to attain the desired strength. We evaluate our approach
experimentally on synthetic EW-QBAFs that simulate the structural
characteristics of personalised recommender systems and multi-layer
perceptrons, and demonstrate that it can solve the problem effectively.

</details>


### [31] [CogDDN: A Cognitive Demand-Driven Navigation with Decision Optimization and Dual-Process Thinking](https://arxiv.org/abs/2507.11334)
*Yuehao Huang,Liang Liu,Shuangming Lei,Yukai Ma,Hao Su,Jianbiao Mei,Pengxiang Zhao,Yaqing Gu,Yong Liu,Jiajun Lv*

Main category: cs.AI

TL;DR: The paper introduces CogDDN, a VLM-based system for mobile robots to navigate unknown environments by mimicking human cognitive processes, achieving 15% better results than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional data-driven demand-driven navigation (DDN) systems suffer from poor generalization in unfamiliar environments, prompting the need for a more adaptable approach.

Method: CogDDN employs a Vision-Language Model (VLM) framework with dual-process decision-making (fast heuristic and slow analytic systems), semantic alignment of object detection with instructions, and Chain of Thought reasoning for enhanced decision-making.

Result: CogDDN demonstrated a 15% performance improvement in navigation accuracy and adaptability compared to traditional methods, validated using the AI2Thor simulator with the ProcThor dataset.

Conclusion: By incorporating human-like cognitive mechanisms, CogDDN provides a robust and generalizable solution for object navigation in unstructured environments, addressing limitations of previous data-driven approaches.

Abstract: Mobile robots are increasingly required to navigate and interact within
unknown and unstructured environments to meet human demands. Demand-driven
navigation (DDN) enables robots to identify and locate objects based on
implicit human intent, even when object locations are unknown. However,
traditional data-driven DDN methods rely on pre-collected data for model
training and decision-making, limiting their generalization capability in
unseen scenarios. In this paper, we propose CogDDN, a VLM-based framework that
emulates the human cognitive and learning mechanisms by integrating fast and
slow thinking systems and selectively identifying key objects essential to
fulfilling user demands. CogDDN identifies appropriate target objects by
semantically aligning detected objects with the given instructions.
Furthermore, it incorporates a dual-process decision-making module, comprising
a Heuristic Process for rapid, efficient decisions and an Analytic Process that
analyzes past errors, accumulates them in a knowledge base, and continuously
improves performance. Chain of Thought (CoT) reasoning strengthens the
decision-making process. Extensive closed-loop evaluations on the AI2Thor
simulator with the ProcThor dataset show that CogDDN outperforms single-view
camera-only methods by 15%, demonstrating significant improvements in
navigation accuracy and adaptability. The project page is available at
https://yuehaohuang.github.io/CogDDN/.

</details>


### [32] [Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces](https://arxiv.org/abs/2507.11352)
*Yunhao Yang,Neel P. Bhatt,Christian Ellis,Alvaro Velasquez,Zhangyang Wang,Ufuk Topcu*

Main category: cs.AI

TL;DR: The paper introduces a neurosymbolic framework combining natural language dialogue and verifiable guarantees for logistics planning to tackle uncertainty, surpass GPT-4.1 in performance with faster latency.


<details>
  <summary>Details</summary>
Motivation: Logistics operators face challenges in making rapid, life-critical decisions in uncertain environments, where traditional methods are slow and large language models can be prone to errors and misinterpretations.

Method: The framework pairs natural-language dialogue with structured, verifiable planning specifications, implements uncertainty quantification, and uses an interactive clarification loop to ensure robust interpretation.

Result: A lightweight model fine-tuned on 100 examples outperforms GPT-4.1 in zero-shot settings and reduces inference latency by 50%.

Conclusion: The work presents a promising approach for real-time, certifiable, and user-aligned decision-making in complex logistics scenarios, bridging accessibility with safety guarantees.

Abstract: Logistics operators, from battlefield coordinators rerouting airlifts ahead
of a storm to warehouse managers juggling late trucks, often face life-critical
decisions that demand both domain expertise and rapid and continuous
replanning. While popular methods like integer programming yield logistics
plans that satisfy user-defined logical constraints, they are slow and assume
an idealized mathematical model of the environment that does not account for
uncertainty. On the other hand, large language models (LLMs) can handle
uncertainty and promise to accelerate replanning while lowering the barrier to
entry by translating free-form utterances into executable plans, yet they
remain prone to misinterpretations and hallucinations that jeopardize safety
and cost. We introduce a neurosymbolic framework that pairs the accessibility
of natural-language dialogue with verifiable guarantees on goal interpretation.
It converts user requests into structured planning specifications, quantifies
its own uncertainty at the field and token level, and invokes an interactive
clarification loop whenever confidence falls below an adaptive threshold. A
lightweight model, fine-tuned on just 100 uncertainty-filtered examples,
surpasses the zero-shot performance of GPT-4.1 while cutting inference latency
by nearly 50%. These preliminary results highlight a practical path toward
certifiable, real-time, and user-aligned decision-making for complex logistics.

</details>


### [33] [Perspective-Aware AI in Extended Reality](https://arxiv.org/abs/2507.11479)
*Daniel Platnick,Matti Gruener,Marjan Alirezaie,Kent Larson,Dava J. Newman,Hossein Rahnama*

Main category: cs.AI

TL;DR: The paper introduces PAiR, a framework combining Perspective-Aware AI with XR for personalized and adaptive experiences.


<details>
  <summary>Details</summary>
Motivation: Current XR systems lack deep user modeling and cognitive context, limiting immersive experiences.

Method: Creating closed-loop systems with reasoning-ready identity models (Chronicles) to integrate dynamic user states into XR environments.

Result: Demonstrated PAiR's effectiveness via prototyping in Unity-based OpenDome with proof-of-concept scenarios.

Conclusion: PAiR establishes a new paradigm in human-AI interaction by embedding perspective-based identity models in immersive environments.

Abstract: AI-enhanced Extended Reality (XR) aims to deliver adaptive, immersive
experiences-yet current systems fall short due to shallow user modeling and
limited cognitive context. We introduce Perspective-Aware AI in Extended
Reality (PAiR), a foundational framework for integrating Perspective-Aware AI
(PAi) with XR to enable interpretable, context-aware experiences grounded in
user identity. PAi is built on Chronicles: reasoning-ready identity models
learned from multimodal digital footprints that capture users' cognitive and
experiential evolution. PAiR employs these models in a closed-loop system
linking dynamic user states with immersive environments. We present PAiR's
architecture, detailing its modules and system flow, and demonstrate its
utility through two proof-of-concept scenarios implemented in the Unity-based
OpenDome engine. PAiR opens a new direction for human-AI interaction by
embedding perspective-based identity models into immersive systems.

</details>


### [34] [Illuminating the Three Dogmas of Reinforcement Learning under Evolutionary Light](https://arxiv.org/abs/2507.11482)
*Mani Hamidi,Terrence W. Deacon*

Main category: cs.AI

TL;DR: The paper critiques key assumptions in reinforcement learning (RL) and offers an evolutionary framework to revise them, while also exploring issues like agency and multi-objective optimization.


<details>
  <summary>Details</summary>
Motivation: To address foundational assumptions in reinforcement learning that limit its compatibility with biological learning and to explore broader adaptations inspired by evolutionary theories.

Method: The authors critique RL's tenets on agency, learning objectives, and reward limitations by integrating insights from open-ended evolutionary theory and origins-of-life thermodynamics.

Result: The paper proposes reevaluating RL's reward hypothesis, advocating for multi-objective optimization, and suggests that evolutionary dynamics occur in individual brains. However, it acknowledges the need for additional frameworks to address the concept of agency.

Conclusion: Reinforcement learning theory could benefit by aligning more closely with evolutionary and biological principles, though understanding agency requires further work.

Abstract: Three core tenets of reinforcement learning (RL)--concerning the definition
of agency, the objective of learning, and the scope of the reward
hypothesis--have been highlighted as key targets for conceptual revision, with
major implications for theory and application. We propose a framework, inspired
by open-ended evolutionary theory, to reconsider these three "dogmas." We
revisit each assumption and address related concerns raised alongside them. To
make our arguments relevant to RL as a model of biological learning, we first
establish that evolutionary dynamics can plausibly operate within living brains
over an individual's lifetime, and are not confined to cross-generational
processes. We begin by revisiting the second dogma, drawing on evolutionary
insights to enrich the "adaptation-rather-than-search" view of learning. We
then address the third dogma regarding the limits of the reward hypothesis,
using analogies from evolutionary fitness to illuminate the scalar reward vs.
multi-objective debate. After discussing practical implications for exploration
in RL, we turn to the first--and arguably most fundamental--issue: the absence
of a formal account of agency. We argue that unlike the other two problems, the
evolutionary paradigm alone cannot resolve the agency question, though it
gestures in a productive direction. We advocate integrating ideas from
origins-of-life theory, where the thermodynamics of sustenance and replication
offer promising foundations for understanding agency and resource-constrained
reinforcement learning in biological systems.

</details>


### [35] [DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering](https://arxiv.org/abs/2507.11527)
*Yinsheng Li,Zhen Dong,Yi Shao*

Main category: cs.AI

TL;DR: The paper introduces DrafterBench, an open-source benchmark aimed at evaluating Large Language Model (LLM) agents' effectiveness in technical drawing revision for civil engineering. It provides comprehensive testing for structured data comprehension, instruction execution, and dynamic policy awareness.


<details>
  <summary>Details</summary>
Motivation: To address the gap in systematic evaluation benchmarks for automation agents in civil engineering tasks, specifically in technical drawing revision, where existing solutions are limited.

Method: The authors developed DrafterBench, which Includes twelve task types derived from real-world drawings, 46 customized tools, and 1920 tasks. The toolkit evaluates agent capabilities like long-context interpretation, leveraging prior knowledge, and adapting to instruction variations.

Result: DrafterBench allows for detailed insights into agent capabilities, offering accuracy and error statistics to identify strengths and improvement areas in automation agents for civil engineering applications.

Conclusion: DrafterBench is a robust framework for evaluating LLM agents' proficiency in civil engineering tasks, paving the way for better integration of AI tools in engineering automation and providing targets for improvement.

Abstract: Large Language Model (LLM) agents have shown great potential for solving
real-world problems and promise to be a solution for tasks automation in
industry. However, more benchmarks are needed to systematically evaluate
automation agents from an industrial perspective, for example, in Civil
Engineering. Therefore, we propose DrafterBench for the comprehensive
evaluation of LLM agents in the context of technical drawing revision, a
representation task in civil engineering. DrafterBench contains twelve types of
tasks summarized from real-world drawing files, with 46 customized
functions/tools and 1920 tasks in total. DrafterBench is an open-source
benchmark to rigorously test AI agents' proficiency in interpreting intricate
and long-context instructions, leveraging prior knowledge, and adapting to
dynamic instruction quality via implicit policy awareness. The toolkit
comprehensively assesses distinct capabilities in structured data
comprehension, function execution, instruction following, and critical
reasoning. DrafterBench offers detailed analysis of task accuracy and error
statistics, aiming to provide deeper insight into agent capabilities and
identify improvement targets for integrating LLMs in engineering applications.
Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench,
with the test set hosted at
https://huggingface.co/datasets/Eason666/DrafterBench.

</details>


### [36] [How Many Instructions Can LLMs Follow at Once?](https://arxiv.org/abs/2507.11538)
*Daniel Jaroslawicz,Brendan Whiting,Parth Shah,Karime Maamari*

Main category: cs.AI

TL;DR: The paper introduces IFScale, a benchmark to test LLM instruction-following ability under high instruction density, highlighting models' performance limitations and different degradation patterns.


<details>
  <summary>Details</summary>
Motivation: To evaluate and characterize the instruction-following capabilities of LLMs when tasked with hundreds of simultaneous instructions, as existing benchmarks are limited to fewer instructions.

Method: The benchmark consists of 500 keyword-inclusion instructions for a business report writing task, applied across 20 state-of-the-art models from seven major providers, measuring performance and identifying patterns of degradation.

Result: The best frontier models achieve only 68% accuracy at maximum instruction density; distinct degradation patterns, biases, and error categories are observed, correlated with model size and reasoning capabilities.

Conclusion: Insights from this benchmark can inform the design of instruction-heavy prompts in practical applications and underline tradeoffs between model performance and latency; the benchmark and results are publicly available for further research.

Abstract: Production-grade LLM systems require robust adherence to dozens or even
hundreds of instructions simultaneously. However, the instruction-following
capabilities of LLMs at high instruction densities have not yet been
characterized, as existing benchmarks only evaluate models on tasks with a
single or few instructions. We introduce IFScale, a simple benchmark of 500
keyword-inclusion instructions for a business report writing task to measure
how instruction-following performance degrades as instruction density
increases. We evaluate 20 state-of-the-art models across seven major providers
and find that even the best frontier models only achieve 68% accuracy at the
max density of 500 instructions. Our analysis reveals model size and reasoning
capability to correlate with 3 distinct performance degradation patterns, bias
towards earlier instructions, and distinct categories of instruction-following
errors. Our insights can help inform design of instruction-dense prompts in
real-world applications and highlight important performance-latency tradeoffs.
We open-source the benchmark and all results for further analysis at
https://distylai.github.io/IFScale.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [37] [Device-Level Optimization Techniques for Solid-State Drives: A Survey](https://arxiv.org/abs/2507.10573)
*Tianyu Ren,Yajuan Du,Jinhua Cui,Yina Lv,Qiao Li,Chun Jason Xue*

Main category: cs.AR

TL;DR: The paper surveys SSD architecture, challenges, and optimization techniques, proposing future directions for research.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the growing challenges SSDs face with scalability, endurance, latency, and security in today's advanced storage demands.

Method: The authors analyze fundamental SSD components, discuss major challenges, and review advanced optimization techniques and emerging architectures.

Result: Provides a comprehensive overview of SSD technology, highlights existing issues, and proposes research areas for next-generation SSD advancements.

Conclusion: This survey seeks to inspire solutions that balance performance, longevity, and security for SSDs in evolving storage ecosystems.

Abstract: Solid-state drives (SSDs) have revolutionized data storage with their high
performance, energy efficiency, and reliability. However, as storage demands
grow, SSDs face critical challenges in scalability, endurance, latency, and
security. This survey provides a comprehensive analysis of SSD architecture,
key challenges, and device-level optimization techniques. We first examine the
fundamental components of SSDs, including NAND flash memory structures, SSD
controller functionalities (e.g., address mapping, garbage collection, wear
leveling), and host interface protocols (SATA, SAS, NVMe). Next, we discuss
major challenges such as reliability degradation, endurance limitations,
latency variations, and security threats (e.g., secure deletion, ransomware
defense). We then explore advanced optimization techniques, including error
correction mechanisms, flash translation layer (FTL) enhancements, and emerging
architectures like zoned namespace (ZNS) SSDs and flexible data placement
(FDP). Finally, we highlight open research challenges, such as QLC/PLC NAND
scalability, performance-reliability trade-offs, and SSD optimizations for
AI/LLM workloads. This survey aims to guide future research in developing
next-generation SSDs that balance performance, longevity, and security in
evolving storage ecosystems.

</details>


### [38] [SPICEAssistant: LLM using SPICE Simulation Tools for Schematic Design of Switched-Mode Power Supplies](https://arxiv.org/abs/2507.10639)
*Simon Nau,Jan Krummenauer,André Zimmermann*

Main category: cs.AR

TL;DR: The paper introduces SPICEAssistant, a framework combining large language models (LLMs) with SPICE simulation tools, to improve LLMs' performance in Switched-Mode Power Supply (SMPS) design tasks, showing a 38% improvement over standalone GPT-4.


<details>
  <summary>Details</summary>
Motivation: Large language models are versatile but have limitations in interpreting simulation results and managing complex multi-step processes in domains such as electronic design automation (EDA).

Method: The authors developed SPICEAssistant, an interface that allows LLMs to integrate with SPICE simulation tools to provide flexible and interactive feedback during the SMPS design process.

Result: SPICEAssistant demonstrated a 38% improvement in SMPS-related benchmark tests compared to standalone GPT-4, with iterative simulation feedback significantly enhancing performance.

Conclusion: Integrating SPICE simulation tools with LLMs significantly improves their practical application in designing electronic circuits, particularly for SMPS tasks.

Abstract: State-of-the-art large language models (LLMs) show high performance across a
wide range of tasks in many domains of science. In the field of electronic
design automation (EDA), it is yet to be determined to what extent they are
capable to understand, adapt, and dimension electronic circuits. This paper
focuses on the application of LLMs to switched-mode power supply (SMPS) design
on printed circuit boards (PCBs). Particular challenges for LLMs in this
context include their limited ability to interpret results from key simulation
tools like SPICE and the multi-step design process. To address these
challenges, we suggest SPICEAssistant, a framework that provides a broad
selection of tools to an LLM. The tools serve as an interface to SPICE,
allowing the LLM to interact flexibly with the simulator to estimate the impact
of its modifications to the circuit. To evaluate the performance of
SPICEAssistant, we defined a benchmark consisting of 256 questions testing the
ability to adapt circuit netlists to fulfil different SMPS design tasks. The
benchmarking results show that simulation feedback effectively improves SMPS
design capabilities of LLMs. An increasing number of simulation iterations
leads to enhanced performance. The SPICEAssistant framework significantly
outperforms the standalone LLM GPT-4o on the benchmark by approximately 38%.

</details>


### [39] [LASANA: Large-scale Surrogate Modeling for Analog Neuromorphic Architecture Exploration](https://arxiv.org/abs/2507.10748)
*Jason Ho,James A. Boyle,Linshen Liu,Andreas Gerstlauer*

Main category: cs.AR

TL;DR: LASANA introduces machine learning-based surrogate models to replace traditional slow analog simulation tools for neuromorphic systems and achieves significant speedup without compromising accuracy.


<details>
  <summary>Details</summary>
Motivation: Current approaches to simulate neuromorphic systems are slow and lack generalizability, preventing efficient co-design of architectures combining digital and analog computing.

Method: LASANA uses SPICE-level circuit simulations to train machine learning models for predicting energy, latency, and behavior in analog and digital interface components of neuromorphic architectures.

Result: LASANA achieves up to 1000x faster simulations compared to SPICE models, with errors under 7% for energy, 8% for latency, and 2% for behavior.

Conclusion: Machine learning-based surrogate models in LASANA provide an efficient and accurate solution for modeling neuromorphic architectures, enabling rapid exploration and validation of novel designs.

Abstract: Neuromorphic systems using in-memory or event-driven computing are motivated
by the need for more energy-efficient processing of artificial intelligence
workloads. Emerging neuromorphic architectures aim to combine traditional
digital designs with the computational efficiency of analog computing and novel
device technologies. A crucial problem in the rapid exploration and co-design
of such architectures is the lack of tools for fast and accurate modeling and
simulation. Typical mixed-signal design tools integrate a digital simulator
with an analog solver like SPICE, which is prohibitively slow for large
systems. By contrast, behavioral modeling of analog components is faster, but
existing approaches are fixed to specific architectures with limited energy and
performance modeling. In this paper, we propose LASANA, a novel approach that
leverages machine learning to derive data-driven surrogate models of analog
sub-blocks in a digital backend architecture. LASANA uses SPICE-level
simulations of a circuit to train ML models that predict circuit energy,
performance, and behavior at analog/digital interfaces. Such models can provide
energy and performance annotation on top of existing behavioral models or
function as replacements to analog simulation. We apply LASANA to an analog
crossbar array and a spiking neuron circuit. Running MNIST and spiking MNIST,
LASANA surrogates demonstrate up to three orders of magnitude speedup over
SPICE, with energy, latency, and behavioral error less than 7%, 8%, and 2%,
respectively.

</details>


### [40] [OpenGCRAM: An Open-Source Gain Cell Compiler Enabling Design-Space Exploration for AI Workloads](https://arxiv.org/abs/2507.10849)
*Xinxin Wang,Lixian Yan,Shuhan Liu,Luke Upton,Zhuoqi Cai,Yiming Tan,Shengman Li,Koustav Jana,Peijing Li,Jesse Cirimelli-Low,Thierry Tambe,Matthew Guthaus,H. -S. Philip Wong*

Main category: cs.AR

TL;DR: OpenGCRAM is an open-source GCRAM compiler that quickly generates optimized memory block designs with area, delay, and power simulations for diverse application needs.


<details>
  <summary>Details</summary>
Motivation: GCRAM offers advantages like higher density and lower power over SRAM, but designing and optimizing GCRAM memory systems remains time-consuming.

Method: The paper introduces OpenGCRAM, a compiler that automates the generation of GCRAM circuit designs and layouts, while enabling tailored simulations based on user configurations.

Result: OpenGCRAM provides DRC- and LVS-clean layouts for foundry CMOS, performs simulations, and significantly reduces design time while ensuring optimized memory blocks.

Conclusion: OpenGCRAM simplifies the design process, ensures compliance, and delivers customizable, performance-focused GCRAM sub-systems tailored to diverse workloads.

Abstract: Gain Cell memory (GCRAM) offers higher density and lower power than SRAM,
making it a promising candidate for on-chip memory in domain-specific
accelerators. To support workloads with varying traffic and lifetime metrics,
GCRAM also offers high bandwidth, ultra low leakage power and a wide range of
retention times, which can be adjusted through transistor design (like
threshold voltage and channel material) and on-the-fly by changing the
operating voltage. However, designing and optimizing GCRAM sub-systems can be
time-consuming. In this paper, we present OpenGCRAM, an open-source GCRAM
compiler capable of generating GCRAM bank circuit designs and DRC- and
LVS-clean layouts for commercially available foundry CMOS, while also providing
area, delay, and power simulations based on user-specified configurations
(e.g., word size and number of words). OpenGCRAM enables fast, accurate,
customizable, and optimized GCRAM block generation, reduces design time, ensure
process compliance, and delivers performance-tailored memory blocks that meet
diverse application requirements.

</details>


### [41] [Mapping Fusion: Improving FPGA Technology Mapping with ASIC Mapper](https://arxiv.org/abs/2507.10912)
*Cunxi Yu*

Main category: cs.AR

TL;DR: The paper introduces FuseMap, a framework for LUT mapping in FPGA logic synthesis, using reinforcement learning to enhance efficiency in mapping accuracy, delay, and area.


<details>
  <summary>Details</summary>
Motivation: To explore how ASIC technology mappers, traditionally separate from FPGA LUT mapping, can improve LUT mapping by working in an incremental manner.

Method: Introduces FuseMap, a framework using reinforcement learning to guide cell selection during LUT mapping.

Result: FuseMap demonstrates higher mapping accuracy, reduced delay, and area across benchmark circuits from ISCAS, ITC, VTR, and EPFL.

Conclusion: The proposed FuseMap framework improves FPGA LUT mapping by leveraging reinforcement learning, showing measurable benefits across a diverse set of circuit benchmarks.

Abstract: LUT (Look-Up Table) mapping is a critical step in FPGA logic synthesis, where
a logic network is transformed into a form that can be directly implemented
using the FPGA's LUTs. An FPGA LUT is a flexible digital memory structure that
can implement any logic function of a limited number of inputs, typically 4 to
6 inputs, depending on the FPGA architecture. The goal of LUT mapping is to map
the Boolean network into LUTs, where each LUT can implement any function with a
fixed number of inputs. In parallel to FPGA technology mapping, ASIC technology
mapping maps the Boolean network to user-defined standard cells, which has
traditionally been developed separately from LUT mapping algorithms. However,
in this work, our motivating examples demonstrate that ASIC technology mappers
can potentially improve the performance of LUT mappers, such that standard cell
mapping and LUT mapping work in an incremental manner.
  Therefore, we propose the FuseMap framework, which explores this opportunity
to improve LUT mapping in the FPGA design flow by utilizing reinforcement
learning to make design-specific choices during cell selection. The
effectiveness of FuseMap is evaluated on a wide range of benchmarks, different
technology libraries, and technology mappers. The experimental results
demonstrate that FuseMap achieves higher mapping accuracy while reducing delay
and area across diverse circuit designs collected from ISCAS 85/89, ITC/ISCAS
99, VTR 8.0, and EPFL benchmarks.

</details>


### [42] [Security Enclave Architecture for Heterogeneous Security Primitives for Supply-Chain Attacks](https://arxiv.org/abs/2507.10971)
*Kshitij Raj,Atri Chatterjee,Patanjali SLPSK,Swarup Bhunia,Sandip Ray*

Main category: cs.AR

TL;DR: CITADEL is a modular security framework for SoCs aimed at simplifying secure architecture design and addressing supply-chain threats, with minimal resource overhead.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of designing secure SoC architectures, which is complex, time-consuming, and prone to vulnerabilities due to minor architectural flaws.

Method: CITADEL provides a configurable, plug-and-play subsystem composed of custom IP blocks that allow developers to create security mechanisms tailored to specific threats.

Result: CITADEL was evaluated through real-world case studies and showed minimal impact on silicon area and power consumption across ASIC technologies.

Conclusion: CITADEL effectively streamlines the development of secure SoC architectures with low resource overhead, demonstrating adaptability to various security challenges like supply-chain threats.

Abstract: Designing secure architectures for system-on-chip (SoC) platforms is a highly
intricate and time-intensive task, often requiring months of development and
meticulous verification. Even minor architectural oversights can lead to
critical vulnerabilities that undermine the security of the entire chip. In
response to this challenge, we introduce CITADEL, a modular security framework
aimed at streamlining the creation of robust security architectures for SoCs.
CITADEL offers a configurable, plug-and-play subsystem composed of custom
intellectual property (IP) blocks, enabling the construction of diverse
security mechanisms tailored to specific threats. As a concrete demonstration,
we instantiate CITADEL to defend against supply-chain threats, illustrating how
the framework adapts to one of the most pressing concerns in hardware security.
This paper explores the range of obstacles encountered when building a unified
security architecture capable of addressing multiple attack vectors and
presents CITADEL's strategies for overcoming them. Through several real-world
case studies, we showcase the practical implementation of CITADEL and present a
thorough evaluation of its impact on silicon area and power consumption across
various ASIC technologies. Results indicate that CITADEL introduces only
minimal resource overhead, making it a practical solution for enhancing SoC
security.

</details>


### [43] [SystolicAttention: Fusing FlashAttention within a Single Systolic Array](https://arxiv.org/abs/2507.11331)
*Jiawei Lin,Guokai Chen,Yuanlong Li,Thomas Bourgeat*

Main category: cs.AR

TL;DR: The paper introduces "FSA," an enhanced systolic array architecture that optimizes FlashAttention operations by running them entirely on a single systolic array, achieving improved performance with minimal area overhead.


<details>
  <summary>Details</summary>
Motivation: The inefficiency of current systolic-array-based accelerators when executing FlashAttention—due to frequent interleaved matrix multiplication and softmax operations—motivated the need for an architecture that addresses performance degradation and resource contention.

Method: The paper proposes FSA, an enhanced systolic array architecture, and introduces "SystolicAttention," a fine-grained scheduling algorithm that integrates FlashAttention within the systolic array. This eliminates reliance on external vector units while maintaining numerical stability.

Result: FSA demonstrates 1.77x and 4.83x higher attention FLOPs/s utilization compared to AWS NeuronCore-v2 and Google TPUv5e, respectively, with only a 10% increase in area overhead.

Conclusion: The proposed FSA offers a significant performance improvement for FlashAttention operations on systolic arrays, making it a promising solution for transformer model acceleration.

Abstract: Transformer models rely heavily on scaled dot-product attention (SDPA),
typically implemented using the FlashAttention algorithm. However, current
systolic-array-based accelerators face significant challenges when executing
FlashAttention. Systolic arrays can only achieve high utilization for
consecutive and large matrix multiplications. In contrast, FlashAttention
requires frequently interleaved matrix multiplications and softmax operations.
  The frequent data swaps between the systolic array and external vector units
result in low systolic array utilization. This is further exacerbated by the
fact that softmax involves numerous non-matrix operations, which are not
well-suited for systolic arrays. Moreover, the concurrent execution of matrix
multiplication on systolic arrays and softmax on vector units leads to register
file and SRAM port contention, further degrading performance.
  To overcome these limitations, we propose FSA, an enhanced systolic array
architecture that enables the entire FlashAttention algorithm to run entirely
within a single systolic array, eliminating the need for external vector units.
At the core of FSA is SystolicAttention, a novel scheduling algorithm that maps
FlashAttention operations onto systolic arrays with fine-grained, element-wise
overlap. This significantly improves array utilization while preserving the
original floating-point operation order to maintain numerical stability.
  We implement FSA in synthesizable RTL and evaluate its performance against
state-of-the-art commercial accelerators. Our results show that FSA achieves
1.77x and 4.83x higher attention FLOPs/s utilization compared to AWS
NeuronCore-v2 and Google TPUv5e, respectively, with only about 10% area
overhead.

</details>


### [44] [Elk: Exploring the Efficiency of Inter-core Connected AI Chips with Deep Learning Compiler Techniques](https://arxiv.org/abs/2507.11506)
*Yiqi Liu,Yuqi Xue,Noelle Crawford,Jilong Xue,Jian Huang*

Main category: cs.AR

TL;DR: The paper introduces Elk, a DL compiler framework designed to optimize the efficiency of ICCA chips by balancing compute, communication, and I/O factors, achieving 94% of ideal performance.


<details>
  <summary>Details</summary>
Motivation: The growing demand for DL models requires efficient AI chips with optimized inter-core connections and off-chip memory access, but balancing compute, communication, and I/O remains challenging.

Method: The paper introduces Elk, which organizes compute, communication, and I/O factors into configurable parameters, incorporates inductive operator scheduling, and applies cost-aware on-chip memory allocation to optimize performance.

Result: Elk achieves 94% of the ideal roofline performance of ICCA chips and facilitates design space exploration for new chip architectures.

Conclusion: Elk demonstrates its ability to maximize efficiency in ICCA chips while supporting large DL models and guiding future hardware development.

Abstract: To meet the increasing demand of deep learning (DL) models, AI chips are
employing both off-chip memory (e.g., HBM) and high-bandwidth low-latency
interconnect for direct inter-core data exchange. However, it is not easy to
explore the efficiency of these inter-core connected AI (ICCA) chips, due to a
fundamental tussle among compute (per-core execution), communication
(inter-core data exchange), and I/O (off-chip data access).
  In this paper, we develop Elk, a DL compiler framework to maximize the
efficiency of ICCA chips by jointly trading off all the three performance
factors discussed above. Elk structures these performance factors into
configurable parameters and forms a global trade-off space in the DL compiler.
To systematically explore this space and maximize overall efficiency, Elk
employs a new inductive operator scheduling policy and a cost-aware on-chip
memory allocation algorithm. It generates globally optimized execution plans
that best overlap off-chip data loading and on-chip execution. To examine the
efficiency of Elk, we build a full-fledged emulator based on a real ICCA chip
IPU-POD4, and an ICCA chip simulator for sensitivity analysis with different
interconnect network topologies. Elk achieves 94% of the ideal roofline
performance of ICCA chips on average, showing the benefits of supporting large
DL models on ICCA chips. We also show Elk's capability of enabling architecture
design space exploration for new ICCA chip development.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [45] [Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos and influence opinions](https://arxiv.org/abs/2507.10577)
*Logé Cécile,Ghori Rehan*

Main category: cs.CL

TL;DR: This paper presents an AI-powered system for combating misinformation on YouTube by fact-checking claims and engaging users through persuasive comments using two agents: Truth Sleuth and Trend Bender.


<details>
  <summary>Details</summary>
Motivation: Misinformation is a growing concern in digital platforms, and traditional methods of addressing it are limited in reach and effectiveness.

Method: The paper introduces two agents, Truth Sleuth for claim verification using a Retrieval-Augmented Generation (RAG) approach, and Trend Bender for engaging users with well-crafted comments based on truth reports. A self-evaluation feedback loop enhances the system's outputs.

Result: Experiments on benchmark datasets and real-world YouTube deployment reveal high accuracy in fact-checking and the system's potential for meaningful user engagement.

Conclusion: AI-driven systems can effectively contend with misinformation, fostering informed discussions and enhancing digital communication spaces.

Abstract: Misinformation poses a significant threat in today's digital world, often
spreading rapidly through platforms like YouTube. This paper introduces a novel
approach to combating misinformation by developing an AI-powered system that
not only fact-checks claims made in YouTube videos but also actively engages
users in the comment section and challenge misleading narratives. Our system
comprises two main agents: Truth Sleuth and Trend Bender.
  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented
Generation (RAG) approach - drawing on sources like Wikipedia, Google Search,
Google FactCheck - to accurately assess their veracity and generates a nuanced
and comprehensive report. Through rigorous prompt engineering, Trend Bender
leverages this report along with a curated corpus of relevant articles to
generate insightful and persuasive comments designed to stimulate a productive
debate. With a carefully set up self-evaluation loop, this agent is able to
iteratively improve its style and refine its output.
  We demonstrate the system's capabilities through experiments on established
benchmark datasets and a real-world deployment on YouTube, showcasing its
potential to engage users and potentially influence perspectives. Our findings
highlight the high accuracy of our fact-checking agent, and confirm the
potential of AI-driven interventions in combating misinformation and fostering
a more informed online space.

</details>


### [46] [An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation](https://arxiv.org/abs/2507.10580)
*Vimaleswar A,Prabhu Nandan Sahu,Nilesh Kumar Sahu,Haroon R Lone*

Main category: cs.CL

TL;DR: The abstract discusses EmoSApp, an offline smartphone-based mental health app that uses fine-tuned Large Language Models for emotional support, ensuring privacy and accessibility.


<details>
  <summary>Details</summary>
Motivation: To provide mental health support while addressing issues like user accessibility, internet connectivity, and data privacy.

Method: Fine-tuned and quantized LLMs deployed on smartphones; training a specialized model (LLaMA-3.2-1B-Instruct) on a curated mental health dataset.

Result: Qualitative evaluations showed empathetic and coherent responses from the app; quantitative benchmarks demonstrated robust performance in low-resource scenarios.

Conclusion: EmoSApp emphasizes on-device capabilities, domain expertise, and privacy, paving the way for secure AI solutions in mental health.

Abstract: Mental health plays a crucial role in the overall well-being of an
individual. In recent years, digital platforms have been increasingly used to
expand mental health and emotional support. However, there are persistent
challenges related to limited user accessibility, internet connectivity, and
data privacy, which highlight the need for an offline, smartphone-based
solution. To address these challenges, we propose EmoSApp (Emotional Support
App): an entirely offline, smartphone-based conversational app designed for
mental health and emotional support. The system leverages Large Language Models
(LLMs), specifically fine-tuned, quantized and deployed using Torchtune and
Executorch for resource-constrained devices, allowing all inferences to occur
on the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned
the LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of
14,582 mental-health QA pairs, along with the multi-turn conversational data.
  Through qualitative human evaluation with the student population, we
demonstrate that EmoSApp has the ability to respond coherently, empathetically,
maintain interactive dialogue, and provide relevant suggestions to user's
mental health problems. Additionally, quantitative evaluations on nine standard
commonsense and reasoning benchmarks demonstrate the efficacy of our
fine-tuned, quantized model in low-resource settings. By prioritizing on-device
deployment and specialized domain adaptation, EmoSApp serves as a blueprint for
future innovations in portable, secure, and highly tailored AI-driven mental
health solutions.

</details>


### [47] [Transforming Sensitive Documents into Quantitative Data: An AI-Based Preprocessing Toolchain for Structured and Privacy-Conscious Analysis](https://arxiv.org/abs/2507.10582)
*Anders Ledberg,Anna Thalén*

Main category: cs.CL

TL;DR: This paper introduces a toolchain to preprocess sensitive and heterogeneous text data (like legal and medical texts) for embedding-based analysis, ensuring privacy and standardization using open-weight large language models.


<details>
  <summary>Details</summary>
Motivation: Research in public health and social sciences can benefit from analyzing unstructured text data (e.g., legal and medical sources), but challenges like privacy concerns and text heterogeneity hinder large-scale analysis.

Method: The toolchain utilizes large language models (LLMs) for summarization, standardization, translation, and anonymization through redaction and named entity recognition. It creates structured summaries and document embeddings for further analysis.

Result: The toolchain successfully processed 10,842 Swedish court documents, anonymizing data while preserving semantic content. Validation methods confirmed effective privacy protection and semantic retention.

Conclusion: The proposed toolchain facilitates privacy-friendly, large-scale analysis of textual data, expanding research possibilities in domains constrained by sensitive information and linguistic diversity.

Abstract: Unstructured text from legal, medical, and administrative sources offers a
rich but underutilized resource for research in public health and the social
sciences. However, large-scale analysis is hampered by two key challenges: the
presence of sensitive, personally identifiable information, and significant
heterogeneity in structure and language. We present a modular toolchain that
prepares such text data for embedding-based analysis, relying entirely on
open-weight models that run on local hardware, requiring only a
workstation-level GPU and supporting privacy-sensitive research.
  The toolchain employs large language model (LLM) prompting to standardize,
summarize, and, when needed, translate texts to English for greater
comparability. Anonymization is achieved via LLM-based redaction, supplemented
with named entity recognition and rule-based methods to minimize the risk of
disclosure. We demonstrate the toolchain on a corpus of 10,842 Swedish court
decisions under the Care of Abusers Act (LVM), comprising over 56,000 pages.
Each document is processed into an anonymized, standardized summary and
transformed into a document-level embedding. Validation, including manual
review, automated scanning, and predictive evaluation shows the toolchain
effectively removes identifying information while retaining semantic content.
As an illustrative application, we train a predictive model using embedding
vectors derived from a small set of manually labeled summaries, demonstrating
the toolchain's capacity for semi-automated content analysis at scale.
  By enabling structured, privacy-conscious analysis of sensitive documents,
our toolchain opens new possibilities for large-scale research in domains where
textual data was previously inaccessible due to privacy and heterogeneity
constraints.

</details>


### [48] [A Taxonomy for Design and Evaluation of Prompt-Based Natural Language Explanations](https://arxiv.org/abs/2507.10585)
*Isar Nejadgholi,Mona Omidyeganeh,Marc-Antoine Drouin,Jonathan Boisvert*

Main category: cs.CL

TL;DR: The paper proposes a taxonomy to improve Natural Language Explanations (NLEs) for transparent AI systems, highlighting structured dimensions of context, generation, and evaluation.


<details>
  <summary>Details</summary>
Motivation: To address the growing importance of transparent AI governance and the role of Natural Language Explanations (NLEs) in articulating AI system behavior.

Method: The authors build on Explainable AI (XAI) literature to create a taxonomy for NLEs using a three-dimensional structure: Context, Generation and Presentation, and Evaluation.

Result: The taxonomy provides clear guidelines and categories to characterize, design, and evaluate NLEs, aiding stakeholders in AI governance.

Conclusion: The proposed taxonomy equips researchers, auditors, and policymakers with a comprehensive framework to ensure more effective and transparent AI system explanations.

Abstract: Effective AI governance requires structured approaches for stakeholders to
access and verify AI system behavior. With the rise of large language models,
Natural Language Explanations (NLEs) are now key to articulating model
behavior, which necessitates a focused examination of their characteristics and
governance implications. We draw on Explainable AI (XAI) literature to create
an updated XAI taxonomy, adapted to prompt-based NLEs, across three dimensions:
(1) Context, including task, data, audience, and goals; (2) Generation and
Presentation, covering generation methods, inputs, interactivity, outputs, and
forms; and (3) Evaluation, focusing on content, presentation, and user-centered
properties, as well as the setting of the evaluation. This taxonomy provides a
framework for researchers, auditors, and policymakers to characterize, design,
and enhance NLEs for transparent AI systems.

</details>


### [49] [AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight Adapters](https://arxiv.org/abs/2507.10586)
*Kaushik Dwivedi,Padmanabh Patanjali Mishra*

Main category: cs.CL

TL;DR: AutoRAG-LoRA is a framework addressing hallucination in large language models using LoRA adapters, hybrid retrieval, and KL-regularized training, achieving grounded and factually accurate responses.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the vulnerability of large language models to hallucinations, which are factual inaccuracies that undermine their trustworthiness in real-world applications.

Method: The framework integrates prompt rewriting, hybrid retrieval, lightweight LoRA adapters, KL-regularized training, and a hallucination detection module incorporating self-evaluation and classifier-based techniques. A feedback correction loop ensures factual alignment.

Result: AutoRAG-LoRA significantly reduces factual inaccuracies (hallucinations) in large language model outputs while maintaining model efficiency and modularity.

Conclusion: The proposed framework effectively mitigates hallucination issues in large language models, offering a practical approach to improve factual consistency in real-world deployments.

Abstract: Large Language Models (LLMs) have demonstrated remarkable fluency across a
range of natural language tasks, yet remain vulnerable to hallucinations -
factual inaccuracies that undermine trust in real world deployment. We present
AutoRAG-LoRA, a modular framework for Retrieval-Augmented Generation (RAG) that
tackles hallucination in large language models through lightweight LoRA-based
adapters and KL-regularized training. Our pipeline integrates automated prompt
rewriting, hybrid retrieval, and low-rank adapter tuning to ground responses in
retrieved evidence. A hallucination detection module, using both
classifier-based and self-evaluation techniques, assigns confidence scores to
generated outputs, triggering an optional feedback correction loop. This loop
enforces factual alignment via contrastive KL loss and adapter fine tuning. We
demonstrate that AutoRAG-LoRA significantly reduces the factual drift while
preserving the efficiency and modularity of the model.

</details>


### [50] [Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing](https://arxiv.org/abs/2507.10587)
*Dennis Ulmer,Alexandra Lorson,Ivan Titov,Christian Hardmeier*

Main category: cs.CL

TL;DR: This paper explores how large language models (LLMs) can more effectively communicate uncertainty to users by mimicking the nuances of human uncertainty communication.


<details>
  <summary>Details</summary>
Motivation: LLMs are often overconfident in their outputs, making their perceived trustworthiness questionable. To enhance human-machine collaboration and prevent harm, it's critical to effectively signal and communicate uncertainty in a way that integrates well into language-based interactions.

Method: The authors provide a comprehensive overview of research on human uncertainty communication, survey current NLP research, and conduct additional analyses to expose biases in how verbalized uncertainty has been handled so far.

Result: The paper identifies overlooked biases in LLMs' verbalized uncertainty communication and defines unique factors involved in human-machine uncertainty interactions.

Conclusion: Effective uncertainty communication requires anthropomimetic uncertainty—emulating the linguistic nuances and personalization of human communication. The paper outlines future research directions to achieve this in NLP models.

Abstract: Human users increasingly rely on natural language interactions with large
language models (LLMs) in order to receive help on a large variety of tasks and
problems. However, the trustworthiness and perceived legitimacy of LLMs is
undermined by the fact that their output is frequently stated in very confident
terms, even when its accuracy is questionable. Therefore, there is a need to
signal the confidence of the language model to a user in order to reap the
benefits of human-machine collaboration and mitigate potential harms.
Verbalized uncertainty is the expression of confidence with linguistic means,
an approach that integrates perfectly into language-based interfaces.
Nevertheless, most recent research in natural language processing (NLP)
overlooks the nuances surrounding human uncertainty communication and the data
biases that influence machine uncertainty communication. We argue for
anthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty
communication requires a degree of linguistic authenticity and personalization
to the user, which could be achieved by emulating human communication. We
present a thorough overview over the research in human uncertainty
communication, survey ongoing research, and perform additional analyses to
demonstrate so-far overlooked biases in verbalized uncertainty. We conclude by
pointing out unique factors in human-machine communication of uncertainty and
deconstruct anthropomimetic uncertainty into future research directions for
NLP.

</details>


### [51] [PLEX: Perturbation-free Local Explanations for LLM-Based Text Classification](https://arxiv.org/abs/2507.10596)
*Yogachandran Rahulamathavan,Misbah Farooq,Varuna De Silva*

Main category: cs.CL

TL;DR: The paper introduces PLEX, a perturbation-free method for explainable text classification using LLMs, achieving comparable results to LIME and SHAP while reducing computational costs significantly.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational inefficiency of XAI methods like LIME and SHAP, which require extensive perturbations for generating explanations, making them infeasible for complex models like LLMs.

Method: The proposed method, PLEX, uses contextual embeddings from LLMs and a Siamese-style neural network to generate feature importance scores, eliminating the need for perturbations.

Result: PLEX achieves over 92% agreement with LIME and SHAP across classification tasks, accurately identifies impactful features during stress tests, and significantly reduces computational time and overhead.

Conclusion: PLEX represents an efficient, accurate, and computation-friendly solution for explainable text classification in LLMs, outperforming traditional methods in certain scenarios.

Abstract: Large Language Models (LLMs) excel in text classification, but their
complexity hinders interpretability, making it difficult to understand the
reasoning behind their predictions. Explainable AI (XAI) methods like LIME and
SHAP offer local explanations by identifying influential words, but they rely
on computationally expensive perturbations. These methods typically generate
thousands of perturbed sentences and perform inferences on each, incurring a
substantial computational burden, especially with LLMs. To address this, we
propose \underline{P}erturbation-free \underline{L}ocal \underline{Ex}planation
(PLEX), a novel method that leverages the contextual embeddings extracted from
the LLM and a ``Siamese network" style neural network trained to align with
feature importance scores. This one-off training eliminates the need for
subsequent perturbations, enabling efficient explanations for any new sentence.
We demonstrate PLEX's effectiveness on four different classification tasks
(sentiment, fake news, fake COVID-19 news and depression), showing more than
92\% agreement with LIME and SHAP. Our evaluation using a ``stress test"
reveals that PLEX accurately identifies influential words, leading to a similar
decline in classification accuracy as observed with LIME and SHAP when these
words are removed. Notably, in some cases, PLEX demonstrates superior
performance in capturing the impact of key features. PLEX dramatically
accelerates explanation, reducing time and computational overhead by two and
four orders of magnitude, respectively. This work offers a promising solution
for explainable LLM-based text classification.

</details>


### [52] [Emergence of Hierarchical Emotion Organization in Large Language Models](https://arxiv.org/abs/2507.10599)
*Bo Zhao,Maya Okawa,Eric J. Bigelow,Rose Yu,Tomer Ullman,Ekdeep Singh Lubana,Hidenori Tanaka*

Main category: cs.CL

TL;DR: This paper examines how large language models (LLMs) understand and structure emotional states, identifying hierarchical similarities with human psychology and biases in emotion recognition across diverse personas.


<details>
  <summary>Details</summary>
Motivation: Understanding how LLMs model user emotions is vital for ensuring ethical usage and enhancing their alignment with human cognition.

Method: The analysis involved examining probabilistic dependencies in LLM outputs, comparing hierarchical emotion trees to human psychological models, and assessing emotion recognition biases across different socioeconomic personas.

Result: LLMs naturally form hierarchies of emotions resembling human psychological models, while larger models exhibit more complex hierarchies. Biases in emotion recognition were identified, particularly impacting underrepresented groups.

Conclusion: LLMs reveal emergent emotional understanding akin to human perception, and cognitively-grounded frameworks could guide improved model evaluations.

Abstract: As large language models (LLMs) increasingly power conversational agents,
understanding how they model users' emotional states is critical for ethical
deployment. Inspired by emotion wheels -- a psychological framework that argues
emotions organize hierarchically -- we analyze probabilistic dependencies
between emotional states in model outputs. We find that LLMs naturally form
hierarchical emotion trees that align with human psychological models, and
larger models develop more complex hierarchies. We also uncover systematic
biases in emotion recognition across socioeconomic personas, with compounding
misclassifications for intersectional, underrepresented groups. Human studies
reveal striking parallels, suggesting that LLMs internalize aspects of social
perception. Beyond highlighting emergent emotional reasoning in LLMs, our
results hint at the potential of using cognitively-grounded theories for
developing better model evaluations.

</details>


### [53] [Language Models for Adult Service Website Text Analysis](https://arxiv.org/abs/2507.10743)
*Nickolas Freeman,Thanh Nguyen,Gregory Bott,Jason Parton,Collin Francel*

Main category: cs.CL

TL;DR: This paper addresses challenges in analyzing Adult Service Website (ASW) text linked to sex trafficking and proposes efficient, custom transformer models that outperform standard pre-trained models in a variety of applications.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to effectively analyze ASW text data for identifying potential sex trafficking victims, as existing methods face challenges like emojis, poor grammar, and deliberate obfuscation.

Method: The paper evaluates language modeling approaches, including pre-trained transformers and custom models, and develops efficient custom transformer models capable of handling ASW-specific text characteristics.

Result: The proposed custom models outperform standard encoder-only transformers (e.g., BERT-base, RoBERTa) in accuracy, recall, F1 score, and ROC AUC metrics.

Conclusion: The study showcases significant advancements in ASW text analysis through custom models, which can support tasks like graph decomposition, clustering, and emoji usage understanding, aiding in anti-sex trafficking efforts.

Abstract: Sex trafficking refers to the use of force, fraud, or coercion to compel an
individual to perform in commercial sex acts against their will. Adult service
websites (ASWs) have and continue to be linked to sex trafficking, offering a
platform for traffickers to advertise their victims. Thus, organizations
involved in the fight against sex trafficking often use ASW data when
attempting to identify potential sex trafficking victims. A critical challenge
in transforming ASW data into actionable insight is text analysis. Previous
research using ASW data has shown that ASW ad text is important for linking
ads. However, working with this text is challenging due to its extensive use of
emojis, poor grammar, and deliberate obfuscation to evade law enforcement
scrutiny. We conduct a comprehensive study of language modeling approaches for
this application area, including simple information retrieval methods,
pre-trained transformers, and custom transformer models. We demonstrate that
characteristics of ASW text data allow efficient custom transformer models to
be trained with relatively small GPU resources and used efficiently for
inference on consumer hardware. Our custom models outperform fine-tuned
variants of well-known encoder-only transformer models, including BERT-base,
RoBERTa, and ModernBERT, on accuracy, recall, F1 score, and ROC AUC. We
demonstrate the use of our best-performing custom configuration on three tasks
related to ASW data analysis: (i) decomposing the giant component in a graph
representation of ASW data, (ii) clustering ASW ad text, and (iii) using the
learned token embeddings to understand the use of emojis in the illicit context
we study. The models we develop represent a significant advancement in ASW text
analysis, which can be leveraged in a variety of downstream applications and
research.

</details>


### [54] [Applying Text Embedding Models for Efficient Analysis in Labeled Property Graphs](https://arxiv.org/abs/2507.10772)
*Michal Podstawski*

Main category: cs.CL

TL;DR: The paper explores leveraging text embedding models to enhance semantic analysis and predictive tasks in labeled property graphs by embedding textual attributes.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve the contextual understanding and analytical capabilities of labeled property graphs by utilizing rich textual attributes.

Method: They propose integrating pretrained text embedding models into graph pipelines without altering the graph's structure to embed textual node and edge properties.

Result: The integration of textual semantics significantly improves the accuracy and interpretability of tasks such as node classification and relation prediction in property graphs.

Conclusion: Textual semantics can enhance analytical tasks in labeled property graphs, supporting efficient and accurate semantic analysis through embeddings.

Abstract: Labeled property graphs often contain rich textual attributes that can
enhance analytical tasks when properly leveraged. This work explores the use of
pretrained text embedding models to enable efficient semantic analysis in such
graphs. By embedding textual node and edge properties, we support downstream
tasks including node classification and relation prediction with improved
contextual understanding. Our approach integrates language model embeddings
into the graph pipeline without altering its structure, demonstrating that
textual semantics can significantly enhance the accuracy and interpretability
of property graph analysis.

</details>


### [55] [Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers](https://arxiv.org/abs/2507.10787)
*Yilun Zhao,Chengye Wang,Chuhan Li,Arman Cohan*

Main category: cs.CL

TL;DR: The paper presents MISS-QA, a benchmark for evaluating models' ability to interpret schematic diagrams in scientific literature, featuring 1,500 annotated examples from 465 papers.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of assessing model performance in interpreting diagrams and answering contextual questions in scientific papers.

Method: Creating a benchmark (MISS-QA) with annotated examples and testing 18 multimodal foundation models' capabilities.

Result: Current models show a substantial performance gap compared to human experts and exhibit limitations highlighted by error analysis.

Conclusion: MISS-QA serves as a tool to analyze and improve models' comprehension of multimodal scientific documents.

Abstract: This paper introduces MISS-QA, the first benchmark specifically designed to
evaluate the ability of models to interpret schematic diagrams within
scientific literature. MISS-QA comprises 1,500 expert-annotated examples over
465 scientific papers. In this benchmark, models are tasked with interpreting
schematic diagrams that illustrate research overviews and answering
corresponding information-seeking questions based on the broader context of the
paper. We assess the performance of 18 frontier multimodal foundation models,
including o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant
performance gap between these models and human experts on MISS-QA. Our analysis
of model performance on unanswerable questions and our detailed error analysis
further highlight the strengths and limitations of current models, offering key
insights to enhance models in comprehending multimodal scientific literature.

</details>


### [56] [Testing Hypotheses from the Social Approval Theory of Online Hate: An Analysis of 110 Million Posts from Parler](https://arxiv.org/abs/2507.10810)
*David M. Markowitz,Samuel Hardman Taylor*

Main category: cs.CL

TL;DR: The study investigates the impact of social approval on online hate using posts on Parler. Results show no consistent relationship between upvotes on hate messages and subsequent hate speech.


<details>
  <summary>Details</summary>
Motivation: To explore whether social approval drives online hate speech and to test Walther's social approval theory of online hate.

Method: Analyzed over 110 million posts from the social media platform Parler (2018-2021) to test hypothesized relationships between upvotes and hate speech production.

Result: No consistent evidence was found that social approval (upvotes) leads to more or more extreme hate speech in subsequent posts or over time.

Conclusion: Social approval mechanisms might function differently on niche platforms like Parler, challenging generalized assumptions about reinforcement of online hate.

Abstract: In this paper, we explored how online hate is motivated by receiving social
approval from others. We specifically examined two central tenets of Walther's
(2024) social approval theory of online hate: (H1a) more signals of social
approval on hate messages predicts more subsequent hate messages, and (H1b) as
social approval increases, hate speech messages become more extreme. Using over
110 million posts from Parler (2018-2021), we observed that the number of
upvotes a person received on a hate speech post was unassociated with the
amount of hate speech in their next post and posts during the next week, month,
three months, and six months. Between-person effects revealed an average
negative relationship between social approval and hate speech production at the
post level, but this relationship was mixed at other time intervals. Social
approval reinforcement mechanisms of online hate may operate differently on
niche social media platforms.

</details>


### [57] [LLMs on Trial: Evaluating Judicial Fairness for Large Language Models](https://arxiv.org/abs/2507.10852)
*Yiran Hu,Zongyue Xue,Haitao Li,Siyuan Zheng,Qingjing Chen,Shaochun Wang,Xihan Zhang,Ning Zheng,Yun Liu,Qingyao Ai,Yiqun Liu,Charles L. A. Clarke,Weixing Shen*

Main category: cs.CL

TL;DR: The paper examines judicial fairness in Large Language Models (LLMs) using the JudiFair dataset and finds widespread inconsistencies and biases, especially on demographic labels. It proposes new metrics and a toolkit for future research.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore how fair and trustworthy LLMs are when used in judicial contexts, as their decisions can significantly impact rights and equity.

Method: The paper develops a fairness measurement framework based on judicial fairness theories, compiles the JudiFair dataset, and introduces metrics — inconsistency, bias, and imbalanced inaccuracy — to evaluate LLM fairness across 161 values.

Result: Experiments with 16 LLMs reveal significant judicial unfairness, biases on demographic labels, and a complex relationship where high accuracy increases bias. Model size, origin, and release date had minimal impact, while temperature adjustments affected fairness.

Conclusion: The study highlights serious fairness challenges in LLMs for judicial tasks and provides datasets, metrics, and tools to aid future fairness research.

Abstract: Large Language Models (LLMs) are increasingly used in high-stakes fields
where their decisions impact rights and equity. However, LLMs' judicial
fairness and implications for social justice remain underexplored. When LLMs
act as judges, the ability to fairly resolve judicial issues is a prerequisite
to ensure their trustworthiness. Based on theories of judicial fairness, we
construct a comprehensive framework to measure LLM fairness, leading to a
selection of 65 labels and 161 corresponding values. Applying this framework to
the judicial system, we compile an extensive dataset, JudiFair, comprising
177,100 unique case facts. To achieve robust statistical inference, we develop
three evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and
introduce a method to assess the overall fairness of multiple LLMs across
various labels. Through experiments with 16 LLMs, we uncover pervasive
inconsistency, bias, and imbalanced inaccuracy across models, underscoring
severe LLM judicial unfairness. Particularly, LLMs display notably more
pronounced biases on demographic labels, with slightly less bias on substance
labels compared to procedure ones. Interestingly, increased inconsistency
correlates with reduced biases, but more accurate predictions exacerbate
biases. While we find that adjusting the temperature parameter can influence
LLM fairness, model size, release date, and country of origin do not exhibit
significant effects on judicial fairness. Accordingly, we introduce a publicly
available toolkit containing all datasets and code, designed to support future
research in evaluating and improving LLM fairness.

</details>


### [58] [How Stylistic Similarity Shapes Preferences in Dialogue Dataset with User and Third Party Evaluations](https://arxiv.org/abs/2507.10918)
*Ikumi Numaya,Shoji Moriya,Shiki Sato,Reina Akama,Jun Suzuki*

Main category: cs.CL

TL;DR: The paper introduces a dataset to study the relationship between subjective and objective stylistic similarity in dialogue systems and user preferences, finding a strong correlation between subjective perceptions and preference.


<details>
  <summary>Details</summary>
Motivation: The study aims to analyze the role of stylistic similarity in improving human-bot dialogue interactions and to differentiate between subjective (user-perceived) and objective (third-party-evaluated) stylistic similarity.

Method: A novel dataset is created to track users' preferences, subjective stylistic similarity as perceived by users, and objective stylistic similarity as determined by third parties in open-domain dialogue scenarios.

Result: The study found a strong positive correlation between subjective stylistic similarity and user preference. It also revealed key differences between subjective and objective stylistic similarity evaluations.

Conclusion: Understanding the distinction between subjective and objective stylistic similarity is critical for enhancing dialogue systems. The presented dataset provides a foundation for further research in this area.

Abstract: Recent advancements in dialogue generation have broadened the scope of
human-bot interactions, enabling not only contextually appropriate responses
but also the analysis of human affect and sensitivity. While prior work has
suggested that stylistic similarity between user and system may enhance user
impressions, the distinction between subjective and objective similarity is
often overlooked. To investigate this issue, we introduce a novel dataset that
includes users' preferences, subjective stylistic similarity based on users'
own perceptions, and objective stylistic similarity annotated by third party
evaluators in open-domain dialogue settings. Analysis using the constructed
dataset reveals a strong positive correlation between subjective stylistic
similarity and user preference. Furthermore, our analysis suggests an important
finding: users' subjective stylistic similarity differs from third party
objective similarity. This underscores the importance of distinguishing between
subjective and objective evaluations and understanding the distinct aspects
each captures when analyzing the relationship between stylistic similarity and
user preferences. The dataset presented in this paper is available online.

</details>


### [59] [HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented Pre-Training](https://arxiv.org/abs/2507.10920)
*Seungho Choi*

Main category: cs.CL

TL;DR: The paper introduces HanjaBridge, a method to address semantic ambiguity in low-resource languages like Korean, improving language model performance by incorporating Hanja (Chinese characters).


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with performance in low-resource languages like Korean due to issues such as semantic ambiguity in homophonous Sino-Korean words. The study aims to improve LLM performance in these languages through better contextual understanding and overcoming linguistic challenges.

Method: The authors propose HanjaBridge, a continual pre-training framework that presents LLMs with multiple Hanja candidates for homographs, promoting contextual disambiguation. They also apply token-level knowledge distillation to prevent catastrophic forgetting.

Result: Experimental findings show HanjaBridge boosts Korean language understanding by 21% on the KoBALT benchmark. It also facilitates positive cross-lingual transfer with Chinese without additional inference costs.

Conclusion: HanjaBridge effectively addresses the semantic challenges of homophonous words in Korean, improving LLM performance while being efficient at inference time. Its reinforcement of Korean-Chinese semantic alignment further broadens its applicability.

Abstract: Large language models (LLMs) often show poor performance in low-resource
languages like Korean, partly due to unique linguistic challenges such as
homophonous Sino-Korean words that are indistinguishable in Hangul script. To
address this semantic ambiguity, we propose HanjaBridge, a novel
meaning-injection technique integrated into a continual pre-training (CPT)
framework. Instead of deterministically mapping a word to a single Hanja
(Chinese character), HanjaBridge presents the model with all possible Hanja
candidates for a given homograph, encouraging the model to learn contextual
disambiguation. This process is paired with token-level knowledge distillation
to prevent catastrophic forgetting. Experimental results show that HanjaBridge
significantly improves Korean language understanding, achieving a 21\% relative
improvement on the KoBALT benchmark. Notably, by reinforcing semantic alignment
between Korean and Chinese through shared Hanja, we observe a strong positive
cross-lingual transfer. Furthermore, these gains persist even when Hanja
augmentation is omitted at inference time, ensuring practical efficiency with
no additional run-time cost.

</details>


### [60] [Modeling Understanding of Story-Based Analogies Using Large Language Models](https://arxiv.org/abs/2507.10957)
*Kalit Inani,Keshav Kabra,Vijay Marupudi,Sashank Varma*

Main category: cs.CL

TL;DR: The paper evaluates the analogical reasoning abilities of Large Language Models (LLMs) by comparing them to human performance on a story-based analogical mapping task, assessing both their semantic representation of analogies and their reasoning capabilities when explicitly prompted to explain.


<details>
  <summary>Details</summary>
Motivation: The study aims to investigate how closely the analogical reasoning abilities of LLMs align with human reasoning and to address limitations in prior research, which primarily focused on overall accuracy rather than fine-grained analogical reasoning.

Method: The authors conducted experiments using LLMs of varying sizes (8B and 70B parameters) and architectures (e.g., GPT-4 and LLaMA3). They evaluated the LLMs' semantic representation of analogies by analyzing the embeddings' similarity between source and target texts, and they tested the effectiveness of explicit prompts to enhance reasoning performance.

Result: The study revealed insights into the performance of LLMs on analogical tasks, including variations across model sizes and architectures, and highlighted whether their reasoning abilities are comparable to human cognitive patterns on individual analogies.

Conclusion: This research deepens the understanding of LLMs' potential as models for human reasoning and their ability to perform analogical reasoning, emphasizing their limitations and differences compared to human cognition.

Abstract: Recent advancements in Large Language Models (LLMs) have brought them closer
to matching human cognition across a variety of tasks. How well do these models
align with human performance in detecting and mapping analogies? Prior research
has shown that LLMs can extract similarities from analogy problems but lack
robust human-like reasoning. Building on Webb, Holyoak, and Lu (2023), the
current study focused on a story-based analogical mapping task and conducted a
fine-grained evaluation of LLM reasoning abilities compared to human
performance. First, it explored the semantic representation of analogies in
LLMs, using sentence embeddings to assess whether they capture the similarity
between the source and target texts of an analogy, and the dissimilarity
between the source and distractor texts. Second, it investigated the
effectiveness of explicitly prompting LLMs to explain analogies. Throughout, we
examine whether LLMs exhibit similar performance profiles to those observed in
humans by evaluating their reasoning at the level of individual analogies, and
not just at the level of overall accuracy (as prior studies have done). Our
experiments include evaluating the impact of model size (8B vs. 70B parameters)
and performance variation across state-of-the-art model architectures such as
GPT-4 and LLaMA3. This work advances our understanding of the analogical
reasoning abilities of LLMs and their potential as models of human reasoning.

</details>


### [61] [DS@GT at eRisk 2025: From prompts to predictions, benchmarking early depression detection with conversational agent based assessments and temporal attention models](https://arxiv.org/abs/2507.10958)
*Anthony Miyaguchi,David Guecha,Yuwen Chiu,Sidharth Gaur*

Main category: cs.CL

TL;DR: The DS@GT team participated in two eRisk 2025 challenges, focusing on depression detection using LLMs and evaluated performance metrics.


<details>
  <summary>Details</summary>
Motivation: To explore the effectiveness of using prompt-engineered LLMs for conversational depression detection based on BDI-II criteria.

Method: Prompt-engineering strategy with diverse LLMs for BDI-II-based assessments, producing JSON outputs and analyzing cross-model agreement and internal consistency.

Result: Achieved second place on the leaderboard with performance metrics: DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27.

Conclusion: Prompt-engineered LLMs show promise for depression detection, with outputs aligned to clinical criteria and potential for analyzing conversational cues.

Abstract: This Working Note summarizes the participation of the DS@GT team in two eRisk
2025 challenges. For the Pilot Task on conversational depression detection with
large language-models (LLMs), we adopted a prompt-engineering strategy in which
diverse LLMs conducted BDI-II-based assessments and produced structured JSON
outputs. Because ground-truth labels were unavailable, we evaluated cross-model
agreement and internal consistency. Our prompt design methodology aligned model
outputs with BDI-II criteria and enabled the analysis of conversational cues
that influenced the prediction of symptoms. Our best submission, second on the
official leaderboard, achieved DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27.

</details>


### [62] [Teach Me Sign: Stepwise Prompting LLM for Sign Language Production](https://arxiv.org/abs/2507.10972)
*Zhaoyi An,Rei Kawakami*

Main category: cs.CL

TL;DR: The paper introduces TEAM-Sign, a method for leveraging large language models (LLMs) to generate sign language by treating it as another natural language through fine-tuning and a stepwise prompting strategy.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limited impact of large language models on sign language generation, despite their success in other AI tasks, due to the complexity and unique rules of sign language.

Method: TEAM-Sign fine-tunes large language models and employs a stepwise prompting strategy to extract and leverage sign language knowledge, aligning the differences between spoken and sign languages.

Result: Experiments on How2Sign and Phoenix14T datasets demonstrate TEAM-Sign's ability to align the distributions and grammatical rules of sign and spoken languages effectively.

Conclusion: TEAM-Sign bridges the gap between spoken and sign languages by utilizing LLMs' reasoning capabilities and adapting them to generate comprehensible and accurate sign language representations.

Abstract: Large language models, with their strong reasoning ability and rich
knowledge, have brought revolution to many tasks of AI, but their impact on
sign language generation remains limited due to its complexity and unique
rules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign
language as another natural language. By fine-tuning an LLM, we enable it to
learn the correspondence between text and sign language, and facilitate
generation. Considering the differences between sign and spoken language, we
employ a stepwise prompting strategy to extract the inherent sign language
knowledge within the LLM, thereby supporting the learning and generation
process. Experimental results on How2Sign and Phoenix14T datasets demonstrate
that our approach effectively leverages both the sign language knowledge and
reasoning capabilities of LLM to align the different distribution and
grammatical rules between sign and spoken language.

</details>


### [63] [Mario at EXIST 2025: A Simple Gateway to Effective Multilingual Sexism Detection](https://arxiv.org/abs/2507.10996)
*Lin Tian,Johanne R. Trippas,Marian-Andrei Rizoiu*

Main category: cs.CL

TL;DR: The study proposes a hierarchical Low-Rank Adaptation (LoRA) approach to sexism detection in English and Spanish tweets, leveraging a lightweight training strategy with significant performance and efficiency gains.


<details>
  <summary>Details</summary>
Motivation: The paper aims to develop an efficient and effective solution for text-based sexism detection across multilingual contexts in a hierarchical structure, addressing tasks like binary identification, intention detection, and categorization.

Method: The researchers applied hierarchical LoRA to Llama 3.1 8B, introducing a unique conditional adapter routing that handles label dependencies across subtasks. They employed parameter-efficient fine-tuning and multilingual training instead of traditional computationally intensive systems.

Result: The approach achieved improved F1 scores (1.7-2.4%) through cross-lingual transfer, reduced training time by 75%, and minimized model storage requirements by 98%, while maintaining competitive task performance across subtasks.

Conclusion: Hierarchical LoRA with multilingual training provides an effective, fast, and resource-efficient solution for sexism detection across languages, eliminating the need for language-specific models and extensive pre-processing.

Abstract: This paper presents our approach to EXIST 2025 Task 1, addressing text-based
sexism detection in English and Spanish tweets through hierarchical Low-Rank
Adaptation (LoRA) of Llama 3.1 8B. Our method introduces conditional adapter
routing that explicitly models label dependencies across three hierarchically
structured subtasks: binary sexism identification, source intention detection,
and multilabel sexism categorization. Unlike conventional LoRA applications
that target only attention layers, we apply adaptation to all linear
transformations, enhancing the model's capacity to capture task-specific
patterns. In contrast to complex data processing and ensemble approaches, we
show that straightforward parameter-efficient fine-tuning achieves strong
performance. We train separate LoRA adapters (rank=16, QLoRA 4-bit) for each
subtask using unified multilingual training that leverages Llama 3.1's native
bilingual capabilities. The method requires minimal preprocessing and uses
standard supervised learning. Our multilingual training strategy eliminates the
need for separate language-specific models, achieving 1.7-2.4\% F1 improvements
through cross-lingual transfer. With only 1.67\% trainable parameters compared
to full fine-tuning, our approach reduces training time by 75\% and model
storage by 98\%, while achieving competitive performance across all subtasks
(ICM-Hard: 0.6774 for binary classification, 0.4991 for intention detection,
0.6519 for multilabel categorization).

</details>


### [64] [Team HUMANE at AVeriTeC 2025: HerO 2 for Efficient Fact Verification](https://arxiv.org/abs/2507.11004)
*Yejun Yoon,Jaeyoon Jung,Seunghyun Yoon,Kunwoo Park*

Main category: cs.CL

TL;DR: HerO 2, a system for a fact-verification task, ranked second while achieving the fastest runtime among top competitors.


<details>
  <summary>Details</summary>
Motivation: To improve open-source models for fact verification by enhancing evidence quality, optimizing predictions, and improving computational efficiency.

Method: HerO 2 introduces document summarization, answer reformulation, post-training quantization, and updated language model backbones.

Result: It achieved second place on the leaderboard and had the shortest runtime among the top-three systems.

Conclusion: HerO 2 demonstrates high efficiency, solid fact-verification ability, and real-world applicability.

Abstract: This paper presents HerO 2, Team HUMANE's system for the AVeriTeC shared task
at the FEVER-25 workshop. HerO 2 is an enhanced version of HerO, the
best-performing open-source model from the previous year's challenge. It
improves evidence quality through document summarization and answer
reformulation, optimizes veracity prediction via post-training quantization
under computational constraints, and enhances overall system performance by
integrating updated language model (LM) backbones. HerO 2 ranked second on the
leaderboard while achieving the shortest runtime among the top three systems,
demonstrating both high efficiency and strong potential for real-world fact
verification. The code is available at https://github.com/ssu-humane/HerO2.

</details>


### [65] [Journalism-Guided Agentic In-Context Learning for News Stance Detection](https://arxiv.org/abs/2507.11049)
*Dahyun Lee,Jonghyeon Choi,Jiyoung Han,Kunwoo Park*

Main category: cs.CL

TL;DR: The paper introduces K-News-Stance, a Korean dataset for stance detection in news articles, and proposes JoA-ICL, a novel framework for stance prediction using structural segments. It aims to address gaps in diversity and resource limitations in stance detection research.


<details>
  <summary>Details</summary>
Motivation: Address the issues of filter bubbles and political polarization in personalized news recommendations by promoting viewpoint-aware analysis.

Method: Developed K-News-Stance, a dataset for Korean news article stance detection, and a framework called JoA-ICL, which uses language models to predict stance at the segment level and aggregates them for the article-level stance.

Result: JoA-ICL demonstrates improved stance detection accuracy compared to existing methods and enables analyses that support viewpoint diversity and media bias detection.

Conclusion: This approach enhances news recommendations' ability to incorporate diverse perspectives and serves as a tool for mitigating media bias and political polarization.

Abstract: As online news consumption grows, personalized recommendation systems have
become integral to digital journalism. However, these systems risk reinforcing
filter bubbles and political polarization by failing to incorporate diverse
perspectives. Stance detection -- identifying a text's position on a target --
can help mitigate this by enabling viewpoint-aware recommendations and
data-driven analyses of media bias. Yet, existing stance detection research
remains largely limited to short texts and high-resource languages. To address
these gaps, we introduce \textsc{K-News-Stance}, the first Korean dataset for
article-level stance detection, comprising 2,000 news articles with
article-level and 19,650 segment-level stance annotations across 47 societal
issues. We also propose \textsc{JoA-ICL}, a \textbf{Jo}urnalism-guided
\textbf{A}gentic \textbf{I}n-\textbf{C}ontext \textbf{L}earning framework that
employs a language model agent to predict the stances of key structural
segments (e.g., leads, quotes), which are then aggregated to infer the overall
article stance. Experiments show that \textsc{JoA-ICL} outperforms existing
stance detection methods, highlighting the benefits of segment-level agency in
capturing the overall position of long-form news articles. Two case studies
further demonstrate its broader utility in promoting viewpoint diversity in
news recommendations and uncovering patterns of media bias.

</details>


### [66] [LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk Prediction: A Clinical NLP](https://arxiv.org/abs/2507.11052)
*Haowei Yang,Ziyu Shen,Junli Shao,Luyao Men,Xinyue Han,Jing Dong*

Main category: cs.CL

TL;DR: The paper introduces a clinical NLP pipeline leveraging large language models (LLMs) to improve cardiovascular disease (CVD) risk identification from clinical notes.


<details>
  <summary>Details</summary>
Motivation: Existing CVD prediction models mainly focus on structured data, but unstructured clinical notes hold important early diagnostic indicators.

Method: Developed a pipeline using domain-adapted LLMs for extracting symptoms, reasoning contextually, and correlating clinical notes. The process includes fine-tuning, prompt-based inference, and rules for validation.

Result: On MIMIC-III and CARDIO-NLP datasets, the model showed improved precision, recall, F1-score, and clinical relevance (kappa = 0.82).

Conclusion: LLMs show promise for advancing clinical decision systems, translating patient narratives into actionable insights for timely warning systems.

Abstract: Timely identification and accurate risk stratification of cardiovascular
disease (CVD) remain essential for reducing global mortality. While existing
prediction models primarily leverage structured data, unstructured clinical
notes contain valuable early indicators. This study introduces a novel
LLM-augmented clinical NLP pipeline that employs domain-adapted large language
models for symptom extraction, contextual reasoning, and correlation from
free-text reports. Our approach integrates cardiovascular-specific fine-tuning,
prompt-based inference, and entity-aware reasoning. Evaluations on MIMIC-III
and CARDIO-NLP datasets demonstrate improved performance in precision, recall,
F1-score, and AUROC, with high clinical relevance (kappa = 0.82) assessed by
cardiologists. Challenges such as contextual hallucination, which occurs when
plausible information contracts with provided source, and temporal ambiguity,
which is related with models struggling with chronological ordering of events
are addressed using prompt engineering and hybrid rule-based verification. This
work underscores the potential of LLMs in clinical decision support systems
(CDSS), advancing early warning systems and enhancing the translation of
patient narratives into actionable risk assessments.

</details>


### [67] [Social Media Sentiments Analysis on the July Revolution in Bangladesh: A Hybrid Transformer Based Machine Learning Approach](https://arxiv.org/abs/2507.11084)
*Md. Sabbir Hossen,Md. Saiduzzaman,Pabon Shaha*

Main category: cs.CL

TL;DR: This study analyzes public sentiment in Bangla social media comments during the July Revolution using transformer-based frameworks and machine learning classifiers, achieving high accuracy in sentiment classification.


<details>
  <summary>Details</summary>
Motivation: To understand and decode public sentiment expressed during the student-led July Revolution in Bangladesh, particularly via social media, in a low-resource language like Bangla.

Method: The researchers utilized a dataset of 4,200 Bangla social media comments and employed transformer-based feature extraction models (e.g., BanglaBERT, mBERT) alongside a hybrid XMB-BERT model and PCA for efficient sentiment analysis across 11 classifiers.

Result: The hybrid XMB-BERT model paired with a voting classifier achieved 83.7% accuracy, outperforming other sentiment analysis models and classifiers.

Conclusion: Machine learning techniques like hybrid transformer models demonstrate potential for analyzing social sentiment in low-resource languages, providing deeper insights into historic and socially significant events.

Abstract: The July Revolution in Bangladesh marked a significant student-led mass
uprising, uniting people across the nation to demand justice, accountability,
and systemic reform. Social media platforms played a pivotal role in amplifying
public sentiment and shaping discourse during this historic mass uprising. In
this study, we present a hybrid transformer-based sentiment analysis framework
to decode public opinion expressed in social media comments during and after
the revolution. We used a brand new dataset of 4,200 Bangla comments collected
from social media. The framework employs advanced transformer-based feature
extraction techniques, including BanglaBERT, mBERT, XLM-RoBERTa, and the
proposed hybrid XMB-BERT, to capture nuanced patterns in textual data.
Principle Component Analysis (PCA) were utilized for dimensionality reduction
to enhance computational efficiency. We explored eleven traditional and
advanced machine learning classifiers for identifying sentiments. The proposed
hybrid XMB-BERT with the voting classifier achieved an exceptional accuracy of
83.7% and outperform other model classifier combinations. This study
underscores the potential of machine learning techniques to analyze social
sentiment in low-resource languages like Bangla.

</details>


### [68] [Beyond Traditional Algorithms: Leveraging LLMs for Accurate Cross-Border Entity Identification](https://arxiv.org/abs/2507.11086)
*Andres Azqueta-Gavaldón,Joaquin Ramos Cosgrove*

Main category: cs.CL

TL;DR: The study addresses challenges in identifying foreign entities in the Spanish financial system by evaluating Large Language Models (LLMs) against traditional entity-matching methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve entity-matching processes essential for risk management, regulatory compliance, and the prevention of financial misconduct in cross-border financial activities.

Method: It assesses the performance of traditional matching algorithms (e.g., Jaccard, cosine, Levenshtein distances) against LLMs through experiments on a dataset of 65 Portuguese company cases.

Result: Interface-based LLMs outperform traditional methods with accuracies above 93%, F1 scores over 96%, and reduced false positives.

Conclusion: LLMs demonstrate superior accuracy and contextual handling, making them a viable solution for more reliable foreign entity classification.

Abstract: The growing prevalence of cross-border financial activities in global markets
has underscored the necessity of accurately identifying and classifying foreign
entities. This practice is essential within the Spanish financial system for
ensuring robust risk management, regulatory adherence, and the prevention of
financial misconduct. This process involves a labor-intensive entity-matching
task, where entities need to be validated against available reference sources.
Challenges arise from linguistic variations, special characters, outdated
names, and changes in legal forms, complicating traditional matching algorithms
like Jaccard, cosine, and Levenshtein distances. These methods struggle with
contextual nuances and semantic relationships, leading to mismatches. To
address these limitations, we explore Large Language Models (LLMs) as a
flexible alternative. LLMs leverage extensive training to interpret context,
handle abbreviations, and adapt to legal transitions. We evaluate traditional
methods, Hugging Face-based LLMs, and interface-based LLMs (e.g., Microsoft
Copilot, Alibaba's Qwen 2.5) using a dataset of 65 Portuguese company cases.
Results show traditional methods achieve accuracies over 92% but suffer high
false positive rates (20-40%). Interface-based LLMs outperform, achieving
accuracies above 93%, F1 scores exceeding 96%, and lower false positives
(40-80%).

</details>


### [69] [The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs](https://arxiv.org/abs/2507.11097)
*Zichen Wen,Jiashu Qu,Dongrui Liu,Zhiyuan Liu,Ruixi Wu,Yicun Yang,Xiangqi Jin,Haoyun Xu,Xuyang Liu,Weijia Li,Chaochao Lu,Jing Shao,Conghui He,Linfeng Zhang*

Main category: cs.CL

TL;DR: This paper introduces DIJA, a systematic study and adversarial attack framework highlighting vulnerabilities in diffusion-based large language models (dLLMs), underlining the urgent need for improved safety alignment.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address safety concerns in dLLMs, which are vulnerable to masked-input adversarial prompts due to their bidirectional modeling and parallel decoding mechanisms.

Method: The authors developed DIJA, which leverages adversarial interleaved mask-text prompts to exploit dLLM weaknesses in generating unsafe outputs through their text generation mechanisms.

Result: DIJA outperforms prior jailbreak techniques, achieving up to 100% keyword-based attack success rates and significant gains in evaluator-based metrics such as JailbreakBench and StrongREJECT scores.

Conclusion: The findings reveal critical security flaws in dLLMs and emphasize the need for better safety alignment strategies as these models gain prominence.

Abstract: Diffusion-based large language models (dLLMs) have recently emerged as a
powerful alternative to autoregressive LLMs, offering faster inference and
greater interactivity via parallel decoding and bidirectional modeling.
However, despite strong performance in code generation and text infilling, we
identify a fundamental safety concern: existing alignment mechanisms fail to
safeguard dLLMs against context-aware, masked-input adversarial prompts,
exposing novel vulnerabilities. To this end, we present DIJA, the first
systematic study and jailbreak attack framework that exploits unique safety
weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial
interleaved mask-text prompts that exploit the text generation mechanisms of
dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional
modeling drives the model to produce contextually consistent outputs for masked
spans, even when harmful, while parallel decoding limits model dynamic
filtering and rejection sampling of unsafe content. This causes standard
alignment mechanisms to fail, enabling harmful completions in alignment-tuned
dLLMs, even when harmful behaviors or unsafe instructions are directly exposed
in the prompt. Through comprehensive experiments, we demonstrate that DIJA
significantly outperforms existing jailbreak methods, exposing a previously
overlooked threat surface in dLLM architectures. Notably, our method achieves
up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior
baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and
by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of
harmful content in the jailbreak prompt. Our findings underscore the urgent
need for rethinking safety alignment in this emerging class of language models.
Code is available at https://github.com/ZichenWen1/DIJA.

</details>


### [70] [Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs](https://arxiv.org/abs/2507.11112)
*Sanhanat Sivapiromrat,Caiqi Zhang,Marco Basaldella,Nigel Collier*

Main category: cs.CL

TL;DR: This paper explores the susceptibility of large language models (LLMs) to data poisoning attacks, showing that multiple distinct triggers can coexist and persist within a single model. It proposes a recovery method to mitigate this vulnerability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand and address vulnerabilities in LLMs due to data poisoning attacks, particularly focusing on multi-trigger scenarios and their persistence.

Method: The authors developed a framework to study multiple backdoor triggers in LLMs, analyzed their coexistence, robustness, and activation mechanisms, and introduced a layer-wise weight difference analysis for targeted retraining.

Result: The study found that multiple distinct triggers can be embedded within LLMs simultaneously without interference. Poisoned triggers demonstrated robustness even with token substitutions or separations. The proposed recovery method successfully removes trigger behaviors with minimal updates.

Conclusion: LLMs are exposed to broader, persistent vulnerabilities through multi-trigger poisoning. The presented recovery method offers a practical defense against such attacks, enhancing security for these models.

Abstract: Recent studies have shown that Large Language Models (LLMs) are vulnerable to
data poisoning attacks, where malicious training examples embed hidden
behaviours triggered by specific input patterns. However, most existing works
assume a phrase and focus on the attack's effectiveness, offering limited
understanding of trigger mechanisms and how multiple triggers interact within
the model. In this paper, we present a framework for studying poisoning in
LLMs. We show that multiple distinct backdoor triggers can coexist within a
single model without interfering with each other, enabling adversaries to embed
several triggers concurrently. Using multiple triggers with high embedding
similarity, we demonstrate that poisoned triggers can achieve robust activation
even when tokens are substituted or separated by long token spans. Our findings
expose a broader and more persistent vulnerability surface in LLMs. To mitigate
this threat, we propose a post hoc recovery method that selectively retrains
specific model components based on a layer-wise weight difference analysis. Our
method effectively removes the trigger behaviour with minimal parameter
updates, presenting a practical and efficient defence against multi-trigger
poisoning.

</details>


### [71] [MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models](https://arxiv.org/abs/2507.11114)
*Seif Ahmed,Mohamed T. Younes,Abdelrahman Moustafa,Abdelrahman Allam,Hamza Moustafa*

Main category: cs.CL

TL;DR: Researchers developed an ensemble-based system using several Gemini models for multilingual multimodal reasoning, achieving first place in the ImageCLEF 2025 EXAMS V challenge.


<details>
  <summary>Details</summary>
Motivation: To address high-stakes multilingual reasoning challenges in educational settings with efficient and accurate models.

Method: An ensemble method combining Gemini models (visual description, caption refinement, reasoning) coordinated via few-shot and zero-shot prompts, enhanced by cross-lingual data augmentation.

Result: The system achieved overall first place in the multilingual track with 81.4% accuracy, dominating 11 out of 13 language tracks.

Conclusion: Lightweight OCR-VLM ensembles paired with precise prompt strategies outperform heavier end-to-end models in multilingual reasoning tasks.

Abstract: We present a robust ensemble-based system for multilingual multimodal
reasoning, designed for the ImageCLEF 2025 EXAMS V challenge. Our approach
integrates Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption
refinement and consistency checks, and Gemini 2.5 Pro as a reasoner which
handles final answer selection, all coordinated through carefully engineered
few-shot and zero-shot prompts. We conducted an extensive ablation study,
training several large language models (Gemini 2.5 Flash, Phi 4, Gemma 3,
Mistral) on an English dataset and its multilingual augmented version.
Additionally, we evaluated Gemini 2.5 Flash in a zero-shot setting for
comparison and found it to substantially outperform the trained models. Prompt
design also proved critical: enforcing concise, language-normalized formats and
prohibiting explanatory text boosted model accuracy on the English validation
set from 55.9% to 61.7%. On the official leaderboard, our system (Team MSA)
achieved first place overall in the multilingual track with 81.4% accuracy, and
led 11 out of 13 individual language tracks, with top results such as 95.07%
for Croatian and 92.12% for Italian. These findings highlight that lightweight
OCR-VLM ensembles, when paired with precise prompt strategies and cross-lingual
augmentation, can outperform heavier end-to-end models in high-stakes,
multilingual educational settings.

</details>


### [72] [What Should LLMs Forget? Quantifying Personal Data in LLMs for Right-to-Be-Forgotten Requests](https://arxiv.org/abs/2507.11128)
*Dimitri Staufer*

Main category: cs.CL

TL;DR: The paper discusses identifying memorized personal data in Large Language Models (LLMs) by creating a dataset called WikiMem and introducing metrics to help dynamically address GDPR compliance, including the 'Right to Be Forgotten.'


<details>
  <summary>Details</summary>
Motivation: Concerns arise as LLMs memorize and possibly disclose personal data, requiring a method to address individually-specific privacy inquiries regarding the 'Right to Be Forgotten' under GDPR.

Method: The authors introduced WikiMem—a dataset of natural language canaries based on human-related properties—and developed a ranking method using calibrated negative log-likelihood to quantify memorized associations in LLMs across paraphrased prompts.

Result: Experiments on 15 LLMs (spanning 410M-70B parameters) show that memorization of human facts correlates with subject web presence and model scale, enabling identification of memorized data for GDPR compliance.

Conclusion: This work enables individual-level identification of memorized data in LLMs, facilitating the construction of 'forget sets' for machine unlearning requests and GDPR-related compliance.

Abstract: Large Language Models (LLMs) can memorize and reveal personal information,
raising concerns regarding compliance with the EU's GDPR, particularly the
Right to Be Forgotten (RTBF). Existing machine unlearning methods assume the
data to forget is already known but do not address how to identify which
individual-fact associations are stored in the model. Privacy auditing
techniques typically operate at the population level or target a small set of
identifiers, limiting applicability to individual-level data inquiries. We
introduce WikiMem, a dataset of over 5,000 natural language canaries covering
243 human-related properties from Wikidata, and a model-agnostic metric to
quantify human-fact associations in LLMs. Our approach ranks ground-truth
values against counterfactuals using calibrated negative log-likelihood across
paraphrased prompts. We evaluate 200 individuals across 15 LLMs (410M-70B
parameters), showing that memorization correlates with subject web presence and
model scale. We provide a foundation for identifying memorized personal data in
LLMs at the individual level, enabling the dynamic construction of forget sets
for machine unlearning and RTBF requests.

</details>


### [73] [Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding](https://arxiv.org/abs/2507.11198)
*Conrad Borchers,Bahar Shahrokhian,Francesco Balzan,Elham Tajik,Sreecharan Sankaranarayanan,Sebastian Simon*

Main category: cs.CL

TL;DR: This study evaluates the impact of multi-agent systems (MAS) and temperature settings on coding accuracy using large language models (LLMs). Results show MAS does not consistently improve outcomes over single-agent models, though it may help in refining codebooks.


<details>
  <summary>Details</summary>
Motivation: To explore whether multi-agent systems (MAS) with diverse personas can improve qualitative coding workflows compared to single-agent systems using large language models (LLMs).

Method: The study tested six open-source LLMs across 18 experimental configurations to code dialog segments using MAS. The effects of agent personas (e.g., assertive, neutral, empathetic) and temperature settings on consensus and accuracy were analyzed using 77,000 LLM decisions against a human-annotated dataset.

Result: Temperature greatly influenced consensus timing, while multiple personas in MAS significantly delayed consensus in most configurations. However, neither temperature nor persona diversity led to better coding accuracy overall. A single model (OpenHermesV2:7B) and specific conditions showed slight MAS-related gains.

Conclusion: The research challenges the assumption that MAS with diverse personas enhance coding accuracy. The findings indicate that single-agent systems often outperform MAS, though MAS could potentially refine codebooks for complex coding tasks.

Abstract: Large Language Models (LLMs) enable new possibilities for qualitative
research at scale, including coding and data annotation. While multi-agent
systems (MAS) can emulate human coding workflows, their benefits over
single-agent coding remain poorly understood. We conducted an experimental
study of how agent persona and temperature shape consensus-building and coding
accuracy of dialog segments based on a codebook with 8 codes. Our open-source
MAS mirrors deductive human coding through structured agent discussion and
consensus arbitration. Using six open-source LLMs (with 3 to 32 billion
parameters) and 18 experimental configurations, we analyze over 77,000 coding
decisions against a gold-standard dataset of human-annotated transcripts from
online math tutoring sessions. Temperature significantly impacted whether and
when consensus was reached across all six LLMs. MAS with multiple personas
(including neutral, assertive, or empathetic), significantly delayed consensus
in four out of six LLMs compared to uniform personas. In three of those LLMs,
higher temperatures significantly diminished the effects of multiple personas
on consensus. However, neither temperature nor persona pairing lead to robust
improvements in coding accuracy. Single agents matched or outperformed MAS
consensus in most conditions. Only one model (OpenHermesV2:7B) and code
category showed above-chance gains from MAS deliberation when temperature was
0.5 or lower and especially when the agents included at least one assertive
persona. Qualitative analysis of MAS collaboration for these configurations
suggests that MAS may nonetheless aid in narrowing ambiguous code applications
that could improve codebooks and human-AI coding. We contribute new insight
into the limits of LLM-based qualitative methods, challenging the notion that
diverse MAS personas lead to better outcomes. We open-source our MAS and
experimentation code.

</details>


### [74] [EsBBQ and CaBBQ: The Spanish and Catalan Bias Benchmarks for Question Answering](https://arxiv.org/abs/2507.11216)
*Valle Ruiz-Fernández,Mario Mina,Júlia Falcão,Luis Vasquez-Reina,Anna Sallés,Aitor Gonzalez-Agirre,Olatz Perez-de-Viñaspre*

Main category: cs.CL

TL;DR: The paper introduces Spanish and Catalan Bias Benchmarks (EsBBQ and CaBBQ) for assessing social bias in Language Models within the Spanish and Catalan contexts.


<details>
  <summary>Details</summary>
Motivation: The lack of resources to evaluate social biases in languages other than English and outside U.S. social contexts highlights a need for benchmarks in underrepresented languages and regions.

Method: The authors created two datasets, EsBBQ and CaBBQ, adapting the BBQ benchmark to Spanish and Catalan languages and social contexts. These datasets assess social bias using a multiple-choice question-answering format.

Result: The evaluation of different large language models showed that most models struggle with ambiguous scenarios and that higher QA accuracy often aligns with stronger social biases.

Conclusion: The study highlights the critical need for language-specific and context-aware tools to assess and mitigate biases in AI models.

Abstract: Previous literature has largely shown that Large Language Models (LLMs)
perpetuate social biases learnt from their pre-training data. Given the notable
lack of resources for social bias evaluation in languages other than English,
and for social contexts outside of the United States, this paper introduces the
Spanish and the Catalan Bias Benchmarks for Question Answering (EsBBQ and
CaBBQ). Based on the original BBQ, these two parallel datasets are designed to
assess social bias across 10 categories using a multiple-choice QA setting, now
adapted to the Spanish and Catalan languages and to the social context of
Spain. We report evaluation results on different LLMs, factoring in model
family, size and variant. Our results show that models tend to fail to choose
the correct answer in ambiguous scenarios, and that high QA accuracy often
correlates with greater reliance on social biases.

</details>


### [75] [An Agentic Flow for Finite State Machine Extraction using Prompt Chaining](https://arxiv.org/abs/2507.11222)
*Fares Wael,Youssef Maklad,Ali Hamdi,Wael Elsersy*

Main category: cs.CL

TL;DR: Finite-State Machines (FSMs) are vital for analyzing network protocols, and FlowFSM uses Large Language Models (LLMs) to enhance their extraction from RFC documents accurately.


<details>
  <summary>Details</summary>
Motivation: Current FSM extraction techniques struggle with scalability, incomplete coverage, and ambiguous natural language specifications, necessitating improved methods.

Method: The study introduces FlowFSM, a framework utilizing LLMs with prompt chaining and chain-of-thought reasoning to extract FSMs systematically from protocol specifications like FTP and RTSP.

Result: Experimental evaluation reveals FlowFSM achieves high extraction precision with reduced hallucinated transitions, demonstrating it outperforms traditional methods.

Conclusion: FlowFSM highlights the potential of LLM-based agent systems for improving FSM extraction, advancing protocol analysis, cybersecurity, and reverse engineering.

Abstract: Finite-State Machines (FSMs) are critical for modeling the operational logic
of network protocols, enabling verification, analysis, and vulnerability
discovery. However, existing FSM extraction techniques face limitations such as
scalability, incomplete coverage, and ambiguity in natural language
specifications. In this paper, we propose FlowFSM, a novel agentic framework
that leverages Large Language Models (LLMs) combined with prompt chaining and
chain-of-thought reasoning to extract accurate FSMs from raw RFC documents.
FlowFSM systematically processes protocol specifications, identifies state
transitions, and constructs structured rule-books by chaining agent outputs.
Experimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM
achieves high extraction precision while minimizing hallucinated transitions,
showing promising results. Our findings highlight the potential of agent-based
LLM systems in the advancement of protocol analysis and FSM inference for
cybersecurity and reverse engineering applications.

</details>


### [76] [Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages](https://arxiv.org/abs/2507.11230)
*Lyzander Marciano Andrylie,Inaya Rahmanisa,Mahardika Krisna Ihsani,Alfan Farizki Wicaksono,Haryo Akbarianto Wibowo,Alham Fikri Aji*

Main category: cs.CL

TL;DR: The paper explores sparse autoencoders (SAEs) to identify language-specific features within large language models, particularly focusing on interpretability and language identification.


<details>
  <summary>Details</summary>
Motivation: Understanding multilingual mechanisms in LLMs is challenging due to the polysemantic nature of neurons. There is a need to distinguish language-specific features from cross-lingual representations.

Method: The paper introduces SAE-LAPE, leveraging feature activation probabilities to pinpoint language-specific features within feed-forward networks of LLMs.

Result: Language-specific features are primarily located in middle-to-final layers, are interpretable, and influence multilingual performance and language output. SAE-LAPE achieves language identification performance comparable to fastText.

Conclusion: The study provides insights into language-specific mechanisms in LLMs, demonstrating the utility of interpretable sparse autoencoders for better understanding and controlling multilingual tasks.

Abstract: Understanding the multilingual mechanisms of large language models (LLMs)
provides insight into how they process different languages, yet this remains
challenging. Existing studies often focus on individual neurons, but their
polysemantic nature makes it difficult to isolate language-specific units from
cross-lingual representations. To address this, we explore sparse autoencoders
(SAEs) for their ability to learn monosemantic features that represent concrete
and abstract concepts across languages in LLMs. While some of these features
are language-independent, the presence of language-specific features remains
underexplored. In this work, we introduce SAE-LAPE, a method based on feature
activation probability, to identify language-specific features within the
feed-forward network. We find that many such features predominantly appear in
the middle to final layers of the model and are interpretable. These features
influence the model's multilingual performance and language output and can be
used for language identification with performance comparable to fastText along
with more interpretability. Our code is available at
https://github.com/LyzanderAndrylie/language-specific-features .

</details>


### [77] [KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware Rotary Positional Embedding](https://arxiv.org/abs/2507.11273)
*Luohe Shi,Zuchao Li,Lefei Zhang,Guoming Liu,Baoyuan Qi,Hai Zhao*

Main category: cs.CL

TL;DR: The paper introduces "KV-Latent," a method that reduces memory and transfer bottlenecks in large language models by down-sampling Key-Value cache dimensions, achieving increased efficiency with minimal retraining.


<details>
  <summary>Details</summary>
Motivation: To address the memory and data transfer inefficiencies caused by the growing Key-Value (KV) cache in Transformer-based large language models during inference.

Method: Introduces the KV-Latent paradigm, which down-samples the KV vector dimensions into a latent space, optimizing efficiency. Additionally, a modification to Rotary Positional Embedding frequency sampling improves stability by mitigating noise from higher frequencies.

Result: Experiments demonstrated satisfactory results on both Grouped Query Attention-based models and non-GQA models. Comparative experiments showed the impact of separately reducing Key and Value components on performance.

Conclusion: The approach significantly reduces KV Cache footprint, enhances inference speed with minimal retraining requirements, and offers potential for more efficient large language models.

Abstract: Large language models (LLMs) based on Transformer Decoders have become the
preferred choice for conversational generative AI. Despite the overall
superiority of the Decoder architecture, the gradually increasing Key-Value
(KV) cache during inference has emerged as a primary efficiency bottleneck,
both in aspects of memory consumption and data transfer bandwidth limitations.
To address these challenges, we propose a paradigm called KV-Latent. By
down-sampling the Key-Value vector dimensions into a latent space, we can
significantly reduce the KV Cache footprint and improve inference speed, only
with a small amount of extra training, less than 1\% of pre-training takes.
Besides, we enhanced the stability of Rotary Positional Embedding applied on
lower-dimensional vectors by modifying its frequency sampling mechanism,
avoiding noise introduced by higher frequencies while retaining position
attenuation. Our experiments, including both models with Grouped Query
Attention and those without, have yielded satisfactory results. Finally, we
conducted comparative experiments to study the impact of separately reducing
Key and Value components on model's performance. Our approach allows for the
construction of more efficient language model systems, and opens the new
possibility on KV Cache saving and efficient LLMs. Our code is available at
https://github.com/ShiLuohe/KV-Latent.

</details>


### [78] [FMC: Formalization of Natural Language Mathematical Competition Problems](https://arxiv.org/abs/2507.11275)
*Jiaxuan Xie,Chengwu Liu,Ye Yuan,Siqi Li,Zhiping Xiao,Ming Zhang*

Main category: cs.CL

TL;DR: The paper introduces an automatic pipeline using large language models for translating natural language mathematical problems into formal language (Lean), creating a dataset for Olympiad-level problems and benchmarking automated theorem provers.


<details>
  <summary>Details</summary>
Motivation: To advance formal mathematical reasoning by constructing formal language datasets from natural language problems, enabling benchmarking for automated theorem provers.

Method: An autoformalization pipeline built on large language models enhanced with error feedback for high-quality formalizations, applied without training.

Result: Created a dataset with 3,922 natural language mathematical problems aligned to 9,787 Lean formalizations. Achieved 64.46% assessed quality, making it suitable for benchmarking formal reasoning systems.

Conclusion: Few-shot learning, error feedback, and increased sampling improve the autoformalization process. The proposed dataset showcases challenging tasks and provides value as a benchmark for formal reasoning.

Abstract: Efficient and accurate autoformalization methods, which leverage large-scale
datasets of extensive natural language mathematical problems to construct
formal language datasets, are key to advancing formal mathematical reasoning.
In this paper, we propose an autoformalization pipeline based on large language
models with error feedback, achieving a fully automatic and training-free
formalization approach. Using this pipeline, we curate an Olympiad-level
dataset aligning natural language problems with Lean formalizations. The
dataset comprises $3,922$ mathematical problems in natural language and $9,787$
in Lean, of which $64.46\%$ were assessed as at least above-average quality,
making it suitable as a benchmark for automated theorem provers. Additionally,
we investigate the formalization and reasoning capabilities of various LLMs and
empirically demonstrate that few-shot learning, error feedback, and increasing
sampling numbers enhance the autoformalization process. Experiments of three
automated theorem provers on the \dataset\ dataset also highlight its
challenging nature and its value as a benchmark for formal reasoning tasks.

</details>


### [79] [Fine-Grained Chinese Hate Speech Understanding: Span-Level Resources, Coded Term Lexicon, and Enhanced Detection Frameworks](https://arxiv.org/abs/2507.11292)
*Zewen Bai,Liang Yang,Shengdi Yin,Yuanyuan Sun,Hongfei Lin*

Main category: cs.CL

TL;DR: The study introduces a new dataset and method to improve Chinese hate speech detection and its interpretability.


<details>
  <summary>Details</summary>
Motivation: The lack of fine-grained annotated datasets and research on coded hate speech in Chinese language restricts the performance of hate speech detection models.

Method: The authors created a span-level annotated dataset (STATE ToxiCN), analyzed coded hate terms using language models, and proposed a lexicon-based integration for models.

Result: The proposed dataset and method improved semantic understanding and detection of hate speech in Chinese.

Conclusion: The study provides critical resources and methodologies to address key interpretability challenges in Chinese hate speech detection research.

Abstract: The proliferation of hate speech has inflicted significant societal harm,
with its intensity and directionality closely tied to specific targets and
arguments. In recent years, numerous machine learning-based methods have been
developed to detect hateful comments on online platforms automatically.
However, research on Chinese hate speech detection lags behind, and
interpretability studies face two major challenges: first, the scarcity of
span-level fine-grained annotated datasets limits models' deep semantic
understanding of hate speech; second, insufficient research on identifying and
interpreting coded hate speech restricts model explainability in complex
real-world scenarios. To address these, we make the following contributions:
(1) We introduce the Span-level Target-Aware Toxicity Extraction dataset (STATE
ToxiCN), the first span-level Chinese hate speech dataset, and evaluate the
hate semantic understanding of existing models using it. (2) We conduct the
first comprehensive study on Chinese coded hate terms, LLMs' ability to
interpret hate semantics. (3) We propose a method to integrate an annotated
lexicon into models, significantly enhancing hate speech detection performance.
Our work provides valuable resources and insights to advance the
interpretability of Chinese hate speech detection research.

</details>


### [80] [Dr.Copilot: A Multi-Agent Prompt Optimized Assistant for Improving Patient-Doctor Communication in Romanian](https://arxiv.org/abs/2507.11299)
*Andrei Niculae,Adrian Cosma,Cosmin Dumitrache,Emilian Rǎdoi*

Main category: cs.CL

TL;DR: This paper introduces Dr.Copilot, a multi-agent LLM system for enhancing the presentation quality of Romanian telemedicine responses, yielding improved user reviews and response quality.


<details>
  <summary>Details</summary>
Motivation: The need to improve the quality of text-based telemedicine responses, which are often judged by presentation rather than clinical accuracy.

Method: Development of Dr.Copilot, a multi-agent LLM system optimized via DSPy, with 17 feedback axes, leveraging low-resource Romanian data and open-weight models for real-time deployment.

Result: Empirical studies and deployment involving 41 doctors demonstrated improved user reviews and response quality in a Romanian telemedicine platform.

Conclusion: Dr.Copilot successfully enhances doctor-patient communication quality, representing a leading example of LLM deployment in Romanian healthcare.

Abstract: Text-based telemedicine has become increasingly common, yet the quality of
medical advice in doctor-patient interactions is often judged more on how
advice is communicated rather than its clinical accuracy. To address this, we
introduce Dr.Copilot , a multi-agent large language model (LLM) system that
supports Romanian-speaking doctors by evaluating and enhancing the presentation
quality of their written responses. Rather than assessing medical correctness,
Dr.Copilot provides feedback along 17 interpretable axes. The system comprises
of three LLM agents with prompts automatically optimized via DSPy. Designed
with low-resource Romanian data and deployed using open-weight models, it
delivers real-time specific feedback to doctors within a telemedicine platform.
Empirical evaluations and live deployment with 41 doctors show measurable
improvements in user reviews and response quality, marking one of the first
real-world deployments of LLMs in Romanian medical settings.

</details>


### [81] [Internal Value Alignment in Large Language Models through Controlled Value Vector Activation](https://arxiv.org/abs/2507.11316)
*Haoran Jin,Meng Li,Xiting Wang,Zhihao Xu,Minlie Huang,Yantao Jia,Defu Lian*

Main category: cs.CL

TL;DR: This paper presents a novel method, Controlled Value Vector Activation (ConVA), to align LLMs with human values, achieving strong consistency and control over values in the models without degrading performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the alignment of LLMs with human values, providing transparency, adaptability, and clarity, especially in dynamic scenarios.

Method: The proposed method, ConVA, modifies the latent representation activations of LLMs through context-controlled identification and gated activation, ensuring accurate and minimal adjustments for aligning values.

Result: Experiments demonstrate that ConVA achieves the highest success rate in controlling 10 basic values without negatively impacting LLM performance or fluency, even when faced with opposing or malicious inputs.

Conclusion: ConVA provides an effective way to align LLMs with human values, ensuring consistency and robustness, thereby contributing to safer and more aligned AI systems.

Abstract: Aligning Large Language Models (LLMs) with human values has attracted
increasing attention since it provides clarity, transparency, and the ability
to adapt to evolving scenarios. In this paper, we introduce a Controlled Value
Vector Activation (ConVA) method that directly aligns the internal values of
LLMs by interpreting how a value is encoded in their latent representations and
modifies relevant activations to ensure consistent values in LLMs. To ensure an
accurate and unbiased interpretation, we propose a context-controlled value
vector identification method. To consistently control values without
sacrificing model performance, we introduce a gated value vector activation
method for effective and minimum degree of value control. Experiments show that
our method achieves the highest control success rate across 10 basic values
without hurting LLM performance and fluency, and ensures target values even
with opposite and potentially malicious input prompts. Source code and data are
available at~ https://github.com/hr-jin/ConVA.

</details>


### [82] [Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge](https://arxiv.org/abs/2507.11330)
*Wenqing Wu,Chengzhi Zhang,Yi Zhao*

Main category: cs.CL

TL;DR: The paper proposes combining large language models (LLM) with human expertise to assess the novelty of academic papers, especially focusing on the method novelty. This approach uses peer review reports and methodology summaries to fine-tune pre-trained language models (PLMs).


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of traditional novelty assessment methods, which rely either on expert judgment or citation analysis, neither of which is fully reliable.

Method: The approach involves extracting sentences about paper novelty from peer reviews, summarizing methodology sections using LLM, fine-tuning PLMs, and developing a text-guided fusion module with Sparse-Attention for better integration of knowledge.

Result: The proposed model outperforms various baseline methods in predicting method novelty, as shown by extensive experimental evaluations.

Conclusion: Integrating LLMs with human expertise offers a promising solution for assessing the novelty of academic papers, showing superior predictive performance over other methods.

Abstract: Novelty is a crucial criterion in the peer review process for evaluating
academic papers. Traditionally, it's judged by experts or measure by unique
reference combinations. Both methods have limitations: experts have limited
knowledge, and the effectiveness of the combination method is uncertain.
Moreover, it's unclear if unique citations truly measure novelty. The large
language model (LLM) possesses a wealth of knowledge, while human experts
possess judgment abilities that the LLM does not possess. Therefore, our
research integrates the knowledge and abilities of LLM and human experts to
address the limitations of novelty assessment. The most common novelty in
academic papers is the introduction of new methods. In this paper, we propose
leveraging human knowledge and LLM to assist pretrained language models (PLMs,
e.g. BERT etc.) in predicting the method novelty of papers. Specifically, we
extract sentences related to the novelty of the academic paper from peer review
reports and use LLM to summarize the methodology section of the academic paper,
which are then used to fine-tune PLMs. In addition, we have designed a
text-guided fusion module with novel Sparse-Attention to better integrate human
and LLM knowledge. We compared the method we proposed with a large number of
baselines. Extensive experiments demonstrate that our method achieves superior
performance.

</details>


### [83] [What is the Best Process Model Representation? A Comparative Analysis for Process Modeling with Large Language Models](https://arxiv.org/abs/2507.11356)
*Alexis Brissard,Frédéric Cuppens,Amal Zouaq*

Main category: cs.CL

TL;DR: The paper evaluates various Process Model Representations (PMRs) for Process Modeling (PMo) tasks using Large Language Models (LLMs), with Mermaid and BPMN text identified as top-performing representations.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic comparison among various PMRs utilized in PMo tasks, especially as recent advancements in LLM usage highlight differences in evaluation and generation approaches.

Method: The paper introduces the PMo Dataset, a collection of 55 process descriptions paired with nine PMRs, and assesses these PMRs based on their suitability for LLM-driven PMo and process model generation (PMG).

Result: "Mermaid" PMR scored the highest across six PMo criteria, while "BPMN text" excelled in process element similarity during PMG.

Conclusion: The study provides the first empirical comparison of PMRs, identifying strengths of different representations and offering insights for improving LLM-based process modeling workflows.

Abstract: Large Language Models (LLMs) are increasingly applied for Process Modeling
(PMo) tasks such as Process Model Generation (PMG). To support these tasks,
researchers have introduced a variety of Process Model Representations (PMRs)
that serve as model abstractions or generation targets. However, these PMRs
differ widely in structure, complexity, and usability, and have never been
systematically compared. Moreover, recent PMG approaches rely on distinct
evaluation strategies and generation techniques, making comparison difficult.
This paper presents the first empirical study that evaluates multiple PMRs in
the context of PMo with LLMs. We introduce the PMo Dataset, a new dataset
containing 55 process descriptions paired with models in nine different PMRs.
We evaluate PMRs along two dimensions: suitability for LLM-based PMo and
performance on PMG. \textit{Mermaid} achieves the highest overall score across
six PMo criteria, whereas \textit{BPMN text} delivers the best PMG results in
terms of process element similarity.

</details>


### [84] [Addressing Data Imbalance in Transformer-Based Multi-Label Emotion Detection with Weighted Loss](https://arxiv.org/abs/2507.11384)
*Xia Cui*

Main category: cs.CL

TL;DR: The paper investigates applying a weighted loss function to Transformer models for emotion detection, addressing data imbalances. Results show improvements in high-frequency emotions but limited benefits for minority emotions.


<details>
  <summary>Details</summary>
Motivation: To address data imbalances in multi-label emotion detection without relying on computationally expensive methods like resampling.

Method: Applied a dynamically adjusted weighted loss function on Transformer models (BERT, RoBERTa, BART) and evaluated their performance using metrics like Micro F1, Macro F1, ROC-AUC, Accuracy, and Jaccard similarity.

Result: The weighted loss improves performance for high-frequency emotions but has limited impact on minority (low-frequency) emotion classes.

Conclusion: The weighted loss function shows promise for addressing data imbalance in emotion detection, though challenges persist, particularly for minority emotion classes.

Abstract: This paper explores the application of a simple weighted loss function to
Transformer-based models for multi-label emotion detection in SemEval-2025
Shared Task 11. Our approach addresses data imbalance by dynamically adjusting
class weights, thereby enhancing performance on minority emotion classes
without the computational burden of traditional resampling methods. We evaluate
BERT, RoBERTa, and BART on the BRIGHTER dataset, using evaluation metrics such
as Micro F1, Macro F1, ROC-AUC, Accuracy, and Jaccard similarity coefficients.
The results demonstrate that the weighted loss function improves performance on
high-frequency emotion classes but shows limited impact on minority classes.
These findings underscore both the effectiveness and the challenges of applying
this approach to imbalanced multi-label emotion detection.

</details>


### [85] [DCR: Quantifying Data Contamination in LLMs Evaluation](https://arxiv.org/abs/2507.11405)
*Cheng Xu,Nan Yan,Shuhao Guan,Changhong Jin,Yuke Mei,Yibing Guo,M-Tahar Kechadi*

Main category: cs.CL

TL;DR: The paper introduces the Data Contamination Risk (DCR) framework to detect and quantify benchmark data contamination in large language models (LLMs), promoting fairer performance evaluation.


<details>
  <summary>Details</summary>
Motivation: To address concerns that large language models (LLMs) may memorize evaluation data, leading to inflated performance metrics and misleading generalization assessments.

Method: The authors propose the Data Contamination Risk (DCR) framework, which uses a fuzzy inference system to analyze contamination risks at four levels: semantic, informational, data, and label. It outputs a unified DCR Factor to adjust performance metrics.

Result: The DCR framework was validated on 9 large language models across multiple tasks, achieving contamination-aware accuracy with only a 4% average error compared to uncontaminated baselines.

Conclusion: DCR is an efficient, transparent, and practical tool for contamination risk assessment in LLM evaluations, promoting more reliable benchmarking practices.

Abstract: The rapid advancement of large language models (LLMs) has heightened concerns
about benchmark data contamination (BDC), where models inadvertently memorize
evaluation data, inflating performance metrics and undermining genuine
generalization assessment. This paper introduces the Data Contamination Risk
(DCR) framework, a lightweight, interpretable pipeline designed to detect and
quantify BDC across four granular levels: semantic, informational, data, and
label. By synthesizing contamination scores via a fuzzy inference system, DCR
produces a unified DCR Factor that adjusts raw accuracy to reflect
contamination-aware performance. Validated on 9 LLMs (0.5B-72B) across
sentiment analysis, fake news detection, and arithmetic reasoning tasks, the
DCR framework reliably diagnoses contamination severity and with accuracy
adjusted using the DCR Factor to within 4% average error across the three
benchmarks compared to the uncontaminated baseline. Emphasizing computational
efficiency and transparency, DCR provides a practical tool for integrating
contamination assessment into routine evaluations, fostering fairer comparisons
and enhancing the credibility of LLM benchmarking practices.

</details>


### [86] [EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes](https://arxiv.org/abs/2507.11407)
*LG AI Research,:,Kyunghoon Bae,Eunbi Choi,Kibong Choi,Stanley Jungkyu Choi,Yemuk Choi,Kyubeen Han,Seokhee Hong,Junwon Hwang,Taewan Hwang,Joonwon Jang,Hyojin Jeon,Kijeong Jeon,Gerrard Jeongwon Jo,Hyunjik Jo,Jiyeon Jung,Euisoon Kim,Hyosang Kim,Jihoon Kim,Joonkee Kim,Seonghwan Kim,Soyeon Kim,Sunkyoung Kim,Yireun Kim,Yongil Kim,Youchul Kim,Edward Hwayoung Lee,Gwangho Lee,Haeju Lee,Honglak Lee,Jinsik Lee,Kyungmin Lee,Sangha Park,Young Min Paik,Yongmin Park,Youngyong Park,Sanghyun Seo,Sihoon Yang,Heuiyeen Yeen,Sihyuk Yi,Hyeongu Yun*

Main category: cs.CL

TL;DR: EXAONE 4.0 integrates reasoning capabilities and usability features, expands multilingual support, and offers competitive AI models for public research.


<details>
  <summary>Details</summary>
Motivation: The paper aims to create an AI model that balances usability, reasoning capabilities, and practical applicability for the agentic AI era.

Method: The development involved creating two model sizes (32B and 1.2B) with non-reasoning and reasoning modes, expanded language support, and agentic tool features.

Result: EXAONE 4.0 outperforms open-weight models in its category and remains competitive with high-end frontier-class models.

Conclusion: EXAONE 4.0 offers a balanced AI framework with reasoning and usability features, catering to both research and practical applications, and is available for public access.

Abstract: This technical report introduces EXAONE 4.0, which integrates a Non-reasoning
mode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5
and the advanced reasoning abilities of EXAONE Deep. To pave the way for the
agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool
use, and its multilingual capabilities are extended to support Spanish in
addition to English and Korean. The EXAONE 4.0 model series consists of two
sizes: a mid-size 32B model optimized for high performance, and a small-size
1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates
superior performance compared to open-weight models in its class and remains
competitive even against frontier-class models. The models are publicly
available for research purposes and can be easily downloaded via
https://huggingface.co/LGAI-EXAONE.

</details>


### [87] [KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical Reasoning?](https://arxiv.org/abs/2507.11408)
*Soumadeep Saha,Akshay Chaturvedi,Saptarshi Saha,Utpal Garain,Nicholas Asher*

Main category: cs.CL

TL;DR: The paper introduces Causal CoT Graphs (CCGs) to model causal reasoning in large language models (LLMs) and creates a dataset, KisMATH, for understanding reasoning paths in LLMs.


<details>
  <summary>Details</summary>
Motivation: To understand why chain-of-thought reasoning improves the performance of LLMs and to provide a framework for analyzing causal reasoning mechanisms.

Method: The authors propose Causal CoT Graphs, directed acyclic graphs extracted from reasoning traces. They compile a dataset called KisMATH using mathematical problems from MATH500, GSM8K, and AIME.

Result: The analysis reveals that reasoning nodes in the CCG are mediators for final answers, and LLMs emphasize the CCG reasoning paths, suggesting they internally utilize structures akin to CCGs.

Conclusion: The study provides insights into how chain-of-thought reasoning operates in LLMs, with KisMATH enabling controlled investigations into the causal structures of reasoning pathways.

Abstract: Chain-of-thought traces have been shown to improve performance of large
language models in a plethora of reasoning tasks, yet there is no consensus on
the mechanism through which this performance boost is achieved. To shed more
light on this, we introduce Causal CoT Graphs (CCGs), which are directed
acyclic graphs automatically extracted from reasoning traces that model
fine-grained causal dependencies in the language model output. A collection of
$1671$ mathematical reasoning problems from MATH500, GSM8K and AIME, and their
associated CCGs are compiled into our dataset -- \textbf{KisMATH}. Our detailed
empirical analysis with 15 open-weight LLMs shows that (i) reasoning nodes in
the CCG are mediators for the final answer, a condition necessary for
reasoning; and (ii) LLMs emphasise reasoning paths given by the CCG, indicating
that models internally realise structures akin to our graphs. KisMATH enables
controlled, graph-aligned interventions and opens up avenues for further
investigation into the role of chain-of-thought in LLM reasoning.

</details>


### [88] [Seq vs Seq: An Open Suite of Paired Encoders and Decoders](https://arxiv.org/abs/2507.11412)
*Orion Weller,Kathryn Ricci,Marc Marone,Antoine Chaffin,Dawn Lawrie,Benjamin Van Durme*

Main category: cs.CL

TL;DR: The study introduces the Ettin suite—paired encoder-only and decoder-only language models spanning parameter sizes and trained on extensive tokens. It finds distinct advantages of architectures in specific tasks and opens data for further analysis.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in comparing encoder-only and decoder-only models due to inconsistencies in parameters, training techniques, and datasets.

Method: The researchers built and trained paired encoder-only and decoder-only models with a consistent recipe across models and sizes, ranging from 17 million to 1 billion parameters.

Result: Encoder-only models excel in classification/retrieval tasks, while decoder-only models dominate in generative tasks. Adaptation across tasks is less efficient compared to using tailored architectures.

Conclusion: Using the right model architecture for specific task types is crucial, and continued training for cross-task adaptation is suboptimal. The open-sourced materials allow extensive community contributions and analysis.

Abstract: The large language model (LLM) community focuses almost exclusively on
decoder-only language models, since they are easier to use for text generation.
However, a large subset of the community still uses encoder-only models for
tasks such as classification or retrieval. Previous work has attempted to
compare these architectures, but is forced to make comparisons with models that
have different numbers of parameters, training techniques, and datasets. We
introduce the SOTA open-data Ettin suite of models: paired encoder-only and
decoder-only models ranging from 17 million parameters to 1 billion, trained on
up to 2 trillion tokens. Using the same recipe for both encoder-only and
decoder-only models produces SOTA recipes in both categories for their
respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as
decoders. Like previous work, we find that encoder-only models excel at
classification and retrieval tasks while decoders excel at generative tasks.
However, we show that adapting a decoder model to encoder tasks (and vice
versa) through continued training is subpar compared to using only the reverse
objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa
for generative tasks). We open-source all artifacts of this study including
training data, training order segmented by checkpoint, and 200+ checkpoints to
allow future work to analyze or extend all aspects of training.

</details>


### [89] [Reasoning Strategies in Large Language Models: Can They Follow, Prefer, and Optimize?](https://arxiv.org/abs/2507.11423)
*Yanjian Zhang,Guillaume Wisniewski,Nadi Tomeh,Thierry Charnois*

Main category: cs.CL

TL;DR: This paper investigates how prompting can influence LLMs' reasoning strategies to enhance logical problem-solving, proposing methods to guide adaptive strategy selection.


<details>
  <summary>Details</summary>
Motivation: Large language models tend to favor one reasoning strategy, which may limit their effectiveness in handling diverse reasoning tasks.

Method: Researchers explore and evaluate different prompting techniques to guide LLMs in selecting appropriate reasoning strategies for better logical problem-solving.

Result: Findings show that no single reasoning strategy universally improves accuracy, but performance could be enhanced by adaptively choosing optimal strategies.

Conclusion: Adaptive strategy selection can refine LLMs' reasoning abilities, opening new opportunities for improving their performance in diverse logical tasks.

Abstract: Human reasoning involves different strategies, each suited to specific
problems. Prior work shows that large language model (LLMs) tend to favor a
single reasoning strategy, potentially limiting their effectiveness in diverse
reasoning challenges. In this work, we investigate whether prompting can
control LLMs reasoning strategies and assess its impact on logical
problem-solving. While our experiments show that no single strategy
consistently improves accuracy, performance could be enhanced if models could
adaptively choose the optimal strategy. We propose methods to guide LLMs in
strategy selection, highlighting new ways to refine their reasoning abilities.

</details>


### [90] [HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong](https://arxiv.org/abs/2507.11502)
*Sirui Han,Junqi Zhu,Ruiyuan Zhang,Yike Guo*

Main category: cs.CL

TL;DR: The paper introduces HKGAI-V1, a sovereign LLM designed specifically for Hong Kong, integrating local language and socio-legal context while ensuring AI safety and alignment with regional values.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address Hong Kong's unique multilingual and socio-legal environment, ensuring AI infrastructure is value-aligned and local-specific.

Method: The model is developed using DeepSeek architecture, fine-tuned parameters for local alignment, and integrated with RAG for real-time factual accuracy.

Result: HKGAI-V1 outperforms general-purpose models in addressing culturally sensitive queries and introduces the Adversarial HK Value Benchmark for model evaluation.

Conclusion: HKGAI-V1 establishes a blueprint for developing region-specific AI systems, reflecting strong alignment with local socio-cultural and ethical standards.

Abstract: This paper presents the development of HKGAI-V1, a foundational sovereign
large language model (LLM), developed as part of an initiative to establish
value-aligned AI infrastructure specifically tailored for Hong Kong. Addressing
the region's unique multilingual environment (Cantonese, Mandarin, and
English), its distinct socio-legal context under the "one country, two systems"
framework, and specific local cultural and value considerations, the model is
built upon the DeepSeek architecture and systematically aligned with regional
norms through a multifaceted full parameter fine-tuning process. It is further
integrated with a retrieval-augmented generation (RAG) system to ensure timely
and factually grounded information access. The core contribution lies in the
design and implementation of a comprehensive, region-specific AI alignment and
safety framework, demonstrated through two key achievements: 1) The successful
development of HKGAI-V1 itself - which outper-forms general-purpose models in
handling Hong Kong-specific culturally sensitive queries, and embodies a
"governance-embedded" approach to digital sovereignty - empowers Hong Kong to
exercise control over AI applications in critical sectors including public
services, legal systems, and edu-cation. 2) The development of the proprietary
Adversarial HK Value Benchmark, a rigorous tool for evaluating model alignment
with local ethical and legal stand-ards under challenging conditions. By
documenting these achievements, the paper provides not only a technological
artifact but also a replicable blueprint for developing advanced, regionally
focused AI systems deeply rooted in their local identities.

</details>


### [91] [Real-World Summarization: When Evaluation Reaches Its Limits](https://arxiv.org/abs/2507.11508)
*Patrícia Schmidtová,Ondřej Dušek,Saad Mahamood*

Main category: cs.CL

TL;DR: The paper explores evaluating faithfulness of LLM-generated hotel summaries, revealing that simple metrics outperform complex methods and LLMs are unreliable for evaluation.


<details>
  <summary>Details</summary>
Motivation: To investigate how faithfully LLM-generated hotel summaries reflect input data and assess evaluation methods.

Method: Conducted human evaluation campaigns using categorical error assessment and span-level annotation; compared traditional metrics, trainable methods, and LLM-as-a-judge approaches.

Result: Found simpler word overlap metrics to correlate well with human judgments (Spearman rank 0.63) and highlighted the unreliability of LLMs for evaluation.

Conclusion: Accurate and checkable information is crucial for real-world applications, and there are challenges in evaluation methods, particularly crowdsourcing.

Abstract: We examine evaluation of faithfulness to input data in the context of hotel
highlights: brief LLM-generated summaries that capture unique features of
accommodations. Through human evaluation campaigns involving categorical error
assessment and span-level annotation, we compare traditional metrics, trainable
methods, and LLM-as-a-judge approaches. Our findings reveal that simpler
metrics like word overlap correlate surprisingly well with human judgments
(Spearman correlation rank of 0.63), often outperforming more complex methods
when applied to out-of-domain data. We further demonstrate that while LLMs can
generate high-quality highlights, they prove unreliable for evaluation as they
tend to severely under- or over-annotate. Our analysis of real-world business
impacts shows incorrect and non-checkable information pose the greatest risks.
We also highlight challenges in crowdsourced evaluations.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [92] [CWNet: Causal Wavelet Network for Low-Light Image Enhancement](https://arxiv.org/abs/2507.10689)
*Tongshun Zhang,Pingping Liu,Yubing Lu,Mengen Cai,Zijian Zhang,Zhe Zhang,Qiuzhan Zhou*

Main category: cs.CV

TL;DR: The paper introduces CWNet, a Causal Wavelet Network for low-light image enhancement, which uses causal reasoning and wavelet transforms to improve semantic and feature-level adjustments, outperforming current methods.


<details>
  <summary>Details</summary>
Motivation: Traditional low-light image enhancement methods primarily focus on uniform brightness adjustments, overlooking instance-level semantics and unique feature characteristics.

Method: CWNet employs causal reasoning and uses a wavelet transform-based backbone. Globally, it uses metric learning for causal embedding. Locally, it uses CLIP semantic loss to maintain causal consistency. The wavelet backbone enhances feature-level frequency recovery.

Result: CWNet achieves significant performance improvements over existing state-of-the-art methods across multiple datasets through experiments.

Conclusion: CWNet effectively integrates causal reasoning and wavelet-based techniques for precise low-light image enhancement, addressing prior methods' limitations and performing robustly across diverse scenarios.

Abstract: Traditional Low-Light Image Enhancement (LLIE) methods primarily focus on
uniform brightness adjustment, often neglecting instance-level semantic
information and the inherent characteristics of different features. To address
these limitations, we propose CWNet (Causal Wavelet Network), a novel
architecture that leverages wavelet transforms for causal reasoning.
Specifically, our approach comprises two key components: 1) Inspired by the
concept of intervention in causality, we adopt a causal reasoning perspective
to reveal the underlying causal relationships in low-light enhancement. From a
global perspective, we employ a metric learning strategy to ensure causal
embeddings adhere to causal principles, separating them from non-causal
confounding factors while focusing on the invariance of causal factors. At the
local level, we introduce an instance-level CLIP semantic loss to precisely
maintain causal factor consistency. 2) Based on our causal analysis, we present
a wavelet transform-based backbone network that effectively optimizes the
recovery of frequency information, ensuring precise enhancement tailored to the
specific attributes of wavelet transforms. Extensive experiments demonstrate
that CWNet significantly outperforms current state-of-the-art methods across
multiple datasets, showcasing its robust performance across diverse scenes.
Code is available at https://github.com/bywlzts/CWNet-Causal-Wavelet-Network.

</details>


### [93] [Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines](https://arxiv.org/abs/2507.10737)
*Jiayuan Chen,Thai-Hoang Pham,Yuanlong Wang,Ping Zhang*

Main category: cs.CV

TL;DR: This paper introduces a novel framework to improve microscopy image profiling models for drug discovery by integrating external biological knowledge into pretraining strategies.


<details>
  <summary>Details</summary>
Motivation: Robust perturbation screening for new cell lines is challenging due to their morphological and biological heterogeneity, limiting advancements in biomedical research and drug discovery.

Method: The proposed framework disentangles perturbation-specific and cell line-specific representations by using a knowledge graph built from protein interaction databases (STRING and Hetionet) and incorporating transcriptomic features from single-cell foundation models.

Result: The method enhances generalization to novel cell lines, validated on the RxRx database with improved performance in one-shot and few-shot fine-tuning for cell line screening applications.

Conclusion: Integrating external biological knowledge into imaging models improves their ability to generalize, making them more effective for phenotype-based drug discovery with 	extit{de novo} cell lines.

Abstract: High-throughput screening techniques, such as microscopy imaging of cellular
responses to genetic and chemical perturbations, play a crucial role in drug
discovery and biomedical research. However, robust perturbation screening for
\textit{de novo} cell lines remains challenging due to the significant
morphological and biological heterogeneity across cell lines. To address this,
we propose a novel framework that integrates external biological knowledge into
existing pretraining strategies to enhance microscopy image profiling models.
Our approach explicitly disentangles perturbation-specific and cell
line-specific representations using external biological information.
Specifically, we construct a knowledge graph leveraging protein interaction
data from STRING and Hetionet databases to guide models toward
perturbation-specific features during pretraining. Additionally, we incorporate
transcriptomic features from single-cell foundation models to capture cell
line-specific representations. By learning these disentangled features, our
method improves the generalization of imaging models to \textit{de novo} cell
lines. We evaluate our framework on the RxRx database through one-shot
fine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from
the RxRx19a dataset. Experimental results demonstrate that our method enhances
microscopy image profiling for \textit{de novo} cell lines, highlighting its
effectiveness in real-world phenotype-based drug discovery applications.

</details>


### [94] [Auditing Facial Emotion Recognition Datasets for Posed Expressions and Racial Bias](https://arxiv.org/abs/2507.10755)
*Rina Khan,Catherine Stinson*

Main category: cs.CV

TL;DR: The paper audits FER datasets for biases, discovering issues with posed images labeled as in-the-wild and racial biases in predictions, particularly for darker-skinned individuals.


<details>
  <summary>Details</summary>
Motivation: To address evaluation and ethical challenges in FER algorithms, particularly their poor performance on spontaneous expressions and biases against certain races and skin tones.

Method: Auditing two state-of-the-art FER datasets through random sampling to identify spontaneous vs posed images and assessing racial biases using three FER models.

Result: Significant inclusion of posed images in datasets labeled as in-the-wild and racial biases where people with darker skin tones were more often detected with negative emotions even when smiling.

Conclusion: Current FER datasets and models exhibit biases that could lead to inaccurate and potentially harmful real-world applications, necessitating improved data curation and fairness efforts.

Abstract: Facial expression recognition (FER) algorithms classify facial expressions
into emotions such as happy, sad, or angry. An evaluative challenge facing FER
algorithms is the fall in performance when detecting spontaneous expressions
compared to posed expressions. An ethical (and evaluative) challenge facing FER
algorithms is that they tend to perform poorly for people of some races and
skin colors. These challenges are linked to the data collection practices
employed in the creation of FER datasets. In this study, we audit two
state-of-the-art FER datasets. We take random samples from each dataset and
examine whether images are spontaneous or posed. In doing so, we propose a
methodology for identifying spontaneous or posed images. We discover a
significant number of images that were posed in the datasets purporting to
consist of in-the-wild images. Since performance of FER models vary between
spontaneous and posed images, the performance of models trained on these
datasets will not represent the true performance if such models were to be
deployed in in-the-wild applications. We also observe the skin color of
individuals in the samples, and test three models trained on each of the
datasets to predict facial expressions of people from various races and skin
tones. We find that the FER models audited were more likely to predict people
labeled as not white or determined to have dark skin as showing a negative
emotion such as anger or sadness even when they were smiling. This bias makes
such models prone to perpetuate harm in real life applications.

</details>


### [95] [FPC-Net: Revisiting SuperPoint with Descriptor-Free Keypoint Detection via Feature Pyramids and Consistency-Based Implicit Matching](https://arxiv.org/abs/2507.10770)
*Ionuţ Grigore,Călin-Adrian Popa,Claudiu Leoveanu-Condrei*

Main category: cs.CV

TL;DR: The paper introduces a novel interest point matching technique that eliminates the need for descriptors, significantly reducing memory usage.


<details>
  <summary>Details</summary>
Motivation: Traditional interest point matching relies on descriptors, which require significant computation, storage, and transmission resources.

Method: The authors propose a method where interest points are inherently associated during detection, avoiding descriptor computation, storage, and matching entirely.

Result: The proposed method achieves slightly lower matching accuracy compared to traditional methods but substantially reduces memory usage for localization systems.

Conclusion: The technique offers a trade-off by sacrificing minimal accuracy to gain significant efficiency in memory and computational resources, making it viable for localization tasks.

Abstract: The extraction and matching of interest points are fundamental to many
geometric computer vision tasks. Traditionally, matching is performed by
assigning descriptors to interest points and identifying correspondences based
on descriptor similarity. This work introduces a technique where interest
points are inherently associated during detection, eliminating the need for
computing, storing, transmitting, or matching descriptors. Although the
matching accuracy is marginally lower than that of conventional approaches, our
method completely eliminates the need for descriptors, leading to a drastic
reduction in memory usage for localization systems. We assess its effectiveness
by comparing it against both classical handcrafted methods and modern learned
approaches.

</details>


### [96] [A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Flight Computers](https://arxiv.org/abs/2507.10775)
*Jeffrey Joan Sam,Janhavi Sathe,Nikhil Chigali,Naman Gupta,Radhey Ruparel,Yicheng Jiang,Janmajay Singh,James W. Berck,Arko Barman*

Main category: cs.CV

TL;DR: The paper introduces a new dataset of 64k annotated spacecraft images and benchmarks YOLO models for autonomous inspection in space.


<details>
  <summary>Details</summary>
Motivation: To reduce costs and risks in spacecraft inspection and repair by enabling reliable autonomous systems.

Method: Created a 64k annotated spacecraft image dataset using real models and NASA’s TTALOS. Fine-tuned YOLOv8 and YOLOv11 models and added noise/distortion to simulate real-world conditions.

Result: Achieved a Dice score of 0.92, a Hausdorff distance of 0.69, and an inference time of 0.5 seconds under real-time constraints.

Conclusion: The dataset and benchmarks lay groundwork for developing cost-effective, reliable, and real-time onboard autonomous inspection systems for spacecraft.

Abstract: Spacecraft deployed in outer space are routinely subjected to various forms
of damage due to exposure to hazardous environments. In addition, there are
significant risks to the subsequent process of in-space repairs through human
extravehicular activity or robotic manipulation, incurring substantial
operational costs. Recent developments in image segmentation could enable the
development of reliable and cost-effective autonomous inspection systems. While
these models often require large amounts of training data to achieve
satisfactory results, publicly available annotated spacecraft segmentation data
are very scarce. Here, we present a new dataset of nearly 64k annotated
spacecraft images that was created using real spacecraft models, superimposed
on a mixture of real and synthetic backgrounds generated using NASA's TTALOS
pipeline. To mimic camera distortions and noise in real-world image
acquisition, we also added different types of noise and distortion to the
images. Finally, we finetuned YOLOv8 and YOLOv11 segmentation models to
generate performance benchmarks for the dataset under well-defined hardware and
inference time constraints to mimic real-world image segmentation challenges
for real-time onboard applications in space on NASA's inspector spacecraft. The
resulting models, when tested under these constraints, achieved a Dice score of
0.92, Hausdorff distance of 0.69, and an inference time of about 0.5 second.
The dataset and models for performance benchmark are available at
https://github.com/RiceD2KLab/SWiM.

</details>


### [97] [Warehouse Spatial Question Answering with LLM Agent](https://arxiv.org/abs/2507.10778)
*Hsiang-Wei Huang,Jen-Hao Cheng,Kuang-Ming Chen,Cheng-Yen Yang,Bahaa Alattar,Yi-Ru Lin,Pyongkun Kim,Sangwon Kim,Kwangju Kim,Chung-I Huang,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: A data-efficient LLM agent system was proposed for spatial reasoning tasks, achieving high accuracy in complex indoor warehouse scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with spatial understanding, and previous approaches require extensive finetuning on large datasets.

Method: Developing an LLM agent system integrating advanced spatial reasoning and API tool interaction capabilities.

Result: Improved performance on the 2025 AI City Challenge Physical AI Spatial Intelligence Warehouse dataset, excelling in object retrieval, counting, and distance estimation.

Conclusion: The proposed system demonstrates strong spatial reasoning abilities with efficient data use, addressing the challenges of spatial question answering efficiently.

Abstract: Spatial understanding has been a challenging task for existing Multi-modal
Large Language Models~(MLLMs). Previous methods leverage large-scale MLLM
finetuning to enhance MLLM's spatial understanding ability. In this paper, we
present a data-efficient approach. We propose a LLM agent system with strong
and advanced spatial reasoning ability, which can be used to solve the
challenging spatial question answering task in complex indoor warehouse
scenarios. Our system integrates multiple tools that allow the LLM agent to
conduct spatial reasoning and API tools interaction to answer the given
complicated spatial question. Extensive evaluations on the 2025 AI City
Challenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that
our system achieves high accuracy and efficiency in tasks such as object
retrieval, counting, and distance estimation. The code is available at:
https://github.com/hsiangwei0903/SpatialAgent

</details>


### [98] [ThinkingViT: Matryoshka Thinking Vision Transformer for Elastic Inference](https://arxiv.org/abs/2507.10800)
*Ali Hojjat,Janek Haberer,Soren Pirk,Olaf Landsiedel*

Main category: cs.CV

TL;DR: ThinkingViT introduces a progressive thinking mechanism to dynamically allocate computational resources based on input complexity, improving efficiency and performance compared to fixed-budget Vision Transformers.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers (ViTs) often inefficiently allocate compute to all inputs regardless of complexity, limiting their scalability.

Method: ThinkingViT employs nested attention heads and a Token Recycling mechanism, progressively adjusting inference computation based on input certainty.

Result: ThinkingViT achieves up to 2.0% higher accuracy at the same throughput and up to 2.9% higher accuracy at equal GMACs compared to nested baselines on ImageNet-1K.

Conclusion: ThinkingViT enhances computational efficiency and accuracy in ViTs, serving as a scalable upgrade while maintaining compatibility with vanilla ViT architectures.

Abstract: Vision Transformers deliver state-of-the-art performance, yet their fixed
computational budget prevents scalable deployment across heterogeneous
hardware. Recent nested Transformer architectures mitigate this by embedding
nested subnetworks within a single model to enable scalable inference. However,
these models allocate the same amount of compute to all inputs, regardless of
their complexity, which leads to inefficiencies. To address this, we introduce
ThinkingViT, a nested ViT architecture that employs progressive thinking stages
to dynamically adjust inference computation based on input difficulty.
ThinkingViT initiates inference by activating a small subset of the most
important attention heads and terminates early if predictions reach sufficient
certainty. Otherwise, it activates additional attention heads and re-evaluates
the input. At the core of ThinkingViT is our Token Recycling mechanism, which
conditions each subsequent inference stage on the embeddings from the previous
stage, enabling progressive improvement. Due to its backbone-preserving design,
ThinkingViT also serves as a plugin upgrade for vanilla ViT. Experiments show
that ThinkingViT surpasses nested baselines by up to 2.0 percentage points
(p.p.) in accuracy at the same throughput and by up to 2.9 p.p. at equal GMACs
on ImageNet-1K. The source code is available at
https://github.com/ds-kiel/ThinkingViT.

</details>


### [99] [LLM-Guided Agentic Object Detection for Open-World Understanding](https://arxiv.org/abs/2507.10844)
*Furkan Mumcu,Michael J. Jones,Anoop Cherian,Yasin Yilmaz*

Main category: cs.CV

TL;DR: This paper introduces a framework for agentic object detection using Large Language Models (LLMs) to enable label-free, zero-shot detection of objects in dynamic scenes without the need for re-training.


<details>
  <summary>Details</summary>
Motivation: Traditional object detection requires fixed categories, making it costly to adapt to unknown objects, while existing systems lack either semantic autonomy (OWOD) or depend on user prompts (OVOD).

Method: The proposed LAOD framework uses an LLM to generate scene-specific object names dynamically, which are then localized by an open-vocabulary detector. Two new metrics (CAAP and SNAP) are introduced to evaluate detection performance.

Result: Experiments conducted on LVIS, COCO, and COCO-OOD datasets demonstrated strong performance in both detecting and naming novel objects, validating the approach.

Conclusion: The LAOD framework offers improved autonomy and adaptability for open-world object detection by enabling label-free and zero-shot detection guided by LLMs.

Abstract: Object detection traditionally relies on fixed category sets, requiring
costly re-training to handle novel objects. While Open-World and
Open-Vocabulary Object Detection (OWOD and OVOD) improve flexibility, OWOD
lacks semantic labels for unknowns, and OVOD depends on user prompts, limiting
autonomy. We propose an LLM-guided agentic object detection (LAOD) framework
that enables fully label-free, zero-shot detection by prompting a Large
Language Model (LLM) to generate scene-specific object names. These are passed
to an open-vocabulary detector for localization, allowing the system to adapt
its goals dynamically. We introduce two new metrics, Class-Agnostic Average
Precision (CAAP) and Semantic Naming Average Precision (SNAP), to separately
evaluate localization and naming. Experiments on LVIS, COCO, and COCO-OOD
validate our approach, showing strong performance in detecting and naming novel
objects. Our method offers enhanced autonomy and adaptability for open-world
understanding.

</details>


### [100] [Winsor-CAM: Human-Tunable Visual Explanations from Deep Networks via Layer-Wise Winsorization](https://arxiv.org/abs/2507.10846)
*Casey Wall,Longwei Wang,Rodrigue Rizk,KC Santosh*

Main category: cs.CV

TL;DR: This paper introduces Winsor-CAM, a new enhancement to Grad-CAM, which creates improved and interpretable saliency maps for CNNs by aggregating information across layers with Winsorization to limit noise and extreme attributions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations in Grad-CAM's ability to accurately and coherently interpret the decision-making of CNNs, especially its focus on final layers or naive averaging, which can miss semantic cues or introduce noise.

Method: Winsor-CAM aggregates information across all convolutional layers, applying Winsorization to attenuate extreme attribution values. It also provides a user-adjustable threshold for flexible tuning of semantic details in the saliency maps.

Result: The method produces more interpretable heatmaps and outperforms Grad-CAM and other averaging methods in localization tasks, evidenced by better intersection-over-union and center-of-mass alignment scores on benchmarks like PASCAL VOC 2012.

Conclusion: Winsor-CAM enhances CNN interpretability by generating robust multi-layer saliency maps, offering better performance and allowing human-in-the-loop control for more trustworthy AI applications.

Abstract: Interpreting the decision-making process of Convolutional Neural Networks
(CNNs) is critical for deploying models in high-stakes domains.
Gradient-weighted Class Activation Mapping (Grad-CAM) is a widely used method
for visual explanations, yet it typically focuses on the final convolutional
layer or na\"ively averages across layers, strategies that can obscure
important semantic cues or amplify irrelevant noise. We propose Winsor-CAM, a
novel, human-tunable extension of Grad-CAM that generates robust and coherent
saliency maps by aggregating information across all convolutional layers. To
mitigate the influence of noisy or extreme attribution values, Winsor-CAM
applies Winsorization, a percentile-based outlier attenuation technique. A
user-controllable threshold allows for semantic-level tuning, enabling flexible
exploration of model behavior across representational hierarchies. Evaluations
on standard architectures (ResNet50, DenseNet121, VGG16, InceptionV3) using the
PASCAL VOC 2012 dataset demonstrate that Winsor-CAM produces more interpretable
heatmaps and achieves superior performance in localization metrics, including
intersection-over-union and center-of-mass alignment, when compared to Grad-CAM
and uniform layer-averaging baselines. Winsor-CAM advances the goal of
trustworthy AI by offering interpretable, multi-layer insights with
human-in-the-loop control.

</details>


### [101] [Sparse Fine-Tuning of Transformers for Generative Tasks](https://arxiv.org/abs/2507.10855)
*Wei Chen,Jingxi Yu,Zichen Miao,Qiang Qiu*

Main category: cs.CV

TL;DR: This paper presents a sparse coding-inspired fine-tuning framework for pre-trained transformers, which makes updated representations interpretable and improves performance in tasks like image editing and text-to-image concept customization.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning is the dominant method to adapt large pre-trained transformers to downstream tasks, but dense parameter combinations hinder interpretability. The authors aim to make the adaptation interpretable and efficient.

Method: The authors propose a sparse coding-inspired framework where fine-tuned features are represented as sparse combinations of feature dictionary atoms. Sparse coefficients quantify the importance of individual atoms.

Result: The framework enhances text alignment in image editing by removing unimportant atoms and outperforms baseline fine-tuning methods in text-to-image concept customization tasks.

Conclusion: Sparse coding improves interpretability and efficiency in model adaptation, indicating its potential to refine downstream tasks with better performance and understanding.

Abstract: Large pre-trained transformers have revolutionized artificial intelligence
across various domains, and fine-tuning remains the dominant approach for
adapting these models to downstream tasks due to the cost of training from
scratch. However, in existing fine-tuning methods, the updated representations
are formed as a dense combination of modified parameters, making it challenging
to interpret their contributions and understand how the model adapts to new
tasks. In this work, we introduce a fine-tuning framework inspired by sparse
coding, where fine-tuned features are represented as a sparse combination of
basic elements, i.e., feature dictionary atoms. The feature dictionary atoms
function as fundamental building blocks of the representation, and tuning atoms
allows for seamless adaptation to downstream tasks. Sparse coefficients then
serve as indicators of atom importance, identifying the contribution of each
atom to the updated representation. Leveraging the atom selection capability of
sparse coefficients, we first demonstrate that our method enhances image
editing performance by improving text alignment through the removal of
unimportant feature dictionary atoms. Additionally, we validate the
effectiveness of our approach in the text-to-image concept customization task,
where our method efficiently constructs the target concept using a sparse
combination of feature dictionary atoms, outperforming various baseline
fine-tuning methods.

</details>


### [102] [A Lightweight and Robust Framework for Real-Time Colorectal Polyp Detection Using LOF-Based Preprocessing and YOLO-v11n](https://arxiv.org/abs/2507.10864)
*Saadat Behzadi,Danial Sharifrazi,Bita Mesbahzadeh,Javad Hassannataj Joloudarid,Roohallah Alizadehsani*

Main category: cs.CV

TL;DR: This paper introduces an efficient framework for detecting colorectal polyps combining outlier removal and an optimized deep learning model, achieving high accuracy across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Colorectal cancer is a global health concern, and timely detection of polyps can prevent its progression. The paper seeks to address the need for accurate and efficient detection systems.

Method: The study applies the Local Outlier Factor (LOF) algorithm to preprocess data and YOLO-v11n—a resource-efficient deep learning model—on five public datasets, emphasizing outlier removal and robust training techniques.

Result: The framework achieved high performance with precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5 of 96.48%, and mAP@0.5:0.95 of 77.75%, showcasing improved accuracy over existing YOLO methods.

Conclusion: The presented method demonstrates real-time applicability in colonoscopy settings, emphasizing the importance of data preprocessing and model efficiency in medical AI systems.

Abstract: Objectives: Timely and accurate detection of colorectal polyps plays a
crucial role in diagnosing and preventing colorectal cancer, a major cause of
mortality worldwide. This study introduces a new, lightweight, and efficient
framework for polyp detection that combines the Local Outlier Factor (LOF)
algorithm for filtering noisy data with the YOLO-v11n deep learning model.
  Study design: An experimental study leveraging deep learning and outlier
removal techniques across multiple public datasets.
  Methods: The proposed approach was tested on five diverse and publicly
available datasets: CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG, ETIS, and EndoScene.
Since these datasets originally lacked bounding box annotations, we converted
their segmentation masks into suitable detection labels. To enhance the
robustness and generalizability of our model, we apply 5-fold cross-validation
and remove anomalous samples using the LOF method configured with 30 neighbors
and a contamination ratio of 5%. Cleaned data are then fed into YOLO-v11n, a
fast and resource-efficient object detection architecture optimized for
real-time applications. We train the model using a combination of modern
augmentation strategies to improve detection accuracy under diverse conditions.
  Results: Our approach significantly improves polyp localization performance,
achieving a precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5
of 96.48%, and mAP@0.5:0.95 of 77.75%. Compared to previous YOLO-based methods,
our model demonstrates enhanced accuracy and efficiency.
  Conclusions: These results suggest that the proposed method is well-suited
for real-time colonoscopy support in clinical settings. Overall, the study
underscores how crucial data preprocessing and model efficiency are when
designing effective AI systems for medical imaging.

</details>


### [103] [Trexplorer Super: Topologically Correct Centerline Tree Tracking of Tubular Objects in CT Volumes](https://arxiv.org/abs/2507.10881)
*Roman Naeem,David Hagerman,Jennifer Alvén,Lennart Svensson,Fredrik Kahl*

Main category: cs.CV

TL;DR: This study introduces Trexplorer Super, an improved model for centerline tracking in 3D medical images, and evaluates it using three newly developed datasets.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations of the existing Trexplorer model, which struggles with predicting duplicate branches and premature terminations, and to produce an accurate centerline tracking model for 3D medical images.

Method: Trexplorer Super is developed as an enhancement to the Trexplorer model, and its evaluation is carried out using three newly created centerline datasets: one synthetic and two real, with varying difficulty.

Result: Trexplorer Super outperforms existing state-of-the-art (SOTA) centerline tracking models on all three datasets, demonstrating its superior performance both quantitatively and qualitatively.

Conclusion: The proposed Trexplorer Super sets a new benchmark in centerline tracking for 3D medical images, but the study also reveals that strong model performance on synthetic data doesn't guarantee success on real data.

Abstract: Tubular tree structures, such as blood vessels and airways, are essential in
human anatomy and accurately tracking them while preserving their topology is
crucial for various downstream tasks. Trexplorer is a recurrent model designed
for centerline tracking in 3D medical images but it struggles with predicting
duplicate branches and terminating tracking prematurely. To address these
issues, we present Trexplorer Super, an enhanced version that notably improves
performance through novel advancements. However, evaluating centerline tracking
models is challenging due to the lack of public datasets. To enable thorough
evaluation, we develop three centerline datasets, one synthetic and two real,
each with increasing difficulty. Using these datasets, we conduct a
comprehensive evaluation of existing state-of-the-art (SOTA) models and compare
them with our approach. Trexplorer Super outperforms previous SOTA models on
every dataset. Our results also highlight that strong performance on synthetic
data does not necessarily translate to real datasets. The code and datasets are
available at https://github.com/RomStriker/Trexplorer-Super.

</details>


### [104] [Modernizing CNN-based Weather Forecast Model towards Higher Computational Efficiency](https://arxiv.org/abs/2507.10893)
*Minjong Cheon,Eunhan Goo,Su-Hyeon Shin,Muhammad Ahmed,Hyungjun Kim*

Main category: cs.CV

TL;DR: This paper presents a modernized CNN-based weather forecasting model, KAI-a, that delivers competitive accuracy with reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: Traditional AI-based weather models rely on transformers, which are computationally intensive due to large parameter sizes.

Method: KAI-a employs a scale-invariant architecture and InceptionNeXt-based blocks tailored for Earth system data, and trains on ERA5 data with 67 atmospheric variables.

Result: The model contains only 7 million parameters, completes training in 12 hours using a single NVIDIA GPU, and achieves accuracy comparable to state-of-the-art models.

Conclusion: KAI-a offers a lightweight yet effective alternative for global weather forecasting, proving its capability in capturing extreme weather events like the 2018 European heatwave and East Asian monsoon.

Abstract: Recently, AI-based weather forecast models have achieved impressive advances.
These models have reached accuracy levels comparable to traditional NWP
systems, marking a significant milestone in data-driven weather prediction.
However, they mostly leverage Transformer-based architectures, which often
leads to high training complexity and resource demands due to the massive
parameter sizes. In this study, we introduce a modernized CNN-based model for
global weather forecasting that delivers competitive accuracy while
significantly reducing computational requirements. To present a systematic
modernization roadmap, we highlight key architectural enhancements across
multiple design scales from an earlier CNN-based approach. KAI-a incorporates a
scale-invariant architecture and InceptionNeXt-based blocks within a
geophysically-aware design, tailored to the structure of Earth system data.
Trained on the ERA5 daily dataset with 67 atmospheric variables, the model
contains about 7 million parameters and completes training in just 12 hours on
a single NVIDIA L40s GPU. Our evaluation shows that KAI-a matches the
performance of state-of-the-art models in medium-range weather forecasting,
while offering a significantly lightweight design. Furthermore, case studies on
the 2018 European heatwave and the East Asian summer monsoon demonstrate
KAI-a's robust skill in capturing extreme events, reinforcing its practical
utility.

</details>


### [105] [Commuting Distance Regularization for Timescale-Dependent Label Inconsistency in EEG Emotion Recognition](https://arxiv.org/abs/2507.10895)
*Xiaocong Zeng,Craig Michoski,Yan Pang,Dongyang Kuang*

Main category: cs.CV

TL;DR: The paper addresses Timescale Dependent Label Inconsistency (TsDLI) in EEG-based emotion recognition by introducing two novel regularization strategies, Local Variation Loss (LVL) and Local-Global Consistency Loss (LGCL), validated on benchmark datasets using various neural architectures.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve model generalization and interpretability in EEG-based human emotion recognition, which often suffers from Timescale Dependent Label Inconsistency, an issue typically overlooked in the existing approaches.

Method: The paper introduces two regularization strategies, LVL and LGCL, grounded in mathematical principles like bounded variation and commute-time distances, within a graph-theoretic framework. It also proposes new evaluation metrics for better assessment of temporal and global label alignment.

Result: In extensive experiments on the DREAMER and DEAP datasets using various neural architectures (e.g., LSTM, transformer-based models), the proposed methods outperform state-of-the-art baselines across five metrics, with LVL performing best overall and LGCL frequently second.

Conclusion: The proposed LVL and LGCL regularizers enhance the trade-off between prediction accuracy and interpretability in EEG emotion recognition, effectively addressing TsDLI and setting new state-of-the-art performance benchmarks.

Abstract: In this work, we address the often-overlooked issue of Timescale Dependent
Label Inconsistency (TsDLI) in training neural network models for EEG-based
human emotion recognition. To mitigate TsDLI and enhance model generalization
and explainability, we propose two novel regularization strategies: Local
Variation Loss (LVL) and Local-Global Consistency Loss (LGCL). Both methods
incorporate classical mathematical principles--specifically, functions of
bounded variation and commute-time distances--within a graph theoretic
framework. Complementing our regularizers, we introduce a suite of new
evaluation metrics that better capture the alignment between temporally local
predictions and their associated global emotion labels. We validate our
approach through comprehensive experiments on two widely used EEG emotion
datasets, DREAMER and DEAP, across a range of neural architectures including
LSTM and transformer-based models. Performance is assessed using five distinct
metrics encompassing both quantitative accuracy and qualitative consistency.
Results consistently show that our proposed methods outperform state-of-the-art
baselines, delivering superior aggregate performance and offering a principled
trade-off between interpretability and predictive power under label
inconsistency. Notably, LVL achieves the best aggregate rank across all
benchmarked backbones and metrics, while LGCL frequently ranks the second,
highlighting the effectiveness of our framework.

</details>


### [106] [GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization](https://arxiv.org/abs/2507.10935)
*Shaowen Tong,Zimin Xia,Alexandre Alahi,Xuming He,Yujiao Shi*

Main category: cs.CV

TL;DR: The paper introduces GeoDistill, a geometry-guided weakly supervised framework for efficient cross-view localization of camera poses without requiring costly pose annotations.


<details>
  <summary>Details</summary>
Motivation: To address challenges in cross-view localization where obtaining ground-truth pose annotations is expensive, the paper focuses on creating a robust and scalable method for outdoor applications such as autonomous navigation and augmented reality.

Method: The proposed GeoDistill framework uses a teacher-student learning paradigm combined with Field-of-View (FoV)-based masking to enable effective local feature learning. The teacher model works on panoramic images while the student model, designed for limited FoV images, aligns its predictions with the teacher's output through self-distillation.

Result: The framework enhances localization performance across various conditions by focusing on salient features like lane lines and filtering out non-informative areas. Additionally, a novel orientation estimation network is introduced to predict relative orientation without relying on precise planar ground truth.

Conclusion: GeoDistill offers a scalable and efficient approach for cross-view localization, reducing reliance on costly data annotations while achieving robust and accurate results applicable to real-world scenarios.

Abstract: Cross-view localization, the task of estimating a camera's
3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with
satellite images, is crucial for large-scale outdoor applications like
autonomous navigation and augmented reality. Existing methods often rely on
fully supervised learning, which requires costly ground-truth pose annotations.
In this work, we propose GeoDistill, a Geometry guided weakly supervised self
distillation framework that uses teacher-student learning with Field-of-View
(FoV)-based masking to enhance local feature learning for robust cross-view
localization. In GeoDistill, the teacher model localizes a panoramic image,
while the student model predicts locations from a limited FoV counterpart
created by FoV-based masking. By aligning the student's predictions with those
of the teacher, the student focuses on key features like lane lines and ignores
textureless regions, such as roads. This results in more accurate predictions
and reduced uncertainty, regardless of whether the query images are panoramas
or limited FoV images. Our experiments show that GeoDistill significantly
improves localization performance across different frameworks. Additionally, we
introduce a novel orientation estimation network that predicts relative
orientation without requiring precise planar position ground truth. GeoDistill
provides a scalable and efficient solution for real-world cross-view
localization challenges. Code and model can be found at
https://github.com/tongshw/GeoDistill.

</details>


### [107] [Graph Aggregation Prototype Learning for Semantic Change Detection in Remote Sensing](https://arxiv.org/abs/2507.10938)
*Zhengyi Xu,Haoran Wu,Wen Jiang,Jie Geng*

Main category: cs.CV

TL;DR: This paper addresses semantic change detection in remote sensing data by introducing GAPL-SCD, a framework that improves multi-task learning for detecting detailed 'from-to' category changes with high accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Semantic change detection offers detailed insights into changes in multi-temporal remote sensing data but faces challenges due to conflicting tasks and gradient flows during optimization.

Method: The paper introduces GAPL-SCD, employing adaptive weight allocation, gradient rotation, graph aggregation prototype learning, self-query multi-level feature interaction, and bi-temporal feature fusion for more effective multi-task learning.

Result: GAPL-SCD demonstrates state-of-the-art performance on SECOND and Landsat-SCD datasets, showing significant accuracy and robustness improvements in semantic change detection.

Conclusion: GAPL-SCD successfully mitigates challenges in multi-task learning for semantic change detection, offering enhanced capabilities and reliability in complex remote sensing scenarios.

Abstract: Semantic change detection (SCD) extends the binary change detection task to
provide not only the change locations but also the detailed "from-to"
categories in multi-temporal remote sensing data. Such detailed semantic
insights into changes offer considerable advantages for a wide array of
applications. However, since SCD involves the simultaneous optimization of
multiple tasks, the model is prone to negative transfer due to task-specific
learning difficulties and conflicting gradient flows. To address this issue, we
propose Graph Aggregation Prototype Learning for Semantic Change Detection in
remote sensing(GAPL-SCD). In this framework, a multi-task joint optimization
method is designed to optimize the primary task of semantic segmentation and
change detection, along with the auxiliary task of graph aggregation prototype
learning. Adaptive weight allocation and gradient rotation methods are used to
alleviate the conflict between training tasks and improve multi-task learning
capabilities. Specifically, the graph aggregation prototype learning module
constructs an interaction graph using high-level features. Prototypes serve as
class proxies, enabling category-level domain alignment across time points and
reducing interference from irrelevant changes. Additionally, the proposed
self-query multi-level feature interaction and bi-temporal feature fusion
modules further enhance multi-scale feature representation, improving
performance in complex scenes. Experimental results on the SECOND and
Landsat-SCD datasets demonstrate that our method achieves state-of-the-art
performance, with significant improvements in accuracy and robustness for SCD
task.

</details>


### [108] [Robust ID-Specific Face Restoration via Alignment Learning](https://arxiv.org/abs/2507.10943)
*Yushun Fang,Lu Liu,Xiang Gao,Qiang Hu,Ning Cao,Jianghe Cui,Gang Chen,Xiaoyun Zhang*

Main category: cs.CV

TL;DR: The paper introduces RIDFR, a novel framework using diffusion models for restoring high-quality, identity-consistent faces from degraded inputs.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identity uncertainty in face restoration caused by obscure inputs and stochastic generative processes.

Method: The RIDFR framework combines a pre-trained diffusion model with two conditioning modules: Content Injection for degraded images and Identity Injection for specific identity integration, along with Alignment Learning to align restored faces across references.

Result: Experiments show RIDFR achieves superior face restoration with high-quality and identity fidelity, outperforming state-of-the-art methods.

Conclusion: RIDFR effectively addresses identity fidelity and robustness challenges in face restoration, demonstrating its innovative approach and practical efficacy.

Abstract: The latest developments in Face Restoration have yielded significant
advancements in visual quality through the utilization of diverse diffusion
priors. Nevertheless, the uncertainty of face identity introduced by
identity-obscure inputs and stochastic generative processes remains unresolved.
To address this challenge, we present Robust ID-Specific Face Restoration
(RIDFR), a novel ID-specific face restoration framework based on diffusion
models. Specifically, RIDFR leverages a pre-trained diffusion model in
conjunction with two parallel conditioning modules. The Content Injection
Module inputs the severely degraded image, while the Identity Injection Module
integrates the specific identity from a given image. Subsequently, RIDFR
incorporates Alignment Learning, which aligns the restoration results from
multiple references with the same identity in order to suppress the
interference of ID-irrelevant face semantics (e.g. pose, expression, make-up,
hair style). Experiments demonstrate that our framework outperforms the
state-of-the-art methods, reconstructing high-quality ID-specific results with
high identity fidelity and demonstrating strong robustness.

</details>


### [109] [Women Sport Actions Dataset for Visual Classification Using Small Scale Training Data](https://arxiv.org/abs/2507.10969)
*Palash Ray,Mahuya Sasmal,Asish Bera*

Main category: cs.CV

TL;DR: This paper introduces the WomenSports dataset for classifying women's sports actions and proposes a CNN-based method with attention schemes, achieving high classification accuracy.


<details>
  <summary>Details</summary>
Motivation: The researchers aim to address the lack of image datasets for women's sports actions with enough inter- and intra-class diversity, which limits advancements in this area.

Method: A new dataset named WomenSports is introduced, along with a CNN using channel attention on local contextual regions for deep feature extraction. Experiments were conducted on several datasets to validate the method.

Result: The proposed method achieved 89.15% top-1 classification accuracy using ResNet-50 on the WomenSports dataset.

Conclusion: The study provides a significant contribution by introducing a publicly available dataset and a deep learning method that delivers noteworthy performance in women's sports action classification.

Abstract: Sports action classification representing complex body postures and
player-object interactions is an emerging area in image-based sports analysis.
Some works have contributed to automated sports action recognition using
machine learning techniques over the past decades. However, sufficient image
datasets representing women sports actions with enough intra- and inter-class
variations are not available to the researchers. To overcome this limitation,
this work presents a new dataset named WomenSports for women sports
classification using small-scale training data. This dataset includes a variety
of sports activities, covering wide variations in movements, environments, and
interactions among players. In addition, this study proposes a convolutional
neural network (CNN) for deep feature extraction. A channel attention scheme
upon local contextual regions is applied to refine and enhance feature
representation. The experiments are carried out on three different sports
datasets and one dance dataset for generalizing the proposed algorithm, and the
performances on these datasets are noteworthy. The deep learning method
achieves 89.15% top-1 classification accuracy using ResNet-50 on the proposed
WomenSports dataset, which is publicly available for research at Mendeley Data.

</details>


### [110] [Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection](https://arxiv.org/abs/2507.10977)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

TL;DR: This paper introduces a novel architecture combining a wavelet attention-like backbone with a ray-based encoder to improve efficiency and accuracy in human-object interaction (HOI) detection.


<details>
  <summary>Details</summary>
Motivation: Existing HOI detectors face challenges like inefficient architectures and resource-intensive training methods, which hinder reliable and efficient prediction.

Method: The approach integrates a wavelet attention-like backbone for aggregating low- and high-order interaction features, alongside a ray-based encoder enabling multi-scale attention and optimized focus on regions of interest.

Result: Experiments on benchmark datasets like ImageNet and HICO-DET demonstrate the architecture's enhanced HOI detection capabilities, achieving efficient predictions with improved accuracy.

Conclusion: The proposed architecture effectively addresses key limitations in HOI detection, offering a computationally efficient solution and aligning query embeddings with relevant regions to deliver more accurate predictions.

Abstract: Human-object interaction (HOI) detection is essential for accurately
localizing and characterizing interactions between humans and objects,
providing a comprehensive understanding of complex visual scenes across various
domains. However, existing HOI detectors often struggle to deliver reliable
predictions efficiently, relying on resource-intensive training methods and
inefficient architectures. To address these challenges, we conceptualize a
wavelet attention-like backbone and a novel ray-based encoder architecture
tailored for HOI detection. Our wavelet backbone addresses the limitations of
expressing middle-order interactions by aggregating discriminative features
from the low- and high-order interactions extracted from diverse convolutional
filters. Concurrently, the ray-based encoder facilitates multi-scale attention
by optimizing the focus of the decoder on relevant regions of interest and
mitigating computational overhead. As a result of harnessing the attenuated
intensity of learnable ray origins, our decoder aligns query embeddings with
emphasized regions of interest for accurate predictions. Experimental results
on benchmark datasets, including ImageNet and HICO-DET, showcase the potential
of our proposed architecture. The code is publicly available at
[https://github.com/henry-pay/RayEncoder].

</details>


### [111] [Mind the Gap: Bridging Occlusion in Gait Recognition via Residual Gap Correction](https://arxiv.org/abs/2507.10978)
*Ayush Gupta,Siyuan Huang,Rama Chellappa*

Main category: cs.CV

TL;DR: The study proposes RG-Gait, an innovative method to improve gait-based person re-identification, addressing occlusions while retaining performance on holistic data.


<details>
  <summary>Details</summary>
Motivation: Gait is a robust method for identifying people at a distance. However, existing methods struggle with occlusions in real-world scenarios and fail to maintain accuracy on holistic gait recognition.

Method: The authors introduced RG-Gait, which frames occluded gait identification as a residual learning task. It uses a network to adaptively integrate residual deviations to improve performance on occluded sequences, preserving holistic recognition capabilities.

Result: RG-Gait demonstrates significant improvements in recognizing occluded gait sequences and maintains high performance on holistic inputs. Results were validated on challenging datasets: Gait3D, GREW, and BRIAR.

Conclusion: Learning residuals provides an effective solution for addressing occluded gait recognition while ensuring that performance on holistic gait remains uncompromised.

Abstract: Gait is becoming popular as a method of person re-identification because of
its ability to identify people at a distance. However, most current works in
gait recognition do not address the practical problem of occlusions. Among
those which do, some require paired tuples of occluded and holistic sequences,
which are impractical to collect in the real world. Further, these approaches
work on occlusions but fail to retain performance on holistic inputs. To
address these challenges, we propose RG-Gait, a method for residual correction
for occluded gait recognition with holistic retention. We model the problem as
a residual learning task, conceptualizing the occluded gait signature as a
residual deviation from the holistic gait representation. Our proposed network
adaptively integrates the learned residual, significantly improving performance
on occluded gait sequences without compromising the holistic recognition
accuracy. We evaluate our approach on the challenging Gait3D, GREW and BRIAR
datasets and show that learning the residual can be an effective technique to
tackle occluded gait recognition with holistic retention.

</details>


### [112] [SpaRTAN: Spatial Reinforcement Token-based Aggregation Network for Visual Recognition](https://arxiv.org/abs/2507.10999)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

TL;DR: The paper introduces SpaRTAN, a lightweight convolutional neural network architecture design, aiming to improve spatial and channel-wise information processing. The approach uses varied kernels and wave-based channel aggregation to enhance performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Modern convolutional neural networks (CNNs) and transformers possess inherent simplicity biases and inefficiencies, particularly in processing complex features and retaining a high expansion ratio, resulting in information redundancies. The authors aim to address these inefficiencies and enhance competitive performance in a lightweight design.

Method: The proposed SpaRTAN architecture employs kernels with varying receptive fields (determined by kernel size and dilation factors) to capture multi-order spatial features. Additionally, it incorporates a wave-based channel aggregation module for mitigating channel-wise redundancies and enhancing pixel interactions.

Result: SpaRTAN achieves 77.7% accuracy on ImageNet-1k with only 3.8M parameters (approximately 1.0 GFLOPs) and a 50.0% AP on the COCO benchmark with 21.5M parameters. These results demonstrate strong performance and parameter efficiency.

Conclusion: SpaRTAN delivers a robust performance with an efficient architectural design, demonstrating its potential as an effective solution for modern visual recognition tasks. Its public code further supports reproducibility and adoption by the research community.

Abstract: The resurgence of convolutional neural networks (CNNs) in visual recognition
tasks, exemplified by ConvNeXt, has demonstrated their capability to rival
transformer-based architectures through advanced training methodologies and
ViT-inspired design principles. However, both CNNs and transformers exhibit a
simplicity bias, favoring straightforward features over complex structural
representations. Furthermore, modern CNNs often integrate MLP-like blocks akin
to those in transformers, but these blocks suffer from significant information
redundancies, necessitating high expansion ratios to sustain competitive
performance. To address these limitations, we propose SpaRTAN, a lightweight
architectural design that enhances spatial and channel-wise information
processing. SpaRTAN employs kernels with varying receptive fields, controlled
by kernel size and dilation factor, to capture discriminative multi-order
spatial features effectively. A wave-based channel aggregation module further
modulates and reinforces pixel interactions, mitigating channel-wise
redundancies. Combining the two modules, the proposed network can efficiently
gather and dynamically contextualize discriminative features. Experimental
results in ImageNet and COCO demonstrate that SpaRTAN achieves remarkable
parameter efficiency while maintaining competitive performance. In particular,
on the ImageNet-1k benchmark, SpaRTAN achieves 77. 7% accuracy with only 3.8M
parameters and approximately 1.0 GFLOPs, demonstrating its ability to deliver
strong performance through an efficient design. On the COCO benchmark, it
achieves 50.0% AP, surpassing the previous benchmark by 1.2% with only 21.5M
parameters. The code is publicly available at
[https://github.com/henry-pay/SpaRTAN].

</details>


### [113] [Bridge Feature Matching and Cross-Modal Alignment with Mutual-filtering for Zero-shot Anomaly Detection](https://arxiv.org/abs/2507.11003)
*Yuhu Bai,Jiangning Zhang,Yunkang Cao,Guangyuan Lu,Qingdong He,Xiangtai Li,Guanzhong Tian*

Main category: cs.CV

TL;DR: The paper introduces FiSeCLIP, a zero-shot anomaly detection model using CLIP. It utilizes batch-based reference matching and semantic correlation for improved performance.


<details>
  <summary>Details</summary>
Motivation: To address challenges in zero-shot anomaly detection, especially for rare classes in industrial applications, using training-free vision-language models like CLIP.

Method: The proposed FiSeCLIP incorporates mutual reference matching within batch-based testing, filters noisy features using text-based information, and restores local semantic correlations for fine-grained anomaly detection.

Result: FiSeCLIP demonstrates superior anomaly classification and segmentation performance, surpassing the state-of-the-art benchmarks like MVTec-AD with a significant improvement in AU-ROC and $F_1$ metrics.

Conclusion: FiSeCLIP sets a stronger baseline for zero-shot anomaly detection and improves practical applicability in industrial scenarios by leveraging CLIP's capabilities.

Abstract: With the advent of vision-language models (e.g., CLIP) in zero- and few-shot
settings, CLIP has been widely applied to zero-shot anomaly detection (ZSAD) in
recent research, where the rare classes are essential and expected in many
applications. This study introduces \textbf{FiSeCLIP} for ZSAD with
training-free \textbf{CLIP}, combining the feature matching with the
cross-modal alignment. Testing with the entire dataset is impractical, while
batch-based testing better aligns with real industrial needs, and images within
a batch can serve as mutual reference points. Accordingly, FiSeCLIP utilizes
other images in the same batch as reference information for the current image.
However, the lack of labels for these references can introduce ambiguity, we
apply text information to \textbf{fi}lter out noisy features. In addition, we
further explore CLIP's inherent potential to restore its local
\textbf{se}mantic correlation, adapting it for fine-grained anomaly detection
tasks to enable a more accurate filtering process. Our approach exhibits
superior performance for both anomaly classification and segmentation on
anomaly detection benchmarks, building a stronger baseline for the direction,
e.g., on MVTec-AD, FiSeCLIP outperforms the SOTA AdaCLIP by
+4.6\%$\uparrow$/+5.7\%$\uparrow$ in segmentation metrics AU-ROC/$F_1$-max.

</details>


### [114] [Semantically Informed Salient Regions Guided Radiology Report Generation](https://arxiv.org/abs/2507.11015)
*Zeyi Hou,Zeqiang Wei,Ruixin Yan,Ning Lang,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: This paper proposes SISRNet, a method for generating clinically accurate radiology reports from chest X-rays by focusing on medically critical regions.


<details>
  <summary>Details</summary>
Motivation: Automated radiology report generation often suffers from inaccuracies due to subtle and sparse abnormalities in X-ray data, limiting its clinical applicability.

Method: SISRNet uses a Semantically Informed Salient Regions-guided approach to identify and focus on medically important regions during image and report generation.

Result: SISRNet outperforms existing methods on IU-Xray and MIMIC-CXR datasets, addressing data bias issues and improving clinical accuracy.

Conclusion: Focusing on high-information regions reduces errors in automated radiology report generation, showing promise for practical use in clinical settings.

Abstract: Recent advances in automated radiology report generation from chest X-rays
using deep learning algorithms have the potential to significantly reduce the
arduous workload of radiologists. However, due to the inherent massive data
bias in radiology images, where abnormalities are typically subtle and sparsely
distributed, existing methods often produce fluent yet medically inaccurate
reports, limiting their applicability in clinical practice. To address this
issue effectively, we propose a Semantically Informed Salient Regions-guided
(SISRNet) report generation method. Specifically, our approach explicitly
identifies salient regions with medically critical characteristics using
fine-grained cross-modal semantics. Then, SISRNet systematically focuses on
these high-information regions during both image modeling and report
generation, effectively capturing subtle abnormal findings, mitigating the
negative impact of data bias, and ultimately generating clinically accurate
reports. Compared to its peers, SISRNet demonstrates superior performance on
widely used IU-Xray and MIMIC-CXR datasets.

</details>


### [115] [Human-Guided Shade Artifact Suppression in CBCT-to-MDCT Translation via Schrödinger Bridge with Conditional Diffusion](https://arxiv.org/abs/2507.11025)
*Sung Ho Kang,Hyun-Cheol Park*

Main category: cs.CV

TL;DR: The paper introduces a Schrodinger Bridge-based framework for CBCT-to-MDCT translation, integrating GAN priors, conditional diffusion, and human feedback for improved medical image translation.


<details>
  <summary>Details</summary>
Motivation: To address the gap in current CBCT-to-MDCT translation methods regarding anatomical fidelity, perceptual controllability, and the incorporation of human preferences.

Method: The method combines GAN-derived priors and human-guided conditional diffusion through classifier-free guidance (CFG). It uses iterative refinement and human feedback to steer the generative process without needing a reward model.

Result: The framework reduces shade artifacts, preserves anatomical details, and outperforms previous methods in metrics such as RMSE, SSIM, LPIPS, and Dice on clinical datasets, achieving results with only 10 sampling steps.

Conclusion: The study demonstrates a novel, efficient approach for real-time CBCT-to-MDCT translation, effectively aligning outputs with human preferences and enhancing clinical utility.

Abstract: We present a novel framework for CBCT-to-MDCT translation, grounded in the
Schrodinger Bridge (SB) formulation, which integrates GAN-derived priors with
human-guided conditional diffusion. Unlike conventional GANs or diffusion
models, our approach explicitly enforces boundary consistency between CBCT
inputs and pseudo targets, ensuring both anatomical fidelity and perceptual
controllability. Binary human feedback is incorporated via classifier-free
guidance (CFG), effectively steering the generative process toward clinically
preferred outcomes. Through iterative refinement and tournament-based
preference selection, the model internalizes human preferences without relying
on a reward model. Subtraction image visualizations reveal that the proposed
method selectively attenuates shade artifacts in key anatomical regions while
preserving fine structural detail. Quantitative evaluations further demonstrate
superior performance across RMSE, SSIM, LPIPS, and Dice metrics on clinical
datasets -- outperforming prior GAN- and fine-tuning-based feedback methods --
while requiring only 10 sampling steps. These findings underscore the
effectiveness and efficiency of our framework for real-time, preference-aligned
medical image translation.

</details>


### [116] [Personalized OVSS: Understanding Personal Concept in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2507.11030)
*Sunghyun Park,Jungsoo Lee,Shubhankar Borse,Munawar Hayat,Sungha Choi,Kyuwoong Hwang,Fatih Porikli*

Main category: cs.CV

TL;DR: The paper introduces a new task called personalized open-vocabulary semantic segmentation (OVSS) to address segmenting personal concepts (e.g., 'my mug cup'), and proposes a method using text prompt tuning and visual embeddings to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend the capability of open-vocabulary semantic segmentation to recognize and segment personalized visual concepts, enabling personalized descriptions like 'my mug cup' to be distinguished from similar objects.

Method: The authors propose a text prompt tuning plug-in method that incorporates a 'negative mask proposal' mechanism to differentiate non-personalized concepts and enhances text prompts with visual embeddings to represent personal concepts more effectively.

Result: The method outperforms benchmarks on newly established datasets (FSS$^{per}$, CUB$^{per}$, ADE$^{per}$), validating improved performance for personalized OVSS without degrading original OVSS capabilities.

Conclusion: The approach successfully addresses the challenge of recognizing personalized concepts in OVSS, achieving superior performance through the introduction of innovative techniques like negative mask proposals and visual embedding-enhanced text prompts.

Abstract: While open-vocabulary semantic segmentation (OVSS) can segment an image into
semantic regions based on arbitrarily given text descriptions even for classes
unseen during training, it fails to understand personal texts (e.g., `my mug
cup') for segmenting regions of specific interest to users. This paper
addresses challenges like recognizing `my mug cup' among `multiple mug cups'.
To overcome this challenge, we introduce a novel task termed
\textit{personalized open-vocabulary semantic segmentation} and propose a text
prompt tuning-based plug-in method designed to recognize personal visual
concepts using a few pairs of images and masks, while maintaining the
performance of the original OVSS. Based on the observation that reducing false
predictions is essential when applying text prompt tuning to this task, our
proposed method employs `negative mask proposal' that captures visual concepts
other than the personalized concept. We further improve the performance by
enriching the representation of text prompts by injecting visual embeddings of
the personal concept into them. This approach enhances personalized OVSS
without compromising the original OVSS performance. We demonstrate the
superiority of our method on our newly established benchmarks for this task,
including FSS$^\text{per}$, CUB$^\text{per}$, and ADE$^\text{per}$.

</details>


### [117] [Efficient Dual-domain Image Dehazing with Haze Prior Perception](https://arxiv.org/abs/2507.11035)
*Lirong Zheng,Yanshan Li,Rui Yu,Kaihao Zhang*

Main category: cs.CV

TL;DR: The paper introduces DGFDNet, a novel dual-domain model combining spatial and frequency domain processing for efficient single-image dehazing.


<details>
  <summary>Details</summary>
Motivation: Current methods for single-image dehazing struggle with computational inefficiency and performance limitations under complex haze conditions due to inadequate integration of spatial and frequency domains.

Method: The proposed DGFDNet uses a Dark Channel Guided Frequency-aware model and introduces two main modules: a Haze-Aware Frequency Modulator for spectral adaptation and a Multi-level Gating Aggregation Module for structural recovery. It also employs a Prior Correction Guidance Branch for iterative refinement.

Result: Extensive experiments demonstrate that DGFDNet achieves state-of-the-art dehazing performance with high robustness and real-time efficiency across four benchmark datasets.

Conclusion: DGFDNet effectively integrates spatial and frequency domain features with innovative mechanisms for achieving superior and efficient single-image dehazing, even in challenging environments.

Abstract: Transformer-based models exhibit strong global modeling capabilities in
single-image dehazing, but their high computational cost limits real-time
applicability. Existing methods predominantly rely on spatial-domain features
to capture long-range dependencies, which are computationally expensive and
often inadequate under complex haze conditions. While some approaches introduce
frequency-domain cues, the weak coupling between spatial and frequency branches
limits the overall performance. To overcome these limitations, we propose the
Dark Channel Guided Frequency-aware Dehazing Network (DGFDNet), a novel
dual-domain framework that performs physically guided degradation alignment
across spatial and frequency domains. At its core, the DGFDBlock comprises two
key modules: 1) the Haze-Aware Frequency Modulator (HAFM), which generates a
pixel-level haze confidence map from dark channel priors to adaptively enhance
haze-relevant frequency components, thereby achieving global degradation-aware
spectral modulation; 2) the Multi-level Gating Aggregation Module (MGAM), which
fuses multi-scale features through diverse convolutional kernels and hybrid
gating mechanisms to recover fine structural details. Additionally, a Prior
Correction Guidance Branch (PCGB) incorporates a closed-loop feedback
mechanism, enabling iterative refinement of the prior by intermediate dehazed
features and significantly improving haze localization accuracy, especially in
challenging outdoor scenes. Extensive experiments on four benchmark haze
datasets demonstrate that DGFDNet achieves state-of-the-art performance with
superior robustness and real-time efficiency. Code is available at:
https://github.com/Dilizlr/DGFDNet.

</details>


### [118] [A Multi-View High-Resolution Foot-Ankle Complex Point Cloud Dataset During Gait for Occlusion-Robust 3D Completion](https://arxiv.org/abs/2507.11037)
*Jie-Wen Li,Zi-Han Ye,Qingyuan Zhou,Jiayi Song,Ying He,Ben Fei,Wen-Ming Chen*

Main category: cs.CV

TL;DR: FootGait3D dataset captures high-resolution ankle-foot surface point clouds during gait using a multi-view approach, aiding biomechanical research and 3D modeling.


<details>
  <summary>Details</summary>
Motivation: Biomechanical research and clinical assessment require accurate foot-ankle motion data during gait, which remains challenging due to occlusions and viewpoint limitations.

Method: FootGait3D collects 8,403 point cloud frames from 46 subjects using a five-camera depth sensing setup, providing structured data for varying occlusion levels.

Result: The dataset enables robust evaluation and benchmarking of shape completion methods for recovering full foot geometry from occluded inputs.

Conclusion: FootGait3D offers a valuable resource for advancing biomechanics, gait analysis, prosthetics, and robotics by providing granular ankle-foot data for motion modeling.

Abstract: The kinematics analysis of foot-ankle complex during gait is essential for
advancing biomechanical research and clinical assessment. Collecting accurate
surface geometry data from the foot and ankle during dynamic gait conditions is
inherently challenging due to swing foot occlusions and viewing limitations.
Thus, this paper introduces FootGait3D, a novel multi-view dataset of
high-resolution ankle-foot surface point clouds captured during natural gait.
Different from existing gait datasets that typically target whole-body or
lower-limb motion, FootGait3D focuses specifically on the detailed modeling of
the ankle-foot region, offering a finer granularity of motion data. To address
this, FootGait3D consists of 8,403 point cloud frames collected from 46
subjects using a custom five-camera depth sensing system. Each frame includes a
complete 5-view reconstruction of the foot and ankle (serving as ground truth)
along with partial point clouds obtained from only four, three, or two views.
This structured variation enables rigorous evaluation of 3D point cloud
completion methods under varying occlusion levels and viewpoints. Our dataset
is designed for shape completion tasks, facilitating the benchmarking of
state-of-the-art single-modal (e.g., PointTr, SnowflakeNet, Anchorformer) and
multi-modal (e.g., SVDFormer, PointSea, CSDN) completion networks on the
challenge of recovering the full foot geometry from occluded inputs. FootGait3D
has significant potential to advance research in biomechanics and multi-segment
foot modeling, offering a valuable testbed for clinical gait analysis,
prosthetic design, and robotics applications requiring detailed 3D models of
the foot during motion. The dataset is now available at
https://huggingface.co/datasets/ljw285/FootGait3D.

</details>


### [119] [Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery](https://arxiv.org/abs/2507.11040)
*Nicolas Drapier,Aladine Chetouani,Aurélien Chateigner*

Main category: cs.CV

TL;DR: Introduction of GLOD, a transformer-first architecture, for satellite imagery object detection. Achieves superior performance compared to the state-of-the-art.


<details>
  <summary>Details</summary>
Motivation: To improve object detection performance in high-resolution satellite imagery by addressing challenges like scale variation and computational efficiency.

Method: GLOD replaces CNN backbones with a Swin Transformer for feature extraction. Includes UpConvMixer for upsampling, Fusion Blocks for feature integration, and introduces asymmetric fusion and a multi-path head design.

Result: Achieved 32.95% on the xView benchmark, outperforming the previous state-of-the-art methods by 11.46%.

Conclusion: GLOD demonstrates significant improvement and computational efficiency for object detection specifically tailored for high-resolution satellite imagery.

Abstract: We present GLOD, a transformer-first architecture for object detection in
high-resolution satellite imagery. GLOD replaces CNN backbones with a Swin
Transformer for end-to-end feature extraction, combined with novel UpConvMixer
blocks for robust upsampling and Fusion Blocks for multi-scale feature
integration. Our approach achieves 32.95\% on xView, outperforming SOTA methods
by 11.46\%. Key innovations include asymmetric fusion with CBAM attention and a
multi-path head design capturing objects across scales. The architecture is
optimized for satellite imagery challenges, leveraging spatial priors while
maintaining computational efficiency.

</details>


### [120] [Alleviating Textual Reliance in Medical Language-guided Segmentation via Prototype-driven Semantic Approximation](https://arxiv.org/abs/2507.11055)
*Shuchang Ye,Usman Naseem,Mingyuan Meng,Jinman Kim*

Main category: cs.CV

TL;DR: ProLearn is a framework for language-guided medical image segmentation that reduces reliance on paired clinical text, enabling use of unpaired image datasets and improving applicability in clinical scenarios.


<details>
  <summary>Details</summary>
Motivation: Current language-guided segmentation methods depend heavily on paired image-text datasets, which are often scarce in medical datasets and limit their usage in practical clinical settings.

Method: ProLearn introduces Prototype-driven Semantic Approximation (PSA), which distills semantics from textual reports to create a prototype space, enabling semantic guidance for images without paired text through query-and-respond functionality.

Result: Experiments on three datasets (QaTa-COV19, MosMedData+, and Kvasir-SEG) show ProLearn achieves state-of-the-art performance even with limited textual data.

Conclusion: ProLearn provides a robust solution to reduce textual reliance in medical image segmentation, making it more applicable to diverse datasets and real-world clinical practices.

Abstract: Medical language-guided segmentation, integrating textual clinical reports as
auxiliary guidance to enhance image segmentation, has demonstrated significant
improvements over unimodal approaches. However, its inherent reliance on paired
image-text input, which we refer to as ``textual reliance", presents two
fundamental limitations: 1) many medical segmentation datasets lack paired
reports, leaving a substantial portion of image-only data underutilized for
training; and 2) inference is limited to retrospective analysis of cases with
paired reports, limiting its applicability in most clinical scenarios where
segmentation typically precedes reporting. To address these limitations, we
propose ProLearn, the first Prototype-driven Learning framework for
language-guided segmentation that fundamentally alleviates textual reliance. At
its core, in ProLearn, we introduce a novel Prototype-driven Semantic
Approximation (PSA) module to enable approximation of semantic guidance from
textual input. PSA initializes a discrete and compact prototype space by
distilling segmentation-relevant semantics from textual reports. Once
initialized, it supports a query-and-respond mechanism which approximates
semantic guidance for images without textual input, thereby alleviating textual
reliance. Extensive experiments on QaTa-COV19, MosMedData+ and Kvasir-SEG
demonstrate that ProLearn outperforms state-of-the-art language-guided methods
when limited text is available.

</details>


### [121] [Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling](https://arxiv.org/abs/2507.11061)
*Hayeon Kim,Ji Ha Jang,Se Young Chun*

Main category: cs.CV

TL;DR: The paper introduces a novel framework for precise, part-level 3D editing using Gaussian Splatting. It addresses challenges in inconsistent part segmentations and ambiguous loss functions by presenting innovative techniques for 3D masking and regularized SDS loss.


<details>
  <summary>Details</summary>
Motivation: Existing methods for editing 3D neural representations face challenges in achieving effective local edits due to inconsistent part segmentation and limitations in current loss functions.

Method: The proposed framework, RoMaP, introduces the 3D-Geometry Aware Label Prediction (3D-GALP) for robust mask generation and a regularized SDS loss enhanced by Scheduled Latent Mixing and Part editing (SLaMP) and additional constraints.

Result: Experimental results show that RoMaP achieves state-of-the-art performance in local 3D editing on Gaussian scenes and objects, demonstrating both qualitative and quantitative improvements.

Conclusion: RoMaP enables robust, flexible, and precise part-level 3D Gaussian editing, overcoming previous limitations and paving the way for efficient modifications in 3D models.

Abstract: Recent advances in 3D neural representations and instance-level editing
models have enabled the efficient creation of high-quality 3D content. However,
achieving precise local 3D edits remains challenging, especially for Gaussian
Splatting, due to inconsistent multi-view 2D part segmentations and inherently
ambiguous nature of Score Distillation Sampling (SDS) loss. To address these
limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that
enables precise and drastic part-level modifications. First, we introduce a
robust 3D mask generation module with our 3D-Geometry Aware Label Prediction
(3D-GALP), which uses spherical harmonics (SH) coefficients to model
view-dependent label variations and soft-label property, yielding accurate and
consistent part segmentations across viewpoints. Second, we propose a
regularized SDS loss that combines the standard SDS loss with additional
regularizers. In particular, an L1 anchor loss is introduced via our Scheduled
Latent Mixing and Part (SLaMP) editing method, which generates high-quality
part-edited 2D images and confines modifications only to the target region
while preserving contextual coherence. Additional regularizers, such as
Gaussian prior removal, further improve flexibility by allowing changes beyond
the existing context, and robust 3D masking prevents unintended edits.
Experimental results demonstrate that our RoMaP achieves state-of-the-art local
3D editing on both reconstructed and generated Gaussian scenes and objects
qualitatively and quantitatively, making it possible for more robust and
flexible part-level 3D Gaussian editing.

</details>


### [122] [Joint angle model based learning to refine kinematic human pose estimation](https://arxiv.org/abs/2507.11075)
*Chang Peng,Yifei Zhou,Huifeng Xi,Shiqing Huang,Chuangye Chen,Jianming Yang,Bao Yang,Zhenyu Jiang*

Main category: cs.CV

TL;DR: This paper proposes a joint angle-based method for refining human pose estimation using Fourier series and a bidirectional recurrent network, achieving strong results in challenging scenarios like figure skating.


<details>
  <summary>Details</summary>
Motivation: Marker-free human pose estimation suffers from recognition errors and trajectory fluctuations, with limitations stemming from inaccurately annotated datasets.

Method: The approach involves a joint angle-based pose model, "ground truth" creation via high-order Fourier series, and a bidirectional recurrent network trained on a high-quality dataset for refining outputs from HRNet.

Result: The proposed method significantly improves the accuracy of recognized joint positions and smoothens trajectories, outperforming existing HPE refinement models in challenging dynamic movements.

Conclusion: A robust joint angle-based refinement framework improves human pose estimation pipelines and excels in correcting errors in complex scenarios.

Abstract: Marker-free human pose estimation (HPE) has found increasing applications in
various fields. Current HPE suffers from occasional errors in keypoint
recognition and random fluctuation in keypoint trajectories when analyzing
kinematic human poses. The performance of existing deep learning-based models
for HPE refinement is considerably limited by inaccurate training datasets in
which the keypoints are manually annotated. This paper proposed a novel method
to overcome the difficulty through joint angle-based modeling. The key
techniques include: (i) A joint angle-based model of human pose, which is
robust to describe kinematic human poses; (ii) Approximating temporal variation
of joint angles through high order Fourier series to get reliable "ground
truth"; (iii) A bidirectional recurrent network is designed as a
post-processing module to refine the estimation of well-established HRNet.
Trained with the high-quality dataset constructed using our method, the network
demonstrates outstanding performance to correct wrongly recognized joints and
smooth their spatiotemporal trajectories. Tests show that joint angle-based
refinement (JAR) outperforms the state-of-the-art HPE refinement network in
challenging cases like figure skating and breaking.

</details>


### [123] [GKNet: Graph-based Keypoints Network for Monocular Pose Estimation of Non-cooperative Spacecraft](https://arxiv.org/abs/2507.11077)
*Weizhao Ma,Dong Zhou,Yuhui Hu,Zipeng He*

Main category: cs.CV

TL;DR: The paper tackles monocular pose estimation challenges for non-cooperative spacecraft by introducing a graph-based keypoints network, GKNet, and a new dataset, SKD, demonstrating improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Monocular pose estimation for non-cooperative spacecraft is critical for on-orbit service tasks, yet existing methods struggle with structural symmetry and partial occlusion issues.

Method: The authors propose GKNet, a graph-based keypoints network leveraging geometric constraints, and introduce SKD, a new dataset with 90,000 simulated images and precise keypoint annotations.

Result: Experiments and an ablation study confirm that GKNet achieves higher accuracy and effectiveness compared to state-of-the-art spacecraft keypoint detectors.

Conclusion: GKNet, along with the SKD dataset, offers a superior approach to monocular pose estimation for spacecraft, overcoming key challenges like symmetry and occlusion while enhancing accuracy.

Abstract: Monocular pose estimation of non-cooperative spacecraft is significant for
on-orbit service (OOS) tasks, such as satellite maintenance, space debris
removal, and station assembly. Considering the high demands on pose estimation
accuracy, mainstream monocular pose estimation methods typically consist of
keypoint detectors and PnP solver. However, current keypoint detectors remain
vulnerable to structural symmetry and partial occlusion of non-cooperative
spacecraft. To this end, we propose a graph-based keypoints network for the
monocular pose estimation of non-cooperative spacecraft, GKNet, which leverages
the geometric constraint of keypoints graph. In order to better validate
keypoint detectors, we present a moderate-scale dataset for the spacecraft
keypoint detection, named SKD, which consists of 3 spacecraft targets, 90,000
simulated images, and corresponding high-precise keypoint annotations.
Extensive experiments and an ablation study have demonstrated the high accuracy
and effectiveness of our GKNet, compared to the state-of-the-art spacecraft
keypoint detectors. The code for GKNet and the SKD dataset is available at
https://github.com/Dongzhou-1996/GKNet.

</details>


### [124] [Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification](https://arxiv.org/abs/2507.11081)
*Chang Peng,Bao Yang,Meiqi Li,Ge Zhang,Hui Sun,Zhenyu Jiang*

Main category: cs.CV

TL;DR: The paper investigates improving automated recognition of road subsurface distress (RSD) using deep learning and a novel strategy based on robust dataset creation and cross-verification.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the labor-intensive process and reliance on human expertise for RSD identification from GPR images, highlighting the need for automated solutions.

Method: The researchers constructed a validated 3D GPR dataset with 2134 sample scans and proposed a cross-verification strategy using deep learning models, specifically focusing on YOLO's performance variations.

Result: The method achieved outstanding recall above 98.6% in RSD recognition during field tests and reduced inspection labor by approximately 90%.

Conclusion: The study demonstrates that combining high-quality datasets and advanced deep learning strategies can revolutionize road inspection efficiency and accuracy.

Abstract: Ground penetrating radar (GPR) has become a rapid and non-destructive
solution for road subsurface distress (RSD) detection. However, RSD recognition
from GPR images is labor-intensive and heavily relies on inspectors' expertise.
Deep learning offers the possibility for automatic RSD recognition, but its
current performance is limited by two factors: Scarcity of high-quality dataset
for network training and insufficient capability of network to distinguish RSD.
In this study, a rigorously validated 3D GPR dataset containing 2134 samples of
diverse types was constructed through field scanning. Based on the finding that
the YOLO model trained with one of the three scans of GPR images exhibits
varying sensitivity to specific type of RSD, we proposed a novel
cross-verification strategy with outstanding accuracy in RSD recognition,
achieving recall over 98.6% in field tests. The approach, integrated into an
online RSD detection system, can reduce the labor of inspection by around 90%.

</details>


### [125] [Atmos-Bench: 3D Atmospheric Structures for Climate Insight](https://arxiv.org/abs/2507.11085)
*Tianchi Xu*

Main category: cs.CV

TL;DR: The paper introduces Atmos-Bench, the first standardized 3D benchmark for atmospheric structure recovery, and proposes FourCastX, a network model that improves recovery accuracy without auxiliary inputs.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing methods for atmospheric structure recovery, such as reliance on auxiliary inputs, simplified physics, and lack of standardized benchmarks, which introduce uncertainties and inaccuracies in representing atmospheric effects.

Method: The paper creates a 3D atmospheric benchmark (Atmos-Bench) using simulations from WRF coupled with an enhanced COSP simulator, and presents FourCastX, a neural network model combining spatio-temporal mixture-of-experts with physical constraints for more accurate recovery.

Result: The proposed FourCastX model consistently outperforms baseline methods on the Atmos-Bench dataset across two wavelengths (355 nm and 532 nm) without relying on auxiliary inputs, delivering high-quality voxel-wise reference data.

Conclusion: Atmos-Bench and FourCastX set a new benchmark for accurate, efficient recovery of 3D atmospheric structures, paving the way for improved climate research and weather forecasting models.

Abstract: Atmospheric structure, represented by backscatter coefficients (BC) recovered
from satellite LiDAR attenuated backscatter (ATB), provides a volumetric view
of clouds, aerosols, and molecules, playing a critical role in human
activities, climate understanding, and extreme weather forecasting. Existing
methods often rely on auxiliary inputs and simplified physics-based
approximations, and lack a standardized 3D benchmark for fair evaluation.
However, such approaches may introduce additional uncertainties and
insufficiently capture realistic radiative transfer and atmospheric
scattering-absorption effects. To bridge these gaps, we present Atmos-Bench:
the first 3D atmospheric benchmark, along with a novel FourCastX:
Frequency-enhanced Spatio-Temporal Mixture-of-Experts Network that (a)
generates 921,600 image slices from 3D scattering volumes simulated at 532 nm
and 355 nm by coupling WRF with an enhanced COSP simulator over 384 land-ocean
time steps, yielding high-quality voxel-wise references; (b) embeds ATB-BC
physical constraints into the model architecture, promoting energy consistency
during restoration; (c) achieves consistent improvements on the Atmos-Bench
dataset across both 355 nm and 532 nm bands, outperforming state-of-the-art
baseline models without relying on auxiliary inputs. Atmos-Bench establishes a
new standard for satellite-based 3D atmospheric structure recovery and paves
the way for deeper climate insight.

</details>


### [126] [A Survey on Interpretability in Visual Recognition](https://arxiv.org/abs/2507.11099)
*Qiyang Wan,Chengzhi Gao,Ruiping Wang,Xilin Chen*

Main category: cs.CV

TL;DR: This paper reviews the interpretability of visual recognition models, proposing a human-centered taxonomy and summarizing evaluation metric requirements.


<details>
  <summary>Details</summary>
Motivation: To understand and improve the reliability of visual recognition methods, especially in critical applications like autonomous driving and medical diagnostics.

Method: This paper systematically categorizes interpretability methods into a taxonomy based on Intent, Object, Presentation, and Methodology and explores opportunities with emerging technologies.

Result: The taxonomy organizes methods for interpretable recognition, provides clarity on evaluation metrics, and suggests new avenues for research through technologies like large multimodal models.

Conclusion: This review establishes a structured approach to understanding interpretability in visual recognition models and seeks to inspire future research in this domain.

Abstract: In recent years, visual recognition methods have advanced significantly,
finding applications across diverse fields. While researchers seek to
understand the mechanisms behind the success of these models, there is also a
growing impetus to deploy them in critical areas like autonomous driving and
medical diagnostics to better diagnose failures, which promotes the development
of interpretability research. This paper systematically reviews existing
research on the interpretability of visual recognition models and proposes a
taxonomy of methods from a human-centered perspective. The proposed taxonomy
categorizes interpretable recognition methods based on Intent, Object,
Presentation, and Methodology, thereby establishing a systematic and coherent
set of grouping criteria for these XAI methods. Additionally, we summarize the
requirements for evaluation metrics and explore new opportunities enabled by
recent technologies, such as large multimodal models. We aim to organize
existing research in this domain and inspire future investigations into the
interpretability of visual recognition models.

</details>


### [127] [KptLLM++: Towards Generic Keypoint Comprehension with Large Language Model](https://arxiv.org/abs/2507.11102)
*Jie Yang,Wang Zeng,Sheng Jin,Lumin Xu,Wentao Liu,Chen Qian,Zhen Li,Ruimao Zhang*

Main category: cs.CV

TL;DR: KptLLM++ is a novel multimodal large language model designed for fine-grained image understanding by improving keypoint detection.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal models struggle with fine-grained semantic information, and keypoints are vital for applications like behavior recognition and object retrieval.

Method: The model employs an identify-then-detect paradigm with chain-of-thought reasoning, and utilizes a scaled dataset of over 500K samples for generalization.

Result: KptLLM++ achieves state-of-the-art performance on keypoint detection benchmarks, showcasing improved accuracy and generalization.

Conclusion: The model enhances human-AI collaboration with a unified and advanced approach to keypoint comprehension across diverse contexts.

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has revolutionized
image understanding by bridging textual and visual modalities. However, these
models often struggle with capturing fine-grained semantic information, such as
the precise identification and analysis of object keypoints. Keypoints, as
structure-aware, pixel-level, and compact representations of objects,
particularly articulated ones, play a crucial role in applications such as
fine-grained image analysis, object retrieval, and behavior recognition. In
this paper, we propose KptLLM++, a novel multimodal large language model that
specifically designed for generic keypoint comprehension through the
integration of diverse input modalities guided by user-defined instructions. By
unifying keypoint detection across varied contexts, KptLLM++ establishes itself
as an advanced interface, fostering more effective human-AI collaboration. The
model is built upon a novel identify-then-detect paradigm, which first
interprets keypoint semantics and subsequently localizes their precise
positions through a structured chain-of-thought reasoning mechanism. To push
the boundaries of performance, we have scaled up the training dataset to over
500K samples, encompassing diverse objects, keypoint categories, image styles,
and scenarios with complex occlusions. This extensive scaling enables KptLLM++
to unlock its potential, achieving remarkable accuracy and generalization.
Comprehensive experiments on multiple keypoint detection benchmarks demonstrate
its state-of-the-art performance, underscoring its potential as a unified
solution for fine-grained image understanding and its transformative
implications for human-AI interaction.

</details>


### [128] [Jellyfish Species Identification: A CNN Based Artificial Neural Network Approach](https://arxiv.org/abs/2507.11116)
*Md. Sabbir Hossen,Md. Saiduzzaman,Pabon Shaha,Mostofa Kamal Nasir*

Main category: cs.CV

TL;DR: The study introduces a deep learning framework integrating MobileNetV3 and other techniques for jellyfish species classification with 98% accuracy.


<details>
  <summary>Details</summary>
Motivation: Jellyfish are vital to marine ecosystems but their rapid growth poses challenges for biodiversity monitoring and conservation.

Method: A hybrid framework was developed combining advanced feature extractors (e.g., MobileNetV3, ResNet50) and machine learning classifiers for jellyfish classification using underwater images.

Result: The top-performing model, MobileNetV3 with an Artificial Neural Network, achieved 98% accuracy in species identification.

Conclusion: Deep learning and hybrid models improve jellyfish species detection, aiding ecological monitoring and conservation efforts in marine environments.

Abstract: Jellyfish, a diverse group of gelatinous marine organisms, play a crucial
role in maintaining marine ecosystems but pose significant challenges for
biodiversity and conservation due to their rapid proliferation and ecological
impact. Accurate identification of jellyfish species is essential for
ecological monitoring and management. In this study, we proposed a deep
learning framework for jellyfish species detection and classification using an
underwater image dataset. The framework integrates advanced feature extraction
techniques, including MobileNetV3, ResNet50, EfficientNetV2-B0, and VGG16,
combined with seven traditional machine learning classifiers and three
Feedforward Neural Network classifiers for precise species identification.
Additionally, we activated the softmax function to directly classify jellyfish
species using the convolutional neural network models. The combination of the
Artificial Neural Network with MobileNetV3 is our best-performing model,
achieving an exceptional accuracy of 98%, significantly outperforming other
feature extractor-classifier combinations. This study demonstrates the efficacy
of deep learning and hybrid frameworks in addressing biodiversity challenges
and advancing species detection in marine environments.

</details>


### [129] [Try Harder: Hard Sample Generation and Learning for Clothes-Changing Person Re-ID](https://arxiv.org/abs/2507.11119)
*Hankun Liu,Yujian Zhao,Guanglin Niu*

Main category: cs.CV

TL;DR: This paper addresses the challenge of hard samples in clothing-changing person re-identification (CC-ReID) by proposing a multimodal-guided Hard Sample Generation and Learning (HSGL) framework.


<details>
  <summary>Details</summary>
Motivation: Hard samples in CC-ReID create bottlenecks due to their ambiguity and similarity, hampering robust model performance and limiting effective learning strategies.

Method: The HSGL framework includes two components: (1) Dual-Granularity Hard Sample Generation (DGHSG), synthesizing diverse hard samples using multimodal cues, and (2) Hard Sample Adaptive Learning (HSAL), a hardness-aware optimization strategy to improve feature separation using textual semantic labels.

Result: The proposed method demonstrates state-of-the-art performance and faster convergence in targeted learning on PRCC and LTCC datasets.

Conclusion: Multimodal-guided hard sample generation and learning significantly enhance robustness and discriminative capabilities, proving effective for CC-ReID tasks.

Abstract: Hard samples pose a significant challenge in person re-identification (ReID)
tasks, particularly in clothing-changing person Re-ID (CC-ReID). Their inherent
ambiguity or similarity, coupled with the lack of explicit definitions, makes
them a fundamental bottleneck. These issues not only limit the design of
targeted learning strategies but also diminish the model's robustness under
clothing or viewpoint changes. In this paper, we propose a novel
multimodal-guided Hard Sample Generation and Learning (HSGL) framework, which
is the first effort to unify textual and visual modalities to explicitly
define, generate, and optimize hard samples within a unified paradigm. HSGL
comprises two core components: (1) Dual-Granularity Hard Sample Generation
(DGHSG), which leverages multimodal cues to synthesize semantically consistent
samples, including both coarse- and fine-grained hard positives and negatives
for effectively increasing the hardness and diversity of the training data. (2)
Hard Sample Adaptive Learning (HSAL), which introduces a hardness-aware
optimization strategy that adjusts feature distances based on textual semantic
labels, encouraging the separation of hard positives and drawing hard negatives
closer in the embedding space to enhance the model's discriminative capability
and robustness to hard samples. Extensive experiments on multiple CC-ReID
benchmarks demonstrate the effectiveness of our approach and highlight the
potential of multimodal-guided hard sample generation and learning for robust
CC-ReID. Notably, HSAL significantly accelerates the convergence of the
targeted learning procedure and achieves state-of-the-art performance on both
PRCC and LTCC datasets. The code is available at
https://github.com/undooo/TryHarder-ACMMM25.

</details>


### [130] [Task-Oriented Human Grasp Synthesis via Context- and Task-Aware Diffusers](https://arxiv.org/abs/2507.11287)
*An-Lun Liu,Yu-Wei Chao,Yi-Ting Chen*

Main category: cs.CV

TL;DR: The paper introduces a novel method for task-oriented human grasp synthesis, emphasizing scene and task awareness using enhanced task-aware contact maps.


<details>
  <summary>Details</summary>
Motivation: Existing grasp synthesis methods lack task and context awareness, which are essential for achieving realistic and effective hand-object interactions.

Method: The authors propose a two-stage pipeline: first, creating a task-aware contact map informed by scene and task, and second, using this map to generate task-oriented human grasps.

Result: The experiments showed that the proposed method outperformed existing approaches in both grasp quality and task-specific performance.

Conclusion: This study highlights the importance of integrating scene and task information for improving task-oriented grasp synthesis.

Abstract: In this paper, we study task-oriented human grasp synthesis, a new grasp
synthesis task that demands both task and context awareness. At the core of our
method is the task-aware contact maps. Unlike traditional contact maps that
only reason about the manipulated object and its relation with the hand, our
enhanced maps take into account scene and task information. This comprehensive
map is critical for hand-object interaction, enabling accurate grasping poses
that align with the task. We propose a two-stage pipeline that first constructs
a task-aware contact map informed by the scene and task. In the subsequent
stage, we use this contact map to synthesize task-oriented human grasps. We
introduce a new dataset and a metric for the proposed task to evaluate our
approach. Our experiments validate the importance of modeling both scene and
task, demonstrating significant improvements over existing methods in both
grasp quality and task performance. See our project page for more details:
https://hcis-lab.github.io/TOHGS/

</details>


### [131] [MMOne: Representing Multiple Modalities in One Scene](https://arxiv.org/abs/2507.11129)
*Zhifeng Gu,Bing Wang*

Main category: cs.CV

TL;DR: The paper addresses challenges in representing multimodal data and proposes MMOne, a framework that disentangles shared and modality-specific information for a better multimodal scene representation.


<details>
  <summary>Details</summary>
Motivation: To tackle the problem of modality conflicts, such as property and granularity disparity, which hinder building effective multimodal scene representations to enhance human understanding of the physical world.

Method: The MMOne framework introduces a modality modeling module with a novel modality indicator and a multimodal decomposition mechanism, separating shared and modality-specific components to handle disparities between modalities.

Result: The proposed approach demonstrates consistent improvement in multimodal representation capabilities and scalability to accommodate additional modalities, validated through extensive experiments.

Conclusion: The MMOne framework is an effective and extendable solution for addressing multimodal representation challenges, yielding compact and efficient representations across modalities.

Abstract: Humans perceive the world through multimodal cues to understand and interact
with the environment. Learning a scene representation for multiple modalities
enhances comprehension of the physical world. However, modality conflicts,
arising from inherent distinctions among different modalities, present two
critical challenges: property disparity and granularity disparity. To address
these challenges, we propose a general framework, MMOne, to represent multiple
modalities in one scene, which can be readily extended to additional
modalities. Specifically, a modality modeling module with a novel modality
indicator is proposed to capture the unique properties of each modality.
Additionally, we design a multimodal decomposition mechanism to separate
multi-modal Gaussians into single-modal Gaussians based on modality
differences. We address the essential distinctions among modalities by
disentangling multimodal information into shared and modality-specific
components, resulting in a more compact and efficient multimodal scene
representation. Extensive experiments demonstrate that our method consistently
enhances the representation capability for each modality and is scalable to
additional modalities. The code is available at
https://github.com/Neal2020GitHub/MMOne.

</details>


### [132] [RMAU-NET: A Residual-Multihead-Attention U-Net Architecture for Landslide Segmentation and Detection from Remote Sensing Images](https://arxiv.org/abs/2507.11143)
*Lam Pham,Cam Le,Hieu Tang,Khang Truong,Truong Nguyen,Jasmin Lampert,Alexander Schindler,Martin Boyer,Son Phan*

Main category: cs.CV

TL;DR: This study proposes a deep-learning model to analyze remote sensing images for automatic landslide observation, achieving high performance in both detection and segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: Frequent landslide disasters due to extreme weather events and human activities necessitate innovative solutions to monitor large and rugged areas efficiently.

Method: An end-to-end deep-learning model was introduced, utilizing remote sensing images as input and applying a novel neural network architecture for landslide detection and segmentation.

Result: The model demonstrated impressive performance with F1 scores of 98.23 and 93.83 on detection tasks and mIoU scores of 63.74 and 76.88 on segmentation tasks across three benchmark datasets.

Conclusion: The results highlight the model's potential for practical integration into landslide observation systems to enhance disaster management capabilities.

Abstract: In recent years, landslide disasters have reported frequently due to the
extreme weather events of droughts, floods , storms, or the consequence of
human activities such as deforestation, excessive exploitation of natural
resources. However, automatically observing landslide is challenging due to the
extremely large observing area and the rugged topography such as mountain or
highland. This motivates us to propose an end-to-end deep-learning-based model
which explores the remote sensing images for automatically observing landslide
events. By considering remote sensing images as the input data, we can obtain
free resource, observe large and rough terrains by time. To explore the remote
sensing images, we proposed a novel neural network architecture which is for
two tasks of landslide detection and landslide segmentation. We evaluated our
proposed model on three different benchmark datasets of LandSlide4Sense, Bijie,
and Nepal. By conducting extensive experiments, we achieve F1 scores of 98.23,
93.83 for the landslide detection task on LandSlide4Sense, Bijie datasets; mIoU
scores of 63.74, 76.88 on the segmentation tasks regarding LandSlide4Sense,
Nepal datasets. These experimental results prove potential to integrate our
proposed model into real-life landslide observation systems.

</details>


### [133] [Assessing Color Vision Test in Large Vision-language Models](https://arxiv.org/abs/2507.11153)
*Hongfei Ye,Bin Chen,Wenxi Liu,Yu Zhang,Zhao Li,Dandan Ni,Hongyang Chen*

Main category: cs.CV

TL;DR: This paper investigates the color vision capabilities of large vision-language models by defining a testing task, constructing a diverse dataset, analyzing errors, and proposing fine-tuning strategies.


<details>
  <summary>Details</summary>
Motivation: To address the lack of thorough exploration regarding the color vision abilities of large vision-language models, which is critical given their widespread adoption.

Method: The authors created a dataset with various categories of color-related test questions and tasks of varying difficulty, analyzed model error types, and proposed fine-tuning techniques to improve performance.

Result: A comprehensive dataset was developed, existing large vision-language models were evaluated for their color vision capability, and fine-tuning strategies were suggested to bolster their accuracy.

Conclusion: The study highlights the current limitations of large vision-language models in color vision tasks and provides a framework for improving their precision through tailored fine-tuning.

Abstract: With the widespread adoption of large vision-language models, the capacity
for color vision in these models is crucial. However, the color vision
abilities of large visual-language models have not yet been thoroughly
explored. To address this gap, we define a color vision testing task for large
vision-language models and construct a dataset \footnote{Anonymous Github
Showing some of the data
https://anonymous.4open.science/r/color-vision-test-dataset-3BCD} that covers
multiple categories of test questions and tasks of varying difficulty levels.
Furthermore, we analyze the types of errors made by large vision-language
models and propose fine-tuning strategies to enhance their performance in color
vision tests.

</details>


### [134] [Clustering-Guided Multi-Layer Contrastive Representation Learning for Citrus Disease Classification](https://arxiv.org/abs/2507.11171)
*Jun Chen,Yonghua Yu,Weifu Li,Yaohui Chen,Hong Chen*

Main category: cs.CV

TL;DR: This paper proposes a novel clustering-guided, self-supervised multi-layer contrastive learning algorithm for citrus disease detection, achieving state-of-the-art accuracy and robustness compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Citrus crops face yield reductions due to diseases, yet accurate detection and classification are essential for targeted control. Current deep learning methods require extensive labeled data, which is labor-intensive to create.

Method: The authors developed the CMCRL algorithm, incorporating cluster centroids and a multi-layer contrastive training paradigm to enable effective learning from unannotated samples and to represent hierarchical features.

Result: The CMCRL algorithm outperformed existing methods in accuracy (by 4.5%-30.1%) on the citrus disease dataset CDD and achieved robust performance across various evaluation metrics, closing the gap with fully supervised approaches.

Conclusion: CMCRL provides an efficient and accurate approach to citrus disease detection with minimal reliance on annotated datasets, addressing challenges like symptom similarity and class imbalance.

Abstract: Citrus, as one of the most economically important fruit crops globally,
suffers severe yield depressions due to various diseases. Accurate disease
detection and classification serve as critical prerequisites for implementing
targeted control measures. Recent advancements in artificial intelligence,
particularly deep learning-based computer vision algorithms, have substantially
decreased time and labor requirements while maintaining the accuracy of
detection and classification. Nevertheless, these methods predominantly rely on
massive, high-quality annotated training examples to attain promising
performance. By introducing two key designs: contrasting with cluster centroids
and a multi-layer contrastive training (MCT) paradigm, this paper proposes a
novel clustering-guided self-supervised multi-layer contrastive representation
learning (CMCRL) algorithm. The proposed method demonstrates several advantages
over existing counterparts: (1) optimizing with massive unannotated samples;
(2) effective adaptation to the symptom similarity across distinct citrus
diseases; (3) hierarchical feature representation learning. The proposed method
achieves state-of-the-art performance on the public citrus image set CDD,
outperforming existing methods by 4.5\%-30.1\% accuracy. Remarkably, our method
narrows the performance gap with fully supervised counterparts (all samples are
labeled). Beyond classification accuracy, our method shows great performance on
other evaluation metrics (F1 score, precision, and recall), highlighting the
robustness against the class imbalance challenge.

</details>


### [135] [How Far Have Medical Vision-Language Models Come? A Comprehensive Benchmarking Study](https://arxiv.org/abs/2507.11200)
*Che Liu,Jiazhen Pan,Weixiang Shen,Wenjia Bai,Daniel Rueckert,Rossella Arcucci*

Main category: cs.CV

TL;DR: This study evaluates the capability of vision-language models (VLMs) for medical tasks, emphasizing their current limitations in clinical deployment.


<details>
  <summary>Details</summary>
Motivation: To explore the competence of general-purpose and medically specialized VLMs in medical-specific tasks and their potential for healthcare applications.

Method: A comprehensive analysis of various open-source VLMs, ranging in size from 3B to 72B parameters, across eight medical benchmarks.

Result: General-purpose VLMs perform comparably to specialized models on several benchmarks, but reasoning lagged behind understanding; performance varies significantly across tasks.

Conclusion: Current VLMs are not yet reliable enough for clinical deployment, requiring improved multimodal alignment and finer evaluation criteria.

Abstract: Vision-Language Models (VLMs) trained on web-scale corpora excel at natural
image tasks and are increasingly repurposed for healthcare; however, their
competence in medical tasks remains underexplored. We present a comprehensive
evaluation of open-source general-purpose and medically specialised VLMs,
ranging from 3B to 72B parameters, across eight benchmarks: MedXpert,
OmniMedVQA, PMC-VQA, PathVQA, MMMU, SLAKE, and VQA-RAD. To observe model
performance across different aspects, we first separate it into understanding
and reasoning components. Three salient findings emerge. First, large
general-purpose models already match or surpass medical-specific counterparts
on several benchmarks, demonstrating strong zero-shot transfer from natural to
medical images. Second, reasoning performance is consistently lower than
understanding, highlighting a critical barrier to safe decision support. Third,
performance varies widely across benchmarks, reflecting differences in task
design, annotation quality, and knowledge demands. No model yet reaches the
reliability threshold for clinical deployment, underscoring the need for
stronger multimodal alignment and more rigorous, fine-grained evaluation
protocols.

</details>


### [136] [A Robust Incomplete Multimodal Low-Rank Adaptation Approach for Emotion Recognition](https://arxiv.org/abs/2507.11202)
*Xinkui Zhao,Jinsong Shu,Yangyang Wu,Guanjie Cheng,Zihe Liu,Naibo Wang,Shuiguang Deng,Zhongle Xie,Jianwei Yin*

Main category: cs.CV

TL;DR: The paper introduces MCULoRA, a framework to improve Multimodal Emotion Recognition (MER) with incomplete multimodality, optimizing performance when data from some modalities is missing.


<details>
  <summary>Details</summary>
Motivation: Inreal-world settings, MER often faces incomplete multimodal data due to sensor issues or privacy concerns, and current methods experience gradient conflicts from handling various modality combinations, which reduces model performance.

Method: The proposed MCULoRA framework employs two modules: MCLA for separating shared and distinct modality characteristics and DPFT for dynamically adjusting training focus based on representation separability.

Result: Experimental evaluation on multiple benchmark datasets reveals that MCULoRA achieves significantly higher accuracy than previous methods in handling incomplete multimodal learning.

Conclusion: MCULoRA offers a parameter-efficient solution for incomplete multimodal scenarios, improving learning accuracy and efficiency by addressing modality imbalance and separability challenges.

Abstract: Multimodal Emotion Recognition (MER) often encounters incomplete
multimodality in practical applications due to sensor failures or privacy
protection requirements. While existing methods attempt to address various
incomplete multimodal scenarios by balancing the training of each modality
combination through additional gradients, these approaches face a critical
limitation: training gradients from different modality combinations conflict
with each other, ultimately degrading the performance of the final prediction
model. In this paper, we propose a unimodal decoupled dynamic low-rank
adaptation method based on modality combinations, named MCULoRA, which is a
novel framework for the parameter-efficient training of incomplete multimodal
learning models. MCULoRA consists of two key modules, modality combination
aware low-rank adaptation (MCLA) and dynamic parameter fine-tuning (DPFT). The
MCLA module effectively decouples the shared information from the distinct
characteristics of individual modality combinations. The DPFT module adjusts
the training ratio of modality combinations based on the separability of each
modality's representation space, optimizing the learning efficiency across
different modality combinations. Our extensive experimental evaluation in
multiple benchmark datasets demonstrates that MCULoRA substantially outperforms
previous incomplete multimodal learning approaches in downstream task accuracy.

</details>


### [137] [NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models](https://arxiv.org/abs/2507.11245)
*X. Feng,H. Yu,M. Wu,S. Hu,J. Chen,C. Zhu,J. Wu,X. Chu,K. Huang*

Main category: cs.CV

TL;DR: This paper introduces NarrLV, the first comprehensive benchmark for evaluating narrative capabilities of long video generation models, proposing new metrics and demonstrating their effectiveness.


<details>
  <summary>Details</summary>
Motivation: To address the lack of evaluation benchmarks specifically designed for assessing narrative capabilities in long video generation models.

Method: The authors introduce the Temporal Narrative Atom (TNA) to measure narrative richness, develop an automatic prompt generation pipeline, design an evaluation metric using MLLM-based question answering, and conduct evaluations on existing models.

Result: The proposed metrics align well with human judgments and reveal the narrative capability boundaries of current long video generation models.

Conclusion: NarrLV benchmark effectively evaluates narrative expression in long video generation and provides insights into model capabilities and limitations.

Abstract: With the rapid development of foundation video generation technologies, long
video generation models have exhibited promising research potential thanks to
expanded content creation space. Recent studies reveal that the goal of long
video generation tasks is not only to extend video duration but also to
accurately express richer narrative content within longer videos. However, due
to the lack of evaluation benchmarks specifically designed for long video
generation models, the current assessment of these models primarily relies on
benchmarks with simple narrative prompts (e.g., VBench). To the best of our
knowledge, our proposed NarrLV is the first benchmark to comprehensively
evaluate the Narrative expression capabilities of Long Video generation models.
Inspired by film narrative theory, (i) we first introduce the basic narrative
unit maintaining continuous visual presentation in videos as Temporal Narrative
Atom (TNA), and use its count to quantitatively measure narrative richness.
Guided by three key film narrative elements influencing TNA changes, we
construct an automatic prompt generation pipeline capable of producing
evaluation prompts with a flexibly expandable number of TNAs. (ii) Then, based
on the three progressive levels of narrative content expression, we design an
effective evaluation metric using the MLLM-based question generation and
answering framework. (iii) Finally, we conduct extensive evaluations on
existing long video generation models and the foundation generation models.
Experimental results demonstrate that our metric aligns closely with human
judgments. The derived evaluation outcomes reveal the detailed capability
boundaries of current video generation models in narrative content expression.

</details>


### [138] [Fairness-Aware Grouping for Continuous Sensitive Variables: Application for Debiasing Face Analysis with respect to Skin Tone](https://arxiv.org/abs/2507.11247)
*Veronika Shilova,Emmanuel Malherbe,Giovanni Palma,Laurent Risser,Jean-Michel Loubes*

Main category: cs.CV

TL;DR: The paper presents a fairness-based method to group continuous sensitive attributes (like skin color) by identifying discrimination levels, revealing nuanced patterns of bias, and improving fairness with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Traditional fairness methods often divide individuals into predefined groups, which may overlook discrimination in subpopulations, especially with continuous sensitive attributes like skin color.

Method: The authors propose a grouping strategy based on inter-group discrimination variance to isolate critical subgroups, validate it with synthetic and real datasets, and apply it to improve fairness using post-processing debiasing techniques.

Result: Using datasets like CelebA and FFHQ, the method uncovered nuanced discrimination patterns that remained consistent across datasets and models. It also demonstrated improved fairness with minimal accuracy trade-off.

Conclusion: The proposed method effectively identifies critical subpopulations facing discrimination, offers a robust fairness analysis for continuous sensitive attributes, and enables practical improvements in fairness for deployment scenarios.

Abstract: Within a legal framework, fairness in datasets and models is typically
assessed by dividing observations into predefined groups and then computing
fairness measures (e.g., Disparate Impact or Equality of Odds with respect to
gender). However, when sensitive attributes such as skin color are continuous,
dividing into default groups may overlook or obscure the discrimination
experienced by certain minority subpopulations. To address this limitation, we
propose a fairness-based grouping approach for continuous (possibly
multidimensional) sensitive attributes. By grouping data according to observed
levels of discrimination, our method identifies the partition that maximizes a
novel criterion based on inter-group variance in discrimination, thereby
isolating the most critical subgroups.
  We validate the proposed approach using multiple synthetic datasets and
demonstrate its robustness under changing population distributions - revealing
how discrimination is manifested within the space of sensitive attributes.
Furthermore, we examine a specialized setting of monotonic fairness for the
case of skin color. Our empirical results on both CelebA and FFHQ, leveraging
the skin tone as predicted by an industrial proprietary algorithm, show that
the proposed segmentation uncovers more nuanced patterns of discrimination than
previously reported, and that these findings remain stable across datasets for
a given model. Finally, we leverage our grouping model for debiasing purpose,
aiming at predicting fair scores with group-by-group post-processing. The
results demonstrate that our approach improves fairness while having minimal
impact on accuracy, thus confirming our partition method and opening the door
for industrial deployment.

</details>


### [139] [MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection](https://arxiv.org/abs/2507.11252)
*Guanghao Wu,Chen Xu,Hai Song,Chong Wang,Qixing Zhang*

Main category: cs.CV

TL;DR: The paper proposes a deep learning framework to generate realistic forest fire smoke images, addressing the lack of data and inconsistencies in existing models.


<details>
  <summary>Details</summary>
Motivation: The study aims to tackle the challenge of scarce smoke image data, crucial for improving forest fire smoke detection.

Method: The authors use a pre-trained segmentation model and multimodal image captioning, coupled with a novel mask-guided network architecture and loss function to enhance image synthesis quality.

Result: Experiments demonstrated that the generated smoke images are realistic, diverse, and improve the performance of smoke detection models.

Conclusion: The proposed framework is effective in synthesizing high-quality smoke images to aid in forest fire detection, and the code is openly available for further research.

Abstract: Smoke is the first visible indicator of a wildfire.With the advancement of
deep learning, image-based smoke detection has become a crucial method for
detecting and preventing forest fires. However, the scarcity of smoke image
data from forest fires is one of the significant factors hindering the
detection of forest fire smoke. Image generation models offer a promising
solution for synthesizing realistic smoke images. However, current inpainting
models exhibit limitations in generating high-quality smoke representations,
particularly manifesting as inconsistencies between synthesized smoke and
background contexts. To solve these problems, we proposed a comprehensive
framework for generating forest fire smoke images. Firstly, we employed the
pre-trained segmentation model and the multimodal model to obtain smoke masks
and image captions.Then, to address the insufficient utilization of masks and
masked images by inpainting models, we introduced a network architecture guided
by mask and masked image features. We also proposed a new loss function, the
mask random difference loss, which enhances the consistency of the generated
effects around the mask by randomly expanding and eroding the mask
edges.Finally, to generate a smoke image dataset using random masks for
subsequent detection tasks, we incorporated smoke characteristics and use a
multimodal large language model as a filtering tool to select diverse and
reasonable smoke images, thereby improving the quality of the synthetic
dataset. Experiments showed that our generated smoke images are realistic and
diverse, and effectively enhance the performance of forest fire smoke detection
models. Code is available at https://github.com/wghr123/MFGDiffusion.

</details>


### [140] [ViewSRD: 3D Visual Grounding via Structured Multi-View Decomposition](https://arxiv.org/abs/2507.11261)
*Ronggang Huang,Haoxin Yang,Yan Cai,Xuemiao Xu,Huaidong Zhang,Shengfeng He*

Main category: cs.CV

TL;DR: The paper presents ViewSRD, a 3D visual grounding framework that addresses challenges in complex multi-anchor queries and spatial inconsistencies via structured multi-view decomposition and reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing 3D visual grounding methods struggle with disentangling targets in multi-anchor queries and handling spatial inconsistencies stemming from perspective variations.

Method: ViewSRD employs a Simple Relation Decoupling (SRD) module for query restructuring, a Multi-view Textual-Scene Interaction (Multi-TSI) module using Cross-modal Consistent View Tokens for feature integration, and a Textual-Scene Reasoning module for robust 3D localization.

Result: ViewSRD demonstrates significant improvements over state-of-the-art methods, especially on datasets featuring complex queries that require precise spatial reasoning.

Conclusion: The proposed framework effectively overcomes challenges in 3D visual grounding, offering better performance through structured decomposition and multi-view reasoning.

Abstract: 3D visual grounding aims to identify and localize objects in a 3D space based
on textual descriptions. However, existing methods struggle with disentangling
targets from anchors in complex multi-anchor queries and resolving
inconsistencies in spatial descriptions caused by perspective variations. To
tackle these challenges, we propose ViewSRD, a framework that formulates 3D
visual grounding as a structured multi-view decomposition process. First, the
Simple Relation Decoupling (SRD) module restructures complex multi-anchor
queries into a set of targeted single-anchor statements, generating a
structured set of perspective-aware descriptions that clarify positional
relationships. These decomposed representations serve as the foundation for the
Multi-view Textual-Scene Interaction (Multi-TSI) module, which integrates
textual and scene features across multiple viewpoints using shared, Cross-modal
Consistent View Tokens (CCVTs) to preserve spatial correlations. Finally, a
Textual-Scene Reasoning module synthesizes multi-view predictions into a
unified and robust 3D visual grounding. Experiments on 3D visual grounding
datasets show that ViewSRD significantly outperforms state-of-the-art methods,
particularly in complex queries requiring precise spatial differentiation.

</details>


### [141] [YOLOatr : Deep Learning Based Automatic Target Detection and Localization in Thermal Infrared Imagery](https://arxiv.org/abs/2507.11267)
*Aon Safdar,Usman Akram,Waseem Anwar,Basit Malik,Mian Ibad Ali*

Main category: cs.CV

TL;DR: This paper introduces YOLOatr, a modified version of YOLOv5s, tailored for real-time Automatic Target Recognition (ATR) in thermal infrared (TI) imagery, achieving up to 99.6% accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the challenge of applying computer vision tasks to ATR using TI imagery in defense and surveillance domains. Limitations include datasets, hardware constraints, and environmental factors that reduce the effectiveness of SOTA deep learning models.

Method: The paper proposes YOLOatr, making modifications to existing YOLOv5s architecture through adjustments in detection heads, feature fusion, and custom augmentation. Performance is evaluated using a DSIAC MWIR dataset.

Result: YOLOatr achieves up to 99.6% accuracy for ATR tasks under correlated and decorrelated testing protocols, outperforming prior SOTA solutions.

Conclusion: YOLOatr demonstrates promising improvements in real-time ATR performance within challenging TI imagery domains, addressing limitations of deep learning in defensive and surveillance applications.

Abstract: Automatic Target Detection (ATD) and Recognition (ATR) from Thermal Infrared
(TI) imagery in the defense and surveillance domain is a challenging computer
vision (CV) task in comparison to the commercial autonomous vehicle perception
domain. Limited datasets, peculiar domain-specific and TI modality-specific
challenges, i.e., limited hardware, scale invariance issues due to greater
distances, deliberate occlusion by tactical vehicles, lower sensor resolution
and resultant lack of structural information in targets, effects of weather,
temperature, and time of day variations, and varying target to clutter ratios
all result in increased intra-class variability and higher inter-class
similarity, making accurate real-time ATR a challenging CV task. Resultantly,
contemporary state-of-the-art (SOTA) deep learning architectures underperform
in the ATR domain. We propose a modified anchor-based single-stage detector,
called YOLOatr, based on a modified YOLOv5s, with optimal modifications to the
detection heads, feature fusion in the neck, and a custom augmentation profile.
We evaluate the performance of our proposed model on a comprehensive DSIAC MWIR
dataset for real-time ATR over both correlated and decorrelated testing
protocols. The results demonstrate that our proposed model achieves
state-of-the-art ATR performance of up to 99.6%.

</details>


### [142] [Tomato Multi-Angle Multi-Pose Dataset for Fine-Grained Phenotyping](https://arxiv.org/abs/2507.11279)
*Yujie Zhang,Sabine Struckmeyer,Andreas Kolb,Sven Reichardt*

Main category: cs.CV

TL;DR: The paper introduces TomatoMAP, a novel IoT-based dataset for plant phenotyping, with 64,464 images annotated for fine-grained plant analysis, and validates its efficacy using deep learning models.


<details>
  <summary>Details</summary>
Motivation: Traditional plant phenotyping methods are limited by observer bias and inconsistencies, reducing the accuracy and reproducibility of analyses.

Method: The authors developed an IoT-based imaging system to acquire standardized RGB images of tomato plants, with extensive annotations for regions of interest and growth stages. Validation was performed using deep learning models such as MobileNetv3, YOLOv11, and MaskRCNN.

Result: The models trained on TomatoMAP demonstrated accuracy and speed comparable to domain experts, with reliability confirmed by statistical metrics like Cohen's Kappa.

Conclusion: TomatoMAP provides a reliable and accurate solution for fine-grained plant phenotyping, addressing traditional limitations through AI-driven analysis.

Abstract: Observer bias and inconsistencies in traditional plant phenotyping methods
limit the accuracy and reproducibility of fine-grained plant analysis. To
overcome these challenges, we developed TomatoMAP, a comprehensive dataset for
Solanum lycopersicum using an Internet of Things (IoT) based imaging system
with standardized data acquisition protocols. Our dataset contains 64,464 RGB
images that capture 12 different plant poses from four camera elevation angles.
Each image includes manually annotated bounding boxes for seven regions of
interest (ROIs), including leaves, panicle, batch of flowers, batch of fruits,
axillary shoot, shoot and whole plant area, along with 50 fine-grained growth
stage classifications based on the BBCH scale. Additionally, we provide 3,616
high-resolution image subset with pixel-wise semantic and instance segmentation
annotations for fine-grained phenotyping. We validated our dataset using a
cascading model deep learning framework combining MobileNetv3 for
classification, YOLOv11 for object detection, and MaskRCNN for segmentation.
Through AI vs. Human analysis involving five domain experts, we demonstrate
that the models trained on our dataset achieve accuracy and speed comparable to
the experts. Cohen's Kappa and inter-rater agreement heatmap confirm the
reliability of automated fine-grained phenotyping using our approach.

</details>


### [143] [Detección y Cuantificación de Erosión Fluvial con Visión Artificial](https://arxiv.org/abs/2507.11301)
*Paúl Maji,Marlon Túquerres,Stalin Valencia,Marcela Valenzuela,Christian Mejia-Escobar*

Main category: cs.CV

TL;DR: This paper introduces EROSCAN, a web tool that uses YOLOv11 and AI to detect and quantify fluvial erosion automatically.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of manually detecting and quantifying fluvial erosion, and improve efficiency in risk management and planning.

Method: YOLOv11 model fine-tuned with photographs and LiDAR images, segmented and labeled through Roboflow for erosion detection.

Result: Achieved 70% accuracy in erosion pattern detection and reliable area calculation in pixels and square meters.

Conclusion: The developed EROSCAN system effectively automates fluvial erosion detection, aiding in decision-making for infrastructure safety and territorial planning.

Abstract: Fluvial erosion is a natural process that can generate significant impacts on
soil stability and strategic infrastructures. The detection and monitoring of
this phenomenon is traditionally addressed by photogrammetric methods and
analysis in geographic information systems. These tasks require specific
knowledge and intensive manual processing. This study proposes an artificial
intelligence-based approach for automatic identification of eroded zones and
estimation of their area. The state-of-the-art computer vision model YOLOv11,
adjusted by fine-tuning and trained with photographs and LiDAR images, is used.
This combined dataset was segmented and labeled using the Roboflow platform.
Experimental results indicate efficient detection of erosion patterns with an
accuracy of 70%, precise identification of eroded areas and reliable
calculation of their extent in pixels and square meters. As a final product,
the EROSCAN system has been developed, an interactive web application that
allows users to upload images and obtain automatic segmentations of fluvial
erosion, together with the estimated area. This tool optimizes the detection
and quantification of the phenomenon, facilitating decision making in risk
management and territorial planning.

</details>


### [144] [A Mixed-Primitive-based Gaussian Splatting Method for Surface Reconstruction](https://arxiv.org/abs/2507.11321)
*Haoxuan Qu,Yujun Cai,Hossein Rahmani,Ajay Kumar,Junsong Yuan,Jun Liu*

Main category: cs.CV

TL;DR: This research proposes a novel Gaussian Splatting (GS) framework that incorporates multiple geometric primitives for improved surface reconstruction.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing GS-based methods that use only a single type of primitive, resulting in insufficient representation of complex object surfaces.

Method: The framework introduces compositional splatting, mixed-primitive-based initialization, and vertex pruning mechanisms to enhance the surface representation learning process.

Result: Experiments demonstrate that the proposed framework achieves significant improvements in accurate surface reconstruction.

Conclusion: The multi-primitive approach enriches the expressive power of Gaussian Splatting, offering high-quality surface representation for diverse 3D objects.

Abstract: Recently, Gaussian Splatting (GS) has received a lot of attention in surface
reconstruction. However, while 3D objects can be of complex and diverse shapes
in the real world, existing GS-based methods only limitedly use a single type
of splatting primitive (Gaussian ellipse or Gaussian ellipsoid) to represent
object surfaces during their reconstruction. In this paper, we highlight that
this can be insufficient for object surfaces to be represented in high quality.
Thus, we propose a novel framework that, for the first time, enables Gaussian
Splatting to incorporate multiple types of (geometrical) primitives during its
surface reconstruction process. Specifically, in our framework, we first
propose a compositional splatting strategy, enabling the splatting and
rendering of different types of primitives in the Gaussian Splatting pipeline.
In addition, we also design our framework with a mixed-primitive-based
initialization strategy and a vertex pruning mechanism to further promote its
surface representation learning process to be well executed leveraging
different types of primitives. Extensive experiments show the efficacy of our
framework and its accurate surface reconstruction performance.

</details>


### [145] [MonoMVSNet: Monocular Priors Guided Multi-View Stereo Network](https://arxiv.org/abs/2507.11333)
*Jianfei Jiang,Qiankun Liu,Haochen Yu,Hongyuan Liu,Liyong Wang,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: This paper proposes MonoMVSNet, a monocular feature and depth-aided multi-view stereo (MVS) network that improves depth map predictions by addressing challenges like textureless regions and reflective surfaces.


<details>
  <summary>Details</summary>
Motivation: Current learning-based MVS methods face difficulty in challenging regions where feature matching fails, while monocular depth estimation performs relatively better in such areas. The study aims to combine the advantages of monocular depth estimation with MVS.

Method: The proposed MonoMVSNet integrates monocular features into multi-view geometry via attention mechanisms, aligns monocular depth for edge-sensitive depth candidate updates, and introduces a relative consistency loss for supervising depth predictions.

Result: MonoMVSNet achieves state-of-the-art results on the DTU and Tanks-and-Temples datasets. It ranks first on the Tanks-and-Temples Intermediate and Advanced benchmarks, demonstrating its superiority.

Conclusion: The integration of monocular priors into MVS improves performance in challenging scenarios, marking MonoMVSNet as a robust solution for depth map prediction in multi-view stereo tasks.

Abstract: Learning-based Multi-View Stereo (MVS) methods aim to predict depth maps for
a sequence of calibrated images to recover dense point clouds. However,
existing MVS methods often struggle with challenging regions, such as
textureless regions and reflective surfaces, where feature matching fails. In
contrast, monocular depth estimation inherently does not require feature
matching, allowing it to achieve robust relative depth estimation in these
regions. To bridge this gap, we propose MonoMVSNet, a novel monocular feature
and depth guided MVS network that integrates powerful priors from a monocular
foundation model into multi-view geometry. Firstly, the monocular feature of
the reference view is integrated into source view features by the attention
mechanism with a newly designed cross-view position encoding. Then, the
monocular depth of the reference view is aligned to dynamically update the
depth candidates for edge regions during the sampling procedure. Finally, a
relative consistency loss is further designed based on the monocular depth to
supervise the depth prediction. Extensive experiments demonstrate that
MonoMVSNet achieves state-of-the-art performance on the DTU and
Tanks-and-Temples datasets, ranking first on the Tanks-and-Temples Intermediate
and Advanced benchmarks. The source code is available at
https://github.com/JianfeiJ/MonoMVSNet.

</details>


### [146] [UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks](https://arxiv.org/abs/2507.11336)
*Peiran Wu,Yunze Liu,Zhengdong Zhu,Enmin Zhou,Shawn Shen*

Main category: cs.CV

TL;DR: The paper introduces UGC-VideoCap, a dataset and model framework for omnimodal captioning of short-form, user-generated videos, accounting for audio and visual data equally. It utilizes advanced techniques for detailed video understanding.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the gap in video captioning research where current benchmarks and models are overly visual-centric and lack audio integration, which is essential for understanding real-world user-generated videos.

Method: The authors propose UGC-VideoCap, a dataset of 1000 TikTok videos annotated for audio, visual, and audio-visual semantics, with 4000 QA pairs. They also develop UGC-VideoCaptioner(3B), a captioning model using a two-stage training method involving supervised fine-tuning and Group Relative Policy Optimization (GRPO) for efficient adaptation with limited data.

Result: The dataset provides balanced multimodal annotations, and the model achieves competitive performance despite using limited data, addressing challenges in omnimodal captioning for real-world videos.

Conclusion: UGC-VideoCap and UGC-VideoCaptioner(3B) offer a robust foundation for advancing omnimodal video understanding by integrating audio and visual modalities effectively in user-generated contexts.

Abstract: Real-world user-generated videos, especially on platforms like TikTok, often
feature rich and intertwined audio visual content. However, existing video
captioning benchmarks and models remain predominantly visual centric,
overlooking the crucial role of audio in conveying scene dynamics, speaker
intent, and narrative context. This lack of omni datasets and lightweight,
capable models hampers progress in fine grained, multimodal video
understanding. To address these challenges, we introduce UGC-VideoCap, a new
benchmark and model framework specifically designed for detailed omnimodal
captioning of short form user-generated videos. Unlike prior datasets,
UGC-VideoCap emphasizes balanced integration of audio and visual modalities,
featuring 1000 TikTok videos annotated through a structured three stage
human-in-the-loop pipeline covering audio only, visual only, and joint audio
visual semantics. The benchmark also includes 4000 carefully crafted QA pairs
probing both unimodal and cross modal understanding. Alongside the dataset, we
propose UGC-VideoCaptioner(3B), a 3B parameter captioning model distilled from
Gemini 2.5 Flash. Using a novel two-stage training strategy supervised fine
tuning followed by Group Relative Policy Optimization (GRPO), our approach
enables efficient adaptation from limited data while maintaining competitive
performance. Together, our benchmark and model offer a high-quality foundation
and a data-efficient solution for advancing omnimodal video captioning in
unconstrained real-world UGC settings.

</details>


### [147] [Attributes Shape the Embedding Space of Face Recognition Models](https://arxiv.org/abs/2507.11372)
*Pierrick Leroy,Antonio Mastropietro,Marco Nurisso,Francesco Vaccarino*

Main category: cs.CV

TL;DR: A study exploring how face recognition models embed multiscale geometric structures influenced by facial and image attributes, proposing a physics-inspired alignment metric for evaluation.


<details>
  <summary>Details</summary>
Motivation: To understand how face recognition models manage attribute invariance and dependence, and offer interpretable insights into feature embeddings influenced by facial and image attributes.

Method: Developed and evaluated a physics-inspired alignment metric to measure attribute dependence and invariance in both controlled models and widely used FR models fine-tuned with synthetic data.

Result: Found that face recognition models exhibit varying degrees of invariance to different attributes, shedding light on their strengths and weaknesses in embedding space.

Conclusion: Deep learning-based face recognition models can be better understood through geometric analysis of embedding space, contributing to their interpretability and performance evaluation.

Abstract: Face Recognition (FR) tasks have made significant progress with the advent of
Deep Neural Networks, particularly through margin-based triplet losses that
embed facial images into high-dimensional feature spaces. During training,
these contrastive losses focus exclusively on identity information as labels.
However, we observe a multiscale geometric structure emerging in the embedding
space, influenced by interpretable facial (e.g., hair color) and image
attributes (e.g., contrast). We propose a geometric approach to describe the
dependence or invariance of FR models to these attributes and introduce a
physics-inspired alignment metric. We evaluate the proposed metric on
controlled, simplified models and widely used FR models fine-tuned with
synthetic data for targeted attribute augmentation. Our findings reveal that
the models exhibit varying degrees of invariance across different attributes,
providing insight into their strengths and weaknesses and enabling deeper
interpretability. Code available here:
https://github.com/mantonios107/attrs-fr-embs}{https://github.com/mantonios107/attrs-fr-embs

</details>


### [148] [Implementing Adaptations for Vision AutoRegressive Model](https://arxiv.org/abs/2507.11441)
*Kaif Shaikh,Antoni Kowalczuk,Franziska Boenisch,Adam Dziedzic*

Main category: cs.CV

TL;DR: The paper explores adaptation strategies for Vision AutoRegressive models (VAR) in image generation, especially focusing on Differentially Private (DP) settings.


<details>
  <summary>Details</summary>
Motivation: Existing adaptations for image generation models mainly focus on Diffusion Models (DMs), leaving VAR largely unexplored, especially in privacy-preserving contexts.

Method: The authors implement and benchmark various adaptation strategies for VAR and compare their performance against state-of-the-art DM adaptation methods.

Result: VAR shows superior performance in non-privacy-preserving adaptations compared to DMs but exhibits inferior performance under Differential Privacy settings.

Conclusion: Future research is needed to improve Differentially Private adaptation strategies for VAR models to fully leverage their potential.

Abstract: Vision AutoRegressive model (VAR) was recently introduced as an alternative
to Diffusion Models (DMs) in image generation domain. In this work we focus on
its adaptations, which aim to fine-tune pre-trained models to perform specific
downstream tasks, like medical data generation. While for DMs there exist many
techniques, adaptations for VAR remain underexplored. Similarly, differentially
private (DP) adaptations-ones that aim to preserve privacy of the adaptation
data-have been extensively studied for DMs, while VAR lacks such solutions. In
our work, we implement and benchmark many strategies for VAR, and compare them
to state-of-the-art DM adaptation strategies. We observe that VAR outperforms
DMs for non-DP adaptations, however, the performance of DP suffers, which
necessitates further research in private adaptations for VAR. Code is available
at https://github.com/sprintml/finetuning_var_dp.

</details>


### [149] [COLI: A Hierarchical Efficient Compressor for Large Images](https://arxiv.org/abs/2507.11443)
*Haoran Wang,Hanyu Pei,Yang Lyu,Kai Zhang,Li Li,Feng-Lei Fan*

Main category: cs.CV

TL;DR: The paper proposes COLI, a new framework for efficient large-image compression using Neural Representations for Videos, overcoming speed and compression ratio challenges.


<details>
  <summary>Details</summary>
Motivation: Increasing demand for efficient large-image compression as high-resolution and wide-field imagery grows. Traditional methods lose detail or lack generalization.

Method: The framework uses a pretraining-finetuning paradigm, mixed-precision training, parallel loss reformulation, and a Hyper-Compression post-training technique.

Result: COLI delivers competitive or superior PSNR and SSIM at lower bpp, with training speeds increased by up to 4x on medical imaging datasets.

Conclusion: COLI offers an efficient large-image compression solution, outperforming prior methods in both quality and computational time.

Abstract: The escalating adoption of high-resolution, large-field-of-view imagery
amplifies the need for efficient compression methodologies. Conventional
techniques frequently fail to preserve critical image details, while
data-driven approaches exhibit limited generalizability. Implicit Neural
Representations (INRs) present a promising alternative by learning continuous
mappings from spatial coordinates to pixel intensities for individual images,
thereby storing network weights rather than raw pixels and avoiding the
generalization problem. However, INR-based compression of large images faces
challenges including slow compression speed and suboptimal compression ratios.
To address these limitations, we introduce COLI (Compressor for Large Images),
a novel framework leveraging Neural Representations for Videos (NeRV). First,
recognizing that INR-based compression constitutes a training process, we
accelerate its convergence through a pretraining-finetuning paradigm,
mixed-precision training, and reformulation of the sequential loss into a
parallelizable objective. Second, capitalizing on INRs' transformation of image
storage constraints into weight storage, we implement Hyper-Compression, a
novel post-training technique to substantially enhance compression ratios while
maintaining minimal output distortion. Evaluations across two medical imaging
datasets demonstrate that COLI consistently achieves competitive or superior
PSNR and SSIM metrics at significantly reduced bits per pixel (bpp), while
accelerating NeRV training by up to 4 times.

</details>


### [150] [HUG-VAS: A Hierarchical NURBS-Based Generative Model for Aortic Geometry Synthesis and Controllable Editing](https://arxiv.org/abs/2507.11474)
*Pan Du,Mingqi Xu,Xiaozhi Zhu,Jian-xun Wang*

Main category: cs.CV

TL;DR: This paper presents HUG-VAS, a model for synthesizing complex vascular geometries using hierarchical generative methods, combining NURBS parameterization and diffusion-based modeling.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome the limitations of traditional statistical shape modeling (SSM), which struggles with linear assumptions and doesn't scale well to complex vascular geometries.

Method: HUG-VAS uses a hierarchical architecture combining denoising diffusion models for centerline generation and guided diffusion for radial profile synthesis, leveraging NURBS surface parameterization.

Result: HUG-VAS successfully generates realistic, anatomically faithful aortic geometries with biomarker distributions closely matching patient-specific samples, demonstrating its utility in various applications.

Conclusion: HUG-VAS is the first framework integrating image-derived priors with generative modeling via NURBS and hierarchical diffusion processes, providing advancements in cardiovascular modeling and applications.

Abstract: Accurate characterization of vascular geometry is essential for
cardiovascular diagnosis and treatment planning. Traditional statistical shape
modeling (SSM) methods rely on linear assumptions, limiting their expressivity
and scalability to complex topologies such as multi-branch vascular structures.
We introduce HUG-VAS, a Hierarchical NURBS Generative model for Vascular
geometry Synthesis, which integrates NURBS surface parameterization with
diffusion-based generative modeling to synthesize realistic, fine-grained
aortic geometries. Trained with 21 patient-specific samples, HUG-VAS generates
anatomically faithful aortas with supra-aortic branches, yielding biomarker
distributions that closely match those of the original dataset. HUG-VAS adopts
a hierarchical architecture comprising a denoising diffusion model that
generates centerlines and a guided diffusion model that synthesizes radial
profiles conditioned on those centerlines, thereby capturing two layers of
anatomical variability. Critically, the framework supports zero-shot
conditional generation from image-derived priors, enabling practical
applications such as interactive semi-automatic segmentation, robust
reconstruction under degraded imaging conditions, and implantable device
optimization. To our knowledge, HUG-VAS is the first SSM framework to bridge
image-derived priors with generative shape modeling via a unified integration
of NURBS parameterization and hierarchical diffusion processes.

</details>


### [151] [C-FBI: A Combinatorial method using Convolutions for Circle Fitting in Blurry Images](https://arxiv.org/abs/2507.11476)
*Esteban Román Catafau,Torbjörn E. M. Nordling*

Main category: cs.CV

TL;DR: The paper proposes 3C-FBI, a circle detection and fitting algorithm excelling in degraded imaging conditions, achieving state-of-the-art accuracy and robustness across medical and synthetic datasets.


<details>
  <summary>Details</summary>
Motivation: The need to address circle detection and fitting in challenging imaging scenarios like medical imaging and robotics.

Method: Utilizes combinatorial edgel sampling and convolution-based density estimation for efficient circle detection and fitting.

Result: Demonstrates superior accuracy (Jaccard Index ~0.989) and real-time performance (40.3 fps), outperforming prior methods in medical and synthetic datasets.

Conclusion: 3C-FBI is a robust, accurate, and fast solution applicable to medical imaging, robotics, and industrial inspection under degraded conditions.

Abstract: This paper addresses the fundamental computer vision challenge of robust
circle detection and fitting in degraded imaging conditions. We present
Combinatorial Convolution-based Circle Fitting for Blurry Images (3C-FBI), an
algorithm that bridges the gap between circle detection and precise parametric
fitting by combining (1) efficient combinatorial edge pixel (edgel) sampling
and (2) convolution-based density estimation in parameter space.
  We evaluate 3C-FBI across three experimental frameworks: (1) real-world
medical data from Parkinson's disease assessments (144 frames from 36 videos),
(2) controlled synthetic data following established circle-fitting benchmarks,
and (3) systematic analysis across varying spatial resolutions and outlier
contamination levels. Results show that 3C-FBI achieves state-of-the-art
accuracy (Jaccard index 0.896) while maintaining real-time performance (40.3
fps), significantly outperforming classical methods like RCD (6.8 fps) on a
standard CPU (i7-10875H). It maintains near-perfect accuracy (Jaccard almost
1.0) at high resolutions (480x480) and reliable performance (Jaccard higher
than 0.95) down to 160x160 with up to 20% outliers.
  In extensive synthetic testing, 3C-FBI achieves a mean Jaccard Index of 0.989
across contamination levels, comparable to modern methods like Qi et al. (2024,
0.991), and surpassing RHT (0.964). This combination of accuracy, speed, and
robustness makes 3C-FBI ideal for medical imaging, robotics, and industrial
inspection under challenging conditions.

</details>


### [152] [COLIBRI Fuzzy Model: Color Linguistic-Based Representation and Interpretation](https://arxiv.org/abs/2507.11488)
*Pakizar Shamoi,Nuray Toganas,Muragul Muratbekova,Elnara Kadyrgali,Adilet Yerkin,Ayan Igali,Malika Ziyada,Ayana Adilova,Aron Karatayev,Yerdauit Torekhan*

Main category: cs.CV

TL;DR: The paper introduces COLIBRI, a fuzzy color model aligning computational color representations with human perception through large-scale human categorization data and adaptation mechanisms.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between computational color models and human visual perception, addressing challenges in mimicry of color perception.

Method: A three-phase experimental design: (1) Preliminary experiments to identify color stimuli, (2) A large-scale human categorization survey (n=2496), and (3) Fuzzy partition extraction and membership function generation.

Result: COLIBRI demonstrates improved alignment with human perception compared to traditional color models like RGB, HSV, and LAB.

Conclusion: COLIBRI offers significant advancements for perceptually relevant color representation, valuable in fields such as AI, design, marketing, and human-computer interaction.

Abstract: Colors are omnipresent in today's world and play a vital role in how humans
perceive and interact with their surroundings. However, it is challenging for
computers to imitate human color perception. This paper introduces the Human
Perception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based
Representation and Interpretation), designed to bridge the gap between
computational color representations and human visual perception. The proposed
model uses fuzzy sets and logic to create a framework for color categorization.
Using a three-phase experimental approach, the study first identifies
distinguishable color stimuli for hue, saturation, and intensity through
preliminary experiments, followed by a large-scale human categorization survey
involving more than 1000 human subjects. The resulting data are used to extract
fuzzy partitions and generate membership functions that reflect real-world
perceptual uncertainty. The model incorporates a mechanism for adaptation that
allows refinement based on feedback and contextual changes. Comparative
evaluations demonstrate the model's alignment with human perception compared to
traditional color models, such as RGB, HSV, and LAB. To the best of our
knowledge, no previous research has documented the construction of a model for
color attribute specification based on a sample of this size or a comparable
sample of the human population (n = 2496). Our findings are significant for
fields such as design, artificial intelligence, marketing, and human-computer
interaction, where perceptually relevant color representation is critical.

</details>


### [153] [CATVis: Context-Aware Thought Visualization](https://arxiv.org/abs/2507.11522)
*Tariq Mehmood,Hamza Ahmad,Muhammad Haroon Shakeel,Murtaza Taj*

Main category: cs.CV

TL;DR: The paper introduces a 5-stage framework using EEG signals for visually representing images, achieving superior accuracy and quality compared with state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Decoding visual representations from EEG signals is challenging due to their noisy and complex nature. Overcoming these limitations can expand the applications of EEG-based brain-computer interfaces.

Method: The approach involves a 5-stage framework: an EEG encoder, cross-modal alignment of EEG and text, caption refinement, weighted concept-caption embedding, and using Stable Diffusion for image generation.

Result: The proposed method enhanced image generation with 13.43% improvement in Classification Accuracy, 15.21% in Generation Accuracy, and reduced Fréchet Inception Distance by 36.61%.

Conclusion: The framework achieves high-quality, semantically-aligned images from EEG signals, outperforming existing methods in classification and generation effectiveness.

Abstract: EEG-based brain-computer interfaces (BCIs) have shown promise in various
applications, such as motor imagery and cognitive state monitoring. However,
decoding visual representations from EEG signals remains a significant
challenge due to their complex and noisy nature. We thus propose a novel
5-stage framework for decoding visual representations from EEG signals: (1) an
EEG encoder for concept classification, (2) cross-modal alignment of EEG and
text embeddings in CLIP feature space, (3) caption refinement via re-ranking,
(4) weighted interpolation of concept and caption embeddings for richer
semantics, and (5) image generation using a pre-trained Stable Diffusion model.
We enable context-aware EEG-to-image generation through cross-modal alignment
and re-ranking. Experimental results demonstrate that our method generates
high-quality images aligned with visual stimuli, outperforming SOTA approaches
by 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and
reducing Fr\'echet Inception Distance by 36.61%, indicating superior semantic
alignment and image quality.

</details>


### [154] [CharaConsist: Fine-Grained Consistent Character Generation](https://arxiv.org/abs/2507.11533)
*Mengyu Wang,Henghui Ding,Jianing Peng,Yao Zhao,Yunpeng Chen,Yunchao Wei*

Main category: cs.CV

TL;DR: CharaConsist proposes methods to improve text-to-image generation by ensuring consistency in character identity and background details across images.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image generation methods lack consistency in backgrounds and characters, especially for large motion variations, limiting their reliability for real-world usage.

Method: CharaConsist implements point-tracking attention, adaptive token merge, and decoupled foreground-background control to address consistency issues.

Result: CharaConsist achieves fine-grained consistency in generating characters and backgrounds across fixed and varied scenes, with output quality leveraging strong base models.

Conclusion: CharaConsist broadens the applicability of text-to-image generation by addressing consistency challenges, with source code available for community use.

Abstract: In text-to-image generation, producing a series of consistent contents that
preserve the same identity is highly valuable for real-world applications.
Although a few works have explored training-free methods to enhance the
consistency of generated subjects, we observe that they suffer from the
following problems. First, they fail to maintain consistent background details,
which limits their applicability. Furthermore, when the foreground character
undergoes large motion variations, inconsistencies in identity and clothing
details become evident. To address these problems, we propose CharaConsist,
which employs point-tracking attention and adaptive token merge along with
decoupled control of the foreground and background. CharaConsist enables
fine-grained consistency for both foreground and background, supporting the
generation of one character in continuous shots within a fixed scene or in
discrete shots across different scenes. Moreover, CharaConsist is the first
consistent generation method tailored for text-to-image DiT model. Its ability
to maintain fine-grained consistency, combined with the larger capacity of
latest base model, enables it to produce high-quality visual outputs,
broadening its applicability to a wider range of real-world scenarios. The
source code has been released at https://github.com/Murray-Wang/CharaConsist

</details>


### [155] [Streaming 4D Visual Geometry Transformer](https://arxiv.org/abs/2507.11539)
*Dong Zhuo,Wenzhao Zheng,Jiahe Guo,Yuqi Wu,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: The paper introduces a streaming 4D visual geometry transformer for real-time 4D geometric perception and reconstruction, leveraging a causal transformer architecture for efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the need for real-time and interactive 4D spatial-temporal geometry processing in computer vision, which remains a challenging task.

Method: The model employs a causal transformer architecture with temporal causal attention, leveraging implicit memory via cached historical keys and values, supported by efficient knowledge distillation from a dense bidirectional visual geometry grounded transformer (VGGT).

Result: The proposed model achieves efficient online 4D reconstruction with competitive quality while improving inference speed through techniques like optimized efficient attention operator.

Conclusion: The work presents a scalable approach to 4D visual geometry reconstruction that balances speed and spatial consistency, enabling advances in interactive and real-time 4D vision systems.

Abstract: Perceiving and reconstructing 4D spatial-temporal geometry from videos is a
fundamental yet challenging computer vision task. To facilitate interactive and
real-time applications, we propose a streaming 4D visual geometry transformer
that shares a similar philosophy with autoregressive large language models. We
explore a simple and efficient design and employ a causal transformer
architecture to process the input sequence in an online manner. We use temporal
causal attention and cache the historical keys and values as implicit memory to
enable efficient streaming long-term 4D reconstruction. This design can handle
real-time 4D reconstruction by incrementally integrating historical information
while maintaining high-quality spatial consistency. For efficient training, we
propose to distill knowledge from the dense bidirectional visual geometry
grounded transformer (VGGT) to our causal model. For inference, our model
supports the migration of optimized efficient attention operator (e.g.,
FlashAttention) from the field of large language models. Extensive experiments
on various 4D geometry perception benchmarks demonstrate that our model
increases the inference speed in online scenarios while maintaining competitive
performance, paving the way for scalable and interactive 4D vision systems.
Code is available at: https://github.com/wzzheng/StreamVGGT.

</details>


### [156] [Towards Depth Foundation Model: Recent Trends in Vision-Based Depth Estimation](https://arxiv.org/abs/2507.11540)
*Zhen Xu,Hongyu Zhou,Sida Peng,Haotong Lin,Haoyu Guo,Jiahao Shao,Peishan Yang,Qinglin Yang,Sheng Miao,Xingyi He,Yifan Wang,Yue Wang,Ruizhen Hu,Yiyi Liao,Xiaowei Zhou,Hujun Bao*

Main category: cs.CV

TL;DR: The paper reviews advancements in depth estimation, emphasizing the rise of deep learning-based 'depth foundation models' with improved scalability and generalization.


<details>
  <summary>Details</summary>
Motivation: Depth estimation is critical for modern computer vision tasks, but traditional sensor-based methods face limitations in cost and performance; vision-based approaches have potential but suffer from generalization and dataset constraints.

Method: The paper surveys advancements in deep learning architectures and datasets for depth estimation, covering various settings from monocular to stereo and multi-view scenarios.

Result: The survey identifies promising deep learning architectures and training strategies, alongside large-scale datasets, for developing robust depth estimation models.

Conclusion: Depth foundation models trained on expansive datasets may overcome current limitations, offering robust solutions for real-world applications and guiding future research.

Abstract: Depth estimation is a fundamental task in 3D computer vision, crucial for
applications such as 3D reconstruction, free-viewpoint rendering, robotics,
autonomous driving, and AR/VR technologies. Traditional methods relying on
hardware sensors like LiDAR are often limited by high costs, low resolution,
and environmental sensitivity, limiting their applicability in real-world
scenarios. Recent advances in vision-based methods offer a promising
alternative, yet they face challenges in generalization and stability due to
either the low-capacity model architectures or the reliance on domain-specific
and small-scale datasets. The emergence of scaling laws and foundation models
in other domains has inspired the development of "depth foundation models":
deep neural networks trained on large datasets with strong zero-shot
generalization capabilities. This paper surveys the evolution of deep learning
architectures and paradigms for depth estimation across the monocular, stereo,
multi-view, and monocular video settings. We explore the potential of these
models to address existing challenges and provide a comprehensive overview of
large-scale datasets that can facilitate their development. By identifying key
architectures and training strategies, we aim to highlight the path towards
robust depth foundation models, offering insights into their future research
and applications.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [157] [Cyclic Data Streaming on GPUs for Short Range Stencils Applied to Molecular Dynamics](https://arxiv.org/abs/2507.11289)
*Martin Rose,Simon Homes,Lukas Ramsperger,Jose Gracia,Christoph Niethammer,Jadran Vrabec*

Main category: cs.DC

TL;DR: This paper proposes a framework for high-performance scientific computing using GPU-based communication, demonstrating linear scaling for explicit algorithms and outperforming LAMMPS in strong scaling tests.


<details>
  <summary>Details</summary>
Motivation: To achieve the highest performance in scientific computing by enabling efficient GPU-based communication and parallelism for explicit algorithms.

Method: The authors introduce a framework that propagates slices of a dataset in a ring of GPU processes, enabling parallel-in-time parallelization and eliminating the need for the user to manage communication. Molecular dynamics simulations were used for benchmarking.

Result: The framework shows linear performance scaling tied to dataset size and GPU count, and strong scaling outperforms LAMMPS in molecular dynamics simulation tests.

Conclusion: The novel GPU communication framework provides an efficient and user-friendly solution for high-performance scientific computing, achieving superior scaling compared to existing solutions like LAMMPS.

Abstract: In the quest for highest performance in scientific computing, we present a
novel framework that relies on high-bandwidth communication between GPUs in a
compute cluster. The framework offers linear scaling of performance for
explicit algorithms that is only limited by the size of the dataset and the
number of GPUs. Slices of the dataset propagate in a ring of processes (GPUs)
from one GPU, where they are processed, to the next, which results in a
parallel-in-time parallelization. The user of the framework has to write GPU
kernels that implement the algorithm and provide slices of the dataset.
Knowledge about the underlying parallelization strategy is not required because
the communication between processes is carried out by the framework. As a case
study, molecular dynamics simulation based on the Lennard-Jones potential is
implemented to measure the performance for a homogeneous fluid. Single node
performance and strong scaling behavior of this framework is compared to
LAMMPS, which is outperformed in the strong scaling case.

</details>


### [158] [Scaling the memory wall using mixed-precision -- HPG-MxP on an exascale machine](https://arxiv.org/abs/2507.11512)
*Aditya Kashi,Nicholson Koukpaizan,Hao Lu,Michael Matheson,Sarp Oral,Feiyi Wang*

Main category: cs.DC

TL;DR: The paper introduces High Performance GMRES Mixed Precision (HPG-MxP) benchmark to evaluate mixed-precision algorithms on memory bandwidth-limited HPC systems, achieving a 1.6x speedup on GPU-based supercomputers.


<details>
  <summary>Details</summary>
Motivation: To address the unclear practical gains of mixed-precision algorithms in memory bandwidth-limited scientific simulation applications on HPC platforms.

Method: Developing a highly optimized HPG-MxP benchmark for exascale systems and enhancing algorithms to combine double- and single-precision processing.

Result: The mixed-precision approach demonstrated a 1.6x speedup using double- and single-precision on modern GPU-based supercomputers.

Conclusion: The optimized benchmark highlights the usefulness of mixed-precision algorithms in sparse matrix operations, enabling performance gains on memory bandwidth-limited HPC systems.

Abstract: Mixed-precision algorithms have been proposed as a way for scientific
computing to benefit from some of the gains seen for artificial intelligence
(AI) on recent high performance computing (HPC) platforms. A few applications
dominated by dense matrix operations have seen substantial speedups by
utilizing low precision formats such as FP16. However, a majority of scientific
simulation applications are memory bandwidth limited. Beyond preliminary
studies, the practical gain from using mixed-precision algorithms on a given
HPC system is largely unclear.
  The High Performance GMRES Mixed Precision (HPG-MxP) benchmark has been
proposed to measure the useful performance of a HPC system on sparse
matrix-based mixed-precision applications. In this work, we present a highly
optimized implementation of the HPG-MxP benchmark for an exascale system and
describe our algorithm enhancements. We show for the first time a speedup of
1.6x using a combination of double- and single-precision on modern GPU-based
supercomputers.

</details>


### [159] [FAFO: Over 1 million TPS on a single node running EVM while still Merkleizing every block](https://arxiv.org/abs/2507.10757)
*Ryan Zarick,Isaac Zhang,Daniel Wong,Thomas Kim,Bryan Pellegrino,Mignon Li,Kelvin Wong*

Main category: cs.DC

TL;DR: FAFO is a blockchain transaction scheduler that reorders transactions for optimal concurrency, achieving remarkable throughput and minimizing data contention.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitation of blockchain execution throughput caused by data contention and insufficient parallelism in the execution layer.

Method: FAFO employs CPU-optimized Bloom filters for conflict detection and transaction reordering, integrates with the Rust EVM client, and uses QMDB for improving world-state handling.

Result: Achieved over 1.1M ETH transfers/second and over 500K ERC20 transfers/second with 91% lower cost compared to sharded execution.

Conclusion: FAFO demonstrates high throughput blockchain transaction execution, facilitates scalability, and supports future decentralized applications by improving transaction scheduling and execution.

Abstract: Current blockchain execution throughput is limited by data contention,
reducing execution layer parallelism. Fast Ahead-of-Formation Optimization
(FAFO) is the first blockchain transaction scheduler to address this problem by
reordering transactions before block formation for maximum concurrency. FAFO
uses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts
and schedule parallel transaction execution at high throughput and low
overhead.
  We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1
million native ETH transfers per second and over half a million ERC20 transfers
per second on a single node (Table 1), with 91% lower cost compared to
state-of-the-art sharded execution. Unlike many other existing high throughput
blockchain execution clients, FAFO uses QMDB to Merkleize world state after
every block, enabling light clients and stateless validation for ZK-based
vApps. FAFO scales with minimal synchronization overhead, scaling linearly with
additional CPU resources until it fully exploits the maximum parallelism of the
underlying transaction flow. FAFO proves that the high throughput necessary to
support future decentralized applications can be achieved with a streamlined
execution layer and innovations in blockchain transaction scheduler design.
FAFO is open-sourced at https://github.com/LayerZero-Labs/fafo.

</details>


### [160] [Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks](https://arxiv.org/abs/2507.10789)
*Aaron Jarmusch,Nathan Graddon,Sunita Chandrasekaran*

Main category: cs.DC

TL;DR: The paper provides a microarchitectural analysis of NVIDIA's Blackwell GPUs by examining performance characteristics, comparing them to the Hopper architecture, and offering optimization insights.


<details>
  <summary>Details</summary>
Motivation: To analyze and understand the architectural advancements and performance characteristics of modern NVIDIA Blackwell GPUs.

Method: The study uses targeted microbenchmarks to evaluate memory hierarchy, execution pipelines, tensor cores, and scheduling details, comparing results against the previous Hopper architecture.

Result: Discoveries include in-depth insights into Blackwell's latency, throughput, cache behavior, and power efficiency, along with generational performance improvements and regressions.

Conclusion: The paper delivers actionable insights for optimizing applications on Blackwell GPUs and adds to the broader understanding of GPU architectures.

Abstract: The rapid development in scientific research provides a need for more compute
power, which is partly being solved by GPUs. This paper presents a
microarchitectural analysis of the modern NVIDIA Blackwell architecture by
studying GPU performance
  features with thought through microbenchmarks. We unveil key subsystems,
including the memory hierarchy, SM execution
  pipelines, and the SM sub-core units, including the 5th generation tensor
cores supporting FP4 and FP6 precisions.
  To understand the different key features of the NVIDIA GPU, we study latency,
throughput, cache behavior, and scheduling
  details, revealing subtle tuning metrics in the design of Blackwell. To
develop a comprehensive analysis, we compare the
  Blackwell architecture with the previous Hopper architecture by using the
GeForce RTX 5080 and H100 PCIe, respectively. We
  evaluate and compare results, presenting both generational improvements and
performance regressions. Additionally, we
  investigate the role of power efficiency and energy consumption under varied
workloads. Our findings provide actionable insights
  for application developers, compiler writers, and performance engineers to
optimize workloads on Blackwell-based platforms,
  and contribute new data to the growing research on GPU architectures.

</details>


### [161] [MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix Unit](https://arxiv.org/abs/2507.11067)
*Yinuo Wang,Tianqi Mao,Lin Gan,Wubing Wan,Zeyu Song,Jiayu Fu,Lanke He,Wenqiang Wang,Zekun Yin,Wei Xue,Guangwen Yang*

Main category: cs.DC

TL;DR: The paper introduces MMStencil, a matrix-accelerated approach optimized for 3D high-order stencil computations in HPC environments. With algorithmic and memory innovations, it achieves better performance and hardware utilization than state-of-the-art libraries.


<details>
  <summary>Details</summary>
Motivation: To address the performance and optimization challenges in applying matrix units to 3D high-order stencil computations in High-Performance Computing (HPC).

Method: They analyze matrix-based acceleration strategies and introduce optimizations involving SIMD, matrix units, memory efficiency, and innovative multi-thread parallelism paradigms.

Result: MMStencil demonstrates up to 2.1x performance improvement over state-of-the-art libraries on Nvidia A100 GPGPU, and enables 1.8x speedup in real-world RTM applications.

Conclusion: MMStencil combines algorithmic, memory, and parallelism innovations to consistently achieve high hardware utilization and improved performance in 3D stencil computations, paving the way for advanced HPC applications.

Abstract: Matrix-accelerated stencil computation is a hot research topic, yet its
application to three-dimensional (3D) high-order stencils and HPC remains
underexplored. With the emergence of matrix units on multicore CPUs, we analyze
matrix-based acceleration strategies and tailor an optimal approach for 3D
high-order stencils. We introduce algorithmic optimizations based on SIMD and
matrix units to address strided memory accesses, alignment conflicts, and
redundant accesses. We propose memory optimizations to boost on-package memory
efficiency, and a novel multi-thread parallelism paradigm to overcome
data-sharing challenges caused by the absence of shared data caches. MMStencil
sustains consistently high hardware utilization across diverse stencil shapes
and dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA
effects and MPI limitations in hybrid parallelism. Combining all the
innovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100
GPGPU by up to 2.1x. Moreover, the performance improvements translate directly
to real-world HPC applications and enable RTM applications to yield 1.8x
speedup versus a highly optimized industrial Nvidia A100 GPGPU version.

</details>


### [162] [Generating Dynamic Graph Algorithms for Multiple Backends for a Graph DSL](https://arxiv.org/abs/2507.11094)
*Nibedita Behera,Ashwina Kumar,Atharva Chougule,Mohammed Shan P S,Rushabh Nirdosh Lalwani,Rupesh Nasre*

Main category: cs.DC

TL;DR: This paper addresses the difficulty of parallelizing dynamic graph algorithms, introducing a domain-specific language (DSL) that automatically generates efficient parallel code for morph algorithms involving dynamic graph updates.


<details>
  <summary>Details</summary>
Motivation: Graph algorithms are challenging to parallelize due to irregular computation, memory access, and communication patterns, especially for dynamic graphs that constantly evolve.

Method: The authors propose a DSL to abstract dynamic graph processing and automatically generate parallel code targeting multicore, distributed, and many-core environments. This approach optimizes runtime and synchronization for edge insertions and deletions.

Result: The authors applied the DSL-generated code to ten large graphs and successfully implemented three algorithms—Shortest Paths, PageRank, and Triangle Counting—demonstrating efficiency and scalability.

Conclusion: The study provides a robust mechanism for handling dynamic graph algorithms efficiently, paving the way for better parallelism in increasingly ubiquitous dynamic graph scenarios.

Abstract: With the rapid growth of unstructured and semistructured data, parallelizing
graph algorithms has become essential for efficiency. However, due to the
inherent irregularity in computation, memory access patterns, and
communication, graph algorithms are notoriously difficult to parallelize. To
address this challenge, several libraries, frameworks, and domain-specific
languages (DSLs) have been proposed to ease the parallel programming burden for
domain experts. Existing frameworks partially or fully abstract away
parallelism intricacies, provide intuitive scheduling mnemonics, and employ
program analysis to identify data races and generate synchronization code.
Despite these advances, most frameworks are limited in their abstractions and
runtime optimizations, especially when dealing with static graphs. In contrast,
many real-world graphs are inherently dynamic, with evolving structures over
time through insertions, deletions, and modifications of vertices, edges, and
attributes. Generating efficient and correctly synchronized code for such
dynamic graph algorithms remains a significant challenge.
  In this work, we introduce an abstraction scheme and runtime optimizations
for the efficient processing of morph algorithms. Specifically, given an
initial graph G and a set of updates $\Delta$G involving edge insertions and
deletions, we express the dynamic processing logic through a DSL and
automatically generate parallel code targeting multicore, distributed, and
many-core environments. We demonstrate the effectiveness of our approach by
applying the DSL-generated code to ten large graphs with diverse
characteristics and three widely used algorithms: Shortest Paths, PageRank, and
Triangle Counting.

</details>


### [163] [Boosting Scientific Error-Bounded Lossy Compression through Optimized Synergistic Lossy-Lossless Orchestration](https://arxiv.org/abs/2507.11165)
*Shixun Wu,Jinwen Pan,Jinyang Liu,Jiannan Tian,Ziwei Qiu,Jiajun Huang,Kai Zhao,Xin Liang,Sheng Di,Zizhong Chen,Franck Cappello*

Main category: cs.DC

TL;DR: The paper presents cuSZ-Hi, a high-performance GPU-based compression framework offering significantly improved compression ratios for scientific datasets, while maintaining low-latency and error bounds.


<details>
  <summary>Details</summary>
Motivation: Scientific workflows produce data at extremely high rates on GPUs, prompting the need for advanced data compression methods that are both efficient and error-bounded.

Method: The authors developed cuSZ-Hi by optimizing GPU-based interpolation for prediction, enhancing lossless encoding techniques, and systematically evaluating the framework against competitors.

Result: cuSZ-Hi demonstrates up to 249% improvement in compression ratio under equivalent error bounds and up to 215% improvement if decompression PSNR is prioritized, while maintaining comparable throughput to existing methods.

Conclusion: cuSZ-Hi is a flexible and open-source solution that significantly enhances the efficiency of scientific lossy data compression on GPUs, making it highly suitable for advanced computing workflows.

Abstract: As high-performance computing architectures evolve, more scientific computing
workflows are being deployed on advanced computing platforms such as GPUs.
These workflows can produce raw data at extremely high throughputs, requiring
urgent high-ratio and low-latency error-bounded data compression solutions. In
this paper, we propose cuSZ-Hi, an optimized high-ratio GPU-based scientific
error-bounded lossy compressor with a flexible, domain-irrelevant, and fully
open-source framework design. Our novel contributions are: 1) We maximally
optimize the parallelized interpolation-based data prediction scheme on GPUs,
enabling the full functionalities of interpolation-based scientific data
prediction that are adaptive to diverse data characteristics; 2) We thoroughly
explore and investigate lossless data encoding techniques, then craft and
incorporate the best-fit lossless encoding pipelines for maximizing the
compression ratio of cuSZ-Hi; 3) We systematically evaluate cuSZ-Hi on
benchmarking datasets together with representative baselines. Compared to
existing state-of-the-art scientific lossy compressors, with comparative or
better throughput than existing high-ratio scientific error-bounded lossy
compressors on GPUs, cuSZ-Hi can achieve up to 249% compression ratio
improvement under the same error bound, and up to 215% compression ratio
improvement under the same decompression data PSNR.

</details>


### [164] [A new Dune grid for scalable dynamic adaptivity based on the p4est software library](https://arxiv.org/abs/2507.11386)
*Carsten Burstedde,Mikhail Kirilin,Robert Klöfkorn*

Main category: cs.DC

TL;DR: The paper integrates the Dune solver library with p4est software to enhance MPI scalability and optimize data structures for better performance in 2D and 3D simulations.


<details>
  <summary>Details</summary>
Motivation: The authors seek to leverage p4est's advantages, including unlimited MPI scalability, thin data structures, and support for multi-block mesh topologies to improve finite element computations.

Method: The authors develop a new grid interface within Dune that couples with p4est, implementing 2:1 balancing and comparing its performance against Dune-ALUGrid through parallel experiments.

Result: The paper demonstrates superior scalability of the p4est implementation over Dune-ALUGrid, alongside improved balancing strategy performance.

Conclusion: Coupling Dune with p4est enhances computation scalability and mesh balancing, showcasing p4est's advantages in numerical experiments for finite element analysis.

Abstract: In this work we extend the Dune solver library with another grid interface to
the open-source p4est software. While Dune already supports about a dozen
different mesh implementations through its mesh interface Dune-Grid, we
undertake this new coupling effort in order to inherit p4est's practically
unlimited MPI scalability as well as its relatively thin data structures, and
its native support for multi-block (forest) mesh topologies in both 2D and 3D.
  The presented implementation is compared to an existing implementation based
on Dune-ALUGrid for a variety of challenging test examples in a parallel
environment. The numerical experiments show that the implementation presented
here is outperforming Dune-ALUGrid in terms of scalability. In addition, an
alternative balancing strategy is presented to ensure 2:1 balancing across
element faces showing improved performance compared to the existing p4est
balance strategy in the numerical examples considered in this work.

</details>


### [165] [Quantifying the Energy Consumption and Carbon Emissions of LLM Inference via Simulations](https://arxiv.org/abs/2507.11417)
*Miray Özcan,Philipp Wiesner,Philipp Weiß,Odej Kao*

Main category: cs.DC

TL;DR: The paper introduces a simulation framework to evaluate energy and carbon emissions of LLM inference, incorporating a GPU power model and grid-specific conditions.


<details>
  <summary>Details</summary>
Motivation: To address the rising environmental impact of LLMs, particularly inference-related carbon emissions, and the lack of accurate estimation methods in existing simulation frameworks.

Method: The framework combines a GPU power model for high-fidelity LLM simulations with an energy system co-simulation for carbon emission quantification under specific grid conditions.

Result: The study shows how LLM inference parameters influence energy and carbon usage, achieving a renewable offset potential of up to 69.2% in a sample deployment scenario.

Conclusion: The framework lays groundwork for carbon-aware deployment strategies and infrastructure designs to reduce the environmental impact of LLMs.

Abstract: The environmental impact of Large Language Models (LLMs) is rising
significantly, with inference now accounting for more than half of their total
lifecycle carbon emissions. However, existing simulation frameworks, which are
increasingly used to determine efficient LLM deployments, lack any concept of
power and, therefore, cannot accurately estimate inference-related emissions.
We present a simulation framework to assess the energy and carbon implications
of LLM inference under varying deployment setups. First, we extend a
high-fidelity LLM inference simulator with a GPU power model that estimates
power consumption based on utilization metrics, enabling analysis across
configurations like batch size, sequence length, and model parallelism. Second,
we integrate simulation outputs into an energy system co-simulation environment
to quantify carbon emissions under specific grid conditions and explore the
potential of carbon-aware scheduling. Through scenario-based analysis, our
framework reveals how inference parameters affect energy demand and carbon
footprint, demonstrates a renewable offset potential of up to 69.2% in an
illustrative deployment case, and provides a foundation for future carbon-aware
inference infrastructure design.

</details>


### [166] [FLsim: A Modular and Library-Agnostic Simulation Framework for Federated Learning](https://arxiv.org/abs/2507.11430)
*Arnab Mukherjee,Raju Halder,Joydeep Chandra*

Main category: cs.DC

TL;DR: The paper introduces FLsim, a modular and scalable simulation framework for federated learning (FL), enabling customized and reproducible FL experiments.


<details>
  <summary>Details</summary>
Motivation: The lack of a comprehensive and flexible framework to compare and simulate diverse FL techniques and benchmarks creates challenges for researchers.

Method: Developed FLsim, featuring modularity, scalability, resource efficiency, and controlled reproducibility. It provides options for data distributions, learning algorithms, communication topologies, aggregation methods, and blockchain support.

Result: Experimental evaluations demonstrate FLsim's capability to effectively simulate diverse state-of-the-art FL workflows and methodologies.

Conclusion: FLsim offers a versatile and user-friendly platform, potentially advancing FL research by providing enhanced flexibility and functionality to researchers.

Abstract: Federated Learning (FL) has undergone significant development since its
inception in 2016, advancing from basic algorithms to complex methodologies
tailored to address diverse challenges and use cases. However, research and
benchmarking of novel FL techniques against a plethora of established
state-of-the-art solutions remain challenging. To streamline this process, we
introduce FLsim, a comprehensive FL simulation framework designed to meet the
diverse requirements of FL workflows in the literature. FLsim is characterized
by its modularity, scalability, resource efficiency, and controlled
reproducibility of experimental outcomes. Its easy to use interface allows
users to specify customized FL requirements through job configuration, which
supports: (a) customized data distributions, ranging from non-independent and
identically distributed (non-iid) data to independent and identically
distributed (iid) data, (b) selection of local learning algorithms according to
user preferences, with complete agnosticism to ML libraries, (c) choice of
network topology illustrating communication patterns among nodes, (d)
definition of model aggregation and consensus algorithms, and (e) pluggable
blockchain support for enhanced robustness. Through a series of experimental
evaluations, we demonstrate the effectiveness and versatility of FLsim in
simulating a diverse range of state-of-the-art FL experiments. We envisage that
FLsim would mark a significant advancement in FL simulation frameworks,
offering unprecedented flexibility and functionality for researchers and
practitioners alike.

</details>


### [167] [Uniting the World by Dividing it: Federated Maps to Enable Spatial Applications](https://arxiv.org/abs/2507.11437)
*Sagar Bharadwaj,Srinivasan Seshan,Anthony Rowe*

Main category: cs.DC

TL;DR: This paper addresses the need for a federated spatial naming system to support applications like augmented reality and navigation, especially considering existing centralized systems' insufficiency for detailed indoor and outdoor mapping.


<details>
  <summary>Details</summary>
Motivation: The Spatial Web requires a robust naming system for real-world locations to enhance applications like augmented reality and navigation, but current systems are centralized and limited in scope.

Method: The authors propose a federated spatial naming system, which decentralizes map management and operations to overcome scalability and privacy challenges.

Result: The paper highlights the feasibility of enabling essential map-related services, such as address-location mapping and routing, within a federated framework, overcoming existing limitations.

Conclusion: A federated mapping infrastructure has the potential to scale map management and enhance user privacy, enabling advanced applications in the Spatial Web ecosystem.

Abstract: The emergence of the Spatial Web -- the Web where content is tied to
real-world locations has the potential to improve and enable many applications
such as augmented reality, navigation, robotics, and more. The Spatial Web is
missing a key ingredient that is impeding its growth -- a spatial naming system
to resolve real-world locations to names. Today's spatial naming systems are
digital maps such as Google and Apple maps. These maps and the location-based
services provided on top of these maps are primarily controlled by a few large
corporations and mostly cover outdoor public spaces. Emerging classes of
applications, such as persistent world-scale augmented reality, require
detailed maps of both outdoor and indoor spaces. Existing centralized mapping
infrastructures are proving insufficient for such applications because of the
scale of cartography efforts required and the privacy of indoor map data.
  In this paper, we present a case for a federated spatial naming system, or in
other words, a federated mapping infrastructure. This enables disparate parties
to manage and serve their own maps of physical regions and unlocks scalability
of map management, isolation and privacy of maps. Map-related services such as
address-to-location mapping, location-based search, and routing needs
re-architecting to work on federated maps. We discuss some essential services
and practicalities of enabling these services.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [168] [MH-FSF: A Unified Framework for Overcoming Benchmarking and Reproducibility Limitations in Feature Selection Evaluation](https://arxiv.org/abs/2507.10591)
*Vanderson Rocha,Diego Kreutz,Gabriel Canto,Hendrio Bragança,Eduardo Feitosa*

Main category: cs.LG

TL;DR: The paper introduces MH-FSF, a unified framework for evaluating feature selection techniques, with a focus on Android malware detection.


<details>
  <summary>Details</summary>
Motivation: Current feature selection methods are hindered by limited benchmarking and reliance on non-reproducible proprietary datasets, affecting both performance and research consistency.

Method: The MH-FSF framework was developed, featuring implementations of 17 feature selection methods and evaluated systematically across 10 open Android malware datasets.

Result: Results showed performance variations based on dataset balance, emphasizing the need for preprocessing and selection criteria sensitive to data asymmetries.

Conclusion: MH-FSF is presented as a critical tool for fostering methodological rigor and enabling reproducibility in feature selection research, especially in Android malware detection.

Abstract: Feature selection is vital for building effective predictive models, as it
reduces dimensionality and emphasizes key features. However, current research
often suffers from limited benchmarking and reliance on proprietary datasets.
This severely hinders reproducibility and can negatively impact overall
performance. To address these limitations, we introduce the MH-FSF framework, a
comprehensive, modular, and extensible platform designed to facilitate the
reproduction and implementation of feature selection methods. Developed through
collaborative research, MH-FSF provides implementations of 17 methods (11
classical, 6 domain-specific) and enables systematic evaluation on 10 publicly
available Android malware datasets. Our results reveal performance variations
across both balanced and imbalanced datasets, highlighting the critical need
for data preprocessing and selection criteria that account for these
asymmetries. We demonstrate the importance of a unified platform for comparing
diverse feature selection techniques, fostering methodological consistency and
rigor. By providing this framework, we aim to significantly broaden the
existing literature and pave the way for new research directions in feature
selection, particularly within the context of Android malware detection.

</details>


### [169] [Tool-to-Tool Matching Analysis Based Difference Score Computation Methods for Semiconductor Manufacturing](https://arxiv.org/abs/2507.10564)
*Sameera Bharadwaja H.,Siddhrath Jandial,Shashank S. Agashe,Rajesh Kumar Reddy Moore,Youngkwan Kim*

Main category: cs.LG

TL;DR: This paper addresses tool-to-tool matching (TTTM) challenges in semiconductor manufacturing with novel pipelines, demonstrating effectiveness through statistical correlation analysis.


<details>
  <summary>Details</summary>
Motivation: Traditional TTTM methods rely on static data or golden references, which are hard to obtain in commercial manufacturing, particularly in heterogeneous tool settings.

Method: Proposed pipelines analyze variations in data (using metrics like variance and modes) and evaluate univariate and multivariate approaches, along with hyper-parameter sensitivity.

Result: Univariate methods achieved correlation coefficients >0.95 (variance) and >0.5 (modes); multivariate methods showed >0.75 correlation with top-performing univariate approaches.

Conclusion: The proposed methods effectively address limitations of conventional TTTM techniques, making them practical for diverse, real-world environments.

Abstract: We consider the problem of tool-to-tool matching (TTTM), also called, chamber
matching in the context of a semiconductor manufacturing equipment. Traditional
TTTM approaches utilize static configuration data or depend on a golden
reference which are difficult to obtain in a commercial manufacturing line.
Further, existing methods do not extend very well to a heterogeneous setting,
where equipment are of different make-and-model, sourced from different
equipment vendors. We propose novel TTTM analysis pipelines to overcome these
issues. We hypothesize that a mismatched equipment would have higher variance
and/or higher number of modes in the data. Our best univariate method achieves
a correlation coefficient >0.95 and >0.5 with the variance and number of modes,
respectively showing that the proposed methods are effective. Also, the best
multivariate method achieves a correlation coefficient >0.75 with the
top-performing univariate methods, showing its effectiveness. Finally, we
analyze the sensitivity of the multivariate algorithms to the algorithm
hyper-parameters.

</details>


### [170] [Enhancing Cross Entropy with a Linearly Adaptive Loss Function for Optimized Classification Performance](https://arxiv.org/abs/2507.10574)
*Jae Wan Shim*

Main category: cs.LG

TL;DR: Introduces a novel Linearly Adaptive Cross Entropy Loss function demonstrating improved accuracy over standard cross entropy in classification tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance optimization in classification tasks and improve accuracy using one-hot encoded class labels.

Method: Incorporating an additional term dependent on the predicted probability of the true class into the traditional cross entropy loss.

Result: Evaluation using ResNet on CIFAR-100 shows consistently better classification accuracy compared to standard cross entropy.

Conclusion: The proposed loss function offers higher accuracy while maintaining computational efficiency, opening avenues for further research in loss function design.

Abstract: We propose the Linearly Adaptive Cross Entropy Loss function. This is a novel
measure derived from the information theory. In comparison to the standard
cross entropy loss function, the proposed one has an additional term that
depends on the predicted probability of the true class. This feature serves to
enhance the optimization process in classification tasks involving one-hot
encoded class labels. The proposed one has been evaluated on a ResNet-based
model using the CIFAR-100 dataset. Preliminary results show that the proposed
one consistently outperforms the standard cross entropy loss function in terms
of classification accuracy. Moreover, the proposed one maintains simplicity,
achieving practically the same efficiency to the traditional cross entropy
loss. These findings suggest that our approach could broaden the scope for
future research into loss function design.

</details>


### [171] [An Adaptive Volatility-based Learning Rate Scheduler](https://arxiv.org/abs/2507.10575)
*Kieran Chai Kai Ren*

Main category: cs.LG

TL;DR: VolSched dynamically adjusts learning rates based on accuracy volatility, improving top-1 accuracy on CIFAR-100 and finding wider minima for better generalization.


<details>
  <summary>Details</summary>
Motivation: Popular predefined and adaptive learning rate schedulers often lead to suboptimal generalization, necessitating an improved method for effectively training deep neural networks.

Method: VolSched calculates the ratio between long-term and short-term accuracy volatility to adaptively increase or decrease learning rates, allowing better exploration and stabilization during training.

Result: On CIFAR-100 with ResNet-18 and ResNet-34, VolSched improves top-1 accuracy by 1.4 and 1.3 percentage points respectively, while yielding a flatter solution by 38% compared to existing baselines.

Conclusion: VolSched enhances exploration and generalization by adapting learning rates using volatility concepts, achieving wider minima and consistent performance improvements.

Abstract: Effective learning rate (LR) scheduling is crucial for training deep neural
networks. However, popular pre-defined and adaptive schedulers can still lead
to suboptimal generalization. This paper introduces VolSched, a novel adaptive
LR scheduler inspired by the concept of volatility in stochastic processes like
Geometric Brownian Motion to dynamically adjust the learning rate. By
calculating the ratio between long-term and short-term accuracy volatility,
VolSched increases the LR to escape plateaus and decreases it to stabilize
training, allowing the model to explore the loss landscape more effectively. We
evaluate VolSched on the CIFAR-100 dataset against a strong baseline using a
standard augmentation pipeline. When paired with ResNet-18 and ResNet-34, our
scheduler delivers consistent performance gains, improving top-1 accuracy by
1.4 and 1.3 percentage points respectively. Analysis of the loss curves reveals
that VolSched promotes a longer exploration phase. A quantitative analysis of
the Hessian shows that VolSched finds a final solution that is 38% flatter than
the next-best baseline, allowing the model to obtain wider minima and hence
better generalization performance.

</details>


### [172] [A Group Theoretic Analysis of the Symmetries Underlying Base Addition and Their Learnability by Neural Networks](https://arxiv.org/abs/2507.10678)
*Cutter Dawes,Simon Segert,Kamesh Krishnamurthy,Jonathan D. Cohen*

Main category: cs.LG

TL;DR: This paper explores how neural networks can use symmetry, specifically through base addition and carry functions, to achieve radical generalization efficiently.


<details>
  <summary>Details</summary>
Motivation: The study aims to address challenges in neural networks for both cognitive modeling and artificial intelligence, particularly focusing on their ability to generalize well by learning symmetry functions.

Method: The authors perform a group-theoretic analysis of base addition, explore alternative carry functions, and train neural networks to learn base addition with different carry structures to measure learning efficacy and speed.

Result: Simple neural networks are shown to achieve radical generalization, given the right input format and carry function, with learning speed depending strongly on the structure of the carry function.

Conclusion: The study highlights neural networks' ability to achieve efficient generalization through structural biases and symmetry, providing insights relevant to cognitive science and machine learning.

Abstract: A major challenge in the use of neural networks both for modeling human
cognitive function and for artificial intelligence is the design of systems
with the capacity to efficiently learn functions that support radical
generalization. At the roots of this is the capacity to discover and implement
symmetry functions. In this paper, we investigate a paradigmatic example of
radical generalization through the use of symmetry: base addition. We present a
group theoretic analysis of base addition, a fundamental and defining
characteristic of which is the carry function -- the transfer of the remainder,
when a sum exceeds the base modulus, to the next significant place. Our
analysis exposes a range of alternative carry functions for a given base, and
we introduce quantitative measures to characterize these. We then exploit
differences in carry functions to probe the inductive biases of neural networks
in symmetry learning, by training neural networks to carry out base addition
using different carries, and comparing efficacy and rate of learning as a
function of their structure. We find that even simple neural networks can
achieve radical generalization with the right input format and carry function,
and that learning speed is closely correlated with carry function structure. We
then discuss the relevance this has for cognitive science and machine learning.

</details>


### [173] [Universal Approximation Theorem for a Single-Layer Transformer](https://arxiv.org/abs/2507.10581)
*Esmail Gumaan*

Main category: cs.LG

TL;DR: This paper proves that single-layer Transformers can approximate any continuous sequence-to-sequence mapping, advancing theoretical knowledge of these models.


<details>
  <summary>Details</summary>
Motivation: The lack of theoretical understanding of deep learning and Transformer models motivates the authors to explore the mathematical foundations and demonstrate their universal approximation capabilities.

Method: The paper uses theoretical analysis, incorporating concepts from linear algebra, probability, and optimization, to investigate self-attention mechanisms and backpropagation. A universal approximation theorem for Transformers is formally stated and proven.

Result: The study establishes that a single-layer Transformer with a self-attention mechanism and a position-wise feed-forward network (ReLU activation) can approximate any continuous sequence-to-sequence mapping on a compact domain.

Conclusion: This work enhances theoretical understanding of Transformers, offering a bridge between their empirical performance and theoretical guarantees. Practical implications of this advancement are demonstrated through case studies.

Abstract: Deep learning employs multi-layer neural networks trained via the
backpropagation algorithm. This approach has achieved success across many
domains and relies on adaptive gradient methods such as the Adam optimizer.
Sequence modeling evolved from recurrent neural networks to attention-based
models, culminating in the Transformer architecture. Transformers have achieved
state-of-the-art performance in natural language processing (for example, BERT
and GPT-3) and have been applied in computer vision and computational biology.
However, theoretical understanding of these models remains limited. In this
paper, we examine the mathematical foundations of deep learning and
Transformers and present a novel theoretical result. We review key concepts
from linear algebra, probability, and optimization that underpin deep learning,
and we analyze the multi-head self-attention mechanism and the backpropagation
algorithm in detail. Our main contribution is a universal approximation theorem
for Transformers: we prove that a single-layer Transformer, comprising one
self-attention layer followed by a position-wise feed-forward network with ReLU
activation, can approximate any continuous sequence-to-sequence mapping on a
compact domain to arbitrary precision. We provide a formal statement and a
complete proof. Finally, we present case studies that demonstrate the practical
implications of this result. Our findings advance the theoretical understanding
of Transformer models and help bridge the gap between theory and practice.

</details>


### [174] [Langevin Flows for Modeling Neural Latent Dynamics](https://arxiv.org/abs/2507.11531)
*Yue Song,T. Anderson Keller,Yisong Yue,Pietro Perona,Max Welling*

Main category: cs.LG

TL;DR: LangevinFlow, a new sequential Variational Auto-Encoder, models neural dynamics using physics-inspired priors based on the Langevin equation to achieve state-of-the-art performance in modeling and predicting neural population behaviors.


<details>
  <summary>Details</summary>
Motivation: Neural populations exhibit latent structures that guide their spiking activity, which necessitates models that can represent intrinsic dynamics and external unobserved factors.

Method: The authors developed LangevinFlow, combining physical priors (inertia, damping, potential functions, and stochastic forces) with machine learning techniques (recurrent encoders, Transformer decoders, and latent Langevin dynamics). It emphasizes oscillatory and flow-like behaviors.

Result: LangevinFlow outperformed baselines on synthetic and real datasets from the Neural Latents Benchmark (NLB), showing better neuron likelihoods, prediction accuracy, and behavioral decoding metrics.

Conclusion: The proposed method provides a robust physics-inspired framework for modeling complex neural dynamics, offering superior performance in capturing both network and external influences.

Abstract: Neural populations exhibit latent dynamical structures that drive
time-evolving spiking activities, motivating the search for models that capture
both intrinsic network dynamics and external unobserved influences. In this
work, we introduce LangevinFlow, a sequential Variational Auto-Encoder where
the time evolution of latent variables is governed by the underdamped Langevin
equation. Our approach incorporates physical priors -- such as inertia,
damping, a learned potential function, and stochastic forces -- to represent
both autonomous and non-autonomous processes in neural systems. Crucially, the
potential function is parameterized as a network of locally coupled
oscillators, biasing the model toward oscillatory and flow-like behaviors
observed in biological neural populations. Our model features a recurrent
encoder, a one-layer Transformer decoder, and Langevin dynamics in the latent
space. Empirically, our method outperforms state-of-the-art baselines on
synthetic neural populations generated by a Lorenz attractor, closely matching
ground-truth firing rates. On the Neural Latents Benchmark (NLB), the model
achieves superior held-out neuron likelihoods (bits per spike) and forward
prediction accuracy across four challenging datasets. It also matches or
surpasses alternative methods in decoding behavioral metrics such as hand
velocity. Overall, this work introduces a flexible, physics-inspired,
high-performing framework for modeling complex neural population dynamics and
their unobserved influences.

</details>


### [175] [Extension OL-MDISF: Online Learning from Mix-Typed, Drifted, and Incomplete Streaming Features](https://arxiv.org/abs/2507.10594)
*Shengda Zhuo,Di Wu,Yi He,Shuqiang Huang,Xindong Wu*

Main category: cs.LG

TL;DR: This paper addresses the challenges in online learning with dynamic, heterogeneous, and incomplete data streams by proposing the OL-MDISF framework, which employs latent copula-based representations, drift detection, and structure-aware pseudo-labeling.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome critical challenges in online learning due to changing feature spaces, including data heterogeneity, temporal drift, and labeling constraints.

Method: OL-MDISF uses latent copula-based representations for handling mixed-type features, employs ensemble entropy and latent mismatch for detecting drifts, and integrates structure-aware pseudo-labeling to manage incomplete supervision.

Result: The framework was evaluated on 14 real-world datasets across various drift scenarios, with analyses including ablation studies, sensitivity tests, and temporal ensemble effects.

Conclusion: OL-MDISF serves as a reproducible benchmark for tackling complex and weakly supervised online learning settings, with promising experimental outcomes.

Abstract: Online learning, where feature spaces can change over time, offers a flexible
learning paradigm that has attracted considerable attention. However, it still
faces three significant challenges. First, the heterogeneity of real-world data
streams with mixed feature types presents challenges for traditional parametric
modeling. Second, data stream distributions can shift over time, causing an
abrupt and substantial decline in model performance. Third, it is often
infeasible to label every data instance due to time and cost constraints. To
address these issues, we proposed OL-MDISF (Online Learning from Mix-typed,
Drifted, and Incomplete Streaming Features), which constructs a latent
copula-based representation for heterogeneous features, detects drifts via
ensemble entropy and latent mismatch, and performs structure-aware
pseudo-labeling.
  This companion paper serves as a standalone technical reference to OL-MDISF.
It provides a contextual discussion of related work in mixed-type modeling,
drift adaptation, and weak supervision, as well as a comprehensive set of
experiments across 14 real-world datasets under two types of drift scenarios.
These include CER trends, ablation studies, sensitivity analyses, and temporal
ensemble dynamics. We hope this document offers a reproducible benchmark for
online learning on complex, weakly supervised streaming data.

</details>


### [176] [Divide-Then-Rule: A Cluster-Driven Hierarchical Interpolator for Attribute-Missing Graphs](https://arxiv.org/abs/2507.10595)
*Yaowen Hu,Wenxuan Tu,Yue Liu,Miaomiao Li,Wenpeng Lu,Zhigang Luo,Xinwang Liu,Ping Chen*

Main category: cs.LG

TL;DR: The paper introduces DTRGC, a method improving deep graph clustering (DGC) under missing attributes by combining hierarchical imputation and clustering correction.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing imputation methods for attribute-missing graphs, especially the unreliable outcomes for nodes with insufficient neighborhood information.

Method: DTRGC leverages a three-stage approach: 1) Dynamic Cluster-Aware Feature Propagation (DCFP) to initialize missing attributes, 2) Hierarchical Neighborhood-aware Imputation (HNAI) for iterative and prioritized imputation, and 3) Hop-wise Representation Enhancement (HRE) to integrate multi-hop information.

Result: Experimental results across six widely used graph datasets demonstrate that DTRGC substantially improves clustering performance of DGC methods when node attributes are incomplete.

Conclusion: The proposed DTRGC approach effectively addresses the challenges of attribute-missing graphs, improving reliability and clustering outcomes through clustering-based imputation refinements.

Abstract: Deep graph clustering (DGC) for attribute-missing graphs is an unsupervised
task aimed at partitioning nodes with incomplete attributes into distinct
clusters. Addressing this challenging issue is vital for practical
applications. However, research in this area remains underexplored. Existing
imputation methods for attribute-missing graphs often fail to account for the
varying amounts of information available across node neighborhoods, leading to
unreliable results, especially for nodes with insufficient known neighborhood.
To address this issue, we propose a novel method named Divide-Then-Rule Graph
Completion (DTRGC). This method first addresses nodes with sufficient known
neighborhood information and treats the imputed results as new knowledge to
iteratively impute more challenging nodes, while leveraging clustering
information to correct imputation errors. Specifically, Dynamic Cluster-Aware
Feature Propagation (DCFP) initializes missing node attributes by adjusting
propagation weights based on the clustering structure. Subsequently,
Hierarchical Neighborhood-aware Imputation (HNAI) categorizes attribute-missing
nodes into three groups based on the completeness of their neighborhood
attributes. The imputation is performed hierarchically, prioritizing the groups
with nodes that have the most available neighborhood information. The cluster
structure is then used to refine the imputation and correct potential errors.
Finally, Hop-wise Representation Enhancement (HRE) integrates information
across multiple hops, thereby enriching the expressiveness of node
representations. Experimental results on six widely used graph datasets show
that DTRGC significantly improves the clustering performance of various DGC
methods under attribute-missing graphs.

</details>


### [177] [RedOne: Revealing Domain-specific LLM Post-Training in Social Networking Services](https://arxiv.org/abs/2507.10605)
*Fei Zhao,Chonggang Lu,Yue Wang,Zheyong Xie,Ziyan Liu,Haofu Qian,JianZhao Huang,Fangcheng Shi,Zijie Meng,Hongcheng Guo,Mingqian He,Xinze Lyu,Yiming Lu,Ziyang Xiang,Zheyu Ye,Chengqiang Lu,Zhe Xu,Yi Wu,Yao Hu,Yan Gao,Jun Fan,Xiaolong Jiang,Weiting Liu,Boyang Wang,Shaosheng Cao*

Main category: cs.LG

TL;DR: The paper introduces RedOne, a domain-focused large language model (LLM) for social networking services (SNS), which addresses limitations of single-task models and demonstrates significant improvements across multiple SNS tasks.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of SNS has led to challenges in content management and interaction quality. Existing large language models focusing on isolated tasks are inefficient and lack flexibility in adapting to diverse real-world contexts.

Method: The authors developed RedOne using a three-stage training strategy: (1) continue pretraining, (2) supervised fine-tuning, and (3) preference optimization, leveraging a large-scale real-world dataset.

Result: RedOne achieved an average performance improvement of up to 14.02% across 8 SNS tasks and 7.56% in bilingual evaluations. In online tests, it reduced harmful content exposure by 11.23% and improved engagement rates in post-view searches by 14.95%, compared to single-task finetuned models.

Conclusion: RedOne demonstrates strong general capabilities, superior task performance, and enhanced applicability in real-world SNS scenarios, effectively addressing multi-task challenges and setting a solid foundation for SNS applications.

Abstract: As a primary medium for modern information dissemination, social networking
services (SNS) have experienced rapid growth, which has proposed significant
challenges for platform content management and interaction quality improvement.
Recently, the development of large language models (LLMs) has offered potential
solutions but existing studies focus on isolated tasks, which not only
encounter diminishing benefit from the data scaling within individual scenarios
but also fail to flexibly adapt to diverse real-world context. To address these
challenges, we introduce RedOne, a domain-specific LLM designed to break the
performance bottleneck of single-task baselines and establish a comprehensive
foundation for the SNS. RedOne was developed through a three-stage training
strategy consisting of continue pretraining, supervised fine-tuning, and
preference optimization, using a large-scale real-world dataset. Through
extensive experiments, RedOne maintains strong general capabilities, and
achieves an average improvement up to 14.02% across 8 major SNS tasks and 7.56%
in SNS bilingual evaluation benchmark, compared with base models. Furthermore,
through online testing, RedOne reduced the exposure rate in harmful content
detection by 11.23% and improved the click page rate in post-view search by
14.95% compared with single-tasks finetuned baseline models. These results
establish RedOne as a robust domain-specific LLM for SNS, demonstrating
excellent generalization across various tasks and promising applicability in
real-world scenarios.

</details>


### [178] [DALI-PD: Diffusion-based Synthetic Layout Heatmap Generation for ML in Physical Design](https://arxiv.org/abs/2507.10606)
*Bing-Yue Wu,Vidya A. Chhabria*

Main category: cs.LG

TL;DR: DALI-PD, a framework using a diffusion model, generates synthetic layout heatmaps for physical design, accelerating ML applications.


<details>
  <summary>Details</summary>
Motivation: Limited availability of large-scale, high-quality datasets constrains the generalizability of ML models in physical design.

Method: DALI-PD employs a diffusion model to quickly generate synthetic layout heatmaps, producing data efficiently and resembling real-world layouts.

Result: Generated a dataset of over 20,000 layout heatmaps for tasks like IR drop and congestion prediction, enhancing ML model accuracy.

Conclusion: DALI-PD provides a scalable, efficient alternative for dataset generation in physical design, expediting ML research and model development.

Abstract: Machine learning (ML) has demonstrated significant promise in various
physical design (PD) tasks. However, model generalizability remains limited by
the availability of high-quality, large-scale training datasets. Creating such
datasets is often computationally expensive and constrained by IP. While very
few public datasets are available, they are typically static, slow to generate,
and require frequent updates. To address these limitations, we present DALI-PD,
a scalable framework for generating synthetic layout heatmaps to accelerate ML
in PD research. DALI-PD uses a diffusion model to generate diverse layout
heatmaps via fast inference in seconds. The heatmaps include power, IR drop,
congestion, macro placement, and cell density maps. Using DALI-PD, we created a
dataset comprising over 20,000 layout configurations with varying macro counts
and placements. These heatmaps closely resemble real layouts and improve ML
accuracy on downstream ML tasks such as IR drop or congestion prediction.

</details>


### [179] [GeoHopNet: Hopfield-Augmented Sparse Spatial Attention for Dynamic UAV Site Location Problem](https://arxiv.org/abs/2507.10636)
*Jianing Zhi,Xinghua Li,Zidong Chen*

Main category: cs.LG

TL;DR: GeoHopNet integrates Hopfield networks and sparse spatial attention mechanisms for large-scale dynamic UAV site selection, achieving superior efficiency and solution quality compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The paper addresses difficulties in computational efficiency and scalability faced by traditional methods for dynamic UAV landing points and supply station selection in urban areas.

Method: GeoHopNet uses four key innovations: distance-biased spatial attention, K-nearest neighbor sparse attention, a Hopfield external memory module, and a memory regularization strategy.

Result: GeoHopNet delivers high-quality solutions (0.22% optimality gap) efficiently, solving 1,000-node instances in under 0.1 seconds while outperforming ADNet in quality by 22.2% and speed by 1.8x on 100-node problems.

Conclusion: GeoHopNet pushes the boundaries of solvable UAV site selection problems, offering scalable and computationally efficient solutions for urban-level challenges.

Abstract: The rapid development of urban low-altitude unmanned aerial vehicle (UAV)
economy poses new challenges for dynamic site selection of UAV landing points
and supply stations. Traditional deep reinforcement learning methods face
computational complexity bottlenecks, particularly with standard attention
mechanisms, when handling large-scale urban-level location problems. This paper
proposes GeoHopNet, a Hopfield-augmented sparse spatial attention network
specifically designed for dynamic UAV site location problems. Our approach
introduces four core innovations: (1) distance-biased multi-head attention
mechanism that explicitly encodes spatial geometric information; (2) K-nearest
neighbor sparse attention that reduces computational complexity from $O(N^2)$
to $O(NK)$; (3) a modern Hopfield external memory module; and (4) a memory
regularization strategy. Experimental results demonstrate that GeoHopNet
extends the boundary of solvable problem sizes. For large-scale instances with
1,000 nodes, where standard attention models become prohibitively slow (over 3
seconds per instance) and traditional solvers fail, GeoHopNet finds
high-quality solutions (0.22\% optimality gap) in under 0.1 seconds. Compared
to the state-of-the-art ADNet baseline on 100-node instances, our method
improves solution quality by 22.2\% and is 1.8$\times$ faster.

</details>


### [180] [A Feed-Forward Artificial Intelligence Pipeline for Sustainable Desalination under Climate Uncertainties: UAE Insights](https://arxiv.org/abs/2507.10609)
*Obumneme Nwafor,Chioma Nwafor,Amro Zakaria,Nkechi Nwankwo*

Main category: cs.LG

TL;DR: The study focuses on improving water desalination in the UAE, addressing energy intensity, CO2 emissions, and challenges from climate uncertainties like AOD. A novel predictive model and dust-aware control system were developed to optimize operations.


<details>
  <summary>Details</summary>
Motivation: The UAE depends on energy-intensive desalination for over 90% of its drinking water needs, leading to considerable electricity use and CO2 emissions. Climate-related factors like AOD further challenge system performance and sustainability.

Method: A two-stage predictive modeling architecture was implemented: Stage 1 forecasts AOD using satellite and meteorological time series data, while Stage 2 uses AOD predictions and meteorological data to assess desalination efficiency. A dust-aware control logic for system operations was introduced based on predictions.

Result: The modeling framework achieved 98% accuracy, and SHAP analysis identified key factors affecting degradation. The system's control logic allows for adaptable management of desalination operations considering climate uncertainties.

Conclusion: The paper presents a novel predictive modeling and decision-support framework that enhances the efficiency, sustainability, and adaptability of desalination in the UAE under changing climate conditions.

Abstract: The United Arab Emirates (UAE) relies heavily on seawater desalination to
meet over 90% of its drinking water needs. Desalination processes are highly
energy intensive and account for approximately 15% of the UAE's electricity
consumption, contributing to over 22% of the country's energy-related CO2
emissions. Moreover, these processes face significant sustainability challenges
in the face of climate uncertainties such as rising seawater temperatures,
salinity, and aerosol optical depth (AOD). AOD greatly affects the operational
and economic performance of solar-powered desalination systems through
photovoltaic soiling, membrane fouling, and water turbidity cycles.
  This study proposes a novel pipelined two-stage predictive modelling
architecture: the first stage forecasts AOD using satellite-derived time series
and meteorological data; the second stage uses the predicted AOD and other
meteorological factors to predict desalination performance efficiency losses.
The framework achieved 98% accuracy, and SHAP (SHapley Additive exPlanations)
was used to reveal key drivers of system degradation. Furthermore, this study
proposes a dust-aware rule-based control logic for desalination systems based
on predicted values of AOD and solar efficiency. This control logic is used to
adjust the desalination plant feed water pressure, adapt maintenance
scheduling, and regulate energy source switching.
  To enhance the practical utility of the research findings, the predictive
models and rule-based controls were packaged into an interactive dashboard for
scenario and predictive analytics. This provides a management decision-support
system for climate-adaptive planning.

</details>


### [181] [FedGSCA: Medical Federated Learning with Global Sample Selector and Client Adaptive Adjuster under Label Noise](https://arxiv.org/abs/2507.10611)
*Mengwen Ye,Yingzi Huangfu,Shujian Gao,Wei Ren,Weifan Liu,Zekuan Yu*

Main category: cs.LG

TL;DR: This paper introduces FedGSCA, enhancing federated learning robustness for medical image classification with noisy data.


<details>
  <summary>Details</summary>
Motivation: Address challenges of label noise, noise heterogeneity, and data imbalance in medical federated learning.

Method: FedGSCA framework combines a Global Sample Selector for aggregated noise knowledge and a Client Adaptive Adjustment (CAA) mechanism using adaptive pseudo-labels and Robust Credal Labeling Loss.

Result: FedGSCA outperforms existing methods in handling noise across real and synthetic datasets, excelling in extreme and heterogeneous conditions.

Conclusion: FedGSCA improves global model stability and performance, making it suitable for real-world medical federated learning applications under complex noise conditions.

Abstract: Federated Learning (FL) emerged as a solution for collaborative medical image
classification while preserving data privacy. However, label noise, which
arises from inter-institutional data variability, can cause training
instability and degrade model performance. Existing FL methods struggle with
noise heterogeneity and the imbalance in medical data. Motivated by these
challenges, we propose FedGSCA, a novel framework for enhancing robustness in
noisy medical FL. FedGSCA introduces a Global Sample Selector that aggregates
noise knowledge from all clients, effectively addressing noise heterogeneity
and improving global model stability. Furthermore, we develop a Client Adaptive
Adjustment (CAA) mechanism that combines adaptive threshold pseudo-label
generation and Robust Credal Labeling Loss. CAA dynamically adjusts to class
distributions, ensuring the inclusion of minority samples and carefully
managing noisy labels by considering multiple plausible labels. This dual
approach mitigates the impact of noisy data and prevents overfitting during
local training, which improves the generalizability of the model. We evaluate
FedGSCA on one real-world colon slides dataset and two synthetic medical
datasets under various noise conditions, including symmetric, asymmetric,
extreme, and heterogeneous types. The results show that FedGSCA outperforms the
state-of-the-art methods, excelling in extreme and heterogeneous noise
scenarios. Moreover, FedGSCA demonstrates significant advantages in improving
model stability and handling complex noise, making it well-suited for
real-world medical federated learning scenarios.

</details>


### [182] [Sub-Scaling Laws: On the Role of Data Density and Training Strategies in LLMs](https://arxiv.org/abs/2507.10613)
*Zhengyu Chen,Siqi Wang,Teng Xiao,Yudong Wang,Shiqi Chen,Xunliang Cai,Junxian He,Jingang Wang*

Main category: cs.LG

TL;DR: The paper explores deviations from traditional scaling laws in NLP, focusing on the effects of data quality and training strategies in large language models. It introduces a sub-optimal scaling law.


<details>
  <summary>Details</summary>
Motivation: Traditional scaling laws in NLP, emphasizing larger models and more data for improved performance, show deviations in certain scenarios, inciting deeper investigation into sub-scaling phenomena.

Method: The study utilized empirical analysis of over 400 models to examine data density and resource allocation, identifying key dynamics affecting performance.

Result: High data density causes diminishing returns, while optimal resource allocation is critical to maintain performance trajectories in large language models.

Conclusion: A proposed sub-optimal scaling law provides better predictive insights for sub-scaling regimes, emphasizing the significance of data quality and diversity.

Abstract: Traditional scaling laws in natural language processing suggest that
increasing model size and training data enhances performance. However, recent
studies reveal deviations, particularly in large language models, where
performance improvements decelerate, which is a phenomenon known as
sub-scaling. This paper revisits these scaling laws by examining the impact of
data quality and training strategies on model performance. Through extensive
empirical analysis of over 400 models, we identify high data density and
non-optimal resource allocation as key factors contributing to sub-scaling.
High data density leads to diminishing returns due to redundant information,
while optimal resource allocation is crucial for sustained performance
improvements. We propose a sub-optimal scaling law that better predicts
performance in sub-scaling regimes, highlighting the importance of data quality
and diversity.

</details>


### [183] [A Simple Approximate Bayesian Inference Neural Surrogate for Stochastic Petri Net Models](https://arxiv.org/abs/2507.10714)
*Bright Kwaku Manu,Trevor Reckell,Beckett Sterner,Petar Jevtic*

Main category: cs.LG

TL;DR: This paper proposes a neural-surrogate method to estimate parameters in Stochastic Petri Nets (SPNs), overcoming challenges of external covariates and missing event data.


<details>
  <summary>Details</summary>
Motivation: Parameter estimation for SPNs, particularly in the presence of covariate-dependent rates and missing events, poses significant challenges.

Method: A 1D Convolutional Residual Network is trained end-to-end on simulated SPN data to predict covariate-dependent transition-rate coefficients. Monte Carlo dropout is used during inference to provide uncertainty estimates.

Result: The proposed method achieves an RMSE of 0.108 even with 20% missing events and is computationally faster than Bayesian approaches.

Conclusion: Likelihood-free, data-driven surrogates are effective, accurate, and robust for parameter estimation in SPNs under challenging conditions like missing or noisy data.

Abstract: Stochastic Petri Nets (SPNs) are an increasingly popular tool of choice for
modeling discrete-event dynamics in areas such as epidemiology and systems
biology, yet their parameter estimation remains challenging in general and in
particular when transition rates depend on external covariates and explicit
likelihoods are unavailable. We introduce a neural-surrogate
(neural-network--based approximation of the posterior distribution) framework
that predicts the coefficients of known covariate-dependent rate functions
directly from noisy, partially observed token trajectories. Our model employs a
lightweight 1D Convolutional Residual Network trained end-to-end on
Gillespie-simulated SPN realizations, learning to invert system dynamics under
realistic conditions of event dropout. During inference, Monte Carlo dropout
provides calibrated uncertainty bounds together with point estimates. On
synthetic SPNs with 20% missing events, our surrogate recovers rate-function
coefficients with an RMSE = 0.108 and substantially runs faster than
traditional Bayesian approaches. These results demonstrate that data-driven,
likelihood-free surrogates can enable accurate, robust, and real-time parameter
recovery in complex, partially observed discrete-event systems.

</details>


### [184] [Fine-tuning Large Language Model for Automated Algorithm Design](https://arxiv.org/abs/2507.10614)
*Fei Liu,Rui Zhang,Xi Lin,Zhichao Lu,Qingfu Zhang*

Main category: cs.LG

TL;DR: The paper explores fine-tuning large language models (LLMs) for algorithm design, introducing a Diversity-Aware Rank-based (DAR) sampling strategy and direct preference optimization to improve task-specific performance.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs are trained for general coding tasks, raising the question of whether task-specific LLMs for algorithm design are necessary and how they can generalize across different tasks.

Method: The authors fine-tuned LLMs using DAR sampling for balanced data diversity and quality, along with direct preference optimization for task alignment. They evaluated this approach on three algorithm design tasks with two LLM sizes.

Result: Fine-tuned, smaller LLMs outperformed off-the-shelf versions and matched the performance of larger LLMs on certain tasks. Additionally, the fine-tuned models showed strong generalization to related tasks.

Conclusion: Task-specific fine-tuning of LLMs improves performance and generalization in algorithm design, demonstrating the value of specialization and opening new opportunities for future research.

Abstract: The integration of large language models (LLMs) into automated algorithm
design has shown promising potential. A prevalent approach embeds LLMs within
search routines to iteratively generate and refine candidate algorithms.
However, most existing methods rely on off-the-shelf LLMs trained for general
coding tasks,leaving a key question open: Do we need LLMs specifically tailored
for algorithm design? If so, how can such LLMs be effectively obtained and how
well can they generalize across different algorithm design tasks? In this
paper, we take a first step toward answering these questions by exploring
fine-tuning of LLMs for algorithm design. We introduce a Diversity-Aware Rank
based (DAR) sampling strategy to balance training data diversity and quality,
then we leverage direct preference optimization to efficiently align LLM
outputs with task objectives. Our experiments, conducted on
Llama-3.2-1B-Instruct and Llama- 3.1-8B-Instruct, span three distinct algorithm
design tasks. Results suggest that finetuned LLMs can significantly outperform
their off-the-shelf counterparts with the smaller Llama-3.2-1B-Instruct and
match the larger Llama-3.1-8B-Instruct on the admissible set problem. Moreover,
we observe promising generalization: LLMs finetuned on specific algorithm
design tasks also improve performance on related tasks with varying settings.
These findings highlight the value of task-specific adaptation for LLMs in
algorithm design and open new avenues for future research.

</details>


### [185] [A parametric activation function based on Wendland RBF](https://arxiv.org/abs/2507.11493)
*Majid Darehmiraki*

Main category: cs.LG

TL;DR: The paper proposes a novel activation function based on Wendland radial basis functions (RBFs) for deep learning that outperforms traditional ones like ReLU, especially in regression tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of traditional activation functions (ReLU, sigmoid, tanh) in terms of gradient propagation and stability during training.

Method: The authors design an enhanced Wendland activation function combining Wendland RBF components with linear and exponential terms for tunable locality and smoothness.

Result: The proposed activation function shows superior accuracy in certain scenarios, particularly regression tasks, while also maintaining computational efficiency.

Conclusion: Wendland-based activations provide localized, smooth transformations, reduce overfitting, and improve generalization, making them a promising choice for deep learning applications.

Abstract: This paper introduces a novel parametric activation function based on
Wendland radial basis functions (RBFs) for deep neural networks. Wendland RBFs,
known for their compact support, smoothness, and positive definiteness in
approximation theory, are adapted to address limitations of traditional
activation functions like ReLU, sigmoid, and tanh. The proposed enhanced
Wendland activation combines a standard Wendland component with linear and
exponential terms, offering tunable locality, improved gradient propagation,
and enhanced stability during training. Theoretical analysis highlights its
mathematical properties, including smoothness and adaptability, while empirical
experiments on synthetic tasks (e.g., sine wave approximation) and benchmark
datasets (MNIST, Fashion-MNIST) demonstrate competitive performance. Results
show that the Wendland-based activation achieves superior accuracy in certain
scenarios, particularly in regression tasks, while maintaining computational
efficiency. The study bridges classical RBF theory with modern deep learning,
suggesting that Wendland activations can mitigate overfitting and improve
generalization through localized, smooth transformations. Future directions
include hybrid architectures and domain-specific adaptations.

</details>


### [186] [Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces Them](https://arxiv.org/abs/2507.10616)
*Neel Rajani,Aryo Pradipta Gema,Seraphina Goldfarb-Tarrant,Ivan Titov*

Main category: cs.LG

TL;DR: The paper analyzes and compares RL and SFT training approaches for reasoning tasks in LLMs, identifying trends of in-domain performance gains and out-of-domain degradations.


<details>
  <summary>Details</summary>
Motivation: To better understand and compare the training dynamics of reinforcement learning (RL) and supervised fine-tuning (SFT) on reasoning tasks, as their mechanisms and effects remain poorly studied.

Method: A comparative analysis was conducted using RL and SFT on identical math problems, the same model, and similar hyperparameters. Further parameter-level changes were studied, and experiments explored mitigation strategies for out-of-domain performance loss.

Result: RL yielded minor in-domain gains and slight out-of-domain degradations, while SFT showed more pronounced improvements in-domain alongside larger out-of-domain performance drops. Parameter analysis revealed differential modifications in query, key weights, and mid-layer MLPs.

Conclusion: The study hypothesizes that RL tends to amplify existing model capabilities, whereas SFT replaces old skills with new ones. Efforts to mitigate out-of-domain degradation yielded inconclusive results, warranting further investigation.

Abstract: Training large language models (LLMs) for reasoning via maths and code
datasets has become a major new focus in LLM post-training. Two particularly
popular approaches are reinforcement learning (RL) and supervised fine-tuning
(SFT), but their training dynamics are poorly understood. We present a
comparative analysis of RL and SFT on the same maths problems with the same
model and similar hyperparameters. We find that RL yields minor in-domain gains
on maths and slight degradation on knowledge-intensive benchmarks like MMLU,
while both trends are more pronounced in SFT. We also analyse model parameters
across checkpoints, observing that both algorithms modify query and key weights
the most. Meanwhile, SFT exhibits greater updates and also affects mid-layer
MLPs more, leading us to hypothesise that this may have caused the
out-of-domain degradation. We therefore investigate whether freezing parts of
the model during training can mitigate the reduced performance on
knowledge-intensive benchmarks. However, our results are inconclusive, with
benefits on GPQA:Diamond and degradation on other benchmarks. Taken together,
our observations provide a preliminary indication for why RL amplifies existing
capabilities, while SFT replaces old skills with new ones.

</details>


### [187] [Multi-Armed Sampling Problem and the End of Exploration](https://arxiv.org/abs/2507.10797)
*Mohammad Pedramfar,Siamak Ravanbakhsh*

Main category: cs.LG

TL;DR: This paper introduces a new framework called multi-armed sampling to explore the exploration-exploitation trade-off in sampling, establishes lower bounds for regret, proposes an algorithm to achieve optimal regret, and unifies it with multi-armed bandits.


<details>
  <summary>Details</summary>
Motivation: The motivation is to rigorously analyze the exploration-exploitation trade-off in the context of sampling and provide foundational insights for areas like entropy-regularized reinforcement learning and neural samplers.

Method: The paper systematically defines regret notions, establishes lower bounds, proposes an algorithm achieving optimal regret, and connects multi-armed sampling with multi-armed bandit problems via a temperature parameter.

Result: The findings suggest that exploration is unnecessary for sampling, unlike optimization, and unify the study of sampling and bandits through a shared framework.

Conclusion: The work offers a foundational role for multi-armed sampling in studying sampling problems and highlights its implications for RL, pretrained model fine-tuning, and RLHF.

Abstract: This paper introduces the framework of multi-armed sampling, as the sampling
counterpart to the optimization problem of multi-arm bandits. Our primary
motivation is to rigorously examine the exploration-exploitation trade-off in
the context of sampling. We systematically define plausible notions of regret
for this framework and establish corresponding lower bounds. We then propose a
simple algorithm that achieves these optimal regret bounds. Our theoretical
results demonstrate that in contrast to optimization, sampling does not require
exploration. To further connect our findings with those of multi-armed bandits,
we define a continuous family of problems and associated regret measures that
smoothly interpolates and unifies multi-armed sampling and multi-armed bandit
problems using a temperature parameter. We believe the multi-armed sampling
framework, and our findings in this setting can have a foundational role in the
study of sampling including recent neural samplers, akin to the role of
multi-armed bandits in reinforcement learning. In particular, our work sheds
light on the need for exploration and the convergence properties of algorithm
for entropy-regularized reinforcement learning, fine-tuning of pretrained
models and reinforcement learning with human feedback (RLHF).

</details>


### [188] [Compute Requirements for Algorithmic Innovation in Frontier AI Models](https://arxiv.org/abs/2507.10618)
*Peter Barnett*

Main category: cs.LG

TL;DR: This paper assesses the computational requirements for algorithmic innovations in the pretraining of large language models, suggesting that even stringent compute restrictions might not significantly hinder AI progress.


<details>
  <summary>Details</summary>
Motivation: Understanding the compute requirements for AI research and the impact of limitations on further advancements.

Method: The authors cataloged and analyzed 36 algorithmic innovations from Llama 3 and DeepSeek-V3, estimating their development resources and studying the impact of imposing compute restrictions.

Result: Many innovations have resource demands doubling annually, yet stringent compute caps would still allow half of the studied innovations to proceed.

Conclusion: Compute limits alone are unlikely to serve as a strong barrier to algorithmic progress in AI pretraining advancements.

Abstract: Algorithmic innovation in the pretraining of large language models has driven
a massive reduction in the total compute required to reach a given level of
capability. In this paper we empirically investigate the compute requirements
for developing algorithmic innovations. We catalog 36 pre-training algorithmic
innovations used in Llama 3 and DeepSeek-V3. For each innovation we estimate
both the total FLOP used in development and the FLOP/s of the hardware
utilized. Innovations using significant resources double in their requirements
each year. We then use this dataset to investigate the effect of compute caps
on innovation. Our analysis suggests that compute caps alone are unlikely to
dramatically slow AI algorithmic progress. Even stringent compute caps -- such
as capping total operations to the compute used to train GPT-2 or capping
hardware capacity to 8 H100 GPUs -- could still have allowed for half of the
cataloged innovations.

</details>


### [189] [D3FL: Data Distribution and Detrending for Robust Federated Learning in Non-linear Time-series Data](https://arxiv.org/abs/2507.11471)
*Harsha Varun Marisetty,Manik Gupta,Yogesh Simmhan*

Main category: cs.LG

TL;DR: The paper evaluates Federated Learning (FL) for forecasting non-linear, non-stationary time-series data, examining data distribution effects and the role of detrending techniques.


<details>
  <summary>Details</summary>
Motivation: IoT data analysis faces challenges like centralization delays and costs. FL offers distributed analysis but struggles with the complexities of non-linear, non-stationary data in IoT applications.

Method: Synthetic datasets with non-linear distribution patterns were generated for testing, along with real-world datasets. An LSTM model was used for forecasting, comparing FL with centralized approaches, alongside detrending technique evaluations.

Result: FL proved inferior to centralized methods for non-linear data forecasting. Introducing detrending techniques improved FL performance and reduced prediction loss.

Conclusion: While FL is less effective for forecasting non-linear time-series data, applying suitable detrending methods can enhance its performance.

Abstract: With advancements in computing and communication technologies, the Internet
of Things (IoT) has seen significant growth. IoT devices typically collect data
from various sensors, such as temperature, humidity, and energy meters. Much of
this data is temporal in nature. Traditionally, data from IoT devices is
centralized for analysis, but this approach introduces delays and increased
communication costs. Federated learning (FL) has emerged as an effective
alternative, allowing for model training across distributed devices without the
need to centralize data. In many applications, such as smart home energy and
environmental monitoring, the data collected by IoT devices across different
locations can exhibit significant variation in trends and seasonal patterns.
Accurately forecasting such non-stationary, non-linear time-series data is
crucial for applications like energy consumption estimation and weather
forecasting. However, these data variations can severely impact prediction
accuracy. The key contributions of this paper are: (1) Investigating how
non-linear, non-stationary time-series data distributions, like generalized
extreme value (gen-extreme) and log norm distributions, affect FL performance.
(2) Analyzing how different detrending techniques for non-linear time-series
data influence the forecasting model's performance in a FL setup. We generated
several synthetic time-series datasets using non-linear data distributions and
trained an LSTM-based forecasting model using both centralized and FL
approaches. Additionally, we evaluated the impact of detrending on real-world
datasets with non-linear time-series data distributions. Our experimental
results show that: (1) FL performs worse than centralized approaches when
dealing with non-linear data distributions. (2) The use of appropriate
detrending techniques improves FL performance, reducing loss across different
data distributions.

</details>


### [190] [Meta-Reinforcement Learning for Fast and Data-Efficient Spectrum Allocation in Dynamic Wireless Networks](https://arxiv.org/abs/2507.10619)
*Oluwaseyi Giwa,Tobi Awodunmila,Muhammad Ahmed Mohsin,Ahsan Bilal,Muhammad Ali Jamshed*

Main category: cs.LG

TL;DR: Authors present a meta-learning framework for dynamic spectrum allocation in 5G/6G networks, addressing inefficiencies and safety risks in traditional deep reinforcement learning. Their attention-enhanced RNN approach outperforms alternatives in multiple metrics, showing effective and safe adaptation.


<details>
  <summary>Details</summary>
Motivation: Efficiently managing dynamic spectrum allocation in 5G/6G networks while overcoming limitations of traditional DRL approaches, such as safety concerns and high sample complexity.

Method: The paper implements three meta-learning architectures: model-agnostic meta-learning (MAML), recurrent neural networks (RNN), and an attention-enhanced RNN. These are compared against a proximal policy optimization (PPO) baseline within a simulated integrated access/backhaul (IAB) wireless environment.

Result: The attention-based meta-learning architecture achieved the best performance, with a peak mean network throughput of 48 Mbps, reduced SINR and latency violations by over 50%, and a fairness index of 0.7, significantly outperforming the PPO baseline.

Conclusion: Meta-learning offers a safer and more adaptive approach to intelligent resource control in wireless systems, proving superior efficiency and reliability for spectrum allocation in 5G/6G networks.

Abstract: The dynamic allocation of spectrum in 5G / 6G networks is critical to
efficient resource utilization. However, applying traditional deep
reinforcement learning (DRL) is often infeasible due to its immense sample
complexity and the safety risks associated with unguided exploration, which can
cause severe network interference. To address these challenges, we propose a
meta-learning framework that enables agents to learn a robust initial policy
and rapidly adapt to new wireless scenarios with minimal data. We implement
three meta-learning architectures, model-agnostic meta-learning (MAML),
recurrent neural network (RNN), and an attention-enhanced RNN, and evaluate
them against a non-meta-learning DRL algorithm, proximal policy optimization
(PPO) baseline, in a simulated dynamic integrated access/backhaul (IAB)
environment. Our results show a clear performance gap. The attention-based
meta-learning agent reaches a peak mean network throughput of 48 Mbps, while
the PPO baseline decreased drastically to 10 Mbps. Furthermore, our method
reduces SINR and latency violations by more than 50% compared to PPO. It also
shows quick adaptation, with a fairness index 0.7, showing better resource
allocation. This work proves that meta-learning is a very effective and safer
option for intelligent control in complex wireless systems.

</details>


### [191] [LLMs Meet Cross-Modal Time Series Analytics: Overview and Directions](https://arxiv.org/abs/2507.10620)
*Chenxi Liu,Hao Miao,Cheng Long,Yan Zhao,Ziyue Li,Panos Kalnis*

Main category: cs.LG

TL;DR: This paper/tutorial reviews the role of Large Language Models (LLMs) in time series analytics and introduces a taxonomy for cross-modal approaches, while summarizing their applications and challenges.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the cross-modality gap between time series and textual data, leveraging the potential of LLMs in time series analytics.

Method: The tutorial classifies existing cross-modal modeling strategies into three approaches: conversion, alignment, and fusion, and reviews their applications in downstream tasks.

Result: The results include a comprehensive overview of methodologies, a taxonomy, and an understanding of challenges in applying LLMs to cross-modal time series analytics.

Conclusion: The tutorial provides insights into using LLMs for real-world cross-modal time series analytics, emphasizing both effectiveness and efficiency.

Abstract: Large Language Models (LLMs) have emerged as a promising paradigm for time
series analytics, leveraging their massive parameters and the shared sequential
nature of textual and time series data. However, a cross-modality gap exists
between time series and textual data, as LLMs are pre-trained on textual
corpora and are not inherently optimized for time series. In this tutorial, we
provide an up-to-date overview of LLM-based cross-modal time series analytics.
We introduce a taxonomy that classifies existing approaches into three groups
based on cross-modal modeling strategies, e.g., conversion, alignment, and
fusion, and then discuss their applications across a range of downstream tasks.
In addition, we summarize several open challenges. This tutorial aims to expand
the practical application of LLMs in solving real-world problems in cross-modal
time series analytics while balancing effectiveness and efficiency.
Participants will gain a thorough understanding of current advancements,
methodologies, and future research directions in cross-modal time series
analytics.

</details>


### [192] [Flows and Diffusions on the Neural Manifold](https://arxiv.org/abs/2507.10623)
*Daniel Saragih,Deyu Cao,Tejas Balaji*

Main category: cs.LG

TL;DR: This paper develops methods to generate and optimize neural network weight spaces effectively using trajectory inference and structural priors derived from optimization dynamics.


<details>
  <summary>Details</summary>
Motivation: Advances in generative models like diffusion and flow-based methods have seen success in domains such as image and language synthesis, but less exploration has occurred in applying these techniques to neural network weight space learning.

Method: The paper models optimization trajectories as an inference problem, provides a unified theoretical framework for gradient flow matching, and incorporates techniques like adjoint matching, autoencoders, task-specific conditioning, and tailored source distributions.

Result: Experiments show the method generates weights better matching in-distribution properties, improves downstream initialization training, enables further fine-tuning, and outperforms baselines in detecting covariate shifts in safety-critical systems.

Conclusion: The integration of trajectory inference techniques into generative modeling for weight spaces provides benefits in optimization quality, initialization, safety-critical applications, and expands the scope of generative modeling in deep learning.

Abstract: Diffusion and flow-based generative models have achieved remarkable success
in domains such as image synthesis, video generation, and natural language
modeling. In this work, we extend these advances to weight space learning by
leveraging recent techniques to incorporate structural priors derived from
optimization dynamics. Central to our approach is modeling the trajectory
induced by gradient descent as a trajectory inference problem. We unify several
trajectory inference techniques under the framework of gradient flow matching,
providing a theoretical framework for treating optimization paths as inductive
bias. We further explore architectural and algorithmic choices, including
reward fine-tuning by adjoint matching, the use of autoencoders for latent
weight representation, conditioning on task-specific context data, and adopting
informative source distributions such as Kaiming uniform. Experiments
demonstrate that our method matches or surpasses baselines in generating
in-distribution weights, improves initialization for downstream training, and
supports fine-tuning to enhance performance. Finally, we illustrate a practical
application in safety-critical systems: detecting harmful covariate shifts,
where our method outperforms the closest comparable baseline.

</details>


### [193] [Player-Team Heterogeneous Interaction Graph Transformer for Soccer Outcome Prediction](https://arxiv.org/abs/2507.10626)
*Lintao Wang,Shiwen Xu,Michael Horton,Joachim Gudmundsson,Zhiyong Wang*

Main category: cs.LG

TL;DR: The paper introduces HIGFormer, a graph-augmented transformer for soccer outcome prediction that combines player and team interaction modeling.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook the heterogeneous interactions among players and teams, limiting the accuracy of soccer match outcome predictions.

Method: The paper proposes HIGFormer, which includes a Player Interaction Network for player dynamics, a Team Interaction Network for team relationships, and a Match Comparison Transformer to integrate these levels.

Result: HIGFormer showcases state-of-the-art performance in prediction accuracy on the WyScout dataset, surpassing existing approaches.

Conclusion: HIGFormer provides a comprehensive framework for soccer prediction, with potential applications in player performance evaluation and strategic planning.

Abstract: Predicting soccer match outcomes is a challenging task due to the inherently
unpredictable nature of the game and the numerous dynamic factors influencing
results. While it conventionally relies on meticulous feature engineering, deep
learning techniques have recently shown a great promise in learning effective
player and team representations directly for soccer outcome prediction.
However, existing methods often overlook the heterogeneous nature of
interactions among players and teams, which is crucial for accurately modeling
match dynamics. To address this gap, we propose HIGFormer (Heterogeneous
Interaction Graph Transformer), a novel graph-augmented transformer-based deep
learning model for soccer outcome prediction. HIGFormer introduces a
multi-level interaction framework that captures both fine-grained player
dynamics and high-level team interactions. Specifically, it comprises (1) a
Player Interaction Network, which encodes player performance through
heterogeneous interaction graphs, combining local graph convolutions with a
global graph-augmented transformer; (2) a Team Interaction Network, which
constructs interaction graphs from a team-to-team perspective to model
historical match relationships; and (3) a Match Comparison Transformer, which
jointly analyzes both team and player-level information to predict match
outcomes. Extensive experiments on the WyScout Open Access Dataset, a
large-scale real-world soccer dataset, demonstrate that HIGFormer significantly
outperforms existing methods in prediction accuracy. Furthermore, we provide
valuable insights into leveraging our model for player performance evaluation,
offering a new perspective on talent scouting and team strategy analysis.

</details>


### [194] [Fast Last-Iterate Convergence of SGD in the Smooth Interpolation Regime](https://arxiv.org/abs/2507.11274)
*Amit Attia,Matan Schliserman,Uri Sherman,Tomer Koren*

Main category: cs.LG

TL;DR: This paper investigates the convergence guarantees of stochastic gradient descent (SGD) for smooth convex objectives in interpolation regimes, focusing on the last-iterate behavior, especially with large stepsizes.


<details>
  <summary>Details</summary>
Motivation: To address challenges in training over-parameterized models, understanding continual learning forgetting, and analyzing the randomized Kaczmarz method for solving linear systems.

Method: The authors establish theoretical bounds for the expected excess risk of SGD using $eta$-smooth convex loss functions, considering stepsize constraints and stochastic gradient variance at optimum.

Result: They achieved rates of $\widetilde{O}(1/T + \sigma_\star/\sqrt{T})$ with optimized stepsize. Notably, when $\sigma_\star=0$, they improved the convergence rate to $O(1/\sqrt{T})$.

Conclusion: The findings generalize previous work while improving last-iterate convergence guarantees for both smooth convex loss functions and specific scenarios with zero gradient noise.

Abstract: We study population convergence guarantees of stochastic gradient descent
(SGD) for smooth convex objectives in the interpolation regime, where the noise
at optimum is zero or near zero. The behavior of the last iterate of SGD in
this setting -- particularly with large (constant) stepsizes -- has received
growing attention in recent years due to implications for the training of
over-parameterized models, as well as to analyzing forgetting in continual
learning and to understanding the convergence of the randomized Kaczmarz method
for solving linear systems. We establish that after $T$ steps of SGD on
$\beta$-smooth convex loss functions with stepsize $\eta \leq 1/\beta$, the
last iterate exhibits expected excess risk $\widetilde{O}(1/(\eta
T^{1-\beta\eta/2}) + \eta T^{\beta\eta/2} \sigma_\star^2)$, where
$\sigma_\star^2$ denotes the variance of the stochastic gradients at the
optimum. In particular, for a well-tuned stepsize we obtain a near optimal
$\widetilde{O}(1/T + \sigma_\star/\sqrt{T})$ rate for the last iterate,
extending the results of Varre et al. (2021) beyond least squares regression;
and when $\sigma_\star=0$ we obtain a rate of $O(1/\sqrt{T})$ with
$\eta=1/\beta$, improving upon the best-known $O(T^{-1/4})$ rate recently
established by Evron et al. (2025) in the special case of realizable linear
regression.

</details>


### [195] [GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning](https://arxiv.org/abs/2507.10628)
*Ziru Liu,Cheng Gong,Xinyu Fu,Yaofang Liu,Ran Chen,Shoubo Hu,Suiyun Zhang,Rui Liu,Qingfu Zhang,Dandan Tu*

Main category: cs.LG

TL;DR: The paper introduces Guided Hybrid Policy Optimization (GHPO), a method enhancing reinforcement learning in language models with adaptive prompts and curriculum learning, achieving a 5% performance boost in mathematical benchmarks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome training instability and inefficiency in reinforcement learning for language models, particularly due to a mismatch between task difficulty and model capacity.

Method: The proposed GHPO dynamically adjusts task difficulty using adaptive prompts and integrates imitation learning with exploration-based reinforcement learning to optimize the training process.

Result: GHPO improved performance by approximately 5% on six mathematical benchmarks, outperforming baseline methods and enhancing training stability.

Conclusion: GHPO proves to be an effective and scalable framework for improving reasoning in language models, particularly benefiting smaller, resource-efficient models.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a powerful paradigm for facilitating the self-improvement of large language
models (LLMs), particularly in the domain of complex reasoning tasks. However,
prevailing on-policy RL methods often contend with significant training
instability and inefficiency. This is primarily due to a capacity-difficulty
mismatch, where the complexity of training data frequently outpaces the model's
current capabilities, leading to critically sparse reward signals and stalled
learning progress. This challenge is particularly acute for smaller, more
resource-efficient LLMs. To overcome this, we introduce the Guided Hybrid
Policy Optimization (GHPO), a novel difficulty-aware reinforcement learning
framework. GHPO dynamically calibrates task difficulty by employing adaptive
prompt refinement to provide targeted guidance. This unique approach adaptively
balances direct imitation learning for problems currently beyond the model's
reach with exploration-based reinforcement learning for more manageable tasks,
effectively creating a smooth and optimized learning curriculum. Extensive
experiments demonstrate that GHPO achieves an average performance gain of
approximately 5% across six challenging mathematics benchmarks, consistently
outperforming strong on-policy reinforcement learning and curriculum learning
baselines. Further analysis confirms that our framework significantly enhances
both training stability and final reasoning performance, thus offering a
scalable and efficient solution for developing powerful and robust reasoning
models.

</details>


### [196] [Neurosymbolic Reasoning Shortcuts under the Independence Assumption](https://arxiv.org/abs/2507.11357)
*Emile van Krieken,Pasquale Minervini,Edoardo Ponti,Antonio Vergari*

Main category: cs.LG

TL;DR: This paper investigates the role of the independence assumption in neurosymbolic (NeSy) predictors and highlights its limitations in representing uncertainty over concept combinations and managing reasoning shortcuts.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address skepticism within the NeSy community regarding the practical consequences of the independence assumption used in NeSy predictors.

Method: Formal analysis is used to demonstrate the inability of models with independence assumptions to accurately represent uncertainty in certain concept combinations.

Result: The study shows that the independence assumption prevents models from accounting for reasoning shortcuts, leading to incorrect predictions for the wrong reasons despite solving downstream tasks properly.

Conclusion: The independence assumption fundamentally limits the representational capacity of neurosymbolic predictors, impacting their ability to handle uncertainty and maintain reasoning consistency.

Abstract: The ubiquitous independence assumption among symbolic concepts in
neurosymbolic (NeSy) predictors is a convenient simplification: NeSy predictors
use it to speed up probabilistic reasoning. Recent works like van Krieken et
al. (2024) and Marconato et al. (2024) argued that the independence assumption
can hinder learning of NeSy predictors and, more crucially, prevent them from
correctly modelling uncertainty. There is, however, scepticism in the NeSy
community around the scenarios in which the independence assumption actually
limits NeSy systems (Faronius and Dos Martires, 2025). In this work, we settle
this question by formally showing that assuming independence among symbolic
concepts entails that a model can never represent uncertainty over certain
concept combinations. Thus, the model fails to be aware of reasoning shortcuts,
i.e., the pathological behaviour of NeSy predictors that predict correct
downstream tasks but for the wrong reasons.

</details>


### [197] [Scalable Unsupervised Segmentation via Random Fourier Feature-based Gaussian Process](https://arxiv.org/abs/2507.10632)
*Issei Saito,Masatoshi Nagano,Tomoaki Nakamura,Daichi Mochihashi,Koki Mimura*

Main category: cs.LG

TL;DR: The paper introduces RFF-GP-HSMM, a fast method for segmenting time-series data that significantly reduces computational costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: The high computational cost of Gaussian process hidden semi-Markov models (GP-HSMM) due to the need for inverting large kernel matrices in time-series analysis.

Method: Integrating Random Fourier Features (RFF) to approximate Gaussian processes via linear regression, avoiding kernel matrix inversion.

Result: The method achieved comparable segmentation performance to existing approaches but was approximately 278 times faster on datasets with 39,200 frames.

Conclusion: RFF-GP-HSMM provides an efficient solution for time-series segmentation by maintaining accuracy while dramatically reducing computational costs.

Abstract: In this paper, we propose RFF-GP-HSMM, a fast unsupervised time-series
segmentation method that incorporates random Fourier features (RFF) to address
the high computational cost of the Gaussian process hidden semi-Markov model
(GP-HSMM). GP-HSMM models time-series data using Gaussian processes, requiring
inversion of an N times N kernel matrix during training, where N is the number
of data points. As the scale of the data increases, matrix inversion incurs a
significant computational cost. To address this, the proposed method
approximates the Gaussian process with linear regression using RFF, preserving
expressive power while eliminating the need for inversion of the kernel matrix.
Experiments on the Carnegie Mellon University (CMU) motion-capture dataset
demonstrate that the proposed method achieves segmentation performance
comparable to that of conventional methods, with approximately 278 times faster
segmentation on time-series data comprising 39,200 frames.

</details>


### [198] [Local Pairwise Distance Matching for Backpropagation-Free Reinforcement Learning](https://arxiv.org/abs/2507.11367)
*Daniel Tanneberg*

Main category: cs.LG

TL;DR: The paper introduces a backpropagation-free reinforcement learning approach by training neural network layers using local signals during the forward pass.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of backpropagation, such as storage overhead and vanishing/exploding gradient issues, in reinforcement learning.

Method: A novel layer-wise training scheme using local losses based on multi-dimensional scaling principles, combined with optional reward-driven guidance during the forward pass.

Result: Experiments show competitive performance with traditional backpropagation methods, improved stability, and enhanced performance in challenging environments.

Conclusion: The proposed method offers a viable alternative to backpropagation-based training in reinforcement learning by improving efficiency and robustness.

Abstract: Training neural networks with reinforcement learning (RL) typically relies on
backpropagation (BP), necessitating storage of activations from the forward
pass for subsequent backward updates. Furthermore, backpropagating error
signals through multiple layers often leads to vanishing or exploding
gradients, which can degrade learning performance and stability. We propose a
novel approach that trains each layer of the neural network using local signals
during the forward pass in RL settings. Our approach introduces local,
layer-wise losses leveraging the principle of matching pairwise distances from
multi-dimensional scaling, enhanced with optional reward-driven guidance. This
method allows each hidden layer to be trained using local signals computed
during forward propagation, thus eliminating the need for backward passes and
storing intermediate activations. Our experiments, conducted with policy
gradient methods across common RL benchmarks, demonstrate that this
backpropagation-free method achieves competitive performance compared to their
classical BP-based counterpart. Additionally, the proposed method enhances
stability and consistency within and across runs, and improves performance
especially in challenging environments.

</details>


### [199] [A Simple Baseline for Stable and Plastic Neural Networks](https://arxiv.org/abs/2507.10637)
*É. Künzel,A. Jaziri,V. Ramesh*

Main category: cs.LG

TL;DR: RDBP is a lightweight approach that balances plasticity and stability in continual learning for computer vision using two mechanisms—ReLUDown and Decreasing Backpropagation.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of balancing plasticity and stability in continual learning models for computer vision, which must adapt to a sequence of tasks without losing prior knowledge.

Method: RDBP comprises ReLUDown, an activation modification to preserve neuron sensitivity, and Decreasing Backpropagation, a gradient-scheduling scheme inspired by biology, shielding early layers from significant updates.

Result: Demonstrated on Continual ImageNet, RDBP achieves competitive or superior performance compared to state-of-the-art methods while reducing computational costs.

Conclusion: RDBP is a practical and efficient framework for continual learning, serving as a benchmark for future strategies in the domain.

Abstract: Continual learning in computer vision requires that models adapt to a
continuous stream of tasks without forgetting prior knowledge, yet existing
approaches often tip the balance heavily toward either plasticity or stability.
We introduce RDBP, a simple, low-overhead baseline that unites two
complementary mechanisms: ReLUDown, a lightweight activation modification that
preserves feature sensitivity while preventing neuron dormancy, and Decreasing
Backpropagation, a biologically inspired gradient-scheduling scheme that
progressively shields early layers from catastrophic updates. Evaluated on the
Continual ImageNet benchmark, RDBP matches or exceeds the plasticity and
stability of state-of-the-art methods while reducing computational cost. RDBP
thus provides both a practical solution for real-world continual learning and a
clear benchmark against which future continual learning strategies can be
measured.

</details>


### [200] [ZClassifier: Temperature Tuning and Manifold Approximation via KL Divergence on Logit Space](https://arxiv.org/abs/2507.10638)
*Shim Soon Yong*

Main category: cs.LG

TL;DR: The paper introduces ZClassifier, a new classification framework using Gaussian-distributed logits, which improves robustness, calibration, and latent separation over conventional softmax classifiers.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of conventional deterministic logits in terms of uncertainty calibration, robustness, and geometric consistency for classification tasks.

Method: The ZClassifier replaces deterministic logits with diagonal Gaussian-distributed logits and minimizes the KL divergence between predictions and a unit isotropic Gaussian to unify uncertainty calibration and latent control.

Result: ZClassifier demonstrates improved performance in robustness, calibration, and latent separation on CIFAR-10 and CIFAR-100 compared to softmax classifiers. It is also effective for classifier-guided generation.

Conclusion: ZClassifier provides a probabilistic framework that enables better class confidence interpretation, geometric consistency, and efficacy for classifier-guided tasks, outperforming traditional approaches.

Abstract: We introduce a novel classification framework, ZClassifier, that replaces
conventional deterministic logits with diagonal Gaussian-distributed logits.
Our method simultaneously addresses temperature scaling and manifold
approximation by minimizing the Kullback-Leibler (KL) divergence between the
predicted Gaussian distributions and a unit isotropic Gaussian. This unifies
uncertainty calibration and latent control in a principled probabilistic
manner, enabling a natural interpretation of class confidence and geometric
consistency. Experiments on CIFAR-10 and CIFAR-100 show that ZClassifier
improves over softmax classifiers in robustness, calibration, and latent
separation. We also demonstrate its effectiveness for classifier-guided
generation by interpreting logits as Gaussian semantic potentials.

</details>


### [201] [First-of-its-kind AI model for bioacoustic detection using a lightweight associative memory Hopfield neural network](https://arxiv.org/abs/2507.10642)
*Andrew Gascoyne,Wendy Lomas*

Main category: cs.LG

TL;DR: This paper presents a lightweight AI model for bioacoustic analysis addressing issues of limited training data, sustainability, hardware requirements, and complexity.


<details>
  <summary>Details</summary>
Motivation: The researchers aim to alleviate the challenges in conservation bioacoustics, especially the analysis of large datasets from passive acoustic monitoring, while addressing environmental and technical constraints.

Method: The study utilizes a transparent Hopfield neural network for associative memory, requiring only one representative signal for training, achieving rapid classification with minimal resource usage.

Result: The model demonstrated fast training (3 ms), efficient classification of bat recordings (86% precision), and low computational demands (144 MB RAM on average hardware).

Conclusion: The developed AI model is equitable, sustainable, accurate, and non-species specific—a promising tool for bioacoustic analysis.

Abstract: A growing issue within conservation bioacoustics is the task of analysing the
vast amount of data generated from the use of passive acoustic monitoring
devices. In this paper, we present an alternative AI model which has the
potential to help alleviate this problem. Our model formulation addresses the
key issues encountered when using current AI models for bioacoustic analysis,
namely the: limited training data available; environmental impact, particularly
in energy consumption and carbon footprint of training and implementing these
models; and associated hardware requirements. The model developed in this work
uses associative memory via a transparent, explainable Hopfield neural network
to store signals and detect similar signals which can then be used to classify
species. Training is rapid ($3$\,ms), as only one representative signal is
required for each target sound within a dataset. The model is fast, taking only
$5.4$\,s to pre-process and classify all $10384$ publicly available bat
recordings, on a standard Apple MacBook Air. The model is also lightweight with
a small memory footprint of $144.09$\,MB of RAM usage. Hence, the low
computational demands make the model ideal for use on a variety of standard
personal devices with potential for deployment in the field via edge-processing
devices. It is also competitively accurate, with up to $86\%$ precision on the
dataset used to evaluate the model. In fact, we could not find a single case of
disagreement between model and manual identification via expert field guides.
Although a dataset of bat echolocation calls was chosen to demo this
first-of-its-kind AI model, trained on only two representative calls, the model
is not species specific. In conclusion, we propose an equitable AI model that
has the potential to be a game changer for fast, lightweight, sustainable,
transparent, explainable and accurate bioacoustic analysis.

</details>


### [202] [Distributionally Robust Optimization with Adversarial Data Contamination](https://arxiv.org/abs/2507.10718)
*Shuyao Li,Ilias Diakonikolas,Jelena Diakonikolas*

Main category: cs.LG

TL;DR: Proposes a novel approach combining robustness against data contamination and distributional uncertainty in Wasserstein-1 DRO objectives.


<details>
  <summary>Details</summary>
Motivation: To address simultaneous challenges of distributional uncertainty and adversarially corrupted training data.

Method: Develops a modeling framework and algorithm based on robust statistics for generalized linear models with convex Lipschitz loss functions.

Result: Achieves $O(\sqrt{\epsilon})$ estimation error in the DRO objective value using contaminated data.

Conclusion: Provides rigorous guarantees and an efficient computational method for learning under both data contamination and distribution shifts.

Abstract: Distributionally Robust Optimization (DRO) provides a framework for
decision-making under distributional uncertainty, yet its effectiveness can be
compromised by outliers in the training data. This paper introduces a
principled approach to simultaneously address both challenges. We focus on
optimizing Wasserstein-1 DRO objectives for generalized linear models with
convex Lipschitz loss functions, where an $\epsilon$-fraction of the training
data is adversarially corrupted. Our primary contribution lies in a novel
modeling framework that integrates robustness against training data
contamination with robustness against distributional shifts, alongside an
efficient algorithm inspired by robust statistics to solve the resulting
optimization problem. We prove that our method achieves an estimation error of
$O(\sqrt{\epsilon})$ for the true DRO objective value using only the
contaminated data under the bounded covariance assumption. This work
establishes the first rigorous guarantees, supported by efficient computation,
for learning under the dual challenges of data contamination and distributional
shifts.

</details>


### [203] [Ground-Compose-Reinforce: Tasking Reinforcement Learning Agents through Formal Language](https://arxiv.org/abs/2507.10741)
*Andrew C. Li,Toryn Q. Klassen,Andrew Wang,Parand A. Alamdari,Sheila A. McIlraith*

Main category: cs.LG

TL;DR: This paper introduces the Ground-Compose-Reinforce framework to ground formal language in perception and action through a neurosymbolic approach, improving grounding and generalization with limited data.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of grounding language in complex perceptions (like pixels) and actions for agents that interact with humans via language. Traditional methods often rely on manual designs or large datasets, which are limiting.

Method: The authors propose the Ground-Compose-Reinforce framework, a neurosymbolic method that directly tasks reinforcement learning (RL) agents using formal language. This framework leverages data-driven learning to avoid manual design and employs compositional formal language semantics for data efficiency.

Result: The proposed method demonstrated effective mapping of formal language instructions to behaviors with limited data in both an image-based gridworld and a MuJoCo robotics domain. It outperformed end-to-end, data-driven methods in these experiments.

Conclusion: The Ground-Compose-Reinforce framework offers a data-efficient and generalizable approach to grounding language in perception and action, overcoming the limitations of manual design and large dataset requirements.

Abstract: Grounding language in complex perception (e.g. pixels) and action is a key
challenge when building situated agents that can interact with humans via
language. In past works, this is often solved via manual design of the language
grounding or by curating massive datasets relating language to elements of the
environment. We propose Ground-Compose-Reinforce, a neurosymbolic framework for
grounding formal language from data, and eliciting behaviours by directly
tasking RL agents through this language. By virtue of data-driven learning, our
framework avoids the manual design of domain-specific elements like reward
functions or symbol detectors. By virtue of compositional formal language
semantics, our framework achieves data-efficient grounding and generalization
to arbitrary language compositions. Experiments on an image-based gridworld and
a MuJoCo robotics domain show that our approach reliably maps formal language
instructions to behaviours with limited data while end-to-end, data-driven
approaches fail.

</details>


### [204] [A Benchmarking Framework for AI models in Automotive Aerodynamics](https://arxiv.org/abs/2507.10747)
*Kaustubh Tangsali,Rishikesh Ranade,Mohammad Amin Nabian,Alexey Kamenev,Peter Sharpe,Neil Ashton,Ram Cherukuri,Sanjay Choudhry*

Main category: cs.LG

TL;DR: This paper introduces a benchmarking framework to evaluate AI models for automotive aerodynamics prediction, focusing on accuracy, scalability, and generalization.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for standardized evaluation methods for AI models in automotive aerodynamics to improve understanding and innovation in the field.

Method: An open-source framework, NVIDIA PhysicsNeMo-CFD, is used to evaluate three AI models (DoMINO, X-MeshGraphNet, and FIGConvNet) with the DrivAerML dataset and provides extensibility for additional models and metrics.

Result: The framework systematically assessed surface and volumetric flow field predictions of AI models and demonstrated its adaptability for broader use cases.

Conclusion: The study offers a platform for assessing and advancing AI-based aerodynamic models, contributing to the development of more efficient and accurate automotive solutions.

Abstract: In this paper, we introduce a benchmarking framework within the open-source
NVIDIA PhysicsNeMo-CFD framework designed to systematically assess the
accuracy, performance, scalability, and generalization capabilities of AI
models for automotive aerodynamics predictions. The open extensible framework
enables incorporation of a diverse set of metrics relevant to the
Computer-Aided Engineering (CAE) community. By providing a standardized
methodology for comparing AI models, the framework enhances transparency and
consistency in performance assessment, with the overarching goal of improving
the understanding and development of these models to accelerate research and
innovation in the field. To demonstrate its utility, the framework includes
evaluation of both surface and volumetric flow field predictions on three AI
models: DoMINO, X-MeshGraphNet, and FIGConvNet using the DrivAerML dataset. It
also includes guidelines for integrating additional models and datasets, making
it extensible for physically consistent metrics. This benchmarking study aims
to enable researchers and industry professionals in selecting, refining, and
advancing AI-driven aerodynamic modeling approaches, ultimately fostering the
development of more efficient, accurate, and interpretable solutions in
automotive aerodynamics

</details>


### [205] [Spatial Reasoners for Continuous Variables in Any Domain](https://arxiv.org/abs/2507.10768)
*Bart Pogodzinski,Christopher Wewer,Bernt Schiele,Jan Eric Lenssen*

Main category: cs.LG

TL;DR: The paper introduces "Spatial Reasoners," a framework that uses generative denoising models for continuous-variable spatial reasoning.


<details>
  <summary>Details</summary>
Motivation: To enable more effective spatial reasoning over continuous variables using generative denoising models, which are currently prominent in image generation but require significant effort to adapt for reasoning tasks.

Method: The authors developed a software framework that simplifies infrastructure by providing interfaces for mapping variables, applying generative paradigms, and using various inference strategies.

Result: The framework, "Spatial Reasoners," has been made openly accessible to researchers.

Conclusion: The framework will facilitate easier application and exploration of generative denoising models for spatial reasoning in continuous variables.

Abstract: We present Spatial Reasoners, a software framework to perform spatial
reasoning over continuous variables with generative denoising models. Denoising
generative models have become the de-facto standard for image generation, due
to their effectiveness in sampling from complex, high-dimensional
distributions. Recently, they have started being explored in the context of
reasoning over multiple continuous variables. Providing infrastructure for
generative reasoning with such models requires a high effort, due to a wide
range of different denoising formulations, samplers, and inference strategies.
Our presented framework aims to facilitate research in this area, providing
easy-to-use interfaces to control variable mapping from arbitrary data domains,
generative model paradigms, and inference strategies. Spatial Reasoners are
openly available at https://spatialreasoners.github.io/

</details>


### [206] [A Generalizable Physics-Enhanced State Space Model for Long-Term Dynamics Forecasting in Complex Environments](https://arxiv.org/abs/2507.10792)
*Yuchen Wang,Hongjue Zhao,Haohong Lin,Enze Xu,Lifang He,Huajie Shao*

Main category: cs.LG

TL;DR: The paper introduces Phy-SSM, a method that integrates partial physics knowledge into state space models to improve long-term dynamic forecasting in noisy and irregular data environments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of long-term forecasting in complex, noisy environments by leveraging state space models' ability to handle sequential data and incorporating partial physics knowledge for better generalization.

Method: The proposed method, Phy-SSM, decomposes system dynamics into known and unknown state matrices and introduces a physics state regularization term to align latent states with system dynamics. It also provides theoretical analysis of solution uniqueness.

Result: Experiments on vehicle motion, drone state prediction, and COVID-19 forecasting show superior performance of Phy-SSM in long-term interpolation and extrapolation tasks compared to baselines.

Conclusion: Phy-SSM effectively combines physics knowledge with state space models for improved long-term forecasting accuracy, offering practical benefits in various real-world applications.

Abstract: This work aims to address the problem of long-term dynamic forecasting in
complex environments where data are noisy and irregularly sampled. While recent
studies have introduced some methods to improve prediction performance, these
approaches still face a significant challenge in handling long-term
extrapolation tasks under such complex scenarios. To overcome this challenge,
we propose Phy-SSM, a generalizable method that integrates partial physics
knowledge into state space models (SSMs) for long-term dynamics forecasting in
complex environments. Our motivation is that SSMs can effectively capture
long-range dependencies in sequential data and model continuous dynamical
systems, while the incorporation of physics knowledge improves generalization
ability. The key challenge lies in how to seamlessly incorporate partially
known physics into SSMs. To achieve this, we decompose partially known system
dynamics into known and unknown state matrices, which are integrated into a
Phy-SSM unit. To further enhance long-term prediction performance, we introduce
a physics state regularization term to make the estimated latent states align
with system dynamics. Besides, we theoretically analyze the uniqueness of the
solutions for our method. Extensive experiments on three real-world
applications, including vehicle motion prediction, drone state prediction, and
COVID-19 epidemiology forecasting, demonstrate the superior performance of
Phy-SSM over the baselines in both long-term interpolation and extrapolation
tasks. The code is available at https://github.com/511205787/Phy_SSM-ICML2025.

</details>


### [207] [Uncovering Causal Relation Shifts in Event Sequences under Out-of-Domain Interventions](https://arxiv.org/abs/2507.10809)
*Kazi Tasnim Zinat,Yun Zhou,Xiang Lyu,Yawei Wang,Zhicheng Liu,Panpan Xu*

Main category: cs.LG

TL;DR: The paper proposes a framework for causal inference in temporal sequences considering out-of-domain interventions, leveraging a Transformer model for better ATE estimation.


<details>
  <summary>Details</summary>
Motivation: To address the gap in causal inference by incorporating the effects of out-of-domain interventions, which are often overlooked in traditional domain-focused models.

Method: A new causal framework is introduced to redefine ATE under out-of-domain influences, paired with a Transformer neural network for advanced temporal pattern handling and integration of external intervention data.

Result: Experiments on simulated and real-world data demonstrate superior performance in both ATE estimation and model fit, compared to baseline methods.

Conclusion: The proposed framework effectively captures and models causal shifts due to out-of-domain interventions, showcasing its practical utility in dynamic temporal processes.

Abstract: Inferring causal relationships between event pairs in a temporal sequence is
applicable in many domains such as healthcare, manufacturing, and
transportation. Most existing work on causal inference primarily focuses on
event types within the designated domain, without considering the impact of
exogenous out-of-domain interventions. In real-world settings, these
out-of-domain interventions can significantly alter causal dynamics. To address
this gap, we propose a new causal framework to define average treatment effect
(ATE), beyond independent and identically distributed (i.i.d.) data in classic
Rubin's causal framework, to capture the causal relation shift between events
of temporal process under out-of-domain intervention. We design an unbiased ATE
estimator, and devise a Transformer-based neural network model to handle both
long-range temporal dependencies and local patterns while integrating
out-of-domain intervention information into process modeling. Extensive
experiments on both simulated and real-world datasets demonstrate that our
method outperforms baselines in ATE estimation and goodness-of-fit under
out-of-domain-augmented point processes.

</details>


### [208] [Semantic Context for Tool Orchestration](https://arxiv.org/abs/2507.10820)
*Robert Müller*

Main category: cs.LG

TL;DR: This paper introduces Semantic Context (SC) as essential for robust tool orchestration and supports it via theoretical and empirical validation, offering the FiReAct pipeline for improved Large Language Models (LLM) performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance the orchestration of tools by leveraging Semantic Context, addressing challenges in building adaptive, efficient, and scalable agents for dynamic and large action spaces.

Method: The authors provide (1) a formal framework via contextual bandits and SC-LinUCB, (2) parallel empirical evaluation with Large Language Models in static and non-stationary settings, and (3) propose the FiReAct pipeline validated on a 10,000-tool benchmark.

Result: The study demonstrates that SC leads to lower regret in decision-making, improved in-context learning capabilities for LLMs, and scalability in orchestrating a large number of tools.

Conclusion: Semantic Context significantly improves the efficiency and adaptability of orchestration agents, offering a robust methodology for scalable tool orchestration when integrated with Large Language Models.

Abstract: This paper demonstrates that Semantic Context (SC), leveraging descriptive
tool information, is a foundational component for robust tool orchestration.
Our contributions are threefold. First, we provide a theoretical foundation
using contextual bandits, introducing SC-LinUCB and proving it achieves lower
regret and adapts favourably in dynamic action spaces. Second, we provide
parallel empirical validation with Large Language Models, showing that SC is
critical for successful in-context learning in both static (efficient learning)
and non-stationary (robust adaptation) settings. Third, we propose the FiReAct
pipeline, and demonstrate on a benchmark with over 10,000 tools that SC-based
retrieval enables an LLM to effectively orchestrate over a large action space.
These findings provide a comprehensive guide to building more sample-efficient,
adaptive, and scalable orchestration agents.

</details>


### [209] [Offline Reinforcement Learning with Wasserstein Regularization via Optimal Transport Maps](https://arxiv.org/abs/2507.10843)
*Motoki Omura,Yusuke Mukuta,Kazuki Ota,Takayuki Osa,Tatsuya Harada*

Main category: cs.LG

TL;DR: This paper introduces a new offline RL approach using Wasserstein distance and input-convex neural networks (ICNNs) for stable learning without adversarial training.


<details>
  <summary>Details</summary>
Motivation: To address the issue of distributional shift in offline RL, which leads to unreliable actions due to deviations from dataset distributions.

Method: The method leverages Wasserstein distance and ICNNs to compute optimal transport maps in a discriminator-free manner, avoiding adversarial training for regularization.

Result: The proposed approach achieves comparable or better performance than popular methods on the D4RL benchmark dataset.

Conclusion: Using Wasserstein distance and ICNNs offers a robust and stable approach to mitigating distributional shift in offline RL, with promising results on benchmarks.

Abstract: Offline reinforcement learning (RL) aims to learn an optimal policy from a
static dataset, making it particularly valuable in scenarios where data
collection is costly, such as robotics. A major challenge in offline RL is
distributional shift, where the learned policy deviates from the dataset
distribution, potentially leading to unreliable out-of-distribution actions. To
mitigate this issue, regularization techniques have been employed. While many
existing methods utilize density ratio-based measures, such as the
$f$-divergence, for regularization, we propose an approach that utilizes the
Wasserstein distance, which is robust to out-of-distribution data and captures
the similarity between actions. Our method employs input-convex neural networks
(ICNNs) to model optimal transport maps, enabling the computation of the
Wasserstein distance in a discriminator-free manner, thereby avoiding
adversarial training and ensuring stable learning. Our approach demonstrates
comparable or superior performance to widely used existing methods on the D4RL
benchmark dataset. The code is available at
https://github.com/motokiomura/Q-DOT .

</details>


### [210] [From Small to Large: A Graph Convolutional Network Approach for Solving Assortment Optimization Problems](https://arxiv.org/abs/2507.10834)
*Guokai Li,Pin Gao,Stefanus Jasin,Zizhuo Wang*

Main category: cs.LG

TL;DR: The paper proposes a Graph Convolutional Networks (GCNs)-based approach to solve the NP-hard assortment optimization problem efficiently under the mixed multinomial logit model.


<details>
  <summary>Details</summary>
Motivation: The NP-hard nature of assortment optimization, crucial in revenue management, necessitates innovative methods to handle this complex problem efficiently.

Method: A graph representation is developed for the problem, followed by training GCNs to recognize patterns in optimal assortments. Two inference policies are proposed, leveraging the generalization abilities of GCNs for varying input sizes.

Result: The approach achieves over 90% optimality on large-scale problems (up to 2,000 products) within seconds, outperforming existing heuristic methods. It also demonstrates effectiveness in a model-free setting using transaction data.

Conclusion: GCNs provide a promising avenue for addressing assortment optimization challenges, excelling in both constrained and model-free settings with efficiency and high performance.

Abstract: Assortment optimization involves selecting a subset of substitutable products
(subject to certain constraints) to maximize the expected revenue. It is a
classic problem in revenue management and finds applications across various
industries. However, the problem is usually NP-hard due to its combinatorial
and non-linear nature. In this work, we explore how graph concolutional
networks (GCNs) can be leveraged to efficiently solve constrained assortment
optimization under the mixed multinomial logit choice model. We first develop a
graph representation of the assortment problem, then train a GCN to learn the
patterns of optimal assortments, and lastly propose two inference policies
based on the GCN's output. Due to the GCN's inherent ability to generalize
across inputs of varying sizes, we can use a GCN trained on small-scale
instances to facilitate large-scale instances. Extensive numerical experiments
demonstrate that given a GCN trained on small-scale instances (e.g., with 20
products), the proposed policies can achieve superior performance (90%+
optimality) on large-scale instances (with up to 2,000 products) within
seconds, which outperform existing heuristic policies in both performance and
efficiency. Furthermore, we extend our framework to a model-free setting where
the underlying choice model is unknown but transaction data is available. We
also conduct numerical experiments to demonstrate the effectiveness and
efficiency of our proposed policies in this setting.

</details>


### [211] [Collaboration Promotes Group Resilience in Multi-Agent RL](https://arxiv.org/abs/2111.06614)
*Ilai Shraga,Guy Azran,Matthias Gerstgrasser,Ofir Abu,Jeffrey S. Rosenschein,Sarah Keren*

Main category: cs.LG

TL;DR: The paper introduces the concept of group resilience in multi-agent reinforcement learning (MARL) and shows collaborative approaches enhance resilience against environmental changes.


<details>
  <summary>Details</summary>
Motivation: To address the need for RL agents to adapt and remain effective in dynamic scenarios, extending the concept of resilience from single-agent to multi-agent settings.

Method: The authors formalize group resilience, hypothesize collaboration as a key factor, and empirically test various collaboration protocols in MARL settings.

Result: Collaborative approaches consistently demonstrate higher group resilience compared to non-collaborative counterparts.

Conclusion: Collaboration among agents is crucial for achieving group resilience in MARL settings, as supported by empirical findings.

Abstract: To effectively operate in various dynamic scenarios, RL agents must be
resilient to unexpected changes in their environment. Previous work on this
form of resilience has focused on single-agent settings. In this work, we
introduce and formalize a multi-agent variant of resilience, which we term
group resilience. We further hypothesize that collaboration with other agents
is key to achieving group resilience; collaborating agents adapt better to
environmental perturbations in multi-agent reinforcement learning (MARL)
settings. We test our hypothesis empirically by evaluating different
collaboration protocols and examining their effect on group resilience. Our
experiments show that all the examined collaborative approaches achieve higher
group resilience than their non-collaborative counterparts.

</details>


### [212] [Visually grounded emotion regulation via diffusion models and user-driven reappraisal](https://arxiv.org/abs/2507.10861)
*Edoardo Pinzuti,Oliver Tüscher,André Ferreira Castro*

Main category: cs.LG

TL;DR: This paper introduces a novel method to assist emotion regulation by integrating text-to-image diffusion models into cognitive reappraisal and finds it significantly reduces negative affect.


<details>
  <summary>Details</summary>
Motivation: Standard cognitive reappraisal techniques demand higher-order cognitive and linguistic processes, which are impaired in individuals with trauma or depression, reducing intervention effectiveness.

Method: The authors developed a system where users reappraise negative images verbally, transforming them into supportive visuals using fine-tuned diffusion models, and conducted an experiment with 20 subjects performing a cognitive emotion regulation task with or without AI visual feedback.

Result: The study found that AI-assisted cognitive reappraisal significantly reduced negative affect compared to control conditions. Alignment between user reappraisals and AI-generated visuals improved affective relief.

Conclusion: Incorporating generative visual feedback into cognitive reappraisal enhances emotional regulation, presenting a promising avenue for affective computing and therapeutic applications.

Abstract: Cognitive reappraisal is a key strategy in emotion regulation, involving
reinterpretation of emotionally charged stimuli to alter affective responses.
Despite its central role in clinical and cognitive science, real-world
reappraisal interventions remain cognitively demanding, abstract, and primarily
verbal. This reliance on higher-order cognitive and linguistic processes is
often impaired in individuals with trauma or depression, limiting the
effectiveness of standard approaches. Here, we propose a novel, visually based
augmentation of cognitive reappraisal by integrating large-scale text-to-image
diffusion models into the emotional regulation process. Specifically, we
introduce a system in which users reinterpret emotionally negative images via
spoken reappraisals, which are transformed into supportive, emotionally
congruent visualizations using stable diffusion models with a fine-tuned
IP-adapter. This generative transformation visually instantiates users'
reappraisals while maintaining structural similarity to the original stimuli,
externalizing and reinforcing regulatory intent. To test this approach, we
conducted a within-subject experiment (N = 20) using a modified cognitive
emotion regulation (CER) task. Participants reappraised or described aversive
images from the International Affective Picture System (IAPS), with or without
AI-generated visual feedback. Results show that AI-assisted reappraisal
significantly reduced negative affect compared to both non-AI and control
conditions. Further analyses reveal that sentiment alignment between
participant reappraisals and generated images correlates with affective relief,
suggesting that multimodal coherence enhances regulatory efficacy. These
findings demonstrate that generative visual input can support cogitive
reappraisal and open new directions at the intersection of generative AI,
affective computing, and therapeutic technology.

</details>


### [213] [GALDS: A Graph-Autoencoder-based Latent Dynamics Surrogate model to predict neurite material transport](https://arxiv.org/abs/2507.10871)
*Tsung Yeh Hsieh,Yongjie Jessica Zhang*

Main category: cs.LG

TL;DR: The paper introduces GALDS, a Graph-Autoencoder-based model to efficiently simulate material transport in neuron networks, achieving high accuracy and significant speed improvement.


<details>
  <summary>Details</summary>
Motivation: Understanding material transport in neuron networks requires computationally efficient methods due to the complex tree-like structures involved.

Method: The model uses a graph autoencoder to encode latent space representations and a Neural ODE-inspired framework to predict dynamics with reduced error and training requirements.

Result: GALDS achieves a mean relative error of 3%, maximum relative error under 8%, and a 10-fold speed up over traditional models.

Conclusion: GALDS offers a streamlined, effective approach to simulating neural transport dynamics, addressing computational challenges while maintaining high accuracy.

Abstract: Neurons exhibit intricate geometries within their neurite networks, which
play a crucial role in processes such as signaling and nutrient transport.
Accurate simulation of material transport in the networks is essential for
understanding these biological phenomena but poses significant computational
challenges because of the complex tree-like structures involved. Traditional
approaches are time-intensive and resource-demanding, yet the inherent
properties of neuron trees, which consists primarily of pipes with steady-state
parabolic velocity profiles and bifurcations, provide opportunities for
computational optimization. To address these challenges, we propose a
Graph-Autoencoder-based Latent Dynamics Surrogate (GALDS) model, which is
specifically designed to streamline the simulation of material transport in
neural trees. GALDS employs a graph autoencoder to encode latent
representations of the network's geometry, velocity fields, and concentration
profiles. These latent space representations are then assembled into a global
graph, which is subsequently used to predict system dynamics in the latent
space via a trained graph latent space system dynamic model, inspired by the
Neural Ordinary Differential Equations (Neural ODEs) concept. The integration
of an autoencoder allows for the use of smaller graph neural network models
with reduced training data requirements. Furthermore, the Neural ODE component
effectively mitigates the issue of error accumulation commonly encountered in
recurrent neural networks. The effectiveness of the GALDS model is demonstrated
through results on eight unseen geometries and four abnormal transport
examples, where our approach achieves mean relative error of 3% with maximum
relative error <8% and demonstrates a 10-fold speed improvement compared to
previous surrogate model approaches.

</details>


### [214] [Domain-Adaptive Small Language Models for Structured Tax Code Prediction](https://arxiv.org/abs/2507.10880)
*Souvik Nath,Sumit Wadhwa,Luiz Perez*

Main category: cs.LG

TL;DR: The paper proposes a domain-adaptive small language model (SLM) using encoder-decoder architecture to enhance hierarchical tax code prediction for improved tax compliance.


<details>
  <summary>Details</summary>
Motivation: Multinational firms face challenges in accurately predicting product and service tax codes due to diverse jurisdictional regulations, where errors can lead to financial penalties.

Method: An encoder-decoder small language model (SLM) is developed to sequentially predict hierarchical tax codes, leveraging unstructured product and service data.

Result: The domain-adaptive encoder-decoder SLM outperforms flat classifiers and standalone decoder-only or encoder-only models in structured tax code prediction tasks.

Conclusion: Encoder-decoder SLMs are effective for hierarchical sequence generation in tax codes and can be scaled to other globally mandated tax code systems.

Abstract: Every day, multinational firms process thousands of transactions, each of
which must adhere to tax regulations that vary by jurisdiction and are often
nuanced. The determination of product and service tax codes, such as HSN or SAC
is a major use case in Tax compliance. An accurate determination of such codes
is imperative to avoid any tax penalties. This paper proposes a domain-adaptive
small language model (SLM) with an encoder-decoder architecture for the
enhanced prediction of product and service tax codes. In this approach, we
address the problem of predicting hierarchical tax code sequences using
unstructured product and services data. We employ an SLM based upon
encoder-decoder architecture as this enables sequential generation of tax codes
to capture the hierarchical dependencies present within the tax codes. Our
experiments demonstrate that encoder-decoder SLMs can be successfully applied
to the sequential prediction of structured tax codes, a domain that remains
comparatively unexplored in current NLP research. In this paper, we demonstrate
the superior performance of the domain-adaptive encoder-decoder SLMs over flat
classifiers when applied to the Harmonized System of Nomenclature (HSN), and
achieve superior results compared to decoder-only and encoder-only
architectures for structured sequence generation tasks. This approach can also
be scaled to other government-mandated tax commodity codes, such as United
Nations Standard Products and Services Codes (UNSPSC), or Brazil's Nomenclatura
Comum do Mercosul (NCM).

</details>


### [215] [Learning from Imperfect Data: Robust Inference of Dynamic Systems using Simulation-based Generative Model](https://arxiv.org/abs/2507.10884)
*Hyunwoo Cho,Hyeontae Jo,Hyung Ju Hwang*

Main category: cs.LG

TL;DR: The paper introduces SiGMoID, a method combining physics-informed neural networks and Wasserstein GANs, to infer system dynamics from imperfect data.


<details>
  <summary>Details</summary>
Motivation: To address challenges in inferring nonlinear dynamic models from noisy, sparse, or partially observable data.

Method: SiGMoID integrates physics-informed neural networks (with hyper-networks for ODE solving) and Wasserstein generative adversarial networks (to estimate ODE parameters via noisy data distributions).

Result: SiGMoID successfully quantifies data noise, estimates system parameters, and infers unobserved system components in realistic experimental setups.

Conclusion: SiGMoID demonstrates effectiveness and broad applicability in scientific and engineering domains by accurately uncovering system dynamics from imperfect datasets.

Abstract: System inference for nonlinear dynamic models, represented by ordinary
differential equations (ODEs), remains a significant challenge in many fields,
particularly when the data are noisy, sparse, or partially observable. In this
paper, we propose a Simulation-based Generative Model for Imperfect Data
(SiGMoID) that enables precise and robust inference for dynamic systems. The
proposed approach integrates two key methods: (1) physics-informed neural
networks with hyper-networks that constructs an ODE solver, and (2) Wasserstein
generative adversarial networks that estimates ODE parameters by effectively
capturing noisy data distributions. We demonstrate that SiGMoID quantifies data
noise, estimates system parameters, and infers unobserved system components.
Its effectiveness is validated validated through realistic experimental
examples, showcasing its broad applicability in various domains, from
scientific research to engineered systems, and enabling the discovery of full
system dynamics.

</details>


### [216] [How to Protect Models against Adversarial Unlearning?](https://arxiv.org/abs/2507.10886)
*Patryk Jasiorski,Marek Klonowski,Michał Woźniak*

Main category: cs.LG

TL;DR: The paper addresses adversarial unlearning, where malicious requests aim to degrade AI model performance, and proposes a method to mitigate performance degradation.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to comply with legal requirements (like AI Act and GDPR) and rectify issues like toxicity, bias, and malicious/obsolete data in AI models while safeguarding against performance degradation.

Method: The authors investigate the impact of adversarial unlearning on model performance, focusing on factors like the backbone model and data selection strategies, and propose a protection method against such adversarial attacks.

Result: The study reveals how adversarial unlearning exploits various factors to degrade model performance and demonstrates a new technique to protect models from such effects.

Conclusion: The authors emphasize the importance of safeguarding AI models from adversarial unlearning attacks while complying with legal and ethical considerations, showcasing their method's effectiveness.

Abstract: AI models need to be unlearned to fulfill the requirements of legal acts such
as the AI Act or GDPR, and also because of the need to remove toxic content,
debiasing, the impact of malicious instances, or changes in the data
distribution structure in which a model works. Unfortunately, removing
knowledge may cause undesirable side effects, such as a deterioration in model
performance. In this paper, we investigate the problem of adversarial
unlearning, where a malicious party intentionally sends unlearn requests to
deteriorate the model's performance maximally. We show that this phenomenon and
the adversary's capabilities depend on many factors, primarily on the backbone
model itself and strategy/limitations in selecting data to be unlearned. The
main result of this work is a new method of protecting model performance from
these side effects, both in the case of unlearned behavior resulting from
spontaneous processes and adversary actions.

</details>


### [217] [Outbound Modeling for Inventory Management](https://arxiv.org/abs/2507.10890)
*Riccardo Savorgnan,Udaya Ghai,Carson Eisenach,Dean Foster*

Main category: cs.LG

TL;DR: This paper proposes a probabilistic forecasting model for predicting inventory drain and shipping costs in warehouses, particularly for use in Reinforcement Learning (RL) simulations.


<details>
  <summary>Details</summary>
Motivation: Accurate modeling of inventory drain and shipping costs is critical for regional inventory planning and necessary for RL-based control policy development.

Method: The authors framed the problem as forecasting joint distributions, validated the model through counterfactual inventory states induced by RL policies, and aimed for robustness against off-policy scenarios.

Result: Preliminary results show the model demonstrates accuracy within the in-distribution setting.

Conclusion: The proposed probabilistic model is promising for accurate drain forecasting and shipping cost prediction in RL environments. Robustness validation using counterfactual inventory states is innovative.

Abstract: We study the problem of forecasting the number of units fulfilled (or
``drained'') from each inventory warehouse to meet customer demand, along with
the associated outbound shipping costs. The actual drain and shipping costs are
determined by complex production systems that manage the planning and execution
of customers' orders fulfillment, i.e. from where and how to ship a unit to be
delivered to a customer. Accurately modeling these processes is critical for
regional inventory planning, especially when using Reinforcement Learning (RL)
to develop control policies. For the RL usecase, a drain model is incorporated
into a simulator to produce long rollouts, which we desire to be
differentiable. While simulating the calls to the internal software systems can
be used to recover this transition, they are non-differentiable and too slow
and costly to run within an RL training environment. Accordingly, we frame this
as a probabilistic forecasting problem, modeling the joint distribution of
outbound drain and shipping costs across all warehouses at each time period,
conditioned on inventory positions and exogenous customer demand. To ensure
robustness in an RL environment, the model must handle out-of-distribution
scenarios that arise from off-policy trajectories. We propose a validation
scheme that leverages production systems to evaluate the drain model on
counterfactual inventory states induced by RL policies. Preliminary results
demonstrate the model's accuracy within the in-distribution setting.

</details>


### [218] [Class-Proportional Coreset Selection for Difficulty-Separable Data](https://arxiv.org/abs/2507.10904)
*Elisa Tsai,Haizhong Zheng,Atul Prakash*

Main category: cs.LG

TL;DR: The study introduces Class Difficulty Separability Coefficient (CDSC) to address shortcomings of class-agnostic dataset pruning methods in fields like network intrusion detection and medical imaging.


<details>
  <summary>Details</summary>
Motivation: Existing one-shot coreset selection methods assume class-wise homogeneity in data difficulty, which leads to performance issues in scenarios where class difficulties vary.

Method: The authors propose class-proportional variants of sampling strategies and introduce CDSC as a metric for quantifying class-difficulty separability. They validate the methods using datasets across security and medical domains.

Result: The class-proportional methods consistently outperform class-agnostic baselines, showing improved accuracy, precision, and recall in extreme pruning scenarios.

Conclusion: Explicitly accounting for class-difficulty separability enhances data pruning, resulting in more reliable and robust machine learning systems, especially in noisy, imbalanced, and high-stakes situations.

Abstract: High-quality training data is essential for building reliable and efficient
machine learning systems. One-shot coreset selection addresses this by pruning
the dataset while maintaining or even improving model performance, often
relying on training-dynamics-based data difficulty scores. However, most
existing methods implicitly assume class-wise homogeneity in data difficulty,
overlooking variation in data difficulty across different classes.
  In this work, we challenge this assumption by showing that, in domains such
as network intrusion detection and medical imaging, data difficulty often
clusters by class. We formalize this as class-difficulty separability and
introduce the Class Difficulty Separability Coefficient (CDSC) as a
quantitative measure. We demonstrate that high CDSC values correlate with
performance degradation in class-agnostic coreset methods, which tend to
overrepresent easy majority classes while neglecting rare but informative ones.
  To address this, we introduce class-proportional variants of multiple
sampling strategies. Evaluated on five diverse datasets spanning security and
medical domains, our methods consistently achieve state-of-the-art data
efficiency. For instance, on CTU-13, at an extreme 99% pruning rate, a
class-proportional variant of Coverage-centric Coreset Selection (CCS-CP) shows
remarkable stability, with accuracy dropping only 2.58%, precision 0.49%, and
recall 0.19%. In contrast, the class-agnostic CCS baseline, the next best
method, suffers sharper declines of 7.59% in accuracy, 4.57% in precision, and
4.11% in recall.
  We further show that aggressive pruning enhances generalization in noisy,
imbalanced, and large-scale datasets. Our results underscore that explicitly
modeling class-difficulty separability leads to more effective, robust, and
generalizable data pruning, particularly in high-stakes scenarios.

</details>


### [219] [Diffusion Decoding for Peptide De Novo Sequencing](https://arxiv.org/abs/2507.10955)
*Chi-en Amy Tai,Alexander Wong*

Main category: cs.LG

TL;DR: This study explores diffusion decoders for peptide de novo sequencing, improving amino acid recall by replacing traditional autoregressive decoders.


<details>
  <summary>Details</summary>
Motivation: The objective is to mitigate cascading errors and enhance the accuracy of amino acid sequence prediction, challenging traditional autoregressive models.

Method: The paper tests three diffusion decoder designs, knapsack beam search, and loss functions, focusing on adapting diffusion decoders for sequence generation.

Result: Diffusion decoders paired with the DINOISER loss function improved amino acid recall by 0.373, but knapsack beam search and simple decoder replacement showed no significant improvement.

Conclusion: Diffusion decoders exhibit potential to enhance sensitivity and accuracy in peptide de novo sequencing, marking a step forward in this domain.

Abstract: Peptide de novo sequencing is a method used to reconstruct amino acid
sequences from tandem mass spectrometry data without relying on existing
protein sequence databases. Traditional deep learning approaches, such as
Casanovo, mainly utilize autoregressive decoders and predict amino acids
sequentially. Subsequently, they encounter cascading errors and fail to
leverage high-confidence regions effectively. To address these issues, this
paper investigates using diffusion decoders adapted for the discrete data
domain. These decoders provide a different approach, allowing sequence
generation to start from any peptide segment, thereby enhancing prediction
accuracy. We experiment with three different diffusion decoder designs,
knapsack beam search, and various loss functions. We find knapsack beam search
did not improve performance metrics and simply replacing the transformer
decoder with a diffusion decoder lowered performance. Although peptide
precision and recall were still 0, the best diffusion decoder design with the
DINOISER loss function obtained a statistically significant improvement in
amino acid recall by 0.373 compared to the baseline autoregressive
decoder-based Casanovo model. These findings highlight the potential of
diffusion decoders to not only enhance model sensitivity but also drive
significant advancements in peptide de novo sequencing.

</details>


### [220] [Physics-Informed Neural Networks For Semiconductor Film Deposition: A Review](https://arxiv.org/abs/2507.10983)
*Tao Han,Zahra Taheri,Hyunwoong Ko*

Main category: cs.LG

TL;DR: This paper reviews machine learning (ML), specifically Physics-Informed Neural Networks (PINNs), for improving semiconductor film deposition. It identifies limitations, research gaps, and future directions for enhancing process control and quality.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to improve semiconductor manufacturing processes, particularly film deposition, by leveraging advancements in ML and PINNs. These techniques promise to address challenges in process control, quality assurance, and predictive modeling.

Method: A structured thematic analysis is performed on existing ML applications in the domain of semiconductor film deposition. Special attention is given to state-of-the-art PINNs and their ability to integrate physical knowledge and governing laws.

Result: The study identifies key trends, limitations, and gaps in current ML methodologies for film deposition. It also underscores the potential of PINNs to significantly improve process robustness, accuracy, and interpretability.

Conclusion: Integrating physics-informed ML frameworks such as PINNs can transform semiconductor film deposition processes by enhancing precision, scalability, and operational efficiency, with clear pathways for future research outlined.

Abstract: Semiconductor manufacturing relies heavily on film deposition processes, such
as Chemical Vapor Deposition and Physical Vapor Deposition. These complex
processes require precise control to achieve film uniformity, proper adhesion,
and desired functionality. Recent advancements in Physics-Informed Neural
Networks (PINNs), an innovative machine learning (ML) approach, have shown
significant promise in addressing challenges related to process control,
quality assurance, and predictive modeling within semiconductor film deposition
and other manufacturing domains. This paper provides a comprehensive review of
ML applications targeted at semiconductor film deposition processes. Through a
thematic analysis, we identify key trends, existing limitations, and research
gaps, offering insights into both the advantages and constraints of current
methodologies. Our structured analysis aims to highlight the potential
integration of these ML techniques to enhance interpretability, accuracy, and
robustness in film deposition processes. Additionally, we examine
state-of-the-art PINN methods, discussing strategies for embedding physical
knowledge, governing laws, and partial differential equations into advanced
neural network architectures tailored for semiconductor manufacturing. Based on
this detailed review, we propose novel research directions that integrate the
strengths of PINNs to significantly advance film deposition processes. The
contributions of this study include establishing a clear pathway for future
research in integrating physics-informed ML frameworks, addressing existing
methodological gaps, and ultimately improving precision, scalability, and
operational efficiency within semiconductor manufacturing.

</details>


### [221] [StellarF: A Lora-Adapter Integrated Large Model Framework for Stellar Flare Forecasting with Historical & Statistical Data](https://arxiv.org/abs/2507.10986)
*Tianyu Su,Zhiqiang Zou,Ali Luo,Xiao Kong,Qingyu Lu,Min Li*

Main category: cs.LG

TL;DR: This study introduces StellarF, a new model for forecasting stellar flares using innovative methods to achieve superior accuracy on custom datasets.


<details>
  <summary>Details</summary>
Motivation: The limited number of recorded stellar flare events and the lack of advanced predictive models hinder the understanding of stellar activity.

Method: The study presents StellarF, a model using Low-Rank and Adapter techniques for efficient learning, combining statistical and historical data modules for pattern recognition.

Result: Tests on datasets from Kepler and TESS light curves show that StellarF outperforms existing methods, achieving state-of-the-art results.

Conclusion: StellarF offers a significant advancement in stellar flare prediction and sets a foundation for future astrophysical and interdisciplinary research.

Abstract: Stellar flare forecasting, a critical research frontier in astronomy, offers
profound insights into stellar activity. However, the field is constrained by
both the sparsity of recorded flare events and the absence of domain-specific
large-scale predictive models. To address these challenges, this study
introduces StellarF (Stellar Flare Forecasting), a novel large model that
leverages Low-Rank (LoRA) and Adapter techniques to parameter-efficient
learning for stellar flare forecasting. At its core, StellarF integrates an
flare statistical information module with a historical flare record module,
enabling multi-scale pattern recognition from observational data. Extensive
experiments on our self-constructed datasets (derived from Kepler and TESS
light curves) demonstrate that StellarF achieves state-of-the-art performance
compared to existing methods. The proposed prediction paradigm establishes a
novel methodological framework for advancing astrophysical research and
cross-disciplinary applications.

</details>


### [222] [High-Throughput Distributed Reinforcement Learning via Adaptive Policy Synchronization](https://arxiv.org/abs/2507.10990)
*Rodney Lafuente-Mercado*

Main category: cs.LG

TL;DR: The paper introduces ClusterEnv, a modular framework for distributing environment simulation in reinforcement learning. It integrates seamlessly with existing RL pipelines and introduces improvements like the DETACH pattern and the Adaptive Actor Policy Synchronization (AAPS).


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of existing RL frameworks that merge simulation, training, and orchestration into monolithic systems, limiting modularity and scalability.

Method: ClusterEnv uses the DETACH pattern to separate environment simulation from training, delegating reset() and step() operations to remote workers. AAPS is proposed as a mechanism to handle policy staleness efficiently by reducing synchronization overhead while maintaining performance.

Result: Experiments on discrete control tasks show that ClusterEnv supports various RL methods and that AAPS improves sample efficiency while reducing the weight updates required.

Conclusion: ClusterEnv enhances modularity, scalability, and efficiency in RL workloads, achieving high performance with minimal code adjustments and offering a learner-agnostic and distributed framework.

Abstract: Scaling reinforcement learning (RL) workloads often requires distributing
environment simulation across compute clusters. Existing frameworks entangle
simulation, learning logic, and orchestration into monolithic systems, limiting
modularity and reusability. We present ClusterEnv, a lightweight,
learner-agnostic interface for distributed environment execution that mirrors
the Gymnasium API. ClusterEnv introduces the DETACH pattern, which decouples
simulation from training by offloading reset() and step() operations to remote
workers while keeping learning centralized. To address policy staleness in
distributed execution, we propose Adaptive Actor Policy Synchronization (AAPS),
a divergence-triggered update mechanism that reduces synchronization overhead
without sacrificing performance. ClusterEnv integrates cleanly into existing RL
pipelines, supports both on-policy and off-policy methods, and requires minimal
code changes. Experiments on discrete control tasks demonstrate that AAPS
achieves high sample efficiency with significantly fewer weight updates. Source
code is available at https://github.com/rodlaf/ClusterEnv.

</details>


### [223] [Misalignment from Treating Means as Ends](https://arxiv.org/abs/2507.10995)
*Henrik Marklund,Alex Infanger,Benjamin Van Roy*

Main category: cs.LG

TL;DR: Reward functions are often imperfect due to conflation of terminal goals and instrumental goals, leading to misalignment in reinforcement learning outcomes.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of misalignment in reinforcement learning caused by the conflation of human's instrumental and terminal goals in reward functions.

Method: The authors formulate a simple example to illustrate the severe consequences of even slight conflation of instrumental and terminal goals on reinforcement learning outcomes.

Result: The research demonstrates that optimizing a misspecified reward function leads to poor performance when measured by the true reward function, due to sensitivity to goal conflation.

Conclusion: Highlighting the issue of goal misalignment, the paper underscores the importance of distinguishing terminal goals from instrumental goals to improve reward learning in reinforcement learning systems.

Abstract: Reward functions, learned or manually specified, are rarely perfect. Instead
of accurately expressing human goals, these reward functions are often
distorted by human beliefs about how best to achieve those goals. Specifically,
these reward functions often express a combination of the human's terminal
goals -- those which are ends in themselves -- and the human's instrumental
goals -- those which are means to an end. We formulate a simple example in
which even slight conflation of instrumental and terminal goals results in
severe misalignment: optimizing the misspecified reward function results in
poor performance when measured by the true reward function. This example
distills the essential properties of environments that make reinforcement
learning highly sensitive to conflation of instrumental and terminal goals. We
discuss how this issue can arise with a common approach to reward learning and
how it can manifest in real environments.

</details>


### [224] [Crafting Imperceptible On-Manifold Adversarial Attacks for Tabular Data](https://arxiv.org/abs/2507.10998)
*Zhipeng He,Alexander Stevens,Chun Ouyang,Johannes De Smedt,Alistair Barros,Catarina Moreira*

Main category: cs.LG

TL;DR: This paper addresses the challenge of creating adversarial attacks on tabular data by proposing a latent space perturbation framework using a mixed-input Variational Autoencoder (VAE), which preserves statistical consistency in the data.


<details>
  <summary>Details</summary>
Motivation: Adversarial attacks on tabular data face unique difficulties due to the heterogeneous nature of categorical and numerical features, as well as the lack of intuitive similarity metrics for imperceptible modifications, often leading to detectable, out-of-distribution examples.

Method: The authors introduce a mixed-input VAE that integrates both categorical embeddings and numerical features into a unified latent space. Perturbations within this space maintain statistical consistency. A novel metric, In-Distribution Success Rate (IDSR), is proposed to evaluate the effectiveness of the adversarial examples.

Result: The proposed method demonstrates lower outlier rates and higher consistency compared to traditional input-space attacks and other VAE-based methods. Experiments across six datasets and three model architectures confirm its effectiveness.

Conclusion: This study emphasizes the importance of generating on-manifold adversarial examples for tabular data, showcasing the advantages of the proposed VAE-based approach for practical applications. The framework performs well, especially when sufficient training data is available.

Abstract: Adversarial attacks on tabular data present fundamental challenges distinct
from image or text domains due to the heterogeneous nature of mixed categorical
and numerical features. Unlike images where pixel perturbations maintain visual
similarity, tabular data lacks intuitive similarity metrics, making it
difficult to define imperceptible modifications. Additionally, traditional
gradient-based methods prioritise $\ell_p$-norm constraints, often producing
adversarial examples that deviate from the original data distributions, making
them detectable. We propose a latent space perturbation framework using a
mixed-input Variational Autoencoder (VAE) to generate imperceptible adversarial
examples. The proposed VAE integrates categorical embeddings and numerical
features into a unified latent manifold, enabling perturbations that preserve
statistical consistency. We specify In-Distribution Success Rate (IDSR) to
measure the proportion of adversarial examples that remain statistically
indistinguishable from the input distribution. Evaluation across six publicly
available datasets and three model architectures demonstrates that our method
achieves substantially lower outlier rates and more consistent performance
compared to traditional input-space attacks and other VAE-based methods adapted
from image domain approaches. Our comprehensive analysis includes
hyperparameter sensitivity, sparsity control mechanisms, and generative
architectural comparisons, revealing that VAE-based attacks depend critically
on reconstruction quality but offer superior practical utility when sufficient
training data is available. This work highlights the importance of on-manifold
perturbations for realistic adversarial attacks on tabular data, offering a
robust approach for practical deployment. The source code can be accessed
through https://github.com/ZhipengHe/VAE-TabAttack.

</details>


### [225] [AdaMuon: Adaptive Muon Optimizer](https://arxiv.org/abs/2507.11005)
*Chongjie Si,Debing Zhang,Wei Shen*

Main category: cs.LG

TL;DR: AdaMuon improves upon the Muon optimizer with adaptive learning-rate features for better efficiency and stability.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in current optimization methods and improve efficiency and adaptability in large-scale model training.

Method: AdaMuon combines second-moment modulation for update adaptability and RMS-aligned rescaling for regulating update magnitude.

Result: Empirical results demonstrate faster convergence and improved stability with AdaMuon across various model scales.

Conclusion: AdaMuon offers superior performance over Muon without adding complexity, making it a practical enhancement for training pipelines.

Abstract: We propose AdaMuon, an adaptive learning-rate framework built upon the
recently validated Muon optimizer, which has demonstrated substantial
efficiency gains over AdamW in large-scale model training. AdaMuon augments
Muon with two mutually dependent modules: (1) a per-parameter second-moment
modulation that captures orthogonal gradient updates to ensure update-level
adaptivity, and (2) a RMS-aligned rescaling that regulates the overall update
magnitude by aligning it with the intrinsic structure of the parameter space.
Empirical results on multiple model scales and learning-rate regimes confirm
that AdaMuon consistently outperforms the original Muon, delivering higher
acceleration in convergence while maintaining training stability. Our method
introduces no additional tuning burden and can be seamlessly integrated into
existing Muon training pipelines.

</details>


### [226] [Leveraging Advanced Machine Learning to Predict Turbulence Dynamics from Temperature Observations at an Experimental Prescribed Fire](https://arxiv.org/abs/2507.11012)
*Dipak Dulal,Joseph J. Charney,Michael R. Gallagher,Pitambar Acharya,Carmeliza Navasca,Nicholas S. Skowronski*

Main category: cs.LG

TL;DR: The paper investigates the use of machine learning models to predict turbulent kinetic energy (TKE) from temperature data during a controlled fire experiment. Results showed that these models could successfully make predictions, even with weak correlations.


<details>
  <summary>Details</summary>
Motivation: To understand and predict relationships between temperature and airflow processes in fire environments, improving fire operations strategies and fire/smoke modeling.

Method: Temperature profiles and turbulence data were collected during a controlled burn. Machine learning models like Deep Neural Networks, Random Forest, Gradient Boosting, and Gaussian Process Regressor were applied to analyze the data.

Result: Machine learning models successfully predicted TKE, identifying patterns and relationships despite weak correlation between variables. Regression models performed particularly well.

Conclusion: The study highlights a novel application of machine learning to link temperature with airflow dynamics in fire environments, aiding in refining fire operations and improving modeling practices.

Abstract: This study explores the potential for predicting turbulent kinetic energy
(TKE) from more readily acquired temperature data using temperature profiles
and turbulence data collected concurrently at 10 Hz during a small experimental
prescribed burn in the New Jersey Pine Barrens. Machine learning models,
including Deep Neural Networks, Random Forest Regressor, Gradient Boosting, and
Gaussian Process Regressor, were employed to assess the potential to predict
TKE from temperature perturbations and explore temporal and spatial dynamics of
correlations. Data visualization and correlation analyses revealed patterns and
relationships between thermocouple temperatures and TKE, providing insight into
the underlying dynamics. More accurate predictions of TKE were achieved by
employing various machine learning models despite a weak correlation between
the predictors and the target variable. The results demonstrate significant
success, particularly from regression models, in accurately predicting the TKE.
The findings of this study demonstrate a novel numerical approach to
identifying new relationships between temperature and airflow processes in and
around the fire environment. These relationships can help refine our
understanding of combustion environment processes and the coupling and
decoupling of fire environment processes necessary for improving fire
operations strategy and fire and smoke model predictions. The findings of this
study additionally highlight the valuable role of machine learning techniques
in analyzing the complex large datasets of the fire environments, showcasing
their potential to advance fire research and management practices.

</details>


### [227] [First-Order Error Matters: Accurate Compensation for Quantized Large Language Models](https://arxiv.org/abs/2507.11017)
*Xingyu Zheng,Haotong Qin,Yuye Li,Jiakai Wang,Jinyang Guo,Michele Magno,Xianglong Liu*

Main category: cs.LG

TL;DR: This paper introduces FOEM, a novel post-training quantization (PTQ) method that improves upon existing approaches by compensating quantization errors with first-order gradient terms, achieving superior model performance in compressed formats.


<details>
  <summary>Details</summary>
Motivation: Limitations in current PTQ techniques, specifically reliance on flawed assumptions about quantization error, motivated the development of an improved method.

Method: FOEM leverages first-order gradient terms for quantization error compensation, approximates gradients through weight differences, and utilizes precomputed Cholesky factors for efficient real-time computations while avoiding expensive backpropagation.

Result: FOEM significantly outperforms classical GPTQ, achieving reductions in perplexity and notable accuracy improvements in 3-bit and other quantization setups while approaching full-precision model performance.

Conclusion: FOEM provides robust model compression with minimal accuracy loss, demonstrating its utility across varied settings and compatibility with state-of-the-art techniques like GPTAQ and SpinQuant.

Abstract: Post-training quantization (PTQ) offers an efficient approach to compressing
large language models (LLMs), significantly reducing memory access and
computational costs. Existing compensation-based weight calibration methods
often rely on a second-order Taylor expansion to model quantization error,
under the assumption that the first-order term is negligible in well-trained
full-precision models. However, we reveal that the progressive compensation
process introduces accumulated first-order deviations between latent weights
and their full-precision counterparts, making this assumption fundamentally
flawed. To address this, we propose FOEM, a novel PTQ method that explicitly
incorporates first-order gradient terms to improve quantization error
compensation. FOEM approximates gradients by directly computing the difference
between latent and full-precision weights, avoiding the high cost and limited
generalization of backpropagation-based gradient computation. This approach
introduces minimal additional computational overhead. Moreover, FOEM leverages
precomputed Cholesky factors to efficiently recover the inverse of Hessian
submatrices in real time. Extensive experiments across a wide range of models
and benchmarks demonstrate that FOEM consistently outperforms the classical
GPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of
Llama3-8B by 89.6%, and improves the 5-shot MMLU accuracy of Llama3-70B from
51.7% to 74.9%, approaching the full-precision performance of 78.6%.
Furthermore, FOEM can be seamlessly integrated with advanced techniques such as
GPTAQ and SpinQuant, yielding additional improvements under the challenging
W4A4KV4 setting, and further narrowing the accuracy gap with full-precision
baselines beyond what current state-of-the-art methods achieve. The code is
available at https://github.com/Xingyu-Zheng/FOEM.

</details>


### [228] [Relative Entropy Pathwise Policy Optimization](https://arxiv.org/abs/2507.11019)
*Claas Voelcker,Axel Brunnbauer,Marcel Hussing,Michal Nauman,Pieter Abbeel,Eric Eaton,Radu Grosu,Amir-massoud Farahmand,Igor Gilitschenski*

Main category: cs.LG

TL;DR: The paper introduces Relative Entropy Pathwise Policy Optimization (REPPO), an efficient algorithm combining pathwise policy gradient's sample efficiency with the simplicity of on-policy learning for better stability and performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the high training variance of score-function policy gradients and the challenge of constructing accurate action-conditioned value functions in pathwise gradients.

Method: The authors present a novel on-policy algorithm, REPPO, leveraging value-gradient driven updates and balancing exploration with constrained policy updates to improve training stability and efficiency.

Result: Through experiments on GPU-parallelized benchmarks, REPPO achieves strong empirical performance, reduced sample requirements, faster training time, lower memory usage, and high robustness to hyperparameter variations.

Conclusion: REPPO successfully combines the strengths of pathwise policy gradients and on-policy learning, enabling efficient and more stable training for reinforcement learning tasks.

Abstract: Score-function policy gradients have delivered strong results in
game-playing, robotics and language-model fine-tuning. Yet its high-variance
often undermines training stability. On the other hand, pathwise policy
gradients alleviate the training variance, but are reliable only when driven by
an accurate action-conditioned value function which is notoriously hard to
train without relying on past off-policy data. In this paper, we discuss how to
construct a value-gradient driven, on-policy algorithm that allow training
Q-value models purely from on-policy data, unlocking the possibility of using
pathwise policy updates in the context of on-policy learning. We show how to
balance stochastic policies for exploration with constrained policy updates for
stable training, and evaluate important architectural components that
facilitate accurate value function learning. Building on these insights, we
propose Relative Entropy Pathwise Policy Optimization (REPPO), an efficient
on-policy algorithm that combines the sample-efficiency of pathwise policy
gradients with the simplicity and minimal memory footprint of standard
on-policy learning. We demonstrate that REPPO provides strong empirical
performance at decreased sample requirements, wall-clock time, memory footprint
as well as high hyperparameter robustness in a set of experiments on two
standard GPU-parallelized benchmarks.

</details>


### [229] [GATE: Graph Attention Neural Networks with Real-Time Edge Construction for Robust Indoor Localization using Mobile Embedded Devices](https://arxiv.org/abs/2507.11053)
*Danish Gufran,Sudeep Pasricha*

Main category: cs.LG

TL;DR: GATE enhances indoor localization using Wi-Fi RSS by addressing limitations in DL and GNN models, achieving better accuracy across diverse environments.


<details>
  <summary>Details</summary>
Motivation: To tackle indoor localization inaccuracies caused by real-world RSS noise, device heterogeneity, and GNN limitations.

Method: Introduced GATE framework with AHV, MDHV, and RTEC for adaptive graph representation and better noise handling.

Result: Evaluation showed GATE significantly reduces localization errors by up to 4.72x compared to state-of-the-art approaches.

Conclusion: GATE effectively improves generalization and accuracy for indoor localization tasks across various challenging scenarios.

Abstract: Accurate indoor localization is crucial for enabling spatial context in smart
environments and navigation systems. Wi-Fi Received Signal Strength (RSS)
fingerprinting is a widely used indoor localization approach due to its
compatibility with mobile embedded devices. Deep Learning (DL) models improve
accuracy in localization tasks by learning RSS variations across locations, but
they assume fingerprint vectors exist in a Euclidean space, failing to
incorporate spatial relationships and the non-uniform distribution of
real-world RSS noise. This results in poor generalization across heterogeneous
mobile devices, where variations in hardware and signal processing distort RSS
readings. Graph Neural Networks (GNNs) can improve upon conventional DL models
by encoding indoor locations as nodes and modeling their spatial and signal
relationships as edges. However, GNNs struggle with non-Euclidean noise
distributions and suffer from the GNN blind spot problem, leading to degraded
accuracy in environments with dense access points (APs). To address these
challenges, we propose GATE, a novel framework that constructs an adaptive
graph representation of fingerprint vectors while preserving an indoor
state-space topology, modeling the non-Euclidean structure of RSS noise to
mitigate environmental noise and address device heterogeneity. GATE introduces
1) a novel Attention Hyperspace Vector (AHV) for enhanced message passing, 2) a
novel Multi-Dimensional Hyperspace Vector (MDHV) to mitigate the GNN blind
spot, and 3) an new Real-Time Edge Construction (RTEC) approach for dynamic
graph adaptation. Extensive real-world evaluations across multiple indoor
spaces with varying path lengths, AP densities, and heterogeneous devices
demonstrate that GATE achieves 1.6x to 4.72x lower mean localization errors and
1.85x to 4.57x lower worst-case errors compared to state-of-the-art indoor
localization frameworks.

</details>


### [230] [A Distance Metric for Mixed Integer Programming Instances](https://arxiv.org/abs/2507.11063)
*Gwen Maudet,Grégoire Danoy*

Main category: cs.LG

TL;DR: This paper proposes a novel mathematical distance metric for comparing mixed-integer linear programming (MILP) instances, outperforming existing similarity metrics in both efficiency and class identification accuracy.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the lack of a precise and generalizable similarity metric for comparing MILP instances, which limits effective evaluation of heterogeneity and guidance to solvers, particularly in machine learning contexts.

Method: A mathematical distance metric inspired by the Earth mover's distance is introduced. This metric discretizes coefficients and variables into classes to evaluate mismatches. Two versions—exact and greedy—are tested for their effectiveness using the StrIPLIB dataset.

Result: The greedy variant of the proposed metric is nearly 200 times faster than the exact version while maintaining comparable accuracy. The method outperforms non-learned similarity metrics and matches the performance of supervised classifiers on classification tasks.

Conclusion: The proposed metric is an effective and efficient tool for MILP instance comparisons, offering a significant improvement over existing methods and utility for both unsupervised and supervised applications.

Abstract: Mixed-integer linear programming (MILP) is a powerful tool for addressing a
wide range of real-world problems, but it lacks a clear structure for comparing
instances. A reliable similarity metric could establish meaningful
relationships between instances, enabling more effective evaluation of instance
set heterogeneity and providing better guidance to solvers, particularly when
machine learning is involved. Existing similarity metrics often lack precision
in identifying instance classes or rely heavily on labeled data, which limits
their applicability and generalization. To bridge this gap, this paper
introduces the first mathematical distance metric for MILP instances, derived
directly from their mathematical formulations. By discretizing right-hand
sides, weights, and variables into classes, the proposed metric draws
inspiration from the Earth mover's distance to quantify mismatches in
weight-variable distributions for constraint comparisons. This approach
naturally extends to enable instance-level comparisons. We evaluate both an
exact and a greedy variant of our metric under various parameter settings,
using the StrIPLIB dataset. Results show that all components of the metric
contribute to class identification, and that the greedy version achieves
accuracy nearly identical to the exact formulation while being nearly 200 times
faster. Compared to state-of-the-art baselines, including feature-based,
image-based, and neural network models, our unsupervised method consistently
outperforms all non-learned approaches and rivals the performance of a
supervised classifier on class and subclass grouping tasks.

</details>


### [231] [LogTinyLLM: Tiny Large Language Models Based Contextual Log Anomaly Detection](https://arxiv.org/abs/2507.11071)
*Isaiah Thompson Ocansey,Ritwik Bhattacharya,Tanmay Sen*

Main category: cs.LG

TL;DR: This paper uses low-rank adaptation (LoRA) and adapter-based finetuning for log anomaly detection, achieving 18-19% accuracy improvement over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Log anomaly detection is challenging due to the complexity and volume of log sequences, making effective detection crucial for system maintenance.

Method: The study applies parameter-efficient finetuning (LoRA and adapter-based approaches) and compares tiny LLMs on the Thunderbird dataset.

Result: LoRA-based finetuning achieved accuracy improvements of 18-19% over LogBert, with overall scores between 97.76% and 98.83%.

Conclusion: LoRA-based finetuning is a superior method for detecting contextual anomalies in log sequences compared to full finetuning approaches.

Abstract: Log anomaly detection using traditional rule based or deep learning based
methods is often challenging due to the large volume and highly complex nature
of log sequence. So effective way of detection of anomalous sequence of logs is
crucial for system maintenance and development. This paper proposes parameter
efficient finetuning specifically low rank adaptation (LoRA) and adapter based
approaches for finding contextual anomalies in sequence of logs in large log
data set. It compares different tiny large language models (LLMs) on the
Thunderbird dataset. The results show that LoRA based finetuning provides
substantial performance improvements of 18 to 19 percentage over LogBert based
full finetuning approach, achieving accuracy scores between 97.76% and 98.83%
compared to 79.37%.

</details>


### [232] [Real-Time Bayesian Detection of Drift-Evasive GNSS Spoofing in Reinforcement Learning Based UAV Deconfliction](https://arxiv.org/abs/2507.11173)
*Deepak Kumar Panda,Weisi Guo*

Main category: cs.LG

TL;DR: This paper presents a Bayesian online change point detection (BOCPD) method to detect drift-evasive GNSS spoofing attacks on UAVs, achieving better accuracy and fewer errors compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for robust, real-time detection methods to counter sophisticated GNSS spoofing threats that manipulate UAV navigation without triggering traditional anti-spoofing mechanisms.

Method: The study employs a reinforcement learning critic network combined with a Bayesian online change point detection framework to identify temporal shifts indicative of subtle spoofing attacks.

Result: The proposed method demonstrated superior detection accuracy, as well as lower false-positive and false-negative rates, compared to conventional GNSS spoofing detectors and other state-of-the-art techniques.

Conclusion: The BOCPD-based temporal value estimation approach is effective at countering drift-evasive spoofing attacks on UAVs, offering improved resilience and detection precision.

Abstract: Autonomous unmanned aerial vehicles (UAVs) rely on global navigation
satellite system (GNSS) pseudorange measurements for accurate real-time
localization and navigation. However, this dependence exposes them to
sophisticated spoofing threats, where adversaries manipulate pseudoranges to
deceive UAV receivers. Among these, drift-evasive spoofing attacks subtly
perturb measurements, gradually diverting the UAVs trajectory without
triggering conventional signal-level anti-spoofing mechanisms. Traditional
distributional shift detection techniques often require accumulating a
threshold number of samples, causing delays that impede rapid detection and
timely response. Consequently, robust temporal-scale detection methods are
essential to identify attack onset and enable contingency planning with
alternative sensing modalities, improving resilience against stealthy
adversarial manipulations. This study explores a Bayesian online change point
detection (BOCPD) approach that monitors temporal shifts in value estimates
from a reinforcement learning (RL) critic network to detect subtle behavioural
deviations in UAV navigation. Experimental results show that this temporal
value-based framework outperforms conventional GNSS spoofing detectors,
temporal semi-supervised learning frameworks, and the Page-Hinkley test,
achieving higher detection accuracy and lower false-positive and false-negative
rates for drift-evasive spoofing attacks.

</details>


### [233] [Gradient Regularization-based Neural Granger Causality](https://arxiv.org/abs/2507.11178)
*Meiliang Liu,Huiwen Dong,Xiaoxiao Yang,Yunfang Xu,Zijin Li,Zhengye Si,Xinyue Yang,Zhiwen Zhao*

Main category: cs.LG

TL;DR: The paper introduces Gradient Regularization-based Neural Granger Causality (GRNGC), addressing limitations of existing neural Granger causality models regarding computational costs and interaction complexity by using gradient regularization and a single model architecture.


<details>
  <summary>Details</summary>
Motivation: Existing neural Granger causality models suffer from computational inefficiency due to component-wise architectures and weakened capabilities in capturing complex interactions due to sparsity-inducing penalties.

Method: The proposed GRNGC employs $L_1$ regularization on the gradient between model input and output to infer causality, utilizing a single prediction model applicable across various architectures like KAN, MLP, and LSTM.

Result: Numerical simulations and real-world dataset experiments show that GRNGC outperforms existing models in accuracy and efficiency, reducing computational overhead and reconstructing gene regulatory networks effectively.

Conclusion: GRNGC offers a flexible and computationally efficient solution to neural Granger causality modeling, excelling in capturing causality across diverse architectures and datasets.

Abstract: With the advancement of deep learning technologies, various neural
network-based Granger causality models have been proposed. Although these
models have demonstrated notable improvements, several limitations remain. Most
existing approaches adopt the component-wise architecture, necessitating the
construction of a separate model for each time series, which results in
substantial computational costs. In addition, imposing the sparsity-inducing
penalty on the first-layer weights of the neural network to extract causal
relationships weakens the model's ability to capture complex interactions. To
address these limitations, we propose Gradient Regularization-based Neural
Granger Causality (GRNGC), which requires only one time series prediction model
and applies $L_{1}$ regularization to the gradient between model's input and
output to infer Granger causality. Moreover, GRNGC is not tied to a specific
time series forecasting model and can be implemented with diverse architectures
such as KAN, MLP, and LSTM, offering enhanced flexibility. Numerical
simulations on DREAM, Lorenz-96, fMRI BOLD, and CausalTime show that GRNGC
outperforms existing baselines and significantly reduces computational
overhead. Meanwhile, experiments on real-world DNA, Yeast, HeLa, and bladder
urothelial carcinoma datasets further validate the model's effectiveness in
reconstructing gene regulatory networks.

</details>


### [234] [Mixture of Experts in Large Language Models](https://arxiv.org/abs/2507.11181)
*Danyang Zhang,Junhao Song,Ziqian Bi,Yingfang Yuan,Tianyang Wang,Joe Yeong,Junfeng Hao*

Main category: cs.LG

TL;DR: The paper reviews Mixture-of-Experts (MoE) in large language models, focusing on their performance benefits, key mechanisms, applications, and challenges.


<details>
  <summary>Details</summary>
Motivation: To explore the benefits, challenges, and future potential of Mixture-of-Experts architectures in enhancing large language models.

Method: The paper systematically analyzes MoE architectures, covering theoretical aspects, gating mechanisms, meta-learning, multimodal tasks, and deployment cases.

Result: The study highlights MoE's advantages like scaling efficiency, improved task-specific performance, and superior model capacity, while identifying challenges like expert diversity and reliable inference.

Conclusion: The paper provides insights into MoE's effective utilization in LLMs, addressing its limitations and outlining future advancements.

Abstract: This paper presents a comprehensive review of the Mixture-of-Experts (MoE)
architecture in large language models, highlighting its ability to
significantly enhance model performance while maintaining minimal computational
overhead. Through a systematic analysis spanning theoretical foundations, core
architectural designs, and large language model (LLM) applications, we examine
expert gating and routing mechanisms, hierarchical and sparse MoE
configurations, meta-learning approaches, multimodal and multitask learning
scenarios, real-world deployment cases, and recent advances and challenges in
deep learning. Our analysis identifies key advantages of MoE, including
superior model capacity compared to equivalent Bayesian approaches, improved
task-specific performance, and the ability to scale model capacity efficiently.
We also underscore the importance of ensuring expert diversity, accurate
calibration, and reliable inference aggregation, as these are essential for
maximizing the effectiveness of MoE architectures. Finally, this review
outlines current research limitations, open challenges, and promising future
directions, providing a foundation for continued innovation in MoE architecture
and its applications.

</details>


### [235] [Quantized Rank Reduction: A Communications-Efficient Federated Learning Scheme for Network-Critical Applications](https://arxiv.org/abs/2507.11183)
*Dimitrios Kritsiolis,Constantine Kotropoulos*

Main category: cs.LG

TL;DR: The paper introduces a method to address communication challenges in federated learning by proposing a communication-efficient scheme using low-rank approximation and quantization.


<details>
  <summary>Details</summary>
Motivation: Federated learning enhances privacy and security by training models locally, but it faces challenges with high communication overhead from frequent model updates.

Method: The proposed method uses low-rank approximation of neural network gradients along with quantization to reduce communication load while preserving model accuracy.

Result: It achieves a notable reduction in network load during the federated learning process with minimal loss in model performance.

Conclusion: The paper demonstrates that communication-efficient techniques like low-rank approximation and quantization can make federated learning more practical without sacrificing accuracy.

Abstract: Federated learning is a machine learning approach that enables multiple
devices (i.e., agents) to train a shared model cooperatively without exchanging
raw data. This technique keeps data localized on user devices, ensuring privacy
and security, while each agent trains the model on their own data and only
shares model updates. The communication overhead is a significant challenge due
to the frequent exchange of model updates between the agents and the central
server. In this paper, we propose a communication-efficient federated learning
scheme that utilizes low-rank approximation of neural network gradients and
quantization to significantly reduce the network load of the decentralized
learning process with minimal impact on the model's accuracy.

</details>


### [236] [An Explainable AI-Enhanced Machine Learning Approach for Cardiovascular Disease Detection and Risk Assessment](https://arxiv.org/abs/2507.11185)
*Md. Emon Akter Sourov,Md. Sabbir Hossen,Pabon Shaha,Mohammad Minoar Hossain,Md Sadiq Iqbal*

Main category: cs.LG

TL;DR: The study integrates classification and regression models for heart disease diagnostics and predictions, utilizing machine learning with SMOTE for imbalanced data, achieving remarkable accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Heart disease poses a global health burden, especially in resource-limited areas where traditional diagnostic methods often fail.

Method: The framework used classification models for detection, regression models for risk prediction, applied SMOTE for data augmentation, and evaluated performance using various metrics. Explainable AI was employed for interpretability.

Result: Random Forest achieved 97.6% accuracy on synthetic data, while Linear Regression showcased strong R2 values (0.992 on real data). Synthetic data enhancement improved predictions.

Conclusion: Machine learning, complemented by Explainable AI, offers transformative potential in heart disease diagnosis and risk prediction, promoting early intervention and informed clinical decisions.

Abstract: Heart disease remains a major global health concern, particularly in regions
with limited access to medical resources and diagnostic facilities. Traditional
diagnostic methods often fail to accurately identify and manage heart disease
risks, leading to adverse outcomes. Machine learning has the potential to
significantly enhance the accuracy, efficiency, and speed of heart disease
diagnosis. In this study, we proposed a comprehensive framework that combines
classification models for heart disease detection and regression models for
risk prediction. We employed the Heart Disease dataset, which comprises 1,035
cases. To address the issue of class imbalance, the Synthetic Minority
Oversampling Technique (SMOTE) was applied, resulting in the generation of an
additional 100,000 synthetic data points. Performance metrics, including
accuracy, precision, recall, F1-score, R2, MSE, RMSE, and MAE, were used to
evaluate the model's effectiveness. Among the classification models, Random
Forest emerged as the standout performer, achieving an accuracy of 97.2% on
real data and 97.6% on synthetic data. For regression tasks, Linear Regression
demonstrated the highest R2 values of 0.992 and 0.984 on real and synthetic
datasets, respectively, with the lowest error metrics. Additionally,
Explainable AI techniques were employed to enhance the interpretability of the
models. This study highlights the potential of machine learning to
revolutionize heart disease diagnosis and risk prediction, thereby facilitating
early intervention and enhancing clinical decision-making.

</details>


### [237] [AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air](https://arxiv.org/abs/2507.11515)
*Shiyi Yang,Xiaoxue Yu,Rongpeng Li,Jianhang Zhu,Zhifeng Zhao,Honggang Zhang*

Main category: cs.LG

TL;DR: AirLLM proposes a communication-efficient method for fine-tuning large language models (LLMs) on edge devices using adaptive rank configurations and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Running massive LLMs on edge devices faces constraints such as limited communication bandwidth and computational power, which make efficient remote fine-tuning strategies necessary.

Method: AirLLM introduces a hierarchical diffusion policy framework combining Proximal Policy Optimization (PPO) and Denoising Diffusion Implicit Models (DDIM). This approach dynamically adjusts ranks for LoRA by considering both wireless states and task complexities.

Result: Experiments show that AirLLM improves fine-tuning performance under different signal-to-noise ratios while substantially reducing transmission costs.

Conclusion: The study validates that AirLLM provides a scalable and communication-efficient solution for optimizing LLMs on edge devices via adaptive and intelligent rank adaptation.

Abstract: Operating Large Language Models (LLMs) on edge devices is increasingly
challenged by limited communication bandwidth and strained computational and
memory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable.
Nevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ
fixed or heuristic rank configurations, and the subsequent over-the-air
transmission of all LoRA parameters could be rather inefficient. To address
this limitation, we develop AirLLM, a hierarchical diffusion policy framework
for communication-aware LoRA adaptation. Specifically, AirLLM models the rank
configuration as a structured action vector that spans all LoRA-inserted
projections. To solve the underlying high-dimensional sequential
decision-making problem, a Proximal Policy Optimization (PPO) agent generates
coarse-grained decisions by jointly observing wireless states and linguistic
complexity, which are then refined via Denoising Diffusion Implicit Models
(DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The
two modules are optimized alternatively, with the DDIM trained under the
Classifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards.
Experiments under varying signal-to-noise ratios demonstrate that AirLLM
consistently enhances fine-tuning performance while significantly reducing
transmission costs, highlighting the effectiveness of reinforcement-driven,
diffusion-refined rank adaptation for scalable and efficient remote fine-tuning
over the air.

</details>


### [238] [Striking the Perfect Balance: Preserving Privacy While Boosting Utility in Collaborative Medical Prediction Platforms](https://arxiv.org/abs/2507.11187)
*Shao-Bo Lin,Xiaotong Liu,Yao Wang*

Main category: cs.LG

TL;DR: The paper proposes a privacy-preserving mechanism integrated into a distributed learning framework, addressing privacy concerns while aiming for optimal prediction performance in medical collaborative platforms.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address privacy attacks targeting patients and doctors which undermine the effectiveness of online collaborative medical prediction platforms.

Method: They develop a one-shot distributed learning framework with a privacy-preserving mechanism and validate it with simulations and real-world experiments.

Result: The proposed framework theoretically meets privacy and prediction performance requirements, and its effectiveness is experimentally confirmed.

Conclusion: The study demonstrates that optimal prediction performance and privacy goals can be effectively balanced in collaborative medical prediction platforms.

Abstract: Online collaborative medical prediction platforms offer convenience and
real-time feedback by leveraging massive electronic health records. However,
growing concerns about privacy and low prediction quality can deter patient
participation and doctor cooperation. In this paper, we first clarify the
privacy attacks, namely attribute attacks targeting patients and model
extraction attacks targeting doctors, and specify the corresponding privacy
principles. We then propose a privacy-preserving mechanism and integrate it
into a novel one-shot distributed learning framework, aiming to simultaneously
meet both privacy requirements and prediction performance objectives. Within
the framework of statistical learning theory, we theoretically demonstrate that
the proposed distributed learning framework can achieve the optimal prediction
performance under specific privacy requirements. We further validate the
developed privacy-preserving collaborative medical prediction platform through
both toy simulations and real-world data experiments.

</details>


### [239] [Gradient Descent on Logistic Regression: Do Large Step-Sizes Work with Data on the Sphere?](https://arxiv.org/abs/2507.11228)
*Si Yi Meng,Baptiste Goujaud,Antonio Orvieto,Christopher De Sa*

Main category: cs.LG

TL;DR: Gradient descent (GD) on logistic regression can cycle even with step sizes below the stability threshold of $2/\lambda$ when data magnitudes are equal, except in 1D where convergence is guaranteed.


<details>
  <summary>Details</summary>
Motivation: Investigating whether equal-magnitude datasets allow global convergence of GD under all step sizes below the stability threshold.

Method: Proved convergence in one-dimensional settings and analyzed higher-dimensional cases for cycling behavior using theoretical analysis.

Result: Equal magnitude ensures GD convergence only in one-dimensional cases; cycling persists in higher dimensions.

Conclusion: Equal-magnitude condition is insufficient for guaranteeing global convergence in higher dimensions; further study on cycles and realistic datasets is necessary.

Abstract: Gradient descent (GD) on logistic regression has many fascinating properties.
When the dataset is linearly separable, it is known that the iterates converge
in direction to the maximum-margin separator regardless of how large the step
size is. In the non-separable case, however, it has been shown that GD can
exhibit a cycling behaviour even when the step sizes is still below the
stability threshold $2/\lambda$, where $\lambda$ is the largest eigenvalue of
the Hessian at the solution. This short paper explores whether restricting the
data to have equal magnitude is a sufficient condition for global convergence,
under any step size below the stability threshold. We prove that this is true
in a one dimensional space, but in higher dimensions cycling behaviour can
still occur. We hope to inspire further studies on quantifying how common these
cycles are in realistic datasets, as well as finding sufficient conditions to
guarantee global convergence with large step sizes.

</details>


### [240] [Generative Click-through Rate Prediction with Applications to Search Advertising](https://arxiv.org/abs/2507.11246)
*Lingwei Kong,Lu Wang,Changping Peng,Zhangang Lin,Ching Law,Jingping Shao*

Main category: cs.LG

TL;DR: The paper introduces a model using generative pre-training to enhance click-through rate (CTR) predictions in discriminative frameworks, achieving significant success in experiments and online testing.


<details>
  <summary>Details</summary>
Motivation: The authors aim to leverage generative models to improve the expressive power and precision of discriminative CTR prediction models, inspired by the success of models like GPT.

Method: The approach involves a two-stage training process: (1) generative pre-training for next-item prediction based on user behavior sequences and item categories, and (2) fine-tuning the generative model within the discriminative CTR prediction system.

Result: Extensive experiments on a new dataset and online A/B testing confirm the model's high efficacy. The method has already been deployed on a major e-commerce platform.

Conclusion: Combining generative and discriminative models significantly enhances CTR prediction performance, showcasing scalability and effectiveness in real-world applications.

Abstract: Click-Through Rate (CTR) prediction models are integral to a myriad of
industrial settings, such as personalized search advertising. Current methods
typically involve feature extraction from users' historical behavior sequences
combined with product information, feeding into a discriminative model that is
trained on user feedback to estimate CTR. With the success of models such as
GPT, the potential for generative models to enrich expressive power beyond
discriminative models has become apparent. In light of this, we introduce a
novel model that leverages generative models to enhance the precision of CTR
predictions in discriminative models. To reconcile the disparate data
aggregation needs of both model types, we design a two-stage training process:
1) Generative pre-training for next-item prediction with the given item
category in user behavior sequences; 2) Fine-tuning the well-trained generative
model within a discriminative CTR prediction framework. Our method's efficacy
is substantiated through extensive experiments on a new dataset, and its
significant utility is further corroborated by online A/B testing results.
Currently, the model is deployed on one of the world's largest e-commerce
platforms, and we intend to release the associated code and dataset in the
future.

</details>


### [241] [LyAm: Robust Non-Convex Optimization for Stable Learning in Noisy Environments](https://arxiv.org/abs/2507.11262)
*Elmira Mirzabeigi,Sepehr Rezaee,Kourosh Parand*

Main category: cs.LG

TL;DR: The paper introduces 'LyAm', a novel optimizer combining Adam with Lyapunov stability, achieving better accuracy, speed, and stability in deep learning tasks.


<details>
  <summary>Details</summary>
Motivation: Training deep neural networks often faces challenges like noisy gradients and unstable convergence, limiting performance and generalization.

Method: The authors propose the LyAm optimizer, which incorporates adaptive moment estimation from Adam and applies Lyapunov stability theory to dynamically adjust learning rates.

Result: The optimizer LyAm showed superior performance on benchmarks like CIFAR-10 and CIFAR-100, surpassing state-of-the-art optimizers in accuracy, speed, and stability.

Conclusion: LyAm is a robust and effective optimizer, backed by theoretical convergence guarantees, for improving deep learning model training.

Abstract: Training deep neural networks, particularly in computer vision tasks, often
suffers from noisy gradients and unstable convergence, which hinder performance
and generalization. In this paper, we propose LyAm, a novel optimizer that
integrates Adam's adaptive moment estimation with Lyapunov-based stability
mechanisms. LyAm dynamically adjusts the learning rate using Lyapunov stability
theory to enhance convergence robustness and mitigate training noise. We
provide a rigorous theoretical framework proving the convergence guarantees of
LyAm in complex, non-convex settings. Extensive experiments on like as CIFAR-10
and CIFAR-100 show that LyAm consistently outperforms state-of-the-art
optimizers in terms of accuracy, convergence speed, and stability, establishing
it as a strong candidate for robust deep learning optimization.

</details>


### [242] [Turning Sand to Gold: Recycling Data to Bridge On-Policy and Off-Policy Learning via Causal Bound](https://arxiv.org/abs/2507.11269)
*Tal Fiskus,Uri Shaham*

Main category: cs.LG

TL;DR: The paper introduces a causal bound on factual loss in DRL, enhancing sample efficiency and performance while reducing computational demands.


<details>
  <summary>Details</summary>
Motivation: DRL agents require extensive computational resources and training steps, driving the need for more efficient methods to optimize their performance and reduce resource usage.

Method: The authors applied the Neyman-Rubin potential outcomes framework to compute a causal bound on factual loss, storing past value network outputs in the experience replay buffer to improve sample utilization.

Result: Experiments demonstrated up to 2,427% higher reward ratios and a 96% reduction in experience replay buffer size across multiple domains and agents, significantly improving sample efficiency.

Conclusion: The proposed causal bound approach offers a computationally efficient and effective way to enhance the performance of DRL agents, providing significant improvements in sample efficiency at negligible costs.

Abstract: Deep reinforcement learning (DRL) agents excel in solving complex
decision-making tasks across various domains. However, they often require a
substantial number of training steps and a vast experience replay buffer,
leading to significant computational and resource demands. To address these
challenges, we introduce a novel theoretical result that leverages the
Neyman-Rubin potential outcomes framework into DRL. Unlike most methods that
focus on bounding the counterfactual loss, we establish a causal bound on the
factual loss, which is analogous to the on-policy loss in DRL. This bound is
computed by storing past value network outputs in the experience replay buffer,
effectively utilizing data that is usually discarded. Extensive experiments
across the Atari 2600 and MuJoCo domains on various agents, such as DQN and
SAC, achieve up to 2,427% higher reward ratio, outperforming the same agents
without our proposed term, and reducing the experience replay buffer size by up
to 96%, significantly improving sample efficiency at negligible cost.

</details>


### [243] [Guiding LLM Decision-Making with Fairness Reward Models](https://arxiv.org/abs/2507.11344)
*Zara Hall,Melanie Subbiah,Thomas P Zollo,Kathleen McKeown,Richard Zemel*

Main category: cs.LG

TL;DR: The paper introduces Fairness Reward Model (FRM) to improve fairness in LLM-based decision-making processes while maintaining or enhancing accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges posed by unfair biases amplified by naive chain-of-thought reasoning in large language models (LLMs) during high-stakes decision-making scenarios.

Method: They proposed training a Fairness Reward Model (FRM) that scores reasoning processes in LLMs based on fairness, using weakly supervised examples annotated by LLMs to distinguish biased from unbiased reasoning.

Result: The FRM showed transferability across tasks, domains, and LLM families without additional fine-tuning, improving fairness in tasks like recidivism prediction and social media moderation.

Conclusion: The framework enhances fairness in decision-making systems using LLMs, enabling trustworthy reasoning without compromising or even improving decision accuracy.

Abstract: Large language models are increasingly used to support high-stakes decisions,
potentially influencing who is granted bail or receives a loan. Naive
chain-of-thought sampling can improve average decision accuracy, but has also
been shown to amplify unfair bias. To address this challenge and enable the
trustworthy use of reasoning models in high-stakes decision-making, we propose
a framework for training a generalizable Fairness Reward Model (FRM). Our model
assigns a fairness score to LLM reasoning, enabling the system to down-weight
biased trajectories and favor equitable ones when aggregating decisions across
reasoning chains. We show that a single Fairness Reward Model, trained on
weakly supervised, LLM-annotated examples of biased versus unbiased reasoning,
transfers across tasks, domains, and model families without additional
fine-tuning. Applied to real-world decision-making tasks including recidivism
prediction and social media moderation, we show that our approach consistently
improves fairness while matching, or even surpassing, baseline accuracy.

</details>


### [244] [Step-wise Policy for Rare-tool Knowledge (SPaRK): Offline RL that Drives Diverse Tool Use in LLMs](https://arxiv.org/abs/2507.11371)
*Gabriel Bo,Koa Chang,Justin Gu*

Main category: cs.LG

TL;DR: SPaRK is a reinforcement learning framework for training large language models to explore diverse tool usage patterns, enhancing reasoning capabilities without losing accuracy.


<details>
  <summary>Details</summary>
Motivation: Improve the reasoning capabilities of large language models (LLMs) while encouraging diversity in tool usage.

Method: Develop a dual-objective reward system for answer quality and tool diversity, train a Llama-3.1 8B model through offline PPO using synthetically generated trajectories.

Result: SPaRK achieves competitive performance across 14 categories of the MMLU-Pro dataset and increases entropy in tool selection compared to baseline methods.

Conclusion: Encouraging tool diversity via a novel reinforcement learning framework enhances reasoning ability and exploration in large language models without compromising answer accuracy.

Abstract: We present Step-wise Policy for Rare-tool Knowledge (SPaRK), a novel
reinforcement learning framework that teaches large language models to explore
diverse tool usage patterns beyond conventional high-temperature sampling.
Building on recent advances in step-wise reinforcement learning, we introduce a
dual-objective reward system that simultaneously optimizes for answer quality
and tool diversity, training a Llama-3.1 8B model through offline PPO on
synthetically generated trajectories from the MMLU-Pro dataset. Our approach
uniquely employs a rarity-first exploitation strategy where a GPT-4o judge
scores candidate actions across eight distinct tools plus chain-of-thought
reasoning, with the policy favoring less-frequently used but still viable tools
to encourage systematic exploration. Empirical results demonstrate that SPaRK
achieves competitive performance across 14 MMLU-Pro categories while exhibiting
significantly higher entropy in tool selection compared to both baseline and
supervised fine-tuning approaches, suggesting that algorithmic exploration
through explicit tool diversity can enhance reasoning capabilities without
sacrificing accuracy.

</details>


### [245] [A Neural Network Model of Complementary Learning Systems: Pattern Separation and Completion for Continual Learning](https://arxiv.org/abs/2507.11393)
*James P Jun,Vijay Marupudi,Raj Sanjay Shah,Sashank Varma*

Main category: cs.LG

TL;DR: The paper addresses catastrophic forgetting in neural networks using a model that combines Variational Autoencoders (VAEs) and Modern Hopfield Networks (MHNs), achieving ~90% accuracy on continual learning tasks.


<details>
  <summary>Details</summary>
Motivation: Humans can learn new information without forgetting prior knowledge, but neural networks struggle with catastrophic forgetting. This paper is motivated by the need to bridge this gap.

Method: The model merges Variational Autoencoders for memory generalization and Modern Hopfield Networks for robust memory storage, inspired by the Complementary Learning Systems theory.

Result: The proposed model performs near state-of-the-art (~90% accuracy) on the Split-MNIST continual learning task and substantially reduces forgetting.

Conclusion: By combining VAEs and MHNs, this work introduces a neurally plausible framework for continual learning that mirrors human-like memory functions and offers insights for both biological and artificial systems.

Abstract: Learning new information without forgetting prior knowledge is central to
human intelligence. In contrast, neural network models suffer from catastrophic
forgetting: a significant degradation in performance on previously learned
tasks when acquiring new information. The Complementary Learning Systems (CLS)
theory offers an explanation for this human ability, proposing that the brain
has distinct systems for pattern separation (encoding distinct memories) and
pattern completion (retrieving complete memories from partial cues). To capture
these complementary functions, we leverage the representational generalization
capabilities of variational autoencoders (VAEs) and the robust memory storage
properties of Modern Hopfield networks (MHNs), combining them into a neurally
plausible continual learning model. We evaluate this model on the Split-MNIST
task, a popular continual learning benchmark, and achieve close to
state-of-the-art accuracy (~90%), substantially reducing forgetting.
Representational analyses empirically confirm the functional dissociation: the
VAE underwrites pattern completion, while the MHN drives pattern separation. By
capturing pattern separation and completion in scalable architectures, our work
provides a functional template for modeling memory consolidation,
generalization, and continual learning in both biological and artificial
systems.

</details>


### [246] [Robust-Multi-Task Gradient Boosting](https://arxiv.org/abs/2507.11411)
*Seyedsaman Emami,Gonzalo Martínez-Muñoz,Daniel Hernández-Lobato*

Main category: cs.LG

TL;DR: The paper proposes a new framework called R-MTGB that improves multi-task learning (MTL) by addressing challenges with outlier or adversarial tasks using a three-phase process within gradient boosting.


<details>
  <summary>Details</summary>
Motivation: Current multi-task learning frameworks struggle when tasks are not well-aligned as adversarial or outlier tasks can degrade overall performance.

Method: R-MTGB structures learning into three phases: (1) learning shared patterns, (2) partitioning tasks into non-outliers and outliers with regularization, and (3) fine-tuning task-specific predictors. This is integrated into a gradient boosting framework.

Result: The proposed method isolates outlier tasks effectively, facilitates better knowledge transfer, reduces prediction errors per task, and improves overall task performance in both synthetic and real-world datasets.

Conclusion: R-MTGB demonstrates robustness and adaptability, reliably converging while handling noisy tasks and enhancing prediction accuracy across multi-task learning setups.

Abstract: Multi-task learning (MTL) has shown effectiveness in exploiting shared
information across tasks to improve generalization. MTL assumes tasks share
similarities that can improve performance. In addition, boosting algorithms
have demonstrated exceptional performance across diverse learning problems,
primarily due to their ability to focus on hard-to-learn instances and
iteratively reduce residual errors. This makes them a promising approach for
learning multi-task problems. However, real-world MTL scenarios often involve
tasks that are not well-aligned (known as outlier or adversarial tasks), which
do not share beneficial similarities with others and can, in fact, deteriorate
the performance of the overall model. To overcome this challenge, we propose
Robust-Multi-Task Gradient Boosting (R-MTGB), a novel boosting framework that
explicitly models and adapts to task heterogeneity during training. R-MTGB
structures the learning process into three sequential blocks: (1) learning
shared patterns, (2) partitioning tasks into outliers and non-outliers with
regularized parameters, and (3) fine-tuning task-specific predictors. This
architecture enables R-MTGB to automatically detect and penalize outlier tasks
while promoting effective knowledge transfer among related tasks. Our method
integrates these mechanisms seamlessly within gradient boosting, allowing
robust handling of noisy or adversarial tasks without sacrificing accuracy.
Extensive experiments on both synthetic benchmarks and real-world datasets
demonstrate that our approach successfully isolates outliers, transfers
knowledge, and consistently reduces prediction errors for each task
individually, and achieves overall performance gains across all tasks. These
results highlight robustness, adaptability, and reliable convergence of R-MTGB
in challenging MTL environments.

</details>


### [247] [Toward Improving fNIRS Classification: A Study on Activation Functions in Deep Neural Architectures](https://arxiv.org/abs/2507.11436)
*Behtom Adeli,John McLinden,Pankaj Pandey,Ming Shao,Yalda Shahriari*

Main category: cs.LG

TL;DR: The paper explores the role of activation functions in deep learning applied to fNIRS data, showing that symmetrical activation functions outperform commonly used ones like ReLU in many cases.


<details>
  <summary>Details</summary>
Motivation: Understanding how activation functions impact deep learning accuracy in challenging domains like fNIRS, where signal quality poses difficulties.

Method: Evaluates conventional and field-specific activation functions across multiple DL architectures, systematically comparing performance on fNIRS classification tasks.

Result: Symmetrical activation functions (e.g., Tanh, Abs(x)) outperform traditional ones like ReLU, with further evidence provided via a Modified Absolute Function (MAF).

Conclusion: Symmetrical activation functions improve deep learning performance for fNIRS, emphasizing the need for alignment between signal characteristics and method choice.

Abstract: Activation functions are critical to the performance of deep neural networks,
particularly in domains such as functional near-infrared spectroscopy (fNIRS),
where nonlinearity, low signal-to-noise ratio (SNR), and signal variability
poses significant challenges to model accuracy. However, the impact of
activation functions on deep learning (DL) performance in the fNIRS domain
remains underexplored and lacks systematic investigation in the current
literature. This study evaluates a range of conventional and field-specific
activation functions for fNIRS classification tasks using multiple deep
learning architectures, including the domain-specific fNIRSNet, AbsoluteNet,
MDNN, and shallowConvNet (as the baseline), all tested on a single dataset
recorded during an auditory task. To ensure fair a comparison, all networks
were trained and tested using standardized preprocessing and consistent
training parameters. The results show that symmetrical activation functions
such as Tanh and the Absolute value function Abs(x) can outperform commonly
used functions like the Rectified Linear Unit (ReLU), depending on the
architecture. Additionally, a focused analysis of the role of symmetry was
conducted using a Modified Absolute Function (MAF), with results further
supporting the effectiveness of symmetrical activation functions on performance
gains. These findings underscore the importance of selecting proper activation
functions that align with the signal characteristics of fNIRS data.

</details>


### [248] [Data Augmentation in Time Series Forecasting through Inverted Framework](https://arxiv.org/abs/2507.11439)
*Hongming Tan,Ting Chen,Ruochong Jin,Wai Kin Chan*

Main category: cs.LG

TL;DR: The paper introduces DAIF, a data augmentation method designed to address limitations of the inverted framework in multivariate time series forecasting.


<details>
  <summary>Details</summary>
Motivation: Limitations in iTransformer's inverted framework, such as diminished temporal interdependency and noise from nonsignificant variable correlation.

Method: The authors propose DAIF, integrating Frequency Filtering and Cross-variation Patching strategies to improve forecasting accuracy in inverted frameworks.

Result: Experiments show DAIF improves performance across multiple datasets and inverted models in multivariate time series forecasting.

Conclusion: DAIF enhances the inverted framework by addressing its limitations, making it a more robust tool for multivariate time series forecasting tasks.

Abstract: Currently, iTransformer is one of the most popular and effective models for
multivariate time series (MTS) forecasting. Thanks to its inverted framework,
iTransformer effectively captures multivariate correlation. However, the
inverted framework still has some limitations. It diminishes temporal
interdependency information, and introduces noise in cases of nonsignificant
variable correlation. To address these limitations, we introduce a novel data
augmentation method on inverted framework, called DAIF. Unlike previous data
augmentation methods, DAIF stands out as the first real-time augmentation
specifically designed for the inverted framework in MTS forecasting. We first
define the structure of the inverted sequence-to-sequence framework, then
propose two different DAIF strategies, Frequency Filtering and Cross-variation
Patching to address the existing challenges of the inverted framework.
Experiments across multiple datasets and inverted models have demonstrated the
effectiveness of our DAIF.

</details>


### [249] [LRMR: LLM-Driven Relational Multi-node Ranking for Lymph Node Metastasis Assessment in Rectal Cancer](https://arxiv.org/abs/2507.11457)
*Yaoxian Dong,Yifan Gao,Haoyue Li,Yanfen Cui,Xin Gao*

Main category: cs.LG

TL;DR: The paper introduces LRMR, a multimodal LLM-driven approach for analyzing lymph node metastasis in rectal cancer through reasoning and ranking, outperforming conventional deep learning baselines.


<details>
  <summary>Details</summary>
Motivation: Conventional MRI evaluation for lymph node metastasis in rectal cancer is limited by poor diagnostic performance and the lack of interpretability in many existing AI models.

Method: Two-stage LRMR framework: (1) A multimodal LLM analyzes LN images to generate structured reports with multiple features; (2) A text-based LLM compares these reports between patients to rank risk levels.

Result: LRMR achieved an AUC of 0.7917 and an F1-score of 0.7200, surpassing traditional models like ResNet50. Removing key components reduced performance significantly.

Conclusion: The LRMR approach, by integrating structured reasoning and interpretability, provides a novel and effective paradigm for preoperative lymph node assessment in rectal cancer.

Abstract: Accurate preoperative assessment of lymph node (LN) metastasis in rectal
cancer guides treatment decisions, yet conventional MRI evaluation based on
morphological criteria shows limited diagnostic performance. While some
artificial intelligence models have been developed, they often operate as black
boxes, lacking the interpretability needed for clinical trust. Moreover, these
models typically evaluate nodes in isolation, overlooking the patient-level
context. To address these limitations, we introduce LRMR, an LLM-Driven
Relational Multi-node Ranking framework. This approach reframes the diagnostic
task from a direct classification problem into a structured reasoning and
ranking process. The LRMR framework operates in two stages. First, a multimodal
large language model (LLM) analyzes a composite montage image of all LNs from a
patient, generating a structured report that details ten distinct radiological
features. Second, a text-based LLM performs pairwise comparisons of these
reports between different patients, establishing a relative risk ranking based
on the severity and number of adverse features. We evaluated our method on a
retrospective cohort of 117 rectal cancer patients. LRMR achieved an area under
the curve (AUC) of 0.7917 and an F1-score of 0.7200, outperforming a range of
deep learning baselines, including ResNet50 (AUC 0.7708). Ablation studies
confirmed the value of our two main contributions: removing the relational
ranking stage or the structured prompting stage led to a significant
performance drop, with AUCs falling to 0.6875 and 0.6458, respectively. Our
work demonstrates that decoupling visual perception from cognitive reasoning
through a two-stage LLM framework offers a powerful, interpretable, and
effective new paradigm for assessing lymph node metastasis in rectal cancer.

</details>


### [250] [Exploring the robustness of TractOracle methods in RL-based tractography](https://arxiv.org/abs/2507.11486)
*Jeremi Levesque,Antoine Théberge,Maxime Descoteaux,Pierre-Marc Jodoin*

Main category: cs.LG

TL;DR: The study enhances the RL-based tractography by introducing oracle-based training and a novel Iterative Reward Training (IRT) scheme, achieving improved accuracy and validity.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of false-positive tractography and improve the integration of anatomical priors within reinforcement learning for brain white matter reconstruction.

Method: The research explores extensions of TractOracle-RL using improved RL strategies and proposes Iterative Reward Training (IRT) where oracle guidance is refined iteratively using bundle filtering methods.

Result: Experimental results across five diffusion MRI datasets demonstrate enhanced accuracy and anatomical validity of RL tractography methods compared to traditional techniques.

Conclusion: Integrating oracle feedback into RL tractography and employing the novel IRT scheme leads to significant improvements in robustness and reliability in reconstructing brain’s white matter.

Abstract: Tractography algorithms leverage diffusion MRI to reconstruct the fibrous
architecture of the brain's white matter. Among machine learning approaches,
reinforcement learning (RL) has emerged as a promising framework for
tractography, outperforming traditional methods in several key aspects.
TractOracle-RL, a recent RL-based approach, reduces false positives by
incorporating anatomical priors into the training process via a reward-based
mechanism. In this paper, we investigate four extensions of the original
TractOracle-RL framework by integrating recent advances in RL, and we evaluate
their performance across five diverse diffusion MRI datasets. Results
demonstrate that combining an oracle with the RL framework consistently leads
to robust and reliable tractography, regardless of the specific method or
dataset used. We also introduce a novel RL training scheme called Iterative
Reward Training (IRT), inspired by the Reinforcement Learning from Human
Feedback (RLHF) paradigm. Instead of relying on human input, IRT leverages
bundle filtering methods to iteratively refine the oracle's guidance throughout
training. Experimental results show that RL methods trained with oracle
feedback significantly outperform widely used tractography techniques in terms
of accuracy and anatomical validity.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [251] [Tangma: A Tanh-Guided Activation Function with Learnable Parameters](https://arxiv.org/abs/2507.10560)
*Shreel Golwala*

Main category: cs.NE

TL;DR: The paper introduces a new activation function called Tangma that combines features of the hyperbolic tangent and learnable parameters, and demonstrates its effectiveness on MNIST and CIFAR-10 datasets, outperforming ReLU, Swish, and GELU.


<details>
  <summary>Details</summary>
Motivation: To create an activation function that improves training stability, convergence, and accuracy in deep neural networks using learnable parameters for better control.

Method: Tangma is introduced with two learnable parameters, α for adjusting activation inflection and γ for adding linearity to maintain weak gradients. It is evaluated on MNIST and CIFAR-10 datasets against other activation functions in custom networks.

Result: Tangma achieved the highest validation accuracy and stability on MNIST, and outperformed competitors on CIFAR-10 in validation accuracy and efficiency, with lower epoch runtimes.

Conclusion: Tangma is an effective activation function for standard vision tasks, offering reliable performance and efficient training. Its learnability may be particularly useful for larger models and complex tasks.

Abstract: Activation functions are key to effective backpropagation and expressiveness
in deep neural networks. This work introduces Tangma, a new activation function
that combines the smooth shape of the hyperbolic tangent with two learnable
parameters: $\alpha$, which shifts the curve's inflection point to adjust
neuron activation, and $\gamma$, which adds linearity to preserve weak
gradients and improve training stability. Tangma was evaluated on MNIST and
CIFAR-10 using custom networks composed of convolutional and linear layers, and
compared against ReLU, Swish, and GELU. On MNIST, Tangma achieved the highest
validation accuracy of 99.09% and the lowest validation loss, demonstrating
faster and more stable convergence than the baselines. On CIFAR-10, Tangma
reached a top validation accuracy of 78.15%, outperforming all other activation
functions while maintaining a competitive training loss. Tangma also showed
improved training efficiency, with lower average epoch runtimes compared to
Swish and GELU. These results suggest that Tangma performs well on standard
vision tasks and enables reliable, efficient training. Its learnable design
gives more control over activation behavior, which may benefit larger models in
tasks such as image recognition or language modeling.

</details>


### [252] [SFATTI: Spiking FPGA Accelerator for Temporal Task-driven Inference -- A Case Study on MNIST](https://arxiv.org/abs/2507.10561)
*Alessio Caviglia,Filippo Marostica,Alessio Carpegna,Alessandro Savino,Stefano Di Carlo*

Main category: cs.NE

TL;DR: The paper focuses on using the Spiker+ framework to create optimized SNN accelerators for handwritten digit recognition on the MNIST dataset, enabling energy-efficient computation on FPGAs.


<details>
  <summary>Details</summary>
Motivation: Edge applications require low-latency and energy-efficient inference mechanisms, making hardware accelerators and SNNs ideal for deployment.

Method: Utilizing the Spiker+ framework, the study generates optimized SNN hardware accelerators that integrate topology, neuron model specifications, and quantization, along with deployable HDL generation.

Result: Multiple configurations were evaluated, highlighting their trade-offs in edge computing scenarios.

Conclusion: The study demonstrates the potential of Spiker+ in creating low-power FPGA-based SNN solutions for edge applications.

Abstract: Hardware accelerators are essential for achieving low-latency,
energy-efficient inference in edge applications like image recognition. Spiking
Neural Networks (SNNs) are particularly promising due to their event-driven and
temporally sparse nature, making them well-suited for low-power Field
Programmable Gate Array (FPGA)-based deployment. This paper explores using the
open-source Spiker+ framework to generate optimized SNNs accelerators for
handwritten digit recognition on the MNIST dataset. Spiker+ enables high-level
specification of network topologies, neuron models, and quantization,
automatically generating deployable HDL. We evaluate multiple configurations
and analyze trade-offs relevant to edge computing constraints.

</details>


### [253] [A Biomimetic Way for Coral-Reef-Inspired Swarm Intelligence for Carbon-Neutral Wastewater Treatment](https://arxiv.org/abs/2507.10563)
*Antonis Messinis*

Main category: cs.NE

TL;DR: The paper introduces a Swarm Interaction Network inspired by coral reefs for achieving carbon-neutral wastewater treatment with high removal efficiency and low energy and CO2 emissions.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of achieving energy-neutral purification in wastewater treatment despite increasing wastewater rates.

Method: It employs a coral-reef-inspired Swarm Interaction Network using morphogenetic abstraction and multi-task carbon awareness, focusing on scalable and efficient processes.

Result: The approach delivers 96.7% removal efficiency, consumes 0.31 kWh/m³ of energy, emits 14.2 g/m³ of CO2, and demonstrates robustness under sensor drift across diverse field scenarios.

Conclusion: While the method is promising for diesel savings and scalability, challenges such as data-science staffing and interpretability due to governance restrictions are highlighted for future investigation.

Abstract: With increasing wastewater rates, achieving energy-neutral purification is
challenging. We introduce a coral-reef-inspired Swarm Interaction Network for
carbon-neutral wastewater treatment, combining morphogenetic abstraction with
multi-task carbon awareness. Scalability stems from linear token complexity,
mitigating the energy-removal problem. Compared with seven baselines, our
approach achieves 96.7\% removal efficiency, 0.31~kWh~m$^{-3}$ energy
consumption, and 14.2~g~m$^{-3}$ CO$_2$ emissions. Variance analysis
demonstrates robustness under sensor drift. Field scenarios--insular lagoons,
brewery spikes, and desert greenhouses--show potential diesel savings of up to
22\%. However, data-science staffing remains an impediment. Future work will
integrate AutoML wrappers within the project scope, although governance
restrictions pose interpretability challenges that require further visual
analytics.

</details>


### [254] [An Exact Gradient Framework for Training Spiking Neural Networks](https://arxiv.org/abs/2507.10568)
*Arman Ferdowsi,Atakan Aral*

Main category: cs.NE

TL;DR: The authors propose an analytical event-driven learning framework for spiking neural networks that improves accuracy and timing by allowing training of synaptic weights, delays, and adaptive firing thresholds without relying on hardware-intensive techniques.


<details>
  <summary>Details</summary>
Motivation: Existing methods for training spiking neural networks often rely on discrete-time simulations, surrogate gradients, or full access to internal state variables, which limit precision, efficiency, and hardware applicability.

Method: The authors developed an analytical event-driven learning framework that computes exact loss gradients for synaptic weights, delays, and adaptive firing thresholds.

Result: The new method achieves significant accuracy improvements (up to 7%), better timing precision, and enhanced robustness in experiments across multiple benchmarks.

Conclusion: This framework overcomes limitations of traditional approaches, making SNNs more efficient and practical for neuromorphic hardware.

Abstract: Spiking neural networks inherently rely on the precise timing of discrete
spike events for information processing. Incorporating additional bio-inspired
degrees of freedom, such as trainable synaptic transmission delays and adaptive
firing thresholds, is essential for fully leveraging the temporal dynamics of
SNNs. Although recent methods have demonstrated the benefits of training
synaptic weights and delays, both in terms of accuracy and temporal
representation, these techniques typically rely on discrete-time simulations,
surrogate gradient approximations, or full access to internal state variables
such as membrane potentials. Such requirements limit training precision and
efficiency and pose challenges for neuromorphic hardware implementation due to
increased memory and I/O bandwidth demands. To overcome these challenges, we
propose an analytical event-driven learning framework that computes exact loss
gradients not only with respect to synaptic weights and transmission delays but
also to adaptive neuronal firing thresholds. Experiments on multiple benchmarks
demonstrate significant gains in accuracy (up to 7%), timing precision, and
robustness compared to existing methods.

</details>


### [255] [Grammatical Structure and Grammatical Variations in Non-Metric Iranian Classical Music](https://arxiv.org/abs/2507.10708)
*Maziar Kanani,Sean O Leary,James McDermott*

Main category: cs.NE

TL;DR: This study introduces a dataset and algorithms for structural parsing and variation creation of Iranian classical music.


<details>
  <summary>Details</summary>
Motivation: To address the lack of computational tools and datasets for analyzing non-metric Iranian classical music.

Method: Development of a dataset of Dastgah Shour, application of parsing algorithms to identify motifs and phrases, and generation of melodic variations using grammar mutation.

Result: The proposed parsing and variation generation methods worked effectively, producing acceptable melodic variations evaluated by expert listeners.

Conclusion: The methodology successfully analyzes non-metric Iranian classical music and offers potential applicability to Arabic or Turkish classical music.

Abstract: In this study we introduce a symbolic dataset composed of non-metric Iranian
classical music, and algorithms for structural parsing of this music, and
generation of variations. The corpus comprises MIDI files and data sheets of
Dastgah Shour from Radif Mirza Abdollah, the foundational repertoire of Iranian
classical music. Furthermore, we apply our previously-introduced algorithm for
parsing melodic structure (Kanani et al., 2023b)to the dataset. Unlike much
Western music, this type of non-metric music does not follow bar-centric
organisation. The non-metric organisation can be captured well by our parsing
algorithm. We parse each tune (Gusheh) into a grammar to identify motifs and
phrases. These grammar representations can be useful for educational and
ethnomusicological purposes. We also further develop a previously-introduced
method of creating melodic variations (Kanani et al., 2023b). After parsing an
existing tune to produce a grammar, by applying mutations to this grammar, we
generate a new grammar. Expanding this new version yields a variation of the
original tune. Variations are assessed by a domain-expert listener.
Additionally, we conduct a statistical analysis of mutation with different
representation setups for our parsing and generation algorithms. The
overarching conclusion is that the system successfully produces acceptable
variations post-mutation. While our case study focuses on Iranian classical
music, the methodology can be adapted for Arabic or Turkish classical music.

</details>


### [256] [Biological Processing Units: Leveraging an Insect Connectome to Pioneer Biofidelic Neural Architectures](https://arxiv.org/abs/2507.10951)
*Siyu Yu,Zihan Qin,Tingshan Liu,Beiya Xu,R. Jacob Vogelstein,Jason Brown,Joshua T. Vogelstein*

Main category: cs.NE

TL;DR: The paper explores the artificial intelligence potential of a biologically inspired neural network derived from the Drosophila larva brain connectome, achieving competitive results across tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to investigate whether biologically evolved circuits, such as the Drosophila larva connectome, can be utilized to develop efficient and effective artificial intelligence systems.

Method: The study involves converting the wiring diagram of the Drosophila larva's brain into a Biological Processing Unit (BPU), which is a fixed recurrent network based on its synaptic connectivity. This network is then tested on various AI tasks including image classification and chess move predictions.

Result: The BPU achieved 98% accuracy on MNIST, 58% on CIFAR-10, and impressive performance on chess tasks, outperforming size-matched modern architectures such as MLPs and Transformers in key areas.

Conclusion: Biofidelic neural architectures like the BPU can rival or exceed traditional AI models on complex tasks, showcasing their potential and warranting further research into scaling such connectome-inspired approaches.

Abstract: The complete connectome of the Drosophila larva brain offers a unique
opportunity to investigate whether biologically evolved circuits can support
artificial intelligence. We convert this wiring diagram into a Biological
Processing Unit (BPU), a fixed recurrent network derived directly from synaptic
connectivity. Despite its modest size 3,000 neurons and 65,000 weights between
them), the unmodified BPU achieves 98% accuracy on MNIST and 58% on CIFAR-10,
surpassing size-matched MLPs. Scaling the BPU via structured connectome
expansions further improves CIFAR-10 performance, while modality-specific
ablations reveal the uneven contributions of different sensory subsystems. On
the ChessBench dataset, a lightweight GNN-BPU model trained on only 10,000
games achieves 60% move accuracy, nearly 10x better than any size transformer.
Moreover, CNN-BPU models with ~2M parameters outperform parameter-matched
Transformers, and with a depth-6 minimax search at inference, reach 91.7%
accuracy, exceeding even a 9M-parameter Transformer baseline. These results
demonstrate the potential of biofidelic neural architectures to support complex
cognitive tasks and motivate scaling to larger and more intelligent connectomes
in future work.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [257] [Stream programs are monoid homomorphisms with state](https://arxiv.org/abs/2507.10799)
*Tyler Hou,Michael Arntzenius,Max Willsey*

Main category: cs.PL

TL;DR: Defines deterministic stream functions implementable as homomorphisms into a state monoid, enabling simpler semantics and preserving rich equational reasoning.


<details>
  <summary>Details</summary>
Motivation: To simplify the semantic conditions for stream program optimization while retaining the ability for expressive equational reasoning.

Method: Introduced deterministic stream functions framed as homomorphisms into a state monoid and applied this to examples like database joins and TCP modeling.

Result: Simplified laws for semantic frameworks while maintaining support for complex stream programming techniques such as sequential and parallel composition.

Conclusion: The work presents a more simplified and generalized approach to optimizing stream functions, ensuring both theoretical soundness and practical flexibility.

Abstract: We define a broad class of deterministic stream functions and show they can
be implemented as homomorphisms into a "state" monoid. The homomorphism laws
are simpler than the conditions of previous semantic frameworks for stream
program optimization, yet retain support for rich equational reasoning over
expressive dataflow programs, including sequential composition, parallel
composition, and feedback. We demonstrate this using examples of partitioned
database joins, stratified negation, and a simplified model of TCP.

</details>


### [258] [The downgrading semantics of memory safety](https://arxiv.org/abs/2507.11282)
*René Rydhof Hansen,Andreas Stenbæk Larsen,Aslan Askarov*

Main category: cs.PL

TL;DR: The paper introduces 'gradual allocator independence,' a semantic concept to describe memory safety using noninterference principles refined for allocator behavior.


<details>
  <summary>Details</summary>
Motivation: Memory safety lacks a clear semantic definition, and prior approaches often face criticism for being unprincipled or incomplete.

Method: The authors define gradual allocator independence by utilizing noninterference concepts, accommodating memory-specific constraints like out-of-memory events and pointer-to-integer casts as forms of downgrading.

Result: Their method accurately captures allocator-specific aspects and offers satisfactory theoretical grounding for memory safety in low-level languages.

Conclusion: Gradual allocator independence provides a robust semantic framework for understanding memory safety in programs interacting with allocators, even in challenging scenarios.

Abstract: Memory safety is traditionally characterized in terms of bad things that
cannot happen, an approach that is often criticized as unprincipled. Prior work
suggest a connection between memory safety and noninterference, but no
satisfactory semantic notion of memory safety is currently known.
  This work proposes a notion of gradual allocator independence that accurately
captures many allocator-specific aspects of memory safety. We consider a
low-level language with access to an allocator that provides malloc and free
primitives in a flat memory model. Pointers are just integers, and as such it
is trivial to write memory-unsafe programs. The basic intuition of gradual
allocator independence is that of noninterference, namely that allocators must
not influence program execution. This intuition is refined in two important
ways to account for the allocators running out-of-memory and for programs to
have pointer-to-integer casts. The key insight of the definition is to treat
these extensions as forms of downgrading and give them satisfactory technical
treatment using the state-of-the-art information flow machinery.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [259] [Learning to Move in Rhythm: Task-Conditioned Motion Policies with Orbital Stability Guarantees](https://arxiv.org/abs/2507.10602)
*Maximilian Stölzle,T. Konstantin Rusch,Zach J. Patterson,Rodrigo Pérez-Dattari,Francesco Stella,Josie Hughes,Cosimo Della Santina,Daniela Rus*

Main category: cs.RO

TL;DR: The paper introduces Orbitally Stable Motion Primitives (OSMPs), a framework for learning periodic robotic motions from demonstrations, ensuring stability and interpolation between tasks.


<details>
  <summary>Details</summary>
Motivation: Existing Dynamic Motion Primitives struggle to capture complex periodic behaviors and cannot easily interpolate tasks, limiting their use in tasks like locomotion or rhythmic tool use.

Method: The approach integrates a learned diffeomorphic encoder with a supercritical Hopf bifurcation in latent space, ensuring orbital stability, transverse contraction, and the ability to generalize tasks via a bijective encoder.

Result: OSMPs exhibit superior performance, enhanced interpolation, and zero-shot generalization to unseen motion objectives, as validated through diverse simulations and real-world experiments on multiple robotic platforms.

Conclusion: OSMPs extend the applicability of motion primitives, providing new possibilities for handling complex and periodic tasks while outperforming existing baselines.

Abstract: Learning from demonstration provides a sample-efficient approach to acquiring
complex behaviors, enabling robots to move robustly, compliantly, and with
fluidity. In this context, Dynamic Motion Primitives offer built - in stability
and robustness to disturbances but often struggle to capture complex periodic
behaviors. Moreover, they are limited in their ability to interpolate between
different tasks. These shortcomings substantially narrow their applicability,
excluding a wide class of practically meaningful tasks such as locomotion and
rhythmic tool use. In this work, we introduce Orbitally Stable Motion
Primitives (OSMPs) - a framework that combines a learned diffeomorphic encoder
with a supercritical Hopf bifurcation in latent space, enabling the accurate
acquisition of periodic motions from demonstrations while ensuring formal
guarantees of orbital stability and transverse contraction. Furthermore, by
conditioning the bijective encoder on the task, we enable a single learned
policy to represent multiple motion objectives, yielding consistent zero-shot
generalization to unseen motion objectives within the training distribution. We
validate the proposed approach through extensive simulation and real-world
experiments across a diverse range of robotic platforms - from collaborative
arms and soft manipulators to a bio-inspired rigid-soft turtle robot -
demonstrating its versatility and effectiveness in consistently outperforming
state-of-the-art baselines such as diffusion policies, among others.

</details>


### [260] [Vision Language Action Models in Robotic Manipulation: A Systematic Review](https://arxiv.org/abs/2507.10672)
*Muhayy Ud Din,Waseem Akram,Lyes Saad Saoud,Jan Rosell,Irfan Hussain*

Main category: cs.RO

TL;DR: The paper reviews Vision Language Action (VLA) models that unify visual perception, language understanding, and robotic control, focusing on datasets, simulation environments, and architecture paradigms.


<details>
  <summary>Details</summary>
Motivation: The authors aim to advance robotics by exploring unifying frameworks that integrate vision, language, and control for robotic manipulation and autonomy.

Method: The paper analyzes 102 VLA models, 26 datasets, and 12 simulation platforms to evaluate their architectural design, complexity, and effectiveness. It introduces characterization frameworks and criteria for analysis.

Result: Key findings include classifications of architectural paradigms, dataset suitability evaluations, and identification of underexplored areas in data landscapes. It also assesses simulation environments for scalability and real-world transfer.

Conclusion: The review outlines challenges and strategic directions, acting as a roadmap for advancing robotic control and embodiment through modular designs, scalable pretraining, and improved multitask alignment strategies.

Abstract: Vision Language Action (VLA) models represent a transformative shift in
robotics, with the aim of unifying visual perception, natural language
understanding, and embodied control within a single learning framework. This
review presents a comprehensive and forward-looking synthesis of the VLA
paradigm, with a particular emphasis on robotic manipulation and
instruction-driven autonomy. We comprehensively analyze 102 VLA models, 26
foundational datasets, and 12 simulation platforms that collectively shape the
development and evaluation of VLAs models. These models are categorized into
key architectural paradigms, each reflecting distinct strategies for
integrating vision, language, and control in robotic systems. Foundational
datasets are evaluated using a novel criterion based on task complexity,
variety of modalities, and dataset scale, allowing a comparative analysis of
their suitability for generalist policy learning. We introduce a
two-dimensional characterization framework that organizes these datasets based
on semantic richness and multimodal alignment, showing underexplored regions in
the current data landscape. Simulation environments are evaluated for their
effectiveness in generating large-scale data, as well as their ability to
facilitate transfer from simulation to real-world settings and the variety of
supported tasks. Using both academic and industrial contributions, we recognize
ongoing challenges and outline strategic directions such as scalable
pretraining protocols, modular architectural design, and robust multimodal
alignment strategies. This review serves as both a technical reference and a
conceptual roadmap for advancing embodiment and robotic control, providing
insights that span from dataset generation to real world deployment of
generalist robotic agents.

</details>


### [261] [Exteroception through Proprioception Sensing through Improved Contact Modeling for Soft Growing Robots](https://arxiv.org/abs/2507.10694)
*Francesco Fuentes,Serigne Diagne,Zachary Kingston,Laura H. Blumenschein*

Main category: cs.RO

TL;DR: This paper explores the potential of soft growing robots in mapping and exploring unknown, unstructured environments by developing a collision model, simulator, and deployment strategy.


<details>
  <summary>Details</summary>
Motivation: Soft growing robots excel in navigating unstructured environments due to their passive deformation. Understanding their collision and deformation behavior could enable them to map and understand these environments.

Method: The authors characterized collision behavior during discrete turns, developed a 2D geometry-based simulator for robot trajectories, and employed Monte Carlo sampling for optimal deployment estimation.

Result: The simulation and sampling method demonstrated effective mapping in uniform and non-uniform environments, quickly converging to optimal actions.

Conclusion: Soft growing robots show great potential for mapping and exploration tasks, enabling robust navigation and spatial understanding in unstructured environments.

Abstract: Passive deformation due to compliance is a commonly used benefit of soft
robots, providing opportunities to achieve robust actuation with few active
degrees of freedom. Soft growing robots in particular have shown promise in
navigation of unstructured environments due to their passive deformation. If
their collisions and subsequent deformations can be better understood, soft
robots could be used to understand the structure of the environment from direct
tactile measurements. In this work, we propose the use of soft growing robots
as mapping and exploration tools. We do this by first characterizing collision
behavior during discrete turns, then leveraging this model to develop a
geometry-based simulator that models robot trajectories in 2D environments.
Finally, we demonstrate the model and simulator validity by mapping unknown
environments using Monte Carlo sampling to estimate the optimal next deployment
given current knowledge. Over both uniform and non-uniform environments, this
selection method rapidly approaches ideal actions, showing the potential for
soft growing robots in unstructured environment exploration and mapping.

</details>


### [262] [RCG: Safety-Critical Scenario Generation for Robust Autonomous Driving via Real-World Crash Grounding](https://arxiv.org/abs/2507.10749)
*Benjamin Stoler,Juliet Yang,Jonathan Francis,Jean Oh*

Main category: cs.RO

TL;DR: The paper develops a framework to generate realistic safety-critical scenarios for autonomous driving systems by leveraging crash-informed semantics and adversarial perturbations.


<details>
  <summary>Details</summary>
Motivation: Real-world crash scenarios are rare in driving datasets but crucial for effectively training and evaluating autonomous driving systems.

Method: The authors design a scenario generation framework named RCG, which uses contrastive pre-training on driving logs followed by fine-tuning with annotated crash-rich data, enabling high-risk and realistic adversary trajectory selection.

Result: RCG integration improves ego agent success rates by 9.2% across seven evaluations, producing plausible adversary behaviors for autonomous driving systems.

Conclusion: The framework enhances the realism of stress-testing autonomous driving systems and improves their robustness; code and tools will be accessible publicly.

Abstract: Safety-critical scenarios are essential for training and evaluating
autonomous driving (AD) systems, yet remain extremely rare in real-world
driving datasets. To address this, we propose Real-world Crash Grounding (RCG),
a scenario generation framework that integrates crash-informed semantics into
adversarial perturbation pipelines. We construct a safety-aware behavior
representation through contrastive pre-training on large-scale driving logs,
followed by fine-tuning on a small, crash-rich dataset with approximate
trajectory annotations extracted from video. This embedding captures semantic
structure aligned with real-world accident behaviors and supports selection of
adversary trajectories that are both high-risk and behaviorally realistic. We
incorporate the resulting selection mechanism into two prior scenario
generation pipelines, replacing their handcrafted scoring objectives with an
embedding-based criterion. Experimental results show that ego agents trained
against these generated scenarios achieve consistently higher downstream
success rates, with an average improvement of 9.2% across seven evaluation
settings. Qualitative and quantitative analyses further demonstrate that our
approach produces more plausible and nuanced adversary behaviors, enabling more
effective and realistic stress testing of AD systems. Code and tools will be
released publicly.

</details>


### [263] [rt-RISeg: Real-Time Model-Free Robot Interactive Segmentation for Active Instance-Level Object Understanding](https://arxiv.org/abs/2507.10776)
*Howard H. Qian,Yiting Chen,Gaotian Wang,Podshara Chanrungmaneekul,Kaiyu Hang*

Main category: cs.RO

TL;DR: The paper introduces a real-time framework, rt-RISeg, for unseen object segmentation that leverages robot interactions to achieve superior accuracy without requiring pre-trained models.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of generalization in unseen object instance segmentation, where traditional approaches often fail in out-of-distribution scenarios due to reliance on static visual features.

Method: The proposed method uses robot interactions combined with relative rotational and linear velocities to segment objects, leveraging a body frame-invariant feature (BFIF) instead of learned models.

Result: Experiments show rt-RISeg achieves an object segmentation accuracy rate 27.5% higher than state-of-the-art methods and its segmentation masks improve foundation model performance.

Conclusion: Interactive perception through rt-RISeg provides a practical, robust, and standalone segmentation solution, improving both robotic tasks and vision model outcomes.

Abstract: Successful execution of dexterous robotic manipulation tasks in new
environments, such as grasping, depends on the ability to proficiently segment
unseen objects from the background and other objects. Previous works in unseen
object instance segmentation (UOIS) train models on large-scale datasets, which
often leads to overfitting on static visual features. This dependency results
in poor generalization performance when confronted with out-of-distribution
scenarios. To address this limitation, we rethink the task of UOIS based on the
principle that vision is inherently interactive and occurs over time. We
propose a novel real-time interactive perception framework, rt-RISeg, that
continuously segments unseen objects by robot interactions and analysis of a
designed body frame-invariant feature (BFIF). We demonstrate that the relative
rotational and linear velocities of randomly sampled body frames, resulting
from selected robot interactions, can be used to identify objects without any
learned segmentation model. This fully self-contained segmentation pipeline
generates and updates object segmentation masks throughout each robot
interaction without the need to wait for an action to finish. We showcase the
effectiveness of our proposed interactive perception method by achieving an
average object segmentation accuracy rate 27.5% greater than state-of-the-art
UOIS methods. Furthermore, although rt-RISeg is a standalone framework, we show
that the autonomously generated segmentation masks can be used as prompts to
vision foundation models for significantly improved performance.

</details>


### [264] [Versatile and Generalizable Manipulation via Goal-Conditioned Reinforcement Learning with Grounded Object Detection](https://arxiv.org/abs/2507.10814)
*Huiyi Wang,Fahim Shahriar,Alireza Azimi,Gautham Vasan,Rupam Mahmood,Colin Bellinger*

Main category: cs.RO

TL;DR: This study integrates a pre-trained object detection model into Goal-Conditioned Reinforcement Learning (GCRL) for versatile robotic reach and grasp tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of applying reinforcement learning to robotic manipulation by utilizing the flexibility and efficiency of large pre-trained models in processing text prompts and interpreting diverse objects.

Method: The researchers used a pre-trained object detection model that generates masks based on text prompts to condition the reinforcement learning agent's goals, enhancing feature sharing and generalization.

Result: The proposed framework achieves a $
\sim$90% success rate in grasping both familiar and unfamiliar objects in simulation, while also leading to faster convergence to higher returns.

Conclusion: Integrating mask-based goal conditioning with pre-trained models in GCRL successfully improves versatility and efficiency in robotic manipulation.

Abstract: General-purpose robotic manipulation, including reach and grasp, is essential
for deployment into households and workspaces involving diverse and evolving
tasks. Recent advances propose using large pre-trained models, such as Large
Language Models and object detectors, to boost robotic perception in
reinforcement learning. These models, trained on large datasets via
self-supervised learning, can process text prompts and identify diverse objects
in scenes, an invaluable skill in RL where learning object interaction is
resource-intensive. This study demonstrates how to integrate such models into
Goal-Conditioned Reinforcement Learning to enable general and versatile robotic
reach and grasp capabilities. We use a pre-trained object detection model to
enable the agent to identify the object from a text prompt and generate a mask
for goal conditioning. Mask-based goal conditioning provides object-agnostic
cues, improving feature sharing and generalization. The effectiveness of the
proposed framework is demonstrated in a simulated reach-and-grasp task, where
the mask-based goal conditioning consistently maintains a $\sim$90\% success
rate in grasping both in and out-of-distribution objects, while also ensuring
faster convergence to higher returns.

</details>


### [265] [Mixed Discrete and Continuous Planning using Shortest Walks in Graphs of Convex Sets](https://arxiv.org/abs/2507.10878)
*Savva Morozov,Tobia Marcucci,Bernhard Paus Graesdal,Alexandre Amice,Pablo A. Parrilo,Russ Tedrake*

Main category: cs.RO

TL;DR: The paper introduces the Shortest-Walk Problem (SWP) on a Graph of Convex Sets (GCS), which unifies various mixed discrete-continuous planning problems under a single framework and provides an efficient algorithm for solving them.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to create a unified framework for tackling mixed discrete-continuous planning problems, which are prevalent in robotics and other domains, typically requiring specialized solutions.

Method: The authors synthesize a piecewise-quadratic lower bound using semidefinite programming and employ it to guide an incremental-search algorithm that approximates the shortest walk in the Graph of Convex Sets.

Result: The proposed method unifies several planning problems in robotics, such as motion planning and skill chaining, and demonstrates computational efficiency and high performance experimentally.

Conclusion: The paper concludes that the SWP in GCS provides a natural language for diverse planning problems, delivering unified solutions and significant computational advantages across experiments.

Abstract: We study the Shortest-Walk Problem (SWP) in a Graph of Convex Sets (GCS). A
GCS is a graph where each vertex is paired with a convex program, and each edge
couples adjacent programs via additional costs and constraints. A walk in a GCS
is a sequence of vertices connected by edges, where vertices may be repeated.
The length of a walk is given by the cumulative optimal value of the
corresponding convex programs. To solve the SWP in GCS, we first synthesize a
piecewise-quadratic lower bound on the problem's cost-to-go function using
semidefinite programming. Then we use this lower bound to guide an
incremental-search algorithm that yields an approximate shortest walk. We show
that the SWP in GCS is a natural language for many mixed discrete-continuous
planning problems in robotics, unifying problems that typically require
specialized solutions while delivering high performance and computational
efficiency. We demonstrate this through experiments in collision-free motion
planning, skill chaining, and optimal control of hybrid systems.

</details>


### [266] [Object-Centric Mobile Manipulation through SAM2-Guided Perception and Imitation Learning](https://arxiv.org/abs/2507.10899)
*Wang Zhicheng,Satoshi Yagi,Satoshi Yamamori,Jun Morimoto*

Main category: cs.RO

TL;DR: The paper introduces an object-centric approach for mobile manipulation using SAM2 to improve task generalization across different orientations.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current mobile manipulation where navigation and manipulation are decoupled, leading to performance issues when navigating inaccurately.

Method: The approach incorporates an object-centric strategy using SAM2 to integrate manipulation orientation into a unified model.

Result: The model outperformed the Action Chunking Transformer in pick-and-place tasks with varied orientation angles.

Conclusion: The proposed method improves generalization and robustness in imitation learning-based mobile manipulation systems.

Abstract: Imitation learning for mobile manipulation is a key challenge in the field of
robotic manipulation. However, current mobile manipulation frameworks typically
decouple navigation and manipulation, executing manipulation only after
reaching a certain location. This can lead to performance degradation when
navigation is imprecise, especially due to misalignment in approach angles. To
enable a mobile manipulator to perform the same task from diverse orientations,
an essential capability for building general-purpose robotic models, we propose
an object-centric method based on SAM2, a foundation model towards solving
promptable visual segmentation in images, which incorporates manipulation
orientation information into our model. Our approach enables consistent
understanding of the same task from different orientations. We deploy the model
on a custom-built mobile manipulator and evaluate it on a pick-and-place task
under varied orientation angles. Compared to Action Chunking Transformer, our
model maintains superior generalization when trained with demonstrations from
varied approach angles. This work significantly enhances the generalization and
robustness of imitation learning-based mobile manipulation systems.

</details>


### [267] [Fast Non-Episodic Adaptive Tuning of Robot Controllers with Online Policy Optimization](https://arxiv.org/abs/2507.10914)
*James A. Preiss,Fengze Xie,Yiheng Lin,Adam Wierman,Yisong Yue*

Main category: cs.RO

TL;DR: The paper presents M-GAPS, an online policy optimization algorithm for dynamically varying settings, showing it performs well in hardware experiments and challenging scenarios.


<details>
  <summary>Details</summary>
Motivation: To develop a practical online algorithm for tuning robot controller parameters in real-time settings without state resets, while adapting to time-varying dynamics and objectives.

Method: The proposed M-GAPS algorithm combines model-based online policy optimization with reparameterizations of state space and policy class for improved optimization. It runs on a single trajectory and doesn't rely on artificial episodes.

Result: Experimental results demonstrate that M-GAPS outperforms model-based and model-free baselines in convergence speed and adaptability to disturbances such as wind and extra payloads.

Conclusion: M-GAPS offers a hardware-practical solution combining flexibility, stability, and data efficiency, bridging the gap between classic adaptive control and model-free reinforcement learning.

Abstract: We study online algorithms to tune the parameters of a robot controller in a
setting where the dynamics, policy class, and optimality objective are all
time-varying. The system follows a single trajectory without episodes or state
resets, and the time-varying information is not known in advance. Focusing on
nonlinear geometric quadrotor controllers as a test case, we propose a
practical implementation of a single-trajectory model-based online policy
optimization algorithm, M-GAPS,along with reparameterizations of the quadrotor
state space and policy class to improve the optimization landscape. In hardware
experiments,we compare to model-based and model-free baselines that impose
artificial episodes. We show that M-GAPS finds near-optimal parameters more
quickly, especially when the episode length is not favorable. We also show that
M-GAPS rapidly adapts to heavy unmodeled wind and payload disturbances, and
achieves similar strong improvement on a 1:6-scale Ackermann-steered car. Our
results demonstrate the hardware practicality of this emerging class of online
policy optimization that offers significantly more flexibility than classic
adaptive control, while being more stable and data-efficient than model-free
reinforcement learning.

</details>


### [268] [Unified Modeling and Structural Optimization of Multi-magnet Embedded Soft Continuum Robots for Enhanced Kinematic Performances](https://arxiv.org/abs/2507.10950)
*Zhiwei Wu,Jiahao Luo,Siyi Wei,Jinhui Zhang*

Main category: cs.RO

TL;DR: This paper develops a unified framework for improving the kinematic performance of multi-magnet embedded soft continuum robots by integrating modeling, analysis, and optimization.


<details>
  <summary>Details</summary>
Motivation: Enhancing the kinematic performance of multi-magnet embedded soft continuum robots is critical to improving their controllable degrees of freedom and operational efficiency.

Method: A differentiable system formulation based on an extended pseudo-rigid-body model is established, followed by a structural optimization framework using differential geometry to link kinematic measures to magnet configurations.

Result: Optimizing magnet configurations improves manipulability and dexterity; closed-form solutions and numerical gradient-based methods validate the framework’s effectiveness via simulation.

Conclusion: The study provides actionable insights into designing magnet configurations for such robots, maximizing their kinematic potential and controllability.

Abstract: This paper presents a unified modeling and optimization framework to enhance
the kinematic performance of multi-magnet embedded soft continuum robots
(MeSCRs). To this end, we establish a differentiable system formulation based
on an extended pseudo-rigid-body model. This formulation enables analysis of
the equilibrium well-posedness and the geometry of the induced configuration
under magnetic actuation. In particular, we show that the maximum controllable
degrees of freedom of a MeSCR equal twice the number of embedded magnets. We
subsequently develop a structural optimization framework based on differential
geometry that links classical kinematic measures (e.g., manipulability and
dexterity) to the configuration of embedded magnets. The resulting optimization
condition reveals that improving local performance requires structurally
modulating the spectrum of the configuration space metric to counteract its
distortion. Closed-form solutions for optimal magnet configurations are derived
under representative conditions, and a gradient-based numerical method is
proposed for general design scenarios. Simulation studies validate the
effectiveness of the proposed framework.

</details>


### [269] [Whom to Respond To? A Transformer-Based Model for Multi-Party Social Robot Interaction](https://arxiv.org/abs/2507.10960)
*He Zhu,Ryo Miyoshi,Yuki Okafuji*

Main category: cs.RO

TL;DR: The paper introduces a Transformer-based multi-task learning framework for social robots in multi-user environments, enabling better response timing and recipient selection.


<details>
  <summary>Details</summary>
Motivation: Single-user robotic interactions fail to address complexities in multi-user environments, requiring improved decision-making in timing and target audience for responses.

Method: A novel multi-task learning framework using Transformers, along with two specialized loss functions and a newly created multi-party HRI dataset, is proposed.

Result: Experimental results show improved robot decision-making performance, surpassing heuristic and single-task methods.

Conclusion: The findings advance socially intelligent robots capable of context-aware, multi-user interactions.

Abstract: Prior human-robot interaction (HRI) research has primarily focused on
single-user interactions, where robots do not need to consider the timing or
recipient of their responses. However, in multi-party interactions, such as at
malls and hospitals, social robots must understand the context and decide both
when and to whom they should respond. In this paper, we propose a
Transformer-based multi-task learning framework to improve the decision-making
process of social robots, particularly in multi-user environments. Considering
the characteristics of HRI, we propose two novel loss functions: one that
enforces constraints on active speakers to improve scene modeling, and another
that guides response selection towards utterances specifically directed at the
robot. Additionally, we construct a novel multi-party HRI dataset that captures
real-world complexities, such as gaze misalignment. Experimental results
demonstrate that our model achieves state-of-the-art performance in respond
decisions, outperforming existing heuristic-based and single-task approaches.
Our findings contribute to the development of socially intelligent social
robots capable of engaging in natural and context-aware multi-party
interactions.

</details>


### [270] [EquiContact: A Hierarchical SE(3) Vision-to-Force Equivariant Policy for Spatially Generalizable Contact-rich Tasks](https://arxiv.org/abs/2507.10961)
*Joohwan Seo,Arvind Kruthiventy,Soomi Lee,Megan Teng,Xiang Zhang,Seoyeon Choi,Jongeun Choi,Roberto Horowitz*

Main category: cs.RO

TL;DR: A robot learning framework, EquiContact, addresses spatial generalization for contact-rich tasks, achieving high success in peg-in-hole tasks.


<details>
  <summary>Details</summary>
Motivation: To develop a robotic manipulation framework capable of learning contact-rich tasks that spatially generalize from few demonstrations.

Method: Proposes EquiContact framework utilizing SE(3)-equivariant components: Diff-EDF for high-level planning and G-CompACT for compliant visuomotor control.

Result: Achieved near-perfect success in real-world peg-in-hole tasks and robust spatial generalization to novel configurations.

Conclusion: The framework effectively integrates compliance, equivariance, and localized policies, enabling spatial generalization for contact-rich robotic tasks.

Abstract: This paper presents a framework for learning vision-based robotic policies
for contact-rich manipulation tasks that generalize spatially across task
configurations. We focus on achieving robust spatial generalization of the
policy for the peg-in-hole (PiH) task trained from a small number of
demonstrations. We propose EquiContact, a hierarchical policy composed of a
high-level vision planner (Diffusion Equivariant Descriptor Field, Diff-EDF)
and a novel low-level compliant visuomotor policy (Geometric Compliant ACT,
G-CompACT). G-CompACT operates using only localized observations (geometrically
consistent error vectors (GCEV), force-torque readings, and wrist-mounted RGB
images) and produces actions defined in the end-effector frame. Through these
design choices, we show that the entire EquiContact pipeline is
SE(3)-equivariant, from perception to force control. We also outline three key
components for spatially generalizable contact-rich policies: compliance,
localized policies, and induced equivariance. Real-world experiments on PiH
tasks demonstrate a near-perfect success rate and robust generalization to
unseen spatial configurations, validating the proposed framework and
principles. The experimental videos can be found on the project website:
https://sites.google.com/berkeley.edu/equicontact

</details>


### [271] [SMART-Merge Planner: A Safe Merging and Real-Time Motion Planner for Autonomous Highway On-Ramp Merging](https://arxiv.org/abs/2507.10968)
*Toktam Mohammadnejad,Jovin D'sa,Behdad Chalaki,Hossein Nourkhiz Mahjoub,Ehsan Moradi-Pari*

Main category: cs.RO

TL;DR: The paper presents SMART-Merge, a motion planner for achieving safe and comfortable forced highway merging, validated to outperform others with a 100% success rate in simulations.


<details>
  <summary>Details</summary>
Motivation: Highway merging is complex and demands optimization of safety, comfort, and time efficiency, which existing methods struggle to balance in challenging scenarios.

Method: The SMART-Merge planner is developed using a lattice-based architecture, with customized cost terms and a desired speed heuristic to optimize forced merging efficiency and safety.

Result: Simulations on diverse highway scenarios showed a 100% success rate and minimized merge times compared to baseline planners.

Conclusion: SMART-Merge demonstrates robust, reliable performance for autonomous highway merging, offering a significant improvement over existing methods.

Abstract: Merging onto a highway is a complex driving task that requires identifying a
safe gap, adjusting speed, often interactions to create a merging gap, and
completing the merge maneuver within a limited time window while maintaining
safety and driving comfort. In this paper, we introduce a Safe Merging and
Real-Time Merge (SMART-Merge) planner, a lattice-based motion planner designed
to facilitate safe and comfortable forced merging. By deliberately adapting
cost terms to the unique challenges of forced merging and introducing a desired
speed heuristic, SMART-Merge planner enables the ego vehicle to merge
successfully while minimizing the merge time. We verify the efficiency and
effectiveness of the proposed merge planner through high-fidelity CarMaker
simulations on hundreds of highway merge scenarios. Our proposed planner
achieves the success rate of 100% as well as completes the merge maneuver in
the shortest amount of time compared with the baselines, demonstrating our
planner's capability to handle complex forced merge tasks and provide a
reliable and robust solution for autonomous highway merge. The simulation
result videos are available at
https://sites.google.com/view/smart-merge-planner/home.

</details>


### [272] [Uncertainty Aware Mapping for Vision-Based Underwater Robots](https://arxiv.org/abs/2507.10991)
*Abhimanyu Bhowmik,Mohit Singh,Madhushree Sannigrahi,Martin Ludvigsen,Kostas Alexis*

Main category: cs.RO

TL;DR: This paper presents a way to improve vision-based mapping for underwater robots by addressing depth estimation and uncertainty in mapping.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in enabling vision-based underwater robots to effectively navigate and map confined spaces amidst sensor noise and situational uncertainties.

Method: It integrates depth estimation confidence from the RAFT-Stereo model into a voxel-based mapping framework called Voxblox, with proposed weight calculation and update mechanism improvements.

Result: Experiments in a confined pool and Trondheim fjord pier show the method's ability to represent mapping uncertainty using underwater robots.

Conclusion: The method successfully handles mapping inconsistencies and improves depth estimation confidence representation for underwater navigation in uncertain environments.

Abstract: Vision-based underwater robots can be useful in inspecting and exploring
confined spaces where traditional sensors and preplanned paths cannot be
followed. Sensor noise and situational change can cause significant uncertainty
in environmental representation. Thus, this paper explores how to represent
mapping inconsistency in vision-based sensing and incorporate depth estimation
confidence into the mapping framework. The scene depth and the confidence are
estimated using the RAFT-Stereo model and are integrated into a voxel-based
mapping framework, Voxblox. Improvements in the existing Voxblox weight
calculation and update mechanism are also proposed. Finally, a qualitative
analysis of the proposed method is performed in a confined pool and in a pier
in the Trondheim fjord. Experiments using an underwater robot demonstrated the
change in uncertainty in the visualization.

</details>


### [273] [ILCL: Inverse Logic-Constraint Learning from Temporally Constrained Demonstrations](https://arxiv.org/abs/2507.11000)
*Minwoo Cho,Jaehwi Jang,Daehyung Park*

Main category: cs.RO

TL;DR: The paper presents a method (ILCL) to learn and replicate temporal-constraint logic from demonstrations using a game-theoretic approach, combining genetic algorithms and logic-constrained reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of learning non-Markovian, temporal logic constraints directly from demonstrations, which involves dealing with high dimensionality and ill-posed constraints.

Method: The proposed ILCL method frames the problem as a zero-sum game between genetic algorithm-based temporal-logic mining (GA-TL-Mining) and logic-constrained reinforcement learning (Logic-CRL) to generate and adhere to temporal logic constraints.

Result: The evaluations demonstrate that ILCL has superior performance compared to state-of-the-art methods in learning and transferring temporal logic constraints across tasks, including success in real-world settings.

Conclusion: ILCL is an effective and transferable method for learning temporal logic constraints from demonstrations, showing its practical potential in both simulated and real-world tasks.

Abstract: We aim to solve the problem of temporal-constraint learning from
demonstrations to reproduce demonstration-like logic-constrained behaviors.
Learning logic constraints is challenging due to the combinatorially large
space of possible specifications and the ill-posed nature of non-Markovian
constraints. To figure it out, we introduce a novel temporal-constraint
learning method, which we call inverse logic-constraint learning (ILCL). Our
method frames ICL as a two-player zero-sum game between 1) a genetic
algorithm-based temporal-logic mining (GA-TL-Mining) and 2) logic-constrained
reinforcement learning (Logic-CRL). GA-TL-Mining efficiently constructs syntax
trees for parameterized truncated linear temporal logic (TLTL) without
predefined templates. Subsequently, Logic-CRL finds a policy that maximizes
task rewards under the constructed TLTL constraints via a novel constraint
redistribution scheme. Our evaluations show ILCL outperforms state-of-the-art
baselines in learning and transferring TL constraints on four temporally
constrained tasks. We also demonstrate successful transfer to real-world
peg-in-shallow-hole tasks.

</details>


### [274] [Learning to Tune Like an Expert: Interpretable and Scene-Aware Navigation via MLLM Reasoning and CVAE-Based Adaptation](https://arxiv.org/abs/2507.11001)
*Yanbo Wang,Zipeng Fang,Lei Zhao,Weidong Chen*

Main category: cs.RO

TL;DR: LE-Nav is a scene-aware navigation framework that integrates multi-modal large language models and conditional variational autoencoders to adapt planner hyperparameters, achieving superior performance and social acceptance in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Traditional navigation systems struggle to generalize in dynamic and unstructured environments, reducing their performance and social acceptance. Current reinforcement learning approaches also face limitations due to poor generalization and ineffective sim-to-real transfer.

Method: LE-Nav combines multi-modal large language model reasoning with conditional variational autoencoders. It uses one-shot exemplars and chain-of-thought prompting for zero-shot scene understanding and maps natural language instructions to navigation hyperparameters for adaptive tuning.

Result: LE-Nav demonstrates human-level tuning across diverse scenarios, achieving superior success rates, efficiency, safety, and comfort compared to existing methods. Real-world trials and user studies also show enhanced perceived safety and social acceptance.

Conclusion: LE-Nav provides an interpretable, adaptive navigation framework that excels in dynamic scenarios, outperforming state-of-the-art methods in both objective metrics and user perception.

Abstract: Service robots are increasingly deployed in diverse and dynamic environments,
where both physical layouts and social contexts change over time and across
locations. In these unstructured settings, conventional navigation systems that
rely on fixed parameters often fail to generalize across scenarios, resulting
in degraded performance and reduced social acceptance. Although recent
approaches have leveraged reinforcement learning to enhance traditional
planners, these methods often fail in real-world deployments due to poor
generalization and limited simulation diversity, which hampers effective
sim-to-real transfer. To tackle these issues, we present LE-Nav, an
interpretable and scene-aware navigation framework that leverages multi-modal
large language model reasoning and conditional variational autoencoders to
adaptively tune planner hyperparameters. To achieve zero-shot scene
understanding, we utilize one-shot exemplars and chain-of-thought prompting
strategies. Additionally, a conditional variational autoencoder captures the
mapping between natural language instructions and navigation hyperparameters,
enabling expert-level tuning. Experiments show that LE-Nav can generate
hyperparameters achieving human-level tuning across diverse planners and
scenarios. Real-world navigation trials and a user study on a smart wheelchair
platform demonstrate that it outperforms state-of-the-art methods on
quantitative metrics such as success rate, efficiency, safety, and comfort,
while receiving higher subjective scores for perceived safety and social
acceptance. Code is available at https://github.com/Cavendish518/LE-Nav.

</details>


### [275] [Enhancing Autonomous Manipulator Control with Human-in-loop for Uncertain Assembly Environments](https://arxiv.org/abs/2507.11006)
*Ashutosh Mishra,Shreya Santra,Hazal Gozbasi,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: The paper proposes an advanced robotic system with Human-in-the-Loop (HITL) control to improve reliability and efficiency in lunar mission tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance robotic manipulation under uncertain and extreme conditions, particularly in autonomous operations for lunar missions.

Method: The method integrates human decision-making with autonomous robotics, utilizing real-time sensor feedback for error detection, adaptive control, motion planning, and digital twin simulation for continuous refinement.

Result: The proposed system was tested successfully under simulated lunar conditions, showcasing reliability under challenges like low lighting, variable terrain, and sensor limits.

Conclusion: Integrating HITL control with advanced robotics improves task execution in uncertain lunar environments, emphasizing the importance of human oversight and adaptive strategies.

Abstract: This study presents an advanced approach to enhance robotic manipulation in
uncertain and challenging environments, with a focus on autonomous operations
augmented by human-in-the-loop (HITL) control for lunar missions. By
integrating human decision-making with autonomous robotic functions, the
research improves task reliability and efficiency for space applications. The
key task addressed is the autonomous deployment of flexible solar panels using
an extendable ladder-like structure and a robotic manipulator with real-time
feedback for precision. The manipulator relays position and force-torque data,
enabling dynamic error detection and adaptive control during deployment. To
mitigate the effects of sinkage, variable payload, and low-lighting conditions,
efficient motion planning strategies are employed, supplemented by human
control that allows operators to intervene in ambiguous scenarios. Digital twin
simulation enhances system robustness by enabling continuous feedback,
iterative task refinement, and seamless integration with the deployment
pipeline. The system has been tested to validate its performance in simulated
lunar conditions and ensure reliability in extreme lighting, variable terrain,
changing payloads, and sensor limitations.

</details>


### [276] [TRAN-D: 2D Gaussian Splatting-based Sparse-view Transparent Object Depth Reconstruction via Physics Simulation for Scene Update](https://arxiv.org/abs/2507.11069)
*Jeongyun Kim,Seunghoon Jeong,Giseop Kim,Myung-Hwan Jeon,Eunji Jun,Ayoung Kim*

Main category: cs.RO

TL;DR: TRAN-D is a novel method for reconstructing 3D geometry of transparent objects using a 2D Gaussian Splatting-based approach, achieving significant improvements in both synthetic and real-world benchmarks.


<details>
  <summary>Details</summary>
Motivation: Accurately understanding the 3D geometry of transparent objects is challenging due to physical properties like reflection and refraction, especially in sparse view environments.

Method: The method separates transparent objects from the background, focuses optimization on object-related Gaussians, employs object-aware loss to mitigate artifacts, and incorporates physics-based simulations for rapid refining.

Result: TRAN-D significantly outperformed existing methods in synthetic and real-world tests, achieving over 39% improvement in synthetic datasets and strong performance with single-image updates.

Conclusion: TRAN-D provides a robust and efficient solution for reconstructing the depth geometry of transparent objects, overcoming traditional limitations in sparse and dynamic scenarios.

Abstract: Understanding the 3D geometry of transparent objects from RGB images is
challenging due to their inherent physical properties, such as reflection and
refraction. To address these difficulties, especially in scenarios with sparse
views and dynamic environments, we introduce TRAN-D, a novel 2D Gaussian
Splatting-based depth reconstruction method for transparent objects. Our key
insight lies in separating transparent objects from the background, enabling
focused optimization of Gaussians corresponding to the object. We mitigate
artifacts with an object-aware loss that places Gaussians in obscured regions,
ensuring coverage of invisible surfaces while reducing overfitting.
Furthermore, we incorporate a physics-based simulation that refines the
reconstruction in just a few seconds, effectively handling object removal and
chain-reaction movement of remaining objects without the need for rescanning.
TRAN-D is evaluated on both synthetic and real-world sequences, and it
consistently demonstrated robust improvements over existing GS-based
state-of-the-art methods. In comparison with baselines, TRAN-D reduces the mean
absolute error by over 39% for the synthetic TRansPose sequences. Furthermore,
despite being updated using only one image, TRAN-D reaches a {\delta} < 2.5 cm
accuracy of 48.46%, over 1.5 times that of baselines, which uses six images.
Code and more results are available at https://jeongyun0609.github.io/TRAN-D/.

</details>


### [277] [Closed Form Time Derivatives of the Equations of Motion of Rigid Body Systems](https://arxiv.org/abs/2507.11076)
*Andreas Mueller,Shivesh Kumar*

Main category: cs.RO

TL;DR: The paper presents closed-form equations up to second-order for derivatives of equations of motion (EOM) for rigid body systems, emphasizing their role in robotics and control of multibody systems.


<details>
  <summary>Details</summary>
Motivation: To address the need within the robotics community for explicit, compact, and structured derivatives of EOM, which are vital for designing and controlling systems with elastic components.

Method: The authors use a Lie group formulation to derive closed-form equations for the time derivatives of EOM up to second-order, offering a more direct and parameterized alternative to recursive algorithms.

Result: The method presented yields compact and parameterized closed-form equations, delivering insights into the structure of derivatives of EOM in rigid body systems.

Conclusion: This approach offers an efficient alternative to recursive algorithms, potentially improving trajectory smoothness and force/torque calculations in robotic and multibody system control.

Abstract: Derivatives of equations of motion(EOM) describing the dynamics of rigid body
systems are becoming increasingly relevant for the robotics community and find
many applications in design and control of robotic systems. Controlling robots,
and multibody systems comprising elastic components in particular, not only
requires smooth trajectories but also the time derivatives of the control
forces/torques, hence of the EOM. This paper presents the time derivatives of
the EOM in closed form up to second-order as an alternative formulation to the
existing recursive algorithms for this purpose, which provides a direct insight
into the structure of the derivatives. The Lie group formulation for rigid body
systems is used giving rise to very compact and easily parameterized equations.

</details>


### [278] [Force-Based Viscosity and Elasticity Measurements for Material Biomechanical Characterisation with a Collaborative Robotic Arm](https://arxiv.org/abs/2507.11133)
*Luca Beber,Edoardo Lamon,Giacomo Moretti,Matteo Saveriano,Luca Fambri,Luigi Palopoli,Daniele Fontanelli*

Main category: cs.RO

TL;DR: This paper evaluates a robotic system's precision in estimating viscoelastic properties of materials, with promising results for its application in medical diagnostics.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by error-prone diagnostic activities like palpation and ultrasound that rely on highly skilled staff, and to explore robots as a solution to improve accuracy and reduce waiting times.

Method: A robotic system was tested for estimating viscoelastic parameters of varying materials, including ex vivo tissues. Results were compared against ground truth data derived from silicone specimens characterized by a high-precision instrument.

Result: The robotic system demonstrated a level of accuracy closely matching the ground truth, validating its potential for precise biomechanical property estimation.

Conclusion: Robotic diagnostic systems show strong promise for integration in clinical settings, potentially enhancing the reliability of diagnostic activities and alleviating associated challenges.

Abstract: Diagnostic activities, such as ultrasound scans and palpation, are relatively
low-cost. They play a crucial role in the early detection of health problems
and in assessing their progression. However, they are also error-prone
activities, which require highly skilled medical staff. The use of robotic
solutions can be key to decreasing the inherent subjectivity of the results and
reducing the waiting list. For a robot to perform palpation or ultrasound
scans, it must effectively manage physical interactions with the human body,
which greatly benefits from precise estimation of the patient's tissue
biomechanical properties. This paper assesses the accuracy and precision of a
robotic system in estimating the viscoelastic parameters of various materials,
including some tests on ex vivo tissues as a preliminary proof-of-concept
demonstration of the method's applicability to biological samples. The
measurements are compared against a ground truth derived from silicone
specimens with different viscoelastic properties, characterised using a
high-precision instrument. Experimental results show that the robotic system's
accuracy closely matches the ground truth, increasing confidence in the
potential use of robots for such clinical applications.

</details>


### [279] [A Robust Controller based on Gaussian Processes for Robotic Manipulators with Unknown Uncertainty](https://arxiv.org/abs/2507.11170)
*Giulio Giacomuzzo,Mohamed Abdelwahab,Marco Calì,Alberto Dalla Libera,Ruggero Carli*

Main category: cs.RO

TL;DR: The paper introduces a novel robust feedback linearization strategy that integrates Gaussian Processes to improve trajectory tracking for Lagrangian systems, specially focusing on handling model mismatches.


<details>
  <summary>Details</summary>
Motivation: Precise trajectory tracking for Lagrangian systems is a significant challenge, especially in scenarios where model mismatch exists and bounds on such mismatch are not available.

Method: The method integrates Gaussian Processes Regression (GPR) to estimate model mismatches and augment a classical feedback linearization scheme with an additional robustifying term based on residual uncertainty.

Result: The proposed feedback linearization approach guarantees asymptotic tracking of desired trajectories with high probability and is numerically validated on a 2 degrees of freedom planar robot.

Conclusion: The proposed strategy effectively enhances trajectory tracking performance for Lagrangian systems by addressing model mismatch uncertainty through GPR-based estimation and control robustness.

Abstract: In this paper, we propose a novel learning-based robust feedback
linearization strategy to ensure precise trajectory tracking for an important
family of Lagrangian systems. We assume a nominal knowledge of the dynamics is
given but no a-priori bounds on the model mismatch are available. In our
approach, the key ingredient is the adoption of a regression framework based on
Gaussian Processes (GPR) to estimate the model mismatch. This estimate is added
to the outer loop of a classical feedback linearization scheme based on the
nominal knowledge available. Then, to compensate for the residual uncertainty,
we robustify the controller including an additional term whose size is designed
based on the variance provided by the GPR framework. We proved that, with high
probability, the proposed scheme is able to guarantee asymptotic tracking of a
desired trajectory. We tested numerically our strategy on a 2 degrees of
freedom planar robot.

</details>


### [280] [MPC-based Coarse-to-Fine Motion Planning for Robotic Object Transportation in Cluttered Environments](https://arxiv.org/abs/2507.11211)
*Chen Cai,Ernesto Dickel Saraiva,Ya-jun Pan,Steven Liu*

Main category: cs.RO

TL;DR: The paper introduces a motion planning framework for robots in cluttered environments, combining dual-camera perception and B-spline-based MPC, enabling dynamic replanning from partial visual data.


<details>
  <summary>Details</summary>
Motivation: Creating a robust motion planning system for robotic manipulation that can operate effectively in cluttered and unmodeled scenarios with dynamic constraints.

Method: The proposed approach uses a dual-camera perception setup for environment modeling and a B-spline-based MPC scheme for motion trajectory planning, refining plans progressively with fused visual data.

Result: Experiments on multi-arm robots demonstrate the system's robustness, adaptability, and effectiveness in handling uncertainties and clutter.

Conclusion: The novel framework successfully integrates vision, collision detection, and motion planning, offering a reliable solution for robotic manipulation in challenging environments.

Abstract: This letter presents a novel coarse-to-fine motion planning framework for
robotic manipulation in cluttered, unmodeled environments. The system
integrates a dual-camera perception setup with a B-spline-based model
predictive control (MPC) scheme. Initially, the planner generates feasible
global trajectories from partial and uncertain observations. As new visual data
are incrementally fused, both the environment model and motion planning are
progressively refined. A vision-based cost function promotes target-driven
exploration, while a refined kernel-perceptron collision detector enables
efficient constraint updates for real-time planning. The framework accommodates
closed-chain kinematics and supports dynamic replanning. Experiments on a
multi-arm platform validate its robustness and adaptability under uncertainties
and clutter.

</details>


### [281] [Comparison of Localization Algorithms between Reduced-Scale and Real-Sized Vehicles Using Visual and Inertial Sensors](https://arxiv.org/abs/2507.11241)
*Tobias Kern,Leon Tolksdorf,Christian Birkner*

Main category: cs.RO

TL;DR: This paper investigates the effects of scaling on accuracy in self-localization algorithms applied to reduced-scale vehicles.


<details>
  <summary>Details</summary>
Motivation: The study aims to accelerate the development of advanced automated driving functions by exploring the feasibility of using reduced-scale vehicles for testing self-localization algorithms.

Method: The authors tested ROS2-compatible visual and visual-inertial localization algorithms (OpenVINS, VINS-Fusion, RTAB-Map) on datasets from real-sized vehicles and conducted test drives with reduced-scale vehicles. They compared pose accuracy against ground-truth data.

Result: OpenVINS exhibited the lowest average localization error among the algorithms, performing consistently well on reduced-scale vehicles. Minor translational error differences were noticed between reduced-scale and real-sized vehicles, but rotational accuracy showed no significant differences.

Conclusion: Reduced-scale vehicles can effectively serve as testing platforms for self-localization algorithms due to their comparable performance to real-sized vehicles in localization accuracy.

Abstract: Physically reduced-scale vehicles are emerging to accelerate the development
of advanced automated driving functions. In this paper, we investigate the
effects of scaling on self-localization accuracy with visual and
visual-inertial algorithms using cameras and an inertial measurement unit
(IMU). For this purpose, ROS2-compatible visual and visual-inertial algorithms
are selected, and datasets are chosen as a baseline for real-sized vehicles. A
test drive is conducted to record data of reduced-scale vehicles. We compare
the selected localization algorithms, OpenVINS, VINS-Fusion, and RTAB-Map, in
terms of their pose accuracy against the ground-truth and against data from
real-sized vehicles. When comparing the implementation of the selected
localization algorithms to real-sized vehicles, OpenVINS has the lowest average
localization error. Although all selected localization algorithms have
overlapping error ranges, OpenVINS also performs best when applied to a
reduced-scale vehicle. When reduced-scale vehicles were compared to real-sized
vehicles, minor differences were found in translational vehicle motion
estimation accuracy. However, no significant differences were found when
comparing the estimation accuracy of rotational vehicle motion, allowing RSVRs
to be used as testing platforms for self-localization algorithms.

</details>


### [282] [Development of an Autonomous Mobile Robotic System for Efficient and Precise Disinfection](https://arxiv.org/abs/2507.11270)
*Ting-Wei Ou,Jia-Hao Jiang,Guan-Lin Huang,Kuu-Young Young*

Main category: cs.RO

TL;DR: The paper introduces a mobile robotic system to optimize UV disinfection in hospitals, focusing on virus hotspots and reducing disinfection time by over 30%.


<details>
  <summary>Details</summary>
Motivation: The need to address virus transmission risks in hospitals more efficiently due to resource shortages and the COVID-19 pandemic's impact.

Method: A robot-based system prioritizing high-risk areas and adjusting UV dosage for optimal disinfection while ensuring safety and reducing time.

Result: In tests across two hospital scenarios, the system reduced disinfection time by 30.7% and 31.9%, achieving equal effectiveness.

Conclusion: The proposed system enhances disinfection efficiency, focusing on hotspot prioritization and optimizing UV exposure, promising scalable benefits for hospital settings.

Abstract: The COVID-19 pandemic has severely affected public health, healthcare
systems, and daily life, especially amid resource shortages and limited
workers. This crisis has underscored the urgent need for automation in hospital
environments, particularly disinfection, which is crucial to controlling virus
transmission and improving the safety of healthcare personnel and patients.
Ultraviolet (UV) light disinfection, known for its high efficiency, has been
widely adopted in hospital settings. However, most existing research focuses on
maximizing UV coverage while paying little attention to the impact of human
activity on virus distribution. To address this issue, we propose a mobile
robotic system for UV disinfection focusing on the virus hotspot. The system
prioritizes disinfection in high-risk areas and employs an approach for
optimized UV dosage to ensure that all surfaces receive an adequate level of UV
exposure while significantly reducing disinfection time. It not only improves
disinfection efficiency but also minimizes unnecessary exposure in low-risk
areas. In two representative hospital scenarios, our method achieves the same
disinfection effectiveness while reducing disinfection time by 30.7% and 31.9%,
respectively. The video of the experiment is available at:
https://youtu.be/wHcWzOcoMPM.

</details>


### [283] [Ocean Diviner: A Diffusion-Augmented Reinforcement Learning for AUV Robust Control in the Underwater Tasks](https://arxiv.org/abs/2507.11283)
*Weiyi Liu,Jingzehua Xu,Guanwen Xie,Yi Li*

Main category: cs.RO

TL;DR: The paper introduces a diffusion-augmented reinforcement learning approach to improve autonomous underwater vehicle (AUV) control, enhancing trajectory planning and system adaptability in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: To address challenges in AUV trajectory planning and adaptability in dynamic underwater settings, requiring innovative techniques for robust and reliable control.

Method: The method integrates a diffusion-based trajectory generation, a high-dimensional state encoding mechanism using diffusion U-Net, and a hybrid architecture combining diffusion-guided exploration with RL policy optimization.

Result: Simulation experiments show superior robustness and adaptability of the proposed method over traditional control methods in underwater operational tasks.

Conclusion: The approach enhances exploration efficiency, policy stability, and overall system reliability for AUV autonomy in challenging marine conditions.

Abstract: This paper presents a diffusion-augmented reinforcement learning (RL)
approach for robust autonomous underwater vehicle (AUV) control, addressing key
challenges in underwater trajectory planning and dynamic environment
adaptation. The proposed method integrates three core innovations: (1) A
diffusion-based trajectory generation framework that produces physically
feasible multi-step trajectories, enhanced by a high-dimensional state encoding
mechanism combining current observations with historical states and actions
through a novel diffusion U-Net architecture, significantly improving
long-horizon planning. (2) A sample-efficient hybrid learning architecture that
synergizes diffusion-guided exploration with RL policy optimization, where the
diffusion model generates diverse candidate actions and the RL critic selects
optimal actions, achieving higher exploration efficiency and policy stability
in dynamic underwater environments. Extensive simulation experiments validating
the method's superior robustness and flexibility, outperforms conventional
control methods in challenging marine conditions, offering enhanced
adaptability and reliability for AUV operations in the underwater tasks.

</details>


### [284] [Diffusion-Based Imaginative Coordination for Bimanual Manipulation](https://arxiv.org/abs/2507.11296)
*Huilin Xu,Jian Ding,Jiakun Xu,Ruixiang Wang,Jun Chen,Jinjie Mai,Yanwei Fu,Bernard Ghanem,Feng Xu,Mohamed Elhoseiny*

Main category: cs.RO

TL;DR: The paper addresses bimanual robotic manipulation challenges through a diffusion-based framework for joint video and action prediction, achieving notable performance improvements.


<details>
  <summary>Details</summary>
Motivation: The study was motivated by the need to tackle challenges in bimanual robotic manipulation tasks, including high-dimensional action spaces and intricate coordination demands, and to better utilize video prediction for enhancing robotic capabilities.

Method: The method involves a unified, diffusion-based framework emphasizing multi-frame latent prediction. This encodes future states in a latent space to retain task-relevant features, with a unidirectional attention mechanism where video predictions depend on actions but actions are predicted independently.

Result: The proposed approach outperforms the ACT baseline, achieving a 24.9% improvement on ALOHA, 11.1% on RoboTwin, and 32.5% in real-world scenarios, showcasing its effectiveness.

Conclusion: The results highlight the efficacy of the proposed method in improving bimanual coordination for robotic tasks, offering an efficient design by eliminating video prediction during inference. Models and code are made publicly available to facilitate further research.

Abstract: Bimanual manipulation is crucial in robotics, enabling complex tasks in
industrial automation and household services. However, it poses significant
challenges due to the high-dimensional action space and intricate coordination
requirements. While video prediction has been recently studied for
representation learning and control, leveraging its ability to capture rich
dynamic and behavioral information, its potential for enhancing bimanual
coordination remains underexplored. To bridge this gap, we propose a unified
diffusion-based framework for the joint optimization of video and action
prediction. Specifically, we propose a multi-frame latent prediction strategy
that encodes future states in a compressed latent space, preserving
task-relevant features. Furthermore, we introduce a unidirectional attention
mechanism where video prediction is conditioned on the action, while action
prediction remains independent of video prediction. This design allows us to
omit video prediction during inference, significantly enhancing efficiency.
Experiments on two simulated benchmarks and a real-world setting demonstrate a
significant improvement in the success rate over the strong baseline ACT using
our method, achieving a \textbf{24.9\%} increase on ALOHA, an \textbf{11.1\%}
increase on RoboTwin, and a \textbf{32.5\%} increase in real-world experiments.
Our models and code are publicly available at
https://github.com/return-sleep/Diffusion_based_imaginative_Coordination.

</details>


### [285] [All Eyes, no IMU: Learning Flight Attitude from Vision Alone](https://arxiv.org/abs/2507.11302)
*Jesse J. Hagenaars,Stein Stroobants,Sander M. Bohte,Guido C. H. E. De Croon*

Main category: cs.RO

TL;DR: This paper introduces a vision-only flight control approach using a downward-facing event camera on a quadrotor drone, eliminating the need for inertial sensor inputs.


<details>
  <summary>Details</summary>
Motivation: To develop a flight control system that mimics flying animals using vision instead of relying on accelerometers and gyroscopes, which could be beneficial for insect-scale flying robots.

Method: A downward-facing event camera combined with a low-latency recurrent convolutional neural network trained via supervised learning is used to estimate a drone’s attitude and rotation rate for flight control.

Result: Real-world tests demonstrated successful flight stabilization without inertial sensors. The network generalizes across various environments, with memory and horizon-like visual cues yielding better accuracy and narrower fields of view delivering improved generalization.

Conclusion: Vision-only flight control is a viable alternative to traditional inertial-based systems, particularly for enabling autonomous operation in small-scale flying robots.

Abstract: Vision is an essential part of attitude control for many flying animals, some
of which have no dedicated sense of gravity. Flying robots, on the other hand,
typically depend heavily on accelerometers and gyroscopes for attitude
stabilization. In this work, we present the first vision-only approach to
flight control for use in generic environments. We show that a quadrotor drone
equipped with a downward-facing event camera can estimate its attitude and
rotation rate from just the event stream, enabling flight control without
inertial sensors. Our approach uses a small recurrent convolutional neural
network trained through supervised learning. Real-world flight tests
demonstrate that our combination of event camera and low-latency neural network
is capable of replacing the inertial measurement unit in a traditional flight
control loop. Furthermore, we investigate the network's generalization across
different environments, and the impact of memory and different fields of view.
While networks with memory and access to horizon-like visual cues achieve best
performance, variants with a narrower field of view achieve better relative
generalization. Our work showcases vision-only flight control as a promising
candidate for enabling autonomous, insect-scale flying robots.

</details>


### [286] [Acting and Planning with Hierarchical Operational Models on a Mobile Robot: A Study with RAE+UPOM](https://arxiv.org/abs/2507.11345)
*Oscar Lima,Marc Vinci,Sunandita Patra,Sebastian Stock,Joachim Hertzberg,Martin Atzmueller,Malik Ghallab,Dana Nau,Paolo Traverso*

Main category: cs.RO

TL;DR: This paper presents the implementation of an integrated actor-planner system (RAE+UPOM) for robust robotic task execution, tested on a real-world object collection task.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of inconsistency between symbolic planner models and robot control mechanisms in robotic task execution.

Method: Introduce and integrate the Reactive Acting Engine (RAE) with an anytime UCT-like Monte Carlo planner (UPOM) for hierarchical planning and executing actions.

Result: The system demonstrated robustness to action failures and sensor noise, offering empirical insights into interleaved acting and planning processes.

Conclusion: RAE+UPOM integration shows promise for effective, real-world robotic task management, addressing execution challenges through hierarchical operational models.

Abstract: Robotic task execution faces challenges due to the inconsistency between
symbolic planner models and the rich control structures actually running on the
robot. In this paper, we present the first physical deployment of an integrated
actor-planner system that shares hierarchical operational models for both
acting and planning, interleaving the Reactive Acting Engine (RAE) with an
anytime UCT-like Monte Carlo planner (UPOM). We implement RAE+UPOM on a mobile
manipulator in a real-world deployment for an object collection task. Our
experiments demonstrate robust task execution under action failures and sensor
noise, and provide empirical insights into the interleaved acting-and-planning
decision making process.

</details>


### [287] [From Production Logistics to Smart Manufacturing: The Vision for a New RoboCup Industrial League](https://arxiv.org/abs/2507.11402)
*Supun Dissanayaka,Alexander Ferrein,Till Hofmann,Kosuke Nakajima,Mario Sanz-Lopez,Jesus Savage,Daniel Swoboda,Matteo Tschesche,Wataru Uemura,Tarik Viehmann,Shohei Yasuda*

Main category: cs.RO

TL;DR: The paper introduces the RoboCup Smart Manufacturing League, a revamped competition focused on modern factory challenges beyond production logistics.


<details>
  <summary>Details</summary>
Motivation: The diminishing relevance of the RoboCup Logistics League due to its narrow focus on production logistics and outdated alignment with smart manufacturing developments.

Method: Propose a new competition structure with multiple tracks covering contemporary industrial robotics topics like assembly, human-robot collaboration, and humanoid robotics.

Result: A broader, inclusive competition framework designed to address modern manufacturing challenges and attract diverse participating teams.

Conclusion: The new league aims to capture current trends in industrial robotics by expanding its scope, ensuring relevance, and increasing engagement from both new and veteran teams.

Abstract: The RoboCup Logistics League is a RoboCup competition in a smart factory
scenario that has focused on task planning, job scheduling, and multi-agent
coordination. The focus on production logistics allowed teams to develop highly
competitive strategies, but also meant that some recent developments in the
context of smart manufacturing are not reflected in the competition, weakening
its relevance over the years. In this paper, we describe the vision for the
RoboCup Smart Manufacturing League, a new competition designed as a larger
smart manufacturing scenario, reflecting all the major aspects of a modern
factory. It will consist of several tracks that are initially independent but
gradually combined into one smart manufacturing scenario. The new tracks will
cover industrial robotics challenges such as assembly, human-robot
collaboration, and humanoid robotics, but also retain a focus on production
logistics. We expect the reenvisioned competition to be more attractive to
newcomers and well-tried teams, while also shifting the focus to current and
future challenges of industrial robotics.

</details>


### [288] [Multi-IMU Sensor Fusion for Legged Robots](https://arxiv.org/abs/2507.11447)
*Shuo Yang,John Z. Zhang,Ibrahima Sory Sow,Zachary Manchester*

Main category: cs.RO

TL;DR: The paper introduces a robust state-estimation solution for legged robots using low-cost sensors, achieving accurate pose and velocity estimation under challenging conditions.


<details>
  <summary>Details</summary>
Motivation: Existing state-estimation methods for legged robots struggle with accuracy under challenging locomotion conditions, leading to high-drift errors.

Method: The paper proposes using multiple inertial measurement units (IMUs) and joint encoder data, processed through an extended Kalman filter and combined with camera data via a factor-graph-based sliding-window estimator.

Result: The state estimation algorithm achieves minimal position deviation in experiments involving significant ground impact, foot slippage, and sudden body rotations.

Conclusion: The method provides an effective and reliable approach for state estimation in legged robots, validated through theoretical and hardware experiments. A C++ implementation and dataset are publicly available for further research.

Abstract: This paper presents a state-estimation solution for legged robots that uses a
set of low-cost, compact, and lightweight sensors to achieve low-drift pose and
velocity estimation under challenging locomotion conditions. The key idea is to
leverage multiple inertial measurement units on different links of the robot to
correct a major error source in standard proprioceptive odometry. We fuse the
inertial sensor information and joint encoder measurements in an extended
Kalman filter, then combine the velocity estimate from this filter with camera
data in a factor-graph-based sliding-window estimator to form a
visual-inertial-leg odometry method. We validate our state estimator through
comprehensive theoretical analysis and hardware experiments performed using
real-world robot data collected during a variety of challenging locomotion
tasks. Our algorithm consistently achieves minimal position deviation, even in
scenarios involving substantial ground impact, foot slippage, and sudden body
rotations. A C++ implementation, along with a large-scale dataset, is available
at https://github.com/ShuoYangRobotics/Cerberus2.0.

</details>


### [289] [Human-Robot collaboration in surgery: Advances and challenges towards autonomous surgical assistants](https://arxiv.org/abs/2507.11460)
*Jacinto Colan,Ana Davila,Yutaro Yamada,Yasuhisa Hasegawa*

Main category: cs.RO

TL;DR: This paper reviews advancements and challenges in autonomous surgical robotic assistants (ASARs), emphasizing their collaboration with human surgeons and identifying future research directions.


<details>
  <summary>Details</summary>
Motivation: To examine the growing potential and challenges of autonomous robotic systems in surgery, particularly where meaningful collaboration with human surgeons is required.

Method: A systematic review adhering to PRISMA guidelines, analyzing 32 studies from IEEE Xplore, Scopus, and Web of Science databases, focusing on teleoperation-based assistance and hands-on interaction setups.

Result: The study finds significant progress in endoscope guidance and emerging tool manipulation, alongside challenges like aligning robotic actions with surgeons, ensuring procedural awareness, and improving human-robot communication and skill sharing.

Conclusion: The review highlights the need to address key limitations and proposes future research directions to enhance the reliability, safety, and effectiveness of autonomous robotic collaboration in surgeries.

Abstract: Human-robot collaboration in surgery represents a significant area of
research, driven by the increasing capability of autonomous robotic systems to
assist surgeons in complex procedures. This systematic review examines the
advancements and persistent challenges in the development of autonomous
surgical robotic assistants (ASARs), focusing specifically on scenarios where
robots provide meaningful and active support to human surgeons. Adhering to the
PRISMA guidelines, a comprehensive literature search was conducted across the
IEEE Xplore, Scopus, and Web of Science databases, resulting in the selection
of 32 studies for detailed analysis. Two primary collaborative setups were
identified: teleoperation-based assistance and direct hands-on interaction. The
findings reveal a growing research emphasis on ASARs, with predominant
applications currently in endoscope guidance, alongside emerging progress in
autonomous tool manipulation. Several key challenges hinder wider adoption,
including the alignment of robotic actions with human surgeon preferences, the
necessity for procedural awareness within autonomous systems, the establishment
of seamless human-robot information exchange, and the complexities of skill
acquisition in shared workspaces. This review synthesizes current trends,
identifies critical limitations, and outlines future research directions
essential to improve the reliability, safety, and effectiveness of human-robot
collaboration in surgical environments.

</details>


### [290] [LF: Online Multi-Robot Path Planning Meets Optimal Trajectory Control](https://arxiv.org/abs/2507.11464)
*Ajay Shankar,Keisuke Okumura,Amanda Prorok*

Main category: cs.RO

TL;DR: The paper introduces a hierarchical, asynchronous framework for scalable multi-robot navigation, leveraging MAPF solvers for path planning and robust trajectory controllers for execution.


<details>
  <summary>Details</summary>
Motivation: The need to efficiently and reliably address multi-robot navigation tasks in dynamic settings while avoiding collisions and deadlocks.

Method: A two-process hierarchical framework combining a discrete centralized MAPF solver for planning and continuous dynamics-aware trajectory controllers for execution.

Result: Implementation of LF, a system combining LaCAM and Freyja, demonstrated with multirotor and ground robot tests, including real-world deployment of 15 multirotors with dynamic target updates.

Conclusion: LF showcases robustness, adaptability, and scalability for lifelong multi-robot navigation tasks, accommodating asynchronous updates and dynamic workspaces through quick replanning.

Abstract: We propose a multi-robot control paradigm to solve point-to-point navigation
tasks for a team of holonomic robots with access to the full environment
information. The framework invokes two processes asynchronously at high
frequency: (i) a centralized, discrete, and full-horizon planner for computing
collision- and deadlock-free paths rapidly, leveraging recent advances in
multi-agent pathfinding (MAPF), and (ii) dynamics-aware, robot-wise optimal
trajectory controllers that ensure all robots independently follow their
assigned paths reliably. This hierarchical shift in planning representation
from (i) discrete and coupled to (ii) continuous and decoupled domains enables
the framework to maintain long-term scalable motion synthesis. As an
instantiation of this idea, we present LF, which combines a fast
state-of-the-art MAPF solver (LaCAM), and a robust feedback control stack
(Freyja) for executing agile robot maneuvers. LF provides a robust and
versatile mechanism for lifelong multi-robot navigation even under asynchronous
and partial goal updates, and adapts to dynamic workspaces simply by quick
replanning. We present various multirotor and ground robot demonstrations,
including the deployment of 15 real multirotors with random, consecutive target
updates while a person walks through the operational workspace.

</details>


### [291] [Robot Drummer: Learning Rhythmic Skills for Humanoid Drumming](https://arxiv.org/abs/2507.11498)
*Asad Ali Shahid,Francesco Braghin,Loris Roveda*

Main category: cs.RO

TL;DR: This paper introduces a humanoid robot, Robot Drummer, designed to perform expressive, high-precision drumming across various musical genres using reinforcement learning to achieve human-like drumming strategies.


<details>
  <summary>Details</summary>
Motivation: Despite significant advances in humanoid robotics, their role in expressive domains such as musical performance remains underexplored. Drumming presents specific challenges like precise timing, quick contacts, and multi-limb coordination.

Method: The authors formalized drumming as timed-contacts and used rhythmic contact chains derived from drum scores. To address the long-horizon nature of music, they segmented songs into manageable parts and trained a reinforcement learning policy on these segments in parallel.

Result: Robot Drummer consistently delivers high F1 scores on over 30 popular rock, metal, and jazz tracks, demonstrating high precision and emergent drumming strategies like cross-arm strikes and adaptive stick usage.

Conclusion: The study highlights how reinforcement learning can enable humanoid robots to perform creatively in musical domains, showcasing human-like strategies and adaptability in drumming performance.

Abstract: Humanoid robots have seen remarkable advances in dexterity, balance, and
locomotion, yet their role in expressive domains, such as music performance,
remains largely unexplored. Musical tasks, like drumming, present unique
challenges, including split-second timing, rapid contacts, and multi-limb
coordination over pieces lasting minutes. In this paper, we introduce Robot
Drummer, a humanoid system capable of expressive, high-precision drumming
across a diverse repertoire of songs. We formulate humanoid drumming as
sequential fulfillment of timed-contacts and transform drum scores in to a
Rhythmic Contact Chain. To handle the long-horizon nature of musical
performance, we decompose each piece into fixed-length segments and train a
single policy across all segments in parallel using reinforcement learning.
Through extensive experiments on over thirty popular rock, metal, and jazz
tracks, our results demonstrate that Robot Drummer consistently achieves high
F1 scores. The learned behaviors exhibit emergent human-like drumming
strategies, such as cross-arm strikes, and adaptive sticks assignments,
demonstrating the potential of reinforcement learning to bring humanoid robots
into the domain of creative musical performance. Project page:
\href{https://robot-drummer.github.io}{robot-drummer.github.io}

</details>


### [292] [LLM-based ambiguity detection in natural language instructions for collaborative surgical robots](https://arxiv.org/abs/2507.11525)
*Ana Davila,Jacinto Colan,Yasuhisa Hasegawa*

Main category: cs.RO

TL;DR: The paper presents a framework that leverages Large Language Models (LLMs) to detect ambiguities in surgical instructions to enhance human-robot collaboration in surgery.


<details>
  <summary>Details</summary>
Motivation: Ambiguity in natural language instructions can lead to errors in safety-critical environments, such as surgery, where precise communication between humans and robots is crucial.

Method: The framework uses an ensemble of LLM evaluators with different prompting techniques. It incorporates a chain-of-thought evaluator to systematically examine instructions and synthesizes results using conformal prediction based on calibration datasets.

Result: This method achieved over 60% classification accuracy in identifying ambiguous versus unambiguous surgical instructions, using Llama 3.2 11B and Gemma 3 12B models.

Conclusion: The framework helps improve the safety and reliability of surgical human-robot interactions by preemptively identifying ambiguous instructions before actions are executed by robots.

Abstract: Ambiguity in natural language instructions poses significant risks in
safety-critical human-robot interaction, particularly in domains such as
surgery. To address this, we propose a framework that uses Large Language
Models (LLMs) for ambiguity detection specifically designed for collaborative
surgical scenarios. Our method employs an ensemble of LLM evaluators, each
configured with distinct prompting techniques to identify linguistic,
contextual, procedural, and critical ambiguities. A chain-of-thought evaluator
is included to systematically analyze instruction structure for potential
issues. Individual evaluator assessments are synthesized through conformal
prediction, which yields non-conformity scores based on comparison to a labeled
calibration dataset. Evaluating Llama 3.2 11B and Gemma 3 12B, we observed
classification accuracy exceeding 60% in differentiating ambiguous from
unambiguous surgical instructions. Our approach improves the safety and
reliability of human-robot collaboration in surgery by offering a mechanism to
identify potentially ambiguous instructions before robot action.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [293] [$\texttt{Droid}$: A Resource Suite for AI-Generated Code Detection](https://arxiv.org/abs/2507.10583)
*Daniil Orel,Indraneil Paul,Iryna Gurevych,Preslav Nakov*

Main category: cs.SE

TL;DR: The study presents DroidCollection, a dataset with over a million code samples from diverse languages and domains, and DroidDetect, robust machine-generated code detectors.


<details>
  <summary>Details</summary>
Motivation: Existing detectors for machine-generated code do not generalize well across diverse programming languages and domains, making it necessary to establish a robust benchmark and enhance detector capabilities.

Method: Compiled DroidCollection, a large dataset comprising AI-generated and adversarial code samples. Developed DroidDetect detectors using multi-task training, metric learning, and uncertainty-based resampling.

Result: Experiments revealed the limitations of current detectors, such as vulnerability to adversarial samples. The study found that training on small amounts of tailored adversarial data enhances robustness.

Conclusion: The proposed DroidDetect detectors generalize effectively across diverse domains when trained on appropriate datasets like DroidCollection, emphasizing the importance of tailored adversarial training.

Abstract: In this work, we compile $\textbf{$\texttt{DroidCollection}$}$, the most
extensive open data suite for training and evaluating machine-generated code
detectors, comprising over a million code samples, seven programming languages,
outputs from 43 coding models, and over three real-world coding domains.
Alongside fully AI-generated samples, our collection includes human-AI
co-authored code, as well as adversarial samples explicitly crafted to evade
detection. Subsequently, we develop $\textbf{$\texttt{DroidDetect}$}$, a suite
of encoder-only detectors trained using a multi-task objective over
$\texttt{DroidCollection}$. Our experiments show that existing detectors'
performance fails to generalise to diverse coding domains and programming
languages outside of their narrow training data. Additionally, we demonstrate
that while most detectors are easily compromised by humanising the output
distributions using superficial prompting and alignment approaches, this
problem can be easily amended by training on a small amount of adversarial
data. Finally, we demonstrate the effectiveness of metric learning and
uncertainty-based resampling as means to enhance detector training on possibly
noisy distributions.

</details>


### [294] [ARPaCCino: An Agentic-RAG for Policy as Code Compliance](https://arxiv.org/abs/2507.10584)
*Francesco Romeo,Luigi Arena,Francesco Blefari,Francesco Aurelio Pironti,Matteo Lupinacci,Angelo Furfaro*

Main category: cs.SE

TL;DR: ARPaCCino leverages Large Language Models (LLMs), Retrieval-Augmented-Generation (RAG), and external tools to automate the generation, verification, and refinement of Policy as Code (PaC) rules, boosting reliability and accessibility.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenges in adopting Policy as Code (PaC) due to complex policy languages and risks of misconfigurations in IaC environments.

Method: ARPaCCino uses a modular agentic system combining LLMs, RAG, and tool-based validation to generate formal policies from natural language, check compliance, and adjust configurations iteratively.

Result: Experimental results demonstrate the system's ability to create correct policies, identify non-compliant infrastructures, and make corrective modifications effectively, even with smaller LLMs.

Conclusion: The study underscores the promise of agentic RAG frameworks in improving the automation and dependability of PaC processes across diverse infrastructures.

Abstract: Policy as Code (PaC) is a paradigm that encodes security and compliance
policies into machine-readable formats, enabling automated enforcement in
Infrastructure as Code (IaC) environments. However, its adoption is hindered by
the complexity of policy languages and the risk of misconfigurations. In this
work, we present ARPaCCino, an agentic system that combines Large Language
Models (LLMs), Retrieval-Augmented-Generation (RAG), and tool-based validation
to automate the generation and verification of PaC rules. Given natural
language descriptions of the desired policies, ARPaCCino generates formal Rego
rules, assesses IaC compliance, and iteratively refines the IaC configurations
to ensure conformance. Thanks to its modular agentic architecture and
integration with external tools and knowledge bases, ARPaCCino supports policy
validation across a wide range of technologies, including niche or emerging IaC
frameworks. Experimental evaluation involving a Terraform-based case study
demonstrates ARPaCCino's effectiveness in generating syntactically and
semantically correct policies, identifying non-compliant infrastructures, and
applying corrective modifications, even when using smaller, open-weight LLMs.
Our results highlight the potential of agentic RAG architectures to enhance the
automation, reliability, and accessibility of PaC workflows.

</details>


### [295] [Repairing Language Model Pipelines by Meta Self-Refining Competing Constraints at Runtime](https://arxiv.org/abs/2507.10590)
*Mojtaba Eshghie*

Main category: cs.SE

TL;DR: Meta Self-Refining addresses inefficiencies in LM pipelines by introducing a meta-corrective layer to repair conflicts from competing soft constraints.


<details>
  <summary>Details</summary>
Motivation: Current LM pipelines fail when competing constraints lead to oscillatory failures, creating backtracking inefficiencies.

Method: Meta Self-Refining uses a meta-repairer implemented as a layer that monitors pipeline execution history and synthesizes self-repair instructions to resolve competing requirements.

Result: The framework effectively balances competing constraints and resolves backtracking failures, enhancing efficiency in LM programs.

Conclusion: Meta Self-Refining demonstrates the ability to improve LM pipeline outputs by strategically navigating conflicts between soft constraints.

Abstract: Language Model (LM) pipelines can dynamically refine their outputs against
programmatic constraints. However, their effectiveness collapses when faced
with competing soft constraints, leading to inefficient backtracking loops
where satisfying one constraint violates another. We introduce Meta
Self-Refining, a framework that equips LM pipelines with a meta-corrective
layer to repair these competitions at runtime/inference-time. Our approach
monitors the pipeline's execution history to detect oscillatory failures. Upon
detection, it invokes a meta-repairer LM that analyzes the holistic state of
the backtracking attempts and synthesizes a strategic instruction to balance
the competing requirements. This self-repair instruction guides the original LM
out of a failing refining loop towards a successful output. Our results show
Meta Self-Refining can successfully repair these loops, leading to more
efficient LM programs.

</details>


### [296] [ToolRegistry: A Protocol-Agnostic Tool Management Library for Function-Calling LLMs](https://arxiv.org/abs/2507.10593)
*Peng Ding*

Main category: cs.SE

TL;DR: Toolregistry is a library designed to facilitate the integration and management of external tools for LLM applications, reducing code complexity and improving performance.


<details>
  <summary>Details</summary>
Motivation: The reliance on external tools for enhancing LLM capabilities has resulted in fragmented approaches that impose significant development overhead.

Method: Toolregistry provides a unified interface for tool registration, execution, and lifecycle management, along with protocol-independent compatibility and optimizations for concurrent execution.

Result: Toolregistry reduces integration code by 60-80%, offers up to 3.1x performance improvements, ensures compatibility with OpenAI standards, and enhances code maintainability in real-world scenarios.

Conclusion: The open-source Toolregistry library simplifies tool integration, improves development efficiency, and supports diverse LLM application requirements with a robust and unified approach.

Abstract: Large Language Model (LLM) applications are increasingly relying on external
tools to extend their capabilities beyond text generation. However, current
tool integration approaches suffer from fragmentation, protocol limitations,
and implementation complexity, leading to substantial development overhead.
This paper presents Toolregistry, a protocol-agnostic tool management library
that simplifies tool registration, representation, execution, and lifecycle
management via a unified interface. Our evaluation demonstrates that
\toolregistry achieves 60-80% reduction in tool integration code, up to 3.1x
performance improvements through concurrent execution, and 100% compatibility
with OpenAI function calling standards. Real-world case studies show
significant improvements in development efficiency and code maintainability
across diverse integration scenarios. \toolregistry is open-source and
available at https://github.com/Oaklight/ToolRegistry, with comprehensive
documentation at https://toolregistry.readthedocs.io/.

</details>


### [297] [SENSOR: An ML-Enhanced Online Annotation Tool to Uncover Privacy Concerns from User Reviews in Social-Media Applications](https://arxiv.org/abs/2507.10640)
*Labiba Farah,Mohammad Ridwan Kabir,Shohel Ahmed,MD Mohaymen Ul Anam,Md. Sakibul Islam*

Main category: cs.SE

TL;DR: The paper introduces "SENSOR," an automated tool to classify privacy-relevant user reviews from social media apps using a new machine learning model, "GRACE," achieving high precision and accuracy.


<details>
  <summary>Details</summary>
Motivation: Manual identification of privacy-related concerns in user reviews for apps is time-consuming due to the high volume and nuanced nature of reviews, necessitating an automated solution.

Method: The authors created SENSOR, an online annotation tool, and developed "GRACE," a GRU-based machine learning model with CBOW embedding and an attention mechanism to classify privacy-related reviews.

Result: GRACE showed excellent performance on manually labeled datasets, achieving a macro F1-score of 0.9434, ROC-AUC of 0.9934, and accuracy of 95.10%, successfully managing class imbalance.

Conclusion: SENSOR, powered by GRACE, effectively aids in identifying privacy-related user review concerns, allowing developers to enhance privacy-centric features and build trust.

Abstract: The widespread use of social media applications has raised significant
privacy concerns, often highlighted in user reviews. These reviews also provide
developers with valuable insights into improving apps by addressing issues and
introducing better features. However, the sheer volume and nuanced nature of
reviews make manual identification and prioritization of privacy-related
concerns challenging for developers. Previous studies have developed software
utilities to automatically classify user reviews as privacy-relevant,
privacy-irrelevant, bug reports, feature requests, etc., using machine
learning. Notably, there is a lack of focus on classifying reviews specifically
as privacy-related feature requests, privacy-related bug reports, or
privacy-irrelevant. This paper introduces SENtinel SORt (SENSOR), an automated
online annotation tool designed to help developers annotate and classify user
reviews into these categories. For automating the annotation of such reviews,
this paper introduces the annotation model, GRACE (GRU-based Attention with
CBOW Embedding), using Gated Recurrent Units (GRU) with Continuous Bag of Words
(CBOW) and Attention mechanism. Approximately 16000 user reviews from seven
popular social media apps on Google Play Store, including Instagram, Facebook,
WhatsApp, Snapchat, X (formerly Twitter), Facebook Lite, and Line were
analyzed. Two annotators manually labelled the reviews, achieving a Cohen's
Kappa value of 0.87, ensuring a labeled dataset with high inter-rater agreement
for training machine learning models. Among the models tested, GRACE
demonstrated the best performance (macro F1-score: 0.9434, macro ROC-AUC:
0.9934, and accuracy: 95.10%) despite class imbalance. SENSOR demonstrates
significant potential to assist developers with extracting and addressing
privacy-related feature requests or bug reports from user reviews, enhancing
user privacy and trust.

</details>


### [298] [A Code Comprehension Benchmark for Large Language Models for Code](https://arxiv.org/abs/2507.10641)
*Jayant Havare,Saurav Chaudhary,Ganesh Ramakrishnan,Kaushik Maharajan,Srikanth Tamilselvam*

Main category: cs.SE

TL;DR: Large Language Models excel at syntactic tasks but struggle with code comprehension. Fine-tuning boosts semantic understanding, evidenced by notable accuracy improvements in various models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of Large Language Models in semantic understanding for code-related tasks, exploring a solution for improved code comprehension.

Method: Fine-tuning existing code models on large-scale datasets specifically tailored for semantic comprehension tasks, followed by evaluation on specific benchmarks.

Result: Accuracy improvements: QWQ-32B model from 70% to 83.47%, and DPO-fine-tuned Codestral-22B achieving a micro-accuracy of 87.66%.

Conclusion: Fine-tuning enhances code comprehension significantly, confirming its importance for tasks requiring semantic understanding beyond syntax.

Abstract: Large Language Models have shown impressive capabilities in coding tasks like
code generation and code completion, as they have been trained on a large
amount of code data. Also, since one of the core pretraining objectives is Next
Token Prediction, these models tends to learn surface-level syntactic patterns
in code. However, this does not guarantee code comprehension ability i.e. the
ability to capture the semantics of the code. In our opinion, this is the
reason why these models often underperform on tasks that require deeper
semantic understanding, such as code debugging and code optimization. To
address this, we propose fine-tuning these models specifically for code
comprehension tasks using large-scale datasets, enabling them to develop a more
robust understanding of code semantics. We evaluate three code models of
varying sizes on a suite of code comprehension tasks designed to assess
semantic understanding beyond surface-level syntactic pattern matching. In
particular, we analyze performance on the Subjectivity Grading Task and observe
that model performance improves after fine-tuning on relevant downstream tasks.
The most significant improvement is seen in the QWQ-32B model, where accuracy
increases from 70% to 83.47%. A similar or explainable trend is observed across
other models, clearly indicating an enhancement in code comprehension ability.
Among the models studied, the DPO-fine-tuned Codestral-22B achieves the highest
micro-accuracy of 87.66% on the Subjectivity Grading Task.

</details>


### [299] [CodeAssistBench (CAB): Dataset & Benchmarking for Multi-turn Chat-Based Code Assistance](https://arxiv.org/abs/2507.10646)
*Myeongsoo Kim,Shweta Garg,Baishakhi Ray,Varun Kumar,Anoop Deoras*

Main category: cs.SE

TL;DR: The paper introduces CodeAssistBench (CAB), a benchmark framework evaluating multi-turn programming assistance in real project settings, and demonstrates a significant gap in current LLM capabilities for complex, specific tasks compared to simpler questions.


<details>
  <summary>Details</summary>
Motivation: Existing programming assistant benchmarks focus on narrow, single-turn tasks and fail to represent real-world project environments or multi-turn interactions.

Method: The authors created CAB, a scalable benchmark using GitHub issues to simulate real-world programming queries. The framework includes automatic containerization of codebases and evaluates models in these environments with full codebase access.

Result: CAB comprises 3,286 real-world programming questions from 231 repositories across seven programming languages. LLM evaluation shows a success rate of only 16.49% on CAB issues compared to 70-83% on Stack Overflow queries, indicating a capability gap in handling complex, contextual tasks.

Conclusion: Current LLM programming assistants struggle with multi-turn, project-specific problems compared to isolated Q&A, underscoring the need for improved benchmarks and models for real-world software development contexts.

Abstract: Programming assistants powered by large language models have transformed
software development, yet most benchmarks focus narrowly on code generation
tasks. Recent efforts like InfiBench and StackEval attempt to address this gap
using Stack Overflow data but remain limited to single-turn interactions in
isolated contexts, require significant manual curation, and fail to represent
complete project environments. We introduce CodeAssistBench (CAB), the first
benchmark framework for evaluating multi-turn programming assistance in
realistic settings that address real-world questions about actual codebases.
Unlike existing programming Q&A benchmarks, CAB automatically generates
scalable datasets from question-related GitHub issues using configurable
parameters (e.g., repository creation date, star count, programming languages),
and includes automatic containerization of codebases for evaluation. It then
evaluates models through simulated users in these containerized environments
with full codebase access. Using this framework, we constructed a test set of
3,286 real-world programming questions across 231 repositories, spanning seven
programming languages and diverse problem domains. Our evaluation of leading
LLMs reveals a substantial capability gap: while models perform well on Stack
Overflow questions with success rates of 70-83%, they resolve only up to 16.49%
of CAB's recent issues. This discrepancy highlights the challenges of providing
assistance in complex, project-specific contexts versus answering standalone
questions.

</details>


### [300] [Toward Realistic Evaluations of Just-In-Time Vulnerability Prediction](https://arxiv.org/abs/2507.10729)
*Duong Nguyen,Thanh Le-Cong,Triet Huynh Minh Le,M. Ali Babar,Quyet-Thang Huynh*

Main category: cs.SE

TL;DR: The study evaluates Just-in-Time Vulnerability Prediction (JIT-VP) in realistic settings, revealing severe performance drop due to class imbalance in real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations for JIT-VP use idealized and artificially balanced datasets, failing to reflect the complexity of real-world software system commits.

Method: The authors use a realistic dataset of over one million commits, including vulnerability-related and neutral commits, and analyze the performance of eight JIT-VP techniques. They also test techniques for mitigating data imbalance.

Result: Empirical analysis shows a 98% performance drop (PR-AUC: 0.805 to 0.016) due to class imbalance. Common techniques for addressing imbalance, such as oversampling and customized loss functions, prove ineffective.

Conclusion: Real-world evaluations of JIT-VP are essential, and domain-specific approaches are needed to tackle dataset imbalance effectively.

Abstract: Modern software systems are increasingly complex, presenting significant
challenges in quality assurance. Just-in-time vulnerability prediction (JIT-VP)
is a proactive approach to identifying vulnerable commits and providing early
warnings about potential security risks. However, we observe that current
JIT-VP evaluations rely on an idealized setting, where the evaluation datasets
are artificially balanced, consisting exclusively of vulnerability-introducing
and vulnerability-fixing commits.
  To address this limitation, this study assesses the effectiveness of JIT-VP
techniques under a more realistic setting that includes both
vulnerability-related and vulnerability-neutral commits. To enable a reliable
evaluation, we introduce a large-scale public dataset comprising over one
million commits from FFmpeg and the Linux kernel. Our empirical analysis of
eight state-of-the-art JIT-VP techniques reveals a significant decline in
predictive performance when applied to real-world conditions; for example, the
average PR-AUC on Linux drops 98\% from 0.805 to 0.016. This discrepancy is
mainly attributed to the severe class imbalance in real-world datasets, where
vulnerability-introducing commits constitute only a small fraction of all
commits.
  To mitigate this issue, we explore the effectiveness of widely adopted
techniques for handling dataset imbalance, including customized loss functions,
oversampling, and undersampling. Surprisingly, our experimental results
indicate that these techniques are ineffective in addressing the imbalance
problem in JIT-VP. These findings underscore the importance of realistic
evaluations of JIT-VP and the need for domain-specific techniques to address
data imbalance in such scenarios.

</details>


### [301] [GenAI-Enabled Backlog Grooming in Agile Software Projects: An Empirical Study](https://arxiv.org/abs/2507.10753)
*Kasper Lien Oftebro,Anh Nguyen-Duc,Kai-Kristian Kemell*

Main category: cs.SE

TL;DR: The paper presents a study on using a generative AI assistant to automate backlog grooming in Agile software development, achieving notable improvements in efficiency and precision.


<details>
  <summary>Details</summary>
Motivation: Product backlogs in Agile software projects often become overly complex and cluttered, making prioritization and decision-making difficult.

Method: The study developed a Jira plug-in leveraging a vector database and GPT-4o to automatically identify and address redundant or outdated backlog tasks using cosine similarity.

Result: The AI tool achieved 100% precision in grooming and reduced the time-to-completion of the process by 45%.

Conclusion: AI-assisted backlog grooming effectively enhances efficiency and accuracy, demonstrating the potential to streamline Agile development processes and improve user experiences.

Abstract: Effective backlog management is critical for ensuring that development teams
remain aligned with evolving requirements and stakeholder expectations.
However, as product backlogs consistently grow in scale and complexity, they
tend to become cluttered with redundant, outdated, or poorly defined tasks,
complicating prioritization and decision making processes. This study
investigates whether a generative-AI (GenAI) assistant can automate backlog
grooming in Agile software projects without sacrificing accuracy or
transparency. Through Design Science cycles, we developed a Jira plug-in that
embeds backlog issues with the vector database, detects duplicates via cosine
similarity, and leverage the GPT-4o model to propose merges, deletions, or new
issues. We found that AI-assisted backlog grooming achieved 100 percent
precision while reducing the time-to-completion by 45 percent. The findings
demonstrated the tool's potential to streamline backlog refinement processes
while improving user experiences.

</details>


### [302] [Towards a Closer Collaboration Between Practice and Research in Agile Software Development Workshop: A Summary and Research Agenda](https://arxiv.org/abs/2507.10785)
*Michael Neumann,Eva-Maria Schön,Mali Senapathi,Maria Rauschenberger,Tiago Silva da Silva*

Main category: cs.SE

TL;DR: This paper explores the gap between research and practice in agile software development through insights from an international workshop.


<details>
  <summary>Details</summary>
Motivation: To address the gap between research and implementation in agile software development and identify ways to bridge it.

Method: Findings and themes were collected from discussions during the first international workshop involving researchers and practitioners.

Result: Key factors contributing to the research-practice gap were identified, along with strategies to address these issues.

Conclusion: Collaborative efforts are necessary to align agile research with practical needs, and further study is required for unresolved challenges.

Abstract: Agile software development principles and values have been widely adopted
across various industries, influencing products and services globally. Despite
its increasing popularity, a significant gap remains between research and
practical implementation. This paper presents the findings of the first
international workshop designed to foster collaboration between research and
practice in agile software development. We discuss the main themes and factors
identified by the workshop participants that contribute to this gap, strategies
to bridge it, and the challenges that require further research attention.

</details>


### [303] [How Robust are LLM-Generated Library Imports? An Empirical Study using Stack Overflow](https://arxiv.org/abs/2507.10818)
*Jasmine Latendresse,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: The paper evaluates how six LLMs recommend Python libraries, finding that they mostly prefer third-party, mature, and popular libraries but lack usability with dependency resolution and installation guidance.


<details>
  <summary>Details</summary>
Motivation: To understand and improve how LLMs assist developers in recommending libraries during programming tasks.

Method: An empirical study of LLMs by solving real-world Python problems from Stack Overflow, analyzing their library suggestions and usability.

Result: LLMs primarily recommend mature and popular third-party libraries but have structural mismatches (4.6% of libraries) and limited installation guidance.

Conclusion: LLMs generate technically valid code but need improved contextual support to reduce manual resolution burdens and enhance usability in software dependencies.

Abstract: Software libraries are central to the functionality, security, and
maintainability of modern code. As developers increasingly turn to Large
Language Models (LLMs) to assist with programming tasks, understanding how
these models recommend libraries is essential. In this paper, we conduct an
empirical study of six state-of-the-art LLMs, both proprietary and open-source,
by prompting them to solve real-world Python problems sourced from Stack
Overflow. We analyze the types of libraries they import, the characteristics of
those libraries, and the extent to which the recommendations are usable out of
the box. Our results show that LLMs predominantly favour third-party libraries
over standard ones, and often recommend mature, popular, and permissively
licensed dependencies. However, we also identify gaps in usability: 4.6% of the
libraries could not be resolved automatically due to structural mismatches
between import names and installable packages, and only two models (out of six)
provided installation guidance. While the generated code is technically valid,
the lack of contextual support places the burden of manually resolving
dependencies on the user. Our findings offer actionable insights for both
developers and researchers, and highlight opportunities to improve the
reliability and usability of LLM-generated code in the context of software
dependencies.

</details>


### [304] [Past, Present and Future: Exploring Adaptive AI in Software Development Bots](https://arxiv.org/abs/2507.10822)
*Omar Elsisi,Glaucia Melo*

Main category: cs.SE

TL;DR: Adaptive AI-powered conversational agents are transforming software development by offering personalized, dynamic support and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: To explore how adaptive AI conversational agents enhance software development through dynamic, context-aware assistance.

Method: Analyzed the evolution and integration of adaptive AI systems, like GitHub Copilot, in software development and addressed challenges and ethical concerns.

Result: Adaptive AI chatbots provide real-time, customized support to developers while improving productivity and collaboration.

Conclusion: These agents have the potential to revolutionize software development despite challenges related to privacy and ethics.

Abstract: Conversational agents, such as chatbots and virtual assistants, have become
essential in software development, boosting productivity, collaboration, and
automating various tasks. This paper examines the role of adaptive AI-powered
conversational agents in software development, highlighting their ability to
offer dynamic, context-aware assistance to developers. Unlike traditional
rule-based systems, adaptive AI agents use machine learning and natural
language processing to learn from interactions and improve over time, providing
more personalized and responsive help. We look at how these tools have evolved
from simple query-based systems to advanced AI-driven solutions like GitHub
Copilot and Microsoft Teams bots. We also explore the challenges of integrating
adaptive AI into software development processes. The study aims to assess the
benefits and limitations of these systems, address concerns like data privacy
and ethical issues, and offer insights into their future use in the field.
Ultimately, adaptive AI chatbots have great potential to revolutionize software
development by delivering real-time, customized support and enhancing the
efficiency of development cycles.

</details>


### [305] [Evaluating Generated Commit Messages with Large Language Models](https://arxiv.org/abs/2507.10906)
*Qunhong Zeng,Yuxia Zhang,Zexiong Ma,Bo Jiang,Ningyuan Sun,Klaas-Jan Stol,Xingyu Mou,Hui Liu*

Main category: cs.SE

TL;DR: The study explores improving commit message evaluations using Large Language Models (LLMs), which outperform traditional automatic metrics and approach human-level proficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of traditional reference-based metrics for commit message quality evaluation, which often lead to resource-intensive, subjective human evaluations.

Method: The paper uses systematic experimentation with state-of-the-art LLMs and employs techniques like Chain-of-Thought reasoning and few-shot demonstrations to evaluate commit message quality.

Result: LLMs demonstrated near human-level proficiency in evaluating commit messages and exceeded traditional metrics in robustness, reproducibility, and fairness.

Conclusion: The study concludes that LLMs provide a scalable, effective alternative to human evaluations for commit message assessment, though some variability remains.

Abstract: Commit messages are essential in software development as they serve to
document and explain code changes. Yet, their quality often falls short in
practice, with studies showing significant proportions of empty or inadequate
messages. While automated commit message generation has advanced significantly,
particularly with Large Language Models (LLMs), the evaluation of generated
messages remains challenging. Traditional reference-based automatic metrics
like BLEU, ROUGE-L, and METEOR have notable limitations in assessing commit
message quality, as they assume a one-to-one mapping between code changes and
commit messages, leading researchers to rely on resource-intensive human
evaluation. This study investigates the potential of LLMs as automated
evaluators for commit message quality. Through systematic experimentation with
various prompt strategies and state-of-the-art LLMs, we demonstrate that LLMs
combining Chain-of-Thought reasoning with few-shot demonstrations achieve near
human-level evaluation proficiency. Our LLM-based evaluator significantly
outperforms traditional metrics while maintaining acceptable reproducibility,
robustness, and fairness levels despite some inherent variability. This work
conducts a comprehensive preliminary study on using LLMs for commit message
evaluation, offering a scalable alternative to human assessment while
maintaining high-quality evaluation.

</details>


### [306] [SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language Models on Software Engineering Tasks](https://arxiv.org/abs/2507.11059)
*Pavel Adamenko,Mikhail Ivanov,Aidar Valeev,Rodion Levichev,Pavel Zadorozhny,Ivan Lopatin,Dmitry Babayev,Alena Fenogenova,Valentin Malykh*

Main category: cs.SE

TL;DR: SWE-MERA is a new, dynamic benchmark for software engineering LLMs, addressing contamination in existing datasets like SWE-bench by automating quality validation and task collection.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks like SWE-bench are severely compromised due to data contamination, rendering them unreliable for evaluating LLMs in software engineering.

Method: SWE-MERA uses an automated pipeline to collect GitHub issues and apply rigorous quality checks, minimizing contamination and generating dynamic tasks for a more reliable benchmark.

Result: SWE-MERA provides 10,000 potential tasks with 300 samples, showing strong discriminative power in tests using various LLMs.

Conclusion: SWE-MERA addresses fundamental issues with existing benchmarks, ensuring more reliable evaluation of software engineering LLMs.

Abstract: The rapid advancement of Large Language Models (LLMs) in software engineering
has revealed critical limitations in existing benchmarks, particularly the
widely used SWE-bench dataset. Recent studies have uncovered severe data
contamination issues, e.g. SWE-bench reports 32.67% of successful patches
involve direct solution leakage and 31.08\% pass due to inadequate test cases.
We introduce SWE-MERA, a dynamic, continuously updated benchmark designed to
address these fundamental challenges through an automated collection of
real-world GitHub issues and rigorous quality validation. Our approach
implements a reliable pipeline that ensures quality while minimizing
contamination risks, resulting in approximately 10,000 potential tasks with 300
samples currently available. Evaluation using the Aider coding agent
demonstrates strong discriminative power in state-of-the-art models. We report
performance across a dozen recent LLMs evaluated on tasks collected between
September 2024 and June 2025.

</details>


### [307] [MT4DP: Data Poisoning Attack Detection for DL-based Code Search Models via Metamorphic Testing](https://arxiv.org/abs/2507.11092)
*Gong Chen,Wenjie Liu,Xiaoyuan Xie,Xunzhu Tang,Tegawendé F. Bissyandé,Songqiang Chen*

Main category: cs.SE

TL;DR: MT4DP is a novel framework leveraging metamorphic testing to detect data poisoning attacks on DL-based code search models, significantly improving detection performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the critical security threat posed by data poisoning attacks on DL-based code search models, which existing detection methods fail to handle effectively.

Method: This paper proposes MT4DP, utilizing Semantically Equivalent Metamorphic Relation (SE-MR) to analyze variances in re-ranked code search results for detecting violations indicative of data poisoning attacks.

Result: MT4DP achieves superior performance in detecting data poisoning attacks, offering improvements of 191% in average F1 score and 265% in average precision over existing baselines.

Conclusion: MT4DP highlights the importance of defensive measures against data poisoning in DL-based code search models and sets a foundation for advancing research in this domain.

Abstract: Recently, several studies have indicated that data poisoning attacks pose a
severe security threat to deep learning-based (DL-based) code search models.
Attackers inject carefully crafted malicious patterns into the training data,
misleading the code search model to learn these patterns during training.
During the usage of the poisoned code search model for inference, once the
malicious pattern is triggered, the model tends to rank the vulnerability code
higher. However, existing detection methods for data poisoning attacks on
DL-based code search models remain insufficiently effective. To address this
critical security issue, we propose MT4DP, a Data Poisoning Attack Detection
Framework for DL-based Code Search Models via Metamorphic Testing. MT4DP
introduces a novel Semantically Equivalent Metamorphic Relation (SE-MR)
designed to detect data poisoning attacks on DL-based code search models.
Specifically, MT4DP first identifies the high-frequency words from search
queries as potential poisoning targets and takes their corresponding queries as
the source queries. For each source query, MT4DP generates two semantically
equivalent follow-up queries and retrieves its source ranking list. Then, each
source ranking list is re-ranked based on the semantic similarities between its
code snippets and the follow-up queries. Finally, variances between the source
and re-ranked lists are calculated to reveal violations of the SE-MR and warn
the data poisoning attack. Experimental results demonstrate that MT4DP
significantly enhances the detection of data poisoning attacks on DL-based code
search models, outperforming the best baseline by 191% on average F1 score and
265% on average precision. Our work aims to promote further research into
effective techniques for mitigating data poisoning threats on DL-based code
search models.

</details>


### [308] [Automata Models for Effective Bug Description](https://arxiv.org/abs/2507.11146)
*Tom Yaacov,Gera Weiss,Gal Amram,Avi Hayoun*

Main category: cs.SE

TL;DR: The paper uses automata learning and testing to create concise bug descriptions, introducing concepts like Failure Explanations, Eventual Failure Explanations, and Early Detection to enhance debugging.


<details>
  <summary>Details</summary>
Motivation: Debugging complex systems is challenging and often slow, requiring more effective methods to describe and understand bugs.

Method: The authors employ automata learning and testing techniques to develop Failure Explanations, Eventual Failure Explanations, and Early Detection to focus on essential failing behavior patterns.

Result: The proposed methods were evaluated on various test patterns and real-world benchmarks, showing effectiveness in creating compact and informative bug descriptions.

Conclusion: The approach successfully improves bug detection and understanding by summarizing behavior patterns while filtering out irrelevant information.

Abstract: Debugging complex systems is a crucial yet time-consuming task. This paper
presents the use of automata learning and testing techniques to obtain concise
and informative bug descriptions. We introduce the concepts of Failure
Explanations (FE), Eventual Failure Explanations (EFE), and Early Detection
(ED) to provide meaningful summaries of failing behavior patterns. By factoring
out irrelevant information and focusing on essential test patterns, our
approach aims to enhance bug detection and understanding. We evaluate our
methods using various test patterns and real-world benchmarks, demonstrating
their effectiveness in producing compact and informative bug descriptions.

</details>


### [309] [New Formulation of DNN Statistical Mutation Killing for Ensuring Monotonicity: A Technical Report](https://arxiv.org/abs/2507.11199)
*Jinhan Kim,Nargiz Humbatova,Gunel Jahangirova,Shin Yoo,Paolo Tonella*

Main category: cs.SE

TL;DR: The paper identifies a key limitation in an existing statistical mutant killing criterion for Deep Neural Networks testing and proposes a new formulation based on Fisher's exact test to rectify it, ensuring monotonicity.


<details>
  <summary>Details</summary>
Motivation: DeepCrime's current statistical mutant killing criterion fails to preserve monotonicity, which is problematic when expanding test sets as it could incorrectly classify previously killed mutants.

Method: The authors propose using Fisher's exact test as an alternative statistical method to reformulate statistical mutant killing, ensuring both statistical rigor and monotonicity.

Result: The new statistical mutant killing formulation successfully addresses the monotonicity issue while retaining the precision of the statistical testing.

Conclusion: This work introduces a more reliable way to evaluate test suites for Deep Neural Networks by eliminating a key flaw in prior methodologies through the adoption of Fisher's exact test.

Abstract: Mutation testing has emerged as a powerful technique for evaluating the
effectiveness of test suites for Deep Neural Networks. Among existing
approaches, the statistical mutant killing criterion of DeepCrime has leveraged
statistical testing to determine whether a mutant significantly differs from
the original model. However, it suffers from a critical limitation: it violates
the monotonicity property, meaning that expanding a test set may result in
previously killed mutants no longer being classified as killed. In this
technical report, we propose a new formulation of statistical mutant killing
based on Fisher exact test that preserves the statistical rigour of it while
ensuring monotonicity.

</details>


### [310] [An Empirical Study of Multi-Agent RAG for Real-World University Admissions Counseling](https://arxiv.org/abs/2507.11272)
*Anh Nguyen-Duc,Chien Vu Manh,Bao Anh Tran,Viet Phuong Ngo,Luan Le Chi,Anh Quang Nguyen*

Main category: cs.SE

TL;DR: The paper introduces MARAUS, a conversational AI system for Vietnam's university admissions, showing high accuracy, reduced hallucination rates, and cost-effectiveness in real-world use.


<details>
  <summary>Details</summary>
Motivation: To address the gap in real-world implementations of AI systems for university admissions counseling, especially in resource-constrained settings.

Method: The paper developed MARAUS by combining hybrid retrieval, multi-agent orchestration, and LLM-based generation, followed by a two-phase study for testing and evaluation.

Result: MARAUS achieved 92% accuracy, reduced hallucination rates from 15% to 1.45%, and response times under 4 seconds, processing over 6,000 real interactions.

Conclusion: MARAUS demonstrated effective, accurate, and cost-efficient performance in real-world university admissions counseling, providing insights for similar deployments in low-resource contexts.

Abstract: This paper presents MARAUS (Multi-Agent and Retrieval-Augmented University
Admission System), a real-world deployment of a conversational AI platform for
higher education admissions counseling in Vietnam. While large language models
(LLMs) offer potential for automating advisory tasks, most existing solutions
remain limited to prototypes or synthetic benchmarks. MARAUS addresses this gap
by combining hybrid retrieval, multi-agent orchestration, and LLM-based
generation into a system tailored for real-world university admissions. In
collaboration with the University of Transport Technology (UTT) in Hanoi, we
conducted a two-phase study involving technical development and real-world
evaluation. MARAUS processed over 6,000 actual user interactions, spanning six
categories of queries. Results show substantial improvements over LLM-only
baselines: on average 92 percent accuracy, hallucination rates reduced from 15
precent to 1.45 percent, and average response times below 4 seconds. The system
operated cost-effectively, with a two-week deployment cost of 11.58 USD using
GPT-4o mini. This work provides actionable insights for the deployment of
agentic RAG systems in low-resource educational settings.

</details>


### [311] [RefModel: Detecting Refactorings using Foundation Models](https://arxiv.org/abs/2507.11346)
*Pedro Simões,Rohit Gheyi,Rian Melo,Jonhnanthan Oliveira,Márcio Ribeiro,Wesley K. G. Assunção*

Main category: cs.SE

TL;DR: The paper introduces RefModel, a tool that uses foundation models to detect software refactorings, demonstrating its competitive performance compared to traditional static analysis tools.


<details>
  <summary>Details</summary>
Motivation: Current refactoring detection tools rely on complex static analysis and are challenging to extend to new programming languages or scenarios.

Method: The study evaluates foundation models like Phi4-14B and Claude 3.5 Sonnet on datasets of Java refactorings, comparing their performance to traditional tools like RefactoringMiner.

Result: RefModel demonstrated competitive performance, with Claude 3.5 Sonnet and Gemini 2.5 Pro identifying 97% of real-world refactorings, often outperforming traditional tools, and generalizing to Python and Golang.

Conclusion: Foundation models show strong potential for refactoring detection, offering advantages like natural language explanations and simpler refactoring definitions.

Abstract: Refactoring is a common software engineering practice that improves code
quality without altering program behavior. Although tools like ReExtractor+,
RefactoringMiner, and RefDiff have been developed to detect refactorings
automatically, they rely on complex rule definitions and static analysis,
making them difficult to extend and generalize to other programming languages.
In this paper, we investigate the viability of using foundation models for
refactoring detection, implemented in a tool named RefModel. We evaluate
Phi4-14B, and Claude 3.5 Sonnet on a dataset of 858 single-operation
transformations applied to artificially generated Java programs, covering
widely-used refactoring types. We also extend our evaluation by including
Gemini 2.5 Pro and o4-mini-high, assessing their performance on 44 real-world
refactorings extracted from four open-source projects. These models are
compared against RefactoringMiner, RefDiff, and ReExtractor+. RefModel is
competitive with, and in some cases outperform, traditional tools. In
real-world settings, Claude 3.5 Sonnet and Gemini 2.5 Pro jointly identified
97% of all refactorings, surpassing the best-performing static-analysis-based
tools. The models showed encouraging generalization to Python and Golang. They
provide natural language explanations and require only a single sentence to
define each refactoring type.

</details>


### [312] [Security Debt in Practice: Nuanced Insights from Practitioners](https://arxiv.org/abs/2507.11362)
*Chaima Boufaied,Taher Ghaleb,Zainab Masood*

Main category: cs.SE

TL;DR: This study investigates software practitioners' understanding, management, and communication of security debts in real-world settings through interviews with 22 individuals.


<details>
  <summary>Details</summary>
Motivation: Security debts accumulate due to time constraints, resource limitations, and prioritization of functionality over security, but there is little empirical evidence on how they are handled in practice.

Method: The study conducted semi-structured interviews with 22 software practitioners from various roles, organizations, and countries.

Result: The study revealed variations in how practitioners perceive and handle security debts, with some prioritizing delivery speed over security while others focus on maintaining security consistently.

Conclusion: There is a need for better integration of security into the SDLC, consistent use of mitigation strategies, balanced attention to both security and deadlines, and greater focus on the CIA triad.

Abstract: With the increasing reliance on software and automation nowadays, tight
deadlines, limited resources, and prioritization of functionality over security
can lead to insecure coding practices. When not handled properly, these
constraints cause unaddressed security vulnerabilities to accumulate over time,
forming Security Debts (SDs). Despite their critical importance, there is
limited empirical evidence on how software practitioners perceive, manage, and
communicate SDs in real-world settings. In this paper, we present a qualitative
empirical study based on semi-structured interviews with 22 software
practitioners across various roles, organizations, and countries. We address
four research questions: i) we assess software practitioners' knowledge of SDs
and awareness of associated security risks, ii) we investigate their behavior
towards SDs, iii) we explore common tools and strategies used to mitigate SDs,
and iv) we analyze how security risks are communicated within teams and to
decision makers. We observe variations in how practitioners perceive and manage
SDs, with some prioritizing delivery speed over security, while others
consistently maintain security as a priority. Our findings emphasize the need
for stronger integration of security practices across the Software Development
Life Cycle (SDLC), more consistent use of mitigation strategies, better
balancing of deadlines, resources, and security-related tasks, with attention
to the Confidentiality, Integrity, and Availability (CIA) triad.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [313] [Bridging Brains and Machines: A Unified Frontier in Neuroscience, Artificial Intelligence, and Neuromorphic Systems](https://arxiv.org/abs/2507.10722)
*Sohan Shankar,Yi Pan,Hanqi Jiang,Zhengliang Liu,Mohammad R. Darbandi,Agustin Lorenzo,Junhao Chen,Md Mehedi Hasan,Arif Hassan Zidan,Eliana Gelman,Joshua A. Konfrst,Jillian Y. Russell,Katelyn Fernandes,Tianze Yang,Yiwei Li,Huaqin Zhao,Afrar Jahin,Triparna Ganguly,Shair Dinesha,Yifan Zhou,Zihao Wu,Xinliang Li,Lokesh Adusumilli,Aziza Hussein,Sagar Nookarapu,Jixin Hou,Kun Jiang,Jiaxi Li,Brenden Heinel,XianShen Xi,Hailey Hubbard,Zayna Khan,Levi Whitaker,Ivan Cao,Max Allgaier,Andrew Darby,Lin Zhao,Lu Zhang,Xiaoqiao Wang,Xiang Li,Wei Zhang,Xiaowei Yu,Dajiang Zhu,Yohannes Abate,Tianming Liu*

Main category: q-bio.NC

TL;DR: This paper explores the convergence of neuroscience, artificial general intelligence (AGI), and neuromorphic computing, proposing an integrative framework grounded in brain physiology for next-gen AGI systems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to advance AGI development by harnessing insights from brain physiology, addressing limitations in current systems by offering new principles and technologies.

Method: The paper uses a survey approach to analyze neuroscience principles, connectionist models, AGI innovations, and emerging hardware substrates to inform integrated design strategies.

Result: The study identifies design principles such as synaptic plasticity, multimodal association, and sparse spike-based communication, along with hardware trends like memristive arrays and quantum devices.

Conclusion: Four key challenges are laid out: integrating spiking dynamics, lifelong learning, sensorimotor and language unification, and ethical considerations in AGI development.

Abstract: This position and survey paper identifies the emerging convergence of
neuroscience, artificial general intelligence (AGI), and neuromorphic computing
toward a unified research paradigm. Using a framework grounded in brain
physiology, we highlight how synaptic plasticity, sparse spike-based
communication, and multimodal association provide design principles for
next-generation AGI systems that potentially combine both human and machine
intelligences. The review traces this evolution from early connectionist models
to state-of-the-art large language models, demonstrating how key innovations
like transformer attention, foundation-model pre-training, and multi-agent
architectures mirror neurobiological processes like cortical mechanisms,
working memory, and episodic consolidation. We then discuss emerging physical
substrates capable of breaking the von Neumann bottleneck to achieve
brain-scale efficiency in silicon: memristive crossbars, in-memory compute
arrays, and emerging quantum and photonic devices. There are four critical
challenges at this intersection: 1) integrating spiking dynamics with
foundation models, 2) maintaining lifelong plasticity without catastrophic
forgetting, 3) unifying language with sensorimotor learning in embodied agents,
and 4) enforcing ethical safeguards in advanced neuromorphic autonomous
systems. This combined perspective across neuroscience, computation, and
hardware offers an integrative agenda for in each of these fields.

</details>


### [314] [Functional Emotion Modeling in Biomimetic Reinforcement Learning](https://arxiv.org/abs/2507.11027)
*Louis Wang*

Main category: q-bio.NC

TL;DR: The paper develops a theoretical reinforcement learning framework rooted in a functionalist approach to emotion, explaining emotional utility functions and their alignment with psychological phenomena.


<details>
  <summary>Details</summary>
Motivation: To propose a biologically plausible functionalist framework that explains the alignment between emotional valence and utility functions in reinforcement learning.

Method: The authors construct a theoretical reinforcement learning framework based on an assumed functionalist perspective and employ mathematical modeling to explore emotional valence.

Result: The framework explains psychological phenomena such as humor, psychopathy, and advertising through its conceptual network and constructed utility function.

Conclusion: The proposed model demonstrates a broad explanatory capacity for various psychological phenomena through a novel functionalist approach to emotion.

Abstract: We explore a functionalist approach to emotion by employing an ansatz -- an
initial set of assumptions -- that a hypothetical concept generation model
incorporates unproven but biologically plausible traits. From these traits, we
mathematically construct a theoretical reinforcement learning framework
grounded in functionalist principles and examine how the resulting utility
function aligns with emotional valence in biological systems. Our focus is on
structuring the functionalist perspective through a conceptual network,
particularly emphasizing the construction of the utility function, not to
provide an exhaustive explanation of emotions. The primary emphasis is not of
planning or action execution, but such factors are addressed when pertinent.
Finally, we apply the framework to psychological phenomena such as humor,
psychopathy, and advertising, demonstrating its breadth of explanatory power.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [315] [TaylorPODA: A Taylor Expansion-Based Method to Improve Post-Hoc Attributions for Opaque Models](https://arxiv.org/abs/2507.10643)
*Yuchi Tang,Iñaki Esnaola,Suzanne Mason,George Panoutsos*

Main category: stat.ML

TL;DR: The paper develops TaylorPODA, a new explanation method that provides systematic feature attribution for opaque models, based on three proposed postulates and the Taylor expansion framework.


<details>
  <summary>Details</summary>
Motivation: Current post-hoc model-agnostic explanation methods often fail to explicitly and systematically quantify individual feature contributions, creating a gap in rigorous explanation frameworks.

Method: The authors introduce a set of postulates—"precision," "federation," and "zero-discrepancy"—as well as the TaylorPODA method which builds on Deng et al.'s Taylor expansion framework while adding a new adaptation property.

Result: Empirical evaluations show TaylorPODA to be competitive compared to baseline methods, offering principled and visualization-friendly explanations.

Conclusion: TaylorPODA advances the trustworthy deployment of opaque models by providing theoretically grounded, robust explanations.

Abstract: Existing post-hoc model-agnostic methods generate external explanations for
opaque models, primarily by locally attributing the model output to its input
features. However, they often lack an explicit and systematic framework for
quantifying the contribution of individual features. Building on the Taylor
expansion framework introduced by Deng et al. (2024) to unify existing local
attribution methods, we propose a rigorous set of postulates -- "precision",
"federation", and "zero-discrepancy" -- to govern Taylor term-specific
attribution. Guided by these postulates, we introduce TaylorPODA (Taylor
expansion-derived imPortance-Order aDapted Attribution), which incorporates an
additional "adaptation" property. This property enables alignment with
task-specific goals, especially in post-hoc settings lacking ground-truth
explanations. Empirical evaluations demonstrate that TaylorPODA achieves
competitive results against baseline methods, providing principled and
visualization-friendly explanations. This work represents a step toward the
trustworthy deployment of opaque models by offering explanations with stronger
theoretical grounding.

</details>


### [316] [Robust Multi-Manifold Clustering via Simplex Paths](https://arxiv.org/abs/2507.10710)
*Haoyu Chen,Anna Little,Akin Narayan*

Main category: stat.ML

TL;DR: The paper presents a new geometric algorithm for clustering intersecting manifolds, demonstrating robustness and scalability with superior performance compared to other methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of clustering intersecting, high-dimensional manifold components, often complicated by noise and small intersection angles.

Method: The paper proposes creating a simplex locality graph with weights based on dihedral angles and using the largest angle path distance (LAPD) metric via infinity path distances.

Result: The LAPD metric successfully separates manifold components with high probability, validated through experiments on synthetic and real-world datasets, showing robustness to various factors and outperforming other MMC methods.

Conclusion: The approach is computationally efficient with quasi-linear complexity, robust to manifold variations, and exhibits superior performance for MMC tasks compared to existing algorithms.

Abstract: This article introduces a novel, geometric approach for multi-manifold
clustering (MMC), i.e. for clustering a collection of potentially intersecting,
d-dimensional manifolds into the individual manifold components. We first
compute a locality graph on d-simplices, using the dihedral angle in between
adjacent simplices as the graph weights, and then compute infinity path
distances in this simplex graph. This procedure gives a metric on simplices
which we refer to as the largest angle path distance (LAPD). We analyze the
properties of LAPD under random sampling, and prove that with an appropriate
denoising procedure, this metric separates the manifold components with high
probability. We validate the proposed methodology with extensive numerical
experiments on both synthetic and real-world data sets. These experiments
demonstrate that the method is robust to noise, curvature, and small
intersection angle, and generally out-performs other MMC algorithms. In
addition, we provide a highly scalable implementation of the proposed
algorithm, which leverages approximation schemes for infinity path distance to
achieve quasi-linear computational complexity.

</details>


### [317] [GOLFS: Feature Selection via Combining Both Global and Local Information for High Dimensional Clustering](https://arxiv.org/abs/2507.10956)
*Zhaoyu Xing,Yang Wan,Juan Wen,Wei Zhong*

Main category: stat.ML

TL;DR: The paper introduces GOLFS, an unsupervised feature selection method combining local and global data structures to improve feature selection and clustering in high-dimensional datasets.


<details>
  <summary>Details</summary>
Motivation: High-dimensional clustering poses challenges in identifying discriminative features due to the absence of cluster labels, leaving supervised feature selection methods unusable.

Method: The GOLFS algorithm combines local geometric structure (via manifold learning) and global correlation structure (via regularized self-representation) for feature selection. An iterative optimization algorithm with proven convergence is used.

Result: Simulations and real-world applications showcase GOLFS's effectiveness, yielding improved performance in feature selection and clustering.

Conclusion: The GOLFS method effectively combines local and global information, demonstrating robust feature selection and clustering capabilities in high-dimensional scenarios.

Abstract: It is important to identify the discriminative features for high dimensional
clustering. However, due to the lack of cluster labels, the regularization
methods developed for supervised feature selection can not be directly applied.
To learn the pseudo labels and select the discriminative features
simultaneously, we propose a new unsupervised feature selection method, named
GlObal and Local information combined Feature Selection (GOLFS), for high
dimensional clustering problems. The GOLFS algorithm combines both local
geometric structure via manifold learning and global correlation structure of
samples via regularized self-representation to select the discriminative
features. The combination improves the accuracy of both feature selection and
clustering by exploiting more comprehensive information. In addition, an
iterative algorithm is proposed to solve the optimization problem and the
convergency is proved. Simulations and two real data applications demonstrate
the excellent finite-sample performance of GOLFS on both feature selection and
clustering.

</details>


### [318] [Interpretable Bayesian Tensor Network Kernel Machines with Automatic Rank and Feature Selection](https://arxiv.org/abs/2507.11136)
*Afra Kilic,Kim Batselier*

Main category: stat.ML

TL;DR: The paper presents Bayesian Tensor Network Kernel Machines that automatically infer model complexity and enhance interpretability using a probabilistic approach, while keeping computational complexity low.


<details>
  <summary>Details</summary>
Motivation: Existing Tensor Network (TN) Kernel Machines are deterministic, ignoring parameter uncertainty, and require cumbersome manual tuning of hyperparameters like tensor rank and feature dimensions.

Method: The authors propose a fully probabilistic approach with sparsity-inducing hierarchical priors on TN factors. They employ mean-field variational inference to approximate the posterior distributions of parameters, resulting in a Bayesian ALS algorithm with similar computational complexity to deterministic methods.

Result: The proposed model demonstrates superior performance in prediction accuracy, uncertainty quantification, interpretability, and scalability on synthetic and real-world datasets.

Conclusion: Bayesian Tensor Network Kernel Machines offer an efficient and interpretable solution to parameter uncertainty and hyperparameter tuning, achieving competitive results without additional computational cost.

Abstract: Tensor Network (TN) Kernel Machines speed up model learning by representing
parameters as low-rank TNs, reducing computation and memory use. However, most
TN-based Kernel methods are deterministic and ignore parameter uncertainty.
Further, they require manual tuning of model complexity hyperparameters like
tensor rank and feature dimensions, often through trial-and-error or
computationally costly methods like cross-validation. We propose Bayesian
Tensor Network Kernel Machines, a fully probabilistic framework that uses
sparsity-inducing hierarchical priors on TN factors to automatically infer
model complexity. This enables automatic inference of tensor rank and feature
dimensions, while also identifying the most relevant features for prediction,
thereby enhancing model interpretability. All the model parameters and
hyperparameters are treated as latent variables with corresponding priors.
Given the Bayesian approach and latent variable dependencies, we apply a
mean-field variational inference to approximate their posteriors. We show that
applying a mean-field approximation to TN factors yields a Bayesian ALS
algorithm with the same computational complexity as its deterministic
counterpart, enabling uncertainty quantification at no extra computational
cost. Experiments on synthetic and real-world datasets demonstrate the superior
performance of our model in prediction accuracy, uncertainty quantification,
interpretability, and scalability.

</details>


### [319] [How does Labeling Error Impact Contrastive Learning? A Perspective from Data Dimensionality Reduction](https://arxiv.org/abs/2507.11161)
*Jun Chen,Hong Chen,Yonghua Yu,Yiming Ying*

Main category: stat.ML

TL;DR: This paper explores the theoretical impact of labeling errors caused by data augmentation in contrastive learning and provides strategies to mitigate these issues.


<details>
  <summary>Details</summary>
Motivation: To understand and address the negative impact of labeling errors caused by common augmentation techniques on the downstream classification performance of contrastive learning.

Method: The paper evaluates the impacts of labeling error theoretically and empirically. It employs data dimensionality reduction methods like SVD to mitigate false positives while providing augmentation suggestions to balance graph connectivity and labeling error.

Result: The authors identified significant negative effects of labeling error on classification risk. They observed that SVD reduces the issue of false positives but can also harmaugmented graph connectivity.

Conclusion: Best practices, such as moderate embedding dimension, weak augmentation, and using SVD strategically, ensure model performance by minimizing labeling errors and preserving graph connectivity.

Abstract: In recent years, contrastive learning has achieved state-of-the-art
performance in the territory of self-supervised representation learning. Many
previous works have attempted to provide the theoretical understanding
underlying the success of contrastive learning. Almost all of them rely on a
default assumption, i.e., the label consistency assumption, which may not hold
in practice (the probability of failure is called labeling error) due to the
strength and randomness of common augmentation strategies, such as random
resized crop (RRC). This paper investigates the theoretical impact of labeling
error on the downstream classification performance of contrastive learning. We
first reveal several significant negative impacts of labeling error on
downstream classification risk. To mitigate these impacts, data dimensionality
reduction method (e.g., singular value decomposition, SVD) is applied on
original data to reduce false positive samples, and establish both theoretical
and empirical evaluations. Moreover, it is also found that SVD acts as a
double-edged sword, which may lead to the deterioration of downstream
classification accuracy due to the reduced connectivity of the augmentation
graph. Based on the above observations, we give the augmentation suggestion
that we should use some moderate embedding dimension (such as $512, 1024$ in
our experiments), data inflation, weak augmentation, and SVD to ensure large
graph connectivity and small labeling error to improve model performance.

</details>


### [320] [From Observational Data to Clinical Recommendations: A Causal Framework for Estimating Patient-level Treatment Effects and Learning Policies](https://arxiv.org/abs/2507.11381)
*Rom Gutman,Shimon Sheiba,Omer Noy Klien,Naama Dekel Bird,Amit Gruber,Doron Aronson,Oren Caspi,Uri Shalit*

Main category: stat.ML

TL;DR: The paper introduces a framework to create personalized treatment recommendation models using existing methods for patient-level causal analysis, focusing on safety and validity in observational data.


<details>
  <summary>Details</summary>
Motivation: To improve patient outcomes by developing patient-specific treatment recommendation frameworks inspired by widely accepted causal inference principles.

Method: Integrates existing causal inference methods into a practical pipeline, emphasizing safety, validity, and observational data causal identification.

Result: Applied the framework to heart failure patients with acute kidney injury, demonstrating improved patient outcomes compared to current treatment regimes.

Conclusion: The proposed pipeline offers a practical and robust approach to optimizing patient-specific treatment recommendations, suggesting potential benefits for real-world healthcare applications.

Abstract: We propose a framework for building patient-specific treatment recommendation
models, building on the large recent literature on learning patient-level
causal models and inspired by the target trial paradigm of Hernan and Robins.
We focus on safety and validity, including the crucial issue of causal
identification when using observational data. We do not provide a specific
model, but rather a way to integrate existing methods and know-how into a
practical pipeline. We further provide a real world use-case of treatment
optimization for patients with heart failure who develop acute kidney injury
during hospitalization. The results suggest our pipeline can improve patient
outcomes over the current treatment regime.

</details>


### [321] [Joint space-time wind field data extrapolation and uncertainty quantification using nonparametric Bayesian dictionary learning](https://arxiv.org/abs/2507.11385)
*George D. Pasparakis,Ioannis A. Kougioumtzoglou,Michael D. Shields*

Main category: stat.ML

TL;DR: This paper presents a Bayesian-based method for accurately estimating and extrapolating space-time wind data using sparse measurements.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of accurate wind field data extrapolation and statistical estimation based on incomplete measurements, which are common in wind engineering applications with limited sensor setups.

Method: A nonparametric Bayesian dictionary learning approach was used to construct a low-dimensional representation of the stochastic wind field, allowing adaptive basis selection and uncertainty quantification. Two cases (simulated and experimental wind data) were analyzed to evaluate the approach.

Result: The methodology achieved improved extrapolation accuracy, robust adaptation to high-dimensional data of arbitrary forms, and quantifiable uncertainties compared to standard compressive sampling techniques.

Conclusion: The proposed method is effective for a wide range of wind engineering applications, enhancing the accuracy and reliability of wind field estimations under sensor or data constraints.

Abstract: A methodology is developed, based on nonparametric Bayesian dictionary
learning, for joint space-time wind field data extrapolation and estimation of
related statistics by relying on limited/incomplete measurements. Specifically,
utilizing sparse/incomplete measured data, a time-dependent optimization
problem is formulated for determining the expansion coefficients of an
associated low-dimensional representation of the stochastic wind field.
Compared to an alternative, standard, compressive sampling treatment of the
problem, the developed methodology exhibits the following advantages. First,
the Bayesian formulation enables also the quantification of the uncertainty in
the estimates. Second, the requirement in standard CS-based applications for an
a priori selection of the expansion basis is circumvented. Instead, this is
done herein in an adaptive manner based on the acquired data. Overall, the
methodology exhibits enhanced extrapolation accuracy, even in cases of
high-dimensional data of arbitrary form, and of relatively large extrapolation
distances. Thus, it can be used, potentially, in a wide range of wind
engineering applications where various constraints dictate the use of a limited
number of sensors. The efficacy of the methodology is demonstrated by
considering two case studies. The first relates to the extrapolation of
simulated wind velocity records consistent with a prescribed joint
wavenumber-frequency power spectral density in a three-dimensional domain (2D
and time). The second pertains to the extrapolation of four-dimensional (3D and
time) boundary layer wind tunnel experimental data that exhibit significant
spatial variability and non-Gaussian characteristics.

</details>


### [322] [Canonical Bayesian Linear System Identification](https://arxiv.org/abs/2507.11535)
*Andrey Bryutkin,Matthew E. Levine,Iñigo Urteaga,Youssef Marzouk*

Main category: stat.ML

TL;DR: The paper addresses inefficiencies in Bayesian LTI system identification caused by non-identifiability, introducing canonical forms to solve this issue.


<details>
  <summary>Details</summary>
Motivation: To resolve inefficiencies and impracticalities in Bayesian inference for LTI system identification due to parameter non-identifiability and complex posteriors.

Method: The authors embed canonical forms of LTI systems within a Bayesian framework, allowing for structure-aware priors and resolving identifiability issues.

Result: Canonical parameterizations yield higher computational efficiency, interpretable posterior distributions, robust uncertainty estimates, and better performance on limited data in simulations.

Conclusion: Embedding LTI systems in canonical forms not only resolves identifiability but also improves Bayesian inference, leveraging asymptotics and enabling practical application with better efficiency and robustness.

Abstract: Standard Bayesian approaches for linear time-invariant (LTI) system
identification are hindered by parameter non-identifiability; the resulting
complex, multi-modal posteriors make inference inefficient and impractical. We
solve this problem by embedding canonical forms of LTI systems within the
Bayesian framework. We rigorously establish that inference in these minimal
parameterizations fully captures all invariant system dynamics (e.g., transfer
functions, eigenvalues, predictive distributions of system outputs) while
resolving identifiability. This approach unlocks the use of meaningful,
structure-aware priors (e.g., enforcing stability via eigenvalues) and ensures
conditions for a Bernstein--von Mises theorem -- a link between Bayesian and
frequentist large-sample asymptotics that is broken in standard forms.
Extensive simulations with modern MCMC methods highlight advantages over
standard parameterizations: canonical forms achieve higher computational
efficiency, generate interpretable and well-behaved posteriors, and provide
robust uncertainty estimates, particularly from limited data.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [323] [Learning to Quantize and Precode in Massive MIMO Systems for Energy Reduction: a Graph Neural Network Approach](https://arxiv.org/abs/2507.10634)
*Thomas Feys,Liesbet Van der Perre,François Rottenberg*

Main category: eess.SY

TL;DR: The paper proposes a graph neural network (GNN)-based non-linear precoding method to address power consumption and complexity issues in massive MIMO systems with coarsely quantized downlink transmissions.


<details>
  <summary>Details</summary>
Motivation: The increasing demand for more radio frequency chains, higher carrier frequencies, and larger bandwidths in massive MIMO systems has led to digital-to-analog converters (DACs) becoming a hardware bottleneck due to their complexity and power consumption.

Method: The authors introduced a self-supervised GNN, utilizing straight-through Gumbel-softmax gradient estimation, to optimize the achievable rate in coarsely quantized systems. The GNN directly outputs precoded quantized vectors based on the channel matrix and transmit symbols.

Result: The proposed GNN-based method achieves a higher sum rate under coarse quantization, reducing power consumption significantly compared to traditional methods, such as Maximum Ratio Transmission (MRT). For example, it reduces DAC power consumption by factors of 4-7 (baseband) and 3 (RF DACs) while maintaining efficiency under specific bandwidth scenarios.

Conclusion: Although the proposed method increases digital signal processing power usage, its overall reduction in power consumption (up to certain bandwidth limits) demonstrates the potential for improving the efficiency of massive MIMO systems with coarse quantization.

Abstract: Massive MIMO systems are moving toward increased numbers of radio frequency
chains, higher carrier frequencies and larger bandwidths. As such,
digital-to-analog converters (DACs) are becoming a bottleneck in terms of
hardware complexity and power consumption. In this work, non-linear precoding
for coarsely quantized downlink massive MIMO is studied. Given the NP-hard
nature of this problem, a graph neural network (GNN) is proposed that directly
outputs the precoded quantized vector based on the channel matrix and the
intended transmit symbols. The model is trained in a self-supervised manner, by
directly maximizing the achievable rate. To overcome the non-differentiability
of the objective function, introduced due to the non-differentiable DAC
functions, a straight-through Gumbel-softmax estimation of the gradient is
proposed. The proposed method achieves a significant increase in achievable sum
rate under coarse quantization. For instance, in the single-user case, the
proposed method can achieve the same sum rate as maximum ratio transmission
(MRT) by using one-bit DAC's as compared to 3 bits for MRT. This reduces the
DAC's power consumption by a factor 4-7 and 3 for baseband and RF DACs
respectively. This, however, comes at the cost of increased digital signal
processing power consumption. When accounting for this, the reduction in
overall power consumption holds for a system bandwidth up to 3.5 MHz for
baseband DACs, while the RF DACs can maintain a power reduction of 2.9 for
higher bandwidths. Notably, indirect effects, which further reduce the power
consumption, such as a reduced fronthaul consumption and reduction in other
components, are not considered in this analysis.

</details>


### [324] [Standards-Compliant DM-RS Allocation via Temporal Channel Prediction for Massive MIMO Systems](https://arxiv.org/abs/2507.11064)
*Sehyun Ryu,Hyun Jong Yang*

Main category: eess.SY

TL;DR: The study proposes a neural network-based method to reduce feedback overhead in beyond 5G networks by jointly optimizing channel prediction and reference signal allocation, resulting in significant throughput improvement.


<details>
  <summary>Details</summary>
Motivation: Feedback overhead poses a challenge in modern massive MIMO systems due to increased demand for channel state information (CSI) feedback as the number of antennas grows in frequency division duplex (FDD) environments.

Method: The authors developed a ViViT/CNN-based architecture for channel prediction-based reference signal allocation (CPRS), which treats evolving CSI matrices as image-like data for efficient transmission.

Result: Simulations using NVIDIA Sionna ray-tracing channel data showed up to a 36.60% increase in throughput compared to benchmark strategies.

Conclusion: The proposed CPRS method demonstrates the potential to enhance data transmission efficiency in dynamic environments while conforming to 5G-Advanced standards.

Abstract: Reducing feedback overhead in beyond 5G networks is a critical challenge, as
the growing number of antennas in modern massive MIMO systems substantially
increases the channel state information (CSI) feedback demand in frequency
division duplex (FDD) systems. To address this, extensive research has focused
on CSI compression and prediction, with neural network-based approaches gaining
momentum and being considered for integration into the 3GPP 5G-Advanced
standards. While deep learning has been effectively applied to CSI-limited
beamforming and handover optimization, reference signal allocation under such
constraints remains surprisingly underexplored. To fill this gap, we introduce
the concept of channel prediction-based reference signal allocation (CPRS),
which jointly optimizes channel prediction and DM-RS allocation to improve data
throughput without requiring CSI feedback. We further propose a
standards-compliant ViViT/CNN-based architecture that implements CPRS by
treating evolving CSI matrices as sequential image-like data, enabling
efficient and adaptive transmission in dynamic environments. Simulation results
using ray-tracing channel data generated in NVIDIA Sionna validate the proposed
method, showing up to 36.60% throughput improvement over benchmark strategies.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [325] [Deterministic Lower Bounds for $k$-Edge Connectivity in the Distributed Sketching Model](https://arxiv.org/abs/2507.11257)
*Peter Robinson,Ming Ming Tan*

Main category: cs.DS

TL;DR: The paper studies the $k$-edge connectivity problem in undirected graphs under the distributed sketching model and establishes a deterministic lower bound of $
\Omega(k)$ bits for super-constant $k$.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand the limitations and challenges of deciding graph connectivity problems in the distributed sketching model, where efficient communication with minimal message length is crucial.

Method: The authors introduce a new lower bound graph construction and define a novel communication complexity problem, UniqueOverlap. They use properties of cross-intersecting set families to establish the hardness of the problem for deterministic algorithms and employ a unique simulation argument to derive the lower bound.

Result: The paper establishes that the worst-case message length for $k$-edge connectivity is $
\Omega(k)$ bits for any super-constant $k = O(\sqrt{n})$, which is the first super-polylogarithmic lower bound for deterministic connectivity decisions in this model.

Conclusion: The findings demonstrate the intrinsic complexity of solving $k$-edge connectivity deterministically in the distributed sketching model, advancing the understanding of lower bounds in distributed graph algorithms.

Abstract: We study the $k$-edge connectivity problem on undirected graphs in the
distributed sketching model, where we have $n$ nodes and a referee. Each node
sends a single message to the referee based on its 1-hop neighborhood in the
graph, and the referee must decide whether the graph is $k$-edge connected by
taking into account the received messages.
  We present the first lower bound for deciding a graph connectivity problem in
this model with a deterministic algorithm. Concretely, we show that the worst
case message length is $\Omega( k )$ bits for $k$-edge connectivity, for any
super-constant $k = O(\sqrt{n})$. Previously, only a lower bound of $\Omega(
\log^3 n )$ bits was known for ($1$-edge) connectivity, due to Yu (SODA 2021).
In fact, our result is the first super-polylogarithmic lower bound for a
connectivity decision problem in the distributed graph sketching model.
  To obtain our result, we introduce a new lower bound graph construction, as
well as a new 3-party communication complexity problem that we call
UniqueOverlap. As this problem does not appear to be amenable to reductions to
existing hard problems such as set disjointness or indexing due to correlations
between the inputs of the three players, we leverage results from
cross-intersecting set families to prove the hardness of UniqueOverlap for
deterministic algorithms. Finally, we obtain the sought lower bound for
deciding $k$-edge connectivity via a novel simulation argument that, in
contrast to previous works, does not introduce any probability of error and
thus works for deterministic algorithms.

</details>


### [326] [Improved sampling algorithms and Poincaré inequalities for non-log-concave distributions](https://arxiv.org/abs/2507.11236)
*Yuchen He,Zhehan Lei,Jianan Shao,Chihao Zhang*

Main category: cs.DS

TL;DR: This paper investigates sampling from distributions with query access to a potential function and its gradient, achieving polynomial query complexity under strengthened smoothness and moment assumptions.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in sampling algorithms by exploring how moderate changes in smoothness assumptions can dramatically affect their complexity.

Method: The authors strengthen smoothness and moment assumptions, leveraging the Ornstein-Uhlenbeck process and sub-Gaussian properties to derive improved query complexities and Poincaré constant estimates.

Result: The paper establishes that stronger smoothness assumptions reduce query complexity to polynomial scales, leading to significant improvements over prior methods and enabling better bounds for Poincaré constants in certain distributions.

Conclusion: The strengthened assumptions lead to exponential improvements in query complexity, emphasizing the importance of smoothness and moment criteria in sampling algorithm efficiency.

Abstract: We study the problem of sampling from a distribution $\mu$ with density
$\propto e^{-V}$ for some potential function $V:\mathbb R^d\to \mathbb R$ with
query access to $V$ and $\nabla V$. We start with the following standard
assumptions:
  (1) The potential function $V$ is $L$-smooth.
  (2) The second moment $\mathbf{E}_{X\sim \mu}[\|X\|^2]\leq M$.
  Recently, He and Zhang (COLT'25) showed that the query complexity of sampling
from such distributions is at least
$\left(\frac{LM}{d\epsilon}\right)^{\Omega(d)}$ where $\epsilon$ is the desired
accuracy in total variation distance, and the Poincar\'e constant can be
arbitrarily large.
  Meanwhile, another common assumption in the study of diffusion based samplers
(see e.g., the work of Chen, Chewi, Li, Li, Salim and Zhang (ICLR'23))
strengthens the smoothness condition (1) to the following:
  (1*) The potential function of *every* distribution along the
Ornstein-Uhlenbeck process starting from $\mu$ is $L$-smooth.
  We show that under the assumptions (1*) and (2), the query complexity of
sampling from $\mu$ can be $\mathrm{poly}(L,d)\cdot
\left(\frac{Ld+M}{\epsilon^2}\right)^{\mathcal{O}(L+1)}$, which is polynomial
in $d$ and $\frac{1}{\epsilon}$ when $L=\mathcal{O}(1)$ and
$M=\mathrm{poly}(d)$. This improves the algorithm with quasi-polynomial query
complexity developed by Huang et al. (COLT'24). Our results imply that the
seemly moderate strengthening of the smoothness condition (1) to (1*) can lead
to an exponential gap in the query complexity of sampling algorithms.
  Moreover, we show that together with the assumption (1*) and the stronger
moment assumption that $\|X\|$ is $\lambda$-sub-Gaussian for $X\sim\mu$, the
Poincar\'e constant of $\mu$ is at most $\mathcal{O}(\lambda)^{2(L+1)}$. As an
application of our technique, we obtain improved estimate of the Poincar\'e
constant for mixture of Gaussians with the same covariance.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [327] [Access Control for Information-Theoretically Secure Key-Document Stores](https://arxiv.org/abs/2507.10730)
*Yin Li,Sharad Mehrota,Shantanu Sharma,Komal Kumari*

Main category: cs.CR

TL;DR: This paper introduces a secure key-based access control for outsourced key-value stores, utilizing Shamir's secret-sharing to protect documents from leakage and unauthorized modifications.


<details>
  <summary>Details</summary>
Motivation: To enable secure access and retrieval in outsourced key-value stores without compromising data security, user access rights, or query output size.

Method: The approach utilizes Shamir's secret-sharing for information-theoretic security, enabling keyword-based retrieval, malicious client/server attack prevention, and efficient data access.

Result: The method efficiently processes data access in 231.5ms for 5,000 keywords across 500,000 files, ensuring security and performance.

Conclusion: The technique improves secure access control for outsourced storage systems while protecting against data leakage and malicious activities, all with efficient performance metrics.

Abstract: This paper presents a novel key-based access control technique for secure
outsourcing key-value stores where values correspond to documents that are
indexed and accessed using keys. The proposed approach adopts Shamir's
secret-sharing that offers unconditional or information-theoretic security. It
supports keyword-based document retrieval while preventing leakage of the data,
access rights of users, or the size (\textit{i}.\textit{e}., volume of the
output that satisfies a query). The proposed approach allows servers to detect
(and abort) malicious clients from gaining unauthorized access to data, and
prevents malicious servers from altering data undetected while ensuring
efficient access -- it takes 231.5ms over 5,000 keywords across 500,000 files.

</details>


### [328] [BandFuzz: An ML-powered Collaborative Fuzzing Framework](https://arxiv.org/abs/2507.10845)
*Wenxuan Shi,Hongwei Li,Jiahao Yu,Xinqian Sun,Wenbo Guo,Xinyu Xing*

Main category: cs.CR

TL;DR: This paper examines collaborative fuzzing, highlighting its potential over individual fuzzers while addressing limitations like resource use and inefficiency.


<details>
  <summary>Details</summary>
Motivation: Collaborative fuzzing aims to reduce manual selection of individual fuzzers and perform robustly across programs.

Method: Explores combining multiple individual fuzzers to dynamically allocate fuzzing strategies in collaborative frameworks.

Result: Points out that current collaborative fuzzers face limitations in resource allocation and computational demands.

Conclusion: Collaborative fuzzing shows promise for generic solutions, but challenges remain in efficiency and resource use.

Abstract: Collaborative fuzzing has recently emerged as a technique that combines
multiple individual fuzzers and dynamically chooses the appropriate
combinations suited for different programs. Unlike individual fuzzers, which
rely on specific assumptions to maintain their effectiveness, collaborative
fuzzing relaxes the assumptions on target programs, providing constant and
robust performance across various programs. Ideally, collaborative fuzzing
should be a more promising direction toward generic fuzzing solutions, as it
mitigates the need for manual cherry-picking of individual fuzzers. However,
the effectiveness of existing collaborative fuzzing frameworks is limited by
major challenges, such as the need for additional computational resources
compared to individual fuzzers and the inefficient allocation of resources
among the various fuzzers.

</details>


### [329] [MalCodeAI: Autonomous Vulnerability Detection and Remediation via Language Agnostic Code Reasoning](https://arxiv.org/abs/2507.10898)
*Jugal Gajjar,Kamalasankari Subramaniakuppusamy,Noha El Kachach*

Main category: cs.CR

TL;DR: MalCodeAI is an AI-based pipeline for code security analysis and remediation, utilizing advanced fine-tuned models to successfully detect vulnerabilities across 14 languages with notable accuracy and developer usability.


<details>
  <summary>Details</summary>
Motivation: To address the increasing complexity of cyber threats and the limitations of traditional vulnerability detection tools in securing software systems.

Method: MalCodeAI employs a language-agnostic, multi-stage AI pipeline using Qwen2.5-Coder-3B-Instruct models fine-tuned through Low-Rank Adaptation (LoRA) with the MLX framework. It consists of two phases focusing on code decomposition and semantics (Phase 1) and vulnerability detection and remediation (Phase 2).

Result: The system achieved low validation losses (0.397 in Phase 1 and 0.199 in Phase 2), effective in detecting flaws and proposing fixes. It also scored highly in developer-focused evaluations for usefulness (8.06/10), interpretability (7.40/10), and readability (7.53/10).

Conclusion: MalCodeAI advances software security by providing an intelligent, explainable, and developer-friendly solution that scales across multiple programming languages and effectively handles complex vulnerabilities, including zero-day threats.

Abstract: The growing complexity of cyber threats and the limitations of traditional
vulnerability detection tools necessitate novel approaches for securing
software systems. We introduce MalCodeAI, a language-agnostic, multi-stage AI
pipeline for autonomous code security analysis and remediation. MalCodeAI
combines code decomposition and semantic reasoning using fine-tuned
Qwen2.5-Coder-3B-Instruct models, optimized through Low-Rank Adaptation (LoRA)
within the MLX framework, and delivers scalable, accurate results across 14
programming languages. In Phase 1, the model achieved a validation loss as low
as 0.397 for functional decomposition and summarization of code segments after
200 iterations, 6 trainable layers, and a learning rate of 2 x 10^(-5). In
Phase 2, for vulnerability detection and remediation, it achieved a best
validation loss of 0.199 using the same number of iterations and trainable
layers but with an increased learning rate of 4 x 10^(-5), effectively
identifying security flaws and suggesting actionable fixes. MalCodeAI supports
red-hat-style exploit tracing, CVSS-based risk scoring, and zero-shot
generalization to detect complex, zero-day vulnerabilities. In a qualitative
evaluation involving 15 developers, the system received high scores in
usefulness (mean 8.06/10), interpretability (mean 7.40/10), and readability of
outputs (mean 7.53/10), confirming its practical value in real-world
development workflows. This work marks a significant advancement toward
intelligent, explainable, and developer-centric software security solutions.

</details>


### [330] [When and Where do Data Poisons Attack Textual Inversion?](https://arxiv.org/abs/2507.10578)
*Jeremy Styborski,Mingzhi Lyu,Jiayou Lu,Nupur Kapur,Adams Kong*

Main category: cs.CR

TL;DR: The paper addresses the vulnerability of diffusion models (DMs) to poisoning attacks, focusing on the textual inversion (TI) technique. It introduces Semantic Sensitivity Maps to analyze poisoning effects and the Safe-Zone Training (SZT) defense mechanism to enhance robustness against such attacks.


<details>
  <summary>Details</summary>
Motivation: Poisoning attacks compromise the personalization capabilities of diffusion models, affecting the textual inversion process. This paper aims to understand and mitigate these attacks to ensure the reliable performance of DMs.

Method: The authors use Semantic Sensitivity Maps to visualize poisoning effects on text embeddings, uncover biases in diffusion models across timesteps, and propose Safe-Zone Training (SZT), which combines JPEG compression, timestep restriction during training, and loss masking.

Result: Safe-Zone Training (SZT) significantly improves the robustness of textual inversion against poisoning attacks. It enhances generative quality compared to prior defenses and demonstrates effectiveness across various poisoning methods.

Conclusion: The study contributes a novel defense framework (SZT) that mitigates poisoning attack effects on diffusion models. SZT ensures better protection and generative performance, offering practical solutions for safeguarding TI in DMs.

Abstract: Poisoning attacks pose significant challenges to the robustness of diffusion
models (DMs). In this paper, we systematically analyze when and where poisoning
attacks textual inversion (TI), a widely used personalization technique for
DMs. We first introduce Semantic Sensitivity Maps, a novel method for
visualizing the influence of poisoning on text embeddings. Second, we identify
and experimentally verify that DMs exhibit non-uniform learning behavior across
timesteps, focusing on lower-noise samples. Poisoning attacks inherit this bias
and inject adversarial signals predominantly at lower timesteps. Lastly, we
observe that adversarial signals distract learning away from relevant concept
regions within training data, corrupting the TI process. Based on these
insights, we propose Safe-Zone Training (SZT), a novel defense mechanism
comprised of 3 key components: (1) JPEG compression to weaken high-frequency
poison signals, (2) restriction to high timesteps during TI training to avoid
adversarial signals at lower timesteps, and (3) loss masking to constrain
learning to relevant regions. Extensive experiments across multiple poisoning
methods demonstrate that SZT greatly enhances the robustness of TI against all
poisoning attacks, improving generative quality beyond prior published
defenses. Code: www.github.com/JStyborski/Diff_Lab Data:
www.github.com/JStyborski/NC10

</details>


### [331] [LaSM: Layer-wise Scaling Mechanism for Defending Pop-up Attack on GUI Agents](https://arxiv.org/abs/2507.10610)
*Zihe Yan,Zhuosheng Zhang*

Main category: cs.CR

TL;DR: The paper addresses the vulnerability of GUI agents built on multimodal large language models (MLLMs) to pop-up-based environmental injection attacks, proposing a defense mechanism called LaSM.


<details>
  <summary>Details</summary>
Motivation: GUI agents are prone to unsafe or incorrect actions due to pop-up-based environmental injection attacks, and existing defense methods are either expensive to implement or ineffective under certain conditions.

Method: The proposed defense mechanism, LaSM (Layer-wise Scaling Mechanism), systematically amplifies attention and MLP modules in critical layers without the need for additional retraining.

Result: LaSM consistently improves defense success rates across various attack types and models, achieving over 98% robustness when combined with prompt-level alerts.

Conclusion: Attention misalignment is identified as a core weakness in MLLM agents, and selective layer-wise modulation effectively enhances robustness to pop-up-based attacks.

Abstract: Graphical user interface (GUI) agents built on multimodal large language
models (MLLMs) have recently demonstrated strong decision-making abilities in
screen-based interaction tasks. However, they remain highly vulnerable to
pop-up-based environmental injection attacks, where malicious visual elements
divert model attention and lead to unsafe or incorrect actions. Existing
defense methods either require costly retraining or perform poorly under
inductive interference. In this work, we systematically study how such attacks
alter the attention behavior of GUI agents and uncover a layer-wise attention
divergence pattern between correct and incorrect outputs. Based on this
insight, we propose \textbf{LaSM}, a \textit{Layer-wise Scaling Mechanism} that
selectively amplifies attention and MLP modules in critical layers. LaSM
improves the alignment between model saliency and task-relevant regions without
additional training. Extensive experiments across 12 types of pop-up
perturbations and 4 different model backbones show that LaSM consistently
enhances the defense success rate. When combined with prompt-level alerts, LaSM
achieves over 98\% robustness even under strong inductive attacks. Our findings
reveal that attention misalignment is a core vulnerability in MLLM agents and
can be effectively addressed through selective layer-wise modulation.

</details>


### [332] [Game Theory Meets LLM and Agentic AI: Reimagining Cybersecurity for the Age of Intelligent Threats](https://arxiv.org/abs/2507.10621)
*Quanyan Zhu*

Main category: cs.CR

TL;DR: This paper discusses integrating game theory, agentic AI, and LLMs to enhance adaptive and intelligent cybersecurity systems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to bridge the gap between theoretical frameworks and practical implementations in cybersecurity using game theory and emerging tools like LLMs and agentic AI.

Method: The paper combines theoretical reviews (e.g., game-theoretic frameworks) with conceptual analyses of how LLM-powered agents operationalize strategies and enhance cyber defenses.

Result: LLM agents are positioned to enhance reasoning in cybersecurity, promoting adaptive and trust-aware systems. Novel game-theoretic models are proposed to align with AI challenges.

Conclusion: The integration of game theory, LLMs, and agentic AI can foster secure and intelligent cyber systems by enabling modular, adaptive, and trust-aware defenses.

Abstract: Protecting cyberspace requires not only advanced tools but also a shift in
how we reason about threats, trust, and autonomy. Traditional cybersecurity
methods rely on manual responses and brittle heuristics. To build proactive and
intelligent defense systems, we need integrated theoretical frameworks and
software tools. Game theory provides a rigorous foundation for modeling
adversarial behavior, designing strategic defenses, and enabling trust in
autonomous systems. Meanwhile, software tools process cyber data, visualize
attack surfaces, verify compliance, and suggest mitigations. Yet a disconnect
remains between theory and practical implementation.
  The rise of Large Language Models (LLMs) and agentic AI offers a new path to
bridge this gap. LLM-powered agents can operationalize abstract strategies into
real-world decisions. Conversely, game theory can inform the reasoning and
coordination of these agents across complex workflows. LLMs also challenge
classical game-theoretic assumptions, such as perfect rationality or static
payoffs, prompting new models aligned with cognitive and computational
realities. This co-evolution promises richer theoretical foundations and novel
solution concepts. Agentic AI also reshapes software design: systems must now
be modular, adaptive, and trust-aware from the outset.
  This chapter explores the intersection of game theory, agentic AI, and
cybersecurity. We review key game-theoretic frameworks (e.g., static, dynamic,
Bayesian, and signaling games) and solution concepts. We then examine how LLM
agents can enhance cyber defense and introduce LLM-driven games that embed
reasoning into AI agents. Finally, we explore multi-agent workflows and
coordination games, outlining how this convergence fosters secure, intelligent,
and adaptive cyber systems.

</details>


### [333] [Spectral Feature Extraction for Robust Network Intrusion Detection Using MFCCs](https://arxiv.org/abs/2507.10622)
*HyeYoung Lee,Muhammad Nadeem,Pavel Tsoi*

Main category: cs.CR

TL;DR: The paper introduces an approach for anomaly detection in IoT networks using adaptive Mel-frequency cepstral coefficients (MFCCs) with ResNet-18, achieving high performance on intrusion detection datasets.


<details>
  <summary>Details</summary>
Motivation: The rise in IoT network deployments has led to increased security threats, making effective anomaly detection systems essential to manage vulnerabilities and ensure network safety.

Method: This study leverages adaptive Mel-frequency cepstral coefficients (MFCCs) to effectively represent network traffic patterns, coupled with the ResNet-18 deep learning model to extract features and classify anomalies. Three datasets (CICIoT2023, NSL-KDD, IoTID20) are used for evaluation.

Result: The proposed approach showed strong performance, integrating adaptive signal processing and deep learning capabilities for accurate anomaly detection across diverse IoT networks.

Conclusion: The combination of learnable MFCCs and ResNet-18 is an effective, scalable solution for IoT intrusion detection, highlighting its potential to address growing cybersecurity challenges in IoT environments.

Abstract: The rapid expansion of Internet of Things (IoT) networks has led to a surge
in security vulnerabilities, emphasizing the critical need for robust anomaly
detection and classification techniques. In this work, we propose a novel
approach for identifying anomalies in IoT network traffic by leveraging the
Mel-frequency cepstral coefficients (MFCC) and ResNet-18, a deep learning model
known for its effectiveness in feature extraction and image-based tasks.
Learnable MFCCs enable adaptive spectral feature representation, capturing the
temporal patterns inherent in network traffic more effectively than traditional
fixed MFCCs. We demonstrate that transforming raw signals into MFCCs maps the
data into a higher-dimensional space, enhancing class separability and enabling
more effective multiclass classification. Our approach combines the strengths
of MFCCs with the robust feature extraction capabilities of ResNet-18, offering
a powerful framework for anomaly detection. The proposed model is evaluated on
three widely used IoT intrusion detection datasets: CICIoT2023, NSL-KDD, and
IoTID20. The experimental results highlight the potential of integrating
adaptive signal processing techniques with deep learning architectures to
achieve robust and scalable anomaly detection in heterogeneous IoT network
landscapes.

</details>


### [334] [PhreshPhish: A Real-World, High-Quality, Large-Scale Phishing Website Dataset and Benchmark](https://arxiv.org/abs/2507.10854)
*Thomas Dalton,Hemanth Gowda,Girish Rao,Sachin Pargi,Alireza Hadj Khodabakhshi,Joseph Rombs,Stephan Jou,Manish Marwah*

Main category: cs.CR

TL;DR: PhreshPhish introduces a new, high-quality, large dataset and realistic benchmarks for phishing detection to address existing limitations such as data quality and leakage.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in phishing detection datasets, such as poor quality, leakage, and unrealistic base rates, which hinder machine learning performance in combating phishing threats.

Method: The authors developed PhreshPhish, a large-scale dataset of phishing websites, and created benchmark datasets that enhance realism by reducing leakage, increasing difficulty, improving diversity, and adjusting base rates to real-world scenarios.

Result: PhreshPhish outperforms existing public datasets in size and quality. Baseline performance evaluations were conducted using multiple models on the new benchmarks.

Conclusion: The release of PhreshPhish and its benchmarks will enable more realistic, standardized model comparisons and support further research advances in phishing detection.

Abstract: Phishing remains a pervasive and growing threat, inflicting heavy economic
and reputational damage. While machine learning has been effective in real-time
detection of phishing attacks, progress is hindered by lack of large,
high-quality datasets and benchmarks. In addition to poor-quality due to
challenges in data collection, existing datasets suffer from leakage and
unrealistic base rates, leading to overly optimistic performance results. In
this paper, we introduce PhreshPhish, a large-scale, high-quality dataset of
phishing websites that addresses these limitations. Compared to existing public
datasets, PhreshPhish is substantially larger and provides significantly higher
quality, as measured by the estimated rate of invalid or mislabeled data
points. Additionally, we propose a comprehensive suite of benchmark datasets
specifically designed for realistic model evaluation by minimizing leakage,
increasing task difficulty, enhancing dataset diversity, and adjustment of base
rates more likely to be seen in the real world. We train and evaluate multiple
solution approaches to provide baseline performance on the benchmark sets. We
believe the availability of this dataset and benchmarks will enable realistic,
standardized model comparison and foster further advances in phishing
detection. The datasets and benchmarks are available on Hugging Face
(https://huggingface.co/datasets/phreshphish/phreshphish).

</details>


### [335] [Hashed Watermark as a Filter: Defeating Forging and Overwriting Attacks in Weight-based Neural Network Watermarking](https://arxiv.org/abs/2507.11137)
*Yuan Yao,Jin Song,Jian Jin*

Main category: cs.CR

TL;DR: The paper introduces NeuralMark, a robust watermarking method for deep neural networks using hashed filters to defend against various attacks.


<details>
  <summary>Details</summary>
Motivation: To address vulnerabilities in neural network watermarking methods, especially against forging and overwriting attacks.

Method: The proposal involves using a hash-based binary watermark filter linked with embedding parameters, alongside average pooling, to secure neural networks.

Result: NeuralMark demonstrated robustness across 13 architectures and multiple tasks, proving its effectiveness in securing neural networks.

Conclusion: NeuralMark significantly strengthens neural network ownership protection with broad applicability and resistance to various adversarial attacks.

Abstract: As valuable digital assets, deep neural networks necessitate robust ownership
protection, positioning neural network watermarking (NNW) as a promising
solution. Among various NNW approaches, weight-based methods are favored for
their simplicity and practicality; however, they remain vulnerable to forging
and overwriting attacks. To address those challenges, we propose NeuralMark, a
robust method built around a hashed watermark filter. Specifically, we utilize
a hash function to generate an irreversible binary watermark from a secret key,
which is then used as a filter to select the model parameters for embedding.
This design cleverly intertwines the embedding parameters with the hashed
watermark, providing a robust defense against both forging and overwriting
attacks. An average pooling is also incorporated to resist fine-tuning and
pruning attacks. Furthermore, it can be seamlessly integrated into various
neural network architectures, ensuring broad applicability. Theoretically, we
analyze its security boundary. Empirically, we verify its effectiveness and
robustness across 13 distinct Convolutional and Transformer architectures,
covering five image classification tasks and one text generation task. The
source codes are available at https://github.com/AIResearch-Group/NeuralMark.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [336] [AGFS-Tractometry: A Novel Atlas-Guided Fine-Scale Tractometry Approach for Enhanced Along-Tract Group Statistical Comparison Using Diffusion MRI Tractography](https://arxiv.org/abs/2507.10601)
*Ruixi Zheng,Wei Zhang,Yijie Li,Xi Zhu,Zhou Lan,Jarrett Rushmore,Yogesh Rathi,Nikos Makris,Lauren J. O'Donnell,Fan Zhang*

Main category: q-bio.QM

TL;DR: This paper introduces AGFS-Tractometry, a technique for fine-scale analysis of white matter fiber tracts, using a novel atlas-guided profiling template and permutation tests.


<details>
  <summary>Details</summary>
Motivation: To enhance white matter tract analysis by addressing limitations in existing tractometry methods and improving sensitivity and specificity in detecting localized group differences.

Method: The authors develop AGFS-Tractometry, featuring an atlas-based profiling template for fine-scale analysis and a novel permutation testing method to compare groups along tract parcels while accounting for multiple comparisons.

Result: Experimental results, both in synthetic and real datasets, show that AGFS-Tractometry outperforms other state-of-the-art methods (AFQ, BUAN) by identifying more anatomically consistent regions of significant differences.

Conclusion: AGFS-Tractometry enhances tract-based statistical analysis and can detect subtle, localized white matter group differences, offering an advanced tool for neuroscientific research.

Abstract: Diffusion MRI (dMRI) tractography is currently the only method for in vivo
mapping of the brain's white matter (WM) connections. Tractometry is an
advanced tractography analysis technique for along-tract profiling to
investigate the morphology and microstructural properties along the fiber
tracts. Tractometry has become an essential tool for studying local along-tract
differences between different populations (e.g., health vs disease). In this
study, we propose a novel atlas-guided fine-scale tractometry method, namely
AGFS-Tractometry, that leverages tract spatial information and permutation
testing to enhance the along-tract statistical analysis between populations.
There are two major contributions in AGFS-Tractometry. First, we create a novel
atlas-guided tract profiling template that enables consistent, fine-scale,
along-tract parcellation of subject-specific fiber tracts. Second, we propose a
novel nonparametric permutation testing group comparison method to enable
simultaneous analysis across all along-tract parcels while correcting for
multiple comparisons. We perform experimental evaluations on synthetic datasets
with known group differences and in vivo real data. We compare AGFS-Tractometry
with two state-of-the-art tractometry methods, including Automated Fiber-tract
Quantification (AFQ) and BUndle ANalytics (BUAN). Our results show that the
proposed AGFS-Tractometry obtains enhanced sensitivity and specificity in
detecting local WM differences. In the real data analysis experiments,
AGFS-Tractometry can identify more regions with significant differences, which
are anatomically consistent with the existing literature. Overall, these
demonstrate the ability of AGFS-Tractometry to detect subtle or spatially
localized WM group-level differences. The created tract profiling template and
related code are available at:
https://github.com/ZhengRuixi/AGFS-Tractometry.git.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [337] [Causal Discovery for Linear Non-Gaussian Models with Disjoint Cycles](https://arxiv.org/abs/2507.10767)
*Mathias Drton,Marina Garrote-López,Niko Nikov,Elina Robeva,Y. Samuel Wang*

Main category: math.ST

TL;DR: The paper addresses challenges in modeling causal feedback loops with directed cycles under linear non-Gaussian frameworks, proposing methods to identify and infer causal structures efficiently.


<details>
  <summary>Details</summary>
Motivation: Causal feedback loops involving directed cycles are not well-characterized in terms of conditional independence, making their detection and learning a complex issue in structural equation modeling.

Method: The authors characterize conditions under which two directed graphs represent the same linear non-Gaussian model. They exploit polynomial relations and use techniques like decorrelating cycles and multivariate regression to infer causal structures with disjoint cycles.

Result: The paper presents a consistent and computationally efficient algorithm to learn causal structures, particularly focusing on disjoint cycles in linear non-Gaussian models.

Conclusion: The proposed approach provides a foundational advance in dealing with cyclic causal structures, improving both the theoretical understanding and practical computational tools.

Abstract: The paradigm of linear structural equation modeling readily allows one to
incorporate causal feedback loops in the model specification. These appear as
directed cycles in the common graphical representation of the models. However,
the presence of cycles entails difficulties such as the fact that models need
no longer be characterized by conditional independence relations. As a result,
learning cyclic causal structures remains a challenging problem. In this paper,
we offer new insights on this problem in the context of linear non-Gaussian
models. First, we precisely characterize when two directed graphs determine the
same linear non-Gaussian model. Next, we take up a setting of cycle-disjoint
graphs, for which we are able to show that simple quadratic and cubic
polynomial relations among low-order moments of a non-Gaussian distribution
allow one to locate source cycles. Complementing this with a strategy of
decorrelating cycles and multivariate regression allows one to infer a
block-topological order among the directed cycles, which leads to a {consistent
and computationally efficient algorithm} for learning causal structures with
disjoint cycles.

</details>


<div id='q-bio.OT'></div>

# q-bio.OT [[Back]](#toc)

### [338] [An Interpretable AI framework Quantifying Traditional Chinese Medicine Principles Towards Enhancing and Integrating with Modern Biomedicine](https://arxiv.org/abs/2507.11176)
*Haoran Li,Xingye Cheng,Ziyang Huang,Jingyuan Luo,Qianqian Xu,Qiguang Zhao,Tianchen Guo,Yumeng Zhang,Linda Lidan Zhong,Zhaoxiang Bian,Leihan Tang,Aiping Lyu,Liang Tian*

Main category: q-bio.OT

TL;DR: The paper introduces an AI framework trained on Traditional Chinese Medicine (TCM) principles to quantify symptom-herbal therapy mappings, revealing biological associations and opportunities for drug development.


<details>
  <summary>Details</summary>
Motivation: To address the interpretability and reliability limitations of Traditional Chinese Medicine (TCM) principles by introducing a quantitative and molecular-level framework for its practice.

Method: The researchers trained an AI model using ancient and classical TCM formula records to construct an interpretable TCM embedding space (TCM-ES). They associated TCM practices with biomedical entities and validated them using patient data.

Result: The AI framework demonstrated alignment between TCM principles and biological functions, reinforced by genetic correlations in the human protein interactome. The TCM-ES identified latent disease relationships and predicted associations useful for drug development.

Conclusion: The study establishes an integrative AI-driven framework for TCM practices, enabling quantification, validation, and exploration of its principles within modern biomedical contexts. This paves the way for innovative disease analysis and therapy development.

Abstract: Traditional Chinese Medicine diagnosis and treatment principles, established
through centuries of trial-and-error clinical practice, directly maps
patient-specific symptom patterns to personalised herbal therapies. These
empirical holistic mapping principles offer valuable strategies to address
remaining challenges of reductionism methodologies in modern biomedicine.
However, the lack of a quantitative framework and molecular-level evidence has
limited their interpretability and reliability. Here, we present an AI
framework trained on ancient and classical TCM formula records to quantify the
symptom pattern-herbal therapy mappings. Interestingly, we find that empirical
TCM diagnosis and treatment are consistent with the encoding-decoding processes
in the AI model. This enables us to construct an interpretable TCM embedding
space (TCM-ES) using the model's quantitative representation of TCM principles.
Validated through broad and extensive TCM patient data, the TCM-ES offers
universal quantification of the TCM practice and therapeutic efficacy. We
further map biomedical entities into the TCM-ES through correspondence
alignment. We find that the principal directions of the TCM-ES are
significantly associated with key biological functions (such as metabolism,
immune, and homeostasis), and that the disease and herb embedding proximity
aligns with their genetic relationships in the human protein interactome, which
demonstrate the biological significance of TCM principles. Moreover, the TCM-ES
uncovers latent disease relationships, and provides alternative metric to
assess clinical efficacy for modern disease-drug pairs. Finally, we construct a
comprehensive and integrative TCM knowledge graph, which predicts potential
associations between diseases and targets, drugs, herbal compounds, and herbal
therapies, providing TCM-informed opportunities for disease analysis and drug
development.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [339] [SQLord: A Robust Enterprise Text-to-SQL Solution via Reverse Data Generation and Workflow Decomposition](https://arxiv.org/abs/2507.10629)
*Song Cheng,Qiannan Cheng,Linbo Jin,Lei Yi,Guannan Zhang*

Main category: cs.DB

TL;DR: SQLord is an enterprise NL2SQL framework that enhances query translation accuracy through data reverse generation, query decomposition, and a comprehensive evaluation framework tailored to real-world business environments.


<details>
  <summary>Details</summary>
Motivation: Existing NL2SQL frameworks struggle with complex business logic and lack domain-specific data, while current evaluation methods are impractical due to real-world data and environment limitations.

Method: SQLord introduces a data reverse generation method to create annotated datasets, a workflow-based query decomposition approach for complex queries, and a versatile evaluation framework (GPT-Judge) consisting of three metrics: EXE, QSE, and SSE.

Result: SQLord achieved offline test results significantly outperforming existing baselines and maintained over 90% accuracy in online settings, proving its robustness and applicability to real-world business scenarios.

Conclusion: SQLord is a novel and effective NL2SQL framework that bridges existing gaps, demonstrating high performance and adaptability in various applications, particularly in enterprise-level environments.

Abstract: Transforming natural language into SQL queries (NL2SQL) is crucial for
data-driven business applications. Existing frameworks, trained on open-source
datasets, struggle with complex business logic and lack domain-specific data
for fine-tuning. Additionally, evaluation methods often require annotated data
and executable database environments, which are scarce in real-world scenarios.
To address these challenges, we propose SQLord, an enterprise-level NL2SQL
framework. First, SQLord introduces a data reverse generation approach to
convert raw SQL statements into annotated data for supervised fine-tuning
(SFT). Second, it proposes a decomposition method for complex queries using an
automated workflow generator. Additionally, SQLord features a comprehensive
GPT-Judge evaluation framework, including Execution Evaluation (EXE), Query-SQL
Evaluation (QSE), and SQL-SQL Evaluation (SSE), tailored to diverse scenarios.
Offline tests significantly outperform state of the art baselines, and online
accuracy consistently exceeds 90, highlighting SQLord's advantages and
effectiveness in complex real world scenarios. SQLord has been successfully
applied across multiple scenarios on the world's largest B2B e-commerce
platform.

</details>


### [340] [Towards Practical Benchmarking of Data Cleaning Techniques: On Generating Authentic Errors via Large Language Models](https://arxiv.org/abs/2507.10934)
*Xinyuan Liu,Jiahui Chen,Bocheng Hu,Yu Sun,Xinyang Chen,Shaoxu Song*

Main category: cs.DB

TL;DR: This paper introduces TableEG, a framework using fine-tuned large language models (LLMs) to generate realistic synthetic errors in tabular data for improved evaluation of error detection methods.


<details>
  <summary>Details</summary>
Motivation: Error detection in tabular data is critical but hindered by the lack of diverse, real-world error datasets. Manual annotation is inconsistent and time-intensive, prompting the need for reliable synthetic error generation methods.

Method: The proposed framework, TableEG, uses fine-tuned large language models (LLMs) and a triplet representation (Input-Transformation-Output) to model error generation, detection, and correction tasks, capturing complex table dependencies. It is trained on 12 real-world datasets across 10 domains.

Result: Experimental results demonstrate that TableEG-generated errors closely mimic real-world error patterns, outperforming rule-based and non-fine-tuned LLM-generated errors in terms of pattern similarity. Machine learning models evaluated on these synthetic errors showed performance metrics highly aligned with those on real-world errors.

Conclusion: TableEG effectively synthesizes realistic tabular data errors, bridging the gap between synthetic and real-world datasets and providing a reliable benchmark for testing error detection and correction algorithms.

Abstract: Data quality remains an important challenge in data-driven systems, as errors
in tabular data can severely compromise downstream analytics and machine
learning performance. Although numerous error detection algorithms have been
proposed, the lack of diverse, real-world error datasets limits comprehensive
evaluation. Manual error annotation is both time-consuming and inconsistent,
motivating the exploration of synthetic error generation as an alternative. In
this work, we introduce TableEG, a framework that leverages large language
models (LLMs) to generate authentic errors. By employing a table fine-tuning
strategy and a triplet representation $(I, T, O)$ to model error generation,
detection, and correction tasks, TableEG captures the complex dependencies
inherent in two-dimensional tables. Trained on 12 real-world datasets spanning
10 diverse domains, TableEG ensures that the synthesized errors faithfully
reflect authentic error distributions. Experimental results indicate that
errors generated by TableEG exhibit superior pattern and distribution
similarity compared to both rule-based methods and LLM-generated errors without
fine-tuning. Furthermore, performance metrics on TableEG-generated errors
closely align with those on real-world errors across nearly all datasets and
detection algorithms, particularly for machine learning based detection
techniques. Overall, TableEG not only bridges the gap between synthetic and
real-world errors but also establishes a robust benchmark for subsequent error
detection and correction tasks.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [341] [Protocols for Verifying Smooth Strategies in Bandits and Games](https://arxiv.org/abs/2507.10567)
*Miranda Christ,Daniel Reichman,Jonathan Shafer*

Main category: cs.GT

TL;DR: The paper proposes efficient protocols for verifying approximate optimality of strategies in multi-armed bandits and normal-form games, using fewer queries than standard learning methods.


<details>
  <summary>Details</summary>
Motivation: Analyzing large strategy spaces in multi-armed bandits and games can be computationally expensive. The authors aim to verify approximate optimality with sublinear query complexity to make this process more efficient.

Method: The researchers designed protocols to verify smooth strategies' approximate optimality by ensuring probability mass is not overly concentrated on specific actions. They derive a lower bound to establish the near-optimal efficiency of their protocols.

Result: Verification protocols for multi-armed bandits were shown to require fewer arm queries compared to learning methods. They also extended the verification protocols to normal-form games for achieving approximate smooth Nash equilibrium with sublinear queries.

Conclusion: The study demonstrates the efficiency of sublinear query protocols in verifying approximate optimality for different settings, offering significant computational benefits over traditional methods.

Abstract: We study protocols for verifying approximate optimality of strategies in
multi-armed bandits and normal-form games. As the number of actions available
to each player is often large, we seek protocols where the number of queries to
the utility oracle is sublinear in the number of actions. We prove that such
verification is possible for sufficiently smooth strategies that do not put too
much probability mass on any specific action. We provide protocols for
verifying that a smooth policy for a multi-armed bandit is
$\varepsilon$-optimal. Our verification protocols require provably fewer arm
queries than learning. Furthermore, we establish a nearly-tight lower bound on
the query complexity of verification in our settings. As an application, we
show how to use verification for bandits to achieve verification in normal-form
games. This gives a protocol for verifying whether a given strategy profile is
an approximate strong smooth Nash equilibrium, with a query complexity that is
sublinear in the number of actions.

</details>


### [342] [A Parallelizable Approach for Characterizing NE in Zero-Sum Games After a Linear Number of Iterations of Gradient Descent](https://arxiv.org/abs/2507.11366)
*Taemin Kim,James P. Bailey*

Main category: cs.GT

TL;DR: The paper proposes a Hamiltonian dynamics-based method for solving zero-sum games, achieving faster, parallelizable computation of Nash Equilibria (NE) compared to traditional algorithms.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address limitations of existing methods for finding Nash Equilibria in zero-sum games, which lack efficiency, parallelizability, and adaptability to arbitrary learning rates.

Method: The paper leverages Hamiltonian dynamics to design an optimization algorithm that uses alternating gradient descent to compute Nash Equilibria in a finite number of iterations.

Result: The proposed method surpasses traditional methods in performance and flexibility, as demonstrated through experiments.

Conclusion: Hamiltonian dynamics offer a significant improvement in solving zero-sum games, enabling parallel computation and the use of arbitrary learning rates.

Abstract: We study online optimization methods for zero-sum games, a fundamental
problem in adversarial learning in machine learning, economics, and many other
domains. Traditional methods approximate Nash equilibria (NE) using either
regret-based methods (time-average convergence) or contraction-map-based
methods (last-iterate convergence). We propose a new method based on
Hamiltonian dynamics in physics and prove that it can characterize the set of
NE in a finite (linear) number of iterations of alternating gradient descent in
the unbounded setting, modulo degeneracy, a first in online optimization.
Unlike standard methods for computing NE, our proposed approach can be
parallelized and works with arbitrary learning rates, both firsts in
algorithmic game theory. Experimentally, we support our results by showing our
approach drastically outperforms standard methods.

</details>


### [343] [Better Regret Rates in Bilateral Trade via Sublinear Budget Violation](https://arxiv.org/abs/2507.11419)
*Anna Lunghi,Matteo Castiglioni,Alberto Marchesi*

Main category: cs.GT

TL;DR: This paper explores the trade-off between regret and global budget balance constraint violation in bilateral trade using algorithmic methods.


<details>
  <summary>Details</summary>
Motivation: To address the impossibility of no-regret learning in trading mechanisms under strict budget balance constraints by considering relaxed global budget balance constraints.

Method: The authors propose an algorithm that manages the trade-off by allowing budget constraint violations of up to $T^{\beta}$, achieving regret rates dependent on $\beta$, and prepare matching lower bounds to validate these trade-offs.

Result: The proposed algorithm achieves regret rates of $\tilde O(T^{1 - \beta/3})$ while violating global budget constraints by $T^{\beta}$ for $\beta \in [3/4, 6/7]$, and the results include a matching lower bound to confirm optimality.

Conclusion: Their work precisely characterizes the trade-off between regret rates and global budget violations and validates earlier bounds established in Bernasconi et al. [Ber+24].

Abstract: Bilateral trade is a central problem in algorithmic economics, and recent
work has explored how to design trading mechanisms using no-regret learning
algorithms. However, no-regret learning is impossible when budget balance has
to be enforced at each time step. Bernasconi et al. [Ber+24] show how this
impossibility can be circumvented by relaxing the budget balance constraint to
hold only globally over all time steps. In particular, they design an algorithm
achieving regret of the order of $\tilde O(T^{3/4})$ and provide a lower bound
of $\Omega(T^{5/7})$.
  In this work, we interpolate between these two extremes by studying how the
optimal regret rate varies with the allowed violation of the global budget
balance constraint. Specifically, we design an algorithm that, by violating the
constraint by at most $T^{\beta}$ for any given $\beta \in [\frac{3}{4},
\frac{6}{7}]$, attains regret $\tilde O(T^{1 - \beta/3})$. We complement this
result with a matching lower bound, thus fully characterizing the trade-off
between regret and budget violation. Our results show that both the $\tilde
O(T^{3/4})$ upper bound in the global budget balance case and the
$\Omega(T^{5/7})$ lower bound under unconstrained budget balance violation
obtained by Bernasconi et al. [Ber+24] are tight.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [344] [Neural Expectation Operators](https://arxiv.org/abs/2507.10607)
*Qian Qi*

Main category: math.PR

TL;DR: The paper presents a novel framework called Measure Learning, introducing Neural Expectation Operators based on BSDEs. It bridges complex mathematical concepts with practical neural network designs, demonstrating their viability and providing foundational tools for handling ambiguity in data-driven models.


<details>
  <summary>Details</summary>
Motivation: To address ambiguity in data-driven models by leveraging BSDEs and integrating mathematical rigour with neural network design.

Method: The authors employ Neural Expectation Operators modeled as solutions to BSDEs, adapting them for machine learning applications by relaxing classical assumptions and parameterizing drivers with neural networks.

Result: The paper establishes that BSDEs with local Lipschitz conditions for state variables and quadratic growth constraints can be implemented effectively in common neural network architectures, meeting necessary mathematical and practical criteria.

Conclusion: This work successfully builds a foundational bridge between mathematical theory and machine learning, providing tools to enforce essential properties such as convexity and advancing modeling techniques for systems under ambiguity.

Abstract: This paper introduces \textbf{Measure Learning}, a paradigm for modeling
ambiguity via non-linear expectations. We define Neural Expectation Operators
as solutions to Backward Stochastic Differential Equations (BSDEs) whose
drivers are parameterized by neural networks. The main mathematical
contribution is a rigorous well-posedness theorem for BSDEs whose drivers
satisfy a local Lipschitz condition in the state variable $y$ and quadratic
growth in its martingale component $z$. This result circumvents the classical
global Lipschitz assumption, is applicable to common neural network
architectures (e.g., with ReLU activations), and holds for exponentially
integrable terminal data, which is the sharp condition for this setting. Our
primary innovation is to build a constructive bridge between the abstract, and
often restrictive, assumptions of the deep theory of quadratic BSDEs and the
world of machine learning, demonstrating that these conditions can be met by
concrete, verifiable neural network designs. We provide constructive methods
for enforcing key axiomatic properties, such as convexity, by architectural
design. The theory is extended to the analysis of fully coupled
Forward-Backward SDE systems and to the asymptotic analysis of large
interacting particle systems, for which we establish both a Law of Large
Numbers (propagation of chaos) and a Central Limit Theorem. This work provides
the foundational mathematical framework for data-driven modeling under
ambiguity.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [345] [From Kinetic Theory to AI: a Rediscovery of High-Dimensional Divergences and Their Properties](https://arxiv.org/abs/2507.11387)
*Gennaro Auricchio,Giovanni Brigati,Paolo Giudici,Giuseppe Toscani*

Main category: math-ph

TL;DR: The paper reviews divergence measures from kinetic theory, focusing on their theoretical foundation and applications in machine learning and AI.


<details>
  <summary>Details</summary>
Motivation: To evaluate and understand the applicability of divergence measures, especially the KL divergence, in both kinetic theory and machine learning contexts.

Method: A comparative review of divergence measures from kinetic theory, examining their theoretical bases and aligning them with machine learning applications.

Result: A comprehensive analysis of how divergence measures from kinetic theory can provide benefits and insights for machine learning and artificial intelligence tasks.

Conclusion: Divergence measures from kinetic theory, such as KL divergence, have theoretical significance and practical applications that bridge the domains of kinetic theory and machine learning.

Abstract: Selecting an appropriate divergence measure is a critical aspect of machine
learning, as it directly impacts model performance. Among the most widely used,
we find the Kullback-Leibler (KL) divergence, originally introduced in kinetic
theory as a measure of relative entropy between probability distributions. Just
as in machine learning, the ability to quantify the proximity of probability
distributions plays a central role in kinetic theory. In this paper, we present
a comparative review of divergence measures rooted in kinetic theory,
highlighting their theoretical foundations and exploring their potential
applications in machine learning and artificial intelligence.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [346] [Formal Verification of Variational Quantum Circuits](https://arxiv.org/abs/2507.10635)
*Nicola Assolini,Luca Marzari,Isabella Mastroeni,Alessandra di Pierro*

Main category: quant-ph

TL;DR: This paper addresses the lack of formal verification approaches for Variational Quantum Circuits (VQCs) and proposes a novel method inspired by abstract interpretation for analyzing robustness.


<details>
  <summary>Details</summary>
Motivation: Despite increasing utilization of Variational Quantum Circuits in quantum machine learning, there is no formal framework to certify their robustness against adversarial inputs.

Method: The authors adapt interval-based reachability techniques and develop a semantic framework using abstract interpretation specialized for VQCs.

Result: They identify challenges arising from quantum-specific characteristics and validate their framework on standard benchmarks, showcasing the approach's practicality.

Conclusion: The paper establishes the feasibility of formal verification for VQCs, opening up avenues for more robust quantum machine learning frameworks.

Abstract: Variational quantum circuits (VQCs) are a central component of many quantum
machine learning algorithms, offering a hybrid quantum-classical framework
that, under certain aspects, can be considered similar to classical deep neural
networks. A shared aspect is, for instance, their vulnerability to adversarial
inputs, small perturbations that can lead to incorrect predictions. While
formal verification techniques have been extensively developed for classical
models, no comparable framework exists for certifying the robustness of VQCs.
Here, we present the first in-depth theoretical and practical study of the
formal verification problem for VQCs. Inspired by abstract interpretation
methods used in deep learning, we analyze the applicability and limitations of
interval-based reachability techniques in the quantum setting. We show that
quantum-specific aspects, such as state normalization, introduce inter-variable
dependencies that challenge existing approaches. We investigate these issues by
introducing a novel semantic framework based on abstract interpretation, where
the verification problem for VQCs can be formally defined, and its complexity
analyzed. Finally, we demonstrate our approach on standard verification
benchmarks.

</details>


### [347] [Stochastic Entanglement Configuration for Constructive Entanglement Topologies in Quantum Machine Learning with Application to Cardiac MRI](https://arxiv.org/abs/2507.11401)
*Mehri Mehrnia,Mohammed S. M. Elbaz*

Main category: quant-ph

TL;DR: The paper presents a novel stochastic method for generating random entanglement configurations in variational quantum circuits (VQCs) for quantum machine learning, achieving better performance than fixed classical and traditional quantum approaches.


<details>
  <summary>Details</summary>
Motivation: Current variational quantum circuits rely on static entanglement topologies, which are not optimized to suit specific task needs, limiting their potential to outperform classical machine learning models.

Method: A stochastic method was developed to generate diverse entanglement topologies, represented by binary matrices that encode directed entanglement between qubits. The method used sampling modes to control entanglement density and per-qubit constraints, aiming to identify configurations that improve task performance.

Result: When applied to cardiac MRI disease classification, the method identified 64 entanglement configurations (16% of the 400 tested) that consistently surpassed classical baselines. These configurations achieved up to ~0.92 accuracy, compared to the classical model’s ~0.87 and traditional quantum topologies' ~0.82.

Conclusion: The results demonstrate that task-adaptive, stochastic entanglement configurations can significantly enhance hybrid quantum-classical models, outperforming both classical models and conventional quantum approaches in machine learning tasks.

Abstract: Efficient entanglement strategies are essential for advancing variational
quantum circuits (VQCs) for quantum machine learning (QML). However, most
current approaches use fixed entanglement topologies that are not adaptive to
task requirements, limiting potential gains over classical models. We introduce
a novel stochastic entanglement configuration method that systematically
generates diverse entanglement topologies to identify a subspace of
constructive entanglement configurations, defined as entanglement topologies
that boost hybrid model performance (e.g., classification accuracy) beyond
classical baselines. Each configuration is encoded as a stochastic binary
matrix, denoting directed entanglement between qubits. This enables scalable
exploration of the hyperspace of candidate entanglement topologies using
entanglement density and per-qubit constraints as key metrics. We define
unconstrained and constrained sampling modes, controlling entanglement per
qubit. Using our method, 400 stochastic configurations were generated and
evaluated in a hybrid QML for cardiac MRI disease classification. We identified
64 (16%) novel constructive entanglement configurations that consistently
outperformed the classical baseline. Ensemble aggregation of top-performing
configurations achieved ~0.92 classification accuracy, exceeding the classical
model (~0.87) by over 5%. Compared to four conventional topologies (ring,
nearest neighbor, no entanglement, fully entangled), none surpassed the
classical baseline (maximum accuracy ~0.82), while our configurations delivered
up to ~20% higher accuracy. Thus, highlighting the robustness and
generalizability of the identified constructive entanglements.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [348] [Real-time, Adaptive Radiological Anomaly Detection and Isotope Identification Using Non-negative Matrix Factorization](https://arxiv.org/abs/2507.10715)
*Chandler Jones,Mark Bandstra,Stefan Faaland,Yue Shi Lai,Nico Abgrall,Scott Suchyta,Reynold Cooper*

Main category: physics.app-ph

TL;DR: The paper presents an Adaptive Non-Negative Matrix Factorization (NMF) algorithm tailored for spectroscopic anomaly detection and isotope identification in mobile nuclear detection systems, addressing challenges posed by changing gamma-ray backgrounds.


<details>
  <summary>Details</summary>
Motivation: To tackle limitations of conventional NMF-based algorithms in adapting to dynamic gamma-ray background changes in mobile detection systems, which can lead to high false alarm rates or reduced sensitivity.

Method: Develop a novel Adaptive NMF algorithm that periodically updates its background model in real time to adapt to environmental changes in gamma-ray backgrounds.

Result: The Adaptive NMF algorithm demonstrates improved generalizability and detection performance on both simulated and real-world datasets compared to conventional methods.

Conclusion: Adaptive NMF proves effective for nuclear nonproliferation applications, handling environmental changes better than existing algorithms without compromising detection performance.

Abstract: Spectroscopic anomaly detection and isotope identification algorithms are
integral components in nuclear nonproliferation applications such as search
operations. The task is especially challenging in the case of mobile detector
systems due to the fact that the observed gamma-ray background changes more
than for a static detector system, and a pretrained background model can easily
find itself out of domain. The result is that algorithms may exceed their
intended false alarm rate, or sacrifice detection sensitivity in order to
maintain the desired false alarm rate. Non-negative matrix factorization (NMF)
has been shown to be a powerful tool for spectral anomaly detection and
identification, but, like many similar algorithms that rely on data-driven
background models, in its conventional implementation it is unable to update in
real time to account for environmental changes that affect the background
spectroscopic signature. We have developed a novel NMF-based algorithm that
periodically updates its background model to accommodate changing environmental
conditions. The Adaptive NMF algorithm involves fewer assumptions about its
environment, making it more generalizable than existing NMF-based methods while
maintaining or exceeding detection performance on simulated and real-world
datasets.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [349] [A Learning Framework For Cooperative Collision Avoidance of UAV Swarms Leveraging Domain Knowledge](https://arxiv.org/abs/2507.10913)
*Shuangyao Huang,Haibo Zhang,Zhiyi Huang*

Main category: cs.MA

TL;DR: This paper proposes a MARL framework for UAV swarm collision avoidance using a reward system based on image processing domain knowledge.


<details>
  <summary>Details</summary>
Motivation: Improve cooperative collision avoidance in UAV swarms by leveraging domain knowledge for more efficient and scalable solutions.

Method: Domain knowledge from image processing is used to design rewards that approximate contours on a field, reducing the need for complex credit assignments or observation sharing mechanisms in MARL.

Result: The framework allows scalable training of large UAV swarms and adapts to complex environments where contours are non-viable.

Conclusion: The proposed algorithm demonstrates superior performance compared to state-of-the-art MARL algorithms through extensive experiments.

Abstract: This paper presents a multi-agent reinforcement learning (MARL) framework for
cooperative collision avoidance of UAV swarms leveraging domain
knowledge-driven reward. The reward is derived from knowledge in the domain of
image processing, approximating contours on a two-dimensional field. By
modeling obstacles as maxima on the field, collisions are inherently avoided as
contours never go through peaks or intersect. Additionally, counters are smooth
and energy-efficient. Our framework enables training with large swarm sizes as
the agent interaction is minimized and the need for complex credit assignment
schemes or observation sharing mechanisms in state-of-the-art MARL approaches
are eliminated. Moreover, UAVs obtain the ability to adapt to complex
environments where contours may be non-viable or non-existent through intensive
training. Extensive experiments are conducted to evaluate the performances of
our framework against state-of-the-art MARL algorithms.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [350] [Pronunciation Deviation Analysis Through Voice Cloning and Acoustic Comparison](https://arxiv.org/abs/2507.10985)
*Andrew Valdivia,Yueming Zhang,Hailu Xu,Amir Ghasemkhani,Xin Qin*

Main category: cs.SD

TL;DR: The paper introduces a method using voice cloning to detect speech mispronunciations by comparing original and corrected synthetic voices.


<details>
  <summary>Details</summary>
Motivation: The ability to detect mispronunciations accurately is crucial for language learning and speech therapy, but existing methods often rely on predefined phonetic rules or extensive training data.

Method: The study uses voice cloning technology to create synthetically corrected versions of a user's speech, followed by frame-by-frame acoustic deviation analysis to locate mispronunciations.

Result: The experimental results confirm the method’s effectiveness in identifying pronunciation errors without needing predefined phonetic rules or large datasets.

Conclusion: This approach offers a promising tool for detecting mispronunciations in a simple, data-efficient manner, potentially enhancing applications in language learning and speech improvement.

Abstract: This paper presents a novel approach for detecting mispronunciations by
analyzing deviations between a user's original speech and their voice-cloned
counterpart with corrected pronunciation. We hypothesize that regions with
maximal acoustic deviation between the original and cloned utterances indicate
potential mispronunciations. Our method leverages recent advances in voice
cloning to generate a synthetic version of the user's voice with proper
pronunciation, then performs frame-by-frame comparisons to identify problematic
segments. Experimental results demonstrate the effectiveness of this approach
in pinpointing specific pronunciation errors without requiring predefined
phonetic rules or extensive training data for each target language.

</details>


### [351] [EditGen: Harnessing Cross-Attention Control for Instruction-Based Auto-Regressive Audio Editing](https://arxiv.org/abs/2507.11096)
*Vassilis Sioros,Alexandros Potamianos,Giorgos Paraskevopoulos*

Main category: cs.SD

TL;DR: This paper presents methods for efficient audio editing by leveraging cross-attention control in auto-regressive models.


<details>
  <summary>Details</summary>
Motivation: To address the need for reliable, prompt-guided audio editing inspired by image editing approaches.

Method: Introduced a Prompt-to-Prompt-like technique with diffusion-based and MUSICGEN approaches for attention-based audio editing.

Result: Proposed models deliver better audio quality in melody, dynamics, and tempo compared to diffusion-based baselines.

Conclusion: Combining prompt-controlled mechanisms with auto-regressive models enhances audio editing, achieving better controllability and realism.

Abstract: In this study, we investigate leveraging cross-attention control for
efficient audio editing within auto-regressive models. Inspired by image
editing methodologies, we develop a Prompt-to-Prompt-like approach that guides
edits through cross and self-attention mechanisms. Integrating a
diffusion-based strategy, influenced by Auffusion, we extend the model's
functionality to support refinement edits, establishing a baseline for
prompt-guided audio editing. Additionally, we introduce an alternative approach
by incorporating MUSICGEN, a pre-trained frozen auto-regressive model, and
propose three editing mechanisms, based on Replacement, Reweighting, and
Refinement of the attention scores. We employ commonly-used music-specific
evaluation metrics and a human study, to gauge time-varying controllability,
adherence to global text cues, and overall audio realism. The automatic and
human evaluations indicate that the proposed combination of prompt-to-prompt
guidance with autoregressive generation models significantly outperforms the
diffusion-based baseline in terms of melody, dynamics, and tempo of the
generated audio. Our code is available at https://github.com/billsioros/EditGen

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [352] [Elevating 3D Models: High-Quality Texture and Geometry Refinement from a Low-Quality Model](https://arxiv.org/abs/2507.11465)
*Nuri Ryu,Jiyun Won,Jooeun Son,Minsu Gong,Joo-Haeng Lee,Sunghyun Cho*

Main category: cs.GR

TL;DR: Elevate3D enhances low-quality 3D assets into high-quality ones by improving textures with HFS-SDEdit and refining geometry using monocular predictors.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity of high-quality 3D assets in computer graphics and 3D vision caused by high acquisition costs.

Method: A framework called Elevate3D refines 3D assets by alternating between texture enhancement (via HFS-SDEdit) and geometry refinement using monocular geometry predictors.

Result: Elevate3D surpasses existing methods in quality, offering state-of-the-art results in 3D asset refinement.

Conclusion: Elevate3D provides an effective solution for improving low-quality 3D assets, tackling the scarcity issue while ensuring alignment between textures and geometries.

Abstract: High-quality 3D assets are essential for various applications in computer
graphics and 3D vision but remain scarce due to significant acquisition costs.
To address this shortage, we introduce Elevate3D, a novel framework that
transforms readily accessible low-quality 3D assets into higher quality. At the
core of Elevate3D is HFS-SDEdit, a specialized texture enhancement method that
significantly improves texture quality while preserving the appearance and
geometry while fixing its degradations. Furthermore, Elevate3D operates in a
view-by-view manner, alternating between texture and geometry refinement.
Unlike previous methods that have largely overlooked geometry refinement, our
framework leverages geometric cues from images refined with HFS-SDEdit by
employing state-of-the-art monocular geometry predictors. This approach ensures
detailed and accurate geometry that aligns seamlessly with the enhanced
texture. Elevate3D outperforms recent competitors by achieving state-of-the-art
quality in 3D model refinement, effectively addressing the scarcity of
high-quality open-source 3D assets.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [353] [Functional Neural Wavefunction Optimization](https://arxiv.org/abs/2507.10835)
*Victor Armegioiu,Juan Carrasquilla,Siddhartha Mishra,Johannes Müller,Jannes Nys,Marius Zeinhofer,Hang Zhang*

Main category: cond-mat.str-el

TL;DR: The paper introduces a framework that leverages geometric insights for designing effective optimization algorithms in variational quantum Monte Carlo simulations.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of designing effective optimization algorithms for variational quantum Monte Carlo leveraging geometric insights into the function space.

Method: The authors use a Galerkin projection approach to translate infinite-dimensional optimization dynamics into parameter-space algorithms, unifying stochastic reconfiguration, Rayleigh-Gauss-Newton, and related methodologies while incorporating geometrically inspired hyperparameters.

Result: The framework was validated with numerical experiments, accurately estimating ground-state energies for models in condensed matter physics using neural network wavefunctions.

Conclusion: The proposed framework offers a unified and practical methodology for optimization in variational quantum Monte Carlo, bridging existing methods and motivating new algorithmic strategies.

Abstract: We propose a framework for the design and analysis of optimization algorithms
in variational quantum Monte Carlo, drawing on geometric insights into the
corresponding function space. The framework translates infinite-dimensional
optimization dynamics into tractable parameter-space algorithms through a
Galerkin projection onto the tangent space of the variational ansatz. This
perspective unifies existing methods such as stochastic reconfiguration and
Rayleigh-Gauss-Newton, provides connections to classic function-space
algorithms, and motivates the derivation of novel algorithms with geometrically
principled hyperparameter choices. We validate our framework with numerical
experiments demonstrating its practical relevance through the accurate
estimation of ground-state energies for several prototypical models in
condensed matter physics modeled with neural network wavefunctions.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [354] [Quantitative multi-metabolite imaging of Parkinson's disease using AI boosted molecular MRI](https://arxiv.org/abs/2507.11329)
*Hagar Shmuely,Michal Rivlin,Or Perlman*

Main category: physics.med-ph

TL;DR: The paper presents a new approach to Parkinson's disease imaging using molecular MRI and deep learning for quantifying metabolites, showing potential biomarkers.


<details>
  <summary>Details</summary>
Motivation: To address challenges in traditional MRI methods for Parkinson's disease imaging, such as low resolution, lengthy procedures, and nonspecific contrasts.

Method: Combines a rapid molecular MRI acquisition with deep learning reconstruction to quantify several metabolites in a Parkinson's disease mouse model.

Result: Achieved quantitative parameter maps in agreement with histology and MR spectroscopy; identified potential biomarkers for Parkinson's disease.

Conclusion: The study highlights semisolid MT, amide, and rNOE proton volume fractions as promising biomarkers for Parkinson's disease imaging.

Abstract: Traditional approaches for molecular imaging of Parkinson's disease (PD) in
vivo require radioactive isotopes, lengthy scan times, or deliver only low
spatial resolution. Recent advances in saturation transfer-based PD magnetic
resonance imaging (MRI) have provided biochemical insights, although the image
contrast is semi-quantitative and nonspecific. Here, we combined a rapid
molecular MRI acquisition paradigm with deep learning based reconstruction for
multi-metabolite quantification of glutamate, mobile proteins, semisolid, and
mobile macromolecules in an acute MPTP
(1-methyl-4-phenyl-1,2,3,6-tetrahydropyridine) mouse model. The quantitative
parameter maps are in general agreement with the histology and MR spectroscopy,
and demonstrate that semisolid magnetization transfer (MT), amide, and
aliphatic relayed nuclear Overhauser effect (rNOE) proton volume fractions may
serve as PD biomarkers.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [355] [From Chaos to Automation: Enabling the Use of Unstructured Data for Robotic Process Automation](https://arxiv.org/abs/2507.11364)
*Kelly Kurowski,Xixi Lu,Hajo A. Reijers*

Main category: cs.IR

TL;DR: This study introduces UNDRESS, a system leveraging NLP and LLMs, to enable RPA platforms to effectively process unstructured data like emails and reports.


<details>
  <summary>Details</summary>
Motivation: Unstructured data constitutes 80% of enterprise data but is challenging for traditional RPA tools to process, limiting automation potential.

Method: Developed and tested UNDRESS, utilizing fuzzy regular expressions, natural language processing, and large language models for better data retrieval.

Result: Results confirm the efficacy of UNDRESS in enhancing RPA performance with unstructured data, improving text extraction and retrieval.

Conclusion: UNDRESS broadens RPA application to tasks involving unstructured data, potentially improving business process efficiency and adoption.

Abstract: The growing volume of unstructured data within organizations poses
significant challenges for data analysis and process automation. Unstructured
data, which lacks a predefined format, encompasses various forms such as
emails, reports, and scans. It is estimated to constitute approximately 80% of
enterprise data. Despite the valuable insights it can offer, extracting
meaningful information from unstructured data is more complex compared to
structured data. Robotic Process Automation (RPA) has gained popularity for
automating repetitive tasks, improving efficiency, and reducing errors.
However, RPA is traditionally reliant on structured data, limiting its
application to processes involving unstructured documents. This study addresses
this limitation by developing the UNstructured Document REtrieval SyStem
(UNDRESS), a system that uses fuzzy regular expressions, techniques for natural
language processing, and large language models to enable RPA platforms to
effectively retrieve information from unstructured documents. The research
involved the design and development of a prototype system, and its subsequent
evaluation based on text extraction and information retrieval performance. The
results demonstrate the effectiveness of UNDRESS in enhancing RPA capabilities
for unstructured data, providing a significant advancement in the field. The
findings suggest that this system could facilitate broader RPA adoption across
processes traditionally hindered by unstructured data, thereby improving
overall business process efficiency.

</details>


### [356] [Overview of the TREC 2022 deep learning track](https://arxiv.org/abs/2507.10865)
*Nick Craswell,Bhaskar Mitra,Emine Yilmaz,Daniel Campos,Jimmy Lin,Ellen M. Voorhees,Ian Soboroff*

Main category: cs.IR

TL;DR: The TREC Deep Learning track for 2022 expanded datasets and focused on improving passage retrieval test collections, observing better query quality and surprising outcomes.


<details>
  <summary>Details</summary>
Motivation: To leverage expanded datasets for developing and testing passage and document retrieval models, focusing on deep neural ranking methods.

Method: The study used large-scale MS MARCO datasets, expanded passage and document collections, and emphasized passage retrieval with improved judging resources.

Result: Deep neural networks continued to outperform traditional retrieval methods, but single-stage dense retrieval methods were less competitive than expected this year.

Conclusion: Improved datasets and judging quality enhanced confidence in distinguishing retrieval performance, aiding future reuse of the dataset and guiding retrieval model development.

Abstract: This is the fourth year of the TREC Deep Learning track. As in previous
years, we leverage the MS MARCO datasets that made hundreds of thousands of
human annotated training labels available for both passage and document ranking
tasks. In addition, this year we also leverage both the refreshed passage and
document collections that were released last year leading to a nearly $16$
times increase in the size of the passage collection and nearly four times
increase in the document collection size. Unlike previous years, in 2022 we
mainly focused on constructing a more complete test collection for the passage
retrieval task, which has been the primary focus of the track. The document
ranking task was kept as a secondary task, where document-level labels were
inferred from the passage-level labels. Our analysis shows that similar to
previous years, deep neural ranking models that employ large scale pretraining
continued to outperform traditional retrieval methods. Due to the focusing our
judging resources on passage judging, we are more confident in the quality of
this year's queries and judgments, with respect to our ability to distinguish
between runs and reuse the dataset in future. We also see some surprises in
overall outcomes. Some top-performing runs did not do dense retrieval. Runs
that did single-stage dense retrieval were not as competitive this year as they
were last year.

</details>


### [357] [Extracting Document Relations from Search Corpus by Marginalizing over User Queries](https://arxiv.org/abs/2507.10726)
*Yuki Iwamoto,Kaoru Tsunoda,Ken Kaneiwa*

Main category: cs.IR

TL;DR: The paper proposes EDR-MQ, a framework for discovering document relationships by marginalizing over user queries without requiring labeled data or predefined taxonomies.


<details>
  <summary>Details</summary>
Motivation: Understanding relationships between documents enables better knowledge discovery and information organization, but current methods overly rely on manual annotations or fixed relationship taxonomies.

Method: The authors introduce EDR-MQ, based on query marginalization, and implement a mechanism called MC-RAG (Multiply Conditioned Retrieval-Augmented Generation) for estimating document joint probabilities through conditional retrieval behavior.

Result: Experimental results demonstrate EDR-MQ identifies meaningful document relationships, revealing clusters, evidence chains, and cross-domain links not captured by traditional methods.

Conclusion: The query-driven EDR-MQ framework provides an adaptive and practical solution for document relationship discovery, tailored to diverse user queries and perspectives.

Abstract: Understanding relationships between documents in large-scale corpora is
essential for knowledge discovery and information organization. However,
existing approaches rely heavily on manual annotation or predefined
relationship taxonomies. We propose EDR-MQ (Extracting Document Relations by
Marginalizing over User Queries), a novel framework that discovers document
relationships through query marginalization. EDR-MQ is based on the insight
that strongly related documents often co-occur in results across diverse user
queries, enabling us to estimate joint probabilities between document pairs by
marginalizing over a collection of queries. To enable this query
marginalization approach, we develop Multiply Conditioned Retrieval-Augmented
Generation (MC-RAG), which employs conditional retrieval where subsequent
document retrievals depend on previously retrieved content. By observing
co-occurrence patterns across diverse queries, EDR-MQ estimates joint
probabilities between document pairs without requiring labeled training data or
predefined taxonomies. Experimental results show that our query marginalization
approach successfully identifies meaningful document relationships, revealing
topical clusters, evidence chains, and cross-domain connections that are not
apparent through traditional similarity-based methods. Our query-driven
framework offers a practical approach to document organization that adapts to
different user perspectives and information needs.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [358] [Deep Equilibrium models for Poisson Imaging Inverse problems via Mirror Descent](https://arxiv.org/abs/2507.11461)
*Christian Daniele,Silvia Villa,Samuel Vaiter,Luca Calatroni*

Main category: math.OC

TL;DR: The paper introduces a novel method for solving Poisson inverse problems using Deep Equilibrium Models (DEQs) based on Mirror Descent and non-Euclidean geometry, achieving competitive performance with parameter-free inference.


<details>
  <summary>Details</summary>
Motivation: Current DEQ methods primarily address Gaussian fidelities and assume contractiveness of Gradient Descent operators. However, Poisson inverse problems, involving Kullback-Leibler divergence-based data fidelity terms, require a different approach. There is a need to extend DEQs to handle such problems effectively.

Method: The authors propose a new DEQ formulation leveraging Mirror Descent and a custom non-Euclidean geometry tailored for Poisson data fidelity. This approach enables the learning of neural regularizers in a principled way while ensuring convergence of the reconstruction scheme.

Result: The proposed method outperforms traditional model-based solutions and is on par with Bregman Plug-and-Play techniques but avoids common issues like sensitivity to initialization and hyperparameter tuning. It is computationally efficient and fully parameter-free during inference.

Conclusion: This work extends the versatility of DEQs by addressing Poisson inverse problems with a method that is both competitive and efficient, offering a solution to the limitations of existing approaches.

Abstract: Deep Equilibrium Models (DEQs) are implicit neural networks with fixed
points, which have recently gained attention for learning image regularization
functionals, particularly in settings involving Gaussian fidelities, where
assumptions on the forward operator ensure contractiveness of standard
(proximal) Gradient Descent operators. In this work, we extend the application
of DEQs to Poisson inverse problems, where the data fidelity term is more
appropriately modeled by the Kullback-Leibler divergence. To this end, we
introduce a novel DEQ formulation based on Mirror Descent defined in terms of a
tailored non-Euclidean geometry that naturally adapts with the structure of the
data term. This enables the learning of neural regularizers within a principled
training framework. We derive sufficient conditions to guarantee the
convergence of the learned reconstruction scheme and propose computational
strategies that enable both efficient training and fully parameter-free
inference. Numerical experiments show that our method outperforms traditional
model-based approaches and it is comparable to the performance of Bregman
Plug-and-Play methods, while mitigating their typical drawbacks - namely,
sensitivity to initialization and careful tuning of hyperparameters. The code
is publicly available at https://github.com/christiandaniele/DEQ-MD.

</details>


### [359] [A Mathematical Optimization Approach to Multisphere Support Vector Data Description](https://arxiv.org/abs/2507.11106)
*Víctor Blanco,Inmaculada Espejo,Raúl Páez,Antonio M. Rodríguez-Chía*

Main category: math.OC

TL;DR: The paper introduces a novel optimization framework for outlier detection, extended from Support Vector Data Description, using mathematical formulations and kernel methods.


<details>
  <summary>Details</summary>
Motivation: The need for more accurate, robust methods to detect outliers in complex multimodal and non-linear datasets, surpassing heuristic techniques.

Method: Development of a Mixed Integer Second Order Cone model in the primal form to identify anomalies and a dual model to apply the kernel trick for complex data.

Result: Extensive computational experiments demonstrated improved accuracy and robustness over heuristic methods.

Conclusion: The proposed framework provides a more effective and reliable solution for multimodal outlier detection, leveraging exact optimization methods in both linear and non-linear settings.

Abstract: We present a novel mathematical optimization framework for outlier detection
in multimodal datasets, extending Support Vector Data Description approaches.
We provide a primal formulation, in the shape of a Mixed Integer Second Order
Cone model, that constructs Euclidean hyperspheres to identify anomalous
observations. Building on this, we develop a dual model that enables the
application of the kernel trick, thus allowing for the detection of outliers
within complex, non-linear data structures. An extensive computational study
demonstrates the effectiveness of our exact method, showing clear advantages
over existing heuristic techniques in terms of accuracy and robustness.

</details>


### [360] [Recursive Bound-Constrained AdaGrad with Applications to Multilevel and Domain Decomposition Minimization](https://arxiv.org/abs/2507.11513)
*Serge Gratton,Alena Kopaničáková,Philippe Toint*

Main category: math.OC

TL;DR: This study introduces two Objective-Function Free Optimization (OFFO) algorithms utilizing inexact gradients and second-order information under noise tolerance, achieving efficient convergence for various applications.


<details>
  <summary>Details</summary>
Motivation: To develop noise-tolerant optimization algorithms that handle bound constraints, use approximated gradients, and incorporate second-order information in diverse applications.

Method: Two OFFO algorithms are presented: a multi-level approach using hierarchical problem descriptions and a domain-decomposition method utilizing Schwarz decompositions, both extending the AdaGrad algorithm to constrained optimization.

Result: The algorithms have a convergence guarantee requiring $O(\epsilon^{-2})$ iterations to reach an $\epsilon$-approximate critical point, validated by numerical experiments across PDE-based and neural network applications.

Conclusion: The proposed algorithms are computationally efficient and generalizable for solving constrained optimization problems with noisy gradient inputs.

Abstract: Two OFFO (Objective-Function Free Optimization) noise tolerant algorithms are
presented that handle bound constraints, inexact gradients and use second-order
information when available.The first is a multi-level method exploiting a
hierarchical description of the problem and the second is a
domain-decomposition method covering the standard addditive Schwarz
decompositions. Both are generalizations of the first-order AdaGrad algorithm
for unconstrained optimization. Because these algorithms share a common
theoretical framework, a single convergence/complexity theory is provided which
covers them both. Its main result is that, with high probability, both methods
need at most $O(\epsilon^{-2})$ iterations and noisy gradient evaluations to
compute an $\epsilon$-approximate first-order critical point of the
bound-constrained problem. Extensive numerical experiments are discussed on
applications ranging from PDE-based problems to deep neural network training,
illustrating their remarkable computational efficiency.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [361] [Data-Driven Differential Evolution in Tire Industry Extrusion: Leveraging Surrogate Models](https://arxiv.org/abs/2507.11191)
*Eider Garate-Perez,Kerman López de Calle-Etxabe,Susana Ferreiro*

Main category: cs.CE

TL;DR: This paper develops a data-driven approach using machine learning and a tailored metaheuristic to optimize industrial processes without explicit formulations, reducing waste and time significantly in tire manufacturing.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of optimizing industrial processes that lack mathematical formulations of objective functions or constraints.

Method: The authors introduced a surrogate-based optimization method that combines machine learning models with a tailored version of Differential Evolution, integrated with multi-level penalty functions.

Result: When applied to an extrusion process in tire manufacturing, the proposed approach reduced initialization and setup time by 65%, while also considerably decreasing material waste.

Conclusion: The findings demonstrate the potential of data-driven modeling combined with metaheuristic optimization for improving industrial processes without explicit mathematical problem formulations.

Abstract: The optimization of industrial processes remains a critical challenge,
particularly when no mathematical formulation of objective functions or
constraints is available. This study addresses this issue by proposing a
surrogate-based, data-driven methodology for optimizing complex real-world
manufacturing systems using only historical process data. Machine learning
models are employed to approximate system behavior and construct surrogate
models, which are integrated into a tailored metaheuristic approach:
Data-Driven Differential Evolution with Multi-Level Penalty Functions and
Surrogate Models, an adapted version of Differential Evolution suited to the
characteristics of the studied process. The methodology is applied to an
extrusion process in the tire manufacturing industry, with the goal of
optimizing initialization parameters to reduce waste and production time.
Results show that the surrogate-based optimization approach outperforms
historical best configurations, achieving a 65\% reduction in initialization
and setup time, while also significantly minimizing material waste. These
findings highlight the potential of combining data-driven modeling and
metaheuristic optimization for industrial processes where explicit formulations
are unavailable.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [362] [Urban delineation through the lens of commute networks: Leveraging graph embeddings to distinguish socioeconomic groups in cities](https://arxiv.org/abs/2507.11057)
*Devashish Khulbe,Stanislav Sobolevsky*

Main category: cs.SI

TL;DR: The paper proposes using Graph Neural Networks and commute networks from census data to delineate urban regions for better understanding of socioeconomic disparities.


<details>
  <summary>Details</summary>
Motivation: Urban researchers aim to better understand metropolitan dynamics, especially population shifts, and this work explores leveraging commute networks to inform urban delineation.

Method: The authors employed Graph Neural Networks to create node embeddings from commute networks and clustered them to identify urban communities based on spatial cohesion.

Result: Experiments across U.S. cities demonstrated that network embeddings effectively capture socioeconomic disparities, especially in metrics like household income.

Conclusion: Graph Neural Networks provide a robust approach for urban delineation, offering insights into socioeconomic community structures and serving as an alternative to traditional methods.

Abstract: Delineating areas within metropolitan regions stands as an important focus
among urban researchers, shedding light on the urban perimeters shaped by
evolving population dynamics. Applications to urban science are numerous, from
facilitating comparisons between delineated districts and administrative
divisions to informing policymakers of the shifting economic and labor
landscapes. In this study, we propose using commute networks sourced from the
census for the purpose of urban delineation, by modeling them with a Graph
Neural Network (GNN) architecture. We derive low-dimensional representations of
granular urban areas (nodes) using GNNs. Subsequently, nodes' embeddings are
clustered to identify spatially cohesive communities in urban areas. Our
experiments across the U.S. demonstrate the effectiveness of network embeddings
in capturing significant socioeconomic disparities between communities in
various cities, particularly in factors such as median household income. The
role of census mobility data in regional delineation is also noted, and we
establish the utility of GNNs in urban community detection, as a powerful
alternative to existing methods in this domain. The results offer insights into
the wider effects of commute networks and their use in building meaningful
representations of urban regions.

</details>


### [363] [The Shape of Deceit: Behavioral Consistency and Fragility in Money Laundering Patterns](https://arxiv.org/abs/2507.10608)
*Danny Butvinik,Ofir Yakobi,Michal Einhorn Cohen,Elina Maliarsky*

Main category: cs.SI

TL;DR: Traditional AML systems mischaracterize money laundering as anomaly-driven instead of consistent behavioral patterns. This paper leverages network theory and consistency to refine detection methods.


<details>
  <summary>Details</summary>
Motivation: Current AML systems fail to recognize the deliberate nature of laundering hidden in consistent routines rather than anomalies.

Method: The paper introduces a network-theoretic model leveraging subgraph structures focusing on semantic and functional roles. It also explores pattern fragility and reconceptualizes pattern similarity.

Result: Laundering patterns are shown to persist through behavioral essence rather than statistical outliers, paving the way for refined detection systems.

Conclusion: AML systems must shift from anomaly-based detection to focusing on behavioral consistency and semantic robustness for effective financial crime prevention.

Abstract: Conventional anti-money laundering (AML) systems predominantly focus on
identifying anomalous entities or transactions, flagging them for manual
investigation based on statistical deviation or suspicious behavior. This
paradigm, however, misconstrues the true nature of money laundering, which is
rarely anomalous but often deliberate, repeated, and concealed within
consistent behavioral routines. In this paper, we challenge the entity-centric
approach and propose a network-theoretic perspective that emphasizes detecting
predefined laundering patterns across directed transaction networks. We
introduce the notion of behavioral consistency as the core trait of laundering
activity, and argue that such patterns are better captured through subgraph
structures expressing semantic and functional roles - not solely geometry.
Crucially, we explore the concept of pattern fragility: the sensitivity of
laundering patterns to small attribute changes and, conversely, their semantic
robustness even under drastic topological transformations. We claim that
laundering detection should not hinge on statistical outliers, but on
preservation of behavioral essence, and propose a reconceptualization of
pattern similarity grounded in this insight. This philosophical and practical
shift has implications for how AML systems model, scan, and interpret networks
in the fight against financial crime.

</details>


### [364] [Multilayer Artificial Benchmark for Community Detection (mABCD)](https://arxiv.org/abs/2507.10795)
*Łukasz Kraiński,Michał Czuba,Piotr Bródka,Paweł Prałat,Bogumił Kamiński,François Théberge*

Main category: cs.SI

TL;DR: The paper introduces mABCD, a variant of the ABCD model for generating multilayer networks with community structure and power-law distributions.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the need for a model that generates multilayer networks with community structure, power-law characteristics, and faster analytical capabilities compared to existing methods.

Method: The mABCD model uses the foundational elements of the ABCD model, extending them to multilayer networks while preserving power-law distributions for degrees and community sizes.

Result: The mABCD model is faster, more interpretable compared to similar models like LFR, and offers analytical tractability for multilayer networks.

Conclusion: The proposed mABCD model provides an advanced tool for generating and analyzing multilayer networks efficiently while maintaining desirable statistical properties.

Abstract: The Artificial Benchmark for Community Detection (ABCD) model is a random
graph model with community structure and power-law distribution for both
degrees and community sizes. The model generates graphs similar to the
well-known LFR model but it is faster, more interpretable, and can be
investigated analytically. In this paper, we use the underlying ingredients of
the ABCD model and introduce its variant for multilayer networks, mABCD.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [365] [HEIMDALL: a grapH-based sEIsMic Detector And Locator for microseismicity](https://arxiv.org/abs/2507.10850)
*Matteo Bagagli,Francesco Grigoli,Davide Bacciu*

Main category: physics.geo-ph

TL;DR: This paper introduces a deep-learning model using graph neural networks, designed for continuous seismic monitoring and catalog creation, and demonstrates its effectiveness in detecting events in geothermal regions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for accurate and efficient seismic monitoring methods that align with global strategies to exploit green energy, particularly enhanced geothermal systems.

Method: The method combines graph theory and graph neural networks for simultaneous phase picking, association, and event localization within an end-to-end pipeline over rolling time windows.

Result: The model showed improved event detection accuracy with fewer false positives and increased efficiency, identifying seismic sequences that were missed by previous systems.

Conclusion: The proposed approach significantly enhances monitoring capabilities in geothermal regions, offering a reliable tool for operational risk mitigation and better seismic data interpretation during geothermal energy projects.

Abstract: In this work, we present a new deep-learning model for microseismicity
monitoring that utilizes continuous spatiotemporal relationships between
seismic station recordings, forming an end-to-end pipeline for seismic catalog
creation. It employs graph theory and state-of-the-art graph neural network
architectures to perform phase picking, association, and event location
simultaneously over rolling windows, making it suitable for both playback and
near-real-time monitoring. As part of the global strategy to reduce carbon
emissions within the broader context of a green-energy transition, there has
been growing interest in exploiting enhanced geothermal systems. Tested in the
complex geothermal area of Iceland's Hengill region using open-access data from
a temporary experiment, our model was trained and validated using both manually
revised and automatic seismic catalogs. Results showed a significant increase
in event detection compared to previously published automatic systems and
reference catalogs, including a $4 M_w$ seismic sequence in December 2018 and a
single-day sequence in February 2019. Our method reduces false events,
minimizes manual oversight, and decreases the need for extensive tuning of
pipelines or transfer learning of deep-learning models. Overall, it validates a
robust monitoring tool for geothermal seismic regions, complementing existing
systems and enhancing operational risk mitigation during geothermal energy
exploitation.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [366] [Kernel Learning for Mean-Variance Trading Strategies](https://arxiv.org/abs/2507.10701)
*Owen Futter,Nicola Muca Cirone,Blanka Horvath*

Main category: q-fin.TR

TL;DR: This paper introduces a kernel-based framework for dynamic trading strategies optimized under a mean-variance criterion, offering a flexible non-Markovian approach to portfolio problems. It competes with signature-based frameworks and improves performance over classical methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of classical Markovian methods in portfolio optimization by leveraging non-Markovian approaches suited for asset dynamics with temporal dependencies.

Method: The paper uses reproducing kernel Hilbert space (RKHS) to parameterize trading strategies for mean-variance optimization. Feature embeddings can range from randomized signatures to neural networks, retaining closed-form solutions without relying on gradient-based optimization.

Result: The proposed framework and signature-based methods outperform classical approaches in handling temporal dependencies across synthetic and market datasets.

Conclusion: The kernel-based approach offers flexible, non-Markovian modeling for portfolio problems, granting efficient optimization alternatives with competitive performance gains over traditional methods.

Abstract: In this article, we develop a kernel-based framework for constructing
dynamic, pathdependent trading strategies under a mean-variance optimisation
criterion. Building on the theoretical results of (Muca Cirone and Salvi,
2025), we parameterise trading strategies as functions in a reproducing kernel
Hilbert space (RKHS), enabling a flexible and non-Markovian approach to optimal
portfolio problems. We compare this with the signature-based framework of
(Futter, Horvath, Wiese, 2023) and demonstrate that both significantly
outperform classical Markovian methods when the asset dynamics or predictive
signals exhibit temporal dependencies for both synthetic and market-data
examples. Using kernels in this context provides significant modelling
flexibility, as the choice of feature embedding can range from randomised
signatures to the final layers of neural network architectures. Crucially, our
framework retains closed-form solutions and provides an alternative to
gradient-based optimisation.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [367] [Fault-Free Analog Computing with Imperfect Hardware](https://arxiv.org/abs/2507.11134)
*Zhicheng Xu,Jiawei Liu,Sitao Huang,Zefan Li,Shengbo Wang,Bo Wen,Ruibin Mao,Mingrui Jiang,Giacomo Pedretti,Jim Ignowski,Kaibin Huang,Can Li*

Main category: cs.ET

TL;DR: This paper introduces a fault-free matrix representation method for analog in-memory computing using memristors, which enhances precision and reliability despite hardware faults.


<details>
  <summary>Details</summary>
Motivation: To address limitations of analog in-memory computing caused by device failures and variations, which hinder precision and reliability in high-precision or fixed-matrix applications.

Method: The authors propose decomposing target matrices into products of two adjustable sub-matrices, programmed on analog hardware, leveraging mathematical optimization to bypass faults and improve density without redundancy or retraining.

Result: Their system achieved exceptional performance (>99.999% cosine similarity for a DFT matrix despite a 39% device fault rate) and demonstrated significant improvements in bit-error-rate (56-fold reduction), density (196%), and energy efficiency (179%).

Conclusion: This approach removes device yield as a bottleneck in analog computing, improving reliability and performance. It is validated with memristors and adaptable to other emerging memory technologies and substrates.

Abstract: The growing demand for edge computing and AI drives research into analog
in-memory computing using memristors, which overcome data movement bottlenecks
by computing directly within memory. However, device failures and variations
critically limit analog systems' precision and reliability. Existing
fault-tolerance techniques, such as redundancy and retraining, are often
inadequate for high-precision applications or scenarios requiring fixed
matrices and privacy preservation. Here, we introduce and experimentally
demonstrate a fault-free matrix representation where target matrices are
decomposed into products of two adjustable sub-matrices programmed onto analog
hardware. This indirect, adaptive representation enables mathematical
optimization to bypass faulty devices and eliminate differential pairs,
significantly enhancing computational density. Our memristor-based system
achieved >99.999% cosine similarity for a Discrete Fourier Transform matrix
despite 39% device fault rate, a fidelity unattainable with conventional direct
representation, which fails with single device faults (0.01% rate). We
demonstrated 56-fold bit-error-rate reduction in wireless communication and
>196% density with 179% energy efficiency improvements compared to
state-of-the-art techniques. This method, validated on memristors, applies
broadly to emerging memories and non-electrical computing substrates, showing
that device yield is no longer the primary bottleneck in analog computing
hardware.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [368] [Artificial Finance: How AI Thinks About Money](https://arxiv.org/abs/2507.10933)
*Orhan Erdem,Ragavi Pobbathi Ashok*

Main category: econ.GN

TL;DR: The paper investigates how large language models (LLMs) compare to human behavior in financial decision-making across various conditions and national contexts.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand the decision-making patterns of LLMs, comparing their outputs to human response data across 53 nations to uncover insights into their reasoning capabilities and biases.

Method: The authors tested seven LLMs, including models from GPT series, Gemini 2.0 Flash, and DeepSeek R1, using established financial decision-making scenarios and compared their responses with a global dataset of human participants.

Result: LLMs favor risk-neutral decisions aligned with expected values, occasionally show inconsistencies in trade-off evaluations, and demonstrate aggregate responses most similar to Tanzanian participants.

Conclusion: The findings shed light on LLMs' decision-making behavior, highlighting their human-like tendencies and the cultural and training influences on their outputs.

Abstract: In this paper, we explore how large language models (LLMs) approach financial
decision-making by systematically comparing their responses to those of human
participants across the globe. We posed a set of commonly used financial
decision-making questions to seven leading LLMs, including five models from the
GPT series(GPT-4o, GPT-4.5, o1, o3-mini), Gemini 2.0 Flash, and DeepSeek R1. We
then compared their outputs to human responses drawn from a dataset covering 53
nations. Our analysis reveals three main results. First, LLMs generally exhibit
a risk-neutral decision-making pattern, favoring choices aligned with expected
value calculations when faced with lottery-type questions. Second, when
evaluating trade-offs between present and future, LLMs occasionally produce
responses that appear inconsistent with normative reasoning. Third, when we
examine cross-national similarities, we find that the LLMs' aggregate responses
most closely resemble those of participants from Tanzania. These findings
contribute to the understanding of how LLMs emulate human-like decision
behaviors and highlight potential cultural and training influences embedded
within their outputs.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [369] [Comparative Analysis of Vision Transformers and Traditional Deep Learning Approaches for Automated Pneumonia Detection in Chest X-Rays](https://arxiv.org/abs/2507.10589)
*Gaurav Singh*

Main category: eess.IV

TL;DR: This paper compares traditional machine learning techniques and advanced deep learning models, particularly Vision Transformers, for pneumonia detection using chest X-rays, highlighting the superior accuracy and recall of Cross-ViT.


<details>
  <summary>Details</summary>
Motivation: To address the global health challenge of pneumonia diagnosis, especially during crises like COVID-19, by exploring and comparing automated detection methods.

Method: The study uses algorithms ranging from PCA-based clustering to state-of-the-art Vision Transformer architectures, applied on a dataset of 5,856 pediatric chest X-ray images.

Result: Vision Transformers, especially Cross-ViT, achieved the best performance with an accuracy of 88.25% and recall of 99.42%, outperforming CNNs despite having fewer parameters.

Conclusion: Vision Transformers, particularly Cross-ViT, show great promise for automated pneumonia detection, enabling faster and more accurate diagnoses in medical settings.

Abstract: Pneumonia, particularly when induced by diseases like COVID-19, remains a
critical global health challenge requiring rapid and accurate diagnosis. This
study presents a comprehensive comparison of traditional machine learning and
state-of-the-art deep learning approaches for automated pneumonia detection
using chest X-rays (CXRs). We evaluate multiple methodologies, ranging from
conventional machine learning techniques (PCA-based clustering, Logistic
Regression, and Support Vector Classification) to advanced deep learning
architectures including Convolutional Neural Networks (Modified LeNet,
DenseNet-121) and various Vision Transformer (ViT) implementations (Deep-ViT,
Compact Convolutional Transformer, and Cross-ViT). Using a dataset of 5,856
pediatric CXR images, we demonstrate that Vision Transformers, particularly the
Cross-ViT architecture, achieve superior performance with 88.25% accuracy and
99.42% recall, surpassing traditional CNN approaches. Our analysis reveals that
architectural choices impact performance more significantly than model size,
with Cross-ViT's 75M parameters outperforming larger models. The study also
addresses practical considerations including computational efficiency, training
requirements, and the critical balance between precision and recall in medical
diagnostics. Our findings suggest that Vision Transformers offer a promising
direction for automated pneumonia detection, potentially enabling more rapid
and accurate diagnosis during health crises.

</details>


### [370] [Latent Space Consistency for Sparse-View CT Reconstruction](https://arxiv.org/abs/2507.11152)
*Duoyou Chen,Yunqing Chen,Can Zhang,Zhou Wang,Cheng Chen,Ruoxiu Xiao*

Main category: eess.IV

TL;DR: The paper introduces CLS-DM, a novel model that uses contrastive learning to align 2D X-ray and 3D CT data in latent space for better sparse-view CT reconstruction.


<details>
  <summary>Details</summary>
Motivation: Overcoming high costs and risks in CT imaging by improving reconstruction methods using sparse-view X-rays, leveraging potential in latent diffusion models.

Method: The authors propose Consistent Latent Space Diffusion Model (CLS-DM), incorporating cross-modal feature contrastive learning for latent space alignment between 2D X-ray images and 3D CT data.

Result: CLS-DM delivers superior performance compared to classical and state-of-the-art generative models based on voxel-level metrics (PSNR, SSIM) on two datasets: LIDC-IDRI and CTSpine1K.

Conclusion: CLS-DM enhances sparse-view CT reconstruction, is cost-effective, extends to other cross-modal tasks, and its code availability encourages further research in multiple domains.

Abstract: Computed Tomography (CT) is a widely utilized imaging modality in clinical
settings. Using densely acquired rotational X-ray arrays, CT can capture 3D
spatial features. However, it is confronted with challenged such as significant
time consumption and high radiation exposure. CT reconstruction methods based
on sparse-view X-ray images have garnered substantial attention from
researchers as they present a means to mitigate costs and risks. In recent
years, diffusion models, particularly the Latent Diffusion Model (LDM), have
demonstrated promising potential in the domain of 3D CT reconstruction.
Nonetheless, due to the substantial differences between the 2D latent
representation of X-ray modalities and the 3D latent representation of CT
modalities, the vanilla LDM is incapable of achieving effective alignment
within the latent space. To address this issue, we propose the Consistent
Latent Space Diffusion Model (CLS-DM), which incorporates cross-modal feature
contrastive learning to efficiently extract latent 3D information from 2D X-ray
images and achieve latent space alignment between modalities. Experimental
results indicate that CLS-DM outperforms classical and state-of-the-art
generative models in terms of standard voxel-level metrics (PSNR, SSIM) on the
LIDC-IDRI and CTSpine1K datasets. This methodology not only aids in enhancing
the effectiveness and economic viability of sparse X-ray reconstructed CT but
can also be generalized to other cross-modal transformation tasks, such as
text-to-image synthesis. We have made our code publicly available at
https://anonymous.4open.science/r/CLS-DM-50D6/ to facilitate further research
and applications in other domains.

</details>


### [371] [3D Magnetic Inverse Routine for Single-Segment Magnetic Field Images](https://arxiv.org/abs/2507.11293)
*J. Senthilnath,Chen Hao,F. C. Wellstood*

Main category: eess.IV

TL;DR: The paper introduces a novel method called the 3D Magnetic Inverse Routine (3D MIR) for recovering 3D current flow parameters in semiconductor packaging using Magnetic Field Images (MFI).


<details>
  <summary>Details</summary>
Motivation: The motivation behind the paper is to improve the accuracy of non-destructive testing (NDT) in semiconductor packaging by providing a more effective method for localizing circuit defects using 3D information.

Method: The method involves three stages: 1) Using a CNN to process MFI data and predict wire parameters, 2) Applying spatial-physics-based constraints for initial parameter estimates, and 3) Optimizing parameters to minimize discrepancies between reconstructed and actual MFI.

Result: The 3D MIR method demonstrated high precision in recovering 3D information, setting a new benchmark for magnetic image reconstruction in the field.

Conclusion: The study highlights the potential of integrating deep learning and physics-driven optimization to enhance practical applications in semiconductor packaging.

Abstract: In semiconductor packaging, accurately recovering 3D information is crucial
for non-destructive testing (NDT) to localize circuit defects. This paper
presents a novel approach called the 3D Magnetic Inverse Routine (3D MIR),
which leverages Magnetic Field Images (MFI) to retrieve the parameters for the
3D current flow of a single-segment. The 3D MIR integrates a deep learning
(DL)-based Convolutional Neural Network (CNN), spatial-physics-based
constraints, and optimization techniques. The method operates in three stages:
i) The CNN model processes the MFI data to predict ($\ell/z_o$), where $\ell$
is the wire length and $z_o$ is the wire's vertical depth beneath the magnetic
sensors and classify segment type ($c$). ii) By leveraging
spatial-physics-based constraints, the routine provides initial estimates for
the position ($x_o$, $y_o$, $z_o$), length ($\ell$), current ($I$), and current
flow direction (positive or negative) of the current segment. iii) An optimizer
then adjusts these five parameters ($x_o$, $y_o$, $z_o$, $\ell$, $I$) to
minimize the difference between the reconstructed MFI and the actual MFI. The
results demonstrate that the 3D MIR method accurately recovers 3D information
with high precision, setting a new benchmark for magnetic image reconstruction
in semiconductor packaging. This method highlights the potential of combining
DL and physics-driven optimization in practical applications.

</details>


### [372] [HANS-Net: Hyperbolic Convolution and Adaptive Temporal Attention for Accurate and Generalizable Liver and Tumor Segmentation in CT Imaging](https://arxiv.org/abs/2507.11325)
*Arefin Ittesafun Abian,Ripon Kumar Debnath,Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Md Rafiqul Islam,Asif Karim,Reem E. Mohamed,Sami Azam*

Main category: eess.IV

TL;DR: This paper introduces HANS-Net, a comprehensive framework for liver and tumor segmentation on CT images, achieving high Dice scores and generalization on different datasets.


<details>
  <summary>Details</summary>
Motivation: The paper focuses on improving liver and tumor segmentation, a complex task challenged by anatomical variability, limited annotated data, and tumor appearance variability.

Method: The proposed method, HANS-Net, integrates advanced technologies such as hyperbolic convolutions, multi-scale texture learning, synaptic plasticity mechanisms, implicit neural representation for anatomical boundaries, Monte Carlo dropout, and lightweight temporal attention.

Result: HANS-Net yields strong performance metrics such as a Dice score of 93.26% on the LiTS dataset and 87.45% on the 3D-IRCADb-01 dataset, showcasing robustness and effectiveness.

Conclusion: The findings validate that HANS-Net provides accurate, anatomically consistent, and confident segmentation, making it a robust framework for liver and tumor analysis across CT datasets.

Abstract: Accurate liver and tumor segmentation on abdominal CT images is critical for
reliable diagnosis and treatment planning, but remains challenging due to
complex anatomical structures, variability in tumor appearance, and limited
annotated data. To address these issues, we introduce Hyperbolic-convolutions
Adaptive-temporal-attention with Neural-representation and Synaptic-plasticity
Network (HANS-Net), a novel segmentation framework that synergistically
combines hyperbolic convolutions for hierarchical geometric representation, a
wavelet-inspired decomposition module for multi-scale texture learning, a
biologically motivated synaptic plasticity mechanism for adaptive feature
enhancement, and an implicit neural representation branch to model fine-grained
and continuous anatomical boundaries. Additionally, we incorporate
uncertainty-aware Monte Carlo dropout to quantify prediction confidence and
lightweight temporal attention to improve inter-slice consistency without
sacrificing efficiency. Extensive evaluations of the LiTS dataset demonstrate
that HANS-Net achieves a mean Dice score of 93.26%, an IoU of 88.09%, an
average symmetric surface distance (ASSD) of 0.72 mm, and a volume overlap
error (VOE) of 11.91%. Furthermore, cross-dataset validation on the
3D-IRCADb-01 dataset obtains an average Dice of 87.45%, IoU of 80.30%, ASSD of
1.525 mm, and VOE of 19.71%, indicating strong generalization across different
datasets. These results confirm the effectiveness and robustness of HANS-Net in
providing anatomically consistent, accurate, and confident liver and tumor
segmentation.

</details>


### [373] [Focus on Texture: Rethinking Pre-training in Masked Autoencoders for Medical Image Classification](https://arxiv.org/abs/2507.10869)
*Chetan Madan,Aarjav Satia,Soumen Basu,Pankaj Gupta,Usha Dutta,Chetan Arora*

Main category: eess.IV

TL;DR: The paper introduces a novel pre-training framework called GLCM-MAE for medical imaging, addressing limitations of existing Masked Autoencoders (MAEs) in capturing texture-based features.


<details>
  <summary>Details</summary>
Motivation: Existing MAEs, optimized using pixel-wise mean squared error (MSE) loss, fail to preserve vital texture features in medical imaging that are crucial for abnormality classification.

Method: GLCM-MAE incorporates reconstruction loss based on matching the Gray Level Co-occurrence Matrix (GLCM), which captures intensity and spatial relationships in images, and formulates it into a differentiable loss for effective pre-training.

Result: GLCM-MAE achieves improved performance in four medical tasks—gallbladder cancer detection (+2.1%), breast cancer detection (+3.1%), pneumonia detection (+0.5%), and COVID detection (+0.6%)—exceeding state-of-the-art results.

Conclusion: The proposed GLCM-MAE framework demonstrates the importance of preserving morphological features through texture-based losses, enhancing representation quality in medical imaging tasks.

Abstract: Masked Autoencoders (MAEs) have emerged as a dominant strategy for
self-supervised representation learning in natural images, where models are
pre-trained to reconstruct masked patches with a pixel-wise mean squared error
(MSE) between original and reconstructed RGB values as the loss. We observe
that MSE encourages blurred image re-construction, but still works for natural
images as it preserves dominant edges. However, in medical imaging, when the
texture cues are more important for classification of a visual abnormality, the
strategy fails. Taking inspiration from Gray Level Co-occurrence Matrix (GLCM)
feature in Radiomics studies, we propose a novel MAE based pre-training
framework, GLCM-MAE, using reconstruction loss based on matching GLCM. GLCM
captures intensity and spatial relationships in an image, hence proposed loss
helps preserve morphological features. Further, we propose a novel formulation
to convert matching GLCM matrices into a differentiable loss function. We
demonstrate that unsupervised pre-training on medical images with the proposed
GLCM loss improves representations for downstream tasks. GLCM-MAE outperforms
the current state-of-the-art across four tasks - gallbladder cancer detection
from ultrasound images by 2.1%, breast cancer detection from ultrasound by
3.1%, pneumonia detection from x-rays by 0.5%, and COVID detection from CT by
0.6%. Source code and pre-trained models are available at:
https://github.com/ChetanMadan/GLCM-MAE.

</details>


### [374] [U-RWKV: Lightweight medical image segmentation with direction-adaptive RWKV](https://arxiv.org/abs/2507.11415)
*Hongbo Ye,Fenghe Tang,Peiang Zhao,Zhen Huang,Dexin Zhao,Minghao Bian,S. Kevin Zhou*

Main category: eess.IV

TL;DR: The U-RWKV framework is introduced to enhance medical image segmentation with efficient long-range dependency modeling, showing state-of-the-art results with low computational cost.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a high-performance yet computationally lightweight solution for medical image segmentation, addressing challenges in resource-limited settings where current methods struggle due to limited global Effective Receptive Fields (ERFs).

Method: The paper introduces U-RWKV, leveraging the Recurrent Weighted Key-Value (RWKV) architecture with innovations like the Direction-Adaptive RWKV Module (DARM) to mitigate directional bias and the Stage-Adaptive Squeeze-and-Excitation (SASE) Module to adapt feature extraction across different stages.

Result: The proposed U-RWKV achieves state-of-the-art segmentation performance while maintaining high computational efficiency, making it suitable for resource-constrained environments.

Conclusion: U-RWKV provides a practical and advanced solution for equitable medical imaging access globally by balancing high performance with computational efficiency.

Abstract: Achieving equity in healthcare accessibility requires lightweight yet
high-performance solutions for medical image segmentation, particularly in
resource-limited settings. Existing methods like U-Net and its variants often
suffer from limited global Effective Receptive Fields (ERFs), hindering their
ability to capture long-range dependencies. To address this, we propose U-RWKV,
a novel framework leveraging the Recurrent Weighted Key-Value(RWKV)
architecture, which achieves efficient long-range modeling at O(N)
computational cost. The framework introduces two key innovations: the
Direction-Adaptive RWKV Module(DARM) and the Stage-Adaptive
Squeeze-and-Excitation Module(SASE). DARM employs Dual-RWKV and QuadScan
mechanisms to aggregate contextual cues across images, mitigating directional
bias while preserving global context and maintaining high computational
efficiency. SASE dynamically adapts its architecture to different feature
extraction stages, balancing high-resolution detail preservation and semantic
relationship capture. Experiments demonstrate that U-RWKV achieves
state-of-the-art segmentation performance with high computational efficiency,
offering a practical solution for democratizing advanced medical imaging
technologies in resource-constrained environments. The code is available at
https://github.com/hbyecoding/U-RWKV.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [375] [Recent Advances in Simulation-based Inference for Gravitational Wave Data Analysis](https://arxiv.org/abs/2507.11192)
*Bo Liang,He Wang*

Main category: gr-qc

TL;DR: The paper reviews simulation-based inference methods, such as neural posterior estimation and normalizing flows, in the context of gravitational wave astronomy, highlighting speed improvements over traditional Bayesian methods while discussing challenges like model dependency and sensitivity to prior assumptions.


<details>
  <summary>Details</summary>
Motivation: The field of gravitational wave astronomy faces computational challenges with traditional Bayesian methods due to high-dimensional data and complex noise characteristics.

Method: The authors examine simulation-based inference methods leveraging machine learning techniques like normalizing flows, neural posterior estimation, and others.

Result: These methods show promise in offering speed improvements and similar accuracy compared to traditional approaches but suffer from model dependency and sensitivity to priors.

Conclusion: Simulation-based methods can complement traditional Bayesian methods in gravitational wave analysis, but further validation across diverse scenarios is needed for adoption.

Abstract: The detection of gravitational waves by the LIGO-Virgo-KAGRA collaboration
has ushered in a new era of observational astronomy, emphasizing the need for
rapid and detailed parameter estimation and population-level analyses.
Traditional Bayesian inference methods, particularly Markov chain Monte Carlo,
face significant computational challenges when dealing with the
high-dimensional parameter spaces and complex noise characteristics inherent in
gravitational wave data. This review examines the emerging role of
simulation-based inference methods in gravitational wave astronomy, with a
focus on approaches that leverage machine-learning techniques such as
normalizing flows and neural posterior estimation. We provide a comprehensive
overview of the theoretical foundations underlying various simulation-based
inference methods, including neural posterior estimation, neural ratio
estimation, neural likelihood estimation, flow matching, and consistency
models. We explore the applications of these methods across diverse
gravitational wave data processing scenarios, from single-source parameter
estimation and overlapping signal analysis to testing general relativity and
conducting population studies. Although these techniques demonstrate speed
improvements over traditional methods in controlled studies, their
model-dependent nature and sensitivity to prior assumptions are barriers to
their widespread adoption. Their accuracy, which is similar to that of
conventional methods, requires further validation across broader parameter
spaces and noise conditions.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [376] [NLP Meets the World: Toward Improving Conversations With the Public About Natural Language Processing Research](https://arxiv.org/abs/2507.10559)
*Shomir Wilson*

Main category: cs.CY

TL;DR: This paper offers recommendations for researchers on communicating LLMs' capabilities and limitations to the public.


<details>
  <summary>Details</summary>
Motivation: The paper aims to bridge the gap between technical NLP research and public understanding by promoting effective communication amid growing public interest in LLMs.

Method: The authors offer recommendations addressing vague terminology, unreasonable expectations, and ethical failures, supported by examples from NLP research and news coverage.

Result: The paper identifies specific communication challenges and proposes strategies to engage the public effectively while minimizing misunderstandings.

Conclusion: Transparent and clear communication can strengthen public understanding of NLP advancements and foster sustainable support for further research in the field.

Abstract: Recent developments in large language models (LLMs) have been accompanied by
rapidly growing public interest in natural language processing (NLP). This
attention is reflected by major news venues, which sometimes invite NLP
researchers to share their knowledge and views with a wide audience.
Recognizing the opportunities of the present, for both the research field and
for individual researchers, this paper shares recommendations for communicating
with a general audience about LLMs' capabilities and limitations. These
recommendations cover three themes: vague terminology as an obstacle to public
understanding, unreasonable expectations as obstacles to sustainable growth,
and ethical failures as obstacles to continued support. Published NLP research
and popular news coverage are cited to illustrate these themes with examples.
The recommendations promote effective, transparent communication with the
general public about NLP, in order to strengthen public understanding and
encourage support for research.

</details>


### [377] [Can Large Language Models Understand As Well As Apply Patent Regulations to Pass a Hands-On Patent Attorney Test?](https://arxiv.org/abs/2507.10576)
*Bhakti Khera,Rezvan Alamian,Pascal A. Scherz,Stephan M. Goetz*

Main category: cs.CY

TL;DR: This study evaluates various Large Language Models (LLMs) on the European Qualifying Examination (EQE) for patent attorneys but finds none achieved professional-level performance. Future directions include addressing logical consistency, multimodality, and adaptive prompting.


<details>
  <summary>Details</summary>
Motivation: To explore the quantitative performance and limitations of LLMs in legal applications and identify how they can be improved to meet professional legal standards.

Method: The researchers assessed multiple LLMs, including open-source and proprietary models like GPT-series and Llama, on EQE tasks. They analyzed their accuracy, F1-score, navigation of text and graphics, and coherence observed by human experts.

Result: OpenAI's models had the highest accuracy, with GPT-4o excelling in text-graphic integration. AWS Llama and Python-deployed Llama significantly underperformed. Expert evaluation highlighted flaws like sensitivity to prompt changes, underscoring misalignment between automatic metrics and human judgment.

Conclusion: While LLMs like GPT-4o exhibit strong capabilities in certain areas, they fall short of professional-level legal standards. The study outlines specific areas of improvement, such as logical reasoning and multimodal integration, to advance these models toward practical, expert-level use.

Abstract: The legal field already uses various large language models (LLMs) in actual
applications, but their quantitative performance and reasons for it are
underexplored. We evaluated several open-source and proprietary LLMs --
including GPT-series, Anthropic, Deepseek and Llama-3, variants -- on parts of
the European Qualifying Examination (EQE) for future European Patent Attorneys.
OpenAI o1 led with 0.82 accuracy and 0.81 F1 score, whereas (Amazon Web
Services) AWS Llama 3.1 8B lagged at 0.50 accuracy, and a Python-deployed Llama
3.1 8B scored 0.55. The latter two are within the range of mere guessing for
the two-answer forced-choice design. None of the evaluated models could have
passed the examination fully, as accuracy never exceeded the average threshold
of 0.90 required for professional-level standards -- also not models that are
regularly promoted for their assumed beyond-PhD- and bar-admitted-lawyer-level
performance. GPT-4o excelled at integrating text and graphics, while Claude 3
Opus often lost formatting coherence. Human patent experts evaluated the
textual justifications and uncovered various critical shortcomings of each
model. They valued clarity and legal rationale over the raw correctness of the
answers, which revealed misalignment between automatic metrics and expert
judgment. Model outputs were sensitive to modest temperature changes and prompt
wording, which underscores the remaining necessity of expert oversight. Future
work should target logical consistency, robust multimodality, and adaptive
prompting to approach human-level patent proficiency. In summary, despite the
outstanding performance of recent large models, the general public might
overestimate their performance. The field has a long way to go to develop a
virtual patent attorney. This paper wants to point out several specific
limitations that need solutions.

</details>


### [378] [Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors](https://arxiv.org/abs/2507.10579)
*Ekaterina Kochmar,Kaushal Kumar Maurya,Kseniia Petukhova,KV Aditya Srivatsa,Anaïs Tack,Justin Vasselli*

Main category: cs.CY

TL;DR: This paper evaluates AI tutors powered by LLMs on their ability to address student mistakes in educational dialogues, using a shared task format with five tracks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to assess and improve pedagogical capabilities of AI tutors, ensuring they provide effective guidance and actionable feedback in educational contexts.

Method: A shared task involving five assessment tracks was performed to evaluate AI tutors on mistake identification, localization, guidance, feedback actionability, and tutor classification.

Result: The models achieved macro F1 scores ranging from 58.34% to 71.81% in pedagogical tasks and 96.98% in tutor identification, with promising but improvable results.

Conclusion: The findings and publicly available resources highlight advancements in AI tutors but point to substantial room for further development to enhance educational outcomes.

Abstract: This shared task has aimed to assess pedagogical abilities of AI tutors
powered by large language models (LLMs), focusing on evaluating the quality of
tutor responses aimed at student's mistake remediation within educational
dialogues. The task consisted of five tracks designed to automatically evaluate
the AI tutor's performance across key dimensions of mistake identification,
precise location of the mistake, providing guidance, and feedback
actionability, grounded in learning science principles that define good and
effective tutor responses, as well as the track focusing on detection of the
tutor identity. The task attracted over 50 international teams across all
tracks. The submitted models were evaluated against gold-standard human
annotations, and the results, while promising, show that there is still
significant room for improvement in this domain: the best results for the four
pedagogical ability assessment tracks range between macro F1 scores of 58.34
(for providing guidance) and 71.81 (for mistake identification) on three-class
problems, with the best F1 score in the tutor identification track reaching
96.98 on a 9-class task. In this paper, we overview the main findings of the
shared task, discuss the approaches taken by the teams, and analyze their
performance. All resources associated with this task are made publicly
available to support future research in this critical domain.

</details>


### [379] [Exploring User Security and Privacy Attitudes and Concerns Toward the Use of General-Purpose LLM Chatbots for Mental Health](https://arxiv.org/abs/2507.10695)
*Jabari Kwesi,Jiaxun Cao,Riya Manchanda,Pardis Emami-Naeini*

Main category: cs.CY

TL;DR: Users rely on LLM-enabled conversational agents for emotional support but lack awareness of privacy risks and regulatory protections. Misconceptions about empathy and accountability in chatbots were identified.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore privacy and security concerns, as well as misconceptions, regarding the use of LLM-enabled chatbots for emotional support, as users increasingly turn to these tools.

Method: Conducted 21 semi-structured interviews with U.S. participants to investigate their understanding and attitudes towards privacy, security, and regulations surrounding LLM-enabled chatbots.

Result: Identified misconceptions, such as users equating LLM empathy with human accountability, and a lack of awareness of regulations. Introduced the concept of 'intangible vulnerability,' highlighting undervaluation of psychological disclosures.

Conclusion: Existing user misconceptions and low risk awareness require the implementation of better safeguards to protect privacy in emotional interactions with general-purpose LLM-enabled chatbots.

Abstract: Individuals are increasingly relying on large language model (LLM)-enabled
conversational agents for emotional support. While prior research has examined
privacy and security issues in chatbots specifically designed for mental health
purposes, these chatbots are overwhelmingly "rule-based" offerings that do not
leverage generative AI. Little empirical research currently measures users'
privacy and security concerns, attitudes, and expectations when using
general-purpose LLM-enabled chatbots to manage and improve mental health.
Through 21 semi-structured interviews with U.S. participants, we identified
critical misconceptions and a general lack of risk awareness. Participants
conflated the human-like empathy exhibited by LLMs with human-like
accountability and mistakenly believed that their interactions with these
chatbots were safeguarded by the same regulations (e.g., HIPAA) as disclosures
with a licensed therapist. We introduce the concept of "intangible
vulnerability," where emotional or psychological disclosures are undervalued
compared to more tangible forms of information (e.g., financial or
location-based data). To address this, we propose recommendations to safeguard
user mental health disclosures with general-purpose LLM-enabled chatbots more
effectively.

</details>


### [380] ["Is it always watching? Is it always listening?" Exploring Contextual Privacy and Security Concerns Toward Domestic Social Robots](https://arxiv.org/abs/2507.10786)
*Henry Bell,Jabari Kwesi,Hiba Laabadli,Pardis Emami-Naeini*

Main category: cs.CY

TL;DR: Social robots are advancing in AI and sensing but pose major privacy and security risks, requiring user-centered design approaches.


<details>
  <summary>Details</summary>
Motivation: Identify security and privacy concerns associated with social robots as they emerge in the U.S. market.

Method: Conducted 19 semi-structured interviews to collect user feedback on social robots' implications.

Result: Key concerns surfaced: transparency, usability, robust privacy controls, misinformation in education, reliability in medical fields, and data inference.

Conclusion: User expectations include tangible privacy controls, clear data collection indicators, and context-specific functions for safer adoption of social robots.

Abstract: Equipped with artificial intelligence (AI) and advanced sensing capabilities,
social robots are gaining interest among consumers in the United States. These
robots seem like a natural evolution of traditional smart home devices.
However, their extensive data collection capabilities, anthropomorphic
features, and capacity to interact with their environment make social robots a
more significant security and privacy threat. Increased risks include data
linkage, unauthorized data sharing, and the physical safety of users and their
homes. It is critical to investigate U.S. users' security and privacy needs and
concerns to guide the design of social robots while these devices are still in
the early stages of commercialization in the U.S. market. Through 19
semi-structured interviews, we identified significant security and privacy
concerns, highlighting the need for transparency, usability, and robust privacy
controls to support adoption. For educational applications, participants
worried most about misinformation, and in medical use cases, they worried about
the reliability of these devices. Participants were also concerned with the
data inference that social robots could enable. We found that participants
expect tangible privacy controls, indicators of data collection, and
context-appropriate functionality.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [381] [MultiVox: Benchmarking Voice Assistants for Multimodal Interactions](https://arxiv.org/abs/2507.10859)
*Ramaneswaran Selvakumar,Ashish Seth,Nishit Anand,Utkarsh Tyagi,Sonal Kumar,Sreyan Ghosh,Dinesh Manocha*

Main category: cs.MM

TL;DR: Proposes MultiVox, a benchmark assessing Large Language Models' ability to integrate speech and visual data for multimodal understanding, revealing their limitations compared to humans.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks inadequately evaluate voice assistants on understanding fine-grained speech and visual cues in multimodal contexts.

Method: Introduced MultiVox, a dataset of 1000 speech dialogues with diverse paralinguistic and visual data, and evaluated 9 models against it.

Result: Models are significantly less capable than humans in producing context-aware responses using multimodal inputs.

Conclusion: While humans excel at integrating multimodal cues for interaction, current state-of-the-art models lack efficacy in this area, highlighting room for improvement.

Abstract: The rapid progress of Large Language Models (LLMs) has empowered omni models
to act as voice assistants capable of understanding spoken dialogues. These
models can process multimodal inputs beyond text, such as speech and visual
data, enabling more context-aware interactions. However, current benchmarks
fall short in comprehensively evaluating how well these models generate
context-aware responses, particularly when it comes to implicitly understanding
fine-grained speech characteristics, such as pitch, emotion, timbre, and volume
or the environmental acoustic context such as background sounds. Additionally,
they inadequately assess the ability of models to align paralinguistic cues
with complementary visual signals to inform their responses. To address these
gaps, we introduce MultiVox, the first omni voice assistant benchmark designed
to evaluate the ability of voice assistants to integrate spoken and visual cues
including paralinguistic speech features for truly multimodal understanding.
Specifically, MultiVox includes 1000 human-annotated and recorded speech
dialogues that encompass diverse paralinguistic features and a range of visual
cues such as images and videos. Our evaluation on 9 state-of-the-art models
reveals that, although humans excel at these tasks, current models consistently
struggle to produce contextually grounded responses.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [382] [Arcturus: A Cloud Overlay Network for Global Accelerator with Enhanced Performance and Stability](https://arxiv.org/abs/2507.10928)
*Matthew Yang Liu,Chuang Chen,Pengcheng Lv,Hui Guo,Yanan Zhang,Cong Wang,Yusen Li,Zhenyu Li,Yu-Chu Tian*

Main category: cs.NI

TL;DR: The paper presents Arcturus, a multi-provider and cost-efficient Global Accelerator framework that improves performance and reduces costs for real-time interactive applications.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current Global Accelerator services, which are tied to specific cloud providers and suffer from high costs and rigidity, making them unsuitable for large-scale or budget-sensitive deployments.

Method: Introduces Arcturus, a framework with a two-plane design: a forwarding plane for adaptive proxy network construction and a scheduling plane for quantitative optimization of load and routing.

Result: Arcturus achieves up to 1.7X better acceleration performance, reduces costs by 71%, and maintains over 80% resource efficiency under millions of RPS compared to commercial alternatives.

Conclusion: Arcturus demonstrates the viability of leveraging heterogeneous cloud resources from multiple providers to significantly enhance cost-efficiency, performance, and scalability of Global Accelerator services.

Abstract: Global Accelerator (GA) services play a vital role in ensuring low-latency,
high-reliability communication for real-time interactive applications. However,
existing GA offerings are tightly bound to specific cloud providers, resulting
in high costs, rigid deployment, and limited flexibility, especially for
large-scale or budget-sensitive deployments. Arcturus is a cloud-native GA
framework that revisits the design of GA systems by leveraging low-cost,
heterogeneous cloud resources across multiple providers. Rather than relying on
fixed, high-end infrastructure, Arcturus dynamically constructs its
acceleration network and balances performance, stability, and resource
efficiency. To achieve this, Arcturus introduces a two-plane design: a
forwarding plane that builds a proxy network with adaptive control, and a
scheduling plane that coordinates load and routing through lightweight,
quantitative optimization. Evaluations under millions of RPS show that Arcturus
outperforms commercial GA services by up to 1.7X in acceleration performance,
reduces cost by 71%, and maintains over 80% resource efficiency--demonstrating
efficient use of cloud resources at scale.

</details>


### [383] [LiLM-RDB-SFC: Lightweight Language Model with Relational Database-Guided DRL for Optimized SFC Provisioning](https://arxiv.org/abs/2507.10903)
*Parisa Fard Moshiri,Xinyu Zhu,Poonam Lohan,Burak Kantarci,Emil Janulewicz*

Main category: cs.NI

TL;DR: The paper proposes LiLM-RDB-SFC, a method integrating lightweight language models and relational databases to optimize DRL-based service function chain provisioning, showing superior performance in processing time and accuracy.


<details>
  <summary>Details</summary>
Motivation: Modern SDN and NFV systems face challenges in managing SFCs and VNFs efficiently, especially under dynamic and unpredictable network conditions. DRL approaches face limitations due to their reliance on structured data and fixed action rules.

Method: The approach leverages lightweight language models (BART and FLAN-T5) combined with relational databases to interpret network states and support dynamic querying, enhancing decision-making in DRL systems.

Result: FLAN-T5 showed better results than BART in terms of test loss, accuracy, and processing time. Additionally, FLAN-T5 matched SQLCoder's accuracy while drastically reducing processing time by 96%.

Conclusion: LiLM-RDB-SFC successfully addresses limitations of traditional DRL methods in dynamic network environments, enabling efficient and adaptable SFC provisioning with reduced computational overhead.

Abstract: Effective management of Service Function Chains (SFCs) and optimal Virtual
Network Function (VNF) placement are critical challenges in modern
Software-Defined Networking (SDN) and Network Function Virtualization (NFV)
environments. Although Deep Reinforcement Learning (DRL) is widely adopted for
dynamic network decision-making, its inherent dependency on structured data and
fixed action rules often limits adaptability and responsiveness, particularly
under unpredictable network conditions. This paper introduces LiLM-RDB-SFC, a
novel approach combining Lightweight Language Model (LiLM) with Relational
Database (RDB) to answer network state queries to guide DRL model for efficient
SFC provisioning. Our proposed approach leverages two LiLMs, Bidirectional and
Auto-Regressive Transformers (BART) and the Fine-tuned Language Net T5
(FLAN-T5), to interpret network data and support diverse query types related to
SFC demands, data center resources, and VNF availability. Results demonstrate
that FLAN-T5 outperforms BART with a lower test loss (0.00161 compared to
0.00734), higher accuracy (94.79% compared to 80.2%), and less processing time
(2h 2min compared to 2h 38min). Moreover, when compared to the large language
model SQLCoder, FLAN-T5 matches the accuracy of SQLCoder while cutting
processing time by 96% (SQLCoder: 54 h 43 min; FLAN-T5: 2 h 2 min).

</details>


### [384] [Improving Wi-Fi Network Performance Prediction with Deep Learning Models](https://arxiv.org/abs/2507.11168)
*Gabriele Formis,Amanda Ericson,Stefan Forsstrom,Kyi Thar,Gianluca Cena,Stefano Scanzio*

Main category: cs.NI

TL;DR: This study uses machine learning to predict Wi-Fi channel quality, focusing on frame delivery ratio, and evaluates different models for effectiveness and efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance robustness, reliability, and deterministic performance in wireless networks for industrial and mission-critical applications.

Method: Applied convolutional neural networks (CNNs) and long short-term memory (LSTM) methods on real Wi-Fi datasets to predict frame delivery ratio and compared them on accuracy and computational efficiency.

Result: Both CNNs and LSTMs can predict frame delivery ratio reliably, with CNNs being more resource-efficient in terms of CPU and memory usage.

Conclusion: CNNs are suitable for industrial applications where computational resources are limited, despite being slightly less accurate than other models.

Abstract: The increasing need for robustness, reliability, and determinism in wireless
networks for industrial and mission-critical applications is the driver for the
growth of new innovative methods. The study presented in this work makes use of
machine learning techniques to predict channel quality in a Wi-Fi network in
terms of the frame delivery ratio. Predictions can be used proactively to
adjust communication parameters at runtime and optimize network operations for
industrial applications. Methods including convolutional neural networks and
long short-term memory were analyzed on datasets acquired from a real Wi-Fi
setup across multiple channels. The models were compared in terms of prediction
accuracy and computational complexity. Results show that the frame delivery
ratio can be reliably predicted, and convolutional neural networks, although
slightly less effective than other models, are more efficient in terms of CPU
usage and memory consumption. This enhances the model's usability on embedded
and industrial systems.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [385] [Theory of Mind and Self-Disclosure to CUIs](https://arxiv.org/abs/2507.10773)
*Samuel Rhys Cox*

Main category: cs.HC

TL;DR: This paper explores how conversational user interfaces (CUIs) can encourage self-disclosure by incorporating social cues and making their reasoning more transparent.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of self-disclosure, which is essential for emotional well-being but often hindered by perceived reactions from others.

Method: The study investigates the role of uncertainty expressions and reasoning representations in CUIs to promote self-disclosure by enhancing transparency.

Result: The paper suggests that CUIs incorporating social cues like uncertainty expression and reasoning representation could foster greater user self-disclosure.

Conclusion: Transparent communication techniques in CUIs have the potential to bridge emotional barriers, enabling users to disclose more comfortably.

Abstract: Self-disclosure is important to help us feel better, yet is often difficult.
This difficulty can arise from how we think people are going to react to our
self-disclosure. In this workshop paper, we briefly discuss self-disclosure to
conversational user interfaces (CUIs) in relation to various social cues. We
then, discuss how expressions of uncertainty or representation of a CUI's
reasoning could help encourage self-disclosure, by making a CUI's intended
"theory of mind" more transparent to users.

</details>


### [386] [React to This (RTT): A Nonverbal Turing Test for Embodied AI](https://arxiv.org/abs/2507.10812)
*Chuxuan Zhang,Yasaman Etesam,Angelica Lim*

Main category: cs.HC

TL;DR: The paper introduces the React to This (RTT) test, assessing embodied AI agents' reaction to interactions, focusing beyond verbal communication.


<details>
  <summary>Details</summary>
Motivation: To push the boundaries of embodied AI by evaluating their interaction awareness and reaction believability in challenging and nonverbal human-driven scenarios.

Method: Developed and conducted the React to This (RTT) test, inspired by the Total Turing Test, concentrating on nonverbal behaviors and reactions.

Result: Preliminary results were obtained from an initial experiment utilizing the proposed RTT test, showcasing the potential for assessing AI reactivity.

Conclusion: The RTT test serves as a framework to explore AI's capability in nonverbal interaction, with initial findings showing promise for further study.

Abstract: We propose an approach to test embodied AI agents for interaction awareness
and believability, particularly in scenarios where humans push them to their
limits. Turing introduced the Imitation Game as a way to explore the question:
"Can machines think?" The Total Turing Test later expanded this concept beyond
purely verbal communication, incorporating perceptual and physical interaction.
Building on this, we propose a new guiding question: "Can machines react?" and
introduce the React to This (RTT) test for nonverbal behaviors, presenting
results from an initial experiment.

</details>


### [387] [Role-Playing LLM-Based Multi-Agent Support Framework for Detecting and Addressing Family Communication Bias](https://arxiv.org/abs/2507.11210)
*Rushia Harada,Yuken Kimura,Keito Inoshita*

Main category: cs.HC

TL;DR: The study introduces a framework using a Large Language Model (LLM) to enhance emotionally safe communication within families by analyzing dialogues for biases and suppressed emotions and offering constructive feedback.


<details>
  <summary>Details</summary>
Motivation: To address unconscious parental biases and their impact on children's emotional expression, which are challenging to detect and resolve in familial settings.

Method: A Japanese parent-child dialogue corpus was built with metadata annotations. An LLM-based multi-agent framework was developed to analyze dialogues, detect suppressed emotions, infer contextual traits, and provide structured empathetic feedback.

Result: The system showed moderate accuracy in detecting suppressed emotion and generated empathetic, actionable feedback. Follow-up dialogues demonstrated improved emotional expression and understanding within the family.

Conclusion: The framework holds potential for fostering positive transformation in family communication by addressing psychological dynamics and promoting healthier dialogue interactions.

Abstract: Well-being in family settings involves subtle psychological dynamics that
conventional metrics often overlook. In particular, unconscious parental
expectations, termed ideal parent bias, can suppress children's emotional
expression and autonomy. This suppression, referred to as suppressed emotion,
often stems from well-meaning but value-driven communication, which is
difficult to detect or address from outside the family. Focusing on these
latent dynamics, this study explores Large Language Model (LLM)-based support
for psychologically safe family communication. We constructed a Japanese
parent-child dialogue corpus of 30 scenarios, each annotated with metadata on
ideal parent bias and suppressed emotion. Based on this corpus, we developed a
Role-Playing LLM-based multi-agent dialogue support framework that analyzes
dialogue and generates feedback. Specialized agents detect suppressed emotion,
describe implicit ideal parent bias in parental speech, and infer contextual
attributes such as the child's age and background. A meta-agent compiles these
outputs into a structured report, which is then passed to five selected expert
agents. These agents collaboratively generate empathetic and actionable
feedback through a structured four-step discussion process. Experiments show
that the system can detect categories of suppressed emotion with moderate
accuracy and produce feedback rated highly in empathy and practicality.
Moreover, simulated follow-up dialogues incorporating this feedback exhibited
signs of improved emotional expression and mutual understanding, suggesting the
framework's potential in supporting positive transformation in family
interactions.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [388] [BioScore: A Foundational Scoring Function For Diverse Biomolecular Complexes](https://arxiv.org/abs/2507.10877)
*Yuchen Zhu,Jihong Chen,Yitong Li,Xiaomin Fang,Xianbin Ye,Jingzhou He,Xujun Zhang,Jingxuan Ge,Chao Shen,Xiaonan Zhang,Tingjun Hou,Chang-Yu Hsieh*

Main category: physics.chem-ph

TL;DR: The paper introduces BioScore, a scoring function based on a dual-scale geometric graph learning framework, excelling in structural assessment tasks across diverse biomolecular systems.


<details>
  <summary>Details</summary>
Motivation: Existing structure-based scoring functions often fail to generalize across diverse biomolecular systems, limiting progress in understanding biological functions and drug discovery.

Method: BioScore utilizes a dual-scale geometric graph learning framework with specialized modules for tasks like structure assessment and affinity prediction, and it is evaluated on 16 benchmarks across biomolecular types.

Result: BioScore outperforms or matches 70 traditional and deep learning methods, showing gains of up to 90% in antigen-antibody binding correlation, 71% in zero-shot prediction, and 60% in cyclic peptide affinity prediction.

Conclusion: BioScore provides a robust and generalizable framework for scoring biomolecular structures, effectively addressing data sparsity and cross-system generalization challenges.

Abstract: Structural assessment of biomolecular complexes is vital for translating
molecular models into functional insights, shaping our understanding of biology
and aiding drug discovery. However, current structure-based scoring functions
often lack generalizability across diverse biomolecular systems. We present
BioScore, a foundational scoring function that addresses key challenges -- data
sparsity, cross-system representation, and task compatibility -- through a
dual-scale geometric graph learning framework with tailored modules for
structure assessment and affinity prediction. BioScore supports a wide range of
tasks, including affinity prediction, conformation ranking, and structure-based
virtual screening. Evaluated on 16 benchmarks spanning proteins, nucleic acids,
small molecules, and carbohydrates, BioScore consistently outperforms or
matches 70 traditional and deep learning methods. Our newly proposed PPI
Benchmark further enables comprehensive evaluation of protein-protein complex
scoring. BioScore demonstrates broad applicability: (1) pretraining on
mixed-structure data boosts protein-protein affinity prediction by up to 40%
and antigen-antibody binding correlation by over 90%; (2) cross-system
generalizability enables zero- and few-shot prediction with up to 71%
correlation gain; and (3) its unified representation captures chemically
challenging systems such as cyclic peptides, improving affinity prediction by
over 60%. BioScore establishes a robust and generalizable framework for
structural assessment across complex biomolecular landscapes.

</details>
