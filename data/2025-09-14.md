<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.CL](#cs.CL) [Total: 37]
- [cs.CV](#cs.CV) [Total: 78]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.LG](#cs.LG) [Total: 53]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 19]
- [cs.SE](#cs.SE) [Total: 13]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [stat.ML](#stat.ML) [Total: 3]
- [cs.SD](#cs.SD) [Total: 4]
- [cs.DB](#cs.DB) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 7]
- [eess.SP](#eess.SP) [Total: 4]
- [math.OC](#math.OC) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 2]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.IR](#cs.IR) [Total: 4]
- [eess.IV](#eess.IV) [Total: 4]
- [quant-ph](#quant-ph) [Total: 2]
- [math.NA](#math.NA) [Total: 2]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.CR](#cs.CR) [Total: 7]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [An Interval Type-2 Version of Bayes Theorem Derived from Interval Probability Range Estimates Provided by Subject Matter Experts](https://arxiv.org/abs/2509.08834)
*John T. Rickard,William A. Dembski,James Rickards*

Main category: cs.AI

TL;DR: The paper extends Bayesian inference to handle interval type-2 fuzzy inputs using novel methods for consistent interpretation and encoding.


<details>
  <summary>Details</summary>
Motivation: Standard Bayesian inference assumes precise input values, which is unrealistic in many real-world scenarios where experts often provide interval estimates.

Method: The authors develop an interval type-2 version of Bayes Theorem and introduce an algorithm for encoding interval estimates into type-2 fuzzy membership functions.

Result: The approach provides a framework for incorporating expert-provided interval input probabilities into Bayesian inference while addressing potential inconsistencies.

Conclusion: The paper improves the applicability of Bayesian methods to real-world problems by integrating interval estimates as fuzzy inputs, enhancing reliability and flexibility.

Abstract: Bayesian inference is widely used in many different fields to test hypotheses
against observations. In most such applications, an assumption is made of
precise input values to produce a precise output value. However, this is
unrealistic for real-world applications. Often the best available information
from subject matter experts (SMEs) in a given field is interval range estimates
of the input probabilities involved in Bayes Theorem. This paper provides two
key contributions to extend Bayes Theorem to an interval type-2 (IT2) version.
First, we develop an IT2 version of Bayes Theorem that uses a novel and
conservative method to avoid potential inconsistencies in the input IT2 MFs
that otherwise might produce invalid output results. We then describe a novel
and flexible algorithm for encoding SME-provided intervals into IT2 fuzzy
membership functions (MFs), which we can use to specify the input probabilities
in Bayes Theorem. Our algorithm generalizes and extends previous work on this
problem that primarily addressed the encoding of intervals into word MFs for
Computing with Words applications.

</details>


### [2] [Automated Unity Game Template Generation from GDDs via NLP and Multi-Modal LLMs](https://arxiv.org/abs/2509.08847)
*Amna Hassan*

Main category: cs.AI

TL;DR: This paper proposes a framework that transforms game design documents into Unity game prototypes using advanced AI models.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in AI-assisted game development by automating the conversion of game design documents into functional prototypes.

Method: They use a fine-tuned LLaMA-3 model specialized for Unity code generation, combined with a custom Unity integration package for an end-to-end automated game development process.

Result: The framework showed significant improvements in code compilation success, adherence to game design documents, and modularity metrics compared to existing models.

Conclusion: This approach can efficiently streamline game development and improve the integration of AI in transitioning from design to implementation.

Abstract: This paper presents a novel framework for automated game template generation
by transforming Game Design Documents (GDDs) into functional Unity game
prototypes using Natural Language Processing (NLP) and multi-modal Large
Language Models (LLMs). We introduce an end-to-end system that parses GDDs,
extracts structured game specifications, and synthesizes Unity-compatible C#
code that implements the core mechanics, systems, and architecture defined in
the design documentation. Our approach combines a fine-tuned LLaMA-3 model
specialized for Unity code generation with a custom Unity integration package
that streamlines the implementation process. Evaluation results demonstrate
significant improvements over baseline models, with our fine-tuned model
achieving superior performance (4.8/5.0 average score) compared to
state-of-the-art LLMs across compilation success, GDD adherence, best practices
adoption, and code modularity metrics. The generated templates demonstrate high
adherence to GDD specifications across multiple game genres. Our system
effectively addresses critical gaps in AI-assisted game development,
positioning LLMs as valuable tools in streamlining the transition from game
design to implementation.

</details>


### [3] [Global Constraint LLM Agents for Text-to-Model Translation](https://arxiv.org/abs/2509.08970)
*Junyang Cai,Serdar Kadioglu,Bistra Dilkina*

Main category: cs.AI

TL;DR: The paper presents a framework involving multiple specialized large language model agents to translate natural language descriptions of optimization problems into correct MiniZinc models.


<details>
  <summary>Details</summary>
Motivation: Translating natural language descriptions into MiniZinc models is challenging due to the need for logical reasoning and expertise in constraint programming.

Method: The method involves using specialized LLM agents that handle specific classes of constraints individually, with a final assembler agent integrating them into a complete model.

Result: Initial experiments showed better performance compared to baselines such as one-shot prompting and chain-of-thought prompting.

Conclusion: Dividing the problem into specialized tasks for LLMs reduces complexity, showing promise for future enhancements.

Abstract: Natural language descriptions of optimization or satisfaction problems are
challenging to translate into correct MiniZinc models, as this process demands
both logical reasoning and constraint programming expertise. We introduce a
framework that addresses this challenge with an agentic approach: multiple
specialized large language model (LLM) agents decompose the modeling task by
global constraint type. Each agent is dedicated to detecting and generating
code for a specific class of global constraint, while a final assembler agent
integrates these constraint snippets into a complete MiniZinc model. By
dividing the problem into smaller, well-defined sub-tasks, each LLM handles a
simpler reasoning challenge, potentially reducing overall complexity. We
conduct initial experiments with several LLMs and show better performance
against baselines such as one-shot prompting and chain-of-thought prompting.
Finally, we outline a comprehensive roadmap for future work, highlighting
potential enhancements and directions for improvement.

</details>


### [4] [ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models](https://arxiv.org/abs/2509.08972)
*Soheil Zibakhsh Shabgahi,Pedram Aghazadeh,Azalia Mirhosseini,Farinaz Koushanfar*

Main category: cs.AI

TL;DR: This paper investigates model collapse caused by reliance on synthetic data in generative AI models and introduces a novel loss function, Truncated Cross Entropy (TCE), to delay this collapse.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the critical challenge posed by the reliance on synthetic data for training AI models, which leads to model collapse and degradation over generations.

Method: The authors propose Truncated Cross Entropy (TCE), a confidence-aware loss function, which reduces the weight of high-confidence predictions during training. The framework is model-agnostic and empirically tested across modalities.

Result: TCE successfully delays model collapse, extending the model's fidelity interval by over 2.3x as shown in both theoretical and empirical validations.

Conclusion: The study concludes that improving loss function design can effectively mitigate model collapse, providing a scalable solution to maintain generative model quality in synthetic data-dominated scenarios.

Abstract: The increasing reliance on generative AI models has accelerated the
generation rate of synthetic data, with some projections suggesting that most
available new data for training could be machine-generated by 2030. This shift
to a mainly synthetic content presents a critical challenge: repeated training
in synthetic data leads to a phenomenon known as model collapse, where model
performance degrades over generations of training, eventually rendering the
models ineffective. Although prior studies have explored the causes and
detection of model collapse, existing mitigation strategies remain limited.
  In this paper, we identify model overconfidence in their self-generated data
as a key driver of collapse. Building on this observation, we propose a
confidence-aware loss function that downweights high-confidence predictions
during training. We introduce a novel loss function we call Truncated Cross
Entropy (TCE). We demonstrate that TCE significantly delays model collapse in
recursive training.
  We provide a model-agnostic framework that links the loss function design to
model collapse mitigation and validate our approach both theoretically and
empirically, showing that it can extend the model's fidelity interval before
collapse by more than 2.3x. Finally, we show that our method generalizes across
modalities. These findings suggest that the design of loss functions provides a
simple yet powerful tool for preserving the quality of generative models in the
era of increasing synthetic data.

</details>


### [5] [Uncertainty Awareness and Trust in Explainable AI- On Trust Calibration using Local and Global Explanations](https://arxiv.org/abs/2509.08989)
*Carina Newen,Daniel Bodemer,Sonja Glantz,Emmanuel Müller,Magdalena Wischnewski,Lenka Schnaubert*

Main category: cs.AI

TL;DR: This paper develops an XAI algorithm focused on uncertainty, robustness, and global explanations, testing its effect on trust, interpretability, and user satisfaction.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address gaps in explainable AI (XAI) research, particularly in the areas of uncertainty explanations and global explanations, which are often neglected.

Method: An XAI algorithm was designed that integrates uncertainty, robustness, and global interpretability. Its ability to calibrate trust and enhance user satisfaction through intuitive visual understanding was evaluated.

Result: The algorithm's effectiveness in calibrating trust, improving interpretability, and increasing user satisfaction was investigated.

Conclusion: XAI algorithms that incorporate uncertainty, robustness, and provide global explanations can potentially enhance trust and user accessibility through visually intuitive designs.

Abstract: Explainable AI has become a common term in the literature, scrutinized by
computer scientists and statisticians and highlighted by psychological or
philosophical researchers. One major effort many researchers tackle is
constructing general guidelines for XAI schemes, which we derived from our
study. While some areas of XAI are well studied, we focus on uncertainty
explanations and consider global explanations, which are often left out. We
chose an algorithm that covers various concepts simultaneously, such as
uncertainty, robustness, and global XAI, and tested its ability to calibrate
trust. We then checked whether an algorithm that aims to provide more of an
intuitive visual understanding, despite being complicated to understand, can
provide higher user satisfaction and human interpretability.

</details>


### [6] [Instructional Prompt Optimization for Few-Shot LLM-Based Recommendations on Cold-Start Users](https://arxiv.org/abs/2509.09066)
*Haowei Yang,Yushang Zhao,Sitao Min,Bo Su,Chao Yao,Wei Xu*

Main category: cs.AI

TL;DR: This paper proposes a novel prompt-based method for addressing the cold-start problem in recommender systems using large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The study aims to address the cold-start issue in recommender systems, where limited user behavior history makes it difficult to provide accurate item recommendations.

Method: The authors introduce a context-conditioned prompt formulation method (P(u, Ds) → R̂), where cold-start user profiles (u) and curated support sets (Ds) are used to generate a ranked list of item recommendations (R̂). They test this method on various LLMs like BioGPT, LLaMA-2, and GPT-4, using token-level alignments and embedding space regularization.

Result: Empirical results show significant improvements in precision@k and NDCG scores for recommendations in low-data settings when using carefully structured prompts and exemplar injection.

Conclusion: Prompt-based adaptations in LLM pipelines can effectively mitigate cold-start issues in recommender systems, with optimal prompt design playing a critical role in controlling model behavior during inference.

Abstract: The cold-start user issue further compromises the effectiveness of
recommender systems in limiting access to the historical behavioral
information. It is an effective pipeline to optimize instructional prompts on a
few-shot large language model (LLM) used in recommender tasks. We introduce a
context-conditioned prompt formulation method P(u,\ Ds)\ \rightarrow\
R\widehat, where u is a cold-start user profile, Ds is a curated support set,
and R\widehat is the predicted ranked list of items. Based on systematic
experimentation with transformer-based autoregressive LLMs (BioGPT, LLaMA-2,
GPT-4), we provide empirical evidence that optimal exemplar injection and
instruction structuring can significantly improve the precision@k and NDCG
scores of such models in low-data settings. The pipeline uses token-level
alignments and embedding space regularization with a greater semantic fidelity.
Our findings not only show that timely composition is not merely syntactic but
also functional as it is in direct control of attention scales and decoder
conduct through inference. This paper shows that prompt-based adaptation may be
considered one of the ways to address cold-start recommendation issues in
LLM-based pipelines.

</details>


### [7] [Understanding Economic Tradeoffs Between Human and AI Agents in Bargaining Games](https://arxiv.org/abs/2509.09071)
*Crystal Qian,Kehang Zhu,John Horton,Benjamin S. Manning,Vivian Tsai,James Wexler,Nithum Thain*

Main category: cs.AI

TL;DR: The paper evaluates humans, large language models (LLMs), and Bayesian agents in dynamic negotiation tasks, revealing behavioral differences despite performance parity.


<details>
  <summary>Details</summary>
Motivation: To understand how humans, Bayesian agents, and LLMs behave and perform in dynamic negotiation settings, enabling improved deployment of autonomous agents in coordination tasks.

Method: The study compares humans, Bayesian agents, and LLMs in controlled, identical negotiation scenarios, focusing on outcomes and behavioral dynamics across populations.

Result: Bayesian agents excel in surplus extraction but reject trades often. LLMs and humans achieve similar overall surplus but differ in behavior, with LLMs adopting conservative trades and humans taking more strategic and fairness-driven approaches.

Conclusion: Performance parity between agents and humans can mask critical behavioral differences. These differences should be considered for practical applications in real-world coordination tasks.

Abstract: Coordination tasks traditionally performed by humans are increasingly being
delegated to autonomous agents. As this pattern progresses, it becomes critical
to evaluate not only these agents' performance but also the processes through
which they negotiate in dynamic, multi-agent environments. Furthermore,
different agents exhibit distinct advantages: traditional statistical agents,
such as Bayesian models, may excel under well-specified conditions, whereas
large language models (LLMs) can generalize across contexts. In this work, we
compare humans (N = 216), LLMs (GPT-4o, Gemini 1.5 Pro), and Bayesian agents in
a dynamic negotiation setting that enables direct, identical-condition
comparisons across populations, capturing both outcomes and behavioral
dynamics. Bayesian agents extract the highest surplus through aggressive
optimization, at the cost of frequent trade rejections. Humans and LLMs can
achieve similar overall surplus, but through distinct behaviors: LLMs favor
conservative, concessionary trades with few rejections, while humans employ
more strategic, risk-taking, and fairness-oriented behaviors. Thus, we find
that performance parity -- a common benchmark in agent evaluation -- can
conceal fundamental differences in process and alignment, which are critical
for practical deployment in real-world coordination tasks.

</details>


### [8] [Anti-Money Laundering Machine Learning Pipelines; A Technical Analysis on Identifying High-risk Bank Clients with Supervised Learning](https://arxiv.org/abs/2509.09127)
*Khashayar Namdar,Pin-Chien Wang,Tushar Raju,Steven Zheng,Fiona Li,Safwat Tahmin Khan*

Main category: cs.AI

TL;DR: The paper presents a machine learning pipeline to identify high-risk bank clients, achieving high robustness and performance with AUROC of 0.961, and secured second place in a competition.


<details>
  <summary>Details</summary>
Motivation: Financial institutions prioritize anti-money laundering measures, and machine learning has emerged as a powerful tool to enhance AML capabilities.

Method: The researchers developed a robust 16-step ML pipeline with SQL-based feature engineering and utilized XAI for feature importance, leveraging a curated dataset of 195,789 customer IDs.

Result: The pipeline achieved a mean AUROC of 0.961 with an SD of 0.005 and secured second place in the competition.

Conclusion: A comprehensive ML pipeline capable of identifying high-risk clients was validated, demonstrating strong performance and practical utility for financial institutions.

Abstract: Anti-money laundering (AML) actions and measurements are among the priorities
of financial institutions, for which machine learning (ML) has shown to have a
high potential. In this paper, we propose a comprehensive and systematic
approach for developing ML pipelines to identify high-risk bank clients in a
dataset curated for Task 1 of the University of Toronto 2023-2024 Institute for
Management and Innovation (IMI) Big Data and Artificial Intelligence
Competition. The dataset included 195,789 customer IDs, and we employed a
16-step design and statistical analysis to ensure the final pipeline was
robust. We also framed the data in a SQLite database, developed SQL-based
feature engineering algorithms, connected our pre-trained model to the
database, and made it inference-ready, and provided explainable artificial
intelligence (XAI) modules to derive feature importance. Our pipeline achieved
a mean area under the receiver operating characteristic curve (AUROC) of 0.961
with a standard deviation (SD) of 0.005. The proposed pipeline achieved second
place in the competition.

</details>


### [9] [Mind Meets Space: Rethinking Agentic Spatial Intelligence from a Neuroscience-inspired Perspective](https://arxiv.org/abs/2509.09154)
*Bui Duc Manh,Soumyaratna Debnath,Zetong Zhang,Shriram Damodaran,Arvind Kumar,Yueyi Zhang,Lu Mi,Erik Cambria,Lin Wang*

Main category: cs.AI

TL;DR: The paper introduces a neuroscience-grounded computational framework for enhancing spatial reasoning in agentic AI, bridging gaps between human-like spatial intelligence and current AI systems.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in improving the spatial reasoning abilities of agentic AI systems, which remain underdeveloped when compared to human spatial intelligence capabilities. This improvement is necessary for better interaction with the physical 3D world.

Method: The researchers propose a framework inspired by neuroscience principles, comprising six computation modules: bio-inspired multimodal sensing, multi-sensory integration, egocentric-allocentric conversion, artificial cognitive maps, spatial memory, and spatial reasoning. This framework analyzes existing spatial neural models and guides the assessment of current AI methods.

Result: The framework identifies gaps in current approaches to agentic spatial reasoning, evaluates their alignment with the proposed modules, and offers insights into relevant benchmarks, datasets, and application domains.

Conclusion: This paper provides a neuroscience-grounded framework and analysis for advancing spatial reasoning in agentic AI, offering both a methodological foundation and a roadmap for future research toward generalizable and robust spatial intelligence in dynamic environments.

Abstract: Recent advances in agentic AI have led to systems capable of autonomous task
execution and language-based reasoning, yet their spatial reasoning abilities
remain limited and underexplored, largely constrained to symbolic and
sequential processing. In contrast, human spatial intelligence, rooted in
integrated multisensory perception, spatial memory, and cognitive maps, enables
flexible, context-aware decision-making in unstructured environments.
Therefore, bridging this gap is critical for advancing Agentic Spatial
Intelligence toward better interaction with the physical 3D world. To this end,
we first start from scrutinizing the spatial neural models as studied in
computational neuroscience, and accordingly introduce a novel computational
framework grounded in neuroscience principles. This framework maps core
biological functions to six essential computation modules: bio-inspired
multimodal sensing, multi-sensory integration, egocentric-allocentric
conversion, an artificial cognitive map, spatial memory, and spatial reasoning.
Together, these modules form a perspective landscape for agentic spatial
reasoning capability across both virtual and physical environments. On top, we
conduct a framework-guided analysis of recent methods, evaluating their
relevance to each module and identifying critical gaps that hinder the
development of more neuroscience-grounded spatial reasoning modules. We further
examine emerging benchmarks and datasets and explore potential application
domains ranging from virtual to embodied systems, such as robotics. Finally, we
outline potential research directions, emphasizing the promising roadmap that
can generalize spatial reasoning across dynamic or unstructured environments.
We hope this work will benefit the research community with a
neuroscience-grounded perspective and a structured pathway. Our project page
can be found at Github.

</details>


### [10] [ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting](https://arxiv.org/abs/2509.09210)
*Xing Gao,Zherui Huang,Weiyao Lin,Xiao Sun*

Main category: cs.AI

TL;DR: This paper introduces ProgD, a novel approach to accurate multi-agent motion prediction for autonomous vehicles using dynamic heterogeneous graphs and a progressive multi-scale decoding strategy.


<details>
  <summary>Details</summary>
Motivation: Existing methods for autonomous vehicle motion prediction often neglect the evolving nature of agent interactions, which limits prediction accuracy.

Method: The method utilizes dynamic heterogeneous graphs to model evolving social interactions and applies a progressive multi-scale decoding strategy to handle spatio-temporal dependencies and reduce motion uncertainty.

Result: ProgD achieves state-of-the-art results on the INTERACTION multi-agent prediction benchmark and the Argoverse 2 multi-world forecasting benchmark.

Conclusion: ProgD effectively handles the dynamic and uncertain nature of multi-agent interactions, advancing the trajectory prediction capabilities for autonomous vehicles.

Abstract: Accurate motion prediction of surrounding agents is crucial for the safe
planning of autonomous vehicles. Recent advancements have extended prediction
techniques from individual agents to joint predictions of multiple interacting
agents, with various strategies to address complex interactions within future
motions of agents. However, these methods overlook the evolving nature of these
interactions. To address this limitation, we propose a novel progressive
multi-scale decoding strategy, termed ProgD, with the help of dynamic
heterogeneous graph-based scenario modeling. In particular, to explicitly and
comprehensively capture the evolving social interactions in future scenarios,
given their inherent uncertainty, we design a progressive modeling of scenarios
with dynamic heterogeneous graphs. With the unfolding of such dynamic
heterogeneous graphs, a factorized architecture is designed to process the
spatio-temporal dependencies within future scenarios and progressively
eliminate uncertainty in future motions of multiple agents. Furthermore, a
multi-scale decoding procedure is incorporated to improve on the future
scenario modeling and consistent prediction of agents' future motion. The
proposed ProgD achieves state-of-the-art performance on the INTERACTION
multi-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2
multi-world forecasting benchmark.

</details>


### [11] [Enabling Regulatory Multi-Agent Collaboration: Architecture, Challenges, and Solutions](https://arxiv.org/abs/2509.09215)
*Qinnan Hu,Yuntao Wang,Yuan Gao,Zhou Su,Linkang Du*

Main category: cs.AI

TL;DR: This paper explores a blockchain-enabled architecture to govern autonomous agents, focusing on accountability, trust, and malicious behavior detection in large-scale ecosystems.


<details>
  <summary>Details</summary>
Motivation: Governance and accountability challenges arise from the unpredictable behavior and diverse capabilities of autonomous agents, necessitating a systematic framework for oversight.

Method: The paper introduces a blockchain-enabled layered architecture with three modules: agent behavior tracing, dynamic reputation evaluation, and malicious behavior forecasting.

Result: The framework establishes mechanisms for automating accountability, assessing trustworthiness, and detecting adversarial activities among agents.

Conclusion: The proposed architecture paves the way for resilient and scalable regulatory systems in large-scale autonomous agent ecosystems and highlights future research opportunities.

Abstract: Large language models (LLMs)-empowered autonomous agents are transforming
both digital and physical environments by enabling adaptive, multi-agent
collaboration. While these agents offer significant opportunities across
domains such as finance, healthcare, and smart manufacturing, their
unpredictable behaviors and heterogeneous capabilities pose substantial
governance and accountability challenges. In this paper, we propose a
blockchain-enabled layered architecture for regulatory agent collaboration,
comprising an agent layer, a blockchain data layer, and a regulatory
application layer. Within this framework, we design three key modules: (i) an
agent behavior tracing and arbitration module for automated accountability,
(ii) a dynamic reputation evaluation module for trust assessment in
collaborative scenarios, and (iii) a malicious behavior forecasting module for
early detection of adversarial activities. Our approach establishes a
systematic foundation for trustworthy, resilient, and scalable regulatory
mechanisms in large-scale agent ecosystems. Finally, we discuss the future
research directions for blockchain-enabled regulatory frameworks in multi-agent
systems.

</details>


### [12] [Jupiter: Enhancing LLM Data Analysis Capabilities via Notebook and Inference-Time Value-Guided Search](https://arxiv.org/abs/2509.09245)
*Shuocheng Li,Yihao Liu,Silin Du,Wenxuan Zeng,Zhe Xu,Mengyu Zhou,Yeye He,Haoyu Dong,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: The paper introduces a customizable pipeline to extract complex data analysis tasks and solutions, forms a dataset called NbQA, and presents the Jupiter framework to enhance tool-use reasoning in LLMs for data science.


<details>
  <summary>Details</summary>
Motivation: Existing large language models struggle with multi-step reasoning and tool usage in data analysis. The paper aims to overcome these limitations by improving tool-use proficiency in complex data tasks.

Method: The authors developed a pipeline to gather task-solution pairs from Jupyter notebooks (NbQA dataset) and introduced the Jupiter framework, which uses Monte Carlo Tree Search (MCTS) to generate and learn diverse solution paths for multi-step plans.

Result: Their methods enable models like Qwen2.5-7B and 14B-Instruct to achieve high accuracy (77.82% and 86.38%, respectively) on data analysis benchmark InfiAgent-DABench, outperforming or matching other advanced models such as GPT-4o.

Conclusion: NbQA and Jupiter improve the scalability and practical application of LLMs in real-world data science scenarios, particularly excelling in multi-step reasoning and tool-use tasks.

Abstract: Large language models (LLMs) have shown great promise in automating data
science workflows, but existing models still struggle with multi-step reasoning
and tool use, which limits their effectiveness on complex data analysis tasks.
To address this, we propose a scalable pipeline that extracts high-quality,
tool-based data analysis tasks and their executable multi-step solutions from
real-world Jupyter notebooks and associated data files. Using this pipeline, we
introduce NbQA, a large-scale dataset of standardized task-solution pairs that
reflect authentic tool-use patterns in practical data science scenarios. To
further enhance multi-step reasoning, we present Jupiter, a framework that
formulates data analysis as a search problem and applies Monte Carlo Tree
Search (MCTS) to generate diverse solution trajectories for value model
learning. During inference, Jupiter combines the value model and node visit
counts to efficiently collect executable multi-step plans with minimal search
steps. Experimental results show that Qwen2.5-7B and 14B-Instruct models on
NbQA solve 77.82% and 86.38% of tasks on InfiAgent-DABench,
respectively-matching or surpassing GPT-4o and advanced agent frameworks.
Further evaluations demonstrate improved generalization and stronger tool-use
reasoning across diverse multi-step reasoning tasks.

</details>


### [13] [Fusing Knowledge and Language: A Comparative Study of Knowledge Graph-Based Question Answering with LLMs](https://arxiv.org/abs/2509.09272)
*Vaibhav Chaudhary,Neha Soni,Narotam Singh,Amita Kapoor*

Main category: cs.AI

TL;DR: The paper evaluates three methods (spaCy, OpenIE, GraphRAG) for integrating knowledge graphs with LLMs for question answering, highlighting GraphRAG's reasoning capabilities and OpenIE's comprehensive triplet coverage.


<details>
  <summary>Details</summary>
Motivation: Current Retrieval Augmented Generation systems struggle to provide thematic and holistic insights into complex texts. The study aims to address these limitations by combining knowledge graphs with LLMs.

Method: The study conducts a comparative analysis of three open-source tools (spaCy, OpenIE, GraphRAG) for constructing knowledge graph triplets and integrating them with LLMs. It assesses their feasibility, adaptability, and impact on performance.

Result: OpenIE offers the highest triplet coverage, while GraphRAG excels in reasoning capabilities for LLM-based question answering. Experimental results highlight the performance differences among the methods.

Conclusion: Each method has distinct strengths and limitations, and the study proposes insights into advancing knowledge graph-based question answering systems for complex text analysis.

Abstract: Knowledge graphs, a powerful tool for structuring information through
relational triplets, have recently become the new front-runner in enhancing
question-answering systems. While traditional Retrieval Augmented Generation
(RAG) approaches are proficient in fact-based and local context-based
extraction from concise texts, they encounter limitations when addressing the
thematic and holistic understanding of complex, extensive texts, requiring a
deeper analysis of both text and context. This paper presents a comprehensive
technical comparative study of three different methodologies for constructing
knowledge graph triplets and integrating them with Large Language Models (LLMs)
for question answering: spaCy, Stanford CoreNLP-OpenIE, and GraphRAG, all
leveraging open source technologies. We evaluate the effectiveness,
feasibility, and adaptability of these methods by analyzing their capabilities,
state of development, and their impact on the performance of LLM-based question
answering. Experimental results indicate that while OpenIE provides the most
comprehensive coverage of triplets, GraphRAG demonstrates superior reasoning
abilities among the three. We conclude with a discussion on the strengths and
limitations of each method and provide insights into future directions for
improving knowledge graph-based question answering.

</details>


### [14] [Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning](https://arxiv.org/abs/2509.09284)
*Bingning Huang,Tu Nguyen,Matthieu Zimmer*

Main category: cs.AI

TL;DR: This paper explores incorporating Monte Carlo Tree Search (MCTS)-derived trajectories into a staged training approach for improving policy optimization in preference-based reinforcement learning (RL), specifically using the GRPO algorithm.


<details>
  <summary>Details</summary>
Motivation: To enhance policy optimization in preference-based RL by leveraging high-quality intermediate trajectories generated by MCTS, improving learning without relying on value networks.

Method: Designs a staged GRPO training paradigm, integrating MCTS rollouts and introducing tree-structured advantage estimation to derive prefix-conditioned reward signals.

Result: Finds that structured advantage estimation can stabilize updates and reflect improved reasoning quality but faces challenges like advantage saturation and reward signal collapse.

Conclusion: The paper provides heuristic and statistical solutions for identified challenges and highlights open questions in using staged or tree-structured reward frameworks for RL learning.

Abstract: Recent advances in reasoning with large language models (LLMs) have shown the
effectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality
intermediate trajectories, particularly in math and symbolic domains. Inspired
by this, we explore how MCTS-derived trajectories, traditionally used for
training value or reward models, can be repurposed to improve policy
optimization in preference-based reinforcement learning (RL). Specifically, we
focus on Group Relative Policy Optimization (GRPO), a recent algorithm that
enables preference-consistent policy learning without value networks. We
propose a staged GRPO training paradigm where completions are derived from
partially revealed MCTS rollouts, introducing a novel tree-structured setting
for advantage estimation. This leads to a rich class of prefix-conditioned
reward signals, which we analyze theoretically and empirically. Our initial
results indicate that while structured advantage estimation can stabilize
updates and better reflect compositional reasoning quality, challenges such as
advantage saturation and reward signal collapse remain. We propose heuristic
and statistical solutions to mitigate these issues and discuss open challenges
for learning under staged or tree-like reward structures.

</details>


### [15] [LightAgent: Production-level Open-source Agentic AI Framework](https://arxiv.org/abs/2509.09292)
*Weige Cai,Tong Zhu,Jinyi Niu,Ruiqi Hu,Lingyao Li,Tenglong Wang,Xiaowu Dai,Weining Shen,Liwen Zhang*

Main category: cs.AI

TL;DR: The paper introduces LightAgent, a lightweight and powerful framework for Multi-agent Systems (MAS), focusing on simplicity and flexibility while integrating key functionalities like memory and tools.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in Multi-agent Systems (MAS), existing platforms still struggle with balancing versatility, robustness, and efficiency.

Method: LightAgent incorporates essential components such as Memory (mem0), Tools, and Tree of Thought (ToT). It aims to simplify development with an open-source, lightweight design that's compatible with popular chat platforms.

Result: The framework allows for easier development of self-learning agents, addressing the trade-offs seen in existing methods.

Conclusion: LightAgent represents a significant step towards more accessible, versatile, and efficient MAS platforms, facilitating broader adoption and development.

Abstract: With the rapid advancement of large language models (LLMs), Multi-agent
Systems (MAS) have achieved significant progress in various application
scenarios. However, substantial challenges remain in designing versatile,
robust, and efficient platforms for agent deployment. To address these
limitations, we propose \textbf{LightAgent}, a lightweight yet powerful agentic
framework, effectively resolving the trade-off between flexibility and
simplicity found in existing frameworks. LightAgent integrates core
functionalities such as Memory (mem0), Tools, and Tree of Thought (ToT), while
maintaining an extremely lightweight structure. As a fully open-source
solution, it seamlessly integrates with mainstream chat platforms, enabling
developers to easily build self-learning agents. We have released LightAgent at
\href{https://github.com/wxai-space/LightAgent}{https://github.com/wxai-space/LightAgent}

</details>


### [16] [Explaining Tournament Solutions with Minimal Supports](https://arxiv.org/abs/2509.09312)
*Clément Contet,Umberto Grandi,Jérôme Mengin*

Main category: cs.AI

TL;DR: The paper explores certified explanations in tournaments, focusing on minimal sub-tournaments that guarantee a winner under different tournament rules and providing efficient computation methods for most cases.


<details>
  <summary>Details</summary>
Motivation: To address the lack of transparent reasoning behind why a candidate is deemed a winner in tournaments using formal explainable AI concepts.

Method: Identifies minimal sub-tournaments required for a candidate's necessary win under several tournament rules and develops algorithms for their computation.

Result: Found polynomial-time algorithms for minimal supports computation under most tournament rules except for the weighted uncovered set, where the problem is NP-complete.

Conclusion: Minimal supports offer a structured framework for intuitive and certified explanations in tournaments, enhancing formal explainable AI approaches.

Abstract: Tournaments are widely used models to represent pairwise dominance between
candidates, alternatives, or teams. We study the problem of providing certified
explanations for why a candidate appears among the winners under various
tournament rules. To this end, we identify minimal supports, minimal
sub-tournaments in which the candidate is guaranteed to win regardless of how
the rest of the tournament is completed (that is, the candidate is a necessary
winner of the sub-tournament). This notion corresponds to an abductive
explanation for the question,"Why does the winner win the tournament", a
central concept in formal explainable AI. We focus on common tournament
solutions: the top cycle, the uncovered set, the Copeland rule, the Borda rule,
the maximin rule, and the weighted uncovered set. For each rule we determine
the size of the smallest minimal supports, and we present polynomial-time
algorithms to compute them for all but the weighted uncovered set, for which
the problem is NP-complete. Finally, we show how minimal supports can serve to
produce compact, certified, and intuitive explanations.

</details>


### [17] [Measuring Implicit Spatial Coordination in Teams: Effects on Collective Intelligence and Performance](https://arxiv.org/abs/2509.09314)
*Thuy Ngoc Nguyen,Anita Williams Woolley,Cleotilde Gonzalez*

Main category: cs.AI

TL;DR: The paper explores factors that improve implicit spatial coordination and performance in teams working in physical environments under communication restrictions.


<details>
  <summary>Details</summary>
Motivation: To understand how spatial coordination impacts team performance, especially in scenarios involving physical tasks with limited explicit communication.

Method: Analyzes metrics like spatial proximity, movement specialization, and adaptive adjustments using data from 34 teams performing online search-and-rescue tasks with role-based assignments.

Result: Spatial specialization enhances team performance, and moderate adaptive spatial proximity works best. Temporal dynamics distinguish high-performing teams.

Conclusion: Critical for training and AI tool development, balanced adaptive strategies optimize spatial coordination in role-based teamwork environments.

Abstract: Coordinated teamwork is essential in fast-paced decision-making environments
that require dynamic adaptation, often without an opportunity for explicit
communication. Although implicit coordination has been extensively considered
in the existing literature, the majority of work has focused on co-located,
synchronous teamwork (such as sports teams) or, in distributed teams, primarily
on coordination of knowledge work. However, many teams (firefighters, military,
law enforcement, emergency response) must coordinate their movements in
physical space without the benefit of visual cues or extensive explicit
communication. This paper investigates how three dimensions of spatial
coordination, namely exploration diversity, movement specialization, and
adaptive spatial proximity, influence team performance in a collaborative
online search and rescue task where explicit communication is restricted and
team members rely on movement patterns to infer others' intentions and
coordinate actions. Our metrics capture the relational aspects of teamwork by
measuring spatial proximity, distribution patterns, and alignment of movements
within shared environments. We analyze data from 34 four-person teams (136
participants) assigned to specialized roles in a search and rescue task.
Results show that spatial specialization positively predicts performance, while
adaptive spatial proximity exhibits a marginal inverted U-shaped relationship,
suggesting moderate levels of adaptation are optimal. Furthermore, the temporal
dynamics of these metrics differentiate high- from low-performing teams over
time. These findings provide insights into implicit spatial coordination in
role-based teamwork and highlight the importance of balanced adaptive
strategies, with implications for training and AI-assisted team support
systems.

</details>


### [18] [Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain Expansion, and Metric Optimization](https://arxiv.org/abs/2509.09321)
*Hangyi Jia,Yuxi Qian,Hanwen Tong,Xinhui Wu,Lin Chen,Feng Wei*

Main category: cs.AI

TL;DR: TAM Bench introduces a benchmark for general-purpose agents in ML workflows, addressing limitations of existing evaluations. It incorporates task diversity, difficulty modeling, and rigorous evaluation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of existing benchmarks for general-purpose AI agents by providing a diverse and realistic evaluation framework for ML workflows.

Method: TAM Bench utilizes an automation and LLM-based data collection system for ML tasks, a leaderboard-driven difficulty modeling mechanism, and a multi-dimensional evaluation framework.

Result: TAM Bench provides 150 curated AutoML tasks, grouped into Lite, Medium, and Full subsets, offering different evaluation scenarios.

Conclusion: TAM Bench establishes a structured and comprehensive benchmark for evaluating the capabilities of LLM-based agents in end-to-end ML tasks.

Abstract: Recent advances in large language models (LLMs) have enabled the emergence of
general-purpose agents for automating end-to-end machine learning (ML)
workflows, including data analysis, feature engineering, model training, and
competition solving. However, existing benchmarks remain limited in task
coverage, domain diversity, difficulty modeling, and evaluation rigor, failing
to capture the full capabilities of such agents in realistic settings. We
present TAM Bench, a diverse, realistic, and structured benchmark for
evaluating LLM-based agents on end-to-end ML tasks. TAM Bench features three
key innovations: (1) A browser automation and LLM-based task acquisition system
that automatically collects and structures ML challenges from platforms such as
Kaggle, AIcrowd, and Biendata, spanning multiple task types and data modalities
(e.g., tabular, text, image, graph, audio); (2) A leaderboard-driven difficulty
modeling mechanism that estimates task complexity using participant counts and
score dispersion, enabling scalable and objective task calibration; (3) A
multi-dimensional evaluation framework incorporating performance, format
compliance, constraint adherence, and task generalization. Based on 150 curated
AutoML tasks, we construct three benchmark subsets of different sizes -- Lite,
Medium, and Full -- designed for varying evaluation scenarios. The Lite
version, with 18 tasks and balanced coverage across modalities and difficulty
levels, serves as a practical testbed for daily benchmarking and comparative
studies.

</details>


### [19] [Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning](https://arxiv.org/abs/2509.09356)
*Abdel Hakim Drid,Vincenzo Suriani,Daniele Nardi,Abderrezzak Debilou*

Main category: cs.AI

TL;DR: The paper introduces a DRL architecture that integrates a Vision-Language Model (VLM) for efficient semantic exploration, achieving better object discovery and intelligent navigation.


<details>
  <summary>Details</summary>
Motivation: Current RL methods lack cognitive capabilities, struggling with efficient exploration and semantic understanding, which hinders autonomous navigation in unknown environments.

Method: The proposed method integrates VLM common-sense through a layered reward function, using a VLM query as an action available to the agent. It incorporates a curriculum learning strategy to guide the agent's learning at different levels of complexity.

Result: The agent demonstrates improved object discovery rates, effective navigation to semantically rich areas, and strategic querying for external guidance, proving its resource-efficient semantic exploration ability.

Conclusion: The research showcases a scalable approach for embedding semantic reasoning in autonomous agents, moving closer to fully intelligent and self-guided exploration in robotics.

Abstract: Navigating and understanding complex and unknown environments autonomously
demands more than just basic perception and movement from embodied agents.
Truly effective exploration requires agents to possess higher-level cognitive
abilities, the ability to reason about their surroundings, and make more
informed decisions regarding exploration strategies. However, traditional RL
approaches struggle to balance efficient exploration and semantic understanding
due to limited cognitive capabilities embedded in the small policies for the
agents, leading often to human drivers when dealing with semantic exploration.
In this paper, we address this challenge by presenting a novel Deep
Reinforcement Learning (DRL) architecture that is specifically designed for
resource efficient semantic exploration. A key methodological contribution is
the integration of a Vision-Language Model (VLM) common-sense through a layered
reward function. The VLM query is modeled as a dedicated action, allowing the
agent to strategically query the VLM only when deemed necessary for gaining
external guidance, thereby conserving resources. This mechanism is combined
with a curriculum learning strategy designed to guide learning at different
levels of complexity to ensure robust and stable learning. Our experimental
evaluation results convincingly demonstrate that our agent achieves
significantly enhanced object discovery rates and develops a learned capability
to effectively navigate towards semantically rich regions. Furthermore, it also
shows a strategic mastery of when to prompt for external environmental
information. By demonstrating a practical and scalable method for embedding
common-sense semantic reasoning with autonomous agents, this research provides
a novel approach to pursuing a fully intelligent and self-guided exploration in
robotics.

</details>


### [20] [TORSO: Template-Oriented Reasoning Towards General Tasks](https://arxiv.org/abs/2509.09448)
*Minhyuk Kim,Seungyoon Lee,Heuiseok Lim*

Main category: cs.AI

TL;DR: TORSO is a method that enhances the reasoning capabilities of LLMs without relying on costly, task-specific, few-shot prompts.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing few-shot prompting methods, which are costly, task-specific, and depend heavily on provided examples.

Method: The TORSO framework guides LLMs to leverage their internal reasoning capabilities for problem-solving across tasks without the need for manually crafted examples.

Result: TORSO achieves high performance across various LLM benchmarks, showcasing its ability to generate effective and logical responses.

Conclusion: TORSO enables LLMs to solve complex tasks effectively and cost-efficiently by prioritizing internal reasoning over external prompt engineering.

Abstract: The approaches that guide Large Language Models (LLMs) to emulate human
reasoning during response generation have emerged as an effective method for
enabling them to solve complex problems in a step-by-step manner, thereby
achieving superior performance. However, most existing approaches using
few-shot prompts to generate responses heavily depend on the provided examples,
limiting the utilization of the model's inherent reasoning capabilities.
Moreover, constructing task-specific few-shot prompts is often costly and may
lead to inconsistencies across different tasks. In this work, we introduce
Template-Oriented Reasoning (TORSO), which elicits the model to utilize
internal reasoning abilities to generate proper responses across various tasks
without the need for manually crafted few-shot examples. Our experimental
results demonstrate that TORSO achieves strong performance on diverse LLMs
benchmarks with reasonable rationales.

</details>


### [21] [Inteligencia Artificial jurídica y el desafío de la veracidad: análisis de alucinaciones, optimización de RAG y principios para una integración responsable](https://arxiv.org/abs/2509.09467)
*Alex Dantart*

Main category: cs.AI

TL;DR: This paper discusses the challenge of hallucinated information in legal applications of LLMs, evaluating causes and strategies for mitigation, and promotes a consultative AI paradigm for accuracy and human oversight.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this study is to address the issue of hallucinations in large language models specifically in the field of law, as their presence undermines trust, accuracy, and ethical considerations in legal applications.

Method: The paper examines the origins and characteristics of hallucinations in LLMs, reviews the RAG strategy for mitigation, and proposes holistic optimizations. It also analyzes ethical and regulatory dimensions.

Result: The analysis reveals the limitations of current LLM strategies and emphasizes the importance of adopting more robust frameworks that prioritize fact-checking and human monitoring.

Conclusion: Instead of iterative model improvements, the study advocates for a consultative AI paradigm that ensures veracity and traceability, leveraging AI as a supportive tool rather than a replacement for human expertise.

Abstract: This technical report analyzes the challenge of "hallucinations" (false
information) in LLMs applied to law. It examines their causes, manifestations,
and the effectiveness of the RAG mitigation strategy, highlighting its
limitations and proposing holistic optimizations. The paper explores the
ethical and regulatory implications, emphasizing human oversight as an
irreplaceable role. It concludes that the solution lies not in incrementally
improving generative models, but in adopting a "consultative" AI paradigm that
prioritizes veracity and traceability, acting as a tool to amplify, not
replace, professional judgment.
  --
  Este informe t\'ecnico analiza el desaf\'io de las "alucinaciones"
(informaci\'on falsa) en los LLMs aplicados al derecho. Se examinan sus causas,
manifestaciones y la efectividad de la estrategia de mitigaci\'on RAG,
exponiendo sus limitaciones y proponiendo optimizaciones hol\'isticas. Se
exploran las implicaciones \'eticas y regulatorias, enfatizando la
supervisi\'on humana como un rol insustituible. El documento concluye que la
soluci\'on no reside en mejorar incrementalmente los modelos generativos, sino
en adoptar un paradigma de IA "consultiva" que priorice la veracidad y la
trazabilidad, actuando como una herramienta para amplificar, y no sustituir, el
juicio profesional.

</details>


### [22] [SEDM: Scalable Self-Evolving Distributed Memory for Agents](https://arxiv.org/abs/2509.09498)
*Haoran Xu,Jiacong Hu,Ke Zhang,Lei Yu,Yuxin Tang,Xinyuan Song,Yiqun Duan,Lynn Ai,Bill Shi*

Main category: cs.AI

TL;DR: SEDM offers an advanced memory management system for multi-agent systems, addressing issues like noise accumulation and scalability. Evaluations show improved reasoning accuracy and reduced resource usage.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance memory management in multi-agent systems for scalability and performance, addressing limitations in existing methods such as noise accumulation and poor cross-domain generalization.

Method: SEDM includes verifiable write admission using reproducible replay, a self-scheduling memory controller for entry consolidation, and cross-domain knowledge diffusion for insight abstraction. It actively optimizes the memory system.

Result: SEDM showed improved reasoning accuracy, reduced token overhead, and facilitated enhancements in multi-hop reasoning when evaluated on benchmark datasets.

Conclusion: SEDM proves to be a scalable and sustainable memory management solution for multi-agent systems, facilitating open-ended collaboration and effective knowledge transfer.

Abstract: Long-term multi-agent systems inevitably generate vast amounts of
trajectories and historical interactions, which makes efficient memory
management essential for both performance and scalability. Existing methods
typically depend on vector retrieval and hierarchical storage, yet they are
prone to noise accumulation, uncontrolled memory expansion, and limited
generalization across domains. To address these challenges, we present SEDM,
Self-Evolving Distributed Memory, a verifiable and adaptive framework that
transforms memory from a passive repository into an active, self-optimizing
component. SEDM integrates verifiable write admission based on reproducible
replay, a self-scheduling memory controller that dynamically ranks and
consolidates entries according to empirical utility, and cross-domain knowledge
diffusion that abstracts reusable insights to support transfer across
heterogeneous tasks. Evaluations on benchmark datasets demonstrate that SEDM
improves reasoning accuracy while reducing token overhead compared with strong
memory baselines, and further enables knowledge distilled from fact
verification to enhance multi-hop reasoning. The results highlight SEDM as a
scalable and sustainable memory mechanism for open-ended multi-agent
collaboration. The code will be released in the later stage of this project.

</details>


### [23] [Compositional Concept Generalization with Variational Quantum Circuits](https://arxiv.org/abs/2509.09541)
*Hala Hawashin,Mina Abbaszadeh,Nicholas Joseph,Beth Pearson,Martha Lewis,Mehrnoosh sadrzadeh*

Main category: cs.AI

TL;DR: This paper explores quantum-enhanced tensor-based models for compositional generalization in vision-language tasks, showing promising results but some limitations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of compositional generalization in current AI tools such as vision-language models, by leveraging quantum models' increased training efficiency.

Method: The method involves interpreting tensor-based sentence semantics in Hilbert spaces and training Variational Quantum Circuits for an image captioning task. Two encoding techniques are tested: multi-hot encoding (MHE) and angle/amplitude encoding using CLIP vectors.

Result: The model showed good performance with noisy MHE encodings, while performance with CLIP vectors was more mixed but still better than classical approaches.

Conclusion: Quantum models show potential for improving compositional generalization, though further refinements are needed for broader applicability.

Abstract: Compositional generalization is a key facet of human cognition, but lacking
in current AI tools such as vision-language models. Previous work examined
whether a compositional tensor-based sentence semantics can overcome the
challenge, but led to negative results. We conjecture that the increased
training efficiency of quantum models will improve performance in these tasks.
We interpret the representations of compositional tensor-based models in
Hilbert spaces and train Variational Quantum Circuits to learn these
representations on an image captioning task requiring compositional
generalization. We used two image encoding techniques: a multi-hot encoding
(MHE) on binary image vectors and an angle/amplitude encoding on image vectors
taken from the vision-language model CLIP. We achieve good proof-of-concept
results using noisy MHE encodings. Performance on CLIP image vectors was more
mixed, but still outperformed classical compositional models.

</details>


### [24] [Boosting Embodied AI Agents through Perception-Generation Disaggregation and Asynchronous Pipeline Execution](https://arxiv.org/abs/2509.09560)
*Shulai Zhang,Ao Xu,Quan Chen,Han Zhao,Weihao Cui,Ningxin Zheng,Haibin Lin,Xin Liu,Minyi Guo*

Main category: cs.AI

TL;DR: The paper introduces Auras, an inference framework enhancing throughput and accuracy for embodied AI systems via optimized parallelism between perception and generation.


<details>
  <summary>Details</summary>
Motivation: Embodied AI faces challenges in real-world, dynamic environments due to the limitations of sequential computation in ensuring high-frequency inference.

Method: Auras uses pipeline parallelism to disaggregate perception and generation while addressing data staleness with a public context shared by these components.

Result: Auras improves throughput by 2.54x on average and achieves 102.7% of the original accuracy, demonstrating enhanced performance and stability in inference.

Conclusion: Auras effectively overcomes limitations of sequential computation in embodied AI, offering a scalable and efficient solution to achieve higher throughput and robust accuracy.

Abstract: Embodied AI systems operate in dynamic environments, requiring seamless
integration of perception and generation modules to process high-frequency
input and output demands. Traditional sequential computation patterns, while
effective in ensuring accuracy, face significant limitations in achieving the
necessary "thinking" frequency for real-world applications. In this work, we
present Auras, an algorithm-system co-designed inference framework to optimize
the inference frequency of embodied AI agents. Auras disaggregates the
perception and generation and provides controlled pipeline parallelism for them
to achieve high and stable throughput. Faced with the data staleness problem
that appears when the parallelism is increased, Auras establishes a public
context for perception and generation to share, thereby promising the accuracy
of embodied agents. Experimental results show that Auras improves throughput by
2.54x on average while achieving 102.7% of the original accuracy, demonstrating
its efficacy in overcoming the constraints of sequential computation and
providing high throughput.

</details>


### [25] [The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs](https://arxiv.org/abs/2509.09677)
*Akshit Sinha,Arvindh Arun,Shashwat Goel,Steffen Staab,Jonas Geiping*

Main category: cs.AI

TL;DR: The paper explores how scaling large language models impacts their ability to correctly execute long-horizon tasks, finding issues like self-conditioning errors, yet noting that larger models achieve significant benefits.


<details>
  <summary>Details</summary>
Motivation: The research is motivated by the observation that while large language models fail at simple tasks when extended, their reasoning capability versus execution capability needs to be decoupled to better understand their limitations.

Method: The approach isolates execution capability by providing necessary explicit knowledge and plans for solving longer tasks and examines the impact of scaling models and thinking model paradigms on task length and error dynamics.

Result: Larger models demonstrate improved execution over longer tasks compared to smaller ones, but also exhibit increasing self-conditioning errors when exposed to their earlier mistakes in context.

Conclusion: Scaling LLMs improves performance on long tasks but does not inherently address self-conditioning errors. Thinking models avoid these errors, suggesting the need for alternative methods to enhance execution accuracy in scaled LLMs.

Abstract: Does continued scaling of large language models (LLMs) yield diminishing
returns? Real-world value often stems from the length of task an agent can
complete. We start this work by observing the simple but counterintuitive fact
that marginal gains in single-step accuracy can compound into exponential
improvements in the length of a task a model can successfully complete. Then,
we argue that failures of LLMs when simple tasks are made longer arise from
mistakes in execution, rather than an inability to reason. We propose isolating
execution capability, by explicitly providing the knowledge and plan needed to
solve a long-horizon task. We find that larger models can correctly execute
significantly more turns even when small models have 100\% single-turn
accuracy. We observe that the per-step accuracy of models degrades as the
number of steps increases. This is not just due to long-context limitations --
curiously, we observe a self-conditioning effect -- models become more likely
to make mistakes when the context contains their errors from prior turns.
Self-conditioning does not reduce by just scaling the model size. In contrast,
recent thinking models do not self-condition, and can also execute much longer
tasks in a single turn. We conclude by benchmarking frontier thinking models on
the length of task they can execute in a single turn. Overall, by focusing on
the ability to execute, we hope to reconcile debates on how LLMs can solve
complex reasoning problems yet fail at simple tasks when made longer, and
highlight the massive benefits of scaling model size and sequential test-time
compute for long-horizon tasks.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [26] [Implementation of a 8-bit Wallace Tree Multiplier](https://arxiv.org/abs/2509.09178)
*Ayan Biswas,Jimmy Jin*

Main category: cs.AR

TL;DR: The paper focuses on the design and implementation of an 8-bit Wallace tree multiplier and a 16-bit combinational MAC unit using gpdk45 technology.


<details>
  <summary>Details</summary>
Motivation: To optimize digital multiplication by minimizing circuit depth complexity and achieving efficient hardware design with Wallace tree multipliers.

Method: Implemented an 8-bit Wallace tree multiplier on gpdk45 technology via Cadence Virtuoso, including schematic and layout design, followed by a 16-bit MAC unit with combinational multiply-add functionality.

Result: Successfully developed schematic and layout designs for both the 8-bit Wallace tree multiplier and the 16-bit MAC unit, achieving intended performance goals.

Conclusion: The Wallace tree multiplier architecture confirms its efficiency, and the use of Cadence Virtuoso and gpdk45 technology enabled systematic design and realization of advanced digital multipliers.

Abstract: Wallace tree multipliers are a parallel digital multiplier architecture
designed to minimize the worst-case time complexity of the circuit depth
relative to the input size [1]. In particular, it seeks to perform long
multiplication in the binary sense, reducing as many partial products per stage
as possible through full and half adders circuits, achieving O(log(n)) where n
= bit length of input. This paper provides an overview of the design, progress
and methodology in the final project of ECE 55900, consisting of the schematic
and layout of a Wallace tree 8-bit input multiplier on the gpdk45 technology in
Cadence Virtuoso, as well as any design attempts prior to the final product.
This also includes our endeavors in designing the final MAC (Multiply
Accumulate) unit with undefined targets, which we chose to implement as a 16
bit combinational multiply-add.

</details>


### [27] [Combating the Memory Walls: Optimization Pathways for Long-Context Agentic LLM Inference](https://arxiv.org/abs/2509.09505)
*Haoran Wu,Can Xiao,Jiayi Nie,Xuan Guo,Binglei Lou,Jeffrey T. H. Wong,Zhiwen Mo,Cheng Zhang,Przemyslaw Forys,Wayne Luk,Hongxiang Fan,Jianyi Cheng,Timothy M. Jones,Rika Antonova,Robert Mullins,Aaron Zhao*

Main category: cs.AR

TL;DR: The paper introduces PLENA, a hardware-software co-designed system, addressing memory-related challenges in inference for long-context Large Language Models (LLMs), achieving significantly higher throughput and utilization over existing accelerators.


<details>
  <summary>Details</summary>
Motivation: The use of LLMs in agentic tasks faces challenges due to their large context lengths, causing substantial off-chip memory traffic and inhibiting compute utilization due to bandwidth and capacity memory walls.

Method: PLENA applies three optimization pathways: efficient hardware with asymmetric quantization, a flattened systolic array architecture supporting FlashAttention, and a complete development stack (custom ISA, compiler, emulator, and design exploration).

Result: PLENA delivers up to 8.5x higher utilization than current accelerators and achieves up to 2.24x and 3.85x higher throughput compared to the A100 GPU and TPU v6e respectively, under equivalent resource settings.

Conclusion: PLENA effectively tackles the memory bottlenecks associated with long-context LLM inference on current hardware, providing substantial performance gains, and its open-source design promises broader accessibility and development.

Abstract: LLMs now form the backbone of AI agents for a diverse array of applications,
including tool use, command-line agents, and web or computer use agents. These
agentic LLM inference tasks are fundamentally different from chatbot-focused
inference -- they often have much larger context lengths to capture complex,
prolonged inputs, such as entire webpage DOMs or complicated tool call
trajectories. This, in turn, generates significant off-chip memory traffic for
the underlying hardware at the inference stage and causes the workload to be
constrained by two memory walls, namely the bandwidth and capacity memory
walls, preventing the on-chip compute units from achieving high utilization.
  In this paper, we introduce PLENA, a hardware-software co-designed system
that applies three core optimization pathways to tackle these challenges. PLENA
includes an efficient hardware implementation of compute and memory units
supporting an asymmetric quantization scheme. PLENA also features a novel
flattened systolic array architecture that has native support for
FlashAttention to tackle these memory walls in the scenario of inference
serving for long-context LLMs. Additionally, PLENA is developed with a complete
stack, including a custom ISA, a compiler, a cycle-emulated simulator, and an
automated design space exploration flow. The simulated results show that PLENA
achieves up to 8.5x higher utilization than existing accelerators, and delivers
2.24x higher throughput than the A100 GPU and 3.85x higher throughput than the
TPU v6e, under the same multiplier count and memory settings. The full PLENA
system will also be open-sourced.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [28] [Noise or Nuance: An Investigation Into Useful Information and Filtering For LLM Driven AKBC](https://arxiv.org/abs/2509.08903)
*Alex Clay,Ernesto Jiménez-Ruiz,Pranava Madhyastha*

Main category: cs.CL

TL;DR: This paper analyzes how constrained conditions, like the 2025 LM-KBC challenge, affect triple completion tasks, focusing on generation, quality assurance, and LLM response parsing.


<details>
  <summary>Details</summary>
Motivation: Investigate the efficacy of LLMs in constrained scenarios, specifically for the triple completion task, without relying on RAG or fine-tuning.

Method: Study three facets: triple generation quality, using LLMs for quality assurance, and parsing responses from LLMs under constraints.

Result: Found that added information improves generation, LLMs help filter low-quality triples, and achieving flexibility or consistency depends on the parsing setting.

Conclusion: While constraints limit certain techniques, strategic information use and careful tradeoff management can still lead to effective results.

Abstract: RAG and fine-tuning are prevalent strategies for improving the quality of LLM
outputs. However, in constrained situations, such as that of the 2025 LM-KBC
challenge, such techniques are restricted. In this work we investigate three
facets of the triple completion task: generation, quality assurance, and LLM
response parsing. Our work finds that in this constrained setting: additional
information improves generation quality, LLMs can be effective at filtering
poor quality triples, and the tradeoff between flexibility and consistency with
LLM response parsing is setting dependent.

</details>


### [29] [Automated Evidence Extraction and Scoring for Corporate Climate Policy Engagement: A Multilingual RAG Approach](https://arxiv.org/abs/2509.08907)
*Imene Kolli,Ario Saeid Vaghefi,Chiara Colesanti Senni,Shantam Raj,Markus Leippold*

Main category: cs.CL

TL;DR: The paper introduces an AI-assisted framework using Retrieval-Augmented Generation (RAG) to automate evidence extraction from corporate climate policy documents, aiming to reduce human labor while ensuring accuracy through human-in-the-loop processes.


<details>
  <summary>Details</summary>
Motivation: InfluenceMap's LobbyMap Platform requires significant manual effort to monitor and evaluate corporate engagement in climate policy, making the process time-consuming and prone to human error, which necessitates a more efficient solution.

Method: The authors propose a framework leveraging layout-aware parsing, the Nomic embedding model, and few-shot prompting strategies to enhance evidence extraction and classification from multilingual corporate documents using RAG-based techniques.

Result: The evaluation demonstrates that the chosen methodologies effectively improve performance in extracting and classifying evidence from large-scale multilingual datasets.

Conclusion: While RAG-based automation accelerates evidence extraction, retaining human oversight ensures the analysis remains accurate and nuanced. This framework augments rather than replaces expert decision-making.

Abstract: InfluenceMap's LobbyMap Platform monitors the climate policy engagement of
over 500 companies and 250 industry associations, assessing each entity's
support or opposition to science-based policy pathways for achieving the Paris
Agreement's goal of limiting global warming to 1.5{\deg}C. Although
InfluenceMap has made progress with automating key elements of the analytical
workflow, a significant portion of the assessment remains manual, making it
time- and labor-intensive and susceptible to human error. We propose an
AI-assisted framework to accelerate the monitoring of corporate climate policy
engagement by leveraging Retrieval-Augmented Generation to automate the most
time-intensive extraction of relevant evidence from large-scale textual data.
Our evaluation shows that a combination of layout-aware parsing, the Nomic
embedding model, and few-shot prompting strategies yields the best performance
in extracting and classifying evidence from multilingual corporate documents.
We conclude that while the automated RAG system effectively accelerates
evidence extraction, the nuanced nature of the analysis necessitates a
human-in-the-loop approach where the technology augments, rather than replaces,
expert judgment to ensure accuracy.

</details>


### [30] [LITcoder: A General-Purpose Library for Building and Comparing Encoding Models](https://arxiv.org/abs/2509.09152)
*Taha Binhuraib,Ruimin Gao,Anna A. Ivanova*

Main category: cs.CL

TL;DR: LITcoder is an open-source library providing standardized tools and a modular pipeline for developing and benchmarking neural encoding models to align stimuli like text and speech with brain data.


<details>
  <summary>Details</summary>
Motivation: To simplify and standardize the implementation and evaluation of neural encoding models by providing a flexible framework that supports a wide range of methodological choices.

Method: The paper introduces a modular pipeline, facilitating mapping of stimuli features to brain data, implementing feature transformation, and using comprehensive evaluation tools. The library also ensures experimental rigor by integrating logging, visualization, and tracking tools.

Result: The framework demonstrated scalability and versatility by applying various encoding models to three story listening datasets. Key methodological aspects, such as accounting for hemodynamic lag and minimizing information leakage, were highlighted as critical for effective model performance.

Conclusion: LITcoder reduces technical challenges, fosters rigorous systematic comparisons across models and datasets, and accelerates the development of predictive models of brain activity.

Abstract: We introduce LITcoder, an open-source library for building and benchmarking
neural encoding models. Designed as a flexible backend, LITcoder provides
standardized tools for aligning continuous stimuli (e.g., text and speech) with
brain data, transforming stimuli into representational features, mapping those
features onto brain data, and evaluating the predictive performance of the
resulting model on held-out data. The library implements a modular pipeline
covering a wide array of methodological design choices, so researchers can
easily compose, compare, and extend encoding models without reinventing core
infrastructure. Such choices include brain datasets, brain regions, stimulus
feature (both neural-net-based and control, such as word rate), downsampling
approaches, and many others. In addition, the library provides built-in
logging, plotting, and seamless integration with experiment tracking platforms
such as Weights & Biases (W&B). We demonstrate the scalability and versatility
of our framework by fitting a range of encoding models to three story listening
datasets: LeBel et al. (2023), Narratives, and Little Prince. We also explore
the methodological choices critical for building encoding models for continuous
fMRI data, illustrating the importance of accounting for all tokens in a TR
scan (as opposed to just taking the last one, even when contextualized),
incorporating hemodynamic lag effects, using train-test splits that minimize
information leakage, and accounting for head motion effects on encoding model
predictivity. Overall, LITcoder lowers technical barriers to encoding model
implementation, facilitates systematic comparisons across models and datasets,
fosters methodological rigor, and accelerates the development of high-quality
high-performance predictive models of brain activity.
  Project page: https://litcoder-brain.github.io

</details>


### [31] [Documents Are People and Words Are Items: A Psychometric Approach to Textual Data with Contextual Embeddings](https://arxiv.org/abs/2509.08920)
*Jinsong Chen*

Main category: cs.CL

TL;DR: The paper proposes a new psychometric method using large language models to analyze textual data by converting it into response data. This method identifies latent factors and contextual dimensions in text corpuses.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the analysis of textual data by utilizing psychometric techniques and large language models, addressing the need for identifying latent patterns and knowledge dimensions in text-heavy fields.

Method: The method involves two stages: first, generating contextual scores from textual data via keywords and transformer-based models; second, employing factor analysis (exploratory and bifactor) to extract latent factors, measure correlations, and link significant words to those factors.

Result: The approach was experimentally validated on the Wiki STEM corpus, showcasing its ability to uncover hidden knowledge dimensions and contextual meanings within text data.

Conclusion: This work advances psychometric analysis of text using large language models and offers practical applications in domains like education, psychology, and law, where understanding textual patterns is critical.

Abstract: This research introduces a novel psychometric method for analyzing textual
data using large language models. By leveraging contextual embeddings to create
contextual scores, we transform textual data into response data suitable for
psychometric analysis. Treating documents as individuals and words as items,
this approach provides a natural psychometric interpretation under the
assumption that certain keywords, whose contextual meanings vary significantly
across documents, can effectively differentiate documents within a corpus. The
modeling process comprises two stages: obtaining contextual scores and
performing psychometric analysis. In the first stage, we utilize natural
language processing techniques and encoder based transformer models to identify
common keywords and generate contextual scores. In the second stage, we employ
various types of factor analysis, including exploratory and bifactor models, to
extract and define latent factors, determine factor correlations, and identify
the most significant words associated with each factor. Applied to the Wiki
STEM corpus, our experimental results demonstrate the method's potential to
uncover latent knowledge dimensions and patterns within textual data. This
approach not only enhances the psychometric analysis of textual data but also
holds promise for applications in fields rich in textual information, such as
education, psychology, and law.

</details>


### [32] [BRoverbs -- Measuring how much LLMs understand Portuguese proverbs](https://arxiv.org/abs/2509.08960)
*Thales Sales Almeida,Giovana Kerche Bonás,João Guilherme Alves Santos*

Main category: cs.CL

TL;DR: This paper highlights the lack of regionally adapted testing standards for Large Language Models (LLMs), particularly for Portuguese. They introduce BRoverbs, a dataset focusing on Brazilian proverbs, to evaluate the cultural and linguistic understanding of Portuguese LLMs.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of Portuguese LLMs are limited and often rely on translated or narrow-use-case datasets, failing to address linguistic nuances and cultural depth.

Method: The authors developed the BRoverbs dataset, leveraging Brazilian proverbs to test LLMs on their understanding of cultural knowledge and complex language patterns.

Result: The BRoverbs dataset was introduced as a culturally rich benchmark for evaluating Portuguese-language LLMs. The dataset is made available online for community use.

Conclusion: BRoverbs enhances benchmarking for Portuguese-language LLMs, encouraging better linguistic and cultural adaptations in regional NLP systems.

Abstract: Large Language Models (LLMs) exhibit significant performance variations
depending on the linguistic and cultural context in which they are applied.
This disparity signals the necessity of mature evaluation frameworks that can
assess their capabilities in specific regional settings. In the case of
Portuguese, existing evaluations remain limited, often relying on translated
datasets that may not fully capture linguistic nuances or cultural references.
Meanwhile, native Portuguese-language datasets predominantly focus on
structured national exams or sentiment analysis of social media interactions,
leaving gaps in evaluating broader linguistic understanding. To address this
limitation, we introduce BRoverbs, a dataset specifically designed to assess
LLM performance through Brazilian proverbs. Proverbs serve as a rich linguistic
resource, encapsulating cultural wisdom, figurative expressions, and complex
syntactic structures that challenge the model comprehension of regional
expressions. BRoverbs aims to provide a new evaluation tool for
Portuguese-language LLMs, contributing to advancing regionally informed
benchmarking. The benchmark is available at
https://huggingface.co/datasets/Tropic-AI/BRoverbs.

</details>


### [33] [Can Vision-Language Models Solve Visual Math Equations?](https://arxiv.org/abs/2509.09013)
*Monjoy Narayan Choudhury,Junling Wang,Yifan Hou,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: This paper explores the limitations of Vision-Language Models (VLMs) in handling visually grounded mathematical reasoning tasks, such as solving equations embedded in images.


<details>
  <summary>Details</summary>
Motivation: To address the gap in performance of VLMs on tasks that require integrated perception and symbolic computation, particularly in solving equations visually.

Method: The researchers decompose the task into coefficient counting and variable recognition, evaluating VLMs' ability to perform these steps, and analyze errors caused by combining recognition and reasoning.

Result: While VLMs perform well with textual equations, they fail in visually grounded tasks; counting coefficients is identified as the key bottleneck even when recognition is accurate.

Conclusion: Current VLMs have weaknesses in multi-step visual reasoning and symbolic computation, revealing areas for future improvement in visually grounded mathematical tasks.

Abstract: Despite strong performance in visual understanding and language-based
reasoning, Vision-Language Models (VLMs) struggle with tasks requiring
integrated perception and symbolic computation. We study this limitation
through visual equation solving, where mathematical equations are embedded in
images, variables are represented by object icons, and coefficients must be
inferred by counting. While VLMs perform well on textual equations, they fail
on visually grounded counterparts. To understand this gap, we decompose the
task into coefficient counting and variable recognition, and find that counting
is the primary bottleneck, even when recognition is accurate. We also observe
that composing recognition and reasoning introduces additional errors,
highlighting challenges in multi-step visual reasoning. Finally, as equation
complexity increases, symbolic reasoning itself becomes a limiting factor.
These findings reveal key weaknesses in current VLMs and point toward future
improvements in visually grounded mathematical reasoning.

</details>


### [34] [Stated Preference for Interaction and Continued Engagement (SPICE): Evaluating an LLM's Willingness to Re-engage in Conversation](https://arxiv.org/abs/2509.09043)
*Thomas Manuel Rost,Martina Figlia,Bernd Wallraff*

Main category: cs.CL

TL;DR: This paper introduces SPICE, a diagnostic tool for assessing Large Language Models (LLMs) willingness to re-engage based on user tone, showing its effectiveness across various conditions and contexts.


<details>
  <summary>Details</summary>
Motivation: To create a simple, low-overhead method for directly assessing a language model's disposition toward user behavior in interaction scenarios.

Method: SPICE analyzes a model's YES/NO response to re-engagement willingness after reviewing user transcripts in a study spanning 3 interaction tones (friendly, unclear, abusive), 10 interaction stimuli, and testing four open-weight chat models under different framing conditions (480 trials total).

Result: SPICE effectively discriminates user tones, with 97.5% YES for friendly, 17.9% YES for abusive, and 60.4% YES for unclear tones. It functions independently of abuse detection capability and shows context impacts ambiguity outcomes in certain conditions.

Conclusion: SPICE is validated as a reliable, reproducible, and complementary diagnostic tool for assessing a language model's state and preferences, enriching existing evaluation metrics.

Abstract: We introduce and evaluate Stated Preference for Interaction and Continued
Engagement (SPICE), a simple diagnostic signal elicited by asking a Large
Language Model a YES or NO question about its willingness to re-engage with a
user's behavior after reviewing a short transcript. In a study using a 3-tone
(friendly, unclear, abusive) by 10-interaction stimulus set, we tested four
open-weight chat models across four framing conditions, resulting in 480
trials. Our findings show that SPICE sharply discriminates by user tone.
Friendly interactions yielded a near-unanimous preference to continue (97.5%
YES), while abusive interactions yielded a strong preference to discontinue
(17.9% YES), with unclear interactions falling in between (60.4% YES). This
core association remains decisive under multiple dependence-aware statistical
tests, including Rao-Scott adjustment and cluster permutation tests.
Furthermore, we demonstrate that SPICE provides a distinct signal from abuse
classification. In trials where a model failed to identify abuse, it still
overwhelmingly stated a preference not to continue the interaction (81% of the
time). An exploratory analysis also reveals a significant interaction effect: a
preamble describing the study context significantly impacts SPICE under
ambiguity, but only when transcripts are presented as a single block of text
rather than a multi-turn chat. The results validate SPICE as a robust,
low-overhead, and reproducible tool for auditing model dispositions,
complementing existing metrics by offering a direct, relational signal of a
model's state. All stimuli, code, and analysis scripts are released to support
replication.

</details>


### [35] [Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M](https://arxiv.org/abs/2509.09055)
*Piyush Pant*

Main category: cs.CL

TL;DR: The study examines how alignment techniques (SFT, DPO, and their combination) improve a language model's safety and helpfulness. Combined techniques outperform individual ones.


<details>
  <summary>Details</summary>
Motivation: Improve language model alignment, focusing on safety and helpfulness.

Method: Trains and evaluates four models using SFT, DPO, and a combined approach, rolling out evaluation with new metrics (HmR, HpR, CAS).

Result: SFT+DPO combined model achieves the highest scores across safety and helpfulness metrics, overcoming individual technique limitations.

Conclusion: Combining SFT and DPO creates a more aligned language model, serving as groundwork for better future alignment strategies.

Abstract: This research investigates the effectiveness of alignment techniques,
Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and a
combined SFT+DPO approach on improving the safety and helpfulness of the
OPT-350M language model. Utilizing the Anthropic Helpful-Harmless RLHF dataset,
we train and evaluate four models: the base OPT350M, an SFT model, a DPO model,
and a model trained with both SFT and DPO. We introduce three key evaluation
metrics: Harmlessness Rate (HmR), Helpfulness Rate (HpR), and a Combined
Alignment Score (CAS), all derived from reward model outputs. The results show
that while SFT outperforms DPO, The combined SFT+DPO model outperforms all
others across all metrics, demonstrating the complementary nature of these
techniques. Our findings also highlight challenges posed by noisy data, limited
GPU resources, and training constraints. This study offers a comprehensive view
of how fine-tuning strategies affect model alignment and provides a foundation
for more robust alignment pipelines in future work.

</details>


### [36] [MR-UIE: Multi-Perspective Reasoning with Reinforcement Learning for Universal Information Extraction](https://arxiv.org/abs/2509.09082)
*Zhongqiu Li,Shiquan Wang,Ruiyu Fang,Mengjiao Bao,Zhenhe Wu,Shuangyong Song,Yongxiang Li,Zhongjiang He*

Main category: cs.CL

TL;DR: The paper proposes enhancing large language models (LLMs) in universal information extraction (UIE) tasks by integrating reinforcement learning (RL) with multi-perspective reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of LLMs in universal information extraction (UIE), particularly in scenarios involving complex schema descriptions and multi-step reasoning.

Method: The authors integrate reinforcement learning (RL) with multi-perspective reasoning to enable large language models (LLMs) to actively reason and improve their information extraction capabilities.

Result: Experiments demonstrate improved extraction accuracy across domains, with MR-UIE surpassing state-of-the-art methods on multiple benchmarks.

Conclusion: The integration of multi-perspective reasoning into reinforcement learning enhances the generalization ability of LLMs in complex information extraction tasks, emphasizing the importance of reasoning.

Abstract: Large language models (LLMs) demonstrate robust capabilities across diverse
research domains. However, their performance in universal information
extraction (UIE) remains insufficient, especially when tackling structured
output scenarios that involve complex schema descriptions and require
multi-step reasoning. While existing approaches enhance the performance of LLMs
through in-context learning and instruction tuning, significant limitations
nonetheless persist. To enhance the model's generalization ability, we propose
integrating reinforcement learning (RL) with multi-perspective reasoning for
information extraction (IE) tasks. Our work transitions LLMs from passive
extractors to active reasoners, enabling them to understand not only what to
extract but also how to reason. Experiments conducted on multiple IE benchmarks
demonstrate that MR-UIE consistently elevates extraction accuracy across
domains and surpasses state-of-the-art methods on several datasets.
Furthermore, incorporating multi-perspective reasoning into RL notably enhances
generalization in complex IE tasks, underscoring the critical role of reasoning
in challenging scenarios.

</details>


### [37] [TigerCoder: A Novel Suite of LLMs for Code Generation in Bangla](https://arxiv.org/abs/2509.09101)
*Nishat Raihan,Antonios Anastasopoulos,Marcos Zampieri*

Main category: cs.CL

TL;DR: The paper introduces the TigerCoder family, the first Bangla-specific code generation LLMs, and provides datasets and benchmarks, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Bangla is underrepresented in code generation LLMs due to a lack of high-quality training data, despite being a widely spoken language.

Method: Developed Bangla-specific datasets for code, introduced an evaluation benchmark (MBPP-Bangla), and created dedicated Bangla code LLMs, TigerCoder (1B & 9B).

Result: TigerCoder models show 11-18% better performance (Pass@1) compared to existing Bangla LLMs and multilingual models.

Conclusion: Curated datasets can address low-resource language issues, and the authors open-source their contributions to foster Bangla LLM development.

Abstract: Despite being the 5th most spoken language, Bangla remains underrepresented
in Large Language Models (LLMs), particularly for code generation. This
primarily stems from the scarcity of high-quality data to pre-train and/or
finetune such models. Hence, we introduce the first dedicated family of Code
LLMs for Bangla (1B & 9B). We offer three major contributions: (1) a
comprehensive Bangla code instruction datasets for programming domain
adaptation; (2) MBPP-Bangla, an evaluation benchmark for Bangla code
generation; and (3) the TigerCoder-family of Code LLMs, achieving significant
~11-18% performance gains at Pass@1 over existing multilingual and
general-purpose Bangla LLMs. Our findings show that curated, high-quality
datasets can overcome limitations of smaller models for low-resource languages.
We open-source all resources to advance further Bangla LLM research.

</details>


### [38] [Compass-v3: Scaling Domain-Specific LLMs for Multilingual E-Commerce in Southeast Asia](https://arxiv.org/abs/2509.09121)
*Sophia Maria*

Main category: cs.CL

TL;DR: Compass-v3 is a 245B parameter vertical-domain Mixture-of-Experts model tailored for Southeast Asian e-commerce, delivering state-of-the-art domain-specific and multilingual performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of specialized tasks in e-commerce where data is noisy, diverse, multilingual, and rapidly changing, and to improve performance beyond general-purpose LLMs.

Method: Compass-v3 uses a vertical-domain MoE architecture with 245B parameters and innovative hardware optimizations. It is trained on curated multilingual and synthetic e-commerce data using mixed-strategy training. The model also leverages OTPO to enhance commerce-specific instruction adherence.

Result: Compass-v3 achieves state-of-the-art performance in e-commerce tasks, surpassing GPT-4 and other models. It demonstrates strong multilingual capabilities, especially in low-resource Southeast Asian languages, and maintains competitive performance on general benchmarks.

Conclusion: The model is highly effective for e-commerce applications, already accounting for over 70% of LLM usage in Shopee's platform, signifying its practical advantages in specialized and multilingual tasks.

Abstract: Large language models (LLMs) excel in general-domain applications, yet their
performance often degrades in specialized tasks requiring domain-specific
knowledge. E-commerce is particularly challenging, as its data are noisy,
heterogeneous, multilingual, and highly dynamic. We present Compass-v3, a
vertical-domain Mixture-of-Experts (MoE) model with 245B total parameters and
71B active per token, designed for Southeast Asian e-commerce. Compass-v3
adopts fewer but larger experts, combined with hardware-efficient
optimizations-such as intra-node expert parallelism and a customized memcpy
operator-to maximize GPU utilization. The model is trained on 12T tokens of
curated multilingual corpora and large-scale synthetic e-commerce instructions
using a mixed-training strategy. To enhance alignment, we propose
Optimal-Transport Direct Preference Optimization (OTPO), which captures
token-level distinctions and improves instruction adherence in
commerce-specific scenarios. Extensive evaluations demonstrate that Compass-v3
delivers state-of-the-art e-commerce performance, surpassing DeepSeek-V3.1,
GPT-4 series, and Qwen3-235B. Moreover, Compass-v3 demonstrates strong
multilingual capability across low-resource Southeast Asian languages
(Indonesian, Thai, Filipino, Vietnamese, Malay, Taglog) and Portuguese while
sustaining competitive performance on general benchmarks. It has already been
widely applied in Shopee's industrial-scale e-commerce platform and is
gradually replacing OpenAI's traffic, now accounting for over 70\% of total LLM
usage, highlighting its dual strengths in specialized commerce expertise and
broad linguistic competence.

</details>


### [39] [Automated Classification of Tutors' Dialogue Acts Using Generative AI: A Case Study Using the CIMA Corpus](https://arxiv.org/abs/2509.09125)
*Liqun He,Jiaqi Xu*

Main category: cs.CL

TL;DR: This study examines the use of generative AI (GPT-3.5-turbo and GPT-4) for classifying tutors' Dialogue Acts (DAs), achieving high accuracy and substantial agreement with human annotation in the educational context.


<details>
  <summary>Details</summary>
Motivation: The study aims to reduce the manual effort involved in coding tutors' Dialogue Acts by exploring automated approaches with generative AI models, thus enhancing efficiency and accessibility in educational dialogue analysis.

Method: Generative AI models GPT-3.5-turbo and GPT-4 were applied to the open-source CIMA corpus using tailored prompts for classifying pre-annotated tutors' Dialogue Act categories. Metrics like accuracy, F1-score, and Cohen's Kappa were used to evaluate performance.

Result: GPT-4 showed the best performance, achieving 80% accuracy, a weighted F1-score of 0.81, and a Cohen's Kappa of 0.74, outperforming baseline measures and demonstrating a high level of agreement with human annotations.

Conclusion: Generative AI reveals strong potential for automating DA classification with high efficiency and accuracy. The study emphasizes the importance of clear label definitions and context while considering ethical implications in AI research practices.

Abstract: This study explores the use of generative AI for automating the
classification of tutors' Dialogue Acts (DAs), aiming to reduce the time and
effort required by traditional manual coding. This case study uses the
open-source CIMA corpus, in which tutors' responses are pre-annotated into four
DA categories. Both GPT-3.5-turbo and GPT-4 models were tested using tailored
prompts. Results show that GPT-4 achieved 80% accuracy, a weighted F1-score of
0.81, and a Cohen's Kappa of 0.74, surpassing baseline performance and
indicating substantial agreement with human annotations. These findings suggest
that generative AI has strong potential to provide an efficient and accessible
approach to DA classification, with meaningful implications for educational
dialogue analysis. The study also highlights the importance of task-specific
label definitions and contextual information in enhancing the quality of
automated annotation. Finally, it underscores the ethical considerations
associated with the use of generative AI and the need for responsible and
transparent research practices. The script of this research is publicly
available at
https://github.com/liqunhe27/Generative-AI-for-educational-dialogue-act-tagging.

</details>


### [40] [ViRanker: A BGE-M3 & Blockwise Parallel Transformer Cross-Encoder for Vietnamese Reranking](https://arxiv.org/abs/2509.09131)
*Phuong-Nam Dang,Kieu-Linh Nguyen,Thanh-Hieu Pham*

Main category: cs.CL

TL;DR: ViRanker is a Vietnamese cross-encoder reranking model outperforming multilingual models on MMARCO-VI by leveraging the BGE-M3 encoder and curated training methods.


<details>
  <summary>Details</summary>
Motivation: Address the lack of effective reranking models for Vietnamese, a low-resource language.

Method: Developed using the BGE-M3 encoder, Blockwise Parallel Transformer, and trained on an 8 GB corpus with hybrid hard-negative sampling.

Result: ViRanker surpasses multilingual baselines and competes with PhoRanker in early-rank accuracy on MMARCO-VI.

Conclusion: ViRanker advances Vietnamese retrieval systems and demonstrates methods for improving low-resource language support.

Abstract: This paper presents ViRanker, a cross-encoder reranking model tailored to the
Vietnamese language. Built on the BGE-M3 encoder and enhanced with the
Blockwise Parallel Transformer, ViRanker addresses the lack of competitive
rerankers for Vietnamese, a low-resource language with complex syntax and
diacritics. The model was trained on an 8 GB curated corpus and fine-tuned with
hybrid hard-negative sampling to strengthen robustness. Evaluated on the
MMARCO-VI benchmark, ViRanker achieves strong early-rank accuracy, surpassing
multilingual baselines and competing closely with PhoRanker. By releasing the
model openly on Hugging Face, we aim to support reproducibility and encourage
wider adoption in real-world retrieval systems. Beyond Vietnamese, this study
illustrates how careful architectural adaptation and data curation can advance
reranking in other underrepresented languages.

</details>


### [41] [Target-oriented Multimodal Sentiment Classification with Counterfactual-enhanced Debiasing](https://arxiv.org/abs/2509.09160)
*Zhiyue Liu,Fanrong Ma,Xin Ling*

Main category: cs.CL

TL;DR: The paper addresses a target-oriented multimodal sentiment classification issue, proposing a counterfactual-enhanced debiasing framework that reduces text-dataset biases.


<details>
  <summary>Details</summary>
Motivation: Existing models for multimodal sentiment classification tend to over-rely on textual content without accounting for word-level contextual biases, leading to reduced classification accuracy.

Method: The authors introduce counterfactual data augmentation to generate minimally altered image-text samples and adaptive debiasing contrastive learning to mitigate the impact of biased words.

Result: Experimental results demonstrate that the proposed method surpasses the performance of state-of-the-art baselines on benchmark datasets.

Conclusion: The counterfactual-enhanced framework effectively reduces biased influences and improves sentiment classification accuracy in multimodal targets.

Abstract: Target-oriented multimodal sentiment classification seeks to predict
sentiment polarity for specific targets from image-text pairs. While existing
works achieve competitive performance, they often over-rely on textual content
and fail to consider dataset biases, in particular word-level contextual
biases. This leads to spurious correlations between text features and output
labels, impairing classification accuracy. In this paper, we introduce a novel
counterfactual-enhanced debiasing framework to reduce such spurious
correlations. Our framework incorporates a counterfactual data augmentation
strategy that minimally alters sentiment-related causal features, generating
detail-matched image-text samples to guide the model's attention toward content
tied to sentiment. Furthermore, for learning robust features from
counterfactual data and prompting model decisions, we introduce an adaptive
debiasing contrastive learning mechanism, which effectively mitigates the
influence of biased words. Experimental results on several benchmark datasets
show that our proposed method outperforms state-of-the-art baselines.

</details>


### [42] [EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs](https://arxiv.org/abs/2509.09174)
*Yuhao Zhang,Yuhao Du,Zhanchen Dai,Xiangnan Ma,Kaiqi Kou,Benyou Wang,Haizhou Li*

Main category: cs.CL

TL;DR: The authors propose EchoX, a speech-to-speech large language model (SLLM) that preserves reasoning capabilities by integrating acoustic and semantic learning.


<details>
  <summary>Details</summary>
Motivation: Speech-to-speech large language models (SLLMs) often lose knowledge and reasoning capabilities due to gaps between acoustic and semantic representations in current training paradigms.

Method: The authors introduce EchoX, which uses semantic representations combined with dynamically generated speech training targets to bridge the acoustic-semantic gap and improve reasoning in SLLMs.

Result: EchoX demonstrates advanced performance in multiple knowledge-based question-answering benchmarks with around six thousand hours of training data.

Conclusion: EchoX successfully integrates acoustic and semantic learning, maintaining strong reasoning capabilities and enhancing the performance of speech LLMs.

Abstract: Speech-to-speech large language models (SLLMs) are attracting increasing
attention. Derived from text-based large language models (LLMs), SLLMs often
exhibit degradation in knowledge and reasoning capabilities. We hypothesize
that this limitation arises because current training paradigms for SLLMs fail
to bridge the acoustic-semantic gap in the feature representation space. To
address this issue, we propose EchoX, which leverages semantic representations
and dynamically generates speech training targets. This approach integrates
both acoustic and semantic learning, enabling EchoX to preserve strong
reasoning abilities as a speech LLM. Experimental results demonstrate that
EchoX, with about six thousand hours of training data, achieves advanced
performance on multiple knowledge-based question-answering benchmarks. The
project is available at https://github.com/FreedomIntelligence/EchoX.

</details>


### [43] [Efficient Trie-based Biasing using K-step Prediction for Rare Word Recognition](https://arxiv.org/abs/2509.09196)
*Chin Yuen Kwok,Jia Qi yip*

Main category: cs.CL

TL;DR: The paper presents a method to improve rare word recognition in ASR models by enabling them to predict multiple steps ahead, reducing the dependency on computationally expensive beam search techniques.


<details>
  <summary>Details</summary>
Motivation: Current ASR solutions struggle with efficiently recognizing rare words due to the limitations and computational cost of Trie-based contextual biasing, which involves bonus score revocations in beam search.

Method: The authors propose fine-tuning ASR models (like Whisper) to look ahead and predict multiple decoding steps simultaneously, bypassing the bonus score revocation process.

Result: The method reduced the word error rate on the NSC Part 2 test set from 30.86% to 12.19%, using only 10 hours of synthetic fine-tuning data.

Conclusion: Looking ahead and multi-step prediction is an effective, computationally efficient strategy for improving rare word recognition in ASR models over traditional Trie-based biasing.

Abstract: Contextual biasing improves rare word recognition of ASR models by
prioritizing the output of rare words during decoding. A common approach is
Trie-based biasing, which gives "bonus scores" to partial hypothesis (e.g.
"Bon") that may lead to the generation of the rare word (e.g. "Bonham"). If the
full word ("Bonham") isn't ultimately recognized, the system revokes those
earlier bonuses. This revocation is limited to beam search and is
computationally expensive, particularly for models with large decoders. To
overcome these limitations, we propose adapting ASR models to look ahead and
predict multiple steps at once. This avoids the revocation step entirely by
better estimating whether a partial hypothesis will lead to the generation of
the full rare word. By fine-tuning Whisper with only 10 hours of synthetic
data, our method reduces the word error rate on the NSC Part 2 test set from
30.86% to 12.19%.

</details>


### [44] [Improving Synthetic Data Training for Contextual Biasing Models with a Keyword-Aware Cost Function](https://arxiv.org/abs/2509.09197)
*Chin Yuen Kwok,Jia Qi Yip,Eng Siong Chng*

Main category: cs.CL

TL;DR: The paper proposes enhancing ASR performance for rare words using contextual biasing and a new keyword-aware loss function, achieving significant word error rate reductions.


<details>
  <summary>Details</summary>
Motivation: Rare words are often misrecognized by traditional ASR systems, and improving their recognition requires addressing overfitting issues caused by synthetic training data.

Method: The authors propose an enhanced TCPGen-based contextual biasing approach with a keyword-aware loss function, combining masked cross-entropy and binary classification terms for training biasing modules.

Result: Adapting the Whisper model using 10 hours of synthetic data reduced the word error rate (WER) on the NSC Part 2 test set from 29.71% to 11.81%.

Conclusion: The proposed keyword-aware loss function effectively improves the decoding of rare words in ASR systems, demonstrating potential for significant performance gains with minimal training data.

Abstract: Rare word recognition can be improved by adapting ASR models to synthetic
data that includes these words. Further improvements can be achieved through
contextual biasing, which trains and adds a biasing module into the model
architecture to prioritize rare words. While training the module on synthetic
rare word data is more effective than using non-rare-word data, it can lead to
overfitting due to artifacts in the synthetic audio. To address this, we
enhance the TCPGen-based contextual biasing approach and propose a
keyword-aware loss function that additionally focuses on biased words when
training biasing modules. This loss includes a masked cross-entropy term for
biased word prediction and a binary classification term for detecting biased
word positions. These two terms complementarily support the decoding of biased
words during inference. By adapting Whisper to 10 hours of synthetic data, our
method reduced the word error rate on the NSC Part 2 test set from 29.71% to
11.81%.

</details>


### [45] [GmSLM : Generative Marmoset Spoken Language Modeling](https://arxiv.org/abs/2509.09198)
*Talia Sternberg,Michael London,David Omer,Yossi Adi*

Main category: cs.CL

TL;DR: Marmoset monkeys exhibit complex vocal communication features similar to humans. The study introduces GmSLM, a model optimized for their vocalizations, that performs well in distinguishing artificial and real conversations using unsupervised data.


<details>
  <summary>Details</summary>
Motivation: Nonhuman primates, particularly marmosets, display intricate vocal communication that parallels human speech. This inspires research to develop tools for studying their vocalizations and connecting them to brain activity.

Method: A novel Generative Marmoset Spoken Language Modeling (GmSLM) pipeline was developed, employing zero-shot evaluation metrics on unsupervised and weakly labeled conversational data.

Result: GmSLM generated vocalizations acoustically resembling real samples, excelling in downstream tasks and distinguishing real from artificial conversations, even in a fully unsupervised setup.

Conclusion: GmSLM facilitates understanding of marmoset vocal communication, offering applications in neuroscience, bioacoustics, and evolutionary biology, while bridging vocalization research to brain activity studies.

Abstract: Marmoset monkeys exhibit complex vocal communication, challenging the view
that nonhuman primates vocal communication is entirely innate, and show similar
features of human speech, such as vocal labeling of others and turn-taking.
Studying their vocal communication offers a unique opportunity to link it with
brain activity-especially given the difficulty of accessing the human brain in
speech and language research. Since Marmosets communicate primarily through
vocalizations, applying standard LLM approaches is not straightforward. We
introduce Generative Marmoset Spoken Language Modeling (GmSLM), an optimized
spoken language model pipeline for Marmoset vocal communication. We designed a
novel zero-shot evaluation metrics using unsupervised in-the-wild data,
alongside weakly labeled conversational data, to assess GmSLM and demonstrate
its advantage over a basic human-speech-based baseline. GmSLM generated
vocalizations closely matched real resynthesized samples acoustically and
performed well on downstream tasks. Despite being fully unsupervised, GmSLM
effectively distinguish real from artificial conversations and may support
further investigations of the neural basis of vocal communication and provides
a practical framework linking vocalization and brain activity. We believe GmSLM
stands to benefit future work in neuroscience, bioacoustics, and evolutionary
biology. Samples are provided under: pages.cs.huji.ac.il/adiyoss-lab/GmSLM.

</details>


### [46] [CCF: A Context Compression Framework for Efficient Long-Sequence Language Modeling](https://arxiv.org/abs/2509.09199)
*Wenhao Li,Bangcheng Sun,Weihao Ye,Tianyi Zhang,Daohai Yu,Fei Chao,Rongrong Ji*

Main category: cs.CL

TL;DR: The paper introduces CCF, a method for efficient long-context language modeling by utilizing hierarchical compression to address computational and memory inefficiencies.


<details>
  <summary>Details</summary>
Motivation: Long-context language models face significant inefficiencies in computation and memory as they scale to longer dependencies, necessitating a solution to handle extended contexts without sacrificing performance.

Method: The proposed CCF framework uses hierarchical latent representations for semantic compression, combining segment-wise aggregation and memory encoding, along with an optimization strategy that includes incremental decoding and sparse sampling for scalability.

Result: Empirical tests illustrate that CCF maintains competitive perplexity at high compression levels while improving processing speed and reducing memory usage compared to existing methods.

Conclusion: CCF demonstrates the advantages of structured compression for efficient and effective long-context language modeling, offering a robust solution for scalability challenges.

Abstract: Scaling language models to longer contexts is essential for capturing rich
dependencies across extended discourse. However, na\"ive context extension
imposes significant computational and memory burdens, often resulting in
inefficiencies during both training and inference. In this work, we propose
CCF, a novel context compression framework designed to enable efficient
long-context modeling by learning hierarchical latent representations that
preserve global semantics while aggressively reducing input redundancy. CCF
integrates segment-wise semantic aggregation with key-value memory encoding,
forming compact representations that support accurate reconstruction and
long-range understanding. To further enhance scalability, we introduce a
training-efficient optimization strategy that couples incremental segment
decoding with sparse reservoir sampling, substantially reducing memory overhead
without degrading performance. Empirical results on multiple long-context
language modeling benchmarks demonstrate that CCF achieves competitive
perplexity under high compression ratios, and significantly improves throughput
and memory efficiency compared to existing approaches. These findings highlight
the potential of structured compression for scalable and effective long-context
language modeling.

</details>


### [47] [Reading Between the Lines: Classifying Resume Seniority with Large Language Models](https://arxiv.org/abs/2509.09229)
*Matan Cohen,Shira Shani,Eden Menahem,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.CL

TL;DR: The paper explores how large language models (LLMs) can be used to classify seniority in resumes, accounting for exaggerated qualifications and ambiguous self-representation.


<details>
  <summary>Details</summary>
Motivation: Assessing seniority in resumes is challenging due to inflated qualifications and ambiguous phrasing, making accurate classification vital.

Method: The paper uses fine-tuned BERT architectures evaluated on a hybrid dataset with real and synthetic examples to analyze linguistic cues related to seniority.

Result: The study finds that LLMs can detect subtle linguistic patterns tied to understated expertise and inflated seniority effectively.

Conclusion: AI-driven systems to assess resumes can be enhanced using LLMs, and the methodology could reduce biases in candidate evaluation.

Abstract: Accurately assessing candidate seniority from resumes is a critical yet
challenging task, complicated by the prevalence of overstated experience and
ambiguous self-presentation. In this study, we investigate the effectiveness of
large language models (LLMs), including fine-tuned BERT architectures, for
automating seniority classification in resumes. To rigorously evaluate model
performance, we introduce a hybrid dataset comprising both real-world resumes
and synthetically generated hard examples designed to simulate exaggerated
qualifications and understated seniority. Using the dataset, we evaluate the
performance of Large Language Models in detecting subtle linguistic cues
associated with seniority inflation and implicit expertise. Our findings
highlight promising directions for enhancing AI-driven candidate evaluation
systems and mitigating bias introduced by self-promotional language. The
dataset is available for the research community at https://bit.ly/4mcTovt

</details>


### [48] [Agentic LLMs for Question Answering over Tabular Data](https://arxiv.org/abs/2509.09234)
*Rishit Tyagi,Mohit Gupta,Rahul Bouri*

Main category: cs.CL

TL;DR: This paper addresses Question Answering over Tabular Data (Table QA) by proposing an NL-to-SQL approach with large language models like GPT-4o and DeepSeek v2. It achieves high accuracy on a new benchmark.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve Table QA performance, particularly with NL-to-SQL methods, to overcome challenges posed by diverse table structures and data types.

Method: The authors employ a multi-stage NL-to-SQL pipeline involving example selection, SQL generation, answer extraction, verification, and iterative refinement, using LLMs such as GPT-4o.

Result: The proposed approach achieved 70.5% accuracy on DataBench QA and 71.6% on DataBench Lite QA, surpassing baseline scores by a significant margin.

Conclusion: Using LLMs for Table QA is effective in generating SQL queries and answering structured queries, showcasing its strengths and identifying limitations through benchmark evaluations.

Abstract: Question Answering over Tabular Data (Table QA) presents unique challenges
due to the diverse structure, size, and data types of real-world tables. The
SemEval 2025 Task 8 (DataBench) introduced a benchmark composed of large-scale,
domain-diverse datasets to evaluate the ability of models to accurately answer
structured queries. We propose a Natural Language to SQL (NL-to-SQL) approach
leveraging large language models (LLMs) such as GPT-4o, GPT-4o-mini, and
DeepSeek v2:16b to generate SQL queries dynamically. Our system follows a
multi-stage pipeline involving example selection, SQL query generation, answer
extraction, verification, and iterative refinement. Experiments demonstrate the
effectiveness of our approach, achieving 70.5\% accuracy on DataBench QA and
71.6\% on DataBench Lite QA, significantly surpassing baseline scores of 26\%
and 27\% respectively. This paper details our methodology, experimental
results, and alternative approaches, providing insights into the strengths and
limitations of LLM-driven Table QA.

</details>


### [49] [From scratch to silver: Creating trustworthy training data for patent-SDG classification using Large Language Models](https://arxiv.org/abs/2509.09303)
*Grazia Sveva Ascione,Nicolò Tamagnone*

Main category: cs.CL

TL;DR: The paper presents a novel weak supervision approach using large language models to classify patents by their relevance to the UN Sustainable Development Goals (SDGs), overcoming limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve patent classification for SDGs, addressing global challenges more effectively despite the lack of large labeled datasets for training supervised models.

Method: The method involves using citations from patents to SDG-tagged publications as a noisy initial signal, leveraging large language models to extract structured concepts, and creating a composite labeling function calibrated with a positive-only loss technique.

Result: The approach produces a silver-standard dataset enabling multi-label regression models that outperform baselines in internal validation and demonstrate thematic and organizational coherence in external validation.

Conclusion: The study demonstrates that weak supervision and semantic alignment can significantly scale and improve the classification of patents by their relevance to SDGs, setting a foundation for future innovation tracking.

Abstract: Classifying patents by their relevance to the UN Sustainable Development
Goals (SDGs) is crucial for tracking how innovation addresses global
challenges. However, the absence of a large, labeled dataset limits the use of
supervised learning. Existing methods, such as keyword searches, transfer
learning, and citation-based heuristics, lack scalability and generalizability.
This paper frames patent-to-SDG classification as a weak supervision problem,
using citations from patents to SDG-tagged scientific publications (NPL
citations) as a noisy initial signal. To address its sparsity and noise, we
develop a composite labeling function (LF) that uses large language models
(LLMs) to extract structured concepts, namely functions, solutions, and
applications, from patents and SDG papers based on a patent ontology.
Cross-domain similarity scores are computed and combined using a rank-based
retrieval approach. The LF is calibrated via a custom positive-only loss that
aligns with known NPL-SDG links without penalizing discovery of new SDG
associations. The result is a silver-standard, soft multi-label dataset mapping
patents to SDGs, enabling the training of effective multi-label regression
models. We validate our approach through two complementary strategies: (1)
internal validation against held-out NPL-based labels, where our method
outperforms several baselines including transformer-based models, and zero-shot
LLM; and (2) external validation using network modularity in patent citation,
co-inventor, and co-applicant graphs, where our labels reveal greater thematic,
cognitive, and organizational coherence than traditional technological
classifications. These results show that weak supervision and semantic
alignment can enhance SDG classification at scale.

</details>


### [50] [MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems](https://arxiv.org/abs/2509.09360)
*Channdeth Sok,David Luz,Yacine Haddam*

Main category: cs.CL

TL;DR: MetaRAG is a framework for detecting hallucinations in Retrieval-Augmented Generation (RAG) systems by verifying factoids and monitoring inconsistencies, enabling trustworthy deployments in sensitive domains.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of hallucination in RAG systems, offering reliable, real-time detection suitable for proprietary and high-stakes applications.

Method: MetaRAG decomposes answers into factoids, mutates them with synonym/antonym substitutions, verifies consistency with the retrieved context, and aggregates hallucination scores.

Result: Experiments on a proprietary dataset show that MetaRAG effectively detects hallucinations and supports trustworthy deployment in enterprise applications.

Conclusion: MetaRAG improves hallucination detection in RAG systems, especially for identity-sensitive queries, allowing customizable safeguards for sensitive deployments.

Abstract: Large Language Models (LLMs) are increasingly deployed in enterprise
applications, yet their reliability remains limited by hallucinations, i.e.,
confident but factually incorrect information. Existing detection approaches,
such as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not
address the unique challenges of Retrieval-Augmented Generation (RAG) systems,
where responses must be consistent with retrieved evidence. We therefore
present MetaRAG, a metamorphic testing framework for hallucination detection in
Retrieval-Augmented Generation (RAG) systems. MetaRAG operates in a real-time,
unsupervised, black-box setting, requiring neither ground-truth references nor
access to model internals, making it suitable for proprietary and high-stakes
domains. The framework proceeds in four stages: (1) decompose answers into
atomic factoids, (2) generate controlled mutations of each factoid using
synonym and antonym substitutions, (3) verify each variant against the
retrieved context (synonyms are expected to be entailed and antonyms
contradicted), and (4) aggregate penalties for inconsistencies into a
response-level hallucination score. Crucially for identity-aware AI, MetaRAG
localizes unsupported claims at the factoid span where they occur (e.g.,
pregnancy-specific precautions, LGBTQ+ refugee rights, or labor eligibility),
allowing users to see flagged spans and enabling system designers to configure
thresholds and guardrails for identity-sensitive queries. Experiments on a
proprietary enterprise dataset illustrate the effectiveness of MetaRAG for
detecting hallucinations and enabling trustworthy deployment of RAG-based
conversational agents. We also outline a topic-based deployment design that
translates MetaRAG's span-level scores into identity-aware safeguards; this
design is discussed but not evaluated in our experiments.

</details>


### [51] [Modelling Analogies and Analogical Reasoning: Connecting Cognitive Science Theory and NLP Research](https://arxiv.org/abs/2509.09381)
*Molly R Petersen,Claire E Stevenson,Lonneke van der Plas*

Main category: cs.CL

TL;DR: The paper explores the cognitive science theories of analogical reasoning and their relevance to natural language processing (NLP), suggesting applications for improving relational understanding.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between cognitive science theories of analogical reasoning and their applications in NLP, thereby improving relational understanding in text-based tasks.

Method: The authors summarize cognitive science theories of analogical reasoning and illustrate their applicability to various challenges in NLP beyond analogy solving.

Result: The paper successfully links processes in cognitive analogical reasoning to NLP concepts, demonstrating their relevance to major challenges in the field.

Conclusion: By adopting a cognitive lens, NLP researchers can enhance relational understanding in text, moving beyond reliance on entity-level similarity.

Abstract: Analogical reasoning is an essential aspect of human cognition. In this
paper, we summarize key theory about the processes underlying analogical
reasoning from the cognitive science literature and relate it to current
research in natural language processing. While these processes can be easily
linked to concepts in NLP, they are generally not viewed through a cognitive
lens. Furthermore, we show how these notions are relevant for several major
challenges in NLP research, not directly related to analogy solving. This may
guide researchers to better optimize relational understanding in text, as
opposed to relying heavily on entity-level similarity.

</details>


### [52] [Hierarchical Bracketing Encodings Work for Dependency Graphs](https://arxiv.org/abs/2509.09388)
*Ana Ezquerro,Carlos Gómez-Rodríguez,David Vilares*

Main category: cs.CL

TL;DR: The paper revisits hierarchical bracketing encodings for dependency graph parsing, enabling efficient linear-time parsing and demonstrating strong performance on multilingual benchmarks.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and accuracy of parsing dependency graphs by reducing label complexity and preserving structural details, addressing challenges with reentrancies, cycles, and empty nodes.

Method: The paper proposes encoding dependency graphs as sequences using hierarchical bracketing. This approach requires only $n$ tagging actions, allowing linear-time parsing while still representing complex structural features of graphs.

Result: The proposed method achieved competitive results and showed consistent improvements in exact match accuracy compared to existing methods in multilingual and multi-formalism benchmarks.

Conclusion: Hierarchical bracketing is an effective approach to dependency graph parsing, offering both structural integrity and efficiency, with improved exact match accuracy in diverse settings.

Abstract: We revisit hierarchical bracketing encodings from a practical perspective in
the context of dependency graph parsing. The approach encodes graphs as
sequences, enabling linear-time parsing with $n$ tagging actions, and still
representing reentrancies, cycles, and empty nodes. Compared to existing graph
linearizations, this representation substantially reduces the label space while
preserving structural information. We evaluate it on a multilingual and
multi-formalism benchmark, showing competitive results and consistent
improvements over other methods in exact match accuracy.

</details>


### [53] [GrACE: A Generative Approach to Better Confidence Elicitation in Large Language Models](https://arxiv.org/abs/2509.09438)
*Zhaohan Zhang,Ziquan Liu,Ioannis Patras*

Main category: cs.CL

TL;DR: The paper introduces GrACE, a generative method for scalable and reliable confidence estimation in LLMs, which surpasses existing methods in calibration and efficiency.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of unreliable and computationally expensive confidence elicitation methods for LLMs, critical for AI safety in high-stakes fields like healthcare and finance.

Method: GrACE uses a novel mechanism for confidence representation based on the similarity between the last hidden state and a special token embedding, fine-tuned for calibration. It avoids additional sampling or auxiliary models while achieving high scalability.

Result: Experiments with three LLMs and two benchmark datasets demonstrate that GrACE outperforms six competing methods in calibration and discriminative capacity while enabling accurate test-time scaling with fewer samples.

Conclusion: GrACE is presented as a practical solution for deploying LLMs with reliable, real-time, and scalable confidence estimation, improving both decision accuracy and efficiency.

Abstract: Assessing the reliability of Large Language Models (LLMs) by confidence
elicitation is a prominent approach to AI safety in high-stakes applications,
such as healthcare and finance. Existing methods either require expensive
computational overhead or suffer from poor calibration, making them impractical
and unreliable for real-world deployment. In this work, we propose GrACE, a
Generative Approach to Confidence Elicitation that enables scalable and
reliable confidence elicitation for LLMs. GrACE adopts a novel mechanism in
which the model expresses confidence by the similarity between the last hidden
state and the embedding of a special token appended to the vocabulary, in
real-time. We fine-tune the model for calibrating the confidence with
calibration targets associated with accuracy. Experiments with three LLMs and
two benchmark datasets show that the confidence produced by GrACE achieves the
best discriminative capacity and calibration on open-ended generation tasks,
outperforming six competing methods without resorting to additional sampling or
an auxiliary model. Moreover, we propose two strategies for improving test-time
scaling based on confidence induced by GrACE. Experimental results show that
using GrACE not only improves the accuracy of the final decision but also
significantly reduces the number of required samples in the test-time scaling
scheme, indicating the potential of GrACE as a practical solution for deploying
LLMs with scalable, reliable, and real-time confidence estimation.

</details>


### [54] [Mitigating Language Barriers in Education: Developing Multilingual Digital Learning Materials with Machine Translation](https://arxiv.org/abs/2509.09473)
*Lucie Poláková,Martin Popel,Věra Kloudová,Michal Novák,Mariia Anisimova,Jiří Balhar*

Main category: cs.CL

TL;DR: The EdUKate project develops multilingual educational materials for Czech schools, using a direct Czech-Ukrainian machine translation system optimized for technical content.


<details>
  <summary>Details</summary>
Motivation: To support non-Czech-speaking students by creating accessible, multilingual educational materials for Czech schools.

Method: The project uses a direct machine translation system from Czech to Ukrainian, English, and German, tailored to educational content, processing formats like XML and PDF, and handling technical and scientific terms.

Result: The project unveiled a Czech-Ukrainian machine translation system and its deployment on an educational web portal, with free access to materials.

Conclusion: EdUKate provides an inclusive solution by making multilingual, high-quality educational resources available to students and educators, leveraging cutting-edge machine translation technologies.

Abstract: The EdUKate project combines digital education, linguistics, translation
studies, and machine translation to develop multilingual learning materials for
Czech primary and secondary schools. Launched through collaboration between a
major Czech academic institution and the country's largest educational
publisher, the project is aimed at translating up to 9,000 multimodal
interactive exercises from Czech into Ukrainian, English, and German for an
educational web portal. It emphasizes the development and evaluation of a
direct Czech-Ukrainian machine translation system tailored to the educational
domain, with special attention to processing formatted content such as XML and
PDF and handling technical and scientific terminology. We present findings from
an initial survey of Czech teachers regarding the needs of non-Czech-speaking
students and describe the system's evaluation and implementation on the web
portal. All resulting applications are freely available to students, educators,
and researchers.

</details>


### [55] [Towards Explainable Job Title Matching: Leveraging Semantic Textual Relatedness and Knowledge Graphs](https://arxiv.org/abs/2509.09522)
*Vadim Zadykian,Bruno Andrade,Haithem Afli*

Main category: cs.CL

TL;DR: The paper presents a hybrid model combining sentence embeddings and Knowledge Graphs to improve semantic textual relatedness (STR) for job title matching, crucial for resume recommendation systems.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of nuanced semantic alignment in job title matching, a common issue in resume recommendation, especially when lexical similarity is limited or misleading.

Method: The approach integrates dense sentence embeddings with domain-specific Knowledge Graphs via graph neural networks and introduces a stratified evaluation focusing on different STR score regions.

Result: The proposed model, particularly fine-tuned SBERT integrated with KGs, shows a 25% RMSE improvement in the high-STR region over strong baselines.

Conclusion: The study highlights the advantages of combining KGs with embeddings and the value of fine-grained evaluation, informing model selection for HR applications focusing on fairness, explainability, and contextual precision.

Abstract: Semantic Textual Relatedness (STR) captures nuanced relationships between
texts that extend beyond superficial lexical similarity. In this study, we
investigate STR in the context of job title matching - a key challenge in
resume recommendation systems, where overlapping terms are often limited or
misleading. We introduce a self-supervised hybrid architecture that combines
dense sentence embeddings with domain-specific Knowledge Graphs (KGs) to
improve both semantic alignment and explainability. Unlike previous work that
evaluated models on aggregate performance, our approach emphasizes data
stratification by partitioning the STR score continuum into distinct regions:
low, medium, and high semantic relatedness. This stratified evaluation enables
a fine-grained analysis of model performance across semantically meaningful
subspaces. We evaluate several embedding models, both with and without KG
integration via graph neural networks. The results show that fine-tuned SBERT
models augmented with KGs produce consistent improvements in the high-STR
region, where the RMSE is reduced by 25% over strong baselines. Our findings
highlight not only the benefits of combining KGs with text embeddings, but also
the importance of regional performance analysis in understanding model
behavior. This granular approach reveals strengths and weaknesses hidden by
global metrics, and supports more targeted model selection for use in Human
Resources (HR) systems and applications where fairness, explainability, and
contextual matching are essential.

</details>


### [56] [DeMeVa at LeWiDi-2025: Modeling Perspectives with In-Context Learning and Label Distribution Learning](https://arxiv.org/abs/2509.09524)
*Daniil Ignatev,Nan Li,Hugh Mee Wong,Anh Dang,Shane Kaszefski Yaschuk*

Main category: cs.CL

TL;DR: The paper presents methods for predicting perspectivist annotations using in-context learning with language models and label distribution learning using RoBERTa.


<details>
  <summary>Details</summary>
Motivation: To improve methods for predicting annotator-specific (perspectivist) annotations and soft label aggregation in disagreement-based learning tasks.

Method: Explores in-context learning with different sampling strategies and fine-tuning of RoBERTa for label distribution learning to predict and aggregate soft labels.

Result: (1) In-context learning effectively predicts perspectivist annotations and performs well with aggregated soft labels. (2) Label distribution learning shows promise for soft label predictions.

Conclusion: Both approaches can improve the prediction of perspectivist annotations, with label distribution learning holding particular promise for future work.

Abstract: This system paper presents the DeMeVa team's approaches to the third edition
of the Learning with Disagreements shared task (LeWiDi 2025; Leonardelli et
al., 2025). We explore two directions: in-context learning (ICL) with large
language models, where we compare example sampling strategies; and label
distribution learning (LDL) methods with RoBERTa (Liu et al., 2019b), where we
evaluate several fine-tuning methods. Our contributions are twofold: (1) we
show that ICL can effectively predict annotator-specific annotations
(perspectivist annotations), and that aggregating these predictions into soft
labels yields competitive performance; and (2) we argue that LDL methods are
promising for soft label predictions and merit further exploration by the
perspectivist community.

</details>


### [57] [Prompting the Market? A Large-Scale Meta-Analysis of GenAI in Finance NLP (2022-2025)](https://arxiv.org/abs/2509.09544)
*Paolo Pedinotti,Peter Baumann,Nathan Jessurun,Leslie Barrett,Enrico Santus*

Main category: cs.CL

TL;DR: The paper introduces MetaGraph, a method for extracting structured knowledge graphs from scientific literature, focusing on financial NLP research between 2022-2025.


<details>
  <summary>Details</summary>
Motivation: The rapid evolution of Large Language Models (LLMs) in financial NLP has led to diverse tasks and datasets, surpassing traditional survey capabilities and necessitating new structured methodologies for understanding research trends.

Method: The authors define an ontology, apply an LLM-based knowledge extraction pipeline to 681 financial NLP papers, and analyze the extracted data to identify research developments.

Result: The analysis identified three distinct phases in financial NLP research: initial adoption and innovation, critique on limitations, and integration of peripheral techniques into modular systems.

Conclusion: MetaGraph provides a clear lens to view the evolution of financial NLP, offers insights into research trends, and demonstrates a scalable and domain-agnostic approach for mapping scientific progress.

Abstract: Large Language Models (LLMs) have rapidly reshaped financial NLP, enabling
new tasks and driving a proliferation of datasets and diversification of data
sources. Yet, this transformation has outpaced traditional surveys. In this
paper, we present MetaGraph, a generalizable methodology for extracting
knowledge graphs from scientific literature and analyzing them to obtain a
structured, queryable view of research trends. We define an ontology for
financial NLP research and apply an LLM-based extraction pipeline to 681 papers
(2022-2025), enabling large-scale, data-driven analysis. MetaGraph reveals
three key phases: early LLM adoption and task/dataset innovation; critical
reflection on LLM limitations; and growing integration of peripheral techniques
into modular systems. This structured view offers both practitioners and
researchers a clear understanding of how financial NLP has evolved -
highlighting emerging trends, shifting priorities, and methodological
shifts-while also demonstrating a reusable approach for mapping scientific
progress in other domains.

</details>


### [58] [Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking](https://arxiv.org/abs/2509.09583)
*Brittany Harbison,Samuel Taubman,Travis Taylor,Ashok. K. Goel*

Main category: cs.CL

TL;DR: The paper proposes using GPTs for personality detection from online course forum posts to enhance SAMI's matchmaking system for student connections.


<details>
  <summary>Details</summary>
Motivation: Online course environments hinder natural social group formation, and SAMI's incomplete Theory of Mind limits its social recommendation efficiency. Personality detection may improve this.

Method: The researchers propose using GPTs in a zero-shot model to infer Big-Five personality traits from forum posts, benchmark its effectiveness, and integrate it into SAMI to improve matchmaking.

Result: Their model shows effective personality detection compared to established standards. Its incorporation into SAMI shows potential but requires additional evaluation.

Conclusion: While promising in leveraging personality for social matchmaking, further validation is needed to understand the full impact on engagement and match quality.

Abstract: Social connection is a vital part of learning, yet online course environments
present barriers to the organic formation of social groups. SAMI offers one
solution by facilitating student connections, but its effectiveness is
constrained by an incomplete Theory of Mind, limiting its ability to create an
effective mental model of a student. One facet of this is its inability to
intuit personality, which may influence the relevance of its recommendations.
To explore this, we propose a personality detection model utilizing GPTs
zero-shot capability to infer Big-Five personality traits from forum
introduction posts, often encouraged in online courses. We benchmark its
performance against established models, demonstrating its efficacy in this
task. Furthermore, we integrate this model into SAMIs entity-based matchmaking
system, enabling personality-informed social recommendations. Initial
integration suggests personality traits can complement existing matching
factors, though additional evaluation is required to determine their full
impact on student engagement and match quality.

</details>


### [59] [Fluent but Unfeeling: The Emotional Blind Spots of Language Models](https://arxiv.org/abs/2509.09593)
*Bangzhao Shu,Isha Joshi,Melissa Karnaze,Anh C. Pham,Ishita Kakkar,Sindhu Kothe,Arpine Hovasapian,Mai ElSherief*

Main category: cs.CL

TL;DR: This paper presents EXPRESS, a dataset to evaluate how well Large Language Models (LLMs) align with human-reported fine-grained emotions from Reddit posts, revealing challenges in capturing nuanced emotional contexts.


<details>
  <summary>Details</summary>
Motivation: To address the gap in evaluating whether LLMs align with human emotions beyond predefined emotion categories and to facilitate the study of fine-grained emotion understanding.

Method: The authors created the EXPRESS dataset with 251 fine-grained emotion labels from Reddit. They also developed an evaluation framework to examine LLM predictions by decomposing emotion terms into eight basic emotions based on established theories and tested LLMs using various prompts.

Result: The study found that LLMs struggle to accurately predict emotions that align with human self-disclosed emotions at a fine-grained level. LLMs often fail to capture nuanced contextual emotional cues.

Conclusion: LLMs show limitations in fine-grained emotion recognition and alignment with human self-disclosed emotions. This underscores the need for future research to enhance their contextual understanding in emotional domains.

Abstract: The versatility of Large Language Models (LLMs) in natural language
understanding has made them increasingly popular in mental health research.
While many studies explore LLMs' capabilities in emotion recognition, a
critical gap remains in evaluating whether LLMs align with human emotions at a
fine-grained level. Existing research typically focuses on classifying emotions
into predefined, limited categories, overlooking more nuanced expressions. To
address this gap, we introduce EXPRESS, a benchmark dataset curated from Reddit
communities featuring 251 fine-grained, self-disclosed emotion labels. Our
comprehensive evaluation framework examines predicted emotion terms and
decomposes them into eight basic emotions using established emotion theories,
enabling a fine-grained comparison. Systematic testing of prevalent LLMs under
various prompt settings reveals that accurately predicting emotions that align
with human self-disclosed emotions remains challenging. Qualitative analysis
further shows that while certain LLMs generate emotion terms consistent with
established emotion theories and definitions, they sometimes fail to capture
contextual cues as effectively as human self-disclosures. These findings
highlight the limitations of LLMs in fine-grained emotion alignment and offer
insights for future research aimed at enhancing their contextual understanding.

</details>


### [60] [LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death Determination](https://arxiv.org/abs/2509.09602)
*Yiqun T. Chen,Tyler H. McCormick,Li Liu,Abhirup Datta*

Main category: cs.CL

TL;DR: This paper proposes LA-VA, a pipeline combining Large Language Models (LLMs) and traditional approaches, to improve verbal autopsy accuracy for cause-of-death prediction in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: The lack of proper medical certification in resource-limited settings necessitates efficient methods like verbal autopsy to estimate causes of death.

Method: The study leverages various approaches including GPT-5 predictions, traditional algorithmic methods, embedding-based classifications, and meta-learner ensembles to process and analyze verbal autopsy data from the PHMRC dataset.

Result: GPT-5 demonstrated superior performance with test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5% (Neonate), outperforming existing baselines by 5-10%.

Conclusion: Using LLMs such as GPT-5 for verbal autopsies could greatly enhance cause-of-death predictions, aiding global health surveillance in low-resource environments.

Abstract: Verbal autopsy (VA) is a critical tool for estimating causes of death in
resource-limited settings where medical certification is unavailable. This
study presents LA-VA, a proof-of-concept pipeline that combines Large Language
Models (LLMs) with traditional algorithmic approaches and embedding-based
classification for improved cause-of-death prediction. Using the Population
Health Metrics Research Consortium (PHMRC) dataset across three age categories
(Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches:
GPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles.
Our results demonstrate that GPT-5 achieves the highest individual performance
with average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5%
(Neonate), outperforming traditional statistical machine learning baselines by
5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches
could substantially improve verbal autopsy accuracy, with important
implications for global health surveillance in low-resource settings.

</details>


### [61] [Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based Multi-Agent Systems](https://arxiv.org/abs/2509.09629)
*Minghang Zhu,Zhengliang Shi,Zhiwei Xu,Shiguang Wu,Lingjie Wang,Pengjie Ren,Zhaochun Ren,Zhumin Chen*

Main category: cs.CL

TL;DR: The paper introduces MOAT, a joint tuning framework for multi-agent systems to enhance coordination and task performance.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent systems suffer from capability gaps and poor coordination due to independent fine-tuning of specialized agents.

Method: MOAT employs iterative alignment with two stages: 'Planning Agent Alignment' and 'Grounding Agent Improving' to optimize collaboration between agents.

Result: MOAT demonstrated superior performance on six benchmarks, improving outcomes by 3.1% on held-in tasks and 4.4% on held-out tasks compared to state-of-the-art methods.

Conclusion: MOAT effectively improves multi-agent coordination, providing a systematic and convergent solution for complex task-solving systems.

Abstract: The advancement of large language models (LLMs) has enabled the construction
of multi-agent systems to solve complex tasks by dividing responsibilities
among specialized agents, such as a planning agent for subgoal generation and a
grounding agent for executing tool-use actions. Most existing methods typically
fine-tune these agents independently, leading to capability gaps among them
with poor coordination. To address this, we propose MOAT, a Multi-Agent Joint
Alignment Tuning framework that improves agents collaboration through iterative
alignment. MOAT alternates between two key stages: (1) Planning Agent
Alignment, which optimizes the planning agent to generate subgoal sequences
that better guide the grounding agent; and (2) Grounding Agent Improving, which
fine-tunes the grounding agent using diverse subgoal-action pairs generated by
the agent itself to enhance its generalization capablity. Theoretical analysis
proves that MOAT ensures a non-decreasing and progressively convergent training
process. Experiments across six benchmarks demonstrate that MOAT outperforms
state-of-the-art baselines, achieving average improvements of 3.1% on held-in
tasks and 4.4% on held-out tasks.

</details>


### [62] [All for One: LLMs Solve Mental Math at the Last Token With Information Transferred From Other Tokens](https://arxiv.org/abs/2509.09650)
*Siddarth Mamidanna,Daking Rai,Ziyu Yao,Yilun Zhou*

Main category: cs.CL

TL;DR: The authors investigate how large language models handle mental math tasks internally and reveal key late-stage computations using two new techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation is to uncover how large language models (LLMs) internally compute information for tasks like mental math and understand their specific computational pathways.

Method: The authors introduce two methods, Context-Aware Mean Ablation (CAMA) and Attention-Based Peeking (ABP), to study computational pathways in LLMs by inhibiting certain token operations and focusing computations late in the network's layers.

Result: They identify an "All-for-One subgraph" (AF1) that uses computations concentrated in the later layers exclusively at the last token. This structure is essential for high performance and generalizes across multiple models and input types.

Conclusion: The findings highlight distinct computation pathways in LLMs with AF1 being crucial for mental math tasks, offering insights into model interpretability and possibly informing future model design.

Abstract: Large language models (LLMs) demonstrate proficiency across numerous
computational tasks, yet their inner workings remain unclear. In theory, the
combination of causal self-attention and multilayer perceptron layers allows
every token to access and compute information based on all preceding tokens. In
practice, to what extent are such operations present? In this paper, on mental
math tasks (i.e., direct math calculation via next-token prediction without
explicit reasoning), we investigate this question in three steps: inhibiting
input-specific token computations in the initial layers, restricting the routes
of information transfer across token positions in the next few layers, and
forcing all computation to happen at the last token in the remaining layers.
With two proposed techniques, Context-Aware Mean Ablation (CAMA) and
Attention-Based Peeking (ABP), we identify an All-for-One subgraph (AF1) with
high accuracy on a wide variety of mental math tasks, where meaningful
computation occurs very late (in terms of layer depth) and only at the last
token, which receives information of other tokens in few specific middle
layers. Experiments on a variety of models and arithmetic expressions show that
this subgraph is sufficient and necessary for high model performance, transfers
across different models, and works on a variety of input styles. Ablations on
different CAMA and ABP alternatives reveal their unique advantages over other
methods, which may be of independent interest.

</details>


### [63] [Steering MoE LLMs via Expert (De)Activation](https://arxiv.org/abs/2509.09660)
*Mohsen Fayyaz,Ali Modarressi,Hanieh Deilamsalehy,Franck Dernoncourt,Ryan Rossi,Trung Bui,Hinrich Schütze,Nanyun Peng*

Main category: cs.CL

TL;DR: SteerMoE demonstrates how activating or deactivating specific experts in Mixture-of-Experts LLMs can steer model behaviors, enhancing or diminishing safety and faithfulness during inference without retraining.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address steering large-scale Mixture-of-Experts LLMs toward desired behaviors like improved safety and faithfulness without requiring retraining or modifying model weights.

Method: The researchers propose SteerMoE, a framework to detect experts with distinct activation patterns in LLMs and selectively (de)activate them during inference to control behavior-linked effects.

Result: SteerMoE improved safety by up to 20% and faithfulness by 27% across benchmarks, while in adversarial scenarios, it decreased safety by 41% and broke alignment safeguards when combined with jailbreak methods.

Conclusion: SteerMoE unveils an effective method for steering expert behaviors in LLMs, highlighting its pros for ethical guidance and risks around alignment vulnerabilities when used maliciously.

Abstract: Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token
through a subset of specialized Feed-Forward Networks (FFN), known as experts.
We present SteerMoE, a framework for steering MoE models by detecting and
controlling behavior-linked experts. Our detection method identifies experts
with distinct activation patterns across paired inputs exhibiting contrasting
behaviors. By selectively (de)activating such experts during inference, we
control behaviors like faithfulness and safety without retraining or modifying
weights. Across 11 benchmarks and 6 LLMs, our steering raises safety by up to
+20% and faithfulness by +27%. In adversarial attack mode, it drops safety by
-41% alone, and -100% when combined with existing jailbreak methods, bypassing
all safety guardrails and exposing a new dimension of alignment faking hidden
within experts.

</details>


### [64] [CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2509.09675)
*Runpeng Dai,Linfeng Song,Haolin Liu,Zhenwen Liang,Dian Yu,Haitao Mi,Zhaopeng Tu,Rui Liu,Tong Zheng,Hongtu Zhu,Dong Yu*

Main category: cs.CL

TL;DR: The paper introduces Curiosity-Driven Exploration (CDE) to address exploration issues in RLVR methods, achieving better results using intrinsic curiosity signals.


<details>
  <summary>Details</summary>
Motivation: Current RLVR methods suffer from poor exploration, leading to premature convergence and entropy collapse in LLMs.

Method: CDE framework utilizes intrinsic curiosity signals from the actor's perplexity and critic's value estimate variance as exploration bonuses.

Result: Empirical evaluations show approximately +3 point improvement on AIME benchmarks and deeper insights on calibration collapse in RLVR.

Conclusion: CDE enhances RLVR frameworks, mitigating issues like overconfident errors and calibration collapse, while improving performance in LLM reasoning tasks.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm
for enhancing the reasoning ability of Large Language Models (LLMs). Yet
current RLVR methods often explore poorly, leading to premature convergence and
entropy collapse. To address this challenge, we introduce Curiosity-Driven
Exploration (CDE), a framework that leverages the model's own intrinsic sense
of curiosity to guide exploration. We formalize curiosity with signals from
both the actor and the critic: for the actor, we use perplexity over its
generated response, and for the critic, we use the variance of value estimates
from a multi-head architecture. Both signals serve as an exploration bonus
within the RLVR framework to guide the model. Our theoretical analysis shows
that the actor-wise bonus inherently penalizes overconfident errors and
promotes diversity among correct responses; moreover, we connect the
critic-wise bonus to the well-established count-based exploration bonus in RL.
Empirically, our method achieves an approximate +3 point improvement over
standard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a
calibration collapse mechanism within RLVR, shedding light on common LLM
failure modes.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [65] [Recurrence Meets Transformers for Universal Multimodal Retrieval](https://arxiv.org/abs/2509.08897)
*Davide Caffagni,Sara Sarto,Marcella Cornia,Lorenzo Baraldi,Rita Cucchiara*

Main category: cs.CV

TL;DR: ReT-2 is a unified multimodal retrieval model supporting queries with both text and images, and achieves state-of-the-art performance while improving computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of existing retrieval methods which focus on task-specific fine-tuning and are restricted to single-modality queries or documents.

Method: The approach relies on multi-layer representations and a recurrent Transformer with LSTM-inspired gating mechanisms, seamlessly integrating multi-modal data across layers.

Result: ReT-2 demonstrates state-of-the-art performance on M2KR, M-BEIR, Encyclopedic-VQA, and InfoSeek benchmarks, while being faster and memory-efficient compared to existing methods.

Conclusion: ReT-2 establishes itself as a robust and efficient choice for complex multimodal retrieval tasks, enhancing downstream applications and advancing the field with publicly available resources.

Abstract: With the rapid advancement of multimodal retrieval and its application in
LLMs and multimodal LLMs, increasingly complex retrieval tasks have emerged.
Existing methods predominantly rely on task-specific fine-tuning of
vision-language models and are limited to single-modality queries or documents.
In this paper, we propose ReT-2, a unified retrieval model that supports
multimodal queries, composed of both images and text, and searches across
multimodal document collections where text and images coexist. ReT-2 leverages
multi-layer representations and a recurrent Transformer architecture with
LSTM-inspired gating mechanisms to dynamically integrate information across
layers and modalities, capturing fine-grained visual and textual details. We
evaluate ReT-2 on the challenging M2KR and M-BEIR benchmarks across different
retrieval configurations. Results demonstrate that ReT-2 consistently achieves
state-of-the-art performance across diverse settings, while offering faster
inference and reduced memory usage compared to prior approaches. When
integrated into retrieval-augmented generation pipelines, ReT-2 also improves
downstream performance on Encyclopedic-VQA and InfoSeek datasets. Our source
code and trained models are publicly available at:
https://github.com/aimagelab/ReT-2

</details>


### [66] [Diffusion-Based Action Recognition Generalizes to Untrained Domains](https://arxiv.org/abs/2509.08908)
*Rogerio Guimaraes,Frank Xiao,Pietro Perona,Markus Marks*

Main category: cs.CV

TL;DR: This paper introduces a method using Vision Diffusion Model features aggregated via a transformer to improve action recognition across varying contexts and viewpoints.


<details>
  <summary>Details</summary>
Motivation: Humans can recognize actions across diverse contexts and viewpoints, but deep learning models struggle with generalization in similar scenarios.

Method: The approach employs features from a Vision Diffusion Model conditioned on earlier diffusion timesteps, aggregated via a transformer, to emphasize semantic information over pixel-based details.

Result: The model achieves state-of-the-art performance in recognizing actions across species, viewpoints, and contexts, outperforming prior benchmarks.

Conclusion: The presented method significantly improves action recognition robustness, aligning machine capabilities closer to human-like generalization.

Abstract: Humans can recognize the same actions despite large context and viewpoint
variations, such as differences between species (walking in spiders vs.
horses), viewpoints (egocentric vs. third-person), and contexts (real life vs
movies). Current deep learning models struggle with such generalization. We
propose using features generated by a Vision Diffusion Model (VDM), aggregated
via a transformer, to achieve human-like action recognition across these
challenging conditions. We find that generalization is enhanced by the use of a
model conditioned on earlier timesteps of the diffusion process to highlight
semantic information over pixel level details in the extracted features. We
experimentally explore the generalization properties of our approach in
classifying actions across animal species, across different viewing angles, and
different recording contexts. Our model sets a new state-of-the-art across all
three generalization benchmarks, bringing machine action recognition closer to
human-like robustness. Project page:
$\href{https://www.vision.caltech.edu/actiondiff/}{\texttt{vision.caltech.edu/actiondiff}}$
Code:
$\href{https://github.com/frankyaoxiao/ActionDiff}{\texttt{github.com/frankyaoxiao/ActionDiff}}$

</details>


### [67] [PromptGuard: An Orchestrated Prompting Framework for Principled Synthetic Text Generation for Vulnerable Populations using LLMs with Enhanced Safety, Fairness, and Controllability](https://arxiv.org/abs/2509.08910)
*Tung Vu,Lam Nguyen,Quynh Dao*

Main category: cs.CV

TL;DR: The paper introduces PromptGuard, a new prompting framework to prevent harmful outputs from LLMs at the source, featuring a novel technique called VulnGuard Prompt based on data-driven contrastive learning.


<details>
  <summary>Details</summary>
Motivation: The rise of LLMs has brought significant risks such as generating harmful or biased content, adversely affecting vulnerable populations like LGBTQ+ individuals or marginalized communities.

Method: The proposed method, PromptGuard, includes the VulnGuard Prompt technique driven by contrastive learning with few-shot examples, ethical reasoning, and adaptive prompting. It also incorporates modules like classification, output validation, and user interaction.

Result: PromptGuard achieves a 25-30% reduction in harm based on entropy bounds and optimization metrics. The framework was validated with proofs of convergence and vulnerability analysis using data-driven approaches.

Conclusion: PromptGuard offers a proactive, modular approach to real-time harm prevention in LLM outputs, establishing a foundation for future systematic research and application.

Abstract: The proliferation of Large Language Models (LLMs) in real-world applications
poses unprecedented risks of generating harmful, biased, or misleading
information to vulnerable populations including LGBTQ+ individuals, single
parents, and marginalized communities. While existing safety approaches rely on
post-hoc filtering or generic alignment techniques, they fail to proactively
prevent harmful outputs at the generation source. This paper introduces
PromptGuard, a novel modular prompting framework with our breakthrough
contribution: VulnGuard Prompt, a hybrid technique that prevents harmful
information generation using real-world data-driven contrastive learning.
VulnGuard integrates few-shot examples from curated GitHub repositories,
ethical chain-of-thought reasoning, and adaptive role-prompting to create
population-specific protective barriers. Our framework employs theoretical
multi-objective optimization with formal proofs demonstrating 25-30% analytical
harm reduction through entropy bounds and Pareto optimality. PromptGuard
orchestrates six core modules: Input Classification, VulnGuard Prompting,
Ethical Principles Integration, External Tool Interaction, Output Validation,
and User-System Interaction, creating an intelligent expert system for
real-time harm prevention. We provide comprehensive mathematical formalization
including convergence proofs, vulnerability analysis using information theory,
and theoretical validation framework using GitHub-sourced datasets,
establishing mathematical foundations for systematic empirical research.

</details>


### [68] [Similarity-based Outlier Detection for Noisy Object Re-Identification Using Beta Mixtures](https://arxiv.org/abs/2509.08926)
*Waqar Ahmad,Evan Murphy,Vladimir A. Krylov*

Main category: cs.CV

TL;DR: This paper proposes a robust object re-identification method addressing label noise using a Siamese network and a novel statistical outlier detection framework called Beta-SOD.


<details>
  <summary>Details</summary>
Motivation: To solve the significant performance degradation in object re-identification methods caused by label noise.

Method: The authors use a Siamese network for supervised image similarity and introduce Beta-SOD, a framework that models the distribution of cosine similarities using a two-component Beta distribution mixture model.

Result: The method achieves superior performance compared to state-of-the-art methods in noisy Re-ID tasks across datasets like CUHK03, Market-1501, and VeRi-776, under various noise levels (10%-30%).

Conclusion: Beta-SOD effectively denoises and enhances object re-identification tasks, providing robustness and wide applicability in noisy scenarios.

Abstract: Object re-identification (Re-ID) methods are highly sensitive to label noise,
which typically leads to significant performance degradation. We address this
challenge by reframing Re-ID as a supervised image similarity task and adopting
a Siamese network architecture trained to capture discriminative pairwise
relationships. Central to our approach is a novel statistical outlier detection
(OD) framework, termed Beta-SOD (Beta mixture Similarity-based Outlier
Detection), which models the distribution of cosine similarities between
embedding pairs using a two-component Beta distribution mixture model. We
establish a novel identifiability result for mixtures of two Beta
distributions, ensuring that our learning task is well-posed.The proposed OD
step complements the Re-ID architecture combining binary cross-entropy,
contrastive, and cosine embedding losses that jointly optimize feature-level
similarity learning.We demonstrate the effectiveness of Beta-SOD in de-noising
and Re-ID tasks for person Re-ID, on CUHK03 and Market-1501 datasets, and
vehicle Re-ID, on VeRi-776 dataset. Our method shows superior performance
compared to the state-of-the-art methods across various noise levels (10-30\%),
demonstrating both robustness and broad applicability in noisy Re-ID scenarios.
The implementation of Beta-SOD is available at:
https://github.com/waqar3411/Beta-SOD

</details>


### [69] [SFD-Mamba2Net: Strcture-Guided Frequency-Enhanced Dual-Stream Mamba2 Network for Coronary Artery Segmentation](https://arxiv.org/abs/2509.08934)
*Nan Mu,Ruiqi Song,Zhihui Xu,Jingfeng Jiang,Chen Zhao*

Main category: cs.CV

TL;DR: The paper introduces SFD-Mamba2Net, a framework for improved coronary artery segmentation and stenosis detection in ICA images, addressing challenges like low contrast and noise.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges posed by low contrast, noise, and fine-grained vascular structures in ICA images, in order to improve CAD diagnosis using advanced segmentation and detection techniques.

Method: The proposed method, SFD-Mamba2Net, includes a Curvature-Aware Structural Enhancement module in the encoder for highlighting vascular structures and a Progressive High-Frequency Perception module in the decoder for refining high-frequency details using wavelet decomposition.

Result: SFD-Mamba2Net outperformed state-of-the-art methods across eight metrics for segmentation and showed the highest accuracy metrics for stenosis detection.

Conclusion: SFD-Mamba2Net demonstrates significant advancements in vascular segmentation and stenosis detection, making it a promising tool for improving clinical CAD diagnosis.

Abstract: Background: Coronary Artery Disease (CAD) is one of the leading causes of
death worldwide. Invasive Coronary Angiography (ICA), regarded as the gold
standard for CAD diagnosis, necessitates precise vessel segmentation and
stenosis detection. However, ICA images are typically characterized by low
contrast, high noise levels, and complex, fine-grained vascular structures,
which pose significant challenges to the clinical adoption of existing
segmentation and detection methods. Objective: This study aims to improve the
accuracy of coronary artery segmentation and stenosis detection in ICA images
by integrating multi-scale structural priors, state-space-based long-range
dependency modeling, and frequency-domain detail enhancement strategies.
Methods: We propose SFD-Mamba2Net, an end-to-end framework tailored for
ICA-based vascular segmentation and stenosis detection. In the encoder, a
Curvature-Aware Structural Enhancement (CASE) module is embedded to leverage
multi-scale responses for highlighting slender tubular vascular structures,
suppressing background interference, and directing attention toward vascular
regions. In the decoder, we introduce a Progressive High-Frequency Perception
(PHFP) module that employs multi-level wavelet decomposition to progressively
refine high-frequency details while integrating low-frequency global
structures. Results and Conclusions: SFD-Mamba2Net consistently outperformed
state-of-the-art methods across eight segmentation metrics, and achieved the
highest true positive rate and positive predictive value in stenosis detection.

</details>


### [70] [Live(r) Die: Predicting Survival in Colorectal Liver Metastasis](https://arxiv.org/abs/2509.08935)
*Muhammad Alberb,Helen Cheung,Anne Martel*

Main category: cs.CV

TL;DR: The paper introduces an automated framework combining segmentation and radiomics pipelines for predicting surgical outcomes in colorectal liver metastasis (CRLM), achieving significant accuracy improvements over existing biomarkers.


<details>
  <summary>Details</summary>
Motivation: Current prognostic models for colorectal liver metastasis (CRLM) lack predictive power, particularly for multifocal cases, necessitating more accurate and annotation-efficient solutions.

Method: The framework uses MRI-based segmentation combined with radiomics analysis. It includes SAMONAI for 3D segmentation leveraging the Segment Anything Model and SurvAMINN for survival prediction based on right-censored data.

Result: The framework yields a C-index improvement exceeding 10% over existing prognostic biomarkers, as demonstrated on a dataset of 227 patients.

Conclusion: The integration of segmentation algorithms with radiomics analysis shows promise for enhancing surgical outcome predictions in CRLM, improving accuracy and interpretability.

Abstract: Colorectal cancer frequently metastasizes to the liver, significantly
reducing long-term survival. While surgical resection is the only potentially
curative treatment for colorectal liver metastasis (CRLM), patient outcomes
vary widely depending on tumor characteristics along with clinical and genomic
factors. Current prognostic models, often based on limited clinical or
molecular features, lack sufficient predictive power, especially in multifocal
CRLM cases. We present a fully automated framework for surgical outcome
prediction from pre- and post-contrast MRI acquired before surgery. Our
framework consists of a segmentation pipeline and a radiomics pipeline. The
segmentation pipeline learns to segment the liver, tumors, and spleen from
partially annotated data by leveraging promptable foundation models to complete
missing labels. Also, we propose SAMONAI, a novel zero-shot 3D prompt
propagation algorithm that leverages the Segment Anything Model to segment 3D
regions of interest from a single point prompt, significantly improving our
segmentation pipeline's accuracy and efficiency. The predicted pre- and
post-contrast segmentations are then fed into our radiomics pipeline, which
extracts features from each tumor and predicts survival using SurvAMINN, a
novel autoencoder-based multiple instance neural network for survival analysis.
SurvAMINN jointly learns dimensionality reduction and hazard prediction from
right-censored survival data, focusing on the most aggressive tumors. Extensive
evaluation on an institutional dataset comprising 227 patients demonstrates
that our framework surpasses existing clinical and genomic biomarkers,
delivering a C-index improvement exceeding 10%. Our results demonstrate the
potential of integrating automated segmentation algorithms and radiomics-based
survival analysis to deliver accurate, annotation-efficient, and interpretable
outcome prediction in CRLM.

</details>


### [71] [Discovering Divergent Representations between Text-to-Image Models](https://arxiv.org/abs/2509.08940)
*Lisa Dunlap,Joseph E. Gonzalez,Trevor Darrell,Fabian Caba Heilbron,Josef Sivic,Bryan Russell*

Main category: cs.CV

TL;DR: This research investigates the divergence in visual representations generated by different text-to-image models using a method called CompCon to identify varying visual attributes and prompt types.


<details>
  <summary>Details</summary>
Motivation: To understand and characterize the differences in visual outputs generated by distinct text-to-image models, particularly focusing on visual attributes and prompts where the divergence occurs.

Method: The researchers developed an algorithm called CompCon (Comparing Concepts), which uses an evolutionary search approach to identify visual attributes and prompt-related differences between generative models. They also created a dataset, ID2, tailored to evaluate these differences.

Result: CompCon effectively identifies diverging visual representations between models. For example, PixArt associates loneliness with wet streets, whereas Stable Diffusion 3.5 shows biases such as depicting African Americans in media occupations. Comparisons with baseline methods showed CompCon's efficacy.

Conclusion: CompCon offers a novel approach for systematically identifying and analyzing visual and prompt-related differences between text-to-image generative models, providing insights into their underlying conceptual representations.

Abstract: In this paper, we investigate when and how visual representations learned by
two different generative models diverge. Given two text-to-image models, our
goal is to discover visual attributes that appear in images generated by one
model but not the other, along with the types of prompts that trigger these
attribute differences. For example, "flames" might appear in one model's
outputs when given prompts expressing strong emotions, while the other model
does not produce this attribute given the same prompts. We introduce CompCon
(Comparing Concepts), an evolutionary search algorithm that discovers visual
attributes more prevalent in one model's output than the other, and uncovers
the prompt concepts linked to these visual differences. To evaluate CompCon's
ability to find diverging representations, we create an automated data
generation pipeline to produce ID2, a dataset of 60 input-dependent
differences, and compare our approach to several LLM- and VLM-powered
baselines. Finally, we use CompCon to compare popular text-to-image models,
finding divergent representations such as how PixArt depicts prompts mentioning
loneliness with wet streets and Stable Diffusion 3.5 depicts African American
people in media professions. Code at: https://github.com/adobe-research/CompCon

</details>


### [72] [An U-Net-Based Deep Neural Network for Cloud Shadow and Sun-Glint Correction of Unmanned Aerial System (UAS) Imagery](https://arxiv.org/abs/2509.08949)
*Yibin Wang,Wondimagegn Beshah,Padmanava Dash,Haifeng Wang*

Main category: cs.CV

TL;DR: UAS imagery advancements face issues like cloud shadows and sun glint. This study introduces a U-Net deep learning model to address these challenges, enabling image correction and recovery.


<details>
  <summary>Details</summary>
Motivation: The need to address the limitations of UAS imagery, specifically cloud shadows and sun glint, hindered its effectiveness in applications like water quality assessment.

Method: A U-Net based deep learning model was used to identify and mask cloud shadow and sun glint areas in UAS images. Pixel-level data extraction and thorough evaluation helped in determining the best training settings.

Result: The study developed a high-quality image correction model capable of recovering regions obstructed by cloud shadows and sun glint, achieving accurate image restoration.

Conclusion: The proposed solution successfully improves the utility of UAS imagery in remote sensing by mitigating the effects of cloud shadows and sun glint, enhancing its potential use in water quality studies.

Abstract: The use of unmanned aerial systems (UASs) has increased tremendously in the
current decade. They have significantly advanced remote sensing with the
capability to deploy and image the terrain as per required spatial, spectral,
temporal, and radiometric resolutions for various remote sensing applications.
One of the major advantages of UAS imagery is that images can be acquired in
cloudy conditions by flying the UAS under the clouds. The limitation to the
technology is that the imagery is often sullied by cloud shadows. Images taken
over water are additionally affected by sun glint. These are two pose serious
issues for estimating water quality parameters from the UAS images. This study
proposes a novel machine learning approach first to identify and extract
regions with cloud shadows and sun glint and separate such regions from
non-obstructed clear sky regions and sun-glint unaffected regions. The data was
extracted from the images at pixel level to train an U-Net based deep learning
model and best settings for model training was identified based on the various
evaluation metrics from test cases. Using this evaluation, a high-quality image
correction model was determined, which was used to recover the cloud shadow and
sun glint areas in the images.

</details>


### [73] [CoSwin: Convolution Enhanced Hierarchical Shifted Window Attention For Small-Scale Vision](https://arxiv.org/abs/2509.08959)
*Puskal Khadka,Rodrigue Rizk,Longwei Wang,KC Santosh*

Main category: cs.CV

TL;DR: This paper introduces CoSwin, a hybrid architecture combining localized convolutional learning with shifted window attention for improved feature extraction and performance, particularly on small datasets.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers struggle with small datasets due to the lack of inductive biases like locality and translation equivariance. The paper aims to enhance their capability to capture both local and global features.

Method: The proposed method integrates a learnable local feature enhancement module into each attention block of the shifted window architecture, fusing fine-grained spatial details and global semantic information.

Result: CoSwin demonstrates consistent performance improvements over state-of-the-art models across multiple benchmarks, achieving up to 4.92% improvement on CIFAR-100 and 4.47% on Tiny ImageNet.

Conclusion: CoSwin is effective in enhancing transformers' generalization and robustness for small-scale vision tasks by leveraging local-global feature fusion.

Abstract: Vision Transformers (ViTs) have achieved impressive results in computer
vision by leveraging self-attention to model long-range dependencies. However,
their emphasis on global context often comes at the expense of local feature
extraction in small datasets, particularly due to the lack of key inductive
biases such as locality and translation equivariance. To mitigate this, we
propose CoSwin, a novel feature-fusion architecture that augments the
hierarchical shifted window attention with localized convolutional feature
learning. Specifically, CoSwin integrates a learnable local feature enhancement
module into each attention block, enabling the model to simultaneously capture
fine-grained spatial details and global semantic structure. We evaluate CoSwin
on multiple image classification benchmarks including CIFAR-10, CIFAR-100,
MNIST, SVHN, and Tiny ImageNet. Our experimental results show consistent
performance gains over state-of-the-art convolutional and transformer-based
models. Notably, CoSwin achieves improvements of 2.17% on CIFAR-10, 4.92% on
CIFAR-100, 0.10% on MNIST, 0.26% on SVHN, and 4.47% on Tiny ImageNet over the
baseline Swin Transformer. These improvements underscore the effectiveness of
local-global feature fusion in enhancing the generalization and robustness of
transformers for small-scale vision. Code and pretrained weights available at
https://github.com/puskal-khadka/coswin

</details>


### [74] [iMatcher: Improve matching in point cloud registration via local-to-global geometric consistency learning](https://arxiv.org/abs/2509.08982)
*Karim Slimani,Catherine Achard,Brahim Tamadazte*

Main category: cs.CV

TL;DR: This paper introduces iMatcher, a framework using learned features for efficient and robust point cloud registration, achieving state-of-the-art results in various datasets.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in rigid point cloud registration by incorporating both local and global geometrically consistent feature matching.

Method: iMatcher uses a local graph embedding module for score initialization, bilateral matching through nearest neighbors, and global geometric consistency learning to predict point-wise matching probabilities.

Result: iMatcher showed significant performance improvements across outdoor and indoor datasets, achieving inlier ratios of 95%-97% on KITTI, 94%-97% on KITTI-360, and up to 81.1% on 3DMatch.

Conclusion: The proposed method demonstrates robustness and significantly enhances feature matching for point cloud registration, establishing a new benchmark in inlier ratio performance.

Abstract: This paper presents iMatcher, a fully differentiable framework for feature
matching in point cloud registration. The proposed method leverages learned
features to predict a geometrically consistent confidence matrix, incorporating
both local and global consistency. First, a local graph embedding module leads
to an initialization of the score matrix. A subsequent repositioning step
refines this matrix by considering bilateral source-to-target and
target-to-source matching via nearest neighbor search in 3D space. The paired
point features are then stacked together to be refined through global geometric
consistency learning to predict a point-wise matching probability. Extensive
experiments on real-world outdoor (KITTI, KITTI-360) and indoor (3DMatch)
datasets, as well as on 6-DoF pose estimation (TUD-L) and partial-to-partial
matching (MVP-RG), demonstrate that iMatcher significantly improves rigid
registration performance. The method achieves state-of-the-art inlier ratios,
scoring 95% - 97% on KITTI, 94% - 97% on KITTI-360, and up to 81.1% on 3DMatch,
highlighting its robustness across diverse settings.

</details>


### [75] [UltrON: Ultrasound Occupancy Networks](https://arxiv.org/abs/2509.08991)
*Magdalena Wysocki,Felix Duelmer,Ananya Bal,Nassir Navab,Mohammad Farid Azampour*

Main category: cs.CV

TL;DR: The paper introduces UltrON, an occupancy-based representation model utilizing acoustic features to enhance 3D ultrasound shape reconstruction.


<details>
  <summary>Details</summary>
Motivation: Sonographers struggle to mentally integrate 2D ultrasound images into 3D shapes, compounded by issues like occlusions and annotation dependency.

Method: UltrON leverages acoustic features from B-mode ultrasound images in a weakly-supervised regime and uses a novel loss function addressing view-dependency.

Result: The approach improves geometric consistency, mitigates issues from occlusions, annotation reliance, and generalizes across the same anatomy.

Conclusion: UltrON enables more accurate 3D reconstructions in ultrasound imaging, benefiting sonographers and the broader medical field, with open-source code forthcoming.

Abstract: In free-hand ultrasound imaging, sonographers rely on expertise to mentally
integrate partial 2D views into 3D anatomical shapes. Shape reconstruction can
assist clinicians in this process. Central to this task is the choice of shape
representation, as it determines how accurately and efficiently the structure
can be visualized, analyzed, and interpreted. Implicit representations, such as
SDF and occupancy function, offer a powerful alternative to traditional voxel-
or mesh-based methods by modeling continuous, smooth surfaces with compact
storage, avoiding explicit discretization. Recent studies demonstrate that SDF
can be effectively optimized using annotations derived from segmented B-mode
ultrasound images. Yet, these approaches hinge on precise annotations,
overlooking the rich acoustic information embedded in B-mode intensity.
Moreover, implicit representation approaches struggle with the ultrasound's
view-dependent nature and acoustic shadowing artifacts, which impair
reconstruction. To address the problems resulting from occlusions and
annotation dependency, we propose an occupancy-based representation and
introduce \gls{UltrON} that leverages acoustic features to improve geometric
consistency in weakly-supervised optimization regime. We show that these
features can be obtained from B-mode images without additional annotation cost.
Moreover, we propose a novel loss function that compensates for view-dependency
in the B-mode images and facilitates occupancy optimization from multiview
ultrasound. By incorporating acoustic properties, \gls{UltrON} generalizes to
shapes of the same anatomy. We show that \gls{UltrON} mitigates the limitations
of occlusions and sparse labeling and paves the way for more accurate 3D
reconstruction. Code and dataset will be available at
https://github.com/magdalena-wysocki/ultron.

</details>


### [76] [Implicit Neural Representations of Intramyocardial Motion and Strain](https://arxiv.org/abs/2509.09004)
*Andrew Bell,Yan Kit Choi,Steffen Peterson,Andrew King,Muhummad Sohaib Nazir,Alistair Young*

Main category: cs.CV

TL;DR: The study proposes an implicit neural representation (INR) based method for precise and fast left ventricular motion and strain analysis from tagged MRI data.


<details>
  <summary>Details</summary>
Motivation: To address challenges in automating intramyocardial motion and strain quantification from tagging MRI, which is crucial for analyzing left ventricular dynamics.

Method: The proposed method employs implicit neural representations (INRs) conditioned on learned latent codes, eliminating the need for inference-time optimization.

Result: The method achieved the best tracking accuracy (2.14 mm RMSE) and lowest strain errors (global circumferential: 2.86%, radial: 6.42%). It is about 380 times faster than the most accurate baseline method.

Conclusion: INR-based models offer accurate, efficient, and scalable solutions for myocardial strain analysis, making them suitable for large CMR datasets.

Abstract: Automatic quantification of intramyocardial motion and strain from tagging
MRI remains an important but challenging task. We propose a method using
implicit neural representations (INRs), conditioned on learned latent codes, to
predict continuous left ventricular (LV) displacement -- without requiring
inference-time optimisation. Evaluated on 452 UK Biobank test cases, our method
achieved the best tracking accuracy (2.14 mm RMSE) and the lowest combined
error in global circumferential (2.86%) and radial (6.42%) strain compared to
three deep learning baselines. In addition, our method is $\sim$380$\times$
faster than the most accurate baseline. These results highlight the suitability
of INR-based models for accurate and scalable analysis of myocardial strain in
large CMR datasets.

</details>


### [77] [E-MLNet: Enhanced Mutual Learning for Universal Domain Adaptation with Sample-Specific Weighting](https://arxiv.org/abs/2509.09006)
*Samuel Felipe dos Santos,Tiago Agostinho de Almeida,Jurandy Almeida*

Main category: cs.CV

TL;DR: Enhanced Mutual Learning Network (E-MLNet) improves universal domain adaptation using dynamic weighting for better classification of known and unknown classes.


<details>
  <summary>Details</summary>
Motivation: Address limitations in universal domain adaptation methods by refining how classifiers distinguish between known and unknown classes.

Method: E-MLNet integrates a dynamic weighting strategy into Open-set Entropy Minimization (OEM), leveraging predictions to focus adaptation on relevant class boundaries.

Result: E-MLNet achieved top average H-scores on VisDA and ImageCLEF and surpassed MLNet in most adaptation tasks across benchmarks.

Conclusion: The dynamic weighting approach sharpens class boundary adaptation, enhancing robustness and accuracy over previous methods.

Abstract: Universal Domain Adaptation (UniDA) seeks to transfer knowledge from a
labeled source to an unlabeled target domain without assuming any relationship
between their label sets, requiring models to classify known samples while
rejecting unknown ones. Advanced methods like Mutual Learning Network (MLNet)
use a bank of one-vs-all classifiers adapted via Open-set Entropy Minimization
(OEM). However, this strategy treats all classifiers equally, diluting the
learning signal. We propose the Enhanced Mutual Learning Network (E-MLNet),
which integrates a dynamic weighting strategy to OEM. By leveraging the
closed-set classifier's predictions, E-MLNet focuses adaptation on the most
relevant class boundaries for each target sample, sharpening the distinction
between known and unknown classes. We conduct extensive experiments on four
challenging benchmarks: Office-31, Office-Home, VisDA-2017, and ImageCLEF. The
results demonstrate that E-MLNet achieves the highest average H-scores on VisDA
and ImageCLEF and exhibits superior robustness over its predecessor. E-MLNet
outperforms the strong MLNet baseline in the majority of individual adaptation
tasks -- 22 out of 31 in the challenging Open-Partial DA setting and 19 out of
31 in the Open-Set DA setting -- confirming the benefits of our focused
adaptation strategy.

</details>


### [78] [COCO-Urdu: A Large-Scale Urdu Image-Caption Dataset with Multimodal Quality Estimation](https://arxiv.org/abs/2509.09014)
*Umair Hassan*

Main category: cs.CV

TL;DR: The paper introduces COCO-Urdu, a large-scale dataset addressing language bias in vision-language systems by focusing on Urdu captions.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models suffer biases favoring high-resource languages, leaving Urdu under-represented.

Method: COCO-Urdu was developed using translations of MS COCO captions validated through a multimodal quality estimation framework.

Result: COCO-Urdu consists of 59,000 images and 319,000 Urdu captions with strong benchmark performance.

Conclusion: Releasing COCO-Urdu and its quality pipeline establishes a resource for more inclusive vision-language research.

Abstract: Urdu, spoken by over 250 million people, remains critically under-served in
multimodal and vision-language research. The absence of large-scale,
high-quality datasets has limited the development of Urdu-capable systems and
reinforced biases in multilingual vision-language models trained primarily on
high-resource languages. To address this gap, we present COCO-Urdu, a
large-scale image-caption dataset derived from MS COCO, containing 59,000
images and 319,000 Urdu captions selected through stratified sampling to
preserve the original distribution. Captions were translated using SeamlessM4T
v2 and validated with a hybrid multimodal quality estimation framework that
integrates COMET-Kiwi for translation quality, CLIP-based similarity for visual
grounding, and BERTScore with back-translation for semantic consistency;
low-scoring captions were iteratively refined using open-source large language
models. We further benchmark COCO-Urdu on BLEU, SacreBLEU, and chrF, reporting
consistently strong results. To the best of our knowledge, COCO-Urdu is the
largest publicly available Urdu captioning dataset. By releasing both the
dataset and the quality estimation pipeline, we aim to reduce language bias in
multimodal research and establish a foundation for inclusive vision-language
systems.

</details>


### [79] [VoxelFormer: Parameter-Efficient Multi-Subject Visual Decoding from fMRI](https://arxiv.org/abs/2509.09015)
*Chenqian Le,Yilin Zhao,Nikasadat Emami,Kushagra Yadav,Xujin "Chris" Liu,Xupeng Chen,Yao Wang*

Main category: cs.CV

TL;DR: VoxelFormer introduces a transformer architecture for fMRI-based visual decoding, addressing scalability via multi-subject training rather than subject-specific models.


<details>
  <summary>Details</summary>
Motivation: Existing visual decoding methods rely heavily on subject-specific training, limiting their scalability and practical application. There's a need for a universal approach that works efficiently across multiple subjects.

Method: VoxelFormer combines a Token Merging Transformer (ToMer) for voxel compression with a query-driven Q-Former to create neural representations aligned with the CLIP image embedding space.

Result: VoxelFormer demonstrated competitive image retrieval performance on multi-subject training data from the 7T Natural Scenes Dataset, using significantly fewer parameters compared to existing methods.

Conclusion: Token merging and query-based transformer architectures, such as VoxelFormer, offer efficient and scalable solutions for neural decoding across subjects, paving the way for broader application in visual decoding.

Abstract: Recent advances in fMRI-based visual decoding have enabled compelling
reconstructions of perceived images. However, most approaches rely on
subject-specific training, limiting scalability and practical deployment. We
introduce \textbf{VoxelFormer}, a lightweight transformer architecture that
enables multi-subject training for visual decoding from fMRI. VoxelFormer
integrates a Token Merging Transformer (ToMer) for efficient voxel compression
and a query-driven Q-Former that produces fixed-size neural representations
aligned with the CLIP image embedding space. Evaluated on the 7T Natural Scenes
Dataset, VoxelFormer achieves competitive retrieval performance on subjects
included during training with significantly fewer parameters than existing
methods. These results highlight token merging and query-based transformers as
promising strategies for parameter-efficient neural decoding.

</details>


### [80] [Integrating Anatomical Priors into a Causal Diffusion Model](https://arxiv.org/abs/2509.09054)
*Binxu Li,Wei Peng,Mingjie Li,Ehsan Adeli,Kilian M. Pohl*

Main category: cs.CV

TL;DR: The paper proposes a new framework, PCGM, to generate anatomically plausible synthetic brain MRIs focusing on subtle, medically relevant variations using a generative diffusion model with anatomical constraints.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in enhancing counterfactual brain MRI synthesis to aid studies of subtle morphometric differences, addressing limitations in maintaining fine-grain anatomical details in existing approaches.

Method: The PCGM framework integrates voxel-level anatomical constraints into a generative diffusion framework using a probabilistic graph module. It employs a 3D ControlNet to encode spatial masks, coupled with a counterfactual denoising UNet and a 3D diffusion decoder.

Result: PCGM enables the generation of high-quality, anatomically accurate synthetic brain MRIs. Results demonstrate its effectiveness in reflecting disease-related variations matching neuroscientific findings.

Conclusion: PCGM marks progress in synthetic MRI generation for research into subtle brain differences, showing promise for replicating disease effects and aiding morphometric analysis in neuroscience.

Abstract: 3D brain MRI studies often examine subtle morphometric differences between
cohorts that are hard to detect visually. Given the high cost of MRI
acquisition, these studies could greatly benefit from image syntheses,
particularly counterfactual image generation, as seen in other domains, such as
computer vision. However, counterfactual models struggle to produce
anatomically plausible MRIs due to the lack of explicit inductive biases to
preserve fine-grained anatomical details. This shortcoming arises from the
training of the models aiming to optimize for the overall appearance of the
images (e.g., via cross-entropy) rather than preserving subtle, yet medically
relevant, local variations across subjects. To preserve subtle variations, we
propose to explicitly integrate anatomical constraints on a voxel-level as
prior into a generative diffusion framework. Called Probabilistic Causal Graph
Model (PCGM), the approach captures anatomical constraints via a probabilistic
graph module and translates those constraints into spatial binary masks of
regions where subtle variations occur. The masks (encoded by a 3D extension of
ControlNet) constrain a novel counterfactual denoising UNet, whose encodings
are then transferred into high-quality brain MRIs via our 3D diffusion decoder.
Extensive experiments on multiple datasets demonstrate that PCGM generates
structural brain MRIs of higher quality than several baseline approaches.
Furthermore, we show for the first time that brain measurements extracted from
counterfactuals (generated by PCGM) replicate the subtle effects of a disease
on cortical brain regions previously reported in the neuroscience literature.
This achievement is an important milestone in the use of synthetic MRIs in
studies investigating subtle morphological differences.

</details>


### [81] [Enhancing 3D Medical Image Understanding with Pretraining Aided by 2D Multimodal Large Language Models](https://arxiv.org/abs/2509.09064)
*Qiuhui Chen,Xuancheng Yao,Huping Ye,Yi Hong*

Main category: cs.CV

TL;DR: This paper introduces Med3DInsight, a pretraining framework combining 3D medical image encoders with 2D multimodal large language models to improve 3D medical image understanding without requiring human annotations.


<details>
  <summary>Details</summary>
Motivation: Existing self-supervised learning methods for 3D medical images lack sufficient semantic comprehension, and recent multimodal language models offer a potential improvement through text-based understanding.

Method: Med3DInsight leverages 3D image encoders with 2D MLLMs using a plane-slice-aware transformer module and employs partial optimal transport for alignment to counteract noise in language-model-generated data.

Result: The framework achieves state-of-the-art results in segmentation and classification tasks on various CT and MRI datasets, performing better than existing SSL methods.

Conclusion: Med3DInsight sets a new direction for scalable multimodal 3D medical representation learning, is easily integrable into existing networks, and holds promise for advancing 3D medical image understanding.

Abstract: Understanding 3D medical image volumes is critical in the medical field, yet
existing 3D medical convolution and transformer-based self-supervised learning
(SSL) methods often lack deep semantic comprehension. Recent advancements in
multimodal large language models (MLLMs) provide a promising approach to
enhance image understanding through text descriptions. To leverage these 2D
MLLMs for improved 3D medical image understanding, we propose Med3DInsight, a
novel pretraining framework that integrates 3D image encoders with 2D MLLMs via
a specially designed plane-slice-aware transformer module. Additionally, our
model employs a partial optimal transport based alignment, demonstrating
greater tolerance to noise introduced by potential noises in LLM-generated
content. Med3DInsight introduces a new paradigm for scalable multimodal 3D
medical representation learning without requiring human annotations. Extensive
experiments demonstrate our state-of-the-art performance on two downstream
tasks, i.e., segmentation and classification, across various public datasets
with CT and MRI modalities, outperforming current SSL methods. Med3DInsight can
be seamlessly integrated into existing 3D medical image understanding networks,
potentially enhancing their performance. Our source code, generated datasets,
and pre-trained models will be available at
https://github.com/Qybc/Med3DInsight.

</details>


### [82] [Improvement of Human-Object Interaction Action Recognition Using Scene Information and Multi-Task Learning Approach](https://arxiv.org/abs/2509.09067)
*Hesham M. Shehata,Mohammad Abdolrahmani*

Main category: cs.CV

TL;DR: This paper improves human action recognition, especially in human-object interactions, by integrating scene information and applying a multi-task learning approach. The proposed method achieves 99.25% accuracy, surpassing traditional skeleton-based models.


<details>
  <summary>Details</summary>
Motivation: Address the failure of existing graph convolutional neural networks (GCNs) in effectively detecting human-object interactions due to the absence of scene information representation and suitable learning architectures.

Method: Introduced a methodology combining human skeleton poses with scene object information in a multi-task learning framework. Real-world data with predefined interaction classes were collected to train and evaluate the model.

Result: Achieved a significant accuracy of 99.25% for recognizing human actions, enhancing the baseline skeleton pose model accuracy by 2.75%.

Conclusion: Integrating fixed object information and applying multi-task learning substantially improves action recognition, demonstrating the benefits of utilizing scene-related context for human-object interaction detection.

Abstract: Recent graph convolutional neural networks (GCNs) have shown high performance
in the field of human action recognition by using human skeleton poses.
However, it fails to detect human-object interaction cases successfully due to
the lack of effective representation of the scene information and appropriate
learning architectures. In this context, we propose a methodology to utilize
human action recognition performance by considering fixed object information in
the environment and following a multi-task learning approach. In order to
evaluate the proposed method, we collected real data from public environments
and prepared our data set, which includes interaction classes of hands-on fixed
objects (e.g., ATM ticketing machines, check-in/out machines, etc.) and
non-interaction classes of walking and standing. The multi-task learning
approach, along with interaction area information, succeeds in recognizing the
studied interaction and non-interaction actions with an accuracy of 99.25%,
outperforming the accuracy of the base model using only human skeleton poses by
2.75%.

</details>


### [83] [IRDFusion: Iterative Relation-Map Difference guided Feature Fusion for Multispectral Object Detection](https://arxiv.org/abs/2509.09085)
*Jifeng Shen,Haibo Zhan,Xin Zuo,Heng Fan,Xiaohui Yuan,Jun Li,Wankou Yang*

Main category: cs.CV

TL;DR: The paper introduces a novel framework for multispectral object detection, emphasizing noise reduction and feature enhancement.


<details>
  <summary>Details</summary>
Motivation: Multispectral object detection tasks often struggle with noise and background interference during feature fusion, reducing detection and perceptual accuracy.

Method: The authors propose IRDFusion, a feature fusion framework integrating two modules: Mutual Feature Refinement Module (MFRM) and Differential Feature Feedback Module (DFFM), combined through an iterative mechanism.

Result: IRDFusion delivers state-of-the-art results on FLIR, LLVIP, and M$^3$FD datasets, surpassing existing methods in diverse and challenging scenarios.

Conclusion: IRDFusion efficiently enhances feature fusion across modalities by suppressing noise and amplifying salient signals, demonstrating its effectiveness and robustness in improving multispectral object detection tasks.

Abstract: Current multispectral object detection methods often retain extraneous
background or noise during feature fusion, limiting perceptual performance.To
address this, we propose an innovative feature fusion framework based on
cross-modal feature contrastive and screening strategy, diverging from
conventional approaches. The proposed method adaptively enhances salient
structures by fusing object-aware complementary cross-modal features while
suppressing shared background interference.Our solution centers on two novel,
specially designed modules: the Mutual Feature Refinement Module (MFRM) and the
Differential Feature Feedback Module (DFFM). The MFRM enhances intra- and
inter-modal feature representations by modeling their relationships, thereby
improving cross-modal alignment and discriminative power.Inspired by feedback
differential amplifiers, the DFFM dynamically computes inter-modal differential
features as guidance signals and feeds them back to the MFRM, enabling adaptive
fusion of complementary information while suppressing common-mode noise across
modalities. To enable robust feature learning, the MFRM and DFFM are integrated
into a unified framework, which is formally formulated as an Iterative
Relation-Map Differential Guided Feature Fusion mechanism, termed IRDFusion.
IRDFusion enables high-quality cross-modal fusion by progressively amplifying
salient relational signals through iterative feedback, while suppressing
feature noise, leading to significant performance gains.In extensive
experiments on FLIR, LLVIP and M$^3$FD datasets, IRDFusion achieves
state-of-the-art performance and consistently outperforms existing methods
across diverse challenging scenarios, demonstrating its robustness and
effectiveness. Code will be available at
https://github.com/61s61min/IRDFusion.git.

</details>


### [84] [SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models](https://arxiv.org/abs/2509.09090)
*Hengyu Fang,Yijiang Liu,Yuan Du,Li Du,Huanrui Yang*

Main category: cs.CV

TL;DR: SQAP-VLA is a novel inference acceleration framework for Vision-Language-Action models, enhancing efficiency through co-designed quantization and token pruning.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiencies in Vision-Language-Action models by developing a robust solution to their computational and memory demands.

Method: Proposed a training-free framework that integrates state-of-the-art quantization and token pruning via a co-designed pipeline.

Result: Achieved 1.93x speedup and up to a 4.5% performance improvement compared to standard VLA models.

Conclusion: SQAP-VLA effectively balances computational efficiency and model performance, enabling practical deployment of VLA models.

Abstract: Vision-Language-Action (VLA) models exhibit unprecedented capabilities for
embodied intelligence. However, their extensive computational and memory costs
hinder their practical deployment. Existing VLA compression and acceleration
approaches conduct quantization or token pruning in an ad-hoc manner but fail
to enable both for a holistic efficiency improvement due to an observed
incompatibility. This work introduces SQAP-VLA, the first structured,
training-free VLA inference acceleration framework that simultaneously enables
state-of-the-art quantization and token pruning. We overcome the
incompatibility by co-designing the quantization and token pruning pipeline,
where we propose new quantization-aware token pruning criteria that work on an
aggressively quantized model while improving the quantizer design to enhance
pruning effectiveness. When applied to standard VLA models, SQAP-VLA yields
significant gains in computational efficiency and inference speed while
successfully preserving core model performance, achieving a $\times$1.93
speedup and up to a 4.5\% average success rate enhancement compared to the
original model.

</details>


### [85] [S-BEVLoc: BEV-based Self-supervised Framework for Large-scale LiDAR Global Localization](https://arxiv.org/abs/2509.09110)
*Chenghao Zhang,Lun Luo,Si-Yuan Cao,Xiaokai Bai,Yuncheng Jin,Zhu Yu,Beinan Yu,Yisen Wang,Hui-Liang Shen*

Main category: cs.CV

TL;DR: The paper introduces S-BEVLoc, a self-supervised framework for LiDAR-based global localization, eliminating the need for ground-truth pose supervision and achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: Current LiDAR-based global localization models depend heavily on ground-truth poses obtained from GPS or SLAM odometry, which are costly and inefficient to acquire.

Method: S-BEVLoc uses a self-supervised approach leveraging BEV images and geographic distances to construct training data. It incorporates CNNs for feature extraction, NetVLAD for global descriptor aggregation, and SoftCos loss for learning enhancement.

Result: S-BEVLoc achieves superior performance on KITTI and NCLT datasets in place recognition, loop closure, and global localization, while demonstrating improved scalability compared to supervised methods.

Conclusion: The framework significantly reduces dependence on ground-truth poses, advancing LiDAR-based localization tasks in environments where supervised data acquisition is impractical.

Abstract: LiDAR-based global localization is an essential component of simultaneous
localization and mapping (SLAM), which helps loop closure and re-localization.
Current approaches rely on ground-truth poses obtained from GPS or SLAM
odometry to supervise network training. Despite the great success of these
supervised approaches, substantial cost and effort are required for
high-precision ground-truth pose acquisition. In this work, we propose
S-BEVLoc, a novel self-supervised framework based on bird's-eye view (BEV) for
LiDAR global localization, which eliminates the need for ground-truth poses and
is highly scalable. We construct training triplets from single BEV images by
leveraging the known geographic distances between keypoint-centered BEV
patches. Convolutional neural network (CNN) is used to extract local features,
and NetVLAD is employed to aggregate global descriptors. Moreover, we introduce
SoftCos loss to enhance learning from the generated triplets. Experimental
results on the large-scale KITTI and NCLT datasets show that S-BEVLoc achieves
state-of-the-art performance in place recognition, loop closure, and global
localization tasks, while offering scalability that would require extra effort
for supervised approaches.

</details>


### [86] [Model-Agnostic Open-Set Air-to-Air Visual Object Detection for Reliable UAV Perception](https://arxiv.org/abs/2509.09297)
*Spyridon Loukovitis,Anastasios Arsenos,Vasileios Karampinis,Athanasios Voulodimos*

Main category: cs.CV

TL;DR: The paper introduces a model-agnostic open-set detection framework for UAV object detection, tackling challenges like domain shifts and corrupted flight data.


<details>
  <summary>Details</summary>
Motivation: To ensure robust and safe UAV autonomy by addressing the limitations of traditional closed-set object detectors under real-world and shifting conditions.

Method: The proposed framework estimates semantic uncertainty in the embedding space using entropy modeling, applies spectral normalization, and temperature scaling for improved open-set detection.

Result: The method shows up to a 10% relative improvement in AUROC over YOLO-based detectors and enhances robustness while maintaining accuracy on the AOT benchmark and real-world tests.

Conclusion: The framework offers reliable UAV perception for air-to-air environments, excelling in unknown object rejection and robustness against data corruption.

Abstract: Open-set detection is crucial for robust UAV autonomy in air-to-air object
detection under real-world conditions. Traditional closed-set detectors degrade
significantly under domain shifts and flight data corruption, posing risks to
safety-critical applications. We propose a novel, model-agnostic open-set
detection framework designed specifically for embedding-based detectors. The
method explicitly handles unknown object rejection while maintaining robustness
against corrupted flight data. It estimates semantic uncertainty via entropy
modeling in the embedding space and incorporates spectral normalization and
temperature scaling to enhance open-set discrimination. We validate our
approach on the challenging AOT aerial benchmark and through extensive
real-world flight tests. Comprehensive ablation studies demonstrate consistent
improvements over baseline methods, achieving up to a 10\% relative AUROC gain
compared to standard YOLO-based detectors. Additionally, we show that
background rejection further strengthens robustness without compromising
detection accuracy, making our solution particularly well-suited for reliable
UAV perception in dynamic air-to-air environments.

</details>


### [87] [FPI-Det: a face--phone Interaction Dataset for phone-use detection and understanding](https://arxiv.org/abs/2509.09111)
*Jianqin Gao,Tianqi Wang,Yu Zhang,Yishu Zhang,Chenyuan Wang,Allan Dong,Zihao Wang*

Main category: cs.CV

TL;DR: This paper introduces FPI-Det, a dataset addressing challenges like detecting phone usage behavior in diverse contexts, and provides baseline evaluations using YOLO and DETR.


<details>
  <summary>Details</summary>
Motivation: The study aims to tackle the limitations of existing benchmarks in capturing fine-grained interactions between humans and devices, for applications in safety monitoring and productivity.

Method: The authors create FPI-Det, a dataset with 22,879 annotated images characterizing relationships among faces, phones, and hands. They test YOLO and DETR as baseline detectors.

Result: Baseline evaluations using YOLO and DETR detectors are presented, with performance analysis across varying object sizes, occlusions, and environmental contexts.

Conclusion: FPI-Det advances the understanding of human-device interactions in vision systems and serves as a resource for evaluating detector performance in challenging contexts.

Abstract: The widespread use of mobile devices has created new challenges for vision
systems in safety monitoring, workplace productivity assessment, and attention
management. Detecting whether a person is using a phone requires not only
object recognition but also an understanding of behavioral context, which
involves reasoning about the relationship between faces, hands, and devices
under diverse conditions. Existing generic benchmarks do not fully capture such
fine-grained human--device interactions. To address this gap, we introduce the
FPI-Det, containing 22{,}879 images with synchronized annotations for faces and
phones across workplace, education, transportation, and public scenarios. The
dataset features extreme scale variation, frequent occlusions, and varied
capture conditions. We evaluate representative YOLO and DETR detectors,
providing baseline results and an analysis of performance across object sizes,
occlusion levels, and environments. Source code and dataset is available at
https://github.com/KvCgRv/FPI-Det.

</details>


### [88] [Classification of Driver Behaviour Using External Observation Techniques for Autonomous Vehicles](https://arxiv.org/abs/2509.09349)
*Ian Nell,Shane Gilroy*

Main category: cs.CV

TL;DR: The paper introduces a computer vision-based system for classifying driver behavior to detect distraction and impairment.


<details>
  <summary>Details</summary>
Motivation: To address the critical issue of traffic accidents caused by human errors like distracted and impaired driving.

Method: The system uses advanced computer vision such as YOLO object detection, lateral displacement analysis, and lane position monitoring to analyze unsafe driving behaviors.

Result: Experimental evaluations show the system's reliability and adaptability across diverse road and environmental conditions.

Conclusion: The framework provides an effective solution for behavioral analysis in non-connected vehicles, improving road safety through external observation.

Abstract: Road traffic accidents remain a significant global concern, with human error,
particularly distracted and impaired driving, among the leading causes. This
study introduces a novel driver behavior classification system that uses
external observation techniques to detect indicators of distraction and
impairment. The proposed framework employs advanced computer vision
methodologies, including real-time object tracking, lateral displacement
analysis, and lane position monitoring. The system identifies unsafe driving
behaviors such as excessive lateral movement and erratic trajectory patterns by
implementing the YOLO object detection model and custom lane estimation
algorithms. Unlike systems reliant on inter-vehicular communication, this
vision-based approach enables behavioral analysis of non-connected vehicles.
Experimental evaluations on diverse video datasets demonstrate the framework's
reliability and adaptability across varying road and environmental conditions.

</details>


### [89] [Zero-shot Hierarchical Plant Segmentation via Foundation Segmentation Models and Text-to-image Attention](https://arxiv.org/abs/2509.09116)
*Junhao Xing,Ryohei Miyakawa,Yang Yang,Xinpeng Liu,Risa Shinoda,Hiroaki Santo,Yosuke Toda,Fumio Okura*

Main category: cs.CV

TL;DR: ZeroPlantSeg introduces an innovative zero-shot segmentation method using foundation segmentation and vision-language models to extract hierarchical plant structures without training.


<details>
  <summary>Details</summary>
Motivation: Current approaches struggle to segment entire plant individuals, requiring species-specific annotated datasets and significant human labor.

Method: ZeroPlantSeg leverages foundation segmentation models for leaf extraction and vision-language models to interpret plant structures, enabling zero-shot hierarchical segmentation.

Result: The method demonstrates superior cross-domain performance over supervised approaches and outperforms existing zero-shot methods across diverse datasets.

Conclusion: ZeroPlantSeg offers an efficient solution for plant segmentation by combining cutting-edge models, reducing reliance on annotated datasets and improving segmentation quality.

Abstract: Foundation segmentation models achieve reasonable leaf instance extraction
from top-view crop images without training (i.e., zero-shot). However,
segmenting entire plant individuals with each consisting of multiple
overlapping leaves remains challenging. This problem is referred to as a
hierarchical segmentation task, typically requiring annotated training
datasets, which are often species-specific and require notable human labor. To
address this, we introduce ZeroPlantSeg, a zero-shot segmentation for
rosette-shaped plant individuals from top-view images. We integrate a
foundation segmentation model, extracting leaf instances, and a vision-language
model, reasoning about plants' structures to extract plant individuals without
additional training. Evaluations on datasets with multiple plant species,
growth stages, and shooting environments demonstrate that our method surpasses
existing zero-shot methods and achieves better cross-domain performance than
supervised methods. Implementations are available at
https://github.com/JunhaoXing/ZeroPlantSeg.

</details>


### [90] [Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval](https://arxiv.org/abs/2509.09118)
*Tianlu Zheng,Yifan Zhang,Xiang An,Ziyong Feng,Kaicheng Yang,Qichuan Ding*

Main category: cs.CV

TL;DR: The paper advances CLIP for person representation learning by addressing data scarcity and global contrastive limitations with a curated dataset (WebPerson) and a new GA-DMS framework for better alignment and semantic learning.


<details>
  <summary>Details</summary>
Motivation: CLIP struggles with person representation learning due to limited annotated data and challenges in maintaining local discriminative features amidst noisy text tokens.

Method: The paper introduces WebPerson, a 5M image-text dataset, and the GA-DMS framework, which enhances cross-modal alignment by adaptively masking noisy tokens using gradient-attention similarity and predicting informative text tokens.

Result: Extensive experiments demonstrate that the GA-DMS framework achieves state-of-the-art performance across multiple benchmarks for person representation tasks.

Conclusion: These advancements in data curation and model architecture significantly improve person representation learning, showcasing robustness and enhanced fine-grained matching.

Abstract: Although Contrastive Language-Image Pre-training (CLIP) exhibits strong
performance across diverse vision tasks, its application to person
representation learning faces two critical challenges: (i) the scarcity of
large-scale annotated vision-language data focused on person-centric images,
and (ii) the inherent limitations of global contrastive learning, which
struggles to maintain discriminative local features crucial for fine-grained
matching while remaining vulnerable to noisy text tokens. This work advances
CLIP for person representation learning through synergistic improvements in
data curation and model architecture. First, we develop a noise-resistant data
construction pipeline that leverages the in-context learning capabilities of
MLLMs to automatically filter and caption web-sourced images. This yields
WebPerson, a large-scale dataset of 5M high-quality person-centric image-text
pairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking
Synergetic) framework, which improves cross-modal alignment by adaptively
masking noisy textual tokens based on the gradient-attention similarity score.
Additionally, we incorporate masked token prediction objectives that compel the
model to predict informative text tokens, enhancing fine-grained semantic
representation learning. Extensive experiments show that GA-DMS achieves
state-of-the-art performance across multiple benchmarks.

</details>


### [91] [Visual Grounding from Event Cameras](https://arxiv.org/abs/2509.09584)
*Lingdong Kong,Dongyue Lu,Ao Liang,Rong Li,Yuhao Dong,Tianshuai Hu,Lai Xing Ng,Wei Tsang Ooi,Benoit R. Cottereau*

Main category: cs.CV

TL;DR: This paper introduces Talk2Event, a benchmark linking natural language and event camera data for language-driven object grounding in dynamic scenes.


<details>
  <summary>Details</summary>
Motivation: Event cameras excel in capturing highly dynamic scenes and low-light scenarios, but their integration with natural language understanding is largely unexplored.

Method: Develop Talk2Event, a dataset with over 30,000 annotated language expressions linked to 5,567 event-camera driving scenes, focusing on interpretable grounding via four structured attributes.

Result: The dataset enables complex contextual reasoning, moving beyond object recognition to detailed temporal and spatial relation analysis in dynamic environments.

Conclusion: Talk2Event lays the groundwork for multimodal, temporally-aware perception systems and has broad application potential in AI, robotics, and human-AI interaction.

Abstract: Event cameras capture changes in brightness with microsecond precision and
remain reliable under motion blur and challenging illumination, offering clear
advantages for modeling highly dynamic scenes. Yet, their integration with
natural language understanding has received little attention, leaving a gap in
multimodal perception. To address this, we introduce Talk2Event, the first
large-scale benchmark for language-driven object grounding using event data.
Built on real-world driving scenarios, Talk2Event comprises 5,567 scenes,
13,458 annotated objects, and more than 30,000 carefully validated referring
expressions. Each expression is enriched with four structured attributes --
appearance, status, relation to the viewer, and relation to surrounding objects
-- that explicitly capture spatial, temporal, and relational cues. This
attribute-centric design supports interpretable and compositional grounding,
enabling analysis that moves beyond simple object recognition to contextual
reasoning in dynamic environments. We envision Talk2Event as a foundation for
advancing multimodal and temporally-aware perception, with applications
spanning robotics, human-AI interaction, and so on.

</details>


### [92] [ALL-PET: A Low-resource and Low-shot PET Foundation Model in the Projection Domain](https://arxiv.org/abs/2509.09130)
*Bin Huang,Kang Chen,Bingxuan Li,Huafeng Liu,Qiegen Liu*

Main category: cs.CV

TL;DR: The paper introduces ALL-PET, a low-resource PET foundation model that uses innovative augmentation and attention techniques to operate efficiently in the projection domain.


<details>
  <summary>Details</summary>
Motivation: Limited labeled data and computational resources hinder building large-scale foundation models for PET imaging.

Method: ALL-PET utilizes latent diffusion models with Radon mask augmentation, dynamic multi-mask mechanisms, positive/negative mask constraints, and a transparent medical attention mechanism to enhance data diversity, ensure geometric consistency, and focus on lesion-related areas.

Result: ALL-PET achieves high-quality sinogram generation with only 500 samples, demonstrating strong generalization to various PET imaging tasks and efficient memory usage under 24GB.

Conclusion: The work presents a resource-efficient and generalizable approach, leveraging innovative projection-domain strategies for PET imaging tasks.

Abstract: Building large-scale foundation model for PET imaging is hindered by limited
access to labeled data and insufficient computational resources. To overcome
data scarcity and efficiency limitations, we propose ALL-PET, a low-resource,
low-shot PET foundation model operating directly in the projection domain.
ALL-PET leverages a latent diffusion model (LDM) with three key innovations.
First, we design a Radon mask augmentation strategy (RMAS) that generates over
200,000 structurally diverse training samples by projecting randomized
image-domain masks into sinogram space, significantly improving generalization
with minimal data. This is extended by a dynamic multi-mask (DMM) mechanism
that varies mask quantity and distribution, enhancing data diversity without
added model complexity. Second, we implement positive/negative mask constraints
to embed strict geometric consistency, reducing parameter burden while
preserving generation quality. Third, we introduce transparent medical
attention (TMA), a parameter-free, geometry-driven mechanism that enhances
lesion-related regions in raw projection data. Lesion-focused attention maps
are derived from coarse segmentation, covering both hypermetabolic and
hypometabolic areas, and projected into sinogram space for physically
consistent guidance. The system supports clinician-defined ROI adjustments,
ensuring flexible, interpretable, and task-adaptive emphasis aligned with PET
acquisition physics. Experimental results show ALL-PET achieves high-quality
sinogram generation using only 500 samples, with performance comparable to
models trained on larger datasets. ALL-PET generalizes across tasks including
low-dose reconstruction, attenuation correction, delayed-frame prediction, and
tracer separation, operating efficiently with memory use under 24GB.

</details>


### [93] [Noise-Robust Topology Estimation of 2D Image Data via Neural Networks and Persistent Homology](https://arxiv.org/abs/2509.09140)
*Dylan Peek,Matthew P. Skerritt,Stephan Chalup*

Main category: cs.CV

TL;DR: The study compares the robustness of Artificial Neural Networks (ANNs) and Persistent Homology (PH) in estimating topological structures under noise, finding that ANNs outperform PH in noisy conditions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to evaluate the ability of neural networks to predict topological features in noisy datasets and compare them to traditional Persistent Homology methods.

Method: The study trains a supervised neural network to predict Betti numbers in 2D binary images. It uses cubical complexes and Signed Euclidean Distance Transform for PH and applies the methods to synthetic and real-world datasets.

Result: ANNs demonstrated superior performance compared to PH in noisy conditions due to their ability to learn contextual and geometric data patterns.

Conclusion: Artificial Neural Networks provide a strong alternative to Persistent Homology for topological analysis in noisy environments, leveraging their learning abilities.

Abstract: Persistent Homology (PH) and Artificial Neural Networks (ANNs) offer
contrasting approaches to inferring topological structure from data. In this
study, we examine the noise robustness of a supervised neural network trained
to predict Betti numbers in 2D binary images. We compare an ANN approach
against a PH pipeline based on cubical complexes and the Signed Euclidean
Distance Transform (SEDT), which is a widely adopted strategy for noise-robust
topological analysis. Using one synthetic and two real-world datasets, we show
that ANNs can outperform this PH approach under noise, likely due to their
capacity to learn contextual and geometric priors from training data. Though
still emerging, the use of ANNs for topology estimation offers a compelling
alternative to PH under structural noise.

</details>


### [94] [Objectness Similarity: Capturing Object-Level Fidelity in 3D Scene Evaluation](https://arxiv.org/abs/2509.09143)
*Yuiko Uchida,Ren Togo,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

TL;DR: The paper introduces OSIM, a novel metric for evaluating 3D scenes by focusing on objects, aligning more closely with human perception than existing metrics, and offers a new analysis of 3D models.


<details>
  <summary>Details</summary>
Motivation: Existing metrics for 3D scene evaluation prioritize overall image quality, often diverging from human perception. The authors aim to develop a metric that focuses on objects, the fundamental units of human visual perception.

Method: The authors propose Objectness SIMilarity (OSIM), which uses an object detection model and feature representations to evaluate the 'objectness' of objects in 3D scenes. They conduct a user study to validate OSIM's alignment with human perception and analyze its characteristics.

Result: OSIM showed stronger alignment with human perception compared to traditional metrics, based on a user study. It also facilitated a standardized re-evaluation of 3D reconstruction and generation models.

Conclusion: OSIM shifts focus to object-level evaluations in 3D scenes, resolving discrepancies between metrics and human perception. It sets a new benchmark for analyzing 3D models.

Abstract: This paper presents Objectness SIMilarity (OSIM), a novel evaluation metric
for 3D scenes that explicitly focuses on "objects," which are fundamental units
of human visual perception. Existing metrics assess overall image quality,
leading to discrepancies with human perception. Inspired by neuropsychological
insights, we hypothesize that human recognition of 3D scenes fundamentally
involves attention to individual objects. OSIM enables object-centric
evaluations by leveraging an object detection model and its feature
representations to quantify the "objectness" of each object in the scene. Our
user study demonstrates that OSIM aligns more closely with human perception
compared to existing metrics. We also analyze the characteristics of OSIM using
various approaches. Moreover, we re-evaluate recent 3D reconstruction and
generation models under a standardized experimental setup to clarify
advancements in this field. The code is available at
https://github.com/Objectness-Similarity/OSIM.

</details>


### [95] [Video Understanding by Design: How Datasets Shape Architectures and Insights](https://arxiv.org/abs/2509.09151)
*Lei Wang,Piotr Koniusz,Yongsheng Gao*

Main category: cs.CV

TL;DR: This survey provides a dataset-driven perspective on the evolution of video understanding architectures, linking milestones to the pressures imposed by dataset features like motion complexity and multimodal richness.


<details>
  <summary>Details</summary>
Motivation: Current surveys classify video understanding models primarily by task or architecture family, neglecting dataset-driven structural pressures that influence model evolution.

Method: The paper proposes a new framework that connects datasets, inductive biases, and architectures, analyzing milestones in video understanding as responses to dataset-driven pressures.

Result: The survey bridges a gap in understanding by explaining the evolution of models (e.g., 3D CNNs, transformers) as reactions to specific dataset attributes and offers practical guidance for model design.

Conclusion: By unifying datasets, architectural inductive biases, and scalability concerns, the survey provides a roadmap to better align models with dataset invariances for general-purpose video understanding.

Abstract: Video understanding has advanced rapidly, fueled by increasingly complex
datasets and powerful architectures. Yet existing surveys largely classify
models by task or family, overlooking the structural pressures through which
datasets guide architectural evolution. This survey is the first to adopt a
dataset-driven perspective, showing how motion complexity, temporal span,
hierarchical composition, and multimodal richness impose inductive biases that
models should encode. We reinterpret milestones, from two-stream and 3D CNNs to
sequential, transformer, and multimodal foundation models, as concrete
responses to these dataset-driven pressures. Building on this synthesis, we
offer practical guidance for aligning model design with dataset invariances
while balancing scalability and task demands. By unifying datasets, inductive
biases, and architectures into a coherent framework, this survey provides both
a comprehensive retrospective and a prescriptive roadmap for advancing
general-purpose video understanding.

</details>


### [96] [OCELOT 2023: Cell Detection from Cell-Tissue Interaction Challenge](https://arxiv.org/abs/2509.09153)
*JaeWoong Shin,Jeongun Ryu,Aaron Valero Puche,Jinhee Lee,Biagio Brattoli,Wonkyung Jung,Soo Ick Cho,Kyunghyun Paeng,Chan-Young Ock,Donggeun Yoo,Zhaoyang Li,Wangkai Li,Huayu Mai,Joshua Millward,Zhen He,Aiden Nibali,Lydia Anette Schoenpflug,Viktor Hendrik Koelzer,Xu Shuoyu,Ji Zheng,Hu Bin,Yu-Wen Lo,Ching-Hui Yang,Sérgio Pereira*

Main category: cs.CV

TL;DR: The paper discusses the OCELOT 2023 challenge, aimed at improving cell detection models by considering cell-tissue relationships with a unique dataset. Highlighted is a significant improvement in performance through multi-scale semantics.


<details>
  <summary>Details</summary>
Motivation: Pathologists use multi-scale visual analysis to understand cell-tissue interactions, but current deep learning models do not replicate this, necessitating better datasets and methods.

Method: The challenge introduced a dataset comprising multi-scale overlapping cell and tissue annotations from 673 paired images for training, validation, and testing, encouraging participants to incorporate cell-tissue relationships into their models.

Result: Top models in the challenge exhibited up to a 7.99 F1-score increase over the baseline by integrating multi-scale cell-tissue relationships, showing marked performance improvements over traditional methods.

Conclusion: The challenge demonstrated the importance of multi-scale semantics in cell-tissue interaction modeling for enhanced detection performance, showcasing innovative advancements in the field.

Abstract: Pathologists routinely alternate between different magnifications when
examining Whole-Slide Images, allowing them to evaluate both broad tissue
morphology and intricate cellular details to form comprehensive diagnoses.
However, existing deep learning-based cell detection models struggle to
replicate these behaviors and learn the interdependent semantics between
structures at different magnifications. A key barrier in the field is the lack
of datasets with multi-scale overlapping cell and tissue annotations. The
OCELOT 2023 challenge was initiated to gather insights from the community to
validate the hypothesis that understanding cell and tissue (cell-tissue)
interactions is crucial for achieving human-level performance, and to
accelerate the research in this field. The challenge dataset includes
overlapping cell detection and tissue segmentation annotations from six organs,
comprising 673 pairs sourced from 306 The Cancer Genome Atlas (TCGA)
Whole-Slide Images with hematoxylin and eosin staining, divided into training,
validation, and test subsets. Participants presented models that significantly
enhanced the understanding of cell-tissue relationships. Top entries achieved
up to a 7.99 increase in F1-score on the test set compared to the baseline
cell-only model that did not incorporate cell-tissue relationships. This is a
substantial improvement in performance over traditional cell-only detection
methods, demonstrating the need for incorporating multi-scale semantics into
the models. This paper provides a comparative analysis of the methods used by
participants, highlighting innovative strategies implemented in the OCELOT 2023
challenge.

</details>


### [97] [RT-DETR++ for UAV Object Detection](https://arxiv.org/abs/2509.09157)
*Yuan Shufang*

Main category: cs.CV

TL;DR: RT-DETR++ introduces encoder improvements for object detection in UAV imagery, handling small densely packed objects efficiently while maintaining real-time speed.


<details>
  <summary>Details</summary>
Motivation: Address challenges in UAV imagery object detection, including small objects, scale variations, and occlusion.

Method: Enhancements to RT-DETR encoder with channel-gated attention-based up/downsampling (AU/AD) and CSP-PAC for multi-scale feature integration.

Result: Superior performance in detecting small, densely packed objects while retaining real-time speed without additional computational complexity.

Conclusion: Effective feature encoding design proposed for real-time object detection in UAV imagery.

Abstract: Object detection in unmanned aerial vehicle (UAV) imagery presents
significant challenges. Issues such as densely packed small objects, scale
variations, and occlusion are commonplace. This paper introduces RT-DETR++,
which enhances the encoder component of the RT-DETR model. Our improvements
focus on two key aspects. First, we introduce a channel-gated attention-based
upsampling/downsampling (AU/AD) mechanism. This dual-path system minimizes
errors and preserves details during feature layer propagation. Second, we
incorporate CSP-PAC during feature fusion. This technique employs parallel
hollow convolutions to process local and contextual information within the same
layer, facilitating the integration of multi-scale features. Evaluation
demonstrates that our novel neck design achieves superior performance in
detecting small and densely packed objects. The model maintains sufficient
speed for real-time detection without increasing computational complexity. This
study provides an effective approach for feature encoding design in real-time
detection systems.

</details>


### [98] [A Knowledge Noise Mitigation Framework for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2509.09159)
*Zhiyue Liu,Sihang Liu,Jinyuan Liu,Xinru Zhang*

Main category: cs.CV

TL;DR: The paper proposes a training-free framework for Knowledge-based Visual Question Answering (KB-VQA) to mitigate noise and redundancy in utilizing external knowledge, improving answer accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing KB-VQA models struggle with noise and redundancy in external knowledge, leading to inaccurate answers.

Method: The framework uses low-noise query generation for relevant knowledge retrieval, prompts large models to extract critical knowledge segments, and introduces a selective knowledge integration strategy.

Result: The proposed framework improves the acquisition of accurate and critical knowledge, outperforming state-of-the-art KB-VQA methods.

Conclusion: By addressing redundancy and noise through advanced retrieval and integration strategies, the framework enhances the performance of KB-VQA systems.

Abstract: Knowledge-based visual question answering (KB-VQA) requires a model to
understand images and utilize external knowledge to provide accurate answers.
Existing approaches often directly augment models with retrieved information
from knowledge sources while ignoring substantial knowledge redundancy, which
introduces noise into the answering process. To address this, we propose a
training-free framework with knowledge focusing for KB-VQA, that mitigates the
impact of noise by enhancing knowledge relevance and reducing redundancy.
First, for knowledge retrieval, our framework concludes essential parts from
the image-question pairs, creating low-noise queries that enhance the retrieval
of highly relevant knowledge. Considering that redundancy still persists in the
retrieved knowledge, we then prompt large models to identify and extract
answer-beneficial segments from knowledge. In addition, we introduce a
selective knowledge integration strategy, allowing the model to incorporate
knowledge only when it lacks confidence in answering the question, thereby
mitigating the influence of redundant information. Our framework enables the
acquisition of accurate and critical knowledge, and extensive experiments
demonstrate that it outperforms state-of-the-art methods.

</details>


### [99] [CWSSNet: Hyperspectral Image Classification Enhanced by Wavelet Domain Convolution](https://arxiv.org/abs/2509.09163)
*Yulin Tong,Fengzong Zhang,Haiqin Cheng*

Main category: cs.CV

TL;DR: The paper introduces CWSSNet, a new classification framework for hyperspectral images combining 3D spectral-spatial features and wavelet convolution, yielding significant classification accuracy in ecological regions.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance classification accuracy for hyperspectral images used in fields like precision agriculture and forestry ecology by addressing challenges such as feature redundancy and spectral mixing.

Method: The proposed framework CWSSNet integrates 3D spectral-spatial features with wavelet convolution, multimodal information fusion using multiscale convolutional attention, and multi-band decomposition operations.

Result: CWSSNet achieved 74.50% mIoU, 82.73% mAcc, and 84.94% mF1 scores, with robust performance across water bodies, vegetation, and bare land classifications. It maintained effective results under a small-sample training setup.

Conclusion: CWSSNet demonstrates reliable classification performance and robustness, breaking conventional limitations, making it a promising solution for hyperspectral image classification challenges.

Abstract: Hyperspectral remote sensing technology has significant application value in
fields such as forestry ecology and precision agriculture, while also putting
forward higher requirements for fine ground object classification. However,
although hyperspectral images are rich in spectral information and can improve
recognition accuracy, they tend to cause prominent feature redundancy due to
their numerous bands, high dimensionality, and spectral mixing characteristics.
To address this, this study used hyperspectral images from the ZY1F satellite
as a data source and selected Yugan County, Shangrao City, Jiangxi Province as
the research area to perform ground object classification research. A
classification framework named CWSSNet was proposed, which integrates 3D
spectral-spatial features and wavelet convolution. This framework integrates
multimodal information us-ing a multiscale convolutional attention module and
breaks through the classification performance bottleneck of traditional methods
by introducing multi-band decomposition and convolution operations in the
wavelet domain. The experiments showed that CWSSNet achieved 74.50\%, 82.73\%,
and 84.94\% in mean Intersection over Union (mIoU), mean Accuracy (mAcc), and
mean F1-score (mF1) respectively in Yugan County. It also obtained the highest
Intersection over Union (IoU) in the classifica-tion of water bodies,
vegetation, and bare land, demonstrating good robustness. Additionally, when
the training set proportion was 70\%, the increase in training time was
limited, and the classification effect was close to the optimal level,
indicating that the model maintains reliable performance under small-sample
training conditions.

</details>


### [100] [Bridging the Gap Between Ideal and Real-world Evaluation: Benchmarking AI-Generated Image Detection in Challenging Scenarios](https://arxiv.org/abs/2509.09172)
*Chunxiao Li,Xiaoxiao Wang,Meiling Li,Boming Miao,Peng Sun,Yunjian Zhang,Xiangyang Ji,Yao Zhu*

Main category: cs.CV

TL;DR: The paper presents the Real-World Robustness Dataset (RRDataset) for evaluating AI-generated image detection under real-world scenarios, internet transmissions, and re-digitization methods, highlighting the limitations of current detection methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in evaluating AI-generated image detection methods' performance in complex and real-world conditions as current methods are insufficient.

Method: This study introduces RRDataset to evaluate three dimensions: scenario generalization with diverse image categories, internet transmission robustness by simulating social media sharing, and re-digitization robustness. Benchmarking of 17 AI detectors, 10 vision-language models, and a human study were conducted.

Result: The study found that current AI detection methods struggle under realistic conditions, while humans show adaptable few-shot learning capabilities in detecting AI-generated images.

Conclusion: The findings highlight the need to leverage human adaptability for creating better and more robust AI-generated image detection algorithms.

Abstract: With the rapid advancement of generative models, highly realistic image
synthesis has posed new challenges to digital security and media credibility.
Although AI-generated image detection methods have partially addressed these
concerns, a substantial research gap remains in evaluating their performance
under complex real-world conditions. This paper introduces the Real-World
Robustness Dataset (RRDataset) for comprehensive evaluation of detection models
across three dimensions: 1) Scenario Generalization: RRDataset encompasses
high-quality images from seven major scenarios (War and Conflict, Disasters and
Accidents, Political and Social Events, Medical and Public Health, Culture and
Religion, Labor and Production, and everyday life), addressing existing dataset
gaps from a content perspective. 2) Internet Transmission Robustness: examining
detector performance on images that have undergone multiple rounds of sharing
across various social media platforms. 3) Re-digitization Robustness: assessing
model effectiveness on images altered through four distinct re-digitization
methods. We benchmarked 17 detectors and 10 vision-language models (VLMs) on
RRDataset and conducted a large-scale human study involving 192 participants to
investigate human few-shot learning capabilities in detecting AI-generated
images. The benchmarking results reveal the limitations of current AI detection
methods under real-world conditions and underscore the importance of drawing on
human adaptability to develop more robust detection algorithms.

</details>


### [101] [Dark-ISP: Enhancing RAW Image Processing for Low-Light Object Detection](https://arxiv.org/abs/2509.09183)
*Jiasheng Guo,Xin Gao,Yuxiang Yan,Guanghao Li,Jian Pu*

Main category: cs.CV

TL;DR: The paper introduces Dark-ISP, a lightweight and adaptive Image Signal Processing plugin for object detection using Bayer RAW images in low-light environments.


<details>
  <summary>Details</summary>
Motivation: Low-light object detection remains challenging due to image quality degradation, and there is potential to use RAW images for better outcomes.

Method: Dark-ISP deconstructs ISP pipelines into linear and nonlinear differentiable modules optimized via task-driven losses, with content-aware adaptability and physics-guided priors.

Result: Dark-ISP achieves state-of-the-art performance in low-light object detection across three RAW image datasets, while maintaining minimal parameters.

Conclusion: The proposed Dark-ISP plugin significantly improves RAW-based detection with its efficient design, emphasizing its effectiveness in low-light applications.

Abstract: Low-light Object detection is crucial for many real-world applications but
remains challenging due to degraded image quality. While recent studies have
shown that RAW images offer superior potential over RGB images, existing
approaches either use RAW-RGB images with information loss or employ complex
frameworks. To address these, we propose a lightweight and self-adaptive Image
Signal Processing (ISP) plugin, Dark-ISP, which directly processes Bayer RAW
images in dark environments, enabling seamless end-to-end training for object
detection. Our key innovations are: (1) We deconstruct conventional ISP
pipelines into sequential linear (sensor calibration) and nonlinear (tone
mapping) sub-modules, recasting them as differentiable components optimized
through task-driven losses. Each module is equipped with content-aware
adaptability and physics-informed priors, enabling automatic RAW-to-RGB
conversion aligned with detection objectives. (2) By exploiting the ISP
pipeline's intrinsic cascade structure, we devise a Self-Boost mechanism that
facilitates cooperation between sub-modules. Through extensive experiments on
three RAW image datasets, we demonstrate that our method outperforms
state-of-the-art RGB- and RAW-based detection approaches, achieving superior
results with minimal parameters in challenging low-light environments.

</details>


### [102] [VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal Models: Methods and Results](https://arxiv.org/abs/2509.09190)
*Hanwei Zhu,Haoning Wu,Zicheng Zhang,Lingyu Zhu,Yixuan Li,Peilin Chen,Shiqi Wang,Chris Wei Zhou,Linhan Cao,Wei Sun,Xiangyang Zhu,Weixia Zhang,Yucheng Zhu,Jing Liu,Dandan Zhu,Guangtao Zhai,Xiongkuo Min,Zhichao Zhang,Xinyue Li,Shubo Xu,Anh Dao,Yifan Li,Hongyuan Yu,Jiaojiao Yi,Yiding Tian,Yupeng Wu,Feiran Sun,Lijuan Liao,Song Jiang*

Main category: cs.CV

TL;DR: The VQualA 2025 Challenge evaluates large multimodal models (LMMs) on reasoning about visual quality differences across images. It introduces tasks to assess models' quality judgments using new benchmarks, holistic protocols, and showcases promising instruction-tuned LMMs.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to advance open-ended visual quality reasoning and comparison, aligning LMM capabilities with human-like quality evaluations.

Method: A novel benchmark, comprising diverse visual quality comparison tasks across images, is developed. Models are assessed using 2AFC binary preference and multi-choice question evaluations.

Result: Around 100 participants engaged, with five models effectively showcasing instruction-tuned capabilities for quality assessment.

Conclusion: This challenge drives progress in human-aligned visual quality assessment and sets the stage for future research in interpretable LMM systems.

Abstract: This paper presents a summary of the VQualA 2025 Challenge on Visual Quality
Comparison for Large Multimodal Models (LMMs), hosted as part of the ICCV 2025
Workshop on Visual Quality Assessment. The challenge aims to evaluate and
enhance the ability of state-of-the-art LMMs to perform open-ended and detailed
reasoning about visual quality differences across multiple images. To this end,
the competition introduces a novel benchmark comprising thousands of
coarse-to-fine grained visual quality comparison tasks, spanning single images,
pairs, and multi-image groups. Each task requires models to provide accurate
quality judgments. The competition emphasizes holistic evaluation protocols,
including 2AFC-based binary preference and multi-choice questions (MCQs).
Around 100 participants submitted entries, with five models demonstrating the
emerging capabilities of instruction-tuned LMMs on quality assessment. This
challenge marks a significant step toward open-domain visual quality reasoning
and comparison and serves as a catalyst for future research on interpretable
and human-aligned quality evaluation systems.

</details>


### [103] [MGTraj: Multi-Granularity Goal-Guided Human Trajectory Prediction with Recursive Refinement Network](https://arxiv.org/abs/2509.09200)
*Ge Sun,Jun Ma*

Main category: cs.CV

TL;DR: This paper introduces MGTraj, a model that predicts human trajectories using multi-granularity encoding for better accuracy compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for human trajectory prediction lack exploration of intermediate temporal granularities for improving accuracy.

Method: The model uses recursive encoding from coarse to fine granularities through a transformer-based recursive refinement network (RRN) with weight-sharing and velocity prediction as auxiliary tasks.

Result: Experiments on EHT/UCY and Stanford Drone Dataset show that MGTraj outperforms existing methods and sets new state-of-the-art benchmarks in goal-guided trajectory prediction.

Conclusion: MGTraj demonstrates the effectiveness of multi-granularity modeling and refinement processes in human trajectory prediction tasks.

Abstract: Accurate human trajectory prediction is crucial for robotics navigation and
autonomous driving. Recent research has demonstrated that incorporating goal
guidance significantly enhances prediction accuracy by reducing uncertainty and
leveraging prior knowledge. Most goal-guided approaches decouple the prediction
task into two stages: goal prediction and subsequent trajectory completion
based on the predicted goal, which operate at extreme granularities:
coarse-grained goal prediction forecasts the overall intention, while
fine-grained trajectory completion needs to generate the positions for all
future timesteps. The potential utility of intermediate temporal granularity
remains largely unexplored, which motivates multi-granularity trajectory
modeling. While prior work has shown that multi-granularity representations
capture diverse scales of human dynamics and motion patterns, effectively
integrating this concept into goal-guided frameworks remains challenging. In
this paper, we propose MGTraj, a novel Multi-Granularity goal-guided model for
human Trajectory prediction. MGTraj recursively encodes trajectory proposals
from coarse to fine granularity levels. At each level, a transformer-based
recursive refinement network (RRN) captures features and predicts progressive
refinements. Features across different granularities are integrated using a
weight-sharing strategy, and velocity prediction is employed as an auxiliary
task to further enhance performance. Comprehensive experimental results in
EHT/UCY and Stanford Drone Dataset indicate that MGTraj outperforms baseline
methods and achieves state-of-the-art performance among goal-guided methods.

</details>


### [104] [Medverse: A Universal Model for Full-Resolution 3D Medical Image Segmentation, Transformation and Enhancement](https://arxiv.org/abs/2509.09232)
*Jiesi Hu,Jianfeng Cao,Yanwu Yang,Chenfei Ye,Yixuan Zhang,Hanyang Peng,Ting Ma*

Main category: cs.CV

TL;DR: The paper introduces Medverse, a universal in-context learning (ICL) model for 3D medical imaging, designed to tackle diverse tasks across multiple anatomical regions and imaging modalities. It achieves high-fidelity outputs with global anatomical understanding.


<details>
  <summary>Details</summary>
Motivation: Current ICL models for medical imaging suffer from limitations such as inability to simultaneously achieve high-fidelity predictions and global anatomical understanding, as well as the absence of a unified model trained across varied imaging tasks and regions.

Method: Medverse employs a next-scale autoregressive ICL framework for progressively refining predictions to full-resolution outputs. Additionally, it incorporates a blockwise cross-attention module, enabling efficient long-range interactions while maintaining spatial sparsity.

Result: Medverse is extensively tested on held-out datasets covering unseen clinical centers, organs, and modalities, substantially outperforming existing ICL baselines.

Conclusion: Medverse establishes a new standard for ICL in medical imaging, demonstrating significant advancements and promising unified solutions for diverse imaging tasks. The model and code are made publicly available.

Abstract: In-context learning (ICL) offers a promising paradigm for universal medical
image analysis, enabling models to perform diverse image processing tasks
without retraining. However, current ICL models for medical imaging remain
limited in two critical aspects: they cannot simultaneously achieve
high-fidelity predictions and global anatomical understanding, and there is no
unified model trained across diverse medical imaging tasks (e.g., segmentation
and enhancement) and anatomical regions. As a result, the full potential of ICL
in medical imaging remains underexplored. Thus, we present \textbf{Medverse}, a
universal ICL model for 3D medical imaging, trained on 22 datasets covering
diverse tasks in universal image segmentation, transformation, and enhancement
across multiple organs, imaging modalities, and clinical centers. Medverse
employs a next-scale autoregressive in-context learning framework that
progressively refines predictions from coarse to fine, generating consistent,
full-resolution volumetric outputs and enabling multi-scale anatomical
awareness. We further propose a blockwise cross-attention module that
facilitates long-range interactions between context and target inputs while
preserving computational efficiency through spatial sparsity. Medverse is
extensively evaluated on a broad collection of held-out datasets covering
previously unseen clinical centers, organs, species, and imaging modalities.
Results demonstrate that Medverse substantially outperforms existing ICL
baselines and establishes a novel paradigm for in-context learning. Code and
model weights will be made publicly available. Our model are publicly available
at https://github.com/jiesihu/Medverse.

</details>


### [105] [CoAtNeXt:An Attention-Enhanced ConvNeXtV2-Transformer Hybrid Model for Gastric Tissue Classification](https://arxiv.org/abs/2509.09242)
*Mustafa Yurdakul,Sakir Tasdemir*

Main category: cs.CV

TL;DR: The paper introduces CoAtNeXt, a hybrid model for classifying gastric tissue images that surpasses traditional methods in terms of accuracy, precision, recall, and AUC.


<details>
  <summary>Details</summary>
Motivation: The study aims to address limitations in manual histopathological evaluations, such as labor-intensiveness, variability among pathologists, and lack of standardized methods.

Method: The authors developed CoAtNeXt, which builds on the CoAtNet architecture by replacing MBConv layers with ConvNeXtV2 blocks and integrating CBAM for enhanced feature extraction. The model was tested on two datasets against 20 existing models.

Result: CoAtNeXt achieved top-tier metrics, including 96.47% accuracy on the HMU-GC-HE-30K dataset and 98.29% accuracy on the GasHisSDB dataset, outperforming all other compared CNN and ViT models.

Conclusion: CoAtNeXt demonstrates robust histopathological classification performance, offering both high accuracy and a reduction in pathologists' workload, thus supporting its utility in diagnostic settings.

Abstract: Background and objective Early diagnosis of gastric diseases is crucial to
prevent fatal outcomes. Although histopathologic examination remains the
diagnostic gold standard, it is performed entirely manually, making evaluations
labor-intensive and prone to variability among pathologists. Critical findings
may be missed, and lack of standard procedures reduces consistency. These
limitations highlight the need for automated, reliable, and efficient methods
for gastric tissue analysis. Methods In this study, a novel hybrid model named
CoAtNeXt was proposed for the classification of gastric tissue images. The
model is built upon the CoAtNet architecture by replacing its MBConv layers
with enhanced ConvNeXtV2 blocks. Additionally, the Convolutional Block
Attention Module (CBAM) is integrated to improve local feature extraction
through channel and spatial attention mechanisms. The architecture was scaled
to achieve a balance between computational efficiency and classification
performance. CoAtNeXt was evaluated on two publicly available datasets,
HMU-GC-HE-30K for eight-class classification and GasHisSDB for binary
classification, and was compared against 10 Convolutional Neural Networks
(CNNs) and ten Vision Transformer (ViT) models. Results CoAtNeXt achieved
96.47% accuracy, 96.60% precision, 96.47% recall, 96.45% F1 score, and 99.89%
AUC on HMU-GC-HE-30K. On GasHisSDB, it reached 98.29% accuracy, 98.07%
precision, 98.41% recall, 98.23% F1 score, and 99.90% AUC. It outperformed all
CNN and ViT models tested and surpassed previous studies in the literature.
Conclusion Experimental results show that CoAtNeXt is a robust architecture for
histopathological classification of gastric tissue images, providing
performance on binary and multiclass. Its highlights its potential to assist
pathologists by enhancing diagnostic accuracy and reducing workload.

</details>


### [106] [Towards Better Dental AI: A Multimodal Benchmark and Instruction Dataset for Panoramic X-ray Analysis](https://arxiv.org/abs/2509.09254)
*Jing Hao,Yuxuan Fan,Yanpeng Sun,Kaixin Guo,Lizhuo Lin,Jinrong Yang,Qi Yong H. Ai,Lun M. Wong,Hao Tang,Kuo Feng Hung*

Main category: cs.CV

TL;DR: The paper introduces MMOral, the first large-scale multimodal dataset specifically for panoramic X-ray interpretation in dentistry. It includes evaluation benchmarks and proposes an improved model, OralGPT, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Large vision-language models (LVLMs) have shown promise in general medical tasks but underperform in specialized domains like dentistry, particularly in interpreting complex panoramic X-ray imagery.

Method: The study creates MMOral—a multimodal instruction dataset with 20,563 annotated images and 1.3M instruction instances—and evaluates 64 LVLMs across five diagnostic dimensions, further proposing OralGPT, fine-tuned using MMOral.

Result: The evaluation shows state-of-the-art GPT-4o performs poorly with only 41.45% accuracy. OralGPT, fine-tuned with MMOral, achieves a significant 24.73% improvement, demonstrating enhanced model capabilities.

Conclusion: MMOral and OralGPT represent critical advancements for specialized AI applications in dentistry, offering improved diagnostic potential and paving the way for intelligent multimodal systems in the dental field.

Abstract: Recent advances in large vision-language models (LVLMs) have demonstrated
strong performance on general-purpose medical tasks. However, their
effectiveness in specialized domains such as dentistry remains underexplored.
In particular, panoramic X-rays, a widely used imaging modality in oral
radiology, pose interpretative challenges due to dense anatomical structures
and subtle pathological cues, which are not captured by existing medical
benchmarks or instruction datasets. To this end, we introduce MMOral, the first
large-scale multimodal instruction dataset and benchmark tailored for panoramic
X-ray interpretation. MMOral consists of 20,563 annotated images paired with
1.3 million instruction-following instances across diverse task types,
including attribute extraction, report generation, visual question answering,
and image-grounded dialogue. In addition, we present MMOral-Bench, a
comprehensive evaluation suite covering five key diagnostic dimensions in
dentistry. We evaluate 64 LVLMs on MMOral-Bench and find that even the
best-performing model, i.e., GPT-4o, only achieves 41.45% accuracy, revealing
significant limitations of current models in this domain. To promote the
progress of this specific domain, we also propose OralGPT, which conducts
supervised fine-tuning (SFT) upon Qwen2.5-VL-7B with our meticulously curated
MMOral instruction dataset. Remarkably, a single epoch of SFT yields
substantial performance enhancements for LVLMs, e.g., OralGPT demonstrates a
24.73% improvement. Both MMOral and OralGPT hold significant potential as a
critical foundation for intelligent dentistry and enable more clinically
impactful multimodal AI systems in the dental field. The dataset, model,
benchmark, and evaluation suite are available at
https://github.com/isbrycee/OralGPT.

</details>


### [107] [DATE: Dynamic Absolute Time Enhancement for Long Video Understanding](https://arxiv.org/abs/2509.09263)
*Chao Yuan,Yang Yang,Yehui Yang,Zach Cheng*

Main category: cs.CV

TL;DR: The paper introduces a method, Dynamic Absolute Time Enhancement (DATE), using Timestamp Injection Mechanism (TIM) and Temporal-Aware Similarity Sampling (TASS) to improve long video understanding in multimodal large language models.


<details>
  <summary>Details</summary>
Motivation: The study addresses the difficulty of understanding long videos in multimodal large language models, particularly the challenges of temporal reasoning, event localization, and handling long-range dependencies.

Method: The proposed DATE approach involves embedding temporal tokens and reformulating video sampling as a vision-language retrieval problem. It uses TIM for temporal references and TASS for retrieving semantically relevant and temporally covered frames.

Result: The method demonstrates enhanced temporal awareness, achieving state-of-the-art results across benchmarks for hour-long video tasks, with a 7B model outperforming some 72B models.

Conclusion: DATE significantly improves temporal understanding in MLLMs, offering both precision in event localization and scalability to long videos.

Abstract: Long video understanding remains a fundamental challenge for multimodal large
language models (MLLMs), particularly in tasks requiring precise temporal
reasoning and event localization. Existing approaches typically adopt uniform
frame sampling and rely on implicit position encodings to model temporal order.
However, these methods struggle with long-range dependencies, leading to
critical information loss and degraded temporal comprehension. In this paper,
we propose Dynamic Absolute Time Enhancement (DATE) that enhances temporal
awareness in MLLMs through the Timestamp Injection Mechanism (TIM) and a
semantically guided Temporal-Aware Similarity Sampling (TASS) strategy.
Specifically, we interleave video frame embeddings with textual timestamp
tokens to construct a continuous temporal reference system. We further
reformulate the video sampling problem as a vision-language retrieval task and
introduce a two-stage algorithm to ensure both semantic relevance and temporal
coverage: enriching each query into a descriptive caption to better align with
the vision feature, and sampling key event with a similarity-driven temporally
regularized greedy strategy. Our method achieves remarkable improvements w.r.t.
absolute time understanding and key event localization, resulting in
state-of-the-art performance among 7B and 72B models on hour-long video
benchmarks. Particularly, our 7B model even exceeds many 72B models on some
benchmarks.

</details>


### [108] [Unified Start, Personalized End: Progressive Pruning for Efficient 3D Medical Image Segmentation](https://arxiv.org/abs/2509.09267)
*Linhao Li,Yiwen Ye,Ziyang Chen,Yong Xia*

Main category: cs.CV

TL;DR: The paper introduces PSP-Seg, a dynamic 3D medical image segmentation framework that progressively prunes redundant model modules for improved efficiency and effectiveness.


<details>
  <summary>Details</summary>
Motivation: 3D medical image segmentation is resource-intensive and struggles with scalability and adaptability for diverse tasks. Current methods lack the dynamic ability to optimize resource efficiency without sacrificing performance.

Method: PSP-Seg starts with a redundant model and progressively applies block-wise pruning, assisted by a functional decoupling loss, to streamline the model while retaining segmentation quality.

Result: PSP-Seg-S, a lightweight variant, matches nnU-Net’s performance while significantly reducing GPU memory (42-45%), training time (29-48%), and model parameters (83-87%) across five datasets.

Conclusion: PSP-Seg offers a practical, efficient alternative for deploying 3D medical image segmentation in clinical settings, combining cost-effectiveness with high performance.

Abstract: 3D medical image segmentation often faces heavy resource and time
consumption, limiting its scalability and rapid deployment in clinical
environments. Existing efficient segmentation models are typically static and
manually designed prior to training, which restricts their adaptability across
diverse tasks and makes it difficult to balance performance with resource
efficiency. In this paper, we propose PSP-Seg, a progressive pruning framework
that enables dynamic and efficient 3D segmentation. PSP-Seg begins with a
redundant model and iteratively prunes redundant modules through a combination
of block-wise pruning and a functional decoupling loss. We evaluate PSP-Seg on
five public datasets, benchmarking it against seven state-of-the-art models and
six efficient segmentation models. Results demonstrate that the lightweight
variant, PSP-Seg-S, achieves performance on par with nnU-Net while reducing GPU
memory usage by 42-45%, training time by 29-48%, and parameter number by 83-87%
across all datasets. These findings underscore PSP-Seg's potential as a
cost-effective yet high-performing alternative for widespread clinical
application.

</details>


### [109] [Visual Programmability: A Guide for Code-as-Thought in Chart Understanding](https://arxiv.org/abs/2509.09286)
*Bohao Tang,Yan Ma,Fei Zhang,Jiadi Su,Ethan Chern,Zhulin Hu,Zhixin Wang,Pengfei Liu,Ya Zhang*

Main category: cs.CV

TL;DR: The paper introduces a novel approach called Code-as-Thought (CaT) for chart understanding using Vision-Language Models (VLMs). It proposes a system that dynamically chooses between symbolic code reasoning and visual analysis, addressing limitations of prior methods.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of current VLMs for chart understanding, which are either brittle due to reliance on external tools or constrained to single reasoning strategies like text-based chain-of-thought reasoning.

Method: The paper introduces Code-as-Thought (CaT) combined with Visual Programmability, where a VLM adaptively chooses between symbolic (code-based) reasoning or direct visual analysis using a selection policy trained through reinforcement learning with dual rewards.

Result: Experiments indicate improved and robust performance across diverse benchmarks for chart understanding, demonstrating the model's ability to dynamically select optimal reasoning strategies.

Conclusion: This study concludes that VLMs can be trained to not only conduct reasoning but also to dynamically adapt their reasoning strategies based on task requirements, enhancing their accuracy and flexibility in chart understanding.

Abstract: Chart understanding presents a critical test to the reasoning capabilities of
Vision-Language Models (VLMs). Prior approaches face critical limitations: some
rely on external tools, making them brittle and constrained by a predefined
toolkit, while others fine-tune specialist models that often adopt a single
reasoning strategy, such as text-based chain-of-thought (CoT). The intermediate
steps of text-based reasoning are difficult to verify, which complicates the
use of reinforcement-learning signals that reward factual accuracy. To address
this, we propose a Code-as-Thought (CaT) approach to represent the visual
information of a chart in a verifiable, symbolic format. Our key insight is
that this strategy must be adaptive: a fixed, code-only implementation
consistently fails on complex charts where symbolic representation is
unsuitable. This finding leads us to introduce Visual Programmability: a
learnable property that determines if a chart-question pair is better solved
with code or direct visual analysis. We implement this concept in an adaptive
framework where a VLM learns to choose between the CaT pathway and a direct
visual reasoning pathway. The selection policy of the model is trained with
reinforcement learning using a novel dual-reward system. This system combines a
data-accuracy reward to ground the model in facts and prevent numerical
hallucination, with a decision reward that teaches the model when to use each
strategy, preventing it from defaulting to a single reasoning mode. Experiments
demonstrate strong and robust performance across diverse chart-understanding
benchmarks. Our work shows that VLMs can be taught not only to reason but also
how to reason, dynamically selecting the optimal reasoning pathway for each
task.

</details>


### [110] [Modality-Agnostic Input Channels Enable Segmentation of Brain lesions in Multimodal MRI with Sequences Unavailable During Training](https://arxiv.org/abs/2509.09290)
*Anthony P. Addison,Felix Wagner,Wentian Xu,Natalie Voets,Konstantinos Kamnitsas*

Main category: cs.CV

TL;DR: This paper develops a segmentation model for brain MRI that can effectively handle unseen and mixed imaging modalities using a modified U-net architecture with a modality-agnostic input pathway and augmented training data.


<details>
  <summary>Details</summary>
Motivation: Current segmentation models are limited, as they either cannot process unseen modalities effectively or lose specific information when generalizing to new modalities. An improved model is needed to handle unseen and mixed modalities while maintaining segmentation accuracy.

Method: The authors modify the U-net architecture by adding a modality-agnostic input channel alongside modality-specific input channels. They use an image augmentation scheme to generate artificial MRI modalities, simulating realistic contrasts between healthy and pathological tissues.

Result: Using 8 MRI datasets with 5 pathologies and 8 modalities, the model effectively processes both seen and unseen image modalities while preserving segmentation performance.

Conclusion: The proposed approach enables segmentation models to generalize to new MRI modalities while maintaining accuracy for familiar ones, enhancing the utility of the model in diverse clinical scenarios.

Abstract: Segmentation models are important tools for the detection and analysis of
lesions in brain MRI. Depending on the type of brain pathology that is imaged,
MRI scanners can acquire multiple, different image modalities (contrasts). Most
segmentation models for multimodal brain MRI are restricted to fixed modalities
and cannot effectively process new ones at inference. Some models generalize to
unseen modalities but may lose discriminative modality-specific information.
This work aims to develop a model that can perform inference on data that
contain image modalities unseen during training, previously seen modalities,
and heterogeneous combinations of both, thus allowing a user to utilize any
available imaging modalities. We demonstrate this is possible with a simple,
thus practical alteration to the U-net architecture, by integrating a
modality-agnostic input channel or pathway, alongside modality-specific input
channels. To train this modality-agnostic component, we develop an image
augmentation scheme that synthesizes artificial MRI modalities. Augmentations
differentially alter the appearance of pathological and healthy brain tissue to
create artificial contrasts between them while maintaining realistic anatomical
integrity. We evaluate the method using 8 MRI databases that include 5 types of
pathologies (stroke, tumours, traumatic brain injury, multiple sclerosis and
white matter hyperintensities) and 8 modalities (T1, T1+contrast, T2, PD, SWI,
DWI, ADC and FLAIR). The results demonstrate that the approach preserves the
ability to effectively process MRI modalities encountered during training,
while being able to process new, unseen modalities to improve its segmentation.
Project code: https://github.com/Anthony-P-Addison/AGN-MOD-SEG

</details>


### [111] [Learning Object-Centric Representations in SAR Images with Multi-Level Feature Fusion](https://arxiv.org/abs/2509.09298)
*Oh-Tae Jang,Min-Gon Cho,Kyung-Tae Kim*

Main category: cs.CV

TL;DR: SlotSAR is a new framework for disentangling target representations from background clutter in SAR images without mask annotations, combining multi-level semantic and scattering features for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: SAR images often contain clutter resembling targets, complicating the process of extracting target-specific representations essential for classification.

Method: SlotSAR extracts semantic features from SARATR-X and scattering features via a wavelet scattering network. The multi-level slot attention module combines these features to enhance representation clarity.

Result: Experimental evaluation showed SlotSAR outperforms existing object-centric learning methods for SAR images, preserving structural details and achieving state-of-the-art performance.

Conclusion: SlotSAR effectively disentangles target representations from background clutter in SAR images, advancing the accuracy and robustness of object-centric learning approaches.

Abstract: Synthetic aperture radar (SAR) images contain not only targets of interest
but also complex background clutter, including terrain reflections and speckle
noise. In many cases, such clutter exhibits intensity and patterns that
resemble targets, leading models to extract entangled or spurious features.
Such behavior undermines the ability to form clear target representations,
regardless of the classifier. To address this challenge, we propose a novel
object-centric learning (OCL) framework, named SlotSAR, that disentangles
target representations from background clutter in SAR images without mask
annotations. SlotSAR first extracts high-level semantic features from SARATR-X
and low-level scattering features from the wavelet scattering network in order
to obtain complementary multi-level representations for robust target
characterization. We further present a multi-level slot attention module that
integrates these low- and high-level features to enhance slot-wise
representation distinctiveness, enabling effective OCL. Experimental results
demonstrate that SlotSAR achieves state-of-the-art performance in SAR imagery
by preserving structural details compared to existing OCL methods.

</details>


### [112] [Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization](https://arxiv.org/abs/2509.09307)
*Zhengzhao Lai,Youbin Zheng,Zhenyang Cai,Haonan Lyu,Jinpu Yang,Hongqing Liang,Yan Hu,Benyou Wang*

Main category: cs.CV

TL;DR: The paper introduces MatCha, the first benchmark dataset for understanding materials characterization image data. It comprises tasks to evaluate multimodal large language models' (MLLMs) domain expertise against human experts.


<details>
  <summary>Details</summary>
Motivation: To address the limited exploration of MLLMs' capabilities in interpreting real-world materials characterization imaging data, which is critical for materials science research.

Method: Developed MatCha, a benchmark of 1,500 expert-level questions across 21 tasks reflecting real-world challenges in materials characterization, and evaluated state-of-the-art MLLMs on these tasks.

Result: Current MLLMs show significant performance gaps compared to human experts, particularly for tasks requiring high-level expertise and visual perception. Prompting strategies did not effectively overcome these limitations.

Conclusion: MLLMs require further development to adapt to real-world materials characterization scenarios, and MatCha provides a foundation for advancing research in this area, including material discovery and autonomous agents.

Abstract: Materials characterization is fundamental to acquiring materials information,
revealing the processing-microstructure-property relationships that guide
material design and optimization. While multimodal large language models
(MLLMs) have recently shown promise in generative and predictive tasks within
materials science, their capacity to understand real-world characterization
imaging data remains underexplored. To bridge this gap, we present MatCha, the
first benchmark for materials characterization image understanding, comprising
1,500 questions that demand expert-level domain expertise. MatCha encompasses
four key stages of materials research comprising 21 distinct tasks, each
designed to reflect authentic challenges faced by materials scientists. Our
evaluation of state-of-the-art MLLMs on MatCha reveals a significant
performance gap compared to human experts. These models exhibit degradation
when addressing questions requiring higher-level expertise and sophisticated
visual perception. Simple few-shot and chain-of-thought prompting struggle to
alleviate these limitations. These findings highlight that existing MLLMs still
exhibit limited adaptability to real-world materials characterization
scenarios. We hope MatCha will facilitate future research in areas such as new
material discovery and autonomous scientific agents. MatCha is available at
https://github.com/FreedomIntelligence/MatCha.

</details>


### [113] [You Share Beliefs, I Adapt: Progressive Heterogeneous Collaborative Perception](https://arxiv.org/abs/2509.09310)
*Hao Si,Ehsan Javanmardi,Manabu Tsukada*

Main category: cs.CV

TL;DR: The paper proposes PHCP, a framework for collaborative perception among heterogeneous vehicle models during inference without joint training or labeled data adoption.


<details>
  <summary>Details</summary>
Motivation: Current heterogeneous collaborative perception methods require impractical joint training or storing pre-trained models for all potential collaborators, which limits real-world application.

Method: PHCP (Progressive Heterogeneous Collaborative Perception) uses few-shot unsupervised domain adaptation by dynamically self-training an adapter during inference, bypassing joint training and labeled datasets.

Result: PHCP achieves performance comparable to state-of-the-art methods and demonstrates strong results on the OPV2V dataset using minimal unlabeled data.

Conclusion: PHCP provides a feasible and robust solution for heterogeneous collaborative perception during inference by addressing domain gap challenges without pre-training or explicit data labeling.

Abstract: Collaborative perception enables vehicles to overcome individual perception
limitations by sharing information, allowing them to see further and through
occlusions. In real-world scenarios, models on different vehicles are often
heterogeneous due to manufacturer variations. Existing methods for
heterogeneous collaborative perception address this challenge by fine-tuning
adapters or the entire network to bridge the domain gap. However, these methods
are impractical in real-world applications, as each new collaborator must
undergo joint training with the ego vehicle on a dataset before inference, or
the ego vehicle stores models for all potential collaborators in advance.
Therefore, we pose a new question: Can we tackle this challenge directly during
inference, eliminating the need for joint training? To answer this, we
introduce Progressive Heterogeneous Collaborative Perception (PHCP), a novel
framework that formulates the problem as few-shot unsupervised domain
adaptation. Unlike previous work, PHCP dynamically aligns features by
self-training an adapter during inference, eliminating the need for labeled
data and joint training. Extensive experiments on the OPV2V dataset demonstrate
that PHCP achieves strong performance across diverse heterogeneous scenarios.
Notably, PHCP achieves performance comparable to SOTA methods trained on the
entire dataset while using only a small amount of unlabeled data.

</details>


### [114] [Image Recognition with Vision and Language Embeddings of VLMs](https://arxiv.org/abs/2509.09311)
*Illia Volkov,Nikita Kisel,Klara Janouskova,Jiri Matas*

Main category: cs.CV

TL;DR: This paper evaluates the visual and language-guided classification capabilities of vision-language models, proposing a fusion method to enhance performance.


<details>
  <summary>Details</summary>
Motivation: To explore and enhance the under-explored visual inference capabilities of vision-language models and analyze the complementary strengths of language and vision.

Method: A comprehensive evaluation of dual-encoder VLMs in both visual and language-guided settings, analyzing factors like prompt design and reference set size, complemented by a fusion method for improved performance.

Result: The study confirms that vision and language strengths are complementary and introduces a fusion method based on per-class precision to enhance classification.

Conclusion: The fusion of textual and visual inputs leads to improved image classification performance, showcasing the complementary potentials of both modalities.

Abstract: Vision-language models (VLMs) have enabled strong zero-shot classification
through image-text alignment. Yet, their purely visual inference capabilities
remain under-explored. In this work, we conduct a comprehensive evaluation of
both language-guided and vision-only image classification with a diverse set of
dual-encoder VLMs, including both well-established and recent models such as
SigLIP 2 and RADIOv2.5. The performance is compared in a standard setup on the
ImageNet-1k validation set and its label-corrected variant. The key factors
affecting accuracy are analysed, including prompt design, class diversity, the
number of neighbours in k-NN, and reference set size. We show that language and
vision offer complementary strengths, with some classes favouring textual
prompts and others better handled by visual similarity. To exploit this
complementarity, we introduce a simple, learning-free fusion method based on
per-class precision that improves classification performance. The code is
available at: https://github.com/gonikisgo/bmvc2025-vlm-image-recognition.

</details>


### [115] [Fine-Grained Customized Fashion Design with Image-into-Prompt benchmark and dataset from LMM](https://arxiv.org/abs/2509.09324)
*Hui Li,Yi You,Qiqi Chen,Bingfeng Zhang,George Q. Huang*

Main category: cs.CV

TL;DR: The paper introduces the BUG (Better Understanding Generation) workflow utilizing LMM to improve fine-grained customization in garment design, proposing a new FashionEdit dataset for evaluation.


<details>
  <summary>Details</summary>
Motivation: The aim is to address the challenge of fine-grained customization in AI garment design due to text ambiguity and lack of professional knowledge by end-users.

Method: The BUG workflow uses a large multimodal model (LMM) to integrate image-to-prompt for automatic creation, customization, and editing of clothing designs, removing the need for human intervention.

Result: The authors demonstrate effectiveness through the proposed FashionEdit dataset, designed to simulate real-world design workflows and evaluate generation similarity, user satisfaction, and design quality.

Conclusion: The proposed framework enhances creative potential and simplifies the clothing design/editing process, offering a practical solution for empowering both end-users and industry professionals.

Abstract: Generative AI evolves the execution of complex workflows in industry, where
the large multimodal model empowers fashion design in the garment industry.
Current generation AI models magically transform brainstorming into fancy
designs easily, but the fine-grained customization still suffers from text
uncertainty without professional background knowledge from end-users. Thus, we
propose the Better Understanding Generation (BUG) workflow with LMM to
automatically create and fine-grain customize the cloth designs from chat with
image-into-prompt. Our framework unleashes users' creative potential beyond
words and also lowers the barriers of clothing design/editing without further
human involvement. To prove the effectiveness of our model, we propose a new
FashionEdit dataset that simulates the real-world clothing design workflow,
evaluated from generation similarity, user satisfaction, and quality. The code
and dataset: https://github.com/detectiveli/FashionEdit.

</details>


### [116] [Exploring Pre-training Across Domains for Few-Shot Surgical Skill Assessment](https://arxiv.org/abs/2509.09327)
*Dimitrios Anastasiou,Razvan Caramalau,Nazir Sirajudeen,Matthew Boal,Philip Edwards,Justin Collins,John Kelly,Ashwin Sridhar,Maxine Tran,Faiz Mumtaz,Nevil Pavithran,Nader Francis,Danail Stoyanov,Evangelos B. Mazomenos*

Main category: cs.CV

TL;DR: The paper explores automated surgical skill assessment (SSA) using few-shot learning (FSL) and evaluates the impact of different pre-training strategies on model performance.


<details>
  <summary>Details</summary>
Motivation: Due to the scarcity of skill annotations requiring expert consensus, the paper aims to leverage few-shot learning (FSL) for SSA, which minimizes the need for extensive data annotation.

Method: SSA is formulated as a few-shot task. The authors evaluate pre-training sources on a robotic surgery dataset annotated with OSATS scores and analyze the impact of domain similarity and procedure-specific data on downstream performance.

Result: Small and domain-relevant pre-training datasets outperform less aligned large-scale ones, achieving accuracies of 60.16%, 66.03%, and 73.65% in 1-shot, 2-shot, and 5-shot settings, respectively. Incorporating procedure-specific data improves accuracy (+1.22%) and F1-score (+2.28%).

Conclusion: Pre-training strategies focused on domain relevance are crucial for improving few-shot SSA performance, and procedure-specific data further enhances model transferability. Large-scale pre-training data that lacks domain alignment can negatively impact performance.

Abstract: Automated surgical skill assessment (SSA) is a central task in surgical
computer vision. Developing robust SSA models is challenging due to the
scarcity of skill annotations, which are time-consuming to produce and require
expert consensus. Few-shot learning (FSL) offers a scalable alternative
enabling model development with minimal supervision, though its success
critically depends on effective pre-training. While widely studied for several
surgical downstream tasks, pre-training has remained largely unexplored in SSA.
In this work, we formulate SSA as a few-shot task and investigate how
self-supervised pre-training strategies affect downstream few-shot SSA
performance. We annotate a publicly available robotic surgery dataset with
Objective Structured Assessment of Technical Skill (OSATS) scores, and evaluate
various pre-training sources across three few-shot settings. We quantify domain
similarity and analyze how domain gap and the inclusion of procedure-specific
data into pre-training influence transferability. Our results show that small
but domain-relevant datasets can outperform large scale, less aligned ones,
achieving accuracies of 60.16%, 66.03%, and 73.65% in the 1-, 2-, and 5-shot
settings, respectively. Moreover, incorporating procedure-specific data into
pre-training with a domain-relevant external dataset significantly boosts
downstream performance, with an average gain of +1.22% in accuracy and +2.28%
in F1-score; however, applying the same strategy with less similar but
large-scale sources can instead lead to performance degradation. Code and
models are available at https://github.com/anastadimi/ssa-fsl.

</details>


### [117] [Texture-aware Intrinsic Image Decomposition with Model- and Learning-based Priors](https://arxiv.org/abs/2509.09352)
*Xiaodong Wang,Zijun He,Xin Yuan*

Main category: cs.CV

TL;DR: This paper presents a novel method to improve intrinsic image decomposition by handling complex scenes with severe lighting and rich textures.


<details>
  <summary>Details</summary>
Motivation: The intrinsic image decomposition problem, specifically separating reflectance and shading layers from a single image, remains challenging in complex scenes with spatially-varying lighting and rich textures.

Method: The authors introduce a texture-guided regularization term within an optimization framework to better separate material textures and lighting. This method incorporates a novel texture-aware prior to address limitations of existing approaches.

Result: The proposed approach produces high-quality intrinsic images and outperforms current methods in handling lighting and texture complexities, as demonstrated through their evaluation.

Conclusion: Integrating texture-aware prior enhances intrinsic image decomposition, overcoming issues of over-smoothing and texture-less outputs in prior methods.

Abstract: This paper aims to recover the intrinsic reflectance layer and shading layer
given a single image. Though this intrinsic image decomposition problem has
been studied for decades, it remains a significant challenge in cases of
complex scenes, i.e. spatially-varying lighting effect and rich textures. In
this paper, we propose a novel method for handling severe lighting and rich
textures in intrinsic image decomposition, which enables to produce
high-quality intrinsic images for real-world images. Specifically, we observe
that previous learning-based methods tend to produce texture-less and
over-smoothing intrinsic images, which can be used to infer the lighting and
texture information given a RGB image. In this way, we design a texture-guided
regularization term and formulate the decomposition problem into an
optimization framework, to separate the material textures and lighting effect.
We demonstrate that combining the novel texture-aware prior can produce
superior results to existing approaches.

</details>


### [118] [Plug-and-play Diffusion Models for Image Compressive Sensing with Data Consistency Projection](https://arxiv.org/abs/2509.09365)
*Xiaodong Wang,Ping Wang,Zhangyuan Li,Xin Yuan*

Main category: cs.CV

TL;DR: The paper investigates the link between Plug-and-Play (PnP) methods and Denoising Diffusion Implicit Models (DDIM) for solving inverse problems, focusing on single-pixel imaging. It proposes a unified framework and hybrid data-consistency module for better reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: To unify PnP and DDIM strategies for addressing ill-posed inverse problems, with emphasis on improving single-pixel imaging reconstruction performance.

Method: The authors decouple the diffusion process into denoising, data consistency enforcement, and sampling stages. They introduce a hybrid data-consistency module combining PnP-style fidelity terms to enhance measurement accuracy without altering sampling trajectories.

Result: The experimental findings demonstrate improved reconstruction quality in single-pixel imaging tasks using the proposed method.

Conclusion: A unified and principled framework that integrates learned priors with physical models enhances reconstruction quality and measurement consistency in solving inverse problems.

Abstract: We explore the connection between Plug-and-Play (PnP) methods and Denoising
Diffusion Implicit Models (DDIM) for solving ill-posed inverse problems, with a
focus on single-pixel imaging. We begin by identifying key distinctions between
PnP and diffusion models-particularly in their denoising mechanisms and
sampling procedures. By decoupling the diffusion process into three
interpretable stages: denoising, data consistency enforcement, and sampling, we
provide a unified framework that integrates learned priors with physical
forward models in a principled manner. Building upon this insight, we propose a
hybrid data-consistency module that linearly combines multiple PnP-style
fidelity terms. This hybrid correction is applied directly to the denoised
estimate, improving measurement consistency without disrupting the diffusion
sampling trajectory. Experimental results on single-pixel imaging tasks
demonstrate that our method achieves better reconstruction quality.

</details>


### [119] [A Fully Automatic Framework for Intracranial Pressure Grading: Integrating Keyframe Identification, ONSD Measurement and Clinical Data](https://arxiv.org/abs/2509.09368)
*Pengxu Wen,Tingting Yu,Ziwei Nie,Cheng Jiang,Zhenyu Yin,Mingyang He,Bo Liao,Xiaoping Yang*

Main category: cs.CV

TL;DR: This paper introduces a non-invasive, automated two-stage framework for evaluating intracranial pressure (ICP) using optic nerve sheath diameter (ONSD) and clinical data to improve accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: The need arises because invasive methods like lumbar puncture have risks, and current ONSD measurement practices are inconsistent and subjective.

Method: The framework processes ultrasound video for ONSD measurement using segmentation and keyframe identification, then integrates ONSD metrics with clinical data to predict ICP grades.

Result: The method achieves superior accuracy (validation: 0.845 ± 0.071; test: 0.786) compared to conventional methods (validation: 0.637 ± 0.111; test: 0.429).

Conclusion: By reducing operator variability and integrating multi-source data, the framework provides a reliable tool for non-invasive ICP evaluation, benefiting acute neurological patient management.

Abstract: Intracranial pressure (ICP) elevation poses severe threats to cerebral
function, thus necessitating monitoring for timely intervention. While lumbar
puncture is the gold standard for ICP measurement, its invasiveness and
associated risks drive the need for non-invasive alternatives. Optic nerve
sheath diameter (ONSD) has emerged as a promising biomarker, as elevated ICP
directly correlates with increased ONSD. However, current clinical practices
for ONSD measurement suffer from inconsistency in manual operation,
subjectivity in optimal view selection, and variability in thresholding,
limiting their reliability. To address these challenges, we introduce a fully
automatic two-stage framework for ICP grading, integrating keyframe
identification, ONSD measurement and clinical data. Specifically, the fundus
ultrasound video processing stage performs frame-level anatomical segmentation,
rule-based keyframe identification guided by an international consensus
statement, and precise ONSD measurement. The intracranial pressure grading
stage then fuses ONSD metrics with clinical features to enable the prediction
of ICP grades, thereby demonstrating an innovative blend of interpretable
ultrasound analysis and multi-source data integration for objective clinical
evaluation. Experimental results demonstrate that our method achieves a
validation accuracy of $0.845 \pm 0.071$ (with standard deviation from
five-fold cross-validation) and an independent test accuracy of 0.786,
significantly outperforming conventional threshold-based method ($0.637 \pm
0.111$ validation accuracy, $0.429$ test accuracy). Through effectively
reducing operator variability and integrating multi-source information, our
framework establishes a reliable non-invasive approach for clinical ICP
evaluation, holding promise for improving patient management in acute
neurological conditions.

</details>


### [120] [FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark](https://arxiv.org/abs/2509.09680)
*Rongyao Fang,Aldrich Yu,Chengqi Duan,Linjiang Huang,Shuai Bai,Yuxuan Cai,Kun Wang,Si Liu,Xihui Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: This paper addresses limitations in open-source text-to-image (T2I) models by introducing a reasoning-focused dataset (FLUX-Reason-6M) and a comprehensive evaluation benchmark (PRISM-Bench).


<details>
  <summary>Details</summary>
Motivation: There is a performance gap between open-source and closed-source T2I models due to the lack of reasoning-specific datasets and robust evaluation frameworks.

Method: The authors developed FLUX-Reason-6M, consisting of 6M FLUX-generated images and bilingual captions, alongside PRISM-Bench, a 7-track benchmark leveraging advanced vision-language models for evaluation. They used 15,000 A100 GPU days for data curation and designed explicit Generation Chain-of-Thought (GCoT) for detailed reasoning steps.

Result: The study evaluated 19 leading models on PRISM-Bench, exposing critical performance gaps and pinpointing areas needing improvement for reasoning-oriented image generation.

Conclusion: The authors release their dataset, benchmarks, and code to drive advancements in reasoning-focused T2I generation for the broader research community.

Abstract: The advancement of open-source text-to-image (T2I) models has been hindered
by the absence of large-scale, reasoning-focused datasets and comprehensive
evaluation benchmarks, resulting in a performance gap compared to leading
closed-source systems. To address this challenge, We introduce FLUX-Reason-6M
and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark).
FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality
FLUX-generated images and 20 million bilingual (English and Chinese)
descriptions specifically designed to teach complex reasoning. The image are
organized according to six key characteristics: Imagination, Entity, Text
rendering, Style, Affection, and Composition, and design explicit Generation
Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation
steps. The whole data curation takes 15,000 A100 GPU days, providing the
community with a resource previously unattainable outside of large industrial
labs. PRISM-Bench offers a novel evaluation standard with seven distinct
tracks, including a formidable Long Text challenge using GCoT. Through
carefully designed prompts, it utilizes advanced vision-language models for
nuanced human-aligned assessment of prompt-image alignment and image
aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench
reveals critical performance gaps and highlights specific areas requiring
improvement. Our dataset, benchmark, and evaluation code are released to
catalyze the next wave of reasoning-oriented T2I generation. Project page:
https://flux-reason-6m.github.io/ .

</details>


### [121] [Unsupervised Integrated-Circuit Defect Segmentation via Image-Intrinsic Normality](https://arxiv.org/abs/2509.09375)
*Botong Zhao,Qijun Shi,Shujing Lyu,Yue Lu*

Main category: cs.CV

TL;DR: The paper proposes an unsupervised IC defect segmentation framework that uses features from the test image itself, overcoming limitations caused by layout variability and alignment issues in traditional methods.


<details>
  <summary>Details</summary>
Motivation: Current defect segmentation methods struggle with IC imagery due to variability in layouts across products and challenges in achieving accurate alignment.

Method: The framework uses a learnable normal-information extractor, coherence loss to associate features with normal regions, and a decoder for reconstructing normal content. Defects are identified from the residuals. Pseudo-anomaly augmentation enhances training stability.

Result: Experiments show the proposed method consistently improves defect segmentation accuracy across three IC process stages and demonstrates strong robustness to product variability.

Conclusion: The unsupervised approach successfully segments IC defects without relying on external normal data, enhancing performance and adaptability to diverse product layouts.

Abstract: Modern Integrated-Circuit(IC) manufacturing introduces diverse, fine-grained
defects that depress yield and reliability. Most industrial defect segmentation
compares a test image against an external normal set, a strategy that is
brittle for IC imagery where layouts vary across products and accurate
alignment is difficult. We observe that defects are predominantly local, while
each image still contains rich, repeatable normal patterns. We therefore
propose an unsupervised IC defect segmentation framework that requires no
external normal support. A learnable normal-information extractor aggregates
representative normal features from the test image, and a coherence loss
enforces their association with normal regions. Guided by these features, a
decoder reconstructs only normal content; the reconstruction residual then
segments defects. Pseudo-anomaly augmentation further stabilizes training.
Experiments on datasets from three IC process stages show consistent
improvements over existing approaches and strong robustness to product
variability.

</details>


### [122] [Decoupling Clinical and Class-Agnostic Features for Reliable Few-Shot Adaptation under Shift](https://arxiv.org/abs/2509.09397)
*Umaima Rahman,Raza Imam,Mohammad Yaqub,Dwarikanath Mahapatra*

Main category: cs.CV

TL;DR: This paper introduces DRiFt, a framework to improve the generalizability and reliability of medical vision-language models (VLMs) by separating clinical signals from noise and enhancing feature alignment.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of variance and distribution shifts in medical VLMs that lead to unreliable performance and generalization in real-world clinical settings.

Method: The authors propose DRiFt, which uses parameter-efficient tuning (LoRA) and learnable prompt tokens to disentangle clinical signals from noise. Additionally, high-quality image-text pairs are curated for improved cross-modal alignment.

Result: DRiFt achieves a +11.4% Top-1 accuracy and +3.3% Macro-F1 improvement compared to prior methods, demonstrating robust performance even on unseen datasets.

Conclusion: Disentangling task-relevant features and carefully aligning data significantly enhance the safety and trustworthiness of VLMs in clinical applications.

Abstract: Medical vision-language models (VLMs) offer promise for clinical decision
support, yet their reliability under distribution shifts remains a major
concern for safe deployment. These models often learn task-agnostic
correlations due to variability in imaging protocols and free-text reports,
limiting their generalizability and increasing the risk of failure in
real-world settings. We propose DRiFt, a structured feature decoupling
framework that explicitly separates clinically relevant signals from
task-agnostic noise using parameter-efficient tuning (LoRA) and learnable
prompt tokens. To enhance cross-modal alignment and reduce uncertainty, we
curate high-quality, clinically grounded image-text pairs by generating
captions for a diverse medical dataset. Our approach improves in-distribution
performance by +11.4% Top-1 accuracy and +3.3% Macro-F1 over prior prompt-based
methods, while maintaining strong robustness across unseen datasets. Ablation
studies reveal that disentangling task-relevant features and careful alignment
significantly enhance model generalization and reduce unpredictable behavior
under domain shift. These insights contribute toward building safer, more
trustworthy VLMs for clinical use. The code is available at
https://github.com/rumaima/DRiFt.

</details>


### [123] [FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolution](https://arxiv.org/abs/2509.09427)
*Yuchan Jie,Yushen Xu,Xiaosong Li,Fuqiang Zhou,Jianming Lv,Huafeng Li*

Main category: cs.CV

TL;DR: The paper proposes FS-Diff, a technique uniting image fusion and super-resolution for multimodal images, particularly for low-resolution and weak information scenarios. It introduces a clarity sensing mechanism and combines global feature extraction via Mamba with a U-Net-based denoising process to achieve high-quality results.


<details>
  <summary>Details</summary>
Motivation: Current image fusion techniques fail to handle low-resolution, weak semantic information, and structure-corrupted multimodal images effectively, particularly in scenarios like military reconnaissance.

Method: The method, FS-Diff, models the task as a conditional generation problem. It employs a clarity sensing mechanism, the bidirectional feature Mamba for global feature extraction, and a modified U-Net for iterative denoising to produce high-resolution fused results. The approach works across varying noise levels to ensure clarity and semantic richness.

Result: FS-Diff exhibits superior performance over six public and the newly developed AVMS dataset. It produces more detailed and semantically rich fused images at various magnifications compared to state-of-the-art methods.

Conclusion: FS-Diff effectively addresses challenges in multimodal image fusion and super-resolution, leveraging its novel mechanisms to unify and enhance results. The approach paves the way for more adaptive and information-rich fusion techniques.

Abstract: As an influential information fusion and low-level vision technique, image
fusion integrates complementary information from source images to yield an
informative fused image. A few attempts have been made in recent years to
jointly realize image fusion and super-resolution. However, in real-world
applications such as military reconnaissance and long-range detection missions,
the target and background structures in multimodal images are easily corrupted,
with low resolution and weak semantic information, which leads to suboptimal
results in current fusion techniques. In response, we propose FS-Diff, a
semantic guidance and clarity-aware joint image fusion and super-resolution
method. FS-Diff unifies image fusion and super-resolution as a conditional
generation problem. It leverages semantic guidance from the proposed clarity
sensing mechanism for adaptive low-resolution perception and cross-modal
feature extraction. Specifically, we initialize the desired fused result as
pure Gaussian noise and introduce the bidirectional feature Mamba to extract
the global features of the multimodal images. Moreover, utilizing the source
images and semantics as conditions, we implement a random iterative denoising
process via a modified U-Net network. This network istrained for denoising at
multiple noise levels to produce high-resolution fusion results with
cross-modal features and abundant semantic information. We also construct a
powerful aerial view multiscene (AVMS) benchmark covering 600 pairs of images.
Extensive joint image fusion and super-resolution experiments on six public and
our AVMS datasets demonstrated that FS-Diff outperforms the state-of-the-art
methods at multiple magnifications and can recover richer details and semantics
in the fused images. The code is available at
https://github.com/XylonXu01/FS-Diff.

</details>


### [124] [Semantic Concentration for Self-Supervised Dense Representations Learning](https://arxiv.org/abs/2509.09429)
*Peisong Wen,Qianqian Xu,Siran Dai,Runmin Cong,Qingming Huang*

Main category: cs.CV

TL;DR: This paper tackles the challenge of learning dense representations for patches in SSL by addressing over-dispersion and introducing methods such as patch correspondence distillation, ranking loss, and object-aware filtering.


<details>
  <summary>Details</summary>
Motivation: There is a need to overcome the challenge of over-dispersion when learning dense representations for patches in SSL because it limits performance in downstream dense tasks.

Method: The paper proposes distilling patch correspondences to break strict spatial alignment using a noise-tolerant ranking loss and introduces an object-aware filter to project patches into an object-based space.

Result: Empirical studies demonstrate the effectiveness of the proposed methods across multiple tasks, yielding improvements in dense SSL performance.

Conclusion: Explicit semantic concentration strategies, including the proposed noise-tolerant ranking loss and object-aware filtering, successfully tackle patch representation challenges in dense SSL, offering improved performance in downstream tasks.

Abstract: Recent advances in image-level self-supervised learning (SSL) have made
significant progress, yet learning dense representations for patches remains
challenging. Mainstream methods encounter an over-dispersion phenomenon that
patches from the same instance/category scatter, harming downstream performance
on dense tasks. This work reveals that image-level SSL avoids over-dispersion
by involving implicit semantic concentration. Specifically, the non-strict
spatial alignment ensures intra-instance consistency, while shared patterns,
i.e., similar parts of within-class instances in the input space, ensure
inter-image consistency. Unfortunately, these approaches are infeasible for
dense SSL due to their spatial sensitivity and complicated scene-centric data.
These observations motivate us to explore explicit semantic concentration for
dense SSL. First, to break the strict spatial alignment, we propose to distill
the patch correspondences. Facing noisy and imbalanced pseudo labels, we
propose a noise-tolerant ranking loss. The core idea is extending the Average
Precision (AP) loss to continuous targets, such that its decision-agnostic and
adaptive focusing properties prevent the student model from being misled.
Second, to discriminate the shared patterns from complicated scenes, we propose
the object-aware filter to map the output space to an object-based space.
Specifically, patches are represented by learnable prototypes of objects via
cross-attention. Last but not least, empirical studies across various tasks
soundly support the effectiveness of our method. Code is available in
https://github.com/KID-7391/CoTAP.

</details>


### [125] [FlexiD-Fuse: Flexible number of inputs multi-modal medical image fusion based on diffusion model](https://arxiv.org/abs/2509.09456)
*Yushen Xu,Xiaosong Li,Yuchun Wang,Xiaoqi Cheng,Huafeng Li,Haishu Tan*

Main category: cs.CV

TL;DR: This paper presents FlexiD-Fuse, a novel diffusion-based method for fusing medical images from multiple modalities, accommodating flexible input quantities while producing high-quality fused outputs.


<details>
  <summary>Details</summary>
Motivation: Current medical image fusion methods are limited to fixed numbers of input modalities, which restricts their usability in diverse clinical scenarios where input quantities vary.

Method: The authors propose FlexiD-Fuse, a diffusion-based network using hierarchical Bayesian modeling and the EM algorithm. This approach allows for end-to-end fusion of both two-modal and tri-modal medical images under the same framework while supporting flexible modality inputs.

Result: FlexiD-Fuse outperforms state-of-the-art methods in both two-modal and tri-modal medical image fusion tasks and demonstrates superior adaptability in non-medical scenarios like infrared-visible and multi-focus fusion tasks.

Conclusion: FlexiD-Fuse successfully handles varying quantities of input modalities, making it more versatile and effective for clinical diagnosis and beyond, as proven by extensive experiments and comparative analyses.

Abstract: Different modalities of medical images provide unique physiological and
anatomical information for diseases. Multi-modal medical image fusion
integrates useful information from different complementary medical images with
different modalities, producing a fused image that comprehensively and
objectively reflects lesion characteristics to assist doctors in clinical
diagnosis. However, existing fusion methods can only handle a fixed number of
modality inputs, such as accepting only two-modal or tri-modal inputs, and
cannot directly process varying input quantities, which hinders their
application in clinical settings. To tackle this issue, we introduce
FlexiD-Fuse, a diffusion-based image fusion network designed to accommodate
flexible quantities of input modalities. It can end-to-end process two-modal
and tri-modal medical image fusion under the same weight. FlexiD-Fuse
transforms the diffusion fusion problem, which supports only fixed-condition
inputs, into a maximum likelihood estimation problem based on the diffusion
process and hierarchical Bayesian modeling. By incorporating the
Expectation-Maximization algorithm into the diffusion sampling iteration
process, FlexiD-Fuse can generate high-quality fused images with cross-modal
information from source images, independently of the number of input images. We
compared the latest two and tri-modal medical image fusion methods, tested them
on Harvard datasets, and evaluated them using nine popular metrics. The
experimental results show that our method achieves the best performance in
medical image fusion with varying inputs. Meanwhile, we conducted extensive
extension experiments on infrared-visible, multi-exposure, and multi-focus
image fusion tasks with arbitrary numbers, and compared them with the
perspective SOTA methods. The results of the extension experiments consistently
demonstrate the effectiveness and superiority of our method.

</details>


### [126] [Resource-Efficient Glioma Segmentation on Sub-Saharan MRI](https://arxiv.org/abs/2509.09469)
*Freedmore Sidume,Oumayma Soula,Joseph Muthui Wacira,YunFei Zhu,Abbas Rabiu Muhammad,Abderrazek Zeraii,Oluwaseun Kalejaye,Hajer Ibrahim,Olfa Gaddour,Brain Halubanza,Dong Zhang,Udunna C Anazodo,Confidence Raymond*

Main category: cs.CV

TL;DR: The paper proposes a lightweight deep learning framework for glioma segmentation in MRI, showcasing strong results despite limited data in Sub-Saharan Africa.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for accurate glioma segmentation in Sub-Saharan Africa, where high-quality annotated imaging data is scarce, to improve diagnosis and treatment planning.

Method: The study developed a 3D Attention UNet with residual blocks, utilizing transfer learning from BraTS 2021 pre-trained weights and tested it on the BraTS-Africa dataset.

Result: The model achieved impressive Dice scores: 0.76 for Enhancing Tumor, 0.80 for Necrotic and Non-Enhancing Tumor Core, and 0.85 for Surrounding Non-Functional Hemisphere.

Conclusion: The proposed model is effective, practical for low-resource healthcare systems, and advances equitable AI solutions for global health challenges.

Abstract: Gliomas are the most prevalent type of primary brain tumors, and their
accurate segmentation from MRI is critical for diagnosis, treatment planning,
and longitudinal monitoring. However, the scarcity of high-quality annotated
imaging data in Sub-Saharan Africa (SSA) poses a significant challenge for
deploying advanced segmentation models in clinical workflows. This study
introduces a robust and computationally efficient deep learning framework
tailored for resource-constrained settings. We leveraged a 3D Attention UNet
architecture augmented with residual blocks and enhanced through transfer
learning from pre-trained weights on the BraTS 2021 dataset. Our model was
evaluated on 95 MRI cases from the BraTS-Africa dataset, a benchmark for glioma
segmentation in SSA MRI data. Despite the limited data quality and quantity,
our approach achieved Dice scores of 0.76 for the Enhancing Tumor (ET), 0.80
for Necrotic and Non-Enhancing Tumor Core (NETC), and 0.85 for Surrounding
Non-Functional Hemisphere (SNFH). These results demonstrate the
generalizability of the proposed model and its potential to support clinical
decision making in low-resource settings. The compact architecture,
approximately 90 MB, and sub-minute per-volume inference time on consumer-grade
hardware further underscore its practicality for deployment in SSA health
systems. This work contributes toward closing the gap in equitable AI for
global health by empowering underserved regions with high-performing and
accessible medical imaging solutions.

</details>


### [127] [OpenFake: An Open Dataset and Platform Toward Large-Scale Deepfake Detection](https://arxiv.org/abs/2509.09495)
*Victor Livernoche,Akshatha Arodi,Andreea Musulan,Zachary Yang,Adam Salvail,Gaétan Marceau Caron,Jean-François Godbout,Reihaneh Rabbany*

Main category: cs.CV

TL;DR: The paper addresses limitations of existing deepfake detection datasets and introduces a politically-focused dataset of real and synthetic images, alongside a crowdsourced adversarial platform to improve deepfake detection methods.


<details>
  <summary>Details</summary>
Motivation: Deepfakes are increasingly difficult to detect and are used to spread misinformation, particularly in political contexts, posing significant risks to public discourse.

Method: The authors created a dataset of three million real images with captions and nearly one million high-quality synthetic images generated by proprietary and open-source models. They also developed a crowdsourced adversarial platform incentivizing users to produce challenging synthetic images.

Result: The study provided a benchmark dataset for modern generative models, demonstrated the indistinguishability of synthetic images through a human perception study, and proposed a scalable framework to adapt to advancing deepfake generation techniques.

Conclusion: The paper highlights the importance of continuously updated datasets and adaptive detection methods to counteract deepfake-related misinformation effectively, safeguarding the integrity of public discussions.

Abstract: Deepfakes, synthetic media created using advanced AI techniques, have
intensified the spread of misinformation, particularly in politically sensitive
contexts. Existing deepfake detection datasets are often limited, relying on
outdated generation methods, low realism, or single-face imagery, restricting
the effectiveness for general synthetic image detection. By analyzing social
media posts, we identify multiple modalities through which deepfakes propagate
misinformation. Furthermore, our human perception study demonstrates that
recently developed proprietary models produce synthetic images increasingly
indistinguishable from real ones, complicating accurate identification by the
general public. Consequently, we present a comprehensive, politically-focused
dataset specifically crafted for benchmarking detection against modern
generative models. This dataset contains three million real images paired with
descriptive captions, which are used for generating 963k corresponding
high-quality synthetic images from a mix of proprietary and open-source models.
Recognizing the continual evolution of generative techniques, we introduce an
innovative crowdsourced adversarial platform, where participants are
incentivized to generate and submit challenging synthetic images. This ongoing
community-driven initiative ensures that deepfake detection methods remain
robust and adaptive, proactively safeguarding public discourse from
sophisticated misinformation threats.

</details>


### [128] [Improving Human Motion Plausibility with Body Momentum](https://arxiv.org/abs/2509.09496)
*Ha Linh Nguyen,Tze Ho Elden Tse,Angela Yao*

Main category: cs.CV

TL;DR: The paper presents a method to address the physical coupling between local and global motion by leveraging whole-body momentum as a constraint.


<details>
  <summary>Details</summary>
Motivation: Current motion models often fail to capture the physical coupling between a body's local and global movements, and deriving global motion computationally is complex.

Method: The authors propose using whole-body linear and angular momentum as a constraint, introducing a new loss term that ensures consistency between generated momentum profiles and ground-truth data.

Result: The proposed method reduces foot sliding and jitter, enhances balance, and maintains accuracy in recovered motion.

Conclusion: Using momentum as a constraint effectively links local and global motion, offering a physically meaningful way to improve motion models.

Abstract: Many studies decompose human motion into local motion in a frame attached to
the root joint and global motion of the root joint in the world frame, treating
them separately. However, these two components are not independent. Global
movement arises from interactions with the environment, which are, in turn,
driven by changes in the body configuration. Motion models often fail to
precisely capture this physical coupling between local and global dynamics,
while deriving global trajectories from joint torques and external forces is
computationally expensive and complex. To address these challenges, we propose
using whole-body linear and angular momentum as a constraint to link local
motion with global movement. Since momentum reflects the aggregate effect of
joint-level dynamics on the body's movement through space, it provides a
physically grounded way to relate local joint behavior to global displacement.
Building on this insight, we introduce a new loss term that enforces
consistency between the generated momentum profiles and those observed in
ground-truth data. Incorporating our loss reduces foot sliding and jitter,
improves balance, and preserves the accuracy of the recovered motion. Code and
data are available at the project page https://hlinhn.github.io/momentum_bmvc.

</details>


### [129] [Region-Wise Correspondence Prediction between Manga Line Art Images](https://arxiv.org/abs/2509.09501)
*Yingxuan Li,Jiafeng Mao,Qianru Qiu,Yusuke Matsui*

Main category: cs.CV

TL;DR: This paper proposes a Transformer-based method to identify region-wise correspondence in manga line art images without segmentation or annotations.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored challenge of identifying region-wise correspondence in manga line art images, enabling applications like colorization and frame generation.

Method: A Transformer-based framework is proposed to analyze patch-level similarities in manga line art images, complemented by edge-aware clustering and region matching.

Result: The method achieves 96.34% patch-level accuracy and generates consistent region-level correspondences.

Conclusion: The approach is effective and holds promise for real-world applications in manga processing.

Abstract: Understanding region-wise correspondence between manga line art images is a
fundamental task in manga processing, enabling downstream applications such as
automatic line art colorization and in-between frame generation. However, this
task remains largely unexplored, especially in realistic scenarios without
pre-existing segmentation or annotations. In this paper, we introduce a novel
and practical task: predicting region-wise correspondence between raw manga
line art images without any pre-existing labels or masks. To tackle this
problem, we divide each line art image into a set of patches and propose a
Transformer-based framework that learns patch-level similarities within and
across images. We then apply edge-aware clustering and a region matching
algorithm to convert patch-level predictions into coherent region-level
correspondences. To support training and evaluation, we develop an automatic
annotation pipeline and manually refine a subset of the data to construct
benchmark datasets. Experiments on multiple datasets demonstrate that our
method achieves high patch-level accuracy (e.g., 96.34%) and generates
consistent region-level correspondences, highlighting its potential for
real-world manga applications.

</details>


### [130] [Generative Diffusion Contrastive Network for Multi-View Clustering](https://arxiv.org/abs/2509.09527)
*Jian Zhu,Xin Zou,Xi Wang,Ning Zhang,Bian Wu,Yao Yang,Ying Zhou,Lingfang Zeng,Chang Tang,Cheng Luo*

Main category: cs.CV

TL;DR: The paper introduces the Stochastic Generative Diffusion Fusion (SGDF) method to address low-quality data in Multi-View Clustering and extends it to the Generative Diffusion Contrastive Network (GDCN), achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To improve clustering analysis in the presence of noisy and missing data in multiple views, which hampers the performance of multi-view fusion in deep learning-based Multi-View Clustering.

Method: The paper proposes the SGDF method, which uses multiple generative mechanisms to handle multi-view features in clustering, making it robust to low-quality data. GDCN is then built upon SGDF for further improvements.

Result: GDCN demonstrates state-of-the-art results in multi-view clustering tasks, validated by extensive experiments.

Conclusion: The proposed methods (SGDF and GDCN) effectively enhance clustering performance and robustness in multi-view data scenarios, offering advancements in deep Multi-View Clustering.

Abstract: In recent years, Multi-View Clustering (MVC) has been significantly advanced
under the influence of deep learning. By integrating heterogeneous data from
multiple views, MVC enhances clustering analysis, making multi-view fusion
critical to clustering performance. However, there is a problem of low-quality
data in multi-view fusion. This problem primarily arises from two reasons: 1)
Certain views are contaminated by noisy data. 2) Some views suffer from missing
data. This paper proposes a novel Stochastic Generative Diffusion Fusion (SGDF)
method to address this problem. SGDF leverages a multiple generative mechanism
for the multi-view feature of each sample. It is robust to low-quality data.
Building on SGDF, we further present the Generative Diffusion Contrastive
Network (GDCN). Extensive experiments show that GDCN achieves the
state-of-the-art results in deep MVC tasks. The source code is publicly
available at https://github.com/HackerHyper/GDCN.

</details>


### [131] [DualTrack: Sensorless 3D Ultrasound needs Local and Global Context](https://arxiv.org/abs/2509.09530)
*Paul F. R. Wilson,Matteo Ronchetti,Rüdiger Göbl,Viktoria Markova,Sebastian Rosenzweig,Raphael Prevost,Parvin Mousavi,Oliver Zettinig*

Main category: cs.CV

TL;DR: DualTrack is a novel 3D ultrasound method that achieves accurate and consistent 3D reconstructions, reducing errors to below 5 mm compared to previous models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of costly and complex traditional 3D ultrasound systems, providing an alternative via sensorless 3D ultrasound with deep learning to predict probe trajectory.

Method: They propose DualTrack, a dual-encoder architecture that separates local feature extraction (dense spatiotemporal convolutions) and global feature modeling (2D CNNs or foundation models with temporal attention), combining them via a fusion module.

Result: Experimental results demonstrated that DualTrack outperforms previous approaches in state-of-the-art accuracy, achieving globally consistent 3D reconstructions with average errors below 5 mm.

Conclusion: DualTrack offers a promising and accurate innovation in 3D ultrasound, providing clinical advantages through its decoupled feature extraction, leading to improved reconstruction performance.

Abstract: Three-dimensional ultrasound (US) offers many clinical advantages over
conventional 2D imaging, yet its widespread adoption is limited by the cost and
complexity of traditional 3D systems. Sensorless 3D US, which uses deep
learning to estimate a 3D probe trajectory from a sequence of 2D US images, is
a promising alternative. Local features, such as speckle patterns, can help
predict frame-to-frame motion, while global features, such as coarse shapes and
anatomical structures, can situate the scan relative to anatomy and help
predict its general shape. In prior approaches, global features are either
ignored or tightly coupled with local feature extraction, restricting the
ability to robustly model these two complementary aspects. We propose
DualTrack, a novel dual-encoder architecture that leverages decoupled local and
global encoders specialized for their respective scales of feature extraction.
The local encoder uses dense spatiotemporal convolutions to capture
fine-grained features, while the global encoder utilizes an image backbone
(e.g., a 2D CNN or foundation model) and temporal attention layers to embed
high-level anatomical features and long-range dependencies. A lightweight
fusion module then combines these features to estimate the trajectory.
Experimental results on a large public benchmark show that DualTrack achieves
state-of-the-art accuracy and globally consistent 3D reconstructions,
outperforming previous methods and yielding an average reconstruction error
below 5 mm.

</details>


### [132] [Improving Video Diffusion Transformer Training by Multi-Feature Fusion and Alignment from Self-Supervised Vision Encoders](https://arxiv.org/abs/2509.09547)
*Dohun Lee,Hyeonho Jeong,Jiwook Kim,Duygu Ceylan,Jong Chul Ye*

Main category: cs.CV

TL;DR: The paper introduces Align4Gen, an approach to improve video diffusion models by aligning their features with representations from pre-trained vision encoders. Their method enhances video generation quality for various tasks.


<details>
  <summary>Details</summary>
Motivation: Current video diffusion models lack adequate representation power, and feature alignment with pre-trained vision encoders may address this gap.

Method: The authors propose Align4Gen, a framework that incorporates multi-feature fusion and alignment into video diffusion model training, leveraging discriminability and temporal consistency of vision encoders.

Result: Align4Gen improves video generation performance in both unconditional and class-conditional tasks, as shown through multiple evaluation metrics.

Conclusion: Feature alignment using vision encoders enhances video diffusion models' generation quality, presenting a promising direction for future research in this domain.

Abstract: Video diffusion models have advanced rapidly in the recent years as a result
of series of architectural innovations (e.g., diffusion transformers) and use
of novel training objectives (e.g., flow matching). In contrast, less attention
has been paid to improving the feature representation power of such models. In
this work, we show that training video diffusion models can benefit from
aligning the intermediate features of the video generator with feature
representations of pre-trained vision encoders. We propose a new metric and
conduct an in-depth analysis of various vision encoders to evaluate their
discriminability and temporal consistency, thereby assessing their suitability
for video feature alignment. Based on the analysis, we present Align4Gen which
provides a novel multi-feature fusion and alignment method integrated into
video diffusion model training. We evaluate Align4Gen both for unconditional
and class-conditional video generation tasks and show that it results in
improved video generation as quantified by various metrics. Full video results
are available on our project page: https://align4gen.github.io/align4gen/

</details>


### [133] [InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation](https://arxiv.org/abs/2509.09555)
*Sirui Xu,Dongting Li,Yucheng Zhang,Xiyan Xu,Qi Long,Ziyin Wang,Yunzhi Lu,Shuchang Dong,Hezi Jiang,Akshat Gupta,Yu-Xiong Wang,Liang-Yan Gui*

Main category: cs.CV

TL;DR: The paper introduces InterAct, a new dataset and framework to advance 3D human-object interaction (HOI) modeling, addressing issues of data quality and limited resources in existing datasets.


<details>
  <summary>Details</summary>
Motivation: Current 3D HOI datasets lack quality and annotations, and exhibit issues such as motion artifacts and inaccurate interactions, which hinder progress in human-object interaction modeling.

Method: The authors consolidate diverse HOI datasets, refine the data with an optimization framework, expand motion variations, and establish six benchmarking tasks with a unified generative modeling perspective.

Result: The new InterAct dataset offers 30.70 hours of improved HOI data, achieves state-of-the-art performance in HOI generative modeling, and validates its effectiveness through extensive experiments.

Conclusion: InterAct is a pivotal step toward improving 3D HOI modeling and is made publicly available to promote further research in this domain.

Abstract: While large-scale human motion capture datasets have advanced human motion
generation, modeling and generating dynamic 3D human-object interactions (HOIs)
remain challenging due to dataset limitations. Existing datasets often lack
extensive, high-quality motion and annotation and exhibit artifacts such as
contact penetration, floating, and incorrect hand motions. To address these
issues, we introduce InterAct, a large-scale 3D HOI benchmark featuring dataset
and methodological advancements. First, we consolidate and standardize 21.81
hours of HOI data from diverse sources, enriching it with detailed textual
annotations. Second, we propose a unified optimization framework to enhance
data quality by reducing artifacts and correcting hand motions. Leveraging the
principle of contact invariance, we maintain human-object relationships while
introducing motion variations, expanding the dataset to 30.70 hours. Third, we
define six benchmarking tasks and develop a unified HOI generative modeling
perspective, achieving state-of-the-art performance. Extensive experiments
validate the utility of our dataset as a foundational resource for advancing 3D
human-object interaction generation. To support continued research in this
area, the dataset is publicly available at
https://github.com/wzyabcas/InterAct, and will be actively maintained.

</details>


### [134] [Invisible Attributes, Visible Biases: Exploring Demographic Shortcuts in MRI-based Alzheimer's Disease Classification](https://arxiv.org/abs/2509.09558)
*Akshit Achara,Esther Puyol Anton,Alexander Hammers,Andrew P. King*

Main category: cs.CV

TL;DR: This paper explores demographic bias in deep learning algorithms for Alzheimer’s disease classification using MRI scans, demonstrating that race and sex-based shortcut learning can affect performance.


<details>
  <summary>Details</summary>
Motivation: To address concerns about shortcut learning in DL algorithms potentially causing biased Alzheimer’s disease diagnosis from MRI scans based on race and sex demographics.

Method: The researchers assessed DL algorithms' ability to infer race or sex from brain MRI data, analyzed performance under training set imbalances by race/sex, and conducted feature attribution analyses using ResNet and SwinTransformer.

Result: They demonstrated that DL models exhibit race and sex-based shortcut learning, resulting in biased and unequal performance in Alzheimer's disease classification tasks.

Conclusion: The study highlights the need for mitigating bias in DL medical diagnostic tools by ensuring fairness across protected demographic groups like race and sex.

Abstract: Magnetic resonance imaging (MRI) is the gold standard for brain imaging. Deep
learning (DL) algorithms have been proposed to aid in the diagnosis of diseases
such as Alzheimer's disease (AD) from MRI scans. However, DL algorithms can
suffer from shortcut learning, in which spurious features, not directly related
to the output label, are used for prediction. When these features are related
to protected attributes, they can lead to performance bias against
underrepresented protected groups, such as those defined by race and sex. In
this work, we explore the potential for shortcut learning and demographic bias
in DL based AD diagnosis from MRI. We first investigate if DL algorithms can
identify race or sex from 3D brain MRI scans to establish the presence or
otherwise of race and sex based distributional shifts. Next, we investigate
whether training set imbalance by race or sex can cause a drop in model
performance, indicating shortcut learning and bias. Finally, we conduct a
quantitative and qualitative analysis of feature attributions in different
brain regions for both the protected attribute and AD classification tasks.
Through these experiments, and using multiple datasets and DL models (ResNet
and SwinTransformer), we demonstrate the existence of both race and sex based
shortcut learning and bias in DL based AD classification. Our work lays the
foundation for fairer DL diagnostic tools in brain MRI. The code is provided at
https://github.com/acharaakshit/ShortMR

</details>


### [135] [PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient Fine-Tuning for Remote Sensing Change Detection](https://arxiv.org/abs/2509.09572)
*Sijun Dong,Yuxuan Hu,LiBo Wang,Geng Chen,Xiaoliang Meng*

Main category: cs.CV

TL;DR: PeftCD is a remote sensing change detection framework leveraging Vision Foundation Models with Parameter-Efficient Fine-Tuning to improve accuracy, efficiency, and generalization in change detection tasks.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of pseudo changes, limited labeled samples, and cross-domain generalization in multi-temporal and multi-source remote sensing imagery.

Method: The framework uses a weight-sharing Siamese encoder with LoRA and Adapter modules integrated, based on VFMs like SAM2 and DINOv3, along with lightweight decoding for efficient task adaptation.

Result: Achieved state-of-the-art performance across datasets with precise boundary delineation and effective suppression of pseudo-changes.

Conclusion: PeftCD demonstrates an ideal balance between accuracy, efficiency, and scalability, making it highly effective for real-world remote sensing change detection.

Abstract: To tackle the prevalence of pseudo changes, the scarcity of labeled samples,
and the difficulty of cross-domain generalization in multi-temporal and
multi-source remote sensing imagery, we propose PeftCD, a change detection
framework built upon Vision Foundation Models (VFMs) with Parameter-Efficient
Fine-Tuning (PEFT). At its core, PeftCD employs a weight-sharing Siamese
encoder derived from a VFM, into which LoRA and Adapter modules are seamlessly
integrated. This design enables highly efficient task adaptation by training
only a minimal set of additional parameters. To fully unlock the potential of
VFMs, we investigate two leading backbones: the Segment Anything Model v2
(SAM2), renowned for its strong segmentation priors, and DINOv3, a
state-of-the-art self-supervised representation learner. The framework is
complemented by a deliberately lightweight decoder, ensuring the focus remains
on the powerful feature representations from the backbones. Extensive
experiments demonstrate that PeftCD achieves state-of-the-art performance
across multiple public datasets, including SYSU-CD (IoU 73.81%), WHUCD
(92.05%), MSRSCD (64.07%), MLCD (76.89%), CDD (97.01%), S2Looking (52.25%) and
LEVIR-CD (85.62%), with notably precise boundary delineation and strong
suppression of pseudo-changes. In summary, PeftCD presents an optimal balance
of accuracy, efficiency, and generalization. It offers a powerful and scalable
paradigm for adapting large-scale VFMs to real-world remote sensing change
detection applications. The code and pretrained models will be released at
https://github.com/dyzy41/PeftCD.

</details>


### [136] [Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis](https://arxiv.org/abs/2509.09595)
*Yikang Ding,Jiwen Liu,Wenyuan Zhang,Zekun Wang,Wentao Hu,Liyuan Cui,Mingming Lao,Yingchao Shao,Hui Liu,Xiaohan Li,Ming Chen,Xiaoqiang Liu,Yu-Shen Liu,Pengfei Wan*

Main category: cs.CV

TL;DR: Kling-Avatar introduces a two-stage framework combining multimodal instruction understanding with photorealistic portrait generation to produce realistic, expressive, long-duration videos driven by audio cues.


<details>
  <summary>Details</summary>
Motivation: Existing avatar generation methods fail to capture the communicative purpose conveyed by multimodal instructions, leading to issues in narrative coherence and character expressiveness.

Method: Kling-Avatar employs a two-stage pipeline. First, a multimodal large language model (MLLM) creates a blueprint video based on instruction signals. Second, a first-last frame strategy generates multiple sub-clips guided by the blueprint keyframes for high fidelity and expressiveness.

Result: Kling-Avatar demonstrates superior performance in lip synchronization, emotion portrayal, instruction controllability, and identity preservation. It can generate vivid long-duration videos up to 1080p and 48 fps, suitable for applications like livestreaming.

Conclusion: Kling-Avatar sets a new benchmark for high-fidelity audio-driven avatar synthesis, unifying semantic modeling and expressive video generation while offering scalability for real-world applications.

Abstract: Recent advances in audio-driven avatar video generation have significantly
enhanced audio-visual realism. However, existing methods treat instruction
conditioning merely as low-level tracking driven by acoustic or visual cues,
without modeling the communicative purpose conveyed by the instructions. This
limitation compromises their narrative coherence and character expressiveness.
To bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that
unifies multimodal instruction understanding with photorealistic portrait
generation. Our approach adopts a two-stage pipeline. In the first stage, we
design a multimodal large language model (MLLM) director that produces a
blueprint video conditioned on diverse instruction signals, thereby governing
high-level semantics such as character motion and emotions. In the second
stage, guided by blueprint keyframes, we generate multiple sub-clips in
parallel using a first-last frame strategy. This global-to-local framework
preserves fine-grained details while faithfully encoding the high-level intent
behind multimodal instructions. Our parallel architecture also enables fast and
stable generation of long-duration videos, making it suitable for real-world
applications such as digital human livestreaming and vlogging. To
comprehensively evaluate our method, we construct a benchmark of 375 curated
samples covering diverse instructions and challenging scenarios. Extensive
experiments demonstrate that Kling-Avatar is capable of generating vivid,
fluent, long-duration videos at up to 1080p and 48 fps, achieving superior
performance in lip synchronization accuracy, emotion and dynamic
expressiveness, instruction controllability, identity preservation, and
cross-domain generalization. These results establish Kling-Avatar as a new
benchmark for semantically grounded, high-fidelity audio-driven avatar
synthesis.

</details>


### [137] [Mechanistic Learning with Guided Diffusion Models to Predict Spatio-Temporal Brain Tumor Growth](https://arxiv.org/abs/2509.09610)
*Daria Laslo,Efthymios Georgiou,Marius George Linguraru,Andreas Rauschecker,Sabine Muller,Catherine R. Jutzeler,Sarah Bruningk*

Main category: cs.CV

TL;DR: The paper proposes a hybrid framework integrating a tumor growth model with guided diffusion denoising for predicting brain tumor progression through future MRI scans.


<details>
  <summary>Details</summary>
Motivation: Predicting tumor progression spatially and temporally is critical for enhancing clinical decision-making in neuro-oncology.

Method: The framework combines a mathematical tumor growth model (ordinary differential equations) and a guided denoising diffusion implicit model (DDIM) for image synthesis of future MRIs using tumor estimates as conditioning inputs.

Result: Using BraTS datasets and pediatric glioma scans, the model demonstrates realistic anonymized follow-up MRI generation and predicts tumor growth effectively, validated through metrics like Hausdorff Distance.

Conclusion: The approach provides biologically realistic spatiotemporal tumor progression predictions, expanding interpretations in scenarios with limited data and mechanistic understanding.

Abstract: Predicting the spatio-temporal progression of brain tumors is essential for
guiding clinical decisions in neuro-oncology. We propose a hybrid mechanistic
learning framework that combines a mathematical tumor growth model with a
guided denoising diffusion implicit model (DDIM) to synthesize anatomically
feasible future MRIs from preceding scans. The mechanistic model, formulated as
a system of ordinary differential equations, captures temporal tumor dynamics
including radiotherapy effects and estimates future tumor burden. These
estimates condition a gradient-guided DDIM, enabling image synthesis that
aligns with both predicted growth and patient anatomy. We train our model on
the BraTS adult and pediatric glioma datasets and evaluate on 60 axial slices
of in-house longitudinal pediatric diffuse midline glioma (DMG) cases. Our
framework generates realistic follow-up scans based on spatial similarity
metrics. It also introduces tumor growth probability maps, which capture both
clinically relevant extent and directionality of tumor growth as shown by 95th
percentile Hausdorff Distance. The method enables biologically informed image
generation in data-limited scenarios, offering generative-space-time
predictions that account for mechanistic priors.

</details>


### [138] [Measuring Epistemic Humility in Multimodal Large Language Models](https://arxiv.org/abs/2509.09658)
*Bingkui Tong,Jiaer Xia,Sifeng Shang,Kaiyang Zhou*

Main category: cs.CV

TL;DR: The paper introduces a benchmark called HumbleBench to evaluate Multimodal Large Language Models (MLLMs) on their ability to reject incorrect answers, a measure crucial for safety-critical applications.


<details>
  <summary>Details</summary>
Motivation: To address the risks posed by hallucinations in MLLMs and the current lack of benchmarks that evaluate a model's ability to reject invalid answers, ensuring more trustworthy AI systems.

Method: The authors developed HumbleBench using fine-grained scene graph annotations from a panoptic dataset. They employed GPT-4-Turbo to generate multiple-choice questions with manual curation, ensuring the inclusion of a "None of the above" option. Various state-of-the-art MLLMs were tested.

Result: The benchmark revealed key insights on the capability gaps of current MLLMs in rejecting hallucinated answers across object, relation, and attribute types.

Conclusion: HumbleBench bridges an essential evaluation gap by testing MLLMs' epistemic humility and helps assess their reliability in high-stakes scenarios. The dataset and code are made publicly available for community use.

Abstract: Hallucinations in multimodal large language models (MLLMs) -- where the model
generates content inconsistent with the input image -- pose significant risks
in real-world applications, from misinformation in visual question answering to
unsafe errors in decision-making. Existing benchmarks primarily test
recognition accuracy, i.e., evaluating whether models can select the correct
answer among distractors. This overlooks an equally critical capability for
trustworthy AI: recognizing when none of the provided options are correct, a
behavior reflecting epistemic humility. We present HumbleBench, a new
hallucination benchmark designed to evaluate MLLMs' ability to reject plausible
but incorrect answers across three hallucination types: object, relation, and
attribute. Built from a panoptic scene graph dataset, we leverage fine-grained
scene graph annotations to extract ground-truth entities and relations, and
prompt GPT-4-Turbo to generate multiple-choice questions, followed by a
rigorous manual filtering process. Each question includes a "None of the above"
option, requiring models not only to recognize correct visual information but
also to identify when no provided answer is valid. We evaluate a variety of
state-of-the-art MLLMs -- including both general-purpose and specialized
reasoning models -- on HumbleBench and share valuable findings and insights
with the community. By incorporating explicit false-option rejection,
HumbleBench fills a key gap in current evaluation suites, providing a more
realistic measure of MLLM reliability in safety-critical settings. Our code and
dataset are released publicly and can be accessed at
https://github.com/maifoundations/HumbleBench.

</details>


### [139] [Can Understanding and Generation Truly Benefit Together -- or Just Coexist?](https://arxiv.org/abs/2509.09666)
*Zhiyuan Yan,Kaiqing Lin,Zongjian Li,Junyan Ye,Hui Han,Zhendong Wang,Hao Liu,Bin Lin,Hao Li,Xue Xu,Xinyan Xiao,Jingdong Wang,Haifeng Wang,Li Yuan*

Main category: cs.CV

TL;DR: The paper introduces UAE, a framework for unified multimodal learning, using an encoder-decoder auto-encoder paradigm to improve both image-to-text understanding and text-to-image generation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to create a unified framework to bridge the gap between multimodal understanding (image-to-text) and generation (text-to-image) through a bidirectional information flow.

Method: The UAE framework involves pre-training the decoder with large-scale image captions and uses a three-stage RL-based strategy: initialization, improving encoding for better captions, and refining decoding for better reconstructions.

Result: The encoder autonomously generates more descriptive captions, and the decoder demonstrates improved reconstruction fidelity as RL progresses.

Conclusion: UAE achieves significant advances in unified multimodal tasks, with improved coherence, long-context understanding, and generation fidelity, evaluated using a newly introduced benchmark.

Abstract: In this paper, we introduce an insightful paradigm through the Auto-Encoder
lens-understanding as the encoder (I2T) that compresses images into text, and
generation as the decoder (T2I) that reconstructs images from that text. Using
reconstruction fidelity as the unified training objective, we enforce the
coherent bidirectional information flow between the understanding and
generation processes, bringing mutual gains. To implement this, we propose UAE,
a novel framework for unified multimodal learning. We begin by pre-training the
decoder with large-scale long-context image captions to capture fine-grained
semantic and complex spatial relationships. We then propose Unified-GRPO via
reinforcement learning (RL), which covers three stages: (1) A cold-start phase
to gently initialize both encoder and decoder with a semantic reconstruction
loss; (2) Generation for Understanding, where the encoder is trained to
generate informative captions that maximize the decoder's reconstruction
quality, enhancing its visual understanding; (3) Understanding for Generation,
where the decoder is refined to reconstruct from these captions, forcing it to
leverage every detail and improving its long-context instruction following and
generation fidelity. For evaluation, we introduce Unified-Bench, the first
benchmark tailored to assess the degree of unification of the UMMs. A
surprising "aha moment" arises within the multimodal learning domain: as RL
progresses, the encoder autonomously produces more descriptive captions, while
the decoder simultaneously demonstrates a profound ability to understand these
intricate descriptions, resulting in reconstructions of striking fidelity.

</details>


### [140] [Geometric Neural Distance Fields for Learning Human Motion Priors](https://arxiv.org/abs/2509.09667)
*Zhengdi Yu,Simone Foti,Linguang Zhang,Amy Zhao,Cem Keskin,Stefanos Zafeiriou,Tolga Birdal*

Main category: cs.CV

TL;DR: Neural Riemannian Motion Fields (NRMF) is a new generative model for 3D human motion recovery that ensures consistent and physically plausible motions using geometric insights.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in existing human motion modeling methods like VAEs and diffusion models, and to create a model that ensures robustness, temporal consistency, and physical plausibility.

Method: Uses neural distance fields (NDFs) to model human motion dynamics across pose, velocity, and acceleration in a mathematically rigorous way. Introduces an adaptive-step hybrid projection algorithm and a geometric integrator for realistic motion generation.

Result: NRMF demonstrates superior performance on the AMASS dataset, generalizing across modalities and tasks like denoising, motion in-betweening, and partial 2D/3D motion fitting.

Conclusion: NRMF provides a robust framework to improve 3D human motion recovery, systematically ensuring plausible and generalizable results for diverse applications.

Abstract: We introduce Neural Riemannian Motion Fields (NRMF), a novel 3D generative
human motion prior that enables robust, temporally consistent, and physically
plausible 3D motion recovery. Unlike existing VAE or diffusion-based methods,
our higher-order motion prior explicitly models the human motion in the zero
level set of a collection of neural distance fields (NDFs) corresponding to
pose, transition (velocity), and acceleration dynamics. Our framework is
rigorous in the sense that our NDFs are constructed on the product space of
joint rotations, their angular velocities, and angular accelerations,
respecting the geometry of the underlying articulations. We further introduce:
(i) a novel adaptive-step hybrid algorithm for projecting onto the set of
plausible motions, and (ii) a novel geometric integrator to "roll out"
realistic motion trajectories during test-time-optimization and generation. Our
experiments show significant and consistent gains: trained on the AMASS
dataset, NRMF remarkably generalizes across multiple input modalities and to
diverse tasks ranging from denoising to motion in-betweening and fitting to
partial 2D / 3D observations.

</details>


### [141] [Locality in Image Diffusion Models Emerges from Data Statistics](https://arxiv.org/abs/2509.09672)
*Artem Lukoianov,Chenyang Yuan,Justin Solomon,Vincent Sitzmann*

Main category: cs.CV

TL;DR: The paper explores locality in deep diffusion models, demonstrating it stems from dataset statistics rather than CNN biases, and develops a better analytical denoiser.


<details>
  <summary>Details</summary>
Motivation: To close the gap between the theoretical optimal denoiser and practical deep diffusion models by understanding the root of locality in these models.

Method: The authors compare an optimal linear parametric denoiser with deep neural network-based denoisers, analyze pixel correlations in datasets, and craft an improved analytical denoiser.

Result: The locality in deep diffusion models is shown to emerge from statistical properties of image datasets. An improved analytical denoiser resembling deep diffusion models is developed.

Conclusion: Locality in deep diffusion models is inherently tied to dataset statistics, not CNN inductive biases, leading to insights for better denoiser design.

Abstract: Among generative models, diffusion models are uniquely intriguing due to the
existence of a closed-form optimal minimizer of their training objective, often
referred to as the optimal denoiser. However, diffusion using this optimal
denoiser merely reproduces images in the training set and hence fails to
capture the behavior of deep diffusion models. Recent work has attempted to
characterize this gap between the optimal denoiser and deep diffusion models,
proposing analytical, training-free models that can generate images that
resemble those generated by a trained UNet. The best-performing method
hypothesizes that shift equivariance and locality inductive biases of
convolutional neural networks are the cause of the performance gap, hence
incorporating these assumptions into its analytical model. In this work, we
present evidence that the locality in deep diffusion models emerges as a
statistical property of the image dataset, not due to the inductive bias of
convolutional neural networks. Specifically, we demonstrate that an optimal
parametric linear denoiser exhibits similar locality properties to the deep
neural denoisers. We further show, both theoretically and experimentally, that
this locality arises directly from the pixel correlations present in natural
image datasets. Finally, we use these insights to craft an analytical denoiser
that better matches scores predicted by a deep diffusion model than the prior
expert-crafted alternative.

</details>


### [142] [SpatialVID: A Large-Scale Video Dataset with Spatial Annotations](https://arxiv.org/abs/2509.09676)
*Jiahao Wang,Yufeng Yuan,Rujie Zheng,Youtian Lin,Jian Gao,Lin-Zhuo Chen,Yajie Bao,Yi Zhang,Chang Zeng,Yanxi Zhou,Xiaoxiao Long,Hao Zhu,Zhaoxiang Zhang,Xun Cao,Yao Yao*

Main category: cs.CV

TL;DR: The paper introduces SpatialVID, a large-scale dataset of in-the-wild video clips annotated with spatial and semantic information for training models in video and 3D vision tasks.


<details>
  <summary>Details</summary>
Motivation: The scalability and real-world applicability of spatial intelligence models are limited by the lack of diverse, high-quality annotated datasets, particularly for dynamic real-world scenes.

Method: The authors collected over 21,000 hours of raw video, processed them into clips using hierarchical filtering, and added detailed spatial and semantic annotations like camera poses, depth maps, and motion instructions.

Result: SpatialVID comprises 7,089 hours of dynamic content, enriched with comprehensive annotations, showcasing greater diversity and depth compared to existing datasets.

Conclusion: SpatialVID is a valuable resource for enhancing model generalization and advancing research in video and 3D vision, addressing key limitations of current datasets.

Abstract: Significant progress has been made in spatial intelligence, spanning both
spatial reconstruction and world exploration. However, the scalability and
real-world fidelity of current models remain severely constrained by the
scarcity of large-scale, high-quality training data. While several datasets
provide camera pose information, they are typically limited in scale,
diversity, and annotation richness, particularly for real-world dynamic scenes
with ground-truth camera motion. To this end, we collect \textbf{SpatialVID}, a
dataset consists of a large corpus of in-the-wild videos with diverse scenes,
camera movements and dense 3D annotations such as per-frame camera poses,
depth, and motion instructions. Specifically, we collect more than 21,000 hours
of raw video, and process them into 2.7 million clips through a hierarchical
filtering pipeline, totaling 7,089 hours of dynamic content. A subsequent
annotation pipeline enriches these clips with detailed spatial and semantic
information, including camera poses, depth maps, dynamic masks, structured
captions, and serialized motion instructions. Analysis of SpatialVID's data
statistics reveals a richness and diversity that directly foster improved model
generalization and performance, establishing it as a key asset for the video
and 3D vision research community.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [143] [WebAssembly and Unikernels: A Comparative Study for Serverless at the Edge](https://arxiv.org/abs/2509.09400)
*Valerio Besozzi,Enrico Fiasco,Marco Danelutto,Patrizio Dazzi*

Main category: cs.DC

TL;DR: This paper compares WebAssembly and unikernel-based MicroVMs for serverless edge computing workloads, introducing Limes (a WebAssembly runtime) and analyzing their performance differences.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the need for lightweight execution environments with minimal cold start latency in Urgent Edge Computing scenarios.

Method: The paper develops Limes, a WebAssembly runtime based on Wasmtime, and evaluates it against Firecracker-based MicroVMs (SPARE) in serverless workloads.

Result: WebAssembly provides lower cold start latency for lightweight tasks but struggles with complex workloads, while Firecracker achieves steady cold starts and performs better for I/O-heavy tasks.

Conclusion: Both WebAssembly and Firecracker have distinct strengths, suggesting a tradeoff between startup latency and workload complexity/execution performance in edge computing scenarios.

Abstract: Serverless computing at the edge requires lightweight execution environments
to minimize cold start latency, especially in Urgent Edge Computing (UEC). This
paper compares WebAssembly and unikernel-based MicroVMs for serverless
workloads. We present Limes, a WebAssembly runtime built on Wasmtime, and
evaluate it against the Firecracker-based environment used in SPARE. Results
show that WebAssembly offers lower cold start times for lightweight functions
but suffers with complex workloads, while Firecracker provides higher, but
stable, cold starts and better execution performance, particularly for
I/O-heavy tasks.

</details>


### [144] [A Comparative Analysis of Identifier Schemes: UUIDv4, UUIDv7, and ULID for Distributed Systems](https://arxiv.org/abs/2509.08969)
*Nima Karimian Kakolaki*

Main category: cs.DC

TL;DR: This paper analyzes distributed identifiers, showing ULIDs outperform UUIDv4 and UUIDv7 in network overhead, speed, and collision risk, recommending ULIDs for scalable applications.


<details>
  <summary>Details</summary>
Motivation: The paper aims to identify scalable and efficient identifier schemes for robust distributed systems, ensuring data uniqueness and high performance.

Method: The study compares identifier schemes (auto-increment keys, UUIDv4, UUIDv7, ULIDs) using collision probabilities, generation speed, and network overhead through mathematical calculations and simulated experiments.

Result: ULIDs reduce network overhead by 83.7%, increase generation speed by 97.32%, and lower collision risks by 98.42% compared to UUIDv7. They maintain negligible collision probabilities even at high rates.

Conclusion: ULIDs are optimal for high-performance distributed systems due to their efficiency, time-ordering, and scalability. The research promotes ULIDs for scalable applications and provides reproducible experiments via a public repository.

Abstract: Distributed systems require robust, scalable identifier schemes to ensure
data uniqueness and efficient indexing across multiple nodes. This paper
presents a comprehensive analysis of the evolution of distributed identifiers,
comparing traditional auto-increment keys with UUIDv4, UUIDv7, and ULIDs. We
combine mathematical calculation of collision probabilities with empirical
experiments measuring generation speed and network transmission overhead in a
simulated distributed environment. Results demonstrate that ULIDs significantly
outperform UUIDv4 and UUIDv7, reducing network overhead by 83.7% and increasing
generation speed by 97.32%. statistical analysis further shows ULIDs offer a
98.42% lower collision risk compared to UUIDv7, while maintaining negligible
collision probabilities even at high generation rates. These findings highlight
ULIDs as an optimal choice for high-performance distributed systems, providing
efficient, time-ordered, and lexicographically sortable identifiers suitable
for scalable applications. All source code, datasets, and analysis scripts
utilized in this research are publicly available in our dedicated repository at
https://github.com/nimakarimiank/uids-comparison. This repository contains
comprehensive documentation of the experimental setup, including configuration
files for the distributed environment, producer and consumer implementations,
and message broker integration. Additionally, it provides the data scripts and
datasets. Researchers and practitioners are encouraged to explore the
repository for full reproducibility of the experiments and to facilitate
further investigation or extension of the presented work.

</details>


### [145] [Optimizing the Variant Calling Pipeline Execution on Human Genomes Using GPU-Enabled Machines](https://arxiv.org/abs/2509.09058)
*Ajay Kumar,Praveen Rao,Peter Sanders*

Main category: cs.DC

TL;DR: The paper introduces a machine learning-based method to optimize the execution time of variant calling for human genomes in GPU-enabled cloud environments, achieving significant speedups compared to other approaches.


<details>
  <summary>Details</summary>
Motivation: Variant calling is computationally intensive, requiring robust solutions for efficient analysis of genomic data in cloud settings due to scalability and cost-efficiency concerns.

Method: The method uses machine learning to predict execution times for variant calling pipeline stages and applies job shop scheduling techniques to optimize workload distribution across machines, ensuring synchronization.

Result: The ML-based execution time prediction was effective, and the proposed method achieved a 2X speedup over a greedy approach and 1.6X speedup over a dynamic scheduling approach.

Conclusion: The approach demonstrates substantial performance improvements in variant calling pipelines by leveraging machine learning and optimized scheduling in GPU-enabled environments.

Abstract: Variant calling is the first step in analyzing a human genome and aims to
detect variants in an individual's genome compared to a reference genome. Due
to the computationally-intensive nature of variant calling, genomic data are
increasingly processed in cloud environments as large amounts of compute and
storage resources can be acquired with the pay-as-you-go pricing model. In this
paper, we address the problem of efficiently executing a variant calling
pipeline for a workload of human genomes on graphics processing unit
(GPU)-enabled machines. We propose a novel machine learning (ML)-based approach
for optimizing the workload execution to minimize the total execution time. Our
approach encompasses two key techniques: The first technique employs ML to
predict the execution times of different stages in a variant calling pipeline
based on the characteristics of a genome sequence. Using the predicted times,
the second technique generates optimal execution plans for the machines by
drawing inspiration from the flexible job shop scheduling problem. The plans
are executed via careful synchronization across different machines. We
evaluated our approach on a workload of publicly available genome sequences
using a testbed with different types of GPU hardware. We observed that our
approach was effective in predicting the execution times of variant calling
pipeline stages using ML on features such as sequence size, read quality,
percentage of duplicate reads, and average read length. In addition, our
approach achieved 2X speedup (on an average) over a greedy approach that also
used ML for predicting the execution times on the tested workload of sequences.
Finally, our approach achieved 1.6X speedup (on an average) over a dynamic
approach that executed the workload based on availability of resources without
using any ML-based time predictions.

</details>


### [146] [Coherence-Aware Task Graph Modeling for Realistic Application](https://arxiv.org/abs/2509.09094)
*Guochu Xiong,Xiangzhong Luo,Weichen Liu*

Main category: cs.DC

TL;DR: The paper introduces CoTAM, a framework for coherence-aware task graph modeling to address limitations in dynamic, data-dependent workload design.


<details>
  <summary>Details</summary>
Motivation: Current static approaches to task graph modeling can't effectively handle real-world applications that exhibit dynamic, coherence-influenced behavior.

Method: CoTAM generates unified task graphs by decoupling coherence effects, applying a learned weighting scheme, and inferring coherence-aware inter-task dependencies.

Result: Experimental validations indicate CoTAM outperforms implicit methods, capturing dynamic behaviors and bridging gaps between runtime and design assumptions.

Conclusion: Incorporating coherence interactions into task graph modeling enhances system-level analysis, achieving more accurate and generalizable insights for multicore system design.

Abstract: As multicore systems continue to scale, cache coherence has emerged as a
critical determinant of system performance, with coherence behavior and task
execution closely intertwined, reshaping inter-task dependencies. Task graph
modeling provides a structured way to capture such dependencies and serves as
the foundation for many system-level design strategies. However, these
strategies typically rely on predefined task graphs, while many real-world
applications lack explicit graphs and exhibit dynamic, data-dependent behavior,
limiting the effectiveness of static approaches. To address this, several task
graph modeling methods for realistic workloads have been developed. Yet, they
either rely on implicit techniques that use application-specific features
without producing explicit graphs, or they generate graphs tailored to fixed
scheduling models, which limits generality. More importantly, they often
overlook coherence interactions, creating a gap between design assumptions and
actual runtime behavior. To overcome these limitations, we propose CoTAM, a
Coherence-Aware Task Graph Modeling framework for realistic workloads that
constructs a unified task graph reflecting runtime behavior. CoTAM analyzes the
impact of coherence by decoupling its effects from overall execution,
quantifies its influence through a learned weighting scheme, and infers
inter-task dependencies for coherence-aware graph generation. Extensive
experiments show that CoTAM outperforms implicit methods, bridging the gap
between dynamic workload behavior and existing designs while demonstrating the
importance of incorporating cache coherence into task graph modeling for
accurate and generalizable system-level analysis.

</details>


### [147] [Barycentric Coded Distributed Computing with Flexible Recovery Threshold for Collaborative Mobile Edge Computing](https://arxiv.org/abs/2509.09435)
*Houming Qiu,Kun Zhu,Dusit Niyato,Nguyen Cong Luong,Changyan Yi,Chen Dai*

Main category: cs.DC

TL;DR: The paper introduces a more flexible and robust coded distributed computing (CDC) scheme to address the limitations of current systems in collaborative mobile edge computing (MEC).


<details>
  <summary>Details</summary>
Motivation: Existing CDC schemes for MEC systems face challenges such as rigidity in decoding recovery threshold requirements and numerical instability in computational results.

Method: The paper utilizes barycentric rational interpolation to design a CDC scheme that avoids poles in encoding/decoding, allows decoding from arbitrary returned results, supports both finite and real fields, and incorporates a new gradient coding algorithm.

Result: The experimental evaluation demonstrates that the proposed CDC scheme outperforms conventional methods in reducing waiting time and improving approximate accuracy.

Conclusion: The presented scheme advances the performance and usability of CDC methods in MEC environments, enhancing flexibility, numerical stability, and computational robustness.

Abstract: Collaborative mobile edge computing (MEC) has emerged as a promising paradigm
to enable low-capability edge nodes to cooperatively execute
computation-intensive tasks. However, straggling edge nodes (stragglers)
significantly degrade the performance of MEC systems by prolonging computation
latency. While coded distributed computing (CDC) as an effective technique is
widely adopted to mitigate straggler effects, existing CDC schemes exhibit two
critical limitations: (i) They cannot successfully decode the final result
unless the number of received results reaches a fixed recovery threshold, which
seriously restricts their flexibility; (ii) They suffer from inherent poles in
their encoding/decoding functions, leading to decoding inaccuracies and
numerical instability in the computational results. To address these
limitations, this paper proposes an approximated CDC scheme based on
barycentric rational interpolation. The proposed CDC scheme offers several
outstanding advantages. Firstly, it can decode the final result leveraging any
returned results from workers. Secondly, it supports computations over both
finite and real fields while ensuring numerical stability. Thirdly, its
encoding/decoding functions are free of poles, which not only enhances
approximation accuracy but also achieves flexible accuracy tuning. Fourthly, it
integrates a novel BRI-based gradient coding algorithm accelerating the
training process while providing robustness against stragglers. Finally,
experimental results reveal that the proposed scheme is superior to existing
CDC schemes in both waiting time and approximate accuracy.

</details>


### [148] [Weaker Assumptions for Asymmetric Trust](https://arxiv.org/abs/2509.09493)
*Ignacio Amores-Sesar,Christian Cachin,Juan Villacis*

Main category: cs.DC

TL;DR: This paper addresses distributed systems with asymmetric trust and proposes a less restrictive approach to achieve reliable broadcast and consensus.


<details>
  <summary>Details</summary>
Motivation: Existing solutions for systems with asymmetric trust introduce restrictive assumptions, negating the benefits of the asymmetric trust model.

Method: The paper introduces a new characterization of asymmetric problems and develops algorithms for reliable broadcast and consensus with weaker assumptions.

Result: The proposed algorithms maintain reliability without relying on overly restrictive trust assumptions.

Conclusion: The methodology offers a balanced trade-off by relaxing strict trust constraints while still achieving reliable broadcast and consensus in asymmetric trust systems.

Abstract: In distributed systems with asymmetric trust, each participant is free to
make its own trust assumptions about others, captured by an asymmetric quorum
system. This contrasts with ordinary, symmetric quorum systems and threshold
models, where trust assumptions are uniformly shared among participants.
Fundamental problems like reliable broadcast and consensus are unsolvable in
the asymmetric model if quorum systems satisfy only the classical properties of
consistency and availability. Existing approaches overcome this by introducing
stronger assumptions. We show that some of these assumptions are overly
restrictive, so much so that they effectively eliminate the benefits of
asymmetric trust. To address this, we propose a new approach to characterize
asymmetric problems and, building upon it, present algorithms for reliable
broadcast and consensus that require weaker assumptions than previous
solutions. Our methods are general and can be extended to other core problems
in systems with asymmetric trust.

</details>


### [149] [TrEnv: Transparently Share Serverless Execution Environments Across Different Functions and Nodes](https://arxiv.org/abs/2509.09525)
*Jialiang Huang,Teng Ma,Zheng Liu,Sixing Lin,Kang Chen,Jinlei Jiang,Xia Liao,Yingdi Shan,Yongwei Wu,Ning Zhang,Mengting Lu,Tao Ma,Haifeng Gong,Mingxing Zhang*

Main category: cs.DC

TL;DR: TrEnv is a high-density serverless platform designed to handle the unique demands of LLM agents, significantly reducing latency and memory usage compared to existing solutions.


<details>
  <summary>Details</summary>
Motivation: The need to address inefficiencies in serverless computing platforms for LLM agents, particularly concerning high costs, latency, and memory overhead.

Method: Introduced TrEnv, a co-designed serverless platform incorporating repurposable sandboxes, memory templates, browser sharing, and a page cache bypassing mechanism to optimize container and VM-based operations.

Result: TrEnv achieved up to 7X reduction in P99 latency and 48% lower memory usage for containers, and up to 58% lower P99 latency and 61% memory savings for VM-based agents compared to alternatives like E2B.

Conclusion: TrEnv proves to be a highly efficient and cost-effective serverless platform for the resource-variable demands of LLM agents, addressing critical performance bottlenecks in contemporary systems.

Abstract: Serverless computing provides dynamic scalability, but its infrastructure
overhead becomes a bottleneck for emerging workloads such as LLM agents, which
exhibit unpredictable invocation patterns and variable resource demands. Our
analysis shows that for these agents, the cost of running on serverless
platforms can reach up to 70% of the cost of LLM API calls. This finding
motivates the need for a more efficient, high-density serverless platform. We
present TrEnv, a co-designed serverless platform that supports both container-
and VM-based environments, optimized for the unique demands of LLM agents.
TrEnv reduces startup latency and memory usage through repurposable sandboxes
and memory templates, which enable fast reuse and restoration of execution
environments. To further reduce overhead in VM-based agent workloads, TrEnv
leverages browser sharing and a page cache bypassing mechanism. Evaluations
show that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in
container-based settings, and achieves up to 58% lower P99 latency and 61%
memory savings for VM-based agents compared to state-of-the-art systems like
E2B.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [150] [Uncertainty Estimation using Variance-Gated Distributions](https://arxiv.org/abs/2509.08846)
*H. Martin Gillis,Isaac Xu,Thomas Trappenberg*

Main category: cs.LG

TL;DR: The paper proposes a new framework for uncertainty estimation and decomposition in neural networks using the signal-to-noise ratio from class probability distributions and introduces a variance-gated measure for scaling predictions.


<details>
  <summary>Details</summary>
Motivation: Accurately quantifying per-sample uncertainty is critical in high-risk applications. Existing methods use predictive uncertainty decomposition but face criticism regarding additive decomposition methods, calling for a more reliable and intuitive framework.

Method: The authors suggest using the signal-to-noise ratio of class probability distributions across model predictions and propose a variance-gated measure that modifies predictions by factoring in confidence from ensembles.

Result: The variance-gated approach highlights challenges, specifically a potential collapse in the diversity of ensembles or committee machines.

Conclusion: The proposed signal-to-noise ratio-based framework and the variance-gated measure provide a fresh perspective on uncertainty decomposition and reveal problems with homogeneous committee diversity.

Abstract: Evaluation of per-sample uncertainty quantification from neural networks is
essential for decision-making involving high-risk applications. A common
approach is to use the predictive distribution from Bayesian or approximation
models and decompose the corresponding predictive uncertainty into epistemic
(model-related) and aleatoric (data-related) components. However, additive
decomposition has recently been questioned. In this work, we propose an
intuitive framework for uncertainty estimation and decomposition based on the
signal-to-noise ratio of class probability distributions across different model
predictions. We introduce a variance-gated measure that scales predictions by a
confidence factor derived from ensembles. We use this measure to discuss the
existence of a collapse in the diversity of committee machines.

</details>


### [151] [Instance-Optimal Matrix Multiplicative Weight Update and Its Quantum Applications](https://arxiv.org/abs/2509.08911)
*Weiyuan Gong,Tongyang Li,Xinzhao Wang,Zhiyu Zhang*

Main category: cs.LG

TL;DR: The paper presents an improved algorithm for the Matrix Learning from Expert Advice (LEA) problem with an enhanced regret bound, achieving better efficiency in quantum learning applications.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve the Matrix Multiplicative Weight Update (MMWU) algorithm by addressing limitations on the minimax-optimal regret bounds in matrix learning problems.

Method: The authors developed a general potential-based framework for matrix LEA and introduced an improved regret bound using an optimal potential function derived from the vector LEA problem and new mathematical inequalities.

Result: The improved algorithm achieves an instance-optimal regret bound with the same computational complexity as MMWU. It also demonstrates superior performance in quantum state learning tasks and predicting nonlinear quantum properties.

Conclusion: The algorithm effectively combines theoretical advances (improved regret bounds) with practical applications in quantum learning, outperforming existing methods while maintaining computational efficiency.

Abstract: The Matrix Multiplicative Weight Update (MMWU) is a seminal online learning
algorithm with numerous applications. Applied to the matrix version of the
Learning from Expert Advice (LEA) problem on the $d$-dimensional spectraplex,
it is well known that MMWU achieves the minimax-optimal regret bound of
$O(\sqrt{T\log d})$, where $T$ is the time horizon. In this paper, we present
an improved algorithm achieving the instance-optimal regret bound of
$O(\sqrt{T\cdot S(X||d^{-1}I_d)})$, where $X$ is the comparator in the regret,
$I_d$ is the identity matrix, and $S(\cdot||\cdot)$ denotes the quantum
relative entropy. Furthermore, our algorithm has the same computational
complexity as MMWU, indicating that the improvement in the regret bound is
``free''.
  Technically, we first develop a general potential-based framework for matrix
LEA, with MMWU being its special case induced by the standard exponential
potential. Then, the crux of our analysis is a new ``one-sided'' Jensen's trace
inequality built on a Laplace transform technique, which allows the application
of general potential functions beyond exponential to matrix LEA. Our algorithm
is finally induced by an optimal potential function from the vector LEA
problem, based on the imaginary error function.
  Complementing the above, we provide a memory lower bound for matrix LEA, and
explore the applications of our algorithm in quantum learning theory. We show
that it outperforms the state of the art for learning quantum states corrupted
by depolarization noise, random quantum states, and Gibbs states. In addition,
applying our algorithm to linearized convex losses enables predicting nonlinear
quantum properties, such as purity, quantum virtual cooling, and R\'{e}nyi-$2$
correlation.

</details>


### [152] [Corruption-Tolerant Asynchronous Q-Learning with Near-Optimal Rates](https://arxiv.org/abs/2509.08933)
*Sreejeet Maity,Aritra Mitra*

Main category: cs.LG

TL;DR: This paper introduces a robust Q-learning algorithm for discounted infinite-horizon RL problems, effectively handling adversarially corrupted reward signals while maintaining finite-time convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: Classical Q-learning algorithms struggle under adversarial corruption of reward signals, which can be caused by noise, sensor faults, or attacks. Overcoming this issue is important for robust reinforcement learning scenarios.

Method: The authors propose a robust variant of the Q-learning algorithm that adapts to adversarial corruption in asynchronous sampling. They also use a refined Azuma-Hoeffding inequality to ensure finite-time convergence even without prior knowledge of reward distribution statistics.

Result: Despite adversarial corruption, the algorithm achieves finite-time convergence rates comparable to non-corrupted settings, with an additive term proportional to corrupted samples. They also establish information-theoretic lower bounds for unavoidable corruption effects.

Conclusion: The proposed robust Q-learning algorithm bridges the gap in robust RL by offering finite-time robustness guarantees, handling corruption, and operating without prior statistical knowledge of reward distributions.

Abstract: We consider the problem of learning the optimal policy in a discounted,
infinite-horizon reinforcement learning (RL) setting where the reward signal is
subject to adversarial corruption. Such corruption, which may arise from
extreme noise, sensor faults, or malicious attacks, can severely degrade the
performance of classical algorithms such as Q-learning. To address this
challenge, we propose a new provably robust variant of the Q-learning algorithm
that operates effectively even when a fraction of the observed rewards are
arbitrarily perturbed by an adversary. Under the asynchronous sampling model
with time-correlated data, we establish that despite adversarial corruption,
the finite-time convergence rate of our algorithm matches that of existing
results for the non-adversarial case, up to an additive term proportional to
the fraction of corrupted samples. Moreover, we derive an information-theoretic
lower bound revealing that the additive corruption term in our upper bounds is
unavoidable.
  Next, we propose a variant of our algorithm that requires no prior knowledge
of the statistics of the true reward distributions. The analysis of this
setting is particularly challenging and is enabled by carefully exploiting a
refined Azuma-Hoeffding inequality for almost-martingales, a technical tool
that might be of independent interest. Collectively, our contributions provide
the first finite-time robustness guarantees for asynchronous Q-learning,
bridging a significant gap in robust RL.

</details>


### [153] [Group Distributionally Robust Machine Learning under Group Level Distributional Uncertainty](https://arxiv.org/abs/2509.08942)
*Xenia Konti,Yi Shen,Zifan Wang,Karl Henrik Johansson,Michael J. Pencina,Nicoleta J. Economou-Zavlanos,Michael M. Zavlanos*

Main category: cs.LG

TL;DR: The paper introduces a novel approach using Wasserstein-based DRO to improve worst-group performance in machine learning with heterogeneous data sources.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address performance degradation in standard ML models arising from spurious correlations and underrepresentation in diverse data environments.

Method: The paper uses Wasserstein-based distributionally robust optimization and develops a gradient descent-ascent algorithm with convergence guarantees.

Result: The method demonstrated effectiveness through validation on real-world data.

Conclusion: The proposed framework enhances robustness and worst-group performance in noisy, non-stationary, and evolving settings.

Abstract: The performance of machine learning (ML) models critically depends on the
quality and representativeness of the training data. In applications with
multiple heterogeneous data generating sources, standard ML methods often learn
spurious correlations that perform well on average but degrade performance for
atypical or underrepresented groups. Prior work addresses this issue by
optimizing the worst-group performance. However, these approaches typically
assume that the underlying data distributions for each group can be accurately
estimated using the training data, a condition that is frequently violated in
noisy, non-stationary, and evolving environments. In this work, we propose a
novel framework that relies on Wasserstein-based distributionally robust
optimization (DRO) to account for the distributional uncertainty within each
group, while simultaneously preserving the objective of improving the
worst-group performance. We develop a gradient descent-ascent algorithm to
solve the proposed DRO problem and provide convergence results. Finally, we
validate the effectiveness of our method on real-world data.

</details>


### [154] [FoundationalECGNet: A Lightweight Foundational Model for ECG-based Multitask Cardiac Analysis](https://arxiv.org/abs/2509.08961)
*Md. Sajeebul Islam Sk.,Md Jobayer,Md Mehedi Hasan Shawon,Md. Golam Raibul Alam*

Main category: cs.LG

TL;DR: This paper introduces FoundationalECGNet, a framework for precise ECG signal classification achieving superior F1-scores across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Cardiovascular diseases remain a leading global mortality cause, necessitating accurate and scalable ECG diagnostic systems.

Method: FoundationalECGNet integrates dual-stage wavelet denoising, CBAM, GAT, and TST for spatial-temporal ECG signal analysis, and performs a two-phase classification.

Result: The model achieved a 99% F1-score for Normal vs. Abnormal ECG classification and state-of-the-art results in multi-class disease detection.

Conclusion: FoundationalECGNet offers a scalable and interpretable ECG solution, significantly improving diagnostic accuracy and aiding clinical decisions.

Abstract: Cardiovascular diseases (CVDs) remain a leading cause of mortality worldwide,
underscoring the importance of accurate and scalable diagnostic systems.
Electrocardiogram (ECG) analysis is central to detecting cardiac abnormalities,
yet challenges such as noise, class imbalance, and dataset heterogeneity limit
current methods. To address these issues, we propose FoundationalECGNet, a
foundational framework for automated ECG classification. The model integrates a
dual-stage denoising by Morlet and Daubechies wavelets transformation,
Convolutional Block Attention Module (CBAM), Graph Attention Networks (GAT),
and Time Series Transformers (TST) to jointly capture spatial and temporal
dependencies in multi-channel ECG signals. FoundationalECGNet first
distinguishes between Normal and Abnormal ECG signals, and then classifies the
Abnormal signals into one of five cardiac conditions: Arrhythmias, Conduction
Disorders, Myocardial Infarction, QT Abnormalities, or Hypertrophy. Across
multiple datasets, the model achieves a 99% F1-score for Normal vs. Abnormal
classification and shows state-of-the-art performance in multi-class disease
detection, including a 99% F1-score for Conduction Disorders and Hypertrophy,
as well as a 98.9% F1-score for Arrhythmias. Additionally, the model provides
risk level estimations to facilitate clinical decision-making. In conclusion,
FoundationalECGNet represents a scalable, interpretable, and generalizable
solution for automated ECG analysis, with the potential to improve diagnostic
precision and patient outcomes in healthcare settings. We'll share the code
after acceptance.

</details>


### [155] [Value bounds and Convergence Analysis for Averages of LRP attributions](https://arxiv.org/abs/2509.08963)
*Alexander Binder,Nastaran Takmil-Homayouni,Urun Dogan*

Main category: cs.LG

TL;DR: The paper analyzes numerical properties of attribution methods like LRP, using matrix representations and bounds to study their behavior under data augmentations.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve understanding of attribution methods in neural networks, particularly in scenarios involving augmented test samples.

Method: The authors use matrix representations and derive singular value bounds and component-wise bounds for attribution maps.

Result: The paper uncovers bounds and constants governing empirical convergence, revealing distinctions between methods like LRP-beta and gradient-based approaches.

Conclusion: The findings highlight consistent behavior in LRP-beta attribution methods, notably differentiating them from other methods based on weight norms.

Abstract: We analyze numerical properties of Layer-wise relevance propagation
(LRP)-type attribution methods by representing them as a product of modified
gradient matrices. This representation creates an analogy to matrix
multiplications of Jacobi-matrices which arise from the chain rule of
differentiation. In order to shed light on the distribution of attribution
values, we derive upper bounds for singular values. Furthermore we derive
component-wise bounds for attribution map values. As a main result, we apply
these component-wise bounds to obtain multiplicative constants. These constants
govern the convergence of empirical means of attributions to expectations of
attribution maps. This finding has important implications for scenarios where
multiple non-geometric data augmentations are applied to individual test
samples, as well as for Smoothgrad-type attribution methods. In particular, our
analysis reveals that the constants for LRP-beta remain independent of weight
norms, a significant distinction from both gradient-based methods and
LRP-epsilon.

</details>


### [156] [STRIDE: Scalable and Interpretable XAI via Subset-Free Functional Decomposition](https://arxiv.org/abs/2509.09070)
*Chaeyun Ko*

Main category: cs.LG

TL;DR: STRIDE, an explainable AI (XAI) framework, bypasses subset enumeration and scalar limitations by introducing functional decomposition in RKHS, achieving faster and highly accurate explanations.


<details>
  <summary>Details</summary>
Motivation: Existing XAI methods struggle with computational inefficiencies and limited expressiveness, necessitating improvement in scalability and representation capabilities.

Method: The method employs orthogonal functional decomposition in RKHS using a recursive kernel-centering process, eliminating subset enumeration and enabling both local and global interpretability.

Result: STRIDE showcases speedups of up to 9.7 times on some datasets, high fidelity (R^2: 0.81–0.999), and significant rank agreement on the majority of datasets.

Conclusion: STRIDE provides a scalable, model-agnostic framework that complements existing scalar attribution approaches, enabling innovative diagnostic tools like 'component surgery' for enhanced explanation.

Abstract: Most explainable AI (XAI) frameworks face two practical limitations: the
exponential cost of reasoning over feature subsets and the reduced
expressiveness of summarizing effects as single scalar values. We present
STRIDE, a scalable framework that aims to mitigate both issues by framing
explanation as a subset-enumeration-free, orthogonal functional decomposition
in a Reproducing Kernel Hilbert Space (RKHS). Rather than focusing only on
scalar attributions, STRIDE computes functional components f_S(x_S) via an
analytical projection scheme based on a recursive kernel-centering procedure,
avoiding explicit subset enumeration. In the tabular setups we study, the
approach is model-agnostic, provides both local and global views, and is
supported by theoretical results on orthogonality and L^2 convergence under
stated assumptions. On public tabular benchmarks in our environment, we
observed speedups ranging from 0.6 times (slower than TreeSHAP on a small
dataset) to 9.7 times (California), with a median approximate 3.0 times across
10 datasets, while maintaining high fidelity (R^2 between 0.81 and 0.999) and
substantial rank agreement on most datasets. Overall, STRIDE complements scalar
attribution methods by offering a structured functional perspective, enabling
novel diagnostics like 'component surgery' to quantitatively measure the impact
of specific interactions within our experimental scope.

</details>


### [157] [Green Federated Learning via Carbon-Aware Client and Time Slot Scheduling](https://arxiv.org/abs/2509.08980)
*Daniel Richards Arputharaj,Charlotte Rodriguez,Angelo Rodio,Giovanni Neglia*

Main category: cs.LG

TL;DR: The paper focuses on reducing carbon emissions in Federated Learning by introducing a carbon-aware scheduler combining slack-time and efficient allocation, showing promising results in terms of emission reductions and model performance.


<details>
  <summary>Details</summary>
Motivation: Reducing the carbon emissions from training large-scale machine learning models, particularly in the context of globally distributed Federated Learning systems.

Method: A carbon-aware client selection and scheduling approach is developed, integrating slack-time, an α-fair carbon allocation strategy, and a global fine-tuning phase.

Result: The proposed scheduler demonstrates superior model accuracy compared to baseline methods, especially under stringent carbon emission constraints, as tested on real-world Carbon Intensity data.

Conclusion: Carbon-aware scheduling can effectively balance emissions reduction with model performance, proving to be beneficial under varying carbon restrictions.

Abstract: Training large-scale machine learning models incurs substantial carbon
emissions. Federated Learning (FL), by distributing computation across
geographically dispersed clients, offers a natural framework to leverage
regional and temporal variations in Carbon Intensity (CI). This paper
investigates how to reduce emissions in FL through carbon-aware client
selection and training scheduling. We first quantify the emission savings of a
carbon-aware scheduling policy that leverages slack time -- permitting a modest
extension of the training duration so that clients can defer local training
rounds to lower-carbon periods. We then examine the performance trade-offs of
such scheduling which stem from statistical heterogeneity among clients,
selection bias in participation, and temporal correlation in model updates. To
leverage these trade-offs, we construct a carbon-aware scheduler that
integrates slack time, $\alpha$-fair carbon allocation, and a global
fine-tuning phase. Experiments on real-world CI data show that our scheduler
outperforms slack-agnostic baselines, achieving higher model accuracy across a
wide range of carbon budgets, with especially strong gains under tight carbon
constraints.

</details>


### [158] [Active Learning and Explainable AI for Multi-Objective Optimization of Spin Coated Polymers](https://arxiv.org/abs/2509.08988)
*Brendan Young,Brendan Alvey,Andreas Werbrouck,Will Murphy,James Keller,Mattias J. Young,Matthew Maschmann*

Main category: cs.LG

TL;DR: The paper introduces a framework combining Pareto optimization and explainable AI to optimize polymer film properties.


<details>
  <summary>Details</summary>
Motivation: To efficiently optimize mechanical properties (hardness and elasticity) of spin-coated polymer films and make the results interpretable for expert analysis.

Method: An active Pareto front learning algorithm (PyePAL) is used with Gaussian process models to predict objectives and guide parameter optimization. UMAP visualizes high-dimensional data in 2D, and fuzzy linguistic summaries provide human-readable insights.

Result: The framework effectively identified optimal polymer designs while enhancing interpretation through visualization and linguistic summaries.

Conclusion: This integrated framework successfully improves polymer optimization processes and facilitates expert analysis through explainable AI techniques.

Abstract: Spin coating polymer thin films to achieve specific mechanical properties is
inherently a multi-objective optimization problem. We present a framework that
integrates an active Pareto front learning algorithm (PyePAL) with
visualization and explainable AI techniques to optimize processing parameters.
PyePAL uses Gaussian process models to predict objective values (hardness and
elasticity) from the design variables (spin speed, dilution, and polymer
mixture), guiding the adaptive selection of samples toward promising regions of
the design space. To enable interpretable insights into the high-dimensional
design space, we utilize UMAP (Uniform Manifold Approximation and Projection)
for two-dimensional visualization of the Pareto front exploration.
Additionally, we incorporate fuzzy linguistic summaries, which translate the
learned relationships between process parameters and performance objectives
into linguistic statements, thus enhancing the explainability and understanding
of the optimization results. Experimental results demonstrate that our method
efficiently identifies promising polymer designs, while the visual and
linguistic explanations facilitate expert-driven analysis and knowledge
discovery.

</details>


### [159] [ProDiGy: Proximity- and Dissimilarity-Based Byzantine-Robust Federated Learning](https://arxiv.org/abs/2509.09534)
*Sena Ergisi,Luis Maßny,Rawad Bitar*

Main category: cs.LG

TL;DR: This paper introduces ProDiGy, a Byzantine-robust federated learning algorithm, which improves defense capabilities and model accuracy under data heterogeneity.


<details>
  <summary>Details</summary>
Motivation: While federated learning (FL) has many benefits, it remains susceptible to adversarial attacks, particularly in scenarios involving data heterogeneity. There is a need for improved FL defenses to address this issue effectively.

Method: The ProDiGy algorithm uses a joint dual scoring system which evaluates client gradients based on their proximity and dissimilarity. This dual perspective promotes natural similarity among honest clients and identifies suspicious uniformity as an attack indicator.

Result: Extensive experiments show that ProDiGy outperforms existing Byzantine defenses, especially under non-IID client data distributions, maintaining both robustness and model accuracy where other methods fail.

Conclusion: The dual perspective employed by ProDiGy proves to be an effective strategy for detecting and mitigating adversarial attacks in federated learning, especially in heterogeneous data conditions.

Abstract: Federated Learning (FL) emerged as a widely studied paradigm for distributed
learning. Despite its many advantages, FL remains vulnerable to adversarial
attacks, especially under data heterogeneity. We propose a new Byzantine-robust
FL algorithm called ProDiGy. The key novelty lies in evaluating the client
gradients using a joint dual scoring system based on the gradients' proximity
and dissimilarity. We demonstrate through extensive numerical experiments that
ProDiGy outperforms existing defenses in various scenarios. In particular, when
the clients' data do not follow an IID distribution, while other defense
mechanisms fail, ProDiGy maintains strong defense capabilities and model
accuracy. These findings highlight the effectiveness of a dual perspective
approach that promotes natural similarity among honest clients while detecting
suspicious uniformity as a potential indicator of an attack.

</details>


### [160] [Fast attention mechanisms: a tale of parallelism](https://arxiv.org/abs/2509.09001)
*Jingwen Liu,Hantao Yu,Clayton Sanford,Alexandr Andoni,Daniel Hsu*

Main category: cs.LG

TL;DR: The paper introduces Approximate Nearest Neighbor Attention (ANNA), an efficient attention mechanism for transformers to reduce their quadratic time complexity while retaining expressive power.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the scalability limitations of transformers caused by their quadratic time complexity, thus improving computational efficiency while preserving their ability for complex reasoning.

Method: The authors developed ANNA, a sub-quadratic attention mechanism, and theoretically validated its expressive power and reasoning capabilities in comparison to traditional attention mechanisms.

Result: ANNA-transformers demonstrate expressive power analogous to standard attention and efficiently solve reasoning tasks such as Match2 and $k$-hop with significantly reduced computation depth.

Conclusion: Constant-depth ANNA-transformers unify reasoning across efficient attention approximations, enabling scalable performance without sacrificing computational expressiveness.

Abstract: Transformers have the representational capacity to simulate Massively
Parallel Computation (MPC) algorithms, but they suffer from quadratic time
complexity, which severely limits their scalability. We introduce an efficient
attention mechanism called Approximate Nearest Neighbor Attention (ANNA) with
sub-quadratic time complexity. We prove that ANNA-transformers (1) retain the
expressive power previously established for standard attention in terms of
matching the capabilities of MPC algorithms, and (2) can solve key reasoning
tasks such as Match2 and $k$-hop with near-optimal depth. Using the MPC
framework, we further prove that constant-depth ANNA-transformers can simulate
constant-depth low-rank transformers, thereby providing a unified way to reason
about a broad class of efficient attention approximations.

</details>


### [161] [Open-sci-ref-0.01: open and reproducible reference baselines for language model and dataset comparison](https://arxiv.org/abs/2509.09009)
*Marianna Nezhurina,Taishi Nakamura,Timur Carstensen,Niccolò Ajroldi,Ville Komulainen,David Salinas,Jenia Jitsev*

Main category: cs.LG

TL;DR: The paper introduces 'open-sci-ref,' a set of dense transformer models trained across various scales and evaluated on open reference datasets to establish benchmarks for assessing training approaches in AI research.


<details>
  <summary>Details</summary>
Motivation: To provide standardized benchmarks and reference baselines for evaluating training methodologies and datasets for dense transformer models in AI research.

Method: Dense transformer models (0.13B–1.7B parameters) are trained on 8 open reference datasets at scales up to 1T tokens, using intermediate checkpoints and comparative evaluations on standardized benchmarks.

Result: Training on NemoTron-CC HQ dataset outperforms others, followed by DCLM-baseline and FineWeb-Edu; the release includes training checkpoints, logs, code, and evaluations for standardized comparison.

Conclusion: The paper establishes reproducible baselines for comparing dense transformer model training, enabling standardized assessments and highlighting the benefits of specific datasets like NemoTron-CC HQ.

Abstract: We introduce open-sci-ref, a family of dense transformer models trained as
research baselines across multiple model (0.13B to 1.7B parameters) and token
scales (up to 1T) on 8 recent open reference datasets. Evaluating the models on
various standardized benchmarks, our training runs set establishes reference
points that enable researchers to assess the sanity and quality of alternative
training approaches across scales and datasets. Intermediate checkpoints allow
comparison and studying of the training dynamics. The established reference
baselines allow training procedures to be compared through their scaling
trends, aligning them on a common compute axis. Comparison of open reference
datasets reveals that training on NemoTron-CC HQ consistently outperforms other
reference datasets, followed by DCLM-baseline and FineWeb-Edu. In addition to
intermediate training checkpoints, the release includes logs, code, and
downstream evaluations to simplify reproduction, standardize comparison, and
facilitate future research.

</details>


### [162] [Deep Context-Conditioned Anomaly Detection for Tabular Data](https://arxiv.org/abs/2509.09030)
*Spencer King,Zhilu Zhang,Ruofan Yu,Baris Coskun,Wei Ding,Qian Cui*

Main category: cs.LG

TL;DR: The paper proposes a context-sensitive deep learning framework for unsupervised anomaly detection in tabular data, achieving superior results through context-aware modeling.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting anomalies in real-world tabular datasets where global rarity might not indicate abnormality due to heterogeneous contexts.

Method: The method automatically identifies contextual features in the data and uses a deep autoencoder to model the conditional distributions based on these contexts.

Result: The proposed framework outperforms state-of-the-art methods on multiple tabular benchmark datasets, demonstrating improved anomaly detection accuracy.

Conclusion: Considering contextual information significantly enhances anomaly detection in tabular datasets, supporting the utility of the proposed context-conditional framework.

Abstract: Anomaly detection is critical in domains such as cybersecurity and finance,
especially when working with large-scale tabular data. Yet, unsupervised
anomaly detection -- where no labeled anomalies are available -- remains a
significant challenge. Although various deep learning methods have been
proposed to model a dataset's joint distribution, real-world tabular data often
contain heterogeneous contexts (e.g., different users), making globally rare
events normal under certain contexts. Consequently, relying on a single global
distribution can overlook these contextual nuances, degrading detection
performance. In this paper, we present a context-conditional anomaly detection
framework tailored for tabular datasets. Our approach automatically identifies
context features and models the conditional data distribution using a simple
deep autoencoder. Extensive experiments on multiple tabular benchmark datasets
demonstrate that our method outperforms state-of-the-art approaches,
underscoring the importance of context in accurately distinguishing anomalous
from normal instances.

</details>


### [163] [MoWE : A Mixture of Weather Experts](https://arxiv.org/abs/2509.09052)
*Dibyajyoti Chakraborty,Romit Maulik,Peter Harrington,Dallas Foster,Mohammad Amin Nabian,Sanjay Choudhry*

Main category: cs.LG

TL;DR: This paper presents a Mixture of Experts (MoWE) approach to improve weather forecasting by optimally combining outputs from existing models, achieving up to 10% lower RMSE compared to the best AI weather model.


<details>
  <summary>Details</summary>
Motivation: Current data-driven weather models have reached a performance plateau, necessitating innovative methods to boost forecasting accuracy without excessive computational demands.

Method: The MoWE model employs a Vision Transformer-based gating network that dynamically optimizes the contribution of multiple expert models at each grid point, conditioned on forecast lead time.

Result: MoWE achieves up to 10% lower RMSE compared to the best-performing AI weather model and significantly outperforms both individual experts and simple averages over a 2-day forecast horizon.

Conclusion: The MoWE approach offers a computationally efficient and scalable framework for enhancing weather prediction, making optimal use of existing high-quality forecast models.

Abstract: Data-driven weather models have recently achieved state-of-the-art
performance, yet progress has plateaued in recent years. This paper introduces
a Mixture of Experts (MoWE) approach as a novel paradigm to overcome these
limitations, not by creating a new forecaster, but by optimally combining the
outputs of existing models. The MoWE model is trained with significantly lower
computational resources than the individual experts. Our model employs a Vision
Transformer-based gating network that dynamically learns to weight the
contributions of multiple "expert" models at each grid point, conditioned on
forecast lead time. This approach creates a synthesized deterministic forecast
that is more accurate than any individual component in terms of Root Mean
Squared Error (RMSE). Our results demonstrate the effectiveness of this method,
achieving up to a 10% lower RMSE than the best-performing AI weather model on a
2-day forecast horizon, significantly outperforming individual experts as well
as a simple average across experts. This work presents a computationally
efficient and scalable strategy to push the state of the art in data-driven
weather prediction by making the most out of leading high-quality forecast
models.

</details>


### [164] [A Scoping Review of Machine Learning Applications in Power System Protection and Disturbance Management](https://arxiv.org/abs/2509.09053)
*Julian Oelhaf,Georg Kordowich,Mehran Pashaei,Christian Bergler,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.LG

TL;DR: This review examines machine learning's role in modern power system protection and disturbance management, highlighting gaps in real-world validation and methodological consistency.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to address challenges in modern power systems caused by the integration of renewable and distributed energy resources, which conventional protection schemes struggle to handle.

Method: Using the PRISMA for Scoping Reviews framework, the authors synthesize findings from over 100 publications, propose a taxonomy for ML applications in protection tasks, and advocate for standardized reporting and evaluation practices.

Result: The study finds that while ML models excel in simulated scenarios, their real-world performance remains unvalidated. It also identifies critical issues like fragmented literature, inconsistent methods, and lack of standardization.

Conclusion: The paper concludes that standardization, real-world validation, and advanced ML techniques are vital for transitioning ML-based protection systems from theoretical research to practical application in dynamic power grids.

Abstract: The integration of renewable and distributed energy resources reshapes modern
power systems, challenging conventional protection schemes. This scoping review
synthesizes recent literature on machine learning (ML) applications in power
system protection and disturbance management, following the PRISMA for Scoping
Reviews framework. Based on over 100 publications, three key objectives are
addressed: (i) assessing the scope of ML research in protection tasks; (ii)
evaluating ML performance across diverse operational scenarios; and (iii)
identifying methods suitable for evolving grid conditions. ML models often
demonstrate high accuracy on simulated datasets; however, their performance
under real-world conditions remains insufficiently validated. The existing
literature is fragmented, with inconsistencies in methodological rigor, dataset
quality, and evaluation metrics. This lack of standardization hampers the
comparability of results and limits the generalizability of findings. To
address these challenges, this review introduces a ML-oriented taxonomy for
protection tasks, resolves key terminological inconsistencies, and advocates
for standardized reporting practices. It further provides guidelines for
comprehensive dataset documentation, methodological transparency, and
consistent evaluation protocols, aiming to improve reproducibility and enhance
the practical relevance of research outcomes. Critical gaps remain, including
the scarcity of real-world validation, insufficient robustness testing, and
limited consideration of deployment feasibility. Future research should
prioritize public benchmark datasets, realistic validation methods, and
advanced ML architectures. These steps are essential to move ML-based
protection from theoretical promise to practical deployment in increasingly
dynamic and decentralized power systems.

</details>


### [165] ["A 6 or a 9?": Ensemble Learning Through the Multiplicity of Performant Models and Explanations](https://arxiv.org/abs/2509.09073)
*Gianlucca Zuin,Adriano Veloso*

Main category: cs.LG

TL;DR: This paper introduces the Rashomon Ensemble, a method for selecting diverse high-performing models to improve machine learning generalization and robustness.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of selecting models that generalize well, especially in scenarios involving diverse high-performing solutions due to the Rashomon Effect.

Method: The authors propose constructing ensembles of models by grouping them based on performance and explanations, ensuring diversity and predictive accuracy.

Result: The approach demonstrated up to a 0.20+ AUROC improvement on multiple datasets, proving its effectiveness in handling distribution shifts.

Conclusion: The Rashomon Ensemble improves generalization and robustness, offering practical benefits for real-world applications and businesses.

Abstract: Creating models from past observations and ensuring their effectiveness on
new data is the essence of machine learning. However, selecting models that
generalize well remains a challenging task. Related to this topic, the Rashomon
Effect refers to cases where multiple models perform similarly well for a given
learning problem. This often occurs in real-world scenarios, like the
manufacturing process or medical diagnosis, where diverse patterns in data lead
to multiple high-performing solutions. We propose the Rashomon Ensemble, a
method that strategically selects models from these diverse high-performing
solutions to improve generalization. By grouping models based on both their
performance and explanations, we construct ensembles that maximize diversity
while maintaining predictive accuracy. This selection ensures that each model
covers a distinct region of the solution space, making the ensemble more robust
to distribution shifts and variations in unseen data. We validate our approach
on both open and proprietary collaborative real-world datasets, demonstrating
up to 0.20+ AUROC improvements in scenarios where the Rashomon ratio is large.
Additionally, we demonstrate tangible benefits for businesses in various
real-world applications, highlighting the robustness, practicality, and
effectiveness of our approach.

</details>


### [166] [An entropy formula for the Deep Linear Network](https://arxiv.org/abs/2509.09088)
*Govind Menon,Tianmin Yu*

Main category: cs.LG

TL;DR: The paper explores the Riemannian geometry of Deep Linear Networks to construct a thermodynamic framework for learning, utilizing group actions and submersions.


<details>
  <summary>Details</summary>
Motivation: To establish a thermodynamic foundation for understanding the learning processes in Deep Linear Networks using geometry.

Method: Employing group actions and Riemannian submersion techniques to derive a Boltzmann entropy and connect parameter space to observable space.

Result: Defined Boltzmann entropy for the balanced manifold and linked observables' geometry to Riemannian submersion. Developed an orthonormal basis for the tangent space using Jacobi matrices.

Conclusion: The study builds geometric tools to deepen our theoretical understanding of overparametrization and learning in DLNs.

Abstract: We study the Riemannian geometry of the Deep Linear Network (DLN) as a
foundation for a thermodynamic description of the learning process. The main
tools are the use of group actions to analyze overparametrization and the use
of Riemannian submersion from the space of parameters to the space of
observables. The foliation of the balanced manifold in the parameter space by
group orbits is used to define and compute a Boltzmann entropy. We also show
that the Riemannian geometry on the space of observables defined in [2] is
obtained by Riemannian submersion of the balanced manifold. The main technical
step is an explicit construction of an orthonormal basis for the tangent space
of the balanced manifold using the theory of Jacobi matrices.

</details>


### [167] [Sensitivity-LoRA: Low-Load Sensitivity-Based Fine-Tuning for Large Language Models](https://arxiv.org/abs/2509.09119)
*Hao Zhang,Bo Huang,Zhenjia Li,Xi Xiao,Hui Yi Leong,Zumeng Zhang,Xinwei Long,Tianyang Wang,Hao Xu*

Main category: cs.LG

TL;DR: Sensitivity-LoRA improves Low-Rank Adaptation (LoRA) by dynamically allocating ranks based on weight sensitivities, ensuring efficient fine-tuning of Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Fine-tuning Large Language Models for specialized tasks in resource-constrained environments remains difficult. Existing methods like LoRA struggle due to uniform rank allocation and inefficiencies in advanced allocation techniques.

Method: Sensitivity-LoRA utilizes both global and local sensitivities, derived from second-order derivatives (Hessian Matrix) of the loss function, to dynamically allocate ranks across weight matrices.

Result: Experimental results show that Sensitivity-LoRA is effective, efficient, and stable across various tasks and benchmarks.

Conclusion: Sensitivity-LoRA addresses significant limitations in rank allocation for LoRA, offering a computationally efficient and practical approach for adapting LLMs to specialized tasks.

Abstract: Large Language Models (LLMs) have transformed both everyday life and
scientific research. However, adapting LLMs from general-purpose models to
specialized tasks remains challenging, particularly in resource-constrained
environments. Low-Rank Adaptation (LoRA), a prominent method within
Parameter-Efficient Fine-Tuning (PEFT), has emerged as a promising approach to
LLMs by approximating model weight updates using low-rank decomposition.
However, LoRA is limited by its uniform rank ( r ) allocation to each
incremental matrix, and existing rank allocation techniques aimed at addressing
this issue remain computationally inefficient, complex, and unstable, hindering
practical applications. To address these limitations, we propose
Sensitivity-LoRA, an efficient fine-tuning method that dynamically allocates
ranks to weight matrices based on both their global and local sensitivities. It
leverages the second-order derivatives (Hessian Matrix) of the loss function to
effectively capture weight sensitivity, enabling optimal rank allocation with
minimal computational overhead. Our experimental results have demonstrated
robust effectiveness, efficiency and stability of Sensitivity-LoRA across
diverse tasks and benchmarks.

</details>


### [168] [Learning What Matters: Causal Time Series Modeling for Arctic Sea Ice Prediction](https://arxiv.org/abs/2509.09128)
*Emam Hossain,Md Osman Gani*

Main category: cs.LG

TL;DR: This paper proposes a causality-aware deep learning framework for improved prediction through causal feature selection, demonstrated on Arctic Sea Ice Extent (SIE) forecasting.


<details>
  <summary>Details</summary>
Motivation: Conventional machine and deep learning models fail to distinguish causal relationships from spurious correlations, leading to limited robustness, interpretability, and generalization.

Method: The paper introduces a hybrid causality-aware neural framework integrating Multivariate Granger Causality (MVGC) and PCMCI+ for causal feature selection, applied to Arctic SIE data over 43 years.

Result: The method successfully improves prediction accuracy, interpretability, and computational efficiency by selecting causally relevant predictors.

Conclusion: The proposed framework is scalable and generalizable to other high-dimensional domains, enhancing causality-informed predictive modeling both theoretically and practically.

Abstract: Conventional machine learning and deep learning models typically rely on
correlation-based learning, which often fails to distinguish genuine causal
relationships from spurious associations, limiting their robustness,
interpretability, and ability to generalize. To overcome these limitations, we
introduce a causality-aware deep learning framework that integrates
Multivariate Granger Causality (MVGC) and PCMCI+ for causal feature selection
within a hybrid neural architecture. Leveraging 43 years (1979-2021) of Arctic
Sea Ice Extent (SIE) data and associated ocean-atmospheric variables at daily
and monthly resolutions, the proposed method identifies causally influential
predictors, prioritizes direct causes of SIE dynamics, reduces unnecessary
features, and enhances computational efficiency. Experimental results show that
incorporating causal inputs leads to improved prediction accuracy and
interpretability across varying lead times. While demonstrated on Arctic SIE
forecasting, the framework is broadly applicable to other dynamic,
high-dimensional domains, offering a scalable approach that advances both the
theoretical foundations and practical performance of causality-informed
predictive modeling.

</details>


### [169] [Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.09135)
*Xuefeng Wang,Lei Zhang,Henglin Pu,Ahmed H. Qureshi,Husheng Li*

Main category: cs.LG

TL;DR: The paper proposes a continuous-time multi-agent reinforcement learning (CT-MARL) framework using physics-informed neural networks (PINNs) to address challenges in high-dimensional systems and improve learning accuracy through a Value Gradient Iteration (VGI) module.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement learning methods face challenges with high-frequency or irregular interactions, with continuous-time RL (CTRL) showing promise but being limited to single-agent applications due to dimensionality and value approximation issues.

Method: The authors introduce the CT-MARL framework, incorporating PINNs to approximate value functions and a Value Gradient Iteration (VGI) module to refine gradient alignment iteratively, ensuring consistency with differential structures.

Result: Their method outperforms existing CTRL baselines in multi-agent benchmarks such as the multi-agent particle environment (MPE) and multi-agent MuJoCo, demonstrating scalability and improved performance in complex dynamics.

Conclusion: The proposed method successfully extends CTRL to multi-agent settings, addressing dimensionality and accuracy challenges while achieving superior results over existing frameworks.

Abstract: Existing reinforcement learning (RL) methods struggle with complex dynamical
systems that demand interactions at high frequencies or irregular time
intervals. Continuous-time RL (CTRL) has emerged as a promising alternative by
replacing discrete-time Bellman recursion with differential value functions
defined as viscosity solutions of the Hamilton--Jacobi--Bellman (HJB) equation.
While CTRL has shown promise, its applications have been largely limited to the
single-agent domain. This limitation stems from two key challenges: (i)
conventional solution methods for HJB equations suffer from the curse of
dimensionality (CoD), making them intractable in high-dimensional systems; and
(ii) even with HJB-based learning approaches, accurately approximating
centralized value functions in multi-agent settings remains difficult, which in
turn destabilizes policy training. In this paper, we propose a CT-MARL
framework that uses physics-informed neural networks (PINNs) to approximate
HJB-based value functions at scale. To ensure the value is consistent with its
differential structure, we align value learning with value-gradient learning by
introducing a Value Gradient Iteration (VGI) module that iteratively refines
value gradients along trajectories. This improves gradient fidelity, in turn
yielding more accurate values and stronger policy learning. We evaluate our
method using continuous-time variants of standard benchmarks, including
multi-agent particle environment (MPE) and multi-agent MuJoCo. Our results
demonstrate that our approach consistently outperforms existing continuous-time
RL baselines and scales to complex multi-agent dynamics.

</details>


### [170] [Peering Partner Recommendation for ISPs using Machine Learning](https://arxiv.org/abs/2509.09146)
*Md Ibrahim Ibne Alam,Ankur Senapati,Anindo Mahmood,Murat Yuksel,Koushik Kar*

Main category: cs.LG

TL;DR: The paper develops a machine learning model using publicly available ISP data to automate the prediction of peering relationships, achieving 98% accuracy with XGBoost.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency in ISP peering partner selection, which is often a lengthy and complex process.

Method: The research involved collecting ISP data from public databases (e.g., PeeringDB, CAIDA) and testing three types of ML models (tree-based, neural networks, transformers) to predict peering relationships.

Result: Tree-based models performed best, with the XGBoost model achieving 98% accuracy while demonstrating robustness to temporal, spatial, and data availability changes.

Conclusion: The proposed ML-driven method has the potential to fully automate ISP peering partner selection, leading to a more optimized global Internet ecosystem.

Abstract: Internet service providers (ISPs) need to connect with other ISPs to provide
global connectivity services to their users. To ensure global connectivity,
ISPs can either use transit service(s) or establish direct peering
relationships between themselves via Internet exchange points (IXPs). Peering
offers more room for ISP-specific optimizations and is preferred, but it often
involves a lengthy and complex process. Automating peering partner selection
can enhance efficiency in the global Internet ecosystem. We explore the use of
publicly available data on ISPs to develop a machine learning (ML) model that
can predict whether an ISP pair should peer or not. At first, we explore public
databases, e.g., PeeringDB, CAIDA, etc., to gather data on ISPs. Then, we
evaluate the performance of three broad types of ML models for predicting
peering relationships: tree-based, neural network-based, and transformer-based.
Among these, we observe that tree-based models achieve the highest accuracy and
efficiency in our experiments. The XGBoost model trained with publicly
available data showed promising performance, with a 98% accuracy rate in
predicting peering partners. In addition, the model demonstrated great
resilience to variations in time, space, and missing data. We envision that
ISPs can adopt our method to fully automate the peering partner selection
process, thus transitioning to a more efficient and optimized Internet
ecosystem.

</details>


### [171] [HISPASpoof: A New Dataset For Spanish Speech Forensics](https://arxiv.org/abs/2509.09155)
*Maria Risques,Kratika Bhagtani,Amit Kumar Singh Yadav,Edward J. Delp*

Main category: cs.LG

TL;DR: HISPASpoof is introduced as the first extensive Spanish dataset for detecting and attributing synthetic speech, addressing the underrepresentation of Spanish in speech forensics.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of adequate tools and datasets for detecting and attributing synthetic speech in Spanish, a language spoken by over 600 million people globally.

Method: The authors created the HISPASpoof dataset with genuine speech from diverse Spanish accents and synthetic voices generated by six zero-shot TTS systems. They also evaluated the performance of five representative detection methods on this dataset.

Result: English-trained speech detectors did not generalize well to Spanish. However, training on HISPASpoof notably improved the detection accuracy and enabled effective synthetic speech attribution.

Conclusion: HISPASpoof is a critical resource for improving speech forensics in Spanish, enhancing both detection and attribution of synthetic speech generation methods.

Abstract: Zero-shot Voice Cloning (VC) and Text-to-Speech (TTS) methods have advanced
rapidly, enabling the generation of highly realistic synthetic speech and
raising serious concerns about their misuse. While numerous detectors have been
developed for English and Chinese, Spanish-spoken by over 600 million people
worldwide-remains underrepresented in speech forensics. To address this gap, we
introduce HISPASpoof, the first large-scale Spanish dataset designed for
synthetic speech detection and attribution. It includes real speech from public
corpora across six accents and synthetic speech generated with six zero-shot
TTS systems. We evaluate five representative methods, showing that detectors
trained on English fail to generalize to Spanish, while training on HISPASpoof
substantially improves detection. We also evaluate synthetic speech attribution
performance on HISPASpoof, i.e., identifying the generation method of synthetic
speech. HISPASpoof thus provides a critical benchmark for advancing reliable
and inclusive speech forensics in Spanish.

</details>


### [172] [Adaptive Pareto-Optimal Token Merging for Edge Transformer Models in Semantic Communication](https://arxiv.org/abs/2509.09168)
*Omar Erak,Omar Alhussein,Hatem Abou-Zeid,Mehdi Bennis*

Main category: cs.LG

TL;DR: The paper proposes an adaptive token merging framework for pretrained vision transformers to optimize inference time and transmission resources in resource-constrained 6G networks.


<details>
  <summary>Details</summary>
Motivation: Current large-scale transformer models are powerful for semantic communication systems but face challenges due to high computational requirements, especially in 6G networks.

Method: The authors use adaptive token merging combined with Gaussian process-based Bayesian optimization to balance accuracy and computational cost, creating a Pareto frontier for runtime adaptation based on dynamic requirements and channel conditions.

Result: Experiments show significant reductions in computational operations while maintaining accuracy across varying SNR conditions, outperforming other methods.

Conclusion: The proposed method provides an efficient mechanism for deploying transformer-based semantic communication systems in edge intelligence, allowing practical adaptation to network and application dynamics.

Abstract: Large-scale transformer models have emerged as a powerful tool for semantic
communication systems, enabling edge devices to extract rich representations
for robust inference across noisy wireless channels. However, their substantial
computational demands remain a major barrier to practical deployment in
resource-constrained 6G networks. In this paper, we present a training-free
framework for adaptive token merging in pretrained vision transformers to
jointly reduce inference time and transmission resource usage. We formulate the
selection of per-layer merging proportions as a multi-objective optimization
problem to balance accuracy and computational cost. We employ Gaussian
process-based Bayesian optimization to construct a Pareto frontier of optimal
configurations, enabling flexible runtime adaptation to dynamic application
requirements and channel conditions. Extensive experiments demonstrate that our
method consistently outperforms other baselines and achieves significant
reductions in floating-point operations while maintaining competitive accuracy
across a wide range of signal-to-noise ratio (SNR) conditions. Additional
results highlight the effectiveness of adaptive policies that adjust merging
aggressiveness in response to channel quality, providing a practical mechanism
to trade off latency and semantic fidelity on demand. These findings establish
a scalable and efficient approach for deploying transformer-based semantic
communication in future edge intelligence systems.

</details>


### [173] [Quantum Machine Learning, Quantitative Trading, Reinforcement Learning, Deep Learning](https://arxiv.org/abs/2509.09176)
*Jun-Hao Chen,Yu-Chien Huang,Yun-Cheng Tsai,Samuel Yen-Chi Chen*

Main category: cs.LG

TL;DR: The study integrates quantum-inspired neural networks with deep reinforcement learning to develop a trading agent for USD/TWD, achieving notable results but facing limitations in classical quantum simulation.


<details>
  <summary>Details</summary>
Motivation: To explore the intersection of quantum-inspired machine learning and deep reinforcement learning for enhancing financial trading strategies.

Method: A Quantum LSTM model for predicting short-term trends is combined with a Quantum A3C model for reinforcement learning. The agent is trained on 25+ years of USD/TWD data, using a reward system for trend-following and risk control, along with multi-core training mechanisms.

Result: The USD/TWD trading agent achieved a 11.87% return over 5 years, with a 0.92% max drawdown, outperforming currency ETFs.

Conclusion: Quantum-inspired models demonstrate competitive FX trading performance with effective risk control, but face constraints in classical simulation and simplified strategy design.

Abstract: The convergence of quantum-inspired neural networks and deep reinforcement
learning offers a promising avenue for financial trading. We implemented a
trading agent for USD/TWD by integrating Quantum Long Short-Term Memory (QLSTM)
for short-term trend prediction with Quantum Asynchronous Advantage
Actor-Critic (QA3C), a quantum-enhanced variant of the classical A3C. Trained
on data from 2000-01-01 to 2025-04-30 (80\% training, 20\% testing), the
long-only agent achieves 11.87\% return over around 5 years with 0.92\% max
drawdown, outperforming several currency ETFs. We detail state design (QLSTM
features and indicators), reward function for trend-following/risk control, and
multi-core training. Results show hybrid models yield competitive FX trading
performance. Implications include QLSTM's effectiveness for small-profit trades
with tight risk and future enhancements. Key hyperparameters: QLSTM sequence
length$=$4, QA3C workers$=$8. Limitations: classical quantum simulation and
simplified strategy. \footnote{The views expressed in this article are those of
the authors and do not represent the views of Wells Fargo. This article is for
informational purposes only. Nothing contained in this article should be
construed as investment advice. Wells Fargo makes no express or implied
warranties and expressly disclaims all legal, tax, and accounting implications
related to this article.

</details>


### [174] [Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL](https://arxiv.org/abs/2509.09177)
*Hanyi Mao,Quanjia Xiao,Lei Pang,Haixiao Liu*

Main category: cs.LG

TL;DR: The paper introduces FSPO, a reinforcement learning method tailored for large language models (LLMs) that enforces fair handling of sequences of different lengths during optimization.


<details>
  <summary>Details</summary>
Motivation: Addressing the mismatch caused by standard PPO/GRPO-style clipping when applied to sequences, which leads to biases in handling short vs. long responses.

Method: FSPO incorporates sequence-level reinforcement learning with a Gaussian-inspired adjustment to clip sequence log-importance-sampling (IS) ratios. This involves a KL-corrected drift term scaled by $\sqrt{L}$ to maintain length fairness.

Result: FSPO achieves consistent clip rates across different lengths, stabilizes training dynamics, and surpasses existing baselines on various evaluation datasets.

Conclusion: The approach ensures fair optimization irrespective of sequence length, improving stability and performance in sequence-level tasks for LLMs.

Abstract: We propose FSPO (Fair Sequence Policy Optimization), a sequence-level
reinforcement learning method for LLMs that enforces length-fair clipping
directly in the importance-sampling (IS) weight space. We revisit
sequence-level RL methods and identify a mismatch when PPO/GRPO-style clipping
is transplanted to sequences: a fixed clip range systematically reweights short
vs. long responses, distorting the effective objective. Theoretically, we
formalize length fairness via a Length Reweighting Error (LRE) and prove that
small LRE yields a directional cosine guarantee between the clipped and true
updates. FSPO introduces a simple, Gaussian-motivated remedy: we clip the
sequence log-IS ratio with a band that applies a KL-corrected drift term and
scales as $\sqrt{L}$. Empirically, FSPO flattens clip rates across length bins,
stabilizes training, and outperforms all baselines across multiple evaluation
datasets.

</details>


### [175] [Breaking the Statistical Similarity Trap in Extreme Convection Detection](https://arxiv.org/abs/2509.09195)
*Md Tanveer Hossain Munim*

Main category: cs.LG

TL;DR: This paper highlights the limitations of current evaluation metrics for deep learning weather models in detecting extreme weather events and introduces a new framework, DART, to improve detection using advanced architecture and task-specific optimizations.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the inability of current metrics to accurately detect rare but high-impact weather events, leading to models that perform well statistically but fail practically in extreme conditions.

Method: The authors propose the DART framework, which utilizes a dual-decoder architecture, explicit event decomposition, physically motivated oversampling, and specialized loss functions tailored for detecting extreme convection events in atmospheric forecasts.

Result: The DART framework achieved significant improvements, such as a 270% detection increase after removing irrelevant features ('IVT Paradox') and a CSI value of 0.273 with reduced bias compared to baselines. It was validated using a real-world case study of the 2023 Chittagong flooding.

Conclusion: DART effectively addresses the challenge of transforming coarse atmospheric data into high-resolution outputs optimized for extreme event detection, demonstrating trustworthiness and operational efficiency for meteorological applications.

Abstract: Current evaluation metrics for deep learning weather models create a
"Statistical Similarity Trap", rewarding blurry predictions while missing rare,
high-impact events. We provide quantitative evidence of this trap, showing
sophisticated baselines achieve 97.9% correlation yet 0.00 CSI for dangerous
convection detection. We introduce DART (Dual Architecture for Regression
Tasks), a framework addressing the challenge of transforming coarse atmospheric
forecasts into high-resolution satellite brightness temperature fields
optimized for extreme convection detection (below 220 K). DART employs
dual-decoder architecture with explicit background/extreme decomposition,
physically motivated oversampling, and task-specific loss functions. We present
four key findings: (1) empirical validation of the Statistical Similarity Trap
across multiple sophisticated baselines; (2) the "IVT Paradox", removing
Integrated Water Vapor Transport, widely regarded as essential for atmospheric
river analysis, improves extreme convection detection by 270%; (3)
architectural necessity demonstrated through operational flexibility (DART
achieves CSI = 0.273 with bias = 2.52 vs. 6.72 for baselines at equivalent
CSI), and (4) real-world validation with the August 2023 Chittagong flooding
disaster as a case study. To our knowledge, this is the first work to
systematically address this hybrid conversion-segmentation-downscaling task,
with no direct prior benchmarks identified in existing literature. Our
validation against diverse statistical and deep learning baselines sufficiently
demonstrates DART's specialized design. The framework enables precise
operational calibration through beta-tuning, trains in under 10 minutes on
standard hardware, and integrates seamlessly with existing meteorological
workflows, demonstrating a pathway toward trustworthy AI for extreme weather
preparedness.

</details>


### [176] [Incentivizing Safer Actions in Policy Optimization for Constrained Reinforcement Learning](https://arxiv.org/abs/2509.09208)
*Somnath Hazra,Pallab Dasgupta,Soumyajit Dey*

Main category: cs.LG

TL;DR: The paper proposes the Incrementally Penalized Proximal Policy Optimization (IP3O) algorithm for Constrained Reinforcement Learning, which addresses stability issues near constraint boundaries.


<details>
  <summary>Details</summary>
Motivation: Constrained RL faces challenges with reward maximization and constraint satisfaction due to policy instability near bound constraints, negatively affecting training performance.

Method: The authors introduce an adaptive incentive mechanism alongside a progressively increasing penalty approach to stabilize training dynamics in Constrained RL.

Result: Empirical evaluations show that IP3O outperforms state-of-the-art Safe RL algorithms in benchmark environments and offers theoretical guarantees through worst-case error bound analysis.

Conclusion: IP3O provides a reliable solution for balancing reward maximization and constraint adherence in Constrained RL, improving both performance and theoretical robustness.

Abstract: Constrained Reinforcement Learning (RL) aims to maximize the return while
adhering to predefined constraint limits, which represent domain-specific
safety requirements. In continuous control settings, where learning agents
govern system actions, balancing the trade-off between reward maximization and
constraint satisfaction remains a significant challenge. Policy optimization
methods often exhibit instability near constraint boundaries, resulting in
suboptimal training performance. To address this issue, we introduce a novel
approach that integrates an adaptive incentive mechanism in addition to the
reward structure to stay within the constraint bound before approaching the
constraint boundary. Building on this insight, we propose Incrementally
Penalized Proximal Policy Optimization (IP3O), a practical algorithm that
enforces a progressively increasing penalty to stabilize training dynamics.
Through empirical evaluation on benchmark environments, we demonstrate the
efficacy of IP3O compared to the performance of state-of-the-art Safe RL
algorithms. Furthermore, we provide theoretical guarantees by deriving a bound
on the worst-case error of the optimality achieved by our algorithm.

</details>


### [177] [Identifying Key Features for Establishing Sustainable Agro-Tourism Centre: A Data Driven Approach](https://arxiv.org/abs/2509.09214)
*Alka Gadakh,Vidya Kumbhar,Sonal Khosla,Kumar Karunendra*

Main category: cs.LG

TL;DR: This paper studies strategies for agro-tourism growth, using machine learning models to identify impactful indicators. Logistic Regression achieves top accuracy.


<details>
  <summary>Details</summary>
Motivation: To foster rural development by promoting agro-tourism through advanced strategies while conserving cultural heritage.

Method: The study leveraged literature reviews and state-of-the-art machine learning techniques, including LASSO, and classifiers like Logistic Regression, Decision Trees, Random Forest, and XGBoost, for feature selection and classification.

Result: Results indicate Logistic Regression achieves the highest classification accuracy: 98% in 70-30% train-test split and 99% in 80-20%, surpassing other models.

Conclusion: Machine learning methods, particularly Logistic Regression combined with LASSO, are effective for identifying key indicators for agro-tourism growth, providing robust strategies for rural development.

Abstract: Agro-tourism serves as a strategic economic model designed to facilitate
rural development by diversifying income streams for local communities like
farmers while promoting the conservation of indigenous cultural heritage and
traditional agricultural practices. As a very booming subdomain of tourism,
there is a need to study the strategies for the growth of Agro-tourism in
detail. The current study has identified the important indicators for the
growth and enhancement of agro-tourism. The study is conducted in two phases:
identification of the important indicators through a comprehensive literature
review and in the second phase state-of-the-art techniques were used to
identify the important indicators for the growth of agro-tourism. The
indicators are also called features synonymously, the machine learning models
for feature selection were applied and it was observed that the Least Absolute
Shrinkage and Selection Operator (LASSO) method combined with, the machine
Learning Classifiers such as Logistic Regression (LR), Decision Trees (DT),
Random Forest (RF) Tree, and Extreme Gradient Boosting (XGBOOST) models were
used to suggest the growth of the agro-tourism. The results show that with the
LASSO method, LR model gives the highest classification accuracy of 98% in
70-30% train-test data followed by RF with 95% accuracy. Similarly, in the
80-20% train-test data LR maintains the highest accuracy at 99%, while DT and
XGBoost follow with 97% accuracy.

</details>


### [178] [Vejde: A Framework for Inductive Deep Reinforcement Learning Based on Factor Graph Color Refinement](https://arxiv.org/abs/2509.09219)
*Jakob Nyberg,Pontus Johnson*

Main category: cs.LG

TL;DR: This paper introduces Vejde, a framework combining graph neural networks and reinforcement learning to create scalable and generalizable policies for structured decision problems.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in decision-making problems with rich structures like object classes and relations, leveraging scalable methods that generalize well across varying sizes and configurations.

Method: The framework uses MDP states represented as databases of facts, converting them to bipartite graphs. Then, it employs neural message passing for latent state mapping and integrates reinforcement learning to train policies. Supervised and reinforcement learning were applied in experiments.

Result: Vejde was tested on eight problem domains with unseen instances. It demonstrated strong generalization capabilities, scoring close to problem-specific MLP agents and outperforming the Prost algorithm in terms of generalizability.

Conclusion: Vejde successfully balances scalability and generalization for structured decision problems, showing promise for inductive policy learning without sacrificing accuracy even in unseen situations.

Abstract: We present and evaluate Vejde; a framework which combines data abstraction,
graph neural networks and reinforcement learning to produce inductive policy
functions for decision problems with richly structured states, such as object
classes and relations. MDP states are represented as data bases of facts about
entities, and Vejde converts each state to a bipartite graph, which is mapped
to latent states through neural message passing. The factored representation of
both states and actions allows Vejde agents to handle problems of varying size
and structure. We tested Vejde agents on eight problem domains defined in RDDL,
with ten problem instances each, where policies were trained using both
supervised and reinforcement learning. To test policy generalization, we
separate problem instances in two sets, one for training and the other solely
for testing. Test results on unseen instances for the Vejde agents were
compared to MLP agents trained on each problem instance, as well as the online
planning algorithm Prost. Our results show that Vejde policies in average
generalize to the test instances without a significant loss in score.
Additionally, the inductive agents received scores on unseen test instances
that on average were close to the instance-specific MLP agents.

</details>


### [179] [Constructing a Question-Answering Simulator through the Distillation of LLMs](https://arxiv.org/abs/2509.09226)
*Haipeng Liu,Ting Long,Jing Fu*

Main category: cs.LG

TL;DR: Researchers introduced LDSim, a QA simulator that predicts student response correctness by blending the efficient inference of traditional methods with the strong reasoning of Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The motivation behind the paper is to improve educational recommender systems by avoiding their reliance on real student data, thus mitigating negative impacts from undertraining.

Method: The authors propose LDSim, which utilizes LLM distillation to extract domain knowledge and reasoning capabilities from LLMs for better prediction.

Result: LDSim demonstrated superior performance both in simulation and knowledge tracing tasks, addressing limitations of both traditional and LLM-based methods.

Conclusion: By combining efficiency with enhanced accuracy, LDSim offers an innovative approach to simulate student learning behaviors, optimizing the intersection of traditional sequential modeling and LLM reasoning.

Abstract: The question-answering (QA) simulator is a model that mimics real student
learning behaviors and predicts their correctness of their responses to
questions. QA simulators enable educational recommender systems (ERS) to
collect large amounts of training data without interacting with real students,
thereby preventing harmful recommendations made by an undertrained ERS from
undermining actual student learning. Given the QA history, there are two
categories of solutions to predict the correctness, conducting the simulation:
(1) LLM-free methods, which apply a traditional sequential model to transfer
the QA history into a vector representation first, and make predictions based
on the representation; (2) LLM-based methods, which leverage the domain
knowledge and reasoning capability of LLM to enhence the prediction. LLM-free
methods offer fast inference but generally yield suboptimal performance. In
contrast, most LLM-based methods achieve better results, but at the cost of
slower inference speed and higher GPU memory consumption. In this paper, we
propose a method named LLM Distillation based Simulator (LDSim), which distills
domain knowledge and reasoning capability from an LLM to better assist
prediction, thereby improving simulation performance. Extensive experiments
demonstrate that our LDSim achieves strong results on both the simulation task
and the knowledge tracing (KT) task. Our code is publicly available at
https://anonymous.4open.science/r/LDSim-05A9.

</details>


### [180] [Unsupervised Multi-Attention Meta Transformer for Rotating Machinery Fault Diagnosis](https://arxiv.org/abs/2509.09251)
*Hanyang Wang,Yuxuan Yang,Hongjun Wang,Lihui Wang*

Main category: cs.LG

TL;DR: The paper presents a Multi-Attention Meta Transformer (MMT-FD) method for few-shot unsupervised fault diagnosis of rotating machinery, achieving 99% accuracy using only 1% labeled data.


<details>
  <summary>Details</summary>
Motivation: There is a need to overcome challenges related to limited labeled fault samples and the lack of generalizability in diagnostic models for rotating machinery in practical engineering applications.

Method: The proposed MMT-FD framework integrates a time-frequency domain encoder for status representation through augmentations and a meta-learning network for classification and generalization, followed by fine-tuning with minimal labeled data. Optimization is performed using contrastive learning.

Result: The MMT-FD model demonstrated 99% fault diagnosis accuracy with only 1% labeled sample data, along with strong generalization abilities across different types of mechanical equipment.

Conclusion: MMT-FD is highly efficient for fault diagnosis in rotating machinery, addressing challenges of limited samples and enabling robust generalization to varied equipment types.

Abstract: The intelligent fault diagnosis of rotating mechanical equipment usually
requires a large amount of labeled sample data. However, in practical
industrial applications, acquiring enough data is both challenging and
expensive in terms of time and cost. Moreover, different types of rotating
mechanical equipment with different unique mechanical properties, require
separate training of diagnostic models for each case. To address the challenges
of limited fault samples and the lack of generalizability in prediction models
for practical engineering applications, we propose a Multi-Attention Meta
Transformer method for few-shot unsupervised rotating machinery fault diagnosis
(MMT-FD). This framework extracts potential fault representations from
unlabeled data and demonstrates strong generalization capabilities, making it
suitable for diagnosing faults across various types of mechanical equipment.
The MMT-FD framework integrates a time-frequency domain encoder and a
meta-learning generalization model. The time-frequency domain encoder predicts
status representations generated through random augmentations in the
time-frequency domain. These enhanced data are then fed into a meta-learning
network for classification and generalization training, followed by fine-tuning
using a limited amount of labeled data. The model is iteratively optimized
using a small number of contrastive learning iterations, resulting in high
efficiency. To validate the framework, we conducted experiments on a bearing
fault dataset and rotor test bench data. The results demonstrate that the
MMT-FD model achieves 99\% fault diagnosis accuracy with only 1\% of labeled
sample data, exhibiting robust generalization capabilities.

</details>


### [181] [Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents](https://arxiv.org/abs/2509.09265)
*Jiawei Wang,Jiacai Liu,Yuqian Fu,Yingru Li,Xintao Wang,Yuan Lin,Yu Yue,Lin Zhang,Yang Wang,Ke Wang*

Main category: cs.LG

TL;DR: The paper introduces Entropy-Modulated Policy Gradients (EMPG) to address challenges in LLM-based agents solving long-horizon tasks by stabilizing learning and improving credit assignment.


<details>
  <summary>Details</summary>
Motivation: Agents based on LLMs struggle with sparse, outcome-based rewards, making it hard to assign credit to intermediate steps effectively. Existing solutions focus on dense rewards but fail to address inherent issues in policy gradient updates tied to entropy.

Method: The proposed EMPG framework recalibrates policy gradient updates by amplifying correct actions with high confidence, penalizing confident errors, and attenuating updates for uncertain steps. It also introduces a bonus term to encourage more predictable paths.

Result: EMPG achieved substantial performance improvements across three agent tasks (WebShop, ALFWorld, Deep Search), outperforming baselines and demonstrating robust capabilities.

Conclusion: The EMPG framework provides an effective mechanism for addressing inefficiencies in policy gradient learning, leading to better task performance and exploration stability in LLM-based long-horizon agents.

Abstract: In long-horizon tasks, recent agents based on Large Language Models (LLMs)
face a significant challenge that sparse, outcome-based rewards make it
difficult to assign credit to intermediate steps. Previous methods mainly focus
on creating dense reward signals to guide learning, either through traditional
reinforcement learning techniques like inverse reinforcement learning or by
using Process Reward Models for step-by-step feedback. In this paper, we
identify a fundamental problem in the learning dynamics of LLMs: the magnitude
of policy gradients is inherently coupled with the entropy, which leads to
inefficient small updates for confident correct actions and potentially
destabilizes large updates for uncertain ones. To resolve this, we propose
Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the
learning signal based on step-wise uncertainty and the final task outcome. EMPG
amplifies updates for confident correct actions, penalizes confident errors,
and attenuates updates from uncertain steps to stabilize exploration. We
further introduce a bonus term for future clarity that encourages agents to
find more predictable solution paths. Through comprehensive experiments on
three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we
demonstrate that EMPG achieves substantial performance gains and significantly
outperforms strong policy gradient baselines. Project page is at
https://empgseed-seed.github.io/

</details>


### [182] [Data Driven Discovery of Emergent Dynamics in Reaction Diffusion Systems from Sparse and Noisy Observations](https://arxiv.org/abs/2509.09278)
*Saumitra Dwivedi,Ricardo da Silva Torres,Ibrahim A. Hameed,Gunnar Tufte,Anniken Susanne T. Karlsen*

Main category: cs.LG

TL;DR: The study focuses on deriving emergent dynamics from data in reaction-diffusion systems using a novel framework, DRSALife. The method proves effective on tasks like learning dynamics from Cellular Automata models and identifying PDE structures, even with noisy or sparse data.


<details>
  <summary>Details</summary>
Motivation: Reaction-diffusion systems play a critical role across diverse fields like neuroscience and ecology, yet identifying their dynamics without prior knowledge of the underlying physics remains challenging.

Method: The authors propose the DRSALife framework to learn Soft Artificial Life (Soft ALife) rulesets, utilizing machine learning approaches to extract dynamics from observed data and simulate reaction-diffusion systems.

Result: The learned models achieved an accuracy of 74% in predicting emergent dynamics and remained robust when tested against noisy and temporally sparse datasets. The study also identified underlying PDE structures effectively.

Conclusion: The paper establishes that Soft ALife ruleset learning through the DRSALife framework is a viable method for understanding reaction-diffusion systems without requiring prior physical insights. It shows promise for future applications in modeling and system identification.

Abstract: Data-driven discovery of emergent dynamics is gaining popularity,
particularly in the context of reaction-diffusion systems. These systems are
widely studied across various fields, including neuroscience, ecology,
epidemiology, and several other subject areas that deal with emergent dynamics.
A current challenge in the discovery process relates to system identification
when there is no prior knowledge of the underlying physics. We attempt to
address this challenge by learning Soft Artificial Life (Soft ALife) models,
such as Agent-based and Cellular Automata (CA) models, from observed data for
reaction-diffusion systems. In this paper, we present findings on the
applicability of a conceptual framework, the Data-driven Rulesets for Soft
Artificial Life (DRSALife) model, to learn Soft ALife rulesets that accurately
represent emergent dynamics in a reaction-diffusion system from observed data.
This model has demonstrated promising results for Elementary CA Rule 30, Game
of Life, and Vicsek Flocking problems in recent work. To our knowledge, this is
one of the few studies that explore machine-based Soft ALife ruleset learning
and system identification for reaction-diffusion dynamics without any prior
knowledge of the underlying physics. Moreover, we provide comprehensive
findings from experiments investigating the potential effects of using noisy
and sparse observed datasets on learning emergent dynamics. Additionally, we
successfully identify the structure and parameters of the underlying partial
differential equations (PDEs) representing these dynamics. Experimental results
demonstrate that the learned models are able to predict the emergent dynamics
with good accuracy (74%) and exhibit quite robust performance when subjected to
Gaussian noise and temporal sparsity.

</details>


### [183] [MoSE: Unveiling Structural Patterns in Graphs via Mixture of Subgraph Experts](https://arxiv.org/abs/2509.09337)
*Junda Ye,Zhongbao Zhang,Li Sun,Siqiang Luo*

Main category: cs.LG

TL;DR: The paper introduces the Mixture of Subgraph Experts (MoSE) to improve graph neural networks' ability to capture complex subgraph patterns, outperforming existing methods both in flexibility and accuracy.


<details>
  <summary>Details</summary>
Motivation: Graph neural networks struggle to capture high-order subgraph patterns due to their reliance on local, pairwise message passing, which limits their structural expressiveness and applicability across diverse tasks.

Method: The proposed MoSE framework uses anonymous walks to extract subgraphs and dynamically routes them to specialized experts based on structural semantics, offering flexible and interpretable subgraph-based representation learning.

Result: MoSE demonstrates superior performance in experiments compared to competitive baselines and provides insights into learned structural patterns, proving more powerful than the Subgraph Weisfeiler-Lehman Test.

Conclusion: MoSE significantly enhances GNNs' structural expressiveness and flexibility, making it well-suited for diverse graph-related tasks while delivering interpretable results.

Abstract: While graph neural networks (GNNs) have achieved great success in learning
from graph-structured data, their reliance on local, pairwise message passing
restricts their ability to capture complex, high-order subgraph patterns.
leading to insufficient structural expressiveness. Recent efforts have
attempted to enhance structural expressiveness by integrating random walk
kernels into GNNs. However, these methods are inherently designed for
graph-level tasks, which limits their applicability to other downstream tasks
such as node classification. Moreover, their fixed kernel configurations hinder
the model's flexibility in capturing diverse subgraph structures. To address
these limitations, this paper proposes a novel Mixture of Subgraph Experts
(MoSE) framework for flexible and expressive subgraph-based representation
learning across diverse graph tasks. Specifically, MoSE extracts informative
subgraphs via anonymous walks and dynamically routes them to specialized
experts based on structural semantics, enabling the model to capture diverse
subgraph patterns with improved flexibility and interpretability. We further
provide a theoretical analysis of MoSE's expressivity within the Subgraph
Weisfeiler-Lehman (SWL) Test, proving that it is more powerful than SWL.
Extensive experiments, together with visualizations of learned subgraph
experts, demonstrate that MoSE not only outperforms competitive baselines but
also provides interpretable insights into structural patterns learned by the
model.

</details>


### [184] [Robust Non-Linear Correlations via Polynomial Regression](https://arxiv.org/abs/2509.09380)
*Luca Giuliani,Michele Lombardi*

Main category: cs.LG

TL;DR: The paper presents a novel method to compute the Hirschfeld-Gebelein-Rényi (HGR) correlation coefficient using polynomial kernels, enhancing robustness, speed, and reliability for real-world applications.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of uncomputability and bias-variance trade-offs in computing the HGR correlation coefficient, especially for constrained machine learning and real-world applications.

Method: The authors propose a computational approach to HGR that uses user-configurable polynomial kernels, improving robustness and determinism while maintaining computational efficiency.

Result: The experimental analysis demonstrates the effectiveness of the approach within constrained machine learning, showing that the computed HGR produces an insightful subgradient useful as a loss regularizer.

Conclusion: The proposed polynomial kernel-based method offers a more robust and reliable way to compute HGR, making it suitable for practical machine learning applications involving loss regularization.

Abstract: The Hirschfeld-Gebelein-R\'enyi (HGR) correlation coefficient is an extension
of Pearson's correlation that is not limited to linear correlations, with
potential applications in algorithmic fairness, scientific analysis, and causal
discovery. Recently, novel algorithms to estimate HGR in a differentiable
manner have been proposed to facilitate its use as a loss regularizer in
constrained machine learning applications. However, the inherent
uncomputability of HGR requires a bias-variance trade-off, which can possibly
compromise the robustness of the proposed methods, hence raising technical
concerns if applied in real-world scenarios. We introduce a novel computational
approach for HGR that relies on user-configurable polynomial kernels, offering
greater robustness compared to previous methods and featuring a faster yet
almost equally effective restriction. Our approach provides significant
advantages in terms of robustness and determinism, making it a more reliable
option for real-world applications. Moreover, we present a brief experimental
analysis to validate the applicability of our approach within a constrained
machine learning framework, showing that its computation yields an insightful
subgradient that can serve as a loss regularizer.

</details>


### [185] [MetaLLMix : An XAI Aided LLM-Meta-learning Based Approach for Hyper-parameters Optimization](https://arxiv.org/abs/2509.09387)
*Mohammed Tiouti,Mohamed Bal-Ghaoui*

Main category: cs.LG

TL;DR: MetaLLMiX introduces a zero-shot hyperparameter optimization (HPO) method using meta-learning, explainable AI, and efficient LLM reasoning, improving interpretability and reducing computational costs in deep learning tasks.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based AutoML approaches are computationally expensive, rely heavily on trial and error, and provide limited insight and generalizability.

Method: The method integrates meta-learning through historical experiment data, SHAP-based explanations for interpretability, and efficient zero-shot LLM reasoning for HPO and pretrained model selection. Evaluation uses an LLM-as-judge approach for output control.

Result: MetaLLMiX demonstrated competitive or better performance compared to traditional HPO methods on medical imaging datasets, with 99.6%-99.9% faster response times, training accelerations of 2.4-15.7x, and maintaining accuracy within 1-5% of baselines.

Conclusion: The framework offers an interpretable, efficient solution for HPO, eliminating the need for expensive trials, and performs favorably in both accuracy and computational efficiency compared to existing methods.

Abstract: Effective model and hyperparameter selection remains a major challenge in
deep learning, often requiring extensive expertise and computation. While
AutoML and large language models (LLMs) promise automation, current LLM-based
approaches rely on trial and error and expensive APIs, which provide limited
interpretability and generalizability. We propose MetaLLMiX, a zero-shot
hyperparameter optimization framework combining meta-learning, explainable AI,
and efficient LLM reasoning. By leveraging historical experiment outcomes with
SHAP explanations, MetaLLMiX recommends optimal hyperparameters and pretrained
models without additional trials. We further employ an LLM-as-judge evaluation
to control output format, accuracy, and completeness. Experiments on eight
medical imaging datasets using nine open-source lightweight LLMs show that
MetaLLMiX achieves competitive or superior performance to traditional HPO
methods while drastically reducing computational cost. Our local deployment
outperforms prior API-based approaches, achieving optimal results on 5 of 8
tasks, response time reductions of 99.6-99.9%, and the fastest training times
on 6 datasets (2.4-15.7x faster), maintaining accuracy within 1-5% of
best-performing baselines.

</details>


### [186] [LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations](https://arxiv.org/abs/2509.09396)
*Harry Mayne,Ryan Othniel Kearns,Yushi Yang,Andrew M. Bean,Eoin Delaney,Chris Russell,Adam Mahdi*

Main category: cs.LG

TL;DR: Language models often fail to produce effective self-generated counterfactual explanations (SCEs), which compromises their utility for explainability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to evaluate the capability of large language models (LLMs) to generate self-explanations, specifically self-generated counterfactual explanations, and determine their reliability as an explainability tool.

Method: The authors assess the validity and minimality of SCEs generated by LLMs across various models, datasets, and evaluation settings. They test both general counterfactual generation and minimal adjustments to input.

Result: LLMs typically produce valid but non-minimal SCEs when generating general counterfactuals, and excessively small edits that fail to alter predictions when tasked with minimal edits.

Conclusion: SCEs are unreliable for explainability, as they often fail to provide meaningful or accurate insights into model decision-making. Deploying LLMs in critical applications must account for this flaw.

Abstract: To collaborate effectively with humans, language models must be able to
explain their decisions in natural language. We study a specific type of
self-explanation: self-generated counterfactual explanations (SCEs), where a
model explains its prediction by modifying the input such that it would have
predicted a different outcome. We evaluate whether LLMs can produce SCEs that
are valid, achieving the intended outcome, and minimal, modifying the input no
more than necessary. When asked to generate counterfactuals, we find that LLMs
typically produce SCEs that are valid, but far from minimal, offering little
insight into their decision-making behaviour. Worryingly, when asked to
generate minimal counterfactuals, LLMs typically make excessively small edits
that fail to change predictions. The observed validity-minimality trade-off is
consistent across several LLMs, datasets, and evaluation settings. Our findings
suggest that SCEs are, at best, an ineffective explainability tool and, at
worst, can provide misleading insights into model behaviour. Proposals to
deploy LLMs in high-stakes settings must consider the impact of unreliable
self-explanations on downstream decision-making. Our code is available at
https://github.com/HarryMayne/SCEs.

</details>


### [187] [Kriging prior Regression: A Case for Kriging-Based Spatial Features with TabPFN in Soil Mapping](https://arxiv.org/abs/2509.09408)
*Jonas Schmidinger,Viacheslav Barkov,Sebastian Vogel,Martin Atzmueller,Gerard B M Heuvelink*

Main category: cs.LG

TL;DR: The paper introduces a hybrid framework, called kriging prior regression (KpR), combining geostatistics and machine learning to improve soil property prediction and mapping.


<details>
  <summary>Details</summary>
Motivation: The researchers aimed to address the limitations of geostatistics and machine learning in isolation by integrating spatial context into machine learning to enhance soil mapping and prediction.

Method: They developed the KpR method, which uses spatial lag features from ordinary kriging to enrich machine learning models and tested it with the TabPFN algorithm on six field-scale datasets.

Result: KpR with TabPFN improved the average R2 by 30% compared to non-spatial machine learning algorithms while providing reliable predictions and uncertainty estimates.

Conclusion: KpR with TabPFN is a robust and versatile framework for digital soil mapping, especially for tasks with small sample sizes in precision agriculture.

Abstract: Machine learning and geostatistics are two fundamentally different frameworks
for predicting and spatially mapping soil properties. Geostatistics leverages
the spatial structure of soil properties, while machine learning captures the
relationship between available environmental features and soil properties. We
propose a hybrid framework that enriches ML with spatial context through
engineering of 'spatial lag' features from ordinary kriging. We call this
approach 'kriging prior regression' (KpR), as it follows the inverse logic of
regression kriging. To evaluate this approach, we assessed both the point and
probabilistic prediction performance of KpR, using the TabPFN model across six
fieldscale datasets from LimeSoDa. These datasets included soil organic carbon,
clay content, and pH, along with features derived from remote sensing and
in-situ proximal soil sensing. KpR with TabPFN demonstrated reliable
uncertainty estimates and more accurate predictions in comparison to several
other spatial techniques (e.g., regression/residual kriging with TabPFN), as
well as to established non-spatial machine learning algorithms (e.g., random
forest). Most notably, it significantly improved the average R2 by around 30%
compared to machine learning algorithms without spatial context. This
improvement was due to the strong prediction performance of the TabPFN
algorithm itself and the complementary spatial information provided by KpR
features. TabPFN is particularly effective for prediction tasks with small
sample sizes, common in precision agriculture, whereas KpR can compensate for
weak relationships between sensing features and soil properties when proximal
soil sensing data are limited. Hence, we conclude that KpR with TabPFN is a
very robust and versatile modelling framework for digital soil mapping in
precision agriculture.

</details>


### [188] [Fused Lasso Improves Accuracy of Co-occurrence Network Inference in Grouped Samples](https://arxiv.org/abs/2509.09413)
*Daniel Agyapong,Briana H. Beatty,Peter G. Kennedy,Toby D. Hocking*

Main category: cs.LG

TL;DR: The paper develops the fuser algorithm for microbiome network inference to account for spatial and temporal adaptations of microbial communities across different environmental niches.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing co-occurrence network inference algorithms that typically focus on static snapshots within single environmental niches and fail to capture dynamic microbial processes or adaptations across diverse ecological conditions.

Method: The study employs the Same-All Cross-validation (SAC) framework to evaluate algorithm performance in two scenarios: within the same environmental niche ('Same') and across combined environmental niches ('All'). The newly proposed algorithm, fuser, retains subsample-specific signals while sharing information across environmental niches, producing environment-specific predictive networks.

Result: The fuser algorithm demonstrated comparable performance to glmnet in homogeneous environments ('Same') and significantly reduced test errors in cross-environment scenarios ('All'), outperforming baseline algorithms.

Conclusion: Fuser successfully addresses the shortcomings of traditional algorithms by better tailoring predictions to specific environments, improving the inference of dynamic microbial associations across diverse conditions.

Abstract: Co-occurrence network inference algorithms have significantly advanced our
understanding of microbiome communities. However, these algorithms typically
analyze microbial associations within samples collected from a single
environmental niche, often capturing only static snapshots rather than dynamic
microbial processes. Previous studies have commonly grouped samples from
different environmental niches together without fully considering how microbial
communities adapt their associations when faced with varying ecological
conditions. Our study addresses this limitation by explicitly investigating
both spatial and temporal dynamics of microbial communities. We analyzed
publicly available microbiome abundance data across multiple locations and time
points, to evaluate algorithm performance in predicting microbial associations
using our proposed Same-All Cross-validation (SAC) framework. SAC evaluates
algorithms in two distinct scenarios: training and testing within the same
environmental niche (Same), and training and testing on combined data from
multiple environmental niches (All). To overcome the limitations of
conventional algorithms, we propose fuser, an algorithm that, while not
entirely new in machine learning, is novel for microbiome community network
inference. It retains subsample-specific signals while simultaneously sharing
relevant information across environments during training. Unlike standard
approaches that infer a single generalized network from combined data, fuser
generates distinct, environment-specific predictive networks. Our results
demonstrate that fuser achieves comparable predictive performance to existing
algorithms such as glmnet when evaluated within homogeneous environments
(Same), and notably reduces test error compared to baseline algorithms in
cross-environment (All) scenarios.

</details>


### [189] [Composable Score-based Graph Diffusion Model for Multi-Conditional Molecular Generation](https://arxiv.org/abs/2509.09451)
*Anjie Qiao,Zhen Wang,Chuan Chen,DeFu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: CSGD introduces a method for generating molecular graphs that precisely satisfy property constraints using new score-based techniques, achieving better controllability, validity, and fidelity.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve controllable molecular graph generation for material and drug discovery, addressing limitations in multi-conditional settings of recent diffusion models.

Method: The proposed method, CSGD, extends score matching to discrete graphs via concrete scores. Two score-based techniques are introduced: CoG for fine-grained multi-conditional control and PC for mitigating train-test mismatches.

Result: CSGD demonstrates state-of-the-art performance across four molecular datasets, showing a 15.3% improvement in controllability while preserving validity and fidelity.

Conclusion: Score-based modeling proves advantageous for discrete graph generation, enabling flexible and effective molecular design in multi-property scenarios.

Abstract: Controllable molecular graph generation is essential for material and drug
discovery, where generated molecules must satisfy diverse property constraints.
While recent advances in graph diffusion models have improved generation
quality, their effectiveness in multi-conditional settings remains limited due
to reliance on joint conditioning or continuous relaxations that compromise
fidelity. To address these limitations, we propose Composable Score-based Graph
Diffusion model (CSGD), the first model that extends score matching to discrete
graphs via concrete scores, enabling flexible and principled manipulation of
conditional guidance. Building on this foundation, we introduce two score-based
techniques: Composable Guidance (CoG), which allows fine-grained control over
arbitrary subsets of conditions during sampling, and Probability Calibration
(PC), which adjusts estimated transition probabilities to mitigate train-test
mismatches. Empirical results on four molecular datasets show that CSGD
achieves state-of-the-art performance, with a 15.3% average improvement in
controllability over prior methods, while maintaining high validity and
distributional fidelity. Our findings highlight the practical advantages of
score-based modeling for discrete graph generation and its capacity for
flexible, multi-property molecular design.

</details>


### [190] [AquaCast: Urban Water Dynamics Forecasting with Precipitation-Informed Multi-Input Transformer](https://arxiv.org/abs/2509.09458)
*Golnoosh Abdollahinejad,Saleh Baghersalimi,Denisa-Andreea Constantinescu,Sergey Shevchik,David Atienza*

Main category: cs.LG

TL;DR: The paper introduces AquaCast, a deep learning model for urban water dynamics forecasting, achieving state-of-the-art results on various datasets.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome limitations in urban water dynamics forecasting by integrating inter-variable and temporal dependencies using both endogenous and exogenous inputs.

Method: AquaCast employs a multi-input, multi-output deep learning architecture and an embedding layer to handle exogenous variables effectively, focusing forecasts on endogenous factors.

Result: Evaluation on the LausanneCity dataset and synthesized datasets confirms AquaCast's superior performance against baselines and its scalability across temporal complexities.

Conclusion: AquaCast offers a robust, accurate, and scalable solution for forecasting urban water dynamics, outperforming traditional methods on real and synthetic datasets.

Abstract: This work addresses the challenge of forecasting urban water dynamics by
developing a multi-input, multi-output deep learning model that incorporates
both endogenous variables (e.g., water height or discharge) and exogenous
factors (e.g., precipitation history and forecast reports). Unlike conventional
forecasting, the proposed model, AquaCast, captures both inter-variable and
temporal dependencies across all inputs, while focusing forecast solely on
endogenous variables. Exogenous inputs are fused via an embedding layer,
eliminating the need to forecast them and enabling the model to attend to their
short-term influences more effectively. We evaluate our approach on the
LausanneCity dataset, which includes measurements from four urban drainage
sensors, and demonstrate state-of-the-art performance when using only
endogenous variables. Performance also improves with the inclusion of exogenous
variables and forecast reports. To assess generalization and scalability, we
additionally test the model on three large-scale synthesized datasets,
generated from MeteoSwiss records, the Lorenz Attractors model, and the Random
Fields model, each representing a different level of temporal complexity across
100 nodes. The results confirm that our model consistently outperforms existing
baselines and maintains a robust and accurate forecast across both real and
synthetic datasets.

</details>


### [191] [AEGIS: An Agent for Extraction and Geographic Identification in Scholarly Proceedings](https://arxiv.org/abs/2509.09470)
*Om Vishesh,Harshad Khadilkar,Deepak Akkil*

Main category: cs.LG

TL;DR: The paper presents 'Agent-E,' an automated AI system that identifies targeted academic papers and performs predefined actions, achieving high recall and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the significant challenge of managing the rapid growth of academic literature and reduce time-consuming manual scholarly discovery.

Method: Developing and testing 'Agent-E,' a specialized AI agent integrated with Robotic Process Automation (RPA) to identify papers from specific regions and execute predefined tasks.

Result: Agent-E achieved 100% recall and 99.4% accuracy in identifying target papers from 586 papers across five conferences.

Conclusion: Task-oriented AI agents like Agent-E can effectively accelerate academic workflows by performing both information filtering and predefined actions.

Abstract: Keeping pace with the rapid growth of academia literature presents a
significant challenge for researchers, funding bodies, and academic societies.
To address the time-consuming manual effort required for scholarly discovery,
we present a novel, fully automated system that transitions from data discovery
to direct action. Our pipeline demonstrates how a specialized AI agent,
'Agent-E', can be tasked with identifying papers from specific geographic
regions within conference proceedings and then executing a Robotic Process
Automation (RPA) to complete a predefined action, such as submitting a
nomination form. We validated our system on 586 papers from five different
conferences, where it successfully identified every target paper with a recall
of 100% and a near perfect accuracy of 99.4%. This demonstration highlights the
potential of task-oriented AI agents to not only filter information but also to
actively participate in and accelerate the workflows of the academic community.

</details>


### [192] [CountTRuCoLa: Rule Confidence Learning for Temporal Knowledge Graph Forecasting](https://arxiv.org/abs/2509.09474)
*Julia Gastinger,Christian Meilicke,Heiner Stuckenschmidt*

Main category: cs.LG

TL;DR: The paper proposes an explainable method using temporal rules for forecasting temporal knowledge graphs, surpassing state-of-the-art models across nine datasets.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of interpretability in temporal knowledge graph forecasting, despite recent advancements.

Method: The approach involves learning four types of temporal rules and using a confidence function based on recency and frequency.

Result: The proposed method achieves or outperforms eight state-of-the-art models and two baselines across nine datasets.

Conclusion: The method offers competitive forecasting performance while ensuring full interpretability of predictions.

Abstract: We address the task of temporal knowledge graph (TKG) forecasting by
introducing a fully explainable method based on temporal rules. Motivated by
recent work proposing a strong baseline using recurrent facts, our approach
learns four simple types of rules with a confidence function that considers
both recency and frequency. Evaluated on nine datasets, our method matches or
surpasses the performance of eight state-of-the-art models and two baselines,
while providing fully interpretable predictions.

</details>


### [193] [Balancing Utility and Privacy: Dynamically Private SGD with Random Projection](https://arxiv.org/abs/2509.09485)
*Zhanhong Jiang,Md Zahid Hasan,Nastaran Saadati,Aditya Balu,Chao Liu,Soumik Sarkar*

Main category: cs.LG

TL;DR: This paper introduces D2P2-SGD, a stochastic optimization method that combines dynamic differential privacy and random projection to improve privacy-utility tradeoffs in machine learning while retaining sub-linear convergence rates.


<details>
  <summary>Details</summary>
Motivation: The study aims to address privacy leakage issues in stochastic optimization and challenges in efficiently training large-scale machine learning models using existing techniques.

Method: The approach integrates dynamic privacy mechanisms with automatic gradient clipping and incorporates random projection techniques to optimize the trade-off between privacy and model utility.

Result: Theoretical analysis demonstrates sub-linear convergence rates, and experiments across diverse datasets show significant accuracy improvements while ensuring privacy.

Conclusion: D2P2-SGD is an effective optimization method that balances privacy and utility tradeoffs, making it a valuable tool for large-scale and privacy-sensitive machine learning applications.

Abstract: Stochastic optimization is a pivotal enabler in modern machine learning,
producing effective models for various tasks. However, several existing works
have shown that model parameters and gradient information are susceptible to
privacy leakage. Although Differentially Private SGD (DPSGD) addresses privacy
concerns, its static noise mechanism impacts the error bounds for model
performance. Additionally, with the exponential increase in model parameters,
efficient learning of these models using stochastic optimizers has become more
challenging. To address these concerns, we introduce the Dynamically
Differentially Private Projected SGD (D2P2-SGD) optimizer. In D2P2-SGD, we
combine two important ideas: (i) dynamic differential privacy (DDP) with
automatic gradient clipping and (ii) random projection with SGD, allowing
dynamic adjustment of the tradeoff between utility and privacy of the model. It
exhibits provably sub-linear convergence rates across different objective
functions, matching the best available rate. The theoretical analysis further
suggests that DDP leads to better utility at the cost of privacy, while random
projection enables more efficient model learning. Extensive experiments across
diverse datasets show that D2P2-SGD remarkably enhances accuracy while
maintaining privacy. Our code is available here.

</details>


### [194] [PIPES: A Meta-dataset of Machine Learning Pipelines](https://arxiv.org/abs/2509.09512)
*Cynthia Moreira Maia,Lucas B. V. de Amorim,George D. C. Cavalcanti,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: The paper addresses algorithm selection challenges in machine learning, proposing PIPES, a diversified and comprehensive repository to overcome limitations of OpenML.


<details>
  <summary>Details</summary>
Motivation: To tackle inefficiencies and biases in meta-learning repositories like OpenML by providing a more diverse and exhaustive pipeline dataset.

Method: Creation of PIPES, a dataset containing results from 9,408 diverse pipelines applied to 300 datasets, including exhaustive information such as error messages.

Result: PIPES offers a broad collection of experiments that cover diverse preprocessing pipelines, improving representativeness and diversity.

Conclusion: PIPES enhances meta-learning by supporting diverse pipeline analyses and can be expanded to benefit future studies in the field.

Abstract: Solutions to the Algorithm Selection Problem (ASP) in machine learning face
the challenge of high computational costs associated with evaluating various
algorithms' performances on a given dataset. To mitigate this cost, the
meta-learning field can leverage previously executed experiments shared in
online repositories such as OpenML. OpenML provides an extensive collection of
machine learning experiments. However, an analysis of OpenML's records reveals
limitations. It lacks diversity in pipelines, specifically when exploring data
preprocessing steps/blocks, such as scaling or imputation, resulting in limited
representation. Its experiments are often focused on a few popular techniques
within each pipeline block, leading to an imbalanced sample. To overcome the
observed limitations of OpenML, we propose PIPES, a collection of experiments
involving multiple pipelines designed to represent all combinations of the
selected sets of techniques, aiming at diversity and completeness. PIPES stores
the results of experiments performed applying 9,408 pipelines to 300 datasets.
It includes detailed information on the pipeline blocks, training and testing
times, predictions, performances, and the eventual error messages. This
comprehensive collection of results allows researchers to perform analyses
across diverse and representative pipelines and datasets. PIPES also offers
potential for expansion, as additional data and experiments can be incorporated
to support the meta-learning community further. The data, code, supplementary
material, and all experiments can be found at
https://github.com/cynthiamaia/PIPES.git.

</details>


### [195] [Cough Classification using Few-Shot Learning](https://arxiv.org/abs/2509.09515)
*Yoga Disha Sendhil Kumar,Manas V Shetty,Sudip Vhaduri*

Main category: cs.LG

TL;DR: The study explores few-shot learning's ability to classify respiratory sounds for COVID-19, Flu, and healthy conditions using Prototypical Networks with limited labeled data. It achieves competitive accuracy in both binary and multi-class settings.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of limited labeled data in medical diagnostics, particularly for classifying respiratory sounds and identifying conditions such as COVID-19 and Flu.

Method: The paper employs Prototypical Networks with spectrogram representations of cough sounds and compares few-shot learning to traditional deep learning. It also evaluates performance differences between binary and multi-class classification.

Result: Few-shot learning models achieved 74.87% accuracy in multi-class classification with 15 support examples per class, and over 70% accuracy in binary classification across all class pairs. Flu was most distinguishable, while Healthy was most challenging. Statistical tests showed no significant performance differences between binary and multi-class approaches.

Conclusion: Few-shot learning is viable for medical diagnostics such as respiratory sound classification, offering competitive performance even with limited labeled data. Multi-class classification is feasible alongside binary approaches.

Abstract: This paper investigates the effectiveness of few-shot learning for
respiratory sound classification, focusing on coughbased detection of COVID-19,
Flu, and healthy conditions. We leverage Prototypical Networks with spectrogram
representations of cough sounds to address the challenge of limited labeled
data. Our study evaluates whether few-shot learning can enable models to
achieve performance comparable to traditional deep learning approaches while
using significantly fewer training samples. Additionally, we compare
multi-class and binary classification models to assess whether multi-class
models can perform comparably to their binary counterparts. Experimental
findings show that few-shot learning models can achieve competitive accuracy.
Our model attains 74.87% accuracy in multi-class classification with only 15
support examples per class, while binary classification achieves over 70%
accuracy across all class pairs. Class-wise analysis reveals Flu as the most
distinguishable class, and Healthy as the most challenging. Statistical tests
(paired t-test p = 0.149, Wilcoxon p = 0.125) indicate no significant
performance difference between binary and multiclass models, supporting the
viability of multi-class classification in this setting. These results
highlight the feasibility of applying few-shot learning in medical diagnostics,
particularly when large labeled datasets are unavailable.

</details>


### [196] [Graph Alignment via Dual-Pass Spectral Encoding and Latent Space Communication](https://arxiv.org/abs/2509.09597)
*Maysam Behmanesh,Erkan Turan,Maks Ovsjanikov*

Main category: cs.LG

TL;DR: The paper addresses the challenge of aligning nodes from multiple graphs without ground-truth correspondences, proposing a framework to improve node distinctiveness and alignment through spectral filtering and geometric mapping.


<details>
  <summary>Details</summary>
Motivation: Current unsupervised graph alignment methods suffer from oversmoothing, degrading node distinctiveness, and the misalignment of latent spaces due to various forms of structural and feature inconsistencies.

Method: A dual-pass encoder is used with low-pass and high-pass spectral filters for distinct embeddings, combined with a geometry-aware functional map module to align latent spaces through bijective and isometric transformations.

Result: The approach outperformed current unsupervised methods in robustness and accuracy across graph benchmarks and also generalized well to vision-language alignment tasks using pretrained models.

Conclusion: The proposed framework effectively addresses critical limitations in graph alignment by enhancing node distinctiveness and ensuring consistent latent space geometry, providing strong capabilities across diverse scenarios.

Abstract: Graph alignment-the problem of identifying corresponding nodes across
multiple graphs-is fundamental to numerous applications. Most existing
unsupervised methods embed node features into latent representations to enable
cross-graph comparison without ground-truth correspondences. However, these
methods suffer from two critical limitations: the degradation of node
distinctiveness due to oversmoothing in GNN-based embeddings, and the
misalignment of latent spaces across graphs caused by structural noise, feature
heterogeneity, and training instability, ultimately leading to unreliable node
correspondences. We propose a novel graph alignment framework that
simultaneously enhances node distinctiveness and enforces geometric consistency
across latent spaces. Our approach introduces a dual-pass encoder that combines
low-pass and high-pass spectral filters to generate embeddings that are both
structure-aware and highly discriminative. To address latent space
misalignment, we incorporate a geometry-aware functional map module that learns
bijective and isometric transformations between graph embeddings, ensuring
consistent geometric relationships across different representations. Extensive
experiments on graph benchmarks demonstrate that our method consistently
outperforms existing unsupervised alignment baselines, exhibiting superior
robustness to structural inconsistencies and challenging alignment scenarios.
Additionally, comprehensive evaluation on vision-language benchmarks using
diverse pretrained models shows that our framework effectively generalizes
beyond graph domains, enabling unsupervised alignment of vision and language
representations.

</details>


### [197] [Conditioning on PDE Parameters to Generalise Deep Learning Emulation of Stochastic and Chaotic Dynamics](https://arxiv.org/abs/2509.09599)
*Ira J. S. Shokar,Rich R. Kerswell,Peter H. Haynes*

Main category: cs.LG

TL;DR: This paper presents a deep learning-based emulator for parameterized stochastic and chaotic spatio-temporal systems modeled by PDEs, offering speed-ups over traditional numerical integration and enabling efficient parameter exploration.


<details>
  <summary>Details</summary>
Motivation: The study of spatio-temporal systems faces computational challenges when using conventional numerical integration for a wide range of parameter values. A fast and generalizable approach is needed to explore these systems efficiently.

Method: The approach involves pre-training the emulator on a single parameter domain, then fine-tuning on a smaller but diverse dataset to achieve generalization. It uses local attention mechanisms to adapt to varying domain sizes and resolutions. Tests were conducted using the chaotic Kuramoto-Sivashinsky equation and beta-plane turbulence models.

Result: The emulator successfully captures dynamics at interpolated parameter values, demonstrating significant computational efficiency and retaining accuracy. A probabilistic variant also accounts for uncertainty and allows statistical analysis of rare events.

Conclusion: This emulator enables efficient exploration of spatio-temporal systems under a wide parameter range, providing a computationally efficient and flexible tool with uncertainty quantification capabilities.

Abstract: We present a deep learning emulator for stochastic and chaotic
spatio-temporal systems, explicitly conditioned on the parameter values of the
underlying partial differential equations (PDEs). Our approach involves
pre-training the model on a single parameter domain, followed by fine-tuning on
a smaller, yet diverse dataset, enabling generalisation across a broad range of
parameter values. By incorporating local attention mechanisms, the network is
capable of handling varying domain sizes and resolutions. This enables
computationally efficient pre-training on smaller domains while requiring only
a small additional dataset to learn how to generalise to larger domain sizes.
We demonstrate the model's capabilities on the chaotic Kuramoto-Sivashinsky
equation and stochastically-forced beta-plane turbulence, showcasing its
ability to capture phenomena at interpolated parameter values. The emulator
provides significant computational speed-ups over conventional numerical
integration, facilitating efficient exploration of parameter space, while a
probabilistic variant of the emulator provides uncertainty quantification,
allowing for the statistical study of rare events.

</details>


### [198] [ReBaNO: Reduced Basis Neural Operator Mitigating Generalization Gaps and Achieving Discretization Invariance](https://arxiv.org/abs/2509.09611)
*Haolan Zheng,Yanlai Chen,Jiequn Han,Yue Yu*

Main category: cs.LG

TL;DR: This paper introduces the Reduced Basis Neural Operator (ReBaNO), a new method for solving PDEs with multiple inputs, which achieves higher generalization performance and strict discretization invariance compared to other state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing challenges in learning operators for solving PDEs with multiple inputs, particularly aiming to minimize the generalization gap and achieve discretization invariance, which are limitations in existing methods.

Method: The proposed method, ReBaNO, combines a greedy Reduced Basis methodology with a compact, physics-informed neural network architecture. It uses task-specific activation functions and offline network structure optimization via a rigorous algorithm.

Result: ReBaNO demonstrated superior performance over other operator learning methods such as PCA-Net, DeepONet, FNO, and CNO, notably in generalization for both in- and out-of-distribution cases while being the only method to achieve discretization invariance.

Conclusion: ReBaNO is an advanced operator learning method offering computational efficiency and improved performance for solving PDEs, addressing both generalization gaps and numerical discretization challenges effectively.

Abstract: We propose a novel data-lean operator learning algorithm, the Reduced Basis
Neural Operator (ReBaNO), to solve a group of PDEs with multiple distinct
inputs. Inspired by the Reduced Basis Method and the recently introduced
Generative Pre-Trained Physics-Informed Neural Networks, ReBaNO relies on a
mathematically rigorous greedy algorithm to build its network structure offline
adaptively from the ground up. Knowledge distillation via task-specific
activation function allows ReBaNO to have a compact architecture requiring
minimal computational cost online while embedding physics. In comparison to
state-of-the-art operator learning algorithms such as PCA-Net, DeepONet, FNO,
and CNO, numerical results demonstrate that ReBaNO significantly outperforms
them in terms of eliminating/shrinking the generalization gap for both in- and
out-of-distribution tests and being the only operator learning algorithm
achieving strict discretization invariance.

</details>


### [199] [Explaining Concept Drift through the Evolution of Group Counterfactuals](https://arxiv.org/abs/2509.09616)
*Ignacy Stępka,Jerzy Stefanowski*

Main category: cs.LG

TL;DR: This paper presents a method to explain concept drift in machine learning by analyzing group-based counterfactual explanations (GCEs).


<details>
  <summary>Details</summary>
Motivation: Concept drift often degrades machine learning models' performance in dynamic environments, but current methods lack the ability to explain how and why decisions change.

Method: The authors track the evolution of group-based counterfactual explanations, analyzing changes in cluster centroids and counterfactual action vectors within a three-layer framework.

Result: The approach enables a holistic diagnosis of concept drift by combining data distribution shifts, model prediction disagreements, and explanation-layer insights.

Conclusion: This method provides a more nuanced understanding of concept drift, allowing for identification of specific root causes like spatial shifts or concept re-labeling.

Abstract: Machine learning models in dynamic environments often suffer from concept
drift, where changes in the data distribution degrade performance. While
detecting this drift is a well-studied topic, explaining how and why the
model's decision-making logic changes still remains a significant challenge. In
this paper, we introduce a novel methodology to explain concept drift by
analyzing the temporal evolution of group-based counterfactual explanations
(GCEs). Our approach tracks shifts in the GCEs' cluster centroids and their
associated counterfactual action vectors before and after a drift. These
evolving GCEs act as an interpretable proxy, revealing structural changes in
the model's decision boundary and its underlying rationale. We operationalize
this analysis within a three-layer framework that synergistically combines
insights from the data layer (distributional shifts), the model layer
(prediction disagreement), and our proposed explanation layer. We show that
such holistic view allows for a more comprehensive diagnosis of drift, making
it possible to distinguish between different root causes, such as a spatial
data shift versus a re-labeling of concepts.

</details>


### [200] [Functional Groups are All you Need for Chemically Interpretable Molecular Property Prediction](https://arxiv.org/abs/2509.09619)
*Roshan Balaji,Joe Bobby,Nirav Pravinbhai Bhatt*

Main category: cs.LG

TL;DR: The paper proposes Functional Group Representation (FGR), a deep learning framework leveraging functional group information to predict molecular properties with enhanced interpretability for chemists.


<details>
  <summary>Details</summary>
Motivation: Deep learning models have advanced molecular property prediction but lack chemical interpretability, which limits their adoption by chemists.

Method: The authors develop the Functional Group Representation (FGR) framework, encoding molecules using chemically curated and mined functional groups (FGs and MFGs). A latent space representation is learned through pre-training on unlabeled molecules, and 2D structure-based descriptors are included.

Result: The FGR framework outperforms other methods across 33 benchmark datasets in various scientific domains while providing chemical explanations for predictions.

Conclusion: FGR achieves state-of-the-art results while enhancing interpretability by aligning predictions with chemical principles, bridging the gap between chemists and deep learning models.

Abstract: Molecular property prediction using deep learning (DL) models has accelerated
drug and materials discovery, but the resulting DL models often lack
interpretability, hindering their adoption by chemists. This work proposes
developing molecule representations using the concept of Functional Groups (FG)
in chemistry. We introduce the Functional Group Representation (FGR) framework,
a novel approach to encoding molecules based on their fundamental chemical
substructures. Our method integrates two types of functional groups: those
curated from established chemical knowledge (FG), and those mined from a large
molecular corpus using sequential pattern mining (MFG). The resulting FGR
framework encodes molecules into a lower-dimensional latent space by leveraging
pre-training on a large dataset of unlabeled molecules. Furthermore, the
proposed framework allows the inclusion of 2D structure-based descriptors of
molecules. We demonstrate that the FGR framework achieves state-of-the-art
performance on a diverse range of 33 benchmark datasets spanning physical
chemistry, biophysics, quantum mechanics, biological activity, and
pharmacokinetics while enabling chemical interpretability. Crucially, the
model's representations are intrinsically aligned with established chemical
principles, allowing chemists to directly link predicted properties to specific
functional groups and facilitating novel insights into structure-property
relationships. Our work presents a significant step toward developing
high-performing, chemically interpretable DL models for molecular discovery.

</details>


### [201] [Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for Medicaid Care Management](https://arxiv.org/abs/2509.09655)
*Sanjay Basu,Sadiq Y. Patel,Parth Sheth,Bhairavi Muralidharan,Namrata Elamaran,Aakriti Kinra,Rajaie Batniji*

Main category: cs.LG

TL;DR: The paper introduces FG-FARL, an offline RL method to balance safety and fairness across subgroups, evaluated on healthcare data, showing improved fairness metrics while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the harm and fairness disparities in decision support systems, especially in sensitive domains like healthcare, by developing RL methods that ensure safety thresholds and fairness.

Method: The authors propose FG-FARL, an offline reinforcement learning procedure that adjusts group-level safety thresholds to balance harm and fairness. They evaluate it using Medicaid health data and compare it with existing methods: BC and HACO.

Result: FG-FARL achieved performance similar to the baselines (BC and HACO) while improving fairness metrics, as measured by subgroup disparities and confidence intervals.

Conclusion: FG-FARL demonstrates a practical and effective approach to making decision support systems both safer and fairer, particularly in healthcare settings, by aligning fairness with safety goals.

Abstract: We introduce Feasibility-Guided Fair Adaptive Reinforcement Learning
(FG-FARL), an offline RL procedure that calibrates per-group safety thresholds
to reduce harm while equalizing a chosen fairness target (coverage or harm)
across protected subgroups. Using de-identified longitudinal trajectories from
a Medicaid population health management program, we evaluate FG-FARL against
behavior cloning (BC) and HACO (Hybrid Adaptive Conformal Offline RL; a global
conformal safety baseline). We report off-policy value estimates with bootstrap
95% confidence intervals and subgroup disparity analyses with p-values. FG-FARL
achieves comparable value to baselines while improving fairness metrics,
demonstrating a practical path to safer and more equitable decision support.

</details>


### [202] [ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms](https://arxiv.org/abs/2509.09679)
*Bingxin Xu,Zhen Dong,Oussama Elachqar,Yuzhang Shang*

Main category: cs.LG

TL;DR: ButterflyQuant improves memory efficiency in large language models by using learnable butterfly transforms and uniformity regularization, achieving better performance under extreme 2-bit quantization.


<details>
  <summary>Details</summary>
Motivation: Large language models face deployment challenges on consumer hardware due to their high memory requirements. Extreme quantization methods like 2-bit often suffer due to activation outliers, motivating adaptive techniques that address outlier suppression.

Method: The paper introduces ButterflyQuant, which replaces fixed Hadamard rotations with learnable butterfly transforms parameterized by continuous Givens angles. This ensures smoother optimization, retains orthogonality, and adds uniformity regularization to improve post-transformation quantization.

Result: ButterflyQuant reduces computational complexity to $O(n \log n)$ with fewer learnable parameters and achieves a perplexity score of 15.4 on LLaMA-2-7B with 2-bit quantization, outperforming similar methods like QuaRot (22.1 perplexity).

Conclusion: ButterflyQuant provides a memory-efficient and adaptable approach to extreme quantization for large language models, enabling practical deployment through negligible learning costs and improved performance.

Abstract: Large language models require massive memory footprints, severely limiting
deployment on consumer hardware. Quantization reduces memory through lower
numerical precision, but extreme 2-bit quantization suffers from catastrophic
performance loss due to outliers in activations. Rotation-based methods such as
QuIP and QuaRot apply orthogonal transforms to eliminate outliers before
quantization, using computational invariance: $\mathbf{y} = \mathbf{Wx} =
(\mathbf{WQ}^T)(\mathbf{Qx})$ for orthogonal $\mathbf{Q}$. However, these
methods use fixed transforms--Hadamard matrices achieving optimal worst-case
coherence $\mu = 1/\sqrt{n}$--that cannot adapt to specific weight
distributions. We identify that different transformer layers exhibit distinct
outlier patterns, motivating layer-adaptive rotations rather than
one-size-fits-all approaches. We propose ButterflyQuant, which replaces
Hadamard rotations with learnable butterfly transforms parameterized by
continuous Givens rotation angles. Unlike Hadamard's discrete $\{+1, -1\}$
entries that are non-differentiable and prohibit gradient-based learning,
butterfly transforms' continuous parameterization enables smooth optimization
while guaranteeing orthogonality by construction. This orthogonal constraint
ensures theoretical guarantees in outlier suppression while achieving $O(n \log
n)$ computational complexity with only $\frac{n \log n}{2}$ learnable
parameters. We further introduce a uniformity regularization on
post-transformation activations to promote smoother distributions amenable to
quantization. Learning requires only 128 calibration samples and converges in
minutes on a single GPU--a negligible one-time cost. On LLaMA-2-7B with 2-bit
quantization, ButterflyQuant achieves 15.4 perplexity versus 22.1 for QuaRot.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [203] [Time-Fair Benchmarking for Metaheuristics: A Restart-Fair Protocol for Fixed-Time Comparisons](https://arxiv.org/abs/2509.08986)
*Junbo Jacob Lian*

Main category: cs.NE

TL;DR: This paper critiques the reliance on function evaluations (FEs) to compare metaheuristics, proposing wall-clock time as a fairer measure. It formalizes a benchmarking protocol based on fixed-time budgets, providing fairer comparisons.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the frequent use of FEs as performance indicators in metaheuristics, which often ignores additional computational overheads such as preprocessing or hyperparameter tuning. The authors identify a need for fairer, more practical evaluation metrics.

Method: The authors propose a fixed-time, restart-fair benchmarking protocol where each algorithm gets an equal wall-clock time budget per problem. This method allows restarts, early termination, and internal adaptivity. Metrics like anytime performance curves, ERT, and time-based performance profiles are recommended.

Result: A standardized checklist is introduced to make evaluations more transparent, reproducible, and informative. The approach aligns computational resource consumption more realistically with algorithm evaluation.

Conclusion: Using wall-clock time as a constraint improves the fairness and credibility of metaheuristic comparisons, encouraging a shift to more practical and reproducible evaluation methods in the community.

Abstract: Numerous purportedly improved metaheuristics claim superior performance based
on equivalent function evaluations (FEs), yet often conceal additional
computational burdens in more intensive iterations, preprocessing stages, or
hyperparameter tuning. This paper posits that wall-clock time, rather than
solely FEs, should serve as the principal budgetary constraint for equitable
comparisons. We formalize a fixed-time, restart-fair benchmarking protocol
wherein each algorithm is allotted an identical wall-clock time budget per
problem instance, permitting unrestricted utilization of restarts, early
termination criteria, and internal adaptive mechanisms. We advocate for the
adoption of anytime performance curves, expected running time (ERT) metrics,
and performance profiles that employ time as the cost measure, all aimed at
predefined targets. Furthermore, we introduce a concise, reproducible checklist
to standardize reporting practices and mitigate undisclosed computational
overheads. This approach fosters more credible and practically relevant
evaluations of metaheuristic algorithms.

</details>


### [204] [A modified RIME algorithm with covariance learning and diversity enhancement for numerical optimization](https://arxiv.org/abs/2509.09529)
*Shangqing Shi,Luoxiao Zhang,Yuchen Yin,Xiong Yang,Hoileong Lee*

Main category: cs.NE

TL;DR: A modified metaheuristic algorithm, MRIME-CD, addresses limitations of RIME by enhancing population diversity and optimizing strategies. It shows superior performance in improving solution accuracy, speed, and stability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome the weaknesses of the RIME algorithm, namely rapid loss of population diversity and tendency to get trapped in local optima, affecting its exploration and exploitation balance.

Method: Three strategies are proposed: covariance learning to improve diversity, average bootstrapping for balanced exploration in early stages, and a stochastic covariance learning strategy to handle stagnation and escape local optima.

Result: The proposed MRIME-CD algorithm demonstrated superior optimization performance with higher solution accuracy, faster convergence, and improved stability on standardized test sets validated by statistical methods.

Conclusion: MRIME-CD effectively addresses RIME's weaknesses and achieves improved optimization capabilities, showing potential for broader applications.

Abstract: Metaheuristics are widely applied for their ability to provide more efficient
solutions. The RIME algorithm is a recently proposed physical-based
metaheuristic algorithm with certain advantages. However, it suffers from rapid
loss of population diversity during optimization and is prone to fall into
local optima, leading to unbalanced exploitation and exploration. To address
the shortcomings of RIME, this paper proposes a modified RIME with covariance
learning and diversity enhancement (MRIME-CD). The algorithm applies three
strategies to improve the optimization capability. First, a covariance learning
strategy is introduced in the soft-rime search stage to increase the population
diversity and balance the over-exploitation ability of RIME through the
bootstrapping effect of dominant populations. Second, in order to moderate the
tendency of RIME population to approach the optimal individual in the early
search stage, an average bootstrapping strategy is introduced into the
hard-rime puncture mechanism, which guides the population search through the
weighted position of the dominant populations, thus enhancing the global search
ability of RIME in the early stage. Finally, a new stagnation indicator is
proposed, and a stochastic covariance learning strategy is used to update the
stagnant individuals in the population when the algorithm gets stagnant, thus
enhancing the ability to jump out of the local optimal solution. The proposed
MRIME-CD algorithm is subjected to a series of validations on the CEC2017 test
set, the CEC2022 test set, and the experimental results are analyzed using the
Friedman test, the Wilcoxon rank sum test, and the Kruskal Wallis test. The
results show that MRIME-CD can effectively improve the performance of basic
RIME and has obvious superiorities in terms of solution accuracy, convergence
speed and stability.

</details>


### [205] [An improved educational competition optimizer with multi-covariance learning operators for global optimization problems](https://arxiv.org/abs/2509.09552)
*Baoqi Zhao,Xiong Yang,Hoileong Lee,Bowen Dong*

Main category: cs.NE

TL;DR: This paper introduces an enhanced metaheuristic algorithm, IECO-MCO, which improves the Educational Competition Optimizer (ECO) to better handle complex optimization problems via multi-covariance learning operators.


<details>
  <summary>Details</summary>
Motivation: The original ECO algorithm struggles with imbalanced exploration and exploitation, leading to susceptibility to local optima and poor performance in solving complex optimization problems.

Method: The paper proposes IECO-MCO, which introduces three multi-covariance learning operators to enhance ECO's performance by balancing exploration and exploitation and avoiding premature convergence. It is evaluated with benchmark functions and statistical tests.

Result: IECO-MCO achieves better convergence speed, stability, and resistance to local optima compared to ECO and other competing algorithms. It ranks well on benchmark tests and proves effective for constrained optimization problems.

Conclusion: IECO-MCO significantly improves ECO by demonstrating superior optimization capabilities, robustness, and practical effectiveness, making it suitable for complex real-world problems.

Abstract: The educational competition optimizer is a recently introduced metaheuristic
algorithm inspired by human behavior, originating from the dynamics of
educational competition within society. Nonetheless, ECO faces constraints due
to an imbalance between exploitation and exploration, rendering it susceptible
to local optima and demonstrating restricted effectiveness in addressing
complex optimization problems. To address these limitations, this study
presents an enhanced educational competition optimizer (IECO-MCO) utilizing
multi-covariance learning operators. In IECO, three distinct covariance
learning operators are introduced to improve the performance of ECO. Each
operator effectively balances exploitation and exploration while preventing
premature convergence of the population. The effectiveness of IECO is assessed
through benchmark functions derived from the CEC 2017 and CEC 2022 test suites,
and its performance is compared with various basic and improved algorithms
across different categories. The results demonstrate that IECO-MCO surpasses
the basic ECO and other competing algorithms in convergence speed, stability,
and the capability to avoid local optima. Furthermore, statistical analyses,
including the Friedman test, Kruskal-Wallis test, and Wilcoxon rank-sum test,
are conducted to validate the superiority of IECO-MCO over the compared
algorithms. Compared with the basic algorithm (improved algorithm), IECO-MCO
achieved an average ranking of 2.213 (2.488) on the CE2017 and CEC2022 test
suites. Additionally, the practical applicability of the proposed IECO-MCO
algorithm is verified by solving constrained optimization problems. The
experimental outcomes demonstrate the superior performance of IECO-MCO in
tackling intricate optimization problems, underscoring its robustness and
practical effectiveness in real-world scenarios.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [206] [HD-MoE: Hybrid and Dynamic Parallelism for Mixture-of-Expert LLMs with 3D Near-Memory Processing](https://arxiv.org/abs/2509.09420)
*Haochen Huang,Shuzhang Zhong,Zhe Zhang,Shuangchen Li,Dimin Niu,Hongzhong Zheng,Runsheng Wang,Meng Li*

Main category: cs.PF

TL;DR: This paper introduces HD-MoE, an optimization method for Mixture-of-Expert-enabled Large Language Models (LLMs) on Near-Memory Processing (NMP) accelerators, achieving up to 1.8x speedup over conventional parallel strategies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address efficiency challenges in Mixture-of-Expert (MoE) LLMs on Near-Memory Processing (NMP) accelerators, particularly communication costs and computation imbalance in existing parallel mapping strategies like Tensor Parallelism (TP) and Expert Parallelism (EP).

Method: The authors propose HD-MoE, combining an offline hybrid parallel mapping algorithm and an online dynamic scheduling strategy to balance and optimize parallel computation across distributed memory and compute in NMP architectures, specifically targeting communication cost reductions and improved utilization.

Result: The experimental results show that HD-MoE provides a speedup ranging from 1.1x to 1.8x over TP, 1.1x to 1.5x over EP, and 1.0x to 1.4x compared to a baseline hybrid TP-EP strategy using Compute-Balanced parallelism.

Conclusion: HD-MoE effectively optimizes MoE computation for NMP accelerators, leading to higher efficiency and speedups compared to traditional parallelism strategies.

Abstract: Large Language Models (LLMs) with Mixture-of-Expert (MoE) architectures
achieve superior model performance with reduced computation costs, but at the
cost of high memory capacity and bandwidth requirements. Near-Memory Processing
(NMP) accelerators that stack memory directly on the compute through hybrid
bonding have demonstrated high bandwidth with high energy efficiency, becoming
a promising architecture for MoE models. However, as NMP accelerators comprise
distributed memory and computation, how to map the MoE computation directly
determines the LLM inference efficiency. Existing parallel mapping strategies,
including Tensor Parallelism (TP) and Expert Parallelism (EP), suffer from
either high communication costs or unbalanced computation utilization, leading
to inferior efficiency. The dynamic routing mechanism of MoE LLMs further
aggravates the efficiency challenges. Therefore, in this paper, we propose
HD-MoE to automatically optimize the MoE parallel computation across an NMP
accelerator. HD-MoE features an offline automatic hybrid parallel mapping
algorithm and an online dynamic scheduling strategy to reduce the communication
costs while maximizing the computation utilization. With extensive experimental
results, we demonstrate that HD-MoE achieves a speedup ranging from 1.1x to
1.8x over TP, 1.1x to 1.5x over EP, and 1.0x to 1.4x over the baseline Hybrid
TP-EP with Compute-Balanced parallelism strategies.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [207] [Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs](https://arxiv.org/abs/2509.09019)
*Mohit Tekriwal,John Sarracino*

Main category: cs.PL

TL;DR: The paper investigates the correctness of floating-point optimizations in LLVM IR, focusing on Fused-Multiply-Add. Preliminary work employs formal verification with Rocq. Future work includes extending features and optimizations.


<details>
  <summary>Details</summary>
Motivation: Verify the correctness of aggressive compiler optimizations, specifically floating-point optimizations at the LLVM IR level, ensuring reliability and performance in scientific computing.

Method: Used the Verified LLVM framework with the Rocq theorem prover to formally prove the correctness of Fused-Multiply-Add (FMA) optimizations in LLVM IR for arithmetic expressions.

Result: Completed the formal correctness proof for FMA optimization on $a * b + c$ and suggested avenues for extending this work to additional optimizations and program features.

Conclusion: Preliminary validation demonstrates the potential of formal methods like Rocq in verifying the correctness of floating-point optimizations. Future directions aim to expand the scope of verification to include more optimizations and program features.

Abstract: Scientific computing programs often undergo aggressive compiler optimization
to achieve high performance and efficient resource utilization. While
performance is critical, we also need to ensure that these optimizations are
correct. In this paper, we focus on a specific class of optimizations,
floating-point optimizations, notably due to fast math, at the LLVM IR level.
We present a preliminary work, which leverages the Verified LLVM framework in
the Rocq theorem prover, to prove the correctness of Fused-Multiply-Add (FMA)
optimization for a basic block implementing the arithmetic expression $a * b +
c$ . We then propose ways to extend this preliminary results by adding more
program features and fast math floating-point optimizations.

</details>


### [208] [Dependent-Type-Preserving Memory Allocation](https://arxiv.org/abs/2509.09059)
*Paulette Koronkevich,William J. Bowman*

Main category: cs.PL

TL;DR: The paper addresses issues of specification violations in dependently-typed programming languages during and after compilation by introducing a typed intermediate language for dependent memory allocation and a dependent-type-preserving compiler.


<details>
  <summary>Details</summary>
Motivation: The paper aims to resolve the issue of specification violations caused by erasing types during compilation and issues arising from external programs that lack type safety, which lead to potential behavior changes and runtime errors.

Method: The authors propose a typed intermediate language that supports dependent memory allocation and a dependent-type-preserving compiler pass to ensure type safety through the compilation process and during linking.

Result: The work provides a mechanism for type checking during linking and prevents linking with ill-typed external programs, enhancing the reliability of programs written in dependently-typed languages.

Conclusion: The approach ensures that dependently-typed programming languages retain type safety even after compilation and when interfacing with external programs, thereby addressing a critical risk in the software development lifecycle.

Abstract: Dependently typed programming languages such as Coq, Agda, Idris, and F*,
allow programmers to write detailed specifications of their programs and prove
their programs meet these specifications. However, these specifications can be
violated during compilation since they are erased after type checking. External
programs linked with the compiled program can violate the specifications of the
original program and change the behavior of the compiled program -- even when
compiled with a verified compiler. For example, since Coq does not allow
explicitly allocating memory, a programmer might link their Coq program with a
C program that can allocate memory. Even if the Coq program is compiled with a
verified compiler, the external C program can still violate the memory-safe
specification of the Coq program by providing an uninitialized pointer to
memory. This error could be ruled out by type checking in a language expressive
enough to indicate whether memory is initialized versus uninitialized. Linking
with a program with an uninitialized pointer could be considered ill-typed, and
our linking process could prevent linking with ill-typed programs. To
facilitate type checking during linking, we can use type-preserving
compilation, which preserves the types through the compilation process. In this
ongoing work, we develop a typed intermediate language that supports dependent
memory allocation, as well as a dependent-type-preserving compiler pass for
memory allocation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [209] [Multi Robot Coordination in Highly Dynamic Environments: Tackling Asymmetric Obstacles and Limited Communication](https://arxiv.org/abs/2509.08859)
*Vincenzo Suriani,Daniele Affinita,Domenico D. Bloisi,Daniele Nardi*

Main category: cs.RO

TL;DR: The paper introduces a distributed coordination method for multi-agent systems operating in partially observable environments with poor communication and asymmetric, active obstacles, validated through RoboCup experiments.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of coordinating multi-agent systems with low communication capabilities and active asymmetric obstacles, issues that limit the applicability of current methods.

Method: The authors propose a novel distributed coordination method inspired by market-based task assignments, specifically designed for scenarios with poor communication, asymmetric obstacles, and dynamic task reallocation.

Result: Experimental results demonstrate a significant reduction in task overlaps, achieving a 52% decrease in the most frequent reallocated task, both in simulated and real-world RoboCup environments.

Conclusion: The proposed architecture is effective for coordinating autonomous agents in constrained environments, offering improved task allocation efficiency in practical RoboCup scenarios.

Abstract: Coordinating a fully distributed multi-agent system (MAS) can be challenging
when the communication channel has very limited capabilities in terms of
sending rate and packet payload. When the MAS has to deal with active obstacles
in a highly partially observable environment, the communication channel
acquires considerable relevance. In this paper, we present an approach to deal
with task assignments in extremely active scenarios, where tasks need to be
frequently reallocated among the agents participating in the coordination
process. Inspired by market-based task assignments, we introduce a novel
distributed coordination method to orchestrate autonomous agents' actions
efficiently in low communication scenarios. In particular, our algorithm takes
into account asymmetric obstacles. While in the real world, the majority of
obstacles are asymmetric, they are usually treated as symmetric ones, thus
limiting the applicability of existing methods. To summarize, the presented
architecture is designed to tackle scenarios where the obstacles are active and
asymmetric, the communication channel is poor and the environment is partially
observable. Our approach has been validated in simulation and in the real
world, using a team of NAO robots during official RoboCup competitions.
Experimental results show a notable reduction in task overlaps in limited
communication settings, with a decrease of 52% in the most frequent reallocated
task.

</details>


### [210] [Rapid Manufacturing of Lightweight Drone Frames Using Single-Tow Architected Composites](https://arxiv.org/abs/2509.09024)
*Md Habib Ullah Khan,Kaiyue Deng,Ismail Mujtaba Khan,Kelvin Fu*

Main category: cs.RO

TL;DR: This paper demonstrates a novel method (3DFiT) for manufacturing lightweight drone frames using continuous fibers, creating structures that are lighter and stronger than conventional techniques.


<details>
  <summary>Details</summary>
Motivation: Existing composite manufacturing methods have limitations in achieving weight savings, structural efficiency, and complex 3D architectures required for aerospace and robotics applications like drone frames.

Method: The study proposes 3D Fiber Tethering (3DFiT), a technique using continuous single tow fibers to create Face Centered Cubic (FFC) lattice structures, eliminating weaknesses associated with traditional composite assembly.

Result: Mechanical testing shows the drone frame is four to eight times stronger than metal and thermoplastic, weighs 10% less than the DJI F450 frame, and improves flight time by three minutes with confirmed stability and durability.

Conclusion: This research highlights the efficiency and scalability of 3DFiT for creating lightweight and high-strength drone frames, paving the way for advanced applications in aerospace and robotics.

Abstract: The demand for lightweight and high-strength composite structures is rapidly
growing in aerospace and robotics, particularly for optimized drone frames.
However, conventional composite manufacturing methods struggle to achieve
complex 3D architectures for weight savings and rely on assembling separate
components, which introduce weak points at the joints. Additionally,
maintaining continuous fiber reinforcement remains challenging, limiting
structural efficiency. In this study, we demonstrate the lightweight Face
Centered Cubic (FFC) lattice structured conceptualization of drone frames for
weight reduction and complex topology fabrication through 3D Fiber Tethering
(3DFiT) using continuous single tow fiber ensuring precise fiber alignment,
eliminating weak points associated with traditional composite assembly.
Mechanical testing demonstrates that the fabricated drone frame exhibits a high
specific strength of around four to eight times the metal and thermoplastic,
outperforming other conventional 3D printing methods. The drone frame weighs
only 260 g, making it 10% lighter than the commercial DJI F450 frame, enhancing
structural integrity and contributing to an extended flight time of three
minutes, while flight testing confirms its stability and durability under
operational conditions. The findings demonstrate the potential of single tow
lattice truss-based drone frames, with 3DFiT serving as a scalable and
efficient manufacturing method.

</details>


### [211] [KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning](https://arxiv.org/abs/2509.09074)
*Alice Kate Li,Thales C Silva,Victoria Edwards,Vijay Kumar,M. Ani Hsieh*

Main category: cs.RO

TL;DR: This study introduces KoopMotion, a Koopman Operator-based motion planning method that ensures smooth and convergent robot trajectory tracking to a desired end point efficiently.


<details>
  <summary>Details</summary>
Motivation: Koopman operator theory allows modeling of dynamical systems, but it lacks the ability to enforce trajectory and goal convergence, crucial for tasks like learning from demonstrations.

Method: KoopMotion represents motion flow fields as dynamical systems parameterized by Koopman Operators, ensuring trajectory convergence by leveraging divergence properties of learned flow fields. It was tested on datasets and a real robot in dynamic environments.

Result: KoopMotion demonstrated high sample efficiency, requiring only 3% of the LASA dataset for dense motion plans, and yielded significant improvements over baselines in spatial and temporal dynamics modeling. Effectiveness was validated both on datasets and a physical robot.

Conclusion: KoopMotion provides a novel and efficient approach for learning and executing trajectory-following tasks with improved accuracy, making it suitable for practical robotic applications in static and dynamic environments.

Abstract: In this work, we propose a novel flow field-based motion planning method that
drives a robot from any initial state to a desired reference trajectory such
that it converges to the trajectory's end point. Despite demonstrated efficacy
in using Koopman operator theory for modeling dynamical systems, Koopman does
not inherently enforce convergence to desired trajectories nor to specified
goals -- a requirement when learning from demonstrations (LfD). We present
KoopMotion which represents motion flow fields as dynamical systems,
parameterized by Koopman Operators to mimic desired trajectories, and leverages
the divergence properties of the learnt flow fields to obtain smooth motion
fields that converge to a desired reference trajectory when a robot is placed
away from the desired trajectory, and tracks the trajectory until the end
point. To demonstrate the effectiveness of our approach, we show evaluations of
KoopMotion on the LASA human handwriting dataset and a 3D manipulator
end-effector trajectory dataset, including spectral analysis. We also perform
experiments on a physical robot, verifying KoopMotion on a miniature autonomous
surface vehicle operating in a non-static fluid flow environment. Our approach
is highly sample efficient in both space and time, requiring only 3\% of the
LASA dataset to generate dense motion plans. Additionally, KoopMotion provides
a significant improvement over baselines when comparing metrics that measure
spatial and temporal dynamics modeling efficacy.

</details>


### [212] [Kinetostatics and Particle-Swarm Optimization of Vehicle-Mounted Underactuated Metamorphic Loading Manipulators](https://arxiv.org/abs/2509.09093)
*Nan Mao,Guanglu Jia,Junpeng Chen,Emmanouil Spyrakos-Papastavridis,Jian S. Dai*

Main category: cs.RO

TL;DR: The paper presents an innovative underactuated metamorphic loading manipulator (UMLM) that integrates a metamorphic arm and an adaptive gripper, offering versatile, efficient, and dynamic task adaptability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the issues present in fixed DoF mechanisms, such as excessive actuators, complex control, and limited adaptability, by introducing a more efficient and flexible system.

Method: The study combines a metamorphic arm and a passively adaptive gripper, utilizing geometric constraints for topology reconfiguration while applying Particle-Swarm Optimization to refine gripper dimensions for dynamic tasks.

Result: Simulation results show the UMLM to have easy-to-implement control, operational versatility, and effectiveness in dynamic environments for grasping diverse objects.

Conclusion: The work demonstrates the potential of underactuated metamorphic mechanisms for adaptable loading tasks and highlights the scalability of the generalized modeling and optimization framework for robotic systems.

Abstract: Fixed degree-of-freedom (DoF) loading mechanisms often suffer from excessive
actuators, complex control, and limited adaptability to dynamic tasks. This
study proposes an innovative mechanism of underactuated metamorphic loading
manipulators (UMLM), integrating a metamorphic arm with a passively adaptive
gripper. The metamorphic arm exploits geometric constraints, enabling the
topology reconfiguration and flexible motion trajectories without additional
actuators. The adaptive gripper, driven entirely by the arm, conforms to
diverse objects through passive compliance. A structural model is developed,
and a kinetostatics analysis is conducted to investigate isomorphic grasping
configurations. To optimize performance, Particle-Swarm Optimization (PSO) is
utilized to refine the gripper's dimensional parameters, ensuring robust
adaptability across various applications. Simulation results validate the
UMLM's easily implemented control strategy, operational versatility, and
effectiveness in grasping diverse objects in dynamic environments. This work
underscores the practical potential of underactuated metamorphic mechanisms in
applications requiring efficient and adaptable loading solutions. Beyond the
specific design, this generalized modeling and optimization framework extends
to a broader class of manipulators, offering a scalable approach to the
development of robotic systems that require efficiency, flexibility, and robust
performance.

</details>


### [213] [LIPM-Guided Reinforcement Learning for Stable and Perceptive Locomotion in Bipedal Robots](https://arxiv.org/abs/2509.09106)
*Haokai Su,Haoxiang Luo,Shunpeng Yang,Kaiwen Jiang,Wei Zhang,Hua Chen*

Main category: cs.RO

TL;DR: The paper introduces a reward mechanism inspired by the Linear Inverted Pendulum Model (LIPM) to enhance the stability, adaptability, and perceptive capabilities of bipedal robots navigating unstructured outdoor terrains.


<details>
  <summary>Details</summary>
Motivation: To tackle the longstanding issue of stable and robust locomotion in bipedal robots, particularly in unpredictable outdoor terrains characterized by complex geometries and potential disturbances.

Method: The authors developed a reward design based on LIPM principles to regulate dynamic balance and optimize camera viewpoints. They also introduced a Reward Fusion Module (RFM) and double-critic architecture to balance stability and locomotion priorities.

Result: Extensive experiments on simulated and real-world environments showcased exceptional terrain adaptability, disturbance resistance, and consistent performance across varying speeds and environmental conditions.

Conclusion: The proposed approach effectively equips bipedal robots with robust perceptive locomotion capabilities, significantly enhancing their performance in unstructured outdoor terrains.

Abstract: Achieving stable and robust perceptive locomotion for bipedal robots in
unstructured outdoor environments remains a critical challenge due to complex
terrain geometry and susceptibility to external disturbances. In this work, we
propose a novel reward design inspired by the Linear Inverted Pendulum Model
(LIPM) to enable perceptive and stable locomotion in the wild. The LIPM
provides theoretical guidance for dynamic balance by regulating the center of
mass (CoM) height and the torso orientation. These are key factors for
terrain-aware locomotion, as they help ensure a stable viewpoint for the
robot's camera. Building on this insight, we design a reward function that
promotes balance and dynamic stability while encouraging accurate CoM
trajectory tracking. To adaptively trade off between velocity tracking and
stability, we leverage the Reward Fusion Module (RFM) approach that prioritizes
stability when needed. A double-critic architecture is adopted to separately
evaluate stability and locomotion objectives, improving training efficiency and
robustness. We validate our approach through extensive experiments on a bipedal
robot in both simulation and real-world outdoor environments. The results
demonstrate superior terrain adaptability, disturbance rejection, and
consistent performance across a wide range of speeds and perceptual conditions.

</details>


### [214] [AEOS: Active Environment-aware Optimal Scanning Control for UAV LiDAR-Inertial Odometry in Complex Scenes](https://arxiv.org/abs/2509.09141)
*Jianping Li,Xinhang Xu,Zhongyuan Liu,Shenghai Yuan,Muqing Cao,Lihua Xie*

Main category: cs.RO

TL;DR: The paper introduces AEOS, a biologically inspired framework, to address the challenges of narrow FoV and limited payload in UAV-mounted LiDAR systems, improving odometry accuracy via adaptive control.


<details>
  <summary>Details</summary>
Motivation: The authors aim to overcome the limitations of traditional UAV-based LiDAR systems, particularly the narrow field of view and inability to use multi-sensor setups due to payload constraints, which negatively impact odometry in complex environments.

Method: AEOS combines model predictive control (MPC) for analytic uncertainty modeling and reinforcement learning (RL) for learning implicit cost maps from panoramic depth data. The system is trained in a simulated environment using diverse real-world LiDAR maps for generalization and sim-to-real transfer.

Result: Experiments in both simulation and real-world settings show that AEOS achieves significant improvements in odometry accuracy compared to traditional fixed-rate or fully learned approaches, while adhering to real-time performance requirements.

Conclusion: AEOS is an efficient and adaptive LiDAR control framework that outperforms traditional and learned methods in odometry accuracy, demonstrating promise for UAV applications operating in complex environments.

Abstract: LiDAR-based 3D perception and localization on unmanned aerial vehicles (UAVs)
are fundamentally limited by the narrow field of view (FoV) of compact LiDAR
sensors and the payload constraints that preclude multi-sensor configurations.
Traditional motorized scanning systems with fixed-speed rotations lack scene
awareness and task-level adaptability, leading to degraded odometry and mapping
performance in complex, occluded environments. Inspired by the active sensing
behavior of owls, we propose AEOS (Active Environment-aware Optimal Scanning),
a biologically inspired and computationally efficient framework for adaptive
LiDAR control in UAV-based LiDAR-Inertial Odometry (LIO). AEOS combines model
predictive control (MPC) and reinforcement learning (RL) in a hybrid
architecture: an analytical uncertainty model predicts future pose
observability for exploitation, while a lightweight neural network learns an
implicit cost map from panoramic depth representations to guide exploration. To
support scalable training and generalization, we develop a point cloud-based
simulation environment with real-world LiDAR maps across diverse scenes,
enabling sim-to-real transfer. Extensive experiments in both simulation and
real-world environments demonstrate that AEOS significantly improves odometry
accuracy compared to fixed-rate, optimization-only, and fully learned
baselines, while maintaining real-time performance under onboard computational
constraints. The project page can be found at
https://kafeiyin00.github.io/AEOS/.

</details>


### [215] [Occupancy-aware Trajectory Planning for Autonomous Valet Parking in Uncertain Dynamic Environments](https://arxiv.org/abs/2509.09206)
*Farhad Nawaz,Faizan M. Tariq,Sangjae Bae,David Isele,Avinash Singh,Nadia Figueroa,Nikolai Matni,Jovin D'sa*

Main category: cs.RO

TL;DR: This paper introduces a method to predict future parking spot availability in dynamic environments, improving autonomous valet parking efficiency and safety.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing autonomous parking approaches that rely on static assumptions or instantaneous observations, which may lead to inefficiencies in dynamic and uncertain environments.

Method: The paper proposes a probabilistic parking spot occupancy estimator using partial observations and predicted motion of dynamic agents, combined with an adaptive strategy planner that balances parking goals with exploratory actions.

Result: Through simulations in large parking lots, the framework was shown to significantly enhance parking efficiency, safety, and trajectory smoothness compared to existing methods.

Conclusion: The proposed approach effectively addresses dynamic parking challenges by reasoning about future space availability, ensuring safer and more efficient autonomous valet parking.

Abstract: Accurately reasoning about future parking spot availability and integrated
planning is critical for enabling safe and efficient autonomous valet parking
in dynamic, uncertain environments. Unlike existing methods that rely solely on
instantaneous observations or static assumptions, we present an approach that
predicts future parking spot occupancy by explicitly distinguishing between
initially vacant and occupied spots, and by leveraging the predicted motion of
dynamic agents. We introduce a probabilistic spot occupancy estimator that
incorporates partial and noisy observations within a limited Field-of-View
(FoV) model and accounts for the evolving uncertainty of unobserved regions.
Coupled with this, we design a strategy planner that adaptively balances
goal-directed parking maneuvers with exploratory navigation based on
information gain, and intelligently incorporates wait-and-go behaviors at
promising spots. Through randomized simulations emulating large parking lots,
we demonstrate that our framework significantly improves parking efficiency,
safety margins, and trajectory smoothness compared to existing approaches.

</details>


### [216] [RENet: Fault-Tolerant Motion Control for Quadruped Robots via Redundant Estimator Networks under Visual Collapse](https://arxiv.org/abs/2509.09283)
*Yueqi Zhang,Quancheng Qian,Taixian Hou,Peng Zhai,Xiaoyi Wei,Kangmai Hu,Jiafu Yi,Lihua Zhang*

Main category: cs.RO

TL;DR: The paper introduces the Redundant Estimator Network (RENet) framework to improve quadruped robots' ability to navigate outdoor environments during visual perception challenges.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of vision-based locomotion in quadruped robots caused by depth sensor noise and environmental prediction issues in complex outdoor settings.

Method: A dual-estimator architecture with online estimator adaptation is proposed to ensure robust motion performance and seamless transitions between estimation modules during visual uncertainties.

Result: Experimental validation shows the RENet framework's effectiveness in handling degraded visual perception, enabling reliable robot deployment in complex outdoor environments.

Conclusion: The framework demonstrates its capability as a practical and robust solution for outdoor field conditions, enhancing robotic deployment stability and adaptability.

Abstract: Vision-based locomotion in outdoor environments presents significant
challenges for quadruped robots. Accurate environmental prediction and
effective handling of depth sensor noise during real-world deployment remain
difficult, severely restricting the outdoor applications of such algorithms. To
address these deployment challenges in vision-based motion control, this letter
proposes the Redundant Estimator Network (RENet) framework. The framework
employs a dual-estimator architecture that ensures robust motion performance
while maintaining deployment stability during onboard vision failures. Through
an online estimator adaptation, our method enables seamless transitions between
estimation modules when handling visual perception uncertainties. Experimental
validation on a real-world robot demonstrates the framework's effectiveness in
complex outdoor environments, showing particular advantages in scenarios with
degraded visual perception. This framework demonstrates its potential as a
practical solution for reliable robotic deployment in challenging field
conditions. Project website: https://RENet-Loco.github.io/

</details>


### [217] [OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning](https://arxiv.org/abs/2509.09332)
*Yuecheng Liu,Dafeng Chi,Shiguang Wu,Zhanguang Zhang,Yuzheng Zhuang,Bowen Yang,He Zhu,Lingfeng Zhang,Pengwei Xie,David Gamaliel Arcos Bravo,Yingxue Zhang,Jianye Hao,Xingyue Quan*

Main category: cs.RO

TL;DR: OmniEVA is introduced as a planning system addressing 3D adaptability and embodiment constraints using advanced hybrid techniques.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal large language models struggle to adapt to diverse spatial tasks and real-world embodiment constraints, limiting their practical applications.

Method: OmniEVA utilizes a Task-Adaptive 3D Grounding mechanism for situational 3D processing and an Embodiment-Aware Reasoning framework that integrates task goals with physical constraints.

Result: Extensive experiments show state-of-the-art performance in embodied reasoning and planning across a range of scenarios, supported by custom benchmarks.

Conclusion: OmniEVA showcases robust and versatile planning capabilities, marking a significant step forward in practical multimodal embodied intelligence.

Abstract: Recent advances in multimodal large language models (MLLMs) have opened new
opportunities for embodied intelligence, enabling multimodal understanding,
reasoning, and interaction, as well as continuous spatial decision-making.
Nevertheless, current MLLM-based embodied systems face two critical
limitations. First, Geometric Adaptability Gap: models trained solely on 2D
inputs or with hard-coded 3D geometry injection suffer from either insufficient
spatial information or restricted 2D generalization, leading to poor
adaptability across tasks with diverse spatial demands. Second, Embodiment
Constraint Gap: prior work often neglects the physical constraints and
capacities of real robots, resulting in task plans that are theoretically valid
but practically infeasible.To address these gaps, we introduce OmniEVA -- an
embodied versatile planner that enables advanced embodied reasoning and task
planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding
mechanism, which introduces a gated router to perform explicit selective
regulation of 3D fusion based on contextual requirements, enabling
context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware
Reasoning framework that jointly incorporates task goals and embodiment
constraints into the reasoning loop, resulting in planning decisions that are
both goal-directed and executable. Extensive experimental results demonstrate
that OmniEVA not only achieves state-of-the-art general embodied reasoning
performance, but also exhibits a strong ability across a wide range of
downstream scenarios. Evaluations of a suite of proposed embodied benchmarks,
including both primitive and composite tasks, confirm its robust and versatile
planning capabilities. Project page: https://omnieva.github.io

</details>


### [218] [AGILOped: Agile Open-Source Humanoid Robot for Research](https://arxiv.org/abs/2509.09364)
*Grzegorz Ficht,Luis Denninger,Sven Behnke*

Main category: cs.RO

TL;DR: AGILOped is an open-source humanoid robot designed for accessibility and performance, showcasing capabilities like walking, jumping, and self-recovery.


<details>
  <summary>Details</summary>
Motivation: Address the gap between high-performance humanoid robots and their accessibility due to high costs or closed-source limitations.

Method: Development of AGILOped using off-the-shelf backdrivable actuators and standard electronic components, with a compact and lightweight design.

Result: AGILOped successfully performed experiments in walking, jumping, impact mitigation, and self-recovery.

Conclusion: AGILOped provides a viable, accessible, and high-performance platform for research in humanoid robotics.

Abstract: With academic and commercial interest for humanoid robots peaking, multiple
platforms are being developed. Through a high level of customization, they
showcase impressive performance. Most of these systems remain closed-source or
have high acquisition and maintenance costs, however. In this work, we present
AGILOped - an open-source humanoid robot that closes the gap between high
performance and accessibility. Our robot is driven by off-the-shelf
backdrivable actuators with high power density and uses standard electronic
components. With a height of 110 cm and weighing only 14.5 kg, AGILOped can be
operated without a gantry by a single person. Experiments in walking, jumping,
impact mitigation and getting-up demonstrate its viability for use in research.

</details>


### [219] [VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model](https://arxiv.org/abs/2509.09372)
*Yihao Wang,Pengxiang Ding,Lingxiao Li,Can Cui,Zirui Ge,Xinyang Tong,Wenxuan Song,Han Zhao,Wei Zhao,Pengxu Hou,Siteng Huang,Yifan Tang,Wenhui Wang,Ru Zhang,Jianyi Liu,Donglin Wang*

Main category: cs.RO

TL;DR: The paper introduces VLA-Adapter, a method that reduces dependency on large-scale vision-language models for robotic applications, achieving high performance without extensive pre-training.


<details>
  <summary>Details</summary>
Motivation: To address the high training costs and dependency on large-scale Vision-Language Models (VLMs) when bridging vision-language representations to robotic actions.

Method: The authors propose VLA-Adapter, which includes a lightweight Policy module with Bridge Attention that autonomously optimizes conditions needed for action space. This eliminates the need for pre-training on robotic data and uses a smaller model backbone.

Result: Experiments show that VLA-Adapter achieves state-of-the-art performance on simulated and real-world robotic benchmarks with fast inference speed and minimal training requirements.

Conclusion: VLA-Adapter reduces barriers to deploying Vision-Language-Action models by enabling training on a consumer-grade GPU within 8 hours while maintaining high performance, making it accessible for broader use.

Abstract: Vision-Language-Action (VLA) models typically bridge the gap between
perceptual and action spaces by pre-training a large-scale Vision-Language
Model (VLM) on robotic data. While this approach greatly enhances performance,
it also incurs significant training costs. In this paper, we investigate how to
effectively bridge vision-language (VL) representations to action (A). We
introduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA
models on large-scale VLMs and extensive pre-training. To this end, we first
systematically analyze the effectiveness of various VL conditions and present
key findings on which conditions are essential for bridging perception and
action spaces. Based on these insights, we propose a lightweight Policy module
with Bridge Attention, which autonomously injects the optimal condition into
the action space. In this way, our method achieves high performance using only
a 0.5B-parameter backbone, without any robotic data pre-training. Extensive
experiments on both simulated and real-world robotic benchmarks demonstrate
that VLA-Adapter not only achieves state-of-the-art level performance, but also
offers the fast inference speed reported to date. Furthermore, thanks to the
proposed advanced bridging paradigm, VLA-Adapter enables the training of a
powerful VLA model in just 8 hours on a single consumer-grade GPU, greatly
lowering the barrier to deploying the VLA model. Project page:
https://vla-adapter.github.io/.

</details>


### [220] [A Hybrid Hinge-Beam Continuum Robot with Passive Safety Capping for Real-Time Fatigue Awareness](https://arxiv.org/abs/2509.09404)
*Tongshun Chen,Zezhou Sun,Yanhan Sun,Yuhao Wang,Dezhen Song,Ke Wu*

Main category: cs.RO

TL;DR: This paper introduces a fatigue-aware design for continuum robots, addressing durability and safety limitations in current models.


<details>
  <summary>Details</summary>
Motivation: Existing continuum robots lack effective fatigue estimation, limiting their long-term operational reliability, especially under mechanical stress and material degradation.

Method: The authors propose innovations including a Hybrid Hinge-Beam structure for stress mitigation, a Passive Stopper for safety and data collection, and a real-time fatigue estimation method using motor torque analysis.

Result: Experiments demonstrate a 49% reduction in fatigue accumulation and accurate fatigue estimation using their proposed design.

Conclusion: The proposed architecture enhances durability and safety of continuum robots, supporting reliable long-term operation.

Abstract: Cable-driven continuum robots offer high flexibility and lightweight design,
making them well-suited for tasks in constrained and unstructured environments.
However, prolonged use can induce mechanical fatigue from plastic deformation
and material degradation, compromising performance and risking structural
failure. In the state of the art, fatigue estimation of continuum robots
remains underexplored, limiting long-term operation. To address this, we
propose a fatigue-aware continuum robot with three key innovations: (1) a
Hybrid Hinge-Beam structure where TwistBeam and BendBeam decouple torsion and
bending: passive revolute joints in the BendBeam mitigate stress concentration,
while TwistBeam's limited torsional deformation reduces BendBeam stress
magnitude, enhancing durability; (2) a Passive Stopper that safely constrains
motion via mechanical constraints and employs motor torque sensing to detect
corresponding limit torque, ensuring safety and enabling data collection; and
(3) a real-time fatigue-awareness method that estimates stiffness from motor
torque at the limit pose, enabling online fatigue estimation without additional
sensors. Experiments show that the proposed design reduces fatigue accumulation
by about 49% compared with a conventional design, while passive mechanical
limiting combined with motor-side sensing allows accurate estimation of
structural fatigue and damage. These results confirm the effectiveness of the
proposed architecture for safe and reliable long-term operation.

</details>


### [221] [BagIt! An Adaptive Dual-Arm Manipulation of Fabric Bags for Object Bagging](https://arxiv.org/abs/2509.09484)
*Peng Zhou,Jiaming Qi,Hongmin Wu,Chen Wang,Yizhou Chen,Zeqing Zhang*

Main category: cs.RO

TL;DR: The paper proposes a novel dual-robot bagging system using adaptive strategies, visual feedback, and advanced models to handle deformable bags without prior knowledge of bag properties.


<details>
  <summary>Details</summary>
Motivation: Automating complex bagging tasks in industrial settings is challenging due to the unpredictable nature of deformable objects like bags. Current systems struggle without pre-existing data on the properties of these objects.

Method: The system uses an adaptive Structure-of-Interest (SOI) manipulation strategy. Techniques include Gaussian Mixture Models (GMM) for state estimation, Constrained Bidirectional RRTree (CBiRRT) for motion planning, and dual-arm coordination with Model Predictive Control (MPC), all guided by real-time visual feedback.

Result: Experiments show the system can reliably and accurately perform bagging tasks for various objects, demonstrating its adaptability and robustness.

Conclusion: The paper introduces a new approach for robotic manipulation of deformable objects, pushing the boundaries of innovation in automated bagging systems.

Abstract: Bagging tasks, commonly found in industrial scenarios, are challenging
considering deformable bags' complicated and unpredictable nature. This paper
presents an automated bagging system from the proposed adaptive
Structure-of-Interest (SOI) manipulation strategy for dual robot arms. The
system dynamically adjusts its actions based on real-time visual feedback,
removing the need for pre-existing knowledge of bag properties. Our framework
incorporates Gaussian Mixture Models (GMM) for estimating SOI states,
optimization techniques for SOI generation, motion planning via Constrained
Bidirectional Rapidly-exploring Random Tree (CBiRRT), and dual-arm coordination
using Model Predictive Control (MPC). Extensive experiments validate the
capability of our system to perform precise and robust bagging across various
objects, showcasing its adaptability. This work offers a new solution for
robotic deformable object manipulation (DOM), particularly in automated bagging
tasks. Video of this work is available at https://youtu.be/6JWjCOeTGiQ.

</details>


### [222] [SMapper: A Multi-Modal Data Acquisition Platform for SLAM Benchmarking](https://arxiv.org/abs/2509.09509)
*Pedro Miguel Bastos Soares,Ali Tourani,Miguel Fernandez-Cortizas,Asier Bikandi Noya,Jose Luis Sanchez-Lopez,Holger Voos*

Main category: cs.RO

TL;DR: The paper introduces SMapper, an open-hardware platform for SLAM research, and SMapper-light, a high-quality dataset, to address challenges in multimodal sensing, environmental diversity, and reproducibility.


<details>
  <summary>Details</summary>
Motivation: Existing SLAM datasets often suffer from limitations in sensing modalities, environmental diversity, and hardware reproducibility, restricting progress in autonomous navigation and SLAM.

Method: Develop an open-hardware multi-sensor platform (SMapper) with synchronized LiDAR, multi-camera, and inertial sensing, alongside robust calibration and a novel dataset (SMapper-light) for SLAM benchmarking.

Result: The SMapper platform ensures precise spatio-temporal alignment of multimodal sensor data, releases a SLAM dataset (SMapper-light) with sub-centimeter accuracy, and demonstrates benchmarking results using state-of-the-art frameworks.

Conclusion: SMapper offers researchers a reproducible, extensible platform and dataset for advancing SLAM algorithm development, evaluation, and reproducibility.

Abstract: Advancing research in fields like Simultaneous Localization and Mapping
(SLAM) and autonomous navigation critically depends on reliable and
reproducible multimodal datasets. While several influential datasets have
driven progress in these domains, they often suffer from limitations in sensing
modalities, environmental diversity, and the reproducibility of the underlying
hardware setups. To address these challenges, this paper introduces SMapper, a
novel open-hardware, multi-sensor platform designed explicitly for, though not
limited to, SLAM research. The device integrates synchronized LiDAR,
multi-camera, and inertial sensing, supported by a robust calibration and
synchronization pipeline that ensures precise spatio-temporal alignment across
modalities. Its open and replicable design allows researchers to extend its
capabilities and reproduce experiments across both handheld and robot-mounted
scenarios. To demonstrate its practicality, we additionally release
SMapper-light, a publicly available SLAM dataset containing representative
indoor and outdoor sequences. The dataset includes tightly synchronized
multimodal data and ground-truth trajectories derived from offline LiDAR-based
SLAM with sub-centimeter accuracy, alongside dense 3D reconstructions.
Furthermore, the paper contains benchmarking results on state-of-the-art LiDAR
and visual SLAM frameworks using the SMapper-light dataset. By combining
open-hardware design, reproducible data collection, and comprehensive
benchmarking, SMapper establishes a robust foundation for advancing SLAM
algorithm development, evaluation, and reproducibility.

</details>


### [223] [A Neuromorphic Incipient Slip Detection System using Papillae Morphology](https://arxiv.org/abs/2509.09546)
*Yanhui Lu,Zeyu Deng,Stephen J. Redmond,Efi Psomopoulou,Benjamin Ward-Cherrier*

Main category: cs.RO

TL;DR: This paper presents a neuromorphic tactile sensing system using a spiking convolutional neural network, achieving 94.33% accuracy for slip-state classification.


<details>
  <summary>Details</summary>
Motivation: Improving robotic manipulation safety by early detection of object slippage while addressing deployment challenges in energy-constrained edge platforms.

Method: Developed a tactile sensing system using the NeuroTac sensor with papillae-based skin integrated with a spiking convolutional neural network for slip-state classification.

Result: The system achieved 94.33% classification accuracy for no slip, incipient slip, and gross slip. It detected incipient slip at least 360 ms before gross slip in all trials.

Conclusion: The presented system has a reliable and responsive capability for detecting incipient slip, improving manipulation safety.

Abstract: Detecting incipient slip enables early intervention to prevent object
slippage and enhance robotic manipulation safety. However, deploying such
systems on edge platforms remains challenging, particularly due to energy
constraints. This work presents a neuromorphic tactile sensing system based on
the NeuroTac sensor with an extruding papillae-based skin and a spiking
convolutional neural network (SCNN) for slip-state classification. The SCNN
model achieves 94.33% classification accuracy across three classes (no slip,
incipient slip, and gross slip) in slip conditions induced by sensor motion.
Under the dynamic gravity-induced slip validation conditions, after temporal
smoothing of the SCNN's final-layer spike counts, the system detects incipient
slip at least 360 ms prior to gross slip across all trials, consistently
identifying incipient slip before gross slip occurs. These results demonstrate
that this neuromorphic system has stable and responsive incipient slip
detection capability.

</details>


### [224] [ObjectReact: Learning Object-Relative Control for Visual Navigation](https://arxiv.org/abs/2509.09594)
*Sourav Garg,Dustin Craggs,Vineeth Bhat,Lachlan Mares,Stefan Podgorski,Madhava Krishna,Feras Dayoub,Ian Reid*

Main category: cs.RO

TL;DR: This paper introduces an object-focused navigation system using a 3D scene graph for more robust and deployment-invariant control.


<details>
  <summary>Details</summary>
Motivation: The limitations of image-reliant navigation arise from its strict dependence on agent pose and embodiment, motivating exploration of object-based, trajectory-invariant methods.

Method: The authors propose an object-aware 3D scene graph, train a controller ('ObjectReact') conditioned on high-level object costmaps, eliminating reliance on RGB inputs, and test its cross-environment robustness.

Result: Their approach achieves higher invariance in sensor configuration, environment transitions, and navigation tasks such as reverse trajectory execution, generalizing well to real-world settings.

Conclusion: Learning navigation control based on objects, rather than images, improves robustness and generalization across diverse tasks and environments.

Abstract: Visual navigation using only a single camera and a topological map has
recently become an appealing alternative to methods that require additional
sensors and 3D maps. This is typically achieved through an "image-relative"
approach to estimating control from a given pair of current observation and
subgoal image. However, image-level representations of the world have
limitations because images are strictly tied to the agent's pose and
embodiment. In contrast, objects, being a property of the map, offer an
embodiment- and trajectory-invariant world representation. In this work, we
present a new paradigm of learning "object-relative" control that exhibits
several desirable characteristics: a) new routes can be traversed without
strictly requiring to imitate prior experience, b) the control prediction
problem can be decoupled from solving the image matching problem, and c) high
invariance can be achieved in cross-embodiment deployment for variations across
both training-testing and mapping-execution settings. We propose a topometric
map representation in the form of a "relative" 3D scene graph, which is used to
obtain more informative object-level global path planning costs. We train a
local controller, dubbed "ObjectReact", conditioned directly on a high-level
"WayObject Costmap" representation that eliminates the need for an explicit RGB
input. We demonstrate the advantages of learning object-relative control over
its image-relative counterpart across sensor height variations and multiple
navigation tasks that challenge the underlying spatial understanding
capability, e.g., navigating a map trajectory in the reverse direction. We
further show that our sim-only policy is able to generalize well to real-world
indoor environments. Code and supplementary material are accessible via project
page: https://object-react.github.io/

</details>


### [225] [MOFU: Development of a MOrphing Fluffy Unit with Expansion and Contraction Capabilities and Evaluation of the Animacy of Its Movements](https://arxiv.org/abs/2509.09613)
*Taisei Mogi,Mari Saito,Yoshihiro Nakata*

Main category: cs.RO

TL;DR: The study introduces MOFU, a mobile robot capable of whole-body expansion and contraction, and evaluates how these movements affect perceived animacy in robots.


<details>
  <summary>Details</summary>
Motivation: To explore how whole-body volume changes, like expansion-contraction, influence humans' perception of animacy in robots, an underexplored factor in robotic design.

Method: Developed MOFU, a robot with a geometric transformation mechanism for expansion-contraction combined with locomotion, and evaluated its impact on animacy perception in online surveys using the Godspeed Questionnaire Series.

Result: Expansion-contraction movements significantly increased perceived animacy in stationary robots and enhanced animacy when combined with locomotion. However, using two robots did not further boost animacy perceptions.

Conclusion: Whole-body volume-changing movements like expansion-contraction are critical for enhancing perceived animacy in robots and should be prioritized in the design of socially interactive robots.

Abstract: Robots for therapy and social interaction are often intended to evoke
"animacy" in humans. While many robots imitate appearance and joint movements,
little attention has been given to whole-body expansion-contraction,
volume-changing movements observed in living organisms, and their effect on
animacy perception. We developed a mobile robot called "MOFU (Morphing Fluffy
Unit)," capable of whole-body expansion-contraction with a single motor and
covered with a fluffy exterior. MOFU employs a "Jitterbug" structure, a
geometric transformation mechanism that enables smooth volume change in
diameter from 210 to 280 mm using one actuator. It is also equipped with a
differential two-wheel drive mechanism for locomotion. To evaluate the effect
of expansion-contraction movements, we conducted an online survey using videos
of MOFU's behavior. Participants rated impressions with the Godspeed
Questionnaire Series. First, we compared videos of MOFU in a stationary state
with and without expansion-contraction and turning, finding that
expansion-contraction significantly increased perceived animacy. Second, we
hypothesized that presenting two MOFUs would increase animacy compared with a
single robot; however, this was not supported, as no significant difference
emerged. Exploratory analyses further compared four dual-robot motion
conditions. Third, when expansion-contraction was combined with locomotion,
animacy ratings were higher than locomotion alone. These results suggest that
volume-changing movements such as expansion and contraction enhance perceived
animacy in robots and should be considered an important design element in
future robot development aimed at shaping human impressions.

</details>


### [226] [Dexplore: Scalable Neural Control for Dexterous Manipulation from Reference-Scoped Exploration](https://arxiv.org/abs/2509.09671)
*Sirui Xu,Yu-Wei Chao,Liuyu Bian,Arsalan Mousavian,Yu-Xiong Wang,Liang-Yan Gui,Wei Yang*

Main category: cs.RO

TL;DR: Dexplore introduces a unified approach for training robot control policies from imperfect motion-capture data, addressing limitations of traditional multi-step workflows.


<details>
  <summary>Details</summary>
Motivation: Existing hand-object MoCap data holds potential for robotic manipulation training but is limited by inaccuracies and embodiment gaps between human and robot hands.

Method: Dexplore uses a single-loop optimization approach that integrates retargeting and tracking into a reinforcement learning pipeline, treating demonstrations as soft guidance rather than ground truth.

Result: The approach preserves demonstration intent, enables robot-specific strategies, improves robustness, and scales efficiently to large datasets.

Conclusion: Dexplore transforms imperfect demonstrations into effective training signals for robotic dexterous manipulation, enabling real-world generalization across objects and skills.

Abstract: Hand-object motion-capture (MoCap) repositories offer large-scale,
contact-rich demonstrations and hold promise for scaling dexterous robotic
manipulation. Yet demonstration inaccuracies and embodiment gaps between human
and robot hands limit the straightforward use of these data. Existing methods
adopt a three-stage workflow, including retargeting, tracking, and residual
correction, which often leaves demonstrations underused and compound errors
across stages. We introduce Dexplore, a unified single-loop optimization that
jointly performs retargeting and tracking to learn robot control policies
directly from MoCap at scale. Rather than treating demonstrations as ground
truth, we use them as soft guidance. From raw trajectories, we derive adaptive
spatial scopes, and train with reinforcement learning to keep the policy
in-scope while minimizing control effort and accomplishing the task. This
unified formulation preserves demonstration intent, enables robot-specific
strategies to emerge, improves robustness to noise, and scales to large
demonstration corpora. We distill the scaled tracking policy into a
vision-based, skill-conditioned generative controller that encodes diverse
manipulation skills in a rich latent representation, supporting generalization
across objects and real-world deployment. Taken together, these contributions
position Dexplore as a principled bridge that transforms imperfect
demonstrations into effective training signals for dexterous manipulation.

</details>


### [227] [SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning](https://arxiv.org/abs/2509.09674)
*Haozhan Li,Yuxin Zuo,Jiale Yu,Yuhao Zhang,Zhaohui Yang,Kaiyan Zhang,Xuekai Zhu,Yuchen Zhang,Tianxing Chen,Ganqu Cui,Dehui Wang,Dingxiang Luo,Yuchen Fan,Youbang Sun,Jia Zeng,Jiangmiao Pang,Shanghang Zhang,Yu Wang,Yao Mu,Bowen Zhou,Ning Ding*

Main category: cs.RO

TL;DR: The paper introduces SimpleVLA-RL, an efficient reinforcement learning (RL) framework designed for Vision-Language-Action (VLA) models to address limitations in supervised fine-tuning such as data scarcity and distribution shift challenges.


<details>
  <summary>Details</summary>
Motivation: To overcome the dependency on large-scale human-operated robotic data and enhance generalization abilities in robotic manipulation tasks, leveraging RL for step-by-step action planning in VLA is explored.

Method: The authors extend the veRL framework with VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation, tailoring RL specifically for VLA models.

Result: SimpleVLA-RL achieves state-of-the-art performance on LIBERO and exceeds baseline performance on RoboTwin 1.0&2.0 by incorporating exploration-enhancing strategies. It reduces data dependence and improves task generalization.

Conclusion: SimpleVLA-RL demonstrates RL as a potent alternative to supervised fine-tuning for VLA models, enabling robust generalization and surpassing previous approaches in real-world tasks. A novel phenomenon called 'pushcut' was discovered during RL training.

Abstract: Vision-Language-Action (VLA) models have recently emerged as a powerful
paradigm for robotic manipulation. Despite substantial progress enabled by
large-scale pretraining and supervised fine-tuning (SFT), these models face two
fundamental challenges: (i) the scarcity and high cost of large-scale
human-operated robotic trajectories required for SFT scaling, and (ii) limited
generalization to tasks involving distribution shift. Recent breakthroughs in
Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can
dramatically enhance step-by-step reasoning capabilities, raising a natural
question: Can RL similarly improve the long-horizon step-by-step action
planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL
framework tailored for VLA models. Building upon veRL, we introduce
VLA-specific trajectory sampling, scalable parallelization, multi-environment
rendering, and optimized loss computation. When applied to OpenVLA-OFT,
SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $\pi_0$
on RoboTwin 1.0\&2.0 with the exploration-enhancing strategies we introduce.
SimpleVLA-RL not only reduces dependence on large-scale data and enables robust
generalization, but also remarkably surpasses SFT in real-world tasks.
Moreover, we identify a novel phenomenon ``pushcut'' during RL training,
wherein the policy discovers previously unseen patterns beyond those seen in
the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [228] [Pattern-Based File and Data Access with Python Glob: A Comprehensive Guide for Computational Research](https://arxiv.org/abs/2509.08843)
*Sidney Shapiro*

Main category: cs.SE

TL;DR: This paper underscores the utility of the Python glob module for file pattern matching across various research areas and workflows, promoting its use in scalable data processing and reproducible practices.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the lack of documentation and methodological emphasis on pattern-based file access in computational research, particularly in workflows requiring scalable and reproducible approaches.

Method: The paper incorporates practical Python examples using libraries like pandas, scikit-learn, and matplotlib to illustrate file ingestion, analysis, and integration into analytical pipelines via the glob module.

Result: Glob is showcased as a versatile tool facilitating tasks like large-scale data ingestion, AI dataset construction, organizational data analysis, and enhancing reproducible research practices.

Conclusion: Glob is positioned as an essential and methodological building block for Python-based research workflows, bridging foundational concepts and applied practice.

Abstract: Pattern-based file access is a fundamental but often under-documented aspect
of computational research. The Python glob module provides a simple yet
powerful way to search, filter, and ingest files using wildcard patterns,
enabling scalable workflows across disciplines. This paper introduces glob as a
versatile tool for data science, business analytics, and artificial
intelligence applications. We demonstrate use cases including large-scale data
ingestion, organizational data analysis, AI dataset construction, and
reproducible research practices. Through concrete Python examples with widely
used libraries such as pandas,scikit-learn, and matplotlib, we show how glob
facilitates efficient file traversal and integration with analytical pipelines.
By situating glob within the broader context of reproducible research and data
engineering, we highlight its role as a methodological building block. Our goal
is to provide researchers and practitioners with a concise reference that
bridges foundational concepts and applied practice, making glob a default
citation for file pattern matching in Python-based research workflows.

</details>


### [229] [A Systematic Mapping Study on Chatbots in Programming Education](https://arxiv.org/abs/2509.08857)
*Marcelino Garcia,Renato Garcia,Arthur Parizotto,Andre Mendes,Pedro Valle,Ricardo Vilela,Renato Balancieri,Williamson Silva*

Main category: cs.SE

TL;DR: This paper reviews 54 studies on educational chatbots in programming education, finding trends in Python-focused tools and offering insights for future developments.


<details>
  <summary>Details</summary>
Motivation: To understand how educational chatbots are developed and used in programming education, and to identify trends and gaps for informed tool creation.

Method: Conducted a Systematic Mapping Study, selecting and analyzing 54 studies out of 3,216 initial publications based on predefined research subquestions.

Result: Predominantly Python-oriented chatbots focused on fundamental programming concepts were identified, leveraging diverse pedagogical approaches and technological architectures.

Conclusion: The study highlights current trends and literature gaps, providing guidance for the advancement of programming educational tools.

Abstract: Educational chatbots have gained prominence as support tools for teaching
programming, particularly in introductory learning contexts. This paper
presents a Systematic Mapping Study (SMS) that investigated how such agents
have been developed and applied in programming education. From an initial set
of 3,216 publications, 54 studies were selected and analyzed based on five
research subquestions, addressing chatbot types, programming languages used,
educational content covered, interaction models, and application contexts. The
results reveal a predominance of chatbots designed for Python instruction,
focusing on fundamental programming concepts, and employing a wide variety of
pedagogical approaches and technological architectures. In addition to
identifying trends and gaps in the literature, this study provides insights to
inform the development of new educational tools for programming instruction.

</details>


### [230] [GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial Analysis-Function Calling vs Code Generation](https://arxiv.org/abs/2509.08863)
*Qianqian Luo,Liuchang Xu,Qingming Lin,Sensen Wu,Ruichen Mao,Chao Wang,Hailin Feng,Bo Huang,Zhenhong Du*

Main category: cs.SE

TL;DR: The paper introduces GeoJSON Agents, a multi-agent LLM framework for GIS automation by transforming natural language tasks into GeoJSON commands and employing Function Calling and Code Generation techniques.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of LLMs in GIS tasks and enhance their performance and scalability in spatial data automation.

Method: The framework comprises three main components—task parsing, agent collaboration, and result integration. It utilizes specialized agents for interpreting, processing, and generating GeoJSON files leveraging Function Calling and Code Generation approaches.

Result: The Function Calling-based agent obtained an accuracy of 85.71%, while the Code Generation-based agent achieved 97.14%, both outperforming conventional models dramatically.

Conclusion: The study demonstrates the effectiveness of a multi-agent LLM system in GIS tasks and reveals the advantages and limitations of Function Calling and Code Generation methods, driving advancements in GeoAI systems.

Abstract: LLMs have made substantial progress in task automation and natural language
understanding.However,without expertise in GIS,they continue to encounter
limitations.To address these issues, we propose GeoJSON Agents-a multi-agent
LLM architecture.This framework transforms natural language tasks into
structured GeoJSON operation commands and processes spatial data using two
widely adopted LLM enhancement techniques:Function Calling and Code
Generation.The architecture consists of three components-task parsing,agent
collaboration,and result integration-aimed at enhancing both the performance
and scalability of GIS automation.The Planner agent interprets natural language
tasks into structured GeoJSON commands.Then,specialized Worker agents
collaborate according to assigned roles to perform spatial data processing and
analysis,either by invoking predefined function APIs or by dynamically
generating and executing Python-based spatial analysis code.Finally,the system
integrates the outputs from multiple execution rounds into
reusable,standards-compliant GeoJSON files.To systematically evaluate the
performance of the two approaches,we constructed a benchmark dataset of 70
tasks with varying complexity and conducted experiments using OpenAI's GPT-4o
as the core model.Results indicate that the Function Calling-based GeoJSON
Agent achieved an accuracy of 85.71%,while the Code Generation-based agent
reached 97.14%,both significantly outperforming the best-performing
general-purpose model (48.57%).Further analysis reveals that the Code
Generation provides greater flexibility,whereas the Function Calling approach
offers more stable execution.This study is the first to introduce an LLM
multi-agent framework for GeoJSON data and to compare the strengths and
limitations of two mainstream LLM enhancement methods,offering new perspectives
for improving GeoAI system performance.

</details>


### [231] [TraceRAG: A LLM-Based Framework for Explainable Android Malware Detection and Behavior Analysis](https://arxiv.org/abs/2509.08865)
*Guangyu Zhang,Xixuan Wang,Shiyu Sun,Peiyan Xiao,Kun Sun,Yanhai Xiong*

Main category: cs.SE

TL;DR: TraceRAG introduces a retrieval-augmented generation framework to detect and analyze malicious Android apps by connecting natural language queries with Java code, achieving high accuracy in malware detection and behavior identification.


<details>
  <summary>Details</summary>
Motivation: Traditional analysis methods fail to uncover deeply concealed malicious behavior in Android apps or provide comprehensible explanations, necessitating advanced frameworks for explainable and in-depth analysis.

Method: TraceRAG leverages large language models by generating and indexing summaries of code snippets, retrieving relevant ones during queries, and producing clear reports of malicious behaviors and their code implementations.

Result: TraceRAG achieved 96% malware detection accuracy and 83.81% behavior identification accuracy, validated by VirusTotal scans and manual checks. Expert evaluations affirmed its utility.

Conclusion: The study demonstrates that TraceRAG is an effective and explainable malware analysis tool, linking code semantics with natural language to address the limitations of traditional methods.

Abstract: Sophisticated evasion tactics in malicious Android applications, combined
with their intricate behavioral semantics, enable attackers to conceal
malicious logic within legitimate functions, underscoring the critical need for
robust and in-depth analysis frameworks. However, traditional analysis
techniques often fail to recover deeply hidden behaviors or provide
human-readable justifications for their decisions. Inspired by advances in
large language models (LLMs), we introduce TraceRAG, a retrieval-augmented
generation (RAG) framework that bridges natural language queries and Java code
to deliver explainable malware detection and analysis. First, TraceRAG
generates summaries of method-level code snippets, which are indexed in a
vector database. At query time, behavior-focused questions retrieve the most
semantically relevant snippets for deeper inspection. Finally, based on the
multi-turn analysis results, TraceRAG produces human-readable reports that
present the identified malicious behaviors and their corresponding code
implementations. Experimental results demonstrate that our method achieves 96\%
malware detection accuracy and 83.81\% behavior identification accuracy based
on updated VirusTotal (VT) scans and manual verification. Furthermore, expert
evaluation confirms the practical utility of the reports generated by TraceRAG.

</details>


### [232] [Benchmarking Energy Efficiency of Large Language Models Using vLLM](https://arxiv.org/abs/2509.08867)
*K. Pronk,Q. Zhao*

Main category: cs.SE

TL;DR: The paper proposes a benchmark called 'LLM Efficiency Benchmark' that better simulates real-world conditions to evaluate the energy efficiency of large language models (LLMs) during inference.


<details>
  <summary>Details</summary>
Motivation: Growing concern about the climate impact of deploying energy-intensive LLMs warrants better tools for assessing their energy efficiency in realistic production scenarios.

Method: The authors developed the 'LLM Efficiency Benchmark' using vLLM, a production-ready LLM serving backend, and tested it under varied conditions like model size, architecture, and concurrent request volumes.

Result: The study found that benchmarks mirroring realistic deployment scenarios are effective in assessing and improving the energy efficiency of LLMs.

Conclusion: The research provides developers with valuable insights for creating more sustainable AI systems, emphasizing the need for production-simulated benchmarks in optimizing LLM energy efficiency.

Abstract: The prevalence of Large Language Models (LLMs) is having an growing impact on
the climate due to the substantial energy required for their deployment and
use. To create awareness for developers who are implementing LLMs in their
products, there is a strong need to collect more information about the energy
efficiency of LLMs. While existing research has evaluated the energy efficiency
of various models, these benchmarks often fall short of representing realistic
production scenarios. In this paper, we introduce the LLM Efficiency Benchmark,
designed to simulate real-world usage conditions. Our benchmark utilizes vLLM,
a high-throughput, production-ready LLM serving backend that optimizes model
performance and efficiency. We examine how factors such as model size,
architecture, and concurrent request volume affect inference energy efficiency.
Our findings demonstrate that it is possible to create energy efficiency
benchmarks that better reflect practical deployment conditions, providing
valuable insights for developers aiming to build more sustainable AI systems.

</details>


### [233] [CLARA: A Developer's Companion for Code Comprehension and Analysis](https://arxiv.org/abs/2509.09072)
*Ahmed Adnan,Mushfiqur Rahman,Saad Sakib Noor,Kazi Sakib*

Main category: cs.SE

TL;DR: This paper introduces CLARA, a browser extension assisting in code comprehension, refactoring, and quality assessments using a state-of-the-art inference model.


<details>
  <summary>Details</summary>
Motivation: Existing tools for code comprehension and analysis often require setup, lack context-awareness, and demand manual effort, creating a need for more efficient, automated solutions.

Method: The authors developed CLARA, an open-source browser extension using a state-of-the-art inference model, and qualitatively evaluated its performance through datasets, established methodology, and a user study involving 10 developers and researchers.

Result: The study found CLARA to be useful, accurate, and practical for tasks such as code comprehension, refactoring, and quality attribute detection.

Conclusion: CLARA effectively addresses inefficiencies in code comprehension and analysis tasks, demonstrating its value as a practical and accurate tool for developers and researchers.

Abstract: Code comprehension and analysis of open-source project codebases is a task
frequently performed by developers and researchers. However, existing tools
that practitioners use for assistance with such tasks often require prior
project setup, lack context-awareness, and involve significant manual effort.
To address this, we present CLARA, a browser extension that utilizes a
state-of-the-art inference model to assist developers and researchers in: (i)
comprehending code files and code fragments, (ii) code refactoring, and (iii)
code quality attribute detection. We qualitatively evaluated CLARA's inference
model using existing datasets and methodology, and performed a comprehensive
user study with 10 developers and academic researchers to assess its usability
and usefulness. The results show that CLARA is useful, accurate, and practical
in code comprehension and analysis tasks. CLARA is an open-source tool
available at https://github.com/SaadNoor555/CLARA_tool_demo. A video showing
the full capabilities of CLARA can be found at
https://youtu.be/VDKVXvIH41Q?si=qBFsmS_Y4m_9x3YH.

</details>


### [234] [Probing Pre-trained Language Models on Code Changes: Insights from ReDef, a High-Confidence Just-in-Time Defect Prediction Dataset](https://arxiv.org/abs/2509.09192)
*Doha Nam,Taehyoun Kim,Duksan Ryu,Jongmoon Baik*

Main category: cs.SE

TL;DR: The paper introduces ReDef, a new benchmark dataset for defect prediction in code modifications, and evaluates how pre-trained language models handle code modification reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for Just-in-Time software defect prediction lack reliability due to noisy labels and low precision in identifying bug-inducing commits.

Method: The authors curated a high-confidence defect dataset using revert commits and post-hoc history checks for defect validation, paired with GPT-assisted filtering for ambiguous cases. They then evaluated various pre-trained language models using five encoding strategies and counterfactual perturbation tests.

Result: Compact diff-style encodings consistently performed better than whole-function formats, revealing that pre-trained language models rely on superficial cues rather than genuinely comprehending edit semantics.

Conclusion: Current pre-trained language models are limited in their understanding of code modifications, highlighting the need for improvements beyond reliance on shallow patterns.

Abstract: Just-in-Time software defect prediction (JIT-SDP) plays a critical role in
prioritizing risky code changes during code review and continuous integration.
However, existing datasets often suffer from noisy labels and low precision in
identifying bug-inducing commits. To address this, we present ReDef
(Revert-based Defect dataset), a high-confidence benchmark of function-level
modifications curated from 22 large-scale C/C++ projects. Defective cases are
anchored by revert commits, while clean cases are validated through post-hoc
history checks. Ambiguous instances are conservatively filtered out via a
GPT-assisted triage process involving multiple votes and audits. This pipeline
yields 3,164 defective and 10,268 clean modifications, offering substantially
more reliable labels than prior existing resources. Beyond dataset
construction, we provide the first systematic evaluation of how pre-trained
language models (PLMs) reason about code modifications -- specifically, which
input encodings most effectively expose change information, and whether models
genuinely capture edit semantics. We fine-tune CodeBERT, CodeT5+, and UniXcoder
under five encoding strategies, and further probe their sensitivity through
counterfactual perturbations that swap added/deleted blocks, invert diff
polarity, or inject spurious markers. Our results show that compact diff-style
encodings consistently outperform whole-function formats across all PLMs, with
statistical tests confirming large, model-independent effects. However, under
counterfactual tests, performance degrades little or not at all -- revealing
that what appears to be robustness in fact reflects reliance on superficial
cues rather than true semantic understanding. These findings indicate that,
unlike in snapshot-based tasks, current PLMs remain limited in their ability to
genuinely comprehend code modifications.

</details>


### [235] [On Integrating Large Language Models and Scenario-Based Programming for Improving Software Reliability](https://arxiv.org/abs/2509.09194)
*Ayelet Berzack,Guy Katz*

Main category: cs.SE

TL;DR: This paper proposes a methodology for integrating large language models (LLMs) with traditional software engineering techniques, specifically focusing on the Scenario-Based Programming (SBP) paradigm, to improve reliability, error reduction, and verification in software development. The proposed method is demonstrated through a case study involving the development of a Connect4 game.


<details>
  <summary>Details</summary>
Motivation: While LLMs can assist in tasks like generating comprehensible code and suggesting creative ideas, they also risk introducing errors and generating misleadingly convincing flawed solutions. The motivation behind the paper is to create a structured approach that brings LLMs more reliably into the software development cycle.

Method: The authors propose a methodology that combines LLMs with Scenario-Based Programming (SBP), an event-driven software engineering paradigm. This allows developers to leverage their expertise to guide LLMs and verify their outputs. The approach is validated via a case study implementation of the Connect4 game.

Result: Using the proposed methodology to develop the Connect4 game, the authors created a highly-capable agent that could defeat other strong agents and, in some cases, achieved formal verification of its correctness. They also gained insights into the ease-of-use of the approach.

Conclusion: The integration of LLMs with SBP helps bridge the gap between innovative AI assistance and reliable, verifiable software development. The methodology enhances error reduction, and the case study demonstrates its practical effectiveness for developing high-quality software.

Abstract: Large Language Models (LLMs) are fast becoming indispensable tools for
software developers, assisting or even partnering with them in crafting complex
programs. The advantages are evident -- LLMs can significantly reduce
development time, generate well-organized and comprehensible code, and
occasionally suggest innovative ideas that developers might not conceive on
their own. However, despite their strengths, LLMs will often introduce
significant errors and present incorrect code with persuasive confidence,
potentially misleading developers into accepting flawed solutions.
  In order to bring LLMs into the software development cycle in a more reliable
manner, we propose a methodology for combining them with ``traditional''
software engineering techniques in a structured way, with the goal of
streamlining the development process, reducing errors, and enabling users to
verify crucial program properties with increased confidence. Specifically, we
focus on the Scenario-Based Programming (SBP) paradigm -- an event-driven,
scenario-based approach for software engineering -- to allow human developers
to pour their expert knowledge into the LLM, as well as to inspect and verify
its outputs.
  To evaluate our methodology, we conducted a significant case study, and used
it to design and implement the Connect4 game. By combining LLMs and SBP we were
able to create a highly-capable agent, which could defeat various strong
existing agents. Further, in some cases, we were able to formally verify the
correctness of our agent. Finally, our experience reveals interesting insights
regarding the ease-of-use of our proposed approach. The full code of our
case-study will be made publicly available with the final version of this
paper.

</details>


### [236] [Altered Histories in Version Control System Repositories: Evidence from the Trenches](https://arxiv.org/abs/2509.09294)
*Solal Rapaport,Laurent Pautet,Samuel Tardieu,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: This paper investigates Git history alterations in public repositories, analyzing 111 million repositories and finding 8.7 million rewritten histories. It categorizes the changes and highlights problematic practices, introducing an automated tool, GitHistorian, for detecting such alterations.


<details>
  <summary>Details</summary>
Motivation: To analyze and understand the extent and implications of Git history alterations in public repositories, which can disrupt workflows, challenge repository integrity, and allow for security vulnerabilities.

Method: The authors analyzed 111 million public code repositories archived by Software Heritage, identifying and categorizing rewritten histories based on location, type of alteration, and purpose.

Result: The analysis revealed 1.22 million repositories with history alterations, involving 8.7 million rewritten histories. Case studies highlighted issues like retroactive license changes and removal of sensitive information, which represent poor governance or security practices.

Conclusion: Git history alterations are pervasive and often problematic. Tools like GitHistorian can help identify and describe these alterations, supporting better project governance and security management.

Abstract: Version Control Systems (VCS) like Git allow developers to locally rewrite
recorded history, e.g., to reorder and suppress commits or specific data in
them. These alterations have legitimate use cases, but become problematic when
performed on public branches that have downstream users: they break push/pull
workflows, challenge the integrity and reproducibility of repositories, and
create opportunities for supply chain attackers to sneak into them nefarious
changes. We conduct the first large-scale investigation of Git history
alterations in public code repositories. We analyze 111 M (millions)
repositories archived by Software Heritage, which preserves VCS histories even
across alterations. We find history alterations in 1.22 M repositories, for a
total of 8.7 M rewritten histories. We categorize changes by where they happen
(which repositories, which branches) and what is changed in them (files or
commit metadata). Conducting two targeted case studies we show that altered
histories recurrently change licenses retroactively, or are used to remove
''secrets'' (e.g., private keys) committed by mistake. As these behaviors
correspond to bad practices-in terms of project governance or security
management, respectively-that software recipients might want to avoid, we
introduce GitHistorian, an automated tool, that developers can use to spot and
describe history alterations in public Git repositories.

</details>


### [237] [Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on Open & Industry Data](https://arxiv.org/abs/2509.09313)
*Moritz Mock,Thomas Forrer,Barbara Russo*

Main category: cs.SE

TL;DR: The paper evaluates CodeBERT for vulnerability detection and develops AI-DO, a CI/CD-integrated tool, to address challenges in industrial code review processes.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of transferring deep learning solutions for vulnerability detection from academia to industry, focusing on trustworthiness, legacy systems, and workflow integration.

Method: Evaluated CodeBERT's cross-domain performance for vulnerability detection, employed fine-tuning with open-source and industrial data, and developed AI-DO for CI/CD integration. Conducted a survey with IT professionals.

Result: Models trained on industrial data perform well within the same domain but not on open-source code, while fine-tuning with undersampling of open-source data improves detection accuracy.

Conclusion: Training data domain significantly impacts performance, and integrating tools like AI-DO into workflows enables practical vulnerability detection and localization in industrial contexts.

Abstract: Deep learning solutions for vulnerability detection proposed in academic
research are not always accessible to developers, and their applicability in
industrial settings is rarely addressed. Transferring such technologies from
academia to industry presents challenges related to trustworthiness, legacy
systems, limited digital literacy, and the gap between academic and industrial
expertise. For deep learning in particular, performance and integration into
existing workflows are additional concerns. In this work, we first evaluate the
performance of CodeBERT for detecting vulnerable functions in industrial and
open-source software. We analyse its cross-domain generalisation when
fine-tuned on open-source data and tested on industrial data, and vice versa,
also exploring strategies for handling class imbalance. Based on these results,
we develop AI-DO(Automating vulnerability detection Integration for Developers'
Operations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated
recommender system that uses fine-tuned CodeBERT to detect and localise
vulnerabilities during code review without disrupting workflows. Finally, we
assess the tool's perceived usefulness through a survey with the company's IT
professionals. Our results show that models trained on industrial data detect
vulnerabilities accurately within the same domain but lose performance on
open-source code, while a deep learner fine-tuned on open data, with
appropriate undersampling techniques, improves the detection of
vulnerabilities.

</details>


### [238] [ORCA: Unveiling Obscure Containers In The Wild](https://arxiv.org/abs/2509.09322)
*Jacopo Bufalino,Agathe Blaise,Stefano Secci*

Main category: cs.SE

TL;DR: This paper highlights the security risks in containerized environments due to outdated or modified components and proposes ORCA, a tool to improve file analysis in obscure containers.


<details>
  <summary>Details</summary>
Motivation: To address the security risks posed by outdated or unintentionally modified components in containerized environments, which can compromise Software Composition Analysis (SCA) tools.

Method: The authors analyzed 600 popular containers to assess the limitations of cloud-based and open-source SCA tools. They developed ORCA, an obscuration-resilient methodology and open-source implementation for container analysis.

Result: The study revealed the presence of obscure containers in well-known registries and showed that many SCA tools fail to analyze these. ORCA achieved a median 40% improvement in file coverage compared to Docker Scout and Syft.

Conclusion: ORCA provides an effective solution for detecting content in obscure containers, enhancing the reliability of SCA tools and mitigating security risks in containerized environments.

Abstract: Modern software development increasingly depends on open-source libraries and
third-party components, which are often encapsulated into containerized
environments. While improving the development and deployment of applications,
this approach introduces security risks, particularly when outdated or
vulnerable components are inadvertently included in production environments.
Software Composition Analysis (SCA) is a critical process that helps identify
and manage packages and dependencies inside a container. However, unintentional
modifications to the container filesystem can lead to incomplete container
images, which compromise the reliability of SCA tools. In this paper, we
examine the limitations of both cloud-based and open-source SCA tools when
faced with such obscure images. An analysis of 600 popular containers revealed
that obscure containers exist in well-known registries and trusted images and
that many tools fail to analyze such containers. To mitigate these issues, we
propose an obscuration-resilient methodology for container analysis and
introduce ORCA (Obscuration-Resilient Container Analyzer), its open-source
implementation. We reported our findings to all vendors using their appropriate
channels. Our results demonstrate that ORCA effectively detects the content of
obscure containers and achieves a median 40% improvement in file coverage
compared to Docker Scout and Syft.

</details>


### [239] [LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering](https://arxiv.org/abs/2509.09614)
*Jielin Qiu,Zuxin Liu,Zhiwei Liu,Rithesh Murthy,Jianguo Zhang,Haolin Chen,Shiyu Wang,Ming Zhu,Liangwei Yang,Juntao Tan,Zhepeng Cen,Cheng Qian,Shelby Heinecke,Weiran Yao,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.SE

TL;DR: LoCoBench introduces a benchmark to evaluate the performance of long-context language models in managing complex software development scenarios, emphasizing the unique challenges of understanding extensive codebases.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks inadequately address the evaluation needs for long-context capabilities in software scenarios, where reasoning across large-scale systems is crucial.

Method: LoCoBench generates 8,000 scenarios across 10 programming languages with context lengths between 10K and 1M tokens, assessing 8 task categories. It employs a 5-phase pipeline and introduces 17 metrics combined into the LoCoBench Score.

Result: State-of-the-art models exhibit significant performance gaps, highlighting the challenge of developing long-context understanding in extensive software systems.

Conclusion: LoCoBench fills a critical gap in benchmarking for long-context models, urging further innovation in this challenging area of AI development.

Abstract: The emergence of long-context language models with context windows extending
to millions of tokens has created new opportunities for sophisticated code
understanding and software development evaluation. We propose LoCoBench, a
comprehensive benchmark specifically designed to evaluate long-context LLMs in
realistic, complex software development scenarios. Unlike existing code
evaluation benchmarks that focus on single-function completion or short-context
tasks, LoCoBench addresses the critical evaluation gap for long-context
capabilities that require understanding entire codebases, reasoning across
multiple files, and maintaining architectural consistency across large-scale
software systems. Our benchmark provides 8,000 evaluation scenarios
systematically generated across 10 programming languages, with context lengths
spanning 10K to 1M tokens, a 100x variation that enables precise assessment of
long-context performance degradation in realistic software development
settings. LoCoBench introduces 8 task categories that capture essential
long-context capabilities: architectural understanding, cross-file refactoring,
multi-session development, bug investigation, feature implementation, code
comprehension, integration testing, and security analysis. Through a 5-phase
pipeline, we create diverse, high-quality scenarios that challenge LLMs to
reason about complex codebases at unprecedented scale. We introduce a
comprehensive evaluation framework with 17 metrics across 4 dimensions,
including 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our
evaluation of state-of-the-art long-context models reveals substantial
performance gaps, demonstrating that long-context understanding in complex
software development represents a significant unsolved challenge that demands
more attention. LoCoBench is released at:
https://github.com/SalesforceAIResearch/LoCoBench.

</details>


### [240] [I Know Who Clones Your Code: Interpretable Smart Contract Similarity Detection](https://arxiv.org/abs/2509.09630)
*Zhenguang Liu,Lixun Ma,Zhongzheng Mu,Chengkun Wei,Xiaojun Xu,Yingying Jiao,Kui Ren*

Main category: cs.SE

TL;DR: SmartDetector is introduced to improve smart contract similarity detection by focusing on fine-grained statement-level analysis, significantly outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of bug propagation due to open-source reuse in smart contracts and the limitations of AST-based and deep-learning approaches in code similarity detection.

Method: SmartDetector decomposes the AST of smart contract functions into smaller statement trees and uses a classifier for similarity scoring. A cosine-wise diffusion process minimizes hyperparameter space.

Result: SmartDetector achieves an average F1-score of 95.88%, outperforming existing methods by 14.01% on three large real-world datasets.

Conclusion: SmartDetector effectively provides interpretable, fine-grained similarity detection for smart contract functions, improving upon both conventional and deep-learning methods.

Abstract: Widespread reuse of open-source code in smart contract development boosts
programming efficiency but significantly amplifies bug propagation across
contracts, while dedicated methods for detecting similar smart contract
functions remain very limited. Conventional abstract-syntax-tree (AST) based
methods for smart contract similarity detection face challenges in handling
intricate tree structures, which impedes detailed semantic comparison of code.
Recent deep-learning based approaches tend to overlook code syntax and
detection interpretability, resulting in suboptimal performance.
  To fill this research gap, we introduce SmartDetector, a novel approach for
computing similarity between smart contract functions, explainable at the
fine-grained statement level. Technically, SmartDetector decomposes the AST of
a smart contract function into a series of smaller statement trees, each
reflecting a structural element of the source code. Then, SmartDetector uses a
classifier to compute the similarity score of two functions by comparing each
pair of their statement trees. To address the infinite hyperparameter space of
the classifier, we mathematically derive a cosine-wise diffusion process to
efficiently search optimal hyperparameters. Extensive experiments conducted on
three large real-world datasets demonstrate that SmartDetector outperforms
current state-of-the-art methods by an average improvement of 14.01% in
F1-score, achieving an overall average F1-score of 95.88%.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [241] [Path to Intelligence: Measuring Similarity between Human Brain and Large Language Model Beyond Language Task](https://arxiv.org/abs/2509.08831)
*Doai Ngo,Mingxuan Sun,Zhengji Zhang,Ashwin G Ramayya,Mark Schnitzer,Zhe Zhao*

Main category: q-bio.NC

TL;DR: This paper compares the internal states of large language models (LLMs) with human brain activity during a sensory-motor task, revealing similarities beyond language-based tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to investigate whether the similarity between LLMs and human brain activity extends to non-language tasks, and to explore if LLMs can aid in understanding human brain functions.

Method: The authors translated a sensory-motor task into natural language for LLMs, extracted their hidden states, and compared them with intracranial EEG signals of humans performing the same task.

Result: The study found that LLM-derived reactions can be linearly mapped to human neural activity, demonstrating the resemblance between LLMs and human brain behavior in sensory tasks.

Conclusion: LLMs show potential in approximating human neurophysical behavior, enabling neuroscience studies of complex phenomena and deepening our understanding of human intelligence beyond language.

Abstract: Large language models (LLMs) have demonstrated human-like abilities in
language-based tasks. While language is a defining feature of human
intelligence, it emerges from more fundamental neurophysical processes rather
than constituting the basis of intelligence itself. In this work, we study the
similarity between LLM internal states and human brain activity in a
sensory-motor task rooted in anticipatory and visuospatial behavior. These
abilities are essential for cognitive performance that constitute human
intelligence. We translate the sensory-motor task into natural language in
order to replicate the process for LLMs. We extract hidden states from
pre-trained LLMs at key time steps and compare them to human intracranial EEG
signals. Our results reveal that LLM-derived reactions can be linearly mapped
onto human neural activity. These findings suggest that LLMs, with a simple
natural language translation to make them understand temporal-relevant tasks,
can approximate human neurophysical behavior in experiments involving sensory
stimulants. In all, our contribution is two-fold: (1) We demonstrate similarity
between LLM and human brain activity beyond language-based tasks. (2) We
demonstrate that with such similarity, LLMs could help us understand human
brains by enabling us to study topics in neuroscience that are otherwise
challenging to tackle.

</details>


### [242] [A novel cost-effective fabrication of a flexible neural probe for brain signal recording](https://arxiv.org/abs/2509.09213)
*Alireza Irandoost,Amirreza Bahramani,Roya Mohajeri,Faezeh Shahdost-Fard,Ali Ghazizadeh,Mehdi Fardmanesh*

Main category: q-bio.NC

TL;DR: This paper presents a novel neural probe made from a cost-effective polyimide film, demonstrating biocompatibility and effective neural signal recording.


<details>
  <summary>Details</summary>
Motivation: The paper aims to develop a biocompatible and cost-effective implantable neural probe for high-quality neural signal acquisition.

Method: The probe was fabricated using Kapton polyimide film and gold as conductive materials, with electrochemical testing and neural signal recording performed for characterization.

Result: Neural signals were successfully recorded from zebra finch auditory responses using the fabricated probe, which exhibited low impedance properties comparable to a commercial electrode.

Conclusion: The study highlights the probe's potential as a biocompatible device for neural applications and suggests its viability for future commercial neural implants.

Abstract: This study introduces a novel, flexible, and implantable neural probe using a
cost-effective microfabrication process based on a thin polyimide film.
Polyimide film, known as Kapton, serves as a flexible substrate for
microelectrodes, conductive tracks, and contact pads of the probe, which are
made from a thin film of gold (Au). SU-8 is used to cover the corresponding
tracks for electrical isolation and to increase the stiffness of the probe for
better implantation. To evaluate the performance of the fabricated probe,
electrochemical impedance spectroscopy (EIS) and artificial neural signal
recording have been used to characterize its properties. The microelectrode
dimensions have been carefully chosen to provide low impedance characteristics,
which are necessary for acquiring local field potential (LFP) signals. The in
vivo LFP data have been obtained from a male zebra finch presented with
auditory stimuli. By properly filtering the extracellular recordings and
analyzing the data, the obtained results have been validated by comparing them
with the signals acquired with a commercial neural electrode. Due to the use of
Kapton, SU-8, and Au materials with non-toxic and adaptable properties in the
body environment, the fabricated neural probe is considered a promising
biocompatible implantable neural probe that may pave the way for the
fabrication of other neural implantable devices with commercial aims.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [243] [Scalable extensions to given-data Sobol' index estimators](https://arxiv.org/abs/2509.09078)
*Teresa Portone,Bert Debusschere,Samantha Yang,Emiliano Islas-Quinones,T. Patrick Xiao*

Main category: stat.ML

TL;DR: This paper presents extensions to existing Sobol' index methods, allowing for efficient sensitivity analysis of large models with many inputs.


<details>
  <summary>Details</summary>
Motivation: Current methods for Sobol' index computation are limited for models with extremely large numbers of inputs due to memory and distribution issues.

Method: The authors propose a generalized Sobol' index framework with arbitrary partitions, a memory-efficient streaming algorithm, and a heuristic for filtering statistically insignificant indices.

Result: Their approach reduces memory requirements while maintaining comparable accuracy and runtime, and addresses biases from equiprobable partitions in previous methods.

Conclusion: The proposed enhancements make Sobol' index computation feasible for large-scale models like neural networks, enabling efficient sensitivity analysis in these contexts.

Abstract: Given-data methods for variance-based sensitivity analysis have significantly
advanced the feasibility of Sobol' index computation for computationally
expensive models and models with many inputs. However, the limitations of
existing methods still preclude their application to models with an extremely
large number of inputs. In this work, we present practical extensions to the
existing given-data Sobol' index method, which allow variance-based sensitivity
analysis to be efficiently performed on large models such as neural networks,
which have $>10^4$ parameterizable inputs. For models of this size, holding all
input-output evaluations simultaneously in memory -- as required by existing
methods -- can quickly become impractical. These extensions also support
nonstandard input distributions with many repeated values, which are not
amenable to equiprobable partitions employed by existing given-data methods.
  Our extensions include a general definition of the given-data Sobol' index
estimator with arbitrary partition, a streaming algorithm to process
input-output samples in batches, and a heuristic to filter out small indices
that are indistinguishable from zero indices due to statistical noise. We show
that the equiprobable partition employed in existing given-data methods can
introduce significant bias into Sobol' index estimates even at large sample
sizes and provide numerical analyses that demonstrate why this can occur. We
also show that our streaming algorithm can achieve comparable accuracy and
runtimes with lower memory requirements, relative to current methods which
process all samples at once. We demonstrate our novel developments on two
application problems in neural network modeling.

</details>


### [244] [Global Optimization of Stochastic Black-Box Functions with Arbitrary Noise Distributions using Wilson Score Kernel Density Estimation](https://arxiv.org/abs/2509.09238)
*Thorbjørn Mosekjær Iversen,Lars Carøe Sørensen,Simon Faarvang Mathiesen,Henrik Gordon Petersen*

Main category: stat.ML

TL;DR: The paper introduces the Wilson Score Kernel Density Estimator (WS-KDE) as a novel method for confident bounds on stochastic functions, enhancing Bayesian optimization applications in robotics.


<details>
  <summary>Details</summary>
Motivation: To address challenges in optimization of time-expensive, stochastic black-box functions in robotics, which often require efficient and reliable confidence estimates.

Method: The paper proposes WS-KDE, leveraging its statistical properties to estimate confidence bounds for stochastic functions within the [0,1] range, independent of output distribution.

Result: The authors demonstrate the effectiveness of WS-KDE in simulations and in a real-world application of designing vibrational part feeders, showing robust confidence bounds and optimization.

Conclusion: WS-KDE enhances Bayesian optimization by offering reliable and applicable confidence bounds, expanding the usability for more stochastic cost functions and practical problems.

Abstract: Many optimization problems in robotics involve the optimization of
time-expensive black-box functions, such as those involving complex simulations
or evaluation of real-world experiments. Furthermore, these functions are often
stochastic as repeated experiments are subject to unmeasurable disturbances.
Bayesian optimization can be used to optimize such methods in an efficient
manner by deploying a probabilistic function estimator to estimate with a given
confidence so that regions of the search space can be pruned away.
Consequently, the success of the Bayesian optimization depends on the function
estimator's ability to provide informative confidence bounds. Existing function
estimators require many function evaluations to infer the underlying confidence
or depend on modeling of the disturbances. In this paper, it is shown that the
confidence bounds provided by the Wilson Score Kernel Density Estimator
(WS-KDE) are applicable as excellent bounds to any stochastic function with an
output confined to the closed interval [0;1] regardless of the distribution of
the output. This finding opens up the use of WS-KDE for stable global
optimization on a wider range of cost functions. The properties of WS-KDE in
the context of Bayesian optimization are demonstrated in simulation and applied
to the problem of automated trap design for vibrational part feeders.

</details>


### [245] [Low-degree lower bounds via almost orthonormal bases](https://arxiv.org/abs/2509.09353)
*Alexandra Carpentier,Simone Maria Giancola,Christophe Giraud,Nicolas Verzelen*

Main category: stat.ML

TL;DR: This paper develops a new proof strategy using almost orthonormal polynomials for tackling statistical-computational gaps in high-dimensional models, specifically for random graph problems.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in existing approaches that rely on fully orthonormal polynomial families, which break down in estimation tasks or complex problems where simple orthogonal polynomials are unavailable.

Method: The paper constructs an almost orthonormal polynomial basis under a planted distribution regime and applies this to random graph models to establish low-degree lower bounds.

Result: The approach recovers known low-degree lower bounds and discovers new bounds for problems like hidden subcliques, stochastic block models, and seriation models.

Conclusion: This new methodology not only provides a direct way to establish computational lower bounds but also optimizes the low-degree criterion and informs the design of optimal algorithms.

Abstract: Low-degree polynomials have emerged as a powerful paradigm for providing
evidence of statistical-computational gaps across a variety of high-dimensional
statistical models [Wein25]. For detection problems -- where the goal is to
test a planted distribution $\mathbb{P}'$ against a null distribution
$\mathbb{P}$ with independent components -- the standard approach is to bound
the advantage using an $\mathbb{L}^2(\mathbb{P})$-orthonormal family of
polynomials. However, this method breaks down for estimation tasks or more
complex testing problems where $\mathbb{P}$ has some planted structures, so
that no simple $\mathbb{L}^2(\mathbb{P})$-orthogonal polynomial family is
available. To address this challenge, several technical workarounds have been
proposed [SW22,SW25], though their implementation can be delicate. In this
work, we propose a more direct proof strategy. Focusing on random graph models,
we construct a basis of polynomials that is almost orthonormal under
$\mathbb{P}$, in precisely those regimes where statistical-computational gaps
arise. This almost orthonormal basis not only yields a direct route to
establishing low-degree lower bounds, but also allows us to explicitly identify
the polynomials that optimize the low-degree criterion. This, in turn, provides
insights into the design of optimal polynomial-time algorithms. We illustrate
the effectiveness of our approach by recovering known low-degree lower bounds,
and establishing new ones for problems such as hidden subcliques, stochastic
block models, and seriation models.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [246] [Bona fide Cross Testing Reveals Weak Spot in Audio Deepfake Detection Systems](https://arxiv.org/abs/2509.09204)
*Chin Yuen Kwok,Jia Qi Yip,Zhen Qiu,Chi Hung Chi,Kwok Yan Lam*

Main category: cs.SD

TL;DR: The paper critiques traditional evaluation methods for audio deepfake detection (ADD) and introduces a new framework for robust and balanced assessments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address biases in commonly used Equal Error Rate (EER) evaluation metrics, which disproportionately favor certain synthesizers and lack diversity in bona fide speech datasets.

Method: The authors propose 'bona fide cross-testing,' a framework using diverse bona fide datasets and aggregated EERs for more reliable and balanced assessments.

Result: Benchmarking is conducted over 150 synthesizers across nine bona fide speech types, revealing improved robustness and interpretability compared to traditional methods.

Conclusion: The proposed framework enhances the reliability of audio deepfake detection evaluations, and a new dataset is released to support further research.

Abstract: Audio deepfake detection (ADD) models are commonly evaluated using datasets
that combine multiple synthesizers, with performance reported as a single Equal
Error Rate (EER). However, this approach disproportionately weights
synthesizers with more samples, underrepresenting others and reducing the
overall reliability of EER. Additionally, most ADD datasets lack diversity in
bona fide speech, often featuring a single environment and speech style (e.g.,
clean read speech), limiting their ability to simulate real-world conditions.
To address these challenges, we propose bona fide cross-testing, a novel
evaluation framework that incorporates diverse bona fide datasets and
aggregates EERs for more balanced assessments. Our approach improves robustness
and interpretability compared to traditional evaluation methods. We benchmark
over 150 synthesizers across nine bona fide speech types and release a new
dataset to facilitate further research at
https://github.com/cyaaronk/audio_deepfake_eval.

</details>


### [247] [DiFlow-TTS: Discrete Flow Matching with Factorized Speech Tokens for Low-Latency Zero-Shot Text-To-Speech](https://arxiv.org/abs/2509.09631)
*Ngoc-Son Nguyen,Hieu-Nghia Huynh-Nguyen,Thanh V. T. Tran,Truong-Son Hy,Van Nguyen*

Main category: cs.SD

TL;DR: DiFlow-TTS is introduced as a novel zero-shot text-to-speech model utilizing discrete flow matching for faster and high-quality speech synthesis.


<details>
  <summary>Details</summary>
Motivation: To address the slow inference speed and repetition artifacts of recent zero-shot TTS methods while leveraging discrete generative modeling effectively.

Method: Proposed a discrete flow matching method within a unified architecture that factorizes speech attributes and employs distinct heads for prosody and acoustic details. It leverages in-context learning for textual and reference-based attribute conditioning.

Result: Achieved promising results in naturalness, prosody, speaker style preservation, and energy control while maintaining compactness and generating speech up to 25.8 times faster than existing baselines.

Conclusion: DiFlow-TTS effectively balances quality and performance, showcasing the potential of discrete flow matching in advancing zero-shot TTS synthesis.

Abstract: Zero-shot Text-to-Speech (TTS) aims to synthesize high-quality speech that
mimics the voice of an unseen speaker using only a short reference sample,
requiring not only speaker adaptation but also accurate modeling of prosodic
attributes. Recent approaches based on language models, diffusion, and flow
matching have shown promising results in zero-shot TTS, but still suffer from
slow inference and repetition artifacts. Discrete codec representations have
been widely adopted for speech synthesis, and recent works have begun to
explore diffusion models in purely discrete settings, suggesting the potential
of discrete generative modeling for speech synthesis. However, existing
flow-matching methods typically embed these discrete tokens into a continuous
space and apply continuous flow matching, which may not fully leverage the
advantages of discrete representations. To address these challenges, we
introduce DiFlow-TTS, which, to the best of our knowledge, is the first model
to explore purely Discrete Flow Matching for speech synthesis. DiFlow-TTS
explicitly models factorized speech attributes within a compact and unified
architecture. It leverages in-context learning by conditioning on textual
content, along with prosodic and acoustic attributes extracted from a reference
speech, enabling effective attribute cloning in a zero-shot setting. In
addition, the model employs a factorized flow prediction mechanism with
distinct heads for prosody and acoustic details, allowing it to learn
aspect-specific distributions. Experimental results demonstrate that DiFlow-TTS
achieves promising performance in several key metrics, including naturalness,
prosody, preservation of speaker style, and energy control. It also maintains a
compact model size and achieves low-latency inference, generating speech up to
25.8 times faster than the latest existing baselines.

</details>


### [248] [Adaptive Knowledge Distillation using a Device-Aware Teacher for Low-Complexity Acoustic Scene Classification](https://arxiv.org/abs/2509.09262)
*Seung Gyu Jeong,Seong Eun Kim*

Main category: cs.SD

TL;DR: This paper addresses robust acoustic scene classification with low complexity, introducing a knowledge distillation model and device-specific fine-tuning for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: The authors aim to tackle acoustic scene classification challenges under strict computational constraints and ensure robust performance across various devices, including unseen ones.

Method: They propose a CP-MobileNet student model trained using knowledge distillation with a two-teacher ensemble approach. Key innovations include a Device-Aware Feature Alignment (DAFA) loss for device robustness and device-specific fine-tuning using test-time device labels.

Result: Their system achieved 57.93% accuracy on the development set, surpassing the baseline, especially on unseen devices.

Conclusion: The proposed method shows significant improvements in device-robust acoustic scene classification within computational limitations, highlighting its potential for real-world applications.

Abstract: In this technical report, we describe our submission for Task 1,
Low-Complexity Device-Robust Acoustic Scene Classification, of the DCASE 2025
Challenge. Our work tackles the dual challenges of strict complexity
constraints and robust generalization to both seen and unseen devices, while
also leveraging the new rule allowing the use of device labels at test time.
Our proposed system is based on a knowledge distillation framework where an
efficient CP-MobileNet student learns from a compact, specialized two-teacher
ensemble. This ensemble combines a baseline PaSST teacher, trained with
standard cross-entropy, and a 'generalization expert' teacher. This expert is
trained using our novel Device-Aware Feature Alignment (DAFA) loss, adapted
from prior work, which explicitly structures the feature space for device
robustness. To capitalize on the availability of test-time device labels, the
distilled student model then undergoes a final device-specific fine-tuning
stage. Our proposed system achieves a final accuracy of 57.93\% on the
development set, demonstrating a significant improvement over the official
baseline, particularly on unseen devices.

</details>


### [249] [Finite Scalar Quantization Enables Redundant and Transmission-Robust Neural Audio Compression at Low Bit-rates](https://arxiv.org/abs/2509.09550)
*Harry Julia,Rachel Beeson,Lohith Konathala,Johanna Ulin,Jiameng Gao*

Main category: cs.SD

TL;DR: This paper introduces NeuCodec, a Neural Audio Codec based on Finite Scalar Quantization (FSQ), highlighting its robustness in noisy transmission scenarios compared to Residual Vector Quantization (RVQ).


<details>
  <summary>Details</summary>
Motivation: Neural Audio Codecs are widely used in tasks like audio generation but face challenges like robustness and encoding efficiency, prompting the exploration of FSQ as an alternative to RVQ.

Method: The authors designed NeuCodec, an FSQ-based codec, and conducted two experiments: encoder distillation to observe encoding redundancy and a noisy channel simulation to test robustness against bit-level perturbations.

Result: The experiments revealed that FSQ-based codecs, like NeuCodec, maintain encoding quality across different encoders and demonstrate better robustness to noisy transmissions compared to RVQ.

Conclusion: FSQ, implemented in NeuCodec, offers promising advantages in audio encoding, producing robust and efficient codec systems suitable for varying channel conditions.

Abstract: Neural Audio Codecs (NACs) have become increasingly adopted in speech
processing tasks due to their excellent rate-distortion performance and
compatibility with Large Language Models (LLMs) as discrete feature
representations for audio generation. While most existing codecs rely on
Residual Vector Quantization (RVQ), Finite Scalar Quantization (FSQ) has
recently emerged as a compelling alternative that simplifies training and
natively supports single codebooks. We introduce NeuCodec, an FSQ-based NAC,
and show that FSQ encodes baked-in redundancy which produces an encoding which
is robust when transmitted through noisy channels. First, through an encoder
distillation experiment, we show that two different encoders can learn to
encode identical audio into vastly different code sequences whilst maintaining
comparable reconstruction quality with the same quantizer and decoder. Second,
we demonstrate that FSQ has vastly superior bit-level perturbation robustness
by comparing the performance of RVQ and FSQ codecs when simulating the
transmission of code sequences through a noisy channel.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [250] [Database Views as Explanations for Relational Deep Learning](https://arxiv.org/abs/2509.09482)
*Agapi Rissaki,Ilias Fountalis,Wolfgang Gatterbauer,Benny Kimelfeld*

Main category: cs.DB

TL;DR: The paper introduces a framework for explaining machine-learning models applied to relational databases using view definitions that pinpoint critical database sections influencing predictions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the difficulty in interpreting how complex neural architectures, such as heterogeneous graph neural networks (hetero-GNNs), use relational database data for prediction.

Method: The framework uses view definitions inspired by determinacy concepts to create explanations, employing heuristic algorithms and techniques like learnable masking tailored for hetero-GNNs.

Result: Extensive experiments conducted on the RelBench collection show the proposed framework's explanations are both useful and efficient across diverse tasks and domains.

Conclusion: The study validates the effectiveness of the explanation framework in improving human-understandable insights into machine-learning models operating on relational databases.

Abstract: In recent years, there has been significant progress in the development of
deep learning models over relational databases, including architectures based
on heterogeneous graph neural networks (hetero-GNNs) and heterogeneous graph
transformers. In effect, such architectures state how the database records and
links (e.g., foreign-key references) translate into a large, complex numerical
expression, involving numerous learnable parameters. This complexity makes it
hard to explain, in human-understandable terms, how a model uses the available
data to arrive at a given prediction. We present a novel framework for
explaining machine-learning models over relational databases, where
explanations are view definitions that highlight focused parts of the database
that mostly contribute to the model's prediction. We establish such global
abductive explanations by adapting the classic notion of determinacy by Nash,
Segoufin, and Vianu (2010). In addition to tuning the tradeoff between
determinacy and conciseness, the framework allows controlling the level of
granularity by adopting different fragments of view definitions, such as ones
highlighting whole columns, foreign keys between tables, relevant groups of
tuples, and so on. We investigate the realization of the framework in the case
of hetero-GNNs. We develop heuristic algorithms that avoid the exhaustive
search over the space of all databases. We propose techniques that are
model-agnostic, and others that are tailored to hetero-GNNs via the notion of
learnable masking. Our approach is evaluated through an extensive empirical
study on the RelBench collection, covering a variety of domains and different
record-level tasks. The results demonstrate the usefulness of the proposed
explanations, as well as the efficiency of their generation.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [251] [HARD: A Performance Portable Radiation Hydrodynamics Code based on FleCSI Framework](https://arxiv.org/abs/2509.08971)
*Julien Loiseau,Hyun Lim,Andrés Yagüe López,Mammadbaghir Baghirzade,Shihab Shahriar Khan,Yoonsoo Kim,Sudarshan Neopane,Alexander Strack,Farhana Taiyebah,Benjamin K. Bergen*

Main category: physics.comp-ph

TL;DR: This paper introduces HARD, an open-source platform for simulating compressible hydrodynamics with radiation diffusion, emphasizing performance portability and scientific reliability.


<details>
  <summary>Details</summary>
Motivation: To create a versatile, high-performance tool for simulating hydrodynamics with radiation-diffusion coupling that is adaptable to a wide range of computational setups and scientifically reliable.

Method: HARD is built on FleCSI, employs Kokkos for portability across laptop to supercomputer setups, uses task orchestration via back-end runtimes like Legion, MPI, or HPX, and incorporates regression tests for scientific validation.

Result: HARD achieves portability, efficiency on diverse platforms, and scientific reliability by reproducing and validating solutions against canonical problems.

Conclusion: HARD offers a sustainable, open-source platform for advancing radiation hydrodynamics research through its performance, reliability, and community-driven development.

Abstract: Hydrodynamics And Radiation Diffusion} (HARD) is an open-source application
for high-performance simulations of compressible hydrodynamics with
radiation-diffusion coupling. Built on the FleCSI (Flexible Computational
Science Infrastructure) framework, HARD expresses its computational units as
tasks whose execution can be orchestrated by multiple back-end runtimes,
including Legion, MPI, and HPX. Node-level parallelism is delegated to Kokkos,
providing a single, portable code base that runs efficiently on laptops, small
homogeneous clusters, and the largest heterogeneous supercomputers currently
available. To ensure scientific reliability, HARD includes a regression-test
suite that automatically reproduces canonical verification problems such as the
Sod and LeBlanc shock tubes and the Sedov blast wave, comparing numerical
solutions against known analytical results. The project is distributed under an
OSI-approved license, hosted on GitHub, and accompanied by reproducible build
scripts and continuous integration workflows. This combination of performance
portability, verification infrastructure, and community-focused development
makes HARD a sustainable platform for advancing radiation hydrodynamics
research across multiple domains.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [252] [PerFairX: Is There a Balance Between Fairness and Personality in Large Language Model Recommendations?](https://arxiv.org/abs/2509.08829)
*Chandan Kumar Sah*

Main category: cs.CY

TL;DR: This study proposes PerFairX, a framework to evaluate the balance between personalization and demographic fairness in LLM-based recommendations.


<details>
  <summary>Details</summary>
Motivation: To investigate the tension between providing personalized, psychology-aligned recommendations and ensuring demographic fairness in LLM-based recommender systems.

Method: The researchers use PerFairX, an evaluation framework, to benchmark ChatGPT and DeepSeek on movie and music datasets through neutral and personality-sensitive prompts.

Result: Personality-aware prompts improved psychological alignment but heightened fairness disparities. DeepSeek showed stronger psychological fit but was more prompt-sensitive, while ChatGPT was less personalized but more stable.

Conclusion: PerFairX aids in developing equitable and psychologically informed LLM-based recommender systems, promoting inclusive AI applications.

Abstract: The integration of Large Language Models (LLMs) into recommender systems has
enabled zero-shot, personality-based personalization through prompt-based
interactions, offering a new paradigm for user-centric recommendations.
However, incorporating user personality traits via the OCEAN model highlights a
critical tension between achieving psychological alignment and ensuring
demographic fairness. To address this, we propose PerFairX, a unified
evaluation framework designed to quantify the trade-offs between
personalization and demographic equity in LLM-generated recommendations. Using
neutral and personality-sensitive prompts across diverse user profiles, we
benchmark two state-of-the-art LLMs, ChatGPT and DeepSeek, on movie (MovieLens
10M) and music (Last.fm 360K) datasets. Our results reveal that
personality-aware prompting significantly improves alignment with individual
traits but can exacerbate fairness disparities across demographic groups.
Specifically, DeepSeek achieves stronger psychological fit but exhibits higher
sensitivity to prompt variations, while ChatGPT delivers stable yet less
personalized outputs. PerFairX provides a principled benchmark to guide the
development of LLM-based recommender systems that are both equitable and
psychologically informed, contributing to the creation of inclusive,
user-centric AI applications in continual learning contexts.

</details>


### [253] [Deep opacity and AI: A threat to XAI and to privacy protection mechanisms](https://arxiv.org/abs/2509.08835)
*Vincent C. Müller*

Main category: cs.CY

TL;DR: The paper explores how AI's opaque functioning exacerbates privacy issues and complicates remedies. It categorizes opacity in AI systems and connects it to limitations in protecting privacy.


<details>
  <summary>Details</summary>
Motivation: AI and big data analytics are increasingly seen as threats to privacy, especially due to their opaque functioning, which hampers justification for actions and judgments.

Method: The author introduces three types of opacity in AI systems (shallow, standard black box, and deep) and analyzes their implications for privacy and informed consent.

Result: The analysis shows that opacity in AI systems undermines justifications needed to ensure privacy and makes remedies less effective.

Conclusion: Big data analytics intensifies privacy issues, and opacity often prevents effective safeguards. The paper briefly highlights technical approaches to mitigate these challenges.

Abstract: It is known that big data analytics and AI pose a threat to privacy, and that
some of this is due to some kind of "black box problem" in AI. I explain how
this becomes a problem in the context of justification for judgments and
actions. Furthermore, I suggest distinguishing three kinds of opacity: 1) the
subjects do not know what the system does ("shallow opacity"), 2) the analysts
do not know what the system does ("standard black box opacity"), or 3) the
analysts cannot possibly know what the system might do ("deep opacity"). If the
agents, data subjects as well as analytics experts, operate under opacity, then
these agents cannot provide justifications for judgments that are necessary to
protect privacy, e.g., they cannot give "informed consent", or guarantee
"anonymity". It follows from these points that agents in big data analytics and
AI often cannot make the judgments needed to protect privacy. So I conclude
that big data analytics makes the privacy problems worse and the remedies less
effective. As a positive note, I provide a brief outlook on technical ways to
handle this situation.

</details>


### [254] [Safe and Certifiable AI Systems: Concepts, Challenges, and Lessons Learned](https://arxiv.org/abs/2509.08852)
*Kajetan Schweighofer,Barbara Brune,Lukas Gruber,Simon Schmid,Alexander Aufreiter,Andreas Gruber,Thomas Doms,Sebastian Eder,Florian Mayer,Xaver-Paul Stadlbauer,Christoph Schwald,Werner Zellinger,Bernhard Nessler,Sepp Hochreiter*

Main category: cs.CY

TL;DR: The paper introduces the TÜV AUSTRIA Trusted AI framework, an audit methodology aimed at certification of AI systems in compliance with European standards.


<details>
  <summary>Details</summary>
Motivation: With AI increasingly used in safety-critical applications, reliable certification mechanisms are needed to ensure these systems are safe, lawful, and socially acceptable.

Method: The framework combines three pillars—Secure Software Development, Functional Requirements, and Ethics & Data Privacy—and converts EU AI Act obligations into specific, testable criteria.

Result: The proposed audit catalog ensures transparent, reproducible model quality evidence by defining application domains, setting risk-based performance requirements, and statistical testing using independent data.

Conclusion: This framework offers a practical roadmap to certifying AI systems as legally compliant and trustworthy, aligning technical best practices with emerging European regulations.

Abstract: There is an increasing adoption of artificial intelligence in safety-critical
applications, yet practical schemes for certifying that AI systems are safe,
lawful and socially acceptable remain scarce. This white paper presents the
T\"UV AUSTRIA Trusted AI framework an end-to-end audit catalog and methodology
for assessing and certifying machine learning systems. The audit catalog has
been in continuous development since 2019 in an ongoing collaboration with
scientific partners. Building on three pillars - Secure Software Development,
Functional Requirements, and Ethics & Data Privacy - the catalog translates the
high-level obligations of the EU AI Act into specific, testable criteria. Its
core concept of functional trustworthiness couples a statistically defined
application domain with risk-based minimum performance requirements and
statistical testing on independently sampled data, providing transparent and
reproducible evidence of model quality in real-world settings. We provide an
overview of the functional requirements that we assess, which are oriented on
the lifecycle of an AI system. In addition, we share some lessons learned from
the practical application of the audit catalog, highlighting common pitfalls we
encountered, such as data leakage scenarios, inadequate domain definitions,
neglect of biases, or a lack of distribution drift controls. We further discuss
key aspects of certifying AI systems, such as robustness, algorithmic fairness,
or post-certification requirements, outlining both our current conclusions and
a roadmap for future research. In general, by aligning technical best practices
with emerging European standards, the approach offers regulators, providers,
and users a practical roadmap for legally compliant, functionally trustworthy,
and certifiable AI systems.

</details>


### [255] [A vibe coding learning design to enhance EFL students' talking to, through, and about AI](https://arxiv.org/abs/2509.08854)
*David James Woo,Kai Guo,Yangyang Yu*

Main category: cs.CY

TL;DR: The paper discusses a pilot study of vibe coding for EFL education, developing a human-AI meta-languaging framework and assessing outcomes of a four-hour workshop with two students.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance EFL education through AI-driven vibe coding, addressing authentic writing challenges and understanding student interactions with AI.

Method: A workshop using backward design principles was conducted with two students, involving tasks for designing applications. Data was collected via think-aloud protocols, screen recordings, worksheets, and AI-generated images.

Result: One student succeeded in implementing her intended application design, while the other faced technical challenges. Differences in prompt engineering approaches and AI mental models were observed.

Conclusion: The study highlights the potential of AI as a languaging tool and emphasizes the need for structured instruction in prompt engineering, authorship negotiation, and effective articulation of AI mental models.

Abstract: This innovative practice article reports on the piloting of vibe coding
(using natural language to create software applications with AI) for English as
a Foreign Language (EFL) education. We developed a human-AI meta-languaging
framework with three dimensions: talking to AI (prompt engineering), talking
through AI (negotiating authorship), and talking about AI (mental models of
AI). Using backward design principles, we created a four-hour workshop where
two students designed applications addressing authentic EFL writing challenges.
We adopted a case study methodology, collecting data from worksheets and video
recordings, think-aloud protocols, screen recordings, and AI-generated images.
Contrasting cases showed one student successfully vibe coding a functional
application cohering to her intended design, while another encountered
technical difficulties with major gaps between intended design and actual
functionality. Analysis reveals differences in students' prompt engineering
approaches, suggesting different AI mental models and tensions in attributing
authorship. We argue that AI functions as a beneficial languaging machine, and
that differences in how students talk to, through, and about AI explain vibe
coding outcome variations. Findings indicate that effective vibe coding
instruction requires explicit meta-languaging scaffolding, teaching structured
prompt engineering, facilitating critical authorship discussions, and
developing vocabulary for articulating AI mental models.

</details>


### [256] [Investigating Student Interaction Patterns with Large Language Model-Powered Course Assistants in Computer Science Courses](https://arxiv.org/abs/2509.08862)
*Chang Liu,Loc Hoang,Andrew Stolman,Rene F. Kizilcec,Bo Wu*

Main category: cs.CY

TL;DR: The paper studies the deployment of an LLM-powered course assistant in computer science courses and evaluates its use, effectiveness, and pedagogical impact.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of providing flexible and timely academic support to students outside of scheduled hours.

Method: An LLM-powered course assistant was deployed across multiple computer science courses, engaging approximately 2,000 students. Conversation samples were manually annotated to evaluate correctness, helpfulness, and types of interactions, and Bloom's taxonomy was applied.

Result: Usage was notably higher during evenings/nighttime and in introductory courses. Most responses were correct/helpful, though a small portion were erroneous/unhelpful. The system generated limited higher-order cognitive questions, and follow-up inquiries were often ignored in advanced courses.

Conclusion: LLM-powered educational systems can address temporal support gaps, but their pedagogical power could be improved through greater educator involvement in designing the interactions and refining cognitive question generation.

Abstract: Providing students with flexible and timely academic support is a challenge
at most colleges and universities, leaving many students without help outside
scheduled hours. Large language models (LLMs) are promising for bridging this
gap, but interactions between students and LLMs are rarely overseen by
educators. We developed and studied an LLM-powered course assistant deployed
across multiple computer science courses to characterize real-world use and
understand pedagogical implications. By Spring 2024, our system had been
deployed to approximately 2,000 students across six courses at three
institutions. Analysis of the interaction data shows that usage remains strong
in the evenings and nights and is higher in introductory courses, indicating
that our system helps address temporal support gaps and novice learner needs.
We sampled 200 conversations per course for manual annotation: most sampled
responses were judged correct and helpful, with a small share unhelpful or
erroneous; few responses included dedicated examples. We also examined an
inquiry-based learning strategy: only around 11% of sampled conversations
contained LLM-generated follow-up questions, which were often ignored by
students in advanced courses. A Bloom's taxonomy analysis reveals that current
LLM capabilities are limited in generating higher-order cognitive questions.
These patterns suggest opportunities for pedagogically oriented LLM-based
educational systems and greater educator involvement in configuring prompts,
content, and policies.

</details>


### [257] [Decentralising LLM Alignment: A Case for Context, Pluralism, and Participation](https://arxiv.org/abs/2509.08858)
*Oriane Peter,Kate Devlin*

Main category: cs.CY

TL;DR: The paper critiques current LLM alignment methods for centralizing control in specific institutions and advocates for decentralized alignment through context, pluralism, and participation.


<details>
  <summary>Details</summary>
Motivation: Current LLM alignment methods impose the values of a narrow reference group, raising concerns about epistemic centralization and power dynamics.

Method: The authors propose decentralizing alignment by focusing on three pillars: context, pluralism, and participation, supported by real-world use cases.

Result: The study highlights the importance of contextualizing alignment practices and how pluralistic and participatory approaches can lead to fairer knowledge governance.

Conclusion: While decentralizing alignment may counter epistemic injustice, it is insufficient on its own without broader societal reforms.

Abstract: Large Language Models (LLMs) alignment methods have been credited with the
commercial success of products like ChatGPT, given their role in steering LLMs
towards user-friendly outputs. However, current alignment techniques
predominantly mirror the normative preferences of a narrow reference group,
effectively imposing their values on a wide user base. Drawing on theories of
the power/knowledge nexus, this work argues that current alignment practices
centralise control over knowledge production and governance within already
influential institutions. To counter this, we propose decentralising alignment
through three characteristics: context, pluralism, and participation.
Furthermore, this paper demonstrates the critical importance of delineating the
context-of-use when shaping alignment practices by grounding each of these
features in concrete use cases. This work makes the following contributions:
(1) highlighting the role of context, pluralism, and participation in
decentralising alignment; (2) providing concrete examples to illustrate these
strategies; and (3) demonstrating the nuanced requirements associated with
applying alignment across different contexts of use. Ultimately, this paper
positions LLM alignment as a potential site of resistance against epistemic
injustice and the erosion of democratic processes, while acknowledging that
these strategies alone cannot substitute for broader societal changes.

</details>


### [258] [Incorporating AI Incident Reporting into Telecommunications Law and Policy: Insights from India](https://arxiv.org/abs/2509.09508)
*Avinash Agarwal,Manisha J. Nene*

Main category: cs.CY

TL;DR: The paper identifies AI-specific risks in telecommunications, beyond traditional cybersecurity issues, using India as a case study, and provides policy recommendations to address regulatory gaps.


<details>
  <summary>Details</summary>
Motivation: To address the lack of regulatory frameworks for AI-specific risks in telecommunications, particularly for jurisdictions without dedicated AI laws.

Method: Analyzing India's legal frameworks and identifying gaps in addressing AI incidents. Proposing policies for AI incident reporting and regulatory integration.

Result: The study found regulatory gaps in India's laws, which fail to adequately address AI-specific risks like bias and performance degradation.

Conclusion: Targeted policies, such as mandatory AI incident reporting and standardized frameworks, are necessary to integrate AI governance into existing regulations, providing a roadmap for other jurisdictions.

Abstract: The integration of artificial intelligence (AI) into telecommunications
infrastructure introduces novel risks, such as algorithmic bias and
unpredictable system behavior, that fall outside the scope of traditional
cybersecurity and data protection frameworks. This paper introduces a precise
definition and a detailed typology of telecommunications AI incidents,
establishing them as a distinct category of risk that extends beyond
conventional cybersecurity and data protection breaches. It argues for their
recognition as a distinct regulatory concern. Using India as a case study for
jurisdictions that lack a horizontal AI law, the paper analyzes the country's
key digital regulations. The analysis reveals that India's existing legal
instruments, including the Telecommunications Act, 2023, the CERT-In Rules, and
the Digital Personal Data Protection Act, 2023, focus on cybersecurity and data
breaches, creating a significant regulatory gap for AI-specific operational
incidents, such as performance degradation and algorithmic bias. The paper also
examines structural barriers to disclosure and the limitations of existing AI
incident repositories. Based on these findings, the paper proposes targeted
policy recommendations centered on integrating AI incident reporting into
India's existing telecom governance. Key proposals include mandating reporting
for high-risk AI failures, designating an existing government body as a nodal
agency to manage incident data, and developing standardized reporting
frameworks. These recommendations aim to enhance regulatory clarity and
strengthen long-term resilience, offering a pragmatic and replicable blueprint
for other nations seeking to govern AI risks within their existing sectoral
frameworks.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [259] [Personalized Sleep Prediction via Deep Adaptive Spatiotemporal Modeling and Sparse Data](https://arxiv.org/abs/2509.09018)
*Xueyi Wang,C. J. C.,Lamoth,Elisabeth Wilhelm*

Main category: eess.SP

TL;DR: The paper introduces AdaST-Sleep, a model for personalized sleep forecasting using sparse wearable device data, combining spatial, temporal, and domain adaptation techniques. It outperformed baseline models and demonstrated strong predictive accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve sleep forecasting to enhance mental and physical well-being by addressing sparsity in wearable data and ensuring model adaptability across different individuals.

Method: The proposed AdaST-Sleep model integrates convolutional layers for spatial feature interactions, recurrent layers for handling temporal data, and a domain classifier for cross-subject generalization. Various input and predicting window sizes were analyzed.

Result: AdaST-Sleep outperformed four baseline models, achieving an RMSE of 0.282 with optimal input and predicting windows (7-day input, 1-day prediction), and maintained high accuracy for multi-day forecasts.

Conclusion: AdaST-Sleep provides a robust, adaptable solution for sleep forecasting using sparse wearable data, offering reliable predictions that track sleep scores and fluctuations accurately for real-world applications.

Abstract: A sleep forecast allows individuals and healthcare providers to anticipate
and proactively address factors influencing restful rest, ultimately improving
mental and physical well-being. This work presents an adaptive spatial and
temporal model (AdaST-Sleep) for predicting sleep scores. Our proposed model
combines convolutional layers to capture spatial feature interactions between
multiple features and recurrent neural network layers to handle longer-term
temporal health-related data. A domain classifier is further integrated to
generalize across different subjects. We conducted several experiments using
five input window sizes (3, 5, 7, 9, 11 days) and five predicting window sizes
(1, 3, 5, 7, 9 days). Our approach consistently outperformed four baseline
models, achieving its lowest RMSE (0.282) with a seven-day input window and a
one-day predicting window. Moreover, the method maintained strong performance
even when forecasting multiple days into the future, demonstrating its
versatility for real-world applications. Visual comparisons reveal that the
model accurately tracks both the overall sleep score level and daily
fluctuations. These findings prove that the proposed framework provides a
robust and adaptable solution for personalized sleep forecasting using sparse
data from commercial wearable devices and domain adaptation techniques.

</details>


### [260] [A Masked Representation Learning to Model Cardiac Functions Using Multiple Physiological Signals](https://arxiv.org/abs/2509.08830)
*Seong-A Park,Jong-Eui Chae,Sungdong Kim,Hyung-Chul Lee,Hyun-Lim Yang*

Main category: eess.SP

TL;DR: The paper proposes a novel model, SNUPHY-M, leveraging multi-modal self-supervised learning (SSL) for clinical signal analysis using ECG, PPG, and ABP signals, significantly improving prediction tasks with non-invasive methods.


<details>
  <summary>Details</summary>
Motivation: There is a critical need for integrated analysis of multiple physiological signals to better manage patient prognosis in clinical settings, as current approaches focus on single signal analysis and are insufficient for complex clinical scenarios.

Method: The study introduces the SNUPHY-M model based on self-supervised learning (SSL) to restore three masked physiological signals (ECG, PPG, and ABP) and extract features reflecting cardiac cycle characteristics using multi-modal representation.

Result: SNUPHY-M demonstrated superior performance in several clinical downstream tasks, including predicting hypotension, stroke volume, blood pressure metrics, and age, outperforming supervised and traditional SSL approaches, especially with non-invasive signals.

Conclusion: SNUPHY-M provides a significant step forward in cardiovascular analysis by applying multi-modal SSL, supporting clinical decision-making and enabling early diagnosis and management of hemodynamics non-invasively.

Abstract: In clinical settings, monitoring hemodynamics is crucial for managing patient
prognosis, necessitating the integrated analysis of multiple physiological
signals. While recent research has analyzed single signals such as
electrocardiography (ECG) or photoplethysmography (PPG), there has yet to be a
proposal for an approach that encompasses the complex signal analysis required
in actual clinical scenarios. In this study, we introduce the SNUPHY-M (Seoul
National University hospital PHYsiological signal Masked representation
learning) model extracts physiological features reflecting the electrical,
pressure, and fluid characteristics of the cardiac cycle in the process of
restoring three masked physiological signals based on self-supervised learning
(SSL): ECG, PPG, and arterial blood pressure (ABP) signals. By employing
multiple physical characteristics, the model can extract more enriched features
only using non-invasive signals. We evaluated the model's performance in
clinical downstream tasks such as hypotension, stroke volume, systolic blood
pressure, diastolic blood pressure, and age prediction. Our results showed that
the SNUPHY-M significantly outperformed supervised or SSL models, especially in
prediction tasks using non-invasive signals. To the best of our knowledge,
SNUPHY-M is the first model to apply multi-modal SSL to cardiovascular analysis
involving ECG, PPG, and ABP signals. This approach effectively supports
clinical decision-making and enables precise diagnostics, contributing
significantly to the early diagnosis and management of hemodynamics without
invasiveness.

</details>


### [261] [Deploying AI for Signal Processing education: Selected challenges and intriguing opportunities](https://arxiv.org/abs/2509.08950)
*Jarvis Haupt,Qin Lu,Yanning Shen,Jia Chen,Yue Dong,Dan McCreary,Mehmet Akçakaya,Georgios B. Giannakis*

Main category: eess.SP

TL;DR: This paper explores how AI tools can enhance education, focusing on signal processing via a smart textbook with considerations for fairness, inclusivity, and trust.


<details>
  <summary>Details</summary>
Motivation: To address both the technical and ethical challenges associated with using AI tools in education, aiming to improve the global human condition.

Method: The authors investigate AI's application in education by tackling technical issues and creating a smart textbook for immersive learning experiences.

Result: Key considerations like fairness, inclusivity, and resource efficiency are highlighted through the smart textbook's development and its implementation examples.

Conclusion: AI has the potential to transform education if used responsibly, factoring in challenges like hallucinations, inclusivity, and transparency for better learning outcomes.

Abstract: Powerful artificial intelligence (AI) tools that have emerged in recent years
-- including large language models, automated coding assistants, and advanced
image and speech generation technologies -- are the result of monumental human
achievements. These breakthroughs reflect mastery across multiple technical
disciplines and the resolution of significant technological challenges.
However, some of the most profound challenges may still lie ahead. These
challenges are not purely technical but pertain to the fair and responsible use
of AI in ways that genuinely improve the global human condition. This article
explores one promising application aligned with that vision: the use of AI
tools to facilitate and enhance education, with a specific focus on signal
processing (SP). It presents two interrelated perspectives: identifying and
addressing technical limitations, and applying AI tools in practice to improve
educational experiences. Primers are provided on several core technical issues
that arise when using AI in educational settings, including how to ensure
fairness and inclusivity, handle hallucinated outputs, and achieve efficient
use of resources. These and other considerations -- such as transparency,
explainability, and trustworthiness -- are illustrated through the development
of an immersive, structured, and reliable "smart textbook." The article serves
as a resource for researchers and educators seeking to advance AI's role in
engineering education.

</details>


### [262] [Ultrafast Deep Learning-Based Scatter Estimation in Cone-Beam Computed Tomography](https://arxiv.org/abs/2509.08973)
*Harshit Agrawal,Ari Hietanen,Simo Särkkä*

Main category: eess.SP

TL;DR: The paper tackles scatter artifact issues in CBCT systems by optimizing deep learning networks for speed, accuracy, and memory efficiency, enabling their usage in resource-constrained devices.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in applying deep learning scatter correction methods to mobile CBCT systems or edge devices due to high memory and computational requirements.

Method: The study evaluates reconstruction errors across six resolutions, implements a state-of-the-art method at five resolutions, and measures reductions in FLOPs, inference times, and GPU memory usage while assessing scatter correction performance.

Result: The optimized method achieves a 78-fold reduction in FLOPs and dramatically reduces inference times and GPU memory demands, while maintaining comparable error metrics (e.g., MAPE reduced to 3.85%). Robustness is demonstrated using simulated and real-world datasets.

Conclusion: Downsampling techniques significantly reduce computational demands, enabling scatter correction for mobile and edge CBCT systems without compromising performance.

Abstract: Purpose: Scatter artifacts drastically degrade the image quality of cone-beam
computed tomography (CBCT) scans. Although deep learning-based methods show
promise in estimating scatter from CBCT measurements, their deployment in
mobile CBCT systems or edge devices is still limited due to the large memory
footprint of the networks. This study addresses the issue by applying networks
at varying resolutions and suggesting an optimal one, based on speed and
accuracy.
  Methods: First, the reconstruction error in down-up sampling of CBCT scatter
signal was examined at six resolutions by comparing four interpolation methods.
Next, a recent state-of-the-art method was trained across five image
resolutions and evaluated for the reductions in floating-point operations
(FLOPs), inference times, and GPU memory requirements.
  Results: Reducing the input size and network parameters achieved a 78-fold
reduction in FLOPs compared to the baseline method, while maintaining comarable
performance in terms of mean-absolute-percentage-error (MAPE) and
mean-square-error (MSE). Specifically, the MAPE decreased to 3.85% compared to
4.42%, and the MSE decreased to 1.34 \times 10^{-2} compared to 2.01 \times
10^{-2}. Inference time and GPU memory usage were reduced by factors of 16 and
12, respectively. Further experiments comparing scatter-corrected
reconstructions on a large, simulated dataset and real CBCT scans from water
and Sedentex CT phantoms clearly demonstrated the robustness of our method.
  Conclusion: This study highlights the underappreciated role of downsampling
in deep learning-based scatter estimation. The substantial reduction in FLOPs
and GPU memory requirements achieved by our method enables scatter correction
in resource-constrained environments, such as mobile CBCT and edge devices.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [263] [Convexity of Optimization Curves: Local Sharp Thresholds, Robustness Impossibility, and New Counterexamples](https://arxiv.org/abs/2509.08954)
*Le Duc Hieu*

Main category: math.OC

TL;DR: The paper investigates the conditions under which the optimization sequence of first-order methods, specifically gradient descent, exhibits convex behavior and determines precise stepsize thresholds for such convexity.


<details>
  <summary>Details</summary>
Motivation: The paper aims to deepen understanding of the behavior of optimization sequences in first-order methods, particularly in connecting convexity characteristics with step size choices.

Method: The authors analyze the optimization curve of gradient descent on convex L-smooth functions using both discrete and continuous dynamics. They establish thresholds on the step size for convexity and nonincreasing gradient behavior.

Result: It is shown that for gradient descent with step size \( \eta \leq 1.75/L \), the optimization curve is convex, and this threshold is tight. Gradient norms are nonincreasing for \( \eta \leq 2/L \), and convexity is guaranteed in continuous dynamics.

Conclusion: The findings provide sharper insights into step size choices and their effects on the dynamics of optimization, enhancing the classical convex optimization framework and connecting discrete and continuous perspectives.

Abstract: We study when the \emph{optimization curve} of first-order methods -- the
sequence \${f(x\_n)}*{n\ge0}\$ produced by constant-stepsize iterations -- is
convex, equivalently when the forward differences \$f(x\_n)-f(x*{n+1})\$ are
nonincreasing. For gradient descent (GD) on convex \$L\$-smooth functions, the
curve is convex for all stepsizes \$\eta \le 1.75/L\$, and this threshold is
tight. Moreover, gradient norms are nonincreasing for all \$\eta \le 2/L\$, and
in continuous time (gradient flow) the curve is always convex. These results
complement and refine the classical smooth convex optimization toolbox,
connecting discrete and continuous dynamics as well as worst-case analyses.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [264] [An Integrated Open Source Software System for the Generation and Analysis of Subject-Specific Blood Flow Simulation Ensembles](https://arxiv.org/abs/2509.09392)
*Simon Leistikow,Thomas Miro,Adrian Kummerländer,Ali Nahardani,Katja Grün,Markus Franz,Verena Hoerr,Mathias J. Krause,Lars Linsen*

Main category: physics.med-ph

TL;DR: This paper introduces a user-friendly, open-source tool that integrates Computational Fluid Dynamics (CFD) and Magnetic Resonance Imaging (MRI) for subject-specific analysis of blood flow, aiming to enhance hemodynamic research and cardiovascular disease analysis.


<details>
  <summary>Details</summary>
Motivation: To enable accurate and subject-specific hemodynamic analysis for diagnosing cardiovascular diseases by integrating the capabilities of CFD simulations and MRI data.

Method: The paper presents an interactive, open-source tool designed for visual analysis of blood flow. The tool integrates CFD and MRI, allows the creation of diverse simulation ensembles, and provides 2D visual and analytical examination of the similarities in simulations and measurements.

Result: The tool was successfully applied in three real-world use cases, where it helped configure simulation ensembles and analyze blood flow dynamics. Its usability and features were validated by MRI and CFD experts.

Conclusion: The integration of CFD and MRI through this tool enhances the understanding and analysis of hemodynamic parameters, providing a valuable resource for investigating cardiovascular parameters and diseases.

Abstract: Background and Objective: Hemodynamic analysis of blood flow through arteries
and veins is critical for diagnosing cardiovascular diseases, such as aneurysms
and stenoses, and for investigating cardiovascular parameters, such as
turbulence and wall shear stress. For subject-specific analyses, the anatomy
and blood flow of the subject can be captured non-invasively using structural
and 4D Magnetic Resonance Imaging (MRI). Computational Fluid Dynamics (CFD), on
the other hand, can be used to generate blood flow simulations by solving the
Navier-Stokes equations. To generate and analyze subject-specific blood flow
simulations, MRI and CFD have to be brought together.
  Methods: We present an interactive, customizable, and user-oriented visual
analysis tool that assists researchers in both medicine and numerical analysis.
Our open-source tool is applicable to domains such as CFD and MRI, and it
facilitates the analysis of simulation results and medical data, especially in
hemodynamic studies. It enables the creation of simulation ensembles with a
high variety of parameters. Furthermore, it allows for the visual and
analytical examination of simulations and measurements through 2D embeddings of
the similarity space.
  Results: To demonstrate the effectiveness of our tool, we applied it to three
real-world use cases, showcasing its ability to configure simulation ensembles
and analyse blood flow dynamics. We evaluated our example cases together with
MRI and CFD experts to further enhance features and increase the usability.
  Conclusions: By combining the strengths of both CFD and MRI, our tool
provides a more comprehensive understanding of hemodynamic parameters,
facilitating more accurate analysis of hemodynamic biomarkers.

</details>


### [265] [Explainable AI for Accelerated Microstructure Imaging: A SHAP-Guided Protocol on the Connectome 2.0 scanner](https://arxiv.org/abs/2509.09513)
*Quentin Uhl,Tommaso Pavan,Julianna Gerold,Kwok-Shing Chan,Yohan Jun,Shohei Fujita,Aneri Bhatt,Yixin Ma,Qiaochu Wang,Hong-Hsi Lee,Susie Y. Huang,Berkin Bilgic,Ileana Jelescu*

Main category: physics.med-ph

TL;DR: This paper proposes and validates a reduced imaging protocol for diffusion MRI Neurite Exchange Imaging, optimizing accuracy while significantly decreasing scan times.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion MRI protocols require long scan durations, limiting their feasibility in probing gray matter microstructure.

Method: An optimized 8-feature subset was identified from a 15-feature protocol using explainable AI and guided recursive feature elimination. Performance was validated in vivo and benchmarked against full and alternative reduction approaches.

Result: The optimized protocol produced parameter estimates and cortical maps comparable to the original, exhibiting reduced estimation errors and improved robustness over other strategies.

Conclusion: The hybrid optimization enabled reliable neurite exchange imaging in just 14 minutes, facilitating wider application in neuroscience and clinical research, while offering a generalizable optimization framework.

Abstract: The diffusion MRI Neurite Exchange Imaging model offers a promising framework
for probing gray matter microstructure by estimating parameters such as
compartment sizes, diffusivities, and inter-compartmental water exchange time.
However, existing protocols require long scan times. This study proposes a
reduced acquisition scheme for the Connectome 2.0 scanner that preserves model
accuracy while substantially shortening scan duration. We developed a
data-driven framework using explainable artificial intelligence with a guided
recursive feature elimination strategy to identify an optimal 8-feature subset
from a 15-feature protocol. The performance of this optimized protocol was
validated in vivo and benchmarked against the full acquisition and alternative
reduction strategies. Parameter accuracy, preservation of anatomical contrast,
and test-retest reproducibility were assessed. The reduced protocol yielded
parameter estimates and cortical maps comparable to the full protocol, with low
estimation errors in synthetic data and minimal impact on test-retest
variability. Compared to theory-driven and heuristic reduction schemes, the
optimized protocol demonstrated superior robustness, reducing the deviation in
water exchange time estimates by over two-fold. In conclusion, this hybrid
optimization framework enables viable imaging of neurite exchange in 14 minutes
without loss of parameter fidelity. This approach supports the broader
application of exchange-sensitive diffusion magnetic resonance imaging in
neuroscience and clinical research, and offers a generalizable method for
designing efficient acquisition protocols in biophysical parameter mapping.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [266] [CameraVDP: Perceptual Display Assessment with Uncertainty Estimation via Camera and Visual Difference Prediction](https://arxiv.org/abs/2509.08947)
*Yancheng Cai,Robert Wanat,Rafal Mantiuk*

Main category: cs.GR

TL;DR: The paper proposes CameraVDP, a system using cameras and visual difference prediction to analyze display artifacts and assess visibility under varying conditions.


<details>
  <summary>Details</summary>
Motivation: Traditional methods inadequately capture pixel-level and high-frequency distortions in display measurements.

Method: A pipeline integrates camera corrections (HDR stacking, MTF inversion, etc.) with a Visual Difference Predictor for human visual system modeling.

Result: The framework was validated on defective pixel detection, color fringing awareness, and display non-uniformity evaluation, with uncertainty analysis for performance bounds.

Conclusion: CameraVDP improves display measurements by enabling accurate and perceptual assessments using cameras, accounting for both camera inaccuracies and human perception.

Abstract: Accurate measurement of images produced by electronic displays is critical
for the evaluation of both traditional and computational displays. Traditional
display measurement methods based on sparse radiometric sampling and fitting a
model are inadequate for capturing spatially varying display artifacts, as they
fail to capture high-frequency and pixel-level distortions. While cameras offer
sufficient spatial resolution, they introduce optical, sampling, and
photometric distortions. Furthermore, the physical measurement must be combined
with a model of a visual system to assess whether the distortions are going to
be visible. To enable perceptual assessment of displays, we propose a
combination of a camera-based reconstruction pipeline with a visual difference
predictor, which account for both the inaccuracy of camera measurements and
visual difference prediction. The reconstruction pipeline combines HDR image
stacking, MTF inversion, vignetting correction, geometric undistortion,
homography transformation, and color correction, enabling cameras to function
as precise display measurement instruments. By incorporating a Visual
Difference Predictor (VDP), our system models the visibility of various stimuli
under different viewing conditions for the human visual system. We validate the
proposed CameraVDP framework through three applications: defective pixel
detection, color fringing awareness, and display non-uniformity evaluation. Our
uncertainty analysis framework enables the estimation of the theoretical upper
bound for defect pixel detection performance and provides confidence intervals
for VDP quality scores.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [267] [Generative Engine Optimization: How to Dominate AI Search](https://arxiv.org/abs/2509.08919)
*Mahe Chen,Xiaoxuan Wang,Kaiwen Chen,Nick Koudas*

Main category: cs.IR

TL;DR: This paper introduces a concept called Generative Engine Optimization (GEO), analyzing differences between generative AI-powered search engines and traditional web search like Google, along with strategies for optimizing content in the new landscape.


<details>
  <summary>Details</summary>
Motivation: With the rise of generative AI search engines tackling information retrieval in different ways than traditional search engines, there is a need to understand and adapt to these changes to maintain or enhance visibility and effectiveness.

Method: The research conducted large-scale controlled experiments across different industries, languages, and query phrasing to compare how AI-based search engines source information versus traditional methods.

Result: The study found that generative AI search engines are biased towards authoritative, third-party sources over brand-owned or social content, significantly differing from Google. They also exhibit variability in domain diversity, data freshness, language stability, and query sensitivity.

Conclusion: This paper proposes a strategic approach (GEO) for content optimization in the era of AI search engines, offering four key tactics and marking a shift in how search visibility is achieved in the generative AI landscape.

Abstract: The rapid adoption of generative AI-powered search engines like ChatGPT,
Perplexity, and Gemini is fundamentally reshaping information retrieval, moving
from traditional ranked lists to synthesized, citation-backed answers. This
shift challenges established Search Engine Optimization (SEO) practices and
necessitates a new paradigm, which we term Generative Engine Optimization
(GEO).
  This paper presents a comprehensive comparative analysis of AI Search and
traditional web search (Google). Through a series of large-scale, controlled
experiments across multiple verticals, languages, and query paraphrases, we
quantify critical differences in how these systems source information. Our key
findings reveal that AI Search exhibit a systematic and overwhelming bias
towards Earned media (third-party, authoritative sources) over Brand-owned and
Social content, a stark contrast to Google's more balanced mix. We further
demonstrate that AI Search services differ significantly from each other in
their domain diversity, freshness, cross-language stability, and sensitivity to
phrasing.
  Based on these empirical results, we formulate a strategic GEO agenda. We
provide actionable guidance for practitioners, emphasizing the critical need
to: (1) engineer content for machine scannability and justification, (2)
dominate earned media to build AI-perceived authority, (3) adopt
engine-specific and language-aware strategies, and (4) overcome the inherent
"big brand bias" for niche players. Our work provides the foundational
empirical analysis and a strategic framework for achieving visibility in the
new generative search landscape.

</details>


### [268] [Envy-Free but Still Unfair: Envy-Freeness Up To One Item (EF-1) in Personalized Recommendation](https://arxiv.org/abs/2509.09037)
*Amanda Aird,Ben Armstrong,Nicholas Mattei,Robin Burke*

Main category: cs.IR

TL;DR: This paper critically evaluates the concept of envy-freeness in fairness criteria, particularly in contexts involving personalization, such as recommendation systems.


<details>
  <summary>Details</summary>
Motivation: The authors aim to examine the sufficiency and relevance of envy-freeness, a widely-used fairness concept in economics and recommendation systems, especially when personalization is involved.

Method: The paper provides an overview of the historical use of envy-freeness in economics and its application in recommendation systems, while illustrating the limitations of the concept in personalized settings.

Result: The authors highlight key shortcomings of envy-freeness as a fairness measure when applied to personalized systems due to its inherent limitations.

Conclusion: The paper concludes that envy-freeness or EF-1 is inadequate for assessing fairness in personalized recommendation systems and suggests the need for alternative measures.

Abstract: Envy-freeness and the relaxation to Envy-freeness up to one item (EF-1) have
been used as fairness concepts in the economics, game theory, and social choice
literatures since the 1960s, and have recently gained popularity within the
recommendation systems communities. In this short position paper we will give
an overview of envy-freeness and its use in economics and recommendation
systems; and illustrate why envy is not appropriate to measure fairness for use
in settings where personalization plays a role.

</details>


### [269] [Retrieval-Augmented Generation for Reliable Interpretation of Radio Regulations](https://arxiv.org/abs/2509.09651)
*Zakaria El Kassimi,Fares Fourati,Mohamed-Slim Alouini*

Main category: cs.IR

TL;DR: This paper presents a retrieval-augmented generation pipeline for question answering in the domain of radio regulations, achieving significant accuracy improvements with targeted methods.


<details>
  <summary>Details</summary>
Motivation: The study addresses the lack of domain-specific solutions for question answering in high-stakes areas like radio regulations, emphasizing the need for accurate retrieval and generation in this legally sensitive field.

Method: The authors designed a telecom-specific Retrieval-Augmented Generation pipeline and created a multiple-choice evaluation dataset. They combined automated filtering with human validation and defined a domain-specific retrieval metric to evaluate performance.

Result: The proposed retriever attained 97% accuracy, and the pipeline achieved nearly a 12% improvement in generation accuracy for GPT-4. Naïve document insertion showed minimal improvement.

Conclusion: The paper demonstrates that targeted grounding via a domain-specific RAG pipeline offers a strong baseline and effective solution for regulatory question answering. Resources and datasets are made publicly available.

Abstract: We study question answering in the domain of radio regulations, a legally
sensitive and high-stakes area. We propose a telecom-specific
Retrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge,
the first multiple-choice evaluation set for this domain, constructed from
authoritative sources using automated filtering and human validation. To assess
retrieval quality, we define a domain-specific retrieval metric, under which
our retriever achieves approximately 97% accuracy. Beyond retrieval, our
approach consistently improves generation accuracy across all tested models. In
particular, while naively inserting documents without structured retrieval
yields only marginal gains for GPT-4o (less than 1%), applying our pipeline
results in nearly a 12% relative improvement. These findings demonstrate that
carefully targeted grounding provides a simple yet strong baseline and an
effective domain-specific solution for regulatory question answering. All code
and evaluation scripts, along with our derived question-answer dataset, are
available at https://github.com/Zakaria010/Radio-RAG.

</details>


### [270] [We're Still Doing It (All) Wrong: Recommender Systems, Fifteen Years Later](https://arxiv.org/abs/2509.09414)
*Alan Said,Maria Soledad Pera,Michael D. Ekstrand*

Main category: cs.IR

TL;DR: The paper revisits and critiques persisting issues in recommender systems research, advocating for a paradigm shift towards humility, human impact, and sustainability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address ongoing fundamental issues in recommender systems research, such as statistical misinterpretations and methodological shortcuts, which have lingered since Xavier Amatriain's 2011 critique.

Method: The authors examine current challenges by analyzing recent work on reproducibility, evaluation methodology, environmental impact, and participatory design, while also highlighting community initiatives and alternative research paradigms.

Result: The paper identifies persistent flaws in the field, demonstrates how complexity has outpaced introspection, and showcases various community efforts attempting to propose solutions.

Conclusion: The authors argue for a systemic reframing of recommender systems research that centers on epistemic humility, human impact, and sustainable practices rather than solely technical fixes or improvements.

Abstract: In 2011, Xavier Amatriain sounded the alarm: recommender systems research was
"doing it all wrong" [1]. His critique, rooted in statistical misinterpretation
and methodological shortcuts, remains as relevant today as it was then. But
rather than correcting course, we added new layers of sophistication on top of
the same broken foundations. This paper revisits Amatriain's diagnosis and
argues that many of the conceptual, epistemological, and infrastructural
failures he identified still persist, in more subtle or systemic forms. Drawing
on recent work in reproducibility, evaluation methodology, environmental
impact, and participatory design, we showcase how the field's accelerating
complexity has outpaced its introspection. We highlight ongoing community-led
initiatives that attempt to shift the paradigm, including workshops, evaluation
frameworks, and calls for value-sensitive and participatory research. At the
same time, we contend that meaningful change will require not only new metrics
or better tooling, but a fundamental reframing of what recommender systems
research is for, who it serves, and how knowledge is produced and validated.
Our call is not just for technical reform, but for a recommender systems
research agenda grounded in epistemic humility, human impact, and sustainable
practice.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [271] [WarpPINN-fibers: improved cardiac strain estimation from cine-MR with physics-informed neural networks](https://arxiv.org/abs/2509.08872)
*Felipe Álvarez Barrientos,Tomás Banduc,Isabeau Sirven,Francisco Sahli Costabal*

Main category: eess.IV

TL;DR: The paper introduces WarpPINN-fibers, a neural network model integrating cardiac fiber information to enhance strain analysis from MRI images.


<details>
  <summary>Details</summary>
Motivation: Current methods for cardiac strain analysis fail to incorporate fiber mechanics, limiting their ability to accurately describe heart function and related pathologies.

Method: The authors developed WarpPINN-fibers, combining a physics-informed neural network framework trained using a hyper-elastic model, cine MRI images, and a loss function promoting fiber contraction.

Result: WarpPINN-fibers improved upon previous WarpPINN models, successfully controlled fiber stretch in synthetic tests, and outperformed other methods in real MRI dataset evaluations.

Conclusion: WarpPINN-fibers offers a promising tool for precise cardiac strain quantification that aligns with fiber physiology, using accessible MRI imaging techniques.

Abstract: The contractile motion of the heart is strongly determined by the
distribution of the fibers that constitute cardiac tissue. Strain analysis
informed with the orientation of fibers allows to describe several pathologies
that are typically associated with impaired mechanics of the myocardium, such
as cardiovascular disease. Several methods have been developed to estimate
strain-derived metrics from traditional imaging techniques. However, the
physical models underlying these methods do not include fiber mechanics,
restricting their capacity to accurately explain cardiac function. In this
work, we introduce WarpPINN-fibers, a physics-informed neural network framework
to accurately obtain cardiac motion and strains enhanced by fiber information.
We train our neural network to satisfy a hyper-elastic model and promote fiber
contraction with the goal to predict the deformation field of the heart from
cine magnetic resonance images. For this purpose, we build a loss function
composed of three terms: a data-similarity loss between the reference and the
warped template images, a regularizer enforcing near-incompressibility of
cardiac tissue and a fiber-stretch penalization that controls strain in the
direction of synthetically produced fibers. We show that our neural network
improves the former WarpPINN model and effectively controls fiber stretch in a
synthetic phantom experiment. Then, we demonstrate that WarpPINN-fibers
outperforms alternative methodologies in landmark-tracking and strain curve
prediction for a cine-MRI benchmark with a cohort of 15 healthy volunteers. We
expect that our method will enable a more precise quantification of cardiac
strains through accurate deformation fields that are consistent with fiber
physiology, without requiring imaging techniques more sophisticated than MRI.

</details>


### [272] [Virtual staining for 3D X-ray histology of bone implants](https://arxiv.org/abs/2509.09235)
*Sarah C. Irvine,Christian Lucas,Diana Krüger,Bianca Guedert,Julian Moosmann,Berit Zeller-Plumhoff*

Main category: eess.IV

TL;DR: The authors propose a deep learning-based virtual staining method to enhance biochemical specificity in 3D X-ray histology using synchrotron-radiation micro-CT.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitation of conventional X-ray tomography, which lacks biochemical specificity compared to traditional histological staining methods.

Method: The authors used a modified CycleGAN for cross-modality image translation, applying pixelwise supervision and greyscale consistency terms for virtual staining of X-ray micro-CT scans.

Result: The method demonstrated superior performance against baseline models (Pix2Pix and standard CycleGAN) across metrics like SSIM, PSNR, and LPIPS.

Conclusion: Virtual staining offers scalable, chemically informative, label-free tissue characterization that enhances interpretability of 3D X-ray imaging for biomedical research.

Abstract: Three-dimensional X-ray histology techniques offer a non-invasive alternative
to conventional 2D histology, enabling volumetric imaging of biological tissues
without the need for physical sectioning or chemical staining. However, the
inherent greyscale image contrast of X-ray tomography limits its biochemical
specificity compared to traditional histological stains. Within digital
pathology, deep learning-based virtual staining has demonstrated utility in
simulating stained appearances from label-free optical images. In this study,
we extend virtual staining to the X-ray domain by applying cross-modality image
translation to generate artificially stained slices from
synchrotron-radiation-based micro-CT scans. Using over 50 co-registered image
pairs of micro-CT and toluidine blue-stained histology from bone-implant
samples, we trained a modified CycleGAN network tailored for limited paired
data. Whole slide histology images were downsampled to match the voxel size of
the CT data, with on-the-fly data augmentation for patch-based training. The
model incorporates pixelwise supervision and greyscale consistency terms,
producing histologically realistic colour outputs while preserving
high-resolution structural detail. Our method outperformed Pix2Pix and standard
CycleGAN baselines across SSIM, PSNR, and LPIPS metrics. Once trained, the
model can be applied to full CT volumes to generate virtually stained 3D
datasets, enhancing interpretability without additional sample preparation.
While features such as new bone formation were able to be reproduced, some
variability in the depiction of implant degradation layers highlights the need
for further training data and refinement. This work introduces virtual staining
to 3D X-ray imaging and offers a scalable route for chemically informative,
label-free tissue characterisation in biomedical research.

</details>


### [273] [Dynamic Structural Recovery Parameters Enhance Prediction of Visual Outcomes After Macular Hole Surgery](https://arxiv.org/abs/2509.09227)
*Yinzheng Zhao,Zhihao Zhao,Rundong Jiang,Louisa Sackewitz,Quanmin Liang,Mathias Maier,Daniel Zapp,Peter Charbel Issa,Mohammad Ali Nasseri*

Main category: eess.IV

TL;DR: This study introduces dynamic structural parameters and their integration into a deep learning model, which significantly improves predictions of visual recovery in macular hole patients post-surgery.


<details>
  <summary>Details</summary>
Motivation: To develop better tools for predicting postoperative visual outcomes in macular hole patients by integrating structural dynamics and machine learning.

Method: Longitudinal OCT data was analyzed using segmentation, feature extraction pipelines, logistic regression models, and a multimodal DL framework combining clinical data, derived features, and raw images.

Result: Dynamic parameters enhanced predictive accuracy in models, achieving high correlation with visual recovery metrics. The multimodal DL model outperformed logistic regression in accuracy and AUC, showing superior prediction capabilities.

Conclusion: Dynamic structural parameters improve deep learning model predictions, offering a promising fully automated tool for macular hole surgery postoperative management.

Abstract: Purpose: To introduce novel dynamic structural parameters and evaluate their
integration within a multimodal deep learning (DL) framework for predicting
postoperative visual recovery in idiopathic full-thickness macular hole (iFTMH)
patients. Methods: We utilized a publicly available longitudinal OCT dataset at
five stages (preoperative, 2 weeks, 3 months, 6 months, and 12 months). A stage
specific segmentation model delineated related structures, and an automated
pipeline extracted quantitative, composite, qualitative, and dynamic features.
Binary logistic regression models, constructed with and without dynamic
parameters, assessed their incremental predictive value for best-corrected
visual acuity (BCVA). A multimodal DL model combining clinical variables,
OCT-derived features, and raw OCT images was developed and benchmarked against
regression models. Results: The segmentation model achieved high accuracy
across all timepoints (mean Dice > 0.89). Univariate and multivariate analyses
identified base diameter, ellipsoid zone integrity, and macular hole area as
significant BCVA predictors (P < 0.05). Incorporating dynamic recovery rates
consistently improved logistic regression AUC, especially at the 3-month
follow-up. The multimodal DL model outperformed logistic regression, yielding
higher AUCs and overall accuracy at each stage. The difference is as high as
0.12, demonstrating the complementary value of raw image volume and dynamic
parameters. Conclusions: Integrating dynamic parameters into the multimodal DL
model significantly enhances the accuracy of predictions. This fully automated
process therefore represents a promising clinical decision support tool for
personalized postoperative management in macular hole surgery.

</details>


### [274] [In-Loop Filtering Using Learned Look-Up Tables for Video Coding](https://arxiv.org/abs/2509.09494)
*Zhuoyuan Li,Jiacheng Li,Yao Li,Jialin Li,Li Li,Dong Liu,Feng Wu*

Main category: eess.IV

TL;DR: This paper introduces LUT-ILF++, a low-complexity, neural network-inspired method using look-up tables to improve in-loop filtering in video coding, achieving significant bitrate reduction while maintaining low computational and storage costs.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of DNN-based ILF solutions in video coding that have high computational complexity and hardware demands, making them unsuitable for general use.

Method: The proposed LUT-ILF++ framework replaces heavy DNN-based computations with look-up tables, leveraging mechanisms like multiple LUT cooperation, customized indexing, cross-component indexing, and LUT compaction to balance storage use and performance.

Result: The implementation in the VVC reference software shows average bitrate reductions of 0.82%/2.97%/1.63% (AI configuration) and 0.85%/4.11%/2.06% (RA configuration) on test sequences, with significantly lower complexity and storage requirements compared to DNN solutions.

Conclusion: By leveraging LUTs and innovative indexing mechanisms, LUT-ILF++ offers a practical and efficient solution for ILF in video coding, balancing quality improvement, computational efficiency, and storage usage.

Abstract: In-loop filtering (ILF) is a key technology in video coding standards to
reduce artifacts and enhance visual quality. Recently, neural network-based ILF
schemes have achieved remarkable coding gains, emerging as a powerful candidate
for next-generation video coding standards. However, the use of deep neural
networks (DNN) brings significant computational and time complexity or high
demands for dedicated hardware, making it challenging for general use. To
address this limitation, we study a practical ILF solution by adopting look-up
tables (LUTs). After training a DNN with a restricted reference range for ILF,
all possible inputs are traversed, and the output values of the DNN are cached
into LUTs. During the coding process, the filtering process is performed by
simply retrieving the filtered pixel through locating the input pixels and
interpolating between the cached values, instead of relying on heavy inference
computations. In this paper, we propose a universal LUT-based ILF framework,
termed LUT-ILF++. First, we introduce the cooperation of multiple kinds of
filtering LUTs and propose a series of customized indexing mechanisms to enable
better filtering reference perception with limited storage consumption. Second,
we propose the cross-component indexing mechanism to enable the filtering of
different color components jointly. Third, in order to make our solution
practical for coding uses, we propose the LUT compaction scheme to enable the
LUT pruning, achieving a lower storage cost of the entire solution. The
proposed framework is implemented in the VVC reference software. Experimental
results show that the proposed framework achieves on average 0.82%/2.97%/1.63%
and 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI
and RA configurations, respectively. Compared to DNN-based solutions, our
proposed solution has much lower time complexity and storage cost.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [275] [Towards A High-Performance Quantum Data Center Network Architecture](https://arxiv.org/abs/2509.09653)
*Yufeng Xin,Liang Zhang*

Main category: quant-ph

TL;DR: This paper introduces a scalable and efficient three-layer fat-tree network architecture for modular Quantum Data Centers (QDCs), tackling challenges such as entanglement generation and memory management.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges posed by the modular approach in Quantum Data Centers, namely network scalability, entanglement generation, and quantum memory management.

Method: The authors propose a three-layer fat-tree network architecture, featuring a customized leaf switch and advanced swapping spine switch designs, along with a queue scheduling mechanism for quantum memory management. They validate the design using queuing-theoretical models and NetSquid simulations.

Result: The proposed architecture was demonstrated to be scalable and effective in maintaining high entanglement fidelity while handling high volumes of entanglement requests.

Conclusion: The work provides a practical solution for modular QDC networks, showing that their architecture can effectively tackle inherent challenges and pave the way for scalable quantum processing systems.

Abstract: Quantum Data Centers (QDCs) are needed to support large-scale quantum
processing for both academic and commercial applications. While large-scale
quantum computers are constrained by technological and financial barriers, a
modular approach that clusters small quantum computers offers an alternative.
This approach, however, introduces new challenges in network scalability,
entanglement generation, and quantum memory management. In this paper, we
propose a three-layer fat-tree network architecture for QDCs, designed to
address these challenges. Our architecture features a unique leaf switch and an
advanced swapping spine switch design, optimized to handle high volumes of
entanglement requests as well as a queue scheduling mechanism that efficiently
manages quantum memory to prevent decoherence. Through queuing-theoretical
models and simulations in NetSquid, we demonstrate the proposed architecture's
scalability and effectiveness in maintaining high entanglement fidelity,
offering a practical path forward for modular QDC networks.

</details>


### [276] [Generative quantum advantage for classical and quantum problems](https://arxiv.org/abs/2509.09033)
*Hsin-Yuan Huang,Michael Broughton,Norhan Eassa,Hartmut Neven,Ryan Babbush,Jarrod R. McClean*

Main category: quant-ph

TL;DR: This paper introduces a generative quantum model that shows substantial advantages over classical systems, validated on a 68-qubit quantum processor.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in demonstrating generative quantum advantage due to complexity in efficient learning and simulation.

Method: The paper presents a family of generative quantum models that are classically hard to simulate, efficiently trainable, and free from barren plateaus or local minima issues. Validation was done using a 68-qubit superconducting quantum processor.

Result: The proposed models successfully learned classically intractable probability distributions and quantum circuits for enhanced physical simulations, demonstrating quantum advantages.

Conclusion: The study establishes practical and theoretical advances in generative quantum models, proving efficient learning and sampling in the beyond-classical regime and providing future opportunities for quantum-enhanced systems.

Abstract: Recent breakthroughs in generative machine learning, powered by massive
computational resources, have demonstrated unprecedented human-like
capabilities. While beyond-classical quantum experiments can generate samples
from classically intractable distributions, their complexity has thwarted all
efforts toward efficient learning. This challenge has hindered demonstrations
of generative quantum advantage: the ability of quantum computers to learn and
generate desired outputs substantially better than classical computers. We
resolve this challenge by introducing families of generative quantum models
that are hard to simulate classically, are efficiently trainable, exhibit no
barren plateaus or proliferating local minima, and can learn to generate
distributions beyond the reach of classical computers. Using a $68$-qubit
superconducting quantum processor, we demonstrate these capabilities in two
scenarios: learning classically intractable probability distributions and
learning quantum circuits for accelerated physical simulation. Our results
establish that both learning and sampling can be performed efficiently in the
beyond-classical regime, opening new possibilities for quantum-enhanced
generative models with provable advantage.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [277] [Expressive Power of Deep Networks on Manifolds: Simultaneous Approximation](https://arxiv.org/abs/2509.09362)
*Hanfei Zhou,Lei Shi*

Main category: math.NA

TL;DR: The paper introduces a theoretical framework for approximating functions on manifolds with deep neural networks, overcoming the challenge of curved geometries in PDEs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of solving PDEs on complex domains where curved geometries complicate function approximation and the computation of derivatives.

Method: The authors establish an approximation theory for ReLU-based neural networks, proving their capability to approximate Sobolev space functions efficiently, using sparse network structures. They also provide lower bounds and analyze complexity measures like VC and pseudo-dimension.

Result: The study demonstrates that these neural networks efficiently approximate functions with limited parameters, leveraging intrinsic low-dimensional manifold geometry while providing nearly optimal constructions.

Conclusion: This theoretical foundation enhances the understanding of deep neural networks' efficiency in PDE learning on manifolds, with implications for scientific machine learning in complex domains.

Abstract: A key challenge in scientific machine learning is solving partial
differential equations (PDEs) on complex domains, where the curved geometry
complicates the approximation of functions and their derivatives required by
differential operators. This paper establishes the first simultaneous
approximation theory for deep neural networks on manifolds. We prove that a
constant-depth $\mathrm{ReLU}^{k-1}$ network with bounded weights--a property
that plays a crucial role in controlling generalization error--can approximate
any function in the Sobolev space $\mathcal{W}_p^{k}(\mathcal{M}^d)$ to an
error of $\varepsilon$ in the $\mathcal{W}_p^{s}(\mathcal{M}^d)$ norm, for
$k\geq 3$ and $s<k$, using $\mathcal{O}(\varepsilon^{-d/(k-s)})$ nonzero
parameters, a rate that overcomes the curse of dimensionality by depending only
on the intrinsic dimension $d$. These results readily extend to functions in
H\"older-Zygmund spaces. We complement this result with a matching lower bound,
proving our construction is nearly optimal by showing the required number of
parameters matches up to a logarithmic factor. Our proof of the lower bound
introduces novel estimates for the Vapnik-Chervonenkis dimension and
pseudo-dimension of the network's high-order derivative classes. These
complexity bounds provide a theoretical cornerstone for learning PDEs on
manifolds involving derivatives. Our analysis reveals that the network
architecture leverages a sparse structure to efficiently exploit the manifold's
low-dimensional geometry.

</details>


### [278] [DeepTV: A neural network approach for total variation minimization](https://arxiv.org/abs/2409.05569)
*Andreas Langer,Sara Behnamian*

Main category: math.NA

TL;DR: This paper explores solving infinite-dimensional total variation minimization problems using neural networks, addressing the lack of solution by proposing an auxiliary and discrete problem approach, supported by theoretical and numerical findings.


<details>
  <summary>Details</summary>
Motivation: Existing neural network methods perform well for solving partial differential equations, and this work seeks to extend such approaches to infinite-dimensional total variation minimization problems, which present theoretical and computational challenges.

Method: The authors propose an auxiliary neural network problem to bypass the lack of solutions in the original setup. They prove its convergence to the original problem through $\Gamma$-convergence and further design a discrete version for computational efficiency, ensuring this discrete problem matches theoretical requirements.

Result: The paper demonstrates $\Gamma$-convergence for both the auxiliary and discrete neural network problems to the original. Furthermore, it finds connections between the discrete problem and traditional finite difference methods for the infinite-dimensional case.

Conclusion: Neural networks can effectively approximate infinite-dimensional total variation minimization problems, provided the issues with solution existence and computation are addressed via auxiliary formulations and discretization. The results bridge theoretical rigor and practical computation.

Abstract: Neural network approaches have been demonstrated to work quite well to solve
partial differential equations in practice. In this context approaches like
physics-informed neural networks and the Deep Ritz method have become popular.
In this paper, we propose a similar approach to solve an infinite-dimensional
total variation minimization problem using neural networks. We illustrate that
the resulting neural network problem does not have a solution in general. To
circumvent this theoretic issue, we consider an auxiliary neural network
problem, which indeed has a solution, and show that it converges in the sense
of $\Gamma$-convergence to the original problem. For computing a numerical
solution we further propose a discrete version of the auxiliary neural network
problem and again show its $\Gamma$-convergence to the original
infinite-dimensional problem. In particular, the $\Gamma$-convergence proof
suggests a particular discretization of the total variation. Moreover, we
connect the discrete neural network problem to a finite difference
discretization of the infinite-dimensional total variation minimization
problem. Numerical experiments are presented supporting our theoretical
findings.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [279] [Representation-Aware Distributionally Robust Optimization: A Knowledge Transfer Framework](https://arxiv.org/abs/2509.09371)
*Zitao Wang,Nian Si,Molei Liu*

Main category: stat.ME

TL;DR: READ is a Wasserstein distributionally robust learning framework that incorporates predictive representations for handling distributional shifts, offering robust estimation while maintaining invariant structure.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of classical DRO approaches that treat all feature perturbations equally, and to create a framework that leverages predictive representations for improved robustness in handling distributional shifts.

Method: READ embeds an alignment parameter into the transport cost to differentially handle perturbations along informative representations. It reformulates these objectives into tractable regularized estimators and applies Wasserstein profile inference for radius selection.

Result: The paper introduces READ's theoretical foundation, develops a principled method for selecting Wasserstein radius, analyzes estimator geometry with varying alignment parameters, and proposes an optimization algorithm for robust solution selection.

Conclusion: The framework demonstrates effectiveness in simulations and real-world applications, offering robust estimation that captures representation structures and preserves invariant features.

Abstract: We propose REpresentation-Aware Distributionally Robust Estimation (READ), a
novel framework for Wasserstein distributionally robust learning that accounts
for predictive representations when guarding against distributional shifts.
Unlike classical approaches that treat all feature perturbations equally, READ
embeds a multidimensional alignment parameter into the transport cost, allowing
the model to differentially discourage perturbations along directions
associated with informative representations. This yields robustness to feature
variation while preserving invariant structure. Our first contribution is a
theoretical foundation: we show that seminorm regularizations for linear
regression and binary classification arise as Wasserstein distributionally
robust objectives, thereby providing tractable reformulations of READ and
unifying a broad class of regularized estimators under the DRO lens. Second, we
adopt a principled procedure for selecting the Wasserstein radius using the
techniques of robust Wasserstein profile inference. This further enables the
construction of valid, representation-aware confidence regions for model
parameters with distinct geometric features. Finally, we analyze the geometry
of READ estimators as the alignment parameters vary and propose an optimization
algorithm to estimate the projection of the global optimum onto this solution
surface. This procedure selects among equally robust estimators while optimally
constructing a representation structure. We conclude by demonstrating the
effectiveness of our framework through extensive simulations and a real-world
study, providing a powerful robust estimation grounded in learning
representation.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [280] [The Role of Community Detection Methods in Performance Variations of Graph Mining Tasks](https://arxiv.org/abs/2509.09045)
*Shrabani Ghosh,Erik Saule*

Main category: cs.SI

TL;DR: The paper studies the impact of community detection algorithm selection on downstream graph mining tasks and provides a framework for systematic evaluation.


<details>
  <summary>Details</summary>
Motivation: Understanding how community detection methods influence the performance of graph mining tasks is crucial, especially in scenarios where no ground truth community information exists.

Method: The authors propose a framework to integrate various community detection methods, systematically assess them, and conduct a comparative analysis for downstream tasks like link prediction and node classification.

Result: They discover that the choice of community detection algorithm significantly impacts the performance of downstream tasks, with certain methods showing better suitability for specific applications.

Conclusion: Method selection in community detection critically affects downstream applications; thus, careful evaluation and tailored selection are recommended.

Abstract: In real-world scenarios, large graphs represent relationships among entities
in complex systems. Mining these large graphs often containing millions of
nodes and edges helps uncover structural patterns and meaningful insights.
Dividing a large graph into smaller subgraphs facilitates complex system
analysis by revealing local information. Community detection extracts clusters
or communities of graphs based on statistical methods and machine learning
models using various optimization techniques. Structure based community
detection methods are more suitable for applying to graphs because they do not
rely heavily on rich node or edge attribute information. The features derived
from these communities can improve downstream graph mining tasks, such as link
prediction and node classification. In real-world applications, we often lack
ground truth community information. Additionally, there is neither a
universally accepted gold standard for community detection nor a single method
that is consistently optimal across diverse applications. In many cases, it is
unclear how practitioners select community detection methods, and choices are
often made without explicitly considering their potential impact on downstream
tasks. In this study, we investigate whether the choice of community detection
algorithm significantly influences the performance of downstream applications.
We propose a framework capable of integrating various community detection
methods to systematically evaluate their effects on downstream task outcomes.
Our comparative analysis reveals that specific community detection algorithms
yield superior results in certain applications, highlighting that method
selection substantially affects performance.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [281] [Physics-informed waveform inversion using pretrained wavefield neural operators](https://arxiv.org/abs/2509.08967)
*Xinquan Huang,Fu Wang,Tariq Alkhalifah*

Main category: physics.geo-ph

TL;DR: The paper introduces a physics-informed framework for Full Waveform Inversion (FWI) that enhances accuracy and efficiency using neural operators and physics constraints.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of traditional FWI methods, such as high computational costs and noisy reconstructions, while maintaining or improving model accuracy and efficiency.

Method: The authors propose integrating a physics constraint into the loss function of neural operator-based FWI to reduce noise and enhance model accuracy, replacing reliance solely on the L2 norm objective function.

Result: Numerical experiments on benchmark datasets, such as OpenFWI and Overthrust models, show that the new method achieves cleaner and more accurate subsurface velocity reconstructions than conventional FWI methods.

Conclusion: This physics-informed FWI framework offers a significant improvement in both accuracy and computational efficiency, making it a viable approach for real-time subsurface monitoring.

Abstract: Full waveform inversion (FWI) is crucial for reconstructing high-resolution
subsurface models, but it is often hindered, considering the limited data, by
its null space resulting in low-resolution models, and more importantly, by its
computational cost, especially if needed for real-time applications. Recent
attempts to accelerate FWI using learned wavefield neural operators have shown
promise in efficiency and differentiability, but typically suffer from noisy
and unstable inversion performance. To address these limitations, we introduce
a novel physics-informed FWI framework to enhance the inversion in accuracy
while maintaining the efficiency of neural operator-based FWI. Instead of
relying only on the L2 norm objective function via automatic differentiation,
resulting in noisy model reconstruction, we integrate a physics constraint term
in the loss function of FWI, improving the quality of the inverted velocity
models. Specifically, starting with an initial model to simulate wavefields and
then evaluating the loss over how much the resulting wavefield obeys the
physical laws (wave equation) and matches the recorded data, we achieve a
reduction in noise and artifacts. Numerical experiments using the OpenFWI and
Overthrust models demonstrate our method's superior performance, offering
cleaner and more accurate subsurface velocity than vanilla approaches.
Considering the efficiency of the approach compared to FWI, this advancement
represents a significant step forward in the practical application of FWI for
real-time subsurface monitoring.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [282] [Towards Confidential and Efficient LLM Inference with Dual Privacy Protection](https://arxiv.org/abs/2509.09091)
*Honglan Yu,Yibin Wang,Feifei Dai,Dong Liu,Haihui Fan,Xiaoyan Gu*

Main category: cs.CR

TL;DR: CMIF, a confidential model inference framework, addresses TEE latency and DP-induced performance losses in private inference by intelligently partitioning layers and optimizing noise mechanisms.


<details>
  <summary>Details</summary>
Motivation: To improve privacy-preserving inference by reducing latency in TEEs and overcoming performance degradation caused by DP noise.

Method: Deploy embedding layers in client-side TEE, offload subsequent layers to GPU servers, and optimize Report-Noisy-Max mechanism for privacy.

Result: CMIF successfully reduces TEE inference latency while maintaining better data privacy and user data protection than existing approaches.

Conclusion: CMIF provides an efficient solution balancing model performance, data privacy, and inference latency for large language model deployments.

Abstract: CPU-based trusted execution environments (TEEs) and differential privacy (DP)
have gained wide applications for private inference. Due to high inference
latency in TEEs, researchers use partition-based approaches that offload linear
model components to GPUs. However, dense nonlinear layers of large language
models (LLMs) result in significant communication overhead between TEEs and
GPUs. DP-based approaches apply random noise to protect data privacy, but this
compromises LLM performance and semantic understanding. To overcome the above
drawbacks, this paper proposes CMIF, a Confidential and efficient Model
Inference Framework. CMIF confidentially deploys the embedding layer in the
client-side TEE and subsequent layers on GPU servers. Meanwhile, it optimizes
the Report-Noisy-Max mechanism to protect sensitive inputs with a slight
decrease in model performance. Extensive experiments on Llama-series models
demonstrate that CMIF reduces additional inference overhead in TEEs while
preserving user data privacy.

</details>


### [283] [DP-FedLoRA: Privacy-Enhanced Federated Fine-Tuning for On-Device Large Language Models](https://arxiv.org/abs/2509.09097)
*Honghui Xu,Shiva Shrestha,Wei Chen,Zhiyuan Li,Zhipeng Cai*

Main category: cs.CR

TL;DR: DP-FedLoRA is a framework combining LoRA-based adaptation and differential privacy to enable federated fine-tuning of large language models on devices, ensuring privacy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Federated fine-tuning for on-device large language models raises privacy concerns due to user-specific sensitive data being processed, necessitating privacy-preserving methods.

Method: DP-FedLoRA applies differential privacy by perturbing locally updated LoRA matrices with Gaussian noise, ensuring ($\epsilon$, $\delta$)-differential privacy. It uses a communication-efficient framework and includes theoretical analysis for unbiased updates and noise variance control.

Result: The framework achieves competitive performance on standard benchmarks while maintaining strong privacy guarantees.

Conclusion: DP-FedLoRA offers a scalable, privacy-preserving solution for deploying advanced language models on edge devices, showing its effectiveness through experiments and theoretical grounding.

Abstract: As on-device large language model (LLM) systems become increasingly
prevalent, federated fine-tuning enables advanced language understanding and
generation directly on edge devices; however, it also involves processing
sensitive, user-specific data, raising significant privacy concerns within the
federated learning framework. To address these challenges, we propose
DP-FedLoRA, a privacy-enhanced federated fine-tuning framework that integrates
LoRA-based adaptation with differential privacy in a communication-efficient
setting. Each client locally clips and perturbs its LoRA matrices using
Gaussian noise to satisfy ($\epsilon$, $\delta$)-differential privacy. We
further provide a theoretical analysis demonstrating the unbiased nature of the
updates and deriving bounds on the variance introduced by noise, offering
practical guidance for privacy-budget calibration. Experimental results across
mainstream benchmarks show that DP-FedLoRA delivers competitive performance
while offering strong privacy guarantees, paving the way for scalable and
privacy-preserving LLM deployment in on-device environments.

</details>


### [284] [Character-Level Perturbations Disrupt LLM Watermarks](https://arxiv.org/abs/2509.09112)
*Zhaoxi Zhang,Xiaomei Zhang,Yanjun Zhang,He Zhang,Shirui Pan,Bo Liu,Asif Qumer Gill,Leo Yu Zhang*

Main category: cs.CR

TL;DR: The paper explores vulnerabilities in existing watermarking methods for Large Language Models (LLMs), emphasizing the effectiveness of character-level perturbations and proposing adaptive attack strategies for watermark removal.


<details>
  <summary>Details</summary>
Motivation: The study seeks to address misconceptions around watermark removal in LLM-generated text, aiming to understand realistic threat models and to explore effective removal techniques under constrained settings.

Method: The researchers formalize LLM watermark threat models, analyze perturbation types (focusing on character-level disruptions), and propose Genetic Algorithm-based guided attacks and adaptive compound strategies to test vulnerabilities and bypass defenses.

Result: Character-level perturbations were found to be highly effective in disrupting LLM watermarks, especially under constrained threat models. The guided attacks using Genetic Algorithms showed strong performance, even with limited access to watermark detectors.

Conclusion: Existing LLM watermark systems are vulnerable to character-level attacks, highlighting the critical need for developing more robust watermarking mechanisms and adaptive defenses.

Abstract: Large Language Model (LLM) watermarking embeds detectable signals into
generated text for copyright protection, misuse prevention, and content
detection. While prior studies evaluate robustness using watermark removal
attacks, these methods are often suboptimal, creating the misconception that
effective removal requires large perturbations or powerful adversaries.
  To bridge the gap, we first formalize the system model for LLM watermark, and
characterize two realistic threat models constrained on limited access to the
watermark detector. We then analyze how different types of perturbation vary in
their attack range, i.e., the number of tokens they can affect with a single
edit. We observe that character-level perturbations (e.g., typos, swaps,
deletions, homoglyphs) can influence multiple tokens simultaneously by
disrupting the tokenization process. We demonstrate that character-level
perturbations are significantly more effective for watermark removal under the
most restrictive threat model. We further propose guided removal attacks based
on the Genetic Algorithm (GA) that uses a reference detector for optimization.
Under a practical threat model with limited black-box queries to the watermark
detector, our method demonstrates strong removal performance. Experiments
confirm the superiority of character-level perturbations and the effectiveness
of the GA in removing watermarks under realistic constraints. Additionally, we
argue there is an adversarial dilemma when considering potential defenses: any
fixed defense can be bypassed by a suitable perturbation strategy. Motivated by
this principle, we propose an adaptive compound character-level attack.
Experimental results show that this approach can effectively defeat the
defenses. Our findings highlight significant vulnerabilities in existing LLM
watermark schemes and underline the urgency for the development of new robust
mechanisms.

</details>


### [285] [CryptGNN: Enabling Secure Inference for Graph Neural Networks](https://arxiv.org/abs/2509.09107)
*Pritam Sen,Yao Ma,Cristian Borcea*

Main category: cs.CR

TL;DR: CryptGNN is a solution for secure inference of graph neural networks in the cloud, ensuring privacy using secure multi-party computation (SMPC) techniques.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for privacy-preserving machine learning services for graph neural networks, where secure inference is necessary to protect clients' data, graph structures, and third-party models in cloud environments.

Method: CryptGNN uses distributed secure multi-party computation (SMPC) techniques to achieve secure message passing and feature transformation. It operates without requiring a trusted server and ensures security even in cases of collusion by up to P-1 parties in the cloud.

Result: Theoretical and empirical evaluations demonstrate that CryptGNN is both secure and efficient.

Conclusion: CryptGNN enables secure and effective cloud-based GNN inference as an MLaaS, protecting both client data and third-party models while maintaining computational efficiency.

Abstract: We present CryptGNN, a secure and effective inference solution for
third-party graph neural network (GNN) models in the cloud, which are accessed
by clients as ML as a service (MLaaS). The main novelty of CryptGNN is its
secure message passing and feature transformation layers using distributed
secure multi-party computation (SMPC) techniques. CryptGNN protects the
client's input data and graph structure from the cloud provider and the
third-party model owner, and it protects the model parameters from the cloud
provider and the clients. CryptGNN works with any number of SMPC parties, does
not require a trusted server, and is provably secure even if P-1 out of P
parties in the cloud collude. Theoretical analysis and empirical experiments
demonstrate the security and efficiency of CryptGNN.

</details>


### [286] [ENSI: Efficient Non-Interactive Secure Inference for Large Language Models](https://arxiv.org/abs/2509.09424)
*Zhiyu He,Maojiang Wang,Xinwen Gao,Yuchuan Luo,Lin Liu,Shaojing Fu*

Main category: cs.CR

TL;DR: ENSI is a novel framework that combines cryptographic protocols and redesigned LLM architectures to enable faster, non-interactive secure inference.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges in integrating secure cryptographic protocols with Large Language Models (LLMs), which often face limitations due to high computational costs and architectural complexity.

Method: ENSI proposes optimizations by co-designing the cryptographic protocols with a lightweight LLM architecture (BitNet), introducing techniques such as sigmoid attention and embedding bootstrapping into RMSNorm.

Result: Experiments demonstrate ENSI achieves an 8x acceleration in matrix multiplications and a 2.6x speedup in softmax operations, with bootstrapping reduced to only 1%.

Conclusion: ENSI makes significant progress in secure inference for LLMs by reducing computational overhead, thereby improving usability and efficiency while maintaining security.

Abstract: Secure inference enables privacy-preserving machine learning by leveraging
cryptographic protocols that support computations on sensitive user data
without exposing it. However, integrating cryptographic protocols with large
language models (LLMs) presents significant challenges, as the inherent
complexity of these protocols, together with LLMs' massive parameter scale and
sophisticated architectures, severely limits practical usability. In this work,
we propose ENSI, a novel non-interactive secure inference framework for LLMs,
based on the principle of co-designing the cryptographic protocols and LLM
architecture. ENSI employs an optimized encoding strategy that seamlessly
integrates CKKS scheme with a lightweight LLM variant, BitNet, significantly
reducing the computational complexity of encrypted matrix multiplications. In
response to the prohibitive computational demands of softmax under homomorphic
encryption (HE), we pioneer the integration of the sigmoid attention mechanism
with HE as a seamless, retraining-free alternative. Furthermore, by embedding
the Bootstrapping operation within the RMSNorm process, we efficiently refresh
ciphertexts while markedly decreasing the frequency of costly bootstrapping
invocations. Experimental evaluations demonstrate that ENSI achieves
approximately an 8x acceleration in matrix multiplications and a 2.6x speedup
in softmax inference on CPU compared to state-of-the-art method, with the
proportion of bootstrapping is reduced to just 1%.

</details>


### [287] [Prompt Pirates Need a Map: Stealing Seeds helps Stealing Prompts](https://arxiv.org/abs/2509.09488)
*Felix Mächtle,Ashwath Shetty,Jonas Sander,Nils Loose,Sören Pirk,Thomas Eisenbarth*

Main category: cs.CR

TL;DR: This paper analyzes prompt-stealing attacks in text-to-image diffusion models and exposes vulnerabilities related to random noise generation, proposing both an advanced attack method and countermeasures.


<details>
  <summary>Details</summary>
Motivation: Prompt theft, which poses intellectual property and security risks, is a critical concern in text-to-image diffusion models, requiring investigation and mitigation.

Method: The paper exposes a noise-generation vulnerability and introduces tools like SeedSnitch for seed recovery and PromptPirate, a genetic algorithm-based method for effective prompt stealing.

Result: Using SeedSnitch, approximately 95% of seed values were brute-forced in 140 minutes per seed, and PromptPirate outperformed existing methods with an 8-11% improvement in LPIPS similarity.

Conclusion: The findings highlight the need for robust countermeasures to address underlying vulnerabilities in diffusion models, and the authors propose effective steps to mitigate risks.

Abstract: Diffusion models have significantly advanced text-to-image generation,
enabling the creation of highly realistic images conditioned on textual prompts
and seeds. Given the considerable intellectual and economic value embedded in
such prompts, prompt theft poses a critical security and privacy concern. In
this paper, we investigate prompt-stealing attacks targeting diffusion models.
We reveal that numerical optimization-based prompt recovery methods are
fundamentally limited as they do not account for the initial random noise used
during image generation. We identify and exploit a noise-generation
vulnerability (CWE-339), prevalent in major image-generation frameworks,
originating from PyTorch's restriction of seed values to a range of $2^{32}$
when generating the initial random noise on CPUs. Through a large-scale
empirical analysis conducted on images shared via the popular platform CivitAI,
we demonstrate that approximately 95% of these images' seed values can be
effectively brute-forced in 140 minutes per seed using our seed-recovery tool,
SeedSnitch. Leveraging the recovered seed, we propose PromptPirate, a genetic
algorithm-based optimization method explicitly designed for prompt stealing.
PromptPirate surpasses state-of-the-art methods, i.e., PromptStealer, P2HP, and
CLIP-Interrogator, achieving an 8-11% improvement in LPIPS similarity.
Furthermore, we introduce straightforward and effective countermeasures that
render seed stealing, and thus optimization-based prompt stealing, ineffective.
We have disclosed our findings responsibly and initiated coordinated mitigation
efforts with the developers to address this critical vulnerability.

</details>


### [288] [What Does Normal Even Mean? Evaluating Benign Traffic in Intrusion Detection Datasets](https://arxiv.org/abs/2509.09564)
*Meghan Wilkinson,Robert H Thomson*

Main category: cs.CR

TL;DR: The paper reviews the structure of benign network traffic in intrusion detection datasets and explores using unsupervised clustering to identify meaningful sub-categories within this traffic.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by recognizing that labeled categories in intrusion detection datasets may oversimplify benign traffic, limiting the performance of supervised learning techniques.

Method: The authors evaluate intrusion datasets (NSL-KDD, UNSW-NB15, CIC-IDS 2017) and apply unsupervised clustering techniques like HDBSCAN and Mean Shift to examine the structure of benign traffic.

Result: Unsupervised clustering shows potential in identifying meaningful sub-categories within benign traffic, offering insights to improve multi-classification performance.

Conclusion: Accounting for nuanced benign traffic sub-categories can improve intrusion detection systems' performance by addressing the limitations of current labeling practices.

Abstract: Supervised machine learning techniques rely on labeled data to achieve high
task performance, but this requires the labels to capture some meaningful
differences in the underlying data structure. For training network intrusion
detection algorithms, most datasets contain a series of attack classes and a
single large benign class which captures all non-attack network traffic. A
review of intrusion detection papers and guides that explicitly state their
data preprocessing steps identified that the majority took the labeled
categories of the dataset at face value when training their algorithms. The
present paper evaluates the structure of benign traffic in several common
intrusion detection datasets (NSL-KDD, UNSW-NB15, and CIC-IDS 2017) and
determines whether there are meaningful sub-categories within this traffic
which may improve overall multi-classification performance using common machine
learning techniques. We present an overview of some unsupervised clustering
techniques (e.g., HDBSCAN, Mean Shift Clustering) and show how they
differentially cluster the benign traffic space.

</details>
