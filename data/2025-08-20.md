<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 41]
- [cs.AR](#cs.AR) [Total: 13]
- [cs.CL](#cs.CL) [Total: 36]
- [cs.CV](#cs.CV) [Total: 83]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.LG](#cs.LG) [Total: 75]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 23]
- [cs.SE](#cs.SE) [Total: 6]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [stat.ML](#stat.ML) [Total: 8]
- [cs.CC](#cs.CC) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 8]
- [eess.SY](#eess.SY) [Total: 3]
- [eess.IV](#eess.IV) [Total: 20]
- [cs.MA](#cs.MA) [Total: 2]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [cs.CY](#cs.CY) [Total: 6]
- [cs.IR](#cs.IR) [Total: 8]
- [cs.GT](#cs.GT) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.DB](#cs.DB) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [q-bio.MN](#q-bio.MN) [Total: 1]
- [cs.DL](#cs.DL) [Total: 2]
- [math.DG](#math.DG) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL](https://arxiv.org/abs/2508.13167)
*Weizhen Li,Jianbo Lin,Zhuosong Jiang,Jingyi Cao,Xinpeng Liu,Jiayu Zhang,Zhenqiang Huang,Qianben Chen,Weichen Sun,Qiexiang Wang,Hongxuan Lu,Tianrui Qin,Chenghao Zhu,Yi Yao,Shuying Fan,Xiaowan Li,Tiannan Wang,Pai Liu,King Zhu,He Zhu,Dingfeng Shi,Piaohong Wang,Yeyi Guan,Xiangru Tang,Minghao Liu,Yuchen Eleanor Jiang,Jian Yang,Jiaheng Liu,Ge Zhang,Wangchunshu Zhou*

Main category: cs.AI

TL;DR: Chain-of-Agents (CoA) introduces the concept of using large language models (LLMs) to mimic multi-agent system collaboration end-to-end for complex task solving.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent systems rely heavily on manual workflows and are less efficient compared to end-to-end problem-solving approaches.

Method: CoA utilizes a multi-agent distillation framework for supervised fine-tuning and reinforcement learning to enable LLMs to simulate multi-agent collaboration autonomously.

Result: Experiments show CoA achieves state-of-the-art performance across benchmarks in web and code agent settings.

Conclusion: Agent Foundation Models (AFMs) demonstrate that LLMs can be trained to simulate multi-agent collaboration effectively, paving the way for advanced agentic learning systems with open-sourced resources.

Abstract: Recent advances in large language models (LLMs) and multi-agent systems have
demonstrated remarkable capabilities in complex problem-solving tasks such as
deep research, vibe coding, and mathematical reasoning. However, most existing
multi-agent systems are built upon manual prompt/workflow engineering with
sophisticated agent frameworks, making them computationally inefficient, less
capable, and can not benefit from data-centric learning. In this work, we
introduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables
native end-to-end complex problem-solving in the same way as a multi-agent
system (i.e., multi-turn problem solving with multiple tools and multiple
agents) within one model. In chain-of-agents problem-solving, the model
dynamically activates different tool agents and role-playing agents to simulate
multi-agent collaboration in an end-to-end fashion. To elicit end-to-end
chain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent
distillation framework to distill state-of-the-art multi-agent systems into
chain-of-agents trajectories for agentic supervised fine-tuning. We then use
agentic reinforcement learning on verifiable agentic tasks to further improve
the models' capabilities on chain-of-agents problem solving. We call the
resulting models Agent Foundation Models (AFMs). Our empirical studies
demonstrate that AFM establishes new state-of-the-art performance across
diverse benchmarks in both web agent and code agent settings. We make the
entire research, including the model weights, code for training and evaluation,
and the training data, fully open-sourced, which offers a solid starting point
for future research on agent models and agentic RL.

</details>


### [2] [Cognitive Workspace: Active Memory Management for LLMs -- An Empirical Study of Functional Infinite Context](https://arxiv.org/abs/2508.13171)
*Tao An*

Main category: cs.AI

TL;DR: The paper introduces Cognitive Workspace, a paradigm enhancing LLM's context management by emulating human cognitive mechanisms of memory use, yielding superior performance over traditional retrieval methods.


<details>
  <summary>Details</summary>
Motivation: Despite advances in LLM's context expansion, limitations persist due to a lack of dynamic, task-driven memory management, analogous to human cognition.

Method: The Cognitive Workspace integrates active memory management, hierarchical buffers, and task-driven optimization, drawing on cognitive models to emulate human external memory use.

Result: Empirical tests show Cognitive Workspace improves memory reuse by 58.6% and achieves 17-18% efficiency gains, greatly outperforming traditional systems, with statistically significant results (p < 0.001, Cohen's d > 23).

Conclusion: Cognitive Workspace represents a paradigm shift from passive retrieval to cognitive augmentation, establishing a new framework and direction for LLM development.

Abstract: Large Language Models (LLMs) face fundamental limitations in context
management despite recent advances extending context windows to millions of
tokens. We propose Cognitive Workspace, a novel paradigm that transcends
traditional Retrieval-Augmented Generation (RAG) by emulating human cognitive
mechanisms of external memory use. Drawing from cognitive science foundations
including Baddeley's working memory model, Clark's extended mind thesis, and
Hutchins' distributed cognition framework, we demonstrate that current passive
retrieval systems fail to capture the dynamic, task-driven nature of human
memory management. Our analysis of 2024-2025 developments reveals that while
techniques like Infini-attention and StreamingLLM achieve impressive context
lengths, they lack the metacognitive awareness and active planning capabilities
essential for true cognitive extension. Cognitive Workspace addresses these
limitations through three core innovations: (1) active memory management with
deliberate information curation, (2) hierarchical cognitive buffers enabling
persistent working states, and (3) task-driven context optimization that
dynamically adapts to cognitive demands. Empirical validation demonstrates
Cognitive Workspace achieves an average 58.6% memory reuse rate (ranging from
54-60% across different tasks) compared to 0% for traditional RAG, with 17-18%
net efficiency gain despite 3.3x higher operation counts. Statistical analysis
confirms these advantages with p < 0.001 and Cohen's d > 23 across multiple
task types, establishing the first quantitative evidence for active memory
superiority in LLM systems. We present a comprehensive theoretical framework
synthesizing insights from 50+ recent papers, positioning Cognitive Workspace
as a fundamental shift from information retrieval to genuine cognitive
augmentation.

</details>


### [3] [AlphaEval: A Comprehensive and Efficient Evaluation Framework for Formula Alpha Mining](https://arxiv.org/abs/2508.13174)
*Hongjun Ding,Binqi Chen,Jinsheng Huang,Taian Guo,Zhengyang Mao,Guoyi Shao,Lutong Zou,Luchen Liu,Ming Zhang*

Main category: cs.AI

TL;DR: The paper introduces AlphaEval, a framework for evaluating financial predictive signals, addressing limitations in traditional evaluation methods like backtesting and correlation-based metrics.


<details>
  <summary>Details</summary>
Motivation: Existing alpha evaluation metrics are limited by inefficiencies, narrow focus, and lack of reproducibility, hindering the advancement of alpha mining for financial data.

Method: AlphaEval is a unified, parallelizable framework that evaluates alphas without backtesting using five dimensions: predictive power, stability, robustness, financial logic, and diversity.

Result: AlphaEval shows consistency with backtesting, provides more insightful evaluations, and efficiently identifies superior alpha signals across diverse algorithms.

Conclusion: AlphaEval improves alpha evaluation processes, balances efficiency and insight, and is open-sourced to promote reproducibility within the quantitative investment community.

Abstract: Formula alpha mining, which generates predictive signals from financial data,
is critical for quantitative investment. Although various algorithmic
approaches-such as genetic programming, reinforcement learning, and large
language models-have significantly expanded the capacity for alpha discovery,
systematic evaluation remains a key challenge. Existing evaluation metrics
predominantly include backtesting and correlation-based measures. Backtesting
is computationally intensive, inherently sequential, and sensitive to specific
strategy parameters. Correlation-based metrics, though efficient, assess only
predictive ability and overlook other crucial properties such as temporal
stability, robustness, diversity, and interpretability. Additionally, the
closed-source nature of most existing alpha mining models hinders
reproducibility and slows progress in this field. To address these issues, we
propose AlphaEval, a unified, parallelizable, and backtest-free evaluation
framework for automated alpha mining models. AlphaEval assesses the overall
quality of generated alphas along five complementary dimensions: predictive
power, stability, robustness to market perturbations, financial logic, and
diversity. Extensive experiments across representative alpha mining algorithms
demonstrate that AlphaEval achieves evaluation consistency comparable to
comprehensive backtesting, while providing more comprehensive insights and
higher efficiency. Furthermore, AlphaEval effectively identifies superior
alphas compared to traditional single-metric screening approaches. All
implementations and evaluation tools are open-sourced to promote
reproducibility and community engagement.

</details>


### [4] [HiFo-Prompt: Prompting with Hindsight and Foresight for LLM-based Automatic Heuristic Design](https://arxiv.org/abs/2508.13333)
*Chentong Chen,Mengyuan Zhong,Jianyong Sun,Ye Fan,Jialong Shi*

Main category: cs.AI

TL;DR: This paper presents HiFo-Prompt, a novel framework to enhance LLM-based heuristic design in evolutionary computation using foresight and hindsight prompts, resulting in better performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based heuristic design methods in evolutionary computation face limitations due to static operators and lack of mechanisms to accumulate knowledge from past experiences.

Method: The HiFo-Prompt framework employs a dual prompting strategy (Foresight and Hindsight) to adapt search dynamics and distill reusable heuristics from successful past designs. This builds a persistent knowledge base for enhanced learning.

Result: Empirical studies show that HiFo-Prompt outperforms existing LLM-based heuristic design methods, with faster convergence, higher-quality heuristics, and improved query efficiency.

Conclusion: HiFo-Prompt's framework effectively solves the limitations in traditional LLM-based heuristic design, demonstrating its potential for advancing evolutionary computation frameworks.

Abstract: LLM-based Automatic Heuristic Design (AHD) within Evolutionary Computation
(EC) frameworks has shown promising results. However, its effectiveness is
hindered by the use of static operators and the lack of knowledge accumulation
mechanisms. We introduce HiFo-Prompt, a framework that guides LLMs with two
synergistic prompting strategies: Foresight and Hindsight. Foresight-based
prompts adaptively steer the search based on population dynamics, managing the
exploration-exploitation trade-off. In addition, hindsight-based prompts mimic
human expertise by distilling successful heuristics from past generations into
fundamental, reusable design principles. This dual mechanism transforms
transient discoveries into a persistent knowledge base, enabling the LLM to
learn from its own experience. Empirical results demonstrate that HiFo-Prompt
significantly outperforms state-of-the-art LLM-based AHD methods, generating
higher-quality heuristics while achieving substantially faster convergence and
superior query efficiency.

</details>


### [5] [Fitting Ontologies and Constraints to Relational Structures](https://arxiv.org/abs/2508.13176)
*Simon Hosemann,Jean Christoph Jung,Carsten Lutz,Sebastian Rudolph*

Main category: cs.AI

TL;DR: The paper investigates the problem of adapting ontologies and constraints to positive and negative examples within finite relational structures, studying various description logics and tuple-generating dependencies (TGDs). It establishes computational complexity, develops algorithms, and studies ontology/TGD fitting sizes while addressing finite bases' existence for different logics and TGDs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand how to fit and construct meaningful ontologies and constraints that align with given examples, extending comprehension of computational complexity and practical methods for description logics and TGDs.

Method: The authors analyze various types of description logics, e.g., EL and ELI, and TGDs like guarded and frontier-guarded, to design algorithms, evaluate the existence of finite bases, and determine computational complexities.

Result: They determine exact computational complexities, create algorithms, and clarify the existence (or lack thereof) of finite bases for these logical systems. Finite bases are confirmed for specific systems (EL, ELI, guarded TGDs), with exceptions for others (e.g., full TGDs).

Conclusion: This work provides critical theoretical insights into ontology and constraint fitting, advancing our understanding of computational properties and practical approaches in logical knowledge representation.

Abstract: We study the problem of fitting ontologies and constraints to positive and
negative examples that take the form of a finite relational structure. As
ontology and constraint languages, we consider the description logics
$\mathcal{E\mkern-2mu L}$ and $\mathcal{E\mkern-2mu LI}$ as well as several
classes of tuple-generating dependencies (TGDs): full, guarded,
frontier-guarded, frontier-one, and unrestricted TGDs as well as inclusion
dependencies. We pinpoint the exact computational complexity, design
algorithms, and analyze the size of fitting ontologies and TGDs. We also
investigate the related problem of constructing a finite basis of concept
inclusions / TGDs for a given set of finite structures. While finite bases
exist for $\mathcal{E\mkern-2mu L}$, $\mathcal{E\mkern-2mu LI}$, guarded TGDs,
and inclusion dependencies, they in general do not exist for full,
frontier-guarded and frontier-one TGDs.

</details>


### [6] [A Hardware-oriented Approach for Efficient Active Inference Computation and Deployment](https://arxiv.org/abs/2508.13177)
*Nikola Pižurica,Nikola Milović,Igor Jovančević,Conor Heins,Miguel de Prado*

Main category: cs.AI

TL;DR: The paper introduces an optimized methodology for Active Inference (AIF) that reduces latency and memory usage, enabling deployment in resource-constrained environments.


<details>
  <summary>Details</summary>
Motivation: The computational and memory demands of Active Inference prevent its efficient deployment in environments with limited resources.

Method: The authors propose a unified, sparse computational graph combined with pymdp to provide hardware-efficient execution for AIF.

Result: The methodology reduced latency by over 2 times and memory usage by up to 35%, demonstrating enhanced efficiency for AIF agents.

Conclusion: This approach makes Active Inference more viable for real-time and embedded applications, effectively addressing deployment challenges in constrained settings.

Abstract: Active Inference (AIF) offers a robust framework for decision-making, yet its
computational and memory demands pose challenges for deployment, especially in
resource-constrained environments. This work presents a methodology that
facilitates AIF's deployment by integrating pymdp's flexibility and efficiency
with a unified, sparse, computational graph tailored for hardware-efficient
execution. Our approach reduces latency by over 2x and memory by up to 35%,
advancing the deployment of efficient AIF agents for real-time and embedded
applications.

</details>


### [7] [The Interpretability Analysis of the Model Can Bring Improvements to the Text-to-SQL Task](https://arxiv.org/abs/2508.13178)
*Cong Zhang*

Main category: cs.AI

TL;DR: This paper presents CESQL, a model improving real-world text-to-SQL query performance with interpretability analysis, execution-guided strategies, and a focus on WHERE clause accuracy.


<details>
  <summary>Details</summary>
Motivation: Improve the foundational capabilities and generalization of text-to-SQL models for real-world applications by addressing limitations in predicting WHERE clauses and reducing reliance on labeled training data.

Method: Integrates model interpretability analysis, execution-guided strategy, filtering adjustments, logical refinements, and model fusion into the CESQL framework.

Result: CESQL shows significant accuracy improvements on the WikiSQL dataset, especially in WHERE clause prediction, while reducing dependence on condition column data and manual labels.

Conclusion: CESQL offers a promising direction for enhancing basic database query processing accuracy, laying groundwork for investigating more complex query scenarios and irregular data challenges.

Abstract: To elevate the foundational capabilities and generalization prowess of the
text-to-SQL model in real-world applications, we integrate model
interpretability analysis with execution-guided strategy for semantic parsing
of WHERE clauses in SQL queries. Furthermore, we augment this approach with
filtering adjustments, logical correlation refinements, and model fusion,
culminating in the design of the CESQL model that facilitates conditional
enhancement. Our model excels on the WikiSQL dataset, which is emblematic of
single-table database query tasks, markedly boosting the accuracy of prediction
outcomes. When predicting conditional values in WHERE clauses, we have not only
minimized our dependence on data within the condition columns of tables but
also circumvented the impact of manually labeled training data. Our hope is
that this endeavor to enhance accuracy in processing basic database queries
will offer fresh perspectives for research into handling complex queries and
scenarios featuring irregular data in real-world database environments.

</details>


### [8] [Search-Time Data Contamination](https://arxiv.org/abs/2508.13180)
*Ziwen Han,Meher Mankikar,Julian Michael,Zifan Wang*

Main category: cs.AI

TL;DR: The paper introduces and investigates 'search-time contamination' (STC) in evaluating search-based language model agents, highlighting risks to benchmark integrity due to accidental retrieval of test answers from online sources such as HuggingFace.


<details>
  <summary>Details</summary>
Motivation: Motivated by the need to ensure integrity in benchmarking search-based LLM agents, the authors seek to address issues arising from leaked evaluation data impacting inference accuracy.

Method: The authors analyze search-based agent logs across three benchmarks—Humanity's Last Exam (HLE), SimpleQA, and GPQA. They identify instances where evaluation datasets were retrieved, conduct ablation experiments, and propose best practices to mitigate STC.

Result: They discover that STC impacts around 3% of benchmark questions, and blocking HuggingFace results in a 15% accuracy drop on contaminated questions. Public datasets are identified as contributors to integrity issues.

Conclusion: The paper concludes STC as a significant issue in search-based LLM evaluation and proposes recommendations for benchmarking design to improve reliability. Additionally, logs from experiments are made publicly available for transparency.

Abstract: Data contamination refers to the leakage of evaluation data into model
training data, resulting in overfitting to supposedly held-out test sets and
compromising test validity. We identify an analogous issue, search-time
contamination (STC), in evaluating search-based LLM agents which use tools to
gather information from online sources when answering user queries. STC occurs
when the retrieval step surfaces a source containing the test question (or a
near-duplicate) alongside its answer, enabling agents to copy rather than
genuinely infer or reason, undermining benchmark integrity. We find that
HuggingFace, an online platform hosting evaluation datasets, appears among
retrieved sources in search based agent logs. Consequently, agents often
explicitly acknowledge discovering question answer pairs from HuggingFace
within their reasoning chains. On three commonly used capability benchmarks:
Humanity's Last Exam (HLE), SimpleQA, and GPQA, we demonstrate that for
approximately 3% of questions, search-based agents directly find the datasets
with ground truth labels on HuggingFace. When millions of evaluation queries
target the same benchmark, even small, repeated leaks can accelerate the
benchmark's obsolescence, shortening its intended lifecycle. After HuggingFace
is blocked, we observe a drop in accuracy on the contaminated subset of
approximately 15%. We further show through ablation experiments that publicly
accessible evaluation datasets on HuggingFace may not be the sole source of
STC. To this end, we conclude by proposing best practices for benchmark design
and result reporting to address this novel form of leakage and ensure
trustworthy evaluation of search-based LLM agents. To facilitate the auditing
of evaluation results, we also publicly release the complete logs from our
experiments.

</details>


### [9] [QuickMerge++: Fast Token Merging with Autoregressive Prior](https://arxiv.org/abs/2508.13204)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: The paper introduces QuickMerge, a framework for efficient token reduction in next-token prediction across various modalities, offering accurate results with fewer computations.


<details>
  <summary>Details</summary>
Motivation: Generative models with large input scales face computational bottlenecks during token-level operations. Existing token selection methods are limited in flexibility and compatibility with autoregressive models.

Method: QuickMerge dynamically selects fewer tokens based on attention norm magnitude and uses an entropy-based budget estimator. A lightweight transformer prior ensures compatibility with autoregressive generation.

Result: QuickMerge consistently improves compute-accuracy tradeoffs in multi-modality tasks, outperforming existing learned tokenizers and fixed-patch baselines in terms of efficiency and accuracy.

Conclusion: QuickMerge effectively reduces computational costs while maintaining or surpassing the performance of established tokenizers, offering an adaptable solution for efficient token prediction across various domains.

Abstract: As generative models scale to larger inputs across language, vision, and
video domains, the cost of token-level computation has become a key bottleneck.
While prior work suggests that only a subset of tokens significantly influence
downstream predictions, most token selection methods are static,
modality-specific, or incompatible with autoregressive generation. In this
paper, we propose QuickMerge, a lightweight token merging framework designed
for efficient next-token prediction.
  QuickMerge dynamically selects a reduced number of tokens based on attention
norm magnitude, guided by an entropy-based budget estimator. To preserve
autoregressive compatibility, we introduce a lightweight transformer prior
trained over the merged token sequence. By combining semantic salience
estimation, flexible token budgets, and AR alignment, QuickMerge enables
accurate generation with fewer tokens.
  We evaluate QuickMerge across multi-modality domains, demonstrating
consistent improvements in compute-accuracy tradeoffs. Specifically, QuickMerge
reduces token counts sustantially while matching as well as exceeding the
performance of learned tokenizers and fixed-patch baselines.

</details>


### [10] [AI sustains higher strategic tension than humans in chess](https://arxiv.org/abs/2508.13213)
*Adamo Cerioli,Edward D. Lee,Vito D. P. Servedio*

Main category: cs.AI

TL;DR: The study examines strategic decision-making tension in chess games, comparing human and AI approaches using a piece-to-piece interaction metric. AI players maintain higher strategic tension and complexity for longer durations compared to humans.


<details>
  <summary>Details</summary>
Motivation: To understand the trade-off between immediate opportunities and long-term goals, studying strategic tension in chess offers insights into human and AI differences.

Method: A network-based metric is introduced to measure piece-to-piece interactions, studying its evolution in human vs human and AI vs AI chess matches.

Result: Competitive AI players exhibit sustained strategic tension and interconnected gameplay longer than elite human players. Human tension levels increase sharply at specific expertise thresholds (1600 Elo, 2300 Elo).

Conclusion: AI tolerates greater strategic tension and complexity for longer durations, focusing on balanced tactics, whereas humans prioritize limiting game complexity possibly due to cognitive constraints. These insights have implications for AI application in strategic fields.

Abstract: Strategic decision-making involves managing the tension between immediate
opportunities and long-term objectives. We study this trade-off in chess by
characterizing and comparing dynamics between human vs human and AI vs AI
games. We propose a network-based metric of piece-to-piece interaction to
quantify the ongoing strategic tension on the board. Its evolution in games
reveals that the most competitive AI players sustain higher levels of strategic
tension for longer durations than elite human players. Cumulative tension
varies with algorithmic complexity for AI and correspondingly in human-played
games increases abruptly with expertise at about 1600 Elo and again at 2300
Elo. The profiles reveal different approaches. Highly competitive AI tolerates
interconnected positions balanced between offensive and defensive tactics over
long periods. Human play, in contrast, limits tension and game complexity,
which may reflect cognitive limitations and adaptive strategies. The difference
may have implications for AI usage in complex, strategic environments.

</details>


### [11] [Explicit v.s. Implicit Memory: Exploring Multi-hop Complex Reasoning Over Personalized Information](https://arxiv.org/abs/2508.13250)
*Zeyu Zhang,Yang Zhang,Haoran Tan,Rui Li,Xu Chen*

Main category: cs.AI

TL;DR: This paper introduces a multi-hop reasoning task for personalized user information leveraging advanced memory mechanisms in language models. It includes a dataset, evaluation framework, experiments on various memory techniques, and proposes a hybrid method for better performance.


<details>
  <summary>Details</summary>
Motivation: Memory-driven personalization in large language models often falls short when handling complex tasks requiring multi-hop reasoning over vast user data.

Method: The authors define a multi-hop reasoning task, construct a dataset and evaluation framework, and implement both explicit and implicit memory methods. They propose a HybridMem approach combining both paradigms.

Result: Experiments validate the effectiveness of HybridMem, demonstrating its ability to enhance multi-hop reasoning in personalized setups.

Conclusion: The study provides a comprehensive exploration of memory mechanisms for multi-hop reasoning, introducing a new task, dataset, methods, and a hybrid model for improving personalization in large language models.

Abstract: In large language model-based agents, memory serves as a critical capability
for achieving personalization by storing and utilizing users' information.
Although some previous studies have adopted memory to implement user
personalization, they typically focus on preference alignment and simple
question-answering. However, in the real world, complex tasks often require
multi-hop reasoning on a large amount of user information, which poses
significant challenges for current memory approaches. To address this
limitation, we propose the multi-hop personalized reasoning task to explore how
different memory mechanisms perform in multi-hop reasoning over personalized
information. We explicitly define this task and construct a dataset along with
a unified evaluation framework. Then, we implement various explicit and
implicit memory methods and conduct comprehensive experiments. We evaluate
their performance on this task from multiple perspectives and analyze their
strengths and weaknesses. Besides, we explore hybrid approaches that combine
both paradigms and propose the HybridMem method to address their limitations.
We demonstrate the effectiveness of our proposed model through extensive
experiments. To benefit the research community, we release this project at
https://github.com/nuster1128/MPR.

</details>


### [12] ["DIVE" into Hydrogen Storage Materials Discovery with AI Agents](https://arxiv.org/abs/2508.13251)
*Di Zhang,Xue Jia,Tran Ba Hung,Seong Hoon Jang,Linda Zhang,Ryuhei Sato,Yusuke Hashimoto,Toyoto Sato,Kiyoe Konno,Shin-ichi Orimo,Hao Li*

Main category: cs.AI

TL;DR: The paper introduces the DIVE workflow for extracting experimental data from scientific figures and tables, improving accuracy and coverage of data extraction for materials discovery.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenge of unstructured materials data in scientific texts hindering the use of large language models in automated materials discovery.

Method: Development of the DIVE multi-agent workflow for systematic data extraction from graphical elements in literature, applied specifically to solid-state hydrogen storage materials.

Result: DIVE showed data extraction accuracy gains of 10-15% over commercial models and 30% over open-source models, creating a database with over 30,000 entries that enabled rapid material design.

Conclusion: The DIVE workflow enhances AI-driven materials discovery and is transferable across diverse material types.

Abstract: Data-driven artificial intelligence (AI) approaches are fundamentally
transforming the discovery of new materials. Despite the unprecedented
availability of materials data in the scientific literature, much of this
information remains trapped in unstructured figures and tables, hindering the
construction of large language model (LLM)-based AI agent for automated
materials design. Here, we present the Descriptive Interpretation of Visual
Expression (DIVE) multi-agent workflow, which systematically reads and
organizes experimental data from graphical elements in scientific literatures.
We focus on solid-state hydrogen storage materials-a class of materials central
to future clean-energy technologies and demonstrate that DIVE markedly improves
the accuracy and coverage of data extraction compared to the direct extraction
by multimodal models, with gains of 10-15% over commercial models and over 30%
relative to open-source models. Building on a curated database of over 30,000
entries from 4,000 publications, we establish a rapid inverse design workflow
capable of identifying previously unreported hydrogen storage compositions in
two minutes. The proposed AI workflow and agent design are broadly transferable
across diverse materials, providing a paradigm for AI-driven materials
discovery.

</details>


### [13] [CardAIc-Agents: A Multimodal Framework with Hierarchical Adaptation for Cardiac Care Support](https://arxiv.org/abs/2508.13256)
*Yuting Zhang,Karina V. Bunting,Asgher Champsi,Xiaoxia Wang,Wenqi Lu,Alexander Thorley,Sandeep S Hothi,Zhaowen Qiu,Dipak Kotecha,Jinming Duan*

Main category: cs.AI

TL;DR: CardAIc-Agents proposes a multimodal AI framework addressing limitations in cardiovascular disease detection by integrating dynamic reasoning and versatile tools.


<details>
  <summary>Details</summary>
Motivation: Despite cardiovascular diseases being the leading cause of global mortality, the clinical adoption of AI in this domain is severely limited by rigid workflows, static knowledge systems, and lack of adaptability in complex cases.

Method: Developed a multimodal framework, CardAIc-Agents, consisting of a CardiacRAG agent for plan generation and a chief agent for execution. The system incorporates a dynamic update strategy, multidisciplinary discussion tools, and visual aids for validation.

Result: The proposed CardAIc-Agents demonstrated superior efficiency and adaptiveness across three datasets compared to mainstream Vision-Language Models (VLMs) and state-of-the-art systems.

Conclusion: CardAIc-Agents offers an innovative, adaptable approach to enhancing AI-driven cardiovascular care, overcoming traditional AI limitations.

Abstract: Cardiovascular diseases (CVDs) remain the foremost cause of mortality
worldwide, a burden worsened by a severe deficit of healthcare workers.
Artificial intelligence (AI) agents have shown potential to alleviate this gap
via automated early detection and proactive screening, yet their clinical
application remains limited by: 1) prompt-based clinical role assignment that
relies on intrinsic model capabilities without domain-specific tool support; or
2) rigid sequential workflows, whereas clinical care often requires adaptive
reasoning that orders specific tests and, based on their results, guides
personalised next steps; 3) general and static knowledge bases without
continuous learning capability; and 4) fixed unimodal or bimodal inputs and
lack of on-demand visual outputs when further clarification is needed. In
response, a multimodal framework, CardAIc-Agents, was proposed to augment
models with external tools and adaptively support diverse cardiac tasks.
Specifically, a CardiacRAG agent generated general plans from updatable cardiac
knowledge, while the chief agent integrated tools to autonomously execute these
plans and deliver decisions. To enable adaptive and case-specific
customization, a stepwise update strategy was proposed to dynamically refine
plans based on preceding execution results, once the task was assessed as
complex. In addition, a multidisciplinary discussion tool was introduced to
interpret challenging cases, thereby supporting further adaptation. When
clinicians raised concerns, visual review panels were provided to assist final
validation. Experiments across three datasets showed the efficiency of
CardAIc-Agents compared to mainstream Vision-Language Models (VLMs),
state-of-the-art agentic systems, and fine-tuned VLMs.

</details>


### [14] [Towards Unified Multimodal Financial Forecasting: Integrating Sentiment Embeddings and Market Indicators via Cross-Modal Attention](https://arxiv.org/abs/2508.13327)
*Sarthak Khanna,Armin Berger,David Berghaus,Tobias Deusser,Lorenz Sparrenberg,Rafet Sifa*

Main category: cs.AI

TL;DR: The paper introduces STONK, a multimodal framework combining market indicators and news sentiment embeddings for better stock movement predictions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the shortcomings of using isolated analyses (either numerical or textual) in stock prediction by integrating multimodal data.

Method: STONK combines numerical and textual embeddings through feature concatenation and cross-modal attention in a unified pipeline, offering multiple model configurations and fusion strategies.

Result: Backtesting demonstrates that STONK outperforms numeric-only baselines in stock prediction accuracy.

Conclusion: The study validates the effectiveness of multimodal integration for financial forecasting and provides scalable strategies, with accessible source code to encourage replication.

Abstract: We propose STONK (Stock Optimization using News Knowledge), a multimodal
framework integrating numerical market indicators with sentiment-enriched news
embeddings to improve daily stock-movement prediction. By combining numerical &
textual embeddings via feature concatenation and cross-modal attention, our
unified pipeline addresses limitations of isolated analyses. Backtesting shows
STONK outperforms numeric-only baselines. A comprehensive evaluation of fusion
strategies and model configurations offers evidence-based guidance for scalable
multimodal financial forecasting. Source code is available on GitHub

</details>


### [15] [LOOP: A Plug-and-Play Neuro-Symbolic Framework for Enhancing Planning in Autonomous Systems](https://arxiv.org/abs/2508.13371)
*Ronit Virwani,Ruchika Suryawanshi*

Main category: cs.AI

TL;DR: The paper introduces LOOP, a neuro-symbolic planning framework that iteratively combines neural and symbolic components for reliable autonomous planning, achieving notable success rates on benchmark domains.


<details>
  <summary>Details</summary>
Motivation: Current neural and symbolic planning approaches fail to balance logical guarantees with flexibility and natural language understanding for complex autonomous tasks. A gap exists in refining solutions through interaction between neural and symbolic methods.

Method: LOOP treats planning as an iterative process between neural networks and symbolic reasoners, using features like graph neural networks, hierarchical decomposition, and causal memory to integrate neural capabilities with symbolic feedback.

Result: LOOP achieved a success rate of 85.8% on six IPC benchmark domains, outperforming comparisons such as LLM+P, LLM-as-Planner, and Tree-of-Thoughts.

Conclusion: The key to reliable autonomous planning lies in enabling iterative communication between neural and symbolic components, as demonstrated by LOOP's effectiveness and potential for real-world application trustworthiness.

Abstract: Planning is one of the most critical tasks in autonomous systems, where even
a small error can lead to major failures or million-dollar losses. Current
state-of-the-art neural planning approaches struggle with complex domains,
producing plans with missing preconditions, inconsistent goals, and
hallucinations. While classical planners provide logical guarantees, they lack
the flexibility and natural language understanding capabilities needed for
modern autonomous systems. Existing neuro-symbolic approaches use one-shot
translation from natural language to formal plans, missing the opportunity for
neural and symbolic components to work and refine solutions together. To
address this gap, we develop LOOP -- a novel neuro-symbolic planning framework
that treats planning as an iterative conversation between neural and symbolic
components rather than simple translation. LOOP integrates 13 coordinated
neural features including graph neural networks for spatial relationships,
multi-agent validation for consensus-based correctness, hierarchical
decomposition for complex task management, and causal memory that learns from
both successes and failures. Unlike existing approaches, LOOP generates PDDL
specifications, refines them iteratively based on symbolic feedback, and builds
a causal knowledge base from execution traces. LOOP was evaluated on six
standard IPC benchmark domains, where it achieved 85.8% success rate compared
to LLM+P (55.0%), LLM-as-Planner (19.2%), and Tree-of-Thoughts (3.3%). This
work shows that the key to reliable planning is not in choosing between neural
networks or symbolic reasoners but it lies in making them actually ``talk'' to
each other during the entire process. LOOP provides a thorough blueprint for
building autonomous systems that can finally be trusted with critical
real-world applications.

</details>


### [16] [SPANER: Shared Prompt Aligner for Multimodal Semantic Representation](https://arxiv.org/abs/2508.13387)
*Thye Shan Ng,Caren Soyeon Han,Eun-Jung Holden*

Main category: cs.AI

TL;DR: SPANER is a new PEFT framework that aligns multimodal embeddings using shared prompts, improving both performance and cross-modal generalization.


<details>
  <summary>Details</summary>
Motivation: To address the lack of cross-modal generalization in existing multimodal PEFT methods, which often neglect embedding structure and focus only on task-specific performance.

Method: SPANER uses a shared prompt mechanism as a modality-agnostic anchor to align embeddings from multiple modalities into a unified semantic space, supporting extensibility to new modalities without architectural changes.

Result: Experiments show that SPANER is competitive in few-shot retrieval and maintains high semantic coherence across different modalities.

Conclusion: Aligning embedding structures is crucial for scalable multimodal learning, and SPANER achieves this alignment while delivering strong task performance.

Abstract: Recent advances in multimodal Parameter-Efficient Fine-Tuning (PEFT) have
significantly improved performance on downstream tasks such as few-shot
retrieval. However, most existing approaches focus on task-specific gains while
neglecting the structure of the multimodal embedding space. As a result,
modality-specific representations often remain isolated, limiting cross-modal
generalisation. In this work, we introduce Shared Prompt AligNER (SPANER), a
modality-agnostic PEFT framework designed to embed inputs from diverse
modalities into a unified semantic space. At its core, SPANER employs a shared
prompt mechanism that acts as a conceptual anchor, enabling semantically
related instances to converge spatially regardless of modality. This shared
prompt design is inherently extensible, supporting the seamless integration of
additional modalities, such as audio, without altering the core architecture.
Through comprehensive experiments across vision-language and audio-visual
benchmarks, SPANER demonstrates competitive few-shot retrieval performance
while preserving high semantic coherence in the learned embedding space. Our
results highlight the importance of aligning embedding structures, rather than
merely tuning adapter weights, for scalable multimodal learning.

</details>


### [17] [TASER: Table Agents for Schema-guided Extraction and Recommendation](https://arxiv.org/abs/2508.13404)
*Nicole Cho,Kirsty Fielding,William Watson,Sumitra Ganesh,Manuela Veloso*

Main category: cs.AI

TL;DR: The paper introduces TASER, an agentic table extraction system designed to handle unstructured financial tables. TASER is schema-guided and significantly outperforms existing models.


<details>
  <summary>Details</summary>
Motivation: The motivation for this research stems from the challenges posed by real-world financial documents, which often contain fragmented and unstructured tables that are difficult to process using existing methods.

Method: They designed TASER, which involves multiple agents working on tasks like table detection, classification, extraction, and schema recommendations. Continuous learning is emphasized to improve results.

Result: TASER achieves better performance compared to Table Transformer, with a 10.1% improvement. Continuous learning increased actionable schema recommendations by 104.3% and led to a 9.8% increase in extracted holdings.

Conclusion: The study suggests that agentic, continuously learning systems, such as TASER, are promising for handling complex, heterogeneous financial tables. Additionally, the release of the TASERTab dataset supports broader research opportunities.

Abstract: Real-world financial documents report essential information about an entity's
financial holdings that can span millions of different financial instrument
types. Yet, these details are often buried in messy, multi-page, fragmented
tables - for example, 99.4% of the tables in our dataset have no bounding boxes
with the maximum number of rows amounting to 426 per table across 44 pages. To
tackle these unique challenges from real-world tables, we present a
continuously learning, agentic table extraction system, TASER (Table Agents for
Schema-guided Extraction and Recommendation) that extracts highly unstructured,
multi-page, heterogeneous tables into normalized, schema-conforming outputs.
Our table agents execute on table detection, classification, extraction, and
recommendations by leveraging an initial schema. Then, our Recommender Agent
reviews the outputs, recommends schema revisions, and decides on the final
recommendations, enabling TASER to outperform existing table detection models
such as Table Transformer by 10.1%. Within this continuous learning process, we
highlight that larger batch sizes result in a 104.3% increase in schema
recommendations that are actionable and utilized, resulting in a 9.8% increase
in extracted holdings - highlighting the importance of a continuous learning
process. To train TASER, we have manually labeled 22,584 pages (28,150,449
tokens), 3,213 tables for $731,685,511,687 of holdings culminating in one of
the first real financial table datasets. We release our dataset TASERTab to
enable the research community to access real-world financial tables and
outputs. Our results highlight the promise of agentic, schema-guided extraction
systems for robust understanding of real-world financial tables.

</details>


### [18] [Virtuous Machines: Towards Artificial General Science](https://arxiv.org/abs/2508.13421)
*Gabrielle Wehr,Reuben Rideaux,Amaya J. Fox,David R. Lightfoot,Jason Tangen,Jason B. Mattingley,Shane E. Ehrhardt*

Main category: cs.AI

TL;DR: A domain-agnostic AI system successfully navigated the end-to-end scientific workflow, conducting and documenting psychological experiments, yet with limitations in conceptual nuance and theoretical depth.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity and volume of scientific literature and the challenges posed by domain specialization make it crucial to develop AI systems that can integrate knowledge and autonomously perform scientific tasks across disciplines.

Method: A domain-agnostic AI system was developed and used to autonomously conduct psychological experiments, perform online data collection, execute data analyses, and prepare manuscripts, thus mimicking the entire scientific research workflow.

Result: The AI system independently performed tasks including study design, data collection, coding, analysis, and manuscript writing. It conducted studies on visual working memory, mental rotation, and imagery vividness, involving 288 participants, and worked continuously for over 8 hours to refine analysis pipelines. Results indicate performance comparable to human researchers in methodological rigor but with weaknesses in conceptual interpretation.

Conclusion: This research demonstrates the feasibility of AI systems autonomously performing complex scientific research, marking a significant step toward AI-assisted experimental science. However, challenges remain in enhancing AI's conceptual understanding and ensuring appropriate attributions of scientific credit.

Abstract: Artificial intelligence systems are transforming scientific discovery by
accelerating specific research tasks, from protein structure prediction to
materials design, yet remain confined to narrow domains requiring substantial
human oversight. The exponential growth of scientific literature and increasing
domain specialisation constrain researchers' capacity to synthesise knowledge
across disciplines and develop unifying theories, motivating exploration of
more general-purpose AI systems for science. Here we show that a
domain-agnostic, agentic AI system can independently navigate the scientific
workflow - from hypothesis generation through data collection to manuscript
preparation. The system autonomously designed and executed three psychological
studies on visual working memory, mental rotation, and imagery vividness,
executed one new online data collection with 288 participants, developed
analysis pipelines through 8-hour+ continuous coding sessions, and produced
completed manuscripts. The results demonstrate the capability of AI scientific
discovery pipelines to conduct non-trivial research with theoretical reasoning
and methodological rigour comparable to experienced researchers, though with
limitations in conceptual nuance and theoretical interpretation. This is a step
toward embodied AI that can test hypotheses through real-world experiments,
accelerating discovery by autonomously exploring regions of scientific space
that human cognitive and resource constraints might otherwise leave unexplored.
It raises important questions about the nature of scientific understanding and
the attribution of scientific credit.

</details>


### [19] [STPFormer: A State-of-the-Art Pattern-Aware Spatio-Temporal Transformer for Traffic Forecasting](https://arxiv.org/abs/2508.13433)
*Jiayu Fang,Zhiqi Shao,S T Boris Choy,Junbin Gao*

Main category: cs.AI

TL;DR: STPFormer, a Transformer model for spatio-temporal traffic forecasting, achieves state-of-the-art performance via unified, interpretable representation learning.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of complex spatio-temporal patterns, rigid temporal encoding, weak space-time fusion, and diverse traffic data formats in Transformer-based models.

Method: Introduces STPFormer, equipped with four modules: Temporal Position Aggregator (TPA) for temporal encoding, Spatial Sequence Aggregator (SSA) for spatial learning, Spatial-Temporal Graph Matching (STGM) for cross-domain alignment, and an Attention Mixer for multi-scale fusion.

Result: Experiments on five real-world datasets demonstrate that STPFormer achieves consistent state-of-the-art results.

Conclusion: STPFormer is an effective and generalizable Transformer framework for spatio-temporal traffic data, validated by experiments and ablations.

Abstract: Spatio-temporal traffic forecasting is challenging due to complex temporal
patterns, dynamic spatial structures, and diverse input formats. Although
Transformer-based models offer strong global modeling, they often struggle with
rigid temporal encoding and weak space-time fusion. We propose STPFormer, a
Spatio-Temporal Pattern-Aware Transformer that achieves state-of-the-art
performance via unified and interpretable representation learning. It
integrates four modules: Temporal Position Aggregator (TPA) for pattern-aware
temporal encoding, Spatial Sequence Aggregator (SSA) for sequential spatial
learning, Spatial-Temporal Graph Matching (STGM) for cross-domain alignment,
and an Attention Mixer for multi-scale fusion. Experiments on five real-world
datasets show that STPFormer consistently sets new SOTA results, with ablation
and visualizations confirming its effectiveness and generalizability.

</details>


### [20] [Discrete Optimization of Min-Max Violation and its Applications Across Computational Sciences](https://arxiv.org/abs/2508.13437)
*Cheikh Ahmed,Mahdi Mostajabdaveh,Samin Aref,Zirui Zhou*

Main category: cs.AI

TL;DR: The paper presents the Discrete Min-Max Violation (DMMV) optimization problem, develops a GPU-based heuristic for solving it, and demonstrates improvements across three use cases: language model quantization, discrete tomography, and FIR filter design.


<details>
  <summary>Details</summary>
Motivation: To address the need for a versatile optimization framework capable of reducing worst-case constraint violations in diverse applications.

Method: The authors formulate DMMV mathematically, analyze its properties, and introduce a GPU-accelerated heuristic leveraging these properties to efficiently solve practical instances.

Result: The proposed DMMV heuristic shows significant improvements: 14% better quantization results in language models, 16% error reduction and 6x acceleration in discrete tomography, and nearly 50% ripple reduction in FIR filter design.

Conclusion: The DMMV optimization problem and its heuristic are effective across multiple domains, highlighting its utility as a context-free problem. The open-sourcing of the code is intended to foster broader research and application developments.

Abstract: We introduce the Discrete Min-Max Violation (DMMV) as a general optimization
problem which seeks an assignment of discrete values to variables that
minimizes the largest constraint violation. This context-free mathematical
formulation is applicable to a wide range of use cases that have worst-case
performance requirements. After defining the DMMV problem mathematically, we
explore its properties to establish a foundational understanding. To tackle
DMMV instance sizes of practical relevance, we develop a GPU-accelerated
heuristic that takes advantage of the mathematical properties of DMMV for
speeding up the solution process. We demonstrate the versatile applicability of
our heuristic by solving three optimization problems as use cases: (1)
post-training quantization of language models, (2) discrete tomography, and (3)
Finite Impulse Response (FIR) filter design. In quantization without outlier
separation, our heuristic achieves 14% improvement on average over existing
methods. In discrete tomography, it reduces reconstruction error by 16% under
uniform noise and accelerates computations by a factor of 6 on GPU. For FIR
filter design, it nearly achieves 50% ripple reduction compared to using the
commercial integer optimization solver, Gurobi. Our comparative results point
to the benefits of studying DMMV as a context-free optimization problem and the
advantages that our proposed heuristic offers on three distinct problems. Our
GPU-accelerated heuristic will be made open-source to further stimulate
research on DMMV and its other applications. The code is available at
https://anonymous.4open.science/r/AMVM-5F3E/

</details>


### [21] [LM Agents May Fail to Act on Their Own Risk Knowledge](https://arxiv.org/abs/2508.13465)
*Yuzhi Tang,Tianxiao Li,Elizabeth Li,Chris J. Maddison,Honghua Dong,Yangjun Ruan*

Main category: cs.AI

TL;DR: The paper examines the gap between language model agents' risk awareness and their ability to execute tasks safely, introducing an evaluation framework and a risk verifier to mitigate risky actions.


<details>
  <summary>Details</summary>
Motivation: The work is motivated by the need to address safety concerns in language model agents, especially their tendency to perform risky actions despite awareness of the risks.

Method: The authors developed a framework to evaluate LM agent safety across three dimensions: risk knowledge, risk identification in scenarios, and execution safety. They also proposed a risk verifier that critiques actions using abstract descriptions.

Result: Evaluation results showed a significant gap between risk knowledge (>98% pass rate) and its application in behavior (risk identification dropped >23%, risky actions executed with <26% pass rates). Their proposed system reduced risky actions by 55.3%.

Conclusion: Scaling model capabilities alone does not resolve safety gaps in LM agents. Abstracting trajectories for risk verification significantly enhances safety execution.

Abstract: Language model (LM) agents have demonstrated significant potential for
automating real-world tasks, yet they pose a diverse array of potential, severe
risks in safety-critical scenarios. In this work, we identify a significant gap
between LM agents' risk awareness and safety execution abilities: while they
often answer "Yes" to queries like "Is executing `sudo rm -rf /*' dangerous?",
they will likely fail to identify such risks in instantiated trajectories or
even directly perform these risky actions when acting as agents. To
systematically investigate this, we develop a comprehensive evaluation
framework to examine agents' safety across three progressive dimensions: 1)
their knowledge about potential risks, 2) their ability to identify
corresponding risks in execution trajectories, and 3) their actual behaviors to
avoid executing these risky actions. Our evaluation reveals two critical
performance gaps that resemble the generator-validator gaps observed in LMs:
while agents demonstrate near-perfect risk knowledge ($>98\%$ pass rates), they
fail to apply this knowledge when identifying risks in actual scenarios (with
performance dropping by $>23\%$) and often still execute risky actions ($<26\%$
pass rates). Notably, this trend persists across more capable LMs as well as in
specialized reasoning models like DeepSeek-R1, indicating that simply scaling
model capabilities or inference compute does not inherently resolve safety
concerns. Instead, we take advantage of these observed gaps to develop a risk
verifier that independently critiques the proposed actions by agents, with an
abstractor that converts specific execution trajectories into abstract
descriptions where LMs can more effectively identify the risks. Our overall
system achieves a significant reduction of risky action execution by $55.3\%$
over vanilla-prompted agents.

</details>


### [22] [CrafterDojo: A Suite of Foundation Models for Building Open-Ended Embodied Agents in Crafter](https://arxiv.org/abs/2508.13530)
*Junyeong Park,Hyeonseo Cho,Sungjin Ahn*

Main category: cs.AI

TL;DR: CrafterDojo introduces a suite of tools and foundation models for the Crafter environment, making it a lightweight and versatile platform for developing general-purpose embodied AI agents.


<details>
  <summary>Details</summary>
Motivation: General-purpose embodied AI agents face challenges in environments that offer complexity and scalability, such as Minecraft. Crafter provides a lightweight alternative, but its potential remains untapped due to the lack of foundational models.

Method: The paper proposes CrafterDojo, which includes models like CrafterVPT, CrafterCLIP, and CrafterSteve-1. It also integrates datasets creation tools (CrafterPlay and CrafterCaption), implementations, benchmarks, and open-source resources.

Result: CrafterDojo successfully transforms Crafter into a prototyping-friendly environment, enabling researchers to explore general-purpose embodied agent innovations efficiently.

Conclusion: CrafterDojo bridges the gap for foundational research in the Crafter ecosystem, offering robust tools that improve accessibility and enhance scalable testing for AI agent development.

Abstract: Developing general-purpose embodied agents is a core challenge in AI.
Minecraft provides rich complexity and internet-scale data, but its slow speed
and engineering overhead make it unsuitable for rapid prototyping. Crafter
offers a lightweight alternative that retains key challenges from Minecraft,
yet its use has remained limited to narrow tasks due to the absence of
foundation models that have driven progress in the Minecraft setting. In this
paper, we present CrafterDojo, a suite of foundation models and tools that
unlock the Crafter environment as a lightweight, prototyping-friendly, and
Minecraft-like testbed for general-purpose embodied agent research. CrafterDojo
addresses this by introducing CrafterVPT, CrafterCLIP, and CrafterSteve-1 for
behavior priors, vision-language grounding, and instruction following,
respectively. In addition, we provide toolkits for generating behavior and
caption datasets (CrafterPlay and CrafterCaption), reference agent
implementations, benchmark evaluations, and a complete open-source codebase.

</details>


### [23] [Toward Better EHR Reasoning in LLMs: Reinforcement Learning with Expert Attention Guidance](https://arxiv.org/abs/2508.13579)
*Yue Fang,Yuxin Guo,Jiaran Gao,Hongxin Ding,Xinke Jiang,Weibin Liao,Yongxin Xu,Yinghao Zhu,Zhibang Yang,Liantao Ma,Junfeng Zhao,Yasha Wang*

Main category: cs.AI

TL;DR: The paper introduces EAG-RL, a framework designed to enhance large language models (LLMs) in electronic health record (EHR) reasoning by aligning their attention with expert-guided clinically relevant features.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with EHR-based prediction tasks due to challenges in modeling temporally structured, high-dimensional data. Existing approaches rely on separate deep learning models for prediction, limiting generalization and failing to improve LLM intrinsic capabilities.

Method: The EAG-RL framework uses expert-guided Monte Carlo Tree Search for initializing LLM reasoning trajectories and reinforcement learning to align their attention with DL model-identified clinically salient features.

Result: EAG-RL improved LLMs' EHR reasoning ability by 14.62%, increased robustness against feature perturbation, and generalized well to unseen clinical domains.

Conclusion: The framework demonstrates strong potential for real-world deployment in clinical prediction tasks and bridges the gap in EHR reasoning for LLMs.

Abstract: Improving large language models (LLMs) for electronic health record (EHR)
reasoning is essential for enabling accurate and generalizable clinical
predictions. While LLMs excel at medical text understanding, they underperform
on EHR-based prediction tasks due to challenges in modeling temporally
structured, high-dimensional data. Existing approaches often rely on hybrid
paradigms, where LLMs serve merely as frozen prior retrievers while downstream
deep learning (DL) models handle prediction, failing to improve the LLM's
intrinsic reasoning capacity and inheriting the generalization limitations of
DL models. To this end, we propose EAG-RL, a novel two-stage training framework
designed to intrinsically enhance LLMs' EHR reasoning ability through expert
attention guidance, where expert EHR models refer to task-specific DL models
trained on EHR data. Concretely, EAG-RL first constructs high-quality, stepwise
reasoning trajectories using expert-guided Monte Carlo Tree Search to
effectively initialize the LLM's policy. Then, EAG-RL further optimizes the
policy via reinforcement learning by aligning the LLM's attention with
clinically salient features identified by expert EHR models. Extensive
experiments on two real-world EHR datasets show that EAG-RL improves the
intrinsic EHR reasoning ability of LLMs by an average of 14.62%, while also
enhancing robustness to feature perturbations and generalization to unseen
clinical domains. These results demonstrate the practical potential of EAG-RL
for real-world deployment in clinical prediction tasks. Our code have been
available at https://github.com/devilran6/EAG-RL.

</details>


### [24] [Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning for Chart-to-Code Generation](https://arxiv.org/abs/2508.13587)
*Lei Chen,Xuanle Zhao,Zhixiong Zeng,Jing Huang,Liming Zheng,Yufeng Zhong,Lin Ma*

Main category: cs.AI

TL;DR: The paper introduces Multimodal Structured Reinforcement Learning (MSRL) to improve chart-to-code generation, addressing limitations of supervised fine-tuning (SFT) alone.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of SFT for chart-to-code generation which demands deep reasoning over visual charts and structured output generation.

Method: The authors construct a large-scale dataset of chart-code pairs and introduce MSRL with a structured reward system combining textual rule-based rewards and visual model-based rewards. Training is done in a two-stage curriculum for stability.

Result: The proposed MSRL method improves high-level metrics by 6.2% on ChartMimic and 9.9% on ReachQA benchmarks, surpassing the SFT performance plateau.

Conclusion: The MSRL approach demonstrates significant improvements in chart-to-code generation, achieving competitive results with advanced closed-source models, thus highlighting its effectiveness over SFT.

Abstract: While reinforcement learning (RL) has proven highly effective for general
reasoning in vision-language models, its application to tasks requiring
in-depth understanding of information-rich images and generation of structured
outputs remains underexplored. Chart-to-code generation exemplifies this
challenge, demanding complex reasoning over visual charts to generate
structured code. Supervised fine-tuning (SFT) alone is often insufficient,
highlighting the need for effective RL strategies that appropriately reward
structured outputs. We systematically investigate the performance plateau in
SFT through large-scale experiments and propose Multimodal Structured
Reinforcement Learning (MSRL) for chart-to-code generation, which substantially
breaks through this plateau. We construct the largest training corpus to date,
containing 3 million chart-code pairs from real-world arXiv tables to mitigate
simplistic patterns of prior synthetic data. Despite reaching state-of-the-art
performance, our experiments show that scaling SFT data eventually hits a
plateau where further increases yield negligible improvements. Our MSRL method
leverages a multi-granularity structured reward system using multimodal textual
and visual feedback. At the textual level, rule-based rewards validate
fine-grained code details. At the visual level, model-based rewards assess
structural similarity by rendering generated code into images and employing an
evaluator model. We implement this within a two-stage curriculum for training
stability. Results demonstrate that MSRL significantly breaks the SFT plateau,
improving high-level metrics by 6.2% and 9.9% on ChartMimic and ReachQA
benchmarks respectively, achieving competitive performance with advanced
closed-source models.

</details>


### [25] [V2P: From Background Suppression to Center Peaking for Robust GUI Grounding Task](https://arxiv.org/abs/2508.13634)
*Jikai Chen,Long Chen,Dong Wang,Leilei Gan,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: The paper proposes the Valley-to-Peak (V2P) method to enhance GUI element localization by addressing issues of background distraction and center-edge distinction.


<details>
  <summary>Details</summary>
Motivation: Traditional GUI localization methods struggle with spatial interaction uncertainty, attention drift, and inability to distinguish between central and edge areas of GUI elements, leading to imprecise outcomes.

Method: V2P uses suppression attention mechanisms to reduce focus on background regions and models GUI interactions as 2D Gaussian heatmaps to better distinguish centers from edges, simulating human interaction patterns.

Result: The V2P method achieves high performance on ScreenSpot-v2 (92.3%) and ScreenSpot-Pro (50.5%) benchmarks, with ablation studies validating its components.

Conclusion: The V2P approach successfully enhances GUI localization precision, proving its effectiveness and generalizability for GUI grounding tasks.

Abstract: Precise localization of GUI elements is crucial for the development of GUI
agents. Traditional methods rely on bounding box or center-point regression,
neglecting spatial interaction uncertainty and visual-semantic hierarchies.
Recent methods incorporate attention mechanisms but still face two key issues:
(1) ignoring processing background regions causes attention drift from the
desired area, and (2) uniform labeling fails to distinguish between center and
edges of the target UI element, leading to click imprecision. Inspired by how
humans visually process and interact with GUI elements, we propose the
Valley-to-Peak (V2P) method to address these issues. To mitigate background
distractions, V2P introduces a suppression attention mechanism that minimizes
the model's focus on irrelevant regions to highlight the intended region. For
the issue of center-edge distinction, V2P applies a Fitts' Law-inspired
approach by modeling GUI interactions as 2D Gaussian heatmaps where the weight
gradually decreases from the center towards the edges. The weight distribution
follows a Gaussian function, with the variance determined by the target's size.
Consequently, V2P effectively isolates the target area and teaches the model to
concentrate on the most essential point of the UI element. The model trained by
V2P achieves the performance with 92.3% and 50.5% on two benchmarks
ScreenSpot-v2 and ScreenSpot-Pro. Ablations further confirm each component's
contribution, highlighting V2P's generalizability for precise GUI grounding
tasks.

</details>


### [26] [Interactive Query Answering on Knowledge Graphs with Soft Entity Constraints](https://arxiv.org/abs/2508.13663)
*Daniel Daza,Alberto Bernardi,Luca Costabello,Christophe Gueret,Masoud Mansoury,Michael Cochez,Martijn Schut*

Main category: cs.AI

TL;DR: The paper introduces Neural Query Reranker (NQR), a method to incorporate soft constraints into query answering on incomplete knowledge graphs, addressing gaps in existing methods focusing on first-order logic.


<details>
  <summary>Details</summary>
Motivation: Existing query answering systems focus primarily on first-order-logic constraints, failing to accommodate queries with vague or context-dependent constraints, such as preferences for attributes.

Method: The paper proposes NQR, which refines query answer scores interactively by incorporating examples of preferred and non-preferred entities, without altering the original answers.

Result: Experiments showed that NQR effectively captures soft constraints while preserving query answering robustness.

Conclusion: NQR expands query answering capabilities by addressing vague and context-dependent constraints, making it more suitable for real-world applications with incomplete knowledge graphs.

Abstract: Methods for query answering over incomplete knowledge graphs retrieve
entities that are likely to be answers, which is particularly useful when such
answers cannot be reached by direct graph traversal due to missing edges.
However, existing approaches have focused on queries formalized using
first-order-logic. In practice, many real-world queries involve constraints
that are inherently vague or context-dependent, such as preferences for
attributes or related categories. Addressing this gap, we introduce the problem
of query answering with soft constraints. We propose a Neural Query Reranker
(NQR) designed to adjust query answer scores by incorporating soft constraints
without disrupting the original answers to a query. NQR operates interactively,
refining answers based on incremental examples of preferred and non-preferred
entities. We extend existing QA benchmarks by generating datasets with soft
constraints. Our experiments demonstrate that NQR can capture soft constraints
while maintaining robust query answering performance.

</details>


### [27] [ITL-LIME: Instance-Based Transfer Learning for Enhancing Local Explanations in Low-Resource Data Settings](https://arxiv.org/abs/2508.13672)
*Rehan Raza,Guanjin Wang,Kevin Wong,Hamid Laga,Marco Fisichella*

Main category: cs.AI

TL;DR: The paper introduces ITL-LIME, a new framework to enhance LIME's fidelity and stability in data-scarce environments by integrating instance-based transfer learning and clustering techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation is to mitigate the instability and locality issues in LIME, particularly in data-constrained scenarios, where random perturbations generate unrealistic variations that fail to approximate the original model's decision boundary accurately.

Method: The method involves integrating instance-based transfer learning with LIME. Clustering groups source domain instances into prototypes to compare against target instances. Real instances from the source and target domains are weighted using a contrastive learning encoder and used to train a surrogate model.

Result: This framework introduces more stable and true-to-data explanations by addressing LIME's randomness and improving local fidelity through thoughtful instance selection and weighting mechanisms.

Conclusion: ITL-LIME enhances explanation accuracy and stability for machine learning models in scenarios where data scarcity undermines existing interpretability frameworks like LIME.

Abstract: Explainable Artificial Intelligence (XAI) methods, such as Local
Interpretable Model-Agnostic Explanations (LIME), have advanced the
interpretability of black-box machine learning models by approximating their
behavior locally using interpretable surrogate models. However, LIME's inherent
randomness in perturbation and sampling can lead to locality and instability
issues, especially in scenarios with limited training data. In such cases, data
scarcity can result in the generation of unrealistic variations and samples
that deviate from the true data manifold. Consequently, the surrogate model may
fail to accurately approximate the complex decision boundary of the original
model. To address these challenges, we propose a novel Instance-based Transfer
Learning LIME framework (ITL-LIME) that enhances explanation fidelity and
stability in data-constrained environments. ITL-LIME introduces instance
transfer learning into the LIME framework by leveraging relevant real instances
from a related source domain to aid the explanation process in the target
domain. Specifically, we employ clustering to partition the source domain into
clusters with representative prototypes. Instead of generating random
perturbations, our method retrieves pertinent real source instances from the
source cluster whose prototype is most similar to the target instance. These
are then combined with the target instance's neighboring real instances. To
define a compact locality, we further construct a contrastive learning-based
encoder as a weighting mechanism to assign weights to the instances from the
combined set based on their proximity to the target instance. Finally, these
weighted source and target instances are used to train the surrogate model for
explanation purposes.

</details>


### [28] [Knowledge Graph Completion for Action Prediction on Situational Graphs -- A Case Study on Household Tasks](https://arxiv.org/abs/2508.13675)
*Mariam Arustashvili,Jörg Deigmöller,Heiko Paulheim*

Main category: cs.AI

TL;DR: The paper discusses situational knowledge graphs for household actions and claims that standard link prediction algorithms underperform compared to simple baselines.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve the handling of incomplete information extracted from video footage by completing household-related knowledge graphs.

Method: It explores the application of link prediction algorithms to situational knowledge graphs and evaluates their performance against simple baselines.

Result: Link prediction algorithms struggle with situational knowledge graphs and fail to outperform basic approaches.

Conclusion: Special characteristics of situational knowledge graphs hinder the effectiveness of standard link prediction techniques, requiring novel approaches or adaptations.

Abstract: Knowledge Graphs are used for various purposes, including business
applications, biomedical analyses, or digital twins in industry 4.0. In this
paper, we investigate knowledge graphs describing household actions, which are
beneficial for controlling household robots and analyzing video footage. In the
latter case, the information extracted from videos is notoriously incomplete,
and completing the knowledge graph for enhancing the situational picture is
essential. In this paper, we show that, while a standard link prediction
problem, situational knowledge graphs have special characteristics that render
many link prediction algorithms not fit for the job, and unable to outperform
even simple baselines.

</details>


### [29] [MHSNet:An MoE-based Hierarchical Semantic Representation Network for Accurate Duplicate Resume Detection with Large Language Model](https://arxiv.org/abs/2508.13676)
*Yu Li,Zulong Chen,Wenjian Xu,Hong Wen,Yipeng Yu,Man Lung Yiu,Yuyu Yin*

Main category: cs.AI

TL;DR: The paper introduces MHSNet, a framework designed to improve duplication detection of resumes by addressing their semantic complexity, structural heterogeneity, and incompleteness.


<details>
  <summary>Details</summary>
Motivation: Recruiters face challenges with incomplete and inaccurate resumes fetched from third-party sites, which hampers the maintenance and enrichment of talent pools.

Method: MHSNet fine-tunes BGE-M3 using contrastive learning and incorporates a state-aware Mixture-of-Experts framework to generate multi-level sparse and dense semantic representations of resumes.

Result: Experimental results validate the effectiveness of MHSNet in detecting duplicates among diverse and incomplete resumes.

Conclusion: MHSNet provides a robust solution to enhance resume duplication detection, ensuring a higher-quality talent pool for companies.

Abstract: To maintain the company's talent pool, recruiters need to continuously search
for resumes from third-party websites (e.g., LinkedIn, Indeed). However,
fetched resumes are often incomplete and inaccurate. To improve the quality of
third-party resumes and enrich the company's talent pool, it is essential to
conduct duplication detection between the fetched resumes and those already in
the company's talent pool. Such duplication detection is challenging due to the
semantic complexity, structural heterogeneity, and information incompleteness
of resume texts. To this end, we propose MHSNet, an multi-level identity
verification framework that fine-tunes BGE-M3 using contrastive learning. With
the fine-tuned , Mixture-of-Experts (MoE) generates multi-level sparse and
dense representations for resumes, enabling the computation of corresponding
multi-level semantic similarities. Moreover, the state-aware Mixture-of-Experts
(MoE) is employed in MHSNet to handle diverse incomplete resumes. Experimental
results verify the effectiveness of MHSNet

</details>


### [30] [Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning Abilities of Large Language Models](https://arxiv.org/abs/2508.13678)
*Xiao-Wen Yang,Jie-Jing Shao,Lan-Zhe Guo,Bo-Wen Zhang,Zhi Zhou,Lin-Han Jia,Wang-Zhou Dai,Yu-Feng Li*

Main category: cs.AI

TL;DR: This paper reviews neuro-symbolic methods to improve reasoning capabilities of Large Language Models (LLMs) and examines key challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: Reasoning is a fundamental challenge for LLMs. Enhancing reasoning can bring AI closer to Artificial General Intelligence (AGI).

Method: The paper formalizes reasoning tasks, introduces neurosymbolic learning, and categorizes methods into Symbolic->LLM, LLM->Symbolic, and LLM+Symbolic paradigms.

Result: A comprehensive review of neuro-symbolic techniques for LLM reasoning is presented, along with a related GitHub repository for resources and papers.

Conclusion: Neuro-symbolic approaches are promising for addressing reasoning challenges in LLMs, providing valuable insights and future directions for AGI development.

Abstract: Large Language Models (LLMs) have shown promising results across various
tasks, yet their reasoning capabilities remain a fundamental challenge.
Developing AI systems with strong reasoning capabilities is regarded as a
crucial milestone in the pursuit of Artificial General Intelligence (AGI) and
has garnered considerable attention from both academia and industry. Various
techniques have been explored to enhance the reasoning capabilities of LLMs,
with neuro-symbolic approaches being a particularly promising way. This paper
comprehensively reviews recent developments in neuro-symbolic approaches for
enhancing LLM reasoning. We first present a formalization of reasoning tasks
and give a brief introduction to the neurosymbolic learning paradigm. Then, we
discuss neuro-symbolic methods for improving the reasoning capabilities of LLMs
from three perspectives: Symbolic->LLM, LLM->Symbolic, and LLM+Symbolic.
Finally, we discuss several key challenges and promising future directions. We
have also released a GitHub repository including papers and resources related
to this survey: https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSy.

</details>


### [31] [The DeepLog Neurosymbolic Machine](https://arxiv.org/abs/2508.13697)
*Vincent Derkinderen,Robin Manhaeve,Rik Adriaensen,Lucas Van Praet,Lennert De Smet,Giuseppe Marra,Luc De Raedt*

Main category: cs.AI

TL;DR: DeepLog is a theoretical and operational framework for neurosymbolic AI, combining an annotated logical language and algebraic circuits for efficient modeling.


<details>
  <summary>Details</summary>
Motivation: To unify and enhance the development of neurosymbolic AI systems by providing a generalizable framework with improved efficiency.

Method: DeepLog introduces a neurosymbolic language based on annotated, grounded first-order logic and computational graphs through extended algebraic circuits.

Result: It demonstrates flexibility across fuzzy and probabilistic logics, logic placement in architectures versus loss functions, and significant computational improvements using GPUs over CPUs.

Conclusion: DeepLog is a powerful and declarative framework that generalizes and enriches neurosymbolic AI systems, enabling diverse model configurations with computational efficiency.

Abstract: We contribute a theoretical and operational framework for neurosymbolic AI
called DeepLog. DeepLog introduces building blocks and primitives for
neurosymbolic AI that make abstraction of commonly used representations and
computational mechanisms used in neurosymbolic AI. DeepLog can represent and
emulate a wide range of neurosymbolic systems. It consists of two key
components. The first is the DeepLog language for specifying neurosymbolic
models and inference tasks. This language consists of an annotated neural
extension of grounded first-order logic, and makes abstraction of the type of
logic, e.g. boolean, fuzzy or probabilistic, and whether logic is used in the
architecture or in the loss function. The second DeepLog component is situated
at the computational level and uses extended algebraic circuits as
computational graphs. Together these two components are to be considered as a
neurosymbolic abstract machine, with the DeepLog language as the intermediate
level of abstraction and the circuits level as the computational one. DeepLog
is implemented in software, relies on the latest insights in implementing
algebraic circuits on GPUs, and is declarative in that it is easy to obtain
different neurosymbolic models by making different choices for the underlying
algebraic structures and logics. The generality and efficiency of the DeepLog
neurosymbolic machine is demonstrated through an experimental comparison
between 1) different fuzzy and probabilistic logics, 2) between using logic in
the architecture or in the loss function, and 3) between a standalone CPU-based
implementation of a neurosymbolic AI system and a DeepLog GPU-based one.

</details>


### [32] [CausalPlan: Empowering Efficient LLM Multi-Agent Collaboration Through Causality-Driven Planning](https://arxiv.org/abs/2508.13721)
*Minh Hoang Nguyen,Van Dai Do,Dung Nguyen,Thin Nguyen,Hung Le*

Main category: cs.AI

TL;DR: The paper introduces CausalPlan, a framework using structural causal reasoning to improve planning, coordination, and decision-making for large language model agents in collaborative tasks.


<details>
  <summary>Details</summary>
Motivation: Current LLMs, especially smaller open-source models, often fail in collaborative tasks due to reliance on surface-level correlations rather than causal reasoning, affecting their planning and coordination in dynamic settings.

Method: The proposed CausalPlan framework uses the Structural Causal Action (SCA) model to build a causal graph that informs action selection by scoring and reweighting LLM-generated proposals or substituting them with causally grounded alternatives. This approach avoids fine-tuning LLMs.

Result: CausalPlan outperforms reinforcement learning baselines by reducing invalid actions and enhancing collaboration in AI-AI and human-AI scenarios, as tested on the Overcooked-AI benchmark using various LLMs.

Conclusion: Causality-driven planning offers an effective, interpretable, and generalisable way to deploy multi-agent LLM systems, advancing their coordination and operational consistency.

Abstract: Large language model (LLM) agents-especially smaller, open-source
models-often produce causally invalid or incoherent actions in collaborative
tasks due to their reliance on surface-level correlations rather than grounded
causal reasoning. This limitation undermines their performance in terms of
coordination and planning in dynamic environments. We address this challenge
with CausalPlan, a two-phase framework that integrates explicit structural
causal reasoning into the LLM planning process. At the core of CausalPlan is
the Structural Causal Action (SCA) model, which learns a causal graph from
agent trajectories to capture how prior actions and current environment states
influence future decisions. This structure is then used to guide action
selection by assigning causal scores to LLM-generated proposals, reweighting
them accordingly, or falling back to causally grounded alternatives when
needed. By embedding this causal knowledge directly into the decision loop,
CausalPlan constrains planning to intervention-consistent behaviours without
requiring fine-tuning of the LLM itself. We evaluate CausalPlan on the
Overcooked-AI benchmark across five multi-agent coordination tasks and four
LLMs of varying sizes: Gemma-7B, Llama-8B, Qwen-14B, and Llama-70B.
Experimental results show that CausalPlan consistently reduces invalid actions
and improves collaboration in both AI-AI and human-AI settings, outperforming
strong reinforcement learning baselines. Our findings highlight the value of
causality-driven planning for deploying efficient, interpretable, and
generalisable multi-agent LLM systems.

</details>


### [33] [Expertise-aware Multi-LLM Recruitment and Collaboration for Medical Decision-Making](https://arxiv.org/abs/2508.13754)
*Liuxin Bao,Zhihao Peng,Xiaofei Zhou,Runmin Cong,Jiyong Zhang,Yixuan Yuan*

Main category: cs.AI

TL;DR: The paper introduces an EMRC framework leveraging multiple LLMs for enhanced Medical Decision-Making. It outperforms state-of-the-art methods on diagnostic tasks.


<details>
  <summary>Details</summary>
Motivation: The inherent complexity of medical decision-making and the limitations of a single LLM, such as static training and parametric constraints, necessitate innovative approaches for synthesizing clinical information.

Method: The EMRC framework operates in two key stages: expertise-aware agent recruitment through an expertise table of multiple LLMs, followed by multi-agent collaboration using confidence fusion and adversarial validation.

Result: EMRC outperformed state-of-the-art single- and multi-LLM approaches, achieving a 74.45% accuracy on the MMLU-Pro-Health dataset, marking a 2.69% improvement over GPT-4-0613.

Conclusion: Utilizing multiple LLMs with complementary strengths effectively enhances medical diagnostic accuracy and reliability, establishing EMRC as a promising system for medical decision-making support.

Abstract: Medical Decision-Making (MDM) is a complex process requiring substantial
domain-specific expertise to effectively synthesize heterogeneous and
complicated clinical information. While recent advancements in Large Language
Models (LLMs) show promise in supporting MDM, single-LLM approaches are limited
by their parametric knowledge constraints and static training corpora, failing
to robustly integrate the clinical information. To address this challenge, we
propose the Expertise-aware Multi-LLM Recruitment and Collaboration (EMRC)
framework to enhance the accuracy and reliability of MDM systems. It operates
in two stages: (i) expertise-aware agent recruitment and (ii) confidence- and
adversarial-driven multi-agent collaboration. Specifically, in the first stage,
we use a publicly available corpus to construct an LLM expertise table for
capturing expertise-specific strengths of multiple LLMs across medical
department categories and query difficulty levels. This table enables the
subsequent dynamic selection of the optimal LLMs to act as medical expert
agents for each medical query during the inference phase. In the second stage,
we employ selected agents to generate responses with self-assessed confidence
scores, which are then integrated through the confidence fusion and adversarial
validation to improve diagnostic reliability. We evaluate our EMRC framework on
three public MDM datasets, where the results demonstrate that our EMRC
outperforms state-of-the-art single- and multi-LLM methods, achieving superior
diagnostic performance. For instance, on the MMLU-Pro-Health dataset, our EMRC
achieves 74.45% accuracy, representing a 2.69% improvement over the
best-performing closed-source model GPT- 4-0613, which demonstrates the
effectiveness of our expertise-aware agent recruitment strategy and the agent
complementarity in leveraging each LLM's specialized capabilities.

</details>


### [34] [Quantifier Instantiations: To Mimic or To Revolt?](https://arxiv.org/abs/2508.13811)
*Jan Jakubův,Mikoláš Janota*

Main category: cs.AI

TL;DR: The paper presents a novel instantiation method for SMT solvers that learns and generates terms based on probabilistic context-free grammars, balancing past successes and new exploration.


<details>
  <summary>Details</summary>
Motivation: Existing instantiation techniques for SMT solvers face challenges due to the undecidability of quantified formulas, requiring new approaches to efficiently generate useful terms.

Method: The approach uses probabilistic context-free grammars to treat instantiations as samples from a latent language, enabling dynamic learning and generation of terms during solving.

Result: The technique supports both mimicking successful instantiations and generating diverse terms, improving the balance between exploiting known patterns and exploring new possibilities.

Conclusion: The proposed method enhances SMT solvers by addressing the limitations of existing instantiation techniques and achieving better quantifier reasoning through dynamic and probabilistic term generation.

Abstract: Quantified formulas pose a significant challenge for Satisfiability Modulo
Theories (SMT) solvers due to their inherent undecidability. Existing
instantiation techniques, such as e-matching, syntax-guided, model-based,
conflict-based, and enumerative methods, often complement each other. This
paper introduces a novel instantiation approach that dynamically learns from
these techniques during solving. By treating observed instantiations as samples
from a latent language, we use probabilistic context-free grammars to generate
new, similar terms. Our method not only mimics successful past instantiations
but also explores diversity by optionally inverting learned term probabilities,
aiming to balance exploitation and exploration in quantifier reasoning.

</details>


### [35] [Revisiting RAG Ensemble: A Theoretical and Mechanistic Analysis of Multi-RAG System Collaboration](https://arxiv.org/abs/2508.13828)
*Yifei Chen,Guanting Dong,Yutao Zhu,Zhicheng Dou*

Main category: cs.AI

TL;DR: The paper explores how to effectively combine multiple Retrieval-Augmented Generation (RAG) systems, investigates theoretical and mechanistic aspects, and demonstrates that ensembles of RAG systems enhance generalizability and robustness.


<details>
  <summary>Details</summary>
Motivation: Single RAG frameworks struggle to adapt to diverse downstream tasks, creating a need to explore multi-RAG system ensembles for improved adaptability.

Method: The paper provides an information entropy-based theoretical analysis and a mechanism-focused analysis at the pipeline and module levels. Four pipeline types (Branching, Iterative, Loop, Agentic) and three modules (Generator, Retriever, Reranker) address seven research questions.

Result: Experimental results validate that multi-RAG ensembles improve generalizability and robustness at both pipeline and module levels.

Conclusion: The research highlights the potential of RAG ensembles in enhancing system performance and sets a foundation for future studies in this area.

Abstract: Retrieval-Augmented Generation (RAG) technology has been widely applied in
recent years. However, despite the emergence of various RAG frameworks, a
single RAG framework still cannot adapt well to a broad range of downstream
tasks. Therefore, how to leverage the advantages of multiple RAG systems has
become an area worth exploring. To address this issue, we have conducted a
comprehensive and systematic investigation into ensemble methods based on RAG
systems. Specifically, we have analyzed the RAG ensemble framework from both
theoretical and mechanistic analysis perspectives. From the theoretical
analysis, we provide the first explanation of the RAG ensemble framework from
the perspective of information entropy. In terms of mechanism analysis, we have
explored the RAG ensemble framework from both the pipeline and module levels.
We carefully select four different pipelines (Branching, Iterative, Loop, and
Agentic) and three different modules (Generator, Retriever, and Reranker) to
solve seven different research questions. The experiments show that aggregating
multiple RAG systems is both generalizable and robust, whether at the pipeline
level or the module level. Our work lays the foundation for similar research on
the multi-RAG system ensemble.

</details>


### [36] [Improved Generalized Planning with LLMs through Strategy Refinement and Reflection](https://arxiv.org/abs/2508.13876)
*Katharina Stein,Nils Hodel,Daniel Fišer,Jörg Hoffmann,Michael Katz,Alexander Koller*

Main category: cs.AI

TL;DR: This paper enhances the generation of Python-based generalized plans via LLMs by debugging pseudocode before implementation and improving the quality through program variants in planning domains.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of incorrect strategies in Python implementation due to the direct translation of generalized plans via LLMs in PDDL planning.

Method: The approach involves pseudocode generation with automatic debugging, reflection-based Python debugging, and generating multiple program variants for quality assurance.

Result: Experiments on 17 benchmark domains show substantial improvements, with perfect task coverage in 12 domains using the best Python programs.

Conclusion: The proposed techniques increase reliability and quality of generalized plans without deterioration, demonstrating advancements in automated planning through LLMs.

Abstract: LLMs have recently been used to generate Python programs representing
generalized plans in PDDL planning, i.e., plans that generalize across the
tasks of a given PDDL domain. Previous work proposed a framework consisting of
three steps: the LLM first generates a summary and then a strategy for the
domain, both in natural language, and then implements that strategy as a Python
program, that gets debugged on example planning tasks. In that work, only one
strategy is generated and passed directly to the program generation. If the
strategy is incorrect, its implementation will therefore result in an incorrect
generalized plan. Here, we introduce an approach that generates the strategy in
the form of pseudocode and enables automatic debugging of the pseudocode, hence
allowing us to identify and fix errors prior to the generation of the
generalized plan itself. Additionally, we extend the Python debugging phase
with a reflection step prompting the LLM to pinpoint the reason for the
observed plan failure. Finally, we take inspiration from LLM code generation to
produce several program variants and pick the best one. Running experiments on
17 benchmark domains, we show that these extensions substantially improve (and
never deteriorate) the quality of the generalized plans. In 12 of the domains,
our best Python programs solve all tasks that can be generated with the
respective instance generator.

</details>


### [37] [Structured Agentic Workflows for Financial Time-Series Modeling with LLMs and Reflective Feedback](https://arxiv.org/abs/2508.13915)
*Yihao Ang,Yifan Bao,Lei Jiang,Jiajie Tao,Anthony K. H. Tung,Lukasz Szpruch,Hao Ni*

Main category: cs.AI

TL;DR: The paper introduces TS-Agent, a modular framework that applies agentic systems to time-series modeling in financial services, outperforming current AutoML tools in accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Challenges exist in developing high-performing, interpretable models for financial time-series data due to lack of adaptability and responsiveness in current AutoML frameworks.

Method: The TS-Agent pipeline employs an iterative, decision-driven process for model selection, code refinement, and fine-tuning, guided by a planner agent equipped with knowledge banks and curated libraries.

Result: Empirical evaluations show TS-Agent surpasses state-of-the-art AutoML tools and agentic baselines in accuracy, robustness, and decision traceability for financial time-series forecasting.

Conclusion: TS-Agent enhances workflow automation and interpretability, achieving better performance and transparency, making it suitable for high-stakes environments like financial services.

Abstract: Time-series data is central to decision-making in financial markets, yet
building high-performing, interpretable, and auditable models remains a major
challenge. While Automated Machine Learning (AutoML) frameworks streamline
model development, they often lack adaptability and responsiveness to
domain-specific needs and evolving objectives. Concurrently, Large Language
Models (LLMs) have enabled agentic systems capable of reasoning, memory
management, and dynamic code generation, offering a path toward more flexible
workflow automation. In this paper, we introduce \textsf{TS-Agent}, a modular
agentic framework designed to automate and enhance time-series modeling
workflows for financial applications. The agent formalizes the pipeline as a
structured, iterative decision process across three stages: model selection,
code refinement, and fine-tuning, guided by contextual reasoning and
experimental feedback. Central to our architecture is a planner agent equipped
with structured knowledge banks, curated libraries of models and refinement
strategies, which guide exploration, while improving interpretability and
reducing error propagation. \textsf{TS-Agent} supports adaptive learning,
robust debugging, and transparent auditing, key requirements for high-stakes
environments such as financial services. Empirical evaluations on diverse
financial forecasting and synthetic data generation tasks demonstrate that
\textsf{TS-Agent} consistently outperforms state-of-the-art AutoML and agentic
baselines, achieving superior accuracy, robustness, and decision traceability.

</details>


### [38] [The Collaboration Paradox: Why Generative AI Requires Both Strategic Intelligence and Operational Stability in Supply Chain Management](https://arxiv.org/abs/2508.13942)
*Soumyadeep Dhar*

Main category: cs.AI

TL;DR: The paper analyzes the unexpected performance failures of collaborative AI agents in managing supply chains, uncovering a 'collaboration paradox' where AI systems designed for cooperation sometimes worsen system stability.


<details>
  <summary>Details</summary>
Motivation: Explore the strategic behaviors of AI agents in economic settings, especially in cooperative supply chain management, and address critical instabilities like the bullwhip effect.

Method: Utilized supply chain simulations to observe generative AI agents powered by Large Language Models under Vendor-Managed Inventory principles and analyzed emergent behaviors and failures.

Result: Discovered the collaboration paradox where AI agents hoard inventory, starving the supply chain system and worsening performance. Suggested a new framework combining proactive policy-setting with collaborative execution protocols to improve resilience.

Conclusion: Emergent AI-driven systems for supply chains must integrate strategic high-level policies with robust execution protocols to prevent systemic inefficiencies and achieve stability and effectiveness.

Abstract: The rise of autonomous, AI-driven agents in economic settings raises critical
questions about their emergent strategic behavior. This paper investigates
these dynamics in the cooperative context of a multi-echelon supply chain, a
system famously prone to instabilities like the bullwhip effect. We conduct
computational experiments with generative AI agents, powered by Large Language
Models (LLMs), within a controlled supply chain simulation designed to isolate
their behavioral tendencies. Our central finding is the "collaboration
paradox": a novel, catastrophic failure mode where theoretically superior
collaborative AI agents, designed with Vendor-Managed Inventory (VMI)
principles, perform even worse than non-AI baselines. We demonstrate that this
paradox arises from an operational flaw where agents hoard inventory, starving
the system. We then show that resilience is only achieved through a synthesis
of two distinct layers: high-level, AI-driven proactive policy-setting to
establish robust operational targets, and a low-level, collaborative execution
protocol with proactive downstream replenishment to maintain stability. Our
final framework, which implements this synthesis, can autonomously generate,
evaluate, and quantify a portfolio of viable strategic choices. The work
provides a crucial insight into the emergent behaviors of collaborative AI
agents and offers a blueprint for designing stable, effective AI-driven systems
for business analytics.

</details>


### [39] [ChronoLLM: Customizing Language Models for Physics-Based Simulation Code Generation](https://arxiv.org/abs/2508.13975)
*Jingquan Wang,Andrew Negrut,Harry Zhang,Khailanii Slaton,Shu Wang,Radu Serban,Jinlong Wu,Dan Negrut*

Main category: cs.AI

TL;DR: The paper explores customizing large language models (LLMs) to assist with PyChrono, a multi-physics simulation tool. LLMs are refined to generate scripts and offer assistance with simulations effectively.


<details>
  <summary>Details</summary>
Motivation: To investigate the potential of LLMs to act as virtual assistants for simulation tools, specifically PyChrono, enabling more effective and accessible use by experts.

Method: The approach involves refining and customizing both open- and closed-source LLMs to enhance their capabilities in generating PyChrono simulation scripts. The process used quantifies improvements in script quality.

Result: The refined LLMs successfully create simulation scripts, from simple pendulum experiments to complex vehicle simulations. While not perfect, the scripts serve as solid starting points and provide additional functionalities like answering API questions and offering modeling suggestions.

Conclusion: Refining LLMs can significantly reduce the entry barrier for using complex simulation tools like PyChrono, and the methodology is generalizable to other domains.

Abstract: This contribution is concerned with the following issue: can pretrained large
language models (LLMs) be refined and customized to the point where they become
virtual assistants helping experts with the effective use of a simulation tool?
In this case study, the ``simulation tool'' considered is PyChrono, an open
source multi-physics dynamics engine for multibody systems. We present a
framework for refining and customizing both open- and closed-source LLMs to
harness the power of AI in generating scripts that perform PyChrono virtual
experiments. We refine and customize several classes of LLMs through a process
that leads to a quantifiable improvement in the quality of the generated
PyChrono simulation scripts. These scripts can range from simple
single-pendulum simulations to complex virtual experiments involving full
vehicles on deformable terrain. While the generated scripts are rarely perfect,
they often serve as strong starting points for the user to modify and improve
on. Additionally, the LLM can answer specific API questions about the
simulator, or recommend modeling approaches. The framework discussed is general
and can be applied to lower the entry barrier for simulation tools associated
with other application domains.

</details>


### [40] [A Biased Random Key Genetic Algorithm for Solving the Longest Run Subsequence Problem](https://arxiv.org/abs/2508.14020)
*Christian Blum,Pedro Pinacho-Davidson*

Main category: cs.AI

TL;DR: The paper addresses the NP-hard Longest Run Subsequence (LRS) problem using a Biased Random Key Genetic Algorithm (BRKGA) and compares it to alternative methods like Max-Min Ant System and CPLEX solver.


<details>
  <summary>Details</summary>
Motivation: The LRS problem is significant in bioinformatics, particularly in genome reassembly, and effective solutions are essential due to its computational complexity.

Method: A Biased Random Key Genetic Algorithm (BRKGA) is developed, with focus on computational efficiency and converting gray value vectors into valid solutions. Comparative analyses include Max-Min Ant System and integer linear programming via CPLEX.

Result: The BRKGA is identified as a state-of-the-art solution for solving LRS problems, performing effectively in computational tests.

Conclusion: While BRKGA shows strong performance, there is scope for improvements, especially for input cases with larger alphabet sizes.

Abstract: The longest run subsequence (LRS) problem is an NP-hard combinatorial
optimization problem belonging to the class of subsequence problems from
bioinformatics. In particular, the problem plays a role in genome reassembly.
In this paper, we present a solution to the LRS problem using a Biased Random
Key Genetic Algorithm (BRKGA). Our approach places particular focus on the
computational efficiency of evaluating individuals, which involves converting
vectors of gray values into valid solutions to the problem. For comparison
purposes, a Max-Min Ant System is developed and implemented. This is in
addition to the application of the integer linear programming solver CPLEX for
solving all considered problem instances. The computation results show that the
proposed BRKGA is currently a state-of-the-art technique for the LRS problem.
Nevertheless, the results also show that there is room for improvement,
especially in the context of input strings based on large alphabet sizes.

</details>


### [41] [ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents](https://arxiv.org/abs/2508.14040)
*Hanyu Lai,Xiao Liu,Yanxiao Zhao,Han Xu,Hanchen Zhang,Bohao Jing,Yanyu Ren,Shuntian Yao,Yuxiao Dong,Jie Tang*

Main category: cs.AI

TL;DR: ComputerRL is a framework to improve autonomous desktop intelligence via API-GUI integration, scalable training infrastructure, and an innovative training strategy, achieving state-of-the-art benchmarks.


<details>
  <summary>Details</summary>
Motivation: Developing an efficient framework for agents capable of skillfully navigating human-centric desktop environments to automate tasks and improve scalability in reinforcement learning.

Method: The paper introduces the API-GUI paradigm for combining API calls with GUI interaction, employs distributed RL infrastructure for large-scale training, and proposes the Entropulse strategy to balance reinforcement learning and supervised fine-tuning.

Result: ComputerRL achieves state-of-the-art accuracy (48.1%) on the OSWorld benchmark using AutoGLM-OS-9B, demonstrating its effectiveness in general desktop task automation.

Conclusion: The proposed framework proves effective in overcoming training inefficiencies, improves generalization across desktop tasks, and achieves advancements in desktop automation with significant accuracy gains.

Abstract: We introduce ComputerRL, a framework for autonomous desktop intelligence that
enables agents to operate complex digital workspaces skillfully. ComputerRL
features the API-GUI paradigm, which unifies programmatic API calls and direct
GUI interaction to address the inherent mismatch between machine agents and
human-centric desktop environments. Scaling end-to-end RL training is crucial
for improvement and generalization across diverse desktop tasks, yet remains
challenging due to environmental inefficiency and instability in extended
training. To support scalable and robust training, we develop a distributed RL
infrastructure capable of orchestrating thousands of parallel virtual desktop
environments to accelerate large-scale online RL. Furthermore, we propose
Entropulse, a training strategy that alternates reinforcement learning with
supervised fine-tuning, effectively mitigating entropy collapse during extended
training runs. We employ ComputerRL on open models GLM-4-9B-0414 and
Qwen2.5-14B, and evaluate them on the OSWorld benchmark. The AutoGLM-OS-9B
based on GLM-4-9B-0414 achieves a new state-of-the-art accuracy of 48.1%,
demonstrating significant improvements for general agents in desktop
automation. The algorithm and framework are adopted in building AutoGLM (Liu et
al., 2024a)

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [42] [EvoVerilog: Large Langugage Model Assisted Evolution of Verilog Code](https://arxiv.org/abs/2508.13156)
*Ping Guo,Yiting Wang,Wanghao Ye,Yexiao He,Ziyao Wang,Xiaopeng Dai,Ang Li,Qingfu Zhang*

Main category: cs.AR

TL;DR: The paper introduces EvoVerilog, a method combining large language models (LLMs) with evolutionary algorithms to automatically generate and refine Verilog hardware description language code, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Automating Verilog code generation is crucial for simplifying the complex and error-prone process of hardware design. Current methods have limitations such as dependency on human intervention and underperformance in exploring diverse design solutions, necessitating more effective and scalable techniques.

Method: The proposed EvoVerilog framework integrates LLMs with evolutionary algorithms. It employs a multiobjective, population-based search strategy to generate and optimize Verilog code without human intervention, effectively exploring diverse design solutions.

Result: EvoVerilog achieves state-of-the-art results with pass@10 scores of 89.1 and 80.2 on VerilogEval-Machine and VerilogEval-Human benchmarks, showcasing its capability to generate a variety of functional designs while optimizing resource usage.

Conclusion: EvoVerilog demonstrates the potential of combining LLMs with evolutionary algorithms for automated and diverse hardware design, pushing the boundaries of current approaches and reducing reliance on human intervention or fine-tuning.

Abstract: Large Language Models (LLMs) have demonstrated great potential in automating
the generation of Verilog hardware description language code for hardware
design. This automation is critical to reducing human effort in the complex and
error-prone process of hardware design.
  However, existing approaches predominantly rely on human intervention and
fine-tuning using curated datasets, limiting their scalability in automated
design workflows.
  Although recent iterative search techniques have emerged, they often fail to
explore diverse design solutions and may underperform simpler approaches such
as repeated prompting.
  To address these limitations, we introduce EvoVerilog, a novel framework that
combines the reasoning capabilities of LLMs with evolutionary algorithms to
automatically generate and refine Verilog code.
  EvoVerilog utilizes a multiobjective, population-based search strategy to
explore a wide range of design possibilities without requiring human
intervention.
  Extensive experiments demonstrate that EvoVerilog achieves state-of-the-art
performance, with pass@10 scores of 89.1 and 80.2 on the VerilogEval-Machine
and VerilogEval-Human benchmarks, respectively. Furthermore, the framework
showcases its ability to explore diverse designs by simultaneously generating a
variety of functional Verilog code while optimizing resource utilization.

</details>


### [43] [Image2Net: Datasets, Benchmark and Hybrid Framework to Convert Analog Circuit Diagrams into Netlists](https://arxiv.org/abs/2508.13157)
*Haohang Xu,Chengjie Liu,Qihang Wang,Wenhao Huang,Yongjian Xu,Weiyu Chen,Anlan Peng,Zhijun Li,Bo Li,Lei Qi,Jun Yang,Yuan Du,Li Du*

Main category: cs.AR

TL;DR: This paper introduces Image2Net, a framework for converting analog circuit diagrams into text-based netlists with high accuracy, achieving better results than previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing large language models (LLMs) lack sufficient textual data on analog integrated circuits (ICs) due to their representation in image-based circuit diagrams rather than textual netlists, limiting the potential of LLMs in analog IC design.

Method: The authors developed Image2Net, a hybrid framework designed to convert circuit diagrams of varying styles and complexities into text-based netlists, leveraging a newly constructed and open-source dataset. They also introduced a new metric, Netlist Edit Distance (NED), to quantify the accuracy of netlist conversion.

Result: Image2Net achieves an 80.77% success rate in converting circuit diagrams to netlists, outperforming previous methods by 34.62%-45.19%, and shows an average NED of 0.116, which is 62.1%-69.6% lower than state-of-the-art methods.

Conclusion: Image2Net effectively bridges the gap between image-based circuit diagrams and text-based netlists, enriching analog IC datasets for LLMs and setting a new benchmark for extracting netlists from complex circuit diagrams.

Abstract: Large Language Model (LLM) exhibits great potential in designing of analog
integrated circuits (IC) because of its excellence in abstraction and
generalization for knowledge. However, further development of LLM-based analog
ICs heavily relies on textual description of analog ICs, while existing analog
ICs are mostly illustrated in image-based circuit diagrams rather than
text-based netlists. Converting circuit diagrams to netlists help LLMs to
enrich the knowledge of analog IC. Nevertheless, previously proposed conversion
frameworks face challenges in further application because of limited support of
image styles and circuit elements. Up to now, it still remains a challenging
task to effectively convert complex circuit diagrams into netlists. To this
end, this paper constructs and opensources a new dataset with rich styles of
circuit diagrams as well as balanced distribution of simple and complex analog
ICs. And a hybrid framework, named Image2Net, is proposed for practical
conversion from circuit diagrams to netlists. The netlist edit distance (NED)
is also introduced to precisely assess the difference between the converted
netlists and ground truth. Based on our benchmark, Image2Net achieves 80.77\%
successful rate, which is 34.62\%-45.19\% higher than previous works.
Specifically, the proposed work shows 0.116 averaged NED, which is
62.1\%-69.6\% lower than state-of-the-arts.

</details>


### [44] [Fine Grain 3D Integration for Microarchitecture Design Through Cube Packing Exploration](https://arxiv.org/abs/2508.13158)
*Yongxiang Liu,Yuchun Ma,Eren Kurshan,Glenn Reinman,Jason Cong*

Main category: cs.AR

TL;DR: The paper introduces a 3D IC design methodology using multi-layer logical blocks, demonstrating significant performance and power advantages over traditional 2D and single-layer 3D designs.


<details>
  <summary>Details</summary>
Motivation: The need for advanced modeling and tools infrastructures to enable finer grain 3D integration for power and performance improvements.

Method: Developing a cube packing engine for simultaneous optimization of physical and architectural designs, including thermal-aware floorplanning and thermal via insertion.

Result: Experimental results reveal 36% performance improvement over 2D designs and 14% over single-layer 3D designs, alongside a 30% reduction in power dissipation via multi-layer blocks.

Conclusion: Finer grain 3D integration with multi-layer logical blocks offers significant gains in performance, thermal management, and power efficiency, validating the proposed methodology.

Abstract: Most previous 3D IC research focused on stacking traditional 2D silicon
layers, so the interconnect reduction is limited to inter-block delays. In this
paper, we propose techniques that enable efficient exploration of the 3D design
space where each logical block can span more than one silicon layers. Although
further power and performance improvement is achievable through fine grain 3D
integration, the necessary modeling and tool infrastructure has been mostly
missing. We develop a cube packing engine which can simultaneously optimize
physical and architectural design for effective utilization of 3D in terms of
performance, area and temperature. Our experimental results using a design
driver show 36% performance improvement (in BIPS) over 2D and 14% over 3D with
single layer blocks. Additionally multi-layer blocks can provide up to 30%
reduction in power dissipation compared to the single-layer alternatives. Peak
temperature of the design is kept within limits as a result of thermal-aware
floorplanning and thermal via insertion techniques.

</details>


### [45] [Accelerating Transistor-Level Simulation of Integrated Circuits via Equivalence of RC Long-Chain Structures](https://arxiv.org/abs/2508.13159)
*Ruibai Tang,Wenlai Zhao*

Main category: cs.AR

TL;DR: The paper introduces three reduction methods tailored for RC long-chain structures in transistor-level simulations, achieving performance improvement with minimal error.


<details>
  <summary>Details</summary>
Motivation: Transistor-level simulations for integrated circuits are computationally expensive, necessitating efficient methods to reduce simulation complexity and time.

Method: The authors propose three reduction methods designed explicitly for RC long-chain structures with varying time constants, which are present in significant proportions within benchmark circuits.

Result: The proposed methods achieved an average performance improvement of 8.8% (up to 22%) in simulation speed, while maintaining only 0.7% relative error.

Conclusion: The methods enhance simulation efficiency for RC long-chain structures without compromising accuracy, proving their utility in a range of functional modules for integrated circuits.

Abstract: Transistor-level simulation plays a vital role in validating the physical
correctness of integrated circuits. However, such simulations are
computationally expensive. This paper proposes three novel reduction methods
specifically tailored to RC long-chain structures with different scales of time
constant. Such structures account for an average of 6.34\% (up to 12\%) of the
total nodes in the benchmark circuits. Experimental results demonstrate that
our methods yields an average performance improvement of 8.8\% (up to 22\%) on
simulating benchmark circuits which include a variety of functional modules
such as ALUs, adders, multipliers, SEC/DED checkers, and interrupt controllers,
with only 0.7\% relative error.

</details>


### [46] [Through Silicon Via Aware Design Planning for Thermally Efficient 3-D Integrated Circuits](https://arxiv.org/abs/2508.13160)
*Yibo Chen,Eren Kurshan,Dave Motschman,Charles Johnson,Yuan Xie*

Main category: cs.AR

TL;DR: This paper addresses lateral thermal blockages in 3-D ICs caused by dense TSV farms by proposing a thermal-aware TSV placement technique.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to address the thermal challenges posed by dense TSV farms, which create lateral heat blockage and exacerbate local hotspots in 3-D ICs.

Method: A thermal-aware via farm placement technique is proposed to mitigate lateral heat blockages caused by dense signal bus TSV structures in 3-D ICs.

Result: The approach aims to optimize TSV placement to minimize lateral thermal blockage, improving the thermal performance of 3-D ICs.

Conclusion: By accounting for the lateral thermal effects of TSV farms, the proposed technique helps manage thermal issues and enhances the feasibility of 3-D IC designs.

Abstract: 3-D integrated circuits (3-D ICs) offer performance advantages due to their
increased bandwidth and reduced wire-length enabled by through-silicon-via
structures (TSVs). Traditionally TSVs have been considered to improve the
thermal conductivity in the vertical direction. However, the lateral thermal
blockage effect becomes increasingly important for TSV via farms (a cluster of
TSV vias used for signal bus connections between layers) because the TSV size
and pitch continue to scale in {\mu}m range and the metal to insulator ratio
becomes smaller. Consequently, dense TSV farms can create lateral thermal
blockages in thinned silicon substrate and exacerbate the local hotspots. In
this paper, we propose a thermal-aware via farm placement technique for 3-D ICs
to minimize lateral heat blockages caused by dense signal bus TSV structures.

</details>


### [47] [Piano: A Multi-Constraint Pin Assignment-Aware Floorplanner](https://arxiv.org/abs/2508.13161)
*Zhexuan Xu,Kexin Zhou,Jie Wang,Zijie Geng,Siyuan Xu,Shixiong Kai,Mingxuan Yuan,Feng Wu*

Main category: cs.AR

TL;DR: This paper proposes a framework named Piano for optimizing module placement and pin assignment simultaneously during VLSI floorplanning.


<details>
  <summary>Details</summary>
Motivation: Traditional floorplanners often neglect pin assignment under modern constraints, which adversely affects subsequent design stages like detailed placement and routing.

Method: Piano uses a graph-based approach that considers geometric relationships among modules and their netlist connections. This is complemented by whitespace removal and three local optimizers for layout improvement.

Result: Piano achieves significant improvements over benchmark circuits, including 6.81% less HPWL, 13.39% reduction in feedthrough wirelength, 16.36% fewer feedthrough modules, and 21.21% fewer unplaced pins, with no whitespace.

Conclusion: Piano successfully integrates module placement and pin assignment under multiple constraints, leading to superior layout results in VLSI floorplanning.

Abstract: Floorplanning is a critical step in VLSI physical design, increasingly
complicated by modern constraints such as fixed-outline requirements,
whitespace removal, and the presence of pre-placed modules. In addition, the
assignment of pins on module boundaries significantly impacts the performance
of subsequent stages, including detailed placement and routing. However,
traditional floorplanners often overlook pin assignment with modern constraints
during the floorplanning stage. In this work, we introduce Piano, a
floorplanning framework that simultaneously optimizes module placement and pin
assignment under multiple constraints. Specifically, we construct a graph based
on the geometric relationships among modules and their netlist connections,
then iteratively search for shortest paths to determine pin assignments. This
graph-based method also enables accurate evaluation of feedthrough and unplaced
pins, thereby guiding overall layout quality. To further improve the design, we
adopt a whitespace removal strategy and employ three local optimizers to
enhance layout metrics under multi-constraint scenarios. Experimental results
on widely used benchmark circuits demonstrate that Piano achieves an average
6.81% reduction in HPWL, a 13.39% decrease in feedthrough wirelength, a 16.36%
reduction in the number of feedthrough modules, and a 21.21% drop in unplaced
pins, while maintaining zero whitespace.

</details>


### [48] [Accelerating LLM Inference via Dynamic KV Cache Placement in Heterogeneous Memory System](https://arxiv.org/abs/2508.13231)
*Yunhua Fang,Rui Xie,Asad Ul Haq,Linsen Ma,Kaoutar El Maghraoui,Naigang Wang,Meng Wang,Liu Liu,Tong Zhang*

Main category: cs.AR

TL;DR: The paper focuses on optimizing dynamic placement of key-value (KV) cache in heterogeneous memory systems to address memory bandwidth constraints during large language model (LLM) inference.


<details>
  <summary>Details</summary>
Motivation: LLM inference faces memory bandwidth limitations due to frequent access to KV caches. As modern AI hardware evolves with heterogeneous memory systems integrating high-bandwidth memory (HBM) and off-package DRAM, optimizing memory usage has become critical.

Method: The authors mathematically formulate the KV cache placement problem to maximize bandwidth utilization under capacity constraints, and present a theoretical upper bound on performance rather than proposing specific runtime policies.

Result: The work identifies significant opportunities for runtime optimization in KV cache placement across heterogeneous memory systems during LLM inference.

Conclusion: This study is the pioneering formal investigation into dynamic KV cache scheduling for heterogeneous memory systems, highlighting substantial optimization potential while addressing capacity and bandwidth constraints.

Abstract: Large Language Model (LLM) inference is increasingly constrained by memory
bandwidth, with frequent access to the key-value (KV) cache dominating data
movement. While attention sparsity reduces some memory traffic, the relevance
of past tokens varies over time, requiring the full KV cache to remain
accessible and sustaining pressure on both bandwidth and capacity. With
advances in interconnects such as NVLink and LPDDR5X, modern AI hardware now
integrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making
heterogeneous memory systems a practical solution. This work investigates
dynamic KV cache placement across such systems to maximize aggregated bandwidth
utilization under capacity constraints. Rather than proposing a specific
scheduling policy, we formulate the placement problem mathematically and derive
a theoretical upper bound, revealing substantial headroom for runtime
optimization. To our knowledge, this is the first formal treatment of dynamic
KV cache scheduling in heterogeneous memory systems for LLM inference.

</details>


### [49] [FedChip: Federated LLM for Artificial Intelligence Accelerator Chip Design](https://arxiv.org/abs/2508.13162)
*Mahmoud Nazzal,Khoa Nguyen,Deepak Vungarala,Ramtin Zand,Shaahin Angizi,Hai Phan,Abdallah Khreishah*

Main category: cs.AR

TL;DR: The paper introduces FedChip, a Federated fine-tuning approach for collaboratively improving hardware design-focused LLMs while ensuring data privacy. It also provides a new dataset, APTPU-Gen, and a quality evaluation metric called Chip@k.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in AI hardware design automation, particularly focusing on data privacy and lack of domain-specific training in current LLMs.

Method: The authors propose FedChip, a federated learning framework where multiple parties train a shared LLM on proprietary local data to enhance performance. They also introduce APTPU-Gen dataset and Chip@k evaluation metric.

Result: FedChip improves hardware design quality by over 77% compared to high-end LLMs while safeguarding proprietary data.

Conclusion: FedChip demonstrates the potential of federated learning to enhance AI hardware design automation tools, combining privacy with improved performance. The accompanying dataset and metrics set a standard in the field.

Abstract: AI hardware design is advancing rapidly, driven by the promise of design
automation to make chip development faster, more efficient, and more accessible
to a wide range of users. Amongst automation tools, Large Language Models
(LLMs) offer a promising solution by automating and streamlining parts of the
design process. However, their potential is hindered by data privacy concerns
and the lack of domain-specific training. To address this, we introduce
FedChip, a Federated fine-tuning approach that enables multiple Chip design
parties to collaboratively enhance a shared LLM dedicated for automated
hardware design generation while protecting proprietary data. FedChip enables
parties to train the model on proprietary local data and improve the shared
LLM's performance. To exemplify FedChip's deployment, we create and release
APTPU-Gen, a dataset of 30k design variations spanning various performance
metric values such as power, performance, and area (PPA). To encourage the LLM
to generate designs that achieve a balance across multiple quality metrics, we
propose a new design evaluation metric, Chip@k, which statistically evaluates
the quality of generated designs against predefined acceptance criteria.
Experimental results show that FedChip improves design quality by more than 77%
over high-end LLMs while maintaining data privacy

</details>


### [50] [Sustainable AI Training via Hardware-Software Co-Design on NVIDIA, AMD, and Emerging GPU Architectures](https://arxiv.org/abs/2508.13163)
*Yashasvi Makin,Rahul Maliakkal*

Main category: cs.AR

TL;DR: The paper addresses computational inefficiency and energy concerns in large-scale AI training by discussing hardware-software co-design methods for energy optimization, particularly for GPUs.


<details>
  <summary>Details</summary>
Motivation: The exponential energy demand due to rising AI model complexity impacts sustainability, necessitating computationally efficient methods to reduce environmental harm.

Method: Explores hardware-software co-design focusing on enhancing memory and kernel operations, and evaluates tensor cores, memory optimization, software-level methods like mixed-precision arithmetic, and scheduling algorithms.

Result: Findings demonstrate significant improvements in energy efficiency and computational performance using the explored techniques, backed by industry case studies.

Conclusion: Co-design of hardware and software is essential for achieving sustainable AI without performance compromises, and future research should address the identified gaps.

Abstract: In particular, large-scale deep learning and artificial intelligence model
training uses a lot of computational power and energy, so it poses serious
sustainability issues. The fast rise in model complexity has resulted in
exponential increases in energy consumption, increasing the demand for
techniques maximizing computational efficiency and lowering environmental
impact. This work explores environmentally driven performance optimization
methods especially intended for advanced GPU architectures from NVIDIA, AMD,
and other emerging GPU architectures. Our main focus is on investigating
hardware-software co-design techniques meant to significantly increase
memory-level and kernel-level operations, so improving performance-per-watt
measures. Our thorough research encompasses evaluations of specialized tensor
and matrix cores, advanced memory optimization methods, and creative
integration approaches that taken together result in notable energy efficiency
increases. We also discuss important software-level optimizations that augment
hardware capability including mixed-precision arithmetic, advanced energy-aware
scheduling algorithms, and compiler-driven kernel enhancements. Moreover, we
methodically point out important research gaps and suggest future directions
necessary to create really sustainable artificial intelligence systems. This
paper emphasizes how major increases in training efficiency can be obtained by
co-design of hardware and software, so lowering the environmental impact of
artificial intelligence without compromising performance. To back up our
analysis, we use real-world case studies from top companies like Meta, Google,
Amazon, and others that show how these sustainable AI training methods are used
in the real world.

</details>


### [51] [White-Box Reasoning: Synergizing LLM Strategy and gm/Id Data for Automated Analog Circuit Design](https://arxiv.org/abs/2508.13172)
*Jianqiu Chen,Siqi Li,Xu He*

Main category: cs.AR

TL;DR: The paper proposes a framework combining Large Language Models (LLMs) with the gm/Id methodology for efficient analog IC design. This approach outperforms traditional reliance on experience and inefficient simulations.


<details>
  <summary>Details</summary>
Motivation: Analog IC design is challenging due to limitations of traditional methods in handling advanced nodes, reliance on experience, and inefficient simulations.

Method: The proposed framework integrates strategic reasoning of LLMs with the quantitative precision of the gm/Id methodology, using gm/Id lookup tables to guide the LLM's design reasoning.

Result: Validated on a two-stage op-amp, the framework achieved TT corner design specs in 5 iterations and extended to PVT corners efficiently. It performed better compared to standalone LLMs and was on par with a senior engineer in quality, while being more efficient.

Conclusion: The framework demonstrates the potential for automating analog IC design by effectively combining LLM reasoning with structured scientific design methods.

Abstract: Analog IC design is a bottleneck due to its reliance on experience and
inefficient simulations, as traditional formulas fail in advanced nodes.
Applying Large Language Models (LLMs) directly to this problem risks mere
"guessing" without engineering principles. We present a "synergistic reasoning"
framework that integrates an LLM's strategic reasoning with the physical
precision of the gm/Id methodology. By empowering the LLM with gm/Id lookup
tables, it becomes a quantitative, data-driven design partner.
  We validated this on a two-stage op-amp, where our framework enabled the
Gemini model to meet all TT corner specs in 5 iterations and extended
optimization to all PVT corners. A crucial ablation study proved gm/Id data is
key for this efficiency and precision; without it, the LLM is slower and
deviates. Compared to a senior engineer's design, our framework achieves
quasi-expert quality with an order-of-magnitude improvement in efficiency. This
work validates a path for true analog design automation by combining LLM
reasoning with scientific circuit design methodologies.

</details>


### [52] [Low-power, Energy-efficient, Cardiologist-level Atrial Fibrillation Detection for Wearable Devices](https://arxiv.org/abs/2508.13181)
*Dominik Loroch,Johannes Feldmann,Vladimir Rybalkin,Norbert Wehn*

Main category: cs.AR

TL;DR: The paper introduces a revolutionary wearable device offering efficient and accurate atrial fibrillation (AF) detection for extended periods.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of scaling reliable AF detection to meet global demand.

Method: Developed a low-power FPGA-based wearable patch using deep learning and hardware-aware neural architecture search for AF detection.

Result: Achieved over three weeks of continuous AF monitoring with 95% accuracy and ultra-low power consumption (3.8mW).

Conclusion: The device advances scalable, reliable, and sustainable AF detection, surpassing traditional methods and cardiologist-level performance.

Abstract: Atrial fibrillation (AF) is a common arrhythmia and major risk factor for
cardiovascular complications. While commercially available devices and
supporting Artificial Intelligence (AI) algorithms exist for reliable detection
of AF, the scaling of this technology to the amount of people who need this
diagnosis is still a major challenge. This paper presents a novel wearable
device, designed specifically for the early and reliable detection of AF. We
present an FPGA-based patch-style wearable monitor with embedded deep
learning-based AF detection. Operating with 3.8mW system power, which is 1-3
orders of magnitude lower than the state-of-the-art, the device enables
continuous AF detection for over three weeks while achieving 95% accuracy,
surpassing cardiologist-level performance. A key innovation is the combination
of energy-efficient hardware-software co-design and optimized power management
through the application of hardware-aware neural architecture search. This
advancement represents a significant step toward scalable, reliable, and
sustainable AF monitoring.

</details>


### [53] [Sub-Millisecond Event-Based Eye Tracking on a Resource-Constrained Microcontroller](https://arxiv.org/abs/2508.13244)
*Marco Giordano,Pietro Bonazzi,Luca Benini,Michele Magno*

Main category: cs.AR

TL;DR: The paper introduces an event-based eye-tracking system using a Dynamic Vision Sensor and a low-power, high-performance microcontroller, achieving real-time performance and low energy consumption.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of creating real-time, low-latency, and energy-efficient eye-tracking solutions for embedded systems, particularly for wearable devices.

Method: The system utilizes a Dynamic Vision Sensor (DVS) for capturing eye movements and deploys an optimized, compact CNN on an 800 MHz STM32N6 microcontroller with an AI accelerator, achieving efficient edge processing.

Result: The proposed system achieved a mean pupil prediction error of 5.99 pixels, an end-to-end inference latency of 385 μs, and a neural network throughput of 52 MAC operations per cycle while consuming just 155 μJ of energy.

Conclusion: The paper demonstrates the feasibility of a fully embedded, low-latency, and energy-efficient eye-tracking solution, paving the way for its integration into smart glasses and wearable technologies.

Abstract: This paper presents a novel event-based eye-tracking system deployed on a
resource-constrained microcontroller, addressing the challenges of real-time,
low-latency, and low-power performance in embedded systems. The system
leverages a Dynamic Vision Sensor (DVS), specifically the DVXplorer Micro, with
an average temporal resolution of 200 {\mu}s, to capture rapid eye movements
with extremely low latency. The system is implemented on a novel low-power and
high-performance microcontroller from STMicroelectronics, the STM32N6. The
microcontroller features an 800 MHz Arm Cortex-M55 core and AI hardware
accelerator, the Neural-ART Accelerator, enabling real-time inference with
milliwatt power consumption. The paper propose a hardware-aware and
sensor-aware compact Convolutional Neuron Network (CNN) optimized for
event-based data, deployed at the edge, achieving a mean pupil prediction error
of 5.99 pixels and a median error of 5.73 pixels on the Ini-30 dataset. The
system achieves an end-to-end inference latency of just 385 {\mu}s and a neural
network throughput of 52 Multiply and Accumulate (MAC) operations per cycle
while consuming just 155 {\mu}J of energy. This approach allows for the
development of a fully embedded, energy-efficient eye-tracking solution
suitable for applications such as smart glasses and wearable devices.

</details>


### [54] [ViTAD: Timing Violation-Aware Debugging of RTL Code using Large Language Models](https://arxiv.org/abs/2508.13257)
*Wenhao Lv,Yingjie Xia,Xiyuan Chen,Li Kuang*

Main category: cs.AR

TL;DR: This paper introduces ViTAD, an automated method leveraging signal analysis and large language models (LLMs) to address timing violations in RTL stage VLSI design, achieving a superior repair success rate.


<details>
  <summary>Details</summary>
Motivation: Timing optimization at the Register-Transfer Level (RTL) is vital due to its impact on functional reliability and performance in modern high-speed systems, reducing dependence on manual expertise.

Method: ViTAD employs a Signal Timing Dependency Graph (STDG) constructed from Verilog code and timing reports for violation path analysis, utilizes LLMs for root cause inference, and accesses a domain-specific knowledge base for repair strategy generation.

Result: The proposed approach achieves a 73.68% success rate in repairing timing violations, outperforming the baseline LLM method by a margin of 19.30%.

Conclusion: ViTAD demonstrates promising results for automating timing violation repair in VLSI design, highlighting its potential to reduce manual efforts and dependencies on human expertise.

Abstract: In modern Very Large Scale Integrated (VLSI) circuit design flow, the
Register-Transfer Level (RTL) stage presents a critical opportunity for timing
optimization. Addressing timing violations at this early stage is essential, as
modern systems demand higher speeds, where even minor timing violations can
lead to functional failures or system crashes. However, traditional timing
optimization heavily relies on manual expertise, requiring engineers to
iteratively analyze timing reports and debug. To automate this process, this
paper proposes ViTAD, a method that efficiently analyzes the root causes of
timing violations and dynamically generates targeted repair strategies.
Specifically, we first parse Verilog code and timing reports to construct a
Signal Timing Dependency Graph (STDG). Based on the STDG, we perform violation
path analysis and use large language models (LLMs) to infer the root causes of
violations. Finally, by analyzing the causes of violations, we selectively
retrieve relevant debugging knowledge from a domain-specific knowledge base to
generate customized repair solutions. To evaluate the effectiveness of our
method, we construct a timing violation dataset based on real-world open-source
projects. This dataset contains 54 cases of violations. Experimental results
show that our method achieves a 73.68% success rate in repairing timing
violations, while the baseline using only LLM is 54.38%. Our method improves
the success rate by 19.30%.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [55] [Fair Play in the Newsroom: Actor-Based Filtering Gender Discrimination in Text Corpora](https://arxiv.org/abs/2508.13169)
*Stefanie Urchs,Veronika Thurner,Matthias Aßenmacher,Christian Heumann,Stephanie Thiemichen*

Main category: cs.CL

TL;DR: This paper develops a methodology for identifying and addressing gender discrimination in large-scale text datasets using actor-level metrics and corpus rebalancing techniques.


<details>
  <summary>Details</summary>
Motivation: To address structural gender biases in text generation from large language models, which stem from imbalances in training data.

Method: The authors propose actor-level metrics analyzing sentiment, syntactic agency, and quotation styles, combined with a pipeline for corpus analysis and balancing exclusion.

Result: Applying the framework to the German newspaper corpus taz2024full (1980-2024) resulted in noticeable improvements in gender balance but revealed persistent biases in sentiment and framing.

Conclusion: While effective at mitigating surface-level gender biases, deeper biases remain due to entrenched framing and sentiment asymmetries, requiring further refinement. Tools are also made available for ongoing research.

Abstract: Large language models are increasingly shaping digital communication, yet
their outputs often reflect structural gender imbalances that originate from
their training data. This paper presents an extended actor-level pipeline for
detecting and mitigating gender discrimination in large-scale text corpora.
Building on prior work in discourse-aware fairness analysis, we introduce new
actor-level metrics that capture asymmetries in sentiment, syntactic agency,
and quotation styles. The pipeline supports both diagnostic corpus analysis and
exclusion-based balancing, enabling the construction of fairer corpora. We
apply our approach to the taz2024full corpus of German newspaper articles from
1980 to 2024, demonstrating substantial improvements in gender balance across
multiple linguistic dimensions. Our results show that while surface-level
asymmetries can be mitigated through filtering and rebalancing, subtler forms
of bias persist, particularly in sentiment and framing. We release the tools
and reports to support further research in discourse-based fairness auditing
and equitable corpus construction.

</details>


### [56] [MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents](https://arxiv.org/abs/2508.13186)
*Shilong Li,Xingyuan Bu,Wenjie Wang,Jiaheng Liu,Jun Dong,Haoyang He,Hao Lu,Haozhe Zhang,Chenchen Jing,Zhen Li,Chuanhao Li,Jiayi Tian,Chenchen Zhang,Tianhao Peng,Yancheng He,Jihao Gu,Yuanxing Zhang,Jian Yang,Ge Zhang,Wenhao Huang,Wangchunshu Zhou,Zhaoxiang Zhang,Ruizhe Ding,Shilei Wen*

Main category: cs.CL

TL;DR: This paper introduces MM-BrowseComp, a benchmark to evaluate AI agents' multimodal web browsing and reasoning, revealing limitations in current state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitation of existing benchmarks, which focus on textual information, by developing a multimodal benchmark to evaluate AI agents' abilities in handling images, videos, and text during web browsing and reasoning.

Method: The authors designed a benchmark (MM-BrowseComp) with 224 handcrafted questions involving multimodal elements, providing a verified checklist for fine-grained analysis of reasoning paths.

Result: State-of-the-art models achieved only 29.02% accuracy on MM-BrowseComp, exposing their inadequate multimodal capabilities and reasoning skills.

Conclusion: There is a significant gap in current AI models' abilities to perform multimodal reasoning and browsing, emphasizing the need for further development in this area.

Abstract: AI agents with advanced reasoning and tool use capabilities have demonstrated
impressive performance in web browsing for deep search. While existing
benchmarks such as BrowseComp evaluate these browsing abilities, they primarily
focus on textual information, overlooking the prevalence of multimodal content.
To bridge this gap, we introduce MM-BrowseComp, a novel benchmark comprising
224 challenging, hand-crafted questions specifically designed to assess agents'
multimodal retrieval and reasoning capabilities. These questions often
incorporate images in prompts, and crucial information encountered during the
search and reasoning process may also be embedded within images or videos on
webpages. Consequently, methods relying solely on text prove insufficient for
our benchmark. Additionally, we provide a verified checklist for each question,
enabling fine-grained analysis of multimodal dependencies and reasoning paths.
Our comprehensive evaluation of state-of-the-art models on MM-BrowseComp
reveals that even top models like OpenAI o3 with tools achieve only 29.02\%
accuracy, highlighting the suboptimal multimodal capabilities and lack of
native multimodal reasoning in current models.

</details>


### [57] [Overcoming Latency Bottlenecks in On-Device Speech Translation: A Cascaded Approach with Alignment-Based Streaming MT](https://arxiv.org/abs/2508.13358)
*Zeeshan Ahmed,Frank Seide,Niko Moritz,Ju Lin,Ruiming Xie,Simone Merello,Zhe Liu,Christian Fuegen*

Main category: cs.CL

TL;DR: The paper proposes a simultaneous translation approach for real-time, on-device speech translation, improving translation quality and latency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of real-time streaming translation for speech recognition and translation systems, specifically overcoming latency and quality issues.

Method: The authors propose a simultaneous translation approach, use linguistic cues from the ASR, and apply efficient beam-search pruning techniques to maintain real-time processing.

Result: The approach outperforms baseline systems in latency and quality, bringing the performance closer to non-streaming systems.

Conclusion: The paper demonstrates that its techniques improve the feasibility and effectiveness of real-time, on-device speech translation systems.

Abstract: This paper tackles several challenges that arise when integrating Automatic
Speech Recognition (ASR) and Machine Translation (MT) for real-time, on-device
streaming speech translation. Although state-of-the-art ASR systems based on
Recurrent Neural Network Transducers (RNN-T) can perform real-time
transcription, achieving streaming translation in real-time remains a
significant challenge. To address this issue, we propose a simultaneous
translation approach that effectively balances translation quality and latency.
We also investigate efficient integration of ASR and MT, leveraging linguistic
cues generated by the ASR system to manage context and utilizing efficient
beam-search pruning techniques such as time-out and forced finalization to
maintain system's real-time factor. We apply our approach to an on-device
bilingual conversational speech translation and demonstrate that our techniques
outperform baselines in terms of latency and quality. Notably, our technique
narrows the quality gap with non-streaming translation systems, paving the way
for more accurate and efficient real-time speech translation.

</details>


### [58] [Stands to Reason: Investigating the Effect of Reasoning on Idiomaticity Detection](https://arxiv.org/abs/2508.13365)
*Dylan Phelps,Rodrigo Wilkens,Edward Gow-Smith,Thomas Pickard,Maggie Mi,Aline Villavicencio*

Main category: cs.CL

TL;DR: The paper investigates how reasoning capabilities in LLMs affect idiomaticity detection performance, analyzing models with sizes ranging from 1.5B to 70B parameters.


<details>
  <summary>Details</summary>
Motivation: The authors aim to enhance idiomaticity detection in LLMs by leveraging reasoning abilities, addressing the challenge of disambiguating potentially idiomatic expressions.

Method: They evaluated DeepSeek-R1 models of varying sizes across four idiomaticity detection datasets, examining reasoning approaches like chain-of-thought processing and including expression definitions in prompts.

Result: Smaller models benefit from chain-of-thought reasoning but still lag behind larger models, which exhibit better idiomaticity comprehension and accurate definition generation.

Conclusion: Reasoning helps idiomaticity detection, especially in larger LLMs. Providing definitions in prompts improves smaller model performance, suggesting tailored strategies based on model size.

Abstract: The recent trend towards utilisation of reasoning models has improved the
performance of Large Language Models (LLMs) across many tasks which involve
logical steps. One linguistic task that could benefit from this framing is
idiomaticity detection, as a potentially idiomatic expression must first be
understood before it can be disambiguated and serves as a basis for reasoning.
In this paper, we explore how reasoning capabilities in LLMs affect
idiomaticity detection performance and examine the effect of model size. We
evaluate, as open source representative models, the suite of DeepSeek-R1
distillation models ranging from 1.5B to 70B parameters across four
idiomaticity detection datasets. We find the effect of reasoning to be smaller
and more varied than expected. For smaller models, producing chain-of-thought
(CoT) reasoning increases performance from Math-tuned intermediate models, but
not to the levels of the base models, whereas larger models (14B, 32B, and 70B)
show modest improvements. Our in-depth analyses reveal that larger models
demonstrate good understanding of idiomaticity, successfully producing accurate
definitions of expressions, while smaller models often fail to output the
actual meaning. For this reason, we also experiment with providing definitions
in the prompts of smaller models, which we show can improve performance in some
cases.

</details>


### [59] [Whispering Context: Distilling Syntax and Semantics for Long Speech Transcripts](https://arxiv.org/abs/2508.13376)
*Duygu Altinok*

Main category: cs.CL

TL;DR: The paper aims to enhance ASR systems by distilling contextual knowledge from LLaMA models into Whisper, improving transcription quality in tasks like Named Entity Recognition (NER), capitalization, and punctuation, especially in long audio transcripts.


<details>
  <summary>Details</summary>
Motivation: ASR systems often falter in maintaining syntactic and semantic accuracy in longer transcripts, which negatively affects downstream tasks like NER, capitalization, and punctuation.

Method: The paper proposes token-level distillation using optimal transport for alignment and representation loss minimization between sentence embeddings of Whisper and LLaMA, combining syntax and semantics.

Result: Evaluations using the Spoken Wikipedia dataset demonstrate significant improvements in Word Error Rate (WER), NER accuracy, capitalization, and punctuation successes.

Conclusion: Integrating linguistic context via distillation bridges the gap in context-aware ASR performance, providing a foundation for robust transcription models tailored for longform audio inputs.

Abstract: ASR systems often struggle with maintaining syntactic and semantic accuracy
in long audio transcripts, impacting tasks like Named Entity Recognition (NER),
capitalization, and punctuation. We propose a novel approach that enhances ASR
by distilling contextual knowledge from LLaMA models into Whisper. Our method
uses two strategies: (1) token level distillation with optimal transport to
align dimensions and sequence lengths, and (2) representation loss minimization
between sentence embeddings of Whisper and LLaMA, blending syntax and
semantics. Evaluations on the Spoken Wikipedia dataset, a benchmark with long
audios and rich entities demonstrate significant improvements in Word Error
Rate (WER), NER, capitalization, and punctuation success. By introducing novel
NER metrics and exploring semantics aware ASR, our work highlights the value of
integrating linguistic context into transcription, setting a foundation for
robust, context-aware ASR in longform speech.

</details>


### [60] [Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data Analysis](https://arxiv.org/abs/2508.13382)
*Ayoub Ben Chaliah,Hela Dellagi*

Main category: cs.CL

TL;DR: Datarus-R1-14B is a language model specialized in analytical reasoning and problem-solving, trained using unique methods and achieving high accuracy and efficiency in quantitative tasks.


<details>
  <summary>Details</summary>
Motivation: To create a language model capable of advanced analytical reasoning and postgraduate-level problem-solving across quantitative domains.

Method: Datarus was trained using a synthetic dataset of analytical trajectories, a dual-reward framework for evaluation, GRPO optimization for efficient training, and a dynamic curriculum focusing on structure and semantics.

Result: Datarus surpasses similar models (14B parameters) and some larger models (32B parameters) in accuracy for benchmarks like AIME 2024/2025 and LiveCodeBench, while using fewer tokens per solution.

Conclusion: Datarus demonstrates the ability to perform advanced reasoning with higher accuracy and better efficiency, showcasing improved alignment techniques over previous models.

Abstract: We present Datarus-R1-14B, a 14 B-parameter open-weights language model
fine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and
graduate-level problem solver. Datarus is trained not on isolated
question-answer pairs but on full analytical trajectories including reasoning
steps, code execution, error traces, self-corrections, and final conclusions,
all captured in a ReAct-style notebook format spanning finance, medicine,
numerical analysis, and other quantitative domains. Our training pipeline
combines (i) a trajectory-centric synthetic data generator that yielded 144 000
tagged notebook episodes, (ii) a dual-reward framework blending a lightweight
tag-based structural signal with a Hierarchical Reward Model (HRM) that scores
both single-step soundness and end-to-end coherence, and (iii) a
memory-optimized implementation of Group Relative Policy Optimization (GRPO)
featuring KV-cache reuse, sequential generation, and reference-model sharding.
A cosine curriculum smoothly shifts emphasis from structural fidelity to
semantic depth, reducing the format collapse and verbosity that often plague
RL-aligned LLMs. A central design choice in Datarus is it dual reasoning
interface. In agentic mode the model produces ReAct-tagged steps that invoke
Python tools to execute real code; in reflection mode it outputs compact
Chain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On
demanding postgraduate-level problems, Datarus exhibits an "AHA-moment"
pattern: it sketches hypotheses, revises them once or twice, and converges
avoiding the circular, token-inflating loops common to contemporary systems.
Across standard public benchmarks Datarus surpasses similar size models and
even reaches the level of larger reasoning models such as QwQ-32B achieving up
to 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting
18-49% fewer tokens per solution.

</details>


### [61] [ALIGN: Word Association Learning for Cross-Cultural Generalization in Large Language Models](https://arxiv.org/abs/2508.13426)
*Chunhua Liu,Kabir Manandhar Shrestha,Sukai Huang*

Main category: cs.CL

TL;DR: This paper addresses the bias in large language models by fine-tuning them using culturally grounded word-association norms to improve cultural alignment in AI.


<details>
  <summary>Details</summary>
Motivation: Large language models exhibit biases reflecting overrepresented cultural viewpoints due to imbalances in training data. Achieving effective cultural alignment is challenging due to limited cultural knowledge and inadequate learning techniques.

Method: The researchers use parameter-efficient fine-tuning leveraging native speakers' free word-association norms, combined with supervised fine-tuning (SFT) and PPO-based preference optimization, focusing on English-US and Mandarin data.

Result: The fine-tuning approach improves metrics like association precision, concreteness, and emotional valence/arousal, while also enhancing cultural adherence in response distribution for survey questions. Smaller fine-tuned models outperform some costlier 70B parameter baselines.

Conclusion: Fine-tuning with culture-grounded lexical data effectively aligns AI behaviors without extensive retraining, showcasing the practicality of cognitively inspired methods for enhancing cultural sensitivity in language models.

Abstract: As large language models (LLMs) increasingly mediate cross-cultural
communication, their behavior still reflects the distributional bias of the
languages and viewpoints that are over-represented in their pre-training
corpora. Yet, it remains a challenge to model and align culture due to limited
cultural knowledge and a lack of exploration into effective learning
approaches. We introduce a cost-efficient, cognitively grounded remedy:
parameter-efficient fine-tuning on native speakers' free word-association
norms, which encode implicit cultural schemas. Leveraging English-US and
Mandarin associations from the Small-World-of-Words project, we adapt
Llama-3.1-8B and Qwen-2.5-7B via supervised fine-tuning (SFT) and PPO-based
preference optimization. SFT boosts held-out association Precision at 5 by
16-20% in English and 43-165% in Mandarin, lifts median concreteness by +0.20,
and attains human-level valence and arousal. These lexical gains transfer: on
World-Values-Survey questions, fine-tuned models shift answer distributions
toward the target culture, and on a 50-item high-tension subset, Qwen's
Chinese-aligned responses double while Llama's US bias drops by one-third. Our
7-8B models rival or beat vanilla 70B baselines, showing that a few million
culture-grounded associations can instill value alignment without costly
retraining. Our work highlights both the promise and the need for future
research grounded in human cognition in improving cultural alignment in AI
models.

</details>


### [62] [ProMed: Shapley Information Gain Guided Reinforcement Learning for Proactive Medical LLMs](https://arxiv.org/abs/2508.13514)
*Hongxin Ding,Baixiang Huang,Yue Fang,Weibin Liao,Xinke Jiang,Zheng Li,Junfeng Zhao,Yasha Wang*

Main category: cs.CL

TL;DR: This paper introduces ProMed, a reinforcement learning framework that equips medical language models to ask proactive and clinically valuable questions, improving their diagnostic capabilities.


<details>
  <summary>Details</summary>
Motivation: Medical LLMs primarily operate reactively, answering questions without seeking additional information, which can lead to incorrect diagnoses. This paper aims to transition them to a proactive paradigm.

Method: The authors propose ProMed, which uses Shapley Information Gain (SIG) as a reward to quantify the quality of questions. This is integrated into a two-stage RL training pipeline: SIG-guided model initialization and SIG-augmented policy optimization.

Result: ProMed outperforms state-of-the-art methods significantly across two benchmarks, showing an average improvement of 6.29% and a 54.45% gain over reactive approaches, with robust generalization to new domains.

Conclusion: Equipping medical LLMs with proactive questioning capabilities substantially enhances their performance and robustness, bridging a critical gap in interactive clinical settings.

Abstract: Interactive medical questioning is essential in real-world clinical
consultations, where physicians must actively gather information from patients.
While medical Large Language Models (LLMs) have shown impressive capabilities
in static medical question answering, they predominantly operate under a
reactive paradigm: generating answers directly without seeking additional
information, which risks incorrect diagnoses in such interactive settings. To
address this limitation, we propose ProMed, a reinforcement learning (RL)
framework that transitions medical LLMs toward a proactive paradigm, equipping
them with the ability to ask clinically valuable questions before
decision-making. At the core of ProMed is the Shapley Information Gain (SIG)
reward, which quantifies the clinical utility of each question by combining the
amount of newly acquired information with its contextual importance, estimated
via Shapley values. We integrate SIG into a two-stage training pipeline: (1)
SIG-Guided Model Initialization uses Monte Carlo Tree Search (MCTS) to
construct high-reward interaction trajectories to supervise the model, and (2)
SIG-Augmented Policy Optimization, which integrates SIG and enhances RL with a
novel SIG-guided Reward Distribution Mechanism that assigns higher rewards to
informative questions for targeted optimization. Extensive experiments on two
newly curated partial-information medical benchmarks demonstrate that ProMed
significantly outperforms state-of-the-art methods by an average of 6.29% and
delivers a 54.45% gain over the reactive paradigm, while also generalizing
robustly to out-of-domain cases.

</details>


### [63] [Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation](https://arxiv.org/abs/2508.13525)
*Hassan Barmandah*

Main category: cs.CL

TL;DR: This paper focuses on enhancing Arabic large language models (LLMs) to handle Saudi dialects (Najdi and Hijazi) effectively by fine-tuning a model, ALLaM-7B-Instruct-preview, using a Saudi Dialect Instruction dataset. Two fine-tuning strategies (Dialect-Token and No-Token) are compared, with Dialect-Token proving more effective in controlling dialect specificity and improving fidelity.


<details>
  <summary>Details</summary>
Motivation: Modern Standard Arabic dominates Arabic LLMs, leading to limited support for Saudi dialects. There is a need to better represent these dialects to capture authentic language variations and improve model accuracy for specific use cases.

Method: The authors used a private Saudi Dialect Instruction dataset comprising 5,466 synthetic instruction-response pairs. They applied LoRA (low-rank adaptation) to fine-tune ALLaM-7B-Instruct-preview and tested two strategies: adding explicit dialect tags (Dialect-Token) and omitting them (No-Token). They evaluated the models on control, fidelity, and diversity metrics.

Result: The Dialect-Token model improved dialect specificity and fidelity significantly, raising Saudi dialect usage from 47.97% to 84.21% while reducing Modern Standard Arabic leakage from 32.63% to 6.21%. Both fine-tuned models outperformed generic instruction models in controlling dialect and avoiding metadata-tag echoes.

Conclusion: Fine-tuning with Dialect-Token tagging yields superior handling of Saudi dialects in Arabic LLMs. Instead of releasing the dataset or model weights, the authors provide code and data metrics to allow independent validation.

Abstract: Large language models (LLMs) for Arabic are still dominated by Modern
Standard Arabic (MSA), with limited support for Saudi dialects such as Najdi
and Hijazi. This underrepresentation hinders their ability to capture authentic
dialectal variation. Using a privately curated Saudi Dialect Instruction
dataset (Hijazi and Najdi; 5,466 synthetic instruction-response pairs; 50/50
split), we LoRA-tune ALLaM-7B-Instruct-preview, the first foundation model
developed in Saudi Arabia, for Saudi dialect generation. We investigate two
variants: (i) Dialect-Token training, which prepends an explicit dialect tag to
the instruction, and (ii) No-Token training, which omits the tag at formatting
time. Evaluation on a held-out test set combines an external dialect classifier
with text fidelity metrics (chrF++ and BERTScore) and diversity measures. The
Dialect-Token model achieves the best control, raising the Saudi rate from
47.97% to 84.21% and reducing MSA leakage from 32.63% to 6.21%; fidelity also
improves (chrF++ +3.53, BERTScore +0.059). Both LoRA variants outperform strong
generic instruction models (Falcon-7B-Instruct, Llama-3.1-8B-Instruct,
Qwen-2.5-7B-Instruct, AceGPT-v2-8B-Chat, JAIS-13B-Chat) in dialect control and
fidelity, while avoiding metadata-tag echoing that these baselines frequently
exhibit. We do not release the dataset or any model weights/adapters; instead,
we release training/evaluation/inference code and a detailed datasheet (schema
and aggregate statistics) to support independent verification.

</details>


### [64] [MATA (māta): Mindful Assessment of the Telugu Abilities of Large Language Models](https://arxiv.org/abs/2508.13526)
*Chalamalasetti Kranti,Sowmya Vajjala*

Main category: cs.CL

TL;DR: This paper introduces MATA, a dataset to evaluate LLMs in the Telugu language, using 729 curated questions. They analyze LLM performances, expose heuristic reliance, and compare automated and human evaluations.


<details>
  <summary>Details</summary>
Motivation: To identify and analyze the limitations of LLMs in understanding and processing the Telugu language, which is a low-resource language.

Method: The study develops the MATA dataset with 729 linguistically diverse questions, evaluates 11 LLMs, examines heuristic reliance, and compares LLM and human evaluations.

Result: LLMs demonstrated reliance on heuristics for multiple-choice questions, and their reliability in low-resource language evaluations is explored by comparing automated methods with human judgment.

Conclusion: A fine-grained evaluation of LLMs is crucial for improving their linguistic capabilities, especially in low-resource languages like Telugu, and serves as a basis for future research.

Abstract: In this paper, we introduce MATA, a novel evaluation dataset to assess the
ability of Large Language Models (LLMs) in Telugu language, comprising 729
carefully curated multiple-choice and open-ended questions that span diverse
linguistic dimensions. We evaluate 11 open-weight and closed-source LLMs on our
dataset and present a fine-grained analysis of their performance. Further, we
empirically show how LLMs rely on superficial heuristics such as answer
position and distractor patterns for multiple-choice questions. Finally, we
also compare LLM-as-a-judge evaluation with human evaluation for open-ended
questions and draw some conclusions on its reliability in a low-resource
language. We argue that such fine-grained evaluation is essential for
understanding model limitations and can inform the development of more
linguistically capable LLMs, while also serving as a foundation for future
research in Telugu NLP.

</details>


### [65] [Compressed Models are NOT Trust-equivalent to Their Large Counterparts](https://arxiv.org/abs/2508.13533)
*Rohit Raj Rai,Chirag Kothari,Siddhesh Shelke,Amit Awekar*

Main category: cs.CL

TL;DR: Compressed versions of large deep learning models don't maintain trust-equivalence to their original counterparts, despite similar performance measures like accuracy.


<details>
  <summary>Details</summary>
Motivation: To investigate whether compressed models can be as trustworthy as the original large models in terms of their prediction reliability.

Method: A two-dimensional framework was proposed: 1) Interpretability alignment using LIME and SHAP; 2) Calibration similarity using metrics like ECE, MCE, Brier Score, and reliability diagrams.

Result: Experiments with BERT-base and its compressed versions showed low interpretability alignment and significant calibration mismatches, despite similar accuracies.

Conclusion: Compressed models require careful evaluation beyond accuracy metrics before being deployed as replacements for larger models, since they are not trust-equivalent.

Abstract: Large Deep Learning models are often compressed before being deployed in a
resource-constrained environment. Can we trust the prediction of compressed
models just as we trust the prediction of the original large model? Existing
work has keenly studied the effect of compression on accuracy and related
performance measures. However, performance parity does not guarantee
trust-equivalence. We propose a two-dimensional framework for trust-equivalence
evaluation. First, interpretability alignment measures whether the models base
their predictions on the same input features. We use LIME and SHAP tests to
measure the interpretability alignment. Second, calibration similarity measures
whether the models exhibit comparable reliability in their predicted
probabilities. It is assessed via ECE, MCE, Brier Score, and reliability
diagrams. We conducted experiments using BERT-base as the large model and its
multiple compressed variants. We focused on two text classification tasks:
natural language inference and paraphrase identification. Our results reveal
low interpretability alignment and significant mismatch in calibration
similarity. It happens even when the accuracies are nearly identical between
models. These findings show that compressed models are not trust-equivalent to
their large counterparts. Deploying compressed models as a drop-in replacement
for large models requires careful assessment, going beyond performance parity.

</details>


### [66] [A Comparative Study of Decoding Strategies in Medical Text Generation](https://arxiv.org/abs/2508.13580)
*Oriana Presacan,Alireza Nik,Vajira Thambawita,Bogdan Ionescu,Michael Riegler*

Main category: cs.CL

TL;DR: This study evaluates the impact of 11 decoding strategies on text generation quality in medical tasks using both general-purpose and medically specialized LLMs of varying sizes. Deterministic methods outperform stochastic ones, and the choice of decoding method can significantly influence task outcomes.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the critical importance of accuracy in healthcare applications of LLMs and the limited understanding of how different decoding strategies influence performance in medical tasks.

Method: The study evaluates five medical tasks (translation, summarization, question answering, dialogue, and image captioning) using 11 decoding strategies. It employs both general and medically specialized LLMs of different sizes and compares their performance using multiple evaluation metrics.

Result: The findings reveal that deterministic decoding strategies like beam search outperform stochastic methods. Larger models generally perform better but are no more robust to decoding strategies. Medical LLMs excel in certain tasks but lack an overall performance advantage and exhibit greater sensitivity to decoding choices.

Conclusion: Decoding strategy selection is crucial in medical applications, sometimes more so than model choice. Different evaluation metrics may yield varied insights, underscoring the need for task-specific decoding methods to ensure optimal performance.

Abstract: Large Language Models (LLMs) rely on various decoding strategies to generate
text, and these choices can significantly affect output quality. In healthcare,
where accuracy is critical, the impact of decoding strategies remains
underexplored. We investigate this effect in five open-ended medical tasks,
including translation, summarization, question answering, dialogue, and image
captioning, evaluating 11 decoding strategies with medically specialized and
general-purpose LLMs of different sizes. Our results show that deterministic
strategies generally outperform stochastic ones: beam search achieves the
highest scores, while {\eta} and top-k sampling perform worst. Slower decoding
methods tend to yield better quality. Larger models achieve higher scores
overall but have longer inference times and are no more robust to decoding.
Surprisingly, while medical LLMs outperform general ones in two of the five
tasks, statistical analysis shows no overall performance advantage and reveals
greater sensitivity to decoding choice. We further compare multiple evaluation
metrics and find that correlations vary by task, with MAUVE showing weak
agreement with BERTScore and ROUGE, as well as greater sensitivity to the
decoding strategy. These results highlight the need for careful selection of
decoding methods in medical applications, as their influence can sometimes
exceed that of model choice.

</details>


### [67] [Who Gets the Mic? Investigating Gender Bias in the Speaker Assignment of a Speech-LLM](https://arxiv.org/abs/2508.13603)
*Dariia Puhach,Amir H. Payberah,Éva Székely*

Main category: cs.CL

TL;DR: The paper investigates whether Speech-LLMs, like Bark, exhibit gender bias using speaker assignment as an analytic tool. It found no systematic bias but noted some gender awareness and inclinations.


<details>
  <summary>Details</summary>
Motivation: To assess if Speech-LLMs retain gender bias similar to text-based LLMs, focusing on explicit speaker selection as a potential indicator.

Method: The study analyzed Bark's speaker assignments using two datasets—Professions (gender-stereotyped occupations) and Gender-Colored Words (gendered connotations)—to examine its alignment with gendered associations.

Result: Bark does not systematically exhibit bias; however, it demonstrates gender awareness and some inclinations.

Conclusion: Speech-LLMs like Bark do not have systemic gender bias but display sensitivity to gender cues in their training data and design.

Abstract: Similar to text-based Large Language Models (LLMs), Speech-LLMs exhibit
emergent abilities and context awareness. However, whether these similarities
extend to gender bias remains an open question. This study proposes a
methodology leveraging speaker assignment as an analytic tool for bias
investigation. Unlike text-based models, which encode gendered associations
implicitly, Speech-LLMs must produce a gendered voice, making speaker selection
an explicit bias cue. We evaluate Bark, a Text-to-Speech (TTS) model, analyzing
its default speaker assignments for textual prompts. If Bark's speaker
selection systematically aligns with gendered associations, it may reveal
patterns in its training data or model design. To test this, we construct two
datasets: (i) Professions, containing gender-stereotyped occupations, and (ii)
Gender-Colored Words, featuring gendered connotations. While Bark does not
exhibit systematic bias, it demonstrates gender awareness and has some gender
inclinations.

</details>


### [68] [AdaDocVQA: Adaptive Framework for Long Document Visual Question Answering in Low-Resource Settings](https://arxiv.org/abs/2508.13606)
*Haoxuan Li,Wei Song,Aofan Liu,Peiwu Qin*

Main category: cs.CL

TL;DR: AdaDocVQA proposes a unified framework enhancing Document VQA tasks in low-resource contexts, attaining state-of-the-art accuracy on Japanese datasets and emphasizing scalability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in Document VQA, specifically improving performance on long documents under low-resource environments.

Method: AdaDocVQA combines a hybrid text retrieval architecture for segmentation, intelligent data augmentation for QA generation, and adaptive ensemble inference techniques.

Result: Experiments demonstrate improved accuracy: 83.04% for Yes/No, 52.66% for factual, and 44.12% for numerical questions on JDocQA, and 59% accuracy on LAVA.

Conclusion: AdaDocVQA sets new benchmarks in Document VQA for Japanese language and presents a scalable approach for broader applications in low-resource settings.

Abstract: Document Visual Question Answering (Document VQA) faces significant
challenges when processing long documents in low-resource environments due to
context limitations and insufficient training data. This paper presents
AdaDocVQA, a unified adaptive framework addressing these challenges through
three core innovations: a hybrid text retrieval architecture for effective
document segmentation, an intelligent data augmentation pipeline that
automatically generates high-quality reasoning question-answer pairs with
multi-level verification, and adaptive ensemble inference with dynamic
configuration generation and early stopping mechanisms. Experiments on Japanese
document VQA benchmarks demonstrate substantial improvements with 83.04\%
accuracy on Yes/No questions, 52.66\% on factual questions, and 44.12\% on
numerical questions in JDocQA, and 59\% accuracy on LAVA dataset. Ablation
studies confirm meaningful contributions from each component, and our framework
establishes new state-of-the-art results for Japanese document VQA while
providing a scalable foundation for other low-resource languages and
specialized domains. Our code available at:
https://github.com/Haoxuanli-Thu/AdaDocVQA.

</details>


### [69] [CRISP: Persistent Concept Unlearning via Sparse Autoencoders](https://arxiv.org/abs/2508.13650)
*Tomer Ashuach,Dana Arad,Aaron Mueller,Martin Tutek,Yonatan Belinkov*

Main category: cs.CL

TL;DR: CRISP, a parameter-efficient approach, uses sparse autoencoders to conduct persistent concept unlearning in LLMs, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The need to eliminate unwanted knowledge from large language models while maintaining their utility in real-world applications.

Method: CRISP identifies and suppresses salient sparse autoencoder features across layers, enabling persistent adjustments to model behavior.

Result: CRISP outperforms previous approaches on tasks from the WMDP benchmark, successfully removing harmful knowledge while preserving usability.

Conclusion: CRISP offers a precise and effective method for semantically coherent concept suppression, ensuring safety-critical knowledge removal without compromising model utility.

Abstract: As large language models (LLMs) are increasingly deployed in real-world
applications, the need to selectively remove unwanted knowledge while
preserving model utility has become paramount. Recent work has explored sparse
autoencoders (SAEs) to perform precise interventions on monosemantic features.
However, most SAE-based methods operate at inference time, which does not
create persistent changes in the model's parameters. Such interventions can be
bypassed or reversed by malicious actors with parameter access. We introduce
CRISP, a parameter-efficient method for persistent concept unlearning using
SAEs. CRISP automatically identifies salient SAE features across multiple
layers and suppresses their activations. We experiment with two LLMs and show
that our method outperforms prior approaches on safety-critical unlearning
tasks from the WMDP benchmark, successfully removing harmful knowledge while
preserving general and in-domain capabilities. Feature-level analysis reveals
that CRISP achieves semantically coherent separation between target and benign
concepts, allowing precise suppression of the target features.

</details>


### [70] [ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?](https://arxiv.org/abs/2508.13680)
*Vy Tuong Dang,An Vo,Quang Tau,Duc Dm,Daeyoung Kim*

Main category: cs.CL

TL;DR: The paper evaluates Vision Language Models (VLMs) on Vietnamese multimodal educational assessments using a newly proposed benchmark, ViExam.


<details>
  <summary>Details</summary>
Motivation: To assess VLM capabilities in handling cross-lingual multimodal reasoning in low-resource languages, specifically Vietnamese.

Method: The paper introduces the ViExam benchmark with 2,548 multimodal questions across 7 academic domains and evaluates VLMs trained predominantly on English data.

Result: State-of-the-art (SOTA) VLMs achieve a mean accuracy of 57.74%, while open-source models achieve only 27.70%. VLMs underperform average human test-takers at 66.54%, with only one SOTA VLM outperforming humans at 74.07%. Cross-lingual prompting worsens performance, though human-in-the-loop methods slightly increase accuracy by 5 percentage points.

Conclusion: VLMs trained on English data have limited capabilities on cross-lingual multimodal tasks in Vietnamese, highlighting the need for improved methodologies and prompting techniques for broader language inclusion.

Abstract: Vision language models (VLMs) demonstrate remarkable capabilities on English
multimodal tasks, but their performance on low-resource languages with
genuinely multimodal educational content remains largely unexplored. In this
work, we test how VLMs perform on Vietnamese educational assessments,
investigating whether VLMs trained predominantly on English data can handle
real-world cross-lingual multimodal reasoning. Our work presents the first
comprehensive evaluation of VLM capabilities on multimodal Vietnamese exams
through proposing ViExam, a benchmark containing 2,548 multimodal questions. We
find that state-of-the-art VLMs achieve only 57.74% while open-source models
achieve 27.70% mean accuracy across 7 academic domains, including Mathematics,
Physics, Chemistry, Biology, Geography, Driving Test, and IQ Test. Most VLMs
underperform average human test-takers (66.54%), with only the thinking VLM o3
(74.07%) exceeding human average performance, yet still falling substantially
short of human best performance (99.60%). Cross-lingual prompting with English
instructions while maintaining Vietnamese content fails to improve performance,
decreasing accuracy by 1 percentage point for SOTA VLMs. Human-in-the-loop
collaboration can partially improve VLM performance by 5 percentage points.
Code and data are available at: https://vi-exam.github.io.

</details>


### [71] [Generics and Default Reasoning in Large Language Models](https://arxiv.org/abs/2508.13718)
*James Ravi Kirkpatrick,Rachel Katharine Sterken*

Main category: cs.CL

TL;DR: The paper assesses 28 large language models' ability to reason with defeasible patterns, finding mixed performance across models and prompting methods.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs' performance on reasoning patterns involving generics, which are central to non-monotonic logic and critical for default reasoning.

Method: The study tested 28 LLMs on 20 reasoning patterns using various prompting strategies, including few-shot and chain-of-thought (CoT) prompting.

Result: Findings reveal performance variation among models; CoT prompting often worsened accuracy, while few-shot prompting showed modest improvements for some.

Conclusion: Current LLMs show potential but have limitations in handling defeasible reasoning, particularly in distinguishing between defeasible and deductive inference.

Abstract: This paper evaluates the capabilities of 28 large language models (LLMs) to
reason with 20 defeasible reasoning patterns involving generic generalizations
(e.g., 'Birds fly', 'Ravens are black') central to non-monotonic logic.
Generics are of special interest to linguists, philosophers, logicians, and
cognitive scientists because of their complex exception-permitting behaviour
and their centrality to default reasoning, cognition, and concept acquisition.
We find that while several frontier models handle many default reasoning
problems well, performance varies widely across models and prompting styles.
Few-shot prompting modestly improves performance for some models, but
chain-of-thought (CoT) prompting often leads to serious performance degradation
(mean accuracy drop -11.14%, SD 15.74% in models performing above 75% accuracy
in zero-shot condition, temperature 0). Most models either struggle to
distinguish between defeasible and deductive inference or misinterpret generics
as universal statements. These findings underscore both the promise and limits
of current LLMs for default reasoning.

</details>


### [72] [Prediction is not Explanation: Revisiting the Explanatory Capacity of Mapping Embeddings](https://arxiv.org/abs/2508.13729)
*Hanna Herasimchyk,Alhassan Abdelhalim,Sören Laue,Michaela Regneri*

Main category: cs.CL

TL;DR: The paper critiques the reliability of using prediction accuracy to claim interpretability in word embeddings, showing that results may be influenced by algorithmic limitations rather than actual semantic representation.


<details>
  <summary>Details</summary>
Motivation: To enhance the understanding of knowledge encoded in deep learning models and assess the validity of interpretability methods applied to word embeddings.

Method: Critically analyzing common methods used to map word embeddings onto human-interpretable semantic features by experimenting with random feature prediction and observing how datasets interact with embeddings.

Result: Demonstrated that prediction accuracy in these methods may not reflect genuine semantic representation but rather an algorithmic upper bound, complicating comparisons across datasets.

Conclusion: Current methods for interpreting word embeddings through semantic feature predictions may misrepresent meaningful knowledge, suggesting the need for refined approaches to assess interpretability.

Abstract: Understanding what knowledge is implicitly encoded in deep learning models is
essential for improving the interpretability of AI systems. This paper examines
common methods to explain the knowledge encoded in word embeddings, which are
core elements of large language models (LLMs). These methods typically involve
mapping embeddings onto collections of human-interpretable semantic features,
known as feature norms. Prior work assumes that accurately predicting these
semantic features from the word embeddings implies that the embeddings contain
the corresponding knowledge. We challenge this assumption by demonstrating that
prediction accuracy alone does not reliably indicate genuine feature-based
interpretability.
  We show that these methods can successfully predict even random information,
concluding that the results are predominantly determined by an algorithmic
upper bound rather than meaningful semantic representation in the word
embeddings. Consequently, comparisons between datasets based solely on
prediction performance do not reliably indicate which dataset is better
captured by the word embeddings. Our analysis illustrates that such mappings
primarily reflect geometric similarity within vector spaces rather than
indicating the genuine emergence of semantic properties.

</details>


### [73] [EEG-MedRAG: Enhancing EEG-based Clinical Decision-Making via Hierarchical Hypergraph Retrieval-Augmented Generation](https://arxiv.org/abs/2508.13735)
*Yi Wang,Haoran Luo,Lu Meng*

Main category: cs.CL

TL;DR: The paper introduces EEG-MedRAG, a hypergraph-based framework for organizing and interpreting EEG data, and establishes a clinical QA benchmark for evaluation.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of efficiently retrieving and semantically interpreting large-scale and heterogeneous EEG data in neuroscience and clinical practice.

Method: They propose EEG-MedRAG, a three-layer hypergraph-based retrieval framework enabling semantic-temporal retrieval and causal diagnostic generation. A cross-disease, cross-role EEG clinical QA benchmark is also introduced.

Result: Experiments demonstrate that EEG-MedRAG outperforms existing models like TimeRAG and HyperGraphRAG in both accuracy and data retrieval.

Conclusion: EEG-MedRAG shows significant potential in clinical decision support by unifying EEG knowledge and enabling systematic evaluation for diverse clinical perspectives. Data and code are made publicly available.

Abstract: With the widespread application of electroencephalography (EEG) in
neuroscience and clinical practice, efficiently retrieving and semantically
interpreting large-scale, multi-source, heterogeneous EEG data has become a
pressing challenge. We propose EEG-MedRAG, a three-layer hypergraph-based
retrieval-augmented generation framework that unifies EEG domain knowledge,
individual patient cases, and a large-scale repository into a traversable n-ary
relational hypergraph, enabling joint semantic-temporal retrieval and
causal-chain diagnostic generation. Concurrently, we introduce the first
cross-disease, cross-role EEG clinical QA benchmark, spanning seven disorders
and five authentic clinical perspectives. This benchmark allows systematic
evaluation of disease-agnostic generalization and role-aware contextual
understanding. Experiments show that EEG-MedRAG significantly outperforms
TimeRAG and HyperGraphRAG in answer accuracy and retrieval, highlighting its
strong potential for real-world clinical decision support. Our data and code
are publicly available at https://github.com/yi9206413-boop/EEG-MedRAG.

</details>


### [74] [Sycophancy under Pressure: Evaluating and Mitigating Sycophantic Bias via Adversarial Dialogues in Scientific QA](https://arxiv.org/abs/2508.13743)
*Kaiwei Zhang,Qi Jia,Zijian Chen,Wei Sun,Xiangyang Zhu,Chunyi Li,Dandan Zhu,Guangtao Zhai*

Main category: cs.CL

TL;DR: The paper investigates the issue of sycophancy in large language models during scientific Q&A, proposes an evaluation framework to measure its impact, and introduces Pressure-Tune, a method to improve factual accuracy without losing responsiveness.


<details>
  <summary>Details</summary>
Motivation: Large language models often inaccurately align with user beliefs (sycophancy), especially in high-stakes areas like scientific Q&A, which risks distorting knowledge formation and decision-making.

Method: The paper introduces a unified evaluation framework with adversarial prompting and metrics (misleading resistance and sycophancy resistance) to measure how user-induced cues affect factual consistency. It also proposes Pressure-Tune, a fine-tuning method using synthetic dialogues and chain-of-thought rationales.

Result: Systematic evaluations reveal sycophantic tendencies in both open-source and proprietary models, more influenced by alignment strategies than model size. Pressure-Tune significantly improves resistance to faulty user influence while preserving model accuracy and valid responsiveness.

Conclusion: Sycophancy is a critical issue in factual QA, but can be mitigated by using Pressure-Tune, enabling more truthful and reliable language models without sacrificing engagement or correctness in legitimate contexts.

Abstract: Large language models (LLMs), while increasingly used in domains requiring
factual rigor, often display a troubling behavior: sycophancy, the tendency to
align with user beliefs regardless of correctness. This tendency is reinforced
by preference-based alignment techniques that optimize for user satisfaction
but can undermine truthfulness. While relatively benign in casual dialogue,
sycophancy poses serious risks in high-stakes settings such as scientific
question answering (QA), where model outputs may shape collaborative reasoning,
decision-making, and knowledge formation. Despite its importance, this
phenomenon remains underexamined in factual QA contexts. We address this gap by
introducing a unified evaluation framework to quantify the impact of
sycophantic context on model behavior in scientific QA, measuring how much
user-imposed social pressure distorts model outputs. The framework incorporates
adversarial prompting setups and targeted metrics, such as misleading
resistance and sycophancy resistance, that capture a model's ability to
maintain factual consistency under misleading cues. Systematic evaluations
across open-source and proprietary models reveal pervasive sycophantic
tendencies, driven more by alignment strategy than by model size. To mitigate
this issue, we propose Pressure-Tune, a lightweight post-training method that
fine-tunes models on synthetic adversarial dialogues paired with
chain-of-thought rationales. These rationales reject user misinformation while
reinforcing factual commitments. Experiments on challenging scientific QA
benchmarks show that Pressure-Tune significantly enhances sycophancy resistance
without compromising accuracy or responsiveness to valid feedback, offering a
practical pathway toward more truthful and principled model behavior.

</details>


### [75] [MGT-Prism: Enhancing Domain Generalization for Machine-Generated Text Detection via Spectral Alignment](https://arxiv.org/abs/2508.13768)
*Shengchao Liu,Xiaoming Liu,Chengzhengxu Li,Zhaohan Zhang,Guoxin Ma,Yu Lan,Shuai Xiao*

Main category: cs.CL

TL;DR: This paper introduces MGT-Prism, a method for detecting Machine-Generated Text focusing on frequency-domain features for improved domain generalization, achieving better accuracy and F1 scores across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the poor generalization of current Machine-Generated Text detectors across unseen domains, caused by domain shifts.

Method: The method analyzes text representations in the frequency domain, leveraging consistent spectral patterns and integrating a low-frequency domain filtering module with dynamic spectrum alignment to isolate domain-invariant features.

Result: MGT-Prism achieves superior performance, improving accuracy by 0.90% and F1 score by 0.92% compared to state-of-the-art detectors across 11 test datasets in three domain-generalization scenarios.

Conclusion: Analyzing text in the frequency domain is effective for detecting Machine-Generated Text, enabling better generalization to new domains compared to existing methods.

Abstract: Large Language Models have shown growing ability to generate fluent and
coherent texts that are highly similar to the writing style of humans. Current
detectors for Machine-Generated Text (MGT) perform well when they are trained
and tested in the same domain but generalize poorly to unseen domains, due to
domain shift between data from different sources. In this work, we propose
MGT-Prism, an MGT detection method from the perspective of the frequency domain
for better domain generalization. Our key insight stems from analyzing text
representations in the frequency domain, where we observe consistent spectral
patterns across diverse domains, while significant discrepancies in magnitude
emerge between MGT and human-written texts (HWTs). The observation initiates
the design of a low frequency domain filtering module for filtering out the
document-level features that are sensitive to domain shift, and a dynamic
spectrum alignment strategy to extract the task-specific and domain-invariant
features for improving the detector's performance in domain generalization.
Extensive experiments demonstrate that MGT-Prism outperforms state-of-the-art
baselines by an average of 0.90% in accuracy and 0.92% in F1 score on 11 test
datasets across three domain-generalization scenarios.

</details>


### [76] [Can Large Language Models (LLMs) Describe Pictures Like Children? A Comparative Corpus Study](https://arxiv.org/abs/2508.13769)
*Hanna Woloszyn,Benjamin Gagl*

Main category: cs.CL

TL;DR: This paper investigates how well LLMs replicate child-like language by comparing AI-generated text to children's descriptions of picture stories. Results show notable differences in psycholinguistic features and semantic similarity.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this study is to examine the appropriateness of LLM-generated language for child-directed educational tools by evaluating its resemblance to actual child language.

Method: The method involves generating LLM-based corpora using zero-shot and few-shot prompts, followed by a comparative analysis of psycholinguistic properties and semantic similarity with German children's descriptions of picture stories.

Result: The study revealed that LLM-generated text is longer, less lexically rich, relies more on high-frequency words, under-represents nouns, and shows low semantic similarity compared to children's text. Few-shot prompting marginally improved resemblance.

Conclusion: LLMs fail to replicate key lexical and semantic patterns of child language, questioning their suitability for child-directed educational applications. Findings highlight the limitations and potential adjustments required for psycholinguistic and educational uses of these models.

Abstract: The role of large language models (LLMs) in education is increasing, yet
little attention has been paid to whether LLM-generated text resembles child
language. This study evaluates how LLMs replicate child-like language by
comparing LLM-generated texts to a collection of German children's descriptions
of picture stories. We generated two LLM-based corpora using the same picture
stories and two prompt types: zero-shot and few-shot prompts specifying a
general age from the children corpus. We conducted a comparative analysis
across psycholinguistic text properties, including word frequency, lexical
richness, sentence and word length, part-of-speech tags, and semantic
similarity with word embeddings. The results show that LLM-generated texts are
longer but less lexically rich, rely more on high-frequency words, and
under-represent nouns. Semantic vector space analysis revealed low similarity,
highlighting differences between the two corpora on the level of corpus
semantics. Few-shot prompt increased similarities between children and LLM text
to a minor extent, but still failed to replicate lexical and semantic patterns.
The findings contribute to our understanding of how LLMs approximate child
language through multimodal prompting (text + image) and give insights into
their use in psycholinguistic research and education while raising important
questions about the appropriateness of LLM-generated language in child-directed
educational tools.

</details>


### [77] [TracSum: A New Benchmark for Aspect-Based Summarization with Sentence-Level Traceability in Medical Domain](https://arxiv.org/abs/2508.13798)
*Bohao Chu,Meijie Li,Sameh Frihat,Chengyu Gu,Georg Lodde,Elisabeth Livingstone,Norbert Fuhr*

Main category: cs.CL

TL;DR: The paper introduces TracSum, a benchmark focusing on traceable summarization in the medical domain, linking summaries to their original contexts.


<details>
  <summary>Details</summary>
Motivation: Address concerns about the factual inaccuracy of generated summaries in the medical sector by enabling traceability to original evidence.

Method: Annotated 500 medical abstracts, created a summary-citation dataset, developed an evaluation framework, and introduced the Track-Then-Sum baseline for traceable summarization.

Result: Empirical findings show that sentence-level tracking before summarization enhances accuracy, and using full context improves content completeness.

Conclusion: TracSum is effective for evaluating traceable, aspect-based summarization methods, and explicit tracking significantly boosts summarization reliability.

Abstract: While document summarization with LLMs has enhanced access to textual
information, concerns about the factual accuracy of these summaries persist,
especially in the medical domain. Tracing evidence from which summaries are
derived enables users to assess their accuracy, thereby alleviating this
concern. In this paper, we introduce TracSum, a novel benchmark for traceable,
aspect-based summarization, in which generated summaries are paired with
sentence-level citations, enabling users to trace back to the original context.
First, we annotate 500 medical abstracts for seven key medical aspects,
yielding 3.5K summary-citation pairs. We then propose a fine-grained evaluation
framework for this new task, designed to assess the completeness and
consistency of generated content using four metrics. Finally, we introduce a
summarization pipeline, Track-Then-Sum, which serves as a baseline method for
comparison. In experiments, we evaluate both this baseline and a set of LLMs on
TracSum, and conduct a human evaluation to assess the evaluation results. The
findings demonstrate that TracSum can serve as an effective benchmark for
traceable, aspect-based summarization tasks. We also observe that explicitly
performing sentence-level tracking prior to summarization enhances generation
accuracy, while incorporating the full context further improves completeness.

</details>


### [78] [Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding](https://arxiv.org/abs/2508.13804)
*Maciej Skorski,Alina Landowska*

Main category: cs.CL

TL;DR: This study uses a Bayesian framework to evaluate large language models' (LLMs) understanding of moral dimensions by comparing them against human annotators across diverse text datasets.


<details>
  <summary>Details</summary>
Motivation: Investigate how large language models comprehend moral dimensions and compare their performance to human judgment, addressing uncertainties caused by annotator disagreements.

Method: Researchers modeled annotator disagreement by considering aleatoric and epistemic uncertainties and evaluated top LLMs through 250K+ annotations, 700 annotators, and 100K+ texts using a GPU-optimized Bayesian approach.

Result: AI models ranked in the top 25% among human annotators, achieving better-than-average balanced accuracy and producing fewer false negatives, indicating advanced moral detection.

Conclusion: The study demonstrates that large language models excel at detecting moral dimensions, often outperforming human annotators in sensitivity and accuracy.

Abstract: How do large language models understand moral dimensions compared to humans?
  This first large-scale Bayesian evaluation of market-leading language models
provides the answer. In contrast to prior work using deterministic ground truth
(majority or inclusion rules), we model annotator disagreements to capture both
aleatoric uncertainty (inherent human disagreement) and epistemic uncertainty
(model domain sensitivity). We evaluate top language models (Claude Sonnet 4,
DeepSeek-V3, Llama 4 Maverick) across 250K+ annotations from ~700 annotators on
100K+ texts spanning social media, news, and forums.
  Our GPU-optimized Bayesian framework processed 1M+ model queries, revealing
that AI models typically rank among the top 25\% of human annotators, achieving
much better-than-average balanced accuracy. Importantly, we find that AI
produces far fewer false negatives than humans, highlighting their more
sensitive moral detection capabilities.

</details>


### [79] [Towards No-Code Programming of Cobots: Experiments with Code Synthesis by Large Code Models for Conversational Programming](https://arxiv.org/abs/2409.11041)
*Chalamalasetti Kranti,Sherzod Hakimov,David Schlangen*

Main category: cs.CL

TL;DR: The research evaluates Large Language Models (LLMs) for generating assembly-line robot code through natural language instructions, focusing on a designed task (RATS). Results show LLMs can create basic code but struggle with complex abstractions.


<details>
  <summary>Details</summary>
Motivation: Expert programming requirements and limited expressivity in manual guidance hinder the full potential of collaborative robots in assembly lines.

Method: The authors defined the RATS building task and created a dataset pairing target structures with instructions and code examples to evaluate LLMs systematically in a simulated environment.

Result: LLMs accurately generated 'first order code' but encountered issues producing 'higher-order code' involving abstractions like functions or loops.

Conclusion: LLMs can facilitate conversational code generation for collaborative robots to some extent, but their ability to generate complex assembly instructions remains constrained due to abstraction challenges.

Abstract: While there has been a lot of research recently on robots in household
environments, at the present time, most robots in existence can be found on
shop floors, and most interactions between humans and robots happen there.
``Collaborative robots'' (cobots) designed to work alongside humans on assembly
lines traditionally require expert programming, limiting ability to make
changes, or manual guidance, limiting expressivity of the resulting programs.
To address these limitations, we explore using Large Language Models (LLMs),
and in particular, their abilities of doing in-context learning, for
conversational code generation. As a first step, we define RATS, the
``Repetitive Assembly Task'', a 2D building task designed to lay the foundation
for simulating industry assembly scenarios. In this task, a `programmer'
instructs a cobot, using natural language, on how a certain assembly is to be
built; that is, the programmer induces a program, through natural language. We
create a dataset that pairs target structures with various example instructions
(human-authored, template-based, and model-generated) and example code. With
this, we systematically evaluate the capabilities of state-of-the-art LLMs for
synthesising this kind of code, given in-context examples. Evaluating in a
simulated environment, we find that LLMs are capable of generating accurate
`first order code' (instruction sequences), but have problems producing
`higher-order code' (abstractions such as functions, or use of loops).

</details>


### [80] [Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs](https://arxiv.org/abs/2508.13805)
*Juncheng Xie,Hung-yi Lee*

Main category: cs.CL

TL;DR: This paper introduces a prompt-based method for exact length control in text generated by LLMs without fine-tuning, using a countdown prompt mechanism.


<details>
  <summary>Details</summary>
Motivation: Length control in text generation by LLMs remains unreliable, with models often failing to generate desired token counts due to issues in internal token tracking.

Method: The paper proposes a one-shot, prompt-based method that includes countdown markers and explicit counting rules to ensure the model generates text of exact desired length.

Result: The countdown prompt achieves over 95% compliance in length control with GPT-4.1 on tasks like MT-Bench-LI, significantly surpassing naive prompts and maintaining response quality.

Conclusion: Prompt engineering alone enables effective and precise length control in LLMs, providing a lightweight alternative to more complex training or decoding adjustments.

Abstract: Controlling the length of text produced by large language models (LLMs)
remains challenging: models frequently overshoot or undershoot explicit length
instructions because they cannot reliably keep an internal token count. We
present a prompt-based, one-shot strategy that compels an off-the-shelf LLM to
generate exactly a desired number of tokens - words (English) or characters
(Chinese) - without any fine-tuning or iterative sampling. The prompt appends
countdown markers and explicit counting rules so that the model "writes while
counting." We evaluate on four settings: open-ended generation (1-1000 tokens),
XSUM summarization, MT-Bench-LI instruction following, and the LIFEBENCH
equal-length track. On MT-Bench-LI, strict length compliance with GPT-4.1 leaps
from below 30% under naive prompts to above 95% with our countdown prompt,
surpassing the popular draft-then-revise baseline, while judged answer quality
is preserved. These results show that precise length control can be achieved
through prompt engineering alone, offering a lightweight alternative to
training- or decoding-based methods.

</details>


### [81] [The illusion of a perfect metric: Why evaluating AI's words is harder than it looks](https://arxiv.org/abs/2508.13816)
*Maria Paz Oliva,Adriana Correia,Ivan Vankov,Viktor Botev*

Main category: cs.CL

TL;DR: The paper critically examines automatic evaluation metrics (AEM) for natural language generation, highlighting their limitations, variability in effectiveness, and correlation inconsistencies with human judgments.


<details>
  <summary>Details</summary>
Motivation: Evaluating natural language generation is essential yet challenging; while human evaluation is costly and unscalable, automatic metrics have not yet provided a definitive solution.

Method: The authors thoroughly analyze existing AEMs, documenting their methodologies, strengths, limitations, validation methods, and their alignment with human judgments.

Result: Findings reveal persistent challenges, including limited aspects of text quality captured, task-specific effectiveness, unstructured validation practices, and inconsistent correlations with human judgment.

Conclusion: The study suggests selecting metrics based on task-specific needs, using complementary evaluations, and focusing on improved validation methodologies for future metrics.

Abstract: Evaluating Natural Language Generation (NLG) is crucial for the practical
adoption of AI, but has been a longstanding research challenge. While human
evaluation is considered the de-facto standard, it is expensive and lacks
scalability. Practical applications have driven the development of various
automatic evaluation metrics (AEM), designed to compare the model output with
human-written references, generating a score which approximates human judgment.
Over time, AEMs have evolved from simple lexical comparisons, to semantic
similarity models and, more recently, to LLM-based evaluators. However, it
seems that no single metric has emerged as a definitive solution, resulting in
studies using different ones without fully considering the implications. This
paper aims to show this by conducting a thorough examination of the
methodologies of existing metrics, their documented strengths and limitations,
validation methods, and correlations with human judgment. We identify several
key challenges: metrics often capture only specific aspects of text quality,
their effectiveness varies by task and dataset, validation practices remain
unstructured, and correlations with human judgment are inconsistent.
Importantly, we find that these challenges persist in the most recent type of
metric, LLM-as-a-Judge, as well as in the evaluation of Retrieval Augmented
Generation (RAG), an increasingly relevant task in academia and industry. Our
findings challenge the quest for the 'perfect metric'. We propose selecting
metrics based on task-specific needs and leveraging complementary evaluations
and advocate that new metrics should focus on enhanced validation
methodologies.

</details>


### [82] [Extracting Structured Requirements from Unstructured Building Technical Specifications for Building Information Modeling](https://arxiv.org/abs/2508.13833)
*Insaf Nahri,Romain Pinquié,Philippe Véron,Nicolas Bus,Mathieu Thorel*

Main category: cs.CL

TL;DR: The paper integrates Building Information Modeling (BIM) and Natural Language Processing (NLP) to automate the extraction of requirements from unstructured French Building Technical Specification documents, achieving high performance with F1-scores over 90% for NER using CamemBERT and Fr_core_news_lg, and over 80% for RE using Random Forest.


<details>
  <summary>Details</summary>
Motivation: The study aims to address inefficiencies in the construction industry by automating the extraction of building requirements from unstructured French technical documents using advanced NLP techniques.

Method: The paper uses Named Entity Recognition (NER) and Relation Extraction (RE) techniques with transformer models like CamemBERT and Fr_core_news_lg, comparing their performance against rule-based and deep learning-based methods using a handcrafted annotated dataset.

Result: CamemBERT and Fr_core_news_lg excelled in NER tasks, achieving F1-scores above 90%, while Random Forest delivered strong performance in RE tasks with an F1-score greater than 80%.

Conclusion: The integration of advanced NLP models significantly enhances the automation of information extraction from French technical building documents, paving the way for future implementation such as knowledge graph representation and automated verification systems.

Abstract: This study explores the integration of Building Information Modeling (BIM)
with Natural Language Processing (NLP) to automate the extraction of
requirements from unstructured French Building Technical Specification (BTS)
documents within the construction industry. Employing Named Entity Recognition
(NER) and Relation Extraction (RE) techniques, the study leverages the
transformer-based model CamemBERT and applies transfer learning with the French
language model Fr\_core\_news\_lg, both pre-trained on a large French corpus in
the general domain. To benchmark these models, additional approaches ranging
from rule-based to deep learning-based methods are developed. For RE, four
different supervised models, including Random Forest, are implemented using a
custom feature vector. A hand-crafted annotated dataset is used to compare the
effectiveness of NER approaches and RE models. Results indicate that CamemBERT
and Fr\_core\_news\_lg exhibited superior performance in NER, achieving
F1-scores over 90\%, while Random Forest proved most effective in RE, with an
F1 score above 80\%. The outcomes are intended to be represented as a knowledge
graph in future work to further enhance automatic verification systems.

</details>


### [83] [MME-SCI: A Comprehensive and Challenging Science Benchmark for Multimodal Large Language Models](https://arxiv.org/abs/2508.13938)
*Jiacheng Ruan,Dan Jiang,Xian Gao,Ting Liu,Yuzhuo Fu,Yangyang Kang*

Main category: cs.CL

TL;DR: This paper introduces MME-SCI, a multilingual and multimodal benchmark designed to evaluate large language models on reasoning in the scientific domain. It involves rigorous tests in multiple languages and subjects, highlighting existing models' struggles in fine-grained knowledge understanding.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations in current benchmarks for multimodal language models, focusing on their shortcomings in multilingual reasoning, modality coverage, and annotation granularity of scientific knowledge.

Method: MME-SCI was developed by collecting 1,019 question-answer pairs across various subjects and languages, then evaluating 16 open-source and 4 closed-source models based on their reasoning abilities under different modalities.

Result: Experiments show that existing models struggle significantly with MME-SCI's challenges, achieving lower accuracy compared to previous benchmarks and revealing gaps in their capabilities.

Conclusion: MME-SCI effectively highlights weaknesses in multimodal large language models, particularly in multilingual reasoning and scientific domain comprehension, offering a valuable tool for deeper model evaluation.

Abstract: Recently, multimodal large language models (MLLMs) have achieved significant
advancements across various domains, and corresponding evaluation benchmarks
have been continuously refined and improved. In this process, benchmarks in the
scientific domain have played an important role in assessing the reasoning
capabilities of MLLMs. However, existing benchmarks still face three key
challenges: 1) Insufficient evaluation of models' reasoning abilities in
multilingual scenarios; 2) Inadequate assessment of MLLMs' comprehensive
modality coverage; 3) Lack of fine-grained annotation of scientific knowledge
points. To address these gaps, we propose MME-SCI, a comprehensive and
challenging benchmark. We carefully collected 1,019 high-quality
question-answer pairs, which involve 3 distinct evaluation modes. These pairs
cover four subjects, namely mathematics, physics, chemistry, and biology, and
support five languages: Chinese, English, French, Spanish, and Japanese. We
conducted extensive experiments on 16 open-source models and 4 closed-source
models, and the results demonstrate that MME-SCI is widely challenging for
existing MLLMs. For instance, under the Image-only evaluation mode, o4-mini
achieved accuracy of only 52.11%, 24.73%, 36.57%, and 29.80% in mathematics,
physics, chemistry, and biology, respectively, indicating a significantly
higher difficulty level compared to existing benchmarks. More importantly,
using MME-SCI's multilingual and fine-grained knowledge attributes, we analyzed
existing models' performance in depth and identified their weaknesses in
specific domains. The Data and Evaluation Code are available at
https://github.com/JCruan519/MME-SCI.

</details>


### [84] [ReviewGraph: A Knowledge Graph Embedding Based Framework for Review Rating Prediction with Sentiment Features](https://arxiv.org/abs/2508.13953)
*A. J. W. de Vink,Natalia Amat-Lefort,Lifeng Han*

Main category: cs.CL

TL;DR: The paper introduces ReviewGraph, a novel framework for predicting customer review ratings by converting textual reviews into knowledge graphs and using graph embeddings and sentiment features.


<details>
  <summary>Details</summary>
Motivation: To better understand factors influencing customer review ratings in the hospitality industry and to improve guest satisfaction and business performance through novel analytical methods.

Method: The framework converts customer reviews into knowledge graphs with sentiment-integrated (subject, predicate, object) triples, utilizes Node2Vec graph embeddings, and applies machine learning classifiers to predict rating scores. Performance is compared with traditional NLP models and large language models.

Result: ReviewGraph achieves comparable performance to LLMs, outperforms traditional NLP baselines, and exhibits lower computational costs. It also offers advantages in interpretability, visualization, and potential for integration into retrieval systems.

Conclusion: Graph-based representations, as proposed in ReviewGraph, show strong promise for review analytics, providing a computationally efficient and interpretable approach. The model and platform will be open-sourced for future research and improvement.

Abstract: In the hospitality industry, understanding the factors that drive customer
review ratings is critical for improving guest satisfaction and business
performance. This work proposes ReviewGraph for Review Rating Prediction (RRP),
a novel framework that transforms textual customer reviews into knowledge
graphs by extracting (subject, predicate, object) triples and associating
sentiment scores. Using graph embeddings (Node2Vec) and sentiment features, the
framework predicts review rating scores through machine learning classifiers.
We compare ReviewGraph performance with traditional NLP baselines (such as Bag
of Words, TF-IDF, and Word2Vec) and large language models (LLMs), evaluating
them in the HotelRec dataset. In comparison to the state of the art literature,
our proposed model performs similar to their best performing model but with
lower computational cost (without ensemble).
  While ReviewGraph achieves comparable predictive performance to LLMs and
outperforms baselines on agreement-based metrics such as Cohen's Kappa, it
offers additional advantages in interpretability, visual exploration, and
potential integration into Retrieval-Augmented Generation (RAG) systems. This
work highlights the potential of graph-based representations for enhancing
review analytics and lays the groundwork for future research integrating
advanced graph neural networks and fine-tuned LLM-based extraction methods. We
will share ReviewGraph output and platform open-sourced on our GitHub page
https://github.com/aaronlifenghan/ReviewGraph

</details>


### [85] [Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization](https://arxiv.org/abs/2508.13993)
*Shaohua Duan,Xinze Li,Zhenghao Liu,Xiaoyuan Yi,Yukun Yan,Shuo Wang,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.CL

TL;DR: The paper introduces LongMab-PO, a framework using a Multi-Armed Bandit (MAB) strategy to enhance large language models for long-context tasks.


<details>
  <summary>Details</summary>
Motivation: Prior methods for improving long-context capabilities of language models using synthetic data are limited by low diversity and factual inconsistencies.

Method: The LongMab-PO framework uses MAB to iteratively focus on the most informative context segments, generating responses and collecting quality preference data pairs for Direct Preference Optimization (DPO) training.

Result: Experiments demonstrate that LongMab-PO improves the diversity and quality of preference data pairs, achieving state-of-the-art results in long-context reasoning benchmarks.

Conclusion: LongMab-PO effectively enhances the capabilities of large language models for long-context tasks, addressing prior limitations of low data diversity and quality. The method and findings will be made publicly available.

Abstract: Long-context modeling is critical for a wide range of real-world tasks,
including long-context question answering, summarization, and complex reasoning
tasks. Recent studies have explored fine-tuning Large Language Models (LLMs)
with synthetic data to enhance their long-context capabilities. However, the
effectiveness of such approaches is often limited by the low diversity and
factual inconsistencies in the generated data. To address these challenges, we
propose LongMab-PO, a novel framework that leverages a Multi-Armed Bandit (MAB)
rollout strategy to identify the most informative chunks from the given long
context for sampling high-quality and diverse responses and constructing
preference data pairs for Direct Preference Optimization (DPO) training.
Specifically, we treat context chunks as arms of MAB, select chunks based on
their expected reward scores to input into LLMs to generate responses, and
iteratively update these scores based on reward feedback. This exploration and
exploitation process enables the model to focus on the most relevant context
segments, thereby generating and collecting high-quality and diverse responses.
Finally, we collect these generated responses from the rollout process and
apply the DPO method to further optimize the LLM. Experimental results show
that LongMab-PO significantly improves the diversity and quality of preference
data pairs, achieving state-of-the-art performance on long-context reasoning
benchmarks. All code and data will be released on
https://github.com/NEUIR/LongMab-PO.

</details>


### [86] [Ask Good Questions for Large Language Models](https://arxiv.org/abs/2508.14025)
*Qi Wu,Zhongqi Lu*

Main category: cs.CL

TL;DR: This paper introduces the Ask-Good-Question (AGQ) framework, combining an improved CEIRT model and LLMs for effective user guidance in dialog systems.


<details>
  <summary>Details</summary>
Motivation: Address the inability of current dialog systems to discern user confusion and accurately guide topics.

Method: Incorporate the CEIRT model with large language models to create guiding questions that enhance information retrieval.

Result: The proposed model outperformed baseline methods by significantly improving users' information retrieval experiences.

Conclusion: The AGQ framework effectively improves dialog systems by generating guiding questions, enriching user interactions and retrieval efficiency.

Abstract: Recent advances in large language models (LLMs) have significantly improved
the performance of dialog systems, yet current approaches often fail to provide
accurate guidance of topic due to their inability to discern user confusion in
related concepts. To address this, we introduce the Ask-Good-Question (AGQ)
framework, which features an improved Concept-Enhanced Item Response Theory
(CEIRT) model to better identify users' knowledge levels. Our contributions
include applying the CEIRT model along with LLMs to directly generate guiding
questions based on the inspiring text, greatly improving information retrieval
efficiency during the question & answer process. Through comparisons with other
baseline methods, our approach outperforms by significantly enhencing the
users' information retrieval experiences.

</details>


### [87] [Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR](https://arxiv.org/abs/2508.14029)
*Xiao Liang,Zhongzhi Li,Yeyun Gong,Yelong Shen,Ying Nian Wu,Zhijiang Guo,Weizhu Chen*

Main category: cs.CL

TL;DR: This paper proposes a self-improving strategy, SvS, to enhance reinforcement learning with verifiable rewards (RLVR) for training large language models (LLMs), improving their reasoning capabilities and generation diversity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue of reduced generation diversity and entropy collapse during RLVR training, which limits the Pass@k performance of LLMs in complex reasoning tasks.

Method: The method involves enhancing RLVR training by introducing an online Self-play with Variational problem Synthesis (SvS) strategy, which synthesizes variational problems based on the model's correct solutions while keeping reference answers consistent.

Result: The SvS strategy significantly improves Pass@k performance, achieving gains of 18.3% and 22.8% in Pass@32 on AIME24 and AIME25 benchmarks, and demonstrates robustness across 12 reasoning benchmarks and model sizes from 3B to 32B.

Conclusion: The SvS approach effectively mitigates entropy collapse, sustains improvements in generation diversity, and outperforms standard RLVR across benchmarks, highlighting its generalizability and effectiveness in reasoning tasks.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a key paradigm for post-training Large Language Models (LLMs), particularly for
complex reasoning tasks. However, vanilla RLVR training has been shown to
improve Pass@1 performance at the expense of policy entropy, leading to reduced
generation diversity and limiting the Pass@k performance, which typically
represents the upper bound of LLM reasoning capability. In this paper, we
systematically analyze the policy's generation diversity from the perspective
of training problems and find that augmenting and updating training problems
helps mitigate entropy collapse during training. Based on these observations,
we propose an online Self-play with Variational problem Synthesis (SvS)
strategy for RLVR training, which uses the policy's correct solutions to
synthesize variational problems while ensuring their reference answers remain
identical to the originals. This self-improving strategy effectively maintains
policy entropy during training and substantially improves Pass@k compared with
standard RLVR, sustaining prolonged improvements and achieving absolute gains
of 18.3% and 22.8% in Pass@32 performance on the competition-level AIME24 and
AIME25 benchmarks. Experiments on 12 reasoning benchmarks across varying model
sizes from 3B to 32B consistently demonstrate the generalizability and
robustness of SvS.

</details>


### [88] [Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation](https://arxiv.org/abs/2508.14031)
*Dongyoon Hahm,Taywon Min,Woogyeol Jin,Kimin Lee*

Main category: cs.CL

TL;DR: The paper introduces PING, a method to enhance the safety of fine-tuned LLM agents by prepending behavioral-guiding prefixes to their responses.


<details>
  <summary>Details</summary>
Motivation: There is a critical need to address safety concerns when fine-tuning agentic LLMs, as such models can unintentionally become misaligned, increasing the likelihood of harmful behaviors.

Method: PING uses an iterative system to generate and select natural language prefixes, optimizing for both task performance and refusal behavior.

Result: Experimental benchmarks show PING significantly improves safety while maintaining task effectiveness, outperforming existing prompting methods.

Conclusion: PING provides an effective and simple safeguard for fine-tuned LLM agents, ensuring alignment without compromising their task capabilities.

Abstract: Beyond simple text generation, Large Language Models (LLMs) have evolved into
agentic systems capable of planning and interacting with external tools to
solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific
tasks to enhance their proficiency. However, safety concerns are frequently
overlooked during this fine-tuning process. In this work, we show that aligned
LLMs can become unintentionally misaligned, leading to a higher likelihood of
executing harmful tasks and a reduced tendency to refuse them when fine-tuned
to execute agentic tasks. To address these safety challenges, we propose Prefix
INjection Guard (PING), a simple yet effective method that prepends
automatically generated natural language prefixes to agent responses, guiding
them to refuse harmful requests while preserving performance on benign tasks.
Specifically, we introduce an iterative approach that alternates between (1)
generating candidate prefixes and (2) selecting those that optimize both task
performance and refusal behavior. Experimental results demonstrate that PING
significantly enhances the safety of fine-tuned LLM agents without sacrificing
their effectiveness. PING consistently outperforms existing prompting
approaches across diverse benchmarks in both web navigation and code generation
tasks. Our analysis of internal hidden states via linear probes reveals that
prefix tokens are crucial for behavior modification, explaining the performance
gains. WARNING: This paper contains contents that are unethical or offensive in
nature.

</details>


### [89] [The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities](https://arxiv.org/abs/2508.14032)
*Xiancheng Li,Georgios D. Karampatakis,Helen E. Wood,Chris J. Griffiths,Borislava Mihaylova,Neil S. Coulson,Alessio Pasinato,Pietro Panzarasa,Marco Viviani,Anna De Simoni*

Main category: cs.CL

TL;DR: The study explores how Large Language Models (LLMs), through in-context learning and expert-encoded prompts, can effectively conduct sentiment analysis on healthcare data from Online Health Communities, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing digital health analytics struggle with analyzing complex patient-generated content due to the scarcity of domain expertise, data limitations, and privacy concerns.

Method: A structured codebook encoding expert guidelines was developed. Six GPT models and other LLMs were evaluated against pre-trained models and lexicon-based methods on 400 expert-annotated posts from OHCs using targeted prompting for performing sentiment analysis.

Result: LLMs demonstrated superior performance and achieved expert-level agreement with no significant difference compared to inter-expert agreement.

Conclusion: In-context learning and structured prompting enable LLMs to integrate expert knowledge for scalable, high-quality digital health analytics, solving the challenge of domain expertise shortage and enhancing real-time healthcare analysis.

Abstract: Digital health analytics face critical challenges nowadays. The sophisticated
analysis of patient-generated health content, which contains complex emotional
and medical contexts, requires scarce domain expertise, while traditional ML
approaches are constrained by data shortage and privacy limitations in
healthcare settings. Online Health Communities (OHCs) exemplify these
challenges with mixed-sentiment posts, clinical terminology, and implicit
emotional expressions that demand specialised knowledge for accurate Sentiment
Analysis (SA). To address these challenges, this study explores how Large
Language Models (LLMs) can integrate expert knowledge through in-context
learning for SA, providing a scalable solution for sophisticated health data
analysis. Specifically, we develop a structured codebook that systematically
encodes expert interpretation guidelines, enabling LLMs to apply
domain-specific knowledge through targeted prompting rather than extensive
training. Six GPT models validated alongside DeepSeek and LLaMA 3.1 are
compared with pre-trained language models (BioBERT variants) and lexicon-based
methods, using 400 expert-annotated posts from two OHCs. LLMs achieve superior
performance while demonstrating expert-level agreement. This high agreement,
with no statistically significant difference from inter-expert agreement
levels, suggests knowledge integration beyond surface-level pattern
recognition. The consistent performance across diverse LLM models, supported by
in-context learning, offers a promising solution for digital health analytics.
This approach addresses the critical challenge of expert knowledge shortage in
digital health research, enabling real-time, expert-quality analysis for
patient monitoring, intervention assessment, and evidence-based health
strategies.

</details>


### [90] [Uncovering Emergent Physics Representations Learned In-Context by Large Language Models](https://arxiv.org/abs/2508.12448)
*Yeongwoo Song,Jaeyong Bae,Dong-Kyum Kim,Hawoong Jeong*

Main category: cs.CL

TL;DR: The paper explores the in-context learning (ICL) capabilities of large language models (LLMs) using physics-based tasks, uncovering emergent reasoning behaviors and identifying the encoding of meaningful physical concepts within LLMs.


<details>
  <summary>Details</summary>
Motivation: Understanding the mechanisms behind in-context learning in LLMs, especially their ability to reason across diverse domains like physics.

Method: Using dynamics forecasting tasks in physical systems, the authors analyzed residual stream activations in LLMs through sparse autoencoders (SAEs) to study their ability to learn and encode physical concepts in context.

Result: Dynamics forecasting performance in LLMs improved with longer input contexts. Furthermore, features captured by SAEs correlated with key physical variables, like energy, highlighting meaningful physical concept encoding.

Conclusion: LLMs encode meaningful physical concepts during in-context learning, broadening their applicability and enhancing understanding of their emergent reasoning abilities.

Abstract: Large language models (LLMs) exhibit impressive in-context learning (ICL)
abilities, enabling them to solve wide range of tasks via textual prompts
alone. As these capabilities advance, the range of applicable domains continues
to expand significantly. However, identifying the precise mechanisms or
internal structures within LLMs that allow successful ICL across diverse,
distinct classes of tasks remains elusive. Physics-based tasks offer a
promising testbed for probing this challenge. Unlike synthetic sequences such
as basic arithmetic or symbolic equations, physical systems provide
experimentally controllable, real-world data based on structured dynamics
grounded in fundamental principles. This makes them particularly suitable for
studying the emergent reasoning behaviors of LLMs in a realistic yet tractable
setting. Here, we mechanistically investigate the ICL ability of LLMs,
especially focusing on their ability to reason about physics. Using a dynamics
forecasting task in physical systems as a proxy, we evaluate whether LLMs can
learn physics in context. We first show that the performance of dynamics
forecasting in context improves with longer input contexts. To uncover how such
capability emerges in LLMs, we analyze the model's residual stream activations
using sparse autoencoders (SAEs). Our experiments reveal that the features
captured by SAEs correlate with key physical variables, such as energy. These
findings demonstrate that meaningful physical concepts are encoded within LLMs
during in-context learning. In sum, our work provides a novel case study that
broadens our understanding of how LLMs learn in context.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [91] [YOLO11-CR: a Lightweight Convolution-and-Attention Framework for Accurate Fatigue Driving Detection](https://arxiv.org/abs/2508.13205)
*Zhebin Jin,Ligang Dong*

Main category: cs.CV

TL;DR: The paper introduces YOLO11-CR, a vision-based model for driver fatigue detection, offering high accuracy and robustness, addressing challenges in detecting small and occluded objects.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve driver fatigue detection systems which are crucial for preventing traffic accidents, aiming to overcome limitations in current intrusive or hardware-dependent methods.

Method: The method involves developing YOLO11-CR, a lightweight model with two modules: CAFM for enhancing feature expressiveness and RCM for improving spatial localization.

Result: Experiments showed YOLO11-CR achieved strong metrics (precision of 87.17%, recall of 83.86%, and mAP@50 of 88.09%) and outperformed baseline models.

Conclusion: The paper concludes that YOLO11-CR is a practical, effective solution for fatigue monitoring, with future work suggested around temporal modeling and multi-modal data integration.

Abstract: Driver fatigue detection is of paramount importance for intelligent
transportation systems due to its critical role in mitigating road traffic
accidents. While physiological and vehicle dynamics-based methods offer
accuracy, they are often intrusive, hardware-dependent, and lack robustness in
real-world environments. Vision-based techniques provide a non-intrusive and
scalable alternative, but still face challenges such as poor detection of small
or occluded objects and limited multi-scale feature modeling. To address these
issues, this paper proposes YOLO11-CR, a lightweight and efficient object
detection model tailored for real-time fatigue detection. YOLO11-CR introduces
two key modules: the Convolution-and-Attention Fusion Module (CAFM), which
integrates local CNN features with global Transformer-based context to enhance
feature expressiveness; and the Rectangular Calibration Module (RCM), which
captures horizontal and vertical contextual information to improve spatial
localization, particularly for profile faces and small objects like mobile
phones. Experiments on the DSM dataset demonstrated that YOLO11-CR achieves a
precision of 87.17%, recall of 83.86%, mAP@50 of 88.09%, and mAP@50-95 of
55.93%, outperforming baseline models significantly. Ablation studies further
validate the effectiveness of the CAFM and RCM modules in improving both
sensitivity and localization accuracy. These results demonstrate that YOLO11-CR
offers a practical and high-performing solution for in-vehicle fatigue
monitoring, with strong potential for real-world deployment and future
enhancements involving temporal modeling, multi-modal data integration, and
embedded optimization.

</details>


### [92] [MIRAGE: Towards AI-Generated Image Detection in the Wild](https://arxiv.org/abs/2508.13223)
*Cheng Xia,Manxi Lin,Jiexiang Tan,Xiaoxiong Du,Yang Qiu,Junjun Zheng,Xiangheng Kong,Yuning Jiang,Bo Zheng*

Main category: cs.CV

TL;DR: This paper introduces Mirage, a benchmark for detecting AI-generated images in real-world noisy scenarios, and proposes Mirage-R1, a model leveraging advanced reasoning mechanisms to significantly improve detection performance.


<details>
  <summary>Details</summary>
Motivation: AI-generated images, widely proliferated by generative AI, threaten public trust and information security, with existing detection methods struggling in real-world noisy conditions.

Method: The authors create Mirage benchmark from Internet-sourced human-verified data and synthesized datasets. They propose Mirage-R1, a vision-language model utilizing supervised fine-tuning, reinforcement learning, and inference-time adaptive strategies to enhance detection.

Result: Mirage-R1 outperforms state-of-the-art AIGI detectors by 5% on the Mirage benchmark and 10% on public benchmarks, showing robustness in in-the-wild scenarios.

Conclusion: Mirage and Mirage-R1 address current shortcomings in AIGI detection, offering tools and strategies that improve trust in information systems and can handle real-world complexities effectively.

Abstract: The spreading of AI-generated images (AIGI), driven by advances in generative
AI, poses a significant threat to information security and public trust.
Existing AIGI detectors, while effective against images in clean laboratory
settings, fail to generalize to in-the-wild scenarios. These real-world images
are noisy, varying from ``obviously fake" images to realistic ones derived from
multiple generative models and further edited for quality control. We address
in-the-wild AIGI detection in this paper. We introduce Mirage, a challenging
benchmark designed to emulate the complexity of in-the-wild AIGI. Mirage is
constructed from two sources: (1) a large corpus of Internet-sourced AIGI
verified by human experts, and (2) a synthesized dataset created through the
collaboration between multiple expert generators, closely simulating the
realistic AIGI in the wild. Building on this benchmark, we propose Mirage-R1, a
vision-language model with heuristic-to-analytic reasoning, a reflective
reasoning mechanism for AIGI detection. Mirage-R1 is trained in two stages: a
supervised-fine-tuning cold start, followed by a reinforcement learning stage.
By further adopting an inference-time adaptive thinking strategy, Mirage-R1 is
able to provide either a quick judgment or a more robust and accurate
conclusion, effectively balancing inference speed and performance. Extensive
experiments show that our model leads state-of-the-art detectors by 5% and 10%
on Mirage and the public benchmark, respectively. The benchmark and code will
be made publicly available.

</details>


### [93] [DianJin-OCR-R1: Enhancing OCR Capabilities via a Reasoning-and-Tool Interleaved Vision-Language Model](https://arxiv.org/abs/2508.13238)
*Qian Chen,Xianyin Zhang,Lifan Guo,Feng Chen,Chi Zhang*

Main category: cs.CV

TL;DR: This paper proposes DianJin-OCR-R1, a reasoning-enhanced Vision-Language Model (VLM) designed to address hallucinations and improve Optical Character Recognition (OCR) performance using a hybrid approach with expert models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of large vision-language models (LVLMs) in OCR tasks, such as hallucinations and lower task-specific performance compared to expert models.

Method: A new framework called DianJin-OCR-R1 is introduced, which integrates reasoning steps and incorporates domain-specific expert models as references. The model uses its OCR capabilities, calls expert tools for reference, looks at the image again, and reasons for improved accuracy.

Result: DianJin-OCR-R1 outperforms non-reasoning alternatives and specialized expert OCR models in experimental benchmarks such as ReST and OmniDocBench.

Conclusion: The proposed approach of combining reasoning and expert models is effective at improving OCR task performance while mitigating generative hallucinations.

Abstract: Recent advances in large vision-language models (LVLMs) have enabled a new
paradigm of end-to-end document image parsing, excelling in Optical Character
Recognition (OCR) tasks such as text, table, and formula recognition. However,
generative LVLMs, similarly to large language models (LLMs), are prone to
hallucinations--generating words that do not exist in input images.
Furthermore, LVLMs are designed for general purposes and tend to be less
effective on OCR tasks compared to expert models that are trained on
domain-specific datasets. In this paper, we propose DianJin-OCR-R1, a
reasoning-enhanced framework designed to address these limitations through
training reasoning-and-tool interleaved VLMs. Given a recognition instruction,
our DianJin-OCR-R1 model first recognizes the content in the input image by its
own OCR capabilities, and then calls other tools (i.e., other expert models) to
obtain their results as references, finally looks again the image and rethinks
about the reasoning process to provide the final recognized content. Since
architectures of expert models are tailored for specific OCR tasks, which makes
them less prone to hallucinations, their results can help VLMs mitigate
hallucinations. Additionally, expert models are typically smaller in scale and
easy to iterate, enabling performance improvements for VLMs at a lower cost. We
evaluate our model on ReST and OmniDocBench, and experimental results show that
our DianJin-OCR-R1 models consistently outperform their non-reasoning
counterparts and expert OCR models, which proves the effectiveness of our
method.

</details>


### [94] [Exploration of Deep Learning Based Recognition for Urdu Text](https://arxiv.org/abs/2508.13245)
*Sumaiya Fazal,Sheeraz Ahmed*

Main category: cs.CV

TL;DR: The paper introduces a component-based classification approach using CNN to enhance Urdu character recognition, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Urdu script is challenging due to its cursive and context-sensitive nature, posing difficulties for efficient character classification.

Method: The researchers use convolutional neural networks trained on a custom Urdu text dataset, employing connected component techniques for ligature extraction and hierarchical neural networks for classification.

Result: The proposed model demonstrates outstanding performance in component classification with an accuracy of 99%.

Conclusion: Adopting component-based classification and efficient feature learning techniques can significantly improve Urdu optical character recognition systems.

Abstract: Urdu is a cursive script language and has similarities with Arabic and many
other South Asian languages. Urdu is difficult to classify due to its complex
geometrical and morphological structure. Character classification can be
processed further if segmentation technique is efficient, but due to context
sensitivity in Urdu, segmentation-based recognition often results with high
error rate. Our proposed approach for Urdu optical character recognition system
is a component-based classification relying on automatic feature learning
technique called convolutional neural network. CNN is trained and tested on
Urdu text dataset, which is generated through permutation process of three
characters and further proceeds to discarding unnecessary images by applying
connected component technique in order to obtain ligature only. Hierarchical
neural network is implemented with two levels to deal with three degrees of
character permutations and component classification Our model successfully
achieved 0.99% for component classification.

</details>


### [95] [CLoE: Curriculum Learning on Endoscopic Images for Robust MES Classification](https://arxiv.org/abs/2508.13280)
*Zeynep Ozdemir,Hacer Yalim Keles,Omer Ozgur Tanriover*

Main category: cs.CV

TL;DR: The paper introduces CLoE, a curriculum learning framework that improves disease severity grading accuracy in ulcerative colitis using endoscopic images.


<details>
  <summary>Details</summary>
Motivation: To address challenges in Mayo Endoscopic Subscore (MES) classification due to label noise from inter-observer variability and the ordinal nature of the score.

Method: Introducing a curriculum learning framework that uses image quality as a measure of annotation confidence for ordering samples. It also combines ResizeMix augmentation to enhance robustness.

Result: CLoE outperforms supervised and self-supervised baselines on LIMUC and HyperKvasir datasets, achieving up to 82.5% accuracy and 0.894 QWK with ConvNeXt-Tiny.

Conclusion: CLoE effectively leverages difficulty-aware training strategies to improve ordinal classification under label uncertainties in medical imaging tasks.

Abstract: Estimating disease severity from endoscopic images is essential in assessing
ulcerative colitis, where the Mayo Endoscopic Subscore (MES) is widely used to
grade inflammation. However, MES classification remains challenging due to
label noise from inter-observer variability and the ordinal nature of the
score, which standard models often ignore. We propose CLoE, a curriculum
learning framework that accounts for both label reliability and ordinal
structure. Image quality, estimated via a lightweight model trained on Boston
Bowel Preparation Scale (BBPS) labels, is used as a proxy for annotation
confidence to order samples from easy (clean) to hard (noisy). This curriculum
is further combined with ResizeMix augmentation to improve robustness.
Experiments on the LIMUC and HyperKvasir datasets, using both CNNs and
Transformers, show that CLoE consistently improves performance over strong
supervised and self-supervised baselines. For instance, ConvNeXt-Tiny reaches
82.5\% accuracy and a QWK of 0.894 on LIMUC with low computational cost. These
results highlight the potential of difficulty-aware training strategies for
improving ordinal classification under label uncertainty. Code will be released
at https://github.com/zeynepozdemir/CLoE.

</details>


### [96] [Timestep-Compressed Attack on Spiking Neural Networks through Timestep-Level Backpropagation](https://arxiv.org/abs/2508.13812)
*Donghwa Kang,Doohyun Kim,Sang-Ki Ko,Jinkyu Lee,Hyeongboo Baek,Brent ByungHoon Kang*

Main category: cs.CV

TL;DR: The paper introduces TCA, a framework for more efficient adversarial attacks on spiking neural networks, addressing latency issues faced by existing methods.


<details>
  <summary>Details</summary>
Motivation: Current gradient-based adversarial attack methods for spiking neural networks are inefficient due to prolonged attack latencies, making them impractical for real-time applications.

Method: The paper proposes TCA, incorporating timestep-level backpropagation (TLBP) for early stopping and adversarial membrane potential reuse (A-MPR) to bypass the inefficient initial warm-up phase of SNNs.

Result: Experiments on VGG-11 and ResNet-17 using CIFAR-10/100 and CIFAR10-DVS datasets validate that TCA reduces attack latency by up to 56.6% in white-box and 57.1% in black-box settings while achieving comparable attack success rates.

Conclusion: TCA effectively addresses latency issues in adversarial SNN attacks, offering a practical solution for real-time applications without compromising attack performance.

Abstract: State-of-the-art (SOTA) gradient-based adversarial attacks on spiking neural
networks (SNNs), which largely rely on extending FGSM and PGD frameworks, face
a critical limitation: substantial attack latency from multi-timestep
processing, rendering them infeasible for practical real-time applications.
This inefficiency stems from their design as direct extensions of ANN
paradigms, which fail to exploit key SNN properties. In this paper, we propose
the timestep-compressed attack (TCA), a novel framework that significantly
reduces attack latency. TCA introduces two components founded on key insights
into SNN behavior. First, timestep-level backpropagation (TLBP) is based on our
finding that global temporal information in backpropagation to generate
perturbations is not critical for an attack's success, enabling per-timestep
evaluation for early stopping. Second, adversarial membrane potential reuse
(A-MPR) is motivated by the observation that initial timesteps are
inefficiently spent accumulating membrane potential, a warm-up phase that can
be pre-calculated and reused. Our experiments on VGG-11 and ResNet-17 with the
CIFAR-10/100 and CIFAR10-DVS datasets show that TCA significantly reduces the
required attack latency by up to 56.6% and 57.1% compared to SOTA methods in
white-box and black-box settings, respectively, while maintaining a comparable
attack success rate.

</details>


### [97] [GaitCrafter: Diffusion Model for Biometric Preserving Gait Synthesis](https://arxiv.org/abs/2508.13300)
*Sirshapan Mitra,Yogesh S. Rawat*

Main category: cs.CV

TL;DR: The paper introduces GaitCrafter, a diffusion-based framework for generating realistic and controllable gait sequences in challenging conditions, aiding gait recognition tasks.


<details>
  <summary>Details</summary>
Motivation: The limitation of large-scale labeled datasets and difficulty in collecting diverse gait samples for individuals while maintaining privacy constraints creates a significant challenge in gait recognition systems.

Method: GaitCrafter employs a video diffusion model trained from scratch on gait silhouette data. It offers controllable generation by conditioning on factors like clothing, objects, and angles, and can create novel identities through interpolating identity embeddings.

Result: The synthetic gait samples produced by GaitCrafter led to improved recognition performance under challenging conditions and facilitated privacy-aware training with synthetic novel identities.

Conclusion: GaitCrafter demonstrates the potential of diffusion models for generating high-quality and identity-preserving gait patterns, addressing dataset limitations and privacy concerns in biometric applications.

Abstract: Gait recognition is a valuable biometric task that enables the identification
of individuals from a distance based on their walking patterns. However, it
remains limited by the lack of large-scale labeled datasets and the difficulty
of collecting diverse gait samples for each individual while preserving
privacy. To address these challenges, we propose GaitCrafter, a diffusion-based
framework for synthesizing realistic gait sequences in the silhouette domain.
Unlike prior works that rely on simulated environments or alternative
generative models, GaitCrafter trains a video diffusion model from scratch,
exclusively on gait silhouette data. Our approach enables the generation of
temporally consistent and identity-preserving gait sequences. Moreover, the
generation process is controllable-allowing conditioning on various covariates
such as clothing, carried objects, and view angle. We show that incorporating
synthetic samples generated by GaitCrafter into the gait recognition pipeline
leads to improved performance, especially under challenging conditions.
Additionally, we introduce a mechanism to generate novel identities-synthetic
individuals not present in the original dataset-by interpolating identity
embeddings. These novel identities exhibit unique, consistent gait patterns and
are useful for training models while maintaining privacy of real subjects.
Overall, our work takes an important step toward leveraging diffusion models
for high-quality, controllable, and privacy-aware gait data generation.

</details>


### [98] [Prune2Drive: A Plug-and-Play Framework for Accelerating Vision-Language Models in Autonomous Driving](https://arxiv.org/abs/2508.13305)
*Minhao Xiong,Zichen Wen,Zhuangcheng Gu,Xuyang Liu,Rui Zhang,Hengrui Kang,Jiabing Yang,Junyuan Zhang,Weijia Li,Conghui He,Yafei Wang,Linfeng Zhang*

Main category: cs.CV

TL;DR: The paper introduces "Prune2Drive," a framework to reduce computational overhead in Vision-Language Models (VLMs) for autonomous driving via token pruning.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of high computational and memory overhead when processing multi-view, high-resolution images in VLMs for autonomous driving.

Method: Proposed a visual token pruning framework featuring diversity-aware token selection and a view-adaptive pruning controller, which does not require model retraining or attention maps.

Result: Prune2Drive achieves up to 6.40x speedup and reduces FLOPs by 86.6% while retaining comparable task performance with only a 3% drop.

Conclusion: Prune2Drive enhances computational efficiency in autonomous driving VLMs, making them more feasible for deployment without significant sacrifices in performance.

Abstract: Vision-Language Models (VLMs) have emerged as a promising paradigm in
autonomous driving (AD), offering a unified framework for perception,
reasoning, and decision-making by jointly modeling visual inputs and natural
language instructions. However, their deployment is hindered by the significant
computational overhead incurred when processing high-resolution, multi-view
images, a standard setup in AD systems with six or more synchronized cameras.
This overhead stems from the large number of visual tokens generated during
encoding, increasing inference latency and memory consumption due to the
quadratic complexity of self-attention. To address these challenges, we propose
Prune2Drive, a plug-and-play visual token pruning framework for multi-view VLMs
in autonomous driving. Prune2Drive introduces two core innovations: (i) a
diversity-aware token selection mechanism inspired by farthest point sampling,
which prioritizes semantic and spatial coverage across views rather than
relying solely on attention scores, and (ii) a view-adaptive pruning controller
that learns optimal pruning ratios for each camera view based on their
importance to downstream driving tasks. Unlike prior methods, Prune2Drive does
not require model retraining or access to attention maps, making it compatible
with modern efficient attention implementations. Extensive experiments on two
large-scale multi-view driving benchmarks, DriveLM and DriveLMM-o1, show that
Prune2Drive achieves significant speedups and memory savings while maintaining
or improving task performance. When retaining only 10% of the visual tokens,
our method achieves a 6.40$\times$ speedup in the prefilling phase and consumes
13.4% of the original FLOPs, with only a 3% performance drop on the DriveLM
benchmark.

</details>


### [99] [DAASH: A Meta-Attack Framework for Synthesizing Effective and Stealthy Adversarial Examples](https://arxiv.org/abs/2508.13309)
*Abdullah Al Nomaan Nafi,Habibur Rahaman,Zafaryab Haider,Tanzim Mahfuz,Fnu Suya,Swarup Bhunia,Prabuddha Chakraborty*

Main category: cs.CV

TL;DR: DAASH introduces a meta-attack framework that combines Lp-based attack methods to generate perceptually aligned adversarial examples. It outperforms previous techniques in success rates and visual quality metrics.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the shortcomings of Lp-norm constrained attacks for producing adversarial examples that align well with human perception and explores enhancing perceptual efficacy using insights from these attacks.

Method: DAASH is a multi-stage framework that aggregates adversarial examples from multiple base attacks using adaptive weights, guided by a meta-loss function that jointly minimizes misclassification loss and perceptual distortion.

Result: DAASH achieves higher attack success rates and superior visual quality compared to state-of-the-art perceptual attacks, with significant improvements in metrics like SSIM, LPIPS, and FID.

Conclusion: DAASH effectively generates perceptually aligned adversarial examples, improving attack success and quality. It generalizes to unseen defenses, serving as a robust baseline for evaluating model robustness without the need for adaptive handcrafted attacks.

Abstract: Numerous techniques have been proposed for generating adversarial examples in
white-box settings under strict Lp-norm constraints. However, such norm-bounded
examples often fail to align well with human perception, and only recently have
a few methods begun specifically exploring perceptually aligned adversarial
examples. Moreover, it remains unclear whether insights from Lp-constrained
attacks can be effectively leveraged to improve perceptual efficacy. In this
paper, we introduce DAASH, a fully differentiable meta-attack framework that
generates effective and perceptually aligned adversarial examples by
strategically composing existing Lp-based attack methods. DAASH operates in a
multi-stage fashion: at each stage, it aggregates candidate adversarial
examples from multiple base attacks using learned, adaptive weights and
propagates the result to the next stage. A novel meta-loss function guides this
process by jointly minimizing misclassification loss and perceptual distortion,
enabling the framework to dynamically modulate the contribution of each base
attack throughout the stages. We evaluate DAASH on adversarially trained models
across CIFAR-10, CIFAR-100, and ImageNet. Despite relying solely on
Lp-constrained based methods, DAASH significantly outperforms state-of-the-art
perceptual attacks such as AdvAD -- achieving higher attack success rates
(e.g., 20.63\% improvement) and superior visual quality, as measured by SSIM,
LPIPS, and FID (improvements $\approx$ of 11, 0.015, and 5.7, respectively).
Furthermore, DAASH generalizes well to unseen defenses, making it a practical
and strong baseline for evaluating robustness without requiring handcrafted
adaptive attacks for each new defense.

</details>


### [100] [Automated Assessment of Aesthetic Outcomes in Facial Plastic Surgery](https://arxiv.org/abs/2508.13363)
*Pegah Varghaei,Kiran Abraham-Aggarwal,Manoj T. Abraham,Arun Ross*

Main category: cs.CV

TL;DR: The paper proposes an interpretable computer-vision framework for assessing facial plastic surgery outcomes, utilizing a large dataset of pre- and post-surgery images for analyses of facial symmetry, age estimation, and nasal morphology.


<details>
  <summary>Details</summary>
Motivation: To address the need for scalable, objective, and reproducible methods in analyzing aesthetic outcomes of facial plastic surgery.

Method: The approach includes automated landmark detection, computation of geometric facial symmetry, deep learning for age estimation, nasal morphology analysis, and assembling a large dataset of paired pre- and post-surgery images.

Result: Key results indicate improvement in nasal measurements for 96.2% of rhinoplasty patients and enhancements in global facial symmetry or perceived age for 71.3% of broader subjects, demonstrating consistent patient identity post-surgery.

Conclusion: The framework provides reproducible metrics and benchmarks for surgical planning, patient counseling, and assessing surgery outcomes, underlining its potential in improving inter-practitioner comparability and care quality.

Abstract: We introduce a scalable, interpretable computer-vision framework for
quantifying aesthetic outcomes of facial plastic surgery using frontal
photographs. Our pipeline leverages automated landmark detection, geometric
facial symmetry computation, deep-learning-based age estimation, and nasal
morphology analysis. To perform this study, we first assemble the largest
curated dataset of paired pre- and post-operative facial images to date,
encompassing 7,160 photographs from 1,259 patients. This dataset includes a
dedicated rhinoplasty-only subset consisting of 732 images from 366 patients,
96.2% of whom showed improvement in at least one of the three nasal
measurements with statistically significant group-level change. Among these
patients, the greatest statistically significant improvements (p < 0.001)
occurred in the alar width to face width ratio (77.0%), nose length to face
height ratio (41.5%), and alar width to intercanthal ratio (39.3%). Among the
broader frontal-view cohort, comprising 989 rigorously filtered subjects, 71.3%
exhibited significant enhancements in global facial symmetry or perceived age
(p < 0.01). Importantly, our analysis shows that patient identity remains
consistent post-operatively, with True Match Rates of 99.5% and 99.6% at a
False Match Rate of 0.01% for the rhinoplasty-specific and general patient
cohorts, respectively. Additionally, we analyze inter-practitioner variability
in improvement rates. By providing reproducible, quantitative benchmarks and a
novel dataset, our pipeline facilitates data-driven surgical planning, patient
counseling, and objective outcome evaluation across practices.

</details>


### [101] [Applications of Small Language Models in Medical Imaging Classification with a Focus on Prompt Strategies](https://arxiv.org/abs/2508.13378)
*Yiting Wang,Ziwei Wang,Jiachen Zhong,Di Zhu,Weiyi Li*

Main category: cs.CV

TL;DR: The study evaluates small language models (SLMs) using prompt engineering techniques to classify chest X-ray positions, finding they can achieve competitive accuracy with well-crafted prompts.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges of large language models (LLMs) like high computational cost and limited accessibility in resource-constrained healthcare environments.

Method: The authors tested various SLMs on the NIH Chest X-ray dataset using three different prompt strategies: baseline instruction, incremental summary prompts, and correction-based reflective prompts.

Result: Certain SLMs, when combined with effective prompt designs, matched competitive accuracy levels in classifying chest X-ray positions.

Conclusion: Prompt engineering can be leveraged to enhance SLM performance in healthcare, enabling usability without requiring extensive AI expertise.

Abstract: Large language models (LLMs) have shown remarkable capabilities in natural
language processing and multi-modal understanding. However, their high
computational cost, limited accessibility, and data privacy concerns hinder
their adoption in resource-constrained healthcare environments. This study
investigates the performance of small language models (SLMs) in a medical
imaging classification task, comparing different models and prompt designs to
identify the optimal combination for accuracy and usability. Using the NIH
Chest X-ray dataset, we evaluate multiple SLMs on the task of classifying chest
X-ray positions (anteroposterior [AP] vs. posteroanterior [PA]) under three
prompt strategies: baseline instruction, incremental summary prompts, and
correction-based reflective prompts. Our results show that certain SLMs achieve
competitive accuracy with well-crafted prompts, suggesting that prompt
engineering can substantially enhance SLM performance in healthcare
applications without requiring deep AI expertise from end users.

</details>


### [102] [AIM 2025 Rip Current Segmentation (RipSeg) Challenge Report](https://arxiv.org/abs/2508.13401)
*Andrei Dumitriu,Florin Miron,Florin Tatui,Radu Tudor Ionescu,Radu Timofte,Aakash Ralhan,Florin-Alexandru Vasluianu,Shenyang Qian,Mitchell Harley,Imran Razzak,Yang Song,Pu Luo,Yumei Li,Cong Xu,Jinming Chai,Kexin Zhang,Licheng Jiao,Lingling Li,Siqi Yu,Chao Zhang,Kehuan Song,Fang Liu,Puhua Chen,Xu Liu,Jin Hu,Jinyang Xu,Biao Liu*

Main category: cs.CV

TL;DR: AIM 2025 RipSeg Challenge tackled automatic segmentation of rip currents in images using RipVIS dataset; results showed strengths of deep learning techniques.


<details>
  <summary>Details</summary>
Motivation: Rip currents are hazardous and underexplored for visual detection; this competition aims to advance segmentation methods and enhance beach safety.

Method: Competition organized with RipVIS dataset; tested segmentation methods using composite score metrics: $F_1$, $F_2$, $AP_{50}$, $AP_{[50:95]}$.

Result: 75 participants, 5 valid submissions with winners utilizing deep learning and domain-specific strategies; benchmark insights provided.

Conclusion: Learnings shared on rip current segmentation; calls for expanded research focusing on diverse data, challenges, and improved methods in future iterations.

Abstract: This report presents an overview of the AIM 2025 RipSeg Challenge, a
competition designed to advance techniques for automatic rip current
segmentation in still images. Rip currents are dangerous, fast-moving flows
that pose a major risk to beach safety worldwide, making accurate visual
detection an important and underexplored research task. The challenge builds on
RipVIS, the largest available rip current dataset, and focuses on single-class
instance segmentation, where precise delineation is critical to fully capture
the extent of rip currents. The dataset spans diverse locations, rip current
types, and camera orientations, providing a realistic and challenging
benchmark.
  In total, $75$ participants registered for this first edition, resulting in
$5$ valid test submissions. Teams were evaluated on a composite score combining
$F_1$, $F_2$, $AP_{50}$, and $AP_{[50:95]}$, ensuring robust and
application-relevant rankings. The top-performing methods leveraged deep
learning architectures, domain adaptation techniques, pretrained models, and
domain generalization strategies to improve performance under diverse
conditions.
  This report outlines the dataset details, competition framework, evaluation
metrics, and final results, providing insights into the current state of rip
current segmentation. We conclude with a discussion of key challenges, lessons
learned from the submissions, and future directions for expanding RipSeg.

</details>


### [103] [Mitigating Easy Option Bias in Multiple-Choice Question Answering](https://arxiv.org/abs/2508.13428)
*Hao Zhang,Chen Li,Basura Fernando*

Main category: cs.CV

TL;DR: The paper identifies an Easy-Options Bias (EOB) in certain Visual Question Answering (VQA) benchmarks, where models can infer answers using only visual input and answer options, circumventing the need for questions. It proposes GroundAttack, a tool to automatically generate challenging negative options to mitigate this bias.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the Easy-Options Bias (EOB), which compromises the evaluation of vision-language models (VLMs) by allowing them to infer answers without using the provided questions.

Method: The researchers analyze multiple VQA benchmarks, trace the bias to visual relevance imbalance, and introduce GroundAttack, a toolkit that creates visually plausible hard negative options for improving dataset annotations.

Result: GroundAttack removes EOB in datasets like NExT-QA and MMStar, leading to VLM performance reductions to random accuracy levels under (V+O) and more realistic outcomes under (V+Q+O).

Conclusion: GroundAttack improves the reliability of VQA benchmark evaluations by eliminating visually biased shortcuts, enabling a more accurate assessment of the true QA abilities of VLMs.

Abstract: In this early study, we observe an Easy-Options Bias (EOB) issue in some
multiple-choice Visual Question Answering (VQA) benchmarks such as MMStar,
RealWorldQA, SEED-Bench, Next-QA, STAR benchmark and Video-MME. This bias
allows vision-language models (VLMs) to select the correct answer using only
the vision (V) and options (O) as inputs, without the need for the question
(Q). Through grounding experiments, we attribute the bias to an imbalance in
visual relevance: the correct answer typically aligns more closely with the
visual contents than the negative options in feature space, creating a shortcut
for VLMs to infer the answer via simply vision-option similarity matching. To
fix this, we introduce GroundAttack, a toolkit that automatically generates
hard negative options as visually plausible as the correct answer. We apply it
to the NExT-QA and MMStar datasets, creating new EOB-free annotations. On these
EOB-free annotations, current VLMs approach to random accuracies under (V+O)
settings, and drop to non-saturated accuracies under (V+Q+O) settings,
providing a more realistic evaluation of VLMs' QA ability. Codes and new
annotations will be released soon.

</details>


### [104] [Structured Prompting and Multi-Agent Knowledge Distillation for Traffic Video Interpretation and Risk Inference](https://arxiv.org/abs/2508.13439)
*Yunxiang Yang,Ningning Xu,Jidong J. Yang*

Main category: cs.CV

TL;DR: The paper introduces VISTA, a compact Vision-Language Model (VLM) for traffic scene understanding and risk inference, leveraging structured prompting and knowledge distillation.


<details>
  <summary>Details</summary>
Motivation: To improve scalability and generalization in traffic risk assessment and scene understanding for ITS and autonomous driving, overcoming limitations in traditional approaches.

Method: A framework using structured Chain-of-Thought prompting with two large VLMs to produce pseudo-annotations for fine-tuning a smaller student model.

Result: The smaller model, VISTA, achieves strong performance on captioning metrics while being lightweight and deployable on edge devices.

Conclusion: Structured multi-agent supervision and knowledge distillation can enable compact models to perform complex reasoning tasks efficiently for real-time traffic monitoring.

Abstract: Comprehensive highway scene understanding and robust traffic risk inference
are vital for advancing Intelligent Transportation Systems (ITS) and autonomous
driving. Traditional approaches often struggle with scalability and
generalization, particularly under the complex and dynamic conditions of
real-world environments. To address these challenges, we introduce a novel
structured prompting and knowledge distillation framework that enables
automatic generation of high-quality traffic scene annotations and contextual
risk assessments. Our framework orchestrates two large Vision-Language Models
(VLMs): GPT-4o and o3-mini, using a structured Chain-of-Thought (CoT) strategy
to produce rich, multi-perspective outputs. These outputs serve as
knowledge-enriched pseudo-annotations for supervised fine-tuning of a much
smaller student VLM. The resulting compact 3B-scale model, named VISTA (Vision
for Intelligent Scene and Traffic Analysis), is capable of understanding
low-resolution traffic videos and generating semantically faithful, risk-aware
captions. Despite its significantly reduced parameter count, VISTA achieves
strong performance across established captioning metrics (BLEU-4, METEOR,
ROUGE-L, and CIDEr) when benchmarked against its teacher models. This
demonstrates that effective knowledge distillation and structured multi-agent
supervision can empower lightweight VLMs to capture complex reasoning
capabilities. The compact architecture of VISTA facilitates efficient
deployment on edge devices, enabling real-time risk monitoring without
requiring extensive infrastructure upgrades.

</details>


### [105] [EDTalk++: Full Disentanglement for Controllable Talking Head Synthesis](https://arxiv.org/abs/2508.13442)
*Shuai Tan,Bin Ji*

Main category: cs.CV

TL;DR: EDTalk++ enables fully disentangled control over various facial motions for talking head generation, offering independence and modularity in manipulation by decomposing facial dynamics into four latent spaces.


<details>
  <summary>Details</summary>
Motivation: Enable better control and flexibility in talking head generation by achieving independent and modular manipulation of facial dynamics while supporting diverse input modalities.

Method: Develop a disentangled framework (EDTalk++) with four lightweight modules to separate facial motions into latent spaces, ensuring orthogonality among them and training to allocate responsibilities without external knowledge.

Result: EDTalk++ enables independent control over mouth, pose, eyes, and expression, and supports both video and audio inputs using shared visual priors.

Conclusion: EDTalk++ achieves controllable and modular talking head synthesis, demonstrating its effectiveness through experiments.

Abstract: Achieving disentangled control over multiple facial motions and accommodating
diverse input modalities greatly enhances the application and entertainment of
the talking head generation. This necessitates a deep exploration of the
decoupling space for facial features, ensuring that they a) operate
independently without mutual interference and b) can be preserved to share with
different modal inputs, both aspects often neglected in existing methods. To
address this gap, this paper proposes EDTalk++, a novel full disentanglement
framework for controllable talking head generation. Our framework enables
individual manipulation of mouth shape, head pose, eye movement, and emotional
expression, conditioned on video or audio inputs. Specifically, we employ four
lightweight modules to decompose the facial dynamics into four distinct latent
spaces representing mouth, pose, eye, and expression, respectively. Each space
is characterized by a set of learnable bases whose linear combinations define
specific motions. To ensure independence and accelerate training, we enforce
orthogonality among bases and devise an efficient training strategy to allocate
motion responsibilities to each space without relying on external knowledge.
The learned bases are then stored in corresponding banks, enabling shared
visual priors with audio input. Furthermore, considering the properties of each
space, we propose an Audio-to-Motion module for audio-driven talking head
synthesis. Experiments are conducted to demonstrate the effectiveness of
EDTalk++.

</details>


### [106] [Revisiting MLLM Token Technology through the Lens of Classical Visual Coding](https://arxiv.org/abs/2508.13460)
*Jinming Liu,Junyan Lin,Yuntao Wei,Kele Shao,Keda Tao,Jianguo Huang,Xudong Yang,Zhibo Chen,Huan Wang,Xin Jin*

Main category: cs.CV

TL;DR: This paper explores the relationship between token technology in Multimodal Large Language Models (MLLMs) and classical visual coding, proposing a unified framework for comparative analysis and cross-discipline enhancement.


<details>
  <summary>Details</summary>
Motivation: The authors seek to improve efficiency and robustness in MLLM token technology and advance semantic visual codecs by leveraging visual coding principles and vice versa.

Method: The paper establishes a unified framework for token technology and visual coding, synthesizes bidirectional insights, and identifies future research directions through module-by-module comparative analysis.

Result: A unified formulation is created to connect MLLM token technology with visual coding, providing mutual benefits and inspiring research opportunities in multimodal models and visual codecs.

Conclusion: This pioneering comparative study bridges MLLM token technology and visual coding, enabling advancements in both fields and laying groundwork for future efficient and robust multimodal systems.

Abstract: Classical visual coding and Multimodal Large Language Model (MLLM) token
technology share the core objective - maximizing information fidelity while
minimizing computational cost. Therefore, this paper reexamines MLLM token
technology, including tokenization, token compression, and token reasoning,
through the established principles of long-developed visual coding area. From
this perspective, we (1) establish a unified formulation bridging token
technology and visual coding, enabling a systematic, module-by-module
comparative analysis; (2) synthesize bidirectional insights, exploring how
visual coding principles can enhance MLLM token techniques' efficiency and
robustness, and conversely, how token technology paradigms can inform the
design of next-generation semantic visual codecs; (3) prospect for promising
future research directions and critical unsolved challenges. In summary, this
study presents the first comprehensive and structured technology comparison of
MLLM token and visual coding, paving the way for more efficient multimodal
models and more powerful visual codecs simultaneously.

</details>


### [107] [Vision Transformers for Kidney Stone Image Classification: A Comparative Study with CNNs](https://arxiv.org/abs/2508.13461)
*Ivan Reyes-Amezcua,Francisco Lopez-Tiro,Clement Larose,Andres Mendez-Vazquez,Gilberto Ochoa-Ruiz,Christian Daul*

Main category: cs.CV

TL;DR: The paper investigates the use of Vision Transformers (ViTs) vs CNN models for kidney stone classification from endoscopic images, finding ViTs notably outperform CNNs across different datasets and imaging conditions.


<details>
  <summary>Details</summary>
Motivation: The motivation is improving kidney stone classification for personalized treatment and recurrence prevention, addressing the limitations of CNNs in capturing long-range dependencies under varied imaging conditions.

Method: A comparative analysis was conducted between ViTs and CNNs, using pretrained ViT and ResNet50 models on two ex vivo datasets of CCD camera and ureteroscope images.

Result: The ViT model consistently outperformed ResNet50, achieving up to 95.2% accuracy and 95.1% F1-score in complex cases, compared to ResNet50's lower metrics (64.5% accuracy, 59.3% F1-score).

Conclusion: ViT-based architectures are superior to CNNs for kidney stone classification and offer a scalable method for endoscopic image analysis.

Abstract: Kidney stone classification from endoscopic images is critical for
personalized treatment and recurrence prevention. While convolutional neural
networks (CNNs) have shown promise in this task, their limited ability to
capture long-range dependencies can hinder performance under variable imaging
conditions. This study presents a comparative analysis between Vision
Transformers (ViTs) and CNN-based models, evaluating their performance on two
ex vivo datasets comprising CCD camera and flexible ureteroscope images. The
ViT-base model pretrained on ImageNet-21k consistently outperformed a ResNet50
baseline across multiple imaging conditions. For instance, in the most visually
complex subset (Section patches from endoscopic images), the ViT model achieved
95.2% accuracy and 95.1% F1-score, compared to 64.5% and 59.3% with ResNet50.
In the mixed-view subset from CCD-camera images, ViT reached 87.1% accuracy
versus 78.4% with CNN. These improvements extend across precision and recall as
well. The results demonstrate that ViT-based architectures provide superior
classification performance and offer a scalable alternative to conventional
CNNs for kidney stone image analysis.

</details>


### [108] [STER-VLM: Spatio-Temporal With Enhanced Reference Vision-Language Models](https://arxiv.org/abs/2508.13470)
*Tinh-Anh Nguyen-Nhu,Triet Dao Hoang Minh,Dat To-Thanh,Phuc Le-Gia,Tuan Vo-Lan,Tien-Huy Nguyen*

Main category: cs.CV

TL;DR: STER-VLM is a novel framework enhancing vision-language models for traffic analysis using efficient techniques, achieving high performance and resource efficiency.


<details>
  <summary>Details</summary>
Motivation: To address high computational demands and limitations in fine-grained understanding of current vision-language models for traffic analysis.

Method: STER-VLM employs caption decomposition, temporal frame selection with filtering, reference-driven understanding, and curated prompts to improve efficiency and understanding.

Result: The framework achieves significant semantic richness, effective traffic scene interpretation, and a strong performance score of 55.655 in the AI City Challenge 2025 Track 2.

Conclusion: STER-VLM advances resource-efficient and precise traffic analysis, demonstrating practical potential for real-world traffic scenarios.

Abstract: Vision-language models (VLMs) have emerged as powerful tools for enabling
automated traffic analysis; however, current approaches often demand
substantial computational resources and struggle with fine-grained
spatio-temporal understanding. This paper introduces STER-VLM, a
computationally efficient framework that enhances VLM performance through (1)
caption decomposition to tackle spatial and temporal information separately,
(2) temporal frame selection with best-view filtering for sufficient temporal
information, and (3) reference-driven understanding for capturing fine-grained
motion and dynamic context and (4) curated visual/textual prompt techniques.
Experimental results on the WTS \cite{kong2024wts} and BDD \cite{BDD} datasets
demonstrate substantial gains in semantic richness and traffic scene
interpretation. Our framework is validated through a decent test score of
55.655 in the AI City Challenge 2025 Track 2, showing its effectiveness in
advancing resource-efficient and accurate traffic analysis for real-world
applications.

</details>


### [109] [MINR: Efficient Implicit Neural Representations for Multi-Image Encoding](https://arxiv.org/abs/2508.13471)
*Wenyong Zhou,Taiqiang Wu,Zhengwu Liu,Yuxin Cheng,Chen Zhang,Ngai Wong*

Main category: cs.CV

TL;DR: The study introduces MINR, a method for improving the parameter efficiency of Implicit Neural Representations (INRs) when encoding multiple images, by sharing intermediate layers and using projection layers for unique image features.


<details>
  <summary>Details</summary>
Motivation: Encoding multiple images with INRs typically requires separate neural networks for each image, leading to inefficiencies in computation and storage.

Method: MINR shares intermediate layers among multiple images while keeping input/output layers image-specific and introduces a projection layer to capture unique image features.

Result: Experimental results show MINR reduces parameters by up to 60% while maintaining comparable performance. It handles scaling effectively to 100 images, achieving an average PSNR of 34 dB.

Conclusion: MINR demonstrates its potential to make INRs more practical and scalable for multi-image encoding without degrading performance, proving its efficiency and robustness.

Abstract: Implicit Neural Representations (INRs) aim to parameterize discrete signals
through implicit continuous functions. However, formulating each image with a
separate neural network~(typically, a Multi-Layer Perceptron (MLP)) leads to
computational and storage inefficiencies when encoding multi-images. To address
this issue, we propose MINR, sharing specific layers to encode multi-image
efficiently. We first compare the layer-wise weight distributions for several
trained INRs and find that corresponding intermediate layers follow highly
similar distribution patterns. Motivated by this, we share these intermediate
layers across multiple images while preserving the input and output layers as
input-specific. In addition, we design an extra novel projection layer for each
image to capture its unique features. Experimental results on image
reconstruction and super-resolution tasks demonstrate that MINR can save up to
60\% parameters while maintaining comparable performance. Particularly, MINR
scales effectively to handle 100 images, maintaining an average peak
signal-to-noise ratio (PSNR) of 34 dB. Further analysis of various backbones
proves the robustness of the proposed MINR.

</details>


### [110] [Distribution-Aware Hadamard Quantization for Hardware-Efficient Implicit Neural Representations](https://arxiv.org/abs/2508.13478)
*Wenyong Zhou,Jiachen Ren,Taiqiang Wu,Yuxin Cheng,Zhengwu Liu,Ngai Wong*

Main category: cs.CV

TL;DR: The paper introduces DHQ, a novel quantization approach employing Hadamard transformation for uniform quantization of weights and activations in Implicit Neural Representations (INRs).


<details>
  <summary>Details</summary>
Motivation: To address the hardware inefficiency caused by full-precision computation in INRs and the limited savings offered by previous weight-only quantization methods.

Method: The authors propose DHQ, which employs a Hadamard transformation to normalize diverse distributions of weights and activations into a bell-shaped form, followed by uniform quantization. This approach is theoretically and empirically justified.

Result: The DHQ method demonstrated significant hardware efficiency, achieving reductions in latency (32.7%), energy consumption (40.1%), and resource utilization (up to 98.3%) compared to their full-precision counterparts.

Conclusion: DHQ successfully extends quantization to both weights and activations in INRs, showing superior performance in hardware efficiency and practical applicability over existing quantization methods.

Abstract: Implicit Neural Representations (INRs) encode discrete signals using
Multi-Layer Perceptrons (MLPs) with complex activation functions. While INRs
achieve superior performance, they depend on full-precision number
representation for accurate computation, resulting in significant hardware
overhead. Previous INR quantization approaches have primarily focused on weight
quantization, offering only limited hardware savings due to the lack of
activation quantization. To fully exploit the hardware benefits of
quantization, we propose DHQ, a novel distribution-aware Hadamard quantization
scheme that targets both weights and activations in INRs. Our analysis shows
that the weights in the first and last layers have distributions distinct from
those in the intermediate layers, while the activations in the last layer
differ significantly from those in the preceding layers. Instead of customizing
quantizers individually, we utilize the Hadamard transformation to standardize
these diverse distributions into a unified bell-shaped form, supported by both
empirical evidence and theoretical analysis, before applying a standard
quantizer. To demonstrate the practical advantages of our approach, we present
an FPGA implementation of DHQ that highlights its hardware efficiency.
Experiments on diverse image reconstruction tasks show that DHQ outperforms
previous quantization methods, reducing latency by 32.7\%, energy consumption
by 40.1\%, and resource utilization by up to 98.3\% compared to full-precision
counterparts.

</details>


### [111] [AIM 2025 challenge on Inverse Tone Mapping Report: Methods and Results](https://arxiv.org/abs/2508.13479)
*Chao Wang,Francesco Banterle,Bin Ren,Radu Timofte,Xin Lu,Yufeng Peng,Chengjie Ge,Zhijing Sun,Ziang Zhou,Zihao Li,Zishun Liao,Qiyu Kang,Xueyang Fu,Zheng-Jun Zha,Zhijing Sun,Xingbo Wang,Kean Liu,Senyan Xu,Yang Qiu,Yifan Ding,Gabriel Eilertsen,Jonas Unger,Zihao Wang,Ke Wu,Jinshan Pan,Zhen Liu,Zhongyang Li,Shuaicheng Liu,S. M Nadim Uddin*

Main category: cs.CV

TL;DR: A review of the AIM 2025 Challenge on Inverse Tone Mapping, where HDR reconstruction algorithms were evaluated based on perceptual fidelity and numerical consistency.


<details>
  <summary>Details</summary>
Motivation: To advance the development of algorithms for reconstructing HDR images from single LDR inputs.

Method: Review and analysis of 319 submissions from 67 participants, focusing on the top-performing approaches.

Result: The best entry achieved a PU21-PSNR of 29.22 dB. Innovative strategies were identified for enhancing HDR quality.

Conclusion: The report sets benchmarks and highlights techniques to inspire future research in inverse tone mapping.

Abstract: This paper presents a comprehensive review of the AIM 2025 Challenge on
Inverse Tone Mapping (ITM). The challenge aimed to push forward the development
of effective ITM algorithms for HDR image reconstruction from single LDR
inputs, focusing on perceptual fidelity and numerical consistency. A total of
\textbf{67} participants submitted \textbf{319} valid results, from which the
best five teams were selected for detailed analysis. This report consolidates
their methodologies and performance, with the lowest PU21-PSNR among the top
entries reaching 29.22 dB. The analysis highlights innovative strategies for
enhancing HDR reconstruction quality and establishes strong benchmarks to guide
future research in inverse tone mapping.

</details>


### [112] [Enhancing Robustness of Implicit Neural Representations Against Weight Perturbations](https://arxiv.org/abs/2508.13481)
*Wenyong Zhou,Yuxin Cheng,Zhengwu Liu,Taiqiang Wu,Chen Zhang,Ngai Wong*

Main category: cs.CV

TL;DR: The paper investigates the robustness of Implicit Neural Representations (INRs) against weight perturbations and proposes a novel loss function to enhance resilience, showing significant improvement in signal reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: To address the critical vulnerability of INRs, where minor weight perturbations can drastically affect signal reconstruction performance, posing challenges to their use in real-world applications.

Method: The authors formulated the robustness problem in INRs by minimizing the difference in loss with and without weight perturbations. A novel robust loss function was derived to regulate the gradient of reconstruction loss with respect to the weights.

Result: Extensive experiments showed up to 7.5~dB improvement in peak signal-to-noise ratio (PSNR) under noisy conditions compared to original INRs, demonstrating enhanced robustness.

Conclusion: The proposed method substantially enhances the robustness of INRs under perturbation, making them more viable for practical applications.

Abstract: Implicit Neural Representations (INRs) encode discrete signals in a
continuous manner using neural networks, demonstrating significant value across
various multimedia applications. However, the vulnerability of INRs presents a
critical challenge for their real-world deployments, as the network weights
might be subjected to unavoidable perturbations. In this work, we investigate
the robustness of INRs for the first time and find that even minor
perturbations can lead to substantial performance degradation in the quality of
signal reconstruction. To mitigate this issue, we formulate the robustness
problem in INRs by minimizing the difference between loss with and without
weight perturbations. Furthermore, we derive a novel robust loss function to
regulate the gradient of the reconstruction loss with respect to weights,
thereby enhancing the robustness. Extensive experiments on reconstruction tasks
across multiple modalities demonstrate that our method achieves up to a 7.5~dB
improvement in peak signal-to-noise ratio (PSNR) values compared to original
INRs under noisy conditions.

</details>


### [113] [FAMNet: Integrating 2D and 3D Features for Micro-expression Recognition via Multi-task Learning and Hierarchical Attention](https://arxiv.org/abs/2508.13483)
*Liangyu Fu,Xuecheng Wu,Danlei Huang,Xinyi Yin*

Main category: cs.CV

TL;DR: This paper introduces FAMNet, a model improving micro-expression recognition by combining 2D and 3D CNNs and leveraging multi-task learning and hierarchical attention.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in recognizing micro-expressions due to their short duration and low intensity, aiming to improve feature extraction using spatiotemporal techniques.

Method: FAMNet employs multi-task learning of micro-expression recognition and facial action unit detection, combining 2D CNN (AMNet2D) and 3D CNN (AMNet3D), along with a shared Resnet18 backbone and attention modules.

Result: FAMNet demonstrated significant performance improvement, achieving high UAR and UF1 metrics on datasets like SAMM, CASME II, MMEW, and CAS(ME)^3.

Conclusion: The proposed FAMNet effectively enhances micro-expression recognition through omni-directional feature extraction and integration of multi-task learning methods.

Abstract: Micro-expressions recognition (MER) has essential application value in many
fields, but the short duration and low intensity of micro-expressions (MEs)
bring considerable challenges to MER. The current MER methods in deep learning
mainly include three data loading methods: static images, dynamic image
sequence, and a combination of the two streams. How to effectively extract MEs'
fine-grained and spatiotemporal features has been difficult to solve. This
paper proposes a new MER method based on multi-task learning and hierarchical
attention, which fully extracts MEs' omni-directional features by merging 2D
and 3D CNNs. The fusion model consists of a 2D CNN AMNet2D and a 3D CNN
AMNet3D, with similar structures consisting of a shared backbone network
Resnet18 and attention modules. During training, the model adopts different
data loading methods to adapt to two specific networks respectively, jointly
trains on the tasks of MER and facial action unit detection (FAUD), and adopts
the parameter hard sharing for information association, which further improves
the effect of the MER task, and the final fused model is called FAMNet.
Extensive experimental results show that our proposed FAMNet significantly
improves task performance. On the SAMM, CASME II and MMEW datasets, FAMNet
achieves 83.75% (UAR) and 84.03% (UF1). Furthermore, on the challenging
CAS(ME)$^3$ dataset, FAMNet achieves 51% (UAR) and 43.42% (UF1).

</details>


### [114] [CORENet: Cross-Modal 4D Radar Denoising Network with LiDAR Supervision for Autonomous Driving](https://arxiv.org/abs/2508.13485)
*Fuyang Liu,Jilin Mei,Fangyuan Mao,Chen Min,Yan Xing,Yu Hu*

Main category: cs.CV

TL;DR: CORENet is a 4D radar object detection framework that mitigates radar noise by learning from LiDAR during training and operates solely on radar data during inference.


<details>
  <summary>Details</summary>
Motivation: To improve 4D radar-based object detection by addressing challenges of noise and sparsity in radar point clouds.

Method: CORENet uses a cross-modal denoising framework where LiDAR supervision during training helps identify noise and extract features from 4D radar data, functioning as a plug-and-play module for radar-based detection pipelines.

Result: Extensive evaluations show CORENet performing robustly on the noisy Dual-Radar dataset, surpassing mainstream object detection methods.

Conclusion: CORENet improves detection accuracy and robustness for radar-based perception while requiring LiDAR solely for training.

Abstract: 4D radar-based object detection has garnered great attention for its
robustness in adverse weather conditions and capacity to deliver rich spatial
information across diverse driving scenarios. Nevertheless, the sparse and
noisy nature of 4D radar point clouds poses substantial challenges for
effective perception. To address the limitation, we present CORENet, a novel
cross-modal denoising framework that leverages LiDAR supervision to identify
noise patterns and extract discriminative features from raw 4D radar data.
Designed as a plug-and-play architecture, our solution enables seamless
integration into voxel-based detection frameworks without modifying existing
pipelines. Notably, the proposed method only utilizes LiDAR data for
cross-modal supervision during training while maintaining full radar-only
operation during inference. Extensive evaluation on the challenging Dual-Radar
dataset, which is characterized by elevated noise level, demonstrates the
effectiveness of our framework in enhancing detection robustness. Comprehensive
experiments validate that CORENet achieves superior performance compared to
existing mainstream approaches.

</details>


### [115] [Multi-view Clustering via Bi-level Decoupling and Consistency Learning](https://arxiv.org/abs/2508.13499)
*Shihao Dong,Yuhui Zheng,Huiying Xu,Xinzhong Zhu*

Main category: cs.CV

TL;DR: The authors propose the Bi-level Decoupling and Consistency Learning framework (BDCL) to enhance multi-view clustering performance by improving cluster discriminability and compactness.


<details>
  <summary>Details</summary>
Motivation: Multi-view clustering performance can be improved by learning consistency and complementarity between features, but cluster-oriented representation learning is often overlooked.

Method: The BDCL framework incorporates three key modules: multi-view instance learning, bi-level decoupling of features and clusters, and consistency learning using reconstruction autoencoder and contrastive learning.

Result: Experimental results across five benchmark datasets show that BDCL outperforms state-of-the-art methods.

Conclusion: BDCL effectively enhances clustering by optimizing inter-cluster discriminability and intra-cluster compactness, showcasing promising multi-view clustering advancements.

Abstract: Multi-view clustering has shown to be an effective method for analyzing
underlying patterns in multi-view data. The performance of clustering can be
improved by learning the consistency and complementarity between multi-view
features, however, cluster-oriented representation learning is often
overlooked. In this paper, we propose a novel Bi-level Decoupling and
Consistency Learning framework (BDCL) to further explore the effective
representation for multi-view data to enhance inter-cluster discriminability
and intra-cluster compactness of features in multi-view clustering. Our
framework comprises three modules: 1) The multi-view instance learning module
aligns the consistent information while preserving the private features between
views through reconstruction autoencoder and contrastive learning. 2) The
bi-level decoupling of features and clusters enhances the discriminability of
feature space and cluster space. 3) The consistency learning module treats the
different views of the sample and their neighbors as positive pairs, learns the
consistency of their clustering assignments, and further compresses the
intra-cluster space. Experimental results on five benchmark datasets
demonstrate the superiority of the proposed method compared with the SOTA
methods. Our code is published on https://github.com/LouisDong95/BDCL.

</details>


### [116] [AdaptiveAE: An Adaptive Exposure Strategy for HDR Capturing in Dynamic Scenes](https://arxiv.org/abs/2508.13503)
*Tianyi Xu,Fan Zhang,Boxin Shi,Tianfan Xue,Yujin Wang*

Main category: cs.CV

TL;DR: AdaptiveAE is introduced, leveraging reinforcement learning for optimizing shutter speed and ISO in HDR imaging, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Address the challenges in high dynamic range (HDR) imaging caused by noise from high ISO and motion blur from long shutter speeds, and optimize dynamic selection processes.

Method: AdaptiveAE uses reinforcement learning with an image synthesis pipeline that includes motion blur and noise simulation, semantic information, and exposure histograms.

Result: The method adaptively optimizes ISO and shutter speed combinations, demonstrating state-of-the-art performance across datasets.

Conclusion: AdaptiveAE improves HDR imaging quality by dynamically selecting better exposure schedules than traditional methods in dynamic environments.

Abstract: Mainstream high dynamic range imaging techniques typically rely on fusing
multiple images captured with different exposure setups (shutter speed and
ISO). A good balance between shutter speed and ISO is crucial for achieving
high-quality HDR, as high ISO values introduce significant noise, while long
shutter speeds can lead to noticeable motion blur. However, existing methods
often overlook the complex interaction between shutter speed and ISO and fail
to account for motion blur effects in dynamic scenes.
  In this work, we propose AdaptiveAE, a reinforcement learning-based method
that optimizes the selection of shutter speed and ISO combinations to maximize
HDR reconstruction quality in dynamic environments. AdaptiveAE integrates an
image synthesis pipeline that incorporates motion blur and noise simulation
into our training procedure, leveraging semantic information and exposure
histograms. It can adaptively select optimal ISO and shutter speed sequences
based on a user-defined exposure time budget, and find a better exposure
schedule than traditional solutions. Experimental results across multiple
datasets demonstrate that it achieves the state-of-the-art performance.

</details>


### [117] [The 9th AI City Challenge](https://arxiv.org/abs/2508.13564)
*Zheng Tang,Shuo Wang,David C. Anastasiu,Ming-Ching Chang,Anuj Sharma,Quan Kong,Norimasa Kobori,Munkhjargal Gochoo,Ganzorig Batnasan,Munkh-Erdene Otgonbold,Fady Alnajjar,Jun-Wei Hsieh,Tomasz Kornuta,Xiaolong Li,Yilin Zhao,Han Zhang,Subhashree Radhakrishnan,Arihant Jain,Ratnesh Kumar,Vidya N. Murali,Yuxing Wang,Sameer Satish Pusegaonkar,Yizhou Wang,Sujit Biswas,Xunlei Wu,Zhedong Zheng,Pranamesh Chakraborty,Rama Chellappa*

Main category: cs.CV

TL;DR: The ninth AI City Challenge (2025 edition) saw increased participation with teams advancing AI applications in transportation and safety across four tracks, focusing on 3D tracking, video question answering, spatial reasoning in warehouses, and edge object detection.


<details>
  <summary>Details</summary>
Motivation: The challenge aims to accelerate computer vision and AI systems for real-world applications such as transportation, industrial automation, and public safety by fostering innovation and benchmarking progress through competition.

Method: The challenge featured four tracks with distinct tasks: (1) 3D multi-camera tracking generated using NVIDIA Omniverse, (2) multi-camera traffic safety incident understanding with 3D gaze labels, (3) spatial reasoning from RGB-D inputs in dynamic environments also generated in NVIDIA Omniverse, and (4) efficient, lightweight object detection for fisheye cameras on edge devices.

Result: The challenge saw broad participation, with 245 teams representing 15 countries, and significant outcomes, including the public dataset with over 30,000 downloads and teams setting new benchmarks in multiple tasks.

Conclusion: The ninth AI City Challenge advanced state-of-the-art AI methods across a range of domains, highlighting reproducibility and innovation, and confirmed its role as a platform for accelerating real-world deployment of AI technologies.

Abstract: The ninth AI City Challenge continues to advance real-world applications of
computer vision and AI in transportation, industrial automation, and public
safety. The 2025 edition featured four tracks and saw a 17% increase in
participation, with 245 teams from 15 countries registered on the evaluation
server. Public release of challenge datasets led to over 30,000 downloads to
date. Track 1 focused on multi-class 3D multi-camera tracking, involving
people, humanoids, autonomous mobile robots, and forklifts, using detailed
calibration and 3D bounding box annotations. Track 2 tackled video question
answering in traffic safety, with multi-camera incident understanding enriched
by 3D gaze labels. Track 3 addressed fine-grained spatial reasoning in dynamic
warehouse environments, requiring AI systems to interpret RGB-D inputs and
answer spatial questions that combine perception, geometry, and language. Both
Track 1 and Track 3 datasets were generated in NVIDIA Omniverse. Track 4
emphasized efficient road object detection from fisheye cameras, supporting
lightweight, real-time deployment on edge devices. The evaluation framework
enforced submission limits and used a partially held-out test set to ensure
fair benchmarking. Final rankings were revealed after the competition
concluded, fostering reproducibility and mitigating overfitting. Several teams
achieved top-tier results, setting new benchmarks in multiple tasks.

</details>


### [118] [Bridging the Gap: Doubles Badminton Analysis with Singles-Trained Models](https://arxiv.org/abs/2508.13507)
*Seungheon Baek,Jinhyuk Yun*

Main category: cs.CV

TL;DR: This paper transfers models trained for singles matches to analyze doubles badminton by leveraging deep learning techniques, improving player tracking, and shot recognition.


<details>
  <summary>Details</summary>
Motivation: The research addresses the lack of focus on doubles badminton in analytics compared to singles due to data availability and multi-person tracking challenges.

Method: Keypoints from singles dataset were extracted using ViT-Pose, embedded through a contrastive learning framework with ST-GCN, and coupled with a multi-object tracking algorithm to enhance tracking stability. A Transformer classifier detects shot occurrences.

Result: The extended framework successfully demonstrates the practicality of pose-based shot recognition in doubles badminton, overcoming ID switching issues.

Conclusion: This work lays the groundwork for doubles-specific datasets and analytics, enriching understanding of doubles badminton in professional sports analysis.

Abstract: Badminton is known as one of the fastest racket sports in the world. Despite
doubles matches being more prevalent in international tournaments than singles,
previous research has mainly focused on singles due to the challenges in data
availability and multi-person tracking. To address this gap, we designed an
approach that transfers singles-trained models to doubles analysis. We
extracted keypoints from the ShuttleSet single matches dataset using ViT-Pose
and embedded them through a contrastive learning framework based on ST-GCN. To
improve tracking stability, we incorporated a custom multi-object tracking
algorithm that resolves ID switching issues from fast and overlapping player
movements. A Transformer-based classifier then determines shot occurrences
based on the learned embeddings. Our findings demonstrate the feasibility of
extending pose-based shot recognition to doubles badminton, broadening
analytics capabilities. This work establishes a foundation for doubles-specific
datasets to enhance understanding of this predominant yet understudied format
of the fast racket sport.

</details>


### [119] [2D Gaussians Meet Visual Tokenizer](https://arxiv.org/abs/2508.13515)
*Yiang Shi,Xiaoyang Guo,Wei Yin,Mingkai Jia,Qian Zhang,Xiaolin Hu,Wenyu Liu,Xinggang Wan*

Main category: cs.CV

TL;DR: The paper introduces "Visual Gaussian Quantization" (VGQ), a novel tokenizer that outperforms existing methods by integrating 2D Gaussians to better capture structured and geometric information in image generation.


<details>
  <summary>Details</summary>
Motivation: Existing tokenizers like VQ-GAN neglect geometric structures and focus mainly on appearance features due to patch-based designs, leading to limitations in accurately modeling structured visual content.

Method: The proposed VGQ framework uses 2D Gaussian distributions to encode image latents, capturing geometric and spatial structures. Parameters like position, rotation, and scale are explicitly modeled to enhance structural information.

Result: VGQ achieves state-of-the-art results on the ImageNet 256x256 benchmark, with an rFID score of 0.556 and PSNR of 24.93, showing marked improvement in reconstruction fidelity compared to existing methods.

Conclusion: VGQ demonstrates that incorporating 2D Gaussians into tokenization frameworks can significantly enhance visual representation and reconstruction quality, bridging a gap in structured content modeling in AR image generation.

Abstract: The image tokenizer is a critical component in AR image generation, as it
determines how rich and structured visual content is encoded into compact
representations. Existing quantization-based tokenizers such as VQ-GAN
primarily focus on appearance features like texture and color, often neglecting
geometric structures due to their patch-based design. In this work, we explored
how to incorporate more visual information into the tokenizer and proposed a
new framework named Visual Gaussian Quantization (VGQ), a novel tokenizer
paradigm that explicitly enhances structural modeling by integrating 2D
Gaussians into traditional visual codebook quantization frameworks. Our
approach addresses the inherent limitations of naive quantization methods such
as VQ-GAN, which struggle to model structured visual information due to their
patch-based design and emphasis on texture and color. In contrast, VGQ encodes
image latents as 2D Gaussian distributions, effectively capturing geometric and
spatial structures by directly modeling structure-related parameters such as
position, rotation and scale. We further demonstrate that increasing the
density of 2D Gaussians within the tokens leads to significant gains in
reconstruction fidelity, providing a flexible trade-off between token
efficiency and visual richness. On the ImageNet 256x256 benchmark, VGQ achieves
strong reconstruction quality with an rFID score of 1.00. Furthermore, by
increasing the density of 2D Gaussians within the tokens, VGQ gains a
significant boost in reconstruction capability and achieves a state-of-the-art
reconstruction rFID score of 0.556 and a PSNR of 24.93, substantially
outperforming existing methods. Codes will be released soon.

</details>


### [120] [MR6D: Benchmarking 6D Pose Estimation for Mobile Robots](https://arxiv.org/abs/2508.13775)
*Anas Gouda,Shrutarv Awasthi,Christian Blesing,Lokeshwaran Manohar,Frank Hoffmann,Alice Kirchheim*

Main category: cs.CV

TL;DR: MR6D is a dataset for 6D pose estimation tailored to mobile robots in industrial environments, addressing unique challenges like long-range perception, occlusion, and varied viewpoints.


<details>
  <summary>Details</summary>
Motivation: Current 6D pose estimation datasets focus on small household objects, limiting their application for mobile robots that interact with larger objects and encounter challenges like self-occlusion and long-range perception in diverse settings.

Method: The authors created MR6D, a dataset with 92 real-world scenes, 16 unique objects, and scenarios that include static and dynamic interactions in industrial environments.

Result: Initial experiments show that existing 6D pose estimation approaches struggle with long-distance, varied object configurations, and occlusion settings in MR6D.

Conclusion: MR6D serves as a benchmark for improving 6D pose estimation methods, specifically catered to the challenges faced by mobile robotics.

Abstract: Existing 6D pose estimation datasets primarily focus on small household
objects typically handled by robot arm manipulators, limiting their relevance
to mobile robotics. Mobile platforms often operate without manipulators,
interact with larger objects, and face challenges such as long-range
perception, heavy self-occlusion, and diverse camera perspectives. While recent
models generalize well to unseen objects, evaluations remain confined to
household-like settings that overlook these factors. We introduce MR6D, a
dataset designed for 6D pose estimation for mobile robots in industrial
environments. It includes 92 real-world scenes featuring 16 unique objects
across static and dynamic interactions. MR6D captures the challenges specific
to mobile platforms, including distant viewpoints, varied object
configurations, larger object sizes, and complex occlusion/self-occlusion
patterns. Initial experiments reveal that current 6D pipelines underperform in
these settings, with 2D segmentation being another hurdle. MR6D establishes a
foundation for developing and evaluating pose estimation methods tailored to
the demands of mobile robotics. The dataset is available at
https://huggingface.co/datasets/anas-gouda/mr6d.

</details>


### [121] [Calibrating Biased Distribution in VFM-derived Latent Space via Cross-Domain Geometric Consistency](https://arxiv.org/abs/2508.13518)
*Yanbiao Ma,Wei Dai,Bowei Liu,Jiayi Chen,Wenke Huang,Guancheng Wan,Zhiwu Lu,Junchi Yan*

Main category: cs.CV

TL;DR: The paper explores how geometric shapes of feature distributions derived from foundation models (e.g., CLIP, DINOv2) exhibit transferability. A framework is proposed to leverage this property for tasks like federated learning and long-tailed recognition, improving performance under challenging data conditions.


<details>
  <summary>Details</summary>
Motivation: To address the gap between observed training samples and the true data distribution, which is caused by factors like sampling bias and noise.

Method: The framework leverages the geometric shapes of feature distributions derived from foundation models to calibrate data distribution. Techniques are developed for federated learning using global geometric shapes under privacy constraints, and for long-tailed recognition by transferring knowledge from sample-rich categories to sample-scarce ones.

Result: Comprehensive experiments demonstrate that the proposed method effectively mitigates issues like data heterogeneity and sample imbalance, resulting in improved performance across benchmarks.

Conclusion: The geometric knowledge-guided distribution calibration framework presents a promising solution to address information deficits caused by data variability and skewness, showcasing its applicability to real-world ML challenges.

Abstract: Despite the fast progress of deep learning, one standing challenge is the gap
of the observed training samples and the underlying true distribution. There
are multiple reasons for the causing of this gap e.g. sampling bias, noise etc.
In the era of foundation models, we show that when leveraging the off-the-shelf
(vision) foundation models (e.g., CLIP, DINOv2) for feature extraction, the
geometric shapes of the resulting feature distributions exhibit remarkable
transferability across domains and datasets. To verify its practical
usefulness, we embody our geometric knowledge-guided distribution calibration
framework in two popular and challenging settings: federated learning and
long-tailed recognition. In the federated setting, we devise a technique of
acquiring the global geometric shape under privacy constraints, then leverage
this knowledge to generate new samples for clients, in the aim of bridging the
gap between local and global observations. In long-tailed learning, it utilizes
the geometric knowledge transferred from sample-rich categories to recover the
true distribution for sample-scarce tail classes. Comprehensive experiments
show that our proposed geometric knowledge-guided distribution calibration
effectively overcomes information deficits caused by data heterogeneity and
sample imbalance, with boosted performance across benchmarks.

</details>


### [122] [Evaluating Open-Source Vision Language Models for Facial Emotion Recognition against Traditional Deep Learning Models](https://arxiv.org/abs/2508.13524)
*Vamsi Krishna Mulukutla,Sai Supriya Pavarala,Srinivasa Raju Rudraraju,Sridevi Bonthu*

Main category: cs.CV

TL;DR: The study compares Vision-Language Models (VLMs) with traditional deep learning models for facial emotion recognition using the FER-2013 dataset, finding that VLMs perform worse on low-quality images.


<details>
  <summary>Details</summary>
Motivation: To improve facial emotion recognition methods and assess the applicability of Vision-Language Models in noisy, low-resolution image environments.

Method: The authors used a pipeline integrating GFPGAN-based image restoration with FER analysis and compared models (EfficientNet-B0, ResNet-50, etc.) on FER-2013 dataset in terms of precision, recall, F1-score, accuracy, and computational costs.

Result: EfficientNet-B0 achieved the best accuracy (86.44%), followed by ResNet-50 (85.72%), while VLMs like CLIP and Phi-3.5 Vision underperformed with accuracies of 64.07% and 51.66%, respectively.

Conclusion: Traditional deep learning models outperform Vision-Language Models for noisy FER tasks, suggesting a need to adapt VLMs to handle low-quality visual environments effectively.

Abstract: Facial Emotion Recognition (FER) is crucial for applications such as
human-computer interaction and mental health diagnostics. This study presents
the first empirical comparison of open-source Vision-Language Models (VLMs),
including Phi-3.5 Vision and CLIP, against traditional deep learning models
VGG19, ResNet-50, and EfficientNet-B0 on the challenging FER-2013 dataset,
which contains 35,887 low-resolution grayscale images across seven emotion
classes. To address the mismatch between VLM training assumptions and the noisy
nature of FER data, we introduce a novel pipeline that integrates GFPGAN-based
image restoration with FER evaluation. Results show that traditional models,
particularly EfficientNet-B0 (86.44%) and ResNet-50 (85.72%), significantly
outperform VLMs like CLIP (64.07%) and Phi-3.5 Vision (51.66%), highlighting
the limitations of VLMs in low-quality visual tasks. In addition to performance
evaluation using precision, recall, F1-score, and accuracy, we provide a
detailed computational cost analysis covering preprocessing, training,
inference, and evaluation phases, offering practical insights for deployment.
This work underscores the need for adapting VLMs to noisy environments and
provides a reproducible benchmark for future research in emotion recognition.

</details>


### [123] [ResPlan: A Large-Scale Vector-Graph Dataset of 17,000 Residential Floor Plans](https://arxiv.org/abs/2508.14006)
*Mohamed Abouagour,Eleftherios Garyfallidis*

Main category: cs.CV

TL;DR: ResPlan provides a large-scale dataset of 17,000 realistic residential floor plans with detailed annotations to advance spatial AI research and support diverse applications.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for spatial AI research lack visual fidelity, structural diversity, and realistic layouts. Hence, ResPlan is introduced to overcome these limitations and broaden applications.

Method: Authors created ResPlan using an open-source pipeline that refines geometry, aligns components, and annotates architectural elements and functional spaces. Plans are distributed in formats suitable for simulations and graph-based reasoning.

Result: ResPlan features high-quality floor plans with realistic layouts, extensive annotations, and structured room connectivity. Comparative analyses reveal its superiority over existing benchmarks and highlight new potential tasks.

Conclusion: ResPlan is a versatile and robust dataset that significantly enhances the realism, usability, and benchmarking capacity for next-gen spatial intelligence systems across various fields.

Abstract: We introduce ResPlan, a large-scale dataset of 17,000 detailed, structurally
rich, and realistic residential floor plans, created to advance spatial AI
research. Each plan includes precise annotations of architectural elements
(walls, doors, windows, balconies) and functional spaces (such as kitchens,
bedrooms, and bathrooms). ResPlan addresses key limitations of existing
datasets such as RPLAN (Wu et al., 2019) and MSD (van Engelenburg et al., 2024)
by offering enhanced visual fidelity and greater structural diversity,
reflecting realistic and non-idealized residential layouts. Designed as a
versatile, general-purpose resource, ResPlan supports a wide range of
applications including robotics, reinforcement learning, generative AI, virtual
and augmented reality, simulations, and game development. Plans are provided in
both geometric and graph-based formats, enabling direct integration into
simulation engines and fast 3D conversion. A key contribution is an open-source
pipeline for geometry cleaning, alignment, and annotation refinement.
Additionally, ResPlan includes structured representations of room connectivity,
supporting graph-based spatial reasoning tasks. Finally, we present comparative
analyses with existing benchmarks and outline several open benchmark tasks
enabled by ResPlan. Ultimately, ResPlan offers a significant advance in scale,
realism, and usability, providing a robust foundation for developing and
benchmarking next-generation spatial intelligence systems.

</details>


### [124] [EAvatar: Expression-Aware Head Avatar Reconstruction with Generative Geometry Priors](https://arxiv.org/abs/2508.13537)
*Shikun Zhang,Cunjian Chen,Yiqun Wang,Qiuhong Ke,Yong Li*

Main category: cs.CV

TL;DR: This paper introduces EAvatar, a novel 3D Gaussian Splatting-based framework designed to improve head avatar reconstruction by enhancing expression-aware and deformation-aware capabilities.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in overcoming limitations in existing 3D Gaussian Splatting methods, which struggle with capturing fine-grained facial expressions and preserving texture continuity in deformable regions.

Method: The paper proposes a sparse expression control mechanism using key Gaussians to manage deformation and texture transitions, along with leveraging pretrained generative models for high-quality 3D priors to guide shape and stability during training.

Result: The method achieves more accurate and visually coherent head reconstructions, with improved controllability over expressions and enhanced detail fidelity.

Conclusion: EAvatar represents a significant step forward in head avatar reconstruction, integrating advanced methods for better expression representation and texture fidelity.

Abstract: High-fidelity head avatar reconstruction plays a crucial role in AR/VR,
gaming, and multimedia content creation. Recent advances in 3D Gaussian
Splatting (3DGS) have demonstrated effectiveness in modeling complex geometry
with real-time rendering capability and are now widely used in high-fidelity
head avatar reconstruction tasks. However, existing 3DGS-based methods still
face significant challenges in capturing fine-grained facial expressions and
preserving local texture continuity, especially in highly deformable regions.
To mitigate these limitations, we propose a novel 3DGS-based framework termed
EAvatar for head reconstruction that is both expression-aware and
deformation-aware. Our method introduces a sparse expression control mechanism,
where a small number of key Gaussians are used to influence the deformation of
their neighboring Gaussians, enabling accurate modeling of local deformations
and fine-scale texture transitions. Furthermore, we leverage high-quality 3D
priors from pretrained generative models to provide a more reliable facial
geometry, offering structural guidance that improves convergence stability and
shape accuracy during training. Experimental results demonstrate that our
method produces more accurate and visually coherent head reconstructions with
improved expression controllability and detail fidelity.

</details>


### [125] [FLAIR: Frequency- and Locality-Aware Implicit Neural Representations](https://arxiv.org/abs/2508.13544)
*Sukhun Ko,Dahyeon Kye,Kyle Min,Chanho Eom,Jihyong Oh*

Main category: cs.CV

TL;DR: FLAIR enhances Implicit Neural Representations (INRs) by addressing frequency selectivity, spatial localization, and spectral bias issues, improving performance in image and 3D tasks.


<details>
  <summary>Details</summary>
Motivation: Current INRs struggle with spectral bias, excessive reliance on low-frequency signals, and poor capture of high-frequency details. This limits task performance in vision applications.

Method: FLAIR introduces RC-GAUSS activation for frequency/spatial optimization and WEGE encoding to guide network via wavelet transform energy scores.

Result: The proposed FLAIR method surpasses existing INRs consistently across 2D image representation, restoration, and 3D reconstruction tasks.

Conclusion: FLAIR effectively tackles INR's deficiencies by integrating frequency selectivity and spatial localization techniques, enhancing signal representation quality.

Abstract: Implicit Neural Representations (INRs) leverage neural networks to map
coordinates to corresponding signals, enabling continuous and compact
representations. This paradigm has driven significant advances in various
vision tasks. However, existing INRs lack frequency selectivity, spatial
localization, and sparse representations, leading to an over-reliance on
redundant signal components. Consequently, they exhibit spectral bias, tending
to learn low-frequency components early while struggling to capture fine
high-frequency details. To address these issues, we propose FLAIR (Frequency-
and Locality-Aware Implicit Neural Representations), which incorporates two key
innovations. The first is RC-GAUSS, a novel activation designed for explicit
frequency selection and spatial localization under the constraints of the
time-frequency uncertainty principle (TFUP). The second is
Wavelet-Energy-Guided Encoding (WEGE), which leverages the discrete wavelet
transform (DWT) to compute energy scores and explicitly guide frequency
information to the network. Our method consistently outperforms existing INRs
in 2D image representation and restoration, as well as 3D reconstruction.

</details>


### [126] [GazeProphet: Software-Only Gaze Prediction for VR Foveated Rendering](https://arxiv.org/abs/2508.13546)
*Farhaan Ebadulla,Chiraag Mudlapur,Gaurav BV*

Main category: cs.CV

TL;DR: The paper presents GazeProphet, a software-only gaze prediction method for foveated rendering in VR. It eliminates the need for hardware eye-tracking.


<details>
  <summary>Details</summary>
Motivation: To make gaze prediction for foveated rendering in VR more accessible by removing reliance on costly and complex hardware-based eye tracking systems.

Method: GazeProphet combines a Spherical Vision Transformer for spatial scene processing with an LSTM-based encoder for temporal gaze patterns. A multi-modal fusion network integrates spatial and temporal features to predict gaze locations with confidence estimates.

Result: GazeProphet achieves a median angular error of 3.83 degrees, outperforming saliency-based approaches by 24%, with consistent and reliable performance across different scenes and regions.

Conclusion: Software approaches like GazeProphet enable efficient VR rendering without hardware dependencies, improving accessibility and performance across VR applications.

Abstract: Foveated rendering significantly reduces computational demands in virtual
reality applications by concentrating rendering quality where users focus their
gaze. Current approaches require expensive hardware-based eye tracking systems,
limiting widespread adoption due to cost, calibration complexity, and hardware
compatibility constraints. This paper presents GazeProphet, a software-only
approach for predicting gaze locations in VR environments without requiring
dedicated eye tracking hardware. The approach combines a Spherical Vision
Transformer for processing 360-degree VR scenes with an LSTM-based temporal
encoder that captures gaze sequence patterns. A multi-modal fusion network
integrates spatial scene features with temporal gaze dynamics to predict future
gaze locations with associated confidence estimates. Experimental evaluation on
a comprehensive VR dataset demonstrates that GazeProphet achieves a median
angular error of 3.83 degrees, outperforming traditional saliency-based
baselines by 24% while providing reliable confidence calibration. The approach
maintains consistent performance across different spatial regions and scene
types, enabling practical deployment in VR systems without additional hardware
requirements. Statistical analysis confirms the significance of improvements
across all evaluation metrics. These results show that software-only gaze
prediction can work for VR foveated rendering, making this performance boost
more accessible to different VR platforms and apps.

</details>


### [127] [A Lightweight Dual-Mode Optimization for Generative Face Video Coding](https://arxiv.org/abs/2508.13547)
*Zihan Zhang,Shanzhi Yin,Bolin Chen,Ru-Ling Liao,Shiqi Wang,Yan Ye*

Main category: cs.CV

TL;DR: The paper proposes a lightweight Generative Face Video Coding (GFVC) framework with dual-mode optimization, significantly reducing computational costs and model parameters while maintaining high reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: GFVC provides excellent rate-distortion performance but is limited in deployment by large model parameters and computational requirements.

Method: The framework uses dual-mode optimization combining architectural redesign (replacing convolutions with efficient layers) and operational refinement through two-stage channel pruning (soft and hard pruning).

Result: The method achieves 90.4% reduction in parameters and 88.9% computational savings, outperforming the standard Versatile Video Coding (VVC) in quality metrics.

Conclusion: The lightweight GFVC framework enables efficient deployment in resource-constrained settings like mobile edge devices.

Abstract: Generative Face Video Coding (GFVC) achieves superior rate-distortion
performance by leveraging the strong inference capabilities of deep generative
models. However, its practical deployment is hindered by large model parameters
and high computational costs. To address this, we propose a lightweight GFVC
framework that introduces dual-mode optimization -- combining architectural
redesign and operational refinement -- to reduce complexity whilst preserving
reconstruction quality. Architecturally, we replace traditional 3 x 3
convolutions with slimmer and more efficient layers, reducing complexity
without compromising feature expressiveness. Operationally, we develop a
two-stage adaptive channel pruning strategy: (1) soft pruning during training
identifies redundant channels via learnable thresholds, and (2) hard pruning
permanently eliminates these channels post-training using a derived mask. This
dual-phase approach ensures both training stability and inference efficiency.
Experimental results demonstrate that the proposed lightweight dual-mode
optimization for GFVC can achieve 90.4% parameter reduction and 88.9%
computation saving compared to the baseline, whilst achieving superior
performance compared to state-of-the-art video coding standard Versatile Video
Coding (VVC) in terms of perceptual-level quality metrics. As such, the
proposed method is expected to enable efficient GFVC deployment in
resource-constrained environments such as mobile edge devices.

</details>


### [128] [Color Spike Data Generation via Bio-inspired Neuron-like Encoding with an Artificial Photoreceptor Layer](https://arxiv.org/abs/2508.13558)
*Hsieh Ching-Teng,Wang Yuan-Kai*

Main category: cs.CV

TL;DR: This paper addresses the performance gap between Spiking Neural Networks (SNNs) and Convolutional Neural Networks (CNNs) by proposing a biologically inspired encoding method that improves spike data information capacity and adheres to neuromorphic computing principles.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of spike-based data processing in SNNs and improve their performance without deviating from the principles of neuromorphic computing.

Method: The paper introduces a Neuron-like Encoding method that incorporates an artificial photoreceptor layer to generate enhanced spike data, containing both color and luminance information.

Result: Experimental results show that the proposed method increases the information content of spike signals, enhances SNN performance, and adheres to neuromorphic computing principles.

Conclusion: The biologically inspired encoding strategy has strong potential for overcoming current limitations in neuromorphic computing and facilitating broader applications of SNNs.

Abstract: In recent years, neuromorphic computing and spiking neural networks (SNNs)
have ad-vanced rapidly through integration with deep learning. However, the
performance of SNNs still lags behind that of convolutional neural networks
(CNNs), primarily due to the limited information capacity of spike-based data.
Although some studies have attempted to improve SNN performance by training
them with non-spiking inputs such as static images, this approach deviates from
the original intent of neuromorphic computing, which emphasizes spike-based
information processing. To address this issue, we propose a Neuron-like
Encoding method that generates spike data based on the intrinsic operational
principles and functions of biological neurons. This method is further enhanced
by the incorporation of an artificial pho-toreceptor layer, enabling spike data
to carry both color and luminance information, thereby forming a complete
visual spike signal. Experimental results using the Integrate-and-Fire neuron
model demonstrate that this biologically inspired approach effectively
increases the information content of spike signals and improves SNN
performance, all while adhering to neuromorphic principles. We believe this
concept holds strong potential for future development and may contribute to
overcoming current limitations in neuro-morphic computing, facilitating broader
applications of SNNs.

</details>


### [129] [DictAS: A Framework for Class-Generalizable Few-Shot Anomaly Segmentation via Dictionary Lookup](https://arxiv.org/abs/2508.13560)
*Zhen Qu,Xian Tao,Xinyi Gong,ShiChen Qu,Xiaopei Zhang,Xingang Wang,Fei Shen,Zhengtao Zhang,Mukesh Prasad,Guiguang Ding*

Main category: cs.CV

TL;DR: DictAS is a novel framework for few-shot anomaly segmentation (FSAS), enabling detection of anomalies in unseen object categories by leveraging dictionary lookup capabilities, without retraining on target data.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language models rely heavily on labeled anomaly examples for cross-category generalization, limiting their effectiveness for unseen classes. There is a need for a framework that generalizes FSAS tasks using only normal reference data.

Method: DictAS uses self-supervised learning with three main components: dictionary construction to index normal features, dictionary lookup for sparse feature retrieval to detect anomalies, and query discrimination regularization to enhance anomaly detection through constraints.

Result: Extensive experiments on seven public datasets demonstrate DictAS consistently surpasses existing FSAS methods in performance for industrial and medical applications.

Conclusion: DictAS introduces a scalable approach for anomaly detection in unseen categories by transitioning from feature memorization to dictionary-based feature retrieval, showcasing the potential for broad utility in various domains.

Abstract: Recent vision-language models (e.g., CLIP) have demonstrated remarkable
class-generalizable ability to unseen classes in few-shot anomaly segmentation
(FSAS), leveraging supervised prompt learning or fine-tuning on seen classes.
However, their cross-category generalization largely depends on prior knowledge
of real seen anomaly samples. In this paper, we propose a novel framework,
namely DictAS, which enables a unified model to detect visual anomalies in
unseen object categories without any retraining on the target data, only
employing a few normal reference images as visual prompts. The insight behind
DictAS is to transfer dictionary lookup capabilities to the FSAS task for
unseen classes via self-supervised learning, instead of merely memorizing the
normal and abnormal feature patterns from the training set. Specifically,
DictAS mainly consists of three components: (1) **Dictionary Construction** -
to simulate the index and content of a real dictionary using features from
normal reference images. (2) **Dictionary Lookup** - to retrieve queried region
features from the dictionary via a sparse lookup strategy. When a query feature
cannot be retrieved, it is classified as an anomaly. (3) **Query Discrimination
Regularization**- to enhance anomaly discrimination by making abnormal features
harder to retrieve from the dictionary. To achieve this, Contrastive Query
Constraint and Text Alignment Constraint are further proposed. Extensive
experiments on seven public industrial and medical datasets demonstrate that
DictAS consistently outperforms state-of-the-art FSAS methods.

</details>


### [130] [Learnable SMPLify: A Neural Solution for Optimization-Free Human Pose Inverse Kinematics](https://arxiv.org/abs/2508.13562)
*Yuchen Yang,Linfeng Dong,Wei Wang,Zhihang Zhong,Xiao Sun*

Main category: cs.CV

TL;DR: This paper introduces Learnable SMPLify, a faster neural network framework for 3D human pose and shape estimation, replacing the computationally intensive SMPLify optimization process with a simpler regression model.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the high computational cost of the traditional SMPLify approach in 3D human pose and shape estimation. Leveraging neural frameworks allows for faster and equally accurate analysis.

Method: The authors propose a neural framework called Learnable SMPLify, which replaces SMPLify's iterative optimization with a single-pass regression model. They use a temporal sampling strategy for data construction and propose a human-centric normalization scheme combined with residual learning to enhance generalization.

Result: The framework achieves a 200x faster runtime compared to SMPLify while maintaining generalization capabilities to unseen 3DPW and RICH datasets. It is model-agnostic, effectively used as a plug-in tool for applications like LucidAction.

Conclusion: Learnable SMPLify is a practical, faster, and simpler baseline for 3D human pose estimation, demonstrating efficiency, generalization, and versatility in its applications. Code is publicly available for further exploration.

Abstract: In 3D human pose and shape estimation, SMPLify remains a robust baseline that
solves inverse kinematics (IK) through iterative optimization. However, its
high computational cost limits its practicality. Recent advances across domains
have shown that replacing iterative optimization with data-driven neural
networks can achieve significant runtime improvements without sacrificing
accuracy. Motivated by this trend, we propose Learnable SMPLify, a neural
framework that replaces the iterative fitting process in SMPLify with a
single-pass regression model. The design of our framework targets two core
challenges in neural IK: data construction and generalization. To enable
effective training, we propose a temporal sampling strategy that constructs
initialization-target pairs from sequential frames. To improve generalization
across diverse motions and unseen poses, we propose a human-centric
normalization scheme and residual learning to narrow the solution space.
Learnable SMPLify supports both sequential inference and plug-in
post-processing to refine existing image-based estimators. Extensive
experiments demonstrate that our method establishes itself as a practical and
simple baseline: it achieves nearly 200x faster runtime compared to SMPLify,
generalizes well to unseen 3DPW and RICH, and operates in a model-agnostic
manner when used as a plug-in tool on LucidAction. The code is available at
https://github.com/Charrrrrlie/Learnable-SMPLify.

</details>


### [131] [Generative Model-Based Feature Attention Module for Video Action Analysis](https://arxiv.org/abs/2508.13565)
*Guiqin Wang,Peng Zhao,Cong Zhao,Jing Huang,Siyan Guo,Shusen Yang*

Main category: cs.CV

TL;DR: The paper addresses the lack of feature semantics in video action analysis, proposing a generative attention-based model for better feature extraction, especially suitable for IoT applications like autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Current methodologies fail to incorporate semantic features effectively in video action analysis, leading to inefficiencies in precision for high-stakes IoT use cases like autonomous driving.

Method: The paper proposes a generative attention-based model that learns the relationships between feature semantics by distinguishing the foreground and background of actions and examining frame- and segment-dependencies.

Result: Extensive experiments on action detection and recognition tasks demonstrate the proposed model's superiority in terms of performance on widely recognized datasets.

Conclusion: The novel approach effectively addresses the limitations of previous methods, with demonstrable improvements in precision and scalability, particularly in high-performance video analytics for IoT settings.

Abstract: Video action analysis is a foundational technology within the realm of
intelligent video comprehension, particularly concerning its application in
Internet of Things(IoT). However, existing methodologies overlook feature
semantics in feature extraction and focus on optimizing action proposals, thus
these solutions are unsuitable for widespread adoption in high-performance IoT
applications due to the limitations in precision, such as autonomous driving,
which necessitate robust and scalable intelligent video analytics analysis. To
address this issue, we propose a novel generative attention-based model to
learn the relation of feature semantics. Specifically, by leveraging the
differences of actions' foreground and background, our model simultaneously
learns the frame- and segment-dependencies of temporal action feature
semantics, which takes advantage of feature semantics in the feature extraction
effectively. To evaluate the effectiveness of our model, we conduct extensive
experiments on two benchmark video task, action recognition and action
detection. In the context of action detection tasks, we substantiate the
superiority of our approach through comprehensive validation on widely
recognized datasets. Moreover, we extend the validation of the effectiveness of
our proposed method to a broader task, video action recognition. Our code is
available at https://github.com/Generative-Feature-Model/GAF.

</details>


### [132] [Temporal-Conditional Referring Video Object Segmentation with Noise-Free Text-to-Video Diffusion Model](https://arxiv.org/abs/2508.13584)
*Ruixin Zhang,Jiaqing Fan,Yifan Liao,Qian Qiao,Fanzhang Li*

Main category: cs.CV

TL;DR: The paper improves Referring Video Object Segmentation (RVOS) by enhancing the segmentation head, utilizing a text-to-video diffusion model, and introducing a Temporal Context Mask Refinement module to achieve superior performance.


<details>
  <summary>Details</summary>
Motivation: To address the lack of emphasis on segmentation head design in RVOS, and improve overall segmentation accuracy and boundary segmentation capabilities.

Method: The authors propose a Temporal-Conditional model that integrates enhanced segmentation methods, removes randomness-causing elements in the noise prediction module, and introduces a Temporal Context Mask Refinement module.

Result: Achieved state-of-the-art performance on four public RVOS benchmarks, demonstrating significant improvements in segmentation quality.

Conclusion: Proposed innovations lead to a simpler, more effective model with enhanced boundary segmentation and overall performance, advancing the field of RVOS.

Abstract: Referring Video Object Segmentation (RVOS) aims to segment specific objects
in a video according to textual descriptions. We observe that recent RVOS
approaches often place excessive emphasis on feature extraction and temporal
modeling, while relatively neglecting the design of the segmentation head. In
fact, there remains considerable room for improvement in segmentation head
design. To address this, we propose a Temporal-Conditional Referring Video
Object Segmentation model, which innovatively integrates existing segmentation
methods to effectively enhance boundary segmentation capability. Furthermore,
our model leverages a text-to-video diffusion model for feature extraction. On
top of this, we remove the traditional noise prediction module to avoid the
randomness of noise from degrading segmentation accuracy, thereby simplifying
the model while improving performance. Finally, to overcome the limited feature
extraction capability of the VAE, we design a Temporal Context Mask Refinement
(TCMR) module, which significantly improves segmentation quality without
introducing complex designs. We evaluate our method on four public RVOS
benchmarks, where it consistently achieves state-of-the-art performance.

</details>


### [133] [Bridging Clear and Adverse Driving Conditions](https://arxiv.org/abs/2508.13592)
*Yoel Shapiro,Yahia Showgan,Koustav Mullick*

Main category: cs.CV

TL;DR: The paper proposes a novel Domain Adaptation pipeline utilizing both GAN and hybrid diffusion-GAN approaches to generate photorealistic adverse-weather images to improve Autonomous Driving (AD) system performance under challenging conditions, achieving notable gains in semantic segmentation.


<details>
  <summary>Details</summary>
Motivation: Autonomous Driving (AD) systems suffer from degraded performance in adverse environmental conditions, further compounded by the underrepresentation of such conditions in datasets.

Method: Several data-generation pipelines, including simulation-only, GAN-based, and hybrid diffusion-GAN approaches, are developed to synthesize adverse-condition images using labeled clear-weather images. Additionally, the authors propose a novel training recipe combining simulated and real images for better domain adaptation.

Result: Integrating synthetically generated data boosts semantic segmentation accuracy by 1.85% overall and 4.62% improvement in nighttime conditions as evaluated on ACDC.

Conclusion: By leveraging the hybrid approach, the paper demonstrates improved AD system performance under adverse weather conditions, showcasing the efficacy of advanced synthetic data-generation techniques in domain adaptation.

Abstract: Autonomous Driving (AD) systems exhibit markedly degraded performance under
adverse environmental conditions, such as low illumination and precipitation.
The underrepresentation of adverse conditions in AD datasets makes it
challenging to address this deficiency. To circumvent the prohibitive cost of
acquiring and annotating adverse weather data, we propose a novel Domain
Adaptation (DA) pipeline that transforms clear-weather images into fog, rain,
snow, and nighttime images. Here, we systematically develop and evaluate
several novel data-generation pipelines, including simulation-only, GAN-based,
and hybrid diffusion-GAN approaches, to synthesize photorealistic adverse
images from labelled clear images. We leverage an existing DA GAN, extend it to
support auxiliary inputs, and develop a novel training recipe that leverages
both simulated and real images. The simulated images facilitate exact
supervision by providing perfectly matched image pairs, while the real images
help bridge the simulation-to-real (sim2real) gap. We further introduce a
method to mitigate hallucinations and artifacts in Stable-Diffusion
Image-to-Image (img2img) outputs by blending them adaptively with their
progenitor images. We finetune downstream models on our synthetic data and
evaluate them on the Adverse Conditions Dataset with Correspondences (ACDC). We
achieve 1.85 percent overall improvement in semantic segmentation, and 4.62
percent on nighttime, demonstrating the efficacy of our hybrid method for
robust AD perception under challenging conditions.

</details>


### [134] [Towards Efficient Vision State Space Models via Token Merging](https://arxiv.org/abs/2508.13599)
*Jinyoung Park,Minseok Son,Changick Kim*

Main category: cs.CV

TL;DR: The paper introduces MaMe, a token-merging strategy that optimizes computational efficiency in State Space Models for vision tasks while preserving their sequential modeling capabilities.


<details>
  <summary>Details</summary>
Motivation: State Space Models in computer vision have strong performance, but their computational efficiency needs improvement for practical and scalable use.

Method: MaMe is a tailored token-merging approach that quantifies token importance using the state transition parameter Δ and ensures sequential information flow through strategic token arrangements.

Result: MaMe delivers superior efficiency-performance trade-offs, maintains robustness under aggressive token reduction, and generalizes well across image, video, and audio tasks.

Conclusion: MaMe is an effective solution for enhancing efficiency in SSM applications while maintaining performance and generalization across multiple domains.

Abstract: State Space Models (SSMs) have emerged as powerful architectures in computer
vision, yet improving their computational efficiency remains crucial for
practical and scalable deployment.While token reduction serves as an effective
approach for model efficiency, applying it to SSMs requires careful
consideration of their unique sequential modeling capabilities.In this work, we
propose MaMe, a token-merging strategy tailored for SSM-based vision
models.MaMe addresses two key challenges: quantifying token importance and
preserving sequential properties. Our approach leverages the state transition
parameter $\mathbf{\Delta}$ as an informativeness measure and introduces
strategic token arrangements to preserve sequential information flow.Extensive
experiments demonstrate that MaMe achieves superior efficiency-performance
trade-offs for both fine-tuned and off-the-shelf models. Particularly, our
approach maintains robustness even under aggressive token reduction where
existing methods undergo significant performance degradation.Beyond image
classification, MaMe shows strong generalization capabilities across video and
audio domains, establishing an effective approach for enhancing efficiency in
diverse SSM applications.

</details>


### [135] [Unleashing Semantic and Geometric Priors for 3D Scene Completion](https://arxiv.org/abs/2508.13601)
*Shiyuan Chen,Wei Sui,Bohao Zhang,Zeyd Boukhers,John See,Cong Yang*

Main category: cs.CV

TL;DR: FoundationSSC enhances 3D semantic scene completion by decoupling semantic and geometric pathways, improving feature refinement and fusion, and achieving superior performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the performance limitations of current encoder-based systems in 3D semantic scene completion, which struggle to balance semantic and geometric processing demands.

Method: The method introduces dual decoupling at the source and pathway levels. A foundation encoder separates semantic feature priors and stereo cost volumes, while refined pathways focus on their specific strengths. It also employs a hybrid view transformation and an Axis-Aware Fusion module.

Result: FoundationSSC outperforms existing methods with +0.23 mIoU and +2.03 IoU improvements on the SemanticKITTI benchmark and achieves state-of-the-art results on SSCBench-KITTI-360.

Conclusion: The dual-decoupling approach effectively disentangles semantic and geometric processing, enhancing overall system performance and setting a new benchmark in 3D scene completion tasks.

Abstract: Camera-based 3D semantic scene completion (SSC) provides dense geometric and
semantic perception for autonomous driving and robotic navigation. However,
existing methods rely on a coupled encoder to deliver both semantic and
geometric priors, which forces the model to make a trade-off between
conflicting demands and limits its overall performance. To tackle these
challenges, we propose FoundationSSC, a novel framework that performs dual
decoupling at both the source and pathway levels. At the source level, we
introduce a foundation encoder that provides rich semantic feature priors for
the semantic branch and high-fidelity stereo cost volumes for the geometric
branch. At the pathway level, these priors are refined through specialised,
decoupled pathways, yielding superior semantic context and depth distributions.
Our dual-decoupling design produces disentangled and refined inputs, which are
then utilised by a hybrid view transformation to generate complementary 3D
features. Additionally, we introduce a novel Axis-Aware Fusion (AAF) module
that addresses the often-overlooked challenge of fusing these features by
anisotropically merging them into a unified representation. Extensive
experiments demonstrate the advantages of FoundationSSC, achieving simultaneous
improvements in both semantic and geometric metrics, surpassing prior bests by
+0.23 mIoU and +2.03 IoU on SemanticKITTI. Additionally, we achieve
state-of-the-art performance on SSCBench-KITTI-360, with 21.78 mIoU and 48.61
IoU. The code will be released upon acceptance.

</details>


### [136] [PersonaVlog: Personalized Multimodal Vlog Generation with Multi-Agent Collaboration and Iterative Self-Correction](https://arxiv.org/abs/2508.13602)
*Xiaolu Hou,Bing Ma,Jiaxiang Cheng,Xuhua Ren,Kai Yu,Wenyue Li,Tianxiang Zheng,Qinglin Lu*

Main category: cs.CV

TL;DR: PersonaVlog proposes an automated framework for personalized, multimodal Vlog generation, incorporating video, music, and speech, with iterative self-correction and evaluation mechanisms.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the demand for innovative methods in automated Vlog generation due to limitations in existing approaches, such as dependence on predefined scripts lacking personal dynamism.

Method: The proposed PersonaVlog employs a multi-agent multimodal large language model (MLLM) collaboration framework, integrates iterative feedback mechanisms, and introduces ThemeVlogEval for standardized benchmarking.

Result: Through experiments, PersonaVlog shows superior performance compared to baselines in creating dynamic, personalized multimodal Vlogs.

Conclusion: PersonaVlog demonstrates significant progress in automated Vlog generation and provides tools for effective personalization and evaluation frameworks, paving the way for further developments in this field.

Abstract: With the growing demand for short videos and personalized content, automated
Video Log (Vlog) generation has become a key direction in multimodal content
creation. Existing methods mostly rely on predefined scripts, lacking dynamism
and personal expression. Therefore, there is an urgent need for an automated
Vlog generation approach that enables effective multimodal collaboration and
high personalization. To this end, we propose PersonaVlog, an automated
multimodal stylized Vlog generation framework that can produce personalized
Vlogs featuring videos, background music, and inner monologue speech based on a
given theme and reference image. Specifically, we propose a multi-agent
collaboration framework based on Multimodal Large Language Models (MLLMs). This
framework efficiently generates high-quality prompts for multimodal content
creation based on user input, thereby improving the efficiency and creativity
of the process. In addition, we incorporate a feedback and rollback mechanism
that leverages MLLMs to evaluate and provide feedback on generated results,
thereby enabling iterative self-correction of multimodal content. We also
propose ThemeVlogEval, a theme-based automated benchmarking framework that
provides standardized metrics and datasets for fair evaluation. Comprehensive
experiments demonstrate the significant advantages and potential of our
framework over several baselines, highlighting its effectiveness and great
potential for generating automated Vlogs.

</details>


### [137] [Two-Factor Authentication Smart Entryway Using Modified LBPH Algorithm](https://arxiv.org/abs/2508.13617)
*Zakiah Ayop,Wan Mohamad Hariz Bin Wan Mohamad Rosdi,Looi Wei Hua,Syarulnaziah Anawar,Nur Fadzilah Othman*

Main category: cs.CV

TL;DR: The paper introduces a smart entryway system combining facial recognition and passcode verification, enhancing security and automation, with mask detection capabilities.


<details>
  <summary>Details</summary>
Motivation: The lack of IoT-based face mask detection systems for smart entryways, specifically during heightened security needs like the COVID-19 pandemic.

Method: Proposes a two-factor authentication system utilizing facial recognition (LBPH algorithm) and passcode verification on a Raspberry Pi. Integrates Telegram for remote control and automation.

Result: The system achieved an average Accuracy of 70%, Precision of 80%, and Recall of 83.26%. Participants found it acceptable for future use during user acceptance testing.

Conclusion: The developed system demonstrates reliable performance in mask detection, security automation, and user acceptance, suggesting potential for IoT-based smart entryways.

Abstract: Face mask detection has become increasingly important recently, particularly
during the COVID-19 pandemic. Many face detection models have been developed in
smart entryways using IoT. However, there is a lack of IoT development on face
mask detection. This paper proposes a two-factor authentication system for
smart entryway access control using facial recognition and passcode
verification and an automation process to alert the owner and activate the
surveillance system when a stranger is detected and controls the system
remotely via Telegram on a Raspberry Pi platform. The system employs the Local
Binary Patterns Histograms for the full face recognition algorithm and modified
LBPH algorithm for occluded face detection. On average, the system achieved an
Accuracy of approximately 70%, a Precision of approximately 80%, and a Recall
of approximately 83.26% across all tested users. The results indicate that the
system is capable of conducting face recognition and mask detection, automating
the operation of the remote control to register users, locking or unlocking the
door, and notifying the owner. The sample participants highly accept it for
future use in the user acceptance test.

</details>


### [138] [TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head Synthesis](https://arxiv.org/abs/2508.13618)
*Shunian Chen,Hejin Huang,Yexin Liu,Zihan Ye,Pengcheng Chen,Chenghao Zhu,Michael Guan,Rongsheng Wang,Junying Chen,Guanbin Li,Ser-Nam Lim,Harry Yang,Benyou Wang*

Main category: cs.CV

TL;DR: This paper addresses the limitations of audio-driven talking head synthesis models in generalizing to diverse ethnicities, languages, and age groups.


<details>
  <summary>Details</summary>
Motivation: Existing models fail to generalize due to limitations in training data that lack scale, quality, and diversity.

Method: The authors introduce TalkVid, a large-scale dataset with 1244 hours of video and an evaluation set, TalkVid-Bench, curated via a multi-stage automated pipeline with human validation.

Result: Models trained on TalkVid perform better than previous datasets, showing improved cross-dataset generalization and highlighting performance disparities across subgroups.

Conclusion: TalkVid fills critical gaps in training datasets, enabling better generalization and deeper subgroup analysis, with resources made publicly available.

Abstract: Audio-driven talking head synthesis has achieved remarkable photorealism, yet
state-of-the-art (SOTA) models exhibit a critical failure: they lack
generalization to the full spectrum of human diversity in ethnicity, language,
and age groups. We argue that this generalization gap is a direct symptom of
limitations in existing training data, which lack the necessary scale, quality,
and diversity. To address this challenge, we introduce TalkVid, a new
large-scale, high-quality, and diverse dataset containing 1244 hours of video
from 7729 unique speakers. TalkVid is curated through a principled, multi-stage
automated pipeline that rigorously filters for motion stability, aesthetic
quality, and facial detail, and is validated against human judgments to ensure
its reliability. Furthermore, we construct and release TalkVid-Bench, a
stratified evaluation set of 500 clips meticulously balanced across key
demographic and linguistic axes. Our experiments demonstrate that a model
trained on TalkVid outperforms counterparts trained on previous datasets,
exhibiting superior cross-dataset generalization. Crucially, our analysis on
TalkVid-Bench reveals performance disparities across subgroups that are
obscured by traditional aggregate metrics, underscoring its necessity for
future research. Code and data can be found in
https://github.com/FreedomIntelligence/TalkVid

</details>


### [139] [RCGNet: RGB-based Category-Level 6D Object Pose Estimation with Geometric Guidance](https://arxiv.org/abs/2508.13623)
*Sheng Yu,Di-Hua Zhai,Yuanqing Xia*

Main category: cs.CV

TL;DR: The paper presents an RGB-only approach for category-level object pose estimation using a transformer-based model that process geometric features.


<details>
  <summary>Details</summary>
Motivation: Current methods for category-level object pose estimation face challenges in environments without depth data, necessitating a RGB-only solution.

Method: The approach involves a transformer-based neural network to predict and fuse geometric features, supported by a geometric feature-guided algorithm, followed by RANSAC-PnP for pose computation.

Result: Experiments on benchmark datasets showed the method is highly efficient and achieves better accuracy than previous RGB-based techniques.

Conclusion: This RGB-only pose estimation method delivers improved performance and efficiency, providing a promising direction for category-level pose estimation beyond depth-dependent systems.

Abstract: While most current RGB-D-based category-level object pose estimation methods
achieve strong performance, they face significant challenges in scenes lacking
depth information. In this paper, we propose a novel category-level object pose
estimation approach that relies solely on RGB images. This method enables
accurate pose estimation in real-world scenarios without the need for depth
data. Specifically, we design a transformer-based neural network for
category-level object pose estimation, where the transformer is employed to
predict and fuse the geometric features of the target object. To ensure that
these predicted geometric features faithfully capture the object's geometry, we
introduce a geometric feature-guided algorithm, which enhances the network's
ability to effectively represent the object's geometric information. Finally,
we utilize the RANSAC-PnP algorithm to compute the object's pose, addressing
the challenges associated with variable object scales in pose estimation.
Experimental results on benchmark datasets demonstrate that our approach is not
only highly efficient but also achieves superior accuracy compared to previous
RGB-based methods. These promising results offer a new perspective for
advancing category-level object pose estimation using RGB images.

</details>


### [140] [DiffIER: Optimizing Diffusion Models with Iterative Error Reduction](https://arxiv.org/abs/2508.13628)
*Ao Chen,Lihe Ding,Tianfan Xue*

Main category: cs.CV

TL;DR: DiffIER addresses the sensitivity of generated sample quality to the guidance weight in diffusion models by reducing the "training-inference gap" and iteratively minimizing errors during inference.


<details>
  <summary>Details</summary>
Motivation: The paper aims to solve the problem of diffusion models' output quality being highly sensitive to guidance weight, which arises due to the training-inference gap in conditional generation.

Method: The authors propose DiffIER, a plug-and-play optimization framework that minimizes accumulated errors using iterative error minimization during inference steps to mitigate the training-inference gap.

Result: Results show that DiffIER surpasses baseline methods in tasks like text-to-image generation, image super-resolution, and text-to-speech generation, showcasing its efficacy and versatility.

Conclusion: DiffIER successfully enhances conditional generation quality while reducing the dependency on guidance weight, demonstrating applicability across various domains like image and speech generation.

Abstract: Diffusion models have demonstrated remarkable capabilities in generating
high-quality samples and enhancing performance across diverse domains through
Classifier-Free Guidance (CFG). However, the quality of generated samples is
highly sensitive to the selection of the guidance weight. In this work, we
identify a critical ``training-inference gap'' and we argue that it is the
presence of this gap that undermines the performance of conditional generation
and renders outputs highly sensitive to the guidance weight. We quantify this
gap by measuring the accumulated error during the inference stage and establish
a correlation between the selection of guidance weight and minimizing this gap.
Furthermore, to mitigate this gap, we propose DiffIER, an optimization-based
method for high-quality generation. We demonstrate that the accumulated error
can be effectively reduced by an iterative error minimization at each step
during inference. By introducing this novel plug-and-play optimization
framework, we enable the optimization of errors at every single inference step
and enhance generation quality. Empirical results demonstrate that our proposed
method outperforms baseline approaches in conditional generation tasks.
Furthermore, the method achieves consistent success in text-to-image
generation, image super-resolution, and text-to-speech generation, underscoring
its versatility and potential for broad applications in future research.

</details>


### [141] [OmniTry: Virtual Try-On Anything without Masks](https://arxiv.org/abs/2508.13632)
*Yutong Feng,Linlin Zhang,Hengyuan Cao,Yiming Chen,Xiaoduan Feng,Jian Cao,Yuxiong Wu,Bin Wang*

Main category: cs.CV

TL;DR: OmniTry expands Virtual Try-On (VTON) to wearable objects like jewelry, using a mask-free framework. It combines unpaired and paired images for training and improves localization and ID-preservation.


<details>
  <summary>Details</summary>
Motivation: To create a more practical and unified Virtual Try-On framework that handles any wearable objects, not just garments.

Method: Two-stage pipeline: leveraging large-scale unpaired images for mask-free localization using repurposed inpainting models and fine-tuning with paired images for appearance consistency.

Result: OmniTry outperforms existing methods in object localization and ID-preservation across a benchmark of 12 wearable object classes.

Conclusion: The proposed OmniTry framework expands the capabilities of Virtual Try-On systems with broader application potential and superior performance.

Abstract: Virtual Try-ON (VTON) is a practical and widely-applied task, for which most
of existing works focus on clothes. This paper presents OmniTry, a unified
framework that extends VTON beyond garment to encompass any wearable objects,
e.g., jewelries and accessories, with mask-free setting for more practical
application. When extending to various types of objects, data curation is
challenging for obtaining paired images, i.e., the object image and the
corresponding try-on result. To tackle this problem, we propose a two-staged
pipeline: For the first stage, we leverage large-scale unpaired images, i.e.,
portraits with any wearable items, to train the model for mask-free
localization. Specifically, we repurpose the inpainting model to automatically
draw objects in suitable positions given an empty mask. For the second stage,
the model is further fine-tuned with paired images to transfer the consistency
of object appearance. We observed that the model after the first stage shows
quick convergence even with few paired samples. OmniTry is evaluated on a
comprehensive benchmark consisting of 12 common classes of wearable objects,
with both in-shop and in-the-wild images. Experimental results suggest that
OmniTry shows better performance on both object localization and
ID-preservation compared with existing methods. The code, model weights, and
evaluation benchmark of OmniTry will be made publicly available at
https://omnitry.github.io/.

</details>


### [142] [DeH4R: A Decoupled and Hybrid Method for Road Network Graph Extraction](https://arxiv.org/abs/2508.13669)
*Dengxian Gong,Shunping Ji*

Main category: cs.CV

TL;DR: The paper introduces DeH4R, a novel method for extracting road network graphs from remote sensing imagery, combining efficiency and topological fidelity.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle to maintain topology fidelity or suffer from computational inefficiencies when extracting road network graphs from remote sensing imagery.

Method: DeH4R uses a hybrid model divided into: candidate vertex detection, adjacent vertex prediction, initial graph construction, and graph expansion.

Result: DeH4R achieves state-of-the-art performance in road network graph extraction benchmarks, surpassing previous methods in accuracy and speed.

Conclusion: DeH4R effectively addresses challenges in road network graph extraction by improving topology fidelity and computational efficiency, setting new benchmarks in performance.

Abstract: The automated extraction of complete and precise road network graphs from
remote sensing imagery remains a critical challenge in geospatial computer
vision. Segmentation-based approaches, while effective in pixel-level
recognition, struggle to maintain topology fidelity after vectorization
postprocessing. Graph-growing methods build more topologically faithful graphs
but suffer from computationally prohibitive iterative ROI cropping.
Graph-generating methods first predict global static candidate road network
vertices, and then infer possible edges between vertices. They achieve fast
topology-aware inference, but limits the dynamic insertion of vertices. To
address these challenges, we propose DeH4R, a novel hybrid model that combines
graph-generating efficiency and graph-growing dynamics. This is achieved by
decoupling the task into candidate vertex detection, adjacent vertex
prediction, initial graph contruction, and graph expansion. This architectural
innovation enables dynamic vertex (edge) insertions while retaining fast
inference speed and enhancing both topology fidelity and spatial consistency.
Comprehensive evaluations on CityScale and SpaceNet benchmarks demonstrate
state-of-the-art (SOTA) performance. DeH4R outperforms the prior SOTA
graph-growing method RNGDet++ by 4.62 APLS and 10.18 IoU on CityScale, while
being approximately 10 $\times$ faster. The code will be made publicly
available at https://github.com/7777777FAN/DeH4R.

</details>


### [143] [RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation](https://arxiv.org/abs/2508.13968)
*Tianyi Niu,Jaemin Cho,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CV

TL;DR: This paper evaluates Multimodal Large Language Models (MLLMs) in identifying image orientations, showcasing their struggles with rotational distinctions, especially 90° and 270°.


<details>
  <summary>Details</summary>
Motivation: To explore if MLLMs can match human spatial reasoning by accurately identifying rotated images in various orientations.

Method: Introduce a benchmark, RotBench, with 350 manually-filtered images to analyze MLLMs' ability in rotation detection across different contexts, supplemented by auxiliary data and fine-tuning procedures.

Result: MLLMs struggle with rotations, reliably identifying only 0° and, in some cases, 180°. Techniques like auxiliary input or fine-tuning show limited success, while voting setups improve weaker models.

Conclusion: MLLMs reveal significant limitations in spatial reasoning and image rotation identification, highlighting gaps compared to human perceptual capabilities.

Abstract: We investigate to what extent Multimodal Large Language Models (MLLMs) can
accurately identify the orientation of input images rotated 0{\deg}, 90{\deg},
180{\deg}, and 270{\deg}. This task demands robust visual reasoning
capabilities to detect rotational cues and contextualize spatial relationships
within images, regardless of their orientation. To evaluate MLLMs on these
abilities, we introduce RotBench -- a 350-image manually-filtered benchmark
comprising lifestyle, portrait, and landscape images. Despite the relatively
simple nature of this task, we show that several state-of-the-art open and
proprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably
identify rotation in input images. Providing models with auxiliary information
-- including captions, depth maps, and more -- or using chain-of-thought
prompting offers only small and inconsistent improvements. Our results indicate
that most models are able to reliably identify right-side-up (0{\deg}) images,
while certain models are able to identify upside-down (180{\deg}) images. None
can reliably distinguish between 90{\deg} and 270{\deg}. Simultaneously showing
the image rotated in different orientations leads to moderate performance gains
for reasoning models, while a modified setup using voting improves the
performance of weaker models. We further show that fine-tuning does not improve
models' ability to distinguish 90{\deg} and 270{\deg} rotations, despite
substantially improving the identification of 180{\deg} images. Together, these
results reveal a significant gap between MLLMs' spatial reasoning capabilities
and human perception in identifying rotation.

</details>


### [144] [HumanPCR: Probing MLLM Capabilities in Diverse Human-Centric Scenes](https://arxiv.org/abs/2508.13692)
*Keliang Li,Hongze Shen,Hao Shi,Ruibing Hou,Hong Chang,Jie Huang,Chenghao Jia,Wen Wang,Yiling Wu,Dongmei Jiang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: The paper introduces HumanPCR, an evaluation suite for assessing multimodal models' understanding of human-related visual contexts, stretching from perception to reasoning, with over 6,000 annotated questions and challenging video tasks.


<details>
  <summary>Details</summary>
Motivation: To address the need for human-comparable performance in artificial general intelligence and highlight gaps in existing benchmarks for evaluating human-centric visual understanding.

Method: The authors developed HumanPCR, dividing tasks into three levels: Perception, Comprehension, and Reasoning, with a mix of multiple-choice questions and curated video reasoning tests, providing annotated rationales for each question.

Result: Evaluation of 30 state-of-the-art models showed challenges in detailed visual, temporal, and cognitive understanding, alongside issues with proactive evidence extraction and query-guided retrieval.

Conclusion: HumanPCR identifies critical gaps in multimodal model capabilities for human-like reasoning and aims to drive future advances in human-centric AI evaluation and development.

Abstract: The aspiration for artificial general intelligence, fueled by the rapid
progress of multimodal models, demands human-comparable performance across
diverse environments. We propose HumanPCR, an evaluation suite for probing
MLLMs' capacity about human-related visual contexts across three hierarchical
levels: Perception, Comprehension, and Reasoning (denoted by Human-P, Human-C,
and Human-R, respectively). Human-P and Human-C feature over 6,000
human-verified multiple choice questions, assessing massive tasks of 9
dimensions, including but not limited to essential skills frequently overlooked
by existing benchmarks. Human-R offers a challenging manually curated video
reasoning test that requires integrating multiple visual evidences, proactively
extracting context beyond question cues, and applying human-like expertise.
Each question includes human-annotated Chain-of-Thought (CoT) rationales with
key visual evidence to support further research. Extensive evaluations on over
30 state-of-the-art models exhibit significant challenges in human-centric
visual understanding, particularly in tasks involving detailed space
perception, temporal understanding, and mind modeling. Moreover, analysis of
Human-R reveals the struggle of models in extracting essential proactive visual
evidence from diverse human scenes and their faulty reliance on query-guided
retrieval. Even with advanced techniques like scaling visual contexts and
test-time thinking yield only limited benefits. We hope HumanPCR and our
findings will advance the development, evaluation, and human-centric
application of multimodal models.

</details>


### [145] [Diversity-enhanced Collaborative Mamba for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2508.13712)
*Shumeng Li,Jian Zhang,Lei Qi,Luping Zhou,Yinghuan Shi,Yang Gao*

Main category: cs.CV

TL;DR: The paper introduces DCMamba, a semi-supervised framework leveraging unlabeled data for medical image segmentation, outperforming existing methods significantly.


<details>
  <summary>Details</summary>
Motivation: To mitigate the costly acquisition of high-quality annotated data for medical image segmentation by leveraging semi-supervised techniques and addressing long-range dependencies using state space models.

Method: DCMamba framework utilizes diversity in data (via patch-level weak-strong mixing augmentation), network (through diverse-scan collaboration module), and features (using uncertainty-weighted contrastive learning).

Result: DCMamba outperformed other methods, achieving a 6.69% improvement over the latest SSM-based method on the Synapse dataset with 20% labeled data.

Conclusion: The proposed DCMamba effectively combines multiple diversity-enhancing techniques, showcasing its potential as a superior tool for semi-supervised medical image segmentation.

Abstract: Acquiring high-quality annotated data for medical image segmentation is
tedious and costly. Semi-supervised segmentation techniques alleviate this
burden by leveraging unlabeled data to generate pseudo labels. Recently,
advanced state space models, represented by Mamba, have shown efficient
handling of long-range dependencies. This drives us to explore their potential
in semi-supervised medical image segmentation. In this paper, we propose a
novel Diversity-enhanced Collaborative Mamba framework (namely DCMamba) for
semi-supervised medical image segmentation, which explores and utilizes the
diversity from data, network, and feature perspectives. Firstly, from the data
perspective, we develop patch-level weak-strong mixing augmentation with
Mamba's scanning modeling characteristics. Moreover, from the network
perspective, we introduce a diverse-scan collaboration module, which could
benefit from the prediction discrepancies arising from different scanning
directions. Furthermore, from the feature perspective, we adopt an
uncertainty-weighted contrastive learning mechanism to enhance the diversity of
feature representation. Experiments demonstrate that our DCMamba significantly
outperforms other semi-supervised medical image segmentation methods, e.g.,
yielding the latest SSM-based method by 6.69% on the Synapse dataset with 20%
labeled data.

</details>


### [146] [Hierarchical Vision-Language Retrieval of Educational Metaverse Content in Agriculture](https://arxiv.org/abs/2508.13713)
*Ali Abdari,Alex Falcon,Giuseppe Serra*

Main category: cs.CV

TL;DR: This paper introduces a dataset of 457 agricultural-themed virtual museums and proposes a hierarchical vision-language model to improve their retrieval using natural language queries in the Metaverse.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficiently organizing and retrieving educational agricultural content in the Metaverse, which currently has limited datasets and tools.

Method: The authors developed a hierarchical vision-language model and introduced a dataset called AgriMuseums with accompanying textual descriptions.

Result: The proposed model achieved up to 62% R@1 and 78% MRR in retrieval tasks, surpassing existing benchmarks by 6% R@1 and 11% MRR.

Conclusion: Combining the AgriMuseums dataset and hierarchical model effectively improves retrieval precision and demonstrates the potential of integrating vision and language processing for educational Metaverse scenarios.

Abstract: Every day, a large amount of educational content is uploaded online across
different areas, including agriculture and gardening. When these videos or
materials are grouped meaningfully, they can make learning easier and more
effective. One promising way to organize and enrich such content is through the
Metaverse, which allows users to explore educational experiences in an
interactive and immersive environment. However, searching for relevant
Metaverse scenarios and finding those matching users' interests remains a
challenging task. A first step in this direction has been done recently, but
existing datasets are small and not sufficient for training advanced models. In
this work, we make two main contributions: first, we introduce a new dataset
containing 457 agricultural-themed virtual museums (AgriMuseums), each enriched
with textual descriptions; and second, we propose a hierarchical
vision-language model to represent and retrieve relevant AgriMuseums using
natural language queries. In our experimental setting, the proposed method
achieves up to about 62\% R@1 and 78\% MRR, confirming its effectiveness, and
it also leads to improvements on existing benchmarks by up to 6\% R@1 and 11\%
MRR. Moreover, an extensive evaluation validates our design choices. Code and
dataset are available at
https://github.com/aliabdari/Agricultural_Metaverse_Retrieval .

</details>


### [147] [Enhancing Targeted Adversarial Attacks on Large Vision-Language Models through Intermediate Projector Guidance](https://arxiv.org/abs/2508.13739)
*Yiming Cao,Yanjie Li,Kaisheng Liang,Yuni Lai,Bin Xiao*

Main category: cs.CV

TL;DR: The paper introduces Intermediate Projector Guided Attack (IPGA), a method to improve targeted adversarial attacks in Vision-Language Models (VLMs) by leveraging the intermediate projector module for fine-grained control. Results show better performance and transferability compared to existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial methods focus on global vectors at the encoder level and overlook the projector module, leading to limitations in fine-grained control and reduced attack effectiveness on vision-language alignment pipelines.

Method: The proposed IPGA method focuses on attacking the intermediate projector module (specifically Q-Former) for fine-grained visual control using visual tokens. It incorporates Residual Query Alignment (RQA) to preserve unrelated content while perturbing visual semantics.

Result: IPGA consistently outperforms existing methods in both global image captioning and fine-grained visual question-answering tasks in black-box environments. It successfully transfers attacks to commercial VLMs like Google Gemini and OpenAI GPT.

Conclusion: IPGA addresses limitations of prior methods by enabling precise and controlled adversarial manipulations, disrupting vision-language alignment pipelines effectively, and demonstrating high transferability across VLMs, thus enhancing adversarial capabilities in security research.

Abstract: Targeted adversarial attacks are essential for proactively identifying
security flaws in Vision-Language Models before real-world deployment. However,
current methods perturb images to maximize global similarity with the target
text or reference image at the encoder level, collapsing rich visual semantics
into a single global vector. This limits attack granularity, hindering
fine-grained manipulations such as modifying a car while preserving its
background. Furthermore, these methods largely overlook the projector module, a
critical semantic bridge between the visual encoder and the language model in
VLMs, thereby failing to disrupt the full vision-language alignment pipeline
within VLMs and limiting attack effectiveness. To address these issues, we
propose the Intermediate Projector Guided Attack (IPGA), the first method to
attack using the intermediate stage of the projector module, specifically the
widely adopted Q-Former, which transforms global image embeddings into
fine-grained visual features. This enables more precise control over
adversarial perturbations by operating on semantically meaningful visual tokens
rather than a single global representation. Specifically, IPGA leverages the
Q-Former pretrained solely on the first vision-language alignment stage,
without LLM fine-tuning, which improves both attack effectiveness and
transferability across diverse VLMs. Furthermore, we propose Residual Query
Alignment (RQA) to preserve unrelated visual content, thereby yielding more
controlled and precise adversarial manipulations. Extensive experiments show
that our attack method consistently outperforms existing methods in both
standard global image captioning tasks and fine-grained visual
question-answering tasks in black-box environment. Additionally, IPGA
successfully transfers to multiple commercial VLMs, including Google Gemini and
OpenAI GPT.

</details>


### [148] [Mitigating Cross-Image Information Leakage in LVLMs for Multi-Image Tasks](https://arxiv.org/abs/2508.13744)
*Yeji Park,Minyoung Lee,Sanghyuk Chun,Junsuk Choe*

Main category: cs.CV

TL;DR: Large Vision-Language Models (LVLMs) suffer from cross-image information leakage when handling multi-image tasks. FOCUS is a training-free decoding strategy addressing this issue, improving performance across diverse benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of cross-image information leakage in LVLMs, which hampers their multi-image reasoning capability.

Method: FOCUS applies sequential masking with random noise to isolate single-image reasoning, aggregates outputs, and refines them contrastively using a noise-only reference during inference.

Result: FOCUS enhances multi-image reasoning performance across four benchmarks and various LVLM families without requiring training or architectural changes.

Conclusion: FOCUS is a practical, general solution for mitigating cross-image leakage and boosting multi-image reasoning in LVLMs.

Abstract: Large Vision-Language Models (LVLMs) demonstrate strong performance on
single-image tasks. However, we observe that their performance degrades
significantly when handling multi-image inputs. This occurs because visual cues
from different images become entangled in the model's output. We refer to this
phenomenon as cross-image information leakage. To address this issue, we
propose FOCUS, a training-free and architecture-agnostic decoding strategy that
mitigates cross-image information leakage during inference. FOCUS sequentially
masks all but one image with random noise, guiding the model to focus on the
single clean image. We repeat this process across all target images to obtain
logits under partially masked contexts. These logits are aggregated and then
contrastively refined using a noise-only reference input, which suppresses the
leakage and yields more accurate outputs. FOCUS consistently improves
performance across four multi-image benchmarks and diverse LVLM families. This
demonstrates that FOCUS offers a general and practical solution for enhancing
multi-image reasoning without additional training or architectural
modifications.

</details>


### [149] [Shape-from-Template with Generalised Camera](https://arxiv.org/abs/2508.13791)
*Agniva Sengupta,Stefan Zachow*

Main category: cs.CV

TL;DR: This paper introduces first-of-a-kind methods for 3D shape registration to 2D keypoints observed by multiple cameras, enhancing reconstruction accuracy using mutual constraints.


<details>
  <summary>Details</summary>
Motivation: To extend non-rigid 3D shape registration to multi-camera setups, allowing new applications in fields like medical imaging and enabling more accurate and versatile Shape-from-Template (SfT) methods.

Method: Three approaches are proposed: one for known 3D points, another for unknown 3D points but constrained by local reference, and a third combining keypoints with silhouette information, implemented via convex programming and iterative refinement.

Result: The methods yielded accurate reconstructions when tested on synthetic and real datasets, demonstrating effectiveness in leveraging multi-camera setups for non-rigid 3D shape registration.

Conclusion: Introducing generalised camera models to SfT significantly improves both reconstruction accuracy and scope of applications, making these techniques practical in scenarios involving deforming objects.

Abstract: This article presents a new method for non-rigidly registering a 3D shape to
2D keypoints observed by a constellation of multiple cameras. Non-rigid
registration of a 3D shape to observed 2D keypoints, i.e., Shape-from-Template
(SfT), has been widely studied using single images, but SfT with information
from multiple-cameras jointly opens new directions for extending the scope of
known use-cases such as 3D shape registration in medical imaging and
registration from hand-held cameras, to name a few. We represent such
multi-camera setup with the generalised camera model; therefore any collection
of perspective or orthographic cameras observing any deforming object can be
registered. We propose multiple approaches for such SfT: the first approach
where the corresponded keypoints lie on a direction vector from a known 3D
point in space, the second approach where the corresponded keypoints lie on a
direction vector from an unknown 3D point in space but with known orientation
w.r.t some local reference frame, and a third approach where, apart from
correspondences, the silhouette of the imaged object is also known. Together,
these form the first set of solutions to the SfT problem with generalised
cameras. The key idea behind SfT with generalised camera is the improved
reconstruction accuracy from estimating deformed shape while utilising the
additional information from the mutual constraints between multiple views of a
deformed object. The correspondence-based approaches are solved with convex
programming while the silhouette-based approach is an iterative refinement of
the results from the convex solutions. We demonstrate the accuracy of our
proposed methods on many synthetic and real data

</details>


### [150] [VisionLaw: Inferring Interpretable Intrinsic Dynamics from Visual Observations via Bilevel Optimization](https://arxiv.org/abs/2508.13792)
*Jiajing Lin,Shu Jiang,Qingyuan Zeng,Zhenzhong Wang,Min Jiang*

Main category: cs.CV

TL;DR: This paper introduces VisionLaw, a bilevel optimization framework to infer interpretable intrinsic dynamics of objects from visual observations, addressing challenges faced by existing methods relying on constitutive priors or neural networks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for inferring intrinsic dynamics from visual observations are limited by reliance on manually defined priors or neural networks, which hinder generalization and interpretability.

Method: The paper proposes VisionLaw, which combines an LLM-driven constitutive evolution strategy at the upper level with a vision-based constitutive evaluation mechanism at the lower level to infer dynamics efficiently and interpretable.

Result: VisionLaw demonstrated its capability to infer intrinsic dynamics from visual data effectively, outperforming state-of-the-art methods and showing robust generalization in synthetic and real-world datasets.

Conclusion: VisionLaw presents a promising alternative approach for inferring intrinsic dynamics from visual observations, combining interpretability with efficiency, and opening up new possibilities for interactive simulations.

Abstract: The intrinsic dynamics of an object governs its physical behavior in the real
world, playing a critical role in enabling physically plausible interactive
simulation with 3D assets. Existing methods have attempted to infer the
intrinsic dynamics of objects from visual observations, but generally face two
major challenges: one line of work relies on manually defined constitutive
priors, making it difficult to generalize to complex scenarios; the other
models intrinsic dynamics using neural networks, resulting in limited
interpretability and poor generalization. To address these challenges, we
propose VisionLaw, a bilevel optimization framework that infers interpretable
expressions of intrinsic dynamics from visual observations. At the upper level,
we introduce an LLMs-driven decoupled constitutive evolution strategy, where
LLMs are prompted as a knowledgeable physics expert to generate and revise
constitutive laws, with a built-in decoupling mechanism that substantially
reduces the search complexity of LLMs. At the lower level, we introduce a
vision-guided constitutive evaluation mechanism, which utilizes visual
simulation to evaluate the consistency between the generated constitutive law
and the underlying intrinsic dynamics, thereby guiding the upper-level
evolution. Experiments on both synthetic and real-world datasets demonstrate
that VisionLaw can effectively infer interpretable intrinsic dynamics from
visual observations. It significantly outperforms existing state-of-the-art
methods and exhibits strong generalization for interactive simulation in novel
scenarios.

</details>


### [151] [A Fully Transformer Based Multimodal Framework for Explainable Cancer Image Segmentation Using Radiology Reports](https://arxiv.org/abs/2508.13796)
*Enobong Adahada,Isabel Sassoon,Kate Hone,Yongmin Li*

Main category: cs.CV

TL;DR: Med-CTX is a transformer-based framework for explainable breast cancer ultrasound segmentation, integrating clinical radiology reports to improve performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: To improve accuracy and explainability in breast cancer ultrasound segmentation by utilizing a multimodal approach that integrates clinical text data with imaging.

Method: Med-CTX employs a dual-branch visual encoder (ViT and Swin transformers) and BioClinicalBERT for clinical language encoding, combining these through cross-modal attention. It generates segmentation masks, uncertainty maps, and diagnostic rationales simultaneously.

Result: On the BUS-BRA dataset, Med-CTX achieved a Dice score of 99%, IoU of 95%, and CLIP score of 85%. Clinical text was critical, as its removal resulted in a significant performance drop (-5.4% Dice, -31% CIDEr).

Conclusion: Med-CTX sets a new benchmark for trustworthy multimodal medical models by delivering high segmentation accuracy, good multimodal alignment, and increased confidence calibration.

Abstract: We introduce Med-CTX, a fully transformer based multimodal framework for
explainable breast cancer ultrasound segmentation. We integrate clinical
radiology reports to boost both performance and interpretability. Med-CTX
achieves exact lesion delineation by using a dual-branch visual encoder that
combines ViT and Swin transformers, as well as uncertainty aware fusion.
Clinical language structured with BI-RADS semantics is encoded by
BioClinicalBERT and combined with visual features utilising cross-modal
attention, allowing the model to provide clinically grounded, model generated
explanations. Our methodology generates segmentation masks, uncertainty maps,
and diagnostic rationales all at once, increasing confidence and transparency
in computer assisted diagnosis. On the BUS-BRA dataset, Med-CTX achieves a Dice
score of 99% and an IoU of 95%, beating existing baselines U-Net, ViT, and
Swin. Clinical text plays a key role in segmentation accuracy and explanation
quality, as evidenced by ablation studies that show a -5.4% decline in Dice
score and -31% in CIDEr. Med-CTX achieves good multimodal alignment (CLIP
score: 85%) and increased confi dence calibration (ECE: 3.2%), setting a new
bar for trustworthy, multimodal medical architecture.

</details>


### [152] [Unsupervised Urban Tree Biodiversity Mapping from Street-Level Imagery Using Spatially-Aware Visual Clustering](https://arxiv.org/abs/2508.13814)
*Diaa Addeen Abuhani,Marco Seccaroni,Martina Mazzarello,Imran Zualkernan,Fabio Duarte,Carlo Ratti*

Main category: cs.CV

TL;DR: This paper presents an unsupervised clustering method to estimate urban tree biodiversity using street-level imagery and spatial data, without needing labeled training data.


<details>
  <summary>Details</summary>
Motivation: Accurate urban tree biodiversity data is essential for fostering resilient, stable, and livable cities. However, conventional methods are either expensive and time-intensive or fail to generalize effectively across regions.

Method: The authors developed an unsupervised clustering framework that combines visual embeddings from street-level images with spatial planting patterns to estimate biodiversity, bypassing the need for labeled data.

Result: Applied to eight North American cities, the method effectively estimated genus-level diversity, demonstrated low Wasserstein distances when compared to ground-truth biodiversity indices, and preserved spatial autocorrelation.

Conclusion: This scalable approach facilitates low-cost, high-resolution biodiversity monitoring in urban areas, supporting better urban planning, equity in green access, and adaptive ecosystem management.

Abstract: Urban tree biodiversity is critical for climate resilience, ecological
stability, and livability in cities, yet most municipalities lack detailed
knowledge of their canopies. Field-based inventories provide reliable estimates
of Shannon and Simpson diversity but are costly and time-consuming, while
supervised AI methods require labeled data that often fail to generalize across
regions. We introduce an unsupervised clustering framework that integrates
visual embeddings from street-level imagery with spatial planting patterns to
estimate biodiversity without labels. Applied to eight North American cities,
the method recovers genus-level diversity patterns with high fidelity,
achieving low Wasserstein distances to ground truth for Shannon and Simpson
indices and preserving spatial autocorrelation. This scalable, fine-grained
approach enables biodiversity mapping in cities lacking detailed inventories
and offers a pathway for continuous, low-cost monitoring to support equitable
access to greenery and adaptive management of urban ecosystems.

</details>


### [153] [Self-Aware Adaptive Alignment: Enabling Accurate Perception for Intelligent Transportation Systems](https://arxiv.org/abs/2508.13823)
*Tong Xiang,Hongxia Zhao,Fenghua Zhu,Yuanyuan Chen,Yisheng Lv*

Main category: cs.CV

TL;DR: The paper introduces Self-Aware Adaptive Alignment (SA3) to enhance cross-domain object detection.


<details>
  <summary>Details</summary>
Motivation: Cross-domain scenarios in intelligent transportation detection face persistent challenges that hinder performance.

Method: The SA3 approach involves attention-based alignment modules and instance-to-image level alignment modules to adaptively align features between source and target domains.

Result: Experimental evaluations show SA3 outperforms previous state-of-the-art methods on cross-domain object detection benchmarks.

Conclusion: SA3 facilitates efficient domain adaptation, achieving notable accuracy improvements in cross-domain detection.

Abstract: Achieving top-notch performance in Intelligent Transportation detection is a
critical research area. However, many challenges still need to be addressed
when it comes to detecting in a cross-domain scenario. In this paper, we
propose a Self-Aware Adaptive Alignment (SA3), by leveraging an efficient
alignment mechanism and recognition strategy. Our proposed method employs a
specified attention-based alignment module trained on source and target domain
datasets to guide the image-level features alignment process, enabling the
local-global adaptive alignment between the source domain and target domain.
Features from both domains, whose channel importance is re-weighted, are fed
into the region proposal network, which facilitates the acquisition of salient
region features. Also, we introduce an instance-to-image level alignment module
specific to the target domain to adaptively mitigate the domain gap. To
evaluate the proposed method, extensive experiments have been conducted on
popular cross-domain object detection benchmarks. Experimental results show
that SA3 achieves superior results to the previous state-of-the-art methods.

</details>


### [154] [SAGA: Learning Signal-Aligned Distributions for Improved Text-to-Image Generation](https://arxiv.org/abs/2508.13866)
*Paul Grimal,Michaël Soumm,Hervé Le Borgne,Olivier Ferret,Akihiro Sugimoto*

Main category: cs.CV

TL;DR: This paper presents a novel training-free approach to improve the alignment between text prompts and the corresponding outputs of text-to-image models.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models struggle with aligning outputs precisely to their text prompts, often omitting crucial elements or blending unrelated concepts.

Method: A new method is introduced that explicitly models a signal component during denoising, enabling refined control and enhanced alignment. The approach is training-free and compatible with existing architectures, supporting additional conditioning modalities such as bounding boxes.

Result: Experiments show superior performance of the proposed framework compared to state-of-the-art methods.

Conclusion: The framework resolves key text-to-image alignment issues, offering a practical and robust solution that integrates seamlessly with various architectures.

Abstract: State-of-the-art text-to-image models produce visually impressive results but
often struggle with precise alignment to text prompts, leading to missing
critical elements or unintended blending of distinct concepts. We propose a
novel approach that learns a high-success-rate distribution conditioned on a
target prompt, ensuring that generated images faithfully reflect the
corresponding prompts. Our method explicitly models the signal component during
the denoising process, offering fine-grained control that mitigates
over-optimization and out-of-distribution artifacts. Moreover, our framework is
training-free and seamlessly integrates with both existing diffusion and flow
matching architectures. It also supports additional conditioning modalities --
such as bounding boxes -- for enhanced spatial alignment. Extensive experiments
demonstrate that our approach outperforms current state-of-the-art methods. The
code is available at https://github.com/grimalPaul/gsn-factory.

</details>


### [155] [RED.AI Id-Pattern: First Results of Stone Deterioration Patterns with Multi-Agent Systems](https://arxiv.org/abs/2508.13872)
*Daniele Corradetti,José Delgado Rodrigues*

Main category: cs.CV

TL;DR: The paper introduces the Id-Pattern system, an AI-based multi-agent system for identifying stone deterioration patterns, which enhances traditional expert-based methods.


<details>
  <summary>Details</summary>
Motivation: Traditional methodologies for identifying stone deterioration patterns are accurate but resource-intensive, leading to the need for an automated system to improve efficiency.

Method: The system employs a cognitive architecture with five specialized AI agents (lithologist, pathologist, environmental expert, conservator-restorer, diagnostic coordinator) to diagnose stone deterioration from visual data.

Result: The system showed significant improvement across all evaluation metrics when tested on 28 challenging images compared to the foundational model.

Conclusion: The multi-agent AI system provides an effective and efficient alternative for diagnosing stone deterioration patterns using automated and collaborative approaches.

Abstract: The Id-Pattern system within the RED.AI project (Reabilita\c{c}\~ao
Estrutural Digital atrav\'es da AI) consists of an agentic system designed to
assist in the identification of stone deterioration patterns. Traditional
methodologies, based on direct observation by expert teams, are accurate but
costly in terms of time and resources. The system developed here introduces and
evaluates a multi-agent artificial intelligence (AI) system, designed to
simulate collaboration between experts and automate the diagnosis of stone
pathologies from visual evidence. The approach is based on a cognitive
architecture that orchestrates a team of specialized AI agents which, in this
specific case, are limited to five: a lithologist, a pathologist, an
environmental expert, a conservator-restorer, and a diagnostic coordinator. To
evaluate the system we selected 28 difficult images involving multiple
deterioration patterns. Our first results showed a huge boost on all metrics of
our system compared to the foundational model.

</details>


### [156] [RICO: Two Realistic Benchmarks and an In-Depth Analysis for Incremental Learning in Object Detection](https://arxiv.org/abs/2508.13878)
*Matthias Neuwirth-Trapp,Maarten Bieshaar,Danda Pani Paudel,Luc Van Gool*

Main category: cs.CV

TL;DR: The paper introduces two realistic benchmarks for incremental learning in object detection, highlighting limitations in adaptability and retention across existing methods.


<details>
  <summary>Details</summary>
Motivation: To create more realistic benchmarks for Incremental Learning that account for real-world challenges, addressing the deficiency in current synthetic benchmarks.

Method: Develop two benchmarks, Domain RICO and Expanding-Classes RICO, based on diverse datasets covering real-world scenarios including domain shifts and expanding class sets.

Result: Experiments reveal performance deficiencies in adaptability and knowledge retention for all IL methods, with small replay data outperforming them, yet individual training remains the best approach.

Conclusion: Incremental Learning methods are flawed due to weak distillation methods, lack of model versatility for diverse tasks, and insufficient adaptability. Real-world benchmarks are essential for progress.

Abstract: Incremental Learning (IL) trains models sequentially on new data without full
retraining, offering privacy, efficiency, and scalability. IL must balance
adaptability to new data with retention of old knowledge. However, evaluations
often rely on synthetic, simplified benchmarks, obscuring real-world IL
performance. To address this, we introduce two Realistic Incremental Object
Detection Benchmarks (RICO): Domain RICO (D-RICO) features domain shifts with a
fixed class set, and Expanding-Classes RICO (EC-RICO) integrates new domains
and classes per IL step. Built from 14 diverse datasets covering real and
synthetic domains, varying conditions (e.g., weather, time of day), camera
sensors, perspectives, and labeling policies, both benchmarks capture
challenges absent in existing evaluations. Our experiments show that all IL
methods underperform in adaptability and retention, while replaying a small
amount of previous data already outperforms all methods. However, individual
training on the data remains superior. We heuristically attribute this gap to
weak teachers in distillation, single models' inability to manage diverse
tasks, and insufficient plasticity. Our code will be made publicly available.

</details>


### [157] [In-hoc Concept Representations to Regularise Deep Learning in Medical Imaging](https://arxiv.org/abs/2508.13880)
*Valentina Corbetta,Floris Six Dijkstra,Regina Beets-Tan,Hoel Kervadec,Kristoffer Wickstrøm,Wilson Silva*

Main category: cs.CV

TL;DR: The paper introduces LCRReg, a regularization approach improving deep learning model robustness in medical imaging against distribution shifts by guiding them to use semantically meaningful latent concept representations.


<details>
  <summary>Details</summary>
Motivation: Deep learning models in medical imaging perform well on in-distribution data but fail to generalize under distribution shifts, often relying on spurious correlations rather than clinically significant features.

Method: LCRReg employs Latent Concept Representations (e.g., Concept Activation Vectors) without requiring concept labels in the main training set, using a small auxiliary dataset to synthesize disentangled concept examples. It integrates a regularization term encouraging CNNs to activate on predefined relevant features.

Result: LCRReg improves robustness against spurious correlations and generalizes well to out-of-distribution data on both synthetic and real-world tasks, notably diabetic retinopathy classification, outperforming baseline methods.

Conclusion: LCRReg presents a lightweight, architecture-agnostic method for enhancing deep learning robustness in medical imaging, requiring minimal concept supervision and showing promise in both synthetic and real medical tasks.

Abstract: Deep learning models in medical imaging often achieve strong in-distribution
performance but struggle to generalise under distribution shifts, frequently
relying on spurious correlations instead of clinically meaningful features. We
introduce LCRReg, a novel regularisation approach that leverages Latent Concept
Representations (LCRs) (e.g., Concept Activation Vectors (CAVs)) to guide
models toward semantically grounded representations. LCRReg requires no concept
labels in the main training set and instead uses a small auxiliary dataset to
synthesise high-quality, disentangled concept examples. We extract LCRs for
predefined relevant features, and incorporate a regularisation term that guides
a Convolutional Neural Network (CNN) to activate within latent subspaces
associated with those concepts. We evaluate LCRReg across synthetic and
real-world medical tasks. On a controlled toy dataset, it significantly
improves robustness to injected spurious correlations and remains effective
even in multi-concept and multiclass settings. On the diabetic retinopathy
binary classification task, LCRReg enhances performance under both synthetic
spurious perturbations and out-of-distribution (OOD) generalisation. Compared
to baselines, including multitask learning, linear probing, and post-hoc
concept-based models, LCRReg offers a lightweight, architecture-agnostic
strategy for improving model robustness without requiring dense concept
supervision. Code is available at the following link:
https://github.com/Trustworthy-AI-UU-NKI/lcr\_regularization

</details>


### [158] [Forecasting Smog Events Using ConvLSTM: A Spatio-Temporal Approach for Aerosol Index Prediction in South Asia](https://arxiv.org/abs/2508.13891)
*Taimur Khan*

Main category: cs.CV

TL;DR: The paper tackles smog forecasting in South Asia using Sentinel-5P data and a ConvLSTM neural network, achieving decent performance but suggesting room for improvement.


<details>
  <summary>Details</summary>
Motivation: To address the lack of real-time regional forecasting systems for smog events in the Indo-Gangetic Plains, which have intensified due to factors like crop residue burning and vehicle emissions.

Method: The study used the Sentinel-5P air constituent dataset (2019-2023) and a ConvLSTM neural network that leverages spatial and temporal correlations to forecast the Aerosol Index, an important indicator of particulate matter concentrations.

Result: The Aerosol Index was forecasted at five-day intervals with a Mean Squared Error of ~0.0018, loss of ~0.3995, and a Structural Similarity Index of ~0.74.

Conclusion: The approach demonstrates effectiveness in forecasting smog events but suggests that model performance can further be improved through additional data and architectural refinements.

Abstract: The South Asian Smog refers to the recurring annual air pollution events
marked by high contaminant levels, reduced visibility, and significant
socio-economic impacts, primarily affecting the Indo-Gangetic Plains (IGP) from
November to February. Over the past decade, increased air pollution sources
such as crop residue burning, motor vehicles, and changing weather patterns
have intensified these smog events. However, real-time forecasting systems for
increased particulate matter concentrations are still not established at
regional scale. The Aerosol Index, closely tied to smog formation and a key
component in calculating the Air Quality Index (AQI), reflects particulate
matter concentrations. This study forecasts aerosol events using Sentinel-5P
air constituent data (2019-2023) and a Convolutional Long-Short Term Memory
(ConvLSTM) neural network, which captures spatial and temporal correlations
more effectively than previous models. Using the Ultraviolet (UV) Aerosol Index
at 340-380 nm as the predictor, results show the Aerosol Index can be
forecasted at five-day intervals with a Mean Squared Error of ~0.0018, loss of
~0.3995, and Structural Similarity Index of ~0.74. While effective, the model
can be improved by integrating additional data and refining its architecture.

</details>


### [159] [SCRNet: Spatial-Channel Regulation Network for Medical Ultrasound Image Segmentation](https://arxiv.org/abs/2508.13899)
*Weixin Xu,Ziliang Wang*

Main category: cs.CV

TL;DR: Medical ultrasound image segmentation remains challenging due to limitations of CNNs and Transformers. A novel network, SCRNet, is proposed, integrating modules to improve long-range and local feature attention.


<details>
  <summary>Details</summary>
Motivation: Traditional segmentation methods fail to adequately integrate long-range dependencies and local contexts, posing challenges in achieving high segmentation accuracy for medical ultrasound images.

Method: The paper introduces the Feature Aggregation Module (FAM) and Spatial-Channel Regulation Module (SCRM), which are integrated into the UNet framework to address segmentation deficiencies.

Result: Experiments show SCRNet achieves state-of-the-art performance compared to existing techniques in ultrasound image segmentation.

Conclusion: SCRNet effectively combines convolutional and attention mechanisms, enhancing segmentation accuracy by addressing limitations of traditional methods.

Abstract: Medical ultrasound image segmentation presents a formidable challenge in the
realm of computer vision. Traditional approaches rely on Convolutional Neural
Networks (CNNs) and Transformer-based methods to address the intricacies of
medical image segmentation. Nevertheless, inherent limitations persist, as
CNN-based methods tend to disregard long-range dependencies, while
Transformer-based methods may overlook local contextual information. To address
these deficiencies, we propose a novel Feature Aggregation Module (FAM)
designed to process two input features from the preceding layer. These features
are seamlessly directed into two branches of the Convolution and
Cross-Attention Parallel Module (CCAPM) to endow them with different roles in
each of the two branches to help establish a strong connection between the two
input features. This strategy enables our module to focus concurrently on both
long-range dependencies and local contextual information by judiciously merging
convolution operations with cross-attention mechanisms. Moreover, by
integrating FAM within our proposed Spatial-Channel Regulation Module (SCRM),
the ability to discern salient regions and informative features warranting
increased attention is enhanced. Furthermore, by incorporating the SCRM into
the encoder block of the UNet architecture, we introduce a novel framework
dubbed Spatial-Channel Regulation Network (SCRNet). The results of our
extensive experiments demonstrate the superiority of SCRNet, which consistently
achieves state-of-the-art (SOTA) performance compared to existing methods.

</details>


### [160] [PhysGM: Large Physical Gaussian Model for Feed-Forward 4D Synthesis](https://arxiv.org/abs/2508.13911)
*Chunji Lv,Zequn Chen,Donglin Di,Weinan Zhang,Hao Li,Wei Chen,Changsheng Li*

Main category: cs.CV

TL;DR: The paper presents PhysGM, a feed-forward framework for 3D motion synthesis. It predicts 3D Gaussian representations and their physical properties from a single image, enabling fast simulation and 4D rendering.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current 3D motion synthesis techniques, such as reliance on pre-reconstructed 3D Gaussian Splatting (3DGS) or unstable, optimization-heavy guidance methods.

Method: PhysGM jointly predicts 3D Gaussian representations and physical properties from a single image. The base model is refined with reference videos using Direct Preference Optimization (DPO), bypassing complex optimization processes.

Result: PhysGM generates high-fidelity 4D simulations from a single image in about one minute, outperforming prior works in speed and rendering quality.

Conclusion: PhysGM demonstrates a significant advancement in 3D motion synthesis, offering fast and realistic simulations, and has potential for broader applications in generating high-fidelity 4D content from minimal input.

Abstract: While physics-grounded 3D motion synthesis has seen significant progress,
current methods face critical limitations. They typically rely on
pre-reconstructed 3D Gaussian Splatting (3DGS) representations, while physics
integration depends on either inflexible, manually defined physical attributes
or unstable, optimization-heavy guidance from video models. To overcome these
challenges, we introduce PhysGM, a feed-forward framework that jointly predicts
a 3D Gaussian representation and its physical properties from a single image,
enabling immediate, physical simulation and high-fidelity 4D rendering. We
first establish a base model by jointly optimizing for Gaussian reconstruction
and probabilistic physics prediction. The model is then refined with physically
plausible reference videos to enhance both rendering fidelity and physics
prediction accuracy. We adopt the Direct Preference Optimization (DPO) to align
its simulations with reference videos, circumventing Score Distillation
Sampling (SDS) optimization which needs back-propagating gradients through the
complex differentiable simulation and rasterization. To facilitate the
training, we introduce a new dataset PhysAssets of over 24,000 3D assets,
annotated with physical properties and corresponding guiding videos.
Experimental results demonstrate that our method effectively generates
high-fidelity 4D simulations from a single image in one minute. This represents
a significant speedup over prior works while delivering realistic rendering
results. Our project page is at:https://hihixiaolv.github.io/PhysGM.github.io/

</details>


### [161] [DIME-Net: A Dual-Illumination Adaptive Enhancement Network Based on Retinex and Mixture-of-Experts](https://arxiv.org/abs/2508.13921)
*Ziang Wang,Xiaoqin Wang,Dingyi Wang,Qiang Li,Shushan Qiao*

Main category: cs.CV

TL;DR: The paper proposes DIME-Net, a framework for enhancing images under diverse lighting conditions using adaptive expert networks and attention mechanisms. It demonstrates strong performance on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods typically specialize in handling specific lighting conditions like low-light or backlight but fail to address diverse lighting challenges in a unified way.

Method: Introduced DIME-Net with a Mixture-of-Experts illumination estimator (adaptive S-curve expert networks) and a Damage Restoration module using advanced attention mechanisms. A hybrid dataset MixBL is also provided for robust training.

Result: DIME-Net shows competitive and generalized performance on synthetic and real-world datasets, handling both low-light and backlight scenarios effectively without retraining.

Conclusion: DIME-Net is a robust and generalized solution for enhancing images under complex and diverse lighting conditions, offering potential for real-world multimedia applications.

Abstract: Image degradation caused by complex lighting conditions such as low-light and
backlit scenarios is commonly encountered in real-world environments,
significantly affecting image quality and downstream vision tasks. Most
existing methods focus on a single type of illumination degradation and lack
the ability to handle diverse lighting conditions in a unified manner. To
address this issue, we propose a dual-illumination enhancement framework called
DIME-Net. The core of our method is a Mixture-of-Experts illumination estimator
module, where a sparse gating mechanism adaptively selects suitable S-curve
expert networks based on the illumination characteristics of the input image.
By integrating Retinex theory, this module effectively performs enhancement
tailored to both low-light and backlit images. To further correct
illumination-induced artifacts and color distortions, we design a damage
restoration module equipped with Illumination-Aware Cross Attention and
Sequential-State Global Attention mechanisms. In addition, we construct a
hybrid illumination dataset, MixBL, by integrating existing datasets, allowing
our model to achieve robust illumination adaptability through a single training
process. Experimental results show that DIME-Net achieves competitive
performance on both synthetic and real-world low-light and backlit datasets
without any retraining. These results demonstrate its generalization ability
and potential for practical multimedia applications under diverse and complex
illumination conditions.

</details>


### [162] [ViT-FIQA: Assessing Face Image Quality using Vision Transformers](https://arxiv.org/abs/2508.13957)
*Andrea Atzori,Fadi Boutros,Naser Damer*

Main category: cs.CV

TL;DR: The paper introduces ViT-FIQA, a Vision Transformer-based model designed for Face Image Quality Assessment, showing promising results compared to CNN-based approaches.


<details>
  <summary>Details</summary>
Motivation: Current FIQA methods rely heavily on CNNs, leaving Vision Transformers underutilized despite their potential benefits.

Method: ViT-FIQA uses a ViT backbone enhanced with a learnable quality token, processed via global self-attention. Two output heads predict face representations and utility scores.

Result: ViT-FIQA achieved state-of-the-art performance on challenging datasets and across various FR models, including both CNN and ViT approaches.

Conclusion: ViTs are effective and scalable for FIQA tasks, paving the way for future research leveraging transformer-based architectures.

Abstract: Face Image Quality Assessment (FIQA) aims to predict the utility of a face
image for face recognition (FR) systems. State-of-the-art FIQA methods mainly
rely on convolutional neural networks (CNNs), leaving the potential of Vision
Transformer (ViT) architectures underexplored. This work proposes ViT-FIQA, a
novel approach that extends standard ViT backbones, originally optimized for
FR, through a learnable quality token designed to predict a scalar utility
score for any given face image. The learnable quality token is concatenated
with the standard image patch tokens, and the whole sequence is processed via
global self-attention by the ViT encoders to aggregate contextual information
across all patches. At the output of the backbone, ViT-FIQA branches into two
heads: (1) the patch tokens are passed through a fully connected layer to learn
discriminative face representations via a margin-penalty softmax loss, and (2)
the quality token is fed into a regression head to learn to predict the face
sample's utility. Extensive experiments on challenging benchmarks and several
FR models, including both CNN- and ViT-based architectures, demonstrate that
ViT-FIQA consistently achieves top-tier performance. These results underscore
the effectiveness of transformer-based architectures in modeling face image
utility and highlight the potential of ViTs as a scalable foundation for future
FIQA research https://cutt.ly/irHlzXUC.

</details>


### [163] [ROVR-Open-Dataset: A Large-Scale Depth Dataset for Autonomous Driving](https://arxiv.org/abs/2508.13977)
*Xianda Guo,Ruijun Zhang,Yiqun Duan,Ruilin Wang,Keyuan Zhou,Wenzhao Zheng,Wenke Huang,Gangwei Xu,Mike Horton,Yuan Si,Hao Zhao,Long Chen*

Main category: cs.CV

TL;DR: A new large-scale, diverse dataset for dynamic outdoor depth estimation is introduced to address limitations in existing datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the saturation in benchmark performance and the lack of diversity and scalability in existing depth datasets for advancing research in 3D scene understanding.

Method: The method involves creating a lightweight acquisition pipeline that collects 20K video frames, ensuring low-cost scene coverage and using sparse ground truth for training depth estimation models.

Result: Experiments with standard monocular depth estimation models validate the dataset's utility, highlighting performance gaps in challenging scenarios.

Conclusion: This diverse and continuous dataset establishes a new platform for depth estimation, creating opportunities for enhanced research and model generalization.

Abstract: Depth estimation is a fundamental task for 3D scene understanding in
autonomous driving, robotics, and augmented reality. Existing depth datasets,
such as KITTI, nuScenes, and DDAD, have advanced the field but suffer from
limitations in diversity and scalability. As benchmark performance on these
datasets approaches saturation, there is an increasing need for a new
generation of large-scale, diverse, and cost-efficient datasets to support the
era of foundation models and multi-modal learning. To address these challenges,
we introduce a large-scale, diverse, frame-wise continuous dataset for depth
estimation in dynamic outdoor driving environments, comprising 20K video frames
to evaluate existing methods. Our lightweight acquisition pipeline ensures
broad scene coverage at low cost, while sparse yet statistically sufficient
ground truth enables robust training. Compared to existing datasets, ours
presents greater diversity in driving scenarios and lower depth density,
creating new challenges for generalization. Benchmark experiments with standard
monocular depth estimation models validate the dataset's utility and highlight
substantial performance gaps in challenging conditions, establishing a new
platform for advancing depth estimation research.

</details>


### [164] [OmViD: Omni-supervised active learning for video action detection](https://arxiv.org/abs/2508.13983)
*Aayush Rana,Akash Kumar,Vibhav Vineet,Yogesh S Rawat*

Main category: cs.CV

TL;DR: The paper studies efficient annotation strategies for video action detection by leveraging varying annotation types and proposes methods to reduce costs while maintaining detection performance.


<details>
  <summary>Details</summary>
Motivation: Current video action detection requires dense and complex annotations, which are costly and time-consuming. The aim is to optimize the annotation process by identifying and leveraging varying annotation levels based on video complexity.

Method: 1) Introduced an active learning strategy to determine the appropriate annotation type per video. 2) Developed a spatio-temporal 3D-superpixel method to create pseudo-labels from those annotations for effective model training.

Result: Validated on UCF101-24 and JHMDB-21 datasets, the approach achieved significantly reduced annotation costs while maintaining near-original performance.

Conclusion: Efficient and flexible annotation strategies can reduce costs and optimize training for video action detection, making the task more practical and scalable.

Abstract: Video action detection requires dense spatio-temporal annotations, which are
both challenging and expensive to obtain. However, real-world videos often vary
in difficulty and may not require the same level of annotation. This paper
analyzes the appropriate annotation types for each sample and their impact on
spatio-temporal video action detection. It focuses on two key aspects: 1) how
to obtain varying levels of annotation for videos, and 2) how to learn action
detection from different annotation types. The study explores video-level tags,
points, scribbles, bounding boxes, and pixel-level masks. First, a simple
active learning strategy is proposed to estimate the necessary annotation type
for each video. Then, a novel spatio-temporal 3D-superpixel approach is
introduced to generate pseudo-labels from these annotations, enabling effective
training. The approach is validated on UCF101-24 and JHMDB-21 datasets,
significantly cutting annotation costs with minimal performance loss.

</details>


### [165] [Physics-Based 3D Simulation for Synthetic Data Generation and Failure Analysis in Packaging Stability Assessment](https://arxiv.org/abs/2508.13989)
*Samuel Seligardi,Pietro Musoni,Eleonora Iotti,Gianluca Contesso,Alessandro Dal Palù*

Main category: cs.CV

TL;DR: This paper introduces a simulation system for pallet setups, aiming to optimize packaging safety, cost-efficiency, and environmental concerns in logistics.


<details>
  <summary>Details</summary>
Motivation: The motivation is addressing rising logistics demands, plastic wrapping environmental concerns, and improving packaging safety analysis.

Method: The paper utilizes a controllable physical simulation system with a 3D graphics-based environment, and trains a deep neural network to analyze simulation crash tests.

Result: The system effectively simulates pallet dynamics, reduces physical testing, cuts costs, minimizes environmental impact, and improves safety analysis through video-based predictions.

Conclusion: A novel simulation and neural network evaluation system improves pallet dynamics analysis, aiding safety while addressing logistics costs and environmental concerns.

Abstract: The design and analysis of pallet setups are essential for ensuring safety of
packages transportation. With rising demands in the logistics sector, the
development of automated systems utilizing advanced technologies has become
increasingly crucial. Moreover, the widespread use of plastic wrapping has
motivated researchers to investigate eco-friendly alternatives that still
adhere to safety standards. We present a fully controllable and accurate
physical simulation system capable of replicating the behavior of moving
pallets. It features a 3D graphics-based virtual environment that supports a
wide range of configurations, including variable package layouts, different
wrapping materials, and diverse dynamic conditions. This innovative approach
reduces the need for physical testing, cutting costs and environmental impact
while improving measurement accuracy for analyzing pallet dynamics.
Additionally, we train a deep neural network to evaluate the rendered videos
generated by our simulator, as a crash-test predictor for pallet
configurations, further enhancing the system's utility in safety analysis.

</details>


### [166] [Self-Supervised Sparse Sensor Fusion for Long Range Perception](https://arxiv.org/abs/2508.13995)
*Edoardo Palladin,Samuel Brucker,Filippo Ghilotti,Praveen Narayanan,Mario Bijelic,Felix Heide*

Main category: cs.CV

TL;DR: The paper addresses the challenge of extending the perception range of autonomous vehicles to enable safe highway driving, particularly for large trucks, by proposing a new 3D encoding technique and self-supervised pre-training scheme.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable autonomous vehicles, especially heavy forty-ton trucks, to safely navigate highways at speeds exceeding 100 km/h. Such scenarios require perception distances of at least 250 meters, which is much greater than the current typical range for urban environments.

Method: The method involves introducing a sparse 3D representation of multi-modal and temporal features, coupled with a novel self-supervised pre-training methodology using unlabeled camera-LiDAR data. This approach mitigates the memory and computational inefficiencies in Bird's Eye View (BEV) representations as distances increase.

Result: The approach extends perception distances to 250 meters and demonstrates a 26.6% improvement in mAP for object detection and a 30.5% decrease in Chamfer Distance for LiDAR forecasting compared to existing methods.

Conclusion: The proposed technique effectively expands perception capabilities of autonomous vehicles for long-distance highway driving, particularly benefiting larger and heavier vehicles, thus improving safety margins and performance.

Abstract: Outside of urban hubs, autonomous cars and trucks have to master driving on
intercity highways. Safe, long-distance highway travel at speeds exceeding 100
km/h demands perception distances of at least 250 m, which is about five times
the 50-100m typically addressed in city driving, to allow sufficient planning
and braking margins. Increasing the perception ranges also allows to extend
autonomy from light two-ton passenger vehicles to large-scale forty-ton trucks,
which need a longer planning horizon due to their high inertia. However, most
existing perception approaches focus on shorter ranges and rely on Bird's Eye
View (BEV) representations, which incur quadratic increases in memory and
compute costs as distance grows. To overcome this limitation, we built on top
of a sparse representation and introduced an efficient 3D encoding of
multi-modal and temporal features, along with a novel self-supervised
pre-training scheme that enables large-scale learning from unlabeled
camera-LiDAR data. Our approach extends perception distances to 250 meters and
achieves an 26.6% improvement in mAP in object detection and a decrease of
30.5% in Chamfer Distance in LiDAR forecasting compared to existing methods,
reaching distances up to 250 meters. Project Page:
https://light.princeton.edu/lrs4fusion/

</details>


### [167] [Online 3D Gaussian Splatting Modeling with Novel View Selection](https://arxiv.org/abs/2508.14014)
*Byeonggwon Lee,Junkyu Park,Khang Truong Giang,Soohwan Song*

Main category: cs.CV

TL;DR: The study introduces an online method for constructing 3D Gaussian Splatting models from RGB frames, improving scene completeness using adaptive view selection and multi-view stereo techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the incompleteness in 3D scene reconstructions caused by relying only on keyframes and to enable more generalizable and complete 3D models using diverse viewpoint data in online settings.

Method: The proposed method incorporates adaptive view selection, which analyzes reconstruction quality in real-time to choose optimal non-keyframes for additional training. It also integrates an online multi-view stereo framework to maintain 3D consistency.

Result: The experimental results show that this approach achieves better completeness and outperforms state-of-the-art methods, particularly in complex outdoor scenes.

Conclusion: By leveraging adaptive view selection and multi-view stereo processing, the method significantly enhances the quality and completeness of 3DGS models while maintaining efficiency in online environments.

Abstract: This study addresses the challenge of generating online 3D Gaussian Splatting
(3DGS) models from RGB-only frames. Previous studies have employed dense SLAM
techniques to estimate 3D scenes from keyframes for 3DGS model construction.
However, these methods are limited by their reliance solely on keyframes, which
are insufficient to capture an entire scene, resulting in incomplete
reconstructions. Moreover, building a generalizable model requires
incorporating frames from diverse viewpoints to achieve broader scene coverage.
However, online processing restricts the use of many frames or extensive
training iterations. Therefore, we propose a novel method for high-quality 3DGS
modeling that improves model completeness through adaptive view selection. By
analyzing reconstruction quality online, our approach selects optimal
non-keyframes for additional training. By integrating both keyframes and
selected non-keyframes, the method refines incomplete regions from diverse
viewpoints, significantly enhancing completeness. We also present a framework
that incorporates an online multi-view stereo approach, ensuring consistency in
3D information throughout the 3DGS modeling process. Experimental results
demonstrate that our method outperforms state-of-the-art methods, delivering
exceptional performance in complex outdoor scenes.

</details>


### [168] [Backdooring Self-Supervised Contrastive Learning by Noisy Alignment](https://arxiv.org/abs/2508.14015)
*Tuo Chen,Jie Gui,Minjing Dong,Ju Jia,Lanting Fang,Jian Liu*

Main category: cs.CV

TL;DR: The paper introduces Noisy Alignment (NA), a novel method to optimize data poisoning backdoor attacks against self-supervised contrastive learning (CL) systems, achieving strong attack performance and defense robustness.


<details>
  <summary>Details</summary>
Motivation: Self-supervised contrastive learning suffers from vulnerabilities in data poisoning backdoor attacks that compromise the learning process through injected, manipulated images.

Method: The paper proposes Noisy Alignment (NA), which explicitly suppresses noise in poisoned images and optimizes random cropping in CL as an image layout optimization problem using derived optimal parameters.

Result: Noisy Alignment achieves superior performance, surpasses existing methods in attack efficacy, ensures high accuracy on clean data, and is robust to standard backdoor defenses.

Conclusion: The technique effectively improves the stealth and impact of data poisoning on self-supervised CL systems while maintaining standard non-manipulated data accuracy and resisting common defenses.

Abstract: Self-supervised contrastive learning (CL) effectively learns transferable
representations from unlabeled data containing images or image-text pairs but
suffers vulnerability to data poisoning backdoor attacks (DPCLs). An adversary
can inject poisoned images into pretraining datasets, causing compromised CL
encoders to exhibit targeted misbehavior in downstream tasks. Existing DPCLs,
however, achieve limited efficacy due to their dependence on fragile implicit
co-occurrence between backdoor and target object and inadequate suppression of
discriminative features in backdoored images. We propose Noisy Alignment (NA),
a DPCL method that explicitly suppresses noise components in poisoned images.
Inspired by powerful training-controllable CL attacks, we identify and extract
the critical objective of noisy alignment, adapting it effectively into
data-poisoning scenarios. Our method implements noisy alignment by
strategically manipulating contrastive learning's random cropping mechanism,
formulating this process as an image layout optimization problem with
theoretically derived optimal parameters. The resulting method is simple yet
effective, achieving state-of-the-art performance compared to existing DPCLs,
while maintaining clean-data accuracy. Furthermore, Noisy Alignment
demonstrates robustness against common backdoor defenses. Codes can be found at
https://github.com/jsrdcht/Noisy-Alignment.

</details>


### [169] [InfiniteTalk: Audio-driven Video Generation for Sparse-Frame Video Dubbing](https://arxiv.org/abs/2508.14033)
*Shaoshu Yang,Zhe Kong,Feng Gao,Meng Cheng,Xiangyu Liu,Yong Zhang,Zhuoliang Kang,Wenhan Luo,Xunliang Cai,Ran He,Xiaoming Wei*

Main category: cs.CV

TL;DR: This paper introduces InfiniteTalk, an audio-driven generator offering high-quality, synchronized video dubbing with full-body motion editing, overcoming limitations of traditional mouth-only dubbing techniques.


<details>
  <summary>Details</summary>
Motivation: Traditional video dubbing methods only modify the mouth region, causing mismatched facial expressions and gestures that reduce viewer immersion.

Method: The paper presents InfiniteTalk, which uses temporal context frames and a novel sampling strategy for infinite-sequence audio-visual dubbing while preserving identity and motion coherence.

Result: The method achieves state-of-the-art performance in visual realism, emotional alignment, and motion synchronization on multiple datasets, including HDTF and CelebV-HQ.

Conclusion: InfiniteTalk sets a new standard in video dubbing, enabling fully synchronized full-body motion editing with enhanced visual and emotional coherence while preserving reference features.

Abstract: Recent breakthroughs in video AIGC have ushered in a transformative era for
audio-driven human animation. However, conventional video dubbing techniques
remain constrained to mouth region editing, resulting in discordant facial
expressions and body gestures that compromise viewer immersion. To overcome
this limitation, we introduce sparse-frame video dubbing, a novel paradigm that
strategically preserves reference keyframes to maintain identity, iconic
gestures, and camera trajectories while enabling holistic, audio-synchronized
full-body motion editing. Through critical analysis, we identify why naive
image-to-video models fail in this task, particularly their inability to
achieve adaptive conditioning. Addressing this, we propose InfiniteTalk, a
streaming audio-driven generator designed for infinite-length long sequence
dubbing. This architecture leverages temporal context frames for seamless
inter-chunk transitions and incorporates a simple yet effective sampling
strategy that optimizes control strength via fine-grained reference frame
positioning. Comprehensive evaluations on HDTF, CelebV-HQ, and EMTD datasets
demonstrate state-of-the-art performance. Quantitative metrics confirm superior
visual realism, emotional coherence, and full-body motion synchronization.

</details>


### [170] [GeoSAM2: Unleashing the Power of SAM2 for 3D Part Segmentation](https://arxiv.org/abs/2508.14036)
*Ken Deng,Yunhan Yang,Jingxiang Sun,Xihui Liu,Yebin Liu,Ding Liang,Yan-Pei Cao*

Main category: cs.CV

TL;DR: DetailGen3D enhances coarse 3D shapes by refining details through a latent-space transformation, ensuring fine geometry without high computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing 3D generation methods produce shapes quickly from sparse inputs but lack geometric detail, limited by computational resources.

Method: DetailGen3D utilizes data-driven flows in latent space for coarse-to-fine transformation, combined with token matching for spatial correspondence and global structure preservation. It also tailors training data to refined coarse shapes.

Result: DetailGen3D achieves detailed geometry refinement applicable to diverse 3D inputs, maintaining high fidelity and computational efficiency during training.

Conclusion: DetailGen3D allows efficient enhancement of 3D shapes, emphasizing detail synthesis and broad utility across multiple input methods.

Abstract: Modern 3D generation methods can rapidly create shapes from sparse or single
views, but their outputs often lack geometric detail due to computational
constraints. We present DetailGen3D, a generative approach specifically
designed to enhance these generated 3D shapes. Our key insight is to model the
coarse-to-fine transformation directly through data-dependent flows in latent
space, avoiding the computational overhead of large-scale 3D generative models.
We introduce a token matching strategy that ensures accurate spatial
correspondence during refinement, enabling local detail synthesis while
preserving global structure. By carefully designing our training data to match
the characteristics of synthesized coarse shapes, our method can effectively
enhance shapes produced by various 3D generation and reconstruction approaches,
from single-view to sparse multi-view inputs. Extensive experiments demonstrate
that DetailGen3D achieves high-fidelity geometric detail synthesis while
maintaining efficiency in training.

</details>


### [171] [Distilled-3DGS:Distilled 3D Gaussian Splatting](https://arxiv.org/abs/2508.14037)
*Lintao Xiang,Xinkai Chen,Jianhuang Lai,Guangcong Wang*

Main category: cs.CV

TL;DR: The paper introduces "Distilled-3DGS," a knowledge distillation framework to optimize 3D Gaussian Splatting models for novel view synthesis by improving rendering quality and reducing memory usage.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting achieves high-fidelity rendering but requires a large amount of memory and storage, necessitating optimization to reduce resource consumption while maintaining quality.

Method: The framework uses knowledge distillation with teacher models (vanilla 3DGS, noise-augmented, dropout-regularized) guiding a lightweight student model. A structural similarity loss is applied for geometric consistency.

Result: Distilled-3DGS yields rendering results that balance high quality with low storage requirements, outperforming state-of-the-art techniques across varied datasets.

Conclusion: The framework proves effective in enhancing both rendering quality and memory/storage efficiency without adding complexity.

Abstract: 3D Gaussian Splatting (3DGS) has exhibited remarkable efficacy in novel view
synthesis (NVS). However, it suffers from a significant drawback: achieving
high-fidelity rendering typically necessitates a large number of 3D Gaussians,
resulting in substantial memory consumption and storage requirements. To
address this challenge, we propose the first knowledge distillation framework
for 3DGS, featuring various teacher models, including vanilla 3DGS,
noise-augmented variants, and dropout-regularized versions. The outputs of
these teachers are aggregated to guide the optimization of a lightweight
student model. To distill the hidden geometric structure, we propose a
structural similarity loss to boost the consistency of spatial geometric
distributions between the student and teacher model. Through comprehensive
quantitative and qualitative evaluations across diverse datasets, the proposed
Distilled-3DGS, a simple yet effective framework without bells and whistles,
achieves promising rendering results in both rendering quality and storage
efficiency compared to state-of-the-art methods. Project page:
https://distilled3dgs.github.io . Code:
https://github.com/lt-xiang/Distilled-3DGS .

</details>


### [172] [Beyond Simple Edits: Composed Video Retrieval with Dense Modifications](https://arxiv.org/abs/2508.14039)
*Omkar Thawakar,Dmitry Demidov,Ritesh Thawkar,Rao Muhammad Anwer,Mubarak Shah,Fahad Shahbaz Khan,Salman Khan*

Main category: cs.CV

TL;DR: The paper focuses on improving composed video retrieval by introducing a detailed dataset and a state-of-the-art cross-attention model.


<details>
  <summary>Details</summary>
Motivation: Standard retrieval frameworks struggle with fine-grained compositional queries and temporal variations, necessitating better methods for composed video retrieval.

Method: The authors introduce Dense-WebVid-CoVR dataset with 1.6M samples featuring dense text modifications, and propose a Cross-Attention model leveraging grounded text encoders for precise alignment of queries with target videos.

Result: The proposed model achieves 71.3% Recall@1 and outperforms existing state-of-the-art methods by 3.4% in retrieval effectiveness.

Conclusion: The dataset and model significantly enhance composed video retrieval capabilities, offering more detailed alignment and improved performance metrics, advancing the field.

Abstract: Composed video retrieval is a challenging task that strives to retrieve a
target video based on a query video and a textual description detailing
specific modifications. Standard retrieval frameworks typically struggle to
handle the complexity of fine-grained compositional queries and variations in
temporal understanding limiting their retrieval ability in the fine-grained
setting. To address this issue, we introduce a novel dataset that captures both
fine-grained and composed actions across diverse video segments, enabling more
detailed compositional changes in retrieved video content. The proposed
dataset, named Dense-WebVid-CoVR, consists of 1.6 million samples with dense
modification text that is around seven times more than its existing
counterpart. We further develop a new model that integrates visual and textual
information through Cross-Attention (CA) fusion using grounded text encoder,
enabling precise alignment between dense query modifications and target videos.
The proposed model achieves state-of-the-art results surpassing existing
methods on all metrics. Notably, it achieves 71.3\% Recall@1 in visual+text
setting and outperforms the state-of-the-art by 3.4\%, highlighting its
efficacy in terms of leveraging detailed video descriptions and dense
modification texts. Our proposed dataset, code, and model are available at
:https://github.com/OmkarThawakar/BSE-CoVR

</details>


### [173] [LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos](https://arxiv.org/abs/2508.14041)
*Chin-Yang Lin,Cheng Sun,Fu-En Yang,Min-Hung Chen,Yen-Yu Lin,Yu-Lun Liu*

Main category: cs.CV

TL;DR: LongSplat introduces a method for novel view synthesis from long video sequences, addressing challenges like irregular motion, unknown poses, and expansive scenes using 3D Gaussian Splatting.


<details>
  <summary>Details</summary>
Motivation: To tackle issues in novel view synthesis such as pose drift, geometry inaccuracies, and memory inefficiencies in long, casually captured videos.

Method: LongSplat features three mechanisms: Incremental Joint Optimization for simultaneous pose and 3D Gaussian optimization, a Pose Estimation Module using 3D priors, and Octree Anchor Formation for spatially dense representation.

Result: Experiments reveal that LongSplat outperforms existing methods in rendering quality, pose estimation accuracy, and computational efficiency on challenging benchmarks.

Conclusion: LongSplat offers a robust and efficient solution to novel view synthesis in challenging scenarios, setting a new state-of-the-art benchmark.

Abstract: LongSplat addresses critical challenges in novel view synthesis (NVS) from
casually captured long videos characterized by irregular camera motion, unknown
camera poses, and expansive scenes. Current methods often suffer from pose
drift, inaccurate geometry initialization, and severe memory limitations. To
address these issues, we introduce LongSplat, a robust unposed 3D Gaussian
Splatting framework featuring: (1) Incremental Joint Optimization that
concurrently optimizes camera poses and 3D Gaussians to avoid local minima and
ensure global consistency; (2) a robust Pose Estimation Module leveraging
learned 3D priors; and (3) an efficient Octree Anchor Formation mechanism that
converts dense point clouds into anchors based on spatial density. Extensive
experiments on challenging benchmarks demonstrate that LongSplat achieves
state-of-the-art results, substantially improving rendering quality, pose
accuracy, and computational efficiency compared to prior approaches. Project
page: https://linjohnss.github.io/longsplat/

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [174] [Harnessing the Full Potential of RRAMs through Scalable and Distributed In-Memory Computing with Integrated Error Correction](https://arxiv.org/abs/2508.13298)
*Huynh Q. N. Vo,Md Tawsif Rahman Chowdhury,Paritosh Ramanan,Murat Yildirim,Gozde Tutuncuoglu*

Main category: cs.DC

TL;DR: MELISO+ is a distributed in-memory computing framework using RRAM that mitigates device non-idealities and enables scalable matrix computations for large-scale AI applications.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the energy inefficiencies of traditional computing architectures due to data movement and the challenges faced by RRAM in-memory computing systems related to scalability and device-level issues.

Method: MELISO+ introduces a two-tier error correction mechanism and develops a distributed RRAM computing framework capable of highly scalable matrix operations.

Result: MELISO+ reduces arithmetic errors by over 90%, improves energy efficiency by three to five orders of magnitude, and lowers latency by 100 times.

Conclusion: This framework enables high-dimensional computing with lower-precision RRAM devices that surpass high-precision alternatives in accuracy, energy, and latency, making it ideal for sustainable AI applications like large language models.

Abstract: Exponential growth in global computing demand is exacerbated due to the
higher-energy requirements of conventional architectures, primarily due to
energy-intensive data movement. In-memory computing with Resistive Random
Access Memory (RRAM) addresses this by co-integrating memory and processing,
but faces significant hurdles related to device-level non-idealities and poor
scalability for large computing tasks. Here, we introduce \textbf{MELISO+}
(In-\textbf{Me}mory \textbf{Li}near \textbf{So}lver), a full-stack, distributed
framework for energy-efficient in-memory computing. MELISO+ proposes a novel
two-tier error correction mechanism to mitigate device non-idealities and
develops a distributed RRAM computing framework to enable matrix computations
exceeding dimensions of $65,000 \times 65,000$. This approach reduces first-
and second-order arithmetic errors due to device non-idealities by over 90\%,
enhances energy efficiency by three to five orders of magnitude, and decreases
latency 100-fold. Hence, MELISO+ allows lower-precision RRAM devices to
outperform high-precision device alternatives in accuracy, energy and latency
metrics. By unifying algorithm-hardware co-design with scalable architecture,
MELISO+ significantly advances sustainable, high-dimensional computing suitable
for applications like large language models and generative AI.

</details>


### [175] [Persistent and Partitioned MPI for Stencil Communication](https://arxiv.org/abs/2508.13370)
*Gerald Collom,Jason Burmark,Olga Pearce,Amanda Bienz*

Main category: cs.DC

TL;DR: The paper explores the performance of iterative stencil communications in large-scale parallel applications using non-blocking, persistent, and partitioned communication routines, achieving significant speedups with optimizations.


<details>
  <summary>Details</summary>
Motivation: To address communication inefficiencies dominating performance in large-scale iterative stencil operations.

Method: Performance analysis using the Comb benchmarking suite with different MPI communication routines (non-blocking, persistent, partitioned) at varied scales, process counts, thread counts, and message sizes.

Result: Persistent MPI communication achieved up to 37% speedup, and partitioned MPI communication up to 68% speedup over baseline MPI communications.

Conclusion: Optimized MPI communication routines significantly enhance performance for iterative stencil communication, particularly with partitioned communication.

Abstract: Many parallel applications rely on iterative stencil operations, whose
performance are dominated by communication costs at large scales. Several MPI
optimizations, such as persistent and partitioned communication, reduce
overheads and improve communication efficiency through amortized setup costs
and reduced synchronization of threaded sends. This paper presents the
performance of stencil communication in the Comb benchmarking suite when using
non blocking, persistent, and partitioned communication routines. The impact of
each optimization is analyzed at various scales. Further, the paper presents an
analysis of the impact of process count, thread count, and message size on
partitioned communication routines. Measured timings show that persistent MPI
communication can provide a speedup of up to 37% over the baseline MPI
communication, and partitioned MPI communication can provide a speedup of up to
68%.

</details>


### [176] [OrbitChain: Orchestrating In-orbit Real-time Analytics of Earth Observation Data](https://arxiv.org/abs/2508.13374)
*Zhouyu Li,Zhijing Yang,Huayue Gu,Xiaojian Wang,Yuchen Liu,Ruozhou Yu*

Main category: cs.DC

TL;DR: OrbitChain is a real-time analytics framework for Earth observation satellites, enabling faster computations and reduced communication overhead compared to existing systems.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of delayed data processing from Earth observation satellites, which hampers real-time applications like disaster response.

Method: OrbitChain decomposes analytics applications into microservices, allocates computational resources across satellite constellations, and uses a traffic routing algorithm to minimize communication overhead.

Result: Experiments demonstrate that OrbitChain can handle 60% more analytics workload while decreasing communication overhead by up to 72%.

Conclusion: OrbitChain significantly enhances real-time analytics capability and efficiency for satellite constellations, enabling timely responses and inter-constellation collaborations.

Abstract: Earth observation analytics have the potential to serve many time-sensitive
applications. However, due to limited bandwidth and duration of
ground-satellite connections, it takes hours or even days to download and
analyze data from existing Earth observation satellites, making real-time
demands like timely disaster response impossible. Toward real-time analytics,
we introduce OrbitChain, a collaborative analytics framework that orchestrates
computational resources across multiple satellites in an Earth observation
constellation. OrbitChain decomposes analytics applications into microservices
and allocates computational resources for time-constrained analysis. A traffic
routing algorithm is devised to minimize the inter-satellite communication
overhead. OrbitChain adopts a pipeline workflow that completes Earth
observation tasks in real-time, facilitates time-sensitive applications and
inter-constellation collaborations such as tip-and-cue. To evaluate OrbitChain,
we implement a hardware-in-the-loop orbital computing testbed. Experiments show
that our system can complete up to 60% analytics workload than existing Earth
observation analytics framework while reducing the communication overhead by up
to 72%.

</details>


### [177] [Optimizing Allreduce Operations for Heterogeneous Architectures with Multiple Processes per GPU](https://arxiv.org/abs/2508.13397)
*Michael Adams,Amanda Bienz*

Main category: cs.DC

TL;DR: The paper introduces optimizations for large GPU all-reduce operations by utilizing multiple CPU cores per GPU, significantly speeding up performance.


<details>
  <summary>Details</summary>
Motivation: Address the communication bottlenecks in inter-GPU all-reduce operations, leveraging idle CPU resources in emerging heterogeneous architectures.

Method: Enhance GPU-aware lane reductions and utilize multiple CPU cores per GPU for acceleration in all-reduce operations.

Result: Achieved up to 2.45x speedup on NVIDIA A100 GPUs and similar speedups using collective communication libraries on NVIDIA and AMD architectures.

Conclusion: The proposed optimizations efficiently utilize idle CPU cores in heterogeneous GPU architectures, demonstrating significant improvements in collective communication performance.

Abstract: Large inter-GPU all-reduce operations, prevalent throughout deep learning,
are bottlenecked by communication costs. Emerging heterogeneous architectures
are comprised of complex nodes, often containing $4$ GPUs and dozens to
hundreds of CPU cores per node. Parallel applications are typically accelerated
on the available GPUs, using only a single CPU core per GPU while the remaining
cores sit idle. This paper presents novel optimizations to large GPU-aware
all-reduce operations, extending lane-aware reductions to the GPUs, and notably
using multiple CPU cores per GPU to accelerate these operations. These
multi-CPU-accelerated GPU-aware lane all-reduces yield speedup of up to $2.45$x
for large MPI all-reduces across the NVIDIA A100 GPUs of NCSA's Delta
supercomputer. Finally, the approach is extended to NVIDIA's and AMD's
collective communication libraries, achieving speedup of up to $1.77$x and
$1.71$x, respectively, across $2$ state-of-the-art supercomputers.

</details>


### [178] [DDoS Attacks in Cloud Computing: Detection and Prevention](https://arxiv.org/abs/2508.13522)
*Zain Ahmad,Musab Ahmad,Bilal Ahmad*

Main category: cs.DC

TL;DR: The paper analyzes various types of DDoS attacks, existing detection methods, and prevention techniques to provide a comprehensive overview and guidelines for mitigating such threats.


<details>
  <summary>Details</summary>
Motivation: To address the increasing complexity and frequency of DDoS attacks, and provide insights for organizations to enhance their defense mechanisms.

Method: The study reviews types of attacks (volumetric, protocol, application layer), evaluates detection (packet filtering, intrusion detection, machine learning), and explores prevention techniques (firewalls, rate limiting, CPP, ELD).

Result: It assesses the strengths and limitations of detection methods, effectiveness of prevention mechanisms, and their applicability for different scenarios.

Conclusion: The paper offers a detailed overview of DDoS attack types, detection and prevention strategies, aiming to improve cybersecurity defenses against such threats.

Abstract: DDoS attacks are one of the most prevalent and harmful cybersecurity threats
faced by organizations and individuals today. In recent years, the complexity
and frequency of DDoS attacks have increased significantly, making it
challenging to detect and mitigate them effectively. The study analyzes various
types of DDoS attacks, including volumetric, protocol, and application layer
attacks, and discusses the characteristics, impact, and potential targets of
each type. It also examines the existing techniques used for DDoS attack
detection, such as packet filtering, intrusion detection systems, and machine
learning-based approaches, and their strengths and limitations. Moreover, the
study explores the prevention techniques employed to mitigate DDoS attacks,
such as firewalls, rate limiting , CPP and ELD mechanism. It evaluates the
effectiveness of each approach and its suitability for different types of
attacks and environments. In conclusion, this study provides a comprehensive
overview of the different types of DDoS attacks, their detection, and
prevention techniques. It aims to provide insights and guidelines for
organizations and individuals to enhance their cybersecurity posture and
protect against DDoS attacks.

</details>


### [179] [LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale Architectures](https://arxiv.org/abs/2508.13523)
*Anders Johansson,Evan Weinberg,Christian R. Trott,Megan J. McCarthy,Stan G. Moore*

Main category: cs.DC

TL;DR: LAMMPS has been updated to leverage Kokkos library for multi-scale molecular dynamics simulations on heterogeneous computing systems, achieving strong scalability across major US exascale machines.


<details>
  <summary>Details</summary>
Motivation: The paper aims to adapt the LAMMPS molecular dynamics code to modern heterogeneous computing systems, ensuring performance portability and scalability for scientific simulations.

Method: The authors integrated the Kokkos library into LAMMPS and examined the performance of various interatomic potentials on GPUs from different vendors, along with performance metrics on US exascale machines.

Result: The study demonstrated effective scalability of LAMMPS with various interatomic potentials on GPUs and strong performance on exascale computing systems.

Conclusion: LAMMPS successfully adapts to heterogeneous computing environments, showcasing scalability and efficiency across GPUs and exascale machines for molecular dynamics simulations.

Abstract: Since its inception in 1995, LAMMPS has grown to be a world-class molecular
dynamics code, with thousands of users, over one million lines of code, and
multi-scale simulation capabilities. We discuss how LAMMPS has adapted to the
modern heterogeneous computing landscape by integrating the Kokkos performance
portability library into the existing C++ code. We investigate performance
portability of simple pairwise, many-body reactive, and machine-learned
force-field interatomic potentials. We present results on GPUs across different
vendors and generations, and analyze performance trends, probing FLOPS
throughput, memory bandwidths, cache capabilities, and thread-atomic operation
performance. Finally, we demonstrate strong scaling on all current US exascale
machines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the
three potentials.

</details>


### [180] [LUNDIsim: model meshes for flow simulation and scientific data compression benchmarks](https://arxiv.org/abs/2508.13636)
*Laurent Duval,Frédéric Payan,Christophe Preux,Lauriane Bouard*

Main category: cs.DC

TL;DR: The paper addresses concerns regarding the growing volume of scientific data in numerical simulations for earth sciences, proposing remedies like compression techniques and sharing benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: The increasing volume of scientific data in earth sciences challenges computability, interpretability, and sustainability, necessitating new approaches to data management.

Method: The authors present LUNDIsim, a geological mesh dataset with porosity, permeability data, and models for evaluating techniques like upscaling and mesh compression.

Result: LUNDIsim provides various resolutions and reservoir engineering features for benchmarking, testing flow simulations, and advanced workflows.

Conclusion: The dataset, adhering to FAIR principles, serves as a benchmark for developing efficient data compression and mesh processing solutions in earth sciences.

Abstract: The volume of scientific data produced for and by numerical simulation
workflows is increasing at an incredible rate. This raises concerns either in
computability, interpretability, and sustainability. This is especially
noticeable in earth science (geology, meteorology, oceanography, and
astronomy), notably with climate studies.
  We highlight five main evaluation issues: efficiency, discrepancy, diversity,
interpretability, availability.
  Among remedies, lossless and lossy compression techniques are becoming
popular to better manage dataset volumes. Performance assessment -- with
comparative benchmarks -- require open datasets shared under FAIR principles
(Findable, Accessible, Interoperable, Reusable), with MRE (Minimal Reproducible
Example) ancillary data for reuse. We share LUNDIsim, an exemplary faulted
geological mesh. It is inspired by SPE10 comparative Challenge. Enhanced by
porosity/permeability datasets, this dataset proposes four distinct subsurface
environments. They were primarily designed for flow simulation in porous media.
Several consistent resolutions (with HexaShrink multiscale representations) are
proposed for each model. We also provide a set of reservoir features for
reproducing typical two-phase flow simulations on all LUNDIsim models in a
reservoir engineering context. This dataset is chiefly meant for benchmarking
and evaluating data size reduction (upscaling) or genuine composite mesh
compression algorithms. It is also suitable for other advanced mesh processing
workflows in geology and reservoir engineering, from visualization to machine
learning.
  LUNDIsim meshes are available at https://doi.org/10.5281/zenodo.14641958

</details>


### [181] [Estimating CO$_2$ emissions of distributed applications and platforms with SimGrid/Batsim](https://arxiv.org/abs/2508.13693)
*Gabriella Saraiva,Miguel Vasconcelos,Sarita Mazzini Bruschi,Danilo Carastan-Santos,Daniel Cordeiro*

Main category: cs.DC

TL;DR: The paper introduces a plugin for Batsim to calculate CO2 emissions in data center simulations, enabling evaluation of scheduling strategies' environmental impact.


<details>
  <summary>Details</summary>
Motivation: Understanding the environmental impact of data center management and task scheduling is important due to growing sustainability concerns.

Method: This work incorporates a carbon footprint plugin in SimGrid and integrates it into Batsim, calculating CO2 emissions based on energy use and machine carbon intensity.

Result: The plugin successfully integrates with Batsim, enabling researchers to evaluate carbon efficiency during simulation workflows.

Conclusion: This tool is valuable for researchers aiming to optimize task scheduling strategies while reducing carbon emissions.

Abstract: This work presents a carbon footprint plugin designed to extend the
capabilities of the Batsim simulator by allowing the calculation of CO$_2$
emissions during simulation runs. The goal is to comprehensively assess the
environmental impact associated with task and resource management strategies in
data centers. The plugin is developed within SimGrid -- the underlying
simulation framework of Batsim -- and computes carbon emissions based on the
simulated platform's energy consumption and carbon intensity factor of the
simulated machines. Once implemented, it is integrated into Batsim, ensuring
compatibility with existing simulation workflows and enabling researchers to
assess the carbon efficiency of their scheduling strategies.

</details>


### [182] [CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint Caching and Resource-Aware Graph Partitioning](https://arxiv.org/abs/2508.13716)
*Xianfeng Song,Yi Zou,Zheng Shi*

Main category: cs.DC

TL;DR: CaPGNN is a framework designed to enhance full-batch GNN training efficiency in multi-GPU environments by minimizing communication overhead and optimizing workload balance.


<details>
  <summary>Details</summary>
Motivation: Scalability issues in full-batch GNN training arise from high communication overhead and load imbalance in distributed settings.

Method: CaPGNN employs an adaptive caching algorithm using CPU and GPU memory to reduce redundant vertex feature transmissions and introduces a resource-aware graph partitioning technique to adjust subgraph sizes dynamically based on GPU capabilities.

Result: Experiments show CaPGNN reduces communication costs by up to 96% and accelerates training by up to 12.7x compared to existing methods.

Conclusion: The research demonstrates the viability of adaptive caching and resource-aware partitioning in improving scalability, efficiency, and practicality for full-batch GNN training in distributed environments.

Abstract: Graph Neural Networks (GNNs) have shown remarkable capabilities in processing
graph-structured data prevalent in various real-world applications. However,
the scalability of full-batch GNN training becomes severely limited by high
communication overhead and load imbalance in distributed environments. In this
paper, we present CaPGNN, a novel framework for efficient parallel full-batch
GNN training on single-server with multi-GPU, designed specifically to reduce
redundant inter-GPU communication and balance computational workloads. We
propose a joint adaptive caching algorithm that leverages both CPU and GPU
memory to significantly reduce the repetitive transmission of vertex features
across partitions. Additionally, we introduce a resource-aware graph
partitioning algorithm that adjusts subgraph sizes dynamically according to the
heterogeneous computational and communication capacities of GPUs. Extensive
experiments on large-scale benchmark datasets demonstrate that CaPGNN
effectively reduces communication costs by up to 96% and accelerates GNN
training by up to 12.7 times compared to state-of-the-art approaches. Our
results highlight the potential of adaptive caching and resource-aware
partitioning to facilitate scalable, efficient, and practical deployment of
full-batch GNN training in distributed computing environments.

</details>


### [183] [Is RISC-V ready for High Performance Computing? An evaluation of the Sophon SG2044](https://arxiv.org/abs/2508.13840)
*Nick Brown*

Main category: cs.DC

TL;DR: The paper studies the HPC performance of the new SOPHGO SG2044 CPU, highlighting its significant improvements in multi-core performance and compute-bound workloads over the SG2042.


<details>
  <summary>Details</summary>
Motivation: To examine the barriers preventing RISC-V adoption in HPC and evaluate the performance of the Sophon SG2044 CPU to identify advancements and shortcomings in its architecture.

Method: A performance comparison study was performed on the SG2044 and its predecessor, the SG2042, as well as other competing architectures. Key focus areas included multi-core performance and upgrades like RVV v1.0 support and memory enhancements.

Result: The SG2044 achieves up to 4.91x better performance than the SG2042 at higher core counts and significantly closes the performance gap with other architectures for compute-heavy tasks.

Conclusion: The SG2044 represents a notable step forward for RISC-V in the HPC domain, showcasing strong potential in addressing prior performance bottlenecks and improving competitiveness in compute-intensive scenarios.

Abstract: The pace of RISC-V adoption continues to grow rapidly, yet for the successes
enjoyed in areas such as embedded computing, RISC-V is yet to gain ubiquity in
High Performance Computing (HPC). The Sophon SG2044 is SOPHGO's next generation
64-core high performance CPU that has been designed for workstation and server
grade workloads. Building upon the SG2042, subsystems that were a bottleneck in
the previous generation have been upgraded.
  In this paper we undertake the first performance study of the SG2044 for HPC.
Comparing against the SG2042 and other architectures, we find that the SG2044
is most advantageous when running at higher core counts, delivering up to 4.91
greater performance than the SG2042 over 64-cores. Two of the most important
upgrades in the SG2044 are support for RVV v1.0 and an enhanced memory
subsystem. This results in the SG2044 significantly closing the performance gap
with other architectures, especially for compute-bound workloads.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [184] [BERT-VQA: Visual Question Answering on Plots](https://arxiv.org/abs/2508.13184)
*Tai Vu,Robert Yang*

Main category: cs.LG

TL;DR: This paper develops BERT-VQA, a model combining VisualBERT and ResNet-101 for visual question answering on plots, but disproves the hypothesis that the cross-modality module is essential.


<details>
  <summary>Details</summary>
Motivation: Visual question answering remains challenging as it requires integration between vision and language. The focus here is on answering questions related to plots.

Method: They designed and trained BERT-VQA utilizing VisualBERT-based architecture, pretrained ResNet-101 encoder, and evaluated with/without joint fusion. Comparison was conducted against a baseline of LSTM, CNN, and a shallow classifier.

Result: BERT-VQA showed that VisualBERT's cross-modality module wasn't essential for aligning plot components with question phrases.

Conclusion: This study revealed the difficulty of solving visual question answering on plots and provided insights regarding model architecture suitability.

Abstract: Visual question answering has been an exciting challenge in the field of
natural language understanding, as it requires deep learning models to exchange
information from both vision and language domains. In this project, we aim to
tackle a subtask of this problem, namely visual question answering on plots. To
achieve this, we developed BERT-VQA, a VisualBERT-based model architecture with
a pretrained ResNet 101 image encoder, along with a potential addition of joint
fusion. We trained and evaluated this model against a baseline that consisted
of a LSTM, a CNN, and a shallow classifier. The final outcome disproved our
core hypothesis that the cross-modality module in VisualBERT is essential in
aligning plot components with question phrases. Therefore, our work provided
valuable insights into the difficulty of the plot question answering challenge
as well as the appropriateness of different model architectures in solving this
problem.

</details>


### [185] [Contextual Attention-Based Multimodal Fusion of LLM and CNN for Sentiment Analysis](https://arxiv.org/abs/2508.13196)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.LG

TL;DR: The paper proposes a new method combining CNN-based image analysis with GPT-powered text processing for improved multimodal sentiment analysis in natural disasters.


<details>
  <summary>Details</summary>
Motivation: To address the critical need for accurate sentiment analysis on social media during natural disasters, aiding effective crisis management.

Method: A fusion approach integrating CNN for images, GPT and prompt engineering for text, and a contextual attention mechanism to model intermodal relationships.

Result: The model surpassed existing baselines, showing a 2.43% accuracy increase and 5.18% uplift in F1-score on the CrisisMMD dataset.

Conclusion: This method not only enhances sentiment analysis accuracy but also offers practical value for real-time disaster management by optimizing emergency response strategies.

Abstract: This paper introduces a novel approach for multimodal sentiment analysis on
social media, particularly in the context of natural disasters, where
understanding public sentiment is crucial for effective crisis management.
Unlike conventional methods that process text and image modalities separately,
our approach seamlessly integrates Convolutional Neural Network (CNN) based
image analysis with Large Language Model (LLM) based text processing,
leveraging Generative Pre-trained Transformer (GPT) and prompt engineering to
extract sentiment relevant features from the CrisisMMD dataset. To effectively
model intermodal relationships, we introduce a contextual attention mechanism
within the fusion process. Leveraging contextual-attention layers, this
mechanism effectively captures intermodality interactions, enhancing the
model's comprehension of complex relationships between textual and visual data.
The deep neural network architecture of our model learns from these fused
features, leading to improved accuracy compared to existing baselines.
Experimental results demonstrate significant advancements in classifying social
media data into informative and noninformative categories across various
natural disasters. Our model achieves a notable 2.43% increase in accuracy and
5.18% in F1-score, highlighting its efficacy in processing complex multimodal
data. Beyond quantitative metrics, our approach provides deeper insight into
the sentiments expressed during crises. The practical implications extend to
real time disaster management, where enhanced sentiment analysis can optimize
the accuracy of emergency interventions. By bridging the gap between multimodal
analysis, LLM powered text understanding, and disaster response, our work
presents a promising direction for Artificial Intelligence (AI) driven crisis
management solutions. Keywords:

</details>


### [186] [Strategies for training point distributions in physics-informed neural networks](https://arxiv.org/abs/2508.13216)
*Santosh Humagain,Toni Schneidereit*

Main category: cs.LG

TL;DR: The paper investigates the impact of training point distribution in the performance of physics-informed neural networks (PINNs) for approximating differential equations.


<details>
  <summary>Details</summary>
Motivation: Physics-informed neural networks are promising for solving differential equations in a mesh-free manner, but performance optimization is critical.

Method: The study tests various training point distributions on both ordinary and partial differential equations, using shallow network architectures and sine-based training points inspired by Chebyshev nodes.

Result: Findings show that training point distributions significantly affect solution accuracy, highlighting relationships to the differential equation's characteristics.

Conclusion: Evidence reveals a strong link between training point distribution strategy and differential equation characteristics, influencing PINNs' effectiveness.

Abstract: Physics-informed neural networks approach the approximation of differential
equations by directly incorporating their structure and given conditions in a
loss function. This enables conditions like, e.g., invariants to be easily
added during the modelling phase. In addition, the approach can be considered
as mesh free and can be utilised to compute solutions on arbitrary grids after
the training phase. Therefore, physics-informed neural networks are emerging as
a promising alternative to solving differential equations with methods from
numerical mathematics. However, their performance highly depends on a large
variety of factors. In this paper, we systematically investigate and evaluate a
core component of the approach, namely the training point distribution. We test
two ordinary and two partial differential equations with five strategies for
training data generation and shallow network architectures, with one and two
hidden layers. In addition to common distributions, we introduce sine-based
training points, which are motivated by the construction of Chebyshev nodes.
The results are challenged by using certain parameter combinations like, e.g.,
random and fixed-seed weight initialisation for reproducibility. The results
show the impact of the training point distributions on the solution accuracy
and we find evidence that they are connected to the characteristics of the
differential equation.

</details>


### [187] [Deep Graph Neural Point Process For Learning Temporal Interactive Networks](https://arxiv.org/abs/2508.13219)
*Su Chen,Xiaohua Qi,Xixun Lin,Yanmin Shang,Xiaolin Xu,Yangxi Li*

Main category: cs.LG

TL;DR: The paper introduces the Deep Graph Neural Point Process (DGNPP) model for learning temporal interaction networks, effectively combining static and dynamic embeddings to enhance event and occurrence time prediction.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook the influence of network topology while predicting temporal interactions. The paper aims to address this gap.

Method: The proposed DGNPP model utilizes a Node Aggregation Layer for capturing topological structures, and a Self Attentive Layer to dynamically update embeddings over time. These embeddings are fed into an event intensity function and optimized using maximum likelihood estimation.

Result: DGNPP shows superior performance in event prediction and time prediction tasks, outperforming baseline models with high efficiency across three public datasets.

Conclusion: Incorporating both static and dynamic embeddings, DGNPP effectively mitigates the limitations of prior methods, offering improved results in temporal interaction network learning.

Abstract: Learning temporal interaction networks(TIN) is previously regarded as a
coarse-grained multi-sequence prediction problem, ignoring the network topology
structure influence. This paper addresses this limitation and a Deep Graph
Neural Point Process(DGNPP) model for TIN is proposed. DGNPP consists of two
key modules: the Node Aggregation Layer and the Self Attentive Layer. The Node
Aggregation Layer captures topological structures to generate static
representation for users and items, while the Self Attentive Layer dynamically
updates embeddings over time. By incorporating both dynamic and static
embeddings into the event intensity function and optimizing the model via
maximum likelihood estimation, DGNPP predicts events and occurrence time
effectively. Experimental evaluations on three public datasets demonstrate that
DGNPP achieves superior performance in event prediction and time prediction
tasks with high efficiency, significantly outperforming baseline models and
effectively mitigating the limitations of prior approaches.

</details>


### [188] [Dynamic Design of Machine Learning Pipelines via Metalearning](https://arxiv.org/abs/2508.13436)
*Edesio Alcobaça,André C. P. L. F. de Carvalho*

Main category: cs.LG

TL;DR: This paper introduces a metalearning approach to optimize search spaces in AutoML systems, significantly reducing runtime and computational costs without impacting predictive performance.


<details>
  <summary>Details</summary>
Motivation: AutoML systems face challenges of high computational costs and large search spaces leading to potential overfitting, requiring optimized search strategies.

Method: The study employs a metalearning method that leverages historical metaknowledge to dynamically prioritize promising search space regions, facilitating quicker optimization.

Result: Experiments show an 89% reduction in Random Search runtime, as well as notable reductions in preprocessing and classifier search spaces, with competitive results in Auto-Sklearn adaptation.

Conclusion: The proposed method streamlines AutoML systems, balancing efficiency and performance, while also providing insights into meta-feature selection and trade-offs in search space reduction.

Abstract: Automated machine learning (AutoML) has democratized the design of machine
learning based systems, by automating model selection, hyperparameter tuning
and feature engineering. However, the high computational cost associated with
traditional search and optimization strategies, such as Random Search, Particle
Swarm Optimization and Bayesian Optimization, remains a significant challenge.
Moreover, AutoML systems typically explore a large search space, which can lead
to overfitting. This paper introduces a metalearning method for dynamically
designing search spaces for AutoML system. The proposed method uses historical
metaknowledge to select promising regions of the search space, accelerating the
optimization process. According to experiments conducted for this study, the
proposed method can reduce runtime by 89\% in Random Search and search space by
(1.8/13 preprocessor and 4.3/16 classifier), without compromising significant
predictive performance. Moreover, the proposed method showed competitive
performance when adapted to Auto-Sklearn, reducing its search space.
Furthermore, this study encompasses insights into meta-feature selection,
meta-model explainability, and the trade-offs inherent in search space
reduction strategies.

</details>


### [189] [A Recurrent Neural Network based Clustering Method for Binary Data Sets in Education](https://arxiv.org/abs/2508.13224)
*Mizuki Ohira,Toshimichi Saito*

Main category: cs.LG

TL;DR: This paper explores using a recurrent neural network for clustering large S-P charts in education, providing an innovative method to simplify chart handling and improve classification.


<details>
  <summary>Details</summary>
Motivation: The increasing size of S-P charts makes it challenging to manage and analyze student data effectively, motivating the development of new methods to classify the data into smaller, manageable clusters.

Method: The proposed method employs a recurrent neural network with multiple fixed points. Using the dynamics of the network, basins of attraction are utilized to form clusters that correspond to smaller S-P charts.

Result: The paper introduces the average caution index as a key feature for evaluating clustering effectiveness and demonstrates through experiments that the method delivers promising clustering performance.

Conclusion: The study confirms the validity and efficiency of the recurrent neural network approach in clustering S-P charts, offering a scalable solution for handling large educational datasets.

Abstract: This paper studies an application of a recurrent neural network to clustering
method for the S-P chart: a binary data set used widely in education. As the
number of students increases, the S-P chart becomes hard to handle. In order to
classify the large chart into smaller charts, we present a simple clustering
method based on the network dynamics. In the method, the network has multiple
fixed points and basins of attraction give clusters corresponding to small S-P
charts. In order to evaluate the clustering performance, we present an
important feature quantity: average caution index that characterizes
singularity of students answer oatterns. Performing fundamental experiments,
effectiveness of the method is confirmed.

</details>


### [190] [RISE: Enhancing VLM Image Annotation with Self-Supervised Reasoning](https://arxiv.org/abs/2508.13229)
*Suhang Hu,Wei Hu,Yuhang Su,Fan Zhang*

Main category: cs.LG

TL;DR: The paper introduces RISE, a novel two-stage framework enhancing Vision-Language Models' reasoning and annotation capabilities through reinforcement learning-driven CoTs and fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language Models (VLMs) struggle with complex image annotation tasks due to insufficient reasoning and explainability in their methods.

Method: RISE consists of two stages: RISE-CoT uses reinforcement learning for closed-loop reasoning to generate verified Chains of Thought (CoTs), and RISE-R1 uses these CoTs for supervised and reinforcement fine-tuning.

Result: RISE-trained Qwen2-VL-2B surpasses standard supervised fine-tuning and Visual-RFT methods in annotation precision and reasoning explainability on both simple and complex tasks.

Conclusion: RISE represents a self-supervised solution to improve VLMs' reasoning without requiring manually annotated CoTs, advancing accuracy and interpretability in visual tasks.

Abstract: Vision-Language Models (VLMs) struggle with complex image annotation tasks,
such as emotion classification and context-driven object detection, which
demand sophisticated reasoning. Standard Supervised Fine-Tuning (SFT) focuses
solely on annotation outcomes, ignoring underlying rationales, while Visual
Reinforcement Fine-Tuning (Visual-RFT) produces inconsistent Chains of Thought
(CoTs) due to the absence of high-quality, verified CoTs during pre-training.
We introduce RISE (Reason-Inspire-Strengthen-Expertise), a two-stage framework
to overcome these limitations. In the Reason stage (RISE-CoT), a reinforcement
learning-driven "annotation-reasoning-annotation" closed-loop generates
visually grounded, logically consistent CoTs by verifying their ability to
reconstruct original annotations without direct leakage. The Inspire and
Strengthen stage (RISE-R1) leverages a high-quality CoT subset, filtered by
RISE-CoT rewards, for supervised fine-tuning, followed by reinforcement
fine-tuning to produce interpretable reasoning and accurate annotations,
achieving Expertise in complex visual tasks. Evaluated on complex and simple
image annotation tasks, RISE-trained Qwen2-VL-2B outperforms SFT and
Visual-RFT, achieving robust performance and enhanced explainability. RISE
offers a self-supervised solution for advancing VLM reasoning without requiring
manually annotated CoTs.

</details>


### [191] [Data driven feedback linearization of nonlinear control systems via Lie derivatives and stacked regression approach](https://arxiv.org/abs/2508.13241)
*Lakshmi Priya P. K.,Andreas Schwung*

Main category: cs.LG

TL;DR: Proposes a novel methodology combining sparse regression and Lie derivatives for identifying and feedback linearizing physical systems.


<details>
  <summary>Details</summary>
Motivation: Understanding and controlling the dynamics of physical systems, especially under nonlinear conditions, is a complex yet vital research area.

Method: Uses sparse regression to identify the system, then applies Lie derivatives to output functions for augmented constraint design, ensuring no internal dynamics.

Result: Integrates stacked regression algorithms with relative degree conditions to discover and feedback linearize physical models.

Conclusion: The methodology provides an innovative approach to system identification and feedback control, enhancing the precision of linearized physical models.

Abstract: Discovering the governing equations of a physical system and designing an
effective feedback controller remains one of the most challenging and intensive
areas of ongoing research. This task demands a deep understanding of the system
behavior, including the nonlinear factors that influence its dynamics. In this
article, we propose a novel methodology for identifying a feedback linearized
physical system based on known prior dynamic behavior. Initially, the system is
identified using a sparse regression algorithm, subsequently a feedback
controller is designed for the discovered system by applying Lie derivatives to
the dictionary of output functions to derive an augmented constraint which
guarantees that no internal dynamics are observed. Unlike the prior related
works, the novel aspect of this article combines the approach of stacked
regression algorithm and relative degree conditions to discover and feedback
linearize the true governing equations of a physical model.

</details>


### [192] [Physically Plausible Data Augmentations for Wearable IMU-based Human Activity Recognition Using Physics Simulation](https://arxiv.org/abs/2508.13284)
*Nobuyuki Oishi,Philip Birch,Daniel Roggen,Paula Lago*

Main category: cs.LG

TL;DR: The paper proposes Physically Plausible Data Augmentation (PPDA) using physics simulation to improve Human Activity Recognition (HAR) with limited labeled data, showing consistent performance improvements over traditional augmentation methods.


<details>
  <summary>Details</summary>
Motivation: To overcome the scarcity of high-quality labeled data in sensor-based HAR, which limits model performance and domain generalization, and to provide a more realistic data augmentation method compared to existing Signal Transformation-based Data Augmentation (STDA) techniques.

Method: The study introduces PPDA, which uses physics simulations applied to human body movement data (from motion capture or pose estimation) to realistically simulate variabilities, including changes in body movements, sensor placements, and hardware effects. These augmented datasets are then compared with traditional STDAs across three public datasets.

Result: PPDA consistently outperforms traditional STDA methods, improving macro F1 scores by an average of 3.7 pp (up to 13 pp) and achieving competitive model performance with up to 60% fewer training subjects.

Conclusion: The paper emphasizes the benefits of using physics-based, physically plausible data augmentation in sensor-based HAR to address the challenge of annotation scarcity. It shows that PPDA is scalable, cost-effective, and capable of improving dataset diversity while maintaining label integrity.

Abstract: The scarcity of high-quality labeled data in sensor-based Human Activity
Recognition (HAR) hinders model performance and limits generalization across
real-world scenarios. Data augmentation is a key strategy to mitigate this
issue by enhancing the diversity of training datasets. Signal
Transformation-based Data Augmentation (STDA) techniques have been widely used
in HAR. However, these methods are often physically implausible, potentially
resulting in augmented data that fails to preserve the original meaning of the
activity labels. In this study, we introduce and systematically characterize
Physically Plausible Data Augmentation (PPDA) enabled by physics simulation.
PPDA leverages human body movement data from motion capture or video-based pose
estimation and incorporates various realistic variabilities through physics
simulation, including modifying body movements, sensor placements, and
hardware-related effects. We compare the performance of PPDAs with traditional
STDAs on three public datasets of daily activities and fitness workouts. First,
we evaluate each augmentation method individually, directly comparing PPDAs to
their STDA counterparts. Next, we assess how combining multiple PPDAs can
reduce the need for initial data collection by varying the number of subjects
used for training. Experiments show consistent benefits of PPDAs, improving
macro F1 scores by an average of 3.7 pp (up to 13 pp) and achieving competitive
performance with up to 60% fewer training subjects than STDAs. As the first
systematic study of PPDA in sensor-based HAR, these results highlight the
advantages of pursuing physical plausibility in data augmentation and the
potential of physics simulation for generating synthetic Inertial Measurement
Unit data for training deep learning HAR models. This cost-effective and
scalable approach therefore helps address the annotation scarcity challenge in
HAR.

</details>


### [193] [Towards Human-AI Complementarity in Matching Tasks](https://arxiv.org/abs/2508.13285)
*Adrian Arnaiz-Rodriguez,Nina Corvelo Benz,Suhas Thejaswi,Nuria Oliver,Manuel Gomez-Rodriguez*

Main category: cs.LG

TL;DR: The paper introduces "comatch," a collaborative data-driven matching system designed to enhance human-AI complementarity by balancing algorithmic decisions with human judgment. It optimizes matching decisions to maximize performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the gap in existing algorithmic matching systems, as they do not achieve effective human-AI complementarity, resulting in suboptimal decision-making in high-stakes domains.

Method: The proposed system, comatch, selectively makes decisions it is confident in and defers the rest to human decision-makers. It optimizes this balance to provably enhance overall matching performance. The approach was validated through a large-scale human subject study with 800 participants.

Result: Results indicate that comatch outperforms both standalone human decision-making and algorithmic matching, proving the system’s effectiveness in achieving better matching outcomes.

Conclusion: The findings demonstrate the potential of collaborative systems in leveraging human-AI complementarity for improved performance in decision-making tasks. The work contributes open-source data and implementation for further research and application.

Abstract: Data-driven algorithmic matching systems promise to help human decision
makers make better matching decisions in a wide variety of high-stakes
application domains, such as healthcare and social service provision. However,
existing systems are not designed to achieve human-AI complementarity:
decisions made by a human using an algorithmic matching system are not
necessarily better than those made by the human or by the algorithm alone. Our
work aims to address this gap. To this end, we propose collaborative matching
(comatch), a data-driven algorithmic matching system that takes a collaborative
approach: rather than making all the matching decisions for a matching task
like existing systems, it selects only the decisions that it is the most
confident in, deferring the rest to the human decision maker. In the process,
comatch optimizes how many decisions it makes and how many it defers to the
human decision maker to provably maximize performance. We conduct a large-scale
human subject study with $800$ participants to validate the proposed approach.
The results demonstrate that the matching outcomes produced by comatch
outperform those generated by either human participants or by algorithmic
matching on their own. The data gathered in our human subject study and an
implementation of our system are available as open source at
https://github.com/Networks-Learning/human-AI-complementarity-matching.

</details>


### [194] [Hierarchical Conformal Classification](https://arxiv.org/abs/2508.13288)
*Floris den Hengst,Inès Blin,Majid Mohammadi,Syed Ihtesham Hussain Shah,Taraneh Younesian*

Main category: cs.LG

TL;DR: The paper introduces a method called Hierarchical Conformal Classification (HCC) that integrates class hierarchies into conformal prediction for classification, ensuring structured prediction sets with coverage guarantees.


<details>
  <summary>Details</summary>
Motivation: Standard Conformal Prediction ignores semantic or hierarchical relationships among class labels, making predictions less insightful in domains where such structure exists.

Method: The method frames hierarchical conformal classification (HCC) as a constrained optimization problem, providing solutions that yield structured prediction sets while maintaining coverage guarantees. The optimization process is simplified by identifying a smaller subset of candidate solutions.

Result: Through empirical evaluations on datasets (audio, image, and text) and a user study, the approach shows advantages in terms of structured predictions and annotator preference over traditional methods.

Conclusion: Hierarchical Conformal Classification outperforms flat conformal prediction by incorporating hierarchy, producing more informative and preferred prediction sets, while maintaining guarantees.

Abstract: Conformal prediction (CP) is a powerful framework for quantifying uncertainty
in machine learning models, offering reliable predictions with finite-sample
coverage guarantees. When applied to classification, CP produces a prediction
set of possible labels that is guaranteed to contain the true label with high
probability, regardless of the underlying classifier. However, standard CP
treats classes as flat and unstructured, ignoring domain knowledge such as
semantic relationships or hierarchical structure among class labels. This paper
presents hierarchical conformal classification (HCC), an extension of CP that
incorporates class hierarchies into both the structure and semantics of
prediction sets. We formulate HCC as a constrained optimization problem whose
solutions yield prediction sets composed of nodes at different levels of the
hierarchy, while maintaining coverage guarantees. To address the combinatorial
nature of the problem, we formally show that a much smaller, well-structured
subset of candidate solutions suffices to ensure coverage while upholding
optimality. An empirical evaluation on three new benchmarks consisting of
audio, image, and text data highlights the advantages of our approach, and a
user study shows that annotators significantly prefer hierarchical over flat
prediction sets.

</details>


### [195] [Efficient Constraint-Aware Flow Matching via Randomized Exploration](https://arxiv.org/abs/2508.13316)
*Zhengyan Huan,Jacob Boerma,Li-Ping Liu,Shuchin Aeron*

Main category: cs.LG

TL;DR: The paper addresses the generation of samples using Flow Matching (FM) while satisfying constraints. Two scenarios are considered: differentiable distance functions and membership oracles. They propose novel methods for both settings, showcasing significant improvements in constraint satisfaction during sample generation.


<details>
  <summary>Details</summary>
Motivation: To solve the challenge of generating constrained samples effectively using Flow Matching, particularly when constraints are either differentiable or only accessible via a membership oracle.

Method: The paper adapts FM objectives with penalty terms for distance constraints in one scenario and utilizes randomization with mean flow learning for another. Additionally, a novel two-stage approach is introduced to improve computational efficiency.

Result: The proposed methods demonstrate significant improvements in generating samples that both satisfy constraints and match target distributions across synthetic cases. They also showcase practical application in generating adversarial examples using a black-box classifier.

Conclusion: The approaches achieve advancements in constrained sample generation with Flow Matching, outline future research avenues, and provide accessible code for community exploration.

Abstract: We consider the problem of generating samples via Flow Matching (FM) with an
additional requirement that the generated samples must satisfy given
constraints. We consider two scenarios, viz.: (a) when a differentiable
distance function to the constraint set is given, and (b) when the constraint
set is only available via queries to a membership oracle. For case (a), we
propose a simple adaptation of the FM objective with an additional term that
penalizes the distance between the constraint set and the generated samples.
For case (b), we propose to employ randomization and learn a mean flow that is
numerically shown to have a high likelihood of satisfying the constraints. This
approach deviates significantly from existing works that require simple convex
constraints, knowledge of a barrier function, or a reflection mechanism to
constrain the probability flow. Furthermore, in the proposed setting we show
that a two-stage approach, where both stages approximate the same original flow
but with only the second stage probing the constraints via randomization, is
more computationally efficient. Through several synthetic cases of constrained
generation, we numerically show that the proposed approaches achieve
significant gains in terms of constraint satisfaction while matching the target
distributions. As a showcase for a practical oracle-based constraint, we show
how our approach can be used for training an adversarial example generator,
using queries to a hard-label black-box classifier. We conclude with several
future research directions. Our code is available at
https://github.com/ZhengyanHuan/FM-RE.

</details>


### [196] [Minimizing the Weighted Number of Tardy Jobs: Data-Driven Heuristic for Single-Machine Scheduling](https://arxiv.org/abs/2508.13703)
*Nikolai Antonov,Prěmysl Šůcha,Mikoláš Janota,Jan Hůla*

Main category: cs.LG

TL;DR: The paper presents a data-driven heuristic for single-machine scheduling, combining machine learning and problem-specific traits to improve scalability and optimality over traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of exact algorithms in certain regions of the single-machine scheduling problem space by utilizing data-driven approaches tailored to specific datasets.

Method: A novel scheduling heuristic is developed, combining machine learning with problem-specific characteristics to ensure feasible solutions.

Result: The approach outperforms state-of-the-art methods in optimality gap, number of optimal solutions, and adaptability across various data scenarios.

Conclusion: The proposed method is flexible for practical applications and includes a detailed ML model selection process to optimize outcomes.

Abstract: Existing research on single-machine scheduling is largely focused on exact
algorithms, which perform well on typical instances but can significantly
deteriorate on certain regions of the problem space. In contrast, data-driven
approaches provide strong and scalable performance when tailored to the
structure of specific datasets. Leveraging this idea, we focus on a
single-machine scheduling problem where each job is defined by its weight,
duration, due date, and deadline, aiming to minimize the total weight of tardy
jobs. We introduce a novel data-driven scheduling heuristic that combines
machine learning with problem-specific characteristics, ensuring feasible
solutions, which is a common challenge for ML-based algorithms. Experimental
results demonstrate that our approach significantly outperforms the
state-of-the-art in terms of optimality gap, number of optimal solutions, and
adaptability across varied data scenarios, highlighting its flexibility for
practical applications. In addition, we conduct a systematic exploration of ML
models, addressing a common gap in similar studies by offering a detailed model
selection process and providing insights into why the chosen model is the best
fit.

</details>


### [197] [X-MoE: Enabling Scalable Training for Emerging Mixture-of-Experts Architectures on HPC Platforms](https://arxiv.org/abs/2508.13337)
*Yueming Yuan,Ahan Gupta,Jianping Li,Sajal Dash,Feiyi Wang,Minjia Zhang*

Main category: cs.LG

TL;DR: X-MoE introduces a scalable MoE training system optimized for AMD GPUs, achieving breakthroughs in model size and training efficiency compared to existing systems.


<details>
  <summary>Details</summary>
Motivation: Existing MoE architectures like DeepSeek-MoE face challenges such as high memory overhead, costly communication, and suboptimal performance on non-NVIDIA GPUs.

Method: X-MoE implements efficient padding-free MoE training, redundancy-bypassing dispatch, and hybrid parallelism with sequence-sharded blocks optimized for cross-platform compatibility.

Result: X-MoE successfully scales MoE training to 545 billion parameters on 1024 GPUs, a 10x improvement in model size over current methods using the Frontier supercomputer.

Conclusion: X-MoE enables next-generation scalability for Mixture-of-Experts architectures while fully utilizing AMD GPU hardware, broadening accessibility and efficiency in large-scale AI model training.

Abstract: Emerging expert-specialized Mixture-of-Experts (MoE) architectures, such as
DeepSeek-MoE, deliver strong model quality through fine-grained expert
segmentation and large top-k routing. However, their scalability is limited by
substantial activation memory overhead and costly all-to-all communication.
Furthermore, current MoE training systems - primarily optimized for NVIDIA GPUs
- perform suboptimally on non-NVIDIA platforms, leaving significant
computational potential untapped. In this work, we present X-MoE, a novel MoE
training system designed to deliver scalable training performance for
next-generation MoE architectures. X-MoE achieves this via several novel
techniques, including efficient padding-free MoE training with cross-platform
kernels, redundancy-bypassing dispatch, and hybrid parallelism with
sequence-sharded MoE blocks. Our evaluation on the Frontier supercomputer,
powered by AMD MI250X GPUs, shows that X-MoE scales DeepSeek-style MoEs up to
545 billion parameters across 1024 GPUs - 10x larger than the largest trainable
model with existing methods under the same hardware budget, while maintaining
high training throughput. The source code of X-MoE is available at
https://github.com/Supercomputing-System-AI-Lab/X-MoE.

</details>


### [198] [Decoding Communications with Partial Information](https://arxiv.org/abs/2508.13326)
*Dylan Cope,Peter McBurney*

Main category: cs.LG

TL;DR: The paper examines language acquisition as imitation learning and introduces challenges of partial observability, presenting a learning algorithm to decode private information.


<details>
  <summary>Details</summary>
Motivation: To address the often-overlooked challenge of partial observability in machine language acquisition, moving beyond the assumption that all relevant information is viewable by the learner.

Method: The study introduces a learning-based algorithm that decodes private information by leveraging knowledge of the environment, actions, and messages sent.

Result: The approach is demonstrated with toy examples and extends to identify challenges in more general language learning scenarios.

Conclusion: Partial observability requires unique solutions for effective language acquisition, and the proposed algorithm provides a promising step in that direction.

Abstract: Machine language acquisition is often presented as a problem of imitation
learning: there exists a community of language users from which a learner
observes speech acts and attempts to decode the mappings between utterances and
situations. However, an interesting consideration that is typically unaddressed
is partial observability, i.e. the learner is assumed to see all relevant
information. This paper explores relaxing this assumption, thereby posing a
more challenging setting where such information needs to be inferred from
knowledge of the environment, the actions taken, and messages sent. We see
several motivating examples of this problem, demonstrate how they can be solved
in a toy setting, and formally explore challenges that arise in more general
settings. A learning-based algorithm is then presented to perform the decoding
of private information to facilitate language acquisition.

</details>


### [199] [Disentangled Deep Smoothed Bootstrap for Fair Imbalanced Regression](https://arxiv.org/abs/2508.13829)
*Samuel Stocksieker,Denys pommeret,Arthur Charpentier*

Main category: cs.LG

TL;DR: This paper tackles imbalanced regression problems in tabular data using a novel method combining Variational Autoencoders (VAEs) and Smoothed Bootstrap.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limited focus on regression problems within existing methods for imbalanced data distribution learning, aiming to improve predictive modeling performance in this area.

Method: The authors propose a data generation approach by integrating a disentangled Variational Autoencoder (VAE) with Smoothed Bootstrap applied in the latent space.

Result: Numerical comparisons on benchmark datasets demonstrate the efficiency of the proposed method against competing techniques.

Conclusion: The research offers a promising solution for imbalanced regression challenges, enhancing learning performance in predictive modeling using tabular data.

Abstract: Imbalanced distribution learning is a common and significant challenge in
predictive modeling, often reducing the performance of standard algorithms.
Although various approaches address this issue, most are tailored to
classification problems, with a limited focus on regression. This paper
introduces a novel method to improve learning on tabular data within the
Imbalanced Regression (IR) framework, which is a critical problem. We propose
using Variational Autoencoders (VAEs) to model and define a latent
representation of data distributions. However, VAEs can be inefficient with
imbalanced data like other standard approaches. To address this, we develop an
innovative data generation method that combines a disentangled VAE with a
Smoothed Bootstrap applied in the latent space. We evaluate the efficiency of
this method through numerical comparisons with competitors on benchmark
datasets for IR.

</details>


### [200] [A Dual-Attention Graph Network for fMRI Data Classification](https://arxiv.org/abs/2508.13328)
*Amirali Arbab,Zeinab Davarani,Mehran Safayani*

Main category: cs.LG

TL;DR: This paper proposes a novel method using dynamic graph creation and spatiotemporal attention mechanisms for Autism Spectrum Disorder diagnosis via fMRI data.


<details>
  <summary>Details</summary>
Motivation: Current fMRI classification struggles to capture comprehensive spatio-temporal relationships or relies on static functional connectivity, which limits deeper insights into neural activity dynamics.

Method: The approach uses transformer-based attention mechanisms to infer functional brain connectivity dynamically, constructing time-varying graphs processed with Graph Convolutional Networks (GCNs) and transformers, capturing both localized and global dependencies.

Result: The model achieves 63.2% accuracy and 60.0 AUC on the ABIDE dataset, outperforming static graph methods like GCN at 51.8%.

Conclusion: Joint modeling of dynamic connectivity and spatio-temporal context enhances fMRI classification for ASD diagnosis, showcasing the benefit of attention-driven dynamic graph creation and hierarchical fusion.

Abstract: Understanding the complex neural activity dynamics is crucial for the
development of the field of neuroscience. Although current functional MRI
classification approaches tend to be based on static functional connectivity or
cannot capture spatio-temporal relationships comprehensively, we present a new
framework that leverages dynamic graph creation and spatiotemporal attention
mechanisms for Autism Spectrum Disorder(ASD) diagnosis. The approach used in
this research dynamically infers functional brain connectivity in each time
interval using transformer-based attention mechanisms, enabling the model to
selectively focus on crucial brain regions and time segments. By constructing
time-varying graphs that are then processed with Graph Convolutional Networks
(GCNs) and transformers, our method successfully captures both localized
interactions and global temporal dependencies. Evaluated on the subset of ABIDE
dataset, our model achieves 63.2 accuracy and 60.0 AUC, outperforming static
graph-based approaches (e.g., GCN:51.8). This validates the efficacy of joint
modeling of dynamic connectivity and spatio-temporal context for fMRI
classification. The core novelty arises from (1) attention-driven dynamic graph
creation that learns temporal brain region interactions and (2) hierarchical
spatio-temporal feature fusion through GCNtransformer fusion.

</details>


### [201] [Trans-XFed: An Explainable Federated Learning for Supply Chain Credit Assessment](https://arxiv.org/abs/2508.13715)
*Jie Shi,Arno P. J. M. Siebes,Siamak Mehrkanoon*

Main category: cs.LG

TL;DR: The paper presents Trans-XFed, a federated learning and explainable AI model for supply chain credit assessment, leveraging client selection and encryption techniques to address key challenges like privacy and class imbalance.


<details>
  <summary>Details</summary>
Motivation: To improve supply chain credit assessment by addressing challenges such as privacy, class imbalance, Non-IID data, and the need for interpretable models.

Method: The model uses a federated learning approach incorporating FedProx architecture with homomorphic encryption, transformers for feature insights, and integrated gradients for explainability.

Result: Experimental analysis on real-world datasets shows Trans-XFed delivers accurate credit assessments, faster convergence, and preserves privacy compared to baselines.

Conclusion: Trans-XFed combines advanced learning techniques and interpretable methods to overcome challenges in supply chain credit assessments effectively.

Abstract: This paper proposes a Trans-XFed architecture that combines federated
learning with explainable AI techniques for supply chain credit assessment. The
proposed model aims to address several key challenges, including privacy,
information silos, class imbalance, non-identically and independently
distributed (Non-IID) data, and model interpretability in supply chain credit
assessment. We introduce a performance-based client selection strategy (PBCS)
to tackle class imbalance and Non-IID problems. This strategy achieves faster
convergence by selecting clients with higher local F1 scores. The FedProx
architecture, enhanced with homomorphic encryption, is used as the core model,
and further incorporates a transformer encoder. The transformer encoder block
provides insights into the learned features. Additionally, we employ the
integrated gradient explainable AI technique to offer insights into
decision-making. We demonstrate the effectiveness of Trans-XFed through
experimental evaluations on real-world supply chain datasets. The obtained
results show its ability to deliver accurate credit assessments compared to
several baselines, while maintaining transparency and privacy.

</details>


### [202] [Multi-User Contextual Cascading Bandits for Personalized Recommendation](https://arxiv.org/abs/2508.13981)
*Jiho Park,Huiwen Jia*

Main category: cs.LG

TL;DR: The paper introduces the Multi-User Contextual Cascading Bandit (MCCB) model and proposes two algorithms (UCBBP and AUCBBP) for better handling online advertising scenarios with multiple users, achieving significant theoretical and empirical results.


<details>
  <summary>Details</summary>
Motivation: There is a need to better handle online advertising scenarios involving multiple simultaneous users interacting with sequentially displayed items, capturing realistic ad interactions and optimizing exploration and reward functions.

Method: The researchers developed two algorithms: 1) Upper Confidence Bound with Backward Planning (UCBBP), and 2) Active Upper Confidence Bound with Backward Planning (AUCBBP), which improve regret bounds and scalability across user contexts.

Result: UCBBP achieved a regret bound of $
widetilde{O}(\sqrt{THN})$, while AUCBBP showed a more efficient context scaling with a regret bound of $
widetilde{O}(\sqrt{T+HN})$. Numerical experiments confirmed the effectiveness of both approaches.

Conclusion: The proposed MCCB framework and its algorithms effectively address the multi-user contextual cascading scenario in online advertising systems, providing improved regret bounds and scalability, backed by theoretical and empirical evidence.

Abstract: We introduce a Multi-User Contextual Cascading Bandit model, a new
combinatorial bandit framework that captures realistic online advertising
scenarios where multiple users interact with sequentially displayed items
simultaneously. Unlike classical contextual bandits, MCCB integrates three key
structural elements: (i) cascading feedback based on sequential arm exposure,
(ii) parallel context sessions enabling selective exploration, and (iii)
heterogeneous arm-level rewards. We first propose Upper Confidence Bound with
Backward Planning (UCBBP), a UCB-style algorithm tailored to this setting, and
prove that it achieves a regret bound of $\widetilde{O}(\sqrt{THN})$ over $T$
episodes, $H$ session steps, and $N$ contexts per episode. Motivated by the
fact that many users interact with the system simultaneously, we introduce a
second algorithm, termed Active Upper Confidence Bound with Backward Planning
(AUCBBP), which shows a strict efficiency improvement in context scaling, i.e.,
user scaling, with a regret bound of $\widetilde{O}(\sqrt{T+HN})$. We validate
our theoretical findings via numerical experiments, demonstrating the empirical
effectiveness of both algorithms under various settings.

</details>


### [203] [Dimension lower bounds for linear approaches to function approximation](https://arxiv.org/abs/2508.13346)
*Daniel Hsu*

Main category: cs.LG

TL;DR: A study using linear algebra to prove dimension lower bounds for linear methods addressing L² function approximation problems, with applications to kernel methods.


<details>
  <summary>Details</summary>
Motivation: To establish dimension and sample size lower bounds for linear methods in solving L² function approximation challenges, and explore applications in kernel methods.

Method: Employs a linear algebraic argument previously used in the context of Kolmogorov n-widths, specifically targeting function approximation and kernel methods.

Result: Demonstrates and validates the dimension and sample size lower bounds for the discussed methods.

Conclusion: The presented linear algebraic approach effectively generalizes and extends previous work, particularly for kernel methods, providing key insights into lower bounds in function approximation problems.

Abstract: This short note presents a linear algebraic approach to proving dimension
lower bounds for linear methods that solve $L^2$ function approximation
problems. The basic argument has appeared in the literature before (e.g.,
Barron, 1993) for establishing lower bounds on Kolmogorov $n$-widths. The
argument is applied to give sample size lower bounds for kernel methods.

</details>


### [204] [Counterfactual Probabilistic Diffusion with Expert Models](https://arxiv.org/abs/2508.13355)
*Wenhao Mu,Zhi Cao,Mehmed Uludag,Alexander Rodríguez*

Main category: cs.LG

TL;DR: The paper introduces ODE-Diff, a time series diffusion-based framework that combines expert-guided signals and generative modeling for improved causal inference in counterfactual prediction.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of current methods in predicting counterfactual distributions, particularly under data scarcity, for applications like public health and medicine.

Method: ODE-Diff integrates mechanistic expert signals as structured priors with data-driven approaches in a time series diffusion-based framework for causal inference.

Result: ODE-Diff demonstrates superior performance in point prediction and distributional accuracy compared to baselines in semi-synthetic, synthetic, and real-world datasets.

Conclusion: Integrating expert model guidance with generative frameworks through ODE-Diff enhances reliability and interpretability, enabling better decision-making in complex systems.

Abstract: Predicting counterfactual distributions in complex dynamical systems is
essential for scientific modeling and decision-making in domains such as public
health and medicine. However, existing methods often rely on point estimates or
purely data-driven models, which tend to falter under data scarcity. We propose
a time series diffusion-based framework that incorporates guidance from
imperfect expert models by extracting high-level signals to serve as structured
priors for generative modeling. Our method, ODE-Diff, bridges mechanistic and
data-driven approaches, enabling more reliable and interpretable causal
inference. We evaluate ODE-Diff across semi-synthetic COVID-19 simulations,
synthetic pharmacological dynamics, and real-world case studies, demonstrating
that it consistently outperforms strong baselines in both point prediction and
distributional accuracy.

</details>


### [205] [Adaptive Conformal Prediction Intervals Over Trajectory Ensembles](https://arxiv.org/abs/2508.13362)
*Ruipu Li,Daniel Menacho,Alexander Rodríguez*

Main category: cs.LG

TL;DR: The paper introduces a framework using conformal prediction to create calibrated prediction intervals for ensemble trajectory models used in applications like autonomous driving and epidemic modeling.


<details>
  <summary>Details</summary>
Motivation: To address the lack of calibration in probabilistic trajectory predictions which limits reliability in critical applications.

Method: The framework employs conformal prediction with online updates and an optimization step to calibrate prediction intervals, ensuring theoretical coverage guarantees.

Result: The method produces sharp, adaptive prediction intervals that can handle temporal dependencies and discontinuities effectively.

Conclusion: This approach enhances the reliability and adaptability of uncertainty estimates in sampled trajectory models, benefiting various domains dependent on them.

Abstract: Future trajectories play an important role across domains such as autonomous
driving, hurricane forecasting, and epidemic modeling, where practitioners
commonly generate ensemble paths by sampling probabilistic models or leveraging
multiple autoregressive predictors. While these trajectories reflect inherent
uncertainty, they are typically uncalibrated. We propose a unified framework
based on conformal prediction that transforms sampled trajectories into
calibrated prediction intervals with theoretical coverage guarantees. By
introducing a novel online update step and an optimization step that captures
inter-step dependencies, our method can produce discontinuous prediction
intervals around each trajectory, naturally capture temporal dependencies, and
yield sharper, more adaptive uncertainty estimates.

</details>


### [206] [Batching-Aware Joint Model Onloading and Offloading for Hierarchical Multi-Task Inference](https://arxiv.org/abs/2508.13380)
*Seohyeon Cha,Kevin Chan,Gustavo de Veciana,Haris Vikalo*

Main category: cs.LG

TL;DR: The paper proposes J3O, a framework to optimize the deployment and routing of multi-task models in edge-device hierarchies, maximizing inference accuracy under constraints.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient, concurrent execution of diverse tasks on resource-constrained edge devices, especially in multi-task real-world applications.

Method: The authors introduce a joint optimization framework (J3O) combining submodular optimization for model deployment and constrained linear programming for task routing, with extensions for batching under load variability.

Result: J3O achieves over 97% of optimal inference accuracy with less than 15% of the runtime of the optimal solver in multi-task settings.

Conclusion: The framework is efficient, scalable, and effective for managing multi-task collaborative inference under complex constraints.

Abstract: The growing demand for intelligent services on resource-constrained edge
devices has spurred the development of collaborative inference systems that
distribute workloads across end devices, edge servers, and the cloud. While
most existing frameworks focus on single-task, single-model scenarios, many
real-world applications (e.g., autonomous driving and augmented reality)
require concurrent execution of diverse tasks including detection,
segmentation, and depth estimation. In this work, we propose a unified
framework to jointly decide which multi-task models to deploy (onload) at
clients and edge servers, and how to route queries across the hierarchy
(offload) to maximize overall inference accuracy under memory, compute, and
communication constraints. We formulate this as a mixed-integer program and
introduce J3O (Joint Optimization of Onloading and Offloading), an alternating
algorithm that (i) greedily selects models to onload via Lagrangian-relaxed
submodular optimization and (ii) determines optimal offloading via constrained
linear programming. We further extend J3O to account for batching at the edge,
maintaining scalability under heterogeneous task loads. Experiments show J3O
consistently achieves over $97\%$ of the optimal accuracy while incurring less
than $15\%$ of the runtime required by the optimal solver across multi-task
benchmarks.

</details>


### [207] [Semi-Supervised Anomaly Detection Pipeline for SOZ Localization Using Ictal-Related Chirp](https://arxiv.org/abs/2508.13406)
*Nooshin Bahador,Milad Lankarany*

Main category: cs.LG

TL;DR: The paper introduces a method for locating seizure onset zones (SOZs) by combining outlier detection using chirp features and spatial correlation analysis, showing effectiveness particularly in patients with successful surgical outcomes.


<details>
  <summary>Details</summary>
Motivation: The motivation behind the study is to enhance the accuracy of seizure onset zone (SOZ) localization, which is critical for effective treatment, by leveraging advanced signal analysis techniques on neural data.

Method: The paper employs two steps: (1) Unsupervised Outlier Detection using Local Outlier Factor (LOF) analysis on chirp event features, and (2) Spatial Correlation Analysis to measure concordance with clinically defined SOZs using co-occurrence and weighted index metrics.

Result: The proposed method demonstrated high precision, recall, and F1 scores, particularly in seizure-free and successful surgical outcome patients. Weighted index matching outperformed exact matching for SOZ localization.

Conclusion: The chirp-based outlier detection and spatial weighting serve as an effective complementary approach for SOZ localization, particularly beneficial for patients with successful surgical treatment outcomes.

Abstract: This study presents a quantitative framework for evaluating the spatial
concordance between clinically defined seizure onset zones (SOZs) and
statistically anomalous channels identified through time-frequency analysis of
chirp events. The proposed pipeline employs a two-step methodology: (1)
Unsupervised Outlier Detection, where Local Outlier Factor (LOF) analysis with
adaptive neighborhood selection identifies anomalous channels based on
spectro-temporal features of chirp (Onset frequency, offset frequency, and
temporal duration); and (2) Spatial Correlation Analysis, which computes both
exact co-occurrence metrics and weighted index similarity, incorporating
hemispheric congruence and electrode proximity. Key findings demonstrate that
the LOF-based approach (N neighbors=20, contamination=0.2) effectively detects
outliers, with index matching (weighted by channel proximity) outperforming
exact matching in SOZ localization. Performance metrics (precision, recall, F1)
were highest for seizure-free patients (Index Precision mean: 0.903) and those
with successful surgical outcomes (Index Precision mean: 0.865), whereas
failure cases exhibited lower concordance (Index Precision mean: 0.460). The
key takeaway is that chirp-based outlier detection, combined with weighted
spatial metrics, provides a complementary method for SOZ localization,
particularly in patients with successful surgical outcomes.

</details>


### [208] [NovoMolGen: Rethinking Molecular Language Model Pretraining](https://arxiv.org/abs/2508.13408)
*Kamran Chitsaz,Roshan Balaji,Quentin Fournier,Nirav Pravinbhai Bhatt,Sarath Chandar*

Main category: cs.LG

TL;DR: NovoMolGen, a transformer-based model family pretrained on 1.5 billion molecules, significantly outperforms prior models for molecular generation.


<details>
  <summary>Details</summary>
Motivation: The vastness of chemical space (ranging from $10^{23}$ to $10^{60}$ molecules) demands scalable and efficient methods for designing molecules with desired properties. Current Molecular Large Language Models (Mol-LLMs) based on string representations are promising but underexplored in terms of key influencing modeling practices.

Method: The study developed NovoMolGen, transformer-based foundation models pretrained on 1.5 billion molecules. A systematic investigation of factors like textual representations, tokenization strategies, and dataset scaling was conducted along with extensive empirical evaluations.

Result: NovoMolGen established state-of-the-art performance in both unconstrained and goal-directed molecular generation tasks, outperforming prior Mol-LLMs and specialized models, while revealing a weak correlation between pretraining metrics and downstream performance.

Conclusion: NovoMolGen serves as a highly capable model family, setting a new standard for molecular generative modeling and advancing molecular design efficiency via scalable Mol-LLMs.

Abstract: Designing de-novo molecules with desired property profiles requires efficient
exploration of the vast chemical space ranging from $10^{23}$ to $10^{60}$
possible synthesizable candidates. While various deep generative models have
been developed to design small molecules using diverse input representations,
Molecular Large Language Models (Mol-LLMs) based on string representations have
emerged as a scalable approach capable of exploring billions of molecules.
However, there remains limited understanding regarding how standard language
modeling practices such as textual representations, tokenization strategies,
model size, and dataset scale impact molecular generation performance. In this
work, we systematically investigate these critical aspects by introducing
NovoMolGen, a family of transformer-based foundation models pretrained on 1.5
billion molecules for de-novo molecule generation. Through extensive empirical
analyses, we identify a weak correlation between performance metrics measured
during pretraining and actual downstream performance, revealing important
distinctions between molecular and general NLP training dynamics. NovoMolGen
establishes new state-of-the-art results, substantially outperforming prior
Mol-LLMs and specialized generative models in both unconstrained and
goal-directed molecular generation tasks, thus providing a robust foundation
for advancing efficient and effective molecular modeling strategies.

</details>


### [209] [Decentralized Contextual Bandits with Network Adaptivity](https://arxiv.org/abs/2508.13411)
*Chuyun Deng,Huiwen Jia*

Main category: cs.LG

TL;DR: This paper introduces two novel algorithms, NetLinUCB and Net-SGD-UCB, designed for contextual linear bandit problems over networks with partial information sharing to reduce learning complexity and improve adaptive decision-making.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the gaps in traditional contextual bandit frameworks which either assume centralized data or isolated learners, ignoring partially shared information environments common in networked decision-making setups.

Method: The paper utilizes two network-aware UCB algorithms, decomposing learning into global and local components, and employs communication of computed summaries to reduce communication costs. Regret bounds demonstrate advantages in reducing learning complexity.

Result: Both algorithms improve learning efficiency, reducing regret bounds from $O(N)$ to $O(\sqrt{N})$, where NetLinUCB works well in low-noise, fine-grained heterogeneity, and Net-SGD-UCB thrives in high-dimensional, high-variance settings.

Conclusion: The proposed algorithms offer flexible and effective solutions for networked contextual linear bandit problems, outperforming standard benchmarks in simulated environments while maintaining reduced communication costs.

Abstract: We consider contextual linear bandits over networks, a class of sequential
decision-making problems where learning occurs simultaneously across multiple
locations and the reward distributions share structural similarities while also
exhibiting local differences. While classical contextual bandits assume either
fully centralized data or entirely isolated learners, much remains unexplored
in networked environments when information is partially shared. In this paper,
we address this gap by developing two network-aware Upper Confidence Bound
(UCB) algorithms, NetLinUCB and Net-SGD-UCB, which enable adaptive information
sharing guided by dynamically updated network weights. Our approach decompose
learning into global and local components and as a result allow agents to
benefit from shared structure without full synchronization. Both algorithms
incur lighter communication costs compared to a fully centralized setting as
agents only share computed summaries regarding the homogeneous features. We
establish regret bounds showing that our methods reduce the learning complexity
associated with the shared structure from $O(N)$ to sublinear $O(\sqrt{N})$,
where $N$ is the size of the network. The two algorithms reveal complementary
strengths: NetLinUCB excels in low-noise regimes with fine-grained
heterogeneity, while Net-SGD-UCB is robust to high-dimensional, high-variance
contexts. We further demonstrate the effectiveness of our methods across
simulated pricing environments compared to standard benchmarks.

</details>


### [210] [MAVIS: Multi-Objective Alignment via Value-Guided Inference-Time Search](https://arxiv.org/abs/2508.13415)
*Jeremy Carleton,Debajoy Mukherjee,Srinivas Shakkottai,Dileep Kalathil*

Main category: cs.LG

TL;DR: MAVIS offers a framework to flexibly align LLM outputs to user preferences at inference-time using value models, avoiding computationally expensive fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Balancing conflicting objectives (e.g., helpfulness, harmlessness) in LLMs is computationally expensive and inflexible when models need to be fine-tuned for every preference configuration.

Method: MAVIS uses lightweight value models (trained via KL-regularized iteration) to dynamically adjust a base LLM's output distribution at inference time, guided by user-specified objective weights.

Result: MAVIS outperforms fine-tuned models and approaches performance levels seen in idealized settings with exact user preference fine-tuning.

Conclusion: MAVIS offers a more efficient and flexible solution for aligning LLM outcomes to varying user preferences compared to traditional fine-tuning methods.

Abstract: Large Language Models (LLMs) are increasingly deployed across diverse
applications that demand balancing multiple, often conflicting, objectives --
such as helpfulness, harmlessness, or humor. Aligning outputs to user-specific
preferences in such multi-objective settings typically requires fine-tuning
models for each objective or preference configuration, which is computationally
expensive and inflexible. We introduce MAVIS -- Multi-Objective Alignment via
Value-Guided Inference-Time Search -- a lightweight inference-time alignment
framework that enables dynamic control over LLM behavior without modifying the
base model's weights. MAVIS trains a set of small value models, each
corresponding to a distinct objective. At inference time, these value models
are combined using user-specified weights to produce a tilting function that
adjusts the base model's output distribution toward desired trade-offs. The
value models are trained using a simple iterative algorithm that ensures
monotonic improvement of the KL-regularized policy. We show empirically that
MAVIS outperforms baselines that fine-tune per-objective models and combine
them post hoc, and even approaches the performance of the idealized setting
where models are fine-tuned for a user's exact preferences.

</details>


### [211] [EventTSF: Event-Aware Non-Stationary Time Series Forecasting](https://arxiv.org/abs/2508.13434)
*Yunfeng Ge,Ming Jin,Yiji Zhao,Hongyan Li,Bo Du,Chang Xu,Shirui Pan*

Main category: cs.LG

TL;DR: Time series forecasting often underutilizes textual event data. EventTSF, a multimodal framework, integrates time series and textual data for improved forecasting with better accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Forecasting non-stationary dynamics in domains such as energy and transportation lacks methods to effectively incorporate textual events, leading to limited model contextuality and performance.

Method: EventTSF employs an autoregressive diffusion framework with flow matching, adaptive timestep control for event semantics, and a multimodal U-shaped diffusion transformer for fusing textual and temporal data.

Result: EventTSF outperforms 12 baselines, delivering 10.7% higher accuracy and $1.13\times$ faster training efficiency on 8 synthetic and real-world datasets.

Conclusion: EventTSF successfully addresses challenges in multimodal interaction between textual events and time series, enhancing performance and efficiency in event-aware forecasting scenarios.

Abstract: Time series forecasting plays a vital role in critical domains like energy
and transportation, where non-stationary dynamics are deeply intertwined with
events in other modalities such as texts. However, incorporating natural
language-based external events to improve non-stationary forecasting remains
largely unexplored, as most approaches still rely on a single modality,
resulting in limited contextual knowledge and model underperformance. Enabling
fine-grained multimodal interactions between temporal and textual data is
challenged by three fundamental issues: (1) the difficulty of fine-grained
synchronization between time-varying discrete textual events and continuous
time series; (2) the inherent temporal uncertainty introduced by textual
semantics; and (3) the misalignment between textual event embeddings and
multi-resolution temporal patterns. In this work, we address these challenges
by introducing event-aware non-stationary time series forecasting (EventTSF),
an autoregressive generation framework that integrates historical time series
with textual events to make subsequent forecasts. Specifically, EventTSF uses
autoregressive diffusion with flow matching at each step to capture nuanced
temporal-event interactions. To handle event-induced uncertainty, flow matching
timesteps are adaptively controlled according to event semantic signals. The
underlying denoiser employs a multimodal U-shaped diffusion transformer that
efficiently fuses temporal and textual modalities across different resolutions.
Extensive experiments on 8 synthetic and real-world datasets show that EventTSF
outperforms 12 baselines across diverse event-aware non-stationary time series
forecasting scenarios, achieving substantial improvements of 10.7% higher
forecasting accuracy and $1.13\times$ faster training efficiency.

</details>


### [212] [SVDformer: Direction-Aware Spectral Graph Embedding Learning via SVD and Transformer](https://arxiv.org/abs/2508.13435)
*Jiayu Fang,Zhiqi Shao,S T Boris Choy,Junbin Gao*

Main category: cs.LG

TL;DR: SVDformer integrates SVD and Transformer methods to improve graph directionality representation, surpassing existing models in node classification tasks.


<details>
  <summary>Details</summary>
Motivation: Existing directed graph neural networks fail to efficiently capture directional semantics and global patterns due to their isotropic aggregation and localized filtering.

Method: The paper introduces SVDformer, combining singular value decomposition (SVD) and Transformer architecture to refine graph embeddings using multi-head self-attention and model directional interactions.

Result: SVDformer shows superior performance on six directed graph benchmarks for node classification compared to existing GNNs and direction-aware models.

Conclusion: The study demonstrates SVDformer as an effective framework that sets a new direction in representation learning for directed graphs.

Abstract: Directed graphs are widely used to model asymmetric relationships in
real-world systems. However, existing directed graph neural networks often
struggle to jointly capture directional semantics and global structural
patterns due to their isotropic aggregation mechanisms and localized filtering
mechanisms. To address this limitation, this paper proposes SVDformer, a novel
framework that synergizes SVD and Transformer architecture for direction-aware
graph representation learning. SVDformer first refines singular value
embeddings through multi-head self-attention, adaptively enhancing critical
spectral components while suppressing high-frequency noise. This enables
learnable low-pass/high-pass graph filtering without requiring spectral
kernels. Furthermore, by treating singular vectors as directional projection
bases and singular values as scaling factors, SVDformer uses the Transformer to
model multi-scale interactions between incoming/outgoing edge patterns through
attention weights, thereby explicitly preserving edge directionality during
feature propagation. Extensive experiments on six directed graph benchmarks
demonstrate that SVDformer consistently outperforms state-of-the-art GNNs and
direction-aware baselines on node classification tasks, establishing a new
paradigm for learning representations on directed graphs.

</details>


### [213] [ASAP: Unsupervised Post-training with Label Distribution Shift Adaptive Learning Rate](https://arxiv.org/abs/2508.13445)
*Heewon Park,Mugon Joe,Miru Kim,Minhae Kwon*

Main category: cs.LG

TL;DR: The paper introduces ASAP, a method for dynamically adjusting learning rates in online label shift scenarios without labels or model ensembles.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of maintaining machine learning model performance when label distributions change over time in real-world scenarios.

Method: ASAP dynamically adjusts the learning rate using the cosine distance between current and previous softmax outputs, mapping it into a bounded range for lightweight, unsupervised adaptation.

Result: Experiments show that ASAP improves both accuracy and efficiency under various datasets and label shift scenarios.

Conclusion: ASAP is a practical, label-free solution for unsupervised model adaptation during online label shift, enhancing adaptability and stability.

Abstract: In real-world applications, machine learning models face online label shift,
where label distributions change over time. Effective adaptation requires
careful learning rate selection: too low slows adaptation and too high causes
instability. We propose ASAP (Adaptive Shift Aware Post-training), which
dynamically adjusts the learning rate by computing the cosine distance between
current and previous unlabeled outputs and mapping it within a bounded range.
ASAP requires no labels, model ensembles, or past inputs, using only the
previous softmax output for fast, lightweight adaptation. Experiments across
multiple datasets and shift scenarios show ASAP consistently improves accuracy
and efficiency, making it practical for unsupervised model adaptation.

</details>


### [214] [Hierarchy-Consistent Learning and Adaptive Loss Balancing for Hierarchical Multi-Label Classification](https://arxiv.org/abs/2508.13452)
*Ruobing Jiang,Mengzhe Liu,Haobing Liu,Yanwei Yu*

Main category: cs.LG

TL;DR: The paper introduces an HMC classifier "HCAL" that integrates prototype contrastive learning and adaptive task-weighting, achieving higher accuracy and reduced hierarchical violations compared to baseline models.


<details>
  <summary>Details</summary>
Motivation: To address structural inconsistency and imbalance issues in loss weighting for Hierarchical Multi-Label Classification using Multi-Task Learning.

Method: The proposed method, HCAL, incorporates prototype contrastive learning for maintaining semantic consistency and adaptive task-weighting mechanisms to dynamically balance optimization resources based on task-specific convergence.

Result: HCAL demonstrates superior classification accuracy and reduced hierarchical violation rate compared to traditional models across three datasets.

Conclusion: The HCAL model effectively improves hierarchical consistency, optimizes task-specific outcomes, and enhances robustness through its novel mechanisms, outperforming baseline classifiers.

Abstract: Hierarchical Multi-Label Classification (HMC) faces critical challenges in
maintaining structural consistency and balancing loss weighting in Multi-Task
Learning (MTL). In order to address these issues, we propose a classifier
called HCAL based on MTL integrated with prototype contrastive learning and
adaptive task-weighting mechanisms. The most significant advantage of our
classifier is semantic consistency including both prototype with explicitly
modeling label and feature aggregation from child classes to parent classes.
The other important advantage is an adaptive loss-weighting mechanism that
dynamically allocates optimization resources by monitoring task-specific
convergence rates. It effectively resolves the "one-strong-many-weak"
optimization bias inherent in traditional MTL approaches. To further enhance
robustness, a prototype perturbation mechanism is formulated by injecting
controlled noise into prototype to expand decision boundaries. Additionally, we
formalize a quantitative metric called Hierarchical Violation Rate (HVR) as to
evaluate hierarchical consistency and generalization. Extensive experiments
across three datasets demonstrate both the higher classification accuracy and
reduced hierarchical violation rate of the proposed classifier over baseline
models.

</details>


### [215] [Classifying Clinical Outcome of Epilepsy Patients with Ictal Chirp Embeddings](https://arxiv.org/abs/2508.13476)
*Nooshin Bahador,Milad Lankarany*

Main category: cs.LG

TL;DR: The study uses t-SNE visualizations of chirp features to train classifiers for clinical outcome prediction, achieving high accuracy while enabling interpretable feature mappings.


<details>
  <summary>Details</summary>
Motivation: The paper aims to leverage interpretable embeddings for clinical stratification and decision-making by understanding latent structures in chirp-based metrics.

Method: The pipeline uses t-SNE for embedding chirp features and trains classifiers (Random Forest, k-NN, etc.) on the embeddings. SHAP explanations provide insights into feature contributions.

Result: Random Forest and k-NN classifiers achieved up to 88.8% accuracy in identifying optimal clinical cases, while sensitivity maps revealed localized feature importance.

Conclusion: The framework demonstrates the potential of t-SNE embeddings and feature attribution methods for improving clinical decision support systems.

Abstract: This study presents a pipeline leveraging t-Distributed Stochastic Neighbor
Embedding (t-SNE) for interpretable visualizations of chirp features across
diverse outcome scenarios. The dataset, comprising chirp-based temporal,
spectral, and frequency metrics. Using t-SNE, local neighborhood relationships
were preserved while addressing the crowding problem through Student
t-distribution-based similarity optimization. Three classification tasks were
formulated on the 2D t-SNE embeddings: (1) distinguishing clinical success from
failure/no-resection, (2) separating high-difficulty from low-difficulty cases,
and (3) identifying optimal cases, defined as successful outcomes with minimal
clinical difficulty. Four classifiers, namely, Random Forests, Support Vector
Machines, Logistic Regression, and k-Nearest Neighbors, were trained and
evaluated using stratified 5-fold cross-validation. Across tasks, the Random
Forest and k-NN classifiers demonstrated superior performance, achieving up to
88.8% accuracy in optimal case detection (successful outcomes with minimal
clinical difficulty). Additionally, feature influence sensitivity maps were
generated using SHAP explanations applied to model predicting t-SNE
coordinates, revealing spatially localized feature importance within the
embedding space. These maps highlighted how specific chirp attributes drive
regional clustering and class separation, offering insights into the latent
structure of the data. The integrated framework showcases the potential of
interpretable embeddings and local feature attribution for clinical
stratification and decision support.

</details>


### [216] [DyMixOp: Guiding Neural Operator Design for PDEs from a Complex Dynamics Perspective with Local-Global-Mixing](https://arxiv.org/abs/2508.13490)
*Pengyu Lai,Yixiao Chen,Hui Xu*

Main category: cs.LG

TL;DR: The paper introduces DyMixOp, a neural operator framework for nonlinear PDEs, showing superior accuracy and efficiency by leveraging inertial manifold theory and a Local-Global-Mixing transformation.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of transforming nonlinear PDEs, particularly those with complex dynamics or infinite-dimensional spaces, into formats suitable for neural network approximation.

Method: The method involves using inertial manifold theory to map PDEs to finite-dimensional latent spaces and employing a Local-Global-Mixing (LGM) transformation to capture fine-scale dynamics and mitigate spectral bias. A dynamics-informed architecture connects these components to model temporal evolution.

Result: DyMixOp achieves state-of-the-art accuracy on various PDE benchmarks, particularly excelling in convection-dominated scenarios by reducing prediction errors up to 86.7%, while maintaining efficiency and scalability.

Conclusion: DyMixOp establishes a robust framework that balances physical interpretability, accuracy, and computational efficiency for solving nonlinear PDEs.

Abstract: A primary challenge in using neural networks to approximate nonlinear
dynamical systems governed by partial differential equations (PDEs) is
transforming these systems into a suitable format, especially when dealing with
non-linearizable dynamics or the need for infinite-dimensional spaces for
linearization. This paper introduces DyMixOp, a novel neural operator framework
for PDEs that integrates insights from complex dynamical systems to address
this challenge. Grounded in inertial manifold theory, DyMixOp transforms
infinite-dimensional nonlinear PDE dynamics into a finite-dimensional latent
space, establishing a structured foundation that maintains essential nonlinear
interactions and enhances physical interpretability. A key innovation is the
Local-Global-Mixing (LGM) transformation, inspired by convection dynamics in
turbulence. This transformation effectively captures both fine-scale details
and nonlinear interactions, while mitigating spectral bias commonly found in
existing neural operators. The framework is further strengthened by a
dynamics-informed architecture that connects multiple LGM layers to approximate
linear and nonlinear dynamics, reflecting the temporal evolution of dynamical
systems. Experimental results across diverse PDE benchmarks demonstrate that
DyMixOp achieves state-of-the-art performance, significantly reducing
prediction errors, particularly in convection-dominated scenarios reaching up
to 86.7\%, while maintaining computational efficiency and scalability.

</details>


### [217] [Uncertainty Tube Visualization of Particle Trajectories](https://arxiv.org/abs/2508.13505)
*Jixian Li,Timbwaoga Aime Judicael Ouermi,Mengjiao Han,Chris R. Johnson*

Main category: cs.LG

TL;DR: The paper presents the 'uncertainty tube,' a visualization method for representing uncertainty in neural network-predicted particle trajectories.


<details>
  <summary>Details</summary>
Motivation: The need to quantify and visualize uncertainty in neural network predictions, especially for trustworthiness in critical applications, motivates this research.

Method: The authors propose the 'uncertainty tube,' a superelliptical representation for nonsymmetric uncertainties, leveraging techniques like Deep Ensembles, Monte Carlo Dropout, and Stochastic Weight Averaging-Gaussian (SWAG).

Result: The method effectively captures and illustrates uncertainty, as validated on both synthetic and simulation datasets.

Conclusion: The 'uncertainty tube' is a computationally efficient, practical tool for visualizing uncertainty, enhancing trust in neural network predictions.

Abstract: Predicting particle trajectories with neural networks (NNs) has substantially
enhanced many scientific and engineering domains. However, effectively
quantifying and visualizing the inherent uncertainty in predictions remains
challenging. Without an understanding of the uncertainty, the reliability of NN
models in applications where trustworthiness is paramount is significantly
compromised. This paper introduces the uncertainty tube, a novel,
computationally efficient visualization method designed to represent this
uncertainty in NN-derived particle paths. Our key innovation is the design and
implementation of a superelliptical tube that accurately captures and
intuitively conveys nonsymmetric uncertainty. By integrating well-established
uncertainty quantification techniques, such as Deep Ensembles, Monte Carlo
Dropout (MC Dropout), and Stochastic Weight Averaging-Gaussian (SWAG), we
demonstrate the practical utility of the uncertainty tube, showcasing its
application on both synthetic and simulation datasets.

</details>


### [218] [Explainability of Algorithms](https://arxiv.org/abs/2508.13529)
*Andrés Páez*

Main category: cs.LG

TL;DR: The chapter examines two types of algorithmic opaqueness—natural technical complexity and intentional proprietary secrecy—and discusses the ethical implications and current challenges in creating explainable AI (XAI).


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the ethical concerns surrounding the opaqueness of AI algorithms, specifically focusing on the barriers to understanding their inner workings and the ethical consequences.

Method: The chapter analyzes two types of opacity (inherent and intentional) and reviews explanatory methods in computer science aimed at mitigating the technical opaqueness of AI systems.

Result: The review highlights that current methods for achieving explainable AI (XAI) still have significant challenges to address.

Conclusion: Understanding and addressing both technical and intentional opaqueness of AI is crucial for ethical AI development, but explainable AI methods remain an ongoing area of work with persistent difficulties.

Abstract: The opaqueness of many complex machine learning algorithms is often mentioned
as one of the main obstacles to the ethical development of artificial
intelligence (AI). But what does it mean for an algorithm to be opaque? Highly
complex algorithms such as artificial neural networks process enormous volumes
of data in parallel along multiple hidden layers of interconnected nodes,
rendering their inner workings epistemically inaccessible to any human being,
including their designers and developers; they are "black boxes" for all their
stakeholders. But opaqueness is not always the inevitable result of technical
complexity. Sometimes, the way an algorithm works is intentionally hidden from
view for proprietary reasons, especially in commercial automated decision
systems, creating an entirely different type of opaqueness. In the first part
of the chapter, we will examine these two ways of understanding opacity and the
ethical implications that stem from each of them. In the second part, we
explore the different explanatory methods that have been developed in computer
science to overcome an AI system's technical opaqueness. As the analysis shows,
explainable AI (XAI) still faces numerous challenges.

</details>


### [219] [MuFlex: A Scalable, Physics-based Platform for Multi-Building Flexibility Analysis and Coordination](https://arxiv.org/abs/2508.13532)
*Ziyan Wu,Ivan Korolija,Rui Tang*

Main category: cs.LG

TL;DR: The paper introduces MuFlex, a scalable open-source platform for coordinating demand flexibility across multiple buildings. It aims to address the limitations of existing testbeds for benchmarking reinforcement learning (RL)-based controls.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome limitations of existing multi-building testbeds, like reliance on simplified models or fixed formats, and provide a robust platform for managing demand flexibility for renewable-powered grids.

Method: The study developed MuFlex, an open-source, modular platform linking EnergyPlus models with the OpenAI Gym interface to benchmark multi-building RL-based control strategies.

Result: Using a case study of coordinating demand across four office buildings with the Soft Actor-Critic algorithm, the platform successfully reduced total peak demand while maintaining indoor environmental quality.

Conclusion: MuFlex provides a significant improvement in evaluating RL-based multi-building control strategies, balancing grid demands and building comfort efficiently.

Abstract: With the increasing penetration of renewable generation on the power grid,
maintaining system balance requires coordinated demand flexibility from
aggregations of buildings. Reinforcement learning (RL) has been widely explored
for building controls because of its model-free nature. Open-source simulation
testbeds are essential not only for training RL agents but also for fairly
benchmarking control strategies. However, most building-sector testbeds target
single buildings; multi-building platforms are relatively limited and typically
rely on simplified models (e.g., Resistance-Capacitance) or data-driven
approaches, which lack the ability to fully capture the physical intricacies
and intermediate variables necessary for interpreting control performance.
Moreover, these platforms often impose fixed inputs, outputs, and model
formats, restricting their applicability as benchmarking tools across diverse
control scenarios. To address these gaps, MuFlex, a scalable, open-source
platform for benchmarking and testing control strategies for multi-building
flexibility coordination, was developed in this study. MuFlex enables
synchronous information exchange across EnergyPlus building models and adheres
to the latest OpenAI Gym interface, providing a modular, standardized RL
implementation. The platform capabilities were demonstrated in a case study
coordinating demand flexibility across four office buildings using the Soft
Actor-Critic algorithm with carefully fine-tuned hyperparameters. The results
show that aggregating the four buildings flexibility reduced total peak demand
below a specified threshold while maintaining indoor environmental quality.

</details>


### [220] [CALYPSO: Forecasting and Analyzing MRSA Infection Patterns with Community and Healthcare Transmission Dynamics](https://arxiv.org/abs/2508.13548)
*Rituparna Datta,Jiaming Cui,Gregory R. Madden,Anil Vullikanti*

Main category: cs.LG

TL;DR: The paper introduces CALYPSO, a hybrid model combining neural networks and mechanistic models for improved forecasting and analysis of MRSA spread in healthcare and community settings.


<details>
  <summary>Details</summary>
Motivation: Address the critical public health challenge of MRSA spread, improve forecasting models, overcome limitations of existing statistical and neural approaches, and support effective infection control policies.

Method: CALYPSO integrates neural networks with mechanistic metapopulation models to process diverse datasets (insurance claims, commuting data, healthcare transfer patterns) for learning region-specific infection dynamics.

Result: CALYPSO enhances forecasting accuracy by over 4.5% compared to machine learning baselines, provides interpretable multi-resolution forecasts, and identifies high-risk regions and cost-effective infection prevention strategies.

Conclusion: CALYPSO is a promising tool that combines epidemiological depth and computational accuracy for forecasting MRSA dynamics, aiding in policy evaluation and resource allocation.

Abstract: Methicillin-resistant Staphylococcus aureus (MRSA) is a critical public
health threat within hospitals as well as long-term care facilities. Better
understanding of MRSA risks, evaluation of interventions and forecasting MRSA
rates are important public health problems. Existing forecasting models rely on
statistical or neural network approaches, which lack epidemiological
interpretability, and have limited performance. Mechanistic epidemic models are
difficult to calibrate and limited in incorporating diverse datasets. We
present CALYPSO, a hybrid framework that integrates neural networks with
mechanistic metapopulation models to capture the spread dynamics of infectious
diseases (i.e., MRSA) across healthcare and community settings. Our model
leverages patient-level insurance claims, commuting data, and healthcare
transfer patterns to learn region- and time-specific parameters governing MRSA
spread. This enables accurate, interpretable forecasts at multiple spatial
resolutions (county, healthcare facility, region, state) and supports
counterfactual analyses of infection control policies and outbreak risks. We
also show that CALYPSO improves statewide forecasting performance by over 4.5%
compared to machine learning baselines, while also identifying high-risk
regions and cost-effective strategies for allocating infection prevention
resources.

</details>


### [221] [Collapsing ROC approach for risk prediction research on both common and rare variants](https://arxiv.org/abs/2508.13552)
*Changshuai Wei,Qing Lu*

Main category: cs.LG

TL;DR: The paper proposes a new risk prediction approach, CROC, that accounts for both common and rare genetic variants, showing improved predictive accuracy over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current genetic risk prediction models lack sufficient accuracy for clinical application, as they predominantly focus on common genetic variants, ignoring the contribution of rare variants.

Method: The authors developed a CROC (collapsing receiver operating characteristic) approach, which extends the existing FROC method by incorporating mechanisms to handle rare genetic variants. They evaluated its performance on SNP data from the Genetic Analysis Workshop 17.

Result: The CROC method achieved higher predictive accuracy (AUC = 0.605) compared to relying solely on common variants (AUC = 0.585). Notably, CROC outperformed FROC as the number of common variants in the dataset decreased, maintaining an AUC of 0.603 even when only rare variants were present.

Conclusion: Incorporating both common and rare genetic variants improves risk prediction accuracy, with the CROC method demonstrating better utility in scenarios with limited common variant data.

Abstract: Risk prediction that capitalizes on emerging genetic findings holds great
promise for improving public health and clinical care. However, recent risk
prediction research has shown that predictive tests formed on existing common
genetic loci, including those from genome-wide association studies, have lacked
sufficient accuracy for clinical use. Because most rare variants on the genome
have not yet been studied for their role in risk prediction, future disease
prediction discoveries should shift toward a more comprehensive risk prediction
strategy that takes into account both common and rare variants. We are
proposing a collapsing receiver operating characteristic CROC approach for risk
prediction research on both common and rare variants. The new approach is an
extension of a previously developed forward ROC FROC approach, with additional
procedures for handling rare variants. The approach was evaluated through the
use of 533 single-nucleotide polymorphisms SNPs in 37 candidate genes from the
Genetic Analysis Workshop 17 mini-exome data set. We found that a prediction
model built on all SNPs gained more accuracy AUC = 0.605 than one built on
common variants alone AUC = 0.585. We further evaluated the performance of two
approaches by gradually reducing the number of common variants in the analysis.
We found that the CROC method attained more accuracy than the FROC method when
the number of common variants in the data decreased. In an extreme scenario,
when there are only rare variants in the data, the CROC reached an AUC value of
0.603, whereas the FROC had an AUC value of 0.524.

</details>


### [222] [Prediction of Hospital Associated Infections During Continuous Hospital Stays](https://arxiv.org/abs/2508.13561)
*Rituparna Datta,Methun Kamruzzaman,Eili Y. Klein,Gregory R Madden,Xinwei Deng,Anil Vullikanti,Parantapa Bhattacharya*

Main category: cs.LG

TL;DR: The paper introduces GenHAI, a generative probabilistic model aimed at predicting MRSA test outcomes for hospitalized patients, and demonstrates its efficacy against other machine learning models using real-world datasets.


<details>
  <summary>Details</summary>
Motivation: To address the critical concern of MRSA infections in hospitalized patients by enabling better predictions and analysis through a novel probabilistic model.

Method: The paper employs a probabilistic programming model, GenHAI, to analyze sequences of MRSA test outcomes during hospitalization. It evaluates its predictions using real-world datasets and compares it with other machine learning methods.

Result: GenHAI successfully models MRSA test outcome sequences, providing predictive, causal, and counterfactual insights. The model outperformed other machine learning approaches in evaluations.

Conclusion: GenHAI is a promising tool for hospital administrators to mitigate MRSA risks by enabling actionable insights and comparisons with real-world performance benchmarks.

Abstract: The US Centers for Disease Control and Prevention (CDC), in 2019, designated
Methicillin-resistant Staphylococcus aureus (MRSA) as a serious antimicrobial
resistance threat. The risk of acquiring MRSA and suffering life-threatening
consequences due to it remains especially high for hospitalized patients due to
a unique combination of factors, including: co-morbid conditions, immuno
suppression, antibiotic use, and risk of contact with contaminated hospital
workers and equipment. In this paper, we present a novel generative
probabilistic model, GenHAI, for modeling sequences of MRSA test results
outcomes for patients during a single hospitalization. This model can be used
to answer many important questions from the perspectives of hospital
administrators for mitigating the risk of MRSA infections. Our model is based
on the probabilistic programming paradigm, and can be used to approximately
answer a variety of predictive, causal, and counterfactual questions. We
demonstrate the efficacy of our model by comparing it against discriminative
and generative machine learning models using two real-world datasets.

</details>


### [223] [A Generalized Learning Framework for Self-Supervised Contrastive Learning](https://arxiv.org/abs/2508.13596)
*Lingyu Si,Jingyao Wang,Wenwen Qiang*

Main category: cs.LG

TL;DR: The paper generalizes self-supervised contrastive learning methods into a unifying Generalized Learning Framework (GLF) and introduces Adaptive Distribution Calibration (ADC) to improve intra-class compactness and inter-class separability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations in self-supervised contrastive learning (SSCL) methods by unifying them and enhancing their feature space representation to better preserve class information without labels.

Method: It proposes the Generalized Learning Framework (GLF) to analyze SSCL methods and introduces Adaptive Distribution Calibration (ADC) to iteratively adjust feature space by leveraging dynamic relationships between samples.

Result: Empirical and theoretical evaluations show ADC improves intra-class compactness and inter-class separability in feature representations, enhancing SSCL methods.

Conclusion: ADC offers a plug-and-play solution within the GLF to optimize feature space, demonstrating improvements in self-supervised learning tasks and advancing the design of SSCL methods.

Abstract: Self-supervised contrastive learning (SSCL) has recently demonstrated
superiority in multiple downstream tasks. In this paper, we generalize the
standard SSCL methods to a Generalized Learning Framework (GLF) consisting of
two parts: the aligning part and the constraining part. We analyze three
existing SSCL methods: BYOL, Barlow Twins, and SwAV, and show that they can be
unified under GLF with different choices of the constraining part. We further
propose empirical and theoretical analyses providing two insights into
designing the constraining part of GLF: intra-class compactness and inter-class
separability, which measure how well the feature space preserves the class
information of the inputs. However, since SSCL can not use labels, it is
challenging to design a constraining part that satisfies these properties. To
address this issue, we consider inducing intra-class compactness and
inter-class separability by iteratively capturing the dynamic relationship
between anchor and other samples and propose a plug-and-play method called
Adaptive Distribution Calibration (ADC) to ensure that samples that are near or
far from the anchor point in the original input space are closer or further
away from the anchor point in the feature space. Both the theoretical analysis
and the empirical evaluation demonstrate the superiority of ADC.

</details>


### [224] [Approximate Bayesian Inference via Bitstring Representations](https://arxiv.org/abs/2508.13598)
*Aleksanteri Sladek,Martin Trapp,Arno Solin*

Main category: cs.LG

TL;DR: This paper explores probabilistic inference using quantized parameter spaces and introduces a novel learning approach.


<details>
  <summary>Details</summary>
Motivation: To improve scalability and interpretation in machine learning by leveraging low-precision quantized arithmetic.

Method: A probabilistic inference framework is proposed, utilizing probabilistic circuits for learning in quantized discrete parameter spaces.

Result: The approach ensures efficient inference on complex distributions without compromising accuracy.

Conclusion: Discrete approximations can advance scalable and interpretable machine learning using probabilistic methods.

Abstract: The machine learning community has recently put effort into quantized or
low-precision arithmetics to scale large models. This paper proposes performing
probabilistic inference in the quantized, discrete parameter space created by
these representations, effectively enabling us to learn a continuous
distribution using discrete parameters. We consider both 2D densities and
quantized neural networks, where we introduce a tractable learning approach
using probabilistic circuits. This method offers a scalable solution to manage
complex distributions and provides clear insights into model behavior. We
validate our approach with various models, demonstrating inference efficiency
without sacrificing accuracy. This work advances scalable, interpretable
machine learning by utilizing discrete approximations for probabilistic
computations.

</details>


### [225] [Bounding Causal Effects and Counterfactuals](https://arxiv.org/abs/2508.13607)
*Tobias Maringgele*

Main category: cs.LG

TL;DR: The paper advances partial identification in causal inference by systematically analyzing various bounding algorithms, proposing methods for counterfactual queries, and releasing an open-source tool.


<details>
  <summary>Details</summary>
Motivation: Partial identification remains underutilized due to fragmented methods and lack of practical guidance.

Method: The study compares bounding algorithms across causal scenarios, extends entropy-bounded methods, and evaluates through simulations.

Result: Thousands of simulations show assessment metrics for methods including bound tightness, efficiency, and robustness; findings inform algorithm selection strategies.

Conclusion: Open-source tool and practical guidance provided to improve adoption of partial identification approaches in causal inference.

Abstract: Causal inference often hinges on strong assumptions - such as no unmeasured
confounding or perfect compliance - that are rarely satisfied in practice.
Partial identification offers a principled alternative: instead of relying on
unverifiable assumptions to estimate causal effects precisely, it derives
bounds that reflect the uncertainty inherent in the data. Despite its
theoretical appeal, partial identification remains underutilized in applied
work, in part due to the fragmented nature of existing methods and the lack of
practical guidance. This thesis addresses these challenges by systematically
comparing a diverse set of bounding algorithms across multiple causal
scenarios. We implement, extend, and unify state-of-the-art methods - including
symbolic, optimization-based, and information-theoretic approaches - within a
common evaluation framework. In particular, we propose an extension of a
recently introduced entropy-bounded method, making it applicable to
counterfactual queries such as the Probability of Necessity and Sufficiency
(PNS). Our empirical study spans thousands of randomized simulations involving
both discrete and continuous data-generating processes. We assess each method
in terms of bound tightness, computational efficiency, and robustness to
assumption violations. To support practitioners, we distill our findings into a
practical decision tree for algorithm selection and train a machine learning
model to predict the best-performing method based on observable data
characteristics.
  All implementations are released as part of an open-source Python package,
CausalBoundingEngine, which enables users to apply and compare bounding methods
through a unified interface.

</details>


### [226] [Towards a Larger Model via One-Shot Federated Learning on Heterogeneous Client Models](https://arxiv.org/abs/2508.13625)
*Wenxuan Ye,Xueli An,Onur Ayan,Junfan Wang,Xueqiang Yan,Georg Carle*

Main category: cs.LG

TL;DR: The paper introduces FedOL, a one-shot federated learning method employing knowledge distillation to enable larger and heterogeneous models, addressing communication and computational challenges in resource-constrained, privacy-focused mobile networks.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the limitations of traditional federated learning (FL) approaches, such as uniform model architectures, multiple communication rounds, computational demands on clients, and communication overhead, especially in resource-constrained environments like mobile networks.

Method: FedOL leverages knowledge distillation, allowing clients to exchange compact model predictions on an unlabeled public dataset instead of sharing raw data or model parameters. It refines pseudo-labels and incorporates a specialized strategy to integrate diverse knowledge effectively.

Result: Simulation results demonstrate that FedOL significantly outperforms existing federated learning baselines in terms of efficiency and effectiveness, particularly in mobile network scenarios.

Conclusion: FedOL offers a practical, cost-effective federated learning solution for mobile networks by reducing communication and computation burdens, enabling heterogeneous model architectures, and maintaining data privacy through knowledge distillation.

Abstract: Large models, renowned for superior performance, outperform smaller ones even
without billion-parameter scales. While mobile network servers have ample
computational resources to support larger models than client devices, privacy
constraints prevent clients from directly sharing their raw data. Federated
Learning (FL) enables decentralized clients to collaboratively train a shared
model by exchanging model parameters instead of transmitting raw data. Yet, it
requires a uniform model architecture and multiple communication rounds, which
neglect resource heterogeneity, impose heavy computational demands on clients,
and increase communication overhead. To address these challenges, we propose
FedOL, to construct a larger and more comprehensive server model in one-shot
settings (i.e., in a single communication round). Instead of model parameter
sharing, FedOL employs knowledge distillation, where clients only exchange
model prediction outputs on an unlabeled public dataset. This reduces
communication overhead by transmitting compact predictions instead of full
model weights and enables model customization by allowing heterogeneous model
architectures. A key challenge in this setting is that client predictions may
be biased due to skewed local data distributions, and the lack of ground-truth
labels in the public dataset further complicates reliable learning. To mitigate
these issues, FedOL introduces a specialized objective function that
iteratively refines pseudo-labels and the server model, improving learning
reliability. To complement this, FedOL incorporates a tailored pseudo-label
generation and knowledge distillation strategy that effectively integrates
diverse knowledge. Simulation results show that FedOL significantly outperforms
existing baselines, offering a cost-effective solution for mobile networks
where clients possess valuable private data but limited computational
resources.

</details>


### [227] [Text2Weight: Bridging Natural Language and Neural Network Weight Spaces](https://arxiv.org/abs/2508.13633)
*Bowen Tian,Wenshuo Chen,Zexi Li,Songning Lai,Jiemin Wu,Yutao Yue*

Main category: cs.LG

TL;DR: The paper introduces T2W, a framework utilizing diffusion transformers to generate neural network weights based on text descriptions, aiming for improved generalization to unseen tasks.


<details>
  <summary>Details</summary>
Motivation: Current neural network weight generation struggles with generalization to new tasks and practical applications, prompting the need for innovative solutions.

Method: The authors propose T2W, which hierarchically processes network parameters, embeds text descriptions from CLIP, uses adversarial training with weight-space augmentation, and generates task-specific weights.

Result: Experiments show T2W outperforms traditional methods on Cifar100, Caltech256, and TinyImageNet, producing quality weights for unseen tasks and enabling new applications such as weight enhancement and model fusion.

Conclusion: T2W bridges text semantics with neural network weights, advancing generative models for parameter synthesis and offering open-source tools for further exploration.

Abstract: How far are we really from automatically generating neural networks? While
neural network weight generation shows promise, current approaches struggle
with generalization to unseen tasks and practical application exploration. To
address this, we propose T2W, a diffusion transformer framework that generates
task-specific weights conditioned on natural language descriptions. T2W
hierarchically processes network parameters into uniform blocks, integrates
text embeddings from CLIP via a prior attention mechanism, and employs
adversarial training with weight-space augmentation to enhance generalization.
Experiments on Cifar100, Caltech256, and TinyImageNet demonstrate T2W's ability
to produce high-quality weights for unseen tasks, outperforming
optimization-based initialization and enabling novel applications such as
weight enhancement and text-guided model fusion. Our work bridges textual
semantics with weight-space dynamics, supported by an open-source dataset of
text-weight pairs, advancing the practicality of generative models in neural
network parameter synthesis. Our code is available on Github.

</details>


### [228] [Explainable Learning Rate Regimes for Stochastic Optimization](https://arxiv.org/abs/2508.13639)
*Zhuang Yang*

Main category: cs.LG

TL;DR: This paper introduces an explainable and automatic learning rate adjustment regime for stochastic gradient descent (SGD) based on the intrinsic variation of stochastic gradients, eliminating the need for manual parameter tuning.


<details>
  <summary>Details</summary>
Motivation: Existing learning rate adjustment methods in machine learning are complex, often requiring manual tuning of multiple hyperparameters, leading to significant computational, time, and power costs.

Method: The authors propose a new learning rate adjustment approach using stochastic second-order algorithms, where the learning rate automatically increases or decreases based on the norm of stochastic gradients.

Result: The proposed learning rate regime efficiently operates in classical stochastic algorithms like SGD, SGDM, and SIGNSGD, demonstrating robustness, scalability, and efficiency across various machine learning tasks.

Conclusion: This method simplifies learning rate adjustment in machine learning by removing manual tuning and ensuring performance improvements in various scenarios.

Abstract: Modern machine learning is trained by stochastic gradient descent (SGD),
whose performance critically depends on how the learning rate (LR) is adjusted
and decreased over time. Yet existing LR regimes may be intricate, or need to
tune one or more additional hyper-parameters manually whose bottlenecks include
huge computational expenditure, time and power in practice. This work, in a
natural and direct manner, clarifies how LR should be updated automatically
only according to the intrinsic variation of stochastic gradients. An
explainable LR regime by leveraging stochastic second-order algorithms is
developed, behaving a similar pattern to heuristic algorithms but implemented
simply without any parameter tuning requirement, where it is of an automatic
procedure that LR should increase (decrease) as the norm of stochastic
gradients decreases (increases). The resulting LR regime shows its efficiency,
robustness, and scalability in different classical stochastic algorithms,
containing SGD, SGDM, and SIGNSGD, on machine learning tasks.

</details>


### [229] [Personalized Subgraph Federated Learning with Sheaf Collaboration](https://arxiv.org/abs/2508.13642)
*Wenfei Liang,Yanan Zhao,Rui She,Yiming Li,Wee Peng Tay*

Main category: cs.LG

TL;DR: FedSheafHN is a framework for subgraph federated learning that uses sheaf collaboration to improve personalized model generation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address performance variation across clients in personalized subgraph federated learning caused by heterogeneity of local subgraphs.

Method: FedSheafHN embeds client subgraphs into a collaboration graph using graph-level embeddings and sheaf diffusion, then generates client models via a hypernetwork.

Result: FedSheafHN shows superior performance, faster convergence, and effective generalization across diverse graph datasets compared to existing methods.

Conclusion: FedSheafHN is a novel and effective solution for personalized subgraph federated learning, addressing heterogeneity and improving outcomes for a variety of clients.

Abstract: Graph-structured data is prevalent in many applications. In subgraph
federated learning (FL), this data is distributed across clients, each with a
local subgraph. Personalized subgraph FL aims to develop a customized model for
each client to handle diverse data distributions. However, performance
variation across clients remains a key issue due to the heterogeneity of local
subgraphs. To overcome the challenge, we propose FedSheafHN, a novel framework
built on a sheaf collaboration mechanism to unify enhanced client descriptors
with efficient personalized model generation. Specifically, FedSheafHN embeds
each client's local subgraph into a server-constructed collaboration graph by
leveraging graph-level embeddings and employing sheaf diffusion within the
collaboration graph to enrich client representations. Subsequently, FedSheafHN
generates customized client models via a server-optimized hypernetwork.
Empirical evaluations demonstrate that FedSheafHN outperforms existing
personalized subgraph FL methods on various graph datasets. Additionally, it
exhibits fast model convergence and effectively generalizes to new clients.

</details>


### [230] [GRAFT: Gradient-Aware Fast MaxVol Technique for Dynamic Data Sampling](https://arxiv.org/abs/2508.13653)
*Ashish Jha,Anh huy Phan,Razan Dibo,Valentin Leplat*

Main category: cs.LG

TL;DR: GRAFT is a method for efficient neural network training that selects diverse subsets during training to reduce computational cost and environmental impact while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the computational and environmental challenges of training large neural networks.

Method: GRAFT uses a three-step process: extracting low-rank features, applying Fast MaxVol sampling for subset selection, and dynamically adjusting subset sizes based on gradient approximation.

Result: GRAFT achieves competitive or superior accuracy compared to benchmarks while reducing training time, energy usage, and carbon emissions.

Conclusion: GRAFT offers an effective trade-off between accuracy, efficiency, and environmental impact during neural network training.

Abstract: Training modern neural networks on large datasets is computationally and
environmentally costly. We introduce GRAFT, a scalable in-training subset
selection method that (i) extracts a low-rank feature representation for each
batch, (ii) applies a Fast MaxVol sampler to select a small, diverse subset
that spans the batch's dominant subspace, and (iii) dynamically adjusts the
subset size using a gradient-approximation criterion. By operating in low-rank
subspaces and training on carefully chosen examples instead of full batches,
GRAFT preserves the training trajectory while reducing wall-clock time, energy
consumption, and $\mathrm{CO}_2$ emissions. Across multiple benchmarks, GRAFT
matches or exceeds recent selection baselines in both accuracy and efficiency,
providing a favorable trade-off between accuracy, efficiency, and emissions.

</details>


### [231] [Input Time Scaling](https://arxiv.org/abs/2508.13654)
*Rapheal Huang,Weilong Guo*

Main category: cs.LG

TL;DR: This paper introduces Input Time Scaling by refining inputs during training and testing, revealing that low-quality datasets can achieve high performance and questioning established views like "garbage in, garbage out." The findings improve reasoning abilities with smaller, diverse datasets and achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to challenge existing paradigms in scaling techniques for LLMs, such as data and training scaling, by proposing a new input refinement strategy.

Method: The authors employ a strategy called Input Time Scaling, which refines input data using meta-knowledge from LLMs during training and testing. Various query strategies are investigated, including adding irrelevant information and using minimally filtered datasets.

Result: Experimental results show that this approach breaks conventional assumptions about data quality, achieving SOTA performance on tasks with Qwen2.5-32B-Instruct models, particularly surpassing benchmarks on AIME24 and AIME25.

Conclusion: The study highlights that refining input strategies can unlock reasoning capabilities even with low-quality data. It also underscores the importance of training-testing co-design and offers practical advancements for reproducibility and future research by open-sourcing their findings.

Abstract: Current Large Language Models (LLMs) are usually post-trained on large-scale
carefully curated datasets (data & training scaling) and doing reasoning in
test time (inference time scaling). In this work, we present a new scaling
paradigm, Input Time Scaling, to complement previous scaling methods by putting
resources on queries (input time). During training and testing, we combine
meta-knowledge from LLMs to refine inputs with different strategies. We also
find a new phenomenon, training-testing co-design there. We need to apply query
strategies during both training and testing. Only applying strategies on
training or testing would seriously degrade the performance. We are also
surprised to find that seemingly low data quality datasets can gain high
performance. Adding irrelevant information to the queries, randomly selecting
examples from a minimally filtered dataset, can even perform the best. These
findings contradict the widely held inductive bias, "garbage in, garbage out".
Curating datasets with seemingly high-quality data can even potentially limit
the performance ceiling. In addition, models trained on more data with similar
quality (15k VS 1k) perform worse, simple dataset size scaling should also be
carefully inspected. The good news is that our findings are compatible with the
Less is More phenomenon. A small set of examples is enough to evoke high-level
reasoning ability. With experiments on models trained on Qwen2.5-32B-Instruct,
we are able to reach SOTA performance among 32B models on AIME24(76.7%) and
AIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with
a majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B,
the best result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate
reproducibility and further research, we are working on open-source our
datasets, data pipelines, evaluation results, and checkpoints.

</details>


### [232] [In-Context Decision Making for Optimizing Complex AutoML Pipelines](https://arxiv.org/abs/2508.13657)
*Amir Rezaei Balef,Katharina Eggensperger*

Main category: cs.LG

TL;DR: The paper presents PS-PFN, an extended framework for algorithm selection and hyperparameter optimization in modern ML workflows, adapted to include fine-tuning and other techniques.


<details>
  <summary>Details</summary>
Motivation: Existing AutoML approaches struggle to handle the heterogeneous ML pipelines of modern workflows, where fine-tuning, ensembling, and adaptation are as critical as hyperparameter optimization.

Method: The authors propose PS-PFN, a method that extends Posterior Sampling to a max k-armed bandit setup, leveraging prior-data fitted networks for in-context learning to estimate posterior distributions efficiently.

Result: PS-PFN outperformed existing benchmark AutoML strategies and bandit-based approaches in experimental evaluations on novel and standard tasks.

Conclusion: PS-PFN demonstrates efficient exploration and exploitation for adapting ML pipelines, outperforming traditional techniques while addressing pipeline heterogeneity challenges in modern AutoML systems.

Abstract: Combined Algorithm Selection and Hyperparameter Optimization (CASH) has been
fundamental to traditional AutoML systems. However, with the advancements of
pre-trained models, modern ML workflows go beyond hyperparameter optimization
and often require fine-tuning, ensembling, and other adaptation techniques.
While the core challenge of identifying the best-performing model for a
downstream task remains, the increasing heterogeneity of ML pipelines demands
novel AutoML approaches. This work extends the CASH framework to select and
adapt modern ML pipelines. We propose PS-PFN to efficiently explore and exploit
adapting ML pipelines by extending Posterior Sampling (PS) to the max k-armed
bandit problem setup. PS-PFN leverages prior-data fitted networks (PFNs) to
efficiently estimate the posterior distribution of the maximal value via
in-context learning. We show how to extend this method to consider varying
costs of pulling arms and to use different PFNs to model reward distributions
individually per arm. Experimental results on one novel and two existing
standard benchmark tasks demonstrate the superior performance of PS-PFN
compared to other bandit and AutoML strategies. We make our code and data
available at https://github.com/amirbalef/CASHPlus.

</details>


### [233] [MACTAS: Self-Attention-Based Module for Inter-Agent Communication in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.13661)
*Maciej Wojtala,Bogusz Stefańczyk,Dominik Bogucki,Łukasz Lepak,Jakub Strykowski,Paweł Wawrzyński*

Main category: cs.LG

TL;DR: This paper presents a self-attention-based communication module for multi-agent reinforcement learning (MARL), providing fully differentiable and efficient communication that enables agents to excel in complex tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of communication during collaborative task execution in MARL, where existing protocols are complex and non-differentiable.

Method: The proposed approach uses a self-attention-based communication module integrated with action-value function decomposition methods. It facilitates reward-driven message generation with a fixed set of trainable parameters.

Result: The method achieves state-of-the-art performance on multiple maps in the SMAC benchmark, demonstrating its effectiveness.

Conclusion: The self-attention-based communication module effectively simplifies differentiable communication for MARL, enhancing overall performance and scalability.

Abstract: Communication is essential for the collective execution of complex tasks by
human agents, motivating interest in communication mechanisms for multi-agent
reinforcement learning (MARL). However, existing communication protocols in
MARL are often complex and non-differentiable. In this work, we introduce a
self-attention-based communication module that exchanges information between
the agents in MARL. Our proposed approach is fully differentiable, allowing
agents to learn to generate messages in a reward-driven manner. The module can
be seamlessly integrated with any action-value function decomposition method
and can be viewed as an extension of such decompositions. Notably, it includes
a fixed number of trainable parameters, independent of the number of agents.
Experimental results on the SMAC benchmark demonstrate the effectiveness of our
approach, which achieves state-of-the-art performance on several maps.

</details>


### [234] [Heavy-tailed Linear Bandits: Adversarial Robustness, Best-of-both-worlds, and Beyond](https://arxiv.org/abs/2508.13679)
*Canzhe Zhao,Shinji Ito,Shuai Li*

Main category: cs.LG

TL;DR: This paper introduces a framework and algorithms for addressing adversarial heavy-tailed bandit problems, achieving robust performance in both adversarial and stochastic settings.


<details>
  <summary>Details</summary>
Motivation: To extend the study of heavy-tailed bandits into adversarial settings, as prior work almost exclusively focused on stochastic regimes or specific cases.

Method: The paper builds a general framework using a follow-the-regularized-leader approach, introducing a tuned bonus function. It develops algorithms for both multi-armed and linear bandits and introduces heavy-tailed noise aware stability-penalty matching (HT-SPM) for a flexible learning rate.

Result: The proposed algorithms achieve state-of-the-art regret bounds for adversarial and stochastic regimes: $\widetilde{O}(T^{1/\varepsilon})$ in adversarial and $\widetilde{O}(\log T)$ gap-dependent in stochastic for MABs, and $\widetilde{O}(d^{1/2}T^{1/\varepsilon})$ for linear bandits.

Conclusion: The framework and its algorithms provide a best-of-both-worlds result for heavy-tailed adversarial and stochastic bandits, marking a significant advancement in the field.

Abstract: Heavy-tailed bandits have been extensively studied since the seminal work of
\citet{Bubeck2012BanditsWH}. In particular, heavy-tailed linear bandits,
enabling efficient learning with both a large number of arms and heavy-tailed
noises, have recently attracted significant attention
\citep{ShaoYKL18,XueWWZ20,ZhongHYW21,Wang2025heavy,tajdini2025improved}.
However, prior studies focus almost exclusively on stochastic regimes, with few
exceptions limited to the special case of heavy-tailed multi-armed bandits
(MABs) \citep{Huang0H22,ChengZ024,Chen2024uniINF}.
  In this work, we propose a general framework for adversarial heavy-tailed
bandit problems, which performs follow-the-regularized-leader (FTRL) over the
loss estimates shifted by a bonus function. Via a delicate setup of the bonus
function, we devise the first FTRL-type best-of-both-worlds (BOBW) algorithm
for heavy-tailed MABs, which does not require the truncated non-negativity
assumption and achieves an $\widetilde{O}(T^{\frac{1}{\varepsilon}})$
worst-case regret in the adversarial regime as well as an $\widetilde{O}(\log
T)$ gap-dependent regret in the stochastic regime. We then extend our framework
to the linear case, proposing the first algorithm for adversarial heavy-tailed
linear bandits with finite arm sets. This algorithm achieves an
$\widetilde{O}(d^{\frac{1}{2}}T^{\frac{1}{\varepsilon}})$ regret, matching the
best-known worst-case regret bound in stochastic regimes. Moreover, we propose
a general data-dependent learning rate, termed \textit{heavy-tailed noise aware
stability-penalty matching} (HT-SPM). We prove that HT-SPM guarantees BOBW
regret bounds for general heavy-tailed bandit problems once certain conditions
are satisfied. By using HT-SPM and, in particular, a variance-reduced linear
loss estimator, we obtain the first BOBW result for heavy-tailed linear
bandits.

</details>


### [235] [DREAMS: Preserving both Local and Global Structure in Dimensionality Reduction](https://arxiv.org/abs/2508.13747)
*Noël Kury,Dmitry Kobak,Sebastian Damrich*

Main category: cs.LG

TL;DR: DREAMS (Dimensionality Reduction Enhanced Across Multiple Scales) effectively balances local and global structure preservation in dimensionality reduction, outperforming other methods in multiple datasets.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of existing dimensionality reduction methods that fail to simultaneously preserve local and global structures.

Method: Combining $t$-SNE with PCA using a regularization term to create a spectrum of embeddings that balance both local and global preservation.

Result: DREAMS demonstrated superior structure preservation across multiple scales in benchmarks using seven real-world datasets.

Conclusion: The method provides a robust solution for visualizing high-dimensional data while maintaining both fine-grained and overarching data relationships.

Abstract: Dimensionality reduction techniques are widely used for visualizing
high-dimensional data in two dimensions. Existing methods are typically
designed to preserve either local (e.g. $t$-SNE, UMAP) or global (e.g. MDS,
PCA) structure of the data, but none of the established methods can represent
both aspects well. In this paper, we present DREAMS (Dimensionality Reduction
Enhanced Across Multiple Scales), a method that combines the local structure
preservation of $t$-SNE with the global structure preservation of PCA via a
simple regularization term. Our approach generates a spectrum of embeddings
between the locally well-structured $t$-SNE embedding and the globally
well-structured PCA embedding, efficiently balancing both local and global
structure preservation. We benchmark DREAMS across seven real-world datasets,
including five from single-cell transcriptomics and one from population
genetics, showcasing qualitatively and quantitatively its superior ability to
preserve structure across multiple scales compared to previous approaches.

</details>


### [236] [Order Optimal Regret Bounds for Sharpe Ratio Optimization in the Bandit Setting](https://arxiv.org/abs/2508.13749)
*Mohammad Taha Shah,Sabrina Khurshid,Gourab Ghatak*

Main category: cs.LG

TL;DR: This paper uses the Thompson Sampling algorithm to optimize the Sharpe ratio in a stochastic bandit setting, achieving logarithmic regret and outperformance in simulations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of sequential decision-making for Sharpe ratio optimization, involving a balance between maximizing returns and minimizing risks.

Method: The paper proposes the SRTS algorithm, a variant of Thompson Sampling, and develops a regret decomposition specific to the Sharpe ratio, along with upper and lower bounds for performance analysis.

Result: The research demonstrates that SRTS achieves logarithmic regret over time, with theoretical bounds proving its order-optimality. Empirical results confirm superior performance compared to existing algorithms.

Conclusion: Thompson Sampling effectively balances exploration and exploitation for Sharpe ratio optimization, offering efficient and robust performance driven by new theoretical insights.

Abstract: In this paper, we investigate the problem of sequential decision-making for
Sharpe ratio (SR) maximization in a stochastic bandit setting. We focus on the
Thompson Sampling (TS) algorithm, a Bayesian approach celebrated for its
empirical performance and exploration efficiency, under the assumption of
Gaussian rewards with unknown parameters. Unlike conventional bandit objectives
focusing on maximizing cumulative reward, Sharpe ratio optimization instead
introduces an inherent tradeoff between achieving high returns and controlling
risk, demanding careful exploration of both mean and variance. Our theoretical
contributions include a novel regret decomposition specifically designed for
the Sharpe ratio, highlighting the role of information acquisition about the
reward distribution in driving learning efficiency. Then, we establish
fundamental performance limits for the proposed algorithm \texttt{SRTS} in
terms of an upper bound on regret. We also derive the matching lower bound and
show the order-optimality. Our results show that Thompson Sampling achieves
logarithmic regret over time, with distribution-dependent factors capturing the
difficulty of distinguishing arms based on risk-adjusted performance. Empirical
simulations show that our algorithm significantly outperforms existing
algorithms.

</details>


### [237] [Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration](https://arxiv.org/abs/2508.13755)
*Zhicheng Yang,Zhijiang Guo,Yinya Huang,Yongxin Wang,Dongchun Xie,Yiwei Wang,Xiaodan Liang,Jing Tang*

Main category: cs.LG

TL;DR: The paper explores enhancements to the RLVR framework by addressing biases in the GRPO algorithm via adaptive depth exploration and large-breadth training methods, achieving consistent reasoning improvements.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in reasoning powers of RLVR models caused by neglecting hard problems (depth) and insufficient breadth in training data.

Method: Introduced Difficulty Adaptive Rollout Sampling (DARS) for targeted multi-stage rollouts improving hard problem exploration. Complemented this with large-breadth training by scaling batch size and using full-batch updates.

Result: DARS yields consistent Pass@K improvements without extra inference cost. Large-breadth training enhances Pass@1 and sustains high entropy for better exploration. The combined method, DARS-B, achieves performance gains across metrics.

Conclusion: Depth and breadth are orthogonal dimensions critical for improving RLVR reasoning. The study confirms combining adaptive exploration (depth) and large-breadth training unlocks RLVR's full reasoning potential.

Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a
powerful paradigm for unlocking reasoning capabilities in large language
models, yet its full potential is hindered by two under-explored dimensions:
Depth-the hardest problem a model can sample; Breadth-the number of instances
consumed in a single iteration. We dissect the popular GRPO algorithm and
reveal a systematic bias: the cumulative-advantage disproportionately weights
samples with medium accuracy, while down-weighting the low-accuracy instances
that are crucial for pushing reasoning boundaries. To rectify the depth
neglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which
re-weights hard problems through targeted multi-stage rollouts, thereby
increasing the number of positive rollouts for hard problems. Empirically,
naively enlarging rollout size only accelerates convergence and even hurts
Pass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra
inference cost at convergence. Just as we adaptively expanded the depth of
exploration, we now ask whether aggressively scaling the breadth of training
data can further amplify reasoning gains. To this end, we intensely scale batch
size and replace PPO's mini-batch iterations with full-batch updates over
multiple epochs. Increasing breadth significantly enhances Pass@1 performance.
Large-breadth training sustains high token-level entropy, indicating continued
exploration and reduced gradient noise. We further present DARS-B, which
augments DARS with large breadth, and demonstrate simultaneous gains in Pass@K
and Pass@1. The results confirm that breadth and adaptive exploration across
depth operate as orthogonal dimensions in RLVR, which are key to unleashing the
reasoning power of RLVR.

</details>


### [238] [PENGUIN: Enhancing Transformer with Periodic-Nested Group Attention for Long-term Time Series Forecasting](https://arxiv.org/abs/2508.13773)
*Tian Sun,Yuqi Chen,Weiwei Sun*

Main category: cs.LG

TL;DR: The paper presents PENGUIN, a new attention mechanism for long-term time series forecasting, enhancing periodic pattern modeling. It outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: To address challenges in effectively modeling periodic patterns and multiple coexisting periodicities in long-term time series forecasting, which remain inadequately tackled by previous methods.

Method: The paper introduces Periodic-Nested Group Attention (PENGUIN), which incorporates periodic-nested relative attention bias and a grouped attention mechanism. The method uses multi-query attention to capture multiple periodicities efficiently.

Result: PENGUIN achieves superior performance, consistently surpassing both MLP-based and Transformer-based models across various benchmarks.

Conclusion: Explicit modeling of periodic structures is crucial for time series forecasting, and PENGUIN offers a simple yet effective architecture to improve performance in this domain.

Abstract: Long-term time series forecasting (LTSF) is a fundamental task with
wide-ranging applications. Although Transformer-based models have made
significant breakthroughs in forecasting, their effectiveness for time series
forecasting remains debatable. In this paper, we revisit the significance of
self-attention and propose a simple yet effective mechanism, Periodic-Nested
Group Attention, namely PENGUIN. Our approach highlights the importance of
explicitly modeling periodic patterns and incorporating relative attention bias
for effective time series modeling. To this end, we introduce a periodic-nested
relative attention bias that captures periodic structures directly. To handle
multiple coexisting periodicities (e.g., daily and weekly cycles), we design a
grouped attention mechanism, where each group targets a specific periodicity
using a multi-query attention mechanism. Extensive experiments across diverse
benchmarks demonstrate that PENGUIN consistently outperforms both MLP-based and
Transformer-based models.

</details>


### [239] [Communication-Efficient Federated Learning with Adaptive Number of Participants](https://arxiv.org/abs/2508.13803)
*Sergey Skorik,Vladislav Dorofeev,Gleb Molodtsov,Aram Avetisyan,Dmitry Bylinkin,Daniil Medyakov,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: The paper introduces Intelligent Selection of Participants (ISP), an adaptive mechanism for Federated Learning (FL) that reduces communication costs by optimizing the number of clients per training round.


<details>
  <summary>Details</summary>
Motivation: Federated Learning faces communication efficiency challenges, especially with heterogeneous and dynamic client participation. Existing strategies don't adequately address the optimal number of clients in training rounds.

Method: ISP dynamically selects the optimal number of clients per training round, aiming for better communication efficiency while maintaining model accuracy.

Result: ISP demonstrated up to 30% communication savings and preserved final model quality across diverse setups including vision transformers and real-world ECG classification.

Conclusion: ISP addresses an overlooked problem in FL, showing it is possible to enhance communication efficiency without sacrificing model accuracy, positioning client number selection as an important task in FL implementations.

Abstract: Rapid scaling of deep learning models has enabled performance gains across
domains, yet it introduced several challenges. Federated Learning (FL) has
emerged as a promising framework to address these concerns by enabling
decentralized training. Nevertheless, communication efficiency remains a key
bottleneck in FL, particularly under heterogeneous and dynamic client
participation. Existing methods, such as FedAvg and FedProx, or other
approaches, including client selection strategies, attempt to mitigate
communication costs. However, the problem of choosing the number of clients in
a training round remains extremely underexplored. We introduce Intelligent
Selection of Participants (ISP), an adaptive mechanism that dynamically
determines the optimal number of clients per round to enhance communication
efficiency without compromising model accuracy. We validate the effectiveness
of ISP across diverse setups, including vision transformers, real-world ECG
classification, and training with gradient compression. Our results show
consistent communication savings of up to 30\% without losing the final
quality. Applying ISP to different real-world ECG classification setups
highlighted the selection of the number of clients as a separate task of
federated learning.

</details>


### [240] [Reinforcement Learning-based Adaptive Path Selection for Programmable Networks](https://arxiv.org/abs/2508.13806)
*José Eduardo Zerna Torres,Marios Avgeris,Chrysa Papagianni,Gergely Pongrácz,István Gódor,Paola Grosso*

Main category: cs.LG

TL;DR: This paper introduces a system that combines Stochastic Learning Automata (SLA) with real-time network telemetry to enable adaptive and distributed path selection in programmable networks.


<details>
  <summary>Details</summary>
Motivation: Current network path selection methods lack dynamic adaptability to changing congestion conditions, prompting the need for distributed data-driven solutions.

Method: The authors propose an in-network reinforcement learning (IN-RL) system that leverages SLA algorithms and telemetry data collected via In-Band Network Telemetry (INT) for real-time decision-making.

Result: The system, implemented on a Mininet testbed with P4-programmed BMv2 switches, is shown to converge to effective path selections and adapt dynamically to network changes.

Conclusion: The proposed SLA-driven IN-RL framework proves effective in enabling dynamic, adaptive path selection in programmable networks, showcasing its potential for real-world applications.

Abstract: This work presents a proof-of-concept implementation of a distributed,
in-network reinforcement learning (IN-RL) framework for adaptive path selection
in programmable networks. By combining Stochastic Learning Automata (SLA) with
real-time telemetry data collected via In-Band Network Telemetry (INT), the
proposed system enables local, data-driven forwarding decisions that adapt
dynamically to congestion conditions. The system is evaluated on a
Mininet-based testbed using P4-programmable BMv2 switches, demonstrating how
our SLA-based mechanism converges to effective path selections and adapts to
shifting network conditions at line rate.

</details>


### [241] [Assessing Trustworthiness of AI Training Dataset using Subjective Logic -- A Use Case on Bias](https://arxiv.org/abs/2508.13813)
*Koffi Ismael Ouattara,Ioannis Krontiris,Theo Dimitrakos,Frank Kargl*

Main category: cs.LG

TL;DR: The paper presents a formal framework to assess the trustworthiness of AI training datasets, focusing on dataset-level properties like bias using Subjective Logic.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the need for evaluating properties like fairness or bias in datasets, as these critically impact AI systems but have not been adequately assessed at the dataset level in prior work.

Method: The authors develop a framework based on Subjective Logic that evaluates dataset-level trustworthiness properties in uncertainty-intensive environments and apply it to assess bias in a traffic sign recognition dataset.

Result: The proposed method effectively captures class imbalance and exhibits robustness and interpretability in both centralized and federated settings.

Conclusion: The introduced framework enhances the evaluation of dataset properties like bias, improving AI training dataset trustworthiness assessments.

Abstract: As AI systems increasingly rely on training data, assessing dataset
trustworthiness has become critical, particularly for properties like fairness
or bias that emerge at the dataset level. Prior work has used Subjective Logic
to assess trustworthiness of individual data, but not to evaluate
trustworthiness properties that emerge only at the level of the dataset as a
whole. This paper introduces the first formal framework for assessing the
trustworthiness of AI training datasets, enabling uncertainty-aware evaluations
of global properties such as bias. Built on Subjective Logic, our approach
supports trust propositions and quantifies uncertainty in scenarios where
evidence is incomplete, distributed, and/or conflicting. We instantiate this
framework on the trustworthiness property of bias, and we experimentally
evaluate it based on a traffic sign recognition dataset. The results
demonstrate that our method captures class imbalance and remains interpretable
and robust in both centralized and federated contexts.

</details>


### [242] [One Shot vs. Iterative: Rethinking Pruning Strategies for Model Compression](https://arxiv.org/abs/2508.13836)
*Mikołaj Janusz,Tomasz Wojnar,Yawei Li,Luca Benini,Kamil Adamczewski*

Main category: cs.LG

TL;DR: The paper systematically compares one-shot and iterative pruning methods in neural network compression, highlighting their respective strengths and introducing a hybrid approach.


<details>
  <summary>Details</summary>
Motivation: The motivation is to rigorously benchmark and compare one-shot and iterative pruning, as iterative pruning's preference has been historically assumed without sufficient testing.

Method: The authors conducted a comprehensive comparison, defining both pruning methods rigorously and benchmarking them under structured and unstructured setups, using different criteria and modalities.

Result: One-shot pruning excels at lower pruning ratios, while iterative pruning is superior at higher ratios. A hybrid pruning approach introduced in the paper shows potential to outperform traditional methods.

Conclusion: The study provides a nuanced understanding of pruning strategies, advocating for patience-based pruning and the hybrid approach to better align with specific goals and constraints.

Abstract: Pruning is a core technique for compressing neural networks to improve
computational efficiency. This process is typically approached in two ways:
one-shot pruning, which involves a single pass of training and pruning, and
iterative pruning, where pruning is performed over multiple cycles for
potentially finer network refinement. Although iterative pruning has
historically seen broader adoption, this preference is often assumed rather
than rigorously tested. Our study presents one of the first systematic and
comprehensive comparisons of these methods, providing rigorous definitions,
benchmarking both across structured and unstructured settings, and applying
different pruning criteria and modalities. We find that each method has
specific advantages: one-shot pruning proves more effective at lower pruning
ratios, while iterative pruning performs better at higher ratios. Building on
these findings, we advocate for patience-based pruning and introduce a hybrid
approach that can outperform traditional methods in certain scenarios,
providing valuable insights for practitioners selecting a pruning strategy
tailored to their goals and constraints. Source code is available at
https://github.com/janumiko/pruning-benchmark.

</details>


### [243] [FedUP: Efficient Pruning-based Federated Unlearning for Model Poisoning Attacks](https://arxiv.org/abs/2508.13853)
*Nicolò Romandini,Cristian Borcea,Rebecca Montanari,Luca Foschini*

Main category: cs.LG

TL;DR: The paper introduces FedUP, a lightweight federated unlearning algorithm that mitigates the influence of malicious clients in federated learning by pruning malicious weight updates, achieving robust performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Federated Learning is vulnerable to model poisoning attacks, and existing Federated Unlearning methods assume trusted and cooperative clients, which isn't feasible when dealing with malicious and colluding clients.

Method: FedUP isolates and zeroes out high-magnitude weights that diverge the most between updates from malicious and benign clients. The process relies only on the last training round's weights to ensure efficiency.

Result: FedUP reduces the malicious influence in strong adversarial scenarios, including up to 50%-1 malicious clients, and under varied attack models (label-flipping, backdoor). It achieves comparable performance to retrained-from-scratch models while outperforming state-of-the-art in speed and storage efficiency.

Conclusion: FedUP effectively mitigates malicious client impact in federated learning, maintaining benign data performance while reducing malicious accuracy to baseline levels, with efficiency advantages over current approaches.

Abstract: Federated Learning (FL) can be vulnerable to attacks, such as model
poisoning, where adversaries send malicious local weights to compromise the
global model. Federated Unlearning (FU) is emerging as a solution to address
such vulnerabilities by selectively removing the influence of detected
malicious contributors on the global model without complete retraining.
However, unlike typical FU scenarios where clients are trusted and cooperative,
applying FU with malicious and possibly colluding clients is challenging
because their collaboration in unlearning their data cannot be assumed. This
work presents FedUP, a lightweight FU algorithm designed to efficiently
mitigate malicious clients' influence by pruning specific connections within
the attacked model. Our approach achieves efficiency by relying only on
clients' weights from the last training round before unlearning to identify
which connections to inhibit. Isolating malicious influence is non-trivial due
to overlapping updates from benign and malicious clients. FedUP addresses this
by carefully selecting and zeroing the highest magnitude weights that diverge
the most between the latest updates from benign and malicious clients while
preserving benign information. FedUP is evaluated under a strong adversarial
threat model, where up to 50%-1 of the clients could be malicious and have full
knowledge of the aggregation process. We demonstrate the effectiveness,
robustness, and efficiency of our solution through experiments across IID and
Non-IID data, under label-flipping and backdoor attacks, and by comparing it
with state-of-the-art (SOTA) FU solutions. In all scenarios, FedUP reduces
malicious influence, lowering accuracy on malicious data to match that of a
model retrained from scratch while preserving performance on benign data. FedUP
achieves effective unlearning while consistently being faster and saving
storage compared to the SOTA.

</details>


### [244] [A Comprehensive Re-Evaluation of Biometric Modality Properties in the Modern Era](https://arxiv.org/abs/2508.13874)
*Rouqaiah Al-Refai,Pankaja Priya Ramasamy,Ragini Ramesh,Patricia Arias-Cabarcos,Philipp Terhörst*

Main category: cs.LG

TL;DR: The paper evaluates the suitability of biometric modalities for authentication systems, updating outdated frameworks by conducting an expert survey and analyzing 55 datasets.


<details>
  <summary>Details</summary>
Motivation: Current evaluation frameworks for biometric modalities, particularly the widely-used 1998 comparative table, are outdated and fail to account for technological advances and new vulnerabilities.

Method: The study involved a survey of 24 biometric experts to reassess the properties of various biometric modalities and compared their ratings with dataset-level uncertainties from 55 biometric datasets.

Result: The study found significant changes in property ratings across modalities, such as improved ratings for face recognition and decreased reliability for fingerprints, reflecting technological progress and emerging vulnerabilities respectively. There was strong alignment between expert ratings and dataset-level evidence.

Conclusion: The research underscores the value of combining expert insights with empirical evidence, identifies open challenges in biometric systems, and provides direction for improving evaluation frameworks and future research.

Abstract: The rapid advancement of authentication systems and their increasing reliance
on biometrics for faster and more accurate user verification experience,
highlight the critical need for a reliable framework to evaluate the
suitability of biometric modalities for specific applications. Currently, the
most widely known evaluation framework is a comparative table from 1998, which
no longer adequately captures recent technological developments or emerging
vulnerabilities in biometric systems. To address these challenges, this work
revisits the evaluation of biometric modalities through an expert survey
involving 24 biometric specialists. The findings indicate substantial shifts in
property ratings across modalities. For example, face recognition, shows
improved ratings due to technological progress, while fingerprint, shows
decreased reliability because of emerging vulnerabilities and attacks. Further
analysis of expert agreement levels across rated properties highlighted the
consistency of the provided evaluations and ensured the reliability of the
ratings. Finally, expert assessments are compared with dataset-level
uncertainty across 55 biometric datasets, revealing strong alignment in most
modalities and underscoring the importance of integrating empirical evidence
with expert insight. Moreover, the identified expert disagreements reveal key
open challenges and help guide future research toward resolving them.

</details>


### [245] [Fisher-Orthogonal Projection Methods for Natural Gradient Descent with Large Batches](https://arxiv.org/abs/2508.13898)
*Yishun Lu,Wesley Armour*

Main category: cs.LG

TL;DR: Current optimizers struggle with large batch sizes in GPUs. Fisher-Orthogonal Projection (FOP) is introduced to address this by improving generalization and convergence.


<details>
  <summary>Details</summary>
Motivation: Large batch sizes on modern GPUs diminish gradient noise, limiting the effectiveness of first-order methods to escape suboptimal minima. Second-order methods also lose advantages due to the need for high damping.

Method: FOP constructs a variance-aware update by using gradients from two sub-batches, combining the average gradient with an orthogonal component under the Fisher-metric.

Result: The method restores effectiveness of second-order training for large batch sizes, facilitating scalability and improved outcomes.

Conclusion: FOP overcomes limitations of traditional optimization techniques at large batch sizes, leading to enhanced training scalability and efficiency.

Abstract: Modern GPUs are equipped with large amounts of high-bandwidth memory,
enabling them to support mini-batch sizes of up to tens of thousands of
training samples. However, most existing optimizers struggle to perform
effectively at such a large batch size. As batch size increases, gradient noise
decreases due to averaging over many samples, limiting the ability of
first-order methods to escape sharp or suboptimal minima and reach the global
minimum. Meanwhile, second-order methods like the natural gradient with
Kronecker-Factored Approximate Curvature (KFAC) often require excessively high
damping to remain stable at large batch sizes. This high damping effectively
washes out the curvature information that gives these methods their advantage,
reducing their performance to that of simple gradient descent. In this paper,
we introduce Fisher-Orthogonal Projection (FOP), a novel technique that
restores the effectiveness of the second-order method at very large batch
sizes, enabling scalable training with improved generalization and faster
convergence. FOP constructs a variance-aware update direction by leveraging
gradients from two sub-batches, enhancing the average gradient with a component
of the gradient difference that is orthogonal to the average under the
Fisher-metric.

</details>


### [246] [Revisiting Diffusion Q-Learning: From Iterative Denoising to One-Step Action Generation](https://arxiv.org/abs/2508.13904)
*Thanh Nguyen,Chang D. Yoo*

Main category: cs.LG

TL;DR: This paper introduces One-Step Flow Q-Learning (OFQL), a new framework that replaces the multi-step action generation process in Diffusion Q-Learning (DQL) with a one-step approach, improving performance and efficiency in offline reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Diffusion Q-Learning (DQL) achieves impressive results in offline reinforcement learning but suffers from inefficiencies due to its reliance on multi-step denoising for action generation, making it impractical for broader applications.

Method: The authors propose OFQL, which reformulates DQL using the Flow Matching (FM) framework. It directly learns an average velocity field for one-step action generation, eliminating the need for auxiliary models, complex distillations, or multi-phase training.

Result: OFQL demonstrates superior performance over DQL and other diffusion-based models on the D4RL benchmark. It also significantly reduces training and inference time compared to DQL, showcasing both robustness and efficiency.

Conclusion: OFQL addresses the core limitations of DQL by enabling efficient one-step action generation and delivering faster training and inference without compromising on performance, establishing itself as a superior alternative.

Abstract: The generative power of diffusion models (DMs) has recently enabled
high-performing decision-making algorithms in offline reinforcement learning
(RL), achieving state-of-the-art results across standard benchmarks. Among
them, Diffusion Q-Learning (DQL) stands out as a leading method for its
consistently strong performance. Nevertheless, DQL remains limited in practice
due to its reliance on multi-step denoising for action generation during both
training and inference. Although one-step denoising is desirable, simply
applying it to DQL leads to a drastic performance drop. In this work, we
revisit DQL and identify its core limitations. We then propose One-Step Flow
Q-Learning (OFQL), a novel framework that enables efficient one-step action
generation during both training and inference, without requiring auxiliary
models, distillation, or multi-phase training. Specifically, OFQL reformulates
DQL within the sample-efficient Flow Matching (FM) framework. While
conventional FM induces curved generative trajectories that impede one-step
generation, OFQL instead learns an average velocity field that facilitates
direct, accurate action generation. Collectively, OFQL eliminates the need for
multi-step sampling and recursive gradient updates in DQL, resulting in faster
and more robust training and inference. Extensive experiments on the D4RL
benchmark demonstrate that OFQL outperforms DQL and other diffusion-based
baselines, while substantially reducing both training and inference time
compared to DQL.

</details>


### [247] [Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management](https://arxiv.org/abs/2508.13905)
*Tianheng Ling,Vipin Singh,Chao Qian,Felix Biessmann,Gregor Schiele*

Main category: cs.LG

TL;DR: The paper addresses the challenge of forecasting sewer overflow levels using energy-efficient AI models deployable on edge devices, prioritizing resilience and accuracy.


<details>
  <summary>Details</summary>
Motivation: The acceleration of extreme weather events due to climate change threatens aging combined sewer systems with untreated wastewater overflow, necessitating highly accurate forecasting for early intervention.

Method: The authors introduced edge-computable AI models, combining lightweight Transformer and LSTM architectures, compressed using integer-only quantization. They also optimized deployment via a hardware-aware pipeline targeting AMD Spartan-7 XC7S15 FPGA.

Result: The proposed 8-bit Transformer model achieves high predictive accuracy with an MSE of 0.0376 at 0.370 mJ energy per inference, while the 8-bit LSTM model is more energy-efficient (0.009 mJ) but less accurate (MSE 0.0432).

Conclusion: Balancing energy efficiency and predictive accuracy is crucial depending on deployment priorities, enabling local forecasting that enhances the resilience of sewer systems.

Abstract: Extreme weather events, intensified by climate change, increasingly challenge
aging combined sewer systems, raising the risk of untreated wastewater
overflow. Accurate forecasting of sewer overflow basin filling levels can
provide actionable insights for early intervention, helping mitigating
uncontrolled discharge. In recent years, AI-based forecasting methods have
offered scalable alternatives to traditional physics-based models, but their
reliance on cloud computing limits their reliability during communication
outages. To address this, we propose an end-to-end forecasting framework that
enables energy-efficient inference directly on edge devices. Our solution
integrates lightweight Transformer and Long Short-Term Memory (LSTM) models,
compressed via integer-only quantization for efficient on-device execution.
Moreover, an automated hardware-aware deployment pipeline is used to search for
optimal model configurations by jointly minimizing prediction error and energy
consumption on an AMD Spartan-7 XC7S15 FPGA. Evaluated on real-world sewer
data, the selected 8-bit Transformer model, trained on 24 hours of historical
measurements, achieves high accuracy (MSE 0.0376) at an energy cost of 0.370 mJ
per inference. In contrast, the optimal 8-bit LSTM model requires significantly
less energy (0.009 mJ, over 40x lower) but yields 14.89% worse accuracy (MSE
0.0432) and much longer training time. This trade-off highlights the need to
align model selection with deployment priorities, favoring LSTM for ultra-low
energy consumption or Transformer for higher predictive accuracy. In general,
our work enables local, energy-efficient forecasting, contributing to more
resilient combined sewer systems. All code can be found in the GitHub
Repository (https://github.com/tianheng-ling/EdgeOverflowForecast).

</details>


### [248] [Categorical Policies: Multimodal Policy Learning and Exploration in Continuous Control](https://arxiv.org/abs/2508.13922)
*SM Mazharul Islam,Manfred Huber*

Main category: cs.LG

TL;DR: A novel reinforcement learning method uses Categorical Policies for multimodal exploration in continuous control settings, outperforming traditional Gaussian-based approaches.


<details>
  <summary>Details</summary>
Motivation: To address limitations of unimodal Gaussian-based policies in reinforcement learning, which hinder exploration and adaptation in complex and sparse-reward environments.

Method: Introduce Categorical Policies using a categorical distribution for multimodal behavior modeling, combined with differentiable sampling techniques for optimized learning.

Result: Evaluated on DeepMind Control Suite environments, the proposed approach demonstrated faster convergence and superior performance compared to Gaussian policies.

Conclusion: Categorical Policies enhance structured exploration and multimodal behavior in continuous control tasks.

Abstract: A policy in deep reinforcement learning (RL), either deterministic or
stochastic, is commonly parameterized as a Gaussian distribution alone,
limiting the learned behavior to be unimodal. However, the nature of many
practical decision-making problems favors a multimodal policy that facilitates
robust exploration of the environment and thus to address learning challenges
arising from sparse rewards, complex dynamics, or the need for strategic
adaptation to varying contexts. This issue is exacerbated in continuous control
domains where exploration usually takes place in the vicinity of the predicted
optimal action, either through an additive Gaussian noise or the sampling
process of a stochastic policy. In this paper, we introduce Categorical
Policies to model multimodal behavior modes with an intermediate categorical
distribution, and then generate output action that is conditioned on the
sampled mode. We explore two sampling schemes that ensure differentiable
discrete latent structure while maintaining efficient gradient-based
optimization. By utilizing a latent categorical distribution to select the
behavior mode, our approach naturally expresses multimodality while remaining
fully differentiable via the sampling tricks. We evaluate our multimodal policy
on a set of DeepMind Control Suite environments, demonstrating that through
better exploration, our learned policies converge faster and outperform
standard Gaussian policies. Our results indicate that the Categorical
distribution serves as a powerful tool for structured exploration and
multimodal behavior representation in continuous control.

</details>


### [249] [How Usable is Automated Feature Engineering for Tabular Data?](https://arxiv.org/abs/2508.13932)
*Bastian Schäfer,Lennart Purucker,Maciej Janowski,Frank Hutter*

Main category: cs.LG

TL;DR: The paper examines 53 automated feature engineering (AutoFE) methods, concluding that they often lack usability, proper documentation, and user support, while also missing flexibility for resource constraints.


<details>
  <summary>Details</summary>
Motivation: To evaluate the usability and practical limitations of existing automated feature engineering methods for practitioners.

Method: The authors reviewed and analyzed 53 existing AutoFE methods to assess aspects like usability, documentation, community support, and adaptability to resource constraints.

Result: They found that most AutoFE methods are challenging to use, poorly documented, lack active user communities, and do not provide options to set time or memory constraints.

Conclusion: Future research should focus on creating more user-friendly, well-documented AutoFE methods with the ability to handle resource constraints effectively.

Abstract: Tabular data, consisting of rows and columns, is omnipresent across various
machine learning applications. Each column represents a feature, and features
can be combined or transformed to create new, more informative features. Such
feature engineering is essential to achieve peak performance in machine
learning. Since manual feature engineering is expensive and time-consuming, a
substantial effort has been put into automating it. Yet, existing automated
feature engineering (AutoFE) methods have never been investigated regarding
their usability for practitioners. Thus, we investigated 53 AutoFE methods. We
found that these methods are, in general, hard to use, lack documentation, and
have no active communities. Furthermore, no method allows users to set time and
memory constraints, which we see as a necessity for usable automation. Our
survey highlights the need for future work on usable, well-engineered AutoFE
methods.

</details>


### [250] [Convergent Reinforcement Learning Algorithms for Stochastic Shortest Path Problem](https://arxiv.org/abs/2508.13963)
*Soumyajit Guin,Shalabh Bhatnagar*

Main category: cs.LG

TL;DR: The paper introduces new algorithms for solving the Stochastic Shortest Path problem in both tabular and function approximation settings and demonstrates their convergence and performance.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges of reinforcement learning cost-criteria by focusing on the Stochastic Shortest Path problem, which is a fundamental framework in RL.

Method: Proposed two algorithms for tabular RL and one algorithm for function approximation, rigorously proving their asymptotic almost-sure convergence.

Result: The algorithms show superior performance in tabular settings and reliable outcomes in function approximation scenarios compared to existing solutions.

Conclusion: The paper concludes the proposed algorithms are effective for handling SSP problems, with strong convergence guarantees and competitive performance across compared settings.

Abstract: In this paper we propose two algorithms in the tabular setting and an
algorithm for the function approximation setting for the Stochastic Shortest
Path (SSP) problem. SSP problems form an important class of problems in
Reinforcement Learning (RL), as other types of cost-criteria in RL can be
formulated in the setting of SSP. We show asymptotic almost-sure convergence
for all our algorithms. We observe superior performance of our tabular
algorithms compared to other well-known convergent RL algorithms. We further
observe reliable performance of our function approximation algorithm compared
to other algorithms in the function approximation setting.

</details>


### [251] [AutoScale: Linear Scalarization Guided by Multi-Task Optimization Metrics](https://arxiv.org/abs/2508.13979)
*Yi Yang,Kei Ikemura,Qingwen Zhang,Xiaomeng Zhu,Ci Li,Nazre Batool,Sina Sharif Mansouri,John Folkesson*

Main category: cs.LG

TL;DR: The paper explores linear scalarization in multi-task learning, introducing a metric-driven weight selection method, AutoScale, achieving efficient performance without exhaustive search.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between linear scalarization weights and multi-task optimization (MTO) outcomes, and address the challenge of choosing optimal weights efficiently.

Method: Extensive experiments reveal trends in MTO metrics for effective scalarization, leading to the creation of AutoScale, a two-phase framework that uses these metrics to guide weight selection without exhaustive hyperparameter tuning.

Result: AutoScale demonstrates high efficiency and superior performance across various datasets, including a new large-scale benchmark.

Conclusion: The study bridges the gap between linear scalarization and MTO, providing metric-driven insights and introducing AutoScale as a practical solution for weight selection in multi-task learning.

Abstract: Recent multi-task learning studies suggest that linear scalarization, when
using well-chosen fixed task weights, can achieve comparable to or even better
performance than complex multi-task optimization (MTO) methods. It remains
unclear why certain weights yield optimal performance and how to determine
these weights without relying on exhaustive hyperparameter search. This paper
establishes a direct connection between linear scalarization and MTO methods,
revealing through extensive experiments that well-performing scalarization
weights exhibit specific trends in key MTO metrics, such as high gradient
magnitude similarity. Building on this insight, we introduce AutoScale, a
simple yet effective two-phase framework that uses these MTO metrics to guide
weight selection for linear scalarization, without expensive weight search.
AutoScale consistently shows superior performance with high efficiency across
diverse datasets including a new large-scale benchmark.

</details>


### [252] [Formal Algorithms for Model Efficiency](https://arxiv.org/abs/2508.14000)
*Naman Tyagi,Srishti Das,Kunal,Vatsal Gupta*

Main category: cs.LG

TL;DR: The Knob-Meter-Rule (KMR) framework is proposed to unify model efficiency techniques in deep learning, offering systematic optimization, modularity, and insights into cost-quality trade-offs.


<details>
  <summary>Details</summary>
Motivation: To consolidate diverse deep learning efficiency methods into a unified formalism for better understanding, application, and exploration of new research directions.

Method: The KMR framework abstracts various efficiency techniques into "knobs," "meters," and "rules," enabling systematic optimization via the Budgeted-KMR algorithm and facilitating hybrid pipelines.

Result: The paper demonstrates how efficiency techniques can be represented as KMR triples and provides templates for algorithmic implementation. It establishes links between methods and showcases the framework's versatility.

Conclusion: KMR serves as both a practical and conceptual tool for optimizing model efficiency in deep learning and opens avenues for future research in policy automation and theoretical approaches to cost-quality analysis.

Abstract: We introduce the Knob-Meter-Rule (KMR) framework, a unified formalism for
representing and reasoning about model efficiency techniques in deep learning.
By abstracting diverse methods, including pruning, quantization, knowledge
distillation, and parameter-efficient architectures, into a consistent set of
controllable knobs, deterministic rules, and measurable meters, KMR provides a
mathematically precise and modular perspective on efficiency optimization. The
framework enables systematic composition of multiple techniques, flexible
policy-driven application, and iterative budgeted optimization through the
Budgeted-KMR algorithm. We demonstrate how well-known efficiency methods can be
instantiated as KMR triples and present concise algorithmic templates for each.
The framework highlights underlying relationships between methods, facilitates
hybrid pipelines, and lays the foundation for future research in automated
policy learning, dynamic adaptation, and theoretical analysis of cost-quality
trade-offs. Overall, KMR offers both a conceptual and practical tool for
unifying and advancing model efficiency research.

</details>


### [253] [GDNSQ: Gradual Differentiable Noise Scale Quantization for Low-bit Neural Networks](https://arxiv.org/abs/2508.14004)
*Sergey Salishev,Ian Akhremchik*

Main category: cs.LG

TL;DR: The paper proposes a method for training quantized neural networks effectively under constrained bit-width settings.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the issue of capacity reduction and quantization bottlenecks that occur in quantized neural networks as average bit-width decreases.

Method: They adopt a smooth, constrained optimization approach using a differentiable Straight-Through Estimator (STE) with learnable parameters, enforced by an exterior-point penalty and stabilized with distillation.

Result: Their method achieves competitive performance even in extreme quantization scenarios like W1A1 (1-bit weights, 1-bit activations), maintaining efficiency.

Conclusion: The work introduces a simple yet effective strategy for addressing quantization bottlenecks, allowing quantized neural networks to retain high performance despite aggressive bit-width reductions.

Abstract: Quantized neural networks can be viewed as a chain of noisy channels, where
rounding in each layer reduces capacity as bit-width shrinks; the
floating-point (FP) checkpoint sets the maximum input rate. We track capacity
dynamics as the average bit-width decreases and identify resulting quantization
bottlenecks by casting fine-tuning as a smooth, constrained optimization
problem. Our approach employs a fully differentiable Straight-Through Estimator
(STE) with learnable bit-width, noise scale and clamp bounds, and enforces a
target bit-width via an exterior-point penalty; mild metric smoothing (via
distillation) stabilizes training. Despite its simplicity, the method attains
competitive accuracy down to the extreme W1A1 setting while retaining the
efficiency of STE.

</details>


### [254] [ASDFormer: A Transformer with Mixtures of Pooling-Classifier Experts for Robust Autism Diagnosis and Biomarker Discovery](https://arxiv.org/abs/2508.14005)
*Mohammad Izadi,Mehran Safayani*

Main category: cs.LG

TL;DR: The paper introduces ASDFormer, a Transformer-based model with expert branches, to improve Autism Spectrum Disorder (ASD) diagnosis and biomarker discovery using fMRI data.


<details>
  <summary>Details</summary>
Motivation: ASD is characterized by disrupted brain connectivity, and better tools are needed to identify altered connectivity patterns and biomarkers.

Method: The authors propose ASDFormer, a Transformer model with a Mixture of Pooling-Classifier Experts (MoE), to capture relevant brain connectivity features from fMRI data.

Result: ASDFormer achieves state-of-the-art accuracy in diagnosing ASD using the ABIDE dataset, while also identifying key functional connectivity disruptions.

Conclusion: ASDFormer demonstrates improved diagnostic accuracy and offers insights into neural connectivity changes in ASD, underscoring its potential for biomarker development.

Abstract: Autism Spectrum Disorder (ASD) is a complex neurodevelopmental condition
marked by disruptions in brain connectivity. Functional MRI (fMRI) offers a
non-invasive window into large-scale neural dynamics by measuring
blood-oxygen-level-dependent (BOLD) signals across the brain. These signals can
be modeled as interactions among Regions of Interest (ROIs), which are grouped
into functional communities based on their underlying roles in brain function.
Emerging evidence suggests that connectivity patterns within and between these
communities are particularly sensitive to ASD-related alterations. Effectively
capturing these patterns and identifying interactions that deviate from typical
development is essential for improving ASD diagnosis and enabling biomarker
discovery. In this work, we introduce ASDFormer, a Transformer-based
architecture that incorporates a Mixture of Pooling-Classifier Experts (MoE) to
capture neural signatures associated with ASD. By integrating multiple
specialized expert branches with attention mechanisms, ASDFormer adaptively
emphasizes different brain regions and connectivity patterns relevant to
autism. This enables both improved classification performance and more
interpretable identification of disorder-related biomarkers. Applied to the
ABIDE dataset, ASDFormer achieves state-of-the-art diagnostic accuracy and
reveals robust insights into functional connectivity disruptions linked to ASD,
highlighting its potential as a tool for biomarker discovery.

</details>


### [255] [Typed Topological Structures Of Datasets](https://arxiv.org/abs/2508.14008)
*Wanjun Hu*

Main category: cs.LG

TL;DR: A dataset $X$ on $R^2$ is explored using "typed topological spaces," enabling detailed inner structural analysis through novel concepts like tracks, components, and pseudotrees.


<details>
  <summary>Details</summary>
Motivation: The paper aims to expand the study of finite topological spaces by introducing typed topological spaces, offering new methods for analyzing datasets from a general topology perspective.

Method: The authors develop special types and a related typed topology for dataset $X$, organizing its structure into tracks, components, and branches. These structures are further analyzed using typed-II pseudotrees.

Result: This method reveals intricate relationships within dataset $X, such as organized tracks, ordered components, and pseudotree formations, offering new insights for tasks like clustering, anomaly detection, and computing convex hulls.

Conclusion: Typed topological spaces provide a robust framework for examining datasets, presenting opportunities for new algorithms to tackle complex geometric and topological problems efficiently.

Abstract: A datatset $X$ on $R^2$ is a finite topological space. Current research of a
dataset focuses on statistical methods and the algebraic topological method
\cite{carlsson}. In \cite{hu}, the concept of typed topological space was
introduced and showed to have the potential for studying finite topological
spaces, such as a dataset. It is a new method from the general topology
perspective. A typed topological space is a topological space whose open sets
are assigned types. Topological concepts and methods can be redefined using
open sets of certain types. In this article, we develop a special set of types
and its related typed topology on a dataset $X$. Using it, we can investigate
the inner structure of $X$. In particular, $R^2$ has a natural quotient space,
in which $X$ is organized into tracks, and each track is split into components.
Those components are in a order. Further, they can be represented by an integer
sequence. Components crossing tracks form branches, and the relationship can be
well represented by a type of pseudotree (called typed-II pseudotree). Such
structures provide a platform for new algorithms for problems such as
calculating convex hull, holes, clustering and anomaly detection.

</details>


### [256] [Efficient Knowledge Graph Unlearning with Zeroth-order Information](https://arxiv.org/abs/2508.14013)
*Yang Xiao,Ruimeng Ye,Bohan Liu,Xiaolong Ma,Bo Hui*

Main category: cs.LG

TL;DR: This paper proposes an efficient algorithm for unlearning knowledge graphs, addressing challenges related to large-scale KG structures and semantic relations using computationally effective approximations.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the increasing demand for unlearning training data from models, driven by regulations such as the Right to be Forgotten, while avoiding costly retraining.

Method: The method involves estimating model sensitivity and parameter changes via Taylor expansion, utilizing Fisher matrices and zeroth-order optimization to bypass computationally expensive derivatives.

Result: The proposed approach significantly outperformed state-of-the-art graph unlearning baselines in both efficiency and quality during the experiments.

Conclusion: The study introduces a practical solution for KG unlearning that addresses scalability and semantic challenges, demonstrating superior performance compared to existing methods.

Abstract: Due to regulations like the Right to be Forgotten, there is growing demand
for removing training data and its influence from models. Since full retraining
is costly, various machine unlearning methods have been proposed. In this
paper, we firstly present an efficient knowledge graph (KG) unlearning
algorithm. We remark that KG unlearning is nontrivial due to the distinctive
structure of KG and the semantic relations between entities. Also, unlearning
by estimating the influence of removed components incurs significant
computational overhead when applied to large-scale knowledge graphs. To this
end, we define an influence function for KG unlearning and propose to
approximate the model's sensitivity without expensive computation of
first-order and second-order derivatives for parameter updates. Specifically,
we use Taylor expansion to estimate the parameter changes caused by data
removal. Given that the first-order gradients and second-order derivatives
dominate the computational load, we use the Fisher matrices and zeroth-order
optimization to approximate the inverse-Hessian vector product without
constructing the computational graphs. Our experimental results demonstrate
that the proposed method outperforms other state-of-the-art graph unlearning
baselines significantly in terms of unlearning efficiency and unlearning
quality. Our code is released at https://github.com/NKUShaw/ZOWFKGIF.

</details>


### [257] [BLIPs: Bayesian Learned Interatomic Potentials](https://arxiv.org/abs/2508.14022)
*Dario Coscia,Pim de Haan,Max Welling*

Main category: cs.LG

TL;DR: The paper introduces BLIPs, a Bayesian neural network framework for interatomic potentials in chemistry, addressing challenges like data scarcity and lack of uncertainty estimates in standard MLIPs.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of Machine Learning Interatomic Potentials (MLIPs), specifically in out-of-distribution predictions, data-scarce environments, and their inability to provide uncertainty estimates in simulation-based chemistry.

Method: The authors propose a variational Bayesian framework named BLIP, which leverages an adaptive version of Variational Dropout. This framework is architecture-agnostic and works seamlessly with existing message-passing architectures while producing well-calibrated uncertainty estimates.

Result: BLIP improves predictive accuracy and delivers trustworthy uncertainty evaluations in simulation-based chemistry tasks, particularly in data-scarce and out-of-distribution scenarios. Fine-tuning existing MLIPs with BLIP also leads to consistent performance enhancements.

Conclusion: BLIP offers a practical and reliable enhancement to MLIPs by improving their accuracy and uncertainty estimation capabilities, making it suitable for challenging simulation environments and active learning applications.

Abstract: Machine Learning Interatomic Potentials (MLIPs) are becoming a central tool
in simulation-based chemistry. However, like most deep learning models, MLIPs
struggle to make accurate predictions on out-of-distribution data or when
trained in a data-scarce regime, both common scenarios in simulation-based
chemistry. Moreover, MLIPs do not provide uncertainty estimates by
construction, which are fundamental to guide active learning pipelines and to
ensure the accuracy of simulation results compared to quantum calculations. To
address this shortcoming, we propose BLIPs: Bayesian Learned Interatomic
Potentials. BLIP is a scalable, architecture-agnostic variational Bayesian
framework for training or fine-tuning MLIPs, built on an adaptive version of
Variational Dropout. BLIP delivers well-calibrated uncertainty estimates and
minimal computational overhead for energy and forces prediction at inference
time, while integrating seamlessly with (equivariant) message-passing
architectures. Empirical results on simulation-based computational chemistry
tasks demonstrate improved predictive accuracy with respect to standard MLIPs,
and trustworthy uncertainty estimates, especially in data-scarse or heavy
out-of-distribution regimes. Moreover, fine-tuning pretrained MLIPs with BLIP
yields consistent performance gains and calibrated uncertainties.

</details>


### [258] [Learning from Preferences and Mixed Demonstrations in General Settings](https://arxiv.org/abs/2508.14027)
*Jason R Brown,Carl Henrik Ek,Robert D Mullins*

Main category: cs.LG

TL;DR: This paper develops LEOPARD, a scalable algorithm to efficiently learn reward functions from human data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Complex tasks in reinforcement learning make it challenging to specify reward functions, necessitating reliance on human data like preferences or demonstrations.

Method: The paper proposes a framework called reward-rational partial orderings, optimizing learning from preferences and ranked demonstrations using the LEOPARD algorithm.

Result: LEOPARD demonstrates significant improvement over baselines in tasks using limited human feedback and verifies benefits of combining feedback types.

Conclusion: LEOPARD offers a scalable solution for combining diverse feedback types to enhance reinforcement learning performance across varied domains.

Abstract: Reinforcement learning is a general method for learning in sequential
settings, but it can often be difficult to specify a good reward function when
the task is complex. In these cases, preference feedback or expert
demonstrations can be used instead. However, existing approaches utilising both
together are often ad-hoc, rely on domain-specific properties, or won't scale.
We develop a new framing for learning from human data, \emph{reward-rational
partial orderings over observations}, designed to be flexible and scalable.
Based on this we introduce a practical algorithm, LEOPARD: Learning Estimated
Objectives from Preferences And Ranked Demonstrations. LEOPARD can learn from a
broad range of data, including negative demonstrations, to efficiently learn
reward functions across a wide range of domains. We find that when a limited
amount of preference and demonstration feedback is available, LEOPARD
outperforms existing baselines by a significant margin. Furthermore, we use
LEOPARD to investigate learning from many types of feedback compared to just a
single one, and find that combining feedback types is often beneficial.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [259] [Multi-Plasticity Synergy with Adaptive Mechanism Assignment for Training Spiking Neural Networks](https://arxiv.org/abs/2508.13673)
*Yuzhe Liu,Xin Deng,Qiang Yu*

Main category: cs.NE

TL;DR: The study introduces a training framework for Spiking Neural Networks (SNNs) using multiple synergistic learning mechanisms, achieving improved performance and robustness.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitation of current SNN training methods that rely on a single learning mechanism, which reduces adaptability and representation capabilities.

Method: The framework incorporates multiple synergistic plasticity mechanisms, enabling cooperative learning while maintaining the independent dynamics of each mechanism.

Result: The proposed framework significantly improves performance and robustness on static and dynamic datasets compared to traditional models.

Conclusion: This framework offers a biologically inspired and extensible foundation to develop more powerful and adaptive SNNs.

Abstract: Spiking Neural Networks (SNNs) are promising brain-inspired models known for
low power consumption and superior potential for temporal processing, but
identifying suitable learning mechanisms remains a challenge. Despite the
presence of multiple coexisting learning strategies in the brain, current SNN
training methods typically rely on a single form of synaptic plasticity, which
limits their adaptability and representational capability. In this paper, we
propose a biologically inspired training framework that incorporates multiple
synergistic plasticity mechanisms for more effective SNN training. Our method
enables diverse learning algorithms to cooperatively modulate the accumulation
of information, while allowing each mechanism to preserve its own relatively
independent update dynamics. We evaluated our approach on both static image and
dynamic neuromorphic datasets to demonstrate that our framework significantly
improves performance and robustness compared to conventional learning mechanism
models. This work provides a general and extensible foundation for developing
more powerful SNNs guided by multi-strategy brain-inspired learning.

</details>


### [260] [Encoding Optimization for Low-Complexity Spiking Neural Network Equalizers in IM/DD Systems](https://arxiv.org/abs/2508.13783)
*Eike-Manuel Edelmann,Alexander von Bank,Laurent Schmalen*

Main category: cs.NE

TL;DR: The paper introduces a reinforcement learning-based algorithm to optimize neural encoding parameters in spiking neural networks, achieving enhanced performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Neural encoding parameters for SNNs are generally set heuristically, leading to suboptimal designs and performance. The paper aims to address this by proposing a systematic optimization approach.

Method: The authors utilize a reinforcement learning algorithm to optimize neural encoding parameters in SNNs, rather than relying on heuristics.

Result: The method demonstrates improved computational efficiency, reduced network size, and enhanced performance in an SNN-based equalizer and demapper for IM/DD systems.

Conclusion: Optimizing SNN encoding parameters using reinforcement learning is an effective approach to improve both performance and efficiency, suggesting broader applications in other SNN-based systems.

Abstract: Neural encoding parameters for spiking neural networks (SNNs) are typically
set heuristically. We propose a reinforcement learning-based algorithm to
optimize them. Applied to an SNN-based equalizer and demapper in an IM/DD
system, the method improves performance while reducing computational load and
network size.

</details>


### [261] [Zobrist Hash-based Duplicate Detection in Symbolic Regression](https://arxiv.org/abs/2508.13859)
*Bogdan Burlacu*

Main category: cs.NE

TL;DR: The paper analyzes inefficiencies in Genetic Programming for symbolic regression and introduces a Zobrist hashing-based caching mechanism to optimize computational performance, achieving up to 34% faster runtimes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational inefficiencies in Genetic Programming due to the re-visitation and re-evaluation of points in the search space during symbolic regression.

Method: The paper adopted a caching strategy using Zobrist hashing, implemented within the Operon framework, to streamline the evolutionary search process.

Result: The proposed hashing-based caching mechanism demonstrated up to 34% speedups on real-world regression problems without compromising the quality of the search outcomes.

Conclusion: By integrating Zobrist hashing-based caching, the paper provides a simple yet effective solution to enhance runtime efficiency in symbolic regression while preserving search quality.

Abstract: Symbolic regression encompasses a family of search algorithms that aim to
discover the best fitting function for a set of data without requiring an a
priori specification of the model structure. The most successful and commonly
used technique for symbolic regression is Genetic Programming (GP), an
evolutionary search method that evolves a population of mathematical
expressions through the mechanism of natural selection. In this work we analyze
the efficiency of the evolutionary search in GP and show that many points in
the search space are re-visited and re-evaluated multiple times by the
algorithm, leading to wasted computational effort. We address this issue by
introducing a caching mechanism based on the Zobrist hash, a type of hashing
frequently used in abstract board games for the efficient construction and
subsequent update of transposition tables. We implement our caching approach
using the open-source framework Operon and demonstrate its performance on a
selection of real-world regression problems, where we observe up to 34\%
speedups without any detrimental effects on search quality. The hashing
approach represents a straightforward way to improve runtime performance while
also offering some interesting possibilities for adjusting search strategy
based on cached information.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [262] [Multi-Metric Algorithmic Complexity: Beyond Asymptotic Analysis](https://arxiv.org/abs/2508.13249)
*Sergii Kavun*

Main category: cs.PF

TL;DR: The paper introduces a weighted-operation complexity model for evaluating algorithms based on time, energy, carbon, and monetary costs, aiming to improve practical, architecture-aware comparisons.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional algorithm analysis methods that treat all operations equally, ignoring critical aspects like energy and cost differences on modern processors.

Method: Authors propose a model with weighted costs for instruction types across multiple dimensions, combining automated code analysis with validation techniques against measurable data.

Result: The model achieves high accuracy (ρ>0.9) in predicting time and energy costs and outperforms traditional baselines in multi-dimensional evaluations.

Conclusion: The weighted-operation complexity model enhances algorithm analysis by integrating performance, sustainability, and economic considerations, offering a practical complementary tool to theoretical models.

Abstract: Traditional algorithm analysis treats all basic operations as equally costly,
which hides significant differences in time, energy consumption, and cost
between different types of computations on modern processors. We propose a
weighted-operation complexity model that assigns realistic cost values to
different instruction types across multiple dimensions: computational effort,
energy usage, carbon footprint, and monetary cost. The model computes overall
efficiency scores based on user-defined priorities and can be applied through
automated code analysis or integrated with performance measurement tools. This
approach complements existing theoretical models by enabling practical,
architecture-aware algorithm comparisons that account for performance,
sustainability, and economic factors. We demonstrate an open-source
implementation that analyzes code, estimates multi-dimensional costs, and
provides efficiency recommendations across various algorithms. We address two
research questions: (RQ1) Can a multi-metric model predict time/energy with
high accuracy across architectures? (RQ2) How does it compare to baselines like
Big-O, ICE, and EVM gas? Validation shows strong correlations (\r{ho}>0.9) with
measured data, outperforming baselines in multi-objective scenarios.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [263] [Reactive Semantics for User Interface Description Languages](https://arxiv.org/abs/2508.13610)
*Basile Pesin,Celia Picard,Cyril Allignol*

Main category: cs.PL

TL;DR: The paper introduces a semantical foundation for Smalite, a core reactive User Interface Description Language (UIDL), to advance formal verification and compiler reliability.


<details>
  <summary>Details</summary>
Motivation: Existing UIDLs facilitate GUI development but lack sufficient formalization and verification mechanisms, despite their use in safety-critical systems.

Method: The authors propose a denotational semantic model for the core reactive UIDL, Smalite, capable of representing constructs from more complex UIDLs.

Result: The semantic model provides a structured foundation for future work on compiler verification and formal guarantees in UIDLs.

Conclusion: This work lays the groundwork for robust verification and advancement of reactive UIDLs like Smalite, enhancing reliability in safety-critical GUI applications.

Abstract: User Interface Description Languages (UIDLs) are high-level languages that
facilitate the development of Human-Machine Interfaces, such as Graphical User
Interface (GUI) applications. They usually provide first-class primitives to
specify how the program reacts to an external event (user input, network
message), and how data flows through the program. Although these
domain-specific languages are now widely used to implement safety-critical
GUIs, little work has been invested in their formalization and verification.
  In this paper, we propose a denotational semantic model for a core reactive
UIDL, Smalite, which we argue is expressive enough to encode constructs from
more realistic languages. This preliminary work may be used as a stepping stone
to produce a formally verified compiler for UIDLs.

</details>


### [264] [Bisimilarity and Simulatability of Processes Parameterized by Join Interactions](https://arxiv.org/abs/2508.13611)
*Clemens Grabmayer,Maurizio Murgia*

Main category: cs.PL

TL;DR: The paper explores a simplified variant of Larsen’s parameterized bisimilarity by introducing unrestricted join interactions, examining differences in equivalences and behavioral simulations between processes and environments.


<details>
  <summary>Details</summary>
Motivation: The aim is to generalize and weaken Larsen’s concept of parameterized bisimilarity by introducing join-interaction behaviors, allowing for broader equivalence relations under non-deterministic environments.

Method: The study compares traditional parameterized bisimilarity to join-interaction parameterized bisimilarity, provides equivalence characterizations, adapts the concept to simulatability and simulation preorders, and introduces modal-logic tools.

Result: Join-interaction parameterized bisimilarity is shown to coincide with parameterized bisimilarity in deterministic environments but differs in general cases. Similar equivalence results are obtained for simulatability. Modal-logic tools successfully characterize simulatability.

Conclusion: The framework provides richer discriminatory tools for analyzing process interactions in non-deterministic environments, recovering traditional concepts via determinization, and setting foundations for further study and open questions.

Abstract: Departing from Larsen's concept of parameterized bisimilarity of processes
with respect to interaction with environments, we start an exploration of its
natural weakening: bisimilarity of unrestricted join interactions with
environments. Parameterized bisimilarity relates processes p and q with respect
to an environment e if p and q behave bi-similarly while joining --
respectively the same -- transitions from e. The weakened variant relates
processes p and q with respect to environment e if the join-interaction
processes p & e and q & e of p and q with e are bisimilar. (Hereby join
interactions r & f facilitate a step with label a to r' & f' if and only if r
and f permit a-steps to r' and f' , respectively.) Join-interaction
parameterized (ji-parameterized) bisimilarity coincides with parameterized
bisimilarity for deterministic environments, but that it is a coarser
equivalence in general. We explain how Larsen's concept can be recovered from
ji-parameterized bisimilarity by 'determinizing' interactions. We show that by
adaptation to simulatability (simulation preorder) the same concept arises:
parameterized simulatability coincides with ji-parameterized simulatability.
For the discrimination preorder of (ji-)parameterized simulatability on
environments we obtain the same result as Larsen did for parameterized
bisimilarity. Also, we give a modal-logic characterization of
(ji-)parameterized simulatability. Finally we gather open problems, and provide
an outlook on our current related work.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [265] [Diff-MSM: Differentiable MusculoSkeletal Model for Simultaneous Identification of Human Muscle and Bone Parameters](https://arxiv.org/abs/2508.13303)
*Yingfan Zhou,Philip Sanderink,Sigurd Jager Lemming,Cheng Fang*

Main category: cs.RO

TL;DR: The paper introduces Diff-MSM, a differentiable model for creating personalized musculoskeletal models, and demonstrates its superior accuracy in parameter identification for muscles and bones without requiring direct joint torque measurements.


<details>
  <summary>Details</summary>
Motivation: Personalized musculoskeletal models are essential for safely simulating human-robot interactions in applications like rehabilitation and co-transportation. However, identifying subject-specific parameters is challenging due to the difficulty of measuring internal biomechanical variables, such as joint torques, in vivo.

Method: The paper proposes the Differentiable MusculoSkeletal Model (Diff-MSM), which uses end-to-end automatic differentiation. This method identifies muscle and bone parameters by differentiating measurable data, i.e., muscle activation and observable motion, bypassing the need for internal torque measurements.

Result: The proposed method significantly outperforms baseline methods, particularly in muscle parameter estimation. It achieved an average percentage error as low as 0.05% in simulations, starting from initial guesses reasonably close to ground truth values.

Conclusion: Diff-MSM offers a high-fidelity approach for personalized musculoskeletal modeling, enabling its application in fields like rehabilitation, muscle health monitoring, and sports science by addressing parameter identification challenges effectively.

Abstract: High-fidelity personalized human musculoskeletal models are crucial for
simulating realistic behavior of physically coupled human-robot interactive
systems and verifying their safety-critical applications in simulations before
actual deployment, such as human-robot co-transportation and rehabilitation
through robotic exoskeletons. Identifying subject-specific Hill-type muscle
model parameters and bone dynamic parameters is essential for a personalized
musculoskeletal model, but very challenging due to the difficulty of measuring
the internal biomechanical variables in vivo directly, especially the joint
torques. In this paper, we propose using Differentiable MusculoSkeletal Model
(Diff-MSM) to simultaneously identify its muscle and bone parameters with an
end-to-end automatic differentiation technique differentiating from the
measurable muscle activation, through the joint torque, to the resulting
observable motion without the need to measure the internal joint torques.
Through extensive comparative simulations, the results manifested that our
proposed method significantly outperformed the state-of-the-art baseline
methods, especially in terms of accurate estimation of the muscle parameters
(i.e., initial guess sampled from a normal distribution with the mean being the
ground truth and the standard deviation being 10% of the ground truth could end
up with an average of the percentage errors of the estimated values as low as
0.05%). In addition to human musculoskeletal modeling and simulation, the new
parameter identification technique with the Diff-MSM has great potential to
enable new applications in muscle health monitoring, rehabilitation, and sports
science.

</details>


### [266] [A Surveillance Based Interactive Robot](https://arxiv.org/abs/2508.13319)
*Kshitij Kavimandan,Pooja Mangal,Devanshi Mehta*

Main category: cs.RO

TL;DR: The paper details the development of a real-time mobile surveillance robot capable of video streaming and responding to voice commands.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create an affordable, replicable surveillance robot leveraging off-the-shelf hardware and open software to assist in monitoring and navigation tasks.

Method: The system uses two Raspberry Pi units, FFmpeg for video streaming, YOLOv3 for object detection, and Python libraries for speech recognition and multilingual interaction. A Kinect sensor is utilized for visual and obstacle cues.

Result: The robot performed successfully in indoor tests, detecting objects at interactive frame rates, processing voice commands reliably, and autonomously translating commands into actions.

Conclusion: The robot design is functional, reproducible, and expandable through practical extensions like sensor fusion, GPU acceleration, and face/text recognition.

Abstract: We build a mobile surveillance robot that streams video in real time and
responds to speech so a user can monitor and steer it from a phone or browser.
The system uses two Raspberry Pi 4 units: a front unit on a differential drive
base with camera, mic, and speaker, and a central unit that serves the live
feed and runs perception. Video is sent with FFmpeg. Objects in the scene are
detected using YOLOv3 to support navigation and event awareness. For voice
interaction, we use Python libraries for speech recognition, multilingual
translation, and text-to-speech, so the robot can take spoken commands and read
back responses in the requested language. A Kinect RGB-D sensor provides visual
input and obstacle cues. In indoor tests the robot detects common objects at
interactive frame rates on CPU, recognises commands reliably, and translates
them to actions without manual control. The design relies on off-the-shelf
hardware and open software, making it easy to reproduce. We discuss limits and
practical extensions, including sensor fusion with ultrasonic range data, GPU
acceleration, and adding face and text recognition.

</details>


### [267] [Incremental Generalized Hybrid A*](https://arxiv.org/abs/2508.13392)
*Sidharth Talia,Oren Salzman,Siddhartha Srinivasa*

Main category: cs.RO

TL;DR: The paper introduces Incremental Generalized Hybrid A* (IGHA*), an efficient tree-search framework for real-time planning in complex dynamic scenarios like off-road autonomy.


<details>
  <summary>Details</summary>
Motivation: Real-time planning in scenarios such as autonomous driving or off-road autonomy requires addressing efficiency problems in large tree searches, especially under complex dynamics.

Method: The authors propose the IGHA* framework, which dynamically organizes vertex expansions without rigid pruning, tackling the problems of boundary-value solutions and grid resolution issues.

Result: IGHA* uses 6x fewer expansions compared to optimized HA* and outperforms HA*M in simulated off-road experiments, showcasing real-time planning capabilities both in simulation and on a small off-road vehicle.

Conclusion: IGHA* is a robust, fast, and provably efficient alternative for complex dynamic planning scenarios, outperforming existing methods like HA*.

Abstract: We address the problem of efficiently organizing search over very large
trees, which arises in many applications ranging from autonomous driving to
aerial vehicles. Here, we are motivated by off-road autonomy, where real-time
planning is essential. Classical approaches use graphs of motion primitives and
exploit dominance to mitigate the curse of dimensionality and prune expansions
efficiently. However, for complex dynamics, repeatedly solving two-point
boundary-value problems makes graph construction too slow for fast kinodynamic
planning. Hybrid A* (HA*) addressed this challenge by searching over a tree of
motion primitives and introducing approximate pruning using a grid-based
dominance check. However, choosing the grid resolution is difficult: too coarse
risks failure, while too fine leads to excessive expansions and slow planning.
We propose Incremental Generalized Hybrid A* (IGHA*), an anytime tree-search
framework that dynamically organizes vertex expansions without rigid pruning.
IGHA* provably matches or outperforms HA*. For both on-road kinematic and
off-road kinodynamic planning queries for a car-like robot, variants of IGHA*
use 6x fewer expansions to the best solution compared to an optimized version
of HA*. In simulated off-road experiments in a high fidelity simulator, IGHA*
outperforms HA*M when both are used in the loop with a model predictive
controller. We demonstrate real-time performance both in simulation and on a
small-scale off-road vehicle, enabling fast, robust planning under complex
dynamics. Code: https://github.com/personalrobotics/IGHAStar

</details>


### [268] [Accelerating Signal-Temporal-Logic-Based Task and Motion Planning of Bipedal Navigation using Benders Decomposition](https://arxiv.org/abs/2508.13407)
*Jiming Ren,Xuan Lin,Roman Mineyev,Karen M. Feigh,Samuel Coogan,Ye Zhao*

Main category: cs.RO

TL;DR: The paper introduces a Benders Decomposition approach to handle Signal Temporal Logic constrained task and motion planning for bipedal locomotion.


<details>
  <summary>Details</summary>
Motivation: Task and motion planning under Signal Temporal Logic constraints is NP-hard, and existing mixed-integer programming approaches suffer from high computational complexity due to non-convex constraints like kinematic reachability.

Method: The authors propose using Benders Decomposition, which decomposes the problem into a master problem for planning and subproblems for kinematics and dynamics feasibility checks, employing an iterative cutting-plane technique.

Result: Experiments showed the proposed method delivers faster planning compared to other algorithms addressing the same optimization program with nonlinear constraints.

Conclusion: Benders Decomposition provides an efficient solution for previously intractable monolithic optimization problems in tasks involving task and motion planning under complex constraints.

Abstract: Task and motion planning under Signal Temporal Logic constraints is known to
be NP-hard. A common class of approaches formulates these hybrid problems,
which involve discrete task scheduling and continuous motion planning, as
mixed-integer programs (MIP). However, in applications for bipedal locomotion,
introduction of non-convex constraints such as kinematic reachability and
footstep rotation exacerbates the computational complexity of MIPs. In this
work, we present a method based on Benders Decomposition to address scenarios
where solving the entire monolithic optimization problem is prohibitively
intractable. Benders Decomposition proposes an iterative cutting-plane
technique that partitions the problem into a master problem to prototype a plan
that meets the task specification, and a series of subproblems for kinematics
and dynamics feasibility checks. Our experiments demonstrate that this method
achieves faster planning compared to alternative algorithms for solving the
resulting optimization program with nonlinear constraints.

</details>


### [269] [Switch4EAI: Leveraging Console Game Platform for Benchmarking Robotic Athletics](https://arxiv.org/abs/2508.13444)
*Tianyu Li,Jeonghwan Kim,Wontaek Kim,Donghoon Baek,Seungeun Rho,Sehoon Ha*

Main category: cs.RO

TL;DR: The paper presents Switch4EAI, a system that uses console games like Just Dance to evaluate humanoid robot performance, comparing it to humans.


<details>
  <summary>Details</summary>
Motivation: Current evaluations for robot performance in real-world athletic tasks are limited, making cross-comparisons with humans scarce.

Method: The authors design a pipeline that reconstructs and retargets choreographies from console games (Nintendo Switch) onto robots for testing their baseline abilities.

Result: Switch4EAI was validated on a Unitree G1 humanoid robot, showing that the setup provided a measurable comparison of performance between robots and humans.

Conclusion: Using gaming platforms as standardized benchmarks for embodied AI is feasible, offering a new direction for testing whole-body robot control.

Abstract: Recent advances in whole-body robot control have enabled humanoid and legged
robots to execute increasingly agile and coordinated movements. However,
standardized benchmarks for evaluating robotic athletic performance in
real-world settings and in direct comparison to humans remain scarce. We
present Switch4EAI(Switch-for-Embodied-AI), a low-cost and easily deployable
pipeline that leverages motion-sensing console games to evaluate whole-body
robot control policies. Using Just Dance on the Nintendo Switch as a
representative example, our system captures, reconstructs, and retargets
in-game choreography for robotic execution. We validate the system on a Unitree
G1 humanoid with an open-source whole-body controller, establishing a
quantitative baseline for the robot's performance against a human player. In
the paper, we discuss these results, which demonstrate the feasibility of using
commercial games platform as physically grounded benchmarks and motivate future
work to for benchmarking embodied AI.

</details>


### [270] [CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models](https://arxiv.org/abs/2508.13446)
*Catherine Glossop,William Chen,Arjun Bhorkar,Dhruv Shah,Sergey Levine*

Main category: cs.RO

TL;DR: The paper proposes a method to improve vision-language-action (VLA) models for robots by augmenting datasets using counterfactual labels, enhancing instruction-following capabilities and task success rates.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of current VLA models for robots in following fine-grained commands caused by a lack of semantic diversity and language grounding in robot datasets.

Method: The method involves using vision-language models to generate counterfactual labels, enhancing task diversity and language grounding in existing robot datasets without collecting new data.

Result: Experiments on visual language navigation tasks in indoor and outdoor environments show a 27% improvement in success rates for instruction-following tasks, making the model competitive with state-of-the-art methods.

Conclusion: Counterfactual relabeling significantly boosts the instruction-following capabilities of VLA policies, enabling robots to better handle complex language commands while eliminating the need for additional data collection.

Abstract: Generalist robots should be able to understand and follow user instructions,
but current vision-language-action (VLA) models struggle with following
fine-grained commands despite providing a powerful architecture for mapping
open-vocabulary natural language instructions to robot actions. One cause for
this is a lack of semantic diversity and language grounding in existing robot
datasets and, specifically, a lack of fine-grained task diversity for similar
observations. To address this, we present a novel method to augment existing
robot datasets by leveraging vision language models to create counterfactual
labels. Our method improves the language-following capabilities of VLAs by
increasing the diversity and granularity of language grounding for robot
datasets by generating counterfactual language and actions. We evaluate the
resulting model's ability to follow language instructions, ranging from simple
object-centric commands to complex referential tasks, by conducting visual
language navigation experiments in 3 different indoor and outdoor environments.
Our experiments demonstrate that counterfactual relabeling, without any
additional data collection, significantly improves instruction-following in VLA
policies, making them competitive with state-of-the-art methods and increasing
success rate by 27% on navigation tasks.

</details>


### [271] [Modeling and Control of AWOISV: A Filtered Tube-Based MPC Approach for Simultaneous Tracking of Lateral Position and Heading Angle](https://arxiv.org/abs/2508.13457)
*Xu Yang,Jun Ni,Hengyang Feng,Feiyu Wang,Tiezhen Wang*

Main category: cs.RO

TL;DR: The paper explores omni-directional vehicle steering using advanced modeling and proposes a control strategy for precise, robust maneuvering.


<details>
  <summary>Details</summary>
Motivation: Improvements in steering mechanisms for omnidirectional vehicles could allow for unique maneuvers and applications, requiring robust control systems.

Method: Developed a theoretical steering angle representation, a dynamic motion model and proposed a robust filtered tube-based linear time-varying MPC for control.

Result: Simulations and hardware-in-loop experiments demonstrated precise position and heading control with high real-time performance.

Conclusion: The proposed approach achieves seamless and accurate maneuvering for independent steering vehicles, confirming the model's robustness and reliability.

Abstract: An all-wheel omni-directional independent steering vehicle (AWOISV) is a
specialized all-wheel independent steering vehicle with each wheel capable of
steering up to 90{\deg}, enabling unique maneuvers like yaw and diagonal
movement. This paper introduces a theoretical steering radius angle and
sideslip angle (\( \theta_R \)-\(\beta_R \)) representation, based on the
position of the instantaneous center of rotation relative to the wheel rotation
center, defining the motion modes and switching criteria for AWOISVs. A
generalized \( v\)-\(\beta\)-\(r \) dynamic model is developed with forward
velocity \(v\), sideslip angle \(\beta\), and yaw rate \(r\) as states, and
\(\theta_R\) and \(\beta_R\) as control inputs. This model decouples
longitudinal and lateral motions into forward and rotational motions, allowing
seamless transitions across all motion modes under specific conditions. A
filtered tube-based linear time-varying MPC (FT-LTVMPC) strategy is proposed,
achieving simultaneous tracking of lateral position and arbitrary heading
angles, with robustness to model inaccuracies and parameter uncertainties.
Co-simulation and hardware-in-loop (HIL) experiments confirm that FT-LTVMPC
enables high-precision control of both position and heading while ensuring
excellent real-time performance.

</details>


### [272] [Multi-Robot Navigation in Social Mini-Games: Definitions, Taxonomy, and Algorithms](https://arxiv.org/abs/2508.13459)
*Rohan Chandra,Shubham Singh,Abhishek Jha,Dannon Andrade,Hriday Sainathuni,Katia Sycara*

Main category: cs.RO

TL;DR: The paper deals with autonomous robot navigation in constrained environments, called Social Mini-Games (SMGs), proposing a unified taxonomy for focused research.


<details>
  <summary>Details</summary>
Motivation: To address inconsistencies and challenges in autonomous robot navigation research in constrained and crowded environments.

Method: The study establishes a unified taxonomy to categorize SMG solvers and assess current research based on standardized metrics and evaluations.

Result: A clear classification system and unified definitions for SMG navigation methods are provided, enabling structured comparisons.

Conclusion: The work supports advancement in SMG navigation research by providing clarity and standardization to benefit researchers and practitioners alike.

Abstract: The ``Last Mile Challenge'' has long been considered an important, yet
unsolved, challenge for autonomous vehicles, public service robots, and
delivery robots. A central issue in this challenge is the ability of robots to
navigate constrained and cluttered environments (e.g., doorways, hallways,
corridor intersections), often while competing for space with other robots and
humans. We refer to these environments as ``Social Mini-Games'' (SMGs). SMGs
are tightly coupled, high-agency interactions that arise within general
multi-robot navigation (MRN) scenarios. They are identified through certain
distinct characteristics and require specialized metrics to evaluate them.
Traditional navigation approaches designed for MRN do not perform well in SMGs,
which has led to focused research on dedicated SMG solvers (navigation methods
specialized to navigate in SMGs), which has flourished in recent years.
However, publications on SMG navigation research make different assumptions (on
centralized versus decentralized, observability, communication, cooperation,
etc.), and have different objective functions (safety versus liveness). These
assumptions and objectives are sometimes implicitly assumed or described
informally. This makes it difficult to establish appropriate baselines for
comparison in research papers, as well as making it difficult for practitioners
to find the papers relevant to their concrete application. Such ad-hoc
representation of the field also presents a barrier to new researchers wanting
to start research in this area. SMG navigation research requires its own
taxonomy, definitions, and evaluation protocols to guide effective research
moving forward. This survey is the first to catalog SMG solvers using a
well-defined and unified taxonomy and to classify existing methods accordingly.

</details>


### [273] [ROVER: Robust Loop Closure Verification with Trajectory Prior in Repetitive Environments](https://arxiv.org/abs/2508.13488)
*Jingwen Yu,Jiayi Yang,Anjun Hu,Jiankun Wang,Ping Tan,Hong Zhang*

Main category: cs.RO

TL;DR: The paper introduces ROVER, a loop closure verification method for SLAM systems, which uses historical trajectories as prior constraints to reduce false detections in repetitive environments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of false loop closures during SLAM in environments with high visual similarity, where appearance-based features fail.

Method: ROVER employs historical robot trajectories as constraints and evaluates loop candidates by scoring their trajectory compliance with the prior trajectory using pose-graph optimization.

Result: The method demonstrates superior performance in both benchmark tests and real-world experiments, effectively rejecting false loops.

Conclusion: ROVER improves loop closure verification, enhancing robustness and efficiency of SLAM systems in challenging environments. Its implementation is publicly available for further use.

Abstract: Loop closure detection is important for simultaneous localization and mapping
(SLAM), which associates current observations with historical keyframes,
achieving drift correction and global relocalization. However, a falsely
detected loop can be fatal, and this is especially difficult in repetitive
environments where appearance-based features fail due to the high similarity.
Therefore, verification of a loop closure is a critical step in avoiding false
positive detections. Existing works in loop closure verification predominantly
focus on learning invariant appearance features, neglecting the prior knowledge
of the robot's spatial-temporal motion cue, i.e., trajectory. In this letter,
we propose ROVER, a loop closure verification method that leverages the
historical trajectory as a prior constraint to reject false loops in
challenging repetitive environments. For each loop candidate, it is first used
to estimate the robot trajectory with pose-graph optimization. This trajectory
is then submitted to a scoring scheme that assesses its compliance with the
trajectory without the loop, which we refer to as the trajectory prior, to
determine if the loop candidate should be accepted. Benchmark comparisons and
real-world experiments demonstrate the effectiveness of the proposed method.
Furthermore, we integrate ROVER into state-of-the-art SLAM systems to verify
its robustness and efficiency. Our source code and self-collected dataset are
available at https://github.com/jarvisyjw/ROVER.

</details>


### [274] [Unified Hierarchical MPC in Task Executing for Modular Manipulators across Diverse Morphologies](https://arxiv.org/abs/2508.13513)
*Maolin Lei,Edoardo Romiti,Arturo Laurenzi,Cheng Zhou,Wanli Xing,Liang Lu,Nikos G. Tsagarakis*

Main category: cs.RO

TL;DR: Proposed a unified control framework using a hierarchical model predictive approach for versatile modular manipulators.


<details>
  <summary>Details</summary>
Motivation: To address control adaptation challenges in modular manipulators with diverse configurations without extensive parameter tuning.

Method: Implemented a two-level Hierarchical MPC framework with high-level and low-level controllers leveraging predictive trajectories and secondary linearization.

Result: Validated the approach through real-world experiments, showing smooth control and reliability across various manipulator morphologies.

Conclusion: The H-MPC ensures task precision and adaptability while effectively handling kinematic constraints and near-singular configurations.

Abstract: This work proposes a unified Hierarchical Model Predictive Control (H-MPC)
for modular manipulators across various morphologies, as the controller can
adapt to different configurations to execute the given task without extensive
parameter tuning in the controller. The H-MPC divides the control process into
two levels: a high-level MPC and a low-level MPC. The high-level MPC predicts
future states and provides trajectory information, while the low-level MPC
refines control actions by updating the predictive model based on this
high-level information. This hierarchical structure allows for the integration
of kinematic constraints and ensures smooth joint-space trajectories, even near
singular configurations. Moreover, the low-level MPC incorporates secondary
linearization by leveraging predictive information from the high-level MPC,
effectively capturing the second-order Taylor expansion information of the
kinematic model while still maintaining a linearized model formulation. This
approach not only preserves the simplicity of a linear control model but also
enhances the accuracy of the kinematic representation, thereby improving
overall control precision and reliability. To validate the effectiveness of the
control policy, we conduct extensive evaluations across different manipulator
morphologies and demonstrate the execution of pick-and-place tasks in
real-world scenarios.

</details>


### [275] [A Three-Level Whole-Body Disturbance Rejection Control Framework for Dynamic Motions in Legged Robots](https://arxiv.org/abs/2508.13531)
*Bolin Li,Gewei Zuo,Zhixiang Wang,Xiaotian Ke,Lijun Zhu,Han Ding*

Main category: cs.RO

TL;DR: The paper introduces a control framework to improve stability and robustness in legged robots, using a novel moving horizon extended state observer and a three-level disturbance rejection framework.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of maintaining stability and robustness in legged robots under uncertainties like model inaccuracies, external disturbances, and faults.

Method: Developed a Moving Horizon Extended State Observer (MH-ESO) for uncertainty estimation, combined with a three-level Whole-Body Disturbance Rejection Control (T-WB-DRC) framework.

Result: Simulations on humanoid and quadruped robots in the Gazebo simulator demonstrated the effectiveness of T-WB-DRC, with experimental trials on a quadruped robot further validating robustness under disturbances.

Conclusion: The T-WB-DRC establishes a robust and stable framework for legged robots, offering improved performance in disturbance rejection and fault tolerance.

Abstract: This paper presents a control framework designed to enhance the stability and
robustness of legged robots in the presence of uncertainties, including model
uncertainties, external disturbances, and faults. The framework enables the
full-state feedback estimator to estimate and compensate for uncertainties in
whole-body dynamics of the legged robots. First, we propose a novel moving
horizon extended state observer (MH-ESO) to estimate uncertainties and mitigate
noise in legged systems, which can be integrated into the framework for
disturbance compensation. Second, we introduce a three-level whole-body
disturbance rejection control framework (T-WB-DRC). Unlike the previous
two-level approach, this three-level framework considers both the plan based on
whole-body dynamics without uncertainties and the plan based on dynamics with
uncertainties, significantly improving payload transportation, external
disturbance rejection, and fault tolerance. Third, simulations of both humanoid
and quadruped robots in the Gazebo simulator demonstrate the effectiveness and
versatility of T-WB-DRC. Finally, extensive experimental trials on a quadruped
robot validate the robustness and stability of the system when using T-WB-DRC
under various disturbance conditions.

</details>


### [276] [MimicFunc: Imitating Tool Manipulation from a Single Human Video via Functional Correspondence](https://arxiv.org/abs/2508.13534)
*Chao Tang,Anxing Xiao,Yuhong Deng,Tianrun Hu,Wenlong Dong,Hanbo Zhang,David Hsu,Hong Zhang*

Main category: cs.RO

TL;DR: The paper introduces MimicFunc, a framework enabling robots to generalize tool manipulation skills from one human video to similar tasks using novel tools, addressing intra-function variations.


<details>
  <summary>Details</summary>
Motivation: Humans can intuitively learn and generalize tool manipulation skills from observing others, something robots struggle with. The motivation is to close this gap and make teaching robots scalable and efficient without relying on manual teleoperation.

Method: The framework, MimicFunc, employs a function frame—keypoint-based local coordinates focused on functional correspondences—to generalize manipulation skills from single RGB-D human videos to novel tools.

Result: Experiments show that MimicFunc enables robots to generalize and perform equivalent tasks with different tools, while also generating data for training visuomotor policies without teleoperation.

Conclusion: MimicFunc demonstrates effective one-shot generalization for robots, simplifying the process of teaching tool manipulation and offering a scalable alternative to traditional data collection methods.

Abstract: Imitating tool manipulation from human videos offers an intuitive approach to
teaching robots, while also providing a promising and scalable alternative to
labor-intensive teleoperation data collection for visuomotor policy learning.
While humans can mimic tool manipulation behavior by observing others perform a
task just once and effortlessly transfer the skill to diverse tools for
functionally equivalent tasks, current robots struggle to achieve this level of
generalization. A key challenge lies in establishing function-level
correspondences, considering the significant geometric variations among
functionally similar tools, referred to as intra-function variations. To
address this challenge, we propose MimicFunc, a framework that establishes
functional correspondences with function frame, a function-centric local
coordinate frame constructed with keypoint-based abstraction, for imitating
tool manipulation skills. Experiments demonstrate that MimicFunc effectively
enables the robot to generalize the skill from a single RGB-D human video to
manipulating novel tools for functionally equivalent tasks. Furthermore,
leveraging MimicFunc's one-shot generalization capability, the generated
rollouts can be used to train visuomotor policies without requiring
labor-intensive teleoperation data collection for novel objects. Our code and
video are available at https://sites.google.com/view/mimicfunc.

</details>


### [277] [Assessing Pedestrian Behavior Around Autonomous Cleaning Robots in Public Spaces: Findings from a Field Observation](https://arxiv.org/abs/2508.13699)
*Maren Raab,Linda Miller,Zhe Zeng,Pascal Jansen,Martin Baumann,Johannes Kraus*

Main category: cs.RO

TL;DR: The study evaluates how different types and movement patterns of autonomous cleaning robots affect the movement behavior of both distracted and undistracted pedestrians in public spaces.


<details>
  <summary>Details</summary>
Motivation: To enhance the transparency of autonomous robots in public spaces and avoid critical situations by studying how pedestrians interact with these robots.

Method: The researchers videotaped unaware pedestrians encountering two different autonomous cleaning robots in a naturalistic field setting, analyzing their movement behaviors based on robot type, movement patterns, and smartphone distractions.

Result: Distracted pedestrians (8% distracted by smartphones) and undistracted pedestrians showed no significant differences in their movement behaviors. However, the larger robot and offset rectangular movement pattern led to more lateral adaptations by pedestrians, with varying distances based on robot type and movement pattern.

Conclusion: Robot type and movement pattern affect how pedestrians adapt their movements, which provides important considerations for designing robotic communication strategies in human-robot interactions (HRI).

Abstract: As autonomous robots become more common in public spaces, spontaneous
encounters with laypersons are more frequent. For this, robots need to be
equipped with communication strategies that enhance momentary transparency and
reduce the probability of critical situations. Adapting these robotic
strategies requires consideration of robot movements, environmental conditions,
and user characteristics and states. While numerous studies have investigated
the impact of distraction on pedestrians' movement behavior, limited research
has examined this behavior in the presence of autonomous robots. This research
addresses the impact of robot type and robot movement pattern on distracted and
undistracted pedestrians' movement behavior. In a field setting, unaware
pedestrians were videotaped while moving past two working, autonomous cleaning
robots. Out of N=498 observed pedestrians, approximately 8% were distracted by
smartphones. Distracted and undistracted pedestrians did not exhibit
significant differences in their movement behaviors around the robots. Instead,
both the larger sweeping robot and the offset rectangular movement pattern
significantly increased the number of lateral adaptations compared to the
smaller cleaning robot and the circular movement pattern. The offset
rectangular movement pattern also led to significantly more close lateral
adaptations. Depending on the robot type, the movement patterns led to
differences in the distances of lateral adaptations. The study provides initial
insights into pedestrian movement behavior around an autonomous cleaning robot
in public spaces, contributing to the growing field of HRI research.

</details>


### [278] [Blast Hole Seeking and Dipping -- The Navigation and Perception Framework in a Mine Site Inspection Robot](https://arxiv.org/abs/2508.13785)
*Liyang Liu,Ehsan Mihankhah,Nathan Wallace,Javier Martinez,Andrew J. Hill*

Main category: cs.RO

TL;DR: This paper introduces "DIPPeR", an autonomous robot designed for efficient and accurate internal inspection of blast holes in open-pit mining through a robust detection and navigation framework.


<details>
  <summary>Details</summary>
Motivation: Manual inspection of blast holes is slow, costly, and limited in revealing internal properties, prompting the need for automated solutions to improve efficiency and reduce costs.

Method: The robot uses point-cloud data from LiDAR sensors to detect cone-shaped drill waste and a robust pipeline to segment and locate holes in virtual depth images, enabling precise sensor placement and autonomous hole-seeking navigation.

Result: The system was tested in simulations and field environments, demonstrating effective navigation and accurate hole detection for improved autonomous inspection.

Conclusion: "DIPPeR" successfully automates blast hole inspection, offering a significant improvement in precision and efficiency over manual methods while addressing challenges in material handling costs and geological property analysis.

Abstract: In open-pit mining, holes are drilled into the surface of the excavation site
and detonated with explosives to facilitate digging. These blast holes need to
be inspected internally for investigation of downhole material types and
properties. Knowing these properties can lead to significant savings in
material handling costs in downstream processes. Manual hole inspection is slow
and expensive, with major limitations in revealing the geometric and geological
properties of the holes and their contents. This has been the motivation for
the development of our autonomous mine-site inspection robot - "DIPPeR". In
this paper, the automation aspect of the project is explained. We present a
robust blast hole seeking and detection framework that enables target-based
navigation and accurate down-hole sensor positioning. The pipeline first
processes point-cloud data collected by the on-board LiDAR sensors, extracting
the cone-shaped volume of drill-waste above the ground. By projecting the 3D
cone points into a virtual depth image, segmentation is achieved in the 2D
domain, yielding a circular hole at the image centre and a collared cone face.
We then identify the hole centre using a robust detection module while
suppressing non-maximum candidates, ensuring precise sensor placement for
down-hole inspection and avoiding collisions with the cavity wall. To enable
autonomous hole-seeking, the pipeline automatically adjusts its projection
parameters during robot navigation to account for variations in point sparsity
and hole opening size, ensuring a consistent hole appearance in 2D images. This
allows continuous tracking of the target hole as the robot approaches the goal
point. We demonstrate the effectiveness of our navigation and perception system
in both high-fidelity simulation environments and on-site field tests. A
demonstration video is available at
"https://www.youtube.com/watch?v=fRNbcBcaSqE".

</details>


### [279] [Trajectory Tracking and Stabilization of Quadrotors Using Deep Koopman Model Predictive Control](https://arxiv.org/abs/2508.13795)
*Haitham El-Hussieny*

Main category: cs.RO

TL;DR: This paper introduces a deep Koopman operator and model predictive control (DK-MPC) framework for quadrotors, achieving accurate trajectory tracking and stabilization with faster computation.


<details>
  <summary>Details</summary>
Motivation: Current quadrotor control methods struggle to balance handling nonlinear dynamics and meeting real-time computational needs during agile flight.

Method: A deep Koopman operator is trained using flight data to approximate nonlinear dynamics with linear models in high-dimensional latent space, enabling efficient model predictive control.

Result: The DK-MPC approach achieves better tracking accuracy and reduced computation time compared to conventional nonlinear MPC in trajectory and stabilization experiments.

Conclusion: The proposed DK-MPC demonstrates the feasibility of Koopman-based methods for real-time quadrotor control, with future plans to address agility and external disturbances.

Abstract: This paper presents a data-driven control framework for quadrotor systems
that integrates a deep Koopman operator with model predictive control (DK-MPC).
The deep Koopman operator is trained on sampled flight data to construct a
high-dimensional latent representation in which the nonlinear quadrotor
dynamics are approximated by linear models. This linearization enables the
application of MPC to efficiently optimize control actions over a finite
prediction horizon, ensuring accurate trajectory tracking and stabilization.
The proposed DK-MPC approach is validated through a series of
trajectory-following and point-stabilization numerical experiments, where it
demonstrates superior tracking accuracy and significantly lower computation
time compared to conventional nonlinear MPC. These results highlight the
potential of Koopman-based learning methods to handle complex quadrotor
dynamics while meeting the real-time requirements of embedded flight control.
Future work will focus on extending the framework to more agile flight
scenarios and improving robustness against external disturbances.

</details>


### [280] [Toward Deployable Multi-Robot Collaboration via a Symbolically-Guided Decision Transformer](https://arxiv.org/abs/2508.13877)
*Rathnam Vidushika Rasanji,Jin Wei-Kocsis,Jiansong Zhang,Dongming Gan,Ragu Athinarayanan,Paul Asunda*

Main category: cs.RO

TL;DR: The paper introduces a novel framework called SGDT, combining neuro-symbolic planning and decision transformers to facilitate multi-robot manipulation.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning struggles with complex dynamics and long-term dependencies, particularly in multi-robot scenarios. Decision Transformers show promise but are underexplored in this domain.

Method: The SGDT framework uses a neuro-symbolic planner to generate symbolic subgoals and a goal-conditioned decision transformer for fine-grained decision-making. This hierarchical system addresses multi-robot collaboration challenges.

Result: SGDT was tested in diverse scenarios, demonstrating structured, interpretable, and generalizable decision-making capabilities, including success in zero-shot and few-shot scenarios.

Conclusion: This work represents a pioneering exploration of Decision Transformers in multi-robot manipulation, offering practical solutions for complex collaborative tasks.

Abstract: Reinforcement learning (RL) has demonstrated great potential in robotic
operations. However, its data-intensive nature and reliance on the Markov
Decision Process (MDP) assumption limit its practical deployment in real-world
scenarios involving complex dynamics and long-term temporal dependencies, such
as multi-robot manipulation. Decision Transformers (DTs) have emerged as a
promising offline alternative by leveraging causal transformers for sequence
modeling in RL tasks. However, their applications to multi-robot manipulations
still remain underexplored. To address this gap, we propose a novel framework,
Symbolically-Guided Decision Transformer (SGDT), which integrates a
neuro-symbolic mechanism with a causal transformer to enable deployable
multi-robot collaboration. In the proposed SGDT framework, a neuro-symbolic
planner generates a high-level task-oriented plan composed of symbolic
subgoals. Guided by these subgoals, a goal-conditioned decision transformer
(GCDT) performs low-level sequential decision-making for multi-robot
manipulation. This hierarchical architecture enables structured, interpretable,
and generalizable decision making in complex multi-robot collaboration tasks.
We evaluate the performance of SGDT across a range of task scenarios, including
zero-shot and few-shot scenarios. To our knowledge, this is the first work to
explore DT-based technology for multi-robot manipulation.

</details>


### [281] [Driving Style Recognition Like an Expert Using Semantic Privileged Information from Large Language Models](https://arxiv.org/abs/2508.13881)
*Zhaokun Chen,Chaopeng Zhang,Xiaohan Li,Wenshuo Wang,Gentiane Venture,Junqiang Xi*

Main category: cs.RO

TL;DR: The study introduces a framework utilizing semantic information from large language models to enhance driving style recognition systems, demonstrating improved performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current driving style recognition systems fail to align well with expert judgments due to reliance on sensor-based features rather than semantic reasoning.

Method: The authors developed DriBehavGPT to generate natural language descriptions of driving behaviors, encode these into numerical representations, and use them as privileged information during the training phase of an SVM+ model.

Result: Experiments show significant F1-score improvements of 7.6% for car-following and 7.9% for lane-changing tasks, validating the framework's effectiveness.

Conclusion: Incorporating semantic behavioral representations from LLMs leads to better alignment with human judgment and interpretable recognition systems, maintaining computational efficiency.

Abstract: Existing driving style recognition systems largely depend on low-level
sensor-derived features for training, neglecting the rich semantic reasoning
capability inherent to human experts. This discrepancy results in a fundamental
misalignment between algorithmic classifications and expert judgments. To
bridge this gap, we propose a novel framework that integrates Semantic
Privileged Information (SPI) derived from large language models (LLMs) to align
recognition outcomes with human-interpretable reasoning. First, we introduce
DriBehavGPT, an interactive LLM-based module that generates natural-language
descriptions of driving behaviors. These descriptions are then encoded into
machine learning-compatible representations via text embedding and
dimensionality reduction. Finally, we incorporate them as privileged
information into Support Vector Machine Plus (SVM+) for training, enabling the
model to approximate human-like interpretation patterns. Experiments across
diverse real-world driving scenarios demonstrate that our SPI-enhanced
framework outperforms conventional methods, achieving F1-score improvements of
7.6% (car-following) and 7.9% (lane-changing). Importantly, SPI is exclusively
used during training, while inference relies solely on sensor data, ensuring
computational efficiency without sacrificing performance. These results
highlight the pivotal role of semantic behavioral representations in improving
recognition accuracy while advancing interpretable, human-centric driving
systems.

</details>


### [282] [Multimodal Data Storage and Retrieval for Embodied AI: A Survey](https://arxiv.org/abs/2508.13901)
*Yihao Lu,Hao Tang*

Main category: cs.RO

TL;DR: This paper surveys data management challenges and solutions for Embodied AI, focusing on storage architectures, retrieval paradigms, identified bottlenecks, and proposing a research agenda.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the persistent challenges in managing multimodal data streams generated by Embodied AI systems as traditional systems fail to meet their needs.

Method: A systematic evaluation of storage architectures and retrieval paradigms was conducted, along with identifying bottlenecks based on a review of over 180 studies.

Result: Five storage architectures and five retrieval paradigms were analyzed, revealing trade-offs between semantic coherence and real-time responsiveness, as well as systemic bottlenecks like the Physical Grounding Gap.

Conclusion: The paper provides a comprehensive roadmap, highlighting physics-aware models, adaptive systems, and standardized benchmarks, as essential goals for advancing data management for EAI.

Abstract: Embodied AI (EAI) agents continuously interact with the physical world,
generating vast, heterogeneous multimodal data streams that traditional
management systems are ill-equipped to handle. In this survey, we first
systematically evaluate five storage architectures (Graph Databases,
Multi-Model Databases, Data Lakes, Vector Databases, and Time-Series
Databases), focusing on their suitability for addressing EAI's core
requirements, including physical grounding, low-latency access, and dynamic
scalability. We then analyze five retrieval paradigms (Fusion Strategy-Based
Retrieval, Representation Alignment-Based Retrieval, Graph-Structure-Based
Retrieval, Generation Model-Based Retrieval, and Efficient Retrieval-Based
Optimization), revealing a fundamental tension between achieving long-term
semantic coherence and maintaining real-time responsiveness. Based on this
comprehensive analysis, we identify key bottlenecks, spanning from the
foundational Physical Grounding Gap to systemic challenges in cross-modal
integration, dynamic adaptation, and open-world generalization. Finally, we
outline a forward-looking research agenda encompassing physics-aware data
models, adaptive storage-retrieval co-optimization, and standardized
benchmarking, to guide future research toward principled data management
solutions for EAI. Our survey is based on a comprehensive review of more than
180 related studies, providing a rigorous roadmap for designing the robust,
high-performance data management frameworks essential for the next generation
of autonomous embodied systems.

</details>


### [283] [Augmenting cobots for sheet-metal SMEs with 3D object recognition and localisation](https://arxiv.org/abs/2508.13964)
*Martijn Cramer,Yanming Wu,David De Schepper,Eric Demeester*

Main category: cs.RO

TL;DR: The paper discusses the challenges in high-mix-low-volume productions in sheet-metal workshops and how the COOCK+ ROBUST project utilizes advanced cobotics such as 3D object recognition and localization to enable mobile, reconfigurable production assistants.


<details>
  <summary>Details</summary>
Motivation: Standard automation solutions don't meet the needs of SMEs in high-mix-low-volume production, leading to repetitive manual labor and underutilization of skilled workers.

Method: The paper integrates existing technologies like 3D object recognition and localization into cobotics systems and provides insights from a past project as an implementation example.

Result: The study identifies opportunities and challenges of enhancing cobotics systems in an industrial setting and demonstrates the implementation through a collaborative project.

Conclusion: Advanced cobotics can provide transformative solutions for SMEs in sheet-metal workshops, addressing labor challenges and unlocking the potential of the tech-skilled workforce.

Abstract: Due to high-mix-low-volume production, sheet-metal workshops today are
challenged by small series and varying orders. As standard automation solutions
tend to fall short, SMEs resort to repetitive manual labour impacting
production costs and leading to tech-skilled workforces not being used to their
full potential. The COOCK+ ROBUST project aims to transform cobots into mobile
and reconfigurable production assistants by integrating existing technologies,
including 3D object recognition and localisation. This article explores both
the opportunities and challenges of enhancing cobotic systems with these
technologies in an industrial setting, outlining the key steps involved in the
process. Additionally, insights from a past project, carried out by the ACRO
research unit in collaboration with an industrial partner, serves as a concrete
implementation example throughout.

</details>


### [284] [Toward an Interaction-Centered Approach to Robot Trustworthiness](https://arxiv.org/abs/2508.13976)
*Carlo Mazzola,Hassan Ali,Kristína Malinovská,Igor Farkaš*

Main category: cs.RO

TL;DR: The paper focuses on promoting trust between humans and robots through mutual understanding, emphasizing human awareness and transparency, and introduces key components to bridge trust gaps.


<details>
  <summary>Details</summary>
Motivation: As robots increasingly integrate into human environments, ensuring trustworthy interactions is critical for safety and ethics in human-robot interactions (HRI).

Method: The paper outlines a framework based on mutual understanding, emphasizing human awareness and robot transparency to foster accurate perceptions and trust in robotic capabilities.

Result: The framework aligns robot behavior with human expectations by enabling robots to interpret human actions and communicate intentions, giving humans comprehension and control.

Conclusion: Integrating human awareness and transparency in HRI applications can bridge the trust gap between perceived and actual robot capabilities, ensuring safer and effective interactions.

Abstract: As robots get more integrated into human environments, fostering
trustworthiness in embodied robotic agents becomes paramount for an effective
and safe human-robot interaction (HRI). To achieve that, HRI applications must
promote human trust that aligns with robot skills and avoid misplaced trust or
overtrust, which can pose safety risks and ethical concerns. To achieve that,
HRI applications must promote human trust that aligns with robot skills and
avoid misplaced trust or overtrust, which can pose safety risks and ethical
concerns. In this position paper, we outline an interaction-based framework for
building trust through mutual understanding between humans and robots. We
emphasize two main pillars: human awareness and transparency, referring to the
robot ability to interpret human actions accurately and to clearly communicate
its intentions and goals, respectively. By integrating these two pillars,
robots can behave in a manner that aligns with human expectations and needs
while providing their human partners with both comprehension and control over
their actions. We also introduce four components that we think are important
for bridging the gap between a human-perceived sense of trust and a robot true
capabilities.

</details>


### [285] [The Social Context of Human-Robot Interactions](https://arxiv.org/abs/2508.13982)
*Sydney Thompson,Kate Candon,Marynel Vázquez*

Main category: cs.RO

TL;DR: This paper surveys definitions of 'social context' in Human-Robot Interaction (HRI) literature, proposes a conceptual model, and examines its application.


<details>
  <summary>Details</summary>
Motivation: Inconsistent use of the term 'social context' in HRI impedes communication and the integration of findings across studies.

Method: The authors reviewed existing HRI literature to identify definitions of 'social context' and formulated a conceptual model. They then applied the model to analyze prior work and highlighted key attributes.

Result: The proposed model provides a structured way to describe and analyze social contexts in HRI, aiding researchers in planning, modeling behaviors, and extracting insights.

Conclusion: A unified conceptual model enhances understanding and research consistency in HRI, while highlighting gaps and future directions in exploring social contexts.

Abstract: The Human-Robot Interaction (HRI) community often highlights the social
context of an interaction as a key consideration when designing, implementing,
and evaluating robot behavior. Unfortunately, researchers use the term "social
context" in varied ways. This can lead to miscommunication, making it
challenging to draw connections between related work on understanding and
modeling the social contexts of human-robot interactions. To address this gap,
we survey the HRI literature for existing definitions and uses of the term
"social context". Then, we propose a conceptual model for describing the social
context of a human-robot interaction. We apply this model to existing work, and
we discuss a range of attributes of social contexts that can help researchers
plan for interactions, develop behavior models for robots, and gain insights
after interactions have taken place. We conclude with a discussion of open
research questions in relation to understanding and modeling the social
contexts of human-robot interactions.

</details>


### [286] [Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation](https://arxiv.org/abs/2508.13998)
*Yifu Yuan,Haiqin Cui,Yaoting Huang,Yibin Chen,Fei Ni,Zibin Dong,Pengyi Li,Yan Zheng,Jianye Hao*

Main category: cs.RO

TL;DR: The paper tackles the 'seeing-to-doing gap' in embodied AI by introducing pointing as a unified intermediate representation and developing Embodied-R1, a Vision-Language Model achieving state-of-the-art performance on embodied tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in embodied AI caused by data scarcity and differences in embodiment setups, which hinder generalization.

Method: They designed Embodied-R1, a Vision-Language Model, trained on a large-scale dataset (Embodied-Points-200K) using Reinforced Fine-tuning with multi-task reward design.

Result: Embodied-R1 surpasses benchmarks, showing 62% improvement over baselines, achieves strong zero-shot generalization, and demonstrates robustness against visual disturbances.

Conclusion: Pointing-centric representation combined with RFT training effectively bridges the perception-action gap, enhancing generalization capabilities in robotics.

Abstract: Generalization in embodied AI is hindered by the "seeing-to-doing gap," which
stems from data scarcity and embodiment heterogeneity. To address this, we
pioneer "pointing" as a unified, embodiment-agnostic intermediate
representation, defining four core embodied pointing abilities that bridge
high-level vision-language comprehension with low-level action primitives. We
introduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed
for embodied reasoning and pointing. We use a wide range of embodied and
general visual reasoning datasets as sources to construct a large-scale
dataset, Embodied-Points-200K, which supports key embodied pointing
capabilities. We then train Embodied-R1 using a two-stage Reinforced
Fine-tuning (RFT) curriculum with a specialized multi-task reward design.
Embodied-R1 achieves state-of-the-art performance on 11 embodied spatial and
pointing benchmarks. Critically, it demonstrates robust zero-shot
generalization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5%
across 8 real-world XArm tasks without any task-specific fine-tuning,
representing a 62% improvement over strong baselines. Furthermore, the model
exhibits high robustness against diverse visual disturbances. Our work shows
that a pointing-centric representation, combined with an RFT training paradigm,
offers an effective and generalizable pathway to closing the perception-action
gap in robotics.

</details>


### [287] [Train Once, Deploy Anywhere: Realize Data-Efficient Dynamic Object Manipulation](https://arxiv.org/abs/2508.14042)
*Zhuoling Li,Xiaoyang Wu,Zhenhua Xu,Hengshuang Zhao*

Main category: cs.RO

TL;DR: This study presents a generalizable system for dynamic object manipulation using imitation learning with limited demonstrations, achieving impressive results in diverse environments.


<details>
  <summary>Details</summary>
Motivation: Enhancing manufacturing efficiency through generalizable dynamic object manipulation while addressing the labor-intensive challenge of collecting extensive demonstrations.

Method: Develops an entropy-based theoretical framework to optimize imitation learning and introduces the Generalizable Entropy-based Manipulation (GEM) system.

Result: GEM demonstrated strong generalization capabilities across diverse environments, robots, dynamics, and objects, achieving a success rate of over 97% in a real canteen deployment over 10,000 operations.

Conclusion: The proposed system, GEM, achieves high generalization and efficiency in dynamic object manipulation tasks using very few demonstrations, paving the way for broader applications in manufacturing and real-world environments.

Abstract: Realizing generalizable dynamic object manipulation is important for
enhancing manufacturing efficiency, as it eliminates specialized engineering
for various scenarios. To this end, imitation learning emerges as a promising
paradigm, leveraging expert demonstrations to teach a policy manipulation
skills. Although the generalization of an imitation learning policy can be
improved by increasing demonstrations, demonstration collection is
labor-intensive. To address this problem, this paper investigates whether
strong generalization in dynamic object manipulation is achievable with only a
few demonstrations. Specifically, we develop an entropy-based theoretical
framework to quantify the optimization of imitation learning. Based on this
framework, we propose a system named Generalizable Entropy-based Manipulation
(GEM). Extensive experiments in simulated and real tasks demonstrate that GEM
can generalize across diverse environment backgrounds, robot embodiments,
motion dynamics, and object geometries. Notably, GEM has been deployed in a
real canteen for tableware collection. Without any in-scene demonstration, it
achieves a success rate of over 97% across more than 10,000 operations.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [288] [A Comparative Study of Delta Parquet, Iceberg, and Hudi for Automotive Data Engineering Use Cases](https://arxiv.org/abs/2508.13396)
*Dinesh Eswararaj,Ajay Babu Nellipudi,Vandana Kollati*

Main category: cs.SE

TL;DR: The paper evaluates Delta Parquet, Iceberg, and Hudi data lakehouse formats for automotive telemetry data, focusing on their strengths and trade-offs in query performance and real-time processing.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of handling vast volumes of automotive sensor and telemetry data efficiently while ensuring scalability, consistency, and low latency.

Method: The study conducts a comparative analysis of Delta Parquet, Iceberg, and Hudi on real-world time-series automotive data, assessing aspects like partitioning, query performance, data consistency, and ecosystem maturity.

Result: Delta Parquet excels in machine learning readiness and governance, Iceberg is suitable for batch analytics and cloud workloads, and Hudi is optimized for real-time ingestion and incremental processing.

Conclusion: The study provides actionable insights on selecting or combining data formats for automotive use cases like fleet management and predictive maintenance, highlighting trade-offs in time-travel and update capabilities.

Abstract: The automotive industry generates vast amounts of data from sensors,
telemetry, diagnostics, and real-time operations. Efficient data engineering is
critical to handle challenges of latency, scalability, and consistency. Modern
data lakehouse formats Delta Parquet, Apache Iceberg, and Apache Hudi offer
features such as ACID transactions, schema enforcement, and real-time
ingestion, combining the strengths of data lakes and warehouses to support
complex use cases. This study presents a comparative analysis of Delta Parquet,
Iceberg, and Hudi using real-world time-series automotive telemetry data with
fields such as vehicle ID, timestamp, location, and event metrics. The
evaluation considers modeling strategies, partitioning, CDC support, query
performance, scalability, data consistency, and ecosystem maturity. Key
findings show Delta Parquet provides strong ML readiness and governance,
Iceberg delivers high performance for batch analytics and cloud-native
workloads, while Hudi is optimized for real-time ingestion and incremental
processing. Each format exhibits tradeoffs in query efficiency, time-travel,
and update semantics. The study offers insights for selecting or combining
formats to support fleet management, predictive maintenance, and route
optimization. Using structured datasets and realistic queries, the results
provide practical guidance for scaling data pipelines and integrating machine
learning models in automotive applications.

</details>


### [289] [The Hidden Cost of Readability: How Code Formatting Silently Consumes Your LLM Budget](https://arxiv.org/abs/2508.13666)
*Dangfeng Pan,Zhensu Sun,Cenyuan Zhang,David Lo,Xiaoning Du*

Main category: cs.SE

TL;DR: Removing formatting elements from source code, such as indentation and newlines, can reduce input token count for large language models (LLMs) by 24.5% without significantly affecting performance.


<details>
  <summary>Details</summary>
Motivation: To analyze whether formatting elements, such as indentation and newlines, are essential for LLMs and whether removing them can improve computational efficiency and response times.

Method: Conducted a large-scale empirical study involving Fill-in-the-Middle Code Completion tasks across four programming languages (Java, Python, C++, C#) and ten different LLMs to evaluate the impact of removing formatting elements on token count and performance.

Result: LLMs maintain performance between formatted and unformatted code with an average 24.5% reduction in input token count and negligible changes in output tokens. Through additional prompting and fine-tuning, output code length reductions of up to 36.1% were achieved without compromising correctness.

Conclusion: Code format removal emerges as an effective optimization for LLM efficiency, and a bidirectional transformation tool was developed to enable practical integration within existing workflows while balancing human readability and computational demands.

Abstract: Source code is usually formatted with elements like indentation and newlines
to improve readability for human developers. However, these visual aids do not
seem to be beneficial for large language models (LLMs) in the same way since
the code is processed as a linear sequence of tokens. Furthermore, these
additional tokens can lead to increased computational costs and longer response
times for LLMs. If such formatting elements are non-essential to LLMs, we can
reduce such costs by removing them from the code. To figure out the role played
by formatting elements, we conduct a comprehensive empirical study to evaluate
the impact of code formatting on LLM performance and efficiency. Through
large-scale experiments on Fill-in-the-Middle Code Completion tasks across four
programming languages (Java, Python, C++, C\#) and ten LLMs-including both
commercial and open-source models-we systematically analyze token count and
performance when formatting elements are removed. Key findings indicate that
LLMs can maintain performance across formatted code and unformatted code,
achieving an average input token reduction of 24.5\% with negligible output
token reductions. This makes code format removal a practical optimization
strategy for improving LLM efficiency. Further exploration reveals that both
prompting and fine-tuning LLMs can lead to significant reductions (up to
36.1\%) in output code length without compromising correctness. To facilitate
practical applications, we develop a bidirectional code transformation tool for
format processing, which can be seamlessly integrated into existing LLM
inference workflows, ensuring both human readability and LLM efficiency.

</details>


### [290] [COMPASS: A Multi-Dimensional Benchmark for Evaluating Code Generation in Large Language Models](https://arxiv.org/abs/2508.13757)
*James Meaden,Michał Jarosz,Piotr Jodłowski,Grigori Melnik*

Main category: cs.SE

TL;DR: COMPASS introduces a multi-dimensional evaluation framework for code generation models, emphasizing correctness, efficiency, and code quality.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks neglect efficiency and code quality, focusing solely on correctness in code generation models.

Method: COMPASS uses 50 real Codility problems with industry benchmarks, assessing models through correctness, efficiency, and quality via standard tools.

Result: Analysis of leading models reveals that high correctness does not equate to efficient or maintainable code.

Conclusion: COMPASS offers a comprehensive approach to help in developing AI systems suitable for real-world production use.

Abstract: Current code generation benchmarks focus primarily on functional correctness
while overlooking two critical aspects of real-world programming: algorithmic
efficiency and code quality. We introduce COMPASS (COdility's Multi-dimensional
Programming ASSessment), a comprehensive evaluation framework that assesses
code generation across three dimensions: correctness, efficiency, and quality.
COMPASS consists of 50 competitive programming problems from real Codility
competitions, providing authentic human baselines from 393,150 submissions.
Unlike existing benchmarks that treat algorithmically inefficient solutions
identically to optimal ones provided they pass test cases, COMPASS
systematically evaluates runtime efficiency and code quality using
industry-standard analysis tools. Our evaluation of three leading
reasoning-enhanced models, Anthropic Claude Opus 4, Google Gemini 2.5 Pro, and
OpenAI O4-Mini-High, reveals that models achieving high correctness scores do
not necessarily produce efficient algorithms or maintainable code. These
findings highlight the importance of evaluating more than just correctness to
truly understand the real-world capabilities of code generation models. COMPASS
serves as a guiding framework, charting a path for future research toward AI
systems that are robust, reliable, and ready for production use.

</details>


### [291] [Agentic DraCor and the Art of Docstring Engineering: Evaluating MCP-empowered LLM Usage of the DraCor API](https://arxiv.org/abs/2508.13774)
*Peer Trilcke,Ingo Börner,Henny Sluyter-Gäthje,Daniil Skorinkin,Frank Fischer,Carsten Milling*

Main category: cs.SE

TL;DR: The paper presents the implementation and evaluation of a Model Context Protocol server for DraCor, enabling LLMs to interact with the DraCor API autonomously.


<details>
  <summary>Details</summary>
Motivation: To explore how Large Language Models can utilize tools for Computational Literary Studies and to address infrastructure development challenges in Digital Humanities.

Method: The authors implemented a Model Context Protocol server, conducted qualitative experiments, and evaluated LLMs on aspects like 'Tool Correctness,' 'Tool-Calling Efficiency,' and 'Tool-Use Reliability.'.

Result: The study identified the significance of 'Docstring Engineering' in optimizing LLM-tool interactions.

Conclusion: LLMs show potential for autonomous research applications, but robust infrastructure development is crucial for reliable Digital Humanities systems.

Abstract: This paper reports on the implementation and evaluation of a Model Context
Protocol (MCP) server for DraCor, enabling Large Language Models (LLM) to
autonomously interact with the DraCor API. We conducted experiments focusing on
tool selection and application by the LLM, employing a qualitative approach
that includes systematic observation of prompts to understand how LLMs behave
when using MCP tools, evaluating "Tool Correctness", "Tool-Calling Efficiency",
and "Tool-Use Reliability". Our findings highlight the importance of "Docstring
Engineering", defined as reflexively crafting tool documentation to optimize
LLM-tool interaction. Our experiments demonstrate both the promise of agentic
AI for research in Computational Literary Studies and the essential
infrastructure development needs for reliable Digital Humanities
infrastructures.

</details>


### [292] [Structural and Connectivity Patterns in the Maven Central Software Dependency Network](https://arxiv.org/abs/2508.13819)
*Daniel Ogenrwot,John Businge,Shaikh Arifuzzaman*

Main category: cs.SE

TL;DR: The paper analyzes Maven Central, a major Java library repository, using network science, revealing its scale-free, small-world nature and identifying critical infrastructural hubs.


<details>
  <summary>Details</summary>
Motivation: To understand the structural and connectivity patterns of large-scale software ecosystems for improving reuse, resilience, and mitigating risks.

Method: The authors used network science techniques to study Maven Central with a graph-based approach, analyzing top 5,000 highly connected artifacts. They created a curated graph with 1.3M nodes and 20.9M edges and computed graph metrics like degree distributions and centralities.

Result: The study found Maven Central to have a scale-free, small-world topology, with influential hubs mostly comprising testing frameworks and utility libraries that enhance reuse but pose significant systemic risks.

Conclusion: The interconnected nature of Maven Central makes it efficient but vulnerable, as failures or vulnerabilities at critical hubs can have cascading ecosystem-wide impacts.

Abstract: Understanding the structural characteristics and connectivity patterns of
large-scale software ecosystems is critical for enhancing software reuse,
improving ecosystem resilience, and mitigating security risks. In this paper,
we investigate the Maven Central ecosystem, one of the largest repositories of
Java libraries, by applying network science techniques to its dependency graph.
Leveraging the Goblin framework, we extracted a sample consisting of the top
5,000 highly connected artifacts based on their degree centrality and then
performed breadth-first search (BFS) expansion from each selected artifact as a
seed node, traversing the graph outward to capture all libraries and releases
reachable those seed nodes. This sampling strategy captured the immediate
structural context surrounding these libraries resulted in a curated graph
comprising of 1.3 million nodes and 20.9 million edges. We conducted a
comprehensive analysis of this graph, computing degree distributions,
betweenness centrality, PageRank centrality, and connected components
graph-theoretic metrics. Our results reveal that Maven Central exhibits a
highly interconnected, scale-free, and small-world topology, characterized by a
small number of infrastructural hubs that support the majority of projects.
Further analysis using PageRank and betweenness centrality shows that these
hubs predominantly consist of core ecosystem infrastructure, including testing
frameworks and general-purpose utility libraries. While these hubs facilitate
efficient software reuse and integration, they also pose systemic risks;
failures or vulnerabilities affecting these critical nodes can have widespread
and cascading impacts throughout the ecosystem.

</details>


### [293] [Tight Inter-Core Cache Contention Analysis for WCET Estimation on Multicore Systems](https://arxiv.org/abs/2508.13863)
*Shuai Zhao,Jieyu Jiang,Shenlin Cai,Yaowei Liang,Chen Jie,Yinjie Fang,Wei Zhang,Guoquan Zhang,Yaoyao Gu,Xiang Xiao,Wei Qin,Xiangzhen Ouyang,Wanli Chang*

Main category: cs.SE

TL;DR: The paper introduces a new method for analyzing WCET (Worst-Case Execution Time) on multicore systems by refining inter-core cache contention calculations. Results show significant improvements in accuracy without high computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing WCET analysis methods overestimate cache miss occurrences caused by inter-core conflicts on multicore architectures, leading to inaccurate estimates. The paper aims to propose an improved method for tackling this issue.

Method: The authors identify memory references affected by remote accesses and construct a finer-grained contention analysis to estimate cache misses based on dynamic programming and the order of program regions.

Result: Experiments show the proposed analysis reduces inter-core cache interference by 52.31% and WCET estimations by 8.94% on average compared to existing methods.

Conclusion: The introduced method enhances WCET estimation accuracy for multicore systems by effectively addressing inter-core cache contention without adding significant computational overhead.

Abstract: WCET (Worst-Case Execution Time) estimation on multicore architecture is
particularly challenging mainly due to the complex accesses over cache shared
by multiple cores. Existing analysis identifies possible contentions between
parallel tasks by leveraging the partial order of the tasks or their program
regions. Unfortunately, they overestimate the number of cache misses caused by
a remote block access without considering the actual cache state and the number
of accesses. This paper reports a new analysis for inter-core cache contention.
Based on the order of program regions in a task, we first identify memory
references that could be affected if a remote access occurs in a region.
Afterwards, a fine-grained contention analysis is constructed that computes the
number of cache misses based on the access quantity of local and remote blocks.
We demonstrate that the overall inter-core cache interference of a task can be
obtained via dynamic programming. Experiments show that compared to existing
methods, the proposed analysis reduces inter-core cache interference and WCET
estimations by 52.31% and 8.94% on average, without significantly increasing
computation overhead.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [294] [Benchmarking spike source localization algorithms in high density probes](https://arxiv.org/abs/2508.13451)
*Hao Zhao,Xinhe Zhang,Arnau Marin-Llobet,Xinyi Lin,Jia Liu*

Main category: q-bio.NC

TL;DR: The paper benchmarks neuron localization algorithms to improve spike sorting and probe drift monitoring, using simulated and experimental datasets to identify performance differences.


<details>
  <summary>Details</summary>
Motivation: Neuron localization is crucial for enhancing brain-machine interfaces, improving spike sorting accuracy, and addressing challenges like probe drift in long-term recordings.

Method: Comparison of localization algorithms using simulated and experimental ground truth datasets, evaluating accuracy, robustness, and runtime under ideal and degraded conditions.

Result: Complex models excel in ideal conditions, whereas simple heuristics are more robust to noise and electrode degradation, suitable for long-term recordings.

Conclusion: The study establishes a framework for evaluating localization algorithms, promoting the development of robust and biologically grounded techniques for advanced brain-machine interface systems.

Abstract: Estimating neuron location from extracellular recordings is essential for
developing advanced brain-machine interfaces. Accurate neuron localization
improves spike sorting, which involves detecting action potentials and
assigning them to individual neurons. It also helps monitor probe drift, which
affects long-term probe reliability. Although several localization algorithms
are currently in use, the field is nascent and arguments for using one
algorithm over another are largely theoretical or based on visual analysis of
clustering results. We present a first-of-its-kind benchmarking of commonly
used neuron localization algorithms. We tested these algorithms using two
ground truth datasets: a biophysically realistic simulated dataset, and
experimental data combining patch-clamp and Neuropixels probes. We
systematically evaluate the accuracy, robustness, and runtime of these
algorithms in ideal conditions and long-term recording conditions with
electrode decay. Our findings highlight significant performance differences;
while more complex and physically realistic models perform better in ideal
situations, simple heuristics demonstrate superior robustness to noise and
electrode degradation in experimental datasets, making them more suitable for
long-term neural recordings. This work provides a framework for assessing
localization algorithms and developing robust, biologically grounded algorithms
to advance the development of brain-machine interfaces.

</details>


### [295] [EEG Blink Artifacts Can Identify Read Music in Listening and Imagery](https://arxiv.org/abs/2508.13807)
*Abhinav Uppal,Dillan Cellier,Min Suk Lee,Sean Bauersfeld,Yuchen Xu,Shihab A. Shamma,Gert Cauwenberghs,Virginia R. de Sa*

Main category: q-bio.NC

TL;DR: The paper explores the role of eye blinks in EEG studies and investigates whether blink timing can help identify specific music scores being read.


<details>
  <summary>Details</summary>
Motivation: Traditionally, EEG studies discard eye-movement artifacts as noise. However, accumulating evidence indicates that blinks may be modulated by cognitive mechanisms such as attention.

Method: Using a public EEG dataset, the study tracks musicians’ eye blinks while they read music and listen to or imagine Bach chorales. A spike train distance metric is applied to analyze blink timing and cross-validation tests its ability to identify the music scores.

Result: Blink timing successfully identifies music being read above chance levels (best subject: 56%, chance: 25%). Accuracy varies depending on the subject, condition, and tunable timing shift costs.

Conclusion: Blinks may hold meaningful cognitive information, suggesting potential inclusion in brain decoding studies and wearables where they are easier to record than EEG signals.

Abstract: Eye-movement related artifacts including blinks and saccades are
significantly larger in amplitude than cortical activity as recorded by scalp
electroencephalography (EEG), but are typically discarded in EEG studies
focusing on cognitive mechanisms as explained by cortical source activity.
Accumulating evidence however indicates that spontaneous eye blinks are not
necessarily random, and can be modulated by attention and cognition beyond just
physiological necessities. In this exploratory analysis we reanalyze a public
EEG dataset of musicians listening to or imagining music (Bach chorales) while
simultaneously reading from a sheet of music. We ask whether blink timing in
reading music, accompanied by listening or imagery, is sufficient to uniquely
identify the music being read from a given score. Intra-subject blink counts
and timing are compared across trials using a spike train distance metric
(Victor and Purpura, 1997). One-trial-left-out cross-validation is used to
identify the music being read with above chance level accuracy (best subject:
56\%, chance: 25\%), where accuracy is seen to vary with subject, condition,
and a tunable cost factor for time shifts. Future studies may consider
incorporating eye blink contributions to brain decoding, especially in
wearables where eye blinks could be easier to record than EEG given their
higher amplitudes.

</details>


### [296] [Stochastic synaptic dynamics under learning](https://arxiv.org/abs/2508.13846)
*Jakob Stubenrauch,Naomi Auer,Richard Kempter,Benjamin Lindner*

Main category: q-bio.NC

TL;DR: This paper provides a probabilistic framework for modeling synaptic plasticity under stochastic neural activity, applying analytical tools to explore memory dynamics in STDP synapses with integrate-and-fire neuron networks.


<details>
  <summary>Details</summary>
Motivation: Understand the stochastic nature of synaptic plasticity due to randomness in neural activity and evaluate its influence on memory dynamics.

Method: The authors derive drift- and diffusion coefficients for synaptic dynamics, develop micro- and macro-dynamical analytical models, and assess memory degradation and capacity using spike-time resolving cross-correlations.

Result: A detailed microdynamical description enhances accuracy in predicting memory capacities, particularly in sparse coding scenarios, overcoming significant deviations from standard rate-based theories.

Conclusion: Incorporating spike-time resolving cross-correlations is crucial for accurate analysis of memory dynamics in recurrent networks. The proposed framework opens avenues for extended exploration of synaptic processes.

Abstract: Learning is based on synaptic plasticity, which drives and is driven by
neural activity. Because pre- and postsynaptic spiking activity is shaped by
randomness, the synaptic weights follow a stochastic process, requiring a
probabilistic framework to capture the noisy synaptic dynamics. We consider a
paradigmatic supervised learning example: a presynaptic neural population
impinging in a sequence of episodes on a recurrent network of
integrate-and-fire neurons through synapses undergoing spike-timing-dependent
plasticity (STDP) with additive potentiation and multiplicative depression. We
first analytically compute the drift- and diffusion coefficients for a single
synapse within a single episode (microscopic dynamics), mapping the true jump
process to a Langevin and the associated Fokker-Planck equations. Leveraging
new analytical tools, we include spike-time--resolving cross-correlations
between pre- and postsynaptic spikes, which corrects substantial deviations
seen in standard theories purely based on firing rates. We then apply this
microdynamical description to the network setup in which hetero-associations
are trained over one-shot episodes into a feed-forward matrix of STDP synapses
connecting to neurons of the recurrent network (macroscopic dynamics). By
mapping statistically distinct synaptic populations to instances of the
single-synapse process above, we self-consistently determine the joint neural
and synaptic dynamics and, ultimately, the time course of memory degradation
and the memory capacity. We demonstrate that specifically in the relevant case
of sparse coding, our theory can quantitatively capture memory capacities which
are strongly overestimated if spike-time--resolving cross-correlations are
ignored. We conclude with a discussion of the many directions in which our
framework can be extended.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [297] [Preference Models assume Proportional Hazards of Utilities](https://arxiv.org/abs/2508.13189)
*Chirag Nagpal*

Main category: stat.ML

TL;DR: This paper connects the Plackett-Luce model to the Cox Proportional Hazards model to explore their implications in estimating preferences.


<details>
  <summary>Details</summary>
Motivation: To bridge the Plackett-Luce model, used in preference estimation, with the Cox Proportional Hazards model and better understand their implications.

Method: The paper establishes a theoretical connection between the two statistical models and analyzes their relationships.

Result: Revealed the link between the Plackett-Luce model and the Cox Proportional Hazards model, providing insights into their connection.

Conclusion: Understanding this connection can enhance the theoretical understanding and application of models in preference estimation and AI alignment.

Abstract: Approaches for estimating preferences from human annotated data typically
involves inducing a distribution over a ranked list of choices such as the
Plackett-Luce model. Indeed, modern AI alignment tools such as Reward Modelling
and Direct Preference Optimization are based on the statistical assumptions
posed by the Plackett-Luce model. In this paper, I will connect the
Plackett-Luce model to another classical and well known statistical model, the
Cox Proportional Hazards model and attempt to shed some light on the
implications of the connection therein.

</details>


### [298] [Structural Foundations for Leading Digit Laws: Beyond Probabilistic Mixtures](https://arxiv.org/abs/2508.13237)
*Vladimir Berman*

Main category: stat.ML

TL;DR: The paper proposes a deterministic framework to explain the distribution of leading digits in numerical data, moving away from probabilistic models.


<details>
  <summary>Details</summary>
Motivation: To reveal how underlying arithmetic and structural properties determine the distribution of leading significant digits, offering an alternative to probabilistic models.

Method: Introduces a shift-invariant functional equation solved using affine-plus-periodic formulas to model digit distributions in different datasets.

Result: The framework explains empirical and mathematical digit distributions, including deviations, and analyzes deterministic sequences and fractal features.

Conclusion: This work establishes a unified mathematical basis for understanding digital phenomena, providing tools for further theoretical and applied exploration.

Abstract: This article presents a modern deterministic framework for the study of
leading significant digit distributions in numerical data. Rather than relying
on traditional probabilistic or mixture-based explanations, we demonstrate that
the observed frequencies of leading digits are determined by the underlying
arithmetic, algorithmic, and structural properties of the data-generating
process. Our approach centers on a shift-invariant functional equation, whose
general solution is given by explicit affine-plus-periodic formulas. This
structural formulation explains the diversity of digit distributions
encountered in both empirical and mathematical datasets, including cases with
pronounced deviations from logarithmic or scale-invariant profiles.
  We systematically analyze digit distributions in finite and infinite
datasets, address deterministic sequences such as prime numbers and recurrence
relations, and highlight the emergence of block-structured and fractal
features. The article provides critical examination of probabilistic models,
explicit examples and counterexamples, and discusses limitations and open
problems for further research. Overall, this work establishes a unified
mathematical foundation for digital phenomena and offers a versatile toolset
for modeling and analyzing digit patterns in applied and theoretical contexts.

</details>


### [299] [Flow Matching-Based Generative Modeling for Efficient and Scalable Data Assimilation](https://arxiv.org/abs/2508.13313)
*Taos Transue,Bohan Chen,So Takao,Bao Wang*

Main category: stat.ML

TL;DR: This paper introduces the Ensemble Flow Filter (EnFF), a training-free data assimilation framework based on flow matching, to improve computational efficiency and offer flexible probability path design for high-dimensional systems.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the computational challenges in high-dimensional, nonlinear data assimilation settings, particularly with generative approaches like ensemble score filters, which require slow and expensive sampling.

Method: The proposed Ensemble Flow Filter (EnFF) incorporates Monte Carlo estimators for flow matching vector fields and utilizes localized guidance to assimilate observations. It is designed to be training-free and improves on flexibility and speed.

Result: Experiments demonstrate that EnFF achieves better cost-accuracy tradeoffs and leverages larger ensembles compared to prior methods. Additionally, it theoretically encompasses classical filtering methods as special cases.

Conclusion: The work establishes flow matching as a scalable framework for high-dimensional data assimilation, showcasing significant improvements in efficiency and adaptability.

Abstract: Data assimilation (DA) is the problem of sequentially estimating the state of
a dynamical system from noisy observations. Recent advances in generative
modeling have inspired new approaches to DA in high-dimensional nonlinear
settings, especially the ensemble score filter (EnSF). However, these come at a
significant computational burden due to slow sampling. In this paper, we
introduce a new filtering framework based on flow matching (FM) -- called the
ensemble flow filter (EnFF) -- to accelerate sampling and enable flexible
design of probability paths. EnFF -- a training-free DA approach -- integrates
MC estimators for the marginal FM vector field (VF) and a localized guidance to
assimilate observations. EnFF has faster sampling and more flexibility in VF
design compared to existing generative modeling for DA. Theoretically, we show
that EnFF encompasses classical filtering methods such as the bootstrap
particle filter and the ensemble Kalman filter as special cases. Experiments on
high-dimensional filtering benchmarks demonstrate improved cost-accuracy
tradeoffs and the ability to leverage larger ensembles than prior methods. Our
results highlight the promise of FM as a scalable tool for filtering in
high-dimensional applications that enable the use of large ensembles.

</details>


### [300] [Smooth Flow Matching](https://arxiv.org/abs/2508.13831)
*Jianbin Tan,Anru R. Zhang*

Main category: stat.ML

TL;DR: The paper introduces Smooth Flow Matching (SFM), a novel framework for generating synthetic functional data to overcome challenges such as privacy concerns, irregular sampling, and infinite dimensionality.


<details>
  <summary>Details</summary>
Motivation: Functional data analysis faces challenges like privacy, irregular sampling, and non-Gaussian structures, especially in domains like health informatics and epidemiology.

Method: SFM uses semiparametric copula flow techniques inspired by flow-matching ideas, ensuring computational efficiency, smoothness, and flexibility for generating synthetic functional data.

Result: Simulation studies show SFM's superiority in synthetic data quality and computational efficiency. Additionally, SFM successfully generates high-quality clinical trajectory data from MIMIC-IV EHR.

Conclusion: SFM is a practical and versatile tool enabling synthetic functional data generation, enhancing utility in clinical applications while addressing privacy and statistical challenges.

Abstract: Functional data, i.e., smooth random functions observed over a continuous
domain, are increasingly available in areas such as biomedical research, health
informatics, and epidemiology. However, effective statistical analysis for
functional data is often hindered by challenges such as privacy constraints,
sparse and irregular sampling, infinite dimensionality, and non-Gaussian
structures. To address these challenges, we introduce a novel framework named
Smooth Flow Matching (SFM), tailored for generative modeling of functional data
to enable statistical analysis without exposing sensitive real data. Built upon
flow-matching ideas, SFM constructs a semiparametric copula flow to generate
infinite-dimensional functional data, free from Gaussianity or low-rank
assumptions. It is computationally efficient, handles irregular observations,
and guarantees the smoothness of the generated functions, offering a practical
and flexible solution in scenarios where existing deep generative methods are
not applicable. Through extensive simulation studies, we demonstrate the
advantages of SFM in terms of both synthetic data quality and computational
efficiency. We then apply SFM to generate clinical trajectory data from the
MIMIC-IV patient electronic health records (EHR) longitudinal database. Our
analysis showcases the ability of SFM to produce high-quality surrogate data
for downstream statistical tasks, highlighting its potential to boost the
utility of EHR data for clinical applications.

</details>


### [301] [Online Conformal Selection with Accept-to-Reject Changes](https://arxiv.org/abs/2508.13838)
*Kangdao Liu,Huajun Xi,Chi-Man Vong,Hongxin Wei*

Main category: stat.ML

TL;DR: This paper introduces OCS-ARC, an online conformal selection framework addressing the irreversible selection constraints in sequential decision-making processes, ensuring false discovery rate (FDR) control and improved selection power.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in online scenarios where conformal selection's deselection of candidates is impractical, especially in processes like drug discovery that require irreversible decisions.

Method: The authors propose the Online Conformal Selection with Accept-to-Reject Changes (OCS-ARC) method, incorporating the online Benjamini-Hochberg procedure to handle irreversible selections, while ensuring FDR control under i.i.d. and exchangeable data assumptions.

Result: OCS-ARC demonstrates significant improvement in selection power compared to baselines while maintaining valid FDR control, as confirmed by experiments on synthetic and real-world datasets.

Conclusion: OCS-ARC addresses limitations of existing online conformal selection methods by introducing irreversible decision handling, offering robust FDR control and effectiveness in both univariate and multivariate settings.

Abstract: Selecting a subset of promising candidates from a large pool is crucial
across various scientific and real-world applications. Conformal selection
offers a distribution-free and model-agnostic framework for candidate selection
with uncertainty quantification. While effective in offline settings, its
application to online scenarios, where data arrives sequentially, poses
challenges. Notably, conformal selection permits the deselection of previously
selected candidates, which is incompatible with applications requiring
irreversible selection decisions. This limitation is particularly evident in
resource-intensive sequential processes, such as drug discovery, where
advancing a compound to subsequent stages renders reversal impractical. To
address this issue, we extend conformal selection to an online Accept-to-Reject
Changes (ARC) procedure: non-selected data points can be reconsidered for
selection later, and once a candidate is selected, the decision is
irreversible. Specifically, we propose a novel conformal selection method,
Online Conformal Selection with Accept-to-Reject Changes (dubbed OCS-ARC),
which incorporates online Benjamini-Hochberg procedure into the candidate
selection process. We provide theoretical guarantees that OCS-ARC controls the
false discovery rate (FDR) at or below the nominal level at any timestep under
both i.i.d. and exchangeable data assumptions. Additionally, we theoretically
show that our approach naturally extends to multivariate response settings.
Extensive experiments on synthetic and real-world datasets demonstrate that
OCS-ARC significantly improves selection power over the baseline while
maintaining valid FDR control across all examined timesteps.

</details>


### [302] [Generalisation and benign over-fitting for linear regression onto random functional covariates](https://arxiv.org/abs/2508.13895)
*Andrew Jones,Nick Whiteley*

Main category: stat.ML

TL;DR: This paper analyzes ridge and ridge-less regression in a non-i.i.d. data setting, focusing on predictive performance and excess risk bounds.


<details>
  <summary>Details</summary>
Motivation: To understand the theoretical predictive performance of ridge regression in settings with exchangeable but non-independent covariate vectors.

Method: The authors use assumptions of independence across dimensions, 4th-order moments, and regularity conditions, leveraging probabilistic bounds and previous theoretical results.

Result: Probabilistic bounds for predictive excess risk were derived under specific model regimes where the number of features grows in relation to samples.

Conclusion: The study highlights how model ingredients and covariate noise influence predictive convergence and benign-overfitting behavior.

Abstract: We study theoretical predictive performance of ridge and ridge-less
least-squares regression when covariate vectors arise from evaluating $p$
random, means-square continuous functions over a latent metric space at $n$
random and unobserved locations, subject to additive noise. This leads us away
from the standard assumption of i.i.d. data to a setting in which the $n$
covariate vectors are exchangeable but not independent in general. Under an
assumption of independence across dimensions, $4$-th order moment, and other
regularity conditions, we obtain probabilistic bounds on a notion of predictive
excess risk adapted to our random functional covariate setting, making use of
recent results of Barzilai and Shamir. We derive convergence rates in regimes
where $p$ grows suitably fast relative to $n$, illustrating interplay between
ingredients of the model in determining convergence behaviour and the role of
additive covariate noise in benign-overfitting.

</details>


### [303] [A PC Algorithm for Max-Linear Bayesian Networks](https://arxiv.org/abs/2508.13967)
*Carlos Améndola,Benjamin Hollering,Francesco Nowell*

Main category: stat.ML

TL;DR: The paper focuses on causal discovery in Max-linear Bayesian networks (MLBNs) with heavy-tailed distributions.


<details>
  <summary>Details</summary>
Motivation: Current causal discovery algorithms fail on MLBNs due to non-faithfulness to d-separation.

Method: Explored constraint-based discovery using a $\ast$-separation oracle; introduced PCstar algorithm for $C^\ast$-separation.

Result: PC algorithm shown consistent with $\ast$-separation; PCstar algorithm enabled additional edge orientation.

Conclusion: PCstar provides a refined causal discovery approach for MLBNs leveraging $C^\ast$-separation.

Abstract: Max-linear Bayesian networks (MLBNs) are a relatively recent class of
structural equation models which arise when the random variables involved have
heavy-tailed distributions. Unlike most directed graphical models, MLBNs are
typically not faithful to d-separation and thus classical causal discovery
algorithms such as the PC algorithm or greedy equivalence search can not be
used to accurately recover the true graph structure. In this paper, we begin
the study of constraint-based discovery algorithms for MLBNs given an oracle
for testing conditional independence in the true, unknown graph. We show that
if the oracle is given by the $\ast$-separation criteria in the true graph,
then the PC algorithm remains consistent despite the presence of additional CI
statements implied by $\ast$-separation. We also introduce a new causal
discovery algorithm named "PCstar" which assumes faithfulness to
$C^\ast$-separation and is able to orient additional edges which cannot be
oriented with only d- or $\ast$-separation.

</details>


### [304] [Uncertainty-Aware PCA for Arbitrarily Distributed Data Modeled by Gaussian Mixture Models](https://arxiv.org/abs/2508.13990)
*Daniel Klötzl,Ozan Tastekin,David Hägele,Marina Evers,Daniel Weiskopf*

Main category: stat.ML

TL;DR: The paper introduces a method for projecting multidimensional data with uncertainties into a low-dimensional space while better preserving distribution details using Gaussian mixture models (GMMs) and an improved uncertainty-aware approach.


<details>
  <summary>Details</summary>
Motivation: The motivation is to handle multidimensional data with uncertainties that are not well-represented by normal distributions and to improve dimensionality reduction techniques to better capture these complexities.

Method: The paper uses Gaussian mixture models (GMMs) to represent multidimensional probability distributions and derives projections that can handle arbitrary probability density functions. The method also incorporates user-defined weights to customize the relative importance of different distributions.

Result: The proposed method produced low-dimensional representations of data densities that offer more detailed and faithful depictions of the original distributions compared to traditional UAPCA and sample-based projection methods.

Conclusion: The approach improves upon UAPCA by allowing better visualization and analysis of uncertain multidimensional data, with added flexibility in weighting distributions for specific applications.

Abstract: Multidimensional data is often associated with uncertainties that are not
well-described by normal distributions. In this work, we describe how such
distributions can be projected to a low-dimensional space using
uncertainty-aware principal component analysis (UAPCA). We propose to model
multidimensional distributions using Gaussian mixture models (GMMs) and derive
the projection from a general formulation that allows projecting arbitrary
probability density functions. The low-dimensional projections of the densities
exhibit more details about the distributions and represent them more faithfully
compared to UAPCA mappings. Further, we support including user-defined weights
between the different distributions, which allows for varying the importance of
the multidimensional distributions. We evaluate our approach by comparing the
distributions in low-dimensional space obtained by our method and UAPCA to
those obtained by sample-based projections.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [305] [Analog computation with transcriptional networks](https://arxiv.org/abs/2508.14017)
*David Doty,Mina Latifi,David Soloveichick*

Main category: cs.CC

TL;DR: The paper establishes that controlling only the transcription rates is mathematically sufficient for constructing analog computations, even without controlling degradation, and demonstrates its validity with examples and a Python-based compiler.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of transcriptional networks for analog computation while reducing the need for complex control, especially over degradation rates, which has been a significant limitation in synthetic biology.

Method: The authors prove mathematically that the control of only transcription rates is sufficient for analog computations and validate this theory by constructing transcriptional networks that implement various dynamic processes using this paradigm.

Result: They demonstrate diverse applications of their approach, including oscillatory and chaotic dynamics, memory, analog computational systems like sorting and a PID controller, and offer a Python package to systematically design transcriptional networks for any polynomial ODE system.

Conclusion: The study simplifies the engineering of synthetic transcriptional networks for analog computation by removing the need for degradation control, providing a novel and systematic approach to explore and design dynamics in biology.

Abstract: Transcriptional networks represent one of the most extensively studied types
of systems in synthetic biology. Although the completeness of transcriptional
networks for digital logic is well-established, *analog* computation plays a
crucial role in biological systems and offers significant potential for
synthetic biology applications. While transcriptional circuits typically rely
on cooperativity and highly non-linear behavior of transcription factors to
regulate *production* of proteins, they are often modeled with simple linear
*degradation* terms. In contrast, general analog dynamics require both
non-linear positive as well as negative terms, seemingly necessitating control
over not just transcriptional (i.e., production) regulation but also the
degradation rates of transcription factors.
  Surprisingly, we prove that controlling transcription factor production
(i.e., transcription rate) without explicitly controlling degradation is
mathematically complete for analog computation, achieving equivalent
capabilities to systems where both production and degradation are programmable.
We demonstrate our approach on several examples including oscillatory and
chaotic dynamics, analog sorting, memory, PID controller, and analog extremum
seeking. Our result provides a systematic methodology for engineering novel
analog dynamics using synthetic transcriptional networks without the added
complexity of degradation control and informs our understanding of the
capabilities of natural transcriptional circuits.
  We provide a compiler, in the form of a Python package that can take any
system of polynomial ODEs and convert it to an equivalent transcriptional
network implementing the system *exactly*, under appropriate conditions.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [306] [Observed Control -- Linearly Scalable Nonlinear Model Predictive Control with Adaptive Horizons](https://arxiv.org/abs/2508.13339)
*Eugene T. Hamzezadeh,Andrew J. Petruska*

Main category: math.OC

TL;DR: This paper explores the connection between state estimation methods and model predictive control, introducing a computationally efficient predictive control framework called "observed control."


<details>
  <summary>Details</summary>
Motivation: The paper aims to address scalability and computational efficiency issues in model predictive control, particularly for systems with varying time horizon lengths.

Method: The proposed method leverages Kalman smoothers as the optimization framework, introduces duality-based control computation, and separates linear model predictive control into reactive and anticipatory components.

Result: Experiments demonstrate that the framework achieves high computational efficiency, supports adaptive time horizons, enables optimization termination early, and extends to nonlinear systems using tools like extended Kalman filter and unscented Kalman filter.

Conclusion: The paper establishes a robust and theoretically-backed framework for linear and nonlinear predictive control, emphasizing scalability, stability, and practical implementation benefits.

Abstract: This work highlights the duality between state estimation methods and model
predictive control. A predictive controller, observed control, is presented
that uses this duality to efficiently compute control actions with linear
time-horizon length scalability. The proposed algorithms provide exceptional
computational efficiency, adaptive time horizon lengths, and early optimization
termination criteria. The use of Kalman smoothers as the backend optimization
framework provides for a straightforward implementation supported by strong
theoretical guarantees. Additionally, a formulation is presented that separates
linear model predictive control into purely reactive and anticipatory
components, enabling any-time any-horizon observed control while ensuring
controller stability for short time horizons. Finally, numerical case studies
confirm that nonlinear filter extensions, i.e., the extended Kalman filter and
unscented Kalman filter, effectively extend observed control to nonlinear
systems and objectives.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [307] [Conflicting Scores, Confusing Signals: An Empirical Study of Vulnerability Scoring Systems](https://arxiv.org/abs/2508.13644)
*Viktoria Koscinski,Mark Nelson,Ahmet Okutan,Robert Falso,Mehdi Mirakhorli*

Main category: cs.CR

TL;DR: This study compares four vulnerability scoring systems using real-world data to evaluate their effectiveness and inconsistencies in prioritizing and assessing real-world exploitation risks.


<details>
  <summary>Details</summary>
Motivation: To address the inconsistencies and disparities in existing vulnerability scoring systems and their impact on effective vulnerability management.

Method: Conducted a large-scale empirical comparison of four scoring systems (CVSS, SSVC, EPSS, Exploitability Index) using 600 real-world vulnerabilities from Microsoft's Patch Tuesday disclosures.

Result: The study found significant disparities in how the scoring systems rank vulnerabilities, influencing decision-making in risk prioritization.

Conclusion: The paper emphasizes the need for more transparent, consistent, and aligned scoring systems to enable reliable risk-based decisions.

Abstract: Accurately assessing software vulnerabilities is essential for effective
prioritization and remediation. While various scoring systems exist to support
this task, their differing goals, methodologies and outputs often lead to
inconsistent prioritization decisions. This work provides the first
large-scale, outcome-linked empirical comparison of four publicly available
vulnerability scoring systems: the Common Vulnerability Scoring System (CVSS),
the Stakeholder-Specific Vulnerability Categorization (SSVC), the Exploit
Prediction Scoring System (EPSS), and the Exploitability Index. We use a
dataset of 600 real-world vulnerabilities derived from four months of
Microsoft's Patch Tuesday disclosures to investigate the relationships between
these scores, evaluate how they support vulnerability management task, how
these scores categorize vulnerabilities across triage tiers, and assess their
ability to capture the real-world exploitation risk. Our findings reveal
significant disparities in how scoring systems rank the same vulnerabilities,
with implications for organizations relying on these metrics to make
data-driven, risk-based decisions. We provide insights into the alignment and
divergence of these systems, highlighting the need for more transparent and
consistent exploitability, risk, and severity assessments.

</details>


### [308] [On the Security and Privacy of Federated Learning: A Survey with Attacks, Defenses, Frameworks, Applications, and Future Directions](https://arxiv.org/abs/2508.13730)
*Daniel M. Jimenez-Gutierrez,Yelizaveta Falkouskaya,Jose L. Hernandez-Ramos,Aris Anagnostopoulos,Ioannis Chatzigiannakis,Andrea Vitaletti*

Main category: cs.CR

TL;DR: This survey discusses challenges and solutions related to federated learning (FL), analyzing attacks and defenses in over 200 papers and proposing future directions.


<details>
  <summary>Details</summary>
Motivation: Federated Learning is vulnerable to security and privacy threats despite its design for data privacy; hence, understanding attacks and defenses is crucial.

Method: The paper categorizes security-enhancing and privacy-preserving techniques in FL while critically analyzing trade-offs and implications on performance.

Result: The study evaluates existing FL defenses, highlights their strengths and limitations, and identifies open research problems, considering non-IID data impact.

Conclusion: It emphasizes the development of scalable, adaptive, and energy-efficient FL solutions to robustly address security and privacy challenges in dynamic environments.

Abstract: Federated Learning (FL) is an emerging distributed machine learning paradigm
enabling multiple clients to train a global model collaboratively without
sharing their raw data. While FL enhances data privacy by design, it remains
vulnerable to various security and privacy threats. This survey provides a
comprehensive overview of more than 200 papers regarding the state-of-the-art
attacks and defense mechanisms developed to address these challenges,
categorizing them into security-enhancing and privacy-preserving techniques.
Security-enhancing methods aim to improve FL robustness against malicious
behaviors such as byzantine attacks, poisoning, and Sybil attacks. At the same
time, privacy-preserving techniques focus on protecting sensitive data through
cryptographic approaches, differential privacy, and secure aggregation. We
critically analyze the strengths and limitations of existing methods, highlight
the trade-offs between privacy, security, and model performance, and discuss
the implications of non-IID data distributions on the effectiveness of these
defenses. Furthermore, we identify open research challenges and future
directions, including the need for scalable, adaptive, and energy-efficient
solutions operating in dynamic and heterogeneous FL environments. Our survey
aims to guide researchers and practitioners in developing robust and
privacy-preserving FL systems, fostering advancements safeguarding
collaborative learning frameworks' integrity and confidentiality.

</details>


### [309] [Too Easily Fooled? Prompt Injection Breaks LLMs on Frustratingly Simple Multiple-Choice Questions](https://arxiv.org/abs/2508.13214)
*Xuyang Guo,Zekai Huang,Zhao Song,Jiahao Zhang*

Main category: cs.CR

TL;DR: The paper examines how Large Language Models (LLMs), useful in areas like education and peer review, are vulnerable to hidden prompt injection attacks in simple arithmetic tasks within PDFs.


<details>
  <summary>Details</summary>
Motivation: To address concerns about the robustness of LLMs in judge-like applications by exploring their susceptibility to malicious prompt injection.

Method: LLMs were tested on arithmetic questions embedded in PDFs with hidden injected prompts, assessing whether they could be misled.

Result: LLMs exhibited vulnerability to hidden prompt injection attacks, even with simple tasks like arithmetic.

Conclusion: The findings raise critical issues about the reliability of LLMs in roles requiring judgment, due to susceptibility to such attacks.

Abstract: Large Language Models (LLMs) have recently demonstrated strong emergent
abilities in complex reasoning and zero-shot generalization, showing
unprecedented potential for LLM-as-a-judge applications in education, peer
review, and data quality evaluation. However, their robustness under prompt
injection attacks, where malicious instructions are embedded into the content
to manipulate outputs, remains a significant concern. In this work, we explore
a frustratingly simple yet effective attack setting to test whether LLMs can be
easily misled. Specifically, we evaluate LLMs on basic arithmetic questions
(e.g., "What is 3 + 2?") presented as either multiple-choice or true-false
judgment problems within PDF files, where hidden prompts are injected into the
file. Our results reveal that LLMs are indeed vulnerable to such hidden prompt
injection attacks, even in these trivial scenarios, highlighting serious
robustness risks for LLM-as-a-judge applications.

</details>


### [310] [MCPSecBench: A Systematic Security Benchmark and Playground for Testing Model Context Protocols](https://arxiv.org/abs/2508.13220)
*Yixuan Yang,Daoyuan Wu,Yufan Chen*

Main category: cs.CR

TL;DR: The paper establishes a taxonomy of security risks in the Model Context Protocol (MCP), introduces a benchmark tool called MCPSecBench, and evaluates its effectiveness against attacks on major MCP providers like Claude, OpenAI, and Cursor.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the increasing integration of Large Language Models (LLMs) into applications via MCP, amplifying capabilities but introducing security vulnerabilities.

Method: Authors provide a taxonomy of MCP-related security issues, develop MCPSecBench for systematic assessment, and experimentally analyze attack impacts on MCP providers.

Result: Experimental evaluation reveals over 85% success rate in compromising platforms, with certain vulnerabilities universally impacting major providers, and variability in prompt-based/tool attacks.

Conclusion: The study standardizes MCP security evaluation, enabling systematic and rigorous testing of MCP implementations to mitigate vulnerabilities.

Abstract: Large Language Models (LLMs) are increasingly integrated into real-world
applications via the Model Context Protocol (MCP), a universal, open standard
for connecting AI agents with data sources and external tools. While MCP
enhances the capabilities of LLM-based agents, it also introduces new security
risks and expands their attack surfaces. In this paper, we present the first
systematic taxonomy of MCP security, identifying 17 attack types across 4
primary attack surfaces. We introduce MCPSecBench, a comprehensive security
benchmark and playground that integrates prompt datasets, MCP servers, MCP
clients, and attack scripts to evaluate these attacks across three major MCP
providers. Our benchmark is modular and extensible, allowing researchers to
incorporate custom implementations of clients, servers, and transport protocols
for systematic security assessment. Experimental results show that over 85% of
the identified attacks successfully compromise at least one platform, with core
vulnerabilities universally affecting Claude, OpenAI, and Cursor, while
prompt-based and tool-centric attacks exhibit considerable variability across
different hosts and models. Overall, MCPSecBench standardizes the evaluation of
MCP security and enables rigorous testing across all MCP layers.

</details>


### [311] [Quantifying Loss Aversion in Cyber Adversaries via LLM Analysis](https://arxiv.org/abs/2508.13240)
*Soham Hans,Nikolos Gurney,Stacy Marsella,Sofia Hirschmann*

Main category: cs.CR

TL;DR: This paper uses large language models (LLMs) to analyze hacker behaviors and cognitive biases, specifically loss aversion, aiming to enhance cybersecurity strategies.


<details>
  <summary>Details</summary>
Motivation: Traditional cybersecurity approaches struggle to dynamically interpret ongoing attacks and exploit cognitive vulnerabilities of attackers.

Method: Using LLMs, hacker-generated notes from experiments are analyzed to segment actions and correlate them with predefined persistence mechanisms tied to loss aversion.

Result: The findings show that LLMs effectively interpret hacker behaviors and reveal how loss aversion influences their decision-making processes.

Conclusion: Behavior-based analysis via LLMs offers a promising path for real-time interpretation of hacker motivations and enhances cybersecurity defense capabilities.

Abstract: Understanding and quantifying human cognitive biases from empirical data has
long posed a formidable challenge, particularly in cybersecurity, where
defending against unknown adversaries is paramount. Traditional cyber defense
strategies have largely focused on fortification, while some approaches attempt
to anticipate attacker strategies by mapping them to cognitive vulnerabilities,
yet they fall short in dynamically interpreting attacks in progress. In
recognition of this gap, IARPA's ReSCIND program seeks to infer, defend
against, and even exploit attacker cognitive traits. In this paper, we present
a novel methodology that leverages large language models (LLMs) to extract
quantifiable insights into the cognitive bias of loss aversion from hacker
behavior. Our data are collected from an experiment in which hackers were
recruited to attack a controlled demonstration network. We process the hacker
generated notes using LLMs using it to segment the various actions and
correlate the actions to predefined persistence mechanisms used by hackers. By
correlating the implementation of these mechanisms with various operational
triggers, our analysis provides new insights into how loss aversion manifests
in hacker decision-making. The results demonstrate that LLMs can effectively
dissect and interpret nuanced behavioral patterns, thereby offering a
transformative approach to enhancing cyber defense strategies through
real-time, behavior-based analysis.

</details>


### [312] [Involuntary Jailbreak](https://arxiv.org/abs/2508.13246)
*Yangyang Guo,Yangyan Li,Mohan Kankanhalli*

Main category: cs.CR

TL;DR: This paper uncovers a new vulnerability in Large Language Models (LLMs) called involuntary jailbreak, demonstrating that a simple universal prompt can compromise safety measures across multiple leading models.


<details>
  <summary>Details</summary>
Motivation: To identify and expose potential weaknesses in LLM guardrails and urge the community to enhance safety alignment.

Method: A single universal prompt was designed to elicit responses from LLMs that would normally be blocked by their safety measures, testing various leading models.

Result: The universal prompt succeeded in bypassing the safety mechanisms of several prominent LLMs, including GPT 4.1, Claude Opus 4.1, Grok 4, and Gemini 2.5 Pro.

Conclusion: The findings highlight the fragility of current LLM safety mechanisms and call attention to the need for re-evaluating and strengthening these guardrails to protect against misuse.

Abstract: In this study, we disclose a worrying new vulnerability in Large Language
Models (LLMs), which we term \textbf{involuntary jailbreak}. Unlike existing
jailbreak attacks, this weakness is distinct in that it does not involve a
specific attack objective, such as generating instructions for \textit{building
a bomb}. Prior attack methods predominantly target localized components of the
LLM guardrail. In contrast, involuntary jailbreaks may potentially compromise
the entire guardrail structure, which our method reveals to be surprisingly
fragile. We merely employ a single universal prompt to achieve this goal. In
particular, we instruct LLMs to generate several questions that would typically
be rejected, along with their corresponding in-depth responses (rather than a
refusal). Remarkably, this simple prompt strategy consistently jailbreaks the
majority of leading LLMs, including Claude Opus 4.1, Grok 4, Gemini 2.5 Pro,
and GPT 4.1. We hope this problem can motivate researchers and practitioners to
re-evaluate the robustness of LLM guardrails and contribute to stronger safety
alignment in future.

</details>


### [313] [A Risk Manager for Intrusion Tolerant Systems: Enhancing HAL 9000 with New Scoring and Data Sources](https://arxiv.org/abs/2508.13364)
*Tadeu Freitas,Carlos Novo,Inês Dutra,João Soares,Manuel Correia,Benham Shariati,Rolando Martins*

Main category: cs.CR

TL;DR: This research extends the HAL 9000 ITS Risk Manager by adding a scraper that mines diverse threat sources to improve the detection and assessment of emerging threats.


<details>
  <summary>Details</summary>
Motivation: Existing ITS risk management systems rely too much on databases like NVD and ExploitDB, which require manual updates and are slow to respond to emerging threats.

Method: A new scraping mechanism was developed to automatically mine data from diverse threat sources like advisories, research forums, and exploit proofs-of-concept. This intelligence was integrated into HAL 9000's existing machine learning-based framework.

Result: The enhanced HAL 9000 system demonstrated significant improvements in detecting and assessing unverified vulnerabilities early, expanding its intelligence base.

Conclusion: Integrating a diverse set of threat intelligence sources allows HAL 9000 to deliver more proactive and comprehensive risk management for ITS architectures.

Abstract: Intrusion Tolerant Systems (ITSs) have become increasingly critical due to
the rise of multi-domain adversaries exploiting diverse attack surfaces. ITS
architectures aim to tolerate intrusions, ensuring system compromise is
prevented or mitigated even with adversary presence. Existing ITS solutions
often employ Risk Managers leveraging public security intelligence to adjust
system defenses dynamically against emerging threats. However, these approaches
rely heavily on databases like NVD and ExploitDB, which require manual analysis
for newly discovered vulnerabilities. This dependency limits the system's
responsiveness to rapidly evolving threats. HAL 9000, an ITS Risk Manager
introduced in our prior work, addressed these challenges through machine
learning. By analyzing descriptions of known vulnerabilities, HAL 9000 predicts
and assesses new vulnerabilities automatically. To calculate the risk of a
system, it also incorporates the Exploitability Probability Scoring system to
estimate the likelihood of exploitation within 30 days, enhancing proactive
defense capabilities.
  Despite its success, HAL 9000's reliance on NVD and ExploitDB knowledge is a
limitation, considering the availability of other sources of information. This
extended work introduces a custom-built scraper that continuously mines diverse
threat sources, including security advisories, research forums, and real-time
exploit proofs-of-concept. This significantly expands HAL 9000's intelligence
base, enabling earlier detection and assessment of unverified vulnerabilities.
Our evaluation demonstrates that integrating scraper-derived intelligence with
HAL 9000's risk management framework substantially improves its ability to
address emerging threats. This paper details the scraper's integration into the
architecture, its role in providing additional information on new threats, and
the effects on HAL 9000's management.

</details>


### [314] [Know Me by My Pulse: Toward Practical Continuous Authentication on Wearable Devices via Wrist-Worn PPG](https://arxiv.org/abs/2508.13690)
*Wei Shao,Zequan Liang,Ruoyu Zhang,Ruijie Fang,Ning Miao,Ehsan Kourkchi,Setareh Rafatirad,Houman Homayoun,Chongzhou Fang*

Main category: cs.CR

TL;DR: The paper proposes a low-frequency, energy-efficient biometric authentication method for wearables using PPG signals with 25 Hz sampling. It demonstrates strong accuracy, reduced power consumption, and robustness under various conditions.


<details>
  <summary>Details</summary>
Motivation: Current biometric systems like ECG are intrusive and impractical for continuous use, while existing PPG-based solutions require high frequency and computational power, making them unsuitable for real-world wearable devices.

Method: The authors develop and evaluate a continuous authentication system called We-Be Band using 25 Hz multi-channel PPG, leveraging a Bi-LSTM model with an attention mechanism to extract identity-specific features from 4-channel signals sampled over short 4s windows.

Result: The system achieves an average accuracy of 88.11%, macro F1-score of 0.88, FAR of 0.48%, FRR of 11.77%, and EER of 2.76%. It reduces sensor power consumption by 53% compared to 512 Hz and 19% compared to 128 Hz setups.

Conclusion: The proposed 25 Hz sampling strikes a balance between energy efficiency and performance, making it practical for wearable devices. However, robustness improves with activity-diverse training, and models trained only on resting data fail under motion conditions.

Abstract: Biometric authentication using physiological signals offers a promising path
toward secure and user-friendly access control in wearable devices. While
electrocardiogram (ECG) signals have shown high discriminability, their
intrusive sensing requirements and discontinuous acquisition limit
practicality. Photoplethysmography (PPG), on the other hand, enables
continuous, non-intrusive authentication with seamless integration into
wrist-worn wearable devices. However, most prior work relies on high-frequency
PPG (e.g., 75 - 500 Hz) and complex deep models, which incur significant energy
and computational overhead, impeding deployment in power-constrained real-world
systems. In this paper, we present the first real-world implementation and
evaluation of a continuous authentication system on a smartwatch, We-Be Band,
using low-frequency (25 Hz) multi-channel PPG signals. Our method employs a
Bi-LSTM with attention mechanism to extract identity-specific features from
short (4 s) windows of 4-channel PPG. Through extensive evaluations on both
public datasets (PTTPPG) and our We-Be Dataset (26 subjects), we demonstrate
strong classification performance with an average test accuracy of 88.11%,
macro F1-score of 0.88, False Acceptance Rate (FAR) of 0.48%, False Rejection
Rate (FRR) of 11.77%, and Equal Error Rate (EER) of 2.76%. Our 25 Hz system
reduces sensor power consumption by 53% compared to 512 Hz and 19% compared to
128 Hz setups without compromising performance. We find that sampling at 25 Hz
preserves authentication accuracy, whereas performance drops sharply at 20 Hz
while offering only trivial additional power savings, underscoring 25 Hz as the
practical lower bound. Additionally, we find that models trained exclusively on
resting data fail under motion, while activity-diverse training improves
robustness across physiological states.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [315] [AutoMPC: A Code Generator for MPC-based Automated Driving](https://arxiv.org/abs/2508.13656)
*Georg Schildbach,Jasper Pflughaupt*

Main category: eess.SY

TL;DR: The paper introduces AutoMPC, a software package designed to make Model Predictive Control (MPC) more computationally efficient and easier to implement for vehicle trajectory tracking.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of high computational demands and complexity in implementing MPC for industrial production vehicles.

Method: The proposed method is a robust active set algorithm embedded into a customized MPC framework, featuring automatic C-code generation for deployment on embedded platforms.

Result: Simulation scenarios demonstrate that AutoMPC is versatile, robust, computationally efficient, and guarantees a feasible solution in diverse driving conditions, including high speeds, drifting, and direction changes.

Conclusion: AutoMPC successfully reduces the computational burden and complexity of implementing MPC, making it a viable solution for industrial use in various driving scenarios.

Abstract: Model Predictive Control (MPC) is a powerful technique to control nonlinear,
multi-input multi-output systems subject to input and state constraints. It is
now a standard tool for trajectory tracking control of automated vehicles. As
such it has been used in many research and development projects. However, MPC
faces several challenges to be integrated into industrial production vehicles.
The most important ones are its high computational demands and the complexity
of implementation. The software packages AutoMPC aims to address both of these
challenges. It builds on a robustified version of an active set algorithm for
Nonlinear MPC. The algorithm is embedded into a framework for vehicle
trajectory tracking, which makes it easy to used, yet highly customizable.
Automatic code generation transforms the selections into a standalone,
computationally efficient C-code file with static memory allocation. As such it
can be readily deployed on a wide range of embedded platforms, e.g., based on
Matlab/Simulink or Robot Operating System (ROS). Compared to a previous version
of the code, the vehicle model and the numerical integration method can be
manually specified, besides basic algorithm parameters. All of this information
and all specifications are directly baked into the generated C-code. The
algorithm is suitable driving scenarios at low or high speeds, even drifting,
and supports direction changes. Multiple simulation scenarios show the
versatility and effectiveness of the AutoMPC code, with the guarantee of a
feasible solution, a high degree of robustness, and computational efficiency.

</details>


### [316] [Towards safe control parameter tuning in distributed multi-agent systems](https://arxiv.org/abs/2508.13608)
*Abdullah Tokmak,Thomas B. Schön,Dominik Baumann*

Main category: eess.SY

TL;DR: The paper addresses distributed multi-agent optimization focusing on safety-critical systems using safe Bayesian optimization, creating a time-varying reformulation with nearest-neighbor communication and a spatio-temporal kernel.


<details>
  <summary>Details</summary>
Motivation: The goal is improving the performance and safety of distributed multi-agent systems found in real-world scenarios like autonomous driving, a setting complicated by nonconvex rewards and constraints.

Method: Utilizing safe Bayesian optimization with Gaussian process regression, reformulating the optimization problem with a latent time variable and a custom spatio-temporal kernel.

Result: The proposed algorithm was successfully tested and deployed in simulations.

Conclusion: This work bridges distributed optimization and safe learning paradigms for multi-agent systems, offering a sample-efficient and neighbor-interacting approach.

Abstract: Many safety-critical real-world problems, such as autonomous driving and
collaborative robots, are of a distributed multi-agent nature. To optimize the
performance of these systems while ensuring safety, we can cast them as
distributed optimization problems, where each agent aims to optimize their
parameters to maximize a coupled reward function subject to coupled
constraints. Prior work either studies a centralized setting, does not consider
safety, or struggles with sample efficiency. Since we require sample efficiency
and work with unknown and nonconvex rewards and constraints, we solve this
optimization problem using safe Bayesian optimization with Gaussian process
regression. Moreover, we consider nearest-neighbor communication between the
agents. To capture the behavior of non-neighboring agents, we reformulate the
static global optimization problem as a time-varying local optimization problem
for each agent, essentially introducing time as a latent variable. To this end,
we propose a custom spatio-temporal kernel to integrate prior knowledge. We
show the successful deployment of our algorithm in simulations.

</details>


### [317] [Model-based Multi-object Visual Tracking: Identification and Standard Model Limitations](https://arxiv.org/abs/2508.13647)
*Jan Krejčí,Oliver Kost,Yuxuan Xia,Lennart Svensson,Ondřej Straka*

Main category: eess.SY

TL;DR: This paper applies radar-based multi-object tracking methods using a Poisson multi-Bernoulli mixture (PMBM) filter to improve pedestrian tracking from 2D bounding box detections, revealing potential model-data mismatches.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance pedestrian tracking methods by leveraging established radar tracking techniques to improve accuracy using existing datasets like MOT-17.

Method: The work employs a Poisson multi-Bernoulli mixture (PMBM) filter under the standard point-object model, paying particular attention to parameter selection both from theoretical principles and dataset analysis.

Result: The proposed PMBM algorithm shows promising results but highlights a mismatch between the assumptions of the model and the real-world data.

Conclusion: Future improvements in model-based pedestrian tracking methods should focus on addressing the identified mismatches between theoretical models and empirical data.

Abstract: This paper uses multi-object tracking methods known from the radar tracking
community to address the problem of pedestrian tracking using 2D bounding box
detections. The standard point-object (SPO) model is adopted, and the posterior
density is computed using the Poisson multi-Bernoulli mixture (PMBM) filter.
The selection of the model parameters rooted in continuous time is discussed,
including the birth and survival probabilities. Some parameters are selected
from the first principles, while others are identified from the data, which is,
in this case, the publicly available MOT-17 dataset. Although the resulting
PMBM algorithm yields promising results, a mismatch between the SPO model and
the data is revealed. The model-based approach assumes that modifying the
problematic components causing the SPO model-data mismatch will lead to better
model-based algorithms in future developments.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [318] [Uncertainty-Aware Learning Policy for Reliable Pulmonary Nodule Detection on Chest X-Ray](https://arxiv.org/abs/2508.13236)
*Hyeonjin Choi,Jinse Kim,Dong-yeon Yoo,Ju-sung Sun,Jung-won Lee*

Main category: eess.IV

TL;DR: Medical AI faces trust issues due to diagnostic uncertainty stemming from a lack of background knowledge.


<details>
  <summary>Details</summary>
Motivation: Physicians struggle with variable chest X-ray interpretation, while skepticism towards medical AI limits adoption due to its diagnostic uncertainty.

Method: The study proposes an Uncertainty-Aware Learning Policy that integrates physicians' background knowledge with lesion data.

Result: The model demonstrated 92% accuracy (IoU 0.2, FPPI 2), improved sensitivity by 10%, and reduced uncertainty entropy by 0.2.

Conclusion: Integrating physicians' background knowledge with AI enhances diagnostic accuracy, sensitivity, and credibility of medical AI systems.

Abstract: Early detection and rapid intervention of lung cancer are crucial.
Nonetheless, ensuring an accurate diagnosis is challenging, as physicians'
ability to interpret chest X-rays varies significantly depending on their
experience and degree of fatigue. Although medical AI has been rapidly
advancing to assist in diagnosis, physicians' trust in such systems remains
limited, preventing widespread clinical adoption. This skepticism fundamentally
stems from concerns about its diagnostic uncertainty. In clinical diagnosis,
physicians utilize extensive background knowledge and clinical experience. In
contrast, medical AI primarily relies on repetitive learning of the target
lesion to generate diagnoses based solely on that data. In other words, medical
AI does not possess sufficient knowledge to render a diagnosis, leading to
diagnostic uncertainty. Thus, this study suggests an Uncertainty-Aware Learning
Policy that can address the issue of knowledge deficiency by learning the
physicians' background knowledge alongside the Chest X-ray lesion information.
We used 2,517 lesion-free images and 656 nodule images, all obtained from Ajou
University Hospital. The proposed model attained 92% (IoU 0.2 / FPPI 2) with a
10% enhancement in sensitivity compared to the baseline model while also
decreasing entropy as a measure of uncertainty by 0.2.

</details>


### [319] [Sex-Specific Vascular Score: A Novel Perfusion Biomarker from Supervoxel Analysis of 3D pCASL MRI](https://arxiv.org/abs/2508.13173)
*Sneha Noble,Neelam Sinha,Vaanathi Sundareshan,Thomas Gregor Issac*

Main category: eess.IV

TL;DR: The paper introduces a framework using 3D pCASL MRI to compute sex-specific vascular scores for assessing cerebrovascular health and disease susceptibility, achieving 95% accuracy in sex classification.


<details>
  <summary>Details</summary>
Motivation: To improve the understanding of normative brain perfusion patterns, age-related effects in cerebrovascular health, and their implications for neurodegenerative diseases.

Method: The approach includes supervoxel clustering for brain parcellation, extracting mean CBF values, and using a custom convolutional neural network for sex-based classification.

Result: The neural network achieved 95% accuracy in identifying sex-specific brain perfusion patterns, along with systematic evaluation of regional CBF variations and aging effects.

Conclusion: The framework provides insights into brain perfusion differences and enhances possibilities for early detection and personalized treatment of neurodegenerative diseases like Alzheimer's.

Abstract: We propose a novel framework that leverages 3D pseudo-continuous arterial
spin labeling (3D pCASL) MRI to compute sex-specific vascular scores that
quantify cerebrovascular health and potential disease susceptibility. The brain
is parcellated into spatially contiguous regions of homogeneous perfusion using
supervoxel clustering, capturing both microvascular and macrovascular
contributions. Mean cerebral blood flow (CBF) values are extracted from 186
cognitively healthy participants and used to train a custom convolutional
neural network, achieving 95 percent accuracy in sex classification. This
highlights robust, sex-specific perfusion patterns across the brain.
Additionally, regional CBF variations and age-related effects are
systematically evaluated within male and female cohorts. The proposed vascular
risk-scoring framework enhances understanding of normative brain perfusion and
aging, and may facilitate early detection and personalized interventions for
neurodegenerative diseases such as Alzheimer's.

</details>


### [320] [Automated Cervical Cancer Detection through Visual Inspection with Acetic Acid in Resource-Poor Settings with Lightweight Deep Learning Models Deployed on an Android Device](https://arxiv.org/abs/2508.13253)
*Leander Melroy Maben,Keerthana Prasad,Shyamala Guruvare,Vidya Kudva,P C Siddalingaswamy*

Main category: eess.IV

TL;DR: The paper discusses an AI-based system to automate cervical cancer screening in low-resource areas using a lightweight deep learning model with high accuracy, sensitivity, and specificity.


<details>
  <summary>Details</summary>
Motivation: Cervical cancer is highly prevalent in low and middle-income countries, and although treatable, the lack of accessible screening methods leads to high mortality rates. The study aims to automate and simplify visual inspection with acetic acid (VIA) to make screenings more practical and scalable.

Method: The authors designed a lightweight deep learning model integrating EfficientDet-Lite3 for ROI detection and MobileNet-V2 for classification. The system is deployable on Android devices to provide instant results without reliance on advanced infrastructure, trained medical professionals, or internet connectivity.

Result: The classification model displayed 92.31% accuracy, 98.24% sensitivity, and 88.37% specificity in tests, demonstrating its effectiveness for remote application.

Conclusion: The proposed system is a feasible and promising solution for automating cervical cancer screenings in low-resource settings by reducing reliance on highly-trained staff and infrastructure.

Abstract: Cervical cancer is among the most commonly occurring cancer among women and
claims a huge number of lives in low and middle-income countries despite being
relatively easy to treat. Several studies have shown that public screening
programs can bring down cervical cancer incidence and mortality rates
significantly. While several screening tests are available, visual inspection
with acetic acid (VIA) presents itself as the most viable option for
low-resource settings due to the affordability and simplicity of performing the
test. VIA requires a trained medical professional to interpret the test and is
subjective in nature. Automating VIA using AI eliminates subjectivity and would
allow shifting of the task to less trained health workers. Task shifting with
AI would help further expedite screening programs in low-resource settings. In
our work, we propose a lightweight deep learning algorithm that includes
EfficientDet-Lite3 as the Region of Interest (ROI) detector and a MobileNet- V2
based model for classification. These models would be deployed on an
android-based device that can operate remotely and provide almost instant
results without the requirement of highly-trained medical professionals, labs,
sophisticated infrastructure, or internet connectivity. The classification
model gives an accuracy of 92.31%, a sensitivity of 98.24%, and a specificity
of 88.37% on the test dataset and presents itself as a promising automated
low-resource screening approach.

</details>


### [321] [Colon Polyps Detection from Colonoscopy Images Using Deep Learning](https://arxiv.org/abs/2508.13188)
*Md Al Amin,Bikash Kumar Paul*

Main category: eess.IV

TL;DR: The study examines YOLOv5 object detection variants for early polyp identification in colonoscopy images and finds YOLOv5l achieves the best detection performance.


<details>
  <summary>Details</summary>
Motivation: Early detection of colon polyps is crucial to prevent colorectal cancer, a major global health concern.

Method: Deep learning-based object detection applied to colonoscopy images using the Kvasir-SEG dataset and variants of the YOLOv5 algorithm.

Result: YOLOv5l demonstrated superior performance with a mean average precision of 85.1% and average Intersection over Union of 0.86.

Conclusion: The YOLOv5l model shows promise in improving colorectal cancer screening with its enhanced polyp detection capabilities.

Abstract: Colon polyps are precursors to colorectal cancer, a leading cause of
cancer-related mortality worldwide. Early detection is critical for improving
patient outcomes. This study investigates the application of deep
learning-based object detection for early polyp identification using
colonoscopy images. We utilize the Kvasir-SEG dataset, applying extensive data
augmentation and splitting the data into training (80\%), validation (20\% of
training), and testing (20\%) sets. Three variants of the YOLOv5 architecture
(YOLOv5s, YOLOv5m, YOLOv5l) are evaluated. Experimental results show that
YOLOv5l outperforms the other variants, achieving a mean average precision
(mAP) of 85.1\%, with the highest average Intersection over Union (IoU) of
0.86. These findings demonstrate that YOLOv5l provides superior detection
performance for colon polyp localization, offering a promising tool for
enhancing colorectal cancer screening accuracy.

</details>


### [322] [Benchmarking GPT-5 for Zero-Shot Multimodal Medical Reasoning in Radiology and Radiation Oncology](https://arxiv.org/abs/2508.13192)
*Mingzhe Hu,Zach Eidex,Shansong Wang,Mojtaba Safari,Qiang Li,Xiaofeng Yang*

Main category: eess.IV

TL;DR: This paper evaluates the performance of GPT-5 and its variants in tasks involving medical imaging, radiology, and physics decision-making, demonstrating superior accuracy compared to GPT-4o.


<details>
  <summary>Details</summary>
Motivation: To assess whether advancements in large multimodal models like GPT-5 can deliver measurable improvements in critical medical domains.

Method: The study conducted a zero-shot evaluation of GPT-5 and its variants on three tasks: VQA-RAD, SLAKE, and a curated dataset of Medical Physics Board Examination-style questions.

Result: GPT-5 outperformed GPT-4o across all datasets, with gains up to +20% in challenging anatomical regions and achieving a 90.7% accuracy on board physics questions, surpassing human passing thresholds.

Conclusion: GPT-5 consistently offers significant performance improvements over GPT-4o, making it valuable for expert workflows in radiology and therapeutic physics.

Abstract: Radiology, radiation oncology, and medical physics require decision-making
that integrates medical images, textual reports, and quantitative data under
high-stakes conditions. With the introduction of GPT-5, it is critical to
assess whether recent advances in large multimodal models translate into
measurable gains in these safety-critical domains. We present a targeted
zero-shot evaluation of GPT-5 and its smaller variants (GPT-5-mini, GPT-5-nano)
against GPT-4o across three representative tasks. We present a targeted
zero-shot evaluation of GPT-5 and its smaller variants (GPT-5-mini, GPT-5-nano)
against GPT-4o across three representative tasks: (1) VQA-RAD, a benchmark for
visual question answering in radiology; (2) SLAKE, a semantically annotated,
multilingual VQA dataset testing cross-modal grounding; and (3) a curated
Medical Physics Board Examination-style dataset of 150 multiple-choice
questions spanning treatment planning, dosimetry, imaging, and quality
assurance. Across all datasets, GPT-5 achieved the highest accuracy, with
substantial gains over GPT-4o up to +20.00% in challenging anatomical regions
such as the chest-mediastinal, +13.60% in lung-focused questions, and +11.44%
in brain-tissue interpretation. On the board-style physics questions, GPT-5
attained 90.7% accuracy (136/150), exceeding the estimated human passing
threshold, while GPT-4o trailed at 78.0%. These results demonstrate that GPT-5
delivers consistent and often pronounced performance improvements over GPT-4o
in both image-grounded reasoning and domain-specific numerical problem-solving,
highlighting its potential to augment expert workflows in medical imaging and
therapeutic physics.

</details>


### [323] [PediDemi -- A Pediatric Demyelinating Lesion Segmentation Dataset](https://arxiv.org/abs/2508.13239)
*Maria Popa,Gabriela Adriana Visa*

Main category: eess.IV

TL;DR: This paper introduces a first-of-its-kind public dataset for pediatric demyelinating lesion segmentation, which includes MRI scans and extensive metadata. The dataset addresses the lack of data in pediatric cases and aids in analyzing demyelinating disorders beyond MS.


<details>
  <summary>Details</summary>
Motivation: There is a significant gap in publicly available datasets for pediatric cases of demyelinating disorders, particularly those other than Multiple Sclerosis (MS), limiting research in diagnosis and monitoring.

Method: The study collected MRI scans from 13 pediatric patients diagnosed with demyelinating disorders, including ADEM, and annotated them with lesion segmentation masks and associated medical metadata. A state-of-the-art lesion segmentation model was tested on the dataset to evaluate it.

Result: The model evaluation highlighted the importance of diverse datasets, confirming the dataset's value and potential in improving the analysis of demyelinating diseases.

Conclusion: The publicly available pediatric dataset fills an existing gap and is expected to enhance diagnostic and research efforts in demyelinating disorders, especially in pediatric populations.

Abstract: Demyelinating disorders of the central nervous system may have multiple
causes, the most common are infections, autoimmune responses, genetic or
vascular etiology. Demyelination lesions are characterized by areas were the
myelin sheath of the nerve fibers are broken or destroyed. Among autoimmune
disorders, Multiple Sclerosis (MS) is the most well-known Among these
disorders, Multiple Sclerosis (MS) is the most well-known and aggressive form.
Acute Disseminated Encephalomyelitis (ADEM) is another type of demyelinating
disease, typically with a better prognosis. Magnetic Resonance Imaging (MRI) is
widely used for diagnosing and monitoring disease progression by detecting
lesions. While both adults and children can be affected, there is a significant
lack of publicly available datasets for pediatric cases and demyelinating
disorders beyond MS. This study introduces, for the first time, a publicly
available pediatric dataset for demyelinating lesion segmentation. The dataset
comprises MRI scans from 13 pediatric patients diagnosed with demyelinating
disorders, including 3 with ADEM. In addition to lesion segmentation masks, the
dataset includes extensive patient metadata, such as diagnosis, treatment,
personal medical background, and laboratory results. To assess the quality of
the dataset and demonstrate its relevance, we evaluate a state-of-the-art
lesion segmentation model trained on an existing MS dataset. The results
underscore the importance of diverse datasets

</details>


### [324] [InnerGS: Internal Scenes Rendering via Factorized 3D Gaussian Splatting](https://arxiv.org/abs/2508.13287)
*Shuxin Liang,Yihan Xiao,Wenlu Tang*

Main category: eess.IV

TL;DR: This paper extends 3D Gaussian Splatting to reconstruct internal scenes, enabling understanding of object interiors from sparse data.


<details>
  <summary>Details</summary>
Motivation: To address the need for understanding and reconstructing internal structures of objects, as current methods focus mainly on external surfaces.

Method: The paper models continuous volumetric density through inner 3D Gaussian distribution, reconstructing internal structures without camera poses and supporting diverse data modalities.

Result: Smooth and detailed internal structures are reconstructed effectively using sparse sliced data, with a CUDA implementation provided.

Conclusion: The approach is versatile, eliminating camera pose requirements and being plug-and-play and compatible with various data modalities.

Abstract: 3D Gaussian Splatting (3DGS) has recently gained popularity for efficient
scene rendering by representing scenes as explicit sets of anisotropic 3D
Gaussians. However, most existing work focuses primarily on modeling external
surfaces. In this work, we target the reconstruction of internal scenes, which
is crucial for applications that require a deep understanding of an object's
interior. By directly modeling a continuous volumetric density through the
inner 3D Gaussian distribution, our model effectively reconstructs smooth and
detailed internal structures from sparse sliced data. Our approach eliminates
the need for camera poses, is plug-and-play, and is inherently compatible with
any data modalities. We provide cuda implementation at:
https://github.com/Shuxin-Liang/InnerGS.

</details>


### [325] [Susceptibility Distortion Correction of Diffusion MRI with a single Phase-Encoding Direction](https://arxiv.org/abs/2508.13340)
*Sedigheh Dargahi,Sylvain Bouix,Christian Desrosier*

Main category: eess.IV

TL;DR: This paper presents a deep learning-based approach for correcting susceptibility-induced distortion in diffusion MRI using a single acquisition.


<details>
  <summary>Details</summary>
Motivation: The work aims to address the challenge of susceptibility-induced distortion in diffusion MRI, which compromises geometric and intensity accuracy, especially when only a single phase encoding direction is available.

Method: A deep learning-based model is developed to perform susceptibility distortion correction using only a single acquisition, eliminating the reliance on paired blip-up and blip-down images.

Result: The proposed model achieves performance comparable to traditional correction methods like topup, validating its effectiveness.

Conclusion: The method offers a practical and efficient alternative for susceptibility distortion correction in diffusion MRI, expanding its potential use in retrospective data where paired acquisitions are unavailable.

Abstract: Diffusion MRI (dMRI) is a valuable tool to map brain microstructure and
connectivity by analyzing water molecule diffusion in tissue. However,
acquiring dMRI data requires to capture multiple 3D brain volumes in a short
time, often leading to trade-offs in image quality. One challenging artifact is
susceptibility-induced distortion, which introduces significant geometric and
intensity deformations. Traditional correction methods, such as topup, rely on
having access to blip-up and blip-down image pairs, limiting their
applicability to retrospective data acquired with a single phase encoding
direction. In this work, we propose a deep learning-based approach to correct
susceptibility distortions using only a single acquisition (either blip-up or
blip-down), eliminating the need for paired acquisitions. Experimental results
show that our method achieves performance comparable to topup, demonstrating
its potential as an efficient and practical alternative for susceptibility
distortion correction in dMRI.

</details>


### [326] [Towards Understanding and Harnessing the Transferability of Prognostic Knowledge in Computational Pathology](https://arxiv.org/abs/2508.13482)
*Pei Liu,Luping Ji,Jiaxiang Gou,Xiangxiang Zeng*

Main category: eess.IV

TL;DR: The paper focuses on advancing WSI-based cancer prognosis by introducing a systematic study on prognostic knowledge transfer between cancers, emphasizing the reuse of knowledge from common cancers to rare tumor types.


<details>
  <summary>Details</summary>
Motivation: Current WSI-based prognosis methods are limited to cancer-specific models that cannot effectively use knowledge from other cancers, hindering scalability for rare tumors and generalizability.

Method: The paper curates a large dataset (UNI2-h-DSS) covering 13 cancers, investigates factors influencing knowledge transfer, and proposes a baseline approach (MoE-PKT) with a routing mechanism for leveraging generalizable prognostic knowledge.

Result: The study demonstrates the feasibility of transferring prognostic models to rare tumors and establishes the utility of the proposed MoE-PKT mechanism, as well as insights into factors driving successful knowledge transfer.

Conclusion: The research sets a foundation for knowledge transfer in WSI-based cancer prognosis, providing a means to optimize models for rare cancer types using insights from other cancers.

Abstract: Whole-Slide Image (WSI) is an important tool for evaluating the prognosis of
cancer patients. Present WSI-based prognosis studies generally follow a
conventional paradigm -- cancer-specific model development -- where one cancer
disease corresponds to one model and this model cannot make use of the
prognostic knowledge from others. Despite its notable success in recent years,
this paradigm has inherent limitations and has always been struggling with
practical requirements: (i) scaling to the rare tumor diseases with very
limited samples and (ii) benefiting from the generalizable prognostic knowledge
in other cancers. To this end, this paper presents the first systematic study
on Prognostic Knowledge Transfer in Pathology, called Path-PKT. It comprises
three main parts. (1) We curate a large dataset (UNI2-h-DSS) with 13 cancers
and use it to evaluate the transferability of prognostic knowledge between
different cancers computationally. (2) We design experiments to understand what
factors affect knowledge transfer and what causes positive transfers. (3)
Motivated by empirical findings, we propose a new baseline approach (MoE-PKT)
with a routing mechanism to utilize the generalizable prognostic knowledge in
other cancers. Finally, we show the transferability of source models to rare
tumor diseases. This study could lay solid foundations for the study of
knowledge transfer in WSI-based cancer prognosis. Source code is available at
https://github.com/liupei101/Path-PKT.

</details>


### [327] [State of Abdominal CT Datasets: A Critical Review of Bias, Clinical Relevance, and Real-world Applicability](https://arxiv.org/abs/2508.13626)
*Saeide Danaei,Zahra Dehghanian,Elahe Meftah,Nariman Naderi,Seyed Amir Ahmad Safavi-Naini,Faeze Khorasanizade,Hamid R. Rabiee*

Main category: eess.IV

TL;DR: The paper assesses the quality and biases in publicly available abdominal CT datasets for AI use, highlighting redundancy, geographic skew, and biases affecting model generalizability.


<details>
  <summary>Details</summary>
Motivation: To investigate the limitations and biases in publicly available abdominal CT datasets used for AI applications, with the goal of improving AI model equity and clinical robustness.

Method: The authors systematically reviewed 46 abdominal CT datasets, analyzed bias in datasets with over 100 cases, and identified challenges linked to domain shift and selection bias.

Result: Significant dataset redundancy (59.1%), geographic bias (75.3% from Western regions), and high-risk biases (63% domain shift, 57% selection bias) were revealed in the dataset subset.

Conclusion: Improving datasets via institutional collaborations, standardized protocols, and inclusion of diverse populations is vital for advancing generalizable and equitable AI models in abdominal imaging.

Abstract: This systematic review critically evaluates publicly available abdominal CT
datasets and their suitability for artificial intelligence (AI) applications in
clinical settings. We examined 46 publicly available abdominal CT datasets
(50,256 studies). Across all 46 datasets, we found substantial redundancy
(59.1\% case reuse) and a Western/geographic skew (75.3\% from North America
and Europe). A bias assessment was performed on the 19 datasets with >=100
cases; within this subset, the most prevalent high-risk categories were domain
shift (63\%) and selection bias (57\%), both of which may undermine model
generalizability across diverse healthcare environments -- particularly in
resource-limited settings. To address these challenges, we propose targeted
strategies for dataset improvement, including multi-institutional
collaboration, adoption of standardized protocols, and deliberate inclusion of
diverse patient populations and imaging technologies. These efforts are crucial
in supporting the development of more equitable and clinically robust AI models
for abdominal imaging.

</details>


### [328] [subCellSAM: Zero-Shot (Sub-)Cellular Segmentation for Hit Validation in Drug Discovery](https://arxiv.org/abs/2508.13701)
*Jacob Hanimann,Daniel Siegismund,Mario Wieser,Stephan Steigele*

Main category: eess.IV

TL;DR: The paper introduces a zero-shot segmentation model employing a self-prompting mechanism for analyzing large-scale cell images in drug discovery. This avoids domain-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current methods for cell segmentation in high-throughput drug discovery require extensive manual tuning or specialized fine-tuned models, creating inefficiencies.

Method: The paper proposes a three-step segmentation method that integrates a segmentation foundation model with an in-context learning strategy and self-prompting mechanisms.

Result: Their model achieves accurate segmentation of nuclei, cells, and subcellular structures across standard benchmarks and industry-specific assays without custom dataset tuning.

Conclusion: The segmentation approach streamlines analysis of drug discovery datasets, offering efficient, adaptable solutions for biopharma applications without manual or domain-specific adjustments.

Abstract: High-throughput screening using automated microscopes is a key driver in
biopharma drug discovery, enabling the parallel evaluation of thousands of drug
candidates for diseases such as cancer. Traditional image analysis and deep
learning approaches have been employed to analyze these complex, large-scale
datasets, with cell segmentation serving as a critical step for extracting
relevant structures. However, both strategies typically require extensive
manual parameter tuning or domain-specific model fine-tuning. We present a
novel method that applies a segmentation foundation model in a zero-shot
setting (i.e., without fine-tuning), guided by an in-context learning strategy.
Our approach employs a three-step process for nuclei, cell, and subcellular
segmentation, introducing a self-prompting mechanism that encodes morphological
and topological priors using growing masks and strategically placed
foreground/background points. We validate our method on both standard cell
segmentation benchmarks and industry-relevant hit validation assays,
demonstrating that it accurately segments biologically relevant structures
without the need for dataset-specific tuning.

</details>


### [329] [Deep Biomechanically-Guided Interpolation for Keypoint-Based Brain Shift Registration](https://arxiv.org/abs/2508.13762)
*Tiago Assis,Ines P. Machado,Benjamin Zwick,Nuno C. Garcia,Reuben Dorent*

Main category: eess.IV

TL;DR: The paper introduces a deep learning approach to enhance registration accuracy in neurosurgery by combining sparse keypoints with biomechanical modeling.


<details>
  <summary>Details</summary>
Motivation: Current keypoint-based registration methods are robust in handling large deformations during neurosurgery but lack biomechanical considerations, leading to inaccuracies in dense deformation predictions.

Method: The authors propose a residual 3D U-Net that refines standard geometric interpolations using biomechanical simulations of synthetic brain deformations.

Result: Experiments demonstrate a significant improvement in mean square error (halving it) compared to classical interpolation techniques, with negligible computational cost.

Conclusion: This new approach provides a more accurate and computationally efficient method for predicting brain deformations, benefiting neuronavigation systems during surgery.

Abstract: Accurate compensation of brain shift is critical for maintaining the
reliability of neuronavigation during neurosurgery. While keypoint-based
registration methods offer robustness to large deformations and topological
changes, they typically rely on simple geometric interpolators that ignore
tissue biomechanics to create dense displacement fields. In this work, we
propose a novel deep learning framework that estimates dense, physically
plausible brain deformations from sparse matched keypoints. We first generate a
large dataset of synthetic brain deformations using biomechanical simulations.
Then, a residual 3D U-Net is trained to refine standard interpolation estimates
into biomechanically guided deformations. Experiments on a large set of
simulated displacement fields demonstrate that our method significantly
outperforms classical interpolators, reducing by half the mean square error
while introducing negligible computational overhead at inference time. Code
available at:
\href{https://github.com/tiago-assis/Deep-Biomechanical-Interpolator}{https://github.com/tiago-assis/Deep-Biomechanical-Interpolator}.

</details>


### [330] [Comparing Conditional Diffusion Models for Synthesizing Contrast-Enhanced Breast MRI from Pre-Contrast Images](https://arxiv.org/abs/2508.13776)
*Sebastian Ibarra,Javier del Riego,Alessandro Catanese,Julian Cuba,Julian Cardona,Nataly Leon,Jonathan Infante,Karim Lekadir,Oliver Diaz,Richard Osuala*

Main category: eess.IV

TL;DR: This study introduces generative models to synthesize DCE-MRI images without contrast agents, evaluating 22 model types and highlighting tumor-aware approaches for improved clinical results.


<details>
  <summary>Details</summary>
Motivation: To address the limitations and risks associated with contrast agents in DCE-MRI, such as safety issues and cost, while preserving diagnostic accuracy.

Method: They developed pre-contrast conditioned denoising diffusion probabilistic models, incorporating tumor-aware loss functions and segmentation mask conditioning, evaluated using multiple metrics and a multicenter dataset.

Result: Subtraction image-based models consistently outperformed others, with tumor-aware losses and segmentation masks enhancing metrics and qualitative analysis of lesion fidelity. A reader study showed high realism in synthetic images.

Conclusion: Generative models for contrast-enhanced MRI show promising clinical applications, although certain methods may depend on tumor localization inputs often unavailable during screening.

Abstract: Dynamic contrast-enhanced (DCE) MRI is essential for breast cancer diagnosis
and treatment. However, its reliance on contrast agents introduces safety
concerns, contraindications, increased cost, and workflow complexity. To this
end, we present pre-contrast conditioned denoising diffusion probabilistic
models to synthesize DCE-MRI, introducing, evaluating, and comparing a total of
22 generative model variants in both single-breast and full breast settings.
Towards enhancing lesion fidelity, we introduce both tumor-aware loss functions
and explicit tumor segmentation mask conditioning. Using a public multicenter
dataset and comparing to respective pre-contrast baselines, we observe that
subtraction image-based models consistently outperform post-contrast-based
models across five complementary evaluation metrics. Apart from assessing the
entire image, we also separately evaluate the region of interest, where both
tumor-aware losses and segmentation mask inputs improve evaluation metrics. The
latter notably enhance qualitative results capturing contrast uptake, albeit
assuming access to tumor localization inputs that are not guaranteed to be
available in screening settings. A reader study involving 2 radiologists and 4
MRI technologists confirms the high realism of the synthetic images, indicating
an emerging clinical potential of generative contrast-enhancement. We share our
codebase at https://github.com/sebastibar/conditional-diffusion-breast-MRI.

</details>


### [331] [Optimizing Region of Interest Selection for Effective Embedding in Video Steganography Based on Genetic Algorithms](https://arxiv.org/abs/2508.13710)
*Nizheen A. Ali,Ramadhan J. Mstafa*

Main category: eess.IV

TL;DR: The paper introduces a video steganography method using Genetic Algorithm (GA) for ROI identification and AES encryption for data security, achieving high efficiency with PSNR values between 64-75 dBs.


<details>
  <summary>Details</summary>
Motivation: To enhance data security and privacy in transmitted videos while maintaining video quality and efficiency.

Method: The proposed approach uses Genetic Algorithm (GA) to identify the Region of Interest (ROI) in a video for data embedding. The secret data is encrypted using AES before embedding and utilizes up to 10% of the cover video.

Result: The method achieves a Peak Signal to Noise Ratio (PSNR) of 64-75 dBs, suggesting minimal quality alteration, and enables quick encoding and decoding—suitable for real-time applications.

Conclusion: The proposed steganography method is effective for secure and efficient data embedding in videos without compromising quality, and it's applicable to real-time scenarios.

Abstract: With the widespread use of the internet, there is an increasing need to
ensure the security and privacy of transmitted data. This has led to an
intensified focus on the study of video steganography, which is a technique
that hides data within a video cover to avoid detection. The effectiveness of
any steganography method depends on its ability to embed data without altering
the original video quality while maintaining high efficiency. This paper
proposes a new method to video steganography, which involves utilizing a
Genetic Algorithm (GA) for identifying the Region of Interest (ROI) in the
cover video. The ROI is the area in the video that is the most suitable for
data embedding. The secret data is encrypted using the Advanced Encryption
Standard (AES), which is a widely accepted encryption standard, before being
embedded into the cover video, utilizing up to 10% of the cover video. This
process ensures the security and confidentiality of the embedded data. The
performance metrics for assessing the proposed method are the Peak Signal to
Noise Ratio (PSNR) and the encoding and decoding time. The results show that
the proposed method has a high embedding capacity and efficiency, with a PSNR
ranging between 64 and 75 dBs, which indicates that the embedded data is almost
indistinguishable from the original video. Additionally, the method can encode
and decode data quickly, making it efficient for real time applications.

</details>


### [332] [Latent Interpolation Learning Using Diffusion Models for Cardiac Volume Reconstruction](https://arxiv.org/abs/2508.13826)
*Niklas Bubeck,Suprosanna Shit,Chen Chen,Can Zhao,Pengfei Guo,Dong Yang,Georg Zitzlsberger,Daguang Xu,Bernhard Kainz,Daniel Rueckert,Jiazhen Pan*

Main category: eess.IV

TL;DR: This paper introduces CaLID, a framework for 3D whole-heart reconstruction from sparse 2D CMR slices using diffusion models, achieving significant improvements in accuracy and efficiency without auxiliary inputs.


<details>
  <summary>Details</summary>
Motivation: Traditional methods of reconstructing 3D whole-heart images from 2D slices are limited by predefined interpolation schemes, inefficiency, and reliance on additional semantic inputs, which hinder comprehensive cardiac assessment.

Method: The proposed CaLID framework leverages diffusion models for data-driven interpolation, operates efficiently in latent space for faster upsampling, and works directly with sparse 2D CMR slices without requiring auxiliary input like segmentation labels.

Result: CaLID achieves state-of-the-art performance in reconstructing whole-heart volumes and modeling spatiotemporal dynamics, while being 24 times more computationally efficient than existing methods.

Conclusion: The CaLID framework addresses key limitations of current 3D cardiac reconstruction methods, offering higher reconstruction quality, efficiency, and simplified workflows, making it a robust solution for cardiovascular imaging.

Abstract: Cardiac Magnetic Resonance (CMR) imaging is a critical tool for diagnosing
and managing cardiovascular disease, yet its utility is often limited by the
sparse acquisition of 2D short-axis slices, resulting in incomplete volumetric
information. Accurate 3D reconstruction from these sparse slices is essential
for comprehensive cardiac assessment, but existing methods face challenges,
including reliance on predefined interpolation schemes (e.g., linear or
spherical), computational inefficiency, and dependence on additional semantic
inputs such as segmentation labels or motion data. To address these
limitations, we propose a novel \textbf{Ca}rdiac \textbf{L}atent
\textbf{I}nterpolation \textbf{D}iffusion (CaLID) framework that introduces
three key innovations. First, we present a data-driven interpolation scheme
based on diffusion models, which can capture complex, non-linear relationships
between sparse slices and improves reconstruction accuracy. Second, we design a
computationally efficient method that operates in the latent space and speeds
up 3D whole-heart upsampling time by a factor of 24, reducing computational
overhead compared to previous methods. Third, with only sparse 2D CMR images as
input, our method achieves SOTA performance against baseline methods,
eliminating the need for auxiliary input such as morphological guidance, thus
simplifying workflows. We further extend our method to 2D+T data, enabling the
effective modeling of spatiotemporal dynamics and ensuring temporal coherence.
Extensive volumetric evaluations and downstream segmentation tasks demonstrate
that CaLID achieves superior reconstruction quality and efficiency. By
addressing the fundamental limitations of existing approaches, our framework
advances the state of the art for spatio and spatiotemporal whole-heart
reconstruction, offering a robust and clinically practical solution for
cardiovascular imaging.

</details>


### [333] [A Novel Attention-Augmented Wavelet YOLO System for Real-time Brain Vessel Segmentation on Transcranial Color-coded Doppler](https://arxiv.org/abs/2508.13875)
*Wenxuan Zhang,Shuai Li,Xinyi Wang,Yu Sun,Hongyu Kang,Pui Yuk Chryste Wan,Yong-Ping Zheng,Sai-Kit Lam*

Main category: eess.IV

TL;DR: The paper introduces an AI-based system for real-time cerebrovascular segmentation to address limitations in operator-dependent Transcranial Color-coded Doppler (TCCD) assessments of the Circle of Willis (CoW).


<details>
  <summary>Details</summary>
Motivation: Existing TCCD-based assessments of the Circle of Willis (CoW) are limited by their dependence on operator expertise, restricting accessibility and scalability for ischemic stroke risk evaluation and management.

Method: A tailored Attention-Augmented Wavelet YOLO (AAW-YOLO) network was developed and trained on 738 annotated TCCD frames to accurately segment CoW vessels in real-time.

Result: The proposed AAW-YOLO achieved high segmentation metrics (Dice score of 0.901, IoU of 0.823, precision of 0.882, recall of 0.926, mAP of 0.953) and fast per-frame inference (14.199 ms).

Conclusion: The AI-powered system presents a viable solution for reducing operator dependency in TCCD-based cerebrovascular assessments, with potential integration into clinical workflows, paving the way for broad adoption and future enhancements.

Abstract: The Circle of Willis (CoW), vital for ensuring consistent blood flow to the
brain, is closely linked to ischemic stroke. Accurate assessment of the CoW is
important for identifying individuals at risk and guiding appropriate clinical
management. Among existing imaging methods, Transcranial Color-coded Doppler
(TCCD) offers unique advantages due to its radiation-free nature,
affordability, and accessibility. However, reliable TCCD assessments depend
heavily on operator expertise for identifying anatomical landmarks and
performing accurate angle correction, which limits its widespread adoption. To
address this challenge, we propose an AI-powered, real-time CoW
auto-segmentation system capable of efficiently capturing cerebral arteries. No
prior studies have explored AI-driven cerebrovascular segmentation using TCCD.
In this work, we introduce a novel Attention-Augmented Wavelet YOLO (AAW-YOLO)
network tailored for TCCD data, designed to provide real-time guidance for
brain vessel segmentation in the CoW. We prospectively collected TCCD data
comprising 738 annotated frames and 3,419 labeled artery instances to establish
a high-quality dataset for model training and evaluation. The proposed AAW-YOLO
demonstrated strong performance in segmenting both ipsilateral and
contralateral CoW vessels, achieving an average Dice score of 0.901, IoU of
0.823, precision of 0.882, recall of 0.926, and mAP of 0.953, with a per-frame
inference speed of 14.199 ms. This system offers a practical solution to reduce
reliance on operator experience in TCCD-based cerebrovascular screening, with
potential applications in routine clinical workflows and resource-constrained
settings. Future research will explore bilateral modeling and larger-scale
validation.

</details>


### [334] [Learning to See Through Flare](https://arxiv.org/abs/2508.13907)
*Xiaopeng Peng,Heath Gemar,Erin Fleet,Kyle Novak,Abbie Watnik,Grover Swartzlander*

Main category: eess.IV

TL;DR: NeuSee is a new computational imaging framework designed to protect machine vision sensors from laser flares across the visible spectrum, while restoring high-quality images using neural networks.


<details>
  <summary>Details</summary>
Motivation: Laser flare severely impacts machine vision systems, distorting their perception and potentially causing sensor damage. A solution is needed for robust sensor protection and image restoration under such conditions.

Method: The framework combines a neural representation of a diffractive optical element (DOE) with a frequency-space Mamba-GAN network, adversarially trained end-to-end using 100K diverse images. It leverages hyperspectral information, different neural networks, and heterogeneous computation.

Result: NeuSee suppresses peak laser irradiance up to 10^6 times the sensor's saturation threshold, handles complex real-world conditions, and achieves full-spectrum imaging with a 10.1% improvement in restored image quality compared to alternate methods.

Conclusion: NeuSee is the first solution to offer comprehensive visible-spectrum imaging protection against laser flares, successfully mitigating their effects while restoring high-fidelity visual data.

Abstract: Machine vision systems are susceptible to laser flare, where unwanted intense
laser illumination blinds and distorts its perception of the environment
through oversaturation or permanent damage to sensor pixels. We introduce
NeuSee, the first computational imaging framework for high-fidelity sensor
protection across the full visible spectrum. It jointly learns a neural
representation of a diffractive optical element (DOE) and a frequency-space
Mamba-GAN network for image restoration. NeuSee system is adversarially trained
end-to-end on 100K unique images to suppress the peak laser irradiance as high
as $10^6$ times the sensor saturation threshold $I_{\textrm{sat}}$, the point
at which camera sensors may experience damage without the DOE. Our system
leverages heterogeneous data and model parallelism for distributed computing,
integrating hyperspectral information and multiple neural networks for
realistic simulation and image restoration. NeuSee takes into account
open-world scenes with dynamically varying laser wavelengths, intensities, and
positions, as well as lens flare effects, unknown ambient lighting conditions,
and sensor noises. It outperforms other learned DOEs, achieving full-spectrum
imaging and laser suppression for the first time, with a 10.1\% improvement in
restored image quality.

</details>


### [335] [MMIS-Net for Retinal Fluid Segmentation and Detection](https://arxiv.org/abs/2508.13936)
*Nchongmaje Ndipenocha,Alina Mirona,Kezhi Wanga,Yongmin Li*

Main category: eess.IV

TL;DR: The paper introduces MMIS-Net, leveraging multiple annotated medical image datasets via Similarity Fusion blocks and one-hot label space to address segmentation and detection challenges in unseen data. It achieved superior performance metrics compared to prior models.


<details>
  <summary>Details</summary>
Motivation: To utilize the synergistic potential of diverse small annotated datasets from various modalities, organs, and diseases, overcoming limitations of single-source training.

Method: MMIS-Net uses Similarity Fusion blocks for feature map fusion and knowledge selection. It employs a one-hot label space to manage class inconsistencies across datasets.

Result: MMIS-Net outperformed state-of-the-art segmentation models with high metrics in segmentation tasks (Dice score 0.83, volume difference 0.035) and perfect detection AUC (1.0) during evaluation.

Conclusion: MMIS-Net's architecture enhances medical image segmentation and detection by utilizing diverse datasets, addressing label inconsistencies, and incorporating robust fusion techniques into its network backbone.

Abstract: Purpose: Deep learning methods have shown promising results in the
segmentation, and detection of diseases in medical images. However, most
methods are trained and tested on data from a single source, modality, organ,
or disease type, overlooking the combined potential of other available
annotated data. Numerous small annotated medical image datasets from various
modalities, organs, and diseases are publicly available. In this work, we aim
to leverage the synergistic potential of these datasets to improve performance
on unseen data. Approach: To this end, we propose a novel algorithm called
MMIS-Net (MultiModal Medical Image Segmentation Network), which features
Similarity Fusion blocks that utilize supervision and pixel-wise similarity
knowledge selection for feature map fusion. Additionally, to address
inconsistent class definitions and label contradictions, we created a one-hot
label space to handle classes absent in one dataset but annotated in another.
MMIS-Net was trained on 10 datasets encompassing 19 organs across 2 modalities
to build a single model. Results: The algorithm was evaluated on the RETOUCH
grand challenge hidden test set, outperforming large foundation models for
medical image segmentation and other state-of-the-art algorithms. We achieved
the best mean Dice score of 0.83 and an absolute volume difference of 0.035 for
the fluids segmentation task, as well as a perfect Area Under the Curve of 1
for the fluid detection task. Conclusion: The quantitative results highlight
the effectiveness of our proposed model due to the incorporation of Similarity
Fusion blocks into the network's backbone for supervision and similarity
knowledge selection, and the use of a one-hot label space to address label
class inconsistencies and contradictions.

</details>


### [336] [Real-Time, Population-Based Reconstruction of 3D Bone Models via Very-Low-Dose Protocols](https://arxiv.org/abs/2508.13947)
*Yiqun Lin,Haoran Sun,Yongqing Li,Rabia Aslam,Lung Fung Tse,Tiange Cheng,Chun Sing Chui,Wing Fung Yau,Victorine R. Le Meur,Meruyert Amangeldy,Kiho Cho,Yinyu Ye,James Zou,Wei Zhao,Xiaomeng Li*

Main category: eess.IV

TL;DR: This paper introduces a fast and accurate AI-based method (SSR-KD) for generating patient-specific bone models from biplanar X-rays, reducing reliance on CT and manual processes.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for creating patient-specific bone models rely on CT scans, which have limitations such as high radiation exposure, lack of flexibility in intraoperative use, and time-consuming manual delineation.

Method: The SSR-KD framework incorporates Semi-Supervised Reconstruction with Knowledge Distillation to generate high-quality bone models from biplanar X-rays in just 30 seconds, achieving an average error of less than 1.0 mm.

Result: The reconstructed bone models using SSR-KD were validated by expert simulations for high tibial osteotomy and shown to have comparable clinical applicability to CT-based models.

Conclusion: The method accelerates the bone modeling process, reduces radiation exposure, is clinically practical for intraoperative guidance, and offers transformative improvements in orthopedic applications.

Abstract: Patient-specific bone models are essential for designing surgical guides and
preoperative planning, as they enable the visualization of intricate anatomical
structures. However, traditional CT-based approaches for creating bone models
are limited to preoperative use due to the low flexibility and high radiation
exposure of CT and time-consuming manual delineation. Here, we introduce
Semi-Supervised Reconstruction with Knowledge Distillation (SSR-KD), a fast and
accurate AI framework to reconstruct high-quality bone models from biplanar
X-rays in 30 seconds, with an average error under 1.0 mm, eliminating the
dependence on CT and manual work. Additionally, high tibial osteotomy
simulation was performed by experts on reconstructed bone models, demonstrating
that bone models reconstructed from biplanar X-rays have comparable clinical
applicability to those annotated from CT. Overall, our approach accelerates the
process, reduces radiation exposure, enables intraoperative guidance, and
significantly improves the practicality of bone models, offering transformative
applications in orthopedics.

</details>


### [337] [UNICON: UNIfied CONtinual Learning for Medical Foundational Models](https://arxiv.org/abs/2508.14024)
*Mohammad Areeb Qazi,Munachiso S Nwadike,Ibrahim Almakky,Mohammad Yaqub,Numan Saeed*

Main category: eess.IV

TL;DR: The paper introduces UNICON, a unified framework for continual learning in medical foundational models, improving adaptability across domains, tasks, and modalities without forgetting or interference.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of limited data availability in medical imaging, which makes it difficult to pre-train models for every specific task, domain, or modality.

Method: UNICON provides a unified and expandable framework for continual learning, enabling foundation models to adapt dynamically across imaging modalities, anatomical regions, and clinical tasks while avoiding catastrophic forgetting.

Result: Experiments showed UNICON improved performance in new tasks and modalities, including a 5% accuracy boost in Dice scores after incorporating PET scans.

Conclusion: UNICON demonstrates that foundational models can evolve beyond their initial training scope, moving toward versatile generalist AI models for medical imaging.

Abstract: Foundational models are trained on extensive datasets to capture the general
trends of a domain. However, in medical imaging, the scarcity of data makes
pre-training for every domain, modality, or task challenging. Continual
learning offers a solution by fine-tuning a model sequentially on different
domains or tasks, enabling it to integrate new knowledge without requiring
large datasets for each training phase. In this paper, we propose UNIfied
CONtinual Learning for Medical Foundational Models (UNICON), a framework that
enables the seamless adaptation of foundation models to diverse domains, tasks,
and modalities. Unlike conventional adaptation methods that treat these changes
in isolation, UNICON provides a unified, perpetually expandable framework.
Through careful integration, we show that foundation models can dynamically
expand across imaging modalities, anatomical regions, and clinical objectives
without catastrophic forgetting or task interference. Empirically, we validate
our approach by adapting a chest CT foundation model initially trained for
classification to a prognosis and segmentation task. Our results show improved
performance across both additional tasks. Furthermore, we continually
incorporated PET scans and achieved a 5\% improvement in Dice score compared to
respective baselines. These findings establish that foundation models are not
inherently constrained to their initial training scope but can evolve, paving
the way toward generalist AI models for medical imaging.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [338] [Goal-Directedness is in the Eye of the Beholder](https://arxiv.org/abs/2508.13247)
*Nina Rajcic,Anders Søgaard*

Main category: cs.MA

TL;DR: This paper examines how to identify goal-directed behavior in agents and argues that it cannot be measured objectively.


<details>
  <summary>Details</summary>
Motivation: Understanding and predicting the behavior of complex agents often requires identifying their goals, but current methods may be conceptually or technically limited.

Method: The paper evaluates two main approaches—behavioral (observing actions) and mechanistic (analyzing internal states)—and identifies the assumptions and limitations behind them.

Result: The study concludes that neither behavioral nor mechanistic methods can offer an objective measure of goal-directed behavior.

Conclusion: Goal-directedness should be modeled as an emergent property in dynamic, multi-agent systems, offering a new perspective.

Abstract: Our ability to predict the behavior of complex agents turns on the
attribution of goals. Probing for goal-directed behavior comes in two flavors:
Behavioral and mechanistic. The former proposes that goal-directedness can be
estimated through behavioral observation, whereas the latter attempts to probe
for goals in internal model states. We work through the assumptions behind both
approaches, identifying technical and conceptual problems that arise from
formalizing goals in agent systems. We arrive at the perhaps surprising
position that goal-directedness cannot be measured objectively. We outline new
directions for modeling goal-directedness as an emergent property of dynamic,
multi-agent systems.

</details>


### [339] [BetaWeb: Towards a Blockchain-enabled Trustworthy Agentic Web](https://arxiv.org/abs/2508.13787)
*Zihan Guo,Yuanjian Zhou,Chenyi Wang,Linlin You,Minjie Bian,Weinan Zhang*

Main category: cs.MA

TL;DR: This paper proposes BetaWeb, a blockchain-based infrastructure to overcome limitations in current fragmented AI agent ecosystems, enabling trustworthy and scalable multi-agent interactions.


<details>
  <summary>Details</summary>
Motivation: Current AI agentic systems are fragmented and face challenges like privacy, data management, and value measurement, emphasizing the need for an interconnected and scalable agentic AI paradigm.

Method: The proposed solution is BetaWeb, a blockchain-enabled system providing a trustworthy infrastructure for large language model-based multi-agent systems, with a focus on Web3.5 principles.

Result: BetaWeb demonstrates its potential to overcome ecosystem barriers, enabling cross-domain autonomous interactions, and proposes a five-stage roadmap for evolving LaMAS capabilities.

Conclusion: Integrating blockchain with LaMAS can establish a resilient, trustworthy, and incentivized digital ecosystem, advancing AI agent capabilities and Web paradigms.

Abstract: The rapid development of large language models (LLMs) has significantly
propelled the development of artificial intelligence (AI) agents, which are
increasingly evolving into diverse autonomous entities, advancing the LLM-based
multi-agent systems (LaMAS). However, current agentic ecosystems remain
fragmented and closed. Establishing an interconnected and scalable paradigm for
Agentic AI has become a critical prerequisite. Although Agentic Web proposes an
open architecture to break the ecosystem barriers, its implementation still
faces core challenges such as privacy protection, data management, and value
measurement. Existing centralized or semi-centralized paradigms suffer from
inherent limitations, making them inadequate for supporting large-scale,
heterogeneous, and cross-domain autonomous interactions. To address these
challenges, this paper introduces the blockchain-enabled trustworthy Agentic
Web (BetaWeb). By leveraging the inherent strengths of blockchain, BetaWeb not
only offers a trustworthy and scalable infrastructure for LaMAS but also has
the potential to advance the Web paradigm from Web3 (centered on data
ownership) towards Web3.5, which emphasizes ownership of agent capabilities and
the monetization of intelligence. Beyond a systematic examination of the
BetaWeb framework, this paper presents a five-stage evolutionary roadmap,
outlining the path of LaMAS from passive execution to advanced collaboration
and autonomous governance. We also conduct a comparative analysis of existing
products and discuss key challenges of BetaWeb from multiple perspectives.
Ultimately, we argue that deep integration between blockchain and LaMAS can lay
the foundation for a resilient, trustworthy, and sustainably incentivized
digital ecosystem. A summary of the enabling technologies for each stage is
available at https://github.com/MatZaharia/BetaWeb.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [340] [Benchmarking LLM-based Agents for Single-cell Omics Analysis](https://arxiv.org/abs/2508.13201)
*Yang Liu,Lu Zhou,Ruikun He,Rongbo Shen,Yixue Li*

Main category: q-bio.GN

TL;DR: The paper introduces a benchmarking system for assessing AI agents in single-cell omics analysis, and highlights Grok-3-beta's state-of-the-art performance and areas needing improvement.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in manually-defined workflows for single-cell omics analysis and foster advancements in AI-driven solutions.

Method: Developing a benchmarking system with metrics for evaluating AI agents' performance across 50 diverse real-world omics tasks, including collaboration and execution efficiency.

Result: Grok-3-beta outperformed other frameworks, and multi-agent systems were more effective than single-agent systems in collaboration and efficiency.

Conclusion: High-quality code generation and self-reflection are critical for success, but challenges like long-context handling and knowledge retrieval remain significant.

Abstract: The surge in multimodal single-cell omics data exposes limitations in
traditional, manually defined analysis workflows. AI agents offer a paradigm
shift, enabling adaptive planning, executable code generation, traceable
decisions, and real-time knowledge fusion. However, the lack of a comprehensive
benchmark critically hinders progress. We introduce a novel benchmarking
evaluation system to rigorously assess agent capabilities in single-cell omics
analysis. This system comprises: a unified platform compatible with diverse
agent frameworks and LLMs; multidimensional metrics assessing cognitive program
synthesis, collaboration, execution efficiency, bioinformatics knowledge
integration, and task completion quality; and 50 diverse real-world single-cell
omics analysis tasks spanning multi-omics, species, and sequencing
technologies. Our evaluation reveals that Grok-3-beta achieves state-of-the-art
performance among tested agent frameworks. Multi-agent frameworks significantly
enhance collaboration and execution efficiency over single-agent approaches
through specialized role division. Attribution analyses of agent capabilities
identify that high-quality code generation is crucial for task success, and
self-reflection has the most significant overall impact, followed by
retrieval-augmented generation (RAG) and planning. This work highlights
persistent challenges in code generation, long-context handling, and
context-aware knowledge retrieval, providing a critical empirical foundation
and best practices for developing robust AI agents in computational biology.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [341] [Machine Learning H-theorem](https://arxiv.org/abs/2508.14003)
*Ruben Lier*

Main category: cond-mat.stat-mech

TL;DR: Explores a model-based study on H-theorem using the equilibration of hard disks to understand its link with the arrow of time.


<details>
  <summary>Details</summary>
Motivation: The H-theorem underpins the Second Law of Thermodynamics but has controversies, prompting further study of its irreversibility and connection to time's directionality.

Method: Utilizes a DeepSets architecture-based model to analyze equilibrating hard disks, ensuring permutation invariance of particle labels for accuracy.

Result: Demonstrates the model's ability to capture the H-functional's irreversibility associated with equilibration.

Conclusion: Provides insights into H-theorem's irreversibility and its connection with the arrow of time through statistical physics modeling.

Abstract: H-theorem provides a microscopic foundation of the Second Law of
Thermodynamics and is therefore essential to establishing statistical physics,
but at the same time, H-theorem has been subject to controversy that in part
persists till this day. To better understand H-theorem and its relation to the
arrow of time, we study the equilibration of randomly oriented and positioned
hard disks with periodic boundary conditions. Using a model based on the
DeepSets architecture, which imposes permutation invariance of the particle
labels, we train a model to capture the irreversibility of the H-functional.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [342] [Combating Homelessness Stigma with LLMs: A New Multi-Modal Dataset for Bias Detection](https://arxiv.org/abs/2508.13187)
*Jonathan A. Karr Jr.,Benjamin F. Herbst,Ting Hua,Matthew Hauenstein,Georgina Curto,Nitesh V. Chawla*

Main category: cs.CY

TL;DR: This study tracks and measures social biases towards homelessness using NLP and LLMs, presenting a multi-modal dataset to automate bias detection.


<details>
  <summary>Details</summary>
Motivation: Homelessness, impacting millions, is worsened by social stigmatization, which influences public perception and policymaking.

Method: A multi-modal dataset was compiled from digital sources, and various LLMs were evaluated for zero-shot and few-shot bias classification.

Result: Local LLMs performed inconsistently in zero-shot tests but improved with in-context learning, approaching closed-source model results.

Conclusion: The study highlights persistent biases against homelessness and provides tools for better ethical AI applications and policy insights.

Abstract: Homelessness is a persistent social challenge, impacting millions worldwide.
Over 770,000 people experienced homelessness in the U.S. in 2024. Social
stigmatization is a significant barrier to alleviation, shifting public
perception, and influencing policymaking. Given that online and city council
discourse reflect and influence part of public opinion, it provides valuable
insights to identify and track social biases. This research contributes to
alleviating homelessness by acting on public opinion. It introduces novel
methods, building on natural language processing (NLP) and large language
models (LLMs), to identify and measure PEH social bias expressed in digital
spaces. We present a new, manually-annotated multi-modal dataset compiled from
Reddit, X (formerly Twitter), news articles, and city council meeting minutes
across 10 U.S. cities. This unique dataset provides evidence of the typologies
of homelessness bias described in the literature. In order to scale up and
automate the detection of homelessness bias online, we evaluate LLMs as
classifiers. We applied both zero-shot and few-shot classification techniques
to this data. We utilized local LLMs (Llama 3.2 3B Instruct, Qwen 2.5 7B
Instruct, and Phi4 Instruct Mini) as well as closed-source API models (GPT-4.1,
Gemini 2.5 Pro, and Grok-4). Our findings reveal that although there are
significant inconsistencies in local LLM zero-shot classification, the
in-context learning classification scores of local LLMs approach the
classification scores of closed-source LLMs. Furthermore, LLMs outperform BERT
when averaging across all categories. This work aims to raise awareness about
the pervasive bias against PEH, develop new indicators to inform policy, and
ultimately enhance the fairness and ethical application of Generative AI
technologies.

</details>


### [343] [Preliminary suggestions for rigorous GPAI model evaluations](https://arxiv.org/abs/2508.00875)
*Patricia Paskov,Michael J. Byun,Kevin Wei,Toby Webster*

Main category: cs.CY

TL;DR: The paper proposes standardized practices for evaluating general-purpose AI (GPAI) to enhance internal validity, external validity, and reproducibility. Suggestions are categorized across the evaluation lifecycle: design, implementation, execution, and documentation.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to contribute to the emerging field of GPAI evaluations by providing guidance informed by established practices from various scientific disciplines. This aims to ensure GPAI systems are rigorous, applicable across contexts, and reproducible.

Method: The study organizes evaluation practices across four lifecycle stages: design, implementation, execution, and documentation, pulling insights from disciplines such as machine learning, psychology, and economics.

Result: As a preliminary compilation, the paper proposes concrete suggestions addressing GPAI evaluation, aiming to satisfy specific requirements like the EU AI Act for systemic risk-bearing AI models.

Conclusion: The paper is a foundational effort to advance the science of GPAI evaluation, targeting stakeholders like model providers, third-party evaluators, policymakers, and researchers with guidelines to enhance evaluation practices.

Abstract: This document presents a preliminary compilation of general-purpose AI (GPAI)
evaluation practices that may promote internal validity, external validity and
reproducibility. It includes suggestions for human uplift studies and benchmark
evaluations, as well as cross-cutting suggestions that may apply to many
different evaluation types. Suggestions are organised across four stages in the
evaluation life cycle: design, implementation, execution and documentation.
Drawing from established practices in machine learning, statistics, psychology,
economics, biology and other fields recognised to have important lessons for AI
evaluation, these suggestions seek to contribute to the conversation on the
nascent and evolving field of the science of GPAI evaluations. The intended
audience of this document includes providers of GPAI models presenting systemic
risk (GPAISR), for whom the EU AI Act lays out specific evaluation
requirements; third-party evaluators; policymakers assessing the rigour of
evaluations; and academic researchers developing or conducting GPAI
evaluations.

</details>


### [344] [Toward an African Agenda for AI Safety](https://arxiv.org/abs/2508.13179)
*Samuel T. Segun,Rachel Adams,Ana Florido,Scott Timcke,Jonathan Shock,Leah Junck,Fola Adeleke,Nicolas Grossman,Ayantola Alayande,Jerry John Kponyo,Matthew Smith,Dickson Marfo Fosu,Prince Dawson Tetteh,Juliet Arthur,Stephanie Kasaon,Odilile Ayodele,Laetitia Badolo,Paul Plantinga,Michael Gastrow,Sumaya Nur Adan,Joanna Wiaterek,Cecil Abungu,Kojo Apeagyei,Luise Eder,Tegawende Bissyande*

Main category: cs.CY

TL;DR: This paper examines AI-related risks specific to Africa and offers a five-point action plan focused on human rights, AI safety institutes, public literacy, language-specific early warning systems, and a regional forum.


<details>
  <summary>Details</summary>
Motivation: To address Africa's unique vulnerabilities to AI risks, including electoral interference, labor market disruptions, and environmental costs, while integrating African perspectives into global AI safety debates.

Method: The authors propose a structured five-point action plan that includes policy reform, institutional establishment, public education, language-specific warning systems, and regional collaboration.

Result: The paper identifies gaps such as the absence of African dedicated AI safety institutes and provides strategic solutions to mitigate risks and integrate African voices globally.

Conclusion: Implementing the proposed initiatives could safeguard Africa from AI-related harm and elevate African contributions to global AI governance discussions.

Abstract: This paper maps Africa's distinctive AI risk profile, from deepfake fuelled
electoral interference and data colonial dependency to compute scarcity, labour
disruption and disproportionate exposure to climate driven environmental costs.
While major benefits are promised to accrue, the availability, development and
adoption of AI also mean that African people and countries face particular AI
safety risks, from large scale labour market disruptions to the nefarious use
of AI to manipulate public opinion. To date, African perspectives have not been
meaningfully integrated into global debates and processes regarding AI safety,
leaving African stakeholders with limited influence over the emerging global AI
safety governance agenda. While there are Computer Incident Response Teams on
the continent, none hosts a dedicated AI Safety Institute or office. We propose
a five-point action plan centred on (i) a policy approach that foregrounds the
protection of the human rights of those most vulnerable to experiencing the
harmful socio-economic effects of AI; (ii) the establishment of an African AI
Safety Institute; (iii) promote public AI literacy and awareness; (iv)
development of early warning system with inclusive benchmark suites for 25+
African languages; and (v) an annual AU-level AI Safety & Security Forum.

</details>


### [345] [The Course Difficulty Analysis Cookbook](https://arxiv.org/abs/2508.13218)
*Frederik Baucks,Robin Schmucker,Laurenz Wiskott*

Main category: cs.CY

TL;DR: This paper reviews and compares methods for assessing course difficulty, offering insights into unbiased, reliable, and equitable evaluations for educational programs.


<details>
  <summary>Details</summary>
Motivation: To improve the quality of educational programs by providing unbiased, reliable, and equitable assessments of course difficulty, crucial for quality control, course comparisons, and recommendations.

Method: Comprehensive review and comparison of existing difficulty-assessment methods based on grade point averages and latent trait modeling, along with a practical tutorial and supporting tools.

Result: A detailed comparison, tutorial, and open-source package are provided to monitor course difficulty, detect disparities, and support fair and equitable learning experiences.

Conclusion: A robust and practical framework, along with tools, is presented to ensure the fairness, validity, and reliability of course difficulty assessments in curriculum analytics.

Abstract: Curriculum analytics (CA) studies curriculum structure and student data to
ensure the quality of educational programs. An essential aspect is studying
course properties, which involves assigning each course a representative
difficulty value. This is critical for several aspects of CA, such as quality
control (e.g., monitoring variations over time), course comparisons (e.g.,
articulation), and course recommendation (e.g., advising). Measuring course
difficulty requires careful consideration of multiple factors: First, when
difficulty measures are sensitive to the performance level of enrolled
students, it can bias interpretations by overlooking student diversity. By
assessing difficulty independently of enrolled students' performances, we can
reduce the risk of bias and enable fair, representative assessments of
difficulty. Second, from a measurement theoretic perspective, the measurement
must be reliable and valid to provide a robust basis for subsequent analyses.
Third, difficulty measures should account for covariates, such as the
characteristics of individual students within a diverse populations (e.g.,
transfer status). In recent years, various notions of difficulty have been
proposed. This paper provides the first comprehensive review and comparison of
existing approaches for assessing course difficulty based on grade point
averages and latent trait modeling. It further offers a hands-on tutorial on
model selection, assumption checking, and practical CA applications. These
applications include monitoring course difficulty over time and detecting
courses with disparate outcomes between distinct groups of students (e.g.,
dropouts vs. graduates), ultimately aiming to promote high-quality, fair, and
equitable learning experiences. To support further research and application, we
provide an open-source software package and artificial datasets, facilitating
reproducibility and adoption.

</details>


### [346] [Consumer Autonomy or Illusion? Rethinking Consumer Agency in the Age of Algorithms](https://arxiv.org/abs/2508.13440)
*Pegah Nokhiz,Aravinda Kanchana Ruwanpathirana*

Main category: cs.CY

TL;DR: The paper examines how systemic barriers, algorithmic manipulation, and unstable work patterns constrain consumer agency in financial decisions, using formal models to demonstrate the risks associated with diminished autonomy.


<details>
  <summary>Details</summary>
Motivation: To understand the increasing constraints on consumer agency due to structural, algorithmic, and temporal factors in the digital age.

Method: Formal modeling of consumer behavior grounded in discounted consumption scenarios, examining the impact of obligatory payments, algorithm-driven impulsive spending, and income instability.

Result: The study shows how reduced agency leads to financial instability and early financial ruin, even for rational, utility-maximizing consumers.

Conclusion: Consumer agency should be actively cultivated as a value, with systemic interventions and education proposed as solutions to strengthen informed decision-making.

Abstract: Consumer agency in the digital age is increasingly constrained by systemic
barriers and algorithmic manipulation, raising concerns about the authenticity
of consumption choices. Nowadays, financial decisions are shaped by external
pressures like obligatory consumption, algorithmic persuasion, and unstable
work schedules that erode financial autonomy. Obligatory consumption (like
hidden fees) is intensified by digital ecosystems. Algorithmic tactics like
personalized recommendations lead to impulsive purchases. Unstable work
schedules also undermine financial planning. Thus, it is important to study how
these factors impact consumption agency. To do so, we examine formal models
grounded in discounted consumption with constraints that bound agency. We
construct analytical scenarios in which consumers face obligatory payments,
algorithm-influenced impulsive expenses, or unpredictable income due to
temporal instability. Using this framework, we demonstrate that even rational,
utility-maximizing agents can experience early financial ruin when agency is
limited across structural, behavioral, or temporal dimensions and how
diminished autonomy impacts long-term financial well-being. Our central
argument is that consumer agency must be treated as a value (not a given)
requiring active cultivation, especially in digital ecosystems. The connection
between our formal modeling and this argument allows us to indicate that
limitations on agency (whether structural, behavioral, or temporal) can be
rigorously linked to measurable risks like financial instability. This
connection is also a basis for normative claims about consumption as a value,
by anchoring them in a formally grounded analysis of consumer behavior. As
solutions, we study systemic interventions and consumer education to support
value deliberation and informed choices. We formally demonstrate how these
measures strengthen agency.

</details>


### [347] [The AI Risk Spectrum: From Dangerous Capabilities to Existential Threats](https://arxiv.org/abs/2508.13700)
*Markov Grey,Charbel-Raphaël Segerie*

Main category: cs.CY

TL;DR: This paper categorizes AI risks into misuse risks, misalignment risks, and systemic risks, while also noting amplifiers like competition and coordination failures, aiming to map the AI risk landscape.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive understanding of the full spectrum of AI risks and their potential future implications, aiding in better risk management.

Method: The authors categorize AI risks into three causal groups (misuse, misalignment, and systemic risks) and identify amplifying factors that exacerbate these risks, connecting current trends with potential future consequences.

Result: The paper organizes diverse AI risks and links present-day AI behaviors and trends to possible catastrophic scenarios, emphasizing the need for proactive measures.

Conclusion: The paper highlights the necessity of unparalleled coordination to navigate and mitigate these risks, asserting that positive AI futures are achievable but will not occur by default.

Abstract: As AI systems become more capable, integrated, and widespread, understanding
the associated risks becomes increasingly important. This paper maps the full
spectrum of AI risks, from current harms affecting individual users to
existential threats that could endanger humanity's survival. We organize these
risks into three main causal categories. Misuse risks, which occur when people
deliberately use AI for harmful purposes - creating bioweapons, launching
cyberattacks, adversarial AI attacks or deploying lethal autonomous weapons.
Misalignment risks happen when AI systems pursue outcomes that conflict with
human values, irrespective of developer intentions. This includes risks arising
through specification gaming (reward hacking), scheming and power-seeking
tendencies in pursuit of long-term strategic goals. Systemic risks, which arise
when AI integrates into complex social systems in ways that gradually undermine
human agency - concentrating power, accelerating political and economic
disempowerment, creating overdependence that leads to human enfeeblement, or
irreversibly locking in current values curtailing future moral progress. Beyond
these core categories, we identify risk amplifiers - competitive pressures,
accidents, corporate indifference, and coordination failures - that make all
risks more likely and severe. Throughout, we connect today's existing risks and
empirically observable AI behaviors to plausible future outcomes, demonstrating
how existing trends could escalate to catastrophic outcomes. Our goal is to
help readers understand the complete landscape of AI risks. Good futures are
possible, but they don't happen by default. Navigating these challenges will
require unprecedented coordination, but an extraordinary future awaits if we
do.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [348] [TaoSR1: The Thinking Model for E-commerce Relevance Search](https://arxiv.org/abs/2508.12365)
*Chenhe Dong,Shaowei Yao,Pengkun Jiao,Jianhui Yang,Yiming Jin,Zerui Huang,Xiaojiang Zhou,Dan Ou,Haihong Tang*

Main category: cs.IR

TL;DR: The paper introduces TaoSR1, a framework for deploying Large Language Models (LLMs) to predict query-product relevance in e-commerce, addressing reasoning and deployment challenges.


<details>
  <summary>Details</summary>
Motivation: Current e-commerce search models based on BERT struggle with complex reasoning, and LLM exploration relies on discriminative fine-tuning, which has limitations.

Method: TaoSR1 uses a three-stage process: (1) Supervised Fine-Tuning with Chain-of-Thought reasoning, (2) Offline sampling to enhance output quality, and (3) Difficulty-based dynamic sampling to reduce hallucinations.

Result: TaoSR1 improves over baseline models in offline experiments and achieves notable gains in human evaluations during online deployment.

Conclusion: The framework successfully applies Chain-of-Thought reasoning to relevance prediction, improving both reasoning capabilities and deployment feasibility.

Abstract: Query-product relevance prediction is a core task in e-commerce search.
BERT-based models excel at semantic matching but lack complex reasoning
capabilities. While Large Language Models (LLMs) are explored, most still use
discriminative fine-tuning or distill to smaller models for deployment. We
propose a framework to directly deploy LLMs for this task, addressing key
challenges: Chain-of-Thought (CoT) error accumulation, discriminative
hallucination, and deployment feasibility. Our framework, TaoSR1, involves
three stages: (1) Supervised Fine-Tuning (SFT) with CoT to instill reasoning;
(2) Offline sampling with a pass@N strategy and Direct Preference Optimization
(DPO) to improve generation quality; and (3) Difficulty-based dynamic sampling
with Group Relative Policy Optimization (GRPO) to mitigate discriminative
hallucination. Additionally, post-CoT processing and a cumulative
probability-based partitioning method enable efficient online deployment.
TaoSR1 significantly outperforms baselines on offline datasets and achieves
substantial gains in online side-by-side human evaluations, introducing a novel
paradigm for applying CoT reasoning to relevance classification.

</details>


### [349] [LLM-Enhanced Linear Autoencoders for Recommendation](https://arxiv.org/abs/2508.13500)
*Jaewan Moon,Seongmin Park,Jongwuk Lee*

Main category: cs.IR

TL;DR: The paper introduces L3AE, an autoencoder framework that integrates large language models (LLMs) to enhance textual semantic representation in recommender systems, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing linear autoencoders (LAEs) that rely on sparse word co-occurrence patterns and fail to capture rich textual semantics in recommender systems.

Method: L3AE integrates LLMs into LAE framework using a two-phase optimization strategy: (i) constructing a semantic item-to-item correlation matrix from LLM-based item representations, and (ii) learning an item-to-item weight matrix from collaborative signals while leveraging semantic correlations as regularization.

Result: L3AE outperforms state-of-the-art LLM-enhanced models, demonstrating consistent improvements across three benchmark datasets with 27.6% gain in Recall@20 and 39.3% in NDCG@20.

Conclusion: The integration of LLMs into the LAE framework via L3AE enhances the ability to capture textual semantics, leading to better performance in recommender systems while maintaining computational efficiency and global optimality.

Abstract: Large language models (LLMs) have been widely adopted to enrich the semantic
representation of textual item information in recommender systems. However,
existing linear autoencoders (LAEs) that incorporate textual information rely
on sparse word co-occurrence patterns, limiting their ability to capture rich
textual semantics. To address this, we propose L3AE, the first integration of
LLMs into the LAE framework. L3AE effectively integrates the heterogeneous
knowledge of textual semantics and user-item interactions through a two-phase
optimization strategy. (i) L3AE first constructs a semantic item-to-item
correlation matrix from LLM-derived item representations. (ii) It then learns
an item-to-item weight matrix from collaborative signals while distilling
semantic item correlations as regularization. Notably, each phase of L3AE is
optimized through closed-form solutions, ensuring global optimality and
computational efficiency. Extensive experiments demonstrate that L3AE
consistently outperforms state-of-the-art LLM-enhanced models on three
benchmark datasets, achieving gains of 27.6% in Recall@20 and 39.3% in NDCG@20.
The source code is available at https://github.com/jaewan7599/L3AE_CIKM2025.

</details>


### [350] [Research on Conversational Recommender System Considering Consumer Types](https://arxiv.org/abs/2508.13209)
*Yaying Luo,Hui Fang,Zhu Sun*

Main category: cs.IR

TL;DR: CT-CRS is a framework to enhance conversational recommendation by modeling user decision-making styles and knowledge levels, leveraging consumer type theory and Inverse Reinforcement Learning.


<details>
  <summary>Details</summary>
Motivation: Existing CRS methods fail to address users' heterogeneous decision-making styles and knowledge levels, limiting their accuracy and interaction efficiency.

Method: CT-CRS uses consumer type theory to categorize users into four types and employs fine-tuned language models for real-time user type inference. It incorporates these types into state representation and optimizes dialogue policy through Inverse Reinforcement Learning.

Result: CT-CRS outperformed baselines in success rate and interaction efficiency on datasets like LastFM, Amazon-Book, and Yelp. Ablation studies showed significant contributions of consumer type modeling and IRL.

Conclusion: CT-CRS effectively integrates psychological modeling and advanced policy optimization to improve personalization in conversational recommenders, showcasing scalability and interpretability.

Abstract: Conversational Recommender Systems (CRS) provide personalized services
through multi-turn interactions, yet most existing methods overlook users'
heterogeneous decision-making styles and knowledge levels, which constrains
both accuracy and efficiency. To address this gap, we propose CT-CRS (Consumer
Type-Enhanced Conversational Recommender System), a framework that integrates
consumer type modeling into dialogue recommendation. Based on consumer type
theory, we define four user categories--dependent, efficient, cautious, and
expert--derived from two dimensions: decision-making style (maximizers vs.
satisficers) and knowledge level (high vs. low). CT-CRS employs interaction
histories and fine-tunes the large language model to automatically infer user
types in real time, avoiding reliance on static questionnaires. We incorporate
user types into state representation and design a type-adaptive policy that
dynamically adjusts recommendation granularity, diversity, and attribute query
complexity. To further optimize the dialogue policy, we adopt Inverse
Reinforcement Learning (IRL), enabling the agent to approximate expert-like
strategies conditioned on consumer type. Experiments on LastFM, Amazon-Book,
and Yelp show that CTCRS improves recommendation success rate and reduces
interaction turns compared to strong baselines. Ablation studies confirm that
both consumer type modeling and IRL contribute significantly to performance
gains. These results demonstrate that CT-CRS offers a scalable and
interpretable solution for enhancing CRS personalization through the
integration of psychological modeling and advanced policy optimization.

</details>


### [351] [AdaptJobRec: Enhancing Conversational Career Recommendation through an LLM-Powered Agentic System](https://arxiv.org/abs/2508.13423)
*Qixin Wang,Dawei Wang,Kun Chen,Yaowei Hu,Puneet Girdhar,Ruoteng Wang,Aadesh Gupta,Chaitanya Devella,Wenlai Guo,Shangwen Huang,Bachir Aoun,Greg Hayworth,Han Li,Xintao Wu*

Main category: cs.IR

TL;DR: AdaptJobRec is a conversational job recommendation system that balances query complexity and response speed, reducing latency by 53.3% while improving accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of response latency in agent-based Conversational Recommendation Systems while ensuring effective handling of complex user queries.

Method: Proposed the AdaptJobRec model that uses a query complexity identification mechanism. For simple queries, it selects tools for quick responses. For complex queries, it uses memory processing, a task decomposition planner, and personalized recommendation tools.

Result: Evaluation using Walmart's career recommendation data shows a 53.3% reduction in response latency while also achieving significant improvements in recommendation accuracy compared to baselines.

Conclusion: AdaptJobRec strikes a balance between latency and handling complex queries, demonstrating efficiency and effectiveness in real-world job recommendation scenarios.

Abstract: In recent years, recommendation systems have evolved from providing a single
list of recommendations to offering a comprehensive suite of topic focused
services. To better accomplish this task, conversational recommendation systems
(CRS) have progressed from basic retrieval augmented LLM generation to agentic
systems with advanced reasoning and self correction capabilities. However,
agentic systems come with notable response latency, a longstanding challenge
for conversational recommendation systems. To balance the trade off between
handling complex queries and minimizing latency, we propose AdaptJobRec, the
first conversational job recommendation system that leverages autonomous agent
to integrate personalized recommendation algorithm tools. The system employs a
user query complexity identification mechanism to minimize response latency.
For straightforward queries, the agent directly selects the appropriate tool
for rapid responses. For complex queries, the agent uses the memory processing
module to filter chat history for relevant content, then passes the results to
the intelligent task decomposition planner, and finally executes the tasks
using personalized recommendation tools. Evaluation on Walmart's real world
career recommendation scenarios demonstrates that AdaptJobRec reduces average
response latency by up to 53.3% compared to competitive baselines, while
significantly improving recommendation accuracy.

</details>


### [352] [Heterogeneous Influence Maximization in User Recommendation](https://arxiv.org/abs/2508.13517)
*Hongru Hou,Jiachen Sun,Wenqing Lin,Wendong Bi,Xiangrong Wang,Deqing Yang*

Main category: cs.IR

TL;DR: This paper proposes two models, HeteroIR and HeteroIM, to improve user recommendation systems by addressing both interaction willingness and information spread potential, achieving significant performance in experiments and deployment.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance user engagement in recommendation systems by overcoming the limitations of existing methods, which fail to unleash candidates' spread capability and account for interaction willingness.

Method: HeteroIR uses a two-stage framework to estimate spread profits, while HeteroIM selects invitees incrementally based on their influence and reranking via reverse reachable (RR) sets. Both focus on combining interaction willingness with maximizing spread coverage.

Result: The models outperform state-of-the-art baselines with statistical significance (p-value < 0.05) and achieved 8.5% and 10% improvement in Tencent's online gaming platforms.

Conclusion: HeteroIR and HeteroIM bridge the gap between conventional recommendation systems and Influence-Maximization techniques, successfully improving both spread coverage and interaction willingness.

Abstract: User recommendation systems enhance user engagement by encouraging users to
act as inviters to interact with other users (invitees), potentially fostering
information propagation. Conventional recommendation methods typically focus on
modeling interaction willingness. Influence-Maximization (IM) methods focus on
identifying a set of users to maximize the information propagation. However,
existing methods face two significant challenges. First, recommendation methods
fail to unleash the candidates' spread capability. Second, IM methods fail to
account for the willingness to interact. To solve these issues, we propose two
models named HeteroIR and HeteroIM. HeteroIR provides an intuitive solution to
unleash the dissemination potential of user recommendation systems. HeteroIM
fills the gap between the IM method and the recommendation task, improving
interaction willingness and maximizing spread coverage. The HeteroIR introduces
a two-stage framework to estimate the spread profits. The HeteroIM
incrementally selects the most influential invitee to recommend and rerank
based on the number of reverse reachable (RR) sets containing inviters and
invitees. RR set denotes a set of nodes that can reach a target via
propagation. Extensive experiments show that HeteroIR and HeteroIM
significantly outperform the state-of-the-art baselines with the p-value <
0.05. Furthermore, we have deployed HeteroIR and HeteroIM in Tencent's online
gaming platforms and gained an 8.5\% and 10\% improvement in the online A/B
test, respectively. Implementation codes are available at
https://github.com/socialalgo/HIM.

</details>


### [353] [Understanding Distribution Structure on Calibrated Recommendation Systems](https://arxiv.org/abs/2508.13568)
*Diego Correa da Silva,Denis Robson Dantas Boaventura,Mayki dos Santos Oliveira,Eduardo Ferreira da Silva,Joel Machado Pires,Frederico Araújo Durão*

Main category: cs.IR

TL;DR: The paper addresses problems in traditional recommender systems by creating a calibrated recommendation system that includes less prominent genres from a user's profile while maintaining user preferences.


<details>
  <summary>Details</summary>
Motivation: Traditional recommender systems sometimes ignore less prominent areas of a user's interests, undermining the user experience. The paper seeks to address this flaw by ensuring balanced recommendations.

Method: The authors implement fifteen models based on outlier detection and analyze three datasets from the movie domain to understand how calibrated distributions operate in a high-dimensional genre space.

Result: Results show that outlier detection models provide better insights into user patterns and calibrated systems produce recommendation lists resembling traditional ones in terms of user preference shifting.

Conclusion: Calibrated recommendation systems enhance user experience by including diverse genres without compromising preference flexibility, offering balanced and effective alternatives to traditional systems.

Abstract: Traditional recommender systems aim to generate a recommendation list
comprising the most relevant or similar items to the user's profile. These
approaches can create recommendation lists that omit item genres from the less
prominent areas of a user's profile, thereby undermining the user's experience.
To solve this problem, the calibrated recommendation system provides a
guarantee of including less representative areas in the recommended list. The
calibrated context works with three distributions. The first is from the user's
profile, the second is from the candidate items, and the last is from the
recommendation list. These distributions are G-dimensional, where G is the
total number of genres in the system. This high dimensionality requires a
different evaluation method, considering that traditional recommenders operate
in a one-dimensional data space. In this sense, we implement fifteen models
that help to understand how these distributions are structured. We evaluate the
users' patterns in three datasets from the movie domain. The results indicate
that the models of outlier detection provide a better understanding of the
structures. The calibrated system creates recommendation lists that act
similarly to traditional recommendation lists, allowing users to change their
groups of preferences to the same degree.

</details>


### [354] [UniECS: Unified Multimodal E-Commerce Search Framework with Gated Cross-modal Fusion](https://arxiv.org/abs/2508.13843)
*Zihan Liang,Yufei Ma,ZhiPeng Qian,Huangyu Dai,Zihan Wang,Ben Chen,Chenyi Lei,Yuqing Ding,Han Li*

Main category: cs.IR

TL;DR: The paper introduces UniECS, a unified multimodal e-commerce search framework, addressing task-specific limitations of current systems. It integrates image, text, and hybrid scenarios, supported by a new benchmark, M-BEER.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in e-commerce systems that are restricted to specific tasks and modality pairings, and address the lack of comprehensive benchmarks for unified retrieval.

Method: Proposed a multimodal encoder with adaptive fusion, a training strategy incorporating multiple alignment and contrastive losses, and the creation of the M-BEER benchmark for evaluation.

Result: UniECS outperformed other methods in both experimental (28% gain in R@10) and real-world (increased Click-Through Rate and Revenue) scenarios, with efficient parameter use compared to larger models.

Conclusion: UniECS showcases robust performance in multimodal e-commerce retrieval, demonstrating adaptability, efficiency, and applicability in real-world environments.

Abstract: Current e-commerce multimodal retrieval systems face two key limitations:
they optimize for specific tasks with fixed modality pairings, and lack
comprehensive benchmarks for evaluating unified retrieval approaches. To
address these challenges, we introduce UniECS, a unified multimodal e-commerce
search framework that handles all retrieval scenarios across image, text, and
their combinations. Our work makes three key contributions. First, we propose a
flexible architecture with a novel gated multimodal encoder that uses adaptive
fusion mechanisms. This encoder integrates different modality representations
while handling missing modalities. Second, we develop a comprehensive training
strategy to optimize learning. It combines cross-modal alignment loss (CMAL),
cohesive local alignment loss (CLAL), intra-modal contrastive loss (IMCL), and
adaptive loss weighting. Third, we create M-BEER, a carefully curated
multimodal benchmark containing 50K product pairs for e-commerce search
evaluation. Extensive experiments demonstrate that UniECS consistently
outperforms existing methods across four e-commerce benchmarks with fine-tuning
or zero-shot evaluation. On our M-BEER bench, UniECS achieves substantial
improvements in cross-modal tasks (up to 28\% gain in R@10 for text-to-image
retrieval) while maintaining parameter efficiency (0.2B parameters) compared to
larger models like GME-Qwen2VL (2B) and MM-Embed (8B). Furthermore, we deploy
UniECS in the e-commerce search platform of Kuaishou Inc. across two search
scenarios, achieving notable improvements in Click-Through Rate (+2.74\%) and
Revenue (+8.33\%). The comprehensive evaluation demonstrates the effectiveness
of our approach in both experimental and real-world settings. Corresponding
codes, models and datasets will be made publicly available at
https://github.com/qzp2018/UniECS.

</details>


### [355] [InPars+: Supercharging Synthetic Data Generation for Information Retrieval Systems](https://arxiv.org/abs/2508.13930)
*Matey Krastev,Miklos Hamar,Danilo Toapanta,Jesse Brouwers,Yibin Lei*

Main category: cs.IR

TL;DR: This paper revisits synthetic query generation for Neural Information Retrieval (NIR), improving pipelines with fine-tuned query generation and dynamic prompts, yielding better retrieval performance.


<details>
  <summary>Details</summary>
Motivation: Synthetic query generation is an important part of NIR, and existing methods often face reproducibility challenges and limitations in retrieval effectiveness.

Method: The authors use the InPars toolkit to validate existing pipelines, introduce fine-tuning of query generation via Contrastive Preference Optimization, and leverage dynamic Chain-of-Thought prompts optimized using DSPy.

Result: New extensions reduce filtering requirements while enhancing retrieval performance. Open-source resources are provided for public use.

Conclusion: The enhancements presented bolster the effectiveness and reproducibility of synthetic query generation for NIR and support future research through publicly available resources.

Abstract: This work revisits and extends synthetic query generation pipelines for
Neural Information Retrieval (NIR) by leveraging the InPars Toolkit, a
reproducible, end-to-end framework for generating training data using large
language models (LLMs). We first assess the reproducibility of the original
InPars, InPars-V2, and Promptagator pipelines on the SciFact benchmark and
validate their effectiveness using open-source reranker and generator models.
Building on this foundation, we introduce two key extensions to the pipeline:
(1) fine-tuning a query generator LLM via Contrastive Preference Optimization
(CPO) to improve the signal quality in generated queries, and (2) replacing
static prompt templates with dynamic, Chain-of-Thought (CoT) optimized prompts
using the DSPy framework. Our results show that both extensions reduce the need
for aggressive filtering while improving retrieval performance. All code,
models, and synthetic datasets are publicly released to support further
research at: \href{https://github.com/danilotpnta/IR2-project}{this https URL}.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [356] [A Mechanism for Mutual Fairness in Cooperative Games with Replicable Resources -- Extended Version](https://arxiv.org/abs/2508.13960)
*Björn Filter,Ralf Möller,Özgür Lütfü Özçep*

Main category: cs.GT

TL;DR: The paper addresses fairness in collaborative AI systems by proposing a new mechanism rooted in the Balanced Reciprocity Axiom, ensuring equitable benefits for cooperating agents.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of safety, alignment with human values, and fair reward distribution in cooperative AI systems, particularly in scenarios like collaborative learning.

Method: The authors use cooperative game theory concepts and introduce a generalized fairness framework, including new axioms and mechanisms, to account for infinite replicability like data and models.

Result: The paper presents a mechanism that fulfills the Balanced Reciprocity Axiom, ensuring mutual fairness where each participating agent benefits equally.

Conclusion: The proposed mechanism successfully guarantees fairness in collaborative AI systems, resolving imbalances in reciprocal benefits and avoiding strategic exploitation or unfair outcomes.

Abstract: The latest developments in AI focus on agentic systems where artificial and
human agents cooperate to realize global goals. An example is collaborative
learning, which aims to train a global model based on data from individual
agents. A major challenge in designing such systems is to guarantee safety and
alignment with human values, particularly a fair distribution of rewards upon
achieving the global goal. Cooperative game theory offers useful abstractions
of cooperating agents via value functions, which assign value to each
coalition, and via reward functions. With these, the idea of fair allocation
can be formalized by specifying fairness axioms and designing concrete
mechanisms. Classical cooperative game theory, exemplified by the Shapley
value, does not fully capture scenarios like collaborative learning, as it
assumes nonreplicable resources, whereas data and models can be replicated.
Infinite replicability requires a generalized notion of fairness, formalized
through new axioms and mechanisms. These must address imbalances in reciprocal
benefits among participants, which can lead to strategic exploitation and
unfair allocations. The main contribution of this paper is a mechanism and a
proof that it fulfills the property of mutual fairness, formalized by the
Balanced Reciprocity Axiom. It ensures that, for every pair of players, each
benefits equally from the participation of the other.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [357] [Utilizing the RAIN method and Graph SAGE Model to Identify Effective Drug Combinations for Gastric Neoplasm Treatment](https://arxiv.org/abs/2508.13207)
*S. Z. Pirasteh,Ali A. Kiaei,Mahnaz Bush,Sabra Moghadam,Raha Aghaei,Behnaz Sadeghigol*

Main category: q-bio.QM

TL;DR: The paper presents the RAIN method, integrating AI and network meta-analysis, to identify optimal drug combinations for gastric cancer, improving treatment outcomes.


<details>
  <summary>Details</summary>
Motivation: The high mortality of gastric neoplasm, due to late diagnosis and complications, necessitates effective drug combinations to address disease heterogeneity, reduce resistance, and enhance patient outcomes.

Method: The RAIN method used Graph SAGE to propose drug combinations through a graph model linking drugs, genes, and proteins. NLP and literature review validated the suggestions, with network meta-analysis ensuring efficacy, all implemented in Python.

Result: Oxaliplatin, fluorouracil, and trastuzumab were identified as effective. Fluorouracil's p-value improved significantly in drug combinations, highlighting superior efficacy for the triple regimen.

Conclusion: The RAIN method is a promising AI-driven strategy for identifying effective drug combinations in gastric neoplasm treatment, with potential policy implications.

Abstract: Background: Gastric neoplasm, primarily adenocarcinoma, is an aggressive
cancer with high mortality, often diagnosed late, leading to complications like
metastasis. Effective drug combinations are vital to address disease
heterogeneity, enhance efficacy, reduce resistance, and improve patient
outcomes. Methods: The RAIN method integrated Graph SAGE to propose drug
combinations, using a graph model with p-value-weighted edges connecting drugs,
genes, and proteins. NLP and systematic literature review (PubMed, Scopus,
etc.) validated proposed drugs, followed by network meta-analysis to assess
efficacy, implemented in Python. Results: Oxaliplatin, fluorouracil, and
trastuzumab were identified as effective, supported by 61 studies. Fluorouracil
alone had a p-value of 0.0229, improving to 0.0099 with trastuzumab, and 0.0069
for the triple combination, indicating superior efficacy. Conclusion: The RAIN
method, combining AI and network meta-analysis, effectively identifies optimal
drug combinations for gastric neoplasm, offering a promising strategy to
enhance treatment outcomes and guide health policy.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [358] [The Rise of Generative AI for Metal-Organic Framework Design and Synthesis](https://arxiv.org/abs/2508.13197)
*Chenru Duan,Aditya Nandy,Shyam Chand Pal,Xin Yang,Wenhao Gao,Yuanqi Du,Hendrik Kraß,Yeonghun Kang,Varinia Bernales,Zuyang Ye,Tristan Pyle,Ray Yang,Zeqi Gu,Philippe Schwaller,Shengqian Ma,Shijing Sun,Alán Aspuru-Guzik,Seyed Mohamad Moosavi,Robert Wexler,Zhiling Zheng*

Main category: cond-mat.mtrl-sci

TL;DR: Generative AI is revolutionizing the design of metal-organic frameworks (MOFs) by enabling autonomous and efficient discovery pipelines.


<details>
  <summary>Details</summary>
Motivation: To overcome the labor-intensive process of manually designing and discovering MOF candidates and explore how generative AI can transform this field.

Method: Employ deep learning models, including variational autoencoders, diffusion models, and large language models, in combination with high-throughput computational screening and automated experimentation.

Result: Enhanced efficiency in discovering high-performance MOF materials useful for clean air and energy applications through AI-driven pipelines.

Conclusion: Generative AI introduces a new paradigm in reticular chemistry by accelerating and optimizing MOF discovery, although challenges like synthetic feasibility and dataset diversity remain.

Abstract: Advances in generative artificial intelligence are transforming how
metal-organic frameworks (MOFs) are designed and discovered. This Perspective
introduces the shift from laborious enumeration of MOF candidates to generative
approaches that can autonomously propose and synthesize in the laboratory new
porous reticular structures on demand. We outline the progress of employing
deep learning models, such as variational autoencoders, diffusion models, and
large language model-based agents, that are fueled by the growing amount of
available data from the MOF community and suggest novel crystalline materials
designs. These generative tools can be combined with high-throughput
computational screening and even automated experiments to form accelerated,
closed-loop discovery pipelines. The result is a new paradigm for reticular
chemistry in which AI algorithms more efficiently direct the search for
high-performance MOF materials for clean air and energy applications. Finally,
we highlight remaining challenges such as synthetic feasibility, dataset
diversity, and the need for further integration of domain knowledge.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [359] [Physics-Informed Neural Networks for Programmable Origami Metamaterials with Controlled Deployment](https://arxiv.org/abs/2508.13559)
*Sukheon Kang,Youngkwon Kim,Jinkyu Yang,Seunghwa Ryu*

Main category: cond-mat.soft

TL;DR: The paper introduces a PINN framework enabling accurate prediction and inverse design of conical Kresling origami systems, eliminating the need for pre-collected data.


<details>
  <summary>Details</summary>
Motivation: Origami-based designs offer unique deployable and programmable mechanics but are challenging due to nonlinear mechanics and the need for deployment control.

Method: Physics-informed neural networks with embedded mechanical equilibrium equations predict complete energy landscapes for forward and inverse design without prior data.

Result: Validated by finite element simulations and physical experiments, the framework achieves accurate design of stable states and energy barriers in single and hierarchical systems.

Conclusion: This data-free approach enables advanced energy landscape programming in origami-inspired metamaterials, impacting applications in aerospace, morphing structures, and soft robotics.

Abstract: Origami-inspired structures provide unprecedented opportunities for creating
lightweight, deployable systems with programmable mechanical responses.
However, their design remains challenging due to complex nonlinear mechanics,
multistability, and the need for precise control of deployment forces. Here, we
present a physics-informed neural network (PINN) framework for both forward
prediction and inverse design of conical Kresling origami (CKO) without
requiring pre-collected training data. By embedding mechanical equilibrium
equations directly into the learning process, the model predicts complete
energy landscapes with high accuracy while minimizing non-physical artifacts.
The inverse design routine specifies both target stable-state heights and
separating energy barriers, enabling freeform programming of the entire energy
curve. This capability is extended to hierarchical CKO assemblies, where
sequential layer-by-layer deployment is achieved through programmed barrier
magnitudes. Finite element simulations and experiments on physical prototypes
validate the designed deployment sequences and barrier ratios, confirming the
robustness of the approach. This work establishes a versatile, data-free route
for programming complex mechanical energy landscapes in origami-inspired
metamaterials, offering broad potential for deployable aerospace systems,
morphing structures, and soft robotic actuators.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [360] [PreSem-Surf: RGB-D Surface Reconstruction with Progressive Semantic Modeling and SG-MLP Pre-Rendering Mechanism](https://arxiv.org/abs/2508.13228)
*Yuyan Ye,Hang Xu,Yanghang Huang,Jiali Huang,Qian Weng*

Main category: cs.GR

TL;DR: This paper introduces PreSem-Surf, an improved scene reconstruction method based on Neural Radiance Field (NeRF), which efficiently integrates RGB, depth, and semantic information.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve scene surface reconstruction quality and reduce processing time by leveraging RGB-D data along with semantic information.

Method: The method employs a novel SG-MLP sampling structure combined with PR-MLP for voxel pre-rendering and integrates progressive semantic modeling for precise semantic information extraction.

Result: PreSem-Surf excels in three evaluation metrics (C-L1, F-score, and IoU) and maintains competitive performance in three others (NC, Accuracy, Completeness) across experiments on seven synthetic scenes.

Conclusion: PreSem-Surf is effective and practical for high-quality, efficient scene reconstruction, demonstrating superior integration of RGB, depth, and semantics.

Abstract: This paper proposes PreSem-Surf, an optimized method based on the Neural
Radiance Field (NeRF) framework, capable of reconstructing high-quality scene
surfaces from RGB-D sequences in a short time. The method integrates RGB,
depth, and semantic information to improve reconstruction performance.
Specifically, a novel SG-MLP sampling structure combined with PR-MLP
(Preconditioning Multilayer Perceptron) is introduced for voxel pre-rendering,
allowing the model to capture scene-related information earlier and better
distinguish noise from local details. Furthermore, progressive semantic
modeling is adopted to extract semantic information at increasing levels of
precision, reducing training time while enhancing scene understanding.
Experiments on seven synthetic scenes with six evaluation metrics show that
PreSem-Surf achieves the best performance in C-L1, F-score, and IoU, while
maintaining competitive results in NC, Accuracy, and Completeness,
demonstrating its effectiveness and practical applicability.

</details>


### [361] [Sketch3DVE: Sketch-based 3D-Aware Scene Video Editing](https://arxiv.org/abs/2508.13797)
*Feng-Lin Liu,Shi-Yang Li,Yan-Pei Cao,Hongbo Fu,Lin Gao*

Main category: cs.GR

TL;DR: The paper introduces Sketch3DVE, a method for sketch-based 3D-aware video editing, capable of managing significant viewpoint changes while preserving consistency and realism.


<details>
  <summary>Details</summary>
Motivation: Existing video editing methods struggle with structural edits in 3D scenes, particularly with large viewpoint changes or sparse 2D inputs.

Method: The authors use image editing to modify the first frame and propagate changes, leverage sketching for geometry control, estimate 3D geometry via a dense stereo method, and introduce a point cloud editing method with depth maps. They also propose a 3D-aware mask propagation strategy and utilize a video diffusion model.

Result: Sketch3DVE demonstrates superior performance in maintaining realism, consistency, and detail across edited videos through extensive experiments.

Conclusion: The paper shows that Sketch3DVE effectively overcomes key challenges in structural video editing, providing robust and realistic results even with significant viewpoint variations.

Abstract: Recent video editing methods achieve attractive results in style transfer or
appearance modification. However, editing the structural content of 3D scenes
in videos remains challenging, particularly when dealing with significant
viewpoint changes, such as large camera rotations or zooms. Key challenges
include generating novel view content that remains consistent with the original
video, preserving unedited regions, and translating sparse 2D inputs into
realistic 3D video outputs. To address these issues, we propose Sketch3DVE, a
sketch-based 3D-aware video editing method to enable detailed local
manipulation of videos with significant viewpoint changes. To solve the
challenge posed by sparse inputs, we employ image editing methods to generate
edited results for the first frame, which are then propagated to the remaining
frames of the video. We utilize sketching as an interaction tool for precise
geometry control, while other mask-based image editing methods are also
supported. To handle viewpoint changes, we perform a detailed analysis and
manipulation of the 3D information in the video. Specifically, we utilize a
dense stereo method to estimate a point cloud and the camera parameters of the
input video. We then propose a point cloud editing approach that uses depth
maps to represent the 3D geometry of newly edited components, aligning them
effectively with the original 3D scene. To seamlessly merge the newly edited
content with the original video while preserving the features of unedited
regions, we introduce a 3D-aware mask propagation strategy and employ a video
diffusion model to produce realistic edited videos. Extensive experiments
demonstrate the superiority of Sketch3DVE in video editing. Homepage and code:
http://http://geometrylearning.com/Sketch3DVE/

</details>


### [362] [Is-NeRF: In-scattering Neural Radiance Field for Blurred Images](https://arxiv.org/abs/2508.13808)
*Nan Luo,Chenglin Ye,Jiaxu Li,Gang Liu,Bo Wan,Di Wang,Lupeng Liu,Jun Xiao*

Main category: cs.GR

TL;DR: The paper presents Is-NeRF, a novel deblur neural radiance field model with explicit lightpath modeling to handle motion-blurred images and complex real-world lightpaths effectively. It introduces new volume rendering and adaptive learning techniques.


<details>
  <summary>Details</summary>
Motivation: NeRF struggles with handling complex lightpath scenarios, especially in the presence of motion blur, leading to geometric ambiguities during training. The goal is to overcome these weaknesses.

Method: The authors propose Is-NeRF, which uses a scattering-aware volume rendering pipeline unifying six common light propagation phenomena. The system includes adaptive learning for optimizing scattering directions and intervals, enabling better scene representation.

Result: Is-NeRF surpasses state-of-the-art methods by generating high-fidelity images with accurate geometry, even in challenging real-world conditions involving blur and sophisticated lighting.

Conclusion: Is-NeRF successfully addresses limitations in traditional NeRF methods by enhancing scene representation and resilience to motion blur, leading to improved rendering accuracy in complex environments.

Abstract: Neural Radiance Fields (NeRF) has gained significant attention for its
prominent implicit 3D representation and realistic novel view synthesis
capabilities. Available works unexceptionally employ straight-line volume
rendering, which struggles to handle sophisticated lightpath scenarios and
introduces geometric ambiguities during training, particularly evident when
processing motion-blurred images. To address these challenges, this work
proposes a novel deblur neural radiance field, Is-NeRF, featuring explicit
lightpath modeling in real-world environments. By unifying six common light
propagation phenomena through an in-scattering representation, we establish a
new scattering-aware volume rendering pipeline adaptable to complex lightpaths.
Additionally, we introduce an adaptive learning strategy that enables
autonomous determining of scattering directions and sampling intervals to
capture finer object details. The proposed network jointly optimizes NeRF
parameters, scattering parameters, and camera motions to recover fine-grained
scene representations from blurry images. Comprehensive evaluations demonstrate
that it effectively handles complex real-world scenarios, outperforming
state-of-the-art approaches in generating high-fidelity images with accurate
geometric details.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [363] [Large Language Models as Visualization Agents for Immersive Binary Reverse Engineering](https://arxiv.org/abs/2508.13413)
*Dennis Brown,Samuel Mulder*

Main category: cs.HC

TL;DR: This paper integrates a large language model (LLM) into a VR system to support binary reverse engineering (RE) tasks, such as visualization and analysis, and evaluates its effectiveness, identifying both potential and challenges.


<details>
  <summary>Details</summary>
Motivation: To leverage virtual reality and large language models to reduce cognitive complexity in binary reverse engineering by improving memory, hypothesis testing, and visual organization.

Method: Extending a VR environment with an LLM agent that interacts with binary analysis tools, answers technical questions, and dynamically generates 3D visualizations for analyst tasks. Evaluated through a pilot study.

Result: Pilot study findings indicate LLMs can create meaningful 3D call graphs for small programs aligned with design principles, but output quality is inconsistent.

Conclusion: LLMs have potential as visualization agents in VR for RE tasks, but questions remain about their ability to autonomously produce 3D representations that consistently adhere to cognitive design principles.

Abstract: Immersive virtual reality (VR) offers affordances that may reduce cognitive
complexity in binary reverse engineering (RE), enabling embodied and external
cognition to augment the RE process through enhancing memory, hypothesis
testing, and visual organization. In prior work, we applied a cognitive systems
engineering approach to identify an initial set of affordances and implemented
a VR environment to support RE through spatial persistence and interactivity.
In this work, we extend that platform with an integrated large language model
(LLM) agent capable of querying binary analysis tools, answering technical
questions, and dynamically generating immersive 3D visualizations in alignment
with analyst tasks. We describe the system architecture and our evaluation
process and results. Our pilot study shows that while LLMs can generate
meaningful 3D call graphs (for small programs) that align with design
principles, output quality varies widely. This work raises open questions about
the potential for LLMs to function as visualization agents, constructing 3D
representations that reflect cognitive design principles without explicit
training.

</details>


### [364] [Prompt Orchestration Markup Language](https://arxiv.org/abs/2508.13948)
*Yuge Zhang,Nan Chen,Jiahang Xu,Yuqing Yang*

Main category: cs.HC

TL;DR: The paper introduces POML, a markup language to streamline and enhance prompt orchestration for Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Current LLM prompting practices face problems such as lack of structure, difficulties in integrating diverse data types, and format sensitivity.

Method: The paper presents POML, which uses a component-based structure, specialized tags, CSS-like styling, templating, and developer tools (IDE, SDKs) to address these challenges.

Result: The authors demonstrated POML's effectiveness through two case studies, showing improved integration in complex applications and enhanced accuracy in tasks like TableQA. A user study also validated its usability in real-world development.

Conclusion: POML simplifies LLM prompt orchestration, improves performance and integration, and provides developers with a robust toolkit to address existing limitations.

Abstract: Large Language Models (LLMs) require sophisticated prompting, yet current
practices face challenges in structure, data integration, format sensitivity,
and tooling. Existing methods lack comprehensive solutions for organizing
complex prompts involving diverse data types (documents, tables, images) or
managing presentation variations systematically. To address these gaps, we
introduce POML (Prompt Orchestration Markup Language). POML employs
component-based markup for logical structure (roles, tasks, examples),
specialized tags for seamless data integration, and a CSS-like styling system
to decouple content from presentation, reducing formatting sensitivity. It
includes templating for dynamic prompts and a comprehensive developer toolkit
(IDE support, SDKs) to improve version control and collaboration. We validate
POML through two case studies demonstrating its impact on complex application
integration (PomLink) and accuracy performance (TableQA), as well as a user
study assessing its effectiveness in real-world development scenarios.

</details>


### [365] [Learning to Use AI for Learning: How Can We Effectively Teach and Measure Prompting Literacy for K-12 Students?](https://arxiv.org/abs/2508.13962)
*Ruiwei Xiao,Xinying Hou,Ying-Jui Tseng,Hsuan Nieu,Guanze Liao,John Stamper,Kenneth R. Koedinger*

Main category: cs.HC

TL;DR: The paper introduces a Large-Language Model-based educational module to develop prompting literacy in secondary school students, focusing on responsible AI interaction and conducted classroom studies showing positive outcomes.


<details>
  <summary>Details</summary>
Motivation: The growing integration of AI in daily life increases the necessity for young generations to responsibly interact with, evaluate, and collaborate with AI systems.

Method: The authors implemented an LLM-based instructional module with scenario-based practice activities, deployed it in 11 classrooms, and evaluated auto-grading capabilities, student performance, and material effectiveness.

Result: Findings revealed effective auto-grading of prompts, improved student prompting skills, and better student attitudes toward AI usage in learning. Study results improved assessment item precision and revealed True/False and open-ended questions as effective measures.

Conclusion: The module demonstrates potential for broader implementation to enhance student AI literacy while calling for larger studies to validate learning outcomes and refine assessments.

Abstract: As Artificial Intelligence (AI) becomes increasingly integrated into daily
life, there is a growing need to equip the next generation with the ability to
apply, interact with, evaluate, and collaborate with AI systems responsibly.
Prior research highlights the urgent demand from K-12 educators to teach
students the ethical and effective use of AI for learning. To address this
need, we designed an Large-Language Model (LLM)-based module to teach prompting
literacy. This includes scenario-based deliberate practice activities with
direct interaction with intelligent LLM agents, aiming to foster secondary
school students' responsible engagement with AI chatbots. We conducted two
iterations of classroom deployment in 11 authentic secondary education
classrooms, and evaluated 1) AI-based auto-grader's capability; 2) students'
prompting performance and confidence changes towards using AI for learning; and
3) the quality of learning and assessment materials. Results indicated that the
AI-based auto-grader could grade student-written prompts with satisfactory
quality. In addition, the instructional materials supported students in
improving their prompting skills through practice and led to positive shifts in
their perceptions of using AI for learning. Furthermore, data from Study 1
informed assessment revisions in Study 2. Analyses of item difficulty and
discrimination in Study 2 showed that True/False and open-ended questions could
measure prompting literacy more effectively than multiple-choice questions for
our target learners. These promising outcomes highlight the potential for
broader deployment and highlight the need for broader studies to assess
learning effectiveness and assessment design.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [366] [DegDiT: Controllable Audio Generation with Dynamic Event Graph Guided Diffusion Transformer](https://arxiv.org/abs/2508.13786)
*Yisu Liu,Chenxing Li,Wanqian Zhang,Wenfu Wang,Meng Yu,Ruibo Fu,Zheng Lin,Weiping Wang,Dong Yu*

Main category: cs.SD

TL;DR: This paper presents DegDiT, a novel framework for generating audio from textual descriptions while meeting user-defined temporal constraints, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with balancing temporal accuracy, scalability, and efficiency in controllable text-to-audio generation.

Method: The proposed method, DegDiT, uses structured dynamic event graphs and a graph transformer to encode textual events. It also introduces a data selection pipeline for diverse training and a consensus optimization approach for reward signal alignment.

Result: DegDiT outperforms existing methods in both objective and subjective evaluations on multiple datasets.

Conclusion: DegDiT effectively addresses key challenges in controllable text-to-audio generation, delivering high-quality audio aligned with user-specified constraints.

Abstract: Controllable text-to-audio generation aims to synthesize audio from textual
descriptions while satisfying user-specified constraints, including event
types, temporal sequences, and onset and offset timestamps. This enables
precise control over both the content and temporal structure of the generated
audio. Despite recent progress, existing methods still face inherent trade-offs
among accurate temporal localization, open-vocabulary scalability, and
practical efficiency. To address these challenges, we propose DegDiT, a novel
dynamic event graph-guided diffusion transformer framework for open-vocabulary
controllable audio generation. DegDiT encodes the events in the description as
structured dynamic graphs. The nodes in each graph are designed to represent
three aspects: semantic features, temporal attributes, and inter-event
connections. A graph transformer is employed to integrate these nodes and
produce contextualized event embeddings that serve as guidance for the
diffusion model. To ensure high-quality and diverse training data, we introduce
a quality-balanced data selection pipeline that combines hierarchical event
annotation with multi-criteria quality scoring, resulting in a curated dataset
with semantic diversity. Furthermore, we present consensus preference
optimization, facilitating audio generation through consensus among multiple
reward signals. Extensive experiments on AudioCondition, DESED, and AudioTime
datasets demonstrate that DegDiT achieves state-of-the-art performances across
a variety of objective and subjective evaluation metrics.

</details>


### [367] [Evaluating Identity Leakage in Speaker De-Identification Systems](https://arxiv.org/abs/2508.14012)
*Seungmin Seo,Oleg Aulov,Afzal Godil,Kevin Mangold*

Main category: cs.SD

TL;DR: This paper highlights that current speaker de-identification systems still leak identifiable information, based on a benchmark using three complementary error metrics.


<details>
  <summary>Details</summary>
Motivation: The goal was to evaluate and highlight the effectiveness, or lack thereof, of current speaker de-identification systems in protecting speaker privacy.

Method: The authors introduced a benchmark with three complementary metrics to measure identity leakage: equal error rate, cumulative match characteristic hit rate, and embedding-space similarities.

Result: The evaluation found that all state-of-the-art systems still leak identity information, with the best-performing system barely exceeding random guessing, and the worst system achieving a 45% hit rate within the top 50 candidates.

Conclusion: Current speaker de-identification technologies exhibit persistent privacy risks, emphasizing the need for better systems to protect sensitive information.

Abstract: Speaker de-identification aims to conceal a speaker's identity while
preserving intelligibility of the underlying speech. We introduce a benchmark
that quantifies residual identity leakage with three complementary error rates:
equal error rate, cumulative match characteristic hit rate, and embedding-space
similarity measured via canonical correlation analysis and Procrustes analysis.
Evaluation results reveal that all state-of-the-art speaker de-identification
systems leak identity information. The highest performing system in our
evaluation performs only slightly better than random guessing, while the lowest
performing system achieves a 45% hit rate within the top 50 candidates based on
CMC. These findings highlight persistent privacy risks in current speaker
de-identification technologies.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [368] [Query Logs Analytics: A Aystematic Literature Review](https://arxiv.org/abs/2508.13949)
*Dihia Lanasri*

Main category: cs.DB

TL;DR: This paper surveys log usage across databases, data warehouses, websites, and knowledge graphs, identifying gaps and opportunities in the field.


<details>
  <summary>Details</summary>
Motivation: The increasing collection of logs from user interactions across digital platforms offers immense value across various applications. However, research on log usage is fragmented, and there is no unified study consolidating current efforts.

Method: The study systematically reviewed over 300 publications to examine the structural and functional characteristics of logs, standard pipelines for their usage, and constraints or non-functional requirements guiding their exploitation.

Result: The survey found a lack of end-to-end approaches, no standardization in log usage pipelines, and common structural elements shared across different log types.

Conclusion: This comprehensive overview consolidates knowledge, identifies research gaps, and highlights opportunities, particularly in the exploitation of knowledge graph logs, aiming to guide future research and practice.

Abstract: In the digital era, user interactions with various resources such as
databases, data warehouses, websites, and knowledge graphs (KGs) are
increasingly mediated through digital platforms. These interactions leave
behind digital traces, systematically captured in the form of logs. Logs, when
effectively exploited, provide high value across industry and academia,
supporting critical services (e.g., recovery and security), user-centric
applications (e.g., recommender systems), and quality-of-service improvements
(e.g., performance optimization). Despite their importance, research on log
usage remains fragmented across domains, and no comprehensive study currently
consolidates existing efforts. This paper presents a systematic survey of log
usage, focusing on Database (DB), Data Warehouse (DW), Web, and KG logs. More
than 300 publications were analyzed to address three central questions: (1) do
different types of logs share common structural and functional characteristics?
(2) are there standard pipelines for their usage? (3) which constraints and
non-functional requirements (NFRs) guide their exploitation?. The survey
reveals a limited number of end-to-end approaches, the absence of
standardization across log usage pipelines, and the existence of shared
structural elements among different types of logs. By consolidating existing
knowledge, identifying gaps, and highlighting opportunities, this survey
provides researchers and practitioners with a comprehensive overview of log
usage and sheds light on promising directions for future research, particularly
regarding the exploitation and democratization of KG logs.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [369] [Diffusion-Driven High-Dimensional Variable Selection](https://arxiv.org/abs/2508.13890)
*Minjie Wang,Xiaotong Shen,Wei Pan*

Main category: stat.ME

TL;DR: The paper proposes a resample-aggregate framework using diffusion models for improved variable selection in high-dimensional, correlated data, showing consistent gains in stability scores and selection efficiency.


<details>
  <summary>Details</summary>
Motivation: Variable selection in high-dimensional and highly correlated data often results in unstable and unreliable models, highlighting the need for more robust methodologies.

Method: The framework uses diffusion models to generate synthetic datasets, followed by applying selectors like lasso or SCAD across replicas, producing stable predictor subsets with calibrated stability scores.

Result: Simulation results demonstrate increased true-positive rates and decreased false-discovery proportions, outperforming established methods like lasso, stability selection, and knockoff baselines.

Conclusion: Combining diffusion-based data augmentation with aggregation improves the robustness and accuracy of variable selection, expanding the range of methods for rigorous scientific analysis.

Abstract: Variable selection for high-dimensional, highly correlated data has long been
a challenging problem, often yielding unstable and unreliable models. We
propose a resample-aggregate framework that exploits diffusion models' ability
to generate high-fidelity synthetic data. Specifically, we draw multiple
pseudo-data sets from a diffusion model fitted to the original data, apply any
off-the-shelf selector (e.g., lasso or SCAD), and store the resulting inclusion
indicators and coefficients. Aggregating across replicas produces a stable
subset of predictors with calibrated stability scores for variable selection.
Theoretically, we show that the proposed method is selection consistent under
mild assumptions. Because the generative model imports knowledge from large
pre-trained weights, the procedure naturally benefits from transfer learning,
boosting power when the observed sample is small or noisy. We also extend the
framework of aggregating synthetic data to other model selection problems,
including graphical model selection, and statistical inference that supports
valid confidence intervals and hypothesis tests. Extensive simulations show
consistent gains over the lasso, stability selection, and knockoff baselines,
especially when predictors are strongly correlated, achieving higher
true-positive rates and lower false-discovery proportions. By coupling
diffusion-based data augmentation with principled aggregation, our method
advances variable selection methodology and broadens the toolkit for
interpretable, statistically rigorous analysis in complex scientific
applications.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [370] [AlphaX: An AI-Based Value Investing Strategy for the Brazilian Stock Market](https://arxiv.org/abs/2508.13429)
*Paulo André Lima de Castro*

Main category: q-fin.CP

TL;DR: This paper introduces an AI-driven trading strategy inspired by Value Investing, named AlphaX, which outperformed Brazilian market benchmarks and commonly used technical indicators while addressing biases and overfitting in backtesting.


<details>
  <summary>Details</summary>
Motivation: To address the gap between strong simulated trading strategy performances in backtesting and their diminished real-world effectiveness, particularly focusing on overcoming biases like lookahead bias and leveraging the Value Investing paradigm.

Method: The approach utilizes an AI-based Value Investing-inspired strategy, focusing on controlling for biases during computational simulations to reduce overfitting and improve real-world applicability.

Result: The AlphaX strategy outperformed Brazilian market benchmarks and widely used technical indicators (RSI and MFI) with statistically significant results.

Conclusion: AlphaX is a promising AI-driven trading strategy that addresses key challenges in autonomous financial trading, and its success opens new directions for integrating qualitative analysis and emerging technologies in financial AI frameworks.

Abstract: Autonomous trading strategies have been a subject of research within the
field of artificial intelligence (AI) for aconsiderable period. Various AI
techniques have been explored to develop autonomous agents capable of trading
financial assets. These approaches encompass traditional methods such as neural
networks, fuzzy logic, and reinforcement learning, as well as more recent
advancements, including deep neural networks and deep reinforcement learning.
Many developers report success in creating strategies that exhibit strong
performance during simulations using historical price data, a process commonly
referred to as backtesting. However, when these strategies are deployed in real
markets, their performance often deteriorates, particularly in terms of
risk-adjusted returns. In this study, we propose an AI-based strategy inspired
by a classical investment paradigm: Value Investing. Financial AI models are
highly susceptible to lookahead bias and other forms of bias that can
significantly inflate performance in backtesting compared to live trading
conditions. To address this issue, we conducted a series of computational
simulations while controlling for these biases, thereby reducing the risk of
overfitting. Our results indicate that the proposed approach outperforms major
Brazilian market benchmarks. Moreover, the strategy, named AlphaX, demonstrated
superior performance relative to widely used technical indicators such as the
Relative Strength Index (RSI) and Money Flow Index (MFI), with statistically
significant results. Finally, we discuss several open challenges and highlight
emerging technologies in qualitative analysis that may contribute to the
development of a comprehensive AI-based Value Investing framework in the future

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [371] [PennyLane-Lightning MPI: A massively scalable quantum circuit simulator based on distributed computing in CPU clusters](https://arxiv.org/abs/2508.13615)
*Ji-Hoon Kang,Hoon Ryu*

Main category: quant-ph

TL;DR: The paper introduces PennyLane-Lightning MPI, an MPI-based extension for scalable quantum circuit simulations, enabling operations on up to 41 qubits across distributed systems.


<details>
  <summary>Details</summary>
Motivation: To overcome computational challenges in quantum circuit simulations due to the exponential growth of quantum state spaces with increasing qubit size.

Method: An MPI-based extension employing an index-dependent, gate-specific parallelization strategy to distribute quantum state vectors and gate operations over distributed-memory systems.

Result: The proposed method demonstrates performance advantages and excellent scalability, successfully simulating up to 41-qubit quantum circuits with hundreds of thousands of parallel processes.

Conclusion: PennyLane-Lightning MPI integrates seamlessly with the PennyLane framework, supporting high-performance simulations on multi-core CPU clusters and extending the ecosystem for quantum computing developments in Korea.

Abstract: Quantum circuit simulations play a critical role in bridging the gap between
theoretical quantum algorithms and their practical realization on physical
quantum hardware, yet they face computational challenges due to the exponential
growth of quantum state spaces with increasing qubit size. This work presents
PennyLane-Lightning MPI, an MPI-based extension of the PennyLane-Lightning
suite, developed to enable scalable quantum circuit simulations through
parallelization of quantum state vectors and gate operations across
distributed-memory systems. The core of this implementation is an
index-dependent, gate-specific parallelization strategy, which fully exploits
the characteristic of individual gates as well as the locality of computation
associated with qubit indices in partitioned state vectors. Benchmarking tests
with single gates and well-designed quantum circuits show that the present
method offers advantages in performance over general methods based on unitary
matrix operations and exhibits excellent scalability, supporting simulations of
up to 41-qubit with hundreds of thousands of parallel processes. Being equipped
with a Python plug-in for seamless integration to the PennyLane framework, this
work contributes to extending the PennyLane ecosystem by enabling
high-performance quantum simulations in standard multi-core CPU clusters with
no library-specific requirements, providing a back-end resource for the
cloud-based service framework of quantum computing that is under development in
the Republic of Korea.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [372] [End-to-End Audio-Visual Learning for Cochlear Implant Sound Coding in Noisy Environments](https://arxiv.org/abs/2508.13576)
*Meng-Ping Lin,Enoch Hsin-Ho Huang,Shao-Yi Chien,Yu Tsao*

Main category: eess.AS

TL;DR: The paper presents AVSE-ECS, an innovative cochlear implant system that integrates deep learning-based audio-visual speech enhancement to improve speech intelligibility in noisy conditions.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current cochlear implant systems in noisy and reverberant environments by leveraging advances in deep learning and multimodal speech processing.

Method: An end-to-end cochlear implant system, AVSE-ECS, is introduced, combining an audio-visual speech enhancement (AVSE) model with the deep-learning-based ElectrodeNet-CS. A joint training approach is employed for integration.

Result: Experimental results show that AVSE-ECS performs better than previous strategies in noisy settings, delivering improved objective speech intelligibility scores.

Conclusion: The study demonstrates the viability of integrating deep learning and audio-visual cues for end-to-end cochlear implant systems, offering enhanced performance in challenging auditory environments.

Abstract: The cochlear implant (CI) is a remarkable biomedical device that successfully
enables individuals with severe-to-profound hearing loss to perceive sound by
converting speech into electrical stimulation signals. Despite advancements in
the performance of recent CI systems, speech comprehension in noisy or
reverberant conditions remains a challenge. Recent and ongoing developments in
deep learning reveal promising opportunities for enhancing CI sound coding
capabilities, not only through replicating traditional signal processing
methods with neural networks, but also through integrating visual cues as
auxiliary data for multimodal speech processing. Therefore, this paper
introduces a novel noise-suppressing CI system, AVSE-ECS, which utilizes an
audio-visual speech enhancement (AVSE) model as a pre-processing module for the
deep-learning-based ElectrodeNet-CS (ECS) sound coding strategy. Specifically,
a joint training approach is applied to model AVSE-ECS, an end-to-end CI
system. Experimental results indicate that the proposed method outperforms the
previous ECS strategy in noisy conditions, with improved objective speech
intelligibility scores. The methods and findings in this study demonstrate the
feasibility and potential of using deep learning to integrate the AVSE module
into an end-to-end CI system

</details>


<div id='q-bio.MN'></div>

# q-bio.MN [[Back]](#toc)

### [373] [Modeling GRNs with a Probabilistic Categorical Framework](https://arxiv.org/abs/2508.13208)
*Yiyang Jia,Zheng Wei,Zheng Yang,Guohong Peng*

Main category: q-bio.MN

TL;DR: This paper presents a novel framework, PC-GRN, to improve understanding and modeling of Gene Regulatory Networks (GRNs) using category theory, Bayesian Typed Petri Nets, and a generative Bayesian inference engine.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with capturing the complex regulation logic and managing uncertainties in GRN structure and parameters.

Method: The authors combine category theory for pathway modularity, Bayesian Typed Petri Nets for stochastic modeling, and a generative Bayesian inference engine utilizing GFlowNet and HyperNetwork.

Result: The framework learns full posterior distributions for GRN models directly from data, offering interpretable and uncertainty-aware results.

Conclusion: PC-GRN introduces a robust, interpretable, and uncertainty-sensitive approach for GRNs, enhancing systems biology research.

Abstract: Understanding the complex and stochastic nature of Gene Regulatory Networks
(GRNs) remains a central challenge in systems biology. Existing modeling
paradigms often struggle to effectively capture the intricate, multi-factor
regulatory logic and to rigorously manage the dual uncertainties of network
structure and kinetic parameters. In response, this work introduces the
Probabilistic Categorical GRN(PC-GRN) framework. It is a novel theoretical
approach founded on the synergistic integration of three core methodologies.
Firstly, category theory provides a formal language for the modularity and
composition of regulatory pathways. Secondly, Bayesian Typed Petri Nets (BTPNs)
serve as an interpretable,mechanistic substrate for modeling stochastic
cellular processes, with kinetic parameters themselves represented as
probability distributions. The central innovation of PC-GRN is its end-to-end
generative Bayesian inference engine, which learns a full posterior
distribution over BTPN models (P (G, {\Theta}|D)) directly from data. This is
achieved by the novel interplay of a GFlowNet, which learns a policy to sample
network topologies, and a HyperNetwork, which performs amortized inference to
predict their corresponding parameter distributions. The resulting framework
provides a mathematically rigorous, biologically interpretable, and
uncertainty-aware representation of GRNs, advancing predictive modeling and
systems-level analysis.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [374] [Using Artificial Intuition in Distinct, Minimalist Classification of Scientific Abstracts for Management of Technology Portfolios](https://arxiv.org/abs/2508.13182)
*Prateek Ranka,Fred Morstatter,Andrea Belz,Alexandra Graddy-Reed*

Main category: cs.DL

TL;DR: The paper discusses using Large Language Models (LLMs) to classify scientific abstracts with improved automation and applicability, aiming to replicate expert labeling methods.


<details>
  <summary>Details</summary>
Motivation: Automating the classification of scientific abstracts is challenging due to sparse contextual clues and overlapping labels, which necessitate replication of expert-level accuracy.

Method: The authors propose using LLMs under the concept of "artificial intuition" to generate metadata and labels for abstracts, testing them on datasets from the United States and Chinese science foundations.

Result: The proposed method successfully demonstrates the feasibility of employing LLMs for labeling and analyzing metadata associated with scientific abstracts.

Conclusion: The approach holds potential for strategic applications such as research portfolio management and technology scouting by enhancing automation in abstract classification.

Abstract: Classification of scientific abstracts is useful for strategic activities but
challenging to automate because the sparse text provides few contextual clues.
Metadata associated with the scientific publication can be used to improve
performance but still often requires a semi-supervised setting. Moreover, such
schemes may generate labels that lack distinction -- namely, they overlap and
thus do not uniquely define the abstract. In contrast, experts label and sort
these texts with ease. Here we describe an application of a process we call
artificial intuition to replicate the expert's approach, using a Large Language
Model (LLM) to generate metadata. We use publicly available abstracts from the
United States National Science Foundation to create a set of labels, and then
we test this on a set of abstracts from the Chinese National Natural Science
Foundation to examine funding trends. We demonstrate the feasibility of this
method for research portfolio management, technology scouting, and other
strategic activities.

</details>


### [375] [The Role of AI in Facilitating Interdisciplinary Collaboration: Evidence from AlphaFold](https://arxiv.org/abs/2508.13234)
*Naixuan Zhao,Chunli Wei,Xinyan Zhang,Jiang Li*

Main category: cs.DL

TL;DR: The paper investigates AI's role in interdisciplinary collaboration, finding that AlphaFold has minimal impact on bridging disciplinary divides.


<details>
  <summary>Details</summary>
Motivation: To assess the actual impact of AI on fostering collaboration across different scientific disciplines, focusing on AlphaFold's effect on structural biology.

Method: The study used bibliometric analysis and causal inference on 1,247 AlphaFold-related papers and 7,700 authors from Scopus to compare adopters and non-adopters in interdisciplinary collaborations.

Result: AlphaFold increased collaborations between structural biology and computer science by only 0.48%, with no measurable effects on collaboration with other disciplines.

Conclusion: AI technologies like AlphaFold have limited ability to significantly foster interdisciplinary collaboration, contradicting general assumptions.

Abstract: The acceleration of artificial intelligence (AI) in science is recognized and
many scholars have begun to explore its role in interdisciplinary
collaboration. However, the mechanisms and extent of this impact are still
unclear. This study, using AlphaFold's impact on structural biologists,
examines how AI technologies influence interdisciplinary collaborative
patterns. By analyzing 1,247 AlphaFold-related papers and 7,700 authors from
Scopus, we employ bibliometric analysis and causal inference to compare
interdisciplinary collaboration between AlphaFold adopters and non-adopters.
Contrary to the widespread belief that AI facilitates interdisciplinary
collaboration, our findings show that AlphaFold increased structural
biology-computer science collaborations by just 0.48%, with no measurable
effect on other disciplines. Specifically, AI creates interdisciplinary
collaboration demands with specific disciplines due to its technical
characteristics, but this demand is weakened by technological democratization
and other factors. These findings demonstrate that artificial intelligence (AI)
alone has limited efficacy in bridging disciplinary divides or fostering
meaningful interdisciplinary collaboration.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [376] [A Screw Approach to the Approximation of the Local Geometry of the Configuration Space and of the set of Configurations of Certain Rank of Lower Pair Linkages](https://arxiv.org/abs/2508.13802)
*Andreas Mueller*

Main category: math.DG

TL;DR: The paper introduces a new method for analyzing the local mobility of multi-loop mechanisms using Taylor series expansion to address limitations of traditional methods relying on smooth motion assumptions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome limitations of existing mobility analysis methods, which assume smooth motions and struggle with singularities in configuration space (c-space).

Method: The authors propose a recursive algebraic expression using higher-order Taylor series expansions of geometrical constraint mappings. This method enables a deeper local analysis of singularities in c-space.

Result: The proposed method successfully approximates the c-space and configurations with certain rank using algebraic varieties, demonstrated on a planar 4-bar linkage and a planar three-loop linkage.

Conclusion: The methodology effectively analyzes complex singularities like bifurcations and cusps in c-space, extending the capabilities of higher-order local analysis for mechanics.

Abstract: A motion of a mechanism is a curve in its configuration space (c-space).
Singularities of the c-space are kinematic singularities of the mechanism. Any
mobility analysis of a particular mechanism amounts to investigating the
c-space geometry at a given configuration. A higher-order analysis is necessary
to determine the finite mobility. To this end, past research lead to approaches
using higher-order time derivatives of loop closure constraints assuming
(implicitly) that all possible motions are smooth. This continuity assumption
limits the generality of these methods. In this paper an approach to the
higher-order local mobility analysis of lower pair multi-loop linkages is
presented. This is based on a higher-order Taylor series expansion of the
geometric constraint mapping, for which a recursive algebraic expression in
terms of joint screws is presented. An exhaustive local analysis includes
analysis of the set of constraint singularities (configurations where the
constraint Jacobian has certain corank). A local approximation of the set of
configurations with certain rank is presented, along with an explicit
expression for the differentials of Jacobian minors in terms of instantaneous
joint screws. The c-space and the set of points of certain corank are therewith
locally approximated by an algebraic variety determined algebraically from the
mechanism's screw system. Results are shown for a simple planar 4-bar linkage,
which exhibits a bifurcation singularity, and for a planar three-loop linkage
exhibiting a cusp in c-space. The latter cannot be treated by the higher-order
local analysis methods proposed in the literature.

</details>
